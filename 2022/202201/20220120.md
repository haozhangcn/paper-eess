# ArXiv eess --Thu, 20 Jan 2022
### 1.Learned Cone-Beam CT Reconstruction Using Neural Ordinary Differential Equations  [ :arrow_down: ](https://arxiv.org/pdf/2201.07562.pdf)
>  Learned iterative reconstruction algorithms for inverse problems offer the flexibility to combine analytical knowledge about the problem with modules learned from data. This way, they achieve high reconstruction performance while ensuring consistency with the measured data. In computed tomography, extending such approaches from 2D fan-beam to 3D cone-beam data is challenging due to the prohibitively high GPU memory that would be needed to train such models. This paper proposes to use neural ordinary differential equations to solve the reconstruction problem in a residual formulation via numerical integration. For training, there is no need to backpropagate through several unrolled network blocks nor through the internals of the solver. Instead, the gradients are obtained very memory-efficiently in the neural ODE setting allowing for training on a single consumer graphics card. The method is able to reduce the root mean squared error by over 30% compared to the best performing classical iterative reconstruction algorithm and produces high quality cone-beam reconstructions even in a sparse view scenario.      
### 2.Consensus of Network of Homogeneous Agents with General Linear Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2201.07532.pdf)
>  This work addresses the synchronization/consensus problem for identical multi-agent system (MAS) where the dynamics of each agent is a general linear system. The focus is on the case where the agents have possibly repeated unstable eigenvalues since those with stable eigenvalues are well-known. It proposes the use of a generalized gain matrix and shows that under reasonable conditions, synchronization/consensus is achieved when the gain is sufficiently large. When the communication network is non time-varying and the dynamics of the agent have eigenvalues in the closed left-half complex plane, the proposed approach includes the standard consensus result. Several connections to standard results are also highlighted. The existences of such gain matrices for consensus are shown under three communication settings: fixed graph, switching among undirected and connected graphs and switching among directed and connected graphs. An adaptive scheme to realize the gain matrix for the case of undirected and fixed network is also shown and illustrated via an example.      
### 3.Damping Identification of an Operational Offshore Wind Turbine using Enhanced Kalman filter-based Subspace Identification  [ :arrow_down: ](https://arxiv.org/pdf/2201.07531.pdf)
>  Operational Modal Analysis (OMA) provides essential insights into the structural dynamics of an Offshore Wind Turbine (OWT). In these dynamics, damping is considered an especially important parameter as it governs the magnitude of the response at the natural frequencies. Violation of the stationary white noise excitation requirement of classical OMA algorithms has troubled the identification of operational OWTs due to harmonic excitation caused by rotor rotation. Recently, a novel algorithm was presented that mitigates harmonics by estimating a harmonic subsignal using a Kalman filter and orthogonally removing this signal from the response signal, after which the Stochastic Subspace Identification algorithm is used to identify the system. Although promising results are achieved using this novel algorithm, several shortcomings are still present like the numerical instability of the conventional Kalman filter and the inability to use large or multiple datasets. This paper addresses these shortcomings and applies an enhanced version to a multi-megawatt operational OWT using an economical sensor setup with two accelerometer levels. The algorithm yielded excellent results for the first three tower bending modes with low variance. A comparison of these results against the established time-domain harmonics-mitigating algorithm, Modified LSCE, and the frequency-domain PolyMAX algorithm demonstrated strong agreement in results.      
### 4.Gaussian Process Position-Dependent Feedforward: With Application to a Wire Bonder  [ :arrow_down: ](https://arxiv.org/pdf/2201.07511.pdf)
>  Mechatronic systems have increasingly stringent performance requirements for motion control, leading to a situation where many factors, such as position-dependency, cannot be neglected in feedforward control. The aim of this paper is to compensate for position-dependent effects by modeling feedforward parameters as a function of position. A framework to model and identify feedforward parameters as a continuous function of position is developed by combining Gaussian processes and feedforward parameter learning techniques. The framework results in a fully data-driven approach, which can be readily implemented for industrial control applications. The framework is experimentally validated and shows a significant performance increase on a commercial wire bonder.      
### 5.Rate-Splitting assisted Massive Machine-Type Communications in Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2201.07508.pdf)
>  This letter focuses on integrating rate-splitting multiple-access (RSMA) with time-division-duplex Cell-free Massive MIMO (multiple-input multiple-output) for massive machine-type communications. Due to the large number of devices, their sporadic access behaviour and limited coherence interval, we assume a random access strategy with all active devices utilizing the same pilot for uplink channel estimation. This gives rise to a highly pilot-contaminated scenario, which inevitably deteriorates channel estimates. Motivated by the robustness of RSMA towards imperfect channel state information, we propose a novel RSMA-assisted downlink transmission framework for cell-free massive MIMO. On the basis of the downlink achievable spectral efficiency of the common and private streams, we devise a heuristic common precoder design and propose a novel max-min power control method for the proposed RSMA-assisted scheme. Numerical results show that RSMA effectively mitigates the effect of pilot contamination in the downlink and achieves a significant performance gain over a conventional cell-free massive MIMO network.      
### 6.Cortical lesions, central vein sign, and paramagnetic rim lesions in multiple sclerosis: emerging machine learning techniques and future avenues  [ :arrow_down: ](https://arxiv.org/pdf/2201.07463.pdf)
>  The current multiple sclerosis (MS) diagnostic criteria lack specificity, and this may lead to misdiagnosis, which remains an issue in present-day clinical practice. In addition, conventional biomarkers only moderately correlate with MS disease progression. Recently, advanced MS lesional imaging biomarkers such as cortical lesions (CL), the central vein sign (CVS), and paramagnetic rim lesions (PRL), visible in specialized magnetic resonance imaging (MRI) sequences, have shown higher specificity in differential diagnosis. Moreover, studies have shown that CL and PRL are potential prognostic biomarkers, the former correlating with cognitive impairments and the latter with early disability progression. As machine learning-based methods have achieved extraordinary performance in the assessment of conventional imaging biomarkers, such as white matter lesion segmentation, several automated or semi-automated methods have been proposed for CL, CVS, and PRL as well. In the present review, we first introduce these advanced MS imaging biomarkers and their imaging methods. Subsequently, we describe the corresponding machine learning-based methods that were used to tackle these clinical questions, putting them into context with respect to the challenges they are still facing, including non-standardized MRI protocols, limited datasets, and moderate inter-rater variability. We conclude by presenting the current limitations that prevent their broader deployment and suggesting future research directions.      
### 7.ISO and DSO Coordination: A Parametric Programming Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.07433.pdf)
>  In this paper, a framework is proposed to coordinate the operation of the independent system operator (ISO) and distribution system operator (DSO) to leverage the wholesale market participation of distributed energy resources (DERs) aggregators while ensuring secure operation of distribution grids. The proposed coordination framework is based on parametric programming. The DSO builds the bid-in cost function based on the distribution system market considering its market player constraints and distribution system physical constraints including the power balance equations and voltage limitation constraints. The DSO submits the resulting bid-in cost function to the wholesale market operated by the ISO. After the clearance of the wholesale market, the DSO determines the share of its retail market participants (i.e., DER aggregators). Case studies are performed to verify the effectiveness of the proposed method.      
### 8.Accurate smartphone camera simulation using 3D scenes  [ :arrow_down: ](https://arxiv.org/pdf/2201.07411.pdf)
>  We assess the accuracy of a smartphone camera simulation. The simulation is an end-to-end analysis that begins with a physical description of a high dynamic range 3D scene and includes a specification of the optics and the image sensor. The simulation is compared to measurements of a physical version of the scene. The image system simulation accurately matched measurements of optical blur, depth of field, spectral quantum efficiency, scene inter-reflections, and sensor noise data. The results support the use of image systems simulation methods for soft prototyping cameras and for producing synthetic data in machine learning applications.      
### 9.Compressed Smooth Sparse Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2201.07404.pdf)
>  Image-based anomaly detection systems are of vital importance in various manufacturing applications. The resolution and acquisition rate of such systems is increasing significantly in recent years under the fast development of image sensing technology. This enables the detection of tiny defects in real-time. However, such a high resolution and acquisition rate of image data not only slows down the speed of image processing algorithms but also increases data storage and transmission cost. To tackle this problem, we propose a fast and data-efficient method with theoretical performance guarantee that is suitable for sparse anomaly detection in images with a smooth background (smooth plus sparse signal). The proposed method, named Compressed Smooth Sparse Decomposition (CSSD), is a one-step method that unifies the compressive image acquisition and decomposition-based image processing techniques. To further enhance its performance in a high-dimensional scenario, a Kronecker Compressed Smooth Sparse Decomposition (KronCSSD) method is proposed. Compared to traditional smooth and sparse decomposition algorithms, significant transmission cost reduction and computational speed boost can be achieved with negligible performance loss. Simulation examples and several case studies in various applications illustrate the effectiveness of the proposed framework.      
### 10.The Role of Pleura and Adipose in Lung Ultrasound AI  [ :arrow_down: ](https://arxiv.org/pdf/2201.07368.pdf)
>  In this paper, we study the significance of the pleura and adipose tissue in lung ultrasound AI analysis. We highlight their more prominent appearance when using high-frequency linear (HFL) instead of curvilinear ultrasound probes, showing HFL reveals better pleura detail. We compare the diagnostic utility of the pleura and adipose tissue using an HFL ultrasound probe. Masking the adipose tissue during training and inference (while retaining the pleural line and Merlin's space artifacts such as A-lines and B-lines) improved the AI model's diagnostic accuracy.      
### 11.Weakly Supervised Contrastive Learning for Better Severity Scoring of Lung Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2201.07357.pdf)
>  With the onset of the COVID-19 pandemic, ultrasound has emerged as an effective tool for bedside monitoring of patients. Due to this, a large amount of lung ultrasound scans have been made available which can be used for AI based diagnosis and analysis. Several AI-based patient severity scoring models have been proposed that rely on scoring the appearance of the ultrasound scans. AI models are trained using ultrasound-appearance severity scores that are manually labeled based on standardized visual features. We address the challenge of labeling every ultrasound frame in the video clips. Our contrastive learning method treats the video clip severity labels as noisy weak severity labels for individual frames, thus requiring only video-level labels. We show that it performs better than the conventional cross-entropy loss based training. We combine frame severity predictions to come up with video severity predictions and show that the frame based model achieves comparable performance to a video based TSM model, on a large dataset combining public and private sources.      
### 12.Lung Swapping Autoencoder: Learning a Disentangled Structure-texture Representation of Chest Radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2201.07344.pdf)
>  Well-labeled datasets of chest radiographs (CXRs) are difficult to acquire due to the high cost of annotation. Thus, it is desirable to learn a robust and transferable representation in an unsupervised manner to benefit tasks that lack labeled data. Unlike natural images, medical images have their own domain prior; e.g., we observe that many pulmonary diseases, such as the COVID-19, manifest as changes in the lung tissue texture rather than the anatomical structure. Therefore, we hypothesize that studying only the texture without the influence of structure variations would be advantageous for downstream prognostic and predictive modeling tasks. In this paper, we propose a generative framework, the Lung Swapping Autoencoder (LSAE), that learns factorized representations of a CXR to disentangle the texture factor from the structure factor. Specifically, by adversarial training, the LSAE is optimized to generate a hybrid image that preserves the lung shape in one image but inherits the lung texture of another. To demonstrate the effectiveness of the disentangled texture representation, we evaluate the texture encoder $Enc^t$ in LSAE on ChestX-ray14 (N=112,120), and our own multi-institutional COVID-19 outcome prediction dataset, COVOC (N=340 (Subset-1) + 53 (Subset-2)). On both datasets, we reach or surpass the state-of-the-art by finetuning $Enc^t$ in LSAE that is 77% smaller than a baseline Inception v3. Additionally, in semi-and-self supervised settings with a similar model budget, $Enc^t$ in LSAE is also competitive with the state-of-the-art MoCo. By "re-mixing" the texture and shape factors, we generate meaningful hybrid images that can augment the training set. This data augmentation method can further improve COVOC prediction performance. The improvement is consistent even when we directly evaluate the Subset-1 trained model on Subset-2 without any fine-tuning.      
### 13.Functional observability and target state estimation in large-scale networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.07256.pdf)
>  The quantitative understanding and precise control of complex dynamical systems can only be achieved by observing their internal states via measurement and/or estimation. In large-scale dynamical networks, it is often difficult or physically impossible to have enough sensor nodes to make the system fully observable. Even if the system is in principle observable, high-dimensionality poses fundamental limits on the computational tractability and performance of a full-state observer. To overcome the curse of dimensionality, we instead require the system to be functionally observable, meaning that a targeted subset of state variables can be reconstructed from the available measurements. Here, we develop a graph-based theory of functional observability, which leads to highly scalable algorithms to i) determine the minimal set of required sensors and ii) design the corresponding state observer of minimum order. Compared to the full-state observer, the proposed functional observer achieves the same estimation quality with substantially less sensing and computational resources, making it suitable for large-scale networks. We apply the proposed methods to the detection of cyber-attacks in power grids from limited phase measurement data and the inference of the prevalence rate of infection during an epidemic under limited testing conditions. The applications demonstrate that the functional observer can significantly scale up our ability to explore otherwise inaccessible dynamical processes on complex networks.      
### 14.Real-time X-ray Phase-contrast Imaging Using SPINNet -- A Speckle-based Phase-contrast Imaging Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2201.07232.pdf)
>  X-ray phase-contrast imaging has become indispensable for visualizing samples with low absorption contrast. In this regard, speckle-based techniques have shown significant advantages in spatial resolution, phase sensitivity, and implementation flexibility compared with traditional methods. However, their computational cost has hindered their wider adoption. By exploiting the power of deep learning, we developed a new speckle-based phase-contrast imaging neural network (SPINNet) that boosts the phase retrieval speed by at least two orders of magnitude compared to existing methods. To achieve this performance, we combined SPINNet with a novel coded-mask-based technique, an enhanced version of the speckle-based method. Using this scheme, we demonstrate a simultaneous reconstruction of absorption and phase images on the order of 100 ms, where a traditional correlation-based analysis would take several minutes even with a cluster. In addition to significant improvement in speed, our experimental results show that the imaging resolution and phase retrieval quality of SPINNet outperform existing single-shot speckle-based methods. Furthermore, we successfully demonstrate its application in 3D X-ray phase-contrast tomography. Our result shows that SPINNet could enable many applications requiring high-resolution and fast data acquisition and processing, such as in-situ and in-operando 2D and 3D phase-contrast imaging and real-time at-wavelength metrology and wavefront sensing.      
### 15.AI-based Carcinoma Detection and Classification Using Histopathological Images: A Systematic Review  [ :arrow_down: ](https://arxiv.org/pdf/2201.07231.pdf)
>  Histopathological image analysis is the gold standard to diagnose cancer. Carcinoma is a subtype of cancer that constitutes more than 80% of all cancer cases. Squamous cell carcinoma and adenocarcinoma are two major subtypes of carcinoma, diagnosed by microscopic study of biopsy slides. However, manual microscopic evaluation is a subjective and time-consuming process. Many researchers have reported methods to automate carcinoma detection and classification. The increasing use of artificial intelligence (AI) in the automation of carcinoma diagnosis also reveals a significant rise in the use of deep network models. In this systematic literature review, we present a comprehensive review of the state-of-the-art approaches reported in carcinoma diagnosis using histopathological images. Studies are selected from well-known databases with strict inclusion/exclusion criteria. We have categorized the articles and recapitulated their methods based on specific organs of carcinoma origin. Further, we have summarized pertinent literature on AI methods, highlighted critical challenges and limitations, and provided insights on future research direction in automated carcinoma diagnosis. Out of 101 articles selected, most of the studies experimented on private datasets with varied image sizes, obtaining accuracy between 63% and 100%. Overall, this review highlights the need for a generalized AI-based carcinoma diagnostic system. Additionally, it is desirable to have accountable approaches to extract microscopic features from images of multiple magnifications that should mimic pathologists' evaluations.      
### 16.Optimal Lockdown to Manage an Epidemic  [ :arrow_down: ](https://arxiv.org/pdf/2201.07229.pdf)
>  We formulate an optimal control problem to determine the lockdown policy to curb an epidemic where other control measures are not available yet. We present a unified framework to model the epidemic and economy that allows us to study the effect of lockdown on both of them together. The objective function considers cost of deaths and infections during the epidemic, as well as economic losses due to reduced interactions due to lockdown. We tune the parameters of our model for Covid-19 epidemic and the economies of Burundi, India, and the United States (the low, medium and high income countries). We study the optimal lockdown policies and effect of system parameters for all of these countries. Our framework and results are useful for policymakers to design optimal lockdown strategies that account for both epidemic related infections and deaths, and economic losses due to lockdown.      
### 17.Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features  [ :arrow_down: ](https://arxiv.org/pdf/2201.07227.pdf)
>  Image classification is widely used to build predictive models for breast cancer diagnosis. Most existing approaches overwhelmingly rely on deep convolutional networks to build such diagnosis pipelines. These model architectures, although remarkable in performance, are black-box systems that provide minimal insight into the inner logic behind their predictions. This is a major drawback as the explainability of prediction is vital for applications such as cancer diagnosis. In this paper, we address this issue by proposing an explainable machine learning pipeline for breast cancer diagnosis based on ultrasound images. We extract first- and second-order texture features of the ultrasound images and use them to build a probabilistic ensemble of decision tree classifiers. Each decision tree learns to classify the input ultrasound image by learning a set of robust decision thresholds for texture features of the image. The decision path of the model predictions can then be interpreted by decomposing the learned decision trees. Our results show that our proposed framework achieves high predictive performance while being explainable.      
### 18.Is Contrastive Learning Suitable for Left Ventricular Segmentation in Echocardiographic Images?  [ :arrow_down: ](https://arxiv.org/pdf/2201.07219.pdf)
>  Contrastive learning has proven useful in many applications where access to labelled data is limited. The lack of annotated data is particularly problematic in medical image segmentation as it is difficult to have clinical experts manually annotate large volumes of data. One such task is the segmentation of cardiac structures in ultrasound images of the heart. In this paper, we argue whether or not contrastive pretraining is helpful for the segmentation of the left ventricle in echocardiography images. Furthermore, we study the effect of this on two segmentation networks, DeepLabV3, as well as the commonly used segmentation network, UNet. Our results show that contrastive pretraining helps improve the performance on left ventricle segmentation, particularly when annotated data is scarce. We show how to achieve comparable results to state-of-the-art fully supervised algorithms when we train our models in a self-supervised fashion followed by fine-tuning on just 5% of the data. We also show that our solution achieves better results than what is currently published on a large public dataset (EchoNet-Dynamic) and we compare the performance of our solution on another smaller dataset (CAMUS) as well.      
### 19.Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation  [ :arrow_down: ](https://arxiv.org/pdf/2201.07786.pdf)
>  Animating high-fidelity video portrait with speech audio is crucial for virtual reality and digital entertainment. While most previous studies rely on accurate explicit structural information, recent works explore the implicit scene representation of Neural Radiance Fields (NeRF) for realistic generation. In order to capture the inconsistent motions as well as the semantic difference between human head and torso, some work models them via two individual sets of NeRF, leading to unnatural results. In this work, we propose Semantic-aware Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven portraits using one unified set of NeRF. The proposed model can handle the detailed local facial semantics and the global head-torso relationship through two semantic-aware modules. Specifically, we first propose a Semantic-Aware Dynamic Ray Sampling module with an additional parsing branch that facilitates audio-driven volume rendering. Moreover, to enable portrait rendering in one unified neural radiance field, a Torso Deformation module is designed to stabilize the large-scale non-rigid torso motions. Extensive evaluations demonstrate that our proposed approach renders more realistic video portraits compared to previous methods. Project page: <a class="link-external link-https" href="https://alvinliu0.github.io/projects/SSP-NeRF" rel="external noopener nofollow">this https URL</a>      
### 20.DeepAlloc: CNN-Based Approach to Efficient Spectrum Allocation in Shared Spectrum Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.07762.pdf)
>  Shared spectrum systems facilitate spectrum allocation to unlicensed users without harming the licensed users; they offer great promise in optimizing spectrum utility, but their management (in particular, efficient spectrum allocation to unlicensed users) is challenging. A significant shortcoming of current allocation methods is that they are either done very conservatively to ensure correctness, or are based on imperfect propagation models and/or spectrum sensing with poor spatial granularity. This leads to poor spectrum utilization, the fundamental objective of shared spectrum systems. <br>To allocate spectrum near-optimally to secondary users in general scenarios, we fundamentally need to have knowledge of the signal path-loss function. In practice, however, even the best known path-loss models have unsatisfactory accuracy, and conducting extensive surveys to gather path-loss values is infeasible. To circumvent this challenge, we propose to learn the spectrum allocation function directly using supervised learning techniques. We particularly address the scenarios when the primary users' information may not be available; for such settings, we make use of a crowdsourced sensing architecture and use the spectrum sensor readings as features. We develop an efficient CNN-based approach (called DeepAlloc) and address various challenges that arise in its application to the learning the spectrum allocation function. Via extensive large-scale simulation and a small testbed, we demonstrate the effectiveness of our developed techniques; in particular, we observe that our approach improves the accuracy of standard learning techniques and prior work by up to 60%.      
### 21.A pipeline for automated processing of Corona KH-4 (1962-1972) stereo imagery  [ :arrow_down: ](https://arxiv.org/pdf/2201.07756.pdf)
>  The Corona KH-4 reconnaissance satellite missions from 1962-1972 acquired panoramic stereo imagery with high spatial resolution of 1.8-7.5 m. The potential of 800,000+ declassified Corona images has not been leveraged due to the complexities arising from handling of panoramic imaging geometry, film distortions and limited availability of the metadata required for georeferencing of the Corona imagery. This paper presents Corona Stereo Pipeline (CoSP): A pipeline for processing of Corona KH-4 stereo panoramic imagery. CoSP utlizes a deep learning based feature matcher SuperGlue to automatically match features point between Corona KH-4 images and recent satellite imagery to generate Ground Control Points (GCPs). To model the imaging geometry and the scanning motion of the panoramic KH-4 cameras, a rigorous camera model consisting of modified collinearity equations with time dependent exterior orientation parameters is employed. The results show that using the entire frame of the Corona image, bundle adjustment using well-distributed GCPs results in an average standard deviation (SD) of less than 2 pixels. The distortion pattern of image residuals of GCPs and y-parallax in epipolar resampled images suggest that film distortions due to long term storage as likely cause of systematic deviations. Compared to the SRTM DEM, the Corona DEM computed using CoSP achieved a Normalized Median Absolute Deviation (NMAD) of elevation differences of ~4 m over an area of approx. 4000 $km^2$. We show that the proposed pipeline can be applied to sequence of complex scenes involving high relief and glacierized terrain and that the resulting DEMs can be used to compute long term glacier elevation changes over large areas.      
### 22.Detection of Correlated Alarms Using Graph Embedding  [ :arrow_down: ](https://arxiv.org/pdf/2201.07748.pdf)
>  Industrial alarm systems have recently progressed considerably in terms of network complexity and the number of alarms. The increase in complexity and number of alarms presents challenges in these systems that decrease system efficiency and cause distrust of the operator, which might result in widespread damages. One contributing factor in alarm inefficiency is the correlated alarms. These alarms do not contain new information and only confuse the operator. This paper tries to present a novel method for detecting correlated alarms based on artificial intelligence methods to help the operator. The proposed method is based on graph embedding and alarm clustering, resulting in the detection of correlated alarms. To evaluate the proposed method, a case study is conducted on the well-known Tennessee-Eastman process.      
### 23.Implicit Tracking-based Distributed Constraint-coupled Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2201.07627.pdf)
>  In this work, a kind of distributed constraint-coupled optimization problem is studied. We first propose an augmented primal-dual dynamics and study its convergence, but the need for knowing the violation of the coupled constraint prevents it from being implemented distributedly. By comprehending a classical continuous-time distributed optimization algorithm from a new perspective, the novel implicit tracking approach is proposed to track the violation distributedly, which leads to the \underline{i}mplicit tracking-based \underline{d}istribut\underline{e}d \underline{a}ugmented primal-dual dynamics (IDEA). To deal with the cases where local constrained sets exist, we further develop a projected variant of IDEA, i.e., Proj-IDEA. Under the undirected and connected communication topology, the convergence of IDEA can be guaranteed when local objective functions are only convex, and if local objective functions are strongly convex and smooth, IDEA can converge exponentially with a quite weak condition about the constraint matrix. In line with IDEA, Proj-IDEA can also guarantee convergence when local objective functions are only convex. Furthermore, we also explore the convergences of IDEA and Proj-IDEA under the directed, strongly connected, and weight-balanced communication topology. Finally, numerical experiments are taken to corroborate our theoretical results.      
### 24.MHTTS: Fast multi-head text-to-speech for spontaneous speech with imperfect transcription  [ :arrow_down: ](https://arxiv.org/pdf/2201.07438.pdf)
>  Neural network based end-to-end Text-to-Speech (TTS) has greatly improved the quality of synthesized speech. While how to use massive spontaneous speech without transcription efficiently still remains an open problem. In this paper, we propose MHTTS, a fast multi-speaker TTS system that is robust to transcription errors and speaking style speech data. Specifically, we introduce a multi-head model and transfer text information from high-quality corpus with manual transcription to spontaneous speech with imperfectly recognized transcription by jointly training them. MHTTS has three advantages: 1) Our system synthesizes better quality multi-speaker voice with faster inference speed. 2) Our system is capable of transferring correct text information to data with imperfect transcription, simulated using corruption, or provided by an Automatic Speech Recogniser (ASR). 3) Our system can utilize massive real spontaneous speech with imperfect transcription and synthesize expressive voice.      
### 25.Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2201.07429.pdf)
>  This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voice synthesis (SVS). The corpus consists of 100 popular Mandarin songs performed by a female professional singer. Audio files are recorded with studio quality at a sampling rate of 44,100 Hz and the corresponding lyrics and musical scores are provided. All singing recordings have been phonetically annotated with phoneme boundaries and syllable (note) boundaries. To demonstrate the reliability of the released data and to provide a baseline for future research, we built baseline deep neural network-based SVS models and evaluated them with both objective metrics and subjective mean opinion score (MOS) measure. Experimental results show that the best SVS model trained on our database achieves 3.70 MOS, indicating the reliability of the provided corpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can be found on the project homepage.      
### 26.RIS-Assisted Communication Radar Coexistence: Joint Beamforming Design and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2201.07399.pdf)
>  Integrated sensing and communication (ISAC) has been regarded as one of the most promising technologies for future wireless communications. However, the mutual interference in the communication radar coexistence system cannot be ignored. Inspired by the studies of reconfigurable intelligent surface (RIS), we propose a double-RIS-assisted coexistence system where two RISs are deployed for enhancing communication signals and suppressing mutual interference. We aim to jointly optimize the beamforming of RISs and radar to maximize communication performance while maintaining radar detection performance. The investigated problem is challenging, and thus we transform it into an equivalent but more tractable form by introducing auxiliary variables. Then, we propose a penalty dual decomposition (PDD)-based algorithm to solve the resultant problem. Moreover, we consider two special cases: the large radar transmit power scenario and the low radar transmit power scenario. For the former, we prove that the beamforming design is only determined by the communication channel and the corresponding optimal joint beamforming strategy can be obtained in closed-form. For the latter, we minimize the mutual interference via the block coordinate descent (BCD) method. By combining the solutions of these two cases, a low-complexity algorithm is also developed. Finally, simulation results show that both the PDD-based and low-complexity algorithms outperform benchmark algorithms.      
### 27.Variational Autoencoder Generative Adversarial Network for Synthetic Data Generation in Smart Home  [ :arrow_down: ](https://arxiv.org/pdf/2201.07387.pdf)
>  Data is the fuel of data science and machine learning techniques for smart grid applications, similar to many other fields. However, the availability of data can be an issue due to privacy concerns, data size, data quality, and so on. To this end, in this paper, we propose a Variational AutoEncoder Generative Adversarial Network (VAE-GAN) as a smart grid data generative model which is capable of learning various types of data distributions and generating plausible samples from the same distribution without performing any prior analysis on the data before the training phase.We compared the Kullback-Leibler (KL) divergence, maximum mean discrepancy (MMD), and Wasserstein distance between the synthetic data (electrical load and PV production) distribution generated by the proposed model, vanilla GAN network, and the real data distribution, to evaluate the performance of our model. Furthermore, we used five key statistical parameters to describe the smart grid data distribution and compared them between synthetic data generated by both models and real data. Experiments indicate that the proposed synthetic data generative model outperforms the vanilla GAN network. The distribution of VAE-GAN synthetic data is the most comparable to that of real data.      
### 28.Model-driven Cluster Resource Management for AI Workloads in Edge Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2201.07312.pdf)
>  Since emerging edge applications such as Internet of Things (IoT) analytics and augmented reality have tight latency constraints, hardware AI accelerators have been recently proposed to speed up deep neural network (DNN) inference run by these applications. Resource-constrained edge servers and accelerators tend to be multiplexed across multiple IoT applications, introducing the potential for performance interference between latency-sensitive workloads. In this paper, we design analytic models to capture the performance of DNN inference workloads on shared edge accelerators, such as GPU and edgeTPU, under different multiplexing and concurrency behaviors. After validating our models using extensive experiments, we use them to design various cluster resource management algorithms to intelligently manage multiple applications on edge accelerators while respecting their latency constraints. We implement a prototype of our system in Kubernetes and show that our system can host 2.3X more DNN applications in heterogeneous multi-tenant edge clusters with no latency violations when compared to traditional knapsack hosting algorithms.      
### 29.Synchronization of Complex Network Systems with Stochastic Disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2201.07213.pdf)
>  We study the robustness of the synchronization of coupled phase oscillators. When fluctuations of phase differences in lines caused by disturbance exceed a certain threshold, the state cannot return to synchrony thus leading to desynchronization. Our main result is the deviation of explicit formulas of a variance matrix that characterizes the severity of these fluctuations. We highlight the utility of these results in two general problems: vulnerable line identification and network design. We find that the vulnerability of lines can be encoded by the cycle space of graphs. It is analytically shown that a line in large-size cycles is more vulnerable than those in small-size cycles and adding a new line or increasing coupling strength of a line reduces the vulnerability of the lines in any cycle including this line, while it does not affect the vulnerability of the other lines.      
### 30.Efficient Training of Spiking Neural Networks with Temporally-Truncated Local Backpropagation through Time  [ :arrow_down: ](https://arxiv.org/pdf/2201.07210.pdf)
>  Directly training spiking neural networks (SNNs) has remained challenging due to complex neural dynamics and intrinsic non-differentiability in firing functions. The well-known backpropagation through time (BPTT) algorithm proposed to train SNNs suffers from large memory footprint and prohibits backward and update unlocking, making it impossible to exploit the potential of locally-supervised training methods. This work proposes an efficient and direct training algorithm for SNNs that integrates a locally-supervised training method with a temporally-truncated BPTT algorithm. The proposed algorithm explores both temporal and spatial locality in BPTT and contributes to significant reduction in computational cost including GPU memory utilization, main memory access and arithmetic operations. We thoroughly explore the design space concerning temporal truncation length and local training block size and benchmark their impact on classification accuracy of different networks running different types of tasks. The results reveal that temporal truncation has a negative effect on the accuracy of classifying frame-based datasets, but leads to improvement in accuracy on dynamic-vision-sensor (DVS) recorded datasets. In spite of resulting information loss, local training is capable of alleviating overfitting. The combined effect of temporal truncation and local training can lead to the slowdown of accuracy drop and even improvement in accuracy. In addition, training deep SNNs models such as AlexNet classifying CIFAR10-DVS dataset leads to 7.26% increase in accuracy, 89.94% reduction in GPU memory, 10.79% reduction in memory access, and 99.64% reduction in MAC operations compared to the standard end-to-end BPTT.      
