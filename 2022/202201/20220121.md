# ArXiv eess --Fri, 21 Jan 2022
### 1.Variational Bayesian Filtering with Subspace Information for Extreme Spatio-Temporal Matrix Completion  [ :arrow_down: ](https://arxiv.org/pdf/2201.08307.pdf)
>  Missing data is a common problem in real-world sensor data collection. The performance of various approaches to impute data degrade rapidly in the extreme scenarios of low data sampling and noisy sampling, a case present in many real-world problems in the field of traffic sensing and environment monitoring, etc. However, jointly exploiting the spatiotemporal and periodic structure, which is generally not captured by classical matrix completion approaches, can improve the imputation performance of sensor data in such real-world conditions. We present a Bayesian approach towards spatiotemporal matrix completion wherein we estimate the underlying temporarily varying subspace using a Variational Bayesian technique. We jointly couple the low-rank matrix completion with the state space autoregressive framework along with a penalty function on the slowly varying subspace to model the temporal and periodic evolution in the data. A major advantage of our method is that a critical parameter like the rank of the model is automatically tuned using the automatic relevance determination (ARD) approach, unlike most matrix/tensor completion techniques. We also propose a robust version of the above formulation, which improves the performance of imputation in the presence of outliers. We evaluate the proposed Variational Bayesian Filtering with Subspace Information (VBFSI) method to impute matrices in real-world traffic and air pollution data. Simulation results demonstrate that the proposed method outperforms the recent state-of-the-art methods and provides a sufficiently accurate imputation for different sampling rates. In particular, we demonstrate that fusing the subspace evolution over days can improve the imputation performance with even 15% of the data sampling.      
### 2.Fault location in High Voltage Multi-terminal dc Networks Using Ensemble Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.08263.pdf)
>  Precise location of faults for large distance power transmission networks is essential for faster repair and restoration process. High Voltage direct current (HVdc) networks using modular multi-level converter (MMC) technology has found its prominence for interconnected multi-terminal networks. This allows for large distance bulk power transmission at lower costs. However, they cope with the challenge of dc faults. Fast and efficient methods to isolate the network under dc faults have been widely studied and investigated. After successful isolation, it is essential to precisely locate the fault. The post-fault voltage and current signatures are a function of multiple factors and thus accurately locating faults on a multi-terminal network is challenging. In this paper, we discuss a novel data-driven ensemble learning based approach for accurate fault location. Here we utilize the eXtreme Gradient Boosting (XGB) method for accurate fault location. The sensitivity of the proposed algorithm to measurement noise, fault location, resistance and current limiting inductance are performed on a radial three-terminal MTdc network designed in Power System Computer Aided Design (PSCAD)/Electromagnetic Transients including dc (EMTdc).      
### 3.OpenIPDM: A Probabilistic Framework for Estimating the Deterioration and Effect of Interventions on Bridges  [ :arrow_down: ](https://arxiv.org/pdf/2201.08254.pdf)
>  This paper describes OpenIPDM software for modelling the deterioration process of infrastructures using network-scale visual inspection data. In addition to the deterioration state estimates, OpenIPDM provides functions for quantifying the effect of interventions, estimating the service life of an intervention, and generating synthetic data for verification purposes. Each of the aforementioned functions are accessible by an interactive graphical user interface. OpenIPDM is designed based on the research work done on a network of bridges in Quebec province, so that the concepts presented in the software have been validated for applications in a real-world context. In addition, this software provides foundations for future developments in the subject area of modelling the deterioration as well as intervention planning.      
### 4.Continuous Phase Modulation of Phase Coded Transmit Waveforms using Multi-Tone Sinusoidal Frequency Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2201.08219.pdf)
>  Phase Coded (PC) waveforms possess desirable Auto-Correlation Function (ACF) properties for use in radar and sonar systems. However, their spectra possess high spectral leakage due to the abrupt phase transitions between the chips in the waveform. This paper describes a method of Continuous Phase Modulation (CPM) to reduce a PC waveform's spectral leakage using the Multi-Tone Sinusoidal Frequency Modulation (MTSFM) model. The MTSFM-CPM model represents the PC waveform's instantaneous phase as a finite Fourier series. This representation smooths the abrupt phase transitions between chips resulting in a spectrally compact waveform. This smoothing of the PC waveform's instantaneous phase introduces perturbations to the waveform's ACF mainlobe and sidelobe structure. Adjusting the MTSFM-CPM waveform's parameters refines its ACF mainlobe and sidelobe structure while also preserving its compact spectral shape.      
### 5.Enhancement or Super-Resolution: Learning-based Adaptive Video Streaming with Client-Side Video Processing  [ :arrow_down: ](https://arxiv.org/pdf/2201.08197.pdf)
>  The rapid development of multimedia and communication technology has resulted in an urgent need for high-quality video streaming. However, robust video streaming under fluctuating network conditions and heterogeneous client computing capabilities remains a challenge. In this paper, we consider an enhancement-enabled video streaming network under a time-varying wireless network and limited computation capacity. "Enhancement" means that the client can improve the quality of the downloaded video segments via image processing modules. We aim to design a joint bitrate adaptation and client-side enhancement algorithm toward maximizing the quality of experience (QoE). We formulate the problem as a Markov decision process (MDP) and propose a deep reinforcement learning (DRL)-based framework, named ENAVS. As video streaming quality is mainly affected by video compression, we demonstrate that the video enhancement algorithm outperforms the super-resolution algorithm in terms of signal-to-noise ratio and frames per second, suggesting a better solution for client processing in video streaming. Ultimately, we implement ENAVS and demonstrate extensive testbed results under real-world bandwidth traces and videos. The simulation shows that ENAVS is capable of delivering 5%-14% more QoE under the same bandwidth and computing power conditions as conventional ABR streaming.      
### 6.GVSoC: A Highly Configurable, Fast and Accurate Full-Platform Simulator for RISC-V based IoT Processors  [ :arrow_down: ](https://arxiv.org/pdf/2201.08166.pdf)
>  The last few years have seen the emergence of IoT processors: ultra-low power systems-on-chips (SoCs) combining lightweight and flexible micro-controller units (MCUs), often based on open-ISA RISC-V cores, with application-specific accelerators to maximize performance and energy efficiency. Overall, this heterogeneity level requires complex hardware and a full-fledged software stack to orchestrate the execution and exploit platform features. For this reason, enabling agile design space exploration becomes a crucial asset for this new class of low-power SoCs. In this scenario, high-level simulators play an essential role in breaking the speed and design effort bottlenecks of cycle-accurate simulators and FPGA prototypes, respectively, while preserving functional and timing accuracy. We present GVSoC, a highly configurable and timing-accurate event-driven simulator that combines the efficiency of C++ models with the flexibility of Python configuration scripts. GVSoC is fully open-sourced, with the intent to drive future research in the area of highly parallel and heterogeneous RISC-V based IoT processors, leveraging three foundational features: Python-based modular configuration of the hardware description, easy calibration of platform parameters for accurate performance estimation, and high-speed simulation. Experimental results show that GVSoC enables practical functional and performance analysis and design exploration at the full-platform level (processors, memory, peripherals and IOs) with a speed-up of 2500x with respect to cycle-accurate simulation with errors typically below 10% for performance analysis.      
### 7.Application of Particle Swarm Optimization method to On-going Monitoring for estimating vehicle-bridge interaction system  [ :arrow_down: ](https://arxiv.org/pdf/2201.08014.pdf)
>  This study proposes a method for estimating the mechanical parameters of vehicles and bridges and the road unevenness, using only vehicle vibration and position data. In the proposed method, vehicle input and bridge vibration are estimated using randomly assumed vehicle and bridge parameters. Then, the road profiles at the front and rear wheels can be determined from the vehicle input and bridge vibration. The difference between the two road profiles is used as the objective function because they are expected to coincide when synchronized. Using the particle swarm optimization (PSO) method, the vehicle and bridge parameters and the road unevenness can be estimated by updating the parameters to minimize the objective function. Numerical experiments also verify the applicability of this method. In the numerical experiments, it is confirmed that the proposed method can estimate the vehicle weight with reasonable accuracy, but the accuracy of other parameters is not sufficient. It is necessary to improve the accuracy of the proposed method in the future.      
### 8.Comparative Study on Reliability Estimation Using Monte Carlo Simulation with Application to Cylindrical Pressure Vessel  [ :arrow_down: ](https://arxiv.org/pdf/2201.08013.pdf)
>  One of the methods to design products is reliability-based design, in which failure probability is usually used instead of safety factors. In the technique, it should not be less than a predetermined value. Choosing the proper design criterion is a challenging problem for designers who are dealing with the technique, particularly, when there are various criteria. One of these kinds of products is a cylindrical pressure vessel which has diverse criteria proposed in the literature to calculate the burst pressure as a start point of the design. In this paper, we are going to evaluate and compare the performances of various burst pressure criteria in estimating failure probability which is used for a sample pressure vessel. For each criterion, Monte Carlo simulation has been employed to calculate the probability of failure due to variations related to major design variables. The design parameters include material properties and operating pressure. First, the effects of variations in standard deviations of the design variables on the calculated burst probabilities have been determined by standard deviation analysis. Then, sensitivity analyses were carried out to assess the sensitivity of each burst pressure criterion against changes in the magnitude of design variables.      
### 9.Distributed Stochastic Model Predictive Control for an Urban Traffic Network  [ :arrow_down: ](https://arxiv.org/pdf/2201.07949.pdf)
>  In this paper, we design a stochastic Model Predictive Control (MPC) traffic signal control method for an urban traffic network when the uncertainties in the estimation of the exogenous (in/out)-flows and the turning ratios of downstream traffic flows are taken into account. Assuming that the traffic model parameters are random variables with known expectations and variance, the traffic signal control and coordination problem is formulated as a quadratic program with linear and second-order cone constraints. In order to reduce computational complexity, we suggest a way to decompose the optimization problem corresponding to the whole traffic network into multiple subproblems. By applying Alternating Direction Method of Multipliers (ADMM), the optimal stochastic traffic signal splits are found in distributed manner. The effectiveness of the designed control method is validated via some simulations using VISSIM and MATLAB.      
### 10.Fusion Learning for 1-Bit CS-based Superimposed CSI Feedback with Bi-Directional Channel Reciprocity  [ :arrow_down: ](https://arxiv.org/pdf/2201.07943.pdf)
>  Due to the discarding of downlink channel state information (CSI) amplitude and the employing of iteration reconstruction algorithms, 1-bit compressed sensing (CS)-based superimposed CSI feedback is challenged by low recovery accuracy and large processing delay. To overcome these drawbacks, this letter proposes a fusion learning scheme by exploiting the bi-directional channel reciprocity. Specifically, a simplified version of the conventional downlink CSI reconstruction is utilized to extract the initial feature of downlink CSI, and a single hidden layer-based amplitude-learning network (AMPL-NET) is designed to learn the auxiliary feature of the downlink CSI amplitude. Then, based on the extracted and learned amplitude features, a simple but effective amplitude-fusion network (AMPF-NET) is developed to perform the amplitude fusion of downlink CSI and thus improves the reconstruction accuracy for 1-bit CS-based superimposed CSI feedback while reducing the processing delay. Simulation results show the effectiveness of the proposed feedback scheme and the robustness against parameter variations.      
### 11.Homogenization of Existing Inertial-Based Datasets to Support Human Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.07891.pdf)
>  Several techniques have been proposed to address the problem of recognizing activities of daily living from signals. Deep learning techniques applied to inertial signals have proven to be effective, achieving significant classification accuracy. Recently, research in human activity recognition (HAR) models has been almost totally model-centric. It has been proven that the number of training samples and their quality are critical for obtaining deep learning models that both perform well independently of their architecture, and that are more robust to intraclass variability and interclass similarity. Unfortunately, publicly available datasets do not always contain hight quality data and a sufficiently large and diverse number of samples (e.g., number of subjects, type of activity performed, and duration of trials). Furthermore, datasets are heterogeneous among them and therefore cannot be trivially combined to obtain a larger set. The final aim of our work is the definition and implementation of a platform that integrates datasets of inertial signals in order to make available to the scientific community large datasets of homogeneous signals, enriched, when possible, with context information (e.g., characteristics of the subjects and device position). The main focus of our platform is to emphasise data quality, which is essential for training efficient models.      
### 12.Convolutional Neural Networks for Spherical Signal Processing via Spherical Haar Tight Framelets  [ :arrow_down: ](https://arxiv.org/pdf/2201.07890.pdf)
>  In this paper, we develop a general theoretical framework for constructing Haar-type tight framelets on any compact set with a hierarchical partition. In particular, we construct a novel area-regular hierarchical partition on the 2-sphere and establish its corresponding spherical Haar tight framelets with directionality. We conclude by evaluating and illustrating the effectiveness of our area-regular spherical Haar tight framelets in several denoising experiments. Furthermore, we propose a convolutional neural network (CNN) model for spherical signal denoising which employs the fast framelet decomposition and reconstruction algorithms. Experiment results show that our proposed CNN model outperforms threshold methods, and processes strong generalization and robustness properties.      
### 13.Machine Learning Enhances Algorithms for Quantifying Non-Equilibrium Dynamics in Correlation Spectroscopy Experiments to Reach Frame-Rate-Limited Time Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2201.07889.pdf)
>  Analysis of X-ray Photon Correlation Spectroscopy (XPCS) data for non-equilibrium dynamics often requires manual binning of age regions of an intensity-intensity correlation function. This leads to a loss of temporal resolution and accumulation of systematic error for the parameters quantifying the dynamics, especially in cases with considerable noise. Moreover, the experiments with high data collection rates create the need for automated online analysis, where manual binning is not possible. Here, we integrate a denoising autoencoder model into algorithms for analysis of non-equilibrium two-time intensity-intensity correlation functions. The model can be applied to an input of an arbitrary size. Noise reduction allows to extract the parameters that characterize the sample dynamics with temporal resolution limited only by frame rates. Not only does it improve the quantitative usage of the data, but it also creates the potential for automating the analytical workflow. Various approaches for uncertainty quantification and extension of the model for anomalies detection are discussed.      
### 14.Adaptive Energy Management for Self-Sustainable Wearables in Mobile Health  [ :arrow_down: ](https://arxiv.org/pdf/2201.07888.pdf)
>  Wearable devices that integrate multiple sensors, processors, and communication technologies have the potential to transform mobile health for remote monitoring of health parameters. However, the small form factor of the wearable devices limits the battery size and operating lifetime. As a result, the devices require frequent recharging, which has limited their widespread adoption. Energy harvesting has emerged as an effective method towards sustainable operation of wearable devices. Unfortunately, energy harvesting alone is not sufficient to fulfill the energy requirements of wearable devices. This paper studies the novel problem of adaptive energy management towards the goal of self-sustainable wearables by using harvested energy to supplement the battery energy and to reduce manual recharging by users. To solve this problem, we propose a principled algorithm referred as AdaEM. There are two key ideas behind AdaEM. First, it uses machine learning (ML) methods to learn predictive models of user activity and energy usage patterns. These models allow us to estimate the potential of energy harvesting in a day as a function of the user activities. Second, it reasons about the uncertainty in predictions and estimations from the ML models to optimize the energy management decisions using a dynamic robust optimization (DyRO) formulation. We propose a light-weight solution for DyRO to meet the practical needs of deployment. We validate the AdaEM approach on a wearable device prototype consisting of solar and motion energy harvesting using real-world data of user activities. Experiments show that AdaEM achieves solutions that are within 5% of the optimal with less than 0.005% execution time and energy overhead.      
### 15.THz-Empowered UAVs in 6G: Opportunities, Challenges, and Trade-Offs  [ :arrow_down: ](https://arxiv.org/pdf/2201.07886.pdf)
>  Envisioned use cases of unmanned aerial vehicles (UAVs) impose new service requirements in terms of data rate, latency, and sensing accuracy, to name a few. If such requirements are satisfactorily met, it can create novel applications and enable highly reliable and harmonized integration of UAVs in the 6G network ecosystem. Towards this, terahertz (THz) bands are perceived as a prospective technological enabler for various improved functionalities such as ultra-high throughput and enhanced sensing capabilities. This paper focuses on THzempowered UAVs with the following capabilities: communication, sensing, localization, imaging, and control. We review the potential opportunities and use cases of THz-empowered UAVs, corresponding novel design challenges, and resulting trade-offs. Furthermore, we overview recent advances in UAV deployments regulations, THz standardization, and health aspects related to THz bands. Finally, we take UAV to UAV (U2U) communication as a case-study to provide numerical insights into the impact of various system design parameters and environment factors.      
### 16.Network-ELAA Beamforming and Coverage Analysis for eMBB/URLLC in Spatially Non-Stationary Rician Channels  [ :arrow_down: ](https://arxiv.org/pdf/2201.07875.pdf)
>  In vehicle-to-infrastructure (V2I) networks, a cluster of multi-antenna access points (APs) can collaboratively conduct transmitter beamforming to provide data services (e.g., eMBB or URLLC). The collaboration between APs effectively forms a networked linear antenna-array with extra-large aperture (i.e., network-ELAA), where the wireless channel exhibits spatial nonstationarity. Major contribution of this work lies in the analysis of beamforming gain and radio coverage for network-ELAA non-stationary Rician channels considering the AP clustering. Assuming that: 1) the total transmit-power is fixed and evenly distributed over APs, 2) the beam is formed only based on the line-of-sight (LoS) path, it is found that the beamforming gain is concave to the cluster size. The optimum size of the AP cluster varies with respect to the user's location, channel uncertainty as well as data services. A user located farther from the ELAA requires a larger cluster size. URLLC is more sensitive to the channel uncertainty when comparing to eMBB, thus requiring a larger cluster size to mitigate the channel fading effect and extend the coverage. Finally, it is shown that the network-ELAA can offer significant coverage extension (50% or more in most of cases) when comparing with the single-AP scenario.      
### 17.An Effective Spatial Modulation Based Scheme for Indoor VLC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.07862.pdf)
>  We propose an enhanced spatial modulation (SM)-based scheme for indoor visible light communication systems. This scheme enhances the achievable throughput of conventional SM schemes by transmitting higher order complex modulation symbol, which is decomposed into three different parts. These parts carry the amplitude, phase, and quadrant components of the complex symbol, which are then represented by unipolar pulse amplitude modulation (PAM) symbols. Superposition coding is exploited to allocate a fraction of the total power to each part before they are all multiplexed and transmitted simultaneously, exploiting the entire available bandwidth. At the receiver, a two-step decoding process is proposed to decode the active light emitting diode index before the complex symbol is retrieved. It is shown that at higher spectral efficiency values, the proposed modulation scheme outperforms conventional SM schemes with PAM symbols in terms of average symbol error rate (ASER), and hence, enhancing the system throughput. Furthermore, since the performance of the proposed modulation scheme is sensitive to the power allocation factors, we formulated an ASER optimization problem and propose a sub-optimal solution using successive convex programming (SCP). Notably, the proposed algorithm converges after only few iterations, whilst the performance with the optimized power allocation coefficients outperforms both random and fixed power allocation.      
### 18.Hybrid Reinforcement Learning-Based Eco-Driving Strategy for Connected and Automated Vehicles at Signalized Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2201.07833.pdf)
>  Taking advantage of both vehicle-to-everything (V2X) communication and automated driving technology, connected and automated vehicles are quickly becoming one of the transformative solutions to many transportation problems. However, in a mixed traffic environment at signalized intersections, it is still a challenging task to improve overall throughput and energy efficiency considering the complexity and uncertainty in the traffic system. In this study, we proposed a hybrid reinforcement learning (HRL) framework which combines the rule-based strategy and the deep reinforcement learning (deep RL) to support connected eco-driving at signalized intersections in mixed traffic. Vision-perceptive methods are integrated with vehicle-to-infrastructure (V2I) communications to achieve higher mobility and energy efficiency in mixed connected traffic. The HRL framework has three components: a rule-based driving manager that operates the collaboration between the rule-based policies and the RL policy; a multi-stream neural network that extracts the hidden features of vision and V2I information; and a deep RL-based policy network that generate both longitudinal and lateral eco-driving actions. In order to evaluate our approach, we developed a Unity-based simulator and designed a mixed-traffic intersection scenario. Moreover, several baselines were implemented to compare with our new design, and numerical experiments were conducted to test the performance of the HRL model. The experiments show that our HRL method can reduce energy consumption by 12.70% and save 11.75% travel time when compared with a state-of-the-art model-based Eco-Driving approach.      
### 19.The Role of Gossiping for Information Dissemination over Networked Agents  [ :arrow_down: ](https://arxiv.org/pdf/2201.08365.pdf)
>  We consider information dissemination over a network of gossiping agents (nodes). In this model, a source keeps the most up-to-date information about a time-varying binary state of the world, and $n$ receiver nodes want to follow the information at the source as accurately as possible. When the information at the source changes, the source first sends updates to a subset of $m\leq n$ nodes. After that, the nodes share their local information during the gossiping period to disseminate the information further. The nodes then estimate the information at the source using the majority rule at the end of the gossiping period. To analyze information dissemination, we introduce a new error metric to find the average percentage of nodes that can accurately obtain the most up-to-date information at the source. We characterize the equations necessary to obtain the steady-state distribution for the average error and then analyze the system behavior under both high and low gossip rates. In the high gossip rate, in which each node can access other nodes' information more frequently, we show that the nodes update their information based on the majority of the information in the network. In the low gossip rate, we introduce and analyze the gossip gain, which is the reduction at the average error due to gossiping. In particular, we develop an adaptive policy that the source can use to determine its current transmission capacity $m$ based on its past transmission rates and the accuracy of the information at the nodes. In numerical results, we show that when the source's transmission capacity $m$ is limited, gossiping can be harmful as it causes incorrect information to disseminate. We then find the optimal gossip rates to minimize the average error for a fixed $m$. Finally, we illustrate the outperformance of our adaptive policy compared to the constant $m$-selection policy even for the high gossip rates.      
### 20.TOAST: Trajectory Optimization and Simultaneous Tracking using Shared Neural Network Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2201.08321.pdf)
>  Neural networks have been increasingly employed in Model Predictive Controller (MPC) to control nonlinear dynamic systems. However, MPC still poses a problem that an achievable update rate is insufficient to cope with model uncertainty and external disturbances. In this paper, we present a novel control scheme that can design an optimal tracking controller using the neural network dynamics of the MPC, making it possible to be applied as a plug-and-play extension for any existing model-based feedforward controller. We also describe how our method handles a neural network containing historical information, which does not follow a general form of dynamics. The proposed method is evaluated by its performance in classical control benchmarks with external disturbances. We also extend our control framework to be applied in an aggressive autonomous driving task with unknown friction. In all experiments, our method outperformed the compared methods by a large margin. Our controller also showed low control chattering levels, demonstrating that our feedback controller does not interfere with the optimal command of MPC.      
### 21.Simple Gray Coding and LLR Calculation for MDS Modulation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.08237.pdf)
>  Due to dependence between codeword elements, index modulation (IM) and related modulation techniques struggle to provide simple solutions for practical problems such as Gray coding between information bits and constellation points; and low-complexity log-likelihood ratio (LLR) calculations for channel-encoded information bits. In this paper, we show that a modulation technique based on a simple maximum distance separable (MDS) code, in other words, MDS modulation, can provide simple yet effective solutions to these problems, rendering the MDS techniques more beneficial in the presence of coding. We also compare the coded error performance of the MDS methods with that of the IM methods and demonstrate that MDS modulation outperforms IM.      
### 22.A 1.5GS/s 8b Pipelined-SAR ADC with Output Level Shifting Settling Technique in 14nm CMOS  [ :arrow_down: ](https://arxiv.org/pdf/2201.08221.pdf)
>  A single channel 1.5GS/s 8-bit pipelined-SAR ADC utilizes a novel output level shifting (OLS) settling technique to reduce the power and enable low-voltage operation of the dynamic residue amplifier. The ADC consists of a 4-bit first stage and a 5-bit second stage, with 1-bit redundancy to relax the offset, gain, and settling requirements of the first stage. Employing the OLS technique allows for an inter-stage gain of ~4 from the dynamic residue amplifier with a settling time that is only 28% of a conventional CML amplifier. The ADC's conversion speed is further improved with the use of parallel comparators in the two asynchronous stages. Fabricated in a 14nm FinFET technology, the ADC occupies 0.0013mm2 core area and operates with a 0.8V supply. 6.6-bit ENOB is achieved at Nyquist while consuming 2.4mW, resulting in an FOM of 16.7fJ/conv.-step.      
### 23.Sequential Bayesian Inference for Uncertain Nonlinear Dynamic Systems: A Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2201.08180.pdf)
>  In this article, an overview of Bayesian methods for sequential simulation from posterior distributions of nonlinear and non-Gaussian dynamic systems is presented. The focus is mainly laid on sequential Monte Carlo methods, which are based on particle representations of probability densities and can be seamlessly generalized to any state-space representation. Within this context, a unified framework of the various Particle Filter (PF) alternatives is presented for the solution of state, state-parameter and input-state-parameter estimation problems on the basis of sparse measurements. The algorithmic steps of each filter are thoroughly presented and a simple illustrative example is utilized for the inference of i) unobserved states, ii) unknown system parameters and iii) unmeasured driving inputs.      
### 24.Category-Association Based Similarity Matching for Novel Object Pick-and-Place Task  [ :arrow_down: ](https://arxiv.org/pdf/2201.08177.pdf)
>  Robotic pick-and-place has been researched for a long time to cope with uncertainty of novel objects and changeable environments. Past works mainly focus on learning-based methods to achieve high precision. However, they have difficulty being generalized for the limitation of specified training models. To break through this drawback of learning-based approaches, we introduce a new perspective of similarity matching between novel objects and a known database based on category-association to achieve pick-and-place tasks with high accuracy and stabilization. We calculate the category name similarity using word embedding to quantify the semantic similarity between the categories of known models and the target real-world objects. With a similar model identified by a similarity prediction function, we preplan a series of robust grasps and imitate them to plan new grasps on the real-world target object. We also propose a distance-based method to infer the in-hand posture of objects and adjust small rotations to achieve stable placements under uncertainty. Through a real-world robotic pick-and-place experiment with a dozen of in-category and out-of-category novel objects, our method achieved an average success rate of 90.6% and 75.9% respectively, validating the capacity of generalization to diverse objects.      
### 25.Secure Rate-Splitting for the MIMO Broadcast Channel with Imperfect CSIT and a Jammer  [ :arrow_down: ](https://arxiv.org/pdf/2201.08169.pdf)
>  In this paper, we investigate the secure rate-splitting for the two-user multiple-input multiple-output (MIMO) broadcast channel with imperfect channel state information at the transmitter (CSIT) and a multiple-antenna jammer, where each receiver has equal number of antennas and the jammer has perfect channel state information (CSI). Specifically, we design the secure rate-splitting multiple-access in this scenario, where the security of splitted private and common messages is ensured by precoder design with joint nulling and aligning the leakage information, regarding to different antenna configurations. As a result, we show that the sum-secure degrees-of-freedom (SDoF) achieved by secure rate-splitting outperforms that by conventional zero-forcing. Therefore, we validate the superiority of rate-splitting for the secure purpose in the two-user MIMO broadcast channel with imperfect CSIT and a jammer.      
### 26.An Automatic Control System with Human-in-the-Loop for Training Skydiving Maneuvers: Proof-of-Concept Experiment  [ :arrow_down: ](https://arxiv.org/pdf/2201.08162.pdf)
>  A real-time motion training system for skydiving is proposed. Aerial maneuvers are performed by changing the body posture and thus deflecting the surrounding airflow. The natural learning process is extremely slow due to unfamiliar free-fall dynamics, stress induced blocking of kinesthetic feedback, and complexity of the required movements. The key idea is to augment the learner with an automatic control system that would be able to perform the trained activity if it had direct access to the learner's body as an actuator. The aiding system will supply the following visual cues to the learner: 1. Feedback of the current body posture; 2. The body posture that would bring the body to perform the desired maneuver; 3. Prediction of the future inertial position and orientation if the body retains its present posture. The system will enable novices to maintain stability in free-fall and perceive the unfamiliar environmental dynamics, thus accelerating the initial stages of skill acquisition. This paper presents results of a Proof-of-Concept experiment, whereby humans controlled a virtual skydiver free-falling in a computer simulation, by the means of their bodies. This task was impossible without the aiding system, enabling all participants to complete the task at the first attempt.      
### 27.WPPNets: Unsupervised CNN Training with Wasserstein Patch Priors for Image Superresolution  [ :arrow_down: ](https://arxiv.org/pdf/2201.08157.pdf)
>  We introduce WPPNets, which are CNNs trained by a new unsupervised loss function for image superresolution of materials microstructures. Instead of requiring access to a large database of registered high- and low-resolution images, we only assume to know a large database of low resolution images, the forward operator and one high-resolution reference image. Then, we propose a loss function based on the Wasserstein patch prior which measures the Wasserstein-2 distance between the patch distributions of the predictions and the reference image. We demonstrate by numerical examples that WPPNets outperform other methods with similar assumptions. In particular, we show that WPPNets are much more stable under inaccurate knowledge or perturbations of the forward operator. This enables us to use them in real-world applications, where neither a large database of registered data nor the exact forward operator are given.      
### 28.Cross-Lingual Text-to-Speech Using Multi-Task Learning and Speaker Classifier Joint Training  [ :arrow_down: ](https://arxiv.org/pdf/2201.08124.pdf)
>  In cross-lingual speech synthesis, the speech in various languages can be synthesized for a monoglot speaker. Normally, only the data of monoglot speakers are available for model training, thus the speaker similarity is relatively low between the synthesized cross-lingual speech and the native language recordings. Based on the multilingual transformer text-to-speech model, this paper studies a multi-task learning framework to improve the cross-lingual speaker similarity. To further improve the speaker similarity, joint training with a speaker classifier is proposed. Here, a scheme similar to parallel scheduled sampling is proposed to train the transformer model efficiently to avoid breaking the parallel training mechanism when introducing joint training. By using multi-task learning and speaker classifier joint training, in subjective and objective evaluations, the cross-lingual speaker similarity can be consistently improved for both the seen and unseen speakers in the training set.      
### 29.Adversarial Jamming for a More Effective Constellation Attack  [ :arrow_down: ](https://arxiv.org/pdf/2201.08052.pdf)
>  The common jamming mode in wireless communication is band barrage jamming, which is controllable and difficult to resist. Although this method is simple to implement, it is obviously not the best jamming waveform. Therefore, based on the idea of adversarial examples, we propose the adversarial jamming waveform, which can independently optimize and find the best jamming waveform. We attack QAM with adversarial jamming and find that the optimal jamming waveform is equivalent to the amplitude and phase between the nearest constellation points. Furthermore, by verifying the jamming performance on a hardware platform, it is shown that our method significantly improves the bit error rate compared to other methods.      
### 30.Learning Estimates At The Edge Using Intermittent And Aged Measurement Updates  [ :arrow_down: ](https://arxiv.org/pdf/2201.08020.pdf)
>  Cyber Physical Systems (CPS) applications have agents that actuate in their local vicinity, while requiring measurements that capture the state of their larger environment to make actuation choices. These measurements are made by sensors and communicated over a network as update packets. Network resource constraints dictate that updates arrive at an agent intermittently and be aged on their arrival. This can be alleviated by providing an agent with a fast enough rate of estimates of the measurements. <br>Often works on estimation assume knowledge of the dynamic model of the system being measured. However, as CPS applications become pervasive, such information may not be available in practice. In this work, we propose a novel deep neural network architecture that leverages Long Short Term Memory (LSTM) networks to learn estimates in a model-free setting using only updates received over the network. We detail an online algorithm that enables training of our architecture. <br>The architecture is shown to provide good estimates of measurements of both a linear and a non-linear dynamic system. It learns good estimates even when the learning proceeds over a generic network setting in which the distributions that govern the rate and age of received measurements may change significantly over time. We demonstrate the efficacy of the architecture by comparing it with the baselines of the Time-varying Kalman Filter and the Unscented Kalman Filter. The architecture enables empirical insights with regards to maintaining the ages of updates at the estimator, which are used by it and also the baselines.      
### 31.EdgeMap: CrowdSourcing High Definition Map in Automotive Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2201.07973.pdf)
>  High definition (HD) map needs to be updated frequently to capture road changes, which is constrained by limited specialized collection vehicles. To maintain an up-to-date map, we explore crowdsourcing data from connected vehicles. Updating the map collaboratively is, however, challenging under constrained transmission and computation resources in dynamic networks. In this paper, we propose EdgeMap, a crowdsourcing HD map to minimize the usage of network resources while maintaining the latency requirements. We design a DATE algorithm to adaptively offload vehicular data on a small time scale and reserve network resources on a large time scale, by leveraging the multi-agent deep reinforcement learning and Gaussian process regression. We evaluate the performance of EdgeMap with extensive network simulations in a time-driven end-to-end simulator. The results show that EdgeMap reduces more than 30% resource usage as compared to state-of-the-art solutions.      
### 32.A Low-Complexity Location-Based Hybrid Multiple Access and Relay Selection in V2X Multicast Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.07950.pdf)
>  This study investigated relay-assisted mode 1 sidelink (SL) multicast transmission, which encounters interference from mode 2 SL transmission, for use in low-latency vehicle-to-everything communications. To accommodate mode 1--mode 2 SL traffic, we use the hybrid multiple access (MA) approach, which combines orthogonal MA (OMA) and nonorthogonal MA (NOMA) schemes. We introduce a low-complexity location-based hybrid MA algorithm and its associated relay selection that can be used when SL channel state information is unavailable.      
### 33.Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.07935.pdf)
>  Developing innovative informatics approaches aimed to enhance fetal monitoring is a burgeoning field of study in reproductive medicine. Several reviews have been conducted regarding Artificial intelligence (AI) techniques to improve pregnancy outcomes. They are limited by focusing on specific data such as mother's care during pregnancy. This systematic survey aims to explore how artificial intelligence (AI) can assist with fetal growth monitoring via Ultrasound (US) image. We used eight medical and computer science bibliographic databases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM Library, Google Scholar, and the Web of Science. We retrieved studies published between 2010 to 2021. Data extracted from studies were synthesized using a narrative approach. Out of 1269 retrieved studies, we included 107 distinct studies from queries that were relevant to the topic in the survey. We found that 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound images (n=19). Classification is the most used method (n=42), followed by segmentation (n=31), classification integrated with segmentation (n=16) and other miscellaneous such as object-detection, regression and reinforcement learning (n=18). The most common areas within the pregnancy domain were the fetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen (n=10), and lastly the fetus face (n=10). In the most recent studies, deep learning techniques were primarily used (n=81), followed by machine learning (n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI techniques played a crucial role in predicting fetal diseases and identifying fetus anatomy structures during pregnancy. More research is required to validate this technology from a physician's perspective, such as pilot studies and randomized controlled trials on AI and its applications in a hospital setting.      
### 34.Experimental Large-Scale Jet Flames' Geometrical Features Extraction for Risk Management Using Infrared Images and Deep Learning Segmentation Methods  [ :arrow_down: ](https://arxiv.org/pdf/2201.07931.pdf)
>  Jet fires are relatively small and have the least severe effects among the diverse fire accidents that can occur in industrial plants; however, they are usually involved in a process known as the domino effect, that leads to more severe events, such as explosions or the initiation of another fire, making the analysis of such fires an important part of risk analysis. This research work explores the application of deep learning models in an alternative approach that uses the semantic segmentation of jet fires flames to extract main geometrical attributes, relevant for fire risk assessments. A comparison is made between traditional image processing methods and some state-of-the-art deep learning models. It is found that the best approach is a deep learning architecture known as UNet, along with its two improvements, Attention UNet and UNet++. The models are then used to segment a group of vertical jet flames of varying pipe outlet diameters to extract their main geometrical characteristics. Attention UNet obtained the best general performance in the approximation of both height and area of the flames, while also showing a statistically significant difference between it and UNet++. UNet obtained the best overall performance for the approximation of the lift-off distances; however, there is not enough data to prove a statistically significant difference between Attention UNet and UNet++. The only instance where UNet++ outperformed the other models, was while obtaining the lift-off distances of the jet flames with 0.01275 m pipe outlet diameter. In general, the explored models show good agreement between the experimental and predicted values for relatively large turbulent propane jet flames, released in sonic and subsonic regimes; thus, making these radiation zones segmentation models, a suitable approach for different jet flame risk management scenarios.      
### 35.PROMPT: Learning Dynamic Resource Allocation Policies for Edge-Network Applications  [ :arrow_down: ](https://arxiv.org/pdf/2201.07916.pdf)
>  A growing number of service providers are exploring methods to improve server utilization, reduce power consumption, and reduce total cost of ownership by co-scheduling high-priority latency-critical workloads with best-effort workloads. This practice requires strict resource allocation between workloads to reduce resource contention and maintain Quality of Service (QoS) guarantees. Prior resource allocation works have been shown to improve server utilization under ideal circumstances, yet often compromise QoS guarantees or fail to find valid resource allocations in more dynamic operating environments. Further, prior works are fundamentally reliant upon QoS measurements that can, in practice, exhibit significant transient fluctuations, thus stable control behavior cannot be reliably achieved. In this paper, we propose a novel framework for dynamic resource allocation based on proactive QoS prediction. These predictions help guide a reinforcement-learning-based resource controller towards optimal resource allocations while avoiding transient QoS violations due to fluctuating workload demands. Evaluation shows that the proposed method incurs 4.3x fewer QoS violations, reduces severity of QoS violations by 3.7x, improves best-effort workload performance, and improves overall power efficiency compared with prior work.      
### 36.Sensing Method for Two-Target Detection in Time-Constrained Vector Poisson Channel  [ :arrow_down: ](https://arxiv.org/pdf/2201.07915.pdf)
>  It is an experimental design problem in which there are two Poisson sources with two possible and known rates, and one counter. Through a switch, the counter can observe the sources individually or the counts can be combined so that the counter observes the sum of the two. The sensor scheduling problem is to determine an optimal proportion of the available time to be allocated toward individual and joint sensing, under a total time constraint. Two different metrics are used for optimization: mutual information between the sources and the observed counts, and probability of detection for the associated source detection problem. Our results, which are primarily computational, indicate similar but not identical results under the two cost functions.      
### 37.Analysis of lane-change conflict between cars and trucks at merging section using UAV video data  [ :arrow_down: ](https://arxiv.org/pdf/2201.07881.pdf)
>  The freeway on-ramp merging section is often identified as a crash-prone spot due to the high frequency of traffic conflicts. Very few traffic conflict analysis studies comprehensively consider different vehicle types at freeway merging section. Thus, the main objective of this study is to analyse conflicts between different vehicle types at freeway merging section. Field data are collected by Unmanned Aerial Vehicle (UAV) at merging areas in Shanghai, China. Vehicle extraction method is utilized to obtain vehicle trajectories. Time-to-collision (TTC) is utilized as the surrogate safety measure. TTC of car-car conflicts are the smallest while TTC of truck-truck conflicts are the largest. Traffic conflicts frequently occur at on-ramp and acceleration lane. Results show the spatial distribution of lane-change conflicts is significantly different between different vehicle types, suggesting that vehicle drivers should maintain safe distance especially car drivers. Besides, in order to decrease lane-change conflict at merging area, traffic management agencies are suggested to change dotted lie to solid lane at the beginning of acceleration lane.      
### 38.Unsupervised Personalization of an Emotion Recognition System: The Unique Properties of the Externalization of Valence in Speech  [ :arrow_down: ](https://arxiv.org/pdf/2201.07876.pdf)
>  The prediction of valence from speech is an important, but challenging problem. The externalization of valence in speech has speaker-dependent cues, which contribute to performances that are often significantly lower than the prediction of other emotional attributes such as arousal and dominance. A practical approach to improve valence prediction from speech is to adapt the models to the target speakers in the test set. Adapting a speech emotion recognition (SER) system to a particular speaker is a hard problem, especially with deep neural networks (DNNs), since it requires optimizing millions of parameters. This study proposes an unsupervised approach to address this problem by searching for speakers in the train set with similar acoustic patterns as the speaker in the test set. Speech samples from the selected speakers are used to create the adaptation set. This approach leverages transfer learning using pre-trained models, which are adapted with these speech samples. We propose three alternative adaptation strategies: unique speaker, oversampling and weighting approaches. These methods differ on the use of the adaptation set in the personalization of the valence models. The results demonstrate that a valence prediction model can be efficiently personalized with these unsupervised approaches, leading to relative improvements as high as 13.52%.      
### 39.Adaptive Bézier Degree Reduction and Splitting for Computationally Efficient Motion Planning  [ :arrow_down: ](https://arxiv.org/pdf/2201.07834.pdf)
>  As a parametric polynomial curve family, Bézier curves are widely used in safe and smooth motion design of intelligent robotic systems from flying drones to autonomous vehicles to robotic manipulators. In such motion planning settings, the critical features of high-order Bézier curves such as curve length, distance-to-collision, maximum curvature/velocity/acceleration are either numerically computed at a high computational cost or inexactly approximated by discrete samples. To address these issues, in this paper we present a novel computationally efficient approach for adaptive approximation of high-order Bézier curves by multiple low-order Bézier segments at any desired level of accuracy that is specified in terms of a Bézier metric. Accordingly, we introduce a new Bézier degree reduction method, called parameterwise matching reduction, that approximates Bézier curves more accurately compared to the standard least squares and Taylor reduction methods. We also propose a new Bézier metric, called the maximum control-point distance, that can be computed analytically, has a strong equivalence relation with other existing Bézier metrics, and defines a geometric relative bound between Bézier curves. We provide extensive numerical evidence to demonstrate the effectiveness of our proposed Bézier approximation approach. As a rule of thumb, based on the degree-one matching reduction error, we conclude that an $n^\text{th}$-order Bézier curve can be accurately approximated by $3(n-1)$ quadratic and $6(n-1)$ linear Bézier segments, which is fundamental for Bézier discretization.      
### 40.Dynamical Dorfman Testing with Quarantine  [ :arrow_down: ](https://arxiv.org/pdf/2201.07204.pdf)
>  We consider dynamical group testing problem with a community structure. With a discrete-time SIR (susceptible, infectious, recovered) model, we use Dorfman's two-step group testing approach to identify infections, and step in whenever necessary to inhibit infection spread via quarantines. We analyze the trade-off between quarantine and test costs as well as disease spread. For the special dynamical i.i.d. model, we show that the optimal first stage Dorfman group size differs in dynamic and static cases. We compare the performance of the proposed dynamic two-stage Dorfman testing with state-of-the-art non-adaptive group testing method in dynamic settings.      
### 41.Dynamics of Bitcoin mining  [ :arrow_down: ](https://arxiv.org/pdf/2201.06072.pdf)
>  What happens to mining when the Bitcoin price changes, when there are mining supply shocks, the price of energy changes, or hardware technology evolves? We give precise answers based on the technical forces and incentives in the system. We then build on these dynamics to consider value: what is the cost and purpose of mining, and is it worth it? Does it use too much energy, is it bad for the environment? Finally we extend our analysis to the long term: is mining economically feasible forever? What will the global hash rate be in 40 years? How is mining impacted by the limits of computation and energy? Is it physically sustainable in the long run? From first principles, we derive a fundamental scale-invariant feasibility constraint, which enables us to analyze the interlocking dynamics, find key invariants, and answer these questions mathematically.      
### 42.Duplex Contextual Relation Network for Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06725.pdf)
>  Polyp segmentation is of great importance in the early diagnosis and treatment of colorectal cancer. Since polyps vary in their shape, size, color, and texture, accurate polyp segmentation is very challenging. One promising way to mitigate the diversity of polyps is to model the contextual relation for each pixel such as using attention mechanism. However, previous methods only focus on learning the dependencies between the position within an individual image and ignore the contextual relation across different images. In this paper, we propose Duplex Contextual Relation Network (DCRNet) to capture both within-image and cross-image contextual relations. Specifically, we first design Interior Contextual-Relation Module to estimate the similarity between each position and all the positions within the same image. Then Exterior Contextual-Relation Module is incorporated to estimate the similarity between each position and the positions across different images. Based on the above two types of similarity, the feature at one position can be further enhanced by the contextual region embedding within and across images. To store the characteristic region embedding from all the images, a memory bank is designed and operates as a queue. Therefore, the proposed method can relate similar features even though they come from different images. We evaluate the proposed method on the EndoScene, Kvasir-SEG and the recently released large-scale PICCOLO dataset. Experimental results show that the proposed DCRNet outperforms the state-of-the-art methods in terms of the widely-used evaluation metrics.      
