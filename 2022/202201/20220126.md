# ArXiv eess --Wed, 26 Jan 2022
### 1.Initial Investigations Towards Non-invasive Monitoring of Chronic Wound Healing Using Deep Learning and Ultrasound Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2201.10511.pdf)
>  Chronic wounds including diabetic and arterial/venous insufficiency injuries have become a major burden for healthcare systems worldwide. Demographic changes suggest that wound care will play an even bigger role in the coming decades. Predicting and monitoring response to therapy in wound care is currently largely based on visual inspection with little information on the underlying tissue. Thus, there is an urgent unmet need for innovative approaches that facilitate personalized diagnostics and treatments at the point-of-care. It has been recently shown that ultrasound imaging can monitor response to therapy in wound care, but this work required onerous manual image annotations. In this study, we present initial results of a deep learning-based automatic segmentation of cross-sectional wound size in ultrasound images and identify requirements and challenges for future research on this application. Evaluation of the segmentation results underscores the potential of the proposed deep learning approach to complement non-invasive imaging with Dice scores of 0.34 (U-Net, FCN) and 0.27 (ResNet-U-Net) but also highlights the need for improving robustness further. We conclude that deep learning-supported analysis of non-invasive ultrasound images is a promising area of research to automatically extract cross-sectional wound size and depth information with potential value in monitoring response to therapy.      
### 2.Motion Estimation and Compensation in Automotive MIMO SAR  [ :arrow_down: ](https://arxiv.org/pdf/2201.10504.pdf)
>  With the advent of self-driving vehicles, autonomous driving systems will have to rely on a vast number of heterogeneous sensors to perform dynamic perception of the surrounding environment. Synthetic Aperture Radar (SAR) systems increase the resolution of conventional mass-market radars by exploiting the vehicle's ego-motion, requiring a very accurate knowledge of the trajectory, usually not compatible with automotive-grade navigation systems. In this regard, this paper deals with the analysis, estimation and compensation of trajectory estimation errors in automotive SAR systems, proposing a complete residual motion estimation and compensation workflow. We start by defining the geometry of the acquisition and the basic processing steps of Multiple-Input Multiple-Output (MIMO) SAR systems. Then, we analytically derive the effects of typical motion errors in automotive SAR imaging. Based on the derived models, the procedure is detailed, outlining the guidelines for its practical implementation. We show the effectiveness of the proposed technique by means of experimental data gathered by a 77 GHz radar mounted in a forward looking configuration.      
### 3.An adaptive closed-loop ECoG decoder for long-term and stable bimanual control of an exoskeleton by a tetraplegic  [ :arrow_down: ](https://arxiv.org/pdf/2201.10449.pdf)
>  Brain-computer interfaces (BCIs) still face many challenges to step out of laboratories to be used in real-life applications. A key one persists in the high performance control of diverse effectors for complex tasks, using chronic and safe recorders. This control must be robust over time and of high decoding performance without continuous recalibration of the decoders. In the article, asynchronous control of an exoskeleton by a tetraplegic patient using a chronically implanted epidural electrocorticography (EpiCoG) implant is demonstrated. For this purpose, an adaptive online tensor-based decoder: the Recursive Exponentially Weighted Markov-Switching multi-Linear Model (REW-MSLM) was developed. We demonstrated over a period of 6 months the stability of the 8-dimensional alternative bimanual control of the exoskeleton and its virtual avatar using REW-MSLM without recalibration of the decoder.      
### 4.Plaque segmentation via masking of the artery wall  [ :arrow_down: ](https://arxiv.org/pdf/2201.10424.pdf)
>  The presence of plaques in the coronary arteries are a major risk to the patients' life. In particular, non-calcified plaques pose a great challenge, as they are harder to detect and more likely to rupture than calcified plaques. While current deep learning techniques allow precise segmentation of regular images, the performance in medical images is still low, caused mostly by blurriness and ambiguous voxel intensities of unrelated parts that fall on the same range. In this paper, we propose a novel methodology for segmenting calcified and non-calcified plaques in CCTA-CPR scans of coronary arteries. The input slices are masked so only the voxels within the wall vessel are considered for segmentation. We also provide an exhaustive evaluation by applying different types of masks, in order to validate the potential of vessel masking for plaque segmentation. Our methodology results in a prominent boost in segmentation performance, in both quantitative and qualitative evaluation, achieving accurate plaque shapes even for the challenging non-calcified plaques. We believe our findings can lead the future research for high-performance plaque segmentation.      
### 5.Ensemble learning priors unfolding for scalable Snapshot Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2201.10419.pdf)
>  Snapshot compressive imaging (SCI) can record the 3D information by a 2D measurement and from this 2D measurement to reconstruct the original 3D information by reconstruction algorithm. As we can see, the reconstruction algorithm plays a vital role in SCI. Recently, deep learning algorithm show its outstanding ability, outperforming the traditional algorithm. Therefore, to improve deep learning algorithm reconstruction accuracy is an inevitable topic for SCI. Besides, deep learning algorithms are usually limited by scalability, and a well trained model in general can not be applied to new systems if lacking the new training process. To address these problems, we develop the ensemble learning priors to further improve the reconstruction accuracy and propose the scalable learning to empower deep learning the scalability just like the traditional algorithm. What's more, our algorithm has achieved the state-of-the-art results, outperforming existing algorithms. Extensive results on both simulation and real datasets demonstrate the superiority of our proposed algorithm. The code and models will be released to the public.      
### 6.Reconfiguration of a satellite constellation in circular formation orbit with decentralized model predictive control  [ :arrow_down: ](https://arxiv.org/pdf/2201.10399.pdf)
>  Satellite constellation missions, consisting of a large number of spacecraft, are increasingly being launched or planned. Such missions require novel control approaches, in particular for what concerns orbital phasing maneuvers. In this context, we consider the problem of reconfiguration of a satellite constellation in a circular formation. In our scenario, a formation of equally spaced spacecraft need to undergo an autonomous reconfiguration due to the deorbiting of a satellite in the formation. The remaining spacecraft have to reconfigure to form again an equidistant formation. To achieve this goal, we consider two decentralized strategies that rely on different sets of information about the neighboring spacecraft in the formation. In the fully decentralized case, each controller knows only the current states of each spacecraft, i.e. position and velocity, while in the second decentralized strategy with with information sharing, the entire planned nominal trajectory of each spacecraft is available to its neighbors. Our numerical simulation results show that, by increasing the amount of information available to each spacecraft, faster reconfiguration maneuvers with smaller fuel consumption can be achieved.      
### 7.Zero-Shot Long-Form Voice Cloning with Dynamic Convolution Attention  [ :arrow_down: ](https://arxiv.org/pdf/2201.10375.pdf)
>  With recent advancements in voice cloning, the performance of speech synthesis for a target speaker has been rendered similar to the human level. However, autoregressive voice cloning systems still suffer from text alignment failures, resulting in an inability to synthesize long sentences. In this work, we propose a variant of attention-based text-to-speech system that can reproduce a target voice from a few seconds of reference speech and generalize to very long utterances as well. The proposed system is based on three independently trained components: a speaker encoder, synthesizer and universal vocoder. Generalization to long utterances is realized using an energy-based attention mechanism known as Dynamic Convolution Attention, in combination with a set of modifications proposed for the synthesizer based on Tacotron 2. Moreover, effective zero-shot speaker adaptation is achieved by conditioning both the synthesizer and vocoder on a speaker encoder that has been pretrained on a large corpus of diverse data. We compare several implementations of voice cloning systems in terms of speech naturalness, speaker similarity, alignment consistency and ability to synthesize long utterances, and conclude that the proposed model can produce intelligible synthetic speech for extremely long utterances, while preserving a high extent of naturalness and similarity for short texts.      
### 8.Resource-efficient Deep Neural Networks for Automotive Radar Interference Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2201.10360.pdf)
>  Radar sensors are crucial for environment perception of driver assistance systems as well as autonomous vehicles. With a rising number of radar sensors and the so far unregulated automotive radar frequency band, mutual interference is inevitable and must be dealt with. Algorithms and models operating on radar data are required to run the early processing steps on specialized radar sensor hardware. This specialized hardware typically has strict resource-constraints, i.e. a low memory capacity and low computational power. Convolutional Neural Network (CNN)-based approaches for denoising and interference mitigation yield promising results for radar processing in terms of performance. Regarding resource-constraints, however, CNNs typically exceed the hardware's capacities by far. <br>In this paper we investigate quantization techniques for CNN-based denoising and interference mitigation of radar signals. We analyze the quantization of (i) weights and (ii) activations of different CNN-based model architectures. This quantization results in reduced memory requirements for model storage and during inference. We compare models with fixed and learned bit-widths and contrast two different methodologies for training quantized CNNs, i.e. the straight-through gradient estimator and training distributions over discrete weights. We illustrate the importance of structurally small real-valued base models for quantization and show that learned bit-widths yield the smallest models. We achieve a memory reduction of around 80\% compared to the real-valued baseline. Due to practical reasons, however, we recommend the use of 8 bits for weights and activations, which results in models that require only 0.2 megabytes of memory.      
### 9.Ultra Low-Parameter Denoising: Trainable Bilateral Filter Layers in Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2201.10345.pdf)
>  Computed tomography is widely used as an imaging tool to visualize three-dimensional structures with expressive bone-soft tissue contrast. However, CT resolution and radiation dose are tightly entangled, highlighting the importance of low-dose CT combined with sophisticated denoising algorithms. Most data-driven denoising techniques are based on deep neural networks and, therefore, contain hundreds of thousands of trainable parameters, making them incomprehensible and prone to prediction failures. Developing understandable and robust denoising algorithms achieving state-of-the-art performance helps to minimize radiation dose while maintaining data integrity. This work presents an open-source CT denoising framework based on the idea of bilateral filtering. We propose a bilateral filter that can be incorporated into a deep learning pipeline and optimized in a purely data-driven way by calculating the gradient flow toward its hyperparameters and its input. Denoising in pure image-to-image pipelines and across different domains such as raw detector data and reconstructed volume, using a differentiable backprojection layer, is demonstrated. Although only using three spatial parameters and one range parameter per filter layer, the proposed denoising pipelines can compete with deep state-of-the-art denoising architectures with several hundred thousand parameters. Competitive denoising performance is achieved on x-ray microscope bone data (0.7053 and 33.10) and the 2016 Low Dose CT Grand Challenge dataset (0.9674 and 43.07) in terms of SSIM and PSNR. Due to the extremely low number of trainable parameters with well-defined effect, prediction reliance and data integrity is guaranteed at any time in the proposed pipelines, in contrast to most other deep learning-based denoising architectures.      
### 10.Addressing the Intra-class Mode Collapse Problem using Adaptive Input Image Normalization in GAN-based X-ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.10324.pdf)
>  Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment and balance datasets. It is important to generate synthetic images that incorporate a diverse range of features such that they accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of machine learning classifiers. The mode collapse problem can impact a Generative Adversarial Network's capacity to generate diversified images. The mode collapse comes in two varieties; intra-class and inter-class. In this paper, the intra-class mode collapse problem is investigated, and its subsequent impact on the diversity of synthetic X-ray images is evaluated. This work contributes an empirical demonstration of the benefits of integrating the adaptive input-image normalization for the Deep Convolutional GAN to alleviate the intra-class mode collapse problem. Results demonstrate that the DCGAN with adaptive input-image normalization outperforms DCGAN with un-normalized X-ray images as evident by the superior diversity scores.      
### 11.Mutual information neural estimation for unsupervised multi-modal registration of brain images  [ :arrow_down: ](https://arxiv.org/pdf/2201.10305.pdf)
>  Many applications in image-guided surgery and therapy require fast and reliable non-linear, multi-modal image registration. Recently proposed unsupervised deep learning-based registration methods have demonstrated superior performance compared to iterative methods in just a fraction of the time. Most of the learning-based methods have focused on mono-modal image registration. The extension to multi-modal registration depends on the use of an appropriate similarity function, such as the mutual information (MI). We propose guiding the training of a deep learning-based registration method with MI estimation between an image-pair in an end-to-end trainable network. Our results show that a small, 2-layer network produces competitive results in both mono- and multimodal registration, with sub-second run-times. Comparisons to both iterative and deep learning-based methods show that our MI-based method produces topologically and qualitatively superior results with an extremely low rate of non-diffeomorphic transformations. Real-time clinical application will benefit from a better visual matching of anatomical structures and less registration failures/outliers.      
### 12.Improved normal-boundary intersection algorithm: a method for energy optimization strategy in smart buildings  [ :arrow_down: ](https://arxiv.org/pdf/2201.10303.pdf)
>  With the widespread use of distributed energy sources, the advantages of smart buildings over traditional buildings are becoming increasingly obvious. Subsequently, its energy optimal scheduling and multi-objective optimization have become more and more complex and need to be solved urgently. This paper presents a novel method to optimize energy utilization in smart buildings. Firstly, multiple transfer-retention ratio (TRR) parameters are added to the evaluation of distributed renewable energy. Secondly, the normal-boundary intersection (NBI) algorithm is improved by the adaptive weight sum, the adjust uniform axes method, and Mahalanobis distance to form the improved normal-boundary intersection (INBI) algorithm. The multi-objective optimization problem in smart buildings is solved by the parameter TRR and INBI algorithm to improve the regulation efficiency. In response to the needs of decision-makers with evaluation indicators, the average deviation is reduced by 60% compared with the previous case. Numerical examples show that the proposed method is superior to the existing technologies in terms of three optimization objectives. The objectives include 8.2% reduction in equipment costs, 7.6% reduction in power supply costs, and 1.6% improvement in occupants' comfort.      
### 13.RadiOrchestra: Proactive Management of Millimeter-wave Self-backhauled Small Cells via Joint Optimization of Beamforming, User Association, Rate Selection, and Admission Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.10297.pdf)
>  Millimeter-wave self-backhauled small cells are a key component of next-generation wireless networks. Their dense deployment will increase data rates, reduce latency, and enable efficient data transport between the access and backhaul networks, providing greater flexibility not previously possible with optical fiber. Despite their high potential, operating dense self-backhauled networks optimally is an open challenge, particularly for radio resource management (RRM). This paper presents, RadiOrchestra, a holistic RRM framework that models and optimizes beamforming, rate selection as well as user association and admission control for self-backhauled networks. The framework is designed to account for practical challenges such as hardware limitations of base stations (e.g., computational capacity, discrete rates), the need for adaptability of backhaul links, and the presence of interference. Our framework is formulated as a nonconvex mixed-integer nonlinear program, which is challenging to solve. To approach this problem, we propose three algorithms that provide a trade-off between complexity and optimality. Furthermore, we derive upper and lower bounds to characterize the performance limits of the system. We evaluate the developed strategies in various scenarios, showing the feasibility of deploying practical self-backhauling in future networks.      
### 14.S2MS: Self-Supervised Learning Driven Multi-Spectral CT Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2201.10294.pdf)
>  Photon counting spectral CT (PCCT) can produce reconstructed attenuation maps in different energy channels, reflecting energy properties of the scanned object. Due to the limited photon numbers and the non-ideal detector response of each energy channel, the reconstructed images usually contain much noise. With the development of Deep Learning (DL) technique, different kinds of DL-based models have been proposed for noise reduction. However, most of the models require clean data set as the training labels, which are not always available in medical imaging field. Inspiring by the similarities of each channel's reconstructed image, we proposed a self-supervised learning based PCCT image enhancement framework via multi-spectral channels (S2MS). In S2MS framework, both the input and output labels are noisy images. Specifically, one single channel image was used as output while images of other single channels and channel-sum image were used as input to train the network, which can fully use the spectral data information without extra cost. The simulation results based on the AAPM Low-dose CT Challenge database showed that the proposed S2MS model can suppress the noise and preserve details more effectively in comparison with the traditional DL models, which has potential to improve the image quality of PCCT in clinical applications.      
### 15.Identification of System Vulnerability under a Smart Sensor Attack via Attack Model Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2201.10247.pdf)
>  In this work, we investigate how to make use of model reduction techniques to identify the vulnerability of a closed-loop system, consisting of a plant and a supervisor, that might invite attacks. Here, the system vulnerability refers to the existence of key observation sequences that could be exploited by a specific smart sensor attack to cause damage infliction. We consider a nondeterministic smart attack, i.e., there might exist more than one attack choice over each received observation, and adopt our previously proposed modeling framework, where such an attack is captured by a standard finite-state automaton. For a given supervisor S and a smart sensor attack model A, another smart attack model A' is called attack equivalent to A with respect to S, if the resulting compromised supervisor, defined as the composition of the supervisor S and attack model A', is control equivalent to the original compromised supervisor, defined as the composition of S and A. Following the spirit of supervisor reduction that relies on the concept of control congruence, we will show that, this problem of synthesizing a reduced smart attack model A' that is attack equivalent to A with respect to S, can be transformed to a classical supervisor reduction problem, making all existing synthesis tools available for supervisor reduction directly applicable to our problem. A simplified and ideally minimum-state attack model can reveal all necessary observation sequences for the attacker to be successful, thus, reminds system designers to take necessary precautions in advance, which may improve system resilience significantly. An example is presented to show the effectiveness of our proposed attack model reduction technique to identify the system vulnerability.      
### 16.Improving the fusion of acoustic and text representations in RNN-T  [ :arrow_down: ](https://arxiv.org/pdf/2201.10240.pdf)
>  The recurrent neural network transducer (RNN-T) has recently become the mainstream end-to-end approach for streaming automatic speech recognition (ASR). To estimate the output distributions over subword units, RNN-T uses a fully connected layer as the joint network to fuse the acoustic representations extracted using the acoustic encoder with the text representations obtained using the prediction network based on the previous subword units. In this paper, we propose to use gating, bilinear pooling, and a combination of them in the joint network to produce more expressive representations to feed into the output layer. A regularisation method is also proposed to enable better acoustic encoder training by reducing the gradients back-propagated into the prediction network at the beginning of RNN-T training. Experimental results on a multilingual ASR setting for voice search over nine languages show that the joint use of the proposed methods can result in 4%--5% relative word error rate reductions with only a few million extra parameters.      
### 17.Learning Controllers from Data via Approximate Nonlinearity Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2201.10232.pdf)
>  We introduce a method to deal with the data-driven control design of nonlinear systems. We derive conditions to design controllers via (approximate) nonlinearity cancellation. These conditions take the compact form of data-dependent semi-definite programs. The method returns controllers that can be certified to stabilize the system even when data are perturbed and disturbances affect the dynamics of the system during the execution of the control task, in which case an estimate of the robustly positively invariant set is provided.      
### 18.Unitary-Precoded Single-Carrier Waveforms for High Mobility: Detection and Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2201.10218.pdf)
>  This paper presents unitary-precoded single-carrier (USC) modulation as a family of waveforms based on multiplexing the information symbols on time domain unitary basis functions. The common property of these basis functions is that they span the entire time and frequency plane. The recently proposed orthogonal time frequency space (OTFS) and orthogonal time sequency multiplexing (OTSM) based on discrete Fourier transform (DFT) and Walsh Hadamard transform (WHT), respectively, fall in the general framework of USC waveforms. In this work, we present channel estimation and detection methods that work for any USC waveform and numerically show that any choice of unitary precoding results in the same error performance. Lastly, we implement some USC systems and compare their performance with OFDM in a real-time indoor setting using an SDR platform.      
### 19.SPIRAL: Self-supervised Perturbation-Invariant Representation Learning for Speech Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2201.10207.pdf)
>  We introduce a new approach for speech pre-training named SPIRAL which works by learning denoising representation of perturbed data in a teacher-student framework. Specifically, given a speech utterance, we first feed the utterance to a teacher network to obtain corresponding representation. Then the same utterance is perturbed and fed to a student network. The student network is trained to output representation resembling that of the teacher. At the same time, the teacher network is updated as moving average of student's weights over training steps. In order to prevent representation collapse, we apply an in-utterance contrastive loss as pre-training objective and impose position randomization on the input to the teacher. SPIRAL achieves competitive or better results compared to state-of-the-art speech pre-training method wav2vec 2.0, with significant reduction of training cost (80% for Base model, 65% for Large model). Furthermore, we address the problem of noise-robustness that is critical to real-world speech applications. We propose multi-condition pre-training by perturbing the student's input with various types of additive noise. We demonstrate that multi-condition pre-trained SPIRAL models are more robust to noisy speech (9.0% - 13.3% relative word error rate reduction on real noisy test data), compared to applying multi-condition training solely in the fine-tuning stage. The code will be released after publication.      
### 20.Run-and-back stitch search: novel block synchronous decoding for streaming encoder-decoder ASR  [ :arrow_down: ](https://arxiv.org/pdf/2201.10190.pdf)
>  A streaming style inference of encoder-decoder automatic speech recognition (ASR) system is important for reducing latency, which is essential for interactive use cases. To this end, we propose a novel blockwise synchronous decoding algorithm with a hybrid approach that combines endpoint prediction and endpoint post-determination. In the endpoint prediction, we compute the expectation of the number of tokens that are yet to be emitted in the encoder features of the current blocks using the CTC posterior. Based on the expectation value, the decoder predicts the endpoint to realize continuous block synchronization, as a running stitch. Meanwhile, endpoint post-determination probabilistically detects backward jump of the source-target attention, which is caused by the misprediction of endpoints. Then it resumes decoding by discarding those hypotheses, as back stitch. We combine these methods into a hybrid approach, namely run-and-back stitch search, which reduces the computational cost and latency. Evaluations of various ASR tasks show the efficiency of our proposed decoding algorithm, which achieves a latency reduction, for instance in the Librispeech test set from 1487 ms to 821 ms at the 90th percentile, while maintaining a high recognition accuracy.      
### 21.Dense Pixel-Labeling for Reverse-Transfer and Diagnostic Learning on Lung Ultrasound for COVID-19 and Pneumonia Detection  [ :arrow_down: ](https://arxiv.org/pdf/2201.10166.pdf)
>  We propose using a pre-trained segmentation model to perform diagnostic classification in order to achieve better generalization and interpretability, terming the technique reverse-transfer learning. We present an architecture to convert segmentation models to classification models. We compare and contrast dense vs sparse segmentation labeling and study its impact on diagnostic classification. We compare the performance of U-Net trained with dense and sparse labels to segment A-lines, B-lines, and Pleural lines on a custom dataset of lung ultrasound scans from 4 patients. Our experiments show that dense labels help reduce false positive detection. We study the classification capability of the dense and sparse trained U-Net and contrast it with a non-pretrained U-Net, to detect and differentiate COVID-19 and Pneumonia on a large ultrasound dataset of about 40k curvilinear and linear probe images. Our segmentation-based models perform better classification when using pretrained segmentation weights, with the dense-label pretrained U-Net performing the best.      
### 22.Adaptive Outlier Detection for Power MOSFETs Based on Gaussian Process Regression  [ :arrow_down: ](https://arxiv.org/pdf/2201.10126.pdf)
>  Outlier detection of semiconductor devices is important since manufacturing variation is inherently inevitable. In order to properly detect outliers, it is necessary to consider the discrepancy from underlying trend. Conventional methods are insufficient as they cannot track spatial changes of the trend}}. This study proposes an adaptive outlier detection using Gaussian process regression (GPR) with Student-t likelihood, which captures a gradual spatial change of characteristic variation. According to the credible interval of the GPR posterior distribution, the devices having excessively large deviations against the underlying trend are detected. The proposed methodology is validated by the experiments using a commercial SiC wafer and simulation.      
### 23.Prediction of Neonatal Respiratory Distress in Term Babies at Birth from Digital Stethoscope Recorded Chest Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2201.10105.pdf)
>  Neonatal respiratory distress is a common condition that if left untreated, can lead to short- and long-term complications. This paper investigates the usage of digital stethoscope recorded chest sounds taken within 1min post-delivery, to enable early detection and prediction of neonatal respiratory distress. Fifty-one term newborns were included in this study, 9 of whom developed respiratory distress. For each newborn, 1min anterior and posterior recordings were taken. These recordings were pre-processed to remove noisy segments and obtain high-quality heart and lung sounds. The random undersampling boosting (RUSBoost) classifier was then trained on a variety of features, such as power and vital sign features extracted from the heart and lung sounds. The RUSBoost algorithm produced specificity, sensitivity, and accuracy results of 85.0%, 66.7% and 81.8%, respectively.      
### 24.Improving non-autoregressive end-to-end speech recognition with pre-trained acoustic and language models  [ :arrow_down: ](https://arxiv.org/pdf/2201.10103.pdf)
>  While Transformers have achieved promising results in end-to-end (E2E) automatic speech recognition (ASR), their autoregressive (AR) structure becomes a bottleneck for speeding up the decoding process. For real-world deployment, ASR systems are desired to be highly accurate while achieving fast inference. Non-autoregressive (NAR) models have become a popular alternative due to their fast inference speed, but they still fall behind AR systems in recognition accuracy. To fulfill the two demands, in this paper, we propose a NAR CTC/attention model utilizing both pre-trained acoustic and language models: wav2vec2.0 and BERT. To bridge the modality gap between speech and text representations obtained from the pre-trained models, we design a novel modality conversion mechanism, which is more suitable for logographic languages. During inference, we employ a CTC branch to generate a target length, which enables the BERT to predict tokens in parallel. We also design a cache-based CTC/attention joint decoding method to improve the recognition accuracy while keeping the decoding speed fast. Experimental results show that the proposed NAR model greatly outperforms our strong wav2vec2.0 CTC baseline (15.1% relative CER reduction on AISHELL-1). The proposed NAR model significantly surpasses previous NAR systems on the AISHELL-1 benchmark and shows a potential for English tasks.      
### 25.A Wearable ECG Monitor for Deep Learning Based Real-Time Cardiovascular Disease Detection  [ :arrow_down: ](https://arxiv.org/pdf/2201.10083.pdf)
>  Cardiovascular disease has become one of the most significant threats endangering human life and health. Recently, Electrocardiogram (ECG) monitoring has been transformed into remote cardiac monitoring by Holter surveillance. However, the widely used Holter can bring a great deal of discomfort and inconvenience to the individuals who carry them. We developed a new wireless ECG patch in this work and applied a deep learning framework based on the Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) models. However, we find that the models using the existing techniques are not able to differentiate two main heartbeat types (Supraventricular premature beat and Atrial fibrillation) in our newly obtained dataset, resulting in low accuracy of 58.0 %. We proposed a semi-supervised method to process the badly labelled data samples with using the confidence-level-based training. The experiment results conclude that the proposed method can approach an average accuracy of 90.2 %, i.e., 5.4 % higher than the accuracy of conventional ECG classification methods.      
### 26.Negative-ResNet: Noisy Ambulatory Electrocardiogram Signal Classification Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2201.10061.pdf)
>  With recently successful applications of deep learning in computer vision and general signal processing, deep learning has shown many unique advantages in medical signal processing. However, data labelling quality has become one of the most significant issues for AI applications, especially when it requires domain knowledge (e.g. medical image labelling). In addition, there might be noisy labels in practical datasets, which might impair the training process of neural networks. In this work, we propose a semi-supervised algorithm for training data samples with noisy labels by performing selected Positive Learning (PL) and Negative Learning (NL). To verify the effectiveness of the proposed scheme, we designed a portable ECG patch -- iRealCare -- and applied the algorithm on a real-life dataset. Our experimental results show that we can achieve an accuracy of 91.0 %, which is 6.2 % higher than a normal training process with ResNet. There are 65 patients in our dataset and we randomly picked 2 patients to perform validation.      
### 27.Underwater Acoustic Communication Channel Modeling using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.10056.pdf)
>  With the recent increase in the number of underwater activities, having effective underwater communication systems has become increasingly important. Underwater acoustic communication has been widely used but greatly impaired due to the complicated nature of the underwater environment. In a bid to better understand the underwater acoustic channel so as to help in the design and improvement of underwater communication systems, attempts have been made to model the underwater acoustic channel using mathematical equations and approximations under some assumptions. In this paper, we explore the capability of machine learning and deep learning methods to learn and accurately model the underwater acoustic channel using real underwater data collected from a water tank with disturbance and from lake Tahoe. Specifically, Deep Neural Network (DNN) and Long Short Term Memory (LSTM) are applied to model the underwater acoustic channel. Experimental results show that these models are able to model the underwater acoustic communication channel well and that deep learning models, especially LSTM are better models in terms of mean absolute percentage error.      
### 28.Variational Autoencoders for Reliability Optimization in Multi-Access Edge Computing Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.10032.pdf)
>  Multi-access edge computing (MEC) is viewed as an integral part of future wireless networks to support new applications with stringent service reliability and latency requirements. However, guaranteeing ultra-reliable and low-latency MEC (URLL MEC) is very challenging due to uncertainties of wireless links, limited communications and computing resources, as well as dynamic network traffic. Enabling URLL MEC mandates taking into account the statistics of the end-to-end (E2E) latency and reliability across the wireless and edge computing systems. In this paper, a novel framework is proposed to optimize the reliability of MEC networks by considering the distribution of E2E service delay, encompassing over-the-air transmission and edge computing latency. The proposed framework builds on correlated variational autoencoders (VAEs) to estimate the full distribution of the E2E service delay. Using this result, a new optimization problem based on risk theory is formulated to maximize the network reliability by minimizing the Conditional Value at Risk (CVaR) as a risk measure of the E2E service delay. To solve this problem, a new algorithm is developed to efficiently allocate users' processing tasks to edge computing servers across the MEC network, while considering the statistics of the E2E service delay learned by VAEs. The simulation results show that the proposed scheme outperforms several baselines that do not account for the risk analyses or statistics of the E2E service delay.      
### 29.Endpoint Detection for Streaming End-to-End Multi-talker ASR  [ :arrow_down: ](https://arxiv.org/pdf/2201.09979.pdf)
>  Streaming end-to-end multi-talker speech recognition aims at transcribing the overlapped speech from conversations or meetings with an all-neural model in a streaming fashion, which is fundamentally different from a modular-based approach that usually cascades the speech separation and the speech recognition models trained independently. Previously, we proposed the Streaming Unmixing and Recognition Transducer (SURT) model based on recurrent neural network transducer (RNN-T) for this problem and presented promising results. However, for real applications, the speech recognition system is also required to determine the timestamp when a speaker finishes speaking for prompt system response. This problem, known as endpoint (EP) detection, has not been studied previously for multi-talker end-to-end models. In this work, we address the EP detection problem in the SURT framework by introducing an end-of-sentence token as an output unit, following the practice of single-talker end-to-end models. Furthermore, we also present a latency penalty approach that can significantly cut down the EP detection latency. Our experimental results based on the 2-speaker LibrispeechMix dataset show that the SURT model can achieve promising EP detection without significantly degradation of the recognition accuracy.      
### 30.An empirical model for feedforward control of laser powder bed fusion  [ :arrow_down: ](https://arxiv.org/pdf/2201.09978.pdf)
>  While considerable progress has recently been made in real-time melt pool monitoring for laser powder bed fusion (LPBF), results in in-situ melt pool control are relatively sparse, a major reason being lack of suitable control-oriented models. This study demonstrates an empirical control-oriented model of geometry-dependent melt pool behavior, and subsequent melt pool regulation with a model-based feedforward controller for laser power. First, it shows that the melt pool "footprint" exponentially increases when the scan lines become shorter. The empirical model of this behavior is developed and validated on different geometries at different laser power levels. Second, the developed model is used to design a feedforward controller for obtaining optimal laser power profiles. This controller is then validated experimentally and is demonstrated to suppress the in-layer geometry-related melt pool signal deviations, for different part geometries.      
### 31.COVID-19 Detection Using CT Image Based On YOLOv5 Network  [ :arrow_down: ](https://arxiv.org/pdf/2201.09972.pdf)
>  Computer aided diagnosis (CAD) increases diagnosis efficiency, helping doctors providing a quick and confident diagnosis, it has played an important role in the treatment of COVID19. In our task, we solve the problem about abnormality detection and classification. The dataset provided by Kaggle platform and we choose YOLOv5 as our model. We introduce some methods on objective detection in the related work section, the objection detection can be divided into two streams: onestage and two stage. The representational model are Faster RCNN and YOLO series. Then we describe the YOLOv5 model in the detail. Compared Experiments and results are shown in section IV. We choose mean average precision (mAP) as our experiments' metrics, and the higher (mean) mAP is, the better result the model will gain. mAP@0.5 of our YOLOv5s is 0.623 which is 0.157 and 0.101 higher than Faster RCNN and EfficientDet respectively.      
### 32.A Deep Learning Approach for the Detection of COVID-19 from Chest X-Ray Images using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.09952.pdf)
>  The COVID-19 (coronavirus) is an ongoing pandemic caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The virus was first identified in mid-December 2019 in the Hubei province of Wuhan, China and by now has spread throughout the planet with more than 75.5 million confirmed cases and more than 1.67 million deaths. With limited number of COVID-19 test kits available in medical facilities, it is important to develop and implement an automatic detection system as an alternative diagnosis option for COVID-19 detection that can used on a commercial scale. Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Computer vision and deep learning techniques can help in determining COVID-19 virus with Chest X-ray Images. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural network for image analysis and classification. In this research, we have proposed a deep convolutional neural network trained on five open access datasets with binary output: Normal and Covid. The performance of the model is compared with four pre-trained convolutional neural network-based models (COVID-Net, ResNet18, ResNet and MobileNet-V2) and it has been seen that the proposed model provides better accuracy on the validation set as compared to the other four pre-trained models. This research work provides promising results which can be further improvise and implement on a commercial scale.      
### 33.Microphone Utility Estimation in Acoustic Sensor Networks using Single-Channel Signal Features  [ :arrow_down: ](https://arxiv.org/pdf/2201.09946.pdf)
>  In multichannel signal processing with distributed sensors, choosing the optimal subset of observed sensor signals to be exploited is crucial in order to maximize algorithmic performance and reduce computational load, ideally both at the same time. In the acoustic domain, signal cross-correlation is a natural choice to quantify the usefulness of microphone signals, i. e., microphone utility, for array processing, but its estimation requires that the uncoded signals are synchronized and transmitted between nodes. In resource-constrained environments like acoustic sensor networks, low data transmission rates often make transmission of all observed signals to the centralized location infeasible, thus discouraging direct estimation of signal cross-correlation. Instead, we employ characteristic features of the recorded signals to estimate the usefulness of individual microphone signals. In this contribution, we provide a comprehensive analysis of model-based microphone utility estimation approaches that use signal features and, as an alternative, also propose machine learning-based estimation methods that identify optimal sensor signal utility features. The performance of both approaches is validated experimentally using both simulated and recorded acoustic data, comprising a variety of realistic and practically relevant acoustic scenarios including moving and static sources.      
### 34.Low Complexity Channel estimation with Neural Network Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2201.09934.pdf)
>  Research on machine learning for channel estimation, especially neural network solutions for wireless communications, is attracting significant current interest. This is because conventional methods cannot meet the present demands of the high speed communication. In the paper, we deploy a general residual convolutional neural network to achieve channel estimation for the orthogonal frequency-division multiplexing (OFDM) signals in a downlink scenario. Our method also deploys a simple interpolation layer to replace the transposed convolutional layer used in other networks to reduce the computation cost. The proposed method is more easily adapted to different pilot patterns and packet sizes. Compared with other deep learning methods for channel estimation, our results for 3GPP channel models suggest improved mean squared error performance for our approach.      
### 35.A Novel Temporal Attentive-Pooling based Convolutional Recurrent Architecture for Acoustic Signal Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2201.09913.pdf)
>  In acoustic signal processing, the target signals usually carry semantic information, which is encoded in a hierarchal structure of short and long-term contexts. However, the background noise distorts these structures in a nonuniform way. The existing deep acoustic signal enhancement (ASE) architectures ignore this kind of local and global effect. To address this problem, we propose to integrate a novel temporal attentive-pooling (TAP) mechanism into a conventional convolutional recurrent neural network, termed as TAP-CRNN. The proposed approach considers both global and local attention for ASE tasks. Specifically, we first utilize a convolutional layer to extract local information of the acoustic signals and then a recurrent neural network (RNN) architecture is used to characterize temporal contextual information. Second, we exploit a novelattention mechanism to contextually process salient regions of the noisy signals. The proposed ASE system is evaluated using a benchmark infant cry dataset and compared with several well-known methods. It is shown that the TAPCRNN can more effectively reduce noise components from infant cry signals in unseen background noises at challenging signal-to-noise levels.      
### 36.AI-Driven Demodulators for Nonlinear Receivers in Shared Spectrum with High-Power Blockers  [ :arrow_down: ](https://arxiv.org/pdf/2201.09911.pdf)
>  Research has shown that communications systems and receivers suffer from high power adjacent channel signals, called blockers, that drive the radio frequency (RF) front end into nonlinear operation. Since simple systems, such as the Internet of Things (IoT), will coexist with sophisticated communications transceivers, radars and other spectrum consumers, these need to be protected employing a simple, yet adaptive solution to RF nonlinearity. This paper therefore proposes a flexible data driven approach that uses a simple artificial neural network (ANN) to aid in the removal of the third order intermodulation distortion (IMD) as part of the demodulation process. We introduce and numerically evaluate two artificial intelligence (AI)-enhanced receivers-ANN as the IMD canceler and ANN as the demodulator. Our results show that a simple ANN structure can significantly improve the bit error rate (BER) performance of nonlinear receivers with strong blockers and that the ANN architecture and configuration depends mainly on the RF front end characteristics, such as the third order intercept point (IP3). We therefore recommend that receivers have hardware tags and ways to monitor those over time so that the AI and software radio processing stack can be effectively customized and automatically updated to deal with changing operating conditions.      
### 37.Augmented RBMLE-UCB Approach for Adaptive Control of Linear Quadratic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.10542.pdf)
>  We consider the problem of controlling a stochastic linear system with quadratic costs, when its system parameters are not known to the agent -- called the adaptive LQG control problem. We re-examine an approach called "Reward-Biased Maximum Likelihood Estimate" (RBMLE) that was proposed more than forty years ago, and which predates the "Upper Confidence Bound" (UCB) method as well as the definition of "regret". It simply added a term favoring parameters with larger rewards to the estimation criterion. We propose an augmented approach that combines the penalty of the RBMLE method with the constraint of the UCB method, uniting the two approaches to optimization in the face of uncertainty. We first establish that theoretically this method retains $\mathcal{O}(\sqrt{T})$ regret, the best known so far. We show through a comprehensive simulation study that this augmented RBMLE method considerably outperforms the UCB and Thompson sampling approaches, with a regret that is typically less than 50\% of the better of their regrets. The simulation study includes all examples from earlier papers as well as a large collection of randomly generated systems.      
### 38.Epidemic Population Games And Evolutionary Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2201.10529.pdf)
>  We propose a system theoretic approach to select and stabilize the endemic equilibrium of an SIRS epidemic model in which the decisions of a population of strategically interacting agents determine the transmission rate. Specifically, the population's agents recurrently revise their choices out of a set of strategies that impact to varying levels the transmission rate. A payoff vector quantifying the incentives provided by a planner for each strategy, after deducting the strategies' intrinsic costs, influences the revision process. An evolutionary dynamics model captures the population's preferences in the revision process by specifying as a function of the payoff vector the rates at which the agents' choices flow toward strategies with higher payoffs. Our main result is a dynamic payoff mechanism that is guaranteed to steer the epidemic variables (via incentives to the population) to the endemic equilibrium with the smallest infectious fraction, subject to cost constraints. We use a Lyapunov function not only to establish convergence but also to obtain an (anytime) upper bound for the peak size of the population's infectious portion.      
### 39.Real-Time Deployment of a Large-Scale Multi-Quadcopter System (MQS)  [ :arrow_down: ](https://arxiv.org/pdf/2201.10509.pdf)
>  This paper presents a continuum mechanics-based approach for real-time deployment (RTD) of a multi-quadcopter system between moving initial and final configurations arbitrarily distributed in a 3-D motion space. The proposed RTD problem is decomposed into spatial planning, temporal planning and acquisition sub-problems. For the spatial planning, the RTD desired coordination is defined by integrating (i) rigid-body rotation, (ii) one-dimensional homogeneous deformation, and (ii) one-dimensional heterogeneous coordination such that necessary conditions for inter-agent collision avoidance between every two quadcopter UAVs are satisfied. By the RTD temporal planning, this paper suffices the inter-agent collision avoidance between every two individual quadcopters, and assures the boundedness of the rotor angular speeds for every individual quadcopter. For the RTD acquisition, each quadcopter modeled by a nonlinear dynamics applies a nonlinear control to stably and safely track the desired RTD trajectory such that the angular speeds of each quadcopter remain bounded and do not exceed a certain upper limit.      
### 40.Load-Altering Attacks under COVID-19 Low-Inertia Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2201.10505.pdf)
>  The COVID-19 pandemic has impacted our society by forcing shutdowns and shifting the way people interacted worldwide. In relation to the impacts on the electric grid, it created a significant decrease in energy demands across the globe. Recent studies have shown that the low load demand conditions caused by COVID-19 lockdowns combined with large renewable generation have resulted in extremely low-inertia grid conditions. In this work, we examine how an attacker could exploit these conditions to cause unsafe grid operating conditions by executing load-altering attacks (LAAs) targeted at compromising hundreds of thousands of IoT-connected high-wattage loads in low-inertia power systems. Our study focuses on analyzing the impact of the COVID-19 mitigation measures on U.S. regional transmission operators (RTOs), formulating a plausible and realistic least-effort LAA targeted at transmission systems with low-inertia conditions, and evaluating the probability of these large-scale LAAs. Theoretical and simulation results are presented based on the WSCC 9-bus test system. Results demonstrate how adversaries could provoke major frequency disturbances by targeting vulnerable load buses in low-inertia systems.      
### 41.State-space computation of quadratic-exponential functional rates for linear quantum stochastic systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.10492.pdf)
>  This paper is concerned with infinite-horizon growth rates of quadratic-exponential functionals (QEFs) for linear quantum stochastic systems driven by multichannel bosonic fields. Such risk-sensitive performance criteria impose an exponential penalty on the integral of a quadratic function of the system variables, and their minimization improves robustness properties of the system with respect to quantum statistical uncertainties and makes its behaviour more conservative in terms of tail distributions. We use a frequency-domain representation of the QEF growth rate for the invariant Gaussian quantum state of the system with vacuum input fields in order to compute it in state space. The QEF rate is related to a similar functional for a classical stationary Gaussian random process generated by an infinite cascade of linear systems. A truncation of this shaping filter allows the QEF rate to be computed with any accuracy by solving a recurrent sequence of algebraic Lyapunov equations together with an algebraic Riccati equation. The state-space computation of the QEF rate and its comparison with the frequency-domain results are demonstrated by a numerical example for an open quantum harmonic oscillator.      
### 42.PILOT: High-Precision Indoor Localization for Autonomous Drones  [ :arrow_down: ](https://arxiv.org/pdf/2201.10488.pdf)
>  In many scenarios, unmanned aerial vehicles (UAVs), aka drones, need to have the capability of autonomous flying to carry out their mission successfully. In order to allow these autonomous flights, drones need to know their location constantly. Then, based on the current position and the final destination, navigation commands will be generated and drones will be guided to their destination. Localization can be easily carried out in outdoor environments using GPS signals and drone inertial measurement units (IMUs). However, such an approach is not feasible in indoor environments or GPS-denied areas. In this paper, we propose a localization scheme for drones called PILOT (High-Precision Indoor Localization for Autonomous Drones) that is specifically designed for indoor environments. PILOT relies on ultrasonic acoustic signals to estimate the target drone's location. In order to have a precise final estimation of the drone's location, PILOT deploys a three-stage localization scheme. The first two stages provide robustness against the multi-path fading effect of indoor environments and mitigate the ranging error. Then, in the third stage, PILOT deploys a simple yet effective technique to reduce the localization error induced by the relative geometry between transmitters and receivers and significantly reduces the height estimation error. The performance of PILOT was assessed under different scenarios and the results indicate that PILOT achieves centimeter-level accuracy for three-dimensional localization of drones.      
### 43.Multi-agent Performative Prediction: From Global Stability and Optimality to Chaos  [ :arrow_down: ](https://arxiv.org/pdf/2201.10483.pdf)
>  The recent framework of performative prediction is aimed at capturing settings where predictions influence the target/outcome they want to predict. In this paper, we introduce a natural multi-agent version of this framework, where multiple decision makers try to predict the same outcome. We showcase that such competition can result in interesting phenomena by proving the possibility of phase transitions from stability to instability and eventually chaos. Specifically, we present settings of multi-agent performative prediction where under sufficient conditions their dynamics lead to global stability and optimality. In the opposite direction, when the agents are not sufficiently cautious in their learning/updates rates, we show that instability and in fact formal chaos is possible. We complement our theoretical predictions with simulations showcasing the predictive power of our results.      
### 44.Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.10439.pdf)
>  Audio-visual automatic speech recognition (AV-ASR) extends the speech recognition by introducing the video modality. In particular, the information contained in the motion of the speaker's mouth is used to augment the audio features. The video modality is traditionally processed with a 3D convolutional neural network (e.g. 3D version of VGG). Recently, image transformer networks <a class="link-https" data-arxiv-id="2010.11929" href="https://arxiv.org/abs/2010.11929">arXiv:2010.11929</a> demonstrated the ability to extract rich visual features for the image classification task. In this work, we propose to replace the 3D convolution with a video transformer video feature extractor. We train our baselines and the proposed model on a large scale corpus of the YouTube videos. Then we evaluate the performance on a labeled subset of YouTube as well as on the public corpus LRS3-TED. Our best model video-only model achieves the performance of 34.9% WER on YTDEV18 and 19.3% on LRS3-TED which is a 10% and 9% relative improvements over the convolutional baseline. We achieve the state of the art performance of the audio-visual recognition on the LRS3-TED after fine-tuning our model (1.6% WER).      
### 45.Sequential Parametric Optimization for Rate-Splitting Precoding in Non-Orthogonal Unicast and Multicast Transmissions  [ :arrow_down: ](https://arxiv.org/pdf/2201.10426.pdf)
>  This paper investigates rate-splitting (RS) precoding for non-orthogonal unicast and multicast (NOUM) transmissions using fully-digital and hybrid precoders. We study the nonconvex weighted sum-rate (WSR) maximization problem subject to a multicast requirement. We propose FALCON, an approach based on sequential parametric optimization, to solve the aforementioned problem. We show that FALCON converges to a local optimum without requiring judicious selection of an initial feasible point. Besides, we show through simulations that by leveraging RS, hybrid precoders can attain nearly the same performance as their fully-digital counterparts under certain specific settings.      
### 46.Improving Proximity Estimation for Contact Tracing using a Multi-channel Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.10401.pdf)
>  Due to the COVID 19 pandemic, smartphone-based proximity tracing systems became of utmost interest. Many of these systems use Bluetooth Low Energy (BLE) signals to estimate the distance between two persons. The quality of this method depends on many factors and, therefore, does not always deliver accurate results. In this paper, we present a multi-channel approach to improve proximity estimation, and a novel, publicly available dataset that contains matched IEEE 802.11 (2.4 GHz and 5 GHz) and BLE signal strength data, measured in four different environments. We have developed and evaluated a combined classification model based on BLE and IEEE 802.11 signals. Our approach significantly improves the distance estimation and consequently also the contact tracing accuracy. We are able to achieve good results with our approach in everyday public transport scenarios. However, in our implementation based on IEEE 802.11 probe requests, we also encountered privacy problems and limitations due to the consistency and interval at which such probes are sent. We discuss these limitations and sketch how our approach could be improved to make it suitable for real-world deployment.      
### 47.Neural Architecture Search for Spiking Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.10355.pdf)
>  Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. However, most prior SNN methods use ANN-like architectures (e.g., VGG-Net or ResNet), which could provide sub-optimal performance for temporal sequence processing of binary information in SNNs. To address this, in this paper, we introduce a novel Neural Architecture Search (NAS) approach for finding better SNN architectures. Inspired by recent NAS approaches that find the optimal architecture from activation patterns at initialization, we select the architecture that can represent diverse spike activation patterns across different data samples without training. Furthermore, to leverage the temporal correlation among the spikes, we search for feed forward connections as well as backward connections (i.e., temporal feedback connections) between layers. Interestingly, SNASNet found by our search algorithm achieves higher performance with backward connections, demonstrating the importance of designing SNN architecture for suitably using temporal information. We conduct extensive experiments on three image recognition benchmarks where we show that SNASNet achieves state-of-the-art performance with significantly lower timesteps (5 timesteps).      
### 48.Chance-constrained DC Optimal Power Flow with Non-Gaussian Distributed Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2201.10336.pdf)
>  Chance-constrained programming (CCP) is a promising approach to handle uncertainties in optimal power flow (OPF). However, conventional CCP usually assumes that uncertainties follow Gaussian distributions, which may not match reality. A few papers employed the Gaussian mixture model (GMM) to extend CCP to cases with non-Gaussian uncertainties, but they are only appropriate for cases with uncertainties on the right-hand side but not applicable to DC OPF that containing left-hand side uncertainties. To address this, we develop a tractable GMM-based chance-constrained DC OPF model. In this model, we not only leverage GMM to capture the probability characteristics of non-Gaussian distributed uncertainties, but also develop a linearization technique to reformulate the chance constraints with non-Gaussian distributed uncertainties on the left-hand side into tractable forms. A mathematical proof is further provided to demonstrate that the corresponding reformulation is a safe approximation of the original problem, which guarantees the feasibility of solutions.      
### 49.Chance-constrained regulation capacity offering for HVAC systems under non-Gaussian uncertainties with mixture-model-based convexification  [ :arrow_down: ](https://arxiv.org/pdf/2201.10329.pdf)
>  Heating, ventilation, and air-conditioning (HVAC) systems are ideal demand-side flexible resources to provide regulation services. However, finding the best hourly regulation capacity offers for HVAC systems in a power market ahead of time is challenging because they are affected by non-Gaussian uncertainties from regulation signals. Moreover, since HVAC systems need to frequently regulate their power according to regulation signals, numerous thermodynamic constraints are introduced, leading to a huge computational burden. This paper proposes a tractable chance-constrained model to address these challenges. It first develops a temporal compression approach, in which the extreme indoor temperatures in the operating hour are estimated and restricted in the comfortable range so that the numerous thermodynamic constraints can be compressed into only a few ones. Then, a novel convexification method is proposed to handle the non-Gaussian uncertainties. This method leverages the Gaussian mixture model to reformulate the chance constraints with non-Gaussian uncertainties on the left-hand side into deterministic non-convex forms. We further prove that these non-convex forms can be approximately convexified by second-order cone constraints with marginal optimality loss. Therefore, the proposed model can be efficiently solved with guaranteed optimality. Numerical experiments are conducted to validate the superiority of the proposed method.      
### 50.SASV Challenge 2022: A Spoofing Aware Speaker Verification Challenge Evaluation Plan  [ :arrow_down: ](https://arxiv.org/pdf/2201.10283.pdf)
>  ASV (automatic speaker verification) systems are intrinsically required to reject both non-target (e.g., voice uttered by different speaker) and spoofed (e.g., synthesised or converted) inputs. However, there is little consideration for how ASV systems themselves should be adapted when they are expected to encounter spoofing attacks, nor when they operate in tandem with CMs (spoofing countermeasures), much less how both systems should be jointly optimised. <br>The goal of the first SASV (spoofing-aware speaker verification) challenge, a special sesscion in ISCA INTERSPEECH 2022, is to promote development of integrated systems that can perform ASV and CM simultaneously.      
### 51.Improved Mispronunciation detection system using a hybrid CTC-ATT based approach for L2 English speakers  [ :arrow_down: ](https://arxiv.org/pdf/2201.10198.pdf)
>  This report proposes state-of-the-art research in the field of Computer Assisted Language Learning (CALL). Mispronunciation detection is one of the core components of Computer Assisted Pronunciation Training (CAPT) systems which is a subset of CALL. Studies on automated pronunciation error detection began in the 1990s, but the development of fullfledged CAPTs has only accelerated in the last decade due to an increase in computing power and availability of mobile devices for recording speech required for pronunciation analysis. Detecting Pronunciation errors is a hard problem to solve as there is no formal definition of correct and incorrect pronunciation. As a result, typically prosodic and phoneme errors such as phoneme substitution, insertion, and deletion are detected. Also, it has been agreed upon that learning pronunciation should focus on speaker intelligibility rather than sounding like an L1 English speaker. Initially, methods were developed on posterior likelihood called Good of Pronunciation using Gaussian Mixture Model-Hidden Markov Model and Deep Neural Network-Hidden Markov Model approaches. These are complex systems to implement when compared with the recently proposed ASR based End-to-End mispronunciations detection systems. The purpose of this research is to create End-to-End (E2E) models using Connectionist Temporal Classification (CTC) and Attention-based sequence decoder. Recently, E2E models have shown considerable improvement in mispronunciation detection accuracy. This research will draw comparison amongst baseline models CNN-RNN-CTC, CNN-RNN-CTC with character sequence-based attention decoder, and CNN-RNN-CTC with phoneme-based decoder systems. This study will help us in deciding a better approach towards developing an efficient mispronunciation detection system.      
### 52.Online Actuator Selection and Controller Design for Linear Quadratic Regulation over a Finite Horizon  [ :arrow_down: ](https://arxiv.org/pdf/2201.10197.pdf)
>  We study the simultaneous actuator selection and controller design problem for linear quadratic regulation over a finite horizon, when the system matrices are unknown a priori. We propose an online actuator selection algorithm to solve the problem which specifies both a set of actuators to be utilized and the control policy corresponding to the set of selected actuators. Specifically, our algorithm is a model based learning algorithm which maintains an estimate of the system matrices using the system trajectories. The algorithm then leverages an algorithm for the multiarmed bandit problem to determine the set of actuators under an actuator selection budget constraint and also identifies the corresponding control policy that minimizes a quadratic cost based on the estimated system matrices. We show that the proposed online actuator selection algorithm yields a sublinear regret.      
### 53.Improving Adversarial Waveform Generation based Singing Voice Conversion with Harmonic Signals  [ :arrow_down: ](https://arxiv.org/pdf/2201.10130.pdf)
>  Adversarial waveform generation has been a popular approach as the backend of singing voice conversion (SVC) to generate high-quality singing audio. However, the instability of GAN also leads to other problems, such as pitch jitters and U/V errors. It affects the smoothness and continuity of harmonics, hence degrades the conversion quality seriously. This paper proposes to feed harmonic signals to the SVC model in advance to enhance audio generation. We extract the sine excitation from the pitch, and filter it with a linear time-varying (LTV) filter estimated by a neural network. Both these two harmonic signals are adopted as the inputs to generate the singing waveform. In our experiments, two mainstream models, MelGAN and ParallelWaveGAN, are investigated to validate the effectiveness of the proposed approach. We conduct a MOS test on clean and noisy test sets. The result shows that both signals significantly improve SVC in fidelity and timbre similarity. Besides, the case analysis further validates that this method enhances the smoothness and continuity of harmonics in the generated audio, and the filtered excitation better matches the target audio.      
### 54.Automated brain parcellation rendering and visualization in R with coldcuts  [ :arrow_down: ](https://arxiv.org/pdf/2201.10116.pdf)
>  Parcellations are fundamental tools in neuroanatomy, allowing researchers to place functional imaging and molecular data within a structural context in the brain. Visualizing these parcellations is critical to guide biological understanding of clinical and experimental datasets in humans and model organisms. However, software used to visualize parcellations is different from the one used to analyze these datasets, greatly limiting the visualization of experimental data within parcellations. We present coldcuts, an open source R package that allows to automatically generate, store and visualize any volume-based parcellation easily and with minimal manual curation. coldcuts allows to integrate external datasets and offers rich 2D and 3D visualizations. coldcuts is freely available at <a class="link-external link-http" href="http://github.com/langleylab/coldcuts" rel="external noopener nofollow">this http URL</a> and several curated coldcuts objects are made available for human, mouse, chimpanzee and Drosophila parcellations at <a class="link-external link-https" href="https://github.com/langleylab/coldcuts_segmentations" rel="external noopener nofollow">this https URL</a>.      
### 55.ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals  [ :arrow_down: ](https://arxiv.org/pdf/2201.10060.pdf)
>  Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. DL models are, however, mainly designed to be applied on sparse sEMG signals. Furthermore, due to their complex structure, typically, we are faced with memory constraints; require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different complex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62 +/- 3.07%, where only 78, 210 number of parameters is utilized. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.      
### 56.Performance Analysis of Multiple-Antenna Ambient Backscatter Systems at Finite Blocklengths  [ :arrow_down: ](https://arxiv.org/pdf/2201.10042.pdf)
>  This paper analyzes the maximal achievable rate for a given blocklength and error probability over a multiple-antenna ambient backscatter channel with perfect channel state information at the receiver. The result consists of a finite blocklength channel coding achievability bound and a converse bound based on the Neyman-Pearson test and the normal approximation based on the Berry- Esseen Theorem. Numerical evaluation of these bounds shows fast convergence to the channel capacity as the blocklength increases and also proves that the channel dispersion is an accurate measure of the backoff from capacity due to finite blocklength.      
### 57.A Holistic Approach for Designing Carbon Aware Datacenters  [ :arrow_down: ](https://arxiv.org/pdf/2201.10036.pdf)
>  Technology companies have been leading the way to a renewable energy transformation, by investing in renewable energy sources to reduce the carbon footprint of their datacenters. In addition to helping build new solar and wind farms, companies make power purchase agreements or purchase carbon offsets, rather than relying on renewable energy every hour of the day, every day of the week (24/7). Relying on renewable energy 24/7 is challenging due to the intermittent nature of wind and solar energy. Inherent variations in solar and wind energy production causes excess or lack of supply at different times. To cope with the fluctuations of renewable energy generation, multiple solutions must be applied. These include: capacity sizing with a mix of solar and wind power, energy storage options, and carbon aware workload scheduling. However, depending on the region and datacenter workload characteristics, the carbon-optimal solution varies. Existing work in this space does not give a holistic view of the trade-offs of each solution and often ignore the embodied carbon cost of the solutions. In this work, we provide a framework, Carbon Explorer, to analyze the multi-dimensional solution space by taking into account operational and embodided footprint of the solutions to help make datacenters operate on renewable energy 24/7. The solutions we analyze include capacity sizing with a mix of solar and wind power, battery storage, and carbon aware workload scheduling, which entails shifting the workloads from times when there is lack of renewable supply to times with abundant supply. Carbon Explorer will be open-sourced soon.      
### 58.Online Convex Optimization Using Coordinate Descent Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2201.10017.pdf)
>  This paper considers the problem of online optimization where the objective function is time-varying. In particular, we extend coordinate descent type algorithms to the online case, where the objective function varies after a finite number of iterations of the algorithm. Instead of solving the problem exactly at each time step, we only apply a finite number of iterations at each time step. Commonly used notions of regret are used to measure the performance of the online algorithm. Moreover, coordinate descent algorithms with different updating rules are considered, including both deterministic and stochastic rules that are developed in the literature of classical offline optimization. A thorough regret analysis is given for each case. Finally, numerical simulations are provided to illustrate the theoretical results.      
### 59.Structural Properties of Optimal Fidelity Selection Policies for Human-in-the-loop Queues  [ :arrow_down: ](https://arxiv.org/pdf/2201.09990.pdf)
>  We study optimal fidelity selection for a human operator servicing a queue of homogeneous tasks. The agent can service a task with a normal or high fidelity level, where fidelity refers to the degree of exactness and precision while servicing the task. Therefore, high fidelity servicing results in higher-quality service but leads to larger service times and increased operator tiredness. We treat the cognitive state of the human operator as a lumped parameter that captures psychological factors such as workload and fatigue. The service time distribution of the human operator depends on her cognitive dynamics and the fidelity level selected for servicing the task. Her cognitive dynamics evolve as a Markov chain in which the cognitive state increases with high probability whenever she is busy and decreases while resting. The tasks arrive according to a Poisson process and each task waiting in the queue loses its value at a fixed rate. We address the trade-off between high-quality service of the task and consequent loss in value of subsequent tasks using a Semi-Markov Decision Process (SMDP) framework. We numerically determine an optimal policy and the corresponding optimal value function. Finally, we establish structural properties of an optimal fidelity policy and provide conditions under which the optimal policy is a threshold-based policy.      
