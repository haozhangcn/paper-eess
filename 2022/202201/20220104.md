# ArXiv eess --Tue, 4 Jan 2022
### 1.Handover Experiments with UAVs: Software Radio Tools and Experimental Research Platform  [ :arrow_down: ](https://arxiv.org/pdf/2201.00779.pdf)
>  Mobility management is the key feature of cellular networks. When integrating unmanned aerial vehicles (UAVs) into cellular networks, their cell association needs to be carefully managed for coexistence with other cellular users. UAVs move in three dimensions and may traverse several cells on their flight path, and so may be subject to several handovers. In order to enable research on mobility management with UAV users, this paper describes the design, implementation, and testing methodology for handover experiments with aerial users. We leverage software-defined radios (SDRs) and implement a series of tools for preparing the experiment in the laboratory and for taking it outdoors for field testing. We use solely commercial off-the-shelf hardware, open-source software, and an experimental license to enable reproducible and scalable experiments. Our initial outdoor results with two SDR base stations connected to an open-source software core network, implementing the 4G long-term evolution protocol, and one low altitude UAV user equipment demonstrate the handover process.      
### 2.BDG-Net: Boundary Distribution Guided Network for Accurate Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.00767.pdf)
>  Colorectal cancer (CRC) is one of the most common fatal cancer in the world. Polypectomy can effectively interrupt the progression of adenoma to adenocarcinoma, thus reducing the risk of CRC development. Colonoscopy is the primary method to find colonic polyps. However, due to the different sizes of polyps and the unclear boundary between polyps and their surrounding mucosa, it is challenging to segment polyps accurately. To address this problem, we design a Boundary Distribution Guided Network (BDG-Net) for accurate polyp segmentation. Specifically, under the supervision of the ideal Boundary Distribution Map (BDM), we use Boundary Distribution Generate Module (BDGM) to aggregate high-level features and generate BDM. Then, BDM is sent to the Boundary Distribution Guided Decoder (BDGD) as complementary spatial information to guide the polyp segmentation. Moreover, a multi-scale feature interaction strategy is adopted in BDGD to improve the segmentation accuracy of polyps with different sizes. Extensive quantitative and qualitative evaluations demonstrate the effectiveness of our model, which outperforms state-of-the-art models remarkably on five public polyp datasets while maintaining low computational complexity.      
### 3.Data Driven Safe Gain-Scheduling Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.00707.pdf)
>  Data-based safe gain-scheduling controllers are presented for discrete-time linear time-varying systems (LPV). First, for LPV systems with polytopic models, $\lambda$-contractivity conditions are provided under which the safety and stability of the LPV systems are unified through their related Minkowski functions. Then, to obviate the requirement to know the system dynamics, a data-based representation of the closed-loop LPV system is provided. This data-based representation is then leveraged to provide data-driven gain-scheduling conditions that guarantee $\lambda$-contractiveness of the safe sets. It is also shown that the problem of designing a data-driven gain-scheduling controller to enforce a given polyhedral (ellipsoidal) set to be contractive in the presented data-based framework amounts to a linear program (a semi-definite program). A simulation example is provided to show the effectiveness of the presented approach.      
### 4.Mitigating Nonlinear Interference by Limiting Energy Variations in Sphere Shaping  [ :arrow_down: ](https://arxiv.org/pdf/2201.00674.pdf)
>  Band-trellis enumerative sphere shaping is proposed to decrease the energy variations in channel input sequences. Against sphere shaping, 0.74 dB SNR gain and up to 9% increase in data rates are demonstrated for single-span systems.      
### 5.H$_2$ Optimal Model Order Reduction over a Finite Time Interval  [ :arrow_down: ](https://arxiv.org/pdf/2201.00662.pdf)
>  For a time-limited version of the H$_2$ norm defined over a fixed time interval, we obtain a closed form expression of the gradients. After that, we use the gradients to propose a time-limited model order reduction method. The method involves obtaining a reduced model which minimizes the time-limited H$_2$ norm, formulated as a nonlinear optimization problem. The optimization problem is solved using standard optimization software.      
### 6.Learning a microlocal priorfor limited-angle tomography  [ :arrow_down: ](https://arxiv.org/pdf/2201.00656.pdf)
>  Digital breast tomosynthesis is an ill posed inverse problem. In this paper, we provide a try to overcome the problem of stretching artefacts of DBT with the help of learning from the microlocal priors.      
### 7.Formal Verification of Unknown Dynamical Systems via Gaussian Process Regression  [ :arrow_down: ](https://arxiv.org/pdf/2201.00655.pdf)
>  Leveraging autonomous systems in safety-critical scenarios requires verifying their behaviors in the presence of uncertainties and black-box components that influence the system dynamics. In this article, we develop a framework for verifying partially-observable, discrete-time dynamical systems with unmodelled dynamics against temporal logic specifications from a given input-output dataset. The verification framework employs Gaussian process (GP) regression to learn the unknown dynamics from the dataset and abstract the continuous-space system as a finite-state, uncertain Markov decision process (MDP). This abstraction relies on space discretization and transition probability intervals that capture the uncertainty due to the error in GP regression by using reproducible kernel Hilbert space analysis as well as the uncertainty induced by discretization. The framework utilizes existing model checking tools for verification of the uncertain MDP abstraction against a given temporal logic specification. We establish the correctness of extending the verification results on the abstraction to the underlying partially-observable system. We show that the computational complexity of the framework is polynomial in the size of the dataset and discrete abstraction. The complexity analysis illustrates a trade-off between the quality of the verification results and the computational burden to handle larger datasets and finer abstractions. Finally, we demonstrate the efficacy of our learning and verification framework on several case studies with linear, nonlinear, and switched dynamical systems.      
### 8.Feature matching as improved transfer learning technique for wearable EEG  [ :arrow_down: ](https://arxiv.org/pdf/2201.00644.pdf)
>  Objective: With the rapid rise of wearable sleep monitoring devices with non-conventional electrode configurations, there is a need for automated algorithms that can perform sleep staging on configurations with small amounts of labeled data. Transfer learning has the ability to adapt neural network weights from a source modality (e.g. standard electrode configuration) to a new target modality (e.g. non-conventional electrode configuration). Methods: We propose feature matching, a new transfer learning strategy as an alternative to the commonly used finetuning approach. This method consists of training a model with larger amounts of data from the source modality and few paired samples of source and target modality. For those paired samples, the model extracts features of the target modality, matching these to the features from the corresponding samples of the source modality. Results: We compare feature matching to finetuning for three different target domains, with two different neural network architectures, and with varying amounts of training data. Particularly on small cohorts (i.e. 2 - 5 labeled recordings in the non-conventional recording setting), feature matching systematically outperforms finetuning with mean relative differences in accuracy ranging from 0.4% to 4.7% for the different scenarios and datasets. Conclusion: Our findings suggest that feature matching outperforms finetuning as a transfer learning approach, especially in very low data regimes. Significance: As such, we conclude that feature matching is a promising new method for wearable sleep staging with novel devices.      
### 9.Improving Feature Extraction from Histopathological Images Through A Fine-tuning ImageNet Model  [ :arrow_down: ](https://arxiv.org/pdf/2201.00636.pdf)
>  Due to lack of annotated pathological images, transfer learning has been the predominant approach in the field of digital pathology.Pre-trained neural networks based on ImageNet database are often used to extract "off the shelf" features, achieving great success in predicting tissue types, molecular features, and clinical outcomes, etc. We hypothesize that fine-tuning the pre-trained models using histopathological images could further improve feature extraction, and downstream prediction performance.We used 100,000 annotated HE image patches for colorectal cancer (CRC) to finetune a pretrained Xception model via a twostep approach.The features extracted from finetuned Xception (FTX2048) model and Imagepretrained (IMGNET2048) model were compared through: (1) tissue classification for HE images from CRC, same image type that was used for finetuning; (2) prediction of immunerelated gene expression and (3) gene mutations for lung adenocarcinoma (LUAD).Fivefold cross validation was used for model performance evaluation. The extracted features from the finetuned FTX2048 exhibited significantly higher accuracy for predicting tisue types of CRC compared to the off the shelf feature directly from Xception based on ImageNet database. Particularly, FTX2048 markedly improved the accuracy for stroma from 87% to 94%. Similarly, features from FTX2048 boosted the prediction of transcriptomic expression of immunerelated genesin LUAD. For the genes that had signigicant relationships with image fetures, the features fgrom the finetuned model imprroved the prediction for the majority of the genes. Inaddition, fetures from FTX2048 improved prediction of mutation for 5 out of 9 most frequently mutated genes in LUAD.      
### 10.Low-cost sensors for indoor PV energy harvesting estimation based on machine learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.00629.pdf)
>  With the number of communicating sensors linked to the Internet of Things (IoT) ecosystem in-creasing dramatically, well-designed indoor light energy harvesting solutions are needed. The first step towards this development is to determine the harvestable energy in real indoor environ-ments. But the harvestable energy varying over time with nature (spectra) and intensity of the light multi-sources, lighting data must be collected for sufficiently long periods. Besides, for a real implementation on-site, studies must be able to be carried out simultaneously in several places to determine locations with the highest energy harvesting potential. In this context, this manuscript presents a very low-cost prototype based on commercial photodiodes (rather than very expensive spectrometers), which measures only a very rudimentary number of spectral data. Thanks to a classification supervised machine learning from Matlab, in which an algorithm learns to classify new observations, and thanks to a simple principle of the superposition approximation model de-veloped for flexible GaAs solar cells, our harvestable energy estimation error is less than 5 percents after more than 2 weeks of observation. To measure this error, the data collected leading to an estimate of the harvestable energy is compared to what has been experimentally harvested in a real IoT system Li-ion battery and compared to what has been estimated using an expensive spectrometer during the same period. Our prototype should allow the development and the massive deploy-ment of a new generation of low cost indoor light energy harvesting sensors for future reliable indoor energy harvesters.      
### 11.An EEG-based approach for Parkinson's disease diagnosis using Capsule network  [ :arrow_down: ](https://arxiv.org/pdf/2201.00628.pdf)
>  As the second most common neurodegenerative disease, Parkinson's disease has caused serious problems worldwide. However, the cause and mechanism of PD are not clear, and no systematic early diagnosis and treatment of PD have been established, many patients with PD have not been diagnosed or misdiagnosed. In this paper, we proposed an EEG-based approach to diagnosing Parkinson's disease, it mapping the frequency band energy of EEG signals to 2-dimensional images using the interpolation method and identifying classification using CapsNet, achieved 89.34% classification accuracy for short-time EEG sections, which exceeds the conventional SVM model.A comparison of separate classification accuracy across different EEG bands revealed the highest accuracy in the gamma bands, suggesting that we need pay more attention to the changes in gamma band changes in the early stages of PD.      
### 12.Uncertainty Detection in EEG Neural Decoding Models  [ :arrow_down: ](https://arxiv.org/pdf/2201.00627.pdf)
>  EEG decoding systems based on deep neural networks have been widely used in decision making of brain computer interfaces (BCI). Their predictions, however, can be unreliable given the significant variance and noise in EEG signals. Previous works on EEG analysis mainly focus on the exploration of noise pattern in the source signal, while the uncertainty during the decoding process is largely unexplored. Automatically detecting and quantifying such decoding uncertainty is important for BCI motor imagery applications such as robotic arm control etc. In this work, we proposed an uncertainty estimation model (UE-EEG) to explore the uncertainty during the EEG decoding process, which considers both the uncertainty in the input signal and the uncertainty in the model. The model utilized dropout oriented method for model uncertainty estimation, and Bayesian neural network is adopted for modeling the uncertainty of input data. The model can be integrated into current widely used deep learning classifiers without change of architecture. We performed extensive experiments for uncertainty estimation in both intra-subject EEG decoding and cross-subject EEG decoding on two public motor imagery datasets, where the proposed model achieves significant improvement on the quality of estimated uncertainty and demonstrates the proposed UE-EEG is a useful tool for BCI applications.      
### 13.Wireless-Enabled Asynchronous Federated Fourier Neural Network for Turbulence Prediction in Urban Air Mobility (UAM)  [ :arrow_down: ](https://arxiv.org/pdf/2201.00626.pdf)
>  To meet the growing mobility needs in intra-city transportation, the concept of urban air mobility (UAM) has been proposed in which vertical takeoff and landing (VTOL) aircraft are used to provide a ride-hailing service. In UAM, aircraft can operate in designated air spaces known as corridors, that link the aerodromes. A reliable communication network between GBSs and aircraft enables UAM to adequately utilize the airspace and create a fast, efficient, and safe transportation system. In this paper, to characterize the wireless connectivity performance for UAM, a spatial model is proposed. For this setup, the distribution of the distance between an arbitrarily selected GBS and its associated aircraft and the Laplace transform of the interference experienced by the GBS are derived. Using these results, the signal-to-interference ratio (SIR)-based connectivity probability is determined to capture the connectivity performance of the UAM aircraft-to-ground communication network. Then, leveraging these connectivity results, a wireless-enabled asynchronous federated learning (AFL) framework that uses a Fourier neural network is proposed to tackle the challenging problem of turbulence prediction during UAM operations. For this AFL scheme, a staleness-aware global aggregation scheme is introduced to expedite the convergence to the optimal turbulence prediction model used by UAM aircraft. Simulation results validate the theoretical derivations for the UAM wireless connectivity. The results also demonstrate that the proposed AFL framework converges to the optimal turbulence prediction model faster than the synchronous federated learning baselines and a staleness-free AFL approach. Furthermore, the results characterize the performance of wireless connectivity and convergence of the aircraft's turbulence model under different parameter settings, offering useful UAM design guidelines.      
### 14.Generative adversarial network for super-resolution imaging through a fiber  [ :arrow_down: ](https://arxiv.org/pdf/2201.00601.pdf)
>  A multimode fiber represents the ultimate limit in miniaturization of imaging endoscopes. Here we propose a fiber imaging approach employing compressive sensing with a data-driven machine learning framework. We implement a generative adversarial network for image reconstruction without relying on a sample sparsity constraint. The proposed method outperforms the conventional compressive imaging algorithms in terms of image quality and noise robustness. We experimentally demonstrate speckle-based imaging below the diffraction limit at a sub-Nyquist speed through a multimode fiber.      
### 15.Evaluation and comparison of SEA torque controllers in a unified framework  [ :arrow_down: ](https://arxiv.org/pdf/2201.00583.pdf)
>  Series elastic actuators (SEA) with their inherent compliance offer a safe torque source for robots that are interacting with various environments, including humans. These applications have high requirements for the SEA torque controllers, both in the torque response as well as interaction behavior with its the environment. To differentiate state of the art torque controllers, this work is introducing a unifying theoretical and experimental framework that compares controllers based on their torque transfer behavior, their apparent impedance behavior, and especially the passivity of the apparent impedance, i.e. their interaction stability, as well as their sensitivity to sensor noise. We compare classical SEA control approaches such as cascaded PID controllers and full state feedback controllers with advanced controllers using disturbance observers, acceleration feedback and adaptation rules. Simulations and experiments demonstrate the trade-off between stable interactions, high bandwidths and low noise levels. Based on these tradeoffs, an application specific controller can be designed and tuned, based on desired interaction with the respective environment.      
### 16.Antenna parameterization for effectiveness in horn shaped antenna for 5G communication as future of Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2201.00573.pdf)
>  Horn antenna is well documented in our research in this paper. We are trying our latent method of radiation by antennas which we suppose to reduce to a significant extent. Why we chose horn antenna is it resonates the sound to explosion as done by horn shaped matter. Horn by its shape makes the sent signals to maximum capability by its shape which is required by the receiver due to the distance separation from sender and receiver. Our research will contain various implementations leading to improvement of previous designs. We will use the traditional methods of Bergen to make the antenna behave smartly in its functioning.      
### 17.Design of Differently shaped antenna by using Major Modifications in Design of copper annealed and FR4 junction  [ :arrow_down: ](https://arxiv.org/pdf/2201.00569.pdf)
>  Antennas are taking design shapes by the orientation of its material and the final structural design they take. Irrespective of the shape, the function and efficiency they produce does not change and vary to much extent. We are trying to simulate a design of antenna which is using the artificial intelligence model to Specific absorption rate minimization. We have successfully used antenna magus and CST suite to produce our improved simulation results. Motion models such as optical flow is used to smartly detect communicating devices inorder to connect and link the devices. The algorithm used by us is closest neighbour algorithm which is connected with antenna to apply it in object detection of mobile devices and antennas. We are trying to have a six second break in which the antenna will operate at minimum frequency but at the same time operating in its functioning. After every six seconds the antenna will radiate its frequency of vibration to detect if there is any new object or mobile device in sight.      
### 18.Ant colonization processed algorithm for design of a toroidal shaped mobile 5G antenna  [ :arrow_down: ](https://arxiv.org/pdf/2201.00567.pdf)
>  There is great potential if we understand how nature functions, particularly the animals taking down from the ant to the larger animals. In this paper we will make an attempt to learn about ants colonization processing by studying their behaviour. Earlier there was particle swarm optimization which helped to solve many scientific problems. Ants communication with each other, their peculiar behaviour of working, colonization, their movements from one place to another, there is large potential in understanding their entire life to design a better 5G antenna.      
### 19.Signal-Aware Direction-of-Arrival Estimation Using Attention Mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2201.00503.pdf)
>  The direction-of-arrival (DOA) of sound sources is an essential acoustic parameter used, e.g., for multi-channel speech enhancement or source tracking. Complex acoustic scenarios consisting of sources-of-interest, interfering sources, reverberation, and noise make the estimation of the DOAs corresponding to the sources-of-interest a challenging task. Recently proposed attention mechanisms allow DOA estimators to focus on the sources-of-interest and disregard interference and noise, i.e., they are signal-aware. The attention is typically obtained by a deep neural network (DNN) from a short-time Fourier transform (STFT) based representation of a single microphone signal. Subsequently, attention has been applied as binary or ratio weighting to STFT-based microphone signal representations to reduce the impact of frequency bins dominated by noise, interference, or reverberation. The impact of attention on DOA estimators and different training strategies for attention and DOA DNNs are not yet studied in depth. In this paper, we evaluate systems consisting of different DNNs and signal processing-based methods for DOA estimation when attention is applied. Additionally, we propose training strategies for attention-based DOA estimation optimized via a DOA objective, i.e., end-to-end. The evaluation of the proposed and the baseline systems is performed using data generated with simulated and measured room impulse responses under various acoustic conditions, like reverberation times, noise, and source array distances. Overall, DOA estimation using attention in combination with signal-processing methods exhibits a far lower computational complexity than a fully DNN-based system; however, it yields comparable results.      
### 20.TFCN: Temporal-Frequential Convolutional Network for Single-Channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2201.00480.pdf)
>  Deep learning based single-channel speech enhancement tries to train a neural network model for the prediction of clean speech signal. There are a variety of popular network structures for single-channel speech enhancement, such as TCNN, UNet, WaveNet, etc. However, these structures usually contain millions of parameters, which is an obstacle for mobile applications. In this work, we proposed a light weight neural network for speech enhancement named TFCN. It is a temporal-frequential convolutional network constructed of dilated convolutions and depth-separable convolutions. We evaluate the performance of TFCN in terms of Short-Time Objective Intelligibility (STOI), perceptual evaluation of speech quality (PESQ) and a series of composite metrics named Csig, Cbak and Covl. Experimental results show that compared with TCN and several other state-of-the-art algorithms, the proposed structure achieves a comparable performance with only 93,000 parameters. Further improvement can be achieved at the cost of more parameters, by introducing dense connections and depth-separable convolutions with normal ones. Experiments also show that the proposed structure can work well both in causal and non-causal situations.      
### 21.RFormer: Transformer-based Generative Adversarial Network for Real Fundus Image Restoration on A New Clinical Benchmark  [ :arrow_down: ](https://arxiv.org/pdf/2201.00466.pdf)
>  Ophthalmologists have used fundus images to screen and diagnose eye diseases. However, different equipments and ophthalmologists pose large variations to the quality of fundus images. Low-quality (LQ) degraded fundus images easily lead to uncertainty in clinical screening and generally increase the risk of misdiagnosis. Thus, real fundus image restoration is worth studying. Unfortunately, real clinical benchmark has not been explored for this task so far. In this paper, we investigate the real clinical fundus image restoration problem. Firstly, We establish a clinical dataset, Real Fundus (RF), including 120 low- and high-quality (HQ) image pairs. Then we propose a novel Transformer-based Generative Adversarial Network (RFormer) to restore the real degradation of clinical fundus images. The key component in our network is the Window-based Self-Attention Block (WSAB) which captures non-local self-similarity and long-range dependencies. To produce more visually pleasant results, a Transformer-based discriminator is introduced. Extensive experiments on our clinical benchmark show that the proposed RFormer significantly outperforms the state-of-the-art (SOTA) methods. In addition, experiments of downstream tasks such as vessel segmentation and optic disc/cup detection demonstrate that our proposed RFormer benefits clinical fundus image analysis and applications. The dataset, code, and models will be released.      
### 22.Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark  [ :arrow_down: ](https://arxiv.org/pdf/2201.00458.pdf)
>  Lung cancer is one of the deadliest cancers, and in part its effective diagnosis and treatment depend on the accurate delineation of the tumor. Human-centered segmentation, which is currently the most common approach, is subject to inter-observer variability, and is also time-consuming, considering the fact that only experts are capable of providing annotations. Automatic and semi-automatic tumor segmentation methods have recently shown promising results. However, as different researchers have validated their algorithms using various datasets and performance metrics, reliably evaluating these methods is still an open challenge. The goal of the Lung-Originated Tumor Segmentation from Computed Tomography Scan (LOTUS) Benchmark created through 2018 IEEE Video and Image Processing (VIP) Cup competition, is to provide a unique dataset and pre-defined metrics, so that different researchers can develop and evaluate their methods in a unified fashion. The 2018 VIP Cup started with a global engagement from 42 countries to access the competition data. At the registration stage, there were 129 members clustered into 28 teams from 10 countries, out of which 9 teams made it to the final stage and 6 teams successfully completed all the required tasks. In a nutshell, all the algorithms proposed during the competition, are based on deep learning models combined with a false positive reduction technique. Methods developed by the three finalists show promising results in tumor segmentation, however, more effort should be put into reducing the false positive rate. This competition manuscript presents an overview of the VIP-Cup challenge, along with the proposed algorithms and results.      
### 23.Image Denoising with Control over Deep Network Hallucination  [ :arrow_down: ](https://arxiv.org/pdf/2201.00429.pdf)
>  Deep image denoisers achieve state-of-the-art results but with a hidden cost. As witnessed in recent literature, these deep networks are capable of overfitting their training distributions, causing inaccurate hallucinations to be added to the output and generalizing poorly to varying data. For better control and interpretability over a deep denoiser, we propose a novel framework exploiting a denoising network. We call it controllable confidence-based image denoising (CCID). In this framework, we exploit the outputs of a deep denoising network alongside an image convolved with a reliable filter. Such a filter can be a simple convolution kernel which does not risk adding hallucinated information. We propose to fuse the two components with a frequency-domain approach that takes into account the reliability of the deep network outputs. With our framework, the user can control the fusion of the two components in the frequency domain. We also provide a user-friendly map estimating spatially the confidence in the output that potentially contains network hallucination. Results show that our CCID not only provides more interpretability and control, but can even outperform both the quantitative performance of the deep denoiser and that of the reliable filter, especially when the test data diverge from the training data.      
### 24.Data-driven Sensor Deployment for Spatiotemporal Field Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2201.00420.pdf)
>  This paper concerns the data-driven sensor deployment problem in large spatiotemporal fields. Traditionally, sensor deployment strategies have been heavily dependent on model-based planning approaches. However, model-based approaches do not typically maximize the information gain in the field, which tends to generate less effective sampling locations and lead to high reconstruction error. In the present paper, a data-driven approach is developed to overcome the drawbacks of the model-based approach and improve the spatiotemporal field reconstruction accuracy. The proposed method can select the most informative sampling locations to represent the entire spatiotemporal field. To this end, the proposed method decomposes the spatiotemporal field using principal component analysis (PCA) and finds the top r essential entities of the principal basis. The corresponding sampling locations of the selected entities are regarded as the sensor deployment locations. The observations collected at the selected sensor deployment locations can then be used to reconstruct the spatiotemporal field, accurately. Results are demonstrated using a National Oceanic and Atmospheric Administration sea surface temperature dataset. In the present study, the proposed method achieved the lowest reconstruction error among all methods.      
### 25.FUSeg: The Foot Ulcer Segmentation Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2201.00414.pdf)
>  Acute and chronic wounds with varying etiologies burden the healthcare systems economically. The advanced wound care market is estimated to reach $22 billion by 2024. Wound care professionals provide proper diagnosis and treatment with heavy reliance on images and image documentation. Segmentation of wound boundaries in images is a key component of the care and diagnosis protocol since it is important to estimate the area of the wound and provide quantitative measurement for the treatment. Unfortunately, this process is very time-consuming and requires a high level of expertise. Recently automatic wound segmentation methods based on deep learning have shown promising performance but require large datasets for training and it is unclear which methods perform better. To address these issues, we propose the Foot Ulcer Segmentation challenge (FUSeg) organized in conjunction with the 2021 International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI). We built a wound image dataset containing 1,210 foot ulcer images collected over 2 years from 889 patients. It is pixel-wise annotated by wound care experts and split into a training set with 1010 images and a testing set with 200 images for evaluation. Teams around the world developed automated methods to predict wound segmentations on the testing set of which annotations were kept private. The predictions were evaluated and ranked based on the average Dice coefficient. The FUSeg challenge remains an open challenge as a benchmark for wound segmentation after the conference.      
### 26.Increasing the Efficiency of Photovoltaic Systems by Using Maximum Power Point Tracking (MPPT)  [ :arrow_down: ](https://arxiv.org/pdf/2201.00403.pdf)
>  Using Photovoltaic systems is gradually expanded by increasing energy demand. Abundance and availability of this energy, has turned to one of the most important sources of renewable energy. Unfortunately, photovoltaic systems have two big problems: first, those have very low energy conversion efficiency (in act between 12 and 42 percent under certain circumstances). Second, the power produced by the solar cell depends on nonlinear conditions such as solar radiation, temperature and charge feature. According to this, received power maximum of photovoltaic cells depends on different non-linear variables, it is necessary to be continuously traced, as maximum received power of the cell (by controller). In this research, the increasing efficiency of photovoltaic systems has been investigated by using Maximum Power Point Tracking (MPPT) in two different modes contained connected to the Grid and disconnected from the grid with simulation by MATLAB software. The obtained results showed that the proposed technique is able to improve the current, voltage and power output of photovoltaic cells.      
### 27.Graph Signal Reconstruction Techniques for IoT Air Pollution Monitoring Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2201.00378.pdf)
>  Air pollution monitoring platforms play a very important role in preventing and mitigating the effects of pollution. Recent advances in the field of graph signal processing have made it possible to describe and analyze air pollution monitoring networks using graphs. One of the main applications is the reconstruction of the measured signal in a graph using a subset of sensors. Reconstructing the signal using information from sensor neighbors can help improve the quality of network data, examples are filling in missing data with correlated neighboring nodes, or correcting a drifting sensor with neighboring sensors that are more accurate. This paper compares the use of various types of graph signal reconstruction methods applied to real data sets of Spanish air pollution reference stations. The methods considered are Laplacian interpolation, graph signal processing low-pass based graph signal reconstruction, and kernel-based graph signal reconstruction, and are compared on actual air pollution data sets measuring O3, NO2, and PM10. The ability of the methods to reconstruct the signal of a pollutant is shown, as well as the computational cost of this reconstruction. The results indicate the superiority of methods based on kernel-based graph signal reconstruction, as well as the difficulties of the methods to scale in an air pollution monitoring network with a large number of low-cost sensors. However, we show that scalability can be overcome with simple methods, such as partitioning the network using a clustering algorithm.      
### 28.Riemannian Nearest-Regularized Subspace Classification for Polarimetric SAR images  [ :arrow_down: ](https://arxiv.org/pdf/2201.00337.pdf)
>  As a representation learning method, nearest regularized subspace(NRS) algorithm is an effective tool to obtain both accuracy and speed for PolSAR image classification. However, existing NRS methods use the polarimetric feature vector but the PolSAR original covariance matrix(known as Hermitian positive definite(HPD)matrix) as the input. Without considering the matrix structure, existing NRS-based methods cannot learn correlation among channels. How to utilize the original covariance matrix to NRS method is a key problem. To address this limit, a Riemannian NRS method is proposed, which consider the HPD matrices endow in the Riemannian space. Firstly, to utilize the PolSAR original data, a Riemannian NRS method(RNRS) is proposed by constructing HPD dictionary and HPD distance metric. Secondly, a new Tikhonov regularization term is designed to reduce the differences within the same class. Finally, the optimal method is developed and the first-order derivation is inferred. During the experimental test, only T matrix is used in the proposed method, while multiple of features are utilized for compared methods. Experimental results demonstrate the proposed method can outperform the state-of-art algorithms even using less features.      
### 29.Recurrent Feature Propagation and Edge Skip-Connections for Automatic Abdominal Organ Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.00317.pdf)
>  Automatic segmentation of abdominal organs in computed tomography (CT) images can support radiation therapy and image-guided surgery workflows. Developing of such automatic solutions remains challenging mainly owing to complex organ interactions and blurry boundaries in CT images. To address these issues, we focus on effective spatial context modeling and explicit edge segmentation priors. Accordingly, we propose a 3D network with four main components trained end-to-end including shared encoder, edge detector, decoder with edge skip-connections (ESCs) and recurrent feature propagation head (RFP-Head). To capture wide-range spatial dependencies, the RFP-Head propagates and harvests local features through directed acyclic graphs (DAGs) formulated with recurrent connections in an efficient slice-wise manner, with regard to spatial arrangement of image units. To leverage edge information, the edge detector learns edge prior knowledge specifically tuned for semantic segmentation by exploiting intermediate features from the encoder with the edge supervision. The ESCs then aggregate the edge knowledge with multi-level decoder features to learn a hierarchy of discriminative features explicitly modeling complementarity between organs' interiors and edges for segmentation. We conduct extensive experiments on two challenging abdominal CT datasets with eight annotated organs. Experimental results show that the proposed network outperforms several state-of-the-art models, especially for the segmentation of small and complicated structures (gallbladder, esophagus, stomach, pancreas and duodenum). The code will be publicly available.      
### 30.DF-SSmVEP: Dual Frequency Aggregated Steady-State Motion Visual Evoked Potential Design with Bifold Canonical Correlation Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2201.00283.pdf)
>  Recent advancements in Electroencephalography (EEG) sensor technologies and signal processing algorithms have paved the way for further evolution of Brain Computer Interfaces (BCI). When it comes to Signal Processing (SP) for BCI, there has been a surge of interest on Steady-State motion-Visual Evoked Potentials (SSmVEP), where motion stimulation is utilized to address key issues associated with conventional light-flashing/flickering. Such benefits, however, come with the price of having less accuracy and less Information Transfer Rate (ITR). In this regard, the paper focuses on the design of a novel SSmVEP paradigm without using resources such as trial time, phase, and/or number of targets to enhance the ITR. The proposed design is based on the intuitively pleasing idea of integrating more than one motion within a single SSmVEP target stimuli, simultaneously. To elicit SSmVEP, we designed a novel and innovative dual frequency aggregated modulation paradigm, referred to as the Dual Frequency Aggregated steady-state motion Visual Evoked Potential (DF-SSmVEP), by concurrently integrating "Radial Zoom" and "Rotation" motions in a single target without increasing the trial length. Compared to conventional SSmVEPs, the proposed DF-SSmVEP framework consists of two motion modes integrated and shown simultaneously each modulated by a specific target frequency. The paper also develops a specific unsupervised classification model, referred to as the Bifold Canonical Correlation Analysis (BCCA), based on two motion frequencies per target. The proposed DF-SSmVEP is evaluated based on a real EEG dataset and the results corroborate its superiority. The proposed DF-SSmVEP outperforms its counterparts and achieved an average ITR of 30.7 +/- 1.97 and an average accuracy of 92.5 +/- 2.04.      
### 31.IQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion  [ :arrow_down: ](https://arxiv.org/pdf/2201.00269.pdf)
>  Prosody modeling is important, but still challenging in expressive voice conversion. As prosody is difficult to model, and other factors, e.g., speaker, environment and content, which are entangled with prosody in speech, should be removed in prosody modeling. In this paper, we present IQDubbing to solve this problem for expressive voice conversion. To model prosody, we leverage the recent advances in discrete self-supervised speech representation (DSSR). Specifically, prosody vector is first extracted from pre-trained VQ-Wav2Vec model, where rich prosody information is embedded while most speaker and environment information are removed effectively by quantization. To further filter out the redundant information except prosody, such as content and partial speaker information, we propose two kinds of prosody filters to sample prosody from the prosody vector. Experiments show that IQDubbing is superior to baseline and comparison systems in terms of speech quality while maintaining prosody consistency and speaker similarity.      
### 32.Subspace modeling for fast and high-sensitivity X-ray chemical imaging  [ :arrow_down: ](https://arxiv.org/pdf/2201.00259.pdf)
>  Resolving morphological chemical phase transformations at the nanoscale is of vital importance to many scientific and industrial applications across various disciplines. The TXM-XANES imaging technique, by combining full field transmission X-ray microscopy (TXM) and X-ray absorption near edge structure (XANES), has been an emerging tool which operates by acquiring a series of microscopy images with multi-energy X-rays and fitting to obtain the chemical map. Its capability, however, is limited by the poor signal-to-noise ratios due to the system errors and low exposure illuminations for fast acquisition. In this work, by exploiting the intrinsic properties and subspace modeling of the TXM-XANES imaging data, we introduce a simple and robust denoising approach to improve the image quality, which enables fast and high-sensitivity chemical imaging. Extensive experiments on both synthetic and real datasets demonstrate the superior performance of the proposed method.      
### 33.Understanding Energy Efficiency and Interference Tolerance in Millimeter Wave Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2201.00229.pdf)
>  Power consumption is a key challenge in millimeter wave (mmWave) receiver front-ends, due to the need to support high dimensional antenna arrays at wide bandwidths. Recently, there has been considerable work in developing low-power front-ends, often based on low-resolution ADCs and low-power mixers. A critical but less studied consequence of such designs is the relatively low-dynamic range which in turn exposes the receiver to adjacent carrier interference and blockers. This paper provides a general mathematical framework for analyzing the performance of mmWave front-ends in the presence of out-of-band interference. The goal is to elucidate the fundamental trade-off of power consumption, interference tolerance and in-band performance. The analysis is combined with detailed network simulations in cellular systems with multiple carriers, as well as detailed circuit simulations of key components at 140 GHz. The analysis reveals critical bottlenecks for low-power interference robustness and suggests designs enhancements for use in practical systems.      
### 34.Deep Learning Applications for Lung Cancer Diagnosis: A systematic review  [ :arrow_down: ](https://arxiv.org/pdf/2201.00227.pdf)
>  Lung cancer has been one of the most prevalent disease in recent years. According to the research of this field, more than 200,000 cases are identified each year in the US. Uncontrolled multiplication and growth of the lung cells result in malignant tumour formation. Recently, deep learning algorithms, especially Convolutional Neural Networks (CNN), have become a superior way to automatically diagnose disease. The purpose of this article is to review different models that lead to different accuracy and sensitivity in the diagnosis of early-stage lung cancer and to help physicians and researchers in this field. The main purpose of this work is to identify the challenges that exist in lung cancer based on deep learning. The survey is systematically written that combines regular mapping and literature review to review 32 conference and journal articles in the field from 2016 to 2021. After analysing and reviewing the articles, the questions raised in the articles are being answered. This research is superior to other review articles in this field due to the complete review of relevant articles and systematic write up.      
### 35.NOMA Computation Over Multi-Access Channels for Multimodal Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2201.00203.pdf)
>  An improved mean squared error (MSE) minimization solution based on eigenvector decomposition approach is conceived for wideband non-orthogonal multiple-access based computation over multi-access channel (NOMA-CoMAC) framework. This work aims at further developing NOMA-CoMAC for next-generation multimodal sensor networks, where a multimodal sensor monitors several environmental parameters such as temperature, pollution, humidity, or pressure. We demonstrate that our proposed scheme achieves an MSE value approximately 0.7 lower at E_b/N_o = 1 dB in comparison to that for the average sum-channel based method. Moreover, the MSE performance gain of our proposed solution increases even more for larger values of subcarriers and sensor nodes due to the benefit of the diversity gain. This, in return, suggests that our proposed scheme is eminently suitable for multimodal sensor networks.      
### 36.Image Restoration using Feature-guidance  [ :arrow_down: ](https://arxiv.org/pdf/2201.00187.pdf)
>  Image restoration is the task of recovering a clean image from a degraded version. In most cases, the degradation is spatially varying, and it requires the restoration network to both localize and restore the affected regions. In this paper, we present a new approach suitable for handling the image-specific and spatially-varying nature of degradation in images affected by practically occurring artifacts such as blur, rain-streaks. We decompose the restoration task into two stages of degradation localization and degraded region-guided restoration, unlike existing methods which directly learn a mapping between the degraded and clean images. Our premise is to use the auxiliary task of degradation mask prediction to guide the restoration process. We demonstrate that the model trained for this auxiliary task contains vital region knowledge, which can be exploited to guide the restoration network's training using attentive knowledge distillation technique. Further, we propose mask-guided convolution and global context aggregation module that focuses solely on restoring the degraded regions. The proposed approach's effectiveness is demonstrated by achieving significant improvement over strong baselines.      
### 37.Dynamic Scene Video Deblurring using Non-Local Attention  [ :arrow_down: ](https://arxiv.org/pdf/2201.00169.pdf)
>  This paper tackles the challenging problem of video deblurring. Most of the existing works depend on implicit or explicit alignment for temporal information fusion which either increase the computational cost or result in suboptimal performance due to wrong alignment. In this study, we propose a factorized spatio-temporal attention to perform non-local operations across space and time to fully utilize the available information without depending on alignment. It shows superior performance compared to existing fusion techniques while being much efficient. Extensive experiments on multiple datasets demonstrate the superiority of our method.      
### 38.Development of Diabetic Foot Ulcer Datasets: An Overview  [ :arrow_down: ](https://arxiv.org/pdf/2201.00163.pdf)
>  This paper provides conceptual foundation and procedures used in the development of diabetic foot ulcer datasets over the past decade, with a timeline to demonstrate progress. We conduct a survey on data capturing methods for foot photographs, an overview of research in developing private and public datasets, the related computer vision tasks (detection, segmentation and classification), the diabetic foot ulcer challenges and the future direction of the development of the datasets. We report the distribution of dataset users by country and year. Our aim is to share the technical challenges that we encountered together with good practices in dataset development, and provide motivation for other researchers to participate in data sharing in this domain.      
### 39.Adaptive Single Image Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2201.00155.pdf)
>  This paper tackles the problem of dynamic scene deblurring. Although end-to-end fully convolutional designs have recently advanced the state-of-the-art in non-uniform motion deblurring, their performance-complexity trade-off is still sub-optimal. Existing approaches achieve a large receptive field by a simple increment in the number of generic convolution layers, kernel-size, which comes with the burden of the increase in model size and inference speed. In this work, we propose an efficient pixel adaptive and feature attentive design for handling large blur variations within and across different images. We also propose an effective content-aware global-local filtering module that significantly improves the performance by considering not only the global dependencies of the pixel but also dynamically using the neighboring pixels. We use a patch hierarchical attentive architecture composed of the above module that implicitly discover the spatial variations in the blur present in the input image and in turn perform local and global modulation of intermediate features. Extensive qualitative and quantitative comparisons with prior art on deblurring benchmarks demonstrate the superiority of the proposed network.      
### 40.Sum-of-Squares Program and Safe Learning On Maximizing the Region of Attraction of Partially Unknown Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.00137.pdf)
>  Recent advances in learning techniques have enabled the modelling of unknown dynamical systems directly from data. However, in many contexts, these learning-based methods are short of safety guarantee and strict stability verification. To address this issue, this paper first approximates the partially unknown nonlinear systems by using a learned state space with Gaussian Processes and Chebyshev interpolants. A Sum-of-Squares Programming based approach is then proposed to synthesize a controller by searching an optimal control Lyapunov Barrier function. In this way, we maximize the estimated region of attraction of partially unknown nonlinear systems, while guaranteeing both safety and stability. It is shown that the proposed method improves the extrapolation performance, and at the same time, generates a significantly larger estimated region of attraction.      
### 41.Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.00100.pdf)
>  Training deep models for RGB-D salient object detection (SOD) often requires a large number of labeled RGB-D images. However, RGB-D data is not easily acquired, which limits the development of RGB-D SOD techniques. To alleviate this issue, we present a Dual-Semi RGB-D Salient Object Detection Network (DS-Net) to leverage unlabeled RGB images for boosting RGB-D saliency detection. We first devise a depth decoupling convolutional neural network (DDCNN), which contains a depth estimation branch and a saliency detection branch. The depth estimation branch is trained with RGB-D images and then used to estimate the pseudo depth maps for all unlabeled RGB images to form the paired data. The saliency detection branch is used to fuse the RGB feature and depth feature to predict the RGB-D saliency. Then, the whole DDCNN is assigned as the backbone in a teacher-student framework for semi-supervised learning. Moreover, we also introduce a consistency loss on the intermediate attention and saliency maps for the unlabeled data, as well as a supervised depth and saliency loss for labeled data. Experimental results on seven widely-used benchmark datasets demonstrate that our DDCNN outperforms state-of-the-art methods both quantitatively and qualitatively. We also demonstrate that our semi-supervised DS-Net can further improve the performance, even when using an RGB image with the pseudo depth map.      
### 42.Performance Comparison of Deep Learning Architectures for Artifact Removal in Gastrointestinal Endoscopic Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2201.00084.pdf)
>  Endoscopic images typically contain several artifacts. The artifacts significantly impact image analysis result in computer-aided diagnosis. Convolutional neural networks (CNNs), a type of deep learning, can removes such artifacts. Various architectures have been proposed for the CNNs, and the accuracy of artifact removal varies depending on the choice of architecture. Therefore, it is necessary to determine the artifact removal accuracy, depending on the selected architecture. In this study, we focus on endoscopic surgical instruments as artifacts, and determine and discuss the artifact removal accuracy using seven different CNN architectures.      
### 43.Stealth Data Injection Attacks with Sparsity Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2201.00065.pdf)
>  Sparse stealth attack constructions that minimize the mutual information between the state variables and the observations are proposed. The attack construction is formulated as the design of a multivariate Gaussian distribution that aims to minimize the mutual information while limiting the Kullback-Leibler divergence between the distribution of the observations under attack and the distribution of the observations without attack. The sparsity constraint is incorporated as a support constraint of the attack distribution. Two heuristic greedy algorithms for the attack construction are proposed. The first algorithm assumes that the attack vector consists of independent entries, and therefore, requires no communication between different attacked locations. The second algorithm considers correlation between the attack vector entries and achieves a better disruption to stealth tradeoff at the cost of requiring communication between different locations. We numerically evaluate the performance of the proposed attack constructions on IEEE test systems and show that it is feasible to construct stealth attacks that generate significant disruption with a low number of compromised sensors.      
### 44.Croesus: Multi-Stage Processing and Transactions for Video-Analytics in Edge-Cloud Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.00063.pdf)
>  Emerging edge applications require both a fast response latency and complex processing. This is infeasible without expensive hardware that can process complex operations -- such as object detection -- within a short time. Many approach this problem by addressing the complexity of the models -- via model compression, pruning and quantization -- or compressing the input. In this paper, we propose a different perspective when addressing the performance challenges. Croesus is a multi-stage approach to edge-cloud systems that provides the ability to find the balance between accuracy and performance. Croesus consists of two stages (that can be generalized to multiple stages): an initial and a final stage. The initial stage performs the computation in real-time using approximate/best-effort computation at the edge. The final stage performs the full computation at the cloud, and uses the results to correct any errors made at the initial stage. In this paper, we demonstrate the implications of such an approach on a video analytics use-case and show how multi-stage processing yields a better balance between accuracy and performance. Moreover, we study the safety of multi-stage transactions via two proposals: multi-stage serializability (MS-SR) and multi-stage invariant confluence with Apologies (MS-IA).      
### 45.Effect of Kinematics and Fluency in Adversarial Synthetic Data Generation for ASL Recognition with RF Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2201.00055.pdf)
>  RF sensors have been recently proposed as a new modality for sign language processing technology. They are non-contact, effective in the dark, and acquire a direct measurement of signing kinematic via exploitation of the micro-Doppler effect. First, this work provides an in depth, comparative examination of the kinematic properties of signing as measured by RF sensors for both fluent ASL users and hearing imitation signers. Second, as ASL recognition techniques utilizing deep learning requires a large amount of training data, this work examines the effect of signing kinematics and subject fluency on adversarial learning techniques for data synthesis. Two different approaches for the synthetic training data generation are proposed: 1) adversarial domain adaptation to minimize the differences between imitation signing and fluent signing data, and 2) kinematically-constrained generative adversarial networks for accurate synthesis of RF signing signatures. The results show that the kinematic discrepancies between imitation signing and fluent signing are so significant that training on data directly synthesized from fluent RF signers offers greater performance (93% top-5 accuracy) than that produced by adaptation of imitation signing (88% top-5 accuracy) when classifying 100 ASL signs.      
### 46.FaceQgen: Semi-Supervised Deep Learning for Face Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2201.00770.pdf)
>  In this paper we develop FaceQgen, a No-Reference Quality Assessment approach for face images based on a Generative Adversarial Network that generates a scalar quality measure related with the face recognition accuracy. FaceQgen does not require labelled quality measures for training. It is trained from scratch using the SCface database. FaceQgen applies image restoration to a face image of unknown quality, transforming it into a canonical high quality image, i.e., frontal pose, homogeneous background, etc. The quality estimation is built as the similarity between the original and the restored images, since low quality images experience bigger changes due to restoration. We compare three different numerical quality measures: a) the MSE between the original and the restored images, b) their SSIM, and c) the output score of the Discriminator of the GAN. The results demonstrate that FaceQgen's quality measures are good estimators of face recognition accuracy. Our experiments include a comparison with other quality assessment methods designed for faces and for general images, in order to position FaceQgen in the state of the art. This comparison shows that, even though FaceQgen does not surpass the best existing face quality assessment methods in terms of face recognition accuracy prediction, it achieves good enough results to demonstrate the potential of semi-supervised learning approaches for quality estimation (in particular, data-driven learning based on a single high quality image per subject), having the capacity to improve its performance in the future with adequate refinement of the model and the significant advantage over competing methods of not needing quality labels for its development. This makes FaceQgen flexible and scalable without expensive data curation.      
### 47.Submodular Maximization with Limited Function Access  [ :arrow_down: ](https://arxiv.org/pdf/2201.00724.pdf)
>  We consider a class of submodular maximization problems in which decision-makers have limited access to the objective function. We explore scenarios where the decision-maker can observe only pairwise information, i.e., can evaluate the objective function on sets of size two. We begin with a negative result that no algorithm using only $k$-wise information can guarantee performance better than $k/n$. We present two algorithms that utilize only pairwise information about the function and characterize their performance relative to the optimal, which depends on the curvature of the submodular function. Additionally, if the submodular function possess a property called supermodularity of conditioning, then we can provide a method to bound the performance based purely on pairwise information. The proposed algorithms offer significant computational speedups over a traditional greedy strategy. A by-product of our study is the introduction of two new notions of curvature, the $k$-Marginal Curvature and the $k$-Cardinality Curvature. Finally, we present experiments highlighting the performance of our proposed algorithms in terms of approximation and time complexity.      
### 48.Neural network training under semidefinite constraints  [ :arrow_down: ](https://arxiv.org/pdf/2201.00632.pdf)
>  This paper is concerned with the training of neural networks (NNs) under semidefinite constraints. This type of training problems has recently gained popularity since semidefinite constraints can be used to verify interesting properties for NNs that include, e.g., the estimation of an upper bound on the Lipschitz constant, which relates to the robustness of an NN, or the stability of dynamic systems with NN controllers. The utilized semidefinite constraints are based on sector constraints satisfied by the underlying activation functions. Unfortunately, one of the biggest bottlenecks of these new results is the required computational effort for incorporating the semidefinite constraints into the training of NNs which is limiting their scalability to large NNs. We address this challenge by developing interior point methods for NN training that we implement using barrier functions for semidefinite constraints. In order to efficiently compute the gradients of the barrier terms, we exploit the structure of the semidefinite constraints. In experiments, we demonstrate the superior efficiency of our training method over previous approaches, which allows us, e.g., to use semidefinite constraints in the training of Wasserstein generative adversarial networks, where the discriminator must satisfy a Lipschitz condition.      
### 49.Learning shared neural manifolds from multi-subject FMRI data  [ :arrow_down: ](https://arxiv.org/pdf/2201.00622.pdf)
>  Functional magnetic resonance imaging (fMRI) is a notoriously noisy measurement of brain activity because of the large variations between individuals, signals marred by environmental differences during collection, and spatiotemporal averaging required by the measurement resolution. In addition, the data is extremely high dimensional, with the space of the activity typically having much lower intrinsic dimension. In order to understand the connection between stimuli of interest and brain activity, and analyze differences and commonalities between subjects, it becomes important to learn a meaningful embedding of the data that denoises, and reveals its intrinsic structure. Specifically, we assume that while noise varies significantly between individuals, true responses to stimuli will share common, low-dimensional features between subjects which are jointly discoverable. Similar approaches have been exploited previously but they have mainly used linear methods such as PCA and shared response modeling (SRM). In contrast, we propose a neural network called MRMD-AE (manifold-regularized multiple decoder, autoencoder), that learns a common embedding from multiple subjects in an experiment while retaining the ability to decode to individual raw fMRI signals. We show that our learned common space represents an extensible manifold (where new points not seen during training can be mapped), improves the classification accuracy of stimulus features of unseen timepoints, as well as improves cross-subject translation of fMRI signals. We believe this framework can be used for many downstream applications such as guided brain-computer interface (BCI) training in the future.      
### 50.Overview of the EEG Pilot Subtask at MediaEval 2021: Predicting Media Memorability  [ :arrow_down: ](https://arxiv.org/pdf/2201.00620.pdf)
>  The aim of the Memorability-EEG pilot subtask at MediaEval'2021 is to promote interest in the use of neural signals -- either alone or in combination with other data sources -- in the context of predicting video memorability by highlighting the utility of EEG data. The dataset created consists of pre-extracted features from EEG recordings of subjects while watching a subset of videos from Predicting Media Memorability subtask 1. This demonstration pilot gives interested researchers a sense of how neural signals can be used without any prior domain knowledge, and enables them to do so in a future memorability task. The dataset can be used to support the exploration of novel machine learning and processing strategies for predicting video memorability, while potentially increasing interdisciplinary interest in the subject of memorability, and opening the door to new combined EEG-computer vision approaches.      
### 51.Artificial Intelligence and Statistical Techniques in Short-Term Load Forecasting: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2201.00437.pdf)
>  Electrical utilities depend on short-term demand forecasting to proactively adjust production and distribution in anticipation of major variations. This systematic review analyzes 240 works published in scholarly journals between 2000 and 2019 that focus on applying Artificial Intelligence (AI), statistical, and hybrid models to short-term load forecasting (STLF). This work represents the most comprehensive review of works on this subject to date. A complete analysis of the literature is conducted to identify the most popular and accurate techniques as well as existing gaps. The findings show that although Artificial Neural Networks (ANN) continue to be the most commonly used standalone technique, researchers have been exceedingly opting for hybrid combinations of different techniques to leverage the combined advantages of individual methods. The review demonstrates that it is commonly possible with these hybrid combinations to achieve prediction accuracy exceeding 99%. The most successful duration for short-term forecasting has been identified as prediction for a duration of one day at an hourly interval. The review has identified a deficiency in access to datasets needed for training of the models. A significant gap has been identified in researching regions other than Asia, Europe, North America, and Australia.      
### 52.MHATC: Autism Spectrum Disorder identification utilizing multi-head attention encoder along with temporal consolidation modules  [ :arrow_down: ](https://arxiv.org/pdf/2201.00404.pdf)
>  Resting-state fMRI is commonly used for diagnosing Autism Spectrum Disorder (ASD) by using network-based functional connectivity. It has been shown that ASD is associated with brain regions and their inter-connections. However, discriminating based on connectivity patterns among imaging data of the control population and that of ASD patients' brains is a non-trivial task. In order to tackle said classification task, we propose a novel deep learning architecture (MHATC) consisting of multi-head attention and temporal consolidation modules for classifying an individual as a patient of ASD. The devised architecture results from an in-depth analysis of the limitations of current deep neural network solutions for similar applications. Our approach is not only robust but computationally efficient, which can allow its adoption in a variety of other research and clinical settings.      
### 53.Fast and High-Quality Image Denoising via Malleable Convolutions  [ :arrow_down: ](https://arxiv.org/pdf/2201.00392.pdf)
>  Many image processing networks apply a single set of static convolutional kernels across the entire input image, which is sub-optimal for natural images, as they often consist of heterogeneous visual patterns. Recent work in classification, segmentation, and image restoration has demonstrated that dynamic kernels outperform static kernels at modeling local image statistics. However, these works often adopt per-pixel convolution kernels, which introduce high memory and computation costs. To achieve spatial-varying processing without significant overhead, we present \textbf{Malle}able \textbf{Conv}olution (\textbf{MalleConv}), as an efficient variant of dynamic convolution. The weights of \ours are dynamically produced by an efficient predictor network capable of generating content-dependent outputs at specific spatial locations. Unlike previous works, \ours generates a much smaller set of spatially-varying kernels from input, which enlarges the network's receptive field and significantly reduces computational and memory costs. These kernels are then applied to a full-resolution feature map through an efficient slice-and-conv operator with minimum memory overhead. We further build a efficient denoising network using MalleConv, coined as \textbf{MalleNet}. It achieves high quality results without very deep architecture, \eg, it is 8.91$\times$ faster than the best performed denoising algorithms (SwinIR), while maintaining similar performance. We also show that a single \ours added to a standard convolution-based backbones can contribute significantly reduce the computational cost or boost image quality at similar cost. Project page: <a class="link-external link-https" href="https://yifanjiang.net/MalleConv.html" rel="external noopener nofollow">this https URL</a>      
### 54.Randomized Signature Layers for Signal Extraction in Time Series Data  [ :arrow_down: ](https://arxiv.org/pdf/2201.00384.pdf)
>  Time series analysis is a widespread task in Natural Sciences, Social Sciences, and Engineering. A fundamental problem is finding an expressive yet efficient-to-compute representation of the input time series to use as a starting point to perform arbitrary downstream tasks. In this paper, we build upon recent works that use the Signature of a path as a feature map and investigate a computationally efficient technique to approximate these features based on linear random projections. We present several theoretical results to justify our approach and empirically validate that our random projections can effectively retrieve the underlying Signature of a path. We show the surprising performance of the proposed random features on several tasks, including (1) mapping the controls of stochastic differential equations to the corresponding solutions and (2) using the Randomized Signatures as time series representation for classification tasks. When compared to corresponding truncated Signature approaches, our Randomizes Signatures are more computationally efficient in high dimensions and often lead to better accuracy and faster training. Besides providing a new tool to extract Signatures and further validating the high level of expressiveness of such features, we believe our results provide interesting conceptual links between several existing research areas, suggesting new intriguing directions for future investigations.      
### 55.Leader-Follower Synchronization of a Network of Boundary-Controlled Parabolic Equations With In-Domain Coupling  [ :arrow_down: ](https://arxiv.org/pdf/2201.00343.pdf)
>  In this letter, we study the leader-synchronization problem for a class of partial differential equations with boundary control and in-domain coupling. We describe the problem in an abstract formulation and we specialize it to a network of parabolic partial differential equations. We consider a setting in which a subset of the followers is connected to the leader through a boundary control, while interconnections among the followers are enforced by distributed in-domain couplings. Sufficient conditions in the form of matrix inequalities for the selection of the control parameters enforcing exponential synchronization are given. Numerical simulations illustrate and corroborate the theoretical findings.      
### 56.Reinforcement Learning for Task Specifications with Action-Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2201.00286.pdf)
>  In this paper, we use concepts from supervisory control theory of discrete event systems to propose a method to learn optimal control policies for a finite-state Markov Decision Process (MDP) in which (only) certain sequences of actions are deemed unsafe (respectively safe). We assume that the set of action sequences that are deemed unsafe and/or safe are given in terms of a finite-state automaton; and propose a supervisor that disables a subset of actions at every state of the MDP so that the constraints on action sequence are satisfied. Then we present a version of the Q-learning algorithm for learning optimal policies in the presence of non-Markovian action-sequence and state constraints, where we use the development of reward machines to handle the state constraints. We illustrate the method using an example that captures the utility of automata-based methods for non-Markovian state and action specifications for reinforcement learning and show the results of simulations in this setting.      
### 57.Industrial Edge-based Cyber-Physical Systems -- Application Needs and Concerns for Realization  [ :arrow_down: ](https://arxiv.org/pdf/2201.00242.pdf)
>  Industry is moving towards advanced Cyber-Physical Systems (CPS), with trends in smartness, automation, connectivity and collaboration. We examine the drivers and requirements for the use of edge computing in critical industrial applications. Our purpose is to provide a better understanding of industrial needs and to initiate a discussion on what role edge computing could take, complementing current industrial and embedded systems, and the cloud. Four domains are chosen for analysis with representative use-cases; manufacturing, transportation, the energy sector and networked applications in the defense domain. We further discuss challenges, open issues and suggested directions that are needed to pave the way for the use of edge computing in industrial CPS.      
### 58.Low-Density Spreading Design Based on an Algebraic Scheme for NOMA Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.00204.pdf)
>  NOMA) technique based on an algebraic design is studied. We propose an improved low-density spreading (LDS) sequence design based on projective geometry. In terms of its bit error rate (BER) performance, our proposed improved LDS code set outperforms the existing LDS designs over the frequency nonselective Rayleigh fading and additive white Gaussian noise (AWGN) channels. We demonstrated that achieving the best BER depends on the minimum distance.      
### 59.Generating Adversarial Samples For Training Wake-up Word Detection Systems Against Confusing Words  [ :arrow_down: ](https://arxiv.org/pdf/2201.00167.pdf)
>  Wake-up word detection models are widely used in real life, but suffer from severe performance degradation when encountering adversarial samples. In this paper we discuss the concept of confusing words in adversarial samples. Confusing words are commonly encountered, which are various kinds of words that sound similar to the predefined keywords. To enhance the wake word detection system's robustness against confusing words, we propose several methods to generate the adversarial confusing samples for simulating real confusing words scenarios in which we usually do not have any real confusing samples in the training set. The generated samples include concatenated audio, synthesized data, and partially masked keywords. Moreover, we use a domain embedding concatenated system to improve the performance. Experimental results show that the adversarial samples generated in our approach help improve the system's robustness in both the common scenario and the confusing words scenario. In addition, we release the confusing words testing database called HI-MIA-CW for future research.      
### 60.Bird Species Classification And Acoustic Features Selection Based on Distributed Neural Network with Two Stage Windowing of Short-Term Features  [ :arrow_down: ](https://arxiv.org/pdf/2201.00124.pdf)
>  Identification of bird species from audio records is one of the challenging tasks due to the existence of multiple species in the same recording, noise in the background, and long-term recording. Besides, choosing a proper acoustic feature from audio recording for bird species classification is another problem. In this paper, a hybrid method is represented comprising both traditional signal processing and a deep learning-based approach to classify bird species from audio recordings of diverse sources and types. Besides, a detailed study with 34 different features helps to select the proper feature set for classification and analysis in real-time applications. Moreover, the proposed deep neural network uses both acoustic and temporal feature learning. The proposed method starts with detecting voice activity from the raw signal, followed by extracting short-term features from the processed recording using 50 ms (with 25ms overlapping) time windows. Later, the short-term features are reshaped using second stage (non-overlapping) windowing to be trained through a distributed 2D Convolutional Neural Network (CNN) that forwards the output features to a Long and Short Term Memory (LSTM) Network. Then a final dense layer classifies the bird species. For the 10 class classifier, the highest accuracy achieved was 90.45\% for a feature set consisting of 13 Mel Frequency Cepstral Coefficients (MFCCs) and 12 Chroma Vectors. The corresponding specificity and AUC scores are 98.94\% and 94.09\%, respectively.      
### 61.A Relaxed Energy Function Based Analog Neural Network Approach to Target Localization in Distributed MIMO Radar  [ :arrow_down: ](https://arxiv.org/pdf/2201.00122.pdf)
>  Analog neural networks are highly effective to solve some optimization problems, and they have been used for target localization in distributed multiple-input multiple-output (MIMO) radar. In this work, we design a new relaxed energy function based neural network (RNFNN) for target localization in distributed MIMO radar. We start with the maximum likelihood (ML) target localization with a complicated objective function, which can be transformed to a tractable one with equality constraints by introducing some auxiliary variables. Different from the existing Lagrangian programming neural network (LPNN) methods, we further relax the optimization problem formulated for target localization, so that the Lagrangian multiplier terms are no longer needed, leading to a relaxed energy function with better convexity. Based on the relaxed energy function, a RNFNN is implemented with much simpler structure and faster convergence speed. Furthermore, the RNFNN method is extended to localization in the presence of transmitter and receiver location errors. It is shown that the performance of the proposed localization approach achieves the Cramr-Rao lower bound (CRLB) within a wider range of signal-to-noise ratios (SNRs). Extensive comparisons with the state-of-the-art approaches are provided, which demonstrate the advantages of the proposed approach in terms of performance improvement and computational complexity (or convergence speed).      
### 62.Role of Data Augmentation Strategies in Knowledge Distillation for Wearable Sensor Data  [ :arrow_down: ](https://arxiv.org/pdf/2201.00111.pdf)
>  Deep neural networks are parametrized by several thousands or millions of parameters, and have shown tremendous success in many classification problems. However, the large number of parameters makes it difficult to integrate these models into edge devices such as smartphones and wearable devices. To address this problem, knowledge distillation (KD) has been widely employed, that uses a pre-trained high capacity network to train a much smaller network, suitable for edge devices. In this paper, for the first time, we study the applicability and challenges of using KD for time-series data for wearable devices. Successful application of KD requires specific choices of data augmentation methods during training. However, it is not yet known if there exists a coherent strategy for choosing an augmentation approach during KD. In this paper, we report the results of a detailed study that compares and contrasts various common choices and some hybrid data augmentation strategies in KD based human activity analysis. Research in this area is often limited as there are not many comprehensive databases available in the public domain from wearable devices. Our study considers databases from small scale publicly available to one derived from a large scale interventional study into human activity and sedentary behavior. We find that the choice of data augmentation techniques during KD have a variable level of impact on end performance, and find that the optimal network choice as well as data augmentation strategies are specific to a dataset at hand. However, we also conclude with a general set of recommendations that can provide a strong baseline performance across databases.      
### 63.Adversarial Attack via Dual-Stage Network Erosion  [ :arrow_down: ](https://arxiv.org/pdf/2201.00097.pdf)
>  Deep neural networks are vulnerable to adversarial examples, which can fool deep models by adding subtle perturbations. Although existing attacks have achieved promising results, it still leaves a long way to go for generating transferable adversarial examples under the black-box setting. To this end, this paper proposes to improve the transferability of adversarial examples, and applies dual-stage feature-level perturbations to an existing model to implicitly create a set of diverse models. Then these models are fused by the longitudinal ensemble during the iterations. The proposed method is termed Dual-Stage Network Erosion (DSNE). We conduct comprehensive experiments both on non-residual and residual networks, and obtain more transferable adversarial examples with the computational cost similar to the state-of-the-art method. In particular, for the residual networks, the transferability of the adversarial examples can be significantly improved by biasing the residual block information to the skip connections. Our work provides new insights into the architectural vulnerability of neural networks and presents new challenges to the robustness of neural networks.      
### 64.Evaluating Deep Music Generation Methods Using Data Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.00052.pdf)
>  Despite advances in deep algorithmic music generation, evaluation of generated samples often relies on human evaluation, which is subjective and costly. We focus on designing a homogeneous, objective framework for evaluating samples of algorithmically generated music. Any engineered measures to evaluate generated music typically attempt to define the samples' musicality, but do not capture qualities of music such as theme or mood. We do not seek to assess the musical merit of generated music, but instead explore whether generated samples contain meaningful information pertaining to emotion or mood/theme. We achieve this by measuring the change in predictive performance of a music mood/theme classifier after augmenting its training data with generated samples. We analyse music samples generated by three models -- SampleRNN, Jukebox, and DDSP -- and employ a homogeneous framework across all methods to allow for objective comparison. This is the first attempt at augmenting a music genre classification dataset with conditionally generated music. We investigate the classification performance improvement using deep music generation and the ability of the generators to make emotional music by using an additional, emotion annotation of the dataset. Finally, we use a classifier trained on real data to evaluate the label validity of class-conditionally generated samples.      
### 65.Experimental realization of the active convolved illumination imaging technique for enhanced signal-to-noise ratio  [ :arrow_down: ](https://arxiv.org/pdf/2201.00046.pdf)
>  Imaging is indispensable for nearly every field of science, engineering, technology, and medicine. However, measurement noise and stochastic distortions pose fundamental limits to accessible spatiotemporal information despite impressive tools such as SIM, PALM/STORM, and STED microscopy. How to combat this challenge ideally has been an open question for decades. Inspired by a "virtual gain" technique to compensate losses in metamaterials, "active convolved illumination" has been recently proposed to significantly improve the signal-to-noise ratio, hence data acquisition. In this technique, the light pattern of the object is superimposed with a correlated auxiliary pattern, the function of which is to reverse the adverse effect of noise and random distortion based on their spectral characteristics. Despite enormous implications in statistics, an experimental realization of this novel technique has been lacking to date. Here, we present the first experimental demonstration. We find that the active convolved illumination does not only boost the resolution limit and image contrast, but also the resistance to pixel saturation. The results confirm the previous theories and opens up new horizons in a wide range of disciplines from atmospheric sciences, seismology, biology, statistical learning, and information processing to quantum noise beyond the fundamental boundaries.      
### 66.Knowledge intensive state design for traffic signal control  [ :arrow_down: ](https://arxiv.org/pdf/2201.00006.pdf)
>  There is a general trend of applying reinforcement learning (RL) techniques for traffic signal control (TSC). Recently, most studies pay attention to the neural network design and rarely concentrate on the state representation. Does the design of state representation has a good impact on TSC? In this paper, we (1) propose an effective state representation as queue length of vehicles with intensive knowledge; (2) present a TSC method called MaxQueue based on our state representation approach; (3) develop a general RL-based TSC template called QL-XLight with queue length as state and reward and generate QL-FRAP, QL-CoLight, and QL-DQN by our QL-XLight template based on traditional and latest RL models.Through comprehensive experiments on multiple real-world datasets, we demonstrate that: (1) our MaxQueue method outperforms the latest RL based methods; (2) QL-FRAP and QL-CoLight achieves a new state-of-the-art (SOTA). In general, state representation with intensive knowledge is also essential for TSC methods. Our code is released on Github.      
