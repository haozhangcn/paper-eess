# ArXiv eess --Thu, 27 Jan 2022
### 1.RTNet: Relation Transformer Network for Diabetic Retinopathy Multi-lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.11037.pdf)
>  Automatic diabetic retinopathy (DR) lesions segmentation makes great sense of assisting ophthalmologists in diagnosis. Although many researches have been conducted on this task, most prior works paid too much attention to the designs of networks instead of considering the pathological association for lesions. Through investigating the pathogenic causes of DR lesions in advance, we found that certain lesions are closed to specific vessels and present relative patterns to each other. Motivated by the observation, we propose a relation transformer block (RTB) to incorporate attention mechanisms at two main levels: a self-attention transformer exploits global dependencies among lesion features, while a cross-attention transformer allows interactions between lesion and vessel features by integrating valuable vascular information to alleviate ambiguity in lesion detection caused by complex fundus structures. In addition, to capture the small lesion patterns first, we propose a global transformer block (GTB) which preserves detailed information in deep network. By integrating the above blocks of dual-branches, our network segments the four kinds of lesions simultaneously. Comprehensive experiments on IDRiD and DDR datasets well demonstrate the superiority of our approach, which achieves competitive performance compared to state-of-the-arts.      
### 2.A global sharing mechanism of resources: modeling a crucial step in the fight against pandemics  [ :arrow_down: ](https://arxiv.org/pdf/2201.11018.pdf)
>  To face pandemics like the one caused by COVID-19, resources such as personal protection equipment (PPE) are needed to reduce the infection rate and protect those in close contact with patients (Heymann and Shindo, 2020; Klompas et al., 2021). The demand for those products increases exponentially as the number of infected grows, outpacing any growth that local production facilities can achieve (Ranney et al., 2020, Wu et al., 2020). Disruptions in the global supply chain by closing factories or scaled-down transport routes can further increase resource scarcity (McMahon et al., 2020). During the first phase of the COVID-19 pandemic, we witnessed a reflex of `our people first' in many regions, countries, and continents (Baldwin and Evenett, 2020). In this paper, however, we show that a cooperative sharing mechanism can substantially improve the ability to face epidemics. We present a stylized model in which communities share their resources such that each can receive resources whenever a local epidemic flares up. This can potentially prevent local resource exhaustion and reduce the total number of infected cases. We also show that the success of sharing resources heavily depends on having a sufficiently long delay between the onset of epidemics in different communities. This means that a global sharing mechanism should be paired with measures to slow down the spread of infections from one community to the other. Our work is a first step towards designing a resilient global supply chain mechanism that can deal with future pandemics by design, rather than being subjected to the coincidental and unequal distribution of opportunities per community at present.      
### 3.A Multi-rater Comparative Study of Automatic Target Localization Methods for Epilepsy Deep Brain Stimulation Procedures  [ :arrow_down: ](https://arxiv.org/pdf/2201.11002.pdf)
>  Epilepsy is the fourth most common neurological disorder and affects people of all ages worldwide. Deep Brain Stimulation (DBS) has emerged as an alternative treatment option when anti-epileptic drugs or resective surgery cannot lead to satisfactory outcomes. To facilitate the planning of the procedure and for its standardization, it is desirable to develop an algorithm to automatically localize the DBS stimulation target, i.e., Anterior Nucleus of Thalamus (ANT), which is a challenging target to plan. In this work, we perform an extensive comparative study by benchmarking various localization methods for ANT-DBS. Specifically, the methods involved in this study include traditional registration method and deep-learning-based methods including heatmap matching and differentiable spatial to numerical transform (DSNT). Our experimental results show that the deep-learning (DL)-based localization methods that are trained with pseudo labels can achieve a performance that is comparable to the inter-rater and intra-rater variability and that they are orders of magnitude faster than traditional methods.      
### 4.One shot PACS: Patient specific Anatomic Context and Shape prior aware recurrent registration-segmentation of longitudinal thoracic cone beam CTs  [ :arrow_down: ](https://arxiv.org/pdf/2201.11000.pdf)
>  Image-guided adaptive lung radiotherapy requires accurate tumor and organs segmentation from during treatment cone-beam CT (CBCT) images. Thoracic CBCTs are hard to segment because of low soft-tissue contrast, imaging artifacts, respiratory motion, and large treatment induced intra-thoracic anatomic changes. Hence, we developed a novel Patient-specific Anatomic Context and Shape prior or PACS-aware 3D recurrent registration-segmentation network for longitudinal thoracic CBCT segmentation. Segmentation and registration networks were concurrently trained in an end-to-end framework and implemented with convolutional long-short term memory models. The registration network was trained in an unsupervised manner using pairs of planning CT (pCT) and CBCT images and produced a progressively deformed sequence of images. The segmentation network was optimized in a one-shot setting by combining progressively deformed pCT (anatomic context) and pCT delineations (shape context) with CBCT images. Our method, one-shot PACS was significantly more accurate (p$&lt;$0.001) for tumor (DSC of 0.83 $\pm$ 0.08, surface DSC [sDSC] of 0.97 $\pm$ 0.06, and Hausdorff distance at $95^{th}$ percentile [HD95] of 3.97$\pm$3.02mm) and the esophagus (DSC of 0.78 $\pm$ 0.13, sDSC of 0.90$\pm$0.14, HD95 of 3.22$\pm$2.02) segmentation than multiple methods. Ablation tests and comparative experiments were also done.      
### 5.Joint Liver and Hepatic Lesion Segmentation using a Hybrid CNN with Transformer Layers  [ :arrow_down: ](https://arxiv.org/pdf/2201.10981.pdf)
>  Deep learning-based segmentation of the liver and hepatic lesions therein steadily gains relevance in clinical practice due to the increasing incidence of liver cancer each year. Whereas various network variants with overall promising results in the field of medical image segmentation have been developed over the last years, almost all of them struggle with the challenge of accurately segmenting hepatic lesions. This lead to the idea of combining elements of convolutional and transformerbased architectures to overcome the existing limitations. This work presents a hybrid network called SWTR-Unet, consisting of a pretrained ResNet, transformer blocks as well as a common Unet-style decoder path. This network was applied to clinical liver MRI, as well as to the publicly available CT data of the liver tumor segmentation (LiTS) challenge. Additionally, multiple state-of-the-art networks were implemented and applied to both datasets, ensuring a direct comparability. Furthermore, correlation analysis and an ablation study were carried out, to investigate various influencing factors on the segmentation accuracy of our presented method. With Dice similarity scores of averaged 98 +- 2 % for liver and 81 +- 28 % lesion segmentation on the MRI dataset and 97 +- 2 % and 79 +- 25 %, respectively on the CT dataset, the proposed SWTR-Unet outperforms each of the additionally implemented state-of-the-art networks. The achieved segmentation accuracy was found to be on par with manually performed expert segmentations as indicated by interobserver variabilities for liver lesion segmentation. In conclusion, the presented method could save valuable time and resources in clinical practice.      
### 6.A Bayesian Based Deep Unrolling Algorithm for Single-Photon Lidar Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.10910.pdf)
>  Deploying 3D single-photon Lidar imaging in real world applications faces multiple challenges including imaging in high noise environments. Several algorithms have been proposed to address these issues based on statistical or learning-based frameworks. Statistical methods provide rich information about the inferred parameters but are limited by the assumed model correlation structures, while deep learning methods show state-of-the-art performance but limited inference guarantees, preventing their extended use in critical applications. This paper unrolls a statistical Bayesian algorithm into a new deep learning architecture for robust image reconstruction from single-photon Lidar data, i.e., the algorithm's iterative steps are converted into neural network layers. The resulting algorithm benefits from the advantages of both statistical and learning based frameworks, providing best estimates with improved network interpretability. Compared to existing learning-based solutions, the proposed architecture requires a reduced number of trainable parameters, is more robust to noise and mismodelling effects, and provides richer information about the estimates including uncertainty measures. Results on synthetic and real data show competitive results regarding the quality of the inference and computational complexity when compared to state-of-the-art algorithms.      
### 7.Hyperparameter Optimization for COVID-19 Chest X-Ray Classification  [ :arrow_down: ](https://arxiv.org/pdf/2201.10885.pdf)
>  Despite the introduction of vaccines, Coronavirus disease (COVID-19) remains a worldwide dilemma, continuously developing new variants such as Delta and the recent Omicron. The current standard for testing is through polymerase chain reaction (PCR). However, PCRs can be expensive, slow, and/or inaccessible to many people. X-rays on the other hand have been readily used since the early 20th century and are relatively cheaper, quicker to obtain, and typically covered by health insurance. With a careful selection of model, hyperparameters, and augmentations, we show that it is possible to develop models with 83% accuracy in binary classification and 64% in multi-class for detecting COVID-19 infections from chest x-rays.      
### 8.Predicting Knee Osteoarthritis Progression from Structural MRI using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.10849.pdf)
>  Accurate prediction of knee osteoarthritis (KOA) progression from structural MRI has a potential to enhance disease understanding and support clinical trials. Prior art focused on manually designed imaging biomarkers, which may not fully exploit all disease-related information present in MRI scan. In contrast, our method learns relevant representations from raw data end-to-end using Deep Learning, and uses them for progression prediction. The method employs a 2D CNN to process the data slice-wise and aggregate the extracted features using a Transformer. Evaluated on a large cohort (n=4,866), the proposed method outperforms conventional 2D and 3D CNN-based models and achieves average precision of $0.58\pm0.03$ and ROC AUC of $0.78\pm0.01$. This paper sets a baseline on end-to-end KOA progression prediction from structural MRI. Our code is publicly available at <a class="link-external link-https" href="https://github.com/MIPT-Oulu/OAProgressionMR" rel="external noopener nofollow">this https URL</a>.      
### 9.A two-step backward compatible fullband speech enhancement system  [ :arrow_down: ](https://arxiv.org/pdf/2201.10809.pdf)
>  Speech enhancement methods based on deep learning have surpassed traditional methods. While many of these new approaches are operating on the wideband (16kHz) sample rate, a new fullband (48kHz) speech enhancement system is proposed in this paper. Compared to the existing fullband systems that utilizes perceptually motivated features to train the fullband speech enhancement using a single network structure, the proposed system is a two-step system ensuring good fullband speech enhancement quality while backward compatible to the existing wideband systems.      
### 10.SkiM: Skipping Memory LSTM for Low-Latency Real-Time Continuous Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2201.10800.pdf)
>  Continuous speech separation for meeting pre-processing has recently become a focused research topic. Compared to the data in utterance-level speech separation, the meeting-style audio stream lasts longer, has an uncertain number of speakers. We adopt the time-domain speech separation method and the recently proposed Graph-PIT to build super low-latency online speech separation model, which is very important for the real application. The low-latency time-domain encoder with a small stride leads to an extremely long feature sequence. We proposed a simple yet efficient model named Skipping Memory (SkiM) for the long sequence modeling. Experimental results show that SkiM achieves on par or even better separation performance than DPRNN. Meanwhile, the computational cost of SkiM is reduced by 75% compared to DPRNN. The strong long sequence modeling capability and low computational cost make SkiM a suitable model for online CSS applications. Our fastest real-time model gets 17.1 dB signal-to-distortion (SDR) improvement with less than 1.0 millisecond latency in the simulated meeting-style evaluation.      
### 11.Bandwidth and Power Allocation for Task-Oriented SemanticCommunication  [ :arrow_down: ](https://arxiv.org/pdf/2201.10795.pdf)
>  Deep learning enabled semantic communication has been studied to improve communication efficiency while guaranteeing intelligent task performance. Different from conventional communications systems, the resource allocation in semantic communications no longer just pursues the bit transmission rate, but focuses on how to better compress and transmit semantic to complete subsequent intelligent tasks. This paper aims to appropriately allocate the bandwidth and power for artificial intelligence (AI) task-oriented semantic communication and proposes a joint compressiom ratio and resource allocation (CRRA) algorithm. We first analyze the relationship between the AI task's performance and the semantic information. Then, to optimize the AI task's perfomance under resource constraints, a bandwidth and power allocation problem is formulated. The problem is first separated into two subproblems due to the non-convexity. The first subproblem is a compression ratio optimization problem with a given resource allocation scheme, which is solved by a enumeration algorithm. The second subproblem is to find the optimal resource allocation scheme, which is transformed into a convex problem by successive convex approximation method, and solved by a convex optimization method. The optimal semantic compression ratio and resource allocation scheme are obtained by iteratively solving these two subproblems. Simulation results show that the proposed algorithm can efficiently improve the AI task's performance by up to 30\% comprared with baselines.      
### 12.Security-Constrained Optimal Operation of Energy-Water Nexus based on a Fast Contingency Filtering Method  [ :arrow_down: ](https://arxiv.org/pdf/2201.10785.pdf)
>  Water and power systems are increasingly interdependent due to the growing number of electricity-driven water facilities. The security of one system can be affected by a contingency in the other system. This paper investigates a security-constrained operation problem of the energy-water nexus (EWN), which is a computationally challenging optimization problem due to the nonlinearity, nonconvexity, and size. We propose a two-step iterative contingency filtering method based on the feasibility and rating of the contingencies to decrease the size of the problem. The optimal power and water flow are obtained in a normal situation by considering the set of contingencies that can not be controlled with corrective actions. The feasibility check of the contingencies is performed in the second step, followed by a rating of the uncontrollable contingencies. Finally, the critical contingencies are obtained and added to the first step for the next iteration. We also employ convex technologies to reduce the computation burden. The proposed method is validated via two case studies. Results indicate that this approach can efficiently attain optimal values.      
### 13.DSFormer: A Dual-domain Self-supervised Transformer for Accelerated Multi-contrast MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2201.10776.pdf)
>  Multi-contrast MRI (MC-MRI) captures multiple complementary imaging modalities to aid in radiological decision-making. Given the need for lowering the time cost of multiple acquisitions, current deep accelerated MRI reconstruction networks focus on exploiting the redundancy between multiple contrasts. However, existing works are largely supervised with paired data and/or prohibitively expensive fully-sampled MRI sequences. Further, reconstruction networks typically rely on convolutional architectures which are limited in their capacity to model long-range interactions and may lead to suboptimal recovery of fine anatomical detail. To these ends, we present a dual-domain self-supervised transformer (DSFormer) for accelerated MC-MRI reconstruction. DSFormer develops a deep conditional cascade transformer (DCCT) consisting of several cascaded Swin transformer reconstruction networks (SwinRN) trained under two deep conditioning strategies to enable MC-MRI information sharing. We further present a dual-domain (image and k-space) self-supervised learning strategy for DCCT to alleviate the costs of acquiring fully sampled training data. DSFormer generates high-fidelity reconstructions which experimentally outperform current fully-supervised baselines. Moreover, we find that DSFormer achieves nearly the same performance when trained either with full supervision or with our proposed dual-domain self-supervision.      
### 14.Hybrid 3D Beamforming Relying on Sensor-Based Training and Channel Estimation for Reconfigurable Intelligent Surface Aided TeraHertz MIMO systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.10757.pdf)
>  Terahertz (THz) systems have the benefit of high bandwidth and hence are capable of supporting ultra-high data rates, albeit at the cost of high pathloss. Hence they tend to harness high-gain beamforming. Therefore a novel hybrid 3D beamformer relying on sophisticated sensor-based beam training and channel estimation is proposed for Reconfigurable Intelligent Surface (RIS) aided THz MIMO systems. A so-called array-of-subarray based THz BS architecture is adopted and the corresponding sub-RIS structure is proposed. The BS, RIS and receiver antenna arrays of the users are all uniform planar arrays (UPAs). The Ultra-wideband (UWB) sensors are integrated into the RIS and the user location information obtained by the UWB sensors is exploited for channel estimation and beamforming. Furthermore, the novel concept of a Precise Beamforming Algorithm (PBA) is proposed, which further improves the beam-forming accuracy by circumventing the performance limitations imposed by positioning errors. Moreover, the conditions of maintaining the orthogonality of the RIS-aided THz channel are derived in support of spatial multiplexing. The closed-form expressions of the near-field and far-field path-loss are also derived. Our simulation results show that the proposed scheme accurately estimates the RIS-aided THz channel and the spectral efficiency is much improved, despite its low complexity. This makes our solution eminently suitable for delay-sensitive applications.      
### 15.Learning Multiple Probabilistic Degradation Generators for Unsupervised Real World Image Super Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2201.10747.pdf)
>  Unsupervised real world super resolution (USR) aims at restoring high-resolution (HR) images given low-resolution (LR) inputs when paired data is unavailable. One of the most common approaches is synthesizing noisy LR images using GANs and utilizing a synthetic dataset to train the model in a supervised manner. The goal of modeling the degradation generator is to approximate the distribution of LR images given a HR image. Previous works simply assumed the conditional distribution as a delta function and learned the deterministic mapping from HR image to a LR image. Instead, we propose the probabilistic degradation generator. Our degradation generator is a deep hierarchical latent variable model and more suitable for modeling the complex distribution. Furthermore, we train multiple degradation generators to enhance the mode coverage and apply the novel collaborative learning. We outperform several baselines on benchmark datasets in terms of PSNR and SSIM and demonstrate the robustness of our method on unseen data distribution.      
### 16.Downlink Precoding for FBMC-based Massive MIMO with Imperfect Channel Reciprocity  [ :arrow_down: ](https://arxiv.org/pdf/2201.10735.pdf)
>  In this paper, a practical precoding method for the downlink of filter bank multicarrier-based (FBMC-based) massive multiple-input multiple-output (MIMO) is developed. The proposed method includes a two-stage precoder consisting of a fractionally spaced prefilter (FSP) per subcarrier for flattening/equalizing the channel across the subcarrier band, followed by a conventional precoder whose goal is to concentrate the signals of different users at their spatial locations. This way, each user receives only the intended information. In this paper, we take note that channel reciprocity may not hold perfectly in practical scenarios due to the mismatch of radio chains in uplink and downlink. Additionally, channel state information (CSI) at the base station may not be perfectly known. This, together with imperfect channel reciprocity can lead to detrimental effects on the downlink precoder performance. We theoretically analyze the performance of the proposed precoder in the presence of imperfect CSI and channel reciprocity calibration errors. This leads to an effective method for compensating these effects. Finally, we numerically evaluate the performance of the proposed precoder. Our results show that the proposed precoder leads to an excellent performance when benchmarked against OFDM.      
### 17.iDROP: Robust Localization for Indoor Navigation of Drones with Optimized Beacon Placement  [ :arrow_down: ](https://arxiv.org/pdf/2201.10698.pdf)
>  Drones in many applications need the ability to fly fully or partially autonomously to accomplish their mission. To allow these fully/partially autonomous flights, first, the drone needs to be able to locate itself constantly. Then the navigation command signal would be generated and passed on to the controller unit of the drone. In this paper, we propose a localization scheme for drones called iDROP (Robust Localization for Indoor Navigation of Drones with Optimized Beacon Placement) that is specifically devised for GPS-denied environments (e.g., indoor spaces). Instead of GPS signals, iDROP relies on speaker-generated ultrasonic acoustic signals to enable a drone to estimate its location. In general, localization error is due to two factors: the ranging error and the error induced by relative geometry between the transmitters and the receiver. iDROP mitigates these two types of errors and provides a high-precision three-dimensional localization scheme for drones. iDROP employs a waveform that is robust against multi-path fading. Moreover, by placing beacons in optimal locations, it reduces the localization error induced by the relative geometry between the transmitters and the receiver.      
### 18.Invertible Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2201.10687.pdf)
>  In this paper, we propose an invertible deep learning framework called INVVC for voice conversion. It is designed against the possible threats that inherently come along with voice conversion systems. Specifically, we develop an invertible framework that makes the source identity traceable. The framework is built on a series of invertible $1\times1$ convolutions and flows consisting of affine coupling layers. We apply the proposed framework to one-to-one voice conversion and many-to-one conversion using parallel training data. Experimental results show that this approach yields impressive performance on voice conversion and, moreover, the converted results can be reversed back to the source inputs utilizing the same parameters as in forwarding.      
### 19.Design and Development of an Autonomous Surface Vehicle for Water Quality Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2201.10685.pdf)
>  Manually monitoring water quality is very exhausting and requires several hours of sampling and laboratory testing for a particular body of water. This article presents a solution to test water properties like electrical conductivity and pH with a remote-controlled floating vehicle that minimizes time intervals. An autonomous surface vehicle (ASV) has been designed mathematically and operated via MATLAB \&amp; Simulink simulation where the Proportional integral derivative (PID) controller has been considered. A PVC model with Small waterplane area twin-hull (SWATH) technology is used to develop this vehicle. Manually collected data is compared to online sensors, suggesting a better solution for determining water properties such as dissolved oxygen (DO), biochemical oxygen demand (BOD), temperature, conductivity, total alkalinity, and bacteria. Preliminary computational results show the promising result, as Sungai Pasu rivers tested water falls in the safe range of pH (~6.8-7.14) using the developed ASV.      
### 20.Deep-Learning Assisted Reconfigurable Intelligent Surfaces for Cooperative Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.10648.pdf)
>  Reconfigurable intelligent surfaces (RISs) are software-controlled passive devices to reflect incoming signals from the source ($S$) to destination ($D$), just like a relay ($R$) with optimum signal strength, improving the performance of wireless communication networks. The configurable nature of the RIS can provide network designers the flexibility to use in a stand-alone or cooperative configuration with many advantages over conventional networks. In this paper, two new deep neural networks (DNN) assisted cooperative RIS models, namely DNN$_R$\:-\:CRIS and DNN$_{R, D}$\:-\:CRIS, are proposed for cooperative communications. In these two models, the potential of RIS deployment as a relaying element in a next-generation cooperative network is investigated using deep learning (DL) techniques as a tool for optimizing the RIS. To reduce maximum likelihood (ML) complexity at the $D$, unlike the DNN$_R$\:-\:CRIS, in the DNN$_{R, D}$\:-\:CRIS model, a new DNN based symbol detection method is presented for the same network model. For a different number of relays and receiver configurations, bit error rate (BER) performance results of the proposed DNN$_R$\:-\:CRIS, DNN$_{R, D}$\:-\:CRIS models and traditional cooperative RIS (CRIS) scheme (without DNN) are presented for a multi-relay cooperative communication scenario with path loss effects.      
### 21.Differentially-Private Heat and Electricity Markets Coordination  [ :arrow_down: ](https://arxiv.org/pdf/2201.10634.pdf)
>  Sector coordination between heat and electricity systems has been identified has an energy-efficient and cost-effective way to transition towards a more sustainable energy system. However, the coordination of sequential markets relies on the exchange of sensitive information between the market operators, namely time series of consumers' loads. To address the privacy concerns arising from this exchange, this paper introduces a novel privacy-preserving Stackelberg mechanism (w-PPSM) which generates differentially-private data streams with high fidelity. The proposed w-PPSM enforces the feasibility and fidelity of the privacy-preserving data with respect to the original problem through a post-processing phase in order to achieve a close-to-optimal coordination between the markets. Multiple numerical simulations in a realistic energy system demonstrate the effectiveness of the w-PPSM, which achieves up to two orders of magnitude reduction in the cost of privacy compared to a traditional differentially-private mechanism.      
### 22.Game-Theoretic Energy Source Allocation Mechanism in Smart-Grids  [ :arrow_down: ](https://arxiv.org/pdf/2201.10630.pdf)
>  This work studies the decentralized and uncoordinated energy source selection problem for smart-grid consumers with heterogeneous energy profiles and risk attitudes, which compete for a limited amount of renewable energy sources. We model this problem as a non-cooperative game and study the existence of mixed-strategy Nash equilibria under the proportional allocation policy. Also, we derive closed-form expressions of the renewable energy demand and social cost under varying consumer profiles, energy costs and availability. We study numerically the efficiency of this decentralized scheme compared to a centralized one via the price of anarchy metric.      
### 23.Rate-Splitting Multiple Access for Multi-Antenna Joint Radar and Communications with Partial CSIT: Precoder Optimization and Link-Level Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2201.10621.pdf)
>  Dual-Functional Radar-Communication (DFRC) systems have been investigated to manage the inter-system interference between radar and communication systems. However, the studies in literature often assume that the DFRC possesses perfect Channel State Information at the Transmitter (CSIT), which is an unrealistic assumption due to the inevitable CSIT errors in practical deployments. In this work, we aim to design a DFRC system under the practical assumption of partial CSIT. To achieve this, the proposed DFRC marries the capabilities of a Multiple-Input Multiple-Output (MIMO) radar with Rate-Splitting Multiple Access (RSMA). RSMA is a powerful downlink communication scheme based on linearly precoded Rate-Splitting (RS) that partially decodes multi-user interference (MUI) and partially treats it as noise and is inherently robust to partial CSIT. Using RSMA, the DFRC precoders are optimized in the presence of partial CSIT to simultaneously maximize the Average Weighted Sum-Rate (AWSR) under Quality-of-Service (QoS) constraints and minimize the DFRC beampattern Mean Squared Error (MSE) against an ideal MIMO radar beampattern. Simulation results demonstrate that the RSMA-based DFRC largely outperforms DFRCs based on other commonly used strategies such as Space Division Multiple Access (SDMA) and Non-Orthogonal Multiple Access (NOMA). Specifically, the common stream unique in the RSMA-based DFRC allows for flexible rate partitioning to guarantee user rate fairness with partial CSIT while also being the main contributor to generating a directional beampattern for effective radar sensing. The practical performance of the DFRC is then further assessed through Link-Level simulations (LLS) to take into account the effects of coding and modulation in the finite length regime as well as the channel aging due to mobility and latency, where the superiority of RSMA is again corroborated.      
### 24.Covert Communications through Imperfect Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2201.10611.pdf)
>  We propose a method for covert communications using an IEEE 802.11 OFDM/QAM packet as a carrier. We show how to hide the covert message so that the transmitted signal does not violate the spectral mask specified by the standard, and we determine its impact on the OFDM packet error rate. We show conditions under which the hidden signal is not usable and those under which it can be retrieved with a usable bit error rate (BER). The hidden signal is extracted by cancellation of the OFDM signal in the covert receiver. We explore the effects of the hidden signal on OFDM parameter estimation and the covert signal BER. We conclude with an experiment using Over-The-Air recordings of 802.11 packets, inject the hidden      
### 25.Self-attention fusion for audiovisual emotion recognition with incomplete data  [ :arrow_down: ](https://arxiv.org/pdf/2201.11095.pdf)
>  In this paper, we consider the problem of multimodal data analysis with a use case of audiovisual emotion recognition. We propose an architecture capable of learning from raw data and describe three variants of it with distinct modality fusion mechanisms. While most of the previous works consider the ideal scenario of presence of both modalities at all times during inference, we evaluate the robustness of the model in the unconstrained settings where one modality is absent or noisy, and propose a method to mitigate these limitations in a form of modality dropout. Most importantly, we find that following this approach not only improves performance drastically under the absence/noisy representations of one modality, but also improves the performance in a standard ideal setting, outperforming the competing methods.      
### 26.Understanding and Compressing Music with Maximal Transformable Patterns  [ :arrow_down: ](https://arxiv.org/pdf/2201.11085.pdf)
>  We present a polynomial-time algorithm that discovers all maximal patterns in a point set, $D\subset\mathbb{R}^k$, that are related by transformations in a user-specified class, $F$, of bijections over $\mathbb{R}^k$. We also present a second algorithm that discovers the set of occurrences for each of these maximal patterns and then uses compact encodings of these occurrence sets to compute a losslessly compressed encoding of the input point set. This encoding takes the form of a set of pairs, $E=\left\lbrace\left\langle P_1, T_1\right\rangle,\left\langle P_2, T_2\right\rangle,\ldots\left\langle P_{\ell}, T_{\ell}\right\rangle\right\rbrace$, where each $\langle P_i,T_i\rangle$ consists of a maximal pattern, $P_i\subseteq D$, and a set, $T_i\subset F$, of transformations that map $P_i$ onto other subsets of $D$. Each transformation is encoded by a vector of real values that uniquely identifies it within $F$ and the length of this vector is used as a measure of the complexity of $F$. We evaluate the new compression algorithm with three transformation classes of differing complexity, on the task of classifying folk-song melodies into tune families. The most complex of the classes tested includes all combinations of the musical transformations of transposition, inversion, retrograde, augmentation and diminution. We found that broadening the transformation class improved performance on this task. However, it did not, on average, improve compression factor, which may be due to the datasets (in this case, folk-song melodies) being too short and simple to benefit from the potentially greater number of pattern relationships that are discoverable with larger transformation classes.      
### 27.Learnable Wavelet Packet Transform for Data-Adapted Spectrograms  [ :arrow_down: ](https://arxiv.org/pdf/2201.11069.pdf)
>  Capturing high-frequency data concerning the condition of complex systems, e.g. by acoustic monitoring, has become increasingly prevalent. Such high-frequency signals typically contain time dependencies ranging over different time scales and different types of cyclic behaviors. Processing such signals requires careful feature engineering, particularly the extraction of meaningful time-frequency features. This can be time-consuming and the performance is often dependent on the choice of parameters. To address these limitations, we propose a deep learning framework for learnable wavelet packet transforms, enabling to learn features automatically from data and optimise them with respect to the defined objective function. The learned features can be represented as a spectrogram, containing the important time-frequency information of the dataset. We evaluate the properties and performance of the proposed approach by evaluating its improved spectral leakage and by applying it to an anomaly detection task for acoustic monitoring.      
### 28.Meta-optic Accelerators for Object Classifiers  [ :arrow_down: ](https://arxiv.org/pdf/2201.11034.pdf)
>  Rapid advances in deep learning have led to paradigm shifts in a number of fields, from medical image analysis to autonomous systems. These advances, however, have resulted in digital neural networks with large computational requirements, resulting in high energy consumption and limitations in real-time decision making when computation resources are limited. Here, we demonstrate a meta-optic based neural network accelerator that can off-load computationally expensive convolution operations into high-speed and low-power optics. In this architecture, metasurfaces enable both spatial multiplexing and additional information channels, such as polarization, in object classification. End-to-end design is used to co-optimize the optical and digital systems resulting in a robust classifier that achieves 95% accurate classification of handwriting digits and 94% accuracy in classifying both the digit and its polarization state. This approach could enable compact, high-speed, and low-power image and information processing systems for a wide range of applications in machine-vision and artificial intelligence.      
### 29.Social Learning under Randomized Collaborations  [ :arrow_down: ](https://arxiv.org/pdf/2201.10957.pdf)
>  We study a social learning scheme where at every time instant, each agent chooses to receive information from one of its neighbors at random. We show that under this sparser communication scheme, the agents learn the truth eventually and the asymptotic convergence rate remains the same as the standard algorithms which use more communication resources. We also derive large deviation estimates of the log-belief ratios for a special case where each agent replaces its belief with that of the chosen neighbor.      
### 30.Dual-Tasks Siamese Transformer Framework for Building Damage Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2201.10953.pdf)
>  Accurate and fine-grained information about the extent of damage to buildings is essential for humanitarian relief and disaster response. However, as the most commonly used architecture in remote sensing interpretation tasks, Convolutional Neural Networks (CNNs) have limited ability to model the non-local relationship between pixels. Recently, Transformer architecture first proposed for modeling long-range dependency in natural language processing has shown promising results in computer vision tasks. Considering the frontier advances of Transformer architecture in the computer vision field, in this paper, we present the first attempt at designing a Transformer-based damage assessment architecture (DamFormer). In DamFormer, a siamese Transformer encoder is first constructed to extract non-local and representative deep features from input multitemporal image-pairs. Then, a multitemporal fusion module is designed to fuse information for downstream tasks. Finally, a lightweight dual-tasks decoder aggregates multi-level features for final prediction. To the best of our knowledge, it is the first time that such a deep Transformer-based network is proposed for multitemporal remote sensing interpretation tasks. The experimental results on the large-scale damage assessment dataset xBD demonstrate the potential of the Transformer-based architecture.      
### 31.FIGARO: Generating Symbolic Music with Fine-Grained Artistic Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.10936.pdf)
>  Generating music with deep neural networks has been an area of active research in recent years. While the quality of generated samples has been steadily increasing, most methods are only able to exert minimal control over the generated sequence, if any. We propose the self-supervised \emph{description-to-sequence} task, which allows for fine-grained controllable generation on a global level by extracting high-level features about the target sequence and learning the conditional distribution of sequences given the corresponding high-level description in a sequence-to-sequence modelling setup. We train FIGARO (FIne-grained music Generation via Attention-based, RObust control) by applying \emph{description-to-sequence} modelling to symbolic music. By combining learned high level features with domain knowledge, which acts as a strong inductive bias, the model achieves state-of-the-art results in controllable symbolic music generation and generalizes well beyond the training distribution.      
### 32.Task-Oriented Semantic Communication Systems Based on Extended Rate-Distortion Theory  [ :arrow_down: ](https://arxiv.org/pdf/2201.10929.pdf)
>  Considering the performance of intelligent task during signal exchange can help the communication system to automatically select those semantic parts which are helpful to perform the target task for compression and reconstruction, which can both greatly reduce the redundancy in signal and ensure the performance of the task. The traditional communication system based on rate-distortion theory treats all the information in the signal equally, but ignores their different importance to accomplish the task, which leads to waste of communication resources. In this paper, combined with the information bottleneck method, we present an extended rate-distortion theory which considers both concise representation and semantic distortion. Based on this theory, a task-oriented semantic image communication system is proposed. In order to verify that the proposed system can achieve performance improvement on different intelligent tasks, we apply the basic system trained with classification task to the system with object detection as the target task. The experimental results demonstrate that the proposed method outperforms the traditional and multi-task based communication system in terms of task performance at the same signal compression degree and noise interference degree. Furthermore, it is necessary to consider a compromise between rate-distortion theory and information bottleneck theory by comparing the pure rate-distortion scheme and the pure IB scheme.      
### 33.J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2201.10896.pdf)
>  In this paper, we construct a Japanese audiobook speech corpus called "J-MAC" for speech synthesis research. With the success of reading-style speech synthesis, the research target is shifting to tasks that use complicated contexts. Audiobook speech synthesis is a good example that requires cross-sentence, expressiveness, etc. Unlike reading-style speech, speaker-specific expressiveness in audiobook speech also becomes the context. To enhance this research, we propose a method of constructing a corpus from audiobooks read by professional speakers. From many audiobooks and their texts, our method can automatically extract and refine the data without any language dependency. Specifically, we use vocal-instrumental separation to extract clean data, connectionist temporal classification to roughly align text and audio, and voice activity detection to refine the alignment. J-MAC is open-sourced in our project page. We also conduct audiobook speech synthesis evaluations, and the results give insights into audiobook speech synthesis.      
### 34.The Norwegian Parliamentary Speech Corpus  [ :arrow_down: ](https://arxiv.org/pdf/2201.10881.pdf)
>  The Norwegian Parliamentary Speech Corpus (NPSC) is a speech dataset with recordings of meetings from Stortinget, the Norwegian parliament. It is the first, publicly available dataset containing unscripted, Norwegian speech designed for training of automatic speech recognition (ASR) systems. The recordings are manually transcribed and annotated with language codes and speakers, and there are detailed metadata about the speakers. The transcriptions exist in both normalized and non-normalized form, and non-standardized words are explicitly marked and annotated with standardized equivalents. To test the usefulness of this dataset, we have compared an ASR system trained on the NPSC with a baseline system trained on only manuscript-read speech. These systems were tested on an independent dataset containing spontaneous, dialectal speech. The NPSC-trained system performed significantly better, with a 22.9% relative improvement in word error rate (WER). Moreover, training on the NPSC is shown to have a "democratizing" effect in terms of dialects, as improvements are generally larger for dialects with higher WER from the baseline system.      
### 35.A Partial Channel Reciprocity-based Codebook for Wideband FDD Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2201.10829.pdf)
>  The acquisition of channel state information (CSI) in Frequency Division Duplex (FDD) massive MIMO has been a formidable challenge. In this paper, we address this problem with a novel CSI feedback framework enabled by the partial reciprocity of uplink and downlink channels in the wideband regime. We first derive the closed-form expression of the rank of the wideband massive MIMO channel covariance matrix for a given angle-delay distribution. A low-rankness property is identified, which generalizes the well-known result of the narrow-band uniform linear array setting. Then we propose a partial channel reciprocity (PCR) codebook, inspired by the low-rankness behavior and the fact that the uplink and downlink channels have similar angle-delay distributions. Compared to the latest codebook in 5G, the proposed PCR codebook scheme achieves higher performance, lower complexity at the user side, and requires a smaller amount of feedback. We derive the feedback overhead necessary to achieve asymptotically error-free CSI feedback. Two low-complexity alternatives are also proposed to further reduce the complexity at the base station side. Simulations with the practical 3GPP channel model show the significant gains over the latest 5G codebook, which prove that our proposed methods are practical solutions for 5G and beyond.      
### 36.Incorporate Day-ahead Robustness and Real-time Incentives for Electricity Market Design  [ :arrow_down: ](https://arxiv.org/pdf/2201.10827.pdf)
>  In this paper, we propose a two-stage electricity market framework to explore the participation of distributed energy resources (DERs) in a day-ahead (DA) market and a real-time (RT) market. The objective is to determine the optimal bidding strategies of the aggregated DERs in the DA market and generate online incentive signals for DER-owners to optimize the social welfare taking into account network operational constraints. Distributionally robust optimization is used to explicitly incorporate data-based statistical information of renewable forecasts into the supply/demand decisions in the DA market. We evaluate the conservativeness of bidding strategies distinguished by different risk aversion settings. In the RT market, a bi-level time-varying optimization problem is proposed to design the online incentive signals to tradeoff the RT imbalance penalty for distribution system operators (DSOs) and the costs of individual DER-owners. This enables tracking their optimal dispatch to provide fast balancing services, in the presence of time-varying network states while satisfying the voltage regulation requirement. Simulation results on both DA wholesale market and RT balancing market demonstrate the necessity of this two-stage design, and its robustness to uncertainties, the performance of convergence, the tracking ability, and the feasibility of the resulting network operations.      
### 37.Dual-Decoder Transformer For end-to-end Mandarin Chinese Speech Recognition with Pinyin and Character  [ :arrow_down: ](https://arxiv.org/pdf/2201.10792.pdf)
>  End-to-end automatic speech recognition (ASR) has achieved promising results. However, most existing end-to-end ASR methods neglect the use of specific language characteristics. For Mandarin Chinese ASR tasks, pinyin and character as writing and spelling systems respectively are mutual promotion in the Mandarin Chinese language. Based on the above intuition, we investigate types of related models that are suitable but not for joint pinyin-character ASR and propose a novel Mandarin Chinese ASR model with dual-decoder Transformer according to the characteristics of the pinyin transcripts and character transcripts. Specifically, the joint pinyin-character layer-wise linear interactive (LWLI) module and phonetic posteriorgrams adapter (PPGA) are proposed to achieve inter-layer multi-level interaction by adaptively fusing pinyin and character information. Furthermore, a two-stage training strategy is proposed to make training more stable and faster convergence. The results on the test sets of AISHELL-1 dataset show that the proposed Speech-Pinyin-Character-Interaction (SPCI) model without a language model achieves 9.85% character error rate (CER) on the test set, which is 17.71% relative reduction compared to baseline models based on Transformer.      
### 38.Class-Aware Generative Adversarial Transformers for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.10737.pdf)
>  Transformers have made remarkable progress towards modeling long-range dependencies within the medical image analysis domain. However, current transformer-based models suffer from several disadvantages: (1) existing methods fail to capture the important features of the images due to the naive tokenization scheme; (2) the models suffer from information loss because they only consider single-scale feature representations; and (3) the segmentation label maps generated by the models are not accurate enough without considering rich semantic contexts and anatomical textures. In this work, we present CA-GANformer, a novel type of generative adversarial transformers, for medical image segmentation. First, we take advantage of the pyramid structure to construct multi-scale representations and handle multi-scale variations. We then design a novel class-aware transformer module to better learn the discriminative regions of objects with semantic structures. Lastly, we utilize an adversarial training strategy that boosts segmentation accuracy and correspondingly allows a transformer-based discriminator to capture high-level semantically correlated contents and low-level anatomical features. Our experiments demonstrate that CA-GANformer dramatically outperforms previous state-of-the-art transformer-based approaches on three benchmarks, obtaining 2.54%-5.88% absolute improvements in Dice over previous models. Further qualitative experiments provide a more detailed picture of the model's inner workings, shed light on the challenges in improved transparency, and demonstrate that transfer learning can greatly improve performance and reduce the size of medical image datasets in training, making CA-GANformer a strong starting point for downstream medical image analysis tasks. Codes and models will be available to the public.      
### 39.A fast-solved model for energy-efficient train control based on convex optimization  [ :arrow_down: ](https://arxiv.org/pdf/2201.10731.pdf)
>  In modern rail transportation, energy-efficient train control (EETC) is concerned with the optimal train speed trajectory or control strategies to achieve the minimum energy cost under various operation and traction constraints. This paper proposes an EETC model based on convex optimization so that the model can be rapidly solved by convex optimization algorithms. The high computational efficiency and robustness of the convex model can be verified by comparing the results achieved by the method proposed by this paper and other mainstream mathematical programming methods including mixed-integer linear programming (MILP) and Radau pseudospectral method (RPM). Based on the characteristics of convex optimization, the proposed method boasts more significant advantages over its counterparts in terms of computational efficiency in the promising online applications for automatic train control systems of various types of rail transportation.      
### 40.Neural Grapheme-to-Phoneme Conversion with Pre-trained Grapheme Models  [ :arrow_down: ](https://arxiv.org/pdf/2201.10716.pdf)
>  Neural network models have achieved state-of-the-art performance on grapheme-to-phoneme (G2P) conversion. However, their performance relies on large-scale pronunciation dictionaries, which may not be available for a lot of languages. Inspired by the success of the pre-trained language model BERT, this paper proposes a pre-trained grapheme model called grapheme BERT (GBERT), which is built by self-supervised training on a large, language-specific word list with only grapheme information. Furthermore, two approaches are developed to incorporate GBERT into the state-of-the-art Transformer-based G2P model, i.e., fine-tuning GBERT or fusing GBERT into the Transformer model by attention. Experimental results on the Dutch, Serbo-Croatian, Bulgarian and Korean datasets of the SIGMORPHON 2021 G2P task confirm the effectiveness of our GBERT-based G2P models under both medium-resource and low-resource data conditions.      
### 41.Toward Data-Driven STAP Radar  [ :arrow_down: ](https://arxiv.org/pdf/2201.10712.pdf)
>  Using an amalgamation of techniques from classical radar, computer vision, and deep learning, we characterize our ongoing data-driven approach to space-time adaptive processing (STAP) radar. We generate a rich example dataset of received radar signals by randomly placing targets of variable strengths in a predetermined region using RFView, a site-specific radio frequency modeling and simulation tool developed by ISL Inc. For each data sample within this region, we generate heatmap tensors in range, azimuth, and elevation of the output power of a minimum variance distortionless response (MVDR) beamformer, which can be replaced with a desired test statistic. These heatmap tensors can be thought of as stacked images, and in an airborne scenario, the moving radar creates a sequence of these time-indexed image stacks, resembling a video. Our goal is to use these images and videos to detect targets and estimate their locations, a procedure reminiscent of computer vision algorithms for object detection$-$namely, the Faster Region-Based Convolutional Neural Network (Faster R-CNN). The Faster R-CNN consists of a proposal generating network for determining regions of interest (ROI), a regression network for positioning anchor boxes around targets, and an object classification algorithm; it is developed and optimized for natural images. Our ongoing research will develop analogous tools for heatmap images of radar data. In this regard, we will generate a large, representative adaptive radar signal processing database for training and testing, analogous in spirit to the COCO dataset for natural images. As a preliminary example, we present a regression network in this paper for estimating target locations to demonstrate the feasibility of and significant improvements provided by our data-driven approach.      
### 42.Noise-robust voice conversion with domain adversarial training  [ :arrow_down: ](https://arxiv.org/pdf/2201.10693.pdf)
>  Voice conversion has made great progress in the past few years under the studio-quality test scenario in terms of speech quality and speaker similarity. However, in real applications, test speech from source speaker or target speaker can be corrupted by various environment noises, which seriously degrade the speech quality and speaker similarity. In this paper, we propose a novel encoder-decoder based noise-robust voice conversion framework, which consists of a speaker encoder, a content encoder, a decoder, and two domain adversarial neural networks. Specifically, we integrate disentangling speaker and content representation technique with domain adversarial training technique. Domain adversarial training makes speaker representations and content representations extracted by speaker encoder and content encoder from clean speech and noisy speech in the same space, respectively. In this way, the learned speaker and content representations are noise-invariant. Therefore, the two noise-invariant representations can be taken as input by the decoder to predict the clean converted spectrum. The experimental results demonstrate that our proposed method can synthesize clean converted speech under noisy test scenarios, where the source speech and target speech can be corrupted by seen or unseen noise types during the training process. Additionally, both speech quality and speaker similarity are improved.      
### 43.OPTILOD: Optimal Beacon Placement for High-Accuracy Indoor Localization of Drones  [ :arrow_down: ](https://arxiv.org/pdf/2201.10691.pdf)
>  For many applications, drones are required to operate entirely or partially autonomously. To fly completely or partially on their own, drones need access to location services to get navigation commands. While using the Global Positioning System (GPS) is an obvious choice, GPS is not always available, can be spoofed or jammed, and is highly error-prone for indoor and underground environments. The ranging method using beacons is one of the popular methods for localization, specially for indoor environments. In general, localization error in this class is due to two factors: the ranging error and the error induced by the relative geometry between the beacons and the target object to localize. This paper proposes OPTILOD (Optimal Beacon Placement for High-Accuracy Indoor Localization of Drones), an optimization algorithm for the optimal placement of beacons deployed in three-dimensional indoor environments. OPTILOD leverages advances in Evolutionary Algorithms to compute the minimum number of beacons and their optimal placement to minimize the localization error. These problems belong to the Mixed Integer Programming (MIP) class and are both considered NP-Hard. Despite that, OPTILOD can provide multiple optimal beacon configurations that minimize the localization error and the number of deployed beacons concurrently and time efficiently.      
### 44.Virtual Adversarial Training for Semi-supervised Breast Mass Classification  [ :arrow_down: ](https://arxiv.org/pdf/2201.10675.pdf)
>  This study aims to develop a novel computer-aided diagnosis (CAD) scheme for mammographic breast mass classification using semi-supervised learning. Although supervised deep learning has achieved huge success across various medical image analysis tasks, its success relies on large amounts of high-quality annotations, which can be challenging to acquire in practice. To overcome this limitation, we propose employing a semi-supervised method, i.e., virtual adversarial training (VAT), to leverage and learn useful information underlying in unlabeled data for better classification of breast masses. Accordingly, our VAT-based models have two types of losses, namely supervised and virtual adversarial losses. The former loss acts as in supervised classification, while the latter loss aims at enhancing model robustness against virtual adversarial perturbation, thus improving model generalizability. To evaluate the performance of our VAT-based CAD scheme, we retrospectively assembled a total of 1024 breast mass images, with equal number of benign and malignant masses. A large CNN and a small CNN were used in this investigation, and both were trained with and without the adversarial loss. When the labeled ratios were 40% and 80%, VAT-based CNNs delivered the highest classification accuracy of 0.740 and 0.760, respectively. The experimental results suggest that the VAT-based CAD scheme can effectively utilize meaningful knowledge from unlabeled data to better classify mammographic breast mass images.      
### 45.Exploiting Hybrid Models of Tensor-Train Networks for Spoken Command Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.10609.pdf)
>  This work aims to design a low complexity spoken command recognition (SCR) system by considering different trade-offs between the number of model parameters and classification accuracy. More specifically, we exploit a deep hybrid architecture of a tensor-train (TT) network to build an end-to-end SRC pipeline. Our command recognition system, namely CNN+(TT-DNN), is composed of convolutional layers at the bottom for spectral feature extraction and TT layers at the top for command classification. Compared with a traditional end-to-end CNN baseline for SCR, our proposed CNN+(TT-DNN) model replaces fully connected (FC) layers with TT ones and it can substantially reduce the number of model parameters while maintaining the baseline performance of the CNN model. We initialize the CNN+(TT-DNN) model in a randomized manner or based on a well-trained CNN+DNN, and assess the CNN+(TT-DNN) models on the Google Speech Command Dataset. Our experimental results show that the proposed CNN+(TT-DNN) model attains a competitive accuracy of 96.31% with 4 times fewer model parameters than the CNN model. Furthermore, the CNN+(TT-DNN) model can obtain a 97.2% accuracy when the number of parameters is increased.      
