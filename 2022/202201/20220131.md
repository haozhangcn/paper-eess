# ArXiv eess --Mon, 31 Jan 2022
### 1.Quantitative Resilience of Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.12278.pdf)
>  Actuator malfunctions may have disastrous consequences for systems not designed to mitigate them. We focus on the loss of control authority over actuators, where some actuators are uncontrolled but remain fully capable. To counteract the undesirable outputs of these malfunctioning actuators, we use real-time measurements and redundant actuators. In this setting, a system that can still reach its target is deemed resilient. To quantify the resilience of a system, we compare the shortest time for the undamaged system to reach the target with the worst-case shortest time for the malfunctioning system to reach the same target, i.e., when the malfunction makes that time the longest. Contrary to prior work on driftless linear systems, the absence of analytical expression for time-optimal controls of general linear systems prevents an exact calculation of quantitative resilience. Instead, relying on Lyapunov theory we derive analytical bounds on the nominal and malfunctioning reach times in order to bound quantitative resilience. We illustrate our work on a temperature control system.      
### 2.On-Demand AoI Minimization in Resource-Constrained Cache-Enabled IoT Networks with Energy Harvesting Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2201.12277.pdf)
>  We consider a resource-constrained IoT network, where multiple users make on-demand requests to a cache-enabled edge node to send status updates about various random processes, each monitored by an energy harvesting sensor. The edge node serves users' requests by deciding whether to command the corresponding sensor to send a fresh status update or retrieve the most recently received measurement from the cache. Our objective is to find the best actions of the edge node to minimize the average age of information (AoI) of the received measurements upon request, i.e., average on-demand AoI, subject to per-slot transmission and energy constraints. First, we derive a Markov decision process model and propose an iterative algorithm that obtains an optimal policy. Then, we develop an asymptotically optimal low-complexity algorithm -- termed relax-then-truncate -- and prove that it is optimal as the number of sensors goes to infinity. Simulation results illustrate that the proposed relax-then-truncate approach significantly reduces the average on-demand AoI compared to a request-aware greedy (myopic) policy and also depict that it performs close to the optimal solution even for moderate numbers of sensors.      
### 3.A Review on Deep-Learning Algorithms for Fetal Ultrasound-Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2201.12260.pdf)
>  Deep-learning (DL) algorithms are becoming the standard for processing ultrasound (US) fetal images. Despite a large number of survey papers already present in this field, most of them are focusing on a broader area of medical-image analysis or not covering all fetal US DL applications. This paper surveys the most recent work in the field, with a total of 145 research papers published after 2017. Each paper is analyzed and commented on from both the methodology and application perspective. We categorized the papers in (i) fetal standard-plane detection, (ii) anatomical-structure analysis, and (iii) biometry parameter estimation. For each category, main limitations and open issues are presented. Summary tables are included to facilitate the comparison among the different approaches. Publicly-available datasets and performance metrics commonly used to assess algorithm performance are summarized, too. This paper ends with a critical summary of the current state of the art on DL algorithms for fetal US image analysis and a discussion on current challenges that have to be tackled by researchers working in the field to translate the research methodology into the actual clinical practice.      
### 4.Carotid artery wall segmentation in ultrasound image sequences using a deep convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2201.12152.pdf)
>  The objective of this study is the segmentation of the intima-media complex of the common carotid artery, on longitudinal ultrasound images, to measure its thickness. We propose a fully automatic region-based segmentation method, involving a supervised region-based deep-learning approach based on a dilated U-net network. It was trained and evaluated using a 5-fold cross-validation on a multicenter database composed of 2176 images annotated by two experts. The resulting mean absolute difference (&lt;120 um) compared to reference annotations was less than the inter-observer variability (180 um). With a 98.7% success rate, i.e., only 1.3% cases requiring manual correction, the proposed method has been shown to be robust and thus may be recommended for use in clinical practice.      
### 5.Energy-Efficient UAV-Sensor Data Harvesting: Dynamic Adaptive Modulation and Height Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.12142.pdf)
>  Leveraging unmanned aerial vehicle (UAV) is convenient to collect data from ground sensor. However, in the presence of unknown urban environment, the data collection is subject to the blockage of urban buildings. In this paper, considering the urban environment during flight, we propose dynamic adaptive modulation and height control for UAV-sensor data harvesting in urban areas. In each time slot, the modulation format and flight height are selected based on current system states, with the aim of minimizing the expected transmission energy of sensor under data volume and flight height constraints. The dynamic adaptive modulation and height control problem is formulated as constrained finite-horizon Markov decision processes (CMDP), which can be solved by backward induction algorithm. The advantage of proposed joint design over modulation selection only is illustrated via the computer simulations, where 48.23% expected transmission energy can be saved for ground sensor.      
### 6.Wireless Information and Power Transfer: A Bottom-Up Cross-Layer Design Framework  [ :arrow_down: ](https://arxiv.org/pdf/2201.12120.pdf)
>  The efficiency of wireless information and power transfer (WIPT) systems requires an essential reevaluation and rethinking of the entire transceiver chain, which is characterized by a bottom-up cross-layer design approach. In this paper, we introduce and describe the key design layers: i) "Mathematical modeling", associated with the investigation of mathematical models for the wireless power transfer process, ii) "Information-theoretic limits", which refers to the fundamental limits of the WIPT channel, iii) "Link design", corresponding to signal processing techniques that make WIPT feasible, iv) "System-level perspective", which studies the developed WIPT techniques from a macroscopic system-level point-of-view, and v) "Experimental studies", that refers to real-world implementation of WIPT systems. These layers are well-connected and their interplay is imperative for the effective design of WIPT systems. Specific case studies are discussed, which demonstrates the interdisciplinary nature of the aforementioned cross-layer design framework.      
### 7.Outage performance analysis of RIS-assisted UAV wireless systems under disorientation and misalignment  [ :arrow_down: ](https://arxiv.org/pdf/2201.12056.pdf)
>  In this paper, we analyze the performance of a reconfigurable intelligent surface (RIS)-assisted unmanned aerial vehicle (UAV) wireless system that is affected by mixture-gamma small-scale fading, stochastic disorientation, and misalignment, as well as transceivers hardware imperfections. First, we statistically characterize the end-to-end channel for both cases, i.e., in the absence as well as in the presence of disorientation and misalignment, by extracting closed-form formulas for the probability density function (PDF) and the cumulative distribution function (CDF). Building on the aforementioned expressions, we extract novel closed-form expressions for the outage probability (OP) in the absence and the presence of disorientation and misalignment as well as hardware imperfections. In addition, high signal-to-noise ratio OP approximations are derived, leading to the extraction of the diversity order. Finally, an OP floor due to disorientation and misalignment is presented.      
### 8.A DNN Based Post-Filter to Enhance the Quality of Coded Speech in MDCT Domain  [ :arrow_down: ](https://arxiv.org/pdf/2201.12039.pdf)
>  Frequency domain processing, and in particular the use of Modified Discrete Cosine Transform (MDCT), is the most widespread approach to audio coding. However, at low bitrates, audio quality, especially for speech, degrades drastically due to the lack of available bits to directly code the transform coefficients. Traditionally, post-filtering has been used to mitigate artefacts in the coded speech by exploiting a-priori information of the source and extra transmitted parameters. Recently, data-driven post-filters have shown better results, but at the cost of significant additional complexity and delay. In this work, we propose a mask-based post-filter operating directly in MDCT domain of the codec, inducing no extra delay. The real-valued mask is applied to the quantized MDCT coefficients and is estimated from a relatively lightweight convolutional encoder-decoder network. Our solution is tested on the recently standardized low-delay, low-complexity codec (LC3) at lowest possible bitrate of 16 kbps. Objective and subjective assessments clearly show the advantage of this approach over the conventional post-filter, with an average improvement of 10 MUSHRA points over the LC3 coded speech.      
### 9.System-Level Analysis of Joint Sensing and Communication based on 5G New Radio  [ :arrow_down: ](https://arxiv.org/pdf/2201.12017.pdf)
>  This work investigates a multibeam system for joint sensing and communication (JSC) based on multiple-input multiple-output (MIMO) 5G new radio (NR) waveforms. In particular, we consider a base station (BS) acting as a monostatic sensor that estimates the range, speed, and direction of arrival (DoA) of multiple targets via beam scanning using a fraction of the transmitted power. The target position is then obtained via range and DoA estimation. We derive the sensing performance in terms of probability of detection and root mean squared error (RMSE) of position and velocity estimation of a target under line-of-sight (LOS) conditions. Furthermore, we evaluate the system performance when multiple targets are present, using the optimal sub-pattern assignment (OSPA) metric. Finally, we provide an in-depth investigation of the dominant factors that affect performance, including the fraction of power reserved for sensing.      
### 10.Image Superresolution using Scale-Recurrent Dense Network  [ :arrow_down: ](https://arxiv.org/pdf/2201.11998.pdf)
>  Recent advances in the design of convolutional neural network (CNN) have yielded significant improvements in the performance of image super-resolution (SR). The boost in performance can be attributed to the presence of residual or dense connections within the intermediate layers of these networks. The efficient combination of such connections can reduce the number of parameters drastically while maintaining the restoration quality. In this paper, we propose a scale recurrent SR architecture built upon units containing series of dense connections within a residual block (Residual Dense Blocks (RDBs)) that allow extraction of abundant local features from the image. Our scale recurrent design delivers competitive performance for higher scale factors while being parametrically more efficient as compared to current state-of-the-art approaches. To further improve the performance of our network, we employ multiple residual connections in intermediate layers (referred to as Multi-Residual Dense Blocks), which improves gradient propagation in existing layers. Recent works have discovered that conventional loss functions can guide a network to produce results which have high PSNRs but are perceptually inferior. We mitigate this issue by utilizing a Generative Adversarial Network (GAN) based framework and deep feature (VGG) losses to train our network. We experimentally demonstrate that different weighted combinations of the VGG loss and the adversarial loss enable our network outputs to traverse along the perception-distortion curve. The proposed networks perform favorably against existing methods, both perceptually and objectively (PSNR-based) with fewer parameters.      
### 11.Deep Networks for Image and Video Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2201.11996.pdf)
>  Efficiency of gradient propagation in intermediate layers of convolutional neural networks is of key importance for super-resolution task. To this end, we propose a deep architecture for single image super-resolution (SISR), which is built using efficient convolutional units we refer to as mixed-dense connection blocks (MDCB). The design of MDCB combines the strengths of both residual and dense connection strategies, while overcoming their limitations. To enable super-resolution for multiple factors, we propose a scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. This leads to improved performance and promotes parametric efficiency for higher factors. We train two versions of our network to enhance complementary image qualities using different loss configurations. We further employ our network for video super-resolution task, where our network learns to aggregate information from multiple frames and maintain spatio-temporal consistency. The proposed networks lead to qualitative and quantitative improvements over state-of-the-art techniques on image and video super-resolution benchmarks.      
### 12.Computer-aided Recognition and Assessment of a Porous Bioelastomer on Ultrasound Images for Regenerative Medicine Applications  [ :arrow_down: ](https://arxiv.org/pdf/2201.11987.pdf)
>  Biodegradable elastic scaffolds have attracted more and more attention in the field of soft tissue repair and tissue engineering. These scaffolds made of porous bioelastomers support tissue ingrowth along with their own degradation. It is necessary to develop a computer-aided analyzing method based on ultrasound images to identify the degradation performance of the scaffold, not only to obviate the need to do destructive testing, but also to monitor the scaffold's degradation and tissue ingrowth over time. It is difficult using a single traditional image processing algorithm to extract continuous and accurate contour of a porous bioelastomer. This paper proposes a joint algorithm for the bioelastomer's contour detection and a texture feature extraction method for monitoring the degradation behavior of the bioelastomer. Mean-shift clustering method is used to obtain the bioelastomer's and native tissue's clustering feature information. Then the OTSU image binarization method automatically selects the optimal threshold value to convert the grayscale ultrasound image into a binary image. The Canny edge detector is used to extract the complete bioelastomer's contour. The first-order and second-order statistical features of texture are extracted. The proposed joint algorithm not only achieves the ideal extraction of the bioelastomer's contours in ultrasound images, but also gives valuable feedback of the degradation behavior of the bioelastomer at the implant site based on the changes of texture characteristics and contour area. The preliminary results of this study suggest that the proposed computer-aided image processing techniques have values and potentials in the non-invasive analysis of tissue scaffolds in vivo based on ultrasound images and may help tissue engineers evaluate the tissue scaffold's degradation and cellular ingrowth progress and improve the scaffold designs.      
### 13.Inertial Navigation Using an Inertial Sensor Array  [ :arrow_down: ](https://arxiv.org/pdf/2201.11983.pdf)
>  We present a comprehensive framework for fusing measurements from multiple and generally placed accelerometers and gyroscopes to perform inertial navigation. Using the angular acceleration provided by the accelerometer array, we show that the numerical integration of the orientation can be done with second-order accuracy, which is more accurate compared to the traditional first-order accuracy that can be achieved when only using the gyroscopes. Since orientation errors are the most significant error source in inertial navigation, improving the orientation estimation reduces the overall navigation error. The practical performance benefit depends on prior knowledge of the inertial sensor array, and therefore we present four different state-space models using different underlying assumptions regarding the orientation modeling. The models are evaluated using a Lie Group Extended Kalman filter through simulations and real-world experiments. We also show how individual accelerometer biases are unobservable and can be replaced by a six-dimensional bias term whose dimension is fixed and independent of the number of accelerometers.      
### 14.DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs  [ :arrow_down: ](https://arxiv.org/pdf/2201.11972.pdf)
>  Denoising diffusion probabilistic models (DDPMs) are expressive generative models that have been used to solve a variety of speech synthesis problems. However, because of their high sampling costs, DDPMs are difficult to use in real-time speech processing applications. In this paper, we introduce DiffGAN-TTS, a novel DDPM-based text-to-speech (TTS) model achieving high-fidelity and efficient speech synthesis. DiffGAN-TTS is based on denoising diffusion generative adversarial networks (GANs), which adopt an adversarially-trained expressive model to approximate the denoising distribution. We show with multi-speaker TTS experiments that DiffGAN-TTS can generate high-fidelity speech samples within only 4 denoising steps. We present an active shallow diffusion mechanism to further speed up inference. A two-stage training scheme is proposed, with a basic TTS acoustic model trained at stage one providing valuable prior information for a DDPM trained at stage two. Our experiments show that DiffGAN-TTS can achieve high synthesis performance with only 1 denoising step.      
### 15.Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2201.11957.pdf)
>  Global and local relational reasoning enable scene understanding models to perform human-like scene analysis and understanding. Scene understanding enables better semantic segmentation and object-to-object interaction detection. In the medical domain, a robust surgical scene understanding model allows the automation of surgical skill evaluation, real-time monitoring of surgeon's performance and post-surgical analysis. This paper introduces a globally-reasoned multi-task surgical scene understanding model capable of performing instrument segmentation and tool-tissue interaction detection. Here, we incorporate global relational reasoning in the latent interaction space and introduce multi-scale local (neighborhood) reasoning in the coordinate space to improve segmentation. Utilizing the multi-task model setup, the performance of the visual-semantic graph attention network in interaction detection is further enhanced through global reasoning. The global interaction space features from the segmentation module are introduced into the graph network, allowing it to detect interactions based on both node-to-node and global interaction reasoning. Our model reduces the computation cost compared to running two independent single-task models by sharing common modules, which is indispensable for practical applications. Using a sequential optimization technique, the proposed multi-task model outperforms other state-of-the-art single-task models on the MICCAI endoscopic vision challenge 2018 dataset. Additionally, we also observe the performance of the multi-task model when trained using the knowledge distillation technique. The official code implementation is made available in GitHub.      
### 16.Data-Driven Modeling of Aggregate Flexibility under Uncertain and Non-Convex Load Models  [ :arrow_down: ](https://arxiv.org/pdf/2201.11952.pdf)
>  Bundling a large number of distributed energy resources through a load aggregator has been advocated as an effective means to integrate such resources into whole-sale energy markets. To ease market clearing, system operators allow aggregators to submit bidding models of simple prespecified polytopic shapes. Aggregators need to carefully design and commit to a polytope that best captures their energy flexibility along a day-ahead scheduling horizon. This work puts forth a model-informed data-based optimal flexibility design for aggregators, which deals with the time-coupled, uncertain, and non-convex models of individual loads. The proposed solution first generates efficiently a labeled dataset of (non)-disaggregatable schedules. The feasible set of the aggregator is then approximated by an ellipsoid upon training a convex quadratic classifier using the labeled dataset. The ellipsoid is subsequently inner approximated by a polytope. Using Farkas lemma, the obtained polytope is finally inner approximated by the polytopic shape dictated by the market. Numerical tests show the effectiveness of the proposed flexibility design framework for designing the feasible sets of small- and large-sized aggregators coordinating solar photovoltaics, thermostatically-controlled loads, batteries, and electric vehicles. The tests further demonstrate that it is crucial for the aggregator to consider time-coupling and uncertainties in optimal flexibility design.      
### 17.Coupled power generators require stability buffers in addition to inertia  [ :arrow_down: ](https://arxiv.org/pdf/2201.11910.pdf)
>  Increasing the inertia is widely considered to be the solution to resolving unstable interactions between coupled oscillators. In power grids, Virtual Synchronous Generators (VSGs) are proposed to compensate the reducing inertia as rotating synchronous generators are being phased out. Yet, modeling how VSGs and rotating generators simultaneously contribute energy and inertia, we surprisingly find that instabilities of a small-signal nature could arise despite fairly high system inertia. Importantly, we show there exist both an optimal and a maximum number of such VSGs that can be safely supported, a previously unknown result directly useful for power utilities in long-term planning and prosumer contracting. Meanwhile, to resolve instabilities in the short term, we argue that the new market should include another commodity that we call stability storage, whereby -- analogous to energy storage buffering energy imbalances -- VSGs act as decentralized stability buffers. While demonstrating the effectiveness of this concept for a wide range of energy futures, we provide policymakers and utilities with a roadmap towards achieving a 100% renewable grid.      
### 18.Photonics-Assisted Joint Communication-Radar System Based on a QPSK-Sliced Linearly Frequency-Modulated Signal  [ :arrow_down: ](https://arxiv.org/pdf/2201.11908.pdf)
>  A photonics-assisted joint communication-radar system is proposed and experimentally demonstrated, by introducing a quadrature phase-shift keying (QPSK)-sliced linearly frequency-modulated (LFM) signal. An LFM signal is carrier-suppressed single-sideband modulated onto the optical carrier in one dual-parallel Mach-Zehnder modulator (DPMZM) of a dual-polarization dual-parallel Mach-Zehnder modulator (DPol-DPMZM). The other DPMZM of the DPol-DPMZM is biased as an IQ modulator to implement QPSK modulation on the optical carrier. The polarization orthogonal optical signals from the DPol-DPMZM are further combined and detected in a photodetector to generate the QPSK-sliced LFM signal, which is used to realize efficient data transmission and high-performance radar functions including ranging and imaging. An experiment is carried out. Radar range detection with an error of less than 4 cm, ISAR imaging with a resolution of 14.99 cm*3.25 cm, and communication with a data rate of 105.26 Mbit/s are successfully verified.      
### 19.Calibrating Histopathology Image Classifiers using Label Smoothing  [ :arrow_down: ](https://arxiv.org/pdf/2201.11866.pdf)
>  The classification of histopathology images fundamentally differs from traditional image classification tasks because histopathology images naturally exhibit a range of diagnostic features, resulting in a diverse range of annotator agreement levels. However, examples with high annotator disagreement are often either assigned the majority label or discarded entirely when training histopathology image classifiers. This widespread practice often yields classifiers that do not account for example difficulty and exhibit poor model calibration. In this paper, we ask: can we improve model calibration by endowing histopathology image classifiers with inductive biases about example difficulty? <br>We propose several label smoothing methods that utilize per-image annotator agreement. Though our methods are simple, we find that they substantially improve model calibration, while maintaining (or even improving) accuracy. For colorectal polyp classification, a common yet challenging task in gastrointestinal pathology, we find that our proposed agreement-aware label smoothing methods reduce calibration error by almost 70%. Moreover, we find that using model confidence as a proxy for annotator agreement also improves calibration and accuracy, suggesting that datasets without multiple annotators can still benefit from our proposed label smoothing methods via our proposed confidence-aware label smoothing methods. <br>Given the importance of calibration (especially in histopathology image analysis), the improvements from our proposed techniques merit further exploration and potential implementation in other histopathology image classification tasks.      
### 20.Classification of White Blood Cell Leukemia with Low Number of Interpretable and Explainable Features  [ :arrow_down: ](https://arxiv.org/pdf/2201.11864.pdf)
>  White Blood Cell (WBC) Leukaemia is detected through image-based classification. Convolutional Neural Networks are used to learn the features needed to classify images of cells a malignant or healthy. However, this type of model requires learning a large number of parameters and is difficult to interpret and explain. Explainable AI (XAI) attempts to alleviate this issue by providing insights to how models make decisions. Therefore, we present an XAI model which uses only 24 explainable and interpretable features and is highly competitive to other approaches by outperforming them by about 4.38\%. Further, our approach provides insight into which variables are the most important for the classification of the cells. This insight provides evidence that when labs treat the WBCs differently, the importance of various metrics changes substantially. Understanding the important features for classification is vital in medical imaging diagnosis and, by extension, understanding the AI models built in scientific pursuits.      
### 21.Distributed Stochastic Model Predictive Control for Human-Leading Heavy-Duty Truck Platoon  [ :arrow_down: ](https://arxiv.org/pdf/2201.11859.pdf)
>  Human-leading truck platooning systems have been proposed to leverage the benefits of both human supervision and vehicle autonomy. Equipped with human guidance and autonomous technology, human-leading truck platooning systems are more versatile to handle uncertain traffic conditions than fully automated platooning systems. This paper presents a novel distributed stochastic model predictive control (DSMPC) design for a human-leading heavy-duty truck platoon. The proposed DSMPC design integrates the stochastic driver behavior model of the human-driven leader truck with a distributed formation control design for the following automated trucks in the platoon. The driver behavior of the human-driven leader truck is learned by a stochastic inverse reinforcement learning (SIRL) approach. The proposed stochastic driver behavior model aims to learn a distribution of cost function, which represents the richness and uniqueness of human driver behaviors, with a given set of driver-specific demonstrations. The distributed formation control includes a serial DSMPC with guaranteed recursive feasibility, closed-loop chance constraint satisfaction, and string stability. Simulation studies are conducted to investigate the efficacy of the proposed design under several realistic traffic scenarios. Compared to the baseline platoon control strategy (deterministic distributed model predictive control), the proposed DSMPC achieves superior controller performance in constraint violations and spacing errors.      
### 22.Adam-based Augmented Random Search for Control Policies for Distributed Energy Resource Cyber Attack Mitigation  [ :arrow_down: ](https://arxiv.org/pdf/2201.11825.pdf)
>  Volt-VAR and Volt-Watt control functions are mechanisms that are included in distributed energy resource (DER) power electronic inverters to mitigate excessively high or low voltages in distribution systems. In the event that a subset of DER have had their Volt-VAR and Volt-Watt settings compromised as part of a cyber-attack, we propose a mechanism to control the remaining set of non-compromised DER to ameliorate large oscillations in system voltages and large voltage imbalances in real time. To do so, we construct control policies for individual non-compromised DER, directly searching the policy space using an Adam-based augmented random search (ARS). In this paper we show that, compared to previous efforts aimed at training policies for DER cybersecurity using deep reinforcement learning (DRL), the proposed approach is able to learn optimal (and sometimes linear) policies an order of magnitude faster than conventional DRL techniques (e.g., Proximal Policy Optimization).      
### 23.Graph-based Algorithm Unfolding for Energy-aware Power Allocation in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.11799.pdf)
>  We develop a novel graph-based trainable framework to maximize the weighted sum energy efficiency (WSEE) for power allocation in wireless communication networks. To address the non-convex nature of the problem, the proposed method consists of modular structures inspired by a classical iterative suboptimal approach and enhanced with learnable components. More precisely, we propose a deep unfolding of the successive concave approximation (SCA) method. In our unfolded SCA (USCA) framework, the originally preset parameters are now learnable via graph convolutional neural networks (GCNs) that directly exploit multi-user channel state information as the underlying graph adjacency matrix. We show the permutation equivariance of the proposed architecture, which promotes generalizability across different network topologies of varying size, density, and channel distribution. The USCA framework is trained through a stochastic gradient descent approach using a progressive training strategy. The unsupervised loss is carefully devised to feature the monotonic property of the objective under maximum power constraints. Comprehensive numerical results demonstrate outstanding performance and robustness of USCA over state-of-the-art benchmarks.      
### 24.The iCanClean Algorithm: How to Remove Artifacts using Reference Noise Recordings  [ :arrow_down: ](https://arxiv.org/pdf/2201.11798.pdf)
>  Data recordings are often corrupted by noise, and it can be difficult to isolate clean data of interest. For example, mobile electroencephalography is commonly corrupted by motion artifact, which limits its use in real-world settings. Here, we describe a novel noise-canceling algorithm that uses canonical correlation analysis to find and remove subspaces of corrupted data recordings that are most strongly correlated with subspaces of reference noise recordings. The algorithm, termed iCanClean, is computationally efficient, which may be useful for real-time applications, such as brain computer interfaces. In future work, we will quantify the algorithm's performance and compare it with alternative cleaning methods.      
### 25.Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder  [ :arrow_down: ](https://arxiv.org/pdf/2201.11795.pdf)
>  Recent advances in deep learning have led to superhuman performance across a variety of applications. Recently, these methods have been successfully employed to improve the rate-distortion performance in the task of image compression. However, current methods either use additional post-processing blocks on the decoder end to improve compression or propose an end-to-end compression scheme based on heuristics. For the majority of these, the trained deep neural networks (DNNs) are not compatible with standard encoders and would be difficult to deply on personal computers and cellphones. In light of this, we propose a system that learns to improve the encoding performance by enhancing its internal neural representations on both the encoder and decoder ends, an approach we call Neural JPEG. We propose frequency domain pre-editing and post-editing methods to optimize the distribution of the DCT coefficients at both encoder and decoder ends in order to improve the standard compression (JPEG) method. Moreover, we design and integrate a scheme for jointly learning quantization tables within this hybrid neural compression framework.Experiments demonstrate that our approach successfully improves the rate-distortion performance over JPEG across various quality metrics, such as PSNR and MS-SSIM, and generates visually appealing images with better color retention quality.      
### 26.Denoising Diffusion Restoration Models  [ :arrow_down: ](https://arxiv.org/pdf/2201.11793.pdf)
>  Many interesting tasks in image restoration can be cast as linear inverse problems. A recent family of approaches for solving these problems uses stochastic algorithms that sample from the posterior distribution of natural images given the measurements. However, efficient solutions often require problem-specific supervised training to model the posterior, whereas unsupervised methods that are not problem-specific typically rely on inefficient iterative methods. This work addresses these issues by introducing Denoising Diffusion Restoration Models (DDRM), an efficient, unsupervised posterior sampling method. Motivated by variational inference, DDRM takes advantage of a pre-trained denoising diffusion generative model for solving any linear inverse problem. We demonstrate DDRM's versatility on several image datasets for super-resolution, deblurring, inpainting, and colorization under various amounts of measurement noise. DDRM outperforms the current leading unsupervised methods on the diverse ImageNet dataset in reconstruction quality, perceptual quality, and runtime, being 5x faster than the nearest competitor. DDRM also generalizes well for natural images out of the distribution of the observed ImageNet training set.      
### 27.Low-Cost Inertial Aiding for Deep-Urban Tightly-Coupled Multi-Antenna Precise GNSS  [ :arrow_down: ](https://arxiv.org/pdf/2201.11776.pdf)
>  A vehicular pose estimation technique is presented that tightly couples multi-antenna carrier-phase differential GNSS (CDGNSS) with a low-cost MEMS inertial sensor and vehicle dynamics constraints. This work is the first to explore the use of consumer-grade inertial sensors for tightly-coupled urban CDGNSS, and first to explore the tightly-coupled combination of multi-antenna CDGNSS and inertial sensing (of any quality) for urban navigation. An unscented linearization permits ambiguity resolution using traditional integer least squares while both implicitly enforcing known-baseline-length constraints and exploiting the multi-baseline problem's inter-baseline correlations. A novel false fix detection and recovery technique is developed to mitigate the effect of conditioning the filter state on incorrect integers. When evaluated on the publicly-available TEX-CUP urban positioning dataset, the proposed technique achieves, with consumer- and industrial-grade inertial sensors, respectively, a 96.6% and 97.5% integer fix availability, and 12.0 cm and 10.1 cm overall (fix and float) 95th percentile horizontal positioning error.      
### 28.Design Optimization of a Three-Phase Transformer Using Finite Element Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2201.11769.pdf)
>  Optimization of design parameters of a transformer is a crucial task to increase efficiency and lower the material cost. This research presents an approach to model a three-phase transformer and optimize design parameters to minimize the volume and loss. ANSYS Maxwell 2D is used to model the transformer and analyze it for different design parameters. The multi-objective differential evolution algorithm is used to find optimum design parameters that minimize the volume and loss. In this paper, we present the optimum design parameters for a 1 kVA transformer with a particular input and output voltage specification. The transformer with these optimum design parameters is then tested for different loading conditions and power factor values. The results show that the maximum efficiency is obtained for 75% loading condition with unity power factor. As the power factor decreases, the efficiency decreases as well.      
### 29.Communication Cost of Two-Database Symmetric Private Information Retrieval: A Conditional Disclosure of Multiple Secrets Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2201.12327.pdf)
>  We consider the total (upload plus download) communication cost of two-database symmetric private information retrieval (SPIR) through its relationship to conditional disclosure of secrets (CDS). In SPIR, a user wishes to retrieve a message out of $K$ messages from $N$ non-colluding and replicated databases without learning anything beyond the retrieved message, while no individual database learns the retrieved message index. In CDS, two parties each holding an individual input and sharing a common secret wish to disclose this secret to an external party in an efficient manner if and only if their inputs satisfy a public deterministic function. As a natural extension of CDS, we introduce conditional disclosure of multiple secrets (CDMS) where two parties share multiple i.i.d.~common secrets rather than a single common secret as in CDS. We show that a special configuration of CDMS is equivalent to two-database SPIR. Inspired by this equivalence, we design download cost efficient SPIR schemes using bipartite graph representation of CDS and CDMS, and determine the exact minimum total communication cost of $N=2$ database SPIR for $K=3$ messages.      
### 30.VRT: A Video Restoration Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2201.12288.pdf)
>  Video restoration (e.g., video super-resolution) aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which either is restricted by frame-by-frame restoration or lacks long-range modelling ability. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction and long-range temporal dependency modelling abilities. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal mutual self attention (TMSA) and parallel warping. TMSA divides the video into small clips, on which mutual attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on three tasks, including video super-resolution, video deblurring and video denoising, demonstrate that VRT outperforms the state-of-the-art methods by large margins ($\textbf{up to 2.16dB}$) on nine benchmark datasets.      
### 31.Joint Differentiable Optimization and Verification for Certified Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.12243.pdf)
>  In model-based reinforcement learning for safety-critical control systems, it is important to formally certify system properties (e.g., safety, stability) under the learned controller. However, as existing methods typically apply formal verification \emph{after} the controller has been learned, it is sometimes difficult to obtain any certificate, even after many iterations between learning and verification. To address this challenge, we propose a framework that jointly conducts reinforcement learning and formal verification by formulating and solving a novel bilevel optimization problem, which is differentiable by the gradients from the value function and certificates. Experiments on a variety of examples demonstrate the significant advantages of our framework over the model-based stochastic value gradient (SVG) method and the model-free proximal policy optimization (PPO) method in finding feasible controllers with barrier functions and Lyapunov functions that ensure system safety and stability.      
### 32.Differential Polarization Shift Keying Through Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2201.12226.pdf)
>  We propose a novel reconfigurable intelligent surface (RIS)-aided differential polarization shift keying modulation scheme for a line-of-sight environment. In this scheme, the RIS exploits the state of polarization (SoP) of the reflected waves over two successive reflection frames to encode the data bit. In particular, the RIS either preserves the SoP of the reflected wave similar to the previous reflection frame or switches it to another orthogonal SoP as a function of the information data bits. The proposed scheme allows non-coherent data detection without the need for polarization mismatch estimation and compensation processes at the receiver.      
### 33.Learning Stationary Nash Equilibrium Policies in $n$-Player Stochastic Games with Independent Chains via Dual Mirror Descent  [ :arrow_down: ](https://arxiv.org/pdf/2201.12224.pdf)
>  We consider a subclass of $n$-player stochastic games, in which players have their own internal state/action spaces while they are coupled through their payoff functions. It is assumed that players' internal chains are driven by independent transition probabilities. Moreover, players can only receive realizations of their payoffs but not the actual functions, nor can they observe each others' states/actions. Under some assumptions on the structure of the payoff functions, we develop efficient learning algorithms based on Dual Averaging and Dual Mirror Descent, which provably converge almost surely or in expectation to the set of $\epsilon$-Nash equilibrium policies. In particular, we derive upper bounds on the number of iterates that scale polynomially in terms of the game parameters to achieve an $\epsilon$-Nash equilibrium policy. Besides Markov potential games and linear-quadratic stochastic games, this work provides another interesting subclass of $n$-player stochastic games that under some assumption provably admit polynomial-time learning algorithm for finding their $\epsilon$-Nash equilibrium policies.      
### 34.Star Temporal Classification: Sequence Classification with Partially Labeled Data  [ :arrow_down: ](https://arxiv.org/pdf/2201.12208.pdf)
>  We develop an algorithm which can learn from partially labeled and unsegmented sequential data. Most sequential loss functions, such as Connectionist Temporal Classification (CTC), break down when many labels are missing. We address this problem with Star Temporal Classification (STC) which uses a special star token to allow alignments which include all possible tokens whenever a token could be missing. We express STC as the composition of weighted finite-state transducers (WFSTs) and use GTN (a framework for automatic differentiation with WFSTs) to compute gradients. We perform extensive experiments on automatic speech recognition. These experiments show that STC can recover most of the performance of supervised baseline when up to 70% of the labels are missing. We also perform experiments in handwriting recognition to show that our method easily applies to other sequence classification tasks.      
### 35.A tomographic workflow to enable deep learning for X-ray based foreign object detection  [ :arrow_down: ](https://arxiv.org/pdf/2201.12184.pdf)
>  Detection of unwanted (`foreign') objects within products is a common procedure in many branches of industry for maintaining production quality. X-ray imaging is a fast, non-invasive and widely applicable method for foreign object detection. Deep learning has recently emerged as a powerful approach for recognizing patterns in radiographs (i.e., X-ray images), enabling automated X-ray based foreign object detection. However, these methods require a large number of training examples and manual annotation of these examples is a subjective and laborious task. In this work, we propose a Computed Tomography (CT) based method for producing training data for supervised learning of foreign object detection, with minimal labour requirements. In our approach, a few representative objects are CT scanned and reconstructed in 3D. The radiographs that have been acquired as part of the CT-scan data serve as input for the machine learning method. High-quality ground truth locations of the foreign objects are obtained through accurate 3D reconstructions and segmentations. Using these segmented volumes, corresponding 2D segmentations are obtained by creating virtual projections. We outline the benefits of objectively and reproducibly generating training data in this way compared to conventional radiograph annotation. In addition, we show how the accuracy depends on the number of objects used for the CT reconstructions. The results show that in this workflow generally only a relatively small number of representative objects (i.e., fewer than 10) are needed to achieve adequate detection performance in an industrial setting. Moreover, for real experimental data we show that the workflow leads to higher foreign object detection accuracies than with standard radiograph annotation.      
### 36.Robotic Tissue Sampling for Safe Post-mortem Biopsy in Infectious Corpses  [ :arrow_down: ](https://arxiv.org/pdf/2201.12168.pdf)
>  In pathology and legal medicine, the histopathological and microbiological analysis of tissue samples from infected deceased is a valuable information for developing treatment strategies during a pandemic such as COVID-19. However, a conventional autopsy carries the risk of disease transmission and may be rejected by relatives. We propose minimally invasive biopsy with robot assistance under CT guidance to minimize the risk of disease transmission during tissue sampling and to improve accuracy. A flexible robotic system for biopsy sampling is presented, which is applied to human corpses placed inside protective body bags. An automatic planning and decision system estimates optimal insertion point. Heat maps projected onto the segmented skin visualize the distance and angle of insertions and estimate the minimum cost of a puncture while avoiding bone collisions. Further, we test multiple insertion paths concerning feasibility and collisions. A custom end effector is designed for inserting needles and extracting tissue samples under robotic guidance. Our robotic post-mortem biopsy (RPMB) system is evaluated in a study during the COVID-19 pandemic on 20 corpses and 10 tissue targets, 5 of them being infected with SARS-CoV-2. The mean planning time including robot path planning is (5.72+-1.67) s. Mean needle placement accuracy is (7.19+-4.22) mm.      
### 37.Stochastic optimization in digital pre-distortion of the signal  [ :arrow_down: ](https://arxiv.org/pdf/2201.12159.pdf)
>  In this paper, we test the performance of some modern stochastic optimization methods and practices in application to digital pre-distortion problem, that is a valuable part of processing signal on base stations providing wireless communication. In first part of our study, we focus on search of the best performing method and its proper modifications. In the second part, we proposed the new, quasi-online, testing framework that allows us to fit our modelling results with the behaviour of real-life DPD prototype, retested some selected of practices considered in previous section and approved the advantages of the method occured to be the best in real-life conditions. For the used model, maximum achieved improvement in depth was 7% in standard regime and 5% in online one (metric itself is of logarithmic scale). We also achieved a halving of the working time preserving 3% and 6% improvement in depth for the standard and online regime, correspondingly. All comparisons are made to the Adam method, which was highlighted as the best stochastic method for DPD problem in paper [Pasechnyuk et al., 2021], and to the Adamax method, that is the best in the proposed online regime.      
### 38.Reducing language context confusion for end-to-end code-switching automatic speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.12155.pdf)
>  Code-switching is about dealing with alternative languages in the communication process. Training end-to-end (E2E) automatic speech recognition (ASR) systems for code-switching is known to be a challenging problem because of the lack of data compounded by the increased language context confusion due to the presence of more than one language. In this paper, we propose a language-related attention mechanism to reduce multilingual context confusion for the E2E code-switching ASR model based on the Equivalence Constraint Theory (EC). The linguistic theory requires that any monolingual fragment that occurs in the code-switching sentence must occur in one of the monolingual sentences. It establishes a bridge between monolingual data and code-switching data. By calculating the respective attention of multiple languages, our method can efficiently transfer language knowledge from rich monolingual data. We evaluate our method on ASRU 2019 Mandarin-English code-switching challenge dataset. Compared with the baseline model, the proposed method achieves 11.37% relative mix error rate reduction.      
### 39.Sampling Theorems for Learning from Incomplete Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2201.12151.pdf)
>  In many real-world settings, only incomplete measurement data are available which can pose a problem for learning. Unsupervised learning of the signal model using a fixed incomplete measurement process is impossible in general, as there is no information in the nullspace of the measurement operator. This limitation can be overcome by using measurements from multiple operators. While this idea has been successfully applied in various applications, a precise characterization of the conditions for learning is still lacking. In this paper, we fill this gap by presenting necessary and sufficient conditions for learning the signal model which indicate the interplay between the number of distinct measurement operators $G$, the number of measurements per operator $m$, the dimension of the model $k$ and the dimension of the signals $n$. In particular, we show that generically unsupervised learning is possible if each operator obtains at least $m&gt;k+n/G$ measurements. Our results are agnostic of the learning algorithm and have implications in a wide range of practical algorithms, from low-rank matrix recovery to deep neural networks.      
### 40.Improving End-to-End Models for Set Prediction in Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2201.12105.pdf)
>  The goal of spoken language understanding (SLU) systems is to determine the meaning of the input speech signal, unlike speech recognition which aims to produce verbatim transcripts. Advances in end-to-end (E2E) speech modeling have made it possible to train solely on semantic entities, which are far cheaper to collect than verbatim transcripts. We focus on this set prediction problem, where entity order is unspecified. Using two classes of E2E models, RNN transducers and attention based encoder-decoders, we show that these models work best when the training entity sequence is arranged in spoken order. To improve E2E SLU models when entity spoken order is unknown, we propose a novel data augmentation technique along with an implicit attention based alignment method to infer the spoken order. F1 scores significantly increased by more than 11% for RNN-T and about 2% for attention based encoder-decoder SLU models, outperforming previously reported results.      
### 41.On feedforward control using physics-guided neural networks: Training cost regularization and optimized initialization  [ :arrow_down: ](https://arxiv.org/pdf/2201.12088.pdf)
>  Performance of model-based feedforward controllers is typically limited by the accuracy of the inverse system dynamics model. Physics-guided neural networks (PGNN), where a known physical model cooperates in parallel with a neural network, were recently proposed as a method to achieve high accuracy of the identified inverse dynamics. However, the flexible nature of neural networks can create overparameterization when employed in parallel with a physical model, which results in a parameter drift during training. This drift may result in parameters of the physical model not corresponding to their physical values, which increases vulnerability of the PGNN to operating conditions not present in the training data. To address this problem, this paper proposes a regularization method via identified physical parameters, in combination with an optimized training initialization that improves training convergence. The regularized PGNN framework is validated on a real-life industrial linear motor, where it delivers better tracking accuracy and extrapolation.      
### 42.Dual Learning Music Composition and Dance Choreography  [ :arrow_down: ](https://arxiv.org/pdf/2201.11999.pdf)
>  Music and dance have always co-existed as pillars of human activities, contributing immensely to the cultural, social, and entertainment functions in virtually all societies. Notwithstanding the gradual systematization of music and dance into two independent disciplines, their intimate connection is undeniable and one art-form often appears incomplete without the other. Recent research works have studied generative models for dance sequences conditioned on music. The dual task of composing music for given dances, however, has been largely overlooked. In this paper, we propose a novel extension, where we jointly model both tasks in a dual learning approach. To leverage the duality of the two modalities, we introduce an optimal transport objective to align feature embeddings, as well as a cycle consistency loss to foster overall consistency. Experimental results demonstrate that our dual learning framework improves individual task performance, delivering generated music compositions and dance choreographs that are realistic and faithful to the conditioned inputs.      
### 43.The need for and feasibility of alternative ground robots to traverse sandy and rocky extraterrestrial terrain  [ :arrow_down: ](https://arxiv.org/pdf/2201.11984.pdf)
>  Robotic spacecraft have helped expand our reach for many planetary exploration missions. Most ground mobile planetary exploration robots use wheeled or modified wheeled platforms. Although extraordinarily successful at completing intended mission goals, because of the limitations of wheeled locomotion, they have been largely limited to benign, solid terrain and avoided extreme terrain with loose soil/sand and large rocks. Unfortunately, such challenging terrain is often scientifically interesting for planetary geology. Although many animals traverse such terrain at ease, robots have not matched their performance and robustness. This is in major part due to a lack of fundamental understanding of how effective locomotion can be generated from controlled interaction with complex terrain on the same level of flight aerodynamics and underwater vehicle hydrodynamics. Early fundamental understanding of legged and limbless locomotor-ground interaction has already enabled stable and efficient bio-inspired robot locomotion on relatively flat ground with small obstacles. Recent progress in the new field of terradynamics of locomotor-terrain interaction begins to reveal the principles of bio-inspired locomotion on loose soil/sand and over large obstacles. Multi-legged and limbless platforms using terradynamics insights hold the promise for serving as robust alternative platforms for traversing extreme extraterrestrial terrain and expanding our reach in planetary exploration.      
### 44.Generalized Visual Quality Assessment of GAN-Generated Face Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.11975.pdf)
>  Recent years have witnessed the dramatically increased interest in face generation with generative adversarial networks (GANs). A number of successful GAN algorithms have been developed to produce vivid face images towards different application scenarios. However, little work has been dedicated to automatic quality assessment of such GAN-generated face images (GFIs), even less have been devoted to generalized and robust quality assessment of GFIs generated with unseen GAN model. Herein, we make the first attempt to study the subjective and objective quality towards generalized quality assessment of GFIs. More specifically, we establish a large-scale database consisting of GFIs from four GAN algorithms, the pseudo labels from image quality assessment (IQA) measures, as well as the human opinion scores via subjective testing. Subsequently, we develop a quality assessment model that is able to deliver accurate quality predictions for GFIs from both available and unseen GAN algorithms based on meta-learning. In particular, to learn shared knowledge from GFIs pairs that are born of limited GAN algorithms, we develop the convolutional block attention (CBA) and facial attributes-based analysis (ABA) modules, ensuring that the learned knowledge tends to be consistent with human visual perception. Extensive experiments exhibit that the proposed model achieves better performance compared with the state-of-the-art IQA models, and is capable of retaining the effectiveness when evaluating GFIs from the unseen GAN algorithms.      
### 45.Infrastructure-Based Object Detection and Tracking for Cooperative Driving Automation: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2201.11871.pdf)
>  Object detection plays a fundamental role in enabling Cooperative Driving Automation (CDA), which is regarded as the revolutionary solution to addressing safety, mobility, and sustainability issues of contemporary transportation systems. Although current computer vision technologies could provide satisfactory object detection results in occlusion-free scenarios, the perception performance of onboard sensors could be inevitably limited by the range and occlusion. Owing to flexible position and pose for sensor installation, infrastructure-based detection and tracking systems can enhance the perception capability for connected vehicles and thus quickly become one of the most popular research topics. In this paper, we review the research progress for infrastructure-based object detection and tracking systems. Architectures of roadside perception systems based on different types of sensors are reviewed to show a high-level description of the workflows for infrastructure-based perception systems. Roadside sensors and different perception methodologies are reviewed and analyzed with detailed literature to provide a low-level explanation for specific methods followed by Datasets and Simulators to draw an overall landscape of infrastructure-based object detection and tracking methods. Discussions are conducted to point out current opportunities, open problems, and anticipated future trends.      
### 46.Neural-FST Class Language Model for End-to-End Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.11867.pdf)
>  We propose Neural-FST Class Language Model (NFCLM) for end-to-end speech recognition, a novel method that combines neural network language models (NNLMs) and finite state transducers (FSTs) in a mathematically consistent framework. Our method utilizes a background NNLM which models generic background text together with a collection of domain-specific entities modeled as individual FSTs. Each output token is generated by a mixture of these components; the mixture weights are estimated with a separately trained neural decider. We show that NFCLM significantly outperforms NNLM by 15.8% relative in terms of Word Error Rate. NFCLM achieves similar performance as traditional NNLM and FST shallow fusion while being less prone to overbiasing and 12 times more compact, making it more suitable for on-device usage.      
### 47.Decentralized Fictitious Play Converges Near a Nash Equilibrium in Near-Potential Games  [ :arrow_down: ](https://arxiv.org/pdf/2201.11854.pdf)
>  We investigate convergence of decentralized fictitious play (DFP) in near-potential games, wherein agents preferences can almost be captured by a potential function. In DFP agents keep local estimates of other agents' empirical frequencies, best-respond against these estimates, and receive information over a time-varying communication network. We prove that empirical frequencies of actions generated by DFP converge around a single Nash Equilibrium (NE) assuming that there are only finitely many Nash equilibria, and the difference in utility functions resulting from unilateral deviations is close enough to the difference in the potential function values. This result assures that DFP has the same convergence properties of standard Fictitious play (FP) in near-potential games.      
### 48.The Western Australian Optical Ground Station  [ :arrow_down: ](https://arxiv.org/pdf/2201.11846.pdf)
>  Free-space communications at optical wavelengths offers the potential for orders-of-magnitude improvement in data rates over conventional radio wavelengths, and this will be needed to meet the demand of future space-to-ground applications. Supporting this new paradigm necessitates a global network of optical ground stations. This paper describes the architecture and commissioning of the Western Australian Optical Ground Station, to be installed on the roof of the physics building at the University of Western Australia. This ground station will incorporate amplitude- and phase-stabilisation technology, previously demonstrated over horizontal free-space links, into the ground station's optical telescope. Trialling this advanced amplitude- and phase-stabilisation technology, the ground station will overcome turbulence-induced noise to establish stable, coherent free-space links between ground-to-air and ground-to-space. These links will enable significant advances in high-speed and quantum-secured communications; positioning, navigation, and timing; and fundamental physics.      
### 49.Sentiment-Aware Automatic Speech Recognition pre-training for enhanced Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.11826.pdf)
>  We propose a novel multi-task pre-training method for Speech Emotion Recognition (SER). We pre-train SER model simultaneously on Automatic Speech Recognition (ASR) and sentiment classification tasks to make the acoustic ASR model more ``emotion aware''. We generate targets for the sentiment classification using text-to-sentiment model trained on publicly available data. Finally, we fine-tune the acoustic ASR on emotion annotated speech data. We evaluated the proposed approach on the MSP-Podcast dataset, where we achieved the best reported concordance correlation coefficient (CCC) of 0.41 for valence prediction.      
### 50.Exploring Graph Representation of Chorales  [ :arrow_down: ](https://arxiv.org/pdf/2201.11745.pdf)
>  This work explores areas overlapping music, graph theory, and machine learning. An embedding representation of a node, in a weighted undirected graph $\mathcal{G}$, is a representation that captures the meaning of nodes in an embedding space. In this work, 383 Bach chorales were compiled and represented as a graph. Two application cases were investigated in this paper (i) learning node embedding representation using \emph{Continuous Bag of Words (CBOW), skip-gram}, and \emph{node2vec} algorithms, and (ii) learning node labels from neighboring nodes based on a collective classification approach. The results of this exploratory study ascertains many salient features of the graph-based representation approach applicable to music applications.      
