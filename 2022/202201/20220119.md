# ArXiv eess --Wed, 19 Jan 2022
### 1.Sensitivity analysis of a mean-value exergy-based internal combustion engine model  [ :arrow_down: ](https://arxiv.org/pdf/2201.07190.pdf)
>  In this work, we conduct a sensitivity analysis of the mean-value internal combustion engine exergy-based model, recently developed by the authors, with respect to different driving cycles, ambient temperatures, and exhaust gas recirculation rates. Such an analysis allows to assess how driving conditions and environment affect the exergetic behavior of the engine, providing insights on the system's inefficiency. Specifically, the work is carried out for a military series hybrid electric vehicle.      
### 2.Reconfigurable Control of a Class of Multicopters  [ :arrow_down: ](https://arxiv.org/pdf/2201.07121.pdf)
>  In this paper, a reconfigurable control scheme for a generalized class of multicopters is presented to overcome single or multiple rotor failures. In a multicopter, the rotor failure heavily affects its dynamics and thus its controllability. Limited controllability leads to limited or no reconfigurability. \emph{Available Control Authority Index} has been recently developed \cite{Du2015} as a measure of controllability of multicopters. In this work, the notion of \emph{Available Reduced-Control Authority Index} (ArCAI) is introduced, which shows that in some uncontrollable failures it is still possible to control reduced set of states. Based on this notion, a reconfigurable control scheme is presented, which comprises a \emph{Nonlinear Dynamic Inversion} based baseline control law, a constrained control allocation scheme along with some modifications to incorporate reconfiguration, and a simplified fault detection and isolation technique. Simulation results for a commonly used configuration of Hexacopter are presented in the presence of single and multiple rotor failures.      
### 3.Co-Pulsing FDA Radar  [ :arrow_down: ](https://arxiv.org/pdf/2201.07107.pdf)
>  Target localization based on frequency diverse array (FDA) radar has lately garnered significant research interest. A linear frequency offset (FO) across FDA antennas yields a range-angle dependent beampattern that allows for joint estimation of range and direction-of-arrival (DoA). Prior works on FDA largely focus on the one-dimensional linear array to estimate only azimuth angle and range while ignoring the elevation and Doppler velocity. However, in many applications, the latter two parameters are also essential for target localization. Further, there is also an interest in radar systems that employ fewer measurements in temporal, Doppler, or spatial signal domains. We address these multiple challenges by proposing a co-prime L-Shaped FDA, wherein co-prime FOs are applied across the elements of L-shaped co-prime array and each element transmits at a non-uniform co-prime pulse repetition interval (C$^3$ or C-Cube). This co-pulsing FDA yields significantly large degrees-of-freedom (DoFs) for target localization in the range-azimuth-elevation-Doppler domain while also reducing the time-on-target and transmit spectral usage. By exploiting these DoFs, we develop C-Cube auto-pairing (CCing) algorithm, in which all the parameters are ipso facto paired during a joint estimation. We show that C-Cube FDA requires at least $2\sqrt{Q+1}-1$ antenna elements and $2\sqrt{Q+1}-1$ pulses to guarantee perfect recovery of $Q$ targets as against $Q+1$ elements and $Q+1$ pulses required by both L-shaped uniform linear array and L-shaped linear FO FDA with uniform pulsing. Numerical experiments with our CCing algorithm show great performance improvements in parameter recovery, wherein C-Cube radar achieves at least $15\%$ higher target hit-rate with shorter dwell time than its uniform counterparts.      
### 4.Joint denoising and HDR for RAW video sequences  [ :arrow_down: ](https://arxiv.org/pdf/2201.07066.pdf)
>  We propose a patch-based method for the simultaneous denoising and fusion of a sequence of RAW multi-exposed images. A spatio-temporal criterion is used to select similar patches along the sequence, and a weighted principal component analysis permits to both denoise and fuse the multi exposed data. The overall strategy permits to denoise and fuse the set of images without the need of recovering each denoised image in the multi-exposure set, leading to a very efficient procedure. Several experiments show that the proposed method permits to obtain state-of-the-art fusion results with real RAW data.      
### 5.AI for Closed-Loop Control Systems --- New Opportunities for Modeling, Designing, and Tuning Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.06961.pdf)
>  Control Systems, particularly closed-loop control systems (CLCS), are frequently used in production machines, vehicles, and robots nowadays. CLCS are needed to actively align actual values of a process to a given reference or set values in real-time with a very high precession. Yet, artificial intelligence (AI) is not used to model, design, optimize, and tune CLCS. This paper will highlight potential AI-empowered and -based control system designs and designing procedures, gathering new opportunities and research direction in the field of control system engineering. Therefore, this paper illustrates which building blocks within the standard block diagram of CLCS can be replaced by AI, i.e., artificial neuronal networks (ANN). Having processes with real-time contains and functional safety in mind, it is discussed if AI-based controller blocks can cope with these demands. By concluding the paper, the pros and cons of AI-empowered as well as -based CLCS designs are discussed, and possible research directions for introducing AI in the domain of control system engineering are given.      
### 6.Sampling color and geometry point clouds from ShapeNet dataset  [ :arrow_down: ](https://arxiv.org/pdf/2201.06935.pdf)
>  The popularisation of acquisition devices capable of capturing volumetric information such as LiDAR scans and depth cameras has lead to an increased interest in point clouds as an imaging modality. Due to the high amount of data needed for their representation, efficient compression solutions are needed to enable practical applications. Among the many techniques that have been proposed in the last years, learning-based methods are receiving large attention due to their high performance and potential for improvement. Such algorithms depend on large and diverse training sets to achieve good compression performance. ShapeNet is a large-scale dataset composed of CAD models with texture and constitute and effective option for training such compression methods. This dataset is entirely composed of meshes, which must go through a sampling process in order to obtain point clouds with geometry and texture information. Although many existing software libraries are able to sample geometry from meshes through simple functions, obtaining an output point cloud with geometry and color of the external faces of the mesh models is not a straightforward process for the ShapeNet dataset. The main difficulty associated with this dataset is that its models are often defined with duplicated faces sharing the same vertices, but with different color values. This document describes a script for sampling the meshes from ShapeNet that circumvent this issue by excluding the internal faces of the mesh models prior to the sampling. The script can be accessed from the following link: <a class="link-external link-https" href="https://github.com/mmspg/mesh-sampling" rel="external noopener nofollow">this https URL</a>.      
### 7.Deep Equilibrium Models for Video Snapshot Compressive Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2201.06931.pdf)
>  The ability of snapshot compressive imaging (SCI) systems to efficiently capture high-dimensional (HD) data has led to an inverse problem, which consists of recovering the HD signal from the compressed and noisy measurement. While reconstruction algorithms grow fast to solve it with the recent advances of deep learning, the fundamental issue of accurate and stable recovery remains. To this end, we propose deep equilibrium models (DEQ) for video SCI, fusing data-driven regularization and stable convergence in a theoretically sound manner. Each equilibrium model implicitly learns a nonexpansive operator and analytically computes the fixed point, thus enabling unlimited iterative steps and infinite network depth with only a constant memory requirement in training and testing. Specifically, we demonstrate how DEQ can be applied to two existing models for video SCI reconstruction: recurrent neural networks (RNN) and Plug-and-Play (PnP) algorithms. On a variety of datasets and real data, both quantitative and qualitative evaluations of our results demonstrate the effectiveness and stability of our proposed method. The code and models will be released to the public.      
### 8.A Study on the Ambiguity in Human Annotation of German Oral History Interviews for Perceived Emotion Recognition and Sentiment Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2201.06868.pdf)
>  For research in audiovisual interview archives often it is not only of interest what is said but also how. Sentiment analysis and emotion recognition can help capture, categorize and make these different facets searchable. In particular, for oral history archives, such indexing technologies can be of great interest. These technologies can help understand the role of emotions in historical remembering. However, humans often perceive sentiments and emotions ambiguously and subjectively. Moreover, oral history interviews have multi-layered levels of complex, sometimes contradictory, sometimes very subtle facets of emotions. Therefore, the question arises of the chance machines and humans have capturing and assigning these into predefined categories. This paper investigates the ambiguity in human perception of emotions and sentiment in German oral history interviews and the impact on machine learning systems. Our experiments reveal substantial differences in human perception for different emotions. Furthermore, we report from ongoing machine learning experiments with different modalities. We show that the human perceptual ambiguity and other challenges, such as class imbalance and lack of training data, currently limit the opportunities of these technologies for oral history archives. Nonetheless, our work uncovers promising observations and possibilities for further research.      
### 9.Human and Automatic Speech Recognition Performance on German Oral History Interviews  [ :arrow_down: ](https://arxiv.org/pdf/2201.06841.pdf)
>  Automatic speech recognition systems have accomplished remarkable improvements in transcription accuracy in recent years. On some domains, models now achieve near-human performance. However, transcription performance on oral history has not yet reached human accuracy. In the present work, we investigate how large this gap between human and machine transcription still is. For this purpose, we analyze and compare transcriptions of three humans on a new oral history data set. We estimate a human word error rate of 8.7% for recent German oral history interviews with clean acoustic conditions. For comparison with recent machine transcription accuracy, we present experiments on the adaptation of an acoustic model achieving near-human performance on broadcast speech. We investigate the influence of different adaptation data on robustness and generalization for clean and noisy oral history interviews. We optimize our acoustic models by 5 to 8% relative for this task and achieve 23.9% WER on noisy and 15.6% word error rate on clean oral history interviews.      
### 10.Sensor Scheduling Design for Complex Networks under a Distributed State Estimation Framework  [ :arrow_down: ](https://arxiv.org/pdf/2201.06819.pdf)
>  This paper investigates sensor scheduling for state estimation of complex networks over shared transmission channels. For a complex network of dynamical systems, referred to as nodes, a sensor network is adopted to measure and estimate the system states in a distributed way, where a sensor is used to measure a node. The estimates are transmitted from sensors to the associated nodes, in the presence of one-step time delay and subject to packet loss. Due to limited transmission capability, only a portion of sensors are allowed to send information at each time step. The goal of this paper is to seek an optimal sensor scheduling policy minimizing the overall estimation errors. Under a distributed state estimation framework, this problem is reformulated as a Markov decision process, where the one-stage reward for each node is strongly coupled. The feasibility of the problem reformulation is ensured. In addition, an easy-to-check condition is established to guarantee the existence of an optimal deterministic and stationary policy. Moreover, it is found that the optimal policies have a threshold, which can be used to reduce the computational complexity in obtaining these policies. Finally, the effectiveness of the theoretical results is illustrated by several simulation examples.      
### 11.Visual Sensor Network Stimulation Model Identification via Gaussian Mixture Model and Deep Embedded Features  [ :arrow_down: ](https://arxiv.org/pdf/2201.06804.pdf)
>  Visual sensor networks constitute a fundamental class of distributed sensing systems, with unique complexity and performance research subjects. One of these novel challenges is represented by the identification of the network stimulation model (SM), which emerges when a set of detectable events trigger different subsets of the cameras. In this direction, the formulation of the related SM identification problem is proposed, along with a proper network observations generation method. Consequently, an approach based on deep embedded features and soft clustering is leveraged to solve the presented identification problem. In detail, the Gaussian Mixture Modeling is employed to provide a suitable description for data distribution and an autoencoder is used to reduce undesired effects due to the so-called curse of dimensionality. Hence, it is shown that a SM can be learnt by solving Maximum A-Posteriori estimation on the encoded features belonging to a space with lower dimensionality. Lastly, numerical results are reported to validate the devised estimation algorithm.      
### 12.Data-Driven Deep Learning Based Hybrid Beamforming for Aerial Massive MIMO-OFDM Systems with Implicit CSI  [ :arrow_down: ](https://arxiv.org/pdf/2201.06778.pdf)
>  In an aerial hybrid massive multiple-input multiple-output (MIMO) and orthogonal frequency division multiplexing (OFDM) system, how to design a spectral-efficient broadband multi-user hybrid beamforming with a limited pilot and feedback overhead is challenging. To this end, by modeling the key transmission modules as an end-to-end (E2E) neural network, this paper proposes a data-driven deep learning (DL)-based unified hybrid beamforming framework for both the time division duplex (TDD) and frequency division duplex (FDD) systems with implicit channel state information (CSI). For TDD systems, the proposed DL-based approach jointly models the uplink pilot combining and downlink hybrid beamforming modules as an E2E neural network. While for FDD systems, we jointly model the downlink pilot transmission, uplink CSI feedback, and downlink hybrid beamforming modules as an E2E neural network. Different from conventional approaches separately processing different modules, the proposed solution simultaneously optimizes all modules with the sum rate as the optimization object. Therefore, by perceiving the inherent property of air-to-ground massive MIMO-OFDM channel samples, the DL-based E2E neural network can establish the mapping function from the channel to the beamformer, so that the explicit channel reconstruction can be avoided with reduced pilot and feedback overhead. Besides, practical low-resolution phase shifters (PSs) introduce the quantization constraint, leading to the intractable gradient backpropagation when training the neural network. To mitigate the performance loss caused by the phase quantization error, we adopt the transfer learning strategy to further fine-tune the E2E neural network based on a pre-trained network that assumes the ideal infinite-resolution PSs. Numerical results show that our DL-based schemes have considerable advantages over state-of-the-art schemes.      
### 13.AI Augmented Digital Metal Component  [ :arrow_down: ](https://arxiv.org/pdf/2201.06735.pdf)
>  The aim of this work is to propose a new paradigm that imparts intelligence to metal parts with the fusion of metal additive manufacturing and artificial intelligence (AI). Our digital metal part classifies the status with real time data processing with convolutional neural network (CNN). The training data for the CNN is collected from a strain gauge embedded in metal parts by laser powder bed fusion process. We implement this approach using additive manufacturing, demonstrate a self-cognitive metal part recognizing partial screw loosening, malfunctioning, and external impacting object. The results indicate that metal part can recognize subtle change of multiple fixation state under repetitive compression with 89.1% accuracy with test sets. The proposed strategy showed promising potential in contributing to the hyper-connectivity for next generation of digital metal based mechanical systems      
### 14.Distributed $H_{\infty}$ Edge Weight Synthesis for Cooperative Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.06730.pdf)
>  This paper studies distributed edge weight synthesis of a cooperative system for a fixed topology to improve $H_{\infty}$ performance, considering that disturbances are injected at interconnection channels. This problem is cast into a linear matrix inequality problem by replacing original cooperative system with an equivalent ideal cooperative system. Derivations of the method relies on dissipative system framework. Proposed method provides an upper bound for the induced $\mathcal{L}_{2}$ norm of the original lumped cooperative system while reducing the computation time. A comparison for computation time illustrates the advantage of the proposed method against the lumped counterpart.      
### 15.Sensitivity of Single-Pulse Radar Detection to Aircraft Pose Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2201.06727.pdf)
>  Mission planners for aircraft that operate in radar detection environments are often concerned the probability of detection. The probability of detection is a nonlinear function of the aircraft pose and radar position. Current path planning techniques for this application assume that the aircraft pose is deterministic. In practice, however, the aircraft pose is estimated using a navigation filter and therefore contains uncertainty. The uncertainty in the aircraft pose induces uncertainty in the probability of detection, but this phenomenon is generally not considered when path planning. This paper provides a method for combining aircraft pose uncertainty with single-pulse radar detection models to aid mission planning efforts. The method linearizes the expression for the probability of detection and three radar cross section models. The linearized models are then used to determine the variability of the probability of detection induced by uncertainty in the aircraft pose. The results of this paper validate the linearization using Monte Carlo analysis and explore the sensitivity of the probability of detection to aircraft pose uncertainty.      
### 16.How Bad Are Artifacts?: Analyzing the Impact of Speech Enhancement Errors on ASR  [ :arrow_down: ](https://arxiv.org/pdf/2201.06685.pdf)
>  It is challenging to improve automatic speech recognition (ASR) performance in noisy conditions with single-channel speech enhancement (SE). In this paper, we investigate the causes of ASR performance degradation by decomposing the SE errors using orthogonal projection-based decomposition (OPD). OPD decomposes the SE errors into noise and artifact components. The artifact component is defined as the SE error signal that cannot be represented as a linear combination of speech and noise sources. We propose manually scaling the error components to analyze their impact on ASR. We experimentally identify the artifact component as the main cause of performance degradation, and we find that mitigating the artifact can greatly improve ASR performance. Furthermore, we demonstrate that the simple observation adding (OA) technique (i.e., adding a scaled version of the observed signal to the enhanced speech) can monotonically increase the signal-to-artifact ratio under a mild condition. Accordingly, we experimentally confirm that OA improves ASR performance for both simulated and real recordings. The findings of this paper provide a better understanding of the influence of SE errors on ASR and open the door to future research on novel approaches for designing effective single-channel SE front-ends for ASR.      
### 17.Limited Information Shared Control, a Potential Game Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.06651.pdf)
>  This paper presents a systematic method for the design of a limited information shared controller (LISC). LISC is used in applications where not all system states or references trajectories are measurable by the automation. Typical examples are partially human controlled systems, in which some subsystems are fully controlled by the automation, while others are controlled by a human. The proposed systematic design uses a novel class of games to model human-machine interaction: The near potential differential games (NPDG). We provide a necessary and sufficient condition for the existence of an NPDG and derive an algorithm for finding a NPDG which completely describes a given differential game. The proposed design method is applied to the control of a large vehicle manipulator system, in which the manipulator is controlled by the human operator and the vehicle is fully automated. The suitability of the NPDG modelling differential games is verified in simulations leading to a faster and more accurate controller design compared to manual tuning. Furthermore, the overall design process is validated in a study with sixteen test subjects indicating the applicability of the proposed concept in real applications.      
### 18.Rate Splitting in FDD Massive MIMO SystemsBased on the Second Order Statisticsof Transmission Channels  [ :arrow_down: ](https://arxiv.org/pdf/2201.06624.pdf)
>  In this work, we present new results for the application of rate splitting multiple access (RSMA) to the downlink (DL) of a massive multiple-input-multiple-output (MaMIMO) system operating in frequency-division-duplex (FDD) mode. Due to the lack of uplink (UL) - DL channel reciprocity in such systems, explicit training in the DL has to be performed in order for the base station (BS) to gain knowledge about the single-antenna users' channels. This is ensured through a feedback link from the users to the BS. Dealing with the DL of a massive MIMO system implies that acquiring the DL channel state information (CSI) comes at the cost of a huge training overhead, which scales with the number of BS antennas. We, therefore, limit the resources allocated to training by reusing the pilot sequences among the BS antennas which results in a contaminated channel observation. Despite this incomplete channel knowledge at the transmitter side, the proposed RS approach combined with a statistical precoder relying on the channels' second-order information achieves excellent results in terms of spectral efficiency compared to the state-of-art techniques. This is demonstrated via Monte-Carlo simulations of typical communication scenarios.      
### 19.Minimax Multi-Agent Persistent Monitoring of a Network System  [ :arrow_down: ](https://arxiv.org/pdf/2201.06607.pdf)
>  We investigate the problem of optimally observing a finite set of targets using a mobile agent over an infinite time horizon. The agent is tasked to move in a network-constrained structure to gather information so as to minimize the worst-case uncertainty about the internal states of the targets. To do this, the agent has to decide its sequence of target-visits and the corresponding dwell-times at each visited target. For a given visiting sequence, we prove that in an optimal dwelling time allocation the peak uncertainty is the same among all the targets. This allows us to formulate the optimization of dwelling times as a resource allocation problem and to solve it using a novel efficient algorithm. Next, we optimize the visiting sequence using a greedy exploration process, using heuristics inspired by others developed in the context of the traveling salesman problem. Numerical results are included to illustrate the contributions.      
### 20.Neural Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2201.06574.pdf)
>  Motion during acquisition of a set of projections can lead to significant motion artifacts in computed tomography reconstructions despite fast acquisition of individual views. In cases such as cardiac imaging, motion may be unavoidable and evaluating motion may be of clinical interest. Reconstructing images with reduced motion artifacts has typically been achieved by developing systems with faster gantry rotation or using algorithms which measure and/or estimate the displacements. However, these approaches have had limited success due to both physical constraints as well as the challenge of estimating/measuring non-rigid, temporally varying, and patient-specific motions. We propose a novel reconstruction framework, NeuralCT, to generate time-resolved images free from motion artifacts. Our approaches utilizes a neural implicit approach and does not require estimation or modeling of the underlying motion. Instead, boundaries are represented using a signed distance metric and neural implicit framework. We utilize `analysis-by-synthesis' to identify a solution consistent with the acquired sinogram as well as spatial and temporal consistency constraints. We illustrate the utility of NeuralCT in three progressively more complex scenarios: translation of a small circle, heartbeat-like change in an ellipse's diameter, and complex topological deformation. Without hyperparameter tuning or change to the architecture, NeuralCT provides high quality image reconstruction for all three motions, as compared to filtered backprojection, using mean-square-error and Dice metrics.      
### 21.Sizing of Energy Storage System for Virtual Inertia Emulation  [ :arrow_down: ](https://arxiv.org/pdf/2201.06566.pdf)
>  The infusion of renewable energy sources into the conventional synchronous generation system decreases the overall system inertia and negatively impacts the stability of its primary frequency response. The lowered inertia is due to the absence of inertia in some of the renewable energy-based systems. To maintain the stability of the system, we need to keep the frequency in the permissible limits and maintain low rotational inertia. Some authors in the literature have used the virtual synchronous generators (VSG) as a solution to this problem. Although the VSG based distributed recourses (DER) exhibits the characteristics and behavior of synchronous generators (SG) such as inertia, frequency droop functions and damping but it does not optimally solve the question of frequency stability. This paper presents a solution for these problems via an empirical model that sizes the Battery Energy Storage System (BESS) required for the inertia emulation and damping control. The tested system consists of a Photovoltaic (PV) based VSG that is connected to a 9-Bus grid and the simulation experiments are carried out using EMTP software. The VSG transient response is initiated by a symmetric fault on the grid side. Our simulations show the battery energy sizing required to emulate the virtual inertia corresponding to several design parameters, i.e., the droop gain, K{\omega}, the droop coefficient, Kd, and the VSG time constant Ta.      
### 22.Cooperative constrained motion coordination of networked heterogeneous vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2201.06399.pdf)
>  We consider the problem of cooperative motion coordination for multiple heterogeneous mobile vehicles subject to various constraints. These include nonholonomic motion constraints, constant speed constraints, holonomic coordination constraints, and equality/inequality geometric constraints. We develop a general framework involving differential-algebraic equations and viability theory to determine coordination feasibility for a coordinated motion control under heterogeneous vehicle dynamics and different types of coordination task constraints. If a coordinated motion solution exists for the derived differential-algebraic equations and/or inequalities, a constructive algorithm is proposed to derive an equivalent dynamical system that generates a set of feasible coordinated motions for each individual vehicle. In case studies on coordinating two vehicles, we derive analytical solutions to motion generation for two-vehicle groups consisting of car-like vehicles, unicycle vehicles, or vehicles with constant speeds, which serve as benchmark coordination tasks for more complex vehicle groups. The motion generation algorithm is well-backed by simulation data for a wide variety of coordination situations involving heterogeneous vehicles. We then extend the vehicle control framework to deal with the cooperative coordination problem with time-varying coordination tasks and leader-follower structure. We show several simulation experiments on multi-vehicle coordination under various constraints to validate the theory and the effectiveness of the proposed schemes.      
### 23.Dual Perceptual Loss for Single Image Super-Resolution Using ESRGAN  [ :arrow_down: ](https://arxiv.org/pdf/2201.06383.pdf)
>  The proposal of perceptual loss solves the problem that per-pixel difference loss function causes the reconstructed image to be overly-smooth, which acquires a significant progress in the field of single image super-resolution reconstruction. Furthermore, the generative adversarial networks (GAN) is applied to the super-resolution field, which effectively improves the visual quality of the reconstructed image. However, under the condtion of high upscaling factors, the excessive abnormal reasoning of the network produces some distorted structures, so that there is a certain deviation between the reconstructed image and the ground-truth image. In order to fundamentally improve the quality of reconstructed images, this paper proposes a effective method called Dual Perceptual Loss (DP Loss), which is used to replace the original perceptual loss to solve the problem of single image super-resolution reconstruction. Due to the complementary property between the VGG features and the ResNet features, the proposed DP Loss considers the advantages of learning two features simultaneously, which significantly improves the reconstruction effect of images. The qualitative and quantitative analysis on benchmark datasets demonstrates the superiority of our proposed method over state-of-the-art super-resolution methods.      
### 24.Few-shot image segmentation for cross-institution male pelvic organs using registration-assisted prototypical learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.06358.pdf)
>  The ability to adapt medical image segmentation networks for a novel class such as an unseen anatomical or pathological structure, when only a few labelled examples of this class are available from local healthcare providers, is sought-after. This potentially addresses two widely recognised limitations in deploying modern deep learning models to clinical practice, expertise-and-labour-intensive labelling and cross-institution generalisation. This work presents the first 3D few-shot interclass segmentation network for medical images, using a labelled multi-institution dataset from prostate cancer patients with eight regions of interest. We propose an image alignment module registering the predicted segmentation of both query and support data, in a standard prototypical learning algorithm, to a reference atlas space. The built-in registration mechanism can effectively utilise the prior knowledge of consistent anatomy between subjects, regardless whether they are from the same institution or not. Experimental results demonstrated that the proposed registration-assisted prototypical learning significantly improved segmentation accuracy (p-values&lt;0.01) on query data from a holdout institution, with varying availability of support data from multiple institutions. We also report the additional benefits of the proposed 3D networks with 75% fewer parameters and an arguably simpler implementation, compared with existing 2D few-shot approaches that segment 2D slices of volumetric medical images.      
### 25.H\&amp;E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin \&amp; Eosin regression  [ :arrow_down: ](https://arxiv.org/pdf/2201.06329.pdf)
>  Computational pathology is a domain that aims to develop algorithms to automatically analyze large digitized histopathology images, called whole slide images (WSI). WSIs are produced scanning thin tissue samples that are stained to make specific structures visible. They show stain colour heterogeneity due to different preparation and scanning settings applied across medical centers. Stain colour heterogeneity is a problem to train convolutional neural networks (CNN), the state-of-the-art algorithms for most computational pathology tasks, since CNNs usually underperform when tested on images including different stain variations than those within data used to train the CNN. Despite several methods that were developed, stain colour heterogeneity is still an unsolved challenge that limits the development of CNNs that can generalize on data from several medical centers. This paper aims to present a novel method to train CNNs that better generalize on data including several colour variations. The method, called H\&amp;E-adversarial CNN, exploits H\&amp;E matrix information to learn stain-invariant features during the training. The method is evaluated on the classification of colon and prostate histopathology images, involving eleven heterogeneous datasets, and compared with five other techniques used to handle stain colour heterogeneity. H\&amp;E-adversarial CNNs show an improvement in performance compared to the other algorithms, demonstrating that it can help to better deal with stain colour heterogeneous images.      
### 26.Wireless Connectivity in the Sub-THz Spectrum: A Path to 6G  [ :arrow_down: ](https://arxiv.org/pdf/2201.06271.pdf)
>  Wireless communication in millimetre wave bands, namely above 20 GHz and up to 300 GHz, is foreseen as a key enabler technology for the next generation of wireless systems. The huge available bandwidth is contemplated to achieve high data-rate wireless communications, and hence, to fulfil the requirements of future wireless networks. In this paper, we discuss and illustrate new paradigms for the sub-THz physical layer, which either aim at maximizing the spectral efficiency, minimizing the device complexity, or finding good tradeoff. The solutions offered by appropriate modulation schemes and multi-antenna systems are assessed based on various potential scenarios.      
### 27.Single-shot blind deconvolution with coded aperture  [ :arrow_down: ](https://arxiv.org/pdf/2201.06267.pdf)
>  In this paper, we present a method for single-shot blind deconvolution incorporating a coded aperture (CA). In this method, we utilize the CA, inserted on the pupil plane, as support constraints in blind deconvolution. Not only an object but also a point spread function of turbulence are estimated from a single captured image by a reconstruction algorithm with the CA support. The proposed method is demonstrated by a simulation and an experiment in which point sources are recovered under severe turbulence.      
### 28.Segmentation of the Carotid Lumen and Vessel Wall using Deep Learning and Location Priors  [ :arrow_down: ](https://arxiv.org/pdf/2201.06259.pdf)
>  In this report we want to present our method and results for the Carotid Artery Vessel Wall Segmentation Challenge. We propose an image-based pipeline utilizing the U-Net architecture and location priors to solve the segmentation problem at hand.      
### 29.Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are?  [ :arrow_down: ](https://arxiv.org/pdf/2201.06251.pdf)
>  Cancer is one of the leading causes of death worldwide, and head and neck (H&amp;N) cancer is amongst the most prevalent types. Positron emission tomography and computed tomography are used to detect and segment the tumor region. Clinically, tumor segmentation is extensively time-consuming and prone to error. Machine learning, and deep learning in particular, can assist to automate this process, yielding results as accurate as the results of a clinician. In this research study, we develop a vision transformers-based method to automatically delineate H&amp;N tumor, and compare its results to leading convolutional neural network (CNN)-based models. We use multi-modal data of CT and PET scans to do this task. We show that the selected transformer-based model can achieve results on a par with CNN-based ones. With cross validation, the model achieves a mean dice similarity coefficient of 0.736, mean precision of 0.766 and mean recall of 0.766. This is only 0.021 less than the 2020 competition winning model in terms of the DSC score. This indicates that the exploration of transformer-based models is a promising research area.      
### 30.Improving Clinical Diagnosis Performance with Automated X-ray Scan Quality Enhancement Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2201.06250.pdf)
>  In clinical diagnosis, diagnostic images that are obtained from the scanning devices serve as preliminary evidence for further investigation in the process of delivering quality healthcare. However, often the medical image may contain fault artifacts, introduced due to noise, blur and faulty equipment. The reason for this may be the low-quality or older scanning devices, the test environment or technicians lack of training etc; however, the net result is that the process of fast and reliable diagnosis is hampered. Resolving these issues automatically can have a significant positive impact in a hospital clinical workflow, where often, there is no other way but to work with faulty/older equipment or inadequately qualified radiology technicians. In this paper, automated image quality improvement approaches for adapted and benchmarked for the task of medical image super-resolution. During experimental evaluation on standard open datasets, the observations showed that certain algorithms perform better and show significant improvement in the diagnostic quality of medical scans, thereby enabling better visualization for human diagnostic purposes.      
### 31.Channel Modeling in RIS-Empowered Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.06241.pdf)
>  One of the most critical aspects of enabling next-generation wireless technologies is developing an accurate and consistent channel model to be validated effectively with the help of real-world measurements. From this point of view, remarkable research has recently been conducted to model propagation channels involving the modification of the wireless propagation environment through the inclusion of reconfigurable intelligent surfaces (RISs). This study mainly aims to present a vision on channel modeling strategies for the RIS-empowered communications systems considering the state-of-the-art channel and propagation modeling efforts in the literature. Moreover, it is also desired to draw attention to open-source and standard-compliant physical channel modeling efforts to provide comprehensive insights regarding the practical use-cases of RISs in future wireless networks.      
### 32.Nonlinear Control Allocation: A Learning Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.06180.pdf)
>  Modern aircraft are designed with redundant control effectors to cater for fault tolerance and maneuverability requirements. This leads to an over-actuated aircraft which requires a control allocation scheme to distribute the control commands among effectors. Traditionally, optimization based control allocation schemes are used; however, for nonlinear allocation problems these methods require large computational resources. In this work, a novel ANN based nonlinear control allocation scheme is proposed. To start, a general nonlinear control allocation problem is posed in a different perspective to seek a function which maps desired moments to control effectors. Few important results on stability and performance of nonlinear allocation schemes in general and this ANN based allocation scheme, in particular, are presented. To demonstrate the efficacy of the proposed scheme, it is compared with standard quadratic programming based method for control allocation.      
### 33.Potential Game Based Decision-Making Frameworks for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2201.06157.pdf)
>  Decision-making for autonomous driving is challenging, considering the complex interactions among multiple traffic agents (e.g., autonomous vehicles (AVs), human drivers, and pedestrians) and the computational load needed to evaluate these interactions. This paper develops two general potential game based frameworks, namely, finite and continuous potential games, for decision-making in autonomous driving. The two frameworks account for the AVs' two types of action spaces, i.e., finite and continuous action spaces, respectively. We show that the developed frameworks provide theoretical guarantees, including 1) existence of pure-strategy Nash equilibria, 2) convergence of the Nash equilibrium (NE) seeking algorithms, and 3) global optimality of the derived NE (in the sense that both self- and team- interests are optimized). In addition, we provide cost function shaping approaches to constructing multi-agent potential games in autonomous driving. Moreover, two solution algorithms, including self-play dynamics (e.g., best response dynamics) and potential function optimization, are developed for each game. The developed frameworks are then applied to two different traffic scenarios, including intersection-crossing and lane-changing in highways. Statistical comparative studies, including 1) finite potential game vs. continuous potential game, and 2) best response dynamics vs. potential function optimization, are conducted to compare the performances of different solution algorithms. It is shown that both developed frameworks are practical (i.e., computationally efficient), reliable (i.e., resulting in satisfying driving performances in diverse scenarios and situations), and robust (i.e., resulting in satisfying driving performances against uncertain behaviors of the surrounding vehicles) for real-time decision-making in autonomous driving.      
### 34.Robust Scatterer Number Density Segmentation of Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.06143.pdf)
>  Quantitative UltraSound (QUS) aims to reveal information about the tissue microstructure using backscattered echo signals from clinical scanners. Among different QUS parameters, scatterer number density is an important property that can affect estimation of other QUS parameters. Scatterer number density can be classified into high or low scatterer densities. If there are more than 10 scatterers inside the resolution cell, the envelope data is considered as Fully Developed Speckle (FDS) and otherwise, as Under Developed Speckle (UDS). In conventional methods, the envelope data is divided into small overlapping windows (a strategy here we refer to as patching), and statistical parameters such as SNR and skewness are employed to classify each patch of envelope data. However, these parameters are system dependent meaning that their distribution can change by the imaging settings and patch size. Therefore, reference phantoms which have known scatterer number density are imaged with the same imaging settings to mitigate system dependency. In this paper, we aim to segment regions of ultrasound data without any patching. A large dataset is generated which has different shapes of scatterer number density and mean scatterer amplitude using a fast simulation method. We employ a convolutional neural network (CNN) for the segmentation task and investigate the effect of domain shift when the network is tested on different datasets with different imaging settings. Nakagami parametric image is employed for the multi-task learning to improve the performance. Furthermore, inspired by the reference phantom methods in QUS, A domain adaptation stage is proposed which requires only two frames of data from FDS and UDS classes. We evaluate our method for different experimental phantoms and in vivo data.      
### 35.Deconvolution in Fluorescence Lifetime imaging microscopy (FLIM)  [ :arrow_down: ](https://arxiv.org/pdf/2201.06136.pdf)
>  Fluorescence lifetime imaging microscopy (FLIM) is an important technique to understand the chemical micro-environment in cells and tissues since it provides additional contrast compared to conventional fluorescence imaging. When two fluorophores within a diffraction limit are excited, the resulting emission leads to non-linear spatial distortion and localization effects in intensity (magnitude) and lifetime (phase) components. To address this issue, in this work, we provide a theoretical model for convolution in FLIM to describe how the resulting behavior differs from conventional fluorescence microscopy. We then present a Richardson-Lucy (RL) based deconvolution including total variation (TV) regularization method to correct for the distortions in FLIM measurements due to optical convolution, and experimentally demonstrate this FLIM deconvolution method on a multi-photon microscopy (MPM)-FLIM images of fluorescent-labeled fixed bovine pulmonary arterial endothelial (BPAE) cells.      
### 36.Multipath-assisted Radio Sensing and Occupancy Detection for Smart In-house Parking in ITS  [ :arrow_down: ](https://arxiv.org/pdf/2201.06128.pdf)
>  Joint, radio-based communication, localization and sensing is a rapidly emerging research field with various application potentials. Greatly benefiting from these capabilities, smart city, mobility, and logistic concepts are key components for maximizing the efficiency of modern transportation systems. In urban environments, both the search for parking space and freight transport are time- and space-consuming and present the bottlenecks for these transportation chains. Providing location information for these heterogeneous requirement profiles (both active and passive localization of objects), can be realized by using retrofittable wireless sensor networks, which are typically only deployed for active localization. An additional passive detection of objects can be achieved by assessing signal reflections and multipath properties of the transmission channel stored within the Channel Impulse Response (CIR). In this work, a proof-of-concept realization and preliminary experimental results of a CIR-based occupancy detection for parking lots are presented. As the time resolution is dependent on available bandwidth, the CIR of Ultra-wideband transceivers are used. For this, the CIR is smoothed and time-variant changes within it are detected by performing a background subtraction. Finally, the reflecting objects are mapped to individual parking lots. The developed method is tested in an in-house parking garage. The work provided is a foundation for passive occupancy detection, whose capabilities can prospectively be enhanced by exploiting additional physical layers, such as 5G or even 6G.      
### 37.Is it Possible to Predict MGMT Promoter Methylation from Brain Tumor MRI Scans using Deep Learning Models?  [ :arrow_down: ](https://arxiv.org/pdf/2201.06086.pdf)
>  Glioblastoma is a common brain malignancy that tends to occur in older adults and is almost always lethal. The effectiveness of chemotherapy, being the standard treatment for most cancer types, can be improved if a particular genetic sequence in the tumor known as MGMT promoter is methylated. However, to identify the state of the MGMT promoter, the conventional approach is to perform a biopsy for genetic analysis, which is time and effort consuming. A couple of recent publications proposed a connection between the MGMT promoter state and the MRI scans of the tumor and hence suggested the use of deep learning models for this purpose. Therefore, in this work, we use one of the most extensive datasets, BraTS 2021, to study the potency of employing deep learning solutions, including 2D and 3D CNN models and vision transformers. After conducting a thorough analysis of the models' performance, we concluded that there seems to be no connection between the MRI scans and the state of the MGMT promoter.      
### 38.Comparison of COVID-19 Prediction Performances of Normalization Methods on Cough Acoustics Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2201.06078.pdf)
>  The disease called the new coronavirus (COVID19) is a new viral respiratory disease that first appeared on January 13, 2020 in Wuhan, China. Some of the symptoms of this disease are fever, cough, shortness of breath and difficulty in breathing. In more serious cases, death may occur as a result of infection. COVID19 emerged as a pandemic that affected the whole world in a little while. The most important issue in the fight against the epidemic is the early diagnosis and follow-up of COVID19 (+) patients. Therefore, in addition to the RT-PCR test, medical imaging methods are also used when identifying COVID 19 (+) patients. In this study, an alternative approach was proposed using cough data, one of the most prominent symptoms of COVID19 (+) patients. The performances of z-normalization and min-max normalization methods were investigated on these data. All features were obtained using discrete wavelet transform method. Support vector machines (SVM) was used as classifier algorithm. The highest performances of accuracy and F1-score were obtained as 100% and 100% using the min-max normalization, respectively. On the other hand, the highest accuracy and highest F1-score performances were obtained as 99.2 % and 99.0 % using the z-normalization, respectively. In light of the results, it is clear that cough acoustic data will contribute significantly to controlling COVID19 cases.      
### 39.Challenges in COVID-19 Chest X-Ray Classification: Problematic Data or Ineffective Approaches?  [ :arrow_down: ](https://arxiv.org/pdf/2201.06052.pdf)
>  The value of quick, accurate, and confident diagnoses cannot be undermined to mitigate the effects of COVID-19 infection, particularly for severe cases. Enormous effort has been put towards developing deep learning methods to classify and detect COVID-19 infections from chest radiography images. However, recently some questions have been raised surrounding the clinical viability and effectiveness of such methods. In this work, we carry out extensive experiments on a large COVID-19 chest X-ray dataset to investigate the challenges faced with creating reliable AI solutions from both the data and machine learning perspectives. Accordingly, we offer an in-depth discussion into the challenges faced by some widely-used deep learning architectures associated with chest X-Ray COVID-19 classification. Finally, we include some possible directions and considerations to improve the performance of the models and the data for use in clinical settings.      
### 40.CISRNet: Compressed Image Super-Resolution Network  [ :arrow_down: ](https://arxiv.org/pdf/2201.06045.pdf)
>  In recent years, tons of research has been conducted on Single Image Super-Resolution (SISR). However, to the best of our knowledge, few of these studies are mainly focused on compressed images. A problem such as complicated compression artifacts hinders the advance of this study in spite of its high practical values. To tackle this problem, we proposed CISRNet; a network that employs a two-stage coarse-to-fine learning framework that is mainly optimized for Compressed Image Super-Resolution Problem. Specifically, CISRNet consists of two main subnetworks; the coarse and refinement network, where recursive and residual learning is employed within these two networks respectively. Extensive experiments show that with a careful design choice, CISRNet performs favorably against competing Single-Image Super-Resolution methods in the Compressed Image Super-Resolution tasks.      
### 41.When Small Gain Meets Small Phase  [ :arrow_down: ](https://arxiv.org/pdf/2201.06041.pdf)
>  In this paper, we investigate the feedback stability of multiple-input multiple-output linear time-invariant systems with combined gain and phase information. To begin with, we explore the stability condition for a class of so-called easily controllable systems, which have small phase at low frequency ranges and low gain at high frequency. Next, we extend the stability condition via frequency-wise gain and phase combination, based on which a mixed small gain and phase condition with necessity, called a small vase theorem, is then obtained. Furthermore, the fusion of gain and phase information is investigated by a geometric approach based on the Davis-Wielandt shell. Finally, for the purpose of efficient computation and controller synthesis, we present a bounded &amp; sectored real lemma, which gives state-space characterization of combined gain and phase properties based on a triple of linear matrix inequalities.      
### 42.Resource allocation for semantic-aware networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.06023.pdf)
>  Semantic communications have shown its great potential to improve the transmission reliability, especially in low signal-to-noise regime. However, the resource allocation for semantic-aware networks still remains unexplored, which is a critical issue in guaranteeing the transmission reliability of semantic symbols and the communication efficiency of users. To fill this gap, we investigate the spectral efficiency in the semantic domain and rethink the semantic-aware resource allocation issue. Specifically, the semantic spectral efficiency (S-SE) is defined for the first time, and is used to optimize resource allocation in semantic-aware networks in terms of channel assignment and the number of transmitted semantic symbols. Additionally, for fair comparison of semantic and conventional communication systems, a transform method is developed to convert the conventional bit-based spectral efficiency to the S-SE. Simulation results demonstrate the validity and feasibility of the proposed semantic-aware resource allocation model, as well as the superiority of semantic communications in terms of the S-SE.      
### 43.A Residual Encoder-Decoder Network for Segmentation of Retinal Image-Based Exudates in Diabetic Retinopathy Screening  [ :arrow_down: ](https://arxiv.org/pdf/2201.05963.pdf)
>  Diabetic retinopathy refers to the pathology of the retina induced by diabetes and is one of the leading causes of preventable blindness in the world. Early detection of diabetic retinopathy is critical to avoid vision problem through continuous screening and treatment. In traditional clinical practice, the involved lesions are manually detected using photographs of the fundus. However, this task is cumbersome and time-consuming and requires intense effort due to the small size of lesion and low contrast of the images. Thus, computer-assisted diagnosis of diabetic retinopathy based on the detection of red lesions is actively being explored recently. In this paper, we present a convolutional neural network with residual skip connection for the segmentation of exudates in retinal images. To improve the performance of network architecture, a suitable image augmentation technique is used. The proposed network can robustly segment exudates with high accuracy, which makes it suitable for diabetic retinopathy screening. Comparative performance analysis of three benchmark databases: HEI-MED, E-ophtha, and DiaretDB1 is presented. It is shown that the proposed method achieves accuracy (0.98, 0.99, 0.98) and sensitivity (0.97, 0.92, and 0.95) on E-ophtha, HEI-MED, and DiaReTDB1, respectively.      
### 44.Master Equation for Discrete-Time Stackelberg Mean Field Games with single leader  [ :arrow_down: ](https://arxiv.org/pdf/2201.05959.pdf)
>  In this paper, we consider a discrete-time Stackelberg mean field game with a leader and an infinite number of followers. The leader and the followers each observe types privately that evolve as conditionally independent controlled Markov processes. The leader commits to a dynamic policy and the followers best respond to that policy and each other. Knowing that the followers would play a mean field game based on her policy, the leader chooses a policy that maximizes her reward. We refer to the resulting outcome as a Stackelberg mean field equilibrium (SMFE). In this paper, we provide a master equation of this game that allows one to compute all SMFE. Based on our framework, we consider two numerical examples. First, we consider an epidemic model where the followers get infected based on the mean field population. The leader chooses subsidies for a vaccine to maximize social welfare and minimize vaccination costs. In the second example, we consider a technology adoption game where the followers decide to adopt a technology or a product and the leader decides the cost of one product that maximizes his returns, which are proportional to the people adopting that technology      
### 45.Joint Planning of Distributed Generations and Energy Storage in Active Distribution Networks: A Bi-Level Programming Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.05932.pdf)
>  In order to improve the penetration of renewable energy resources for distribution networks, a joint planning model of distributed generations (DGs) and energy storage is proposed for an active distribution network by using a bi-level programming approach in this paper. In this model, the upper-level aims to seek the optimal location and capacity of DGs and energy storage, while the lower-level optimizes the operation of energy storage devices. To solve this model, an improved binary particle swarm optimization (IBPSO) algorithm based on chaos optimization is developed, and the optimal joint planning is achieved through alternating iterations between the two levels. The simulation results on the PG &amp; E 69-bus distribution system demonstrate that the presented approach manages to reduce the planning deviation caused by the uncertainties of DG outputs and remarkably improve the voltage profile and operational economy of distribution systems.      
### 46.ViTBIS: Vision Transformer for Biomedical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.05920.pdf)
>  In this paper, we propose a novel network named Vision Transformer for Biomedical Image Segmentation (ViTBIS). Our network splits the input feature maps into three parts with $1\times 1$, $3\times 3$ and $5\times 5$ convolutions in both encoder and decoder. Concat operator is used to merge the features before being fed to three consecutive transformer blocks with attention mechanism embedded inside it. Skip connections are used to connect encoder and decoder transformer blocks. Similarly, transformer blocks and multi scale architecture is used in decoder before being linearly projected to produce the output segmentation map. We test the performance of our network using Synapse multi-organ segmentation dataset, Automated cardiac diagnosis challenge dataset, Brain tumour MRI segmentation dataset and Spleen CT segmentation dataset. Without bells and whistles, our network outperforms most of the previous state of the art CNN and transformer based models using Dice score and the Hausdorff distance as the evaluation metrics.      
### 47.Using Voltage Phasor Control to Avoid Distribution Network Constraint Violations  [ :arrow_down: ](https://arxiv.org/pdf/2201.05919.pdf)
>  In this paper, we introduce Voltage Phasor Control (VPC), also known as Phasor Based Control, as a novel way of implementing Optimal Power Flow (OPF). Unlike conventional OPF, in which the power flow optimization broadcasts power injections, the VPC power flow optimization broadcasts voltage phasor setpoints to feedback controllers distributed throughout the network which respond to disturbances in real time. In this paper, we demonstrate that VPC can actively manage distributed energy resources to avoid voltage magnitude and line flow constraint violations in power distribution networks. We provide sensitivities and bounds that quantify how the distributed voltage phasor feedback control reduces the effect of disturbances on the network voltages and line flows. Using simulations, we compare the performance of VPC with the related but more conventional Voltage Magnitude Control (VMC). The sensitivities, bounds, and simulations provide implementation insights, which we highlight, for how distribution network operators can use VPC and VMC to avoid constraint violations.      
### 48.Skydiving Technique Analysis from a Control Engineering Perspective: Developing a Tool for Aiding Motor Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.05917.pdf)
>  This study offers an interdisciplinary approach to movement technique analysis, designed to deal with intensive interaction between an environment and a trainee. The free-fall stage of skydiving is investigated, when aerial maneuvers are performed by changing the body posture and thus deflecting the surrounding airflow. The natural learning process of body flight is hard and protracted since the required movements are not similar to our daily movement repertoire, and often counter-intuitive. The proposed method can provide a valuable insight into the subject's learning process and may be used by coaches to identify potentially successful technique changes. The main novelty is that instead of comparing directly the trainee's movements to a template or a movement pattern, extracted from a top-rated athlete in the field, we offer an independent way of technique analysis. We incorporate design tools of automatic control theory to link the trainee's movement patterns to specific performance characteristics. This makes it possible to suggest technique modifications that provide the desired performance improvement, taking into account the individual body parameters and constraints. Representing the performed maneuvers in terms of a dynamic response of closed-loop control system offers an unconventional insight into the motor equivalence problem. The method is demonstrated on three case studies of aerial rotation of skilled, less-skilled, and elite skydivers.      
### 49.Common Phone: A Multilingual Dataset for Robust Acoustic Modelling  [ :arrow_down: ](https://arxiv.org/pdf/2201.05912.pdf)
>  Current state of the art acoustic models can easily comprise more than 100 million parameters. This growing complexity demands larger training datasets to maintain a decent generalization of the final decision function. An ideal dataset is not necessarily large in size, but large with respect to the amount of unique speakers, utilized hardware and varying recording conditions. This enables a machine learning model to explore as much of the domain-specific input space as possible during parameter estimation. This work introduces Common Phone, a gender-balanced, multilingual corpus recorded from more than 76.000 contributors via Mozilla's Common Voice project. It comprises around 116 hours of speech enriched with automatically generated phonetic segmentation. A Wav2Vec 2.0 acoustic model was trained with the Common Phone to perform phonetic symbol recognition and validate the quality of the generated phonetic annotation. The architecture achieved a PER of 18.1 % on the entire test set, computed with all 101 unique phonetic symbols, showing slight differences between the individual languages. We conclude that Common Phone provides sufficient variability and reliable phonetic annotation to help bridging the gap between research and application of acoustic models.      
### 50.SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data  [ :arrow_down: ](https://arxiv.org/pdf/2201.05905.pdf)
>  Capsule network is a recent new deep network architecture that has been applied successfully for medical image segmentation tasks. This work extends capsule networks for volumetric medical image segmentation with self-supervised learning. To improve on the problem of weight initialization compared to previous capsule networks, we leverage self-supervised learning for capsule networks pre-training, where our pretext-task is optimized by self-reconstruction. Our capsule network, SS-3DCapsNet, has a UNet-based architecture with a 3D Capsule encoder and 3D CNNs decoder. Our experiments on multiple datasets including iSeg-2017, Hippocampus, and Cardiac demonstrate that our 3D capsule network with self-supervised pre-training considerably outperforms previous capsule networks and 3D-UNets.      
### 51.SDT-DCSCN for Simultaneous Super-Resolution and Deblurring of Text Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.05865.pdf)
>  Deep convolutional neural networks (Deep CNN) have achieved hopeful performance for single image super-resolution. In particular, the Deep CNN skip Connection and Network in Network (DCSCN) architecture has been successfully applied to natural images super-resolution. In this work we propose an approach called SDT-DCSCN that jointly performs super-resolution and deblurring of low-resolution blurry text images based on DCSCN. Our approach uses subsampled blurry images in the input and original sharp images as ground truth. The used architecture is consists of a higher number of filters in the input CNN layer to a better analysis of the text details. The quantitative and qualitative evaluation on different datasets prove the high performance of our model to reconstruct high-resolution and sharp text images. In addition, in terms of computational time, our proposed method gives competitive performance compared to state of the art methods.      
### 52.Recent Progress in the CUHK Dysarthric Speech Recognition System  [ :arrow_down: ](https://arxiv.org/pdf/2201.05845.pdf)
>  Despite the rapid progress of automatic speech recognition (ASR) technologies in the past few decades, recognition of disordered speech remains a highly challenging task to date. Disordered speech presents a wide spectrum of challenges to current data intensive deep neural networks (DNNs) based ASR technologies that predominantly target normal speech. This paper presents recent research efforts at the Chinese University of Hong Kong (CUHK) to improve the performance of disordered speech recognition systems on the largest publicly available UASpeech dysarthric speech corpus. A set of novel modelling techniques including neural architectural search, data augmentation using spectra-temporal perturbation, model based speaker adaptation and cross-domain generation of visual features within an audio-visual speech recognition (AVSR) system framework were employed to address the above challenges. The combination of these techniques produced the lowest published word error rate (WER) of 25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER reduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric speech recognition system featuring a 6-way DNN system combination and cross adaptation of out-of-domain normal speech data trained systems. Bayesian model adaptation further allows rapid adaptation to individual dysarthric speakers to be performed using as little as 3.06 seconds of speech. The efficacy of these techniques were further demonstrated on a CUDYS Cantonese dysarthric speech recognition task.      
### 53.Cooperative Multi-Agent Deep Reinforcement Learning for Reliable Surveillance via Autonomous Multi-UAV Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.05843.pdf)
>  CCTV-based surveillance using unmanned aerial vehicles (UAVs) is considered a key technology for security in smart city environments. This paper creates a case where the UAVs with CCTV-cameras fly over the city area for flexible and reliable surveillance services. UAVs should be deployed to cover a large area while minimize overlapping and shadow areas for a reliable surveillance system. However, the operation of UAVs is subject to high uncertainty, necessitating autonomous recovery systems. This work develops a multi-agent deep reinforcement learning-based management scheme for reliable industry surveillance in smart city applications. The core idea this paper employs is autonomously replenishing the UAV's deficient network requirements with communications. Via intensive simulations, our proposed algorithm outperforms the state-of-the-art algorithms in terms of surveillance coverage, user support capability, and computational costs.      
### 54.Adaptive Sign Algorithm for Graph Signal Processing  [ :arrow_down: ](https://arxiv.org/pdf/2201.05821.pdf)
>  Efficient and robust online processing technique of irregularly structured data is crucial in the current era of data abundance. In this paper, we propose a graph/network version of the classical adaptive Sign algorithm for online graph signal estimation under impulsive noise. Recently introduced graph adaptive least mean squares algorithm is unstable under non-Gaussian impulsive noise and has high computational complexity. The Graph-Sign algorithm proposed in this work is based on the minimum dispersion criterion and therefore impulsive noise does not hinder its estimation quality. Unlike the recently proposed graph adaptive least mean p-th power algorithm, our Graph-Sign algorithm can operate without prior knowledge of the noise distribution. The proposed Graph-Sign algorithm has a faster run time because of its low computational complexity compared to the existing adaptive graph signal processing algorithms. Experimenting on steady-state and time-varying graph signals estimation utilizing spectral properties of bandlimitedness and sampling, the Graph-Sign algorithm demonstrates fast, stable, and robust graph signal estimation performance under impulsive noise modeled by alpha stable, Cauchy, Student's t, or Laplace distributions.      
### 55.Design and Operation of Hybrid Multi-Terminal Soft Open Points using Feeder Selector Switches for Flexible Distribution System Interconnection  [ :arrow_down: ](https://arxiv.org/pdf/2201.05817.pdf)
>  Distribution systems will require new cost-effective solutions to provide network capacity and increased flexibility to accommodate Low Carbon Technologies. To address this need, we propose the Hybrid Multi-Terminal Soft Open Point (Hybrid MT-SOP) to efficiently provide distribution system interconnection capacity. Each leg of the Hybrid MT-SOP has an AC/DC converter connected in series with a bank of AC switches (Feeder Selector Switches) to allow the converter to connect to any of the feeders at a node. Asymmetric converter sizing is shown to increase feasible power transfers by up to 50% in the three-terminal case, whilst a conic mixed-integer program is formulated to optimally select the device configuration and power transfers. A case study shows the Hybrid MT-SOP increasing utilization of the converters by more than one third, with a 13% increase in system loss reduction as compared to an equally-sized MT-SOP.      
### 56.Two-Stage is Enough: A Concise Deep Unfolding Reconstruction Network for Flexible Video Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2201.05810.pdf)
>  We consider the reconstruction problem of video compressive sensing (VCS) under the deep unfolding/rolling structure. Yet, we aim to build a flexible and concise model using minimum stages. Different from existing deep unfolding networks used for inverse problems, where more stages are used for higher performance but without flexibility to different masks and scales, hereby we show that a 2-stage deep unfolding network can lead to the state-of-the-art (SOTA) results (with a 1.7dB gain in PSNR over the single stage model, RevSCI) in VCS. The proposed method possesses the properties of adaptation to new masks and ready to scale to large data without any additional training thanks to the advantages of deep unfolding. Furthermore, we extend the proposed model for color VCS to perform joint reconstruction and demosaicing. Experimental results demonstrate that our 2-stage model has also achieved SOTA on color VCS reconstruction, leading to a &gt;2.3dB gain in PSNR over the previous SOTA algorithm based on plug-and-play framework, meanwhile speeds up the reconstruction by &gt;17 times. In addition, we have found that our network is also flexible to the mask modulation and scale size for color VCS reconstruction so that a single trained network can be applied to different hardware systems. The code and models will be released to the public.      
### 57.Submarine Cable Network Design for Regional Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2201.05802.pdf)
>  This paper optimizes path planning for a trunkand-branch topology network in an irregular 2-dimensional manifold embedded in 3-dimensional Euclidean space with application to submarine cable network planning. We go beyond our earlier focus on the costs of cable construction (including labor, equipment and materials) together with additional cost to enhance cable resilience, to incorporate the overall cost of branching units (again including material, construction and laying) and the choice of submarine cable landing stations, where such a station can be anywhere on the coast in a connected region. These are important issues for the economics of cable laying and significantly change the model and the optimization process. We pose the problem as a variant of the Steiner tree problem, but one in which the Steiner nodes can vary in number, while incurring a penalty. We refer to it as the weighted Steiner node problem. It differs from the Euclidean Steiner tree problem, where Steiner points are forced to have degree three; this is no longer the case, in general, when nodes incur a cost. We are able to prove that our algorithm is applicable to Steiner nodes with degree greater than three, enabling optimization of network costs in this context. The optimal solution is achieved in polynomialtime using dynamic programming.      
### 58.Efficient demodulation scheme for multilevel modulation based optical camera communication  [ :arrow_down: ](https://arxiv.org/pdf/2201.05784.pdf)
>  We proposed and experimentally demonstrated a new hybrid code structure based on the overlapping of two light sources to produce the effect of multi-voltage amplitudes. And we also proposed an efficient polarity reversal threshold (RRT) algorithm for multilevel based OCC system. Then taking the issue of SPO into account, a novel adaptive sampling method (ASM) is proposed, which can effectively alleviate the problem of SPO and further enhance the performance of multilevel OCC. It is demonstrated that a data rate of 8.4 Kbit/s can be achieved by applying the proposed two algorithms.      
### 59.KazakhTTS2: Extending the Open-Source Kazakh TTS Corpus With More Data, Speakers, and Topics  [ :arrow_down: ](https://arxiv.org/pdf/2201.05771.pdf)
>  We present an expanded version of our previously released Kazakh text-to-speech (KazakhTTS) synthesis corpus. In the new KazakhTTS2 corpus, the overall size is increased from 93 hours to 271 hours, the number of speakers has risen from two to five (three females and two males), and the topic coverage is diversified with the help of new sources, including a book and Wikipedia articles. This corpus is necessary for building high-quality TTS systems for Kazakh, a Central Asian agglutinative language from the Turkic family, which presents several linguistic challenges. We describe the corpus construction process and provide the details of the training and evaluation procedures for the TTS system. Our experimental results indicate that the constructed corpus is sufficient to build robust TTS models for real-world applications, with a subjective mean opinion score of above 4.0 for all the five speakers. We believe that our corpus will facilitate speech and language research for Kazakh and other Turkic languages, which are widely considered to be low-resource due to the limited availability of free linguistic data. The constructed corpus, code, and pretrained models are publicly available in our GitHub repository.      
### 60.Spectral Compressive Imaging Reconstruction Using Convolution and Spectral Contextual Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2201.05768.pdf)
>  Spectral compressive imaging (SCI) is able to encode the high-dimensional hyperspectral image to a 2D measurement, and then uses algorithms to reconstruct the spatio-spectral data-cube. At present, the main bottleneck of SCI is the reconstruction algorithm, and the state-of-the-art (SOTA) reconstruction methods generally face the problem of long reconstruction time and/or poor detail recovery. In this paper, we propose a novel hybrid network module, namely CSCoT (Convolution and Spectral Contextual Transformer) block, which can acquire the local perception of convolution and the global perception of transformer simultaneously, and is conducive to improving the quality of reconstruction to restore fine details. We integrate the proposed CSCoT block into deep unfolding framework based on the generalized alternating projection algorithm, and further propose the GAP-CSCoT network. Finally, we apply the GAP-CSCoT algorithm to SCI reconstruction. Through the experiments of extensive synthetic and real data, our proposed model achieves higher reconstruction quality ($&gt;$2dB in PSNR on simulated benchmark datasets) and shorter running time than existing SOTA algorithms by a large margin. The code and models will be released to the public.      
### 61.Robust Safe Control Synthesis with Disturbance Observer-Based Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2201.05758.pdf)
>  In a complex real-time operating environment, external disturbances and uncertainties adversely affect the safety, stability, and performance of dynamical systems. This paper presents a robust stabilizing safety-critical controller synthesis framework with control Lyapunov functions (CLFs) and control barrier functions (CBFs) in the presence of disturbance. A high-gain input observer method is adapted to estimate the time-varying unmodelled dynamics of the CBF with an error bound using the first-order time derivative of the CBF. This approach leads to an easily tunable low order disturbance estimator structure with a design parameter as it utilizes only the CBF constraint. The estimated unknown input and associated error bound are used to ensure robust safety and exponential stability by formulating a CLF-CBF quadratic program. The proposed method is applicable to both relative degree one and higher relative degree CBF constraints. The efficacy of the proposed approach is demonstrated using a numerical simulations of an adaptive cruise control system and a Segway platform with an external disturbance.      
### 62.Indirect Adaptive Control of Nonlinearly Parameterized Nonlinear Dissipative Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.05749.pdf)
>  In this note we address the problem of indirect adaptive (regulation or tracking) control of nonlinear, input affine dissipative systems. It is assumed that the supply rate, the storage and the internal dissipation functions may be expressed as nonlinearly parameterized regression equations where the mappings (depending on the unknown parameters) satisfy a monotonicity condition -- this encompasses a large class of physical systems, including passive systems. We propose to estimate the system parameters using the "power-balance" equation, which is the differential version of the classical dissipation inequality, with a new estimator that ensures global, exponential, parameter convergence under the very weak assumption of interval excitation of the power-balance equation regressor. To design the indirect adaptive controller we make the standard assumption of existence of an asymptotically stabilizing controller that depends -- possibly nonlinearly -- on the unknown plant parameters, and apply a certainty-equivalent control law. The benefits of the proposed approach, with respect to other existing solutions, are illustrated with examples.      
### 63.Diffusion Tensor Estimation with Transformer Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.05701.pdf)
>  Diffusion tensor imaging (DTI) is the most widely used tool for studying brain white matter development and degeneration. However, standard DTI estimation methods depend on a large number of high-quality measurements. This would require long scan times and can be particularly difficult to achieve with certain patient populations such as neonates. Here, we propose a method that can accurately estimate the diffusion tensor from only six diffusion-weighted measurements. Our method achieves this by learning to exploit the relationships between the diffusion signals and tensors in neighboring voxels. Our model is based on transformer networks, which represent the state of the art in modeling the relationship between signals in a sequence. In particular, our model consists of two such networks. The first network estimates the diffusion tensor based on the diffusion signals in a neighborhood of voxels. The second network provides more accurate tensor estimations by learning the relationships between the diffusion signals as well as the tensors estimated by the first network in neighboring voxels. Our experiments with three datasets show that our proposed method achieves highly accurate estimations of the diffusion tensor and is significantly superior to three competing methods. Estimations produced by our method with six measurements are comparable with those of standard estimation methods with 30-88 measurements. Hence, our method promises shorter scan times and more reliable assessment of brain white matter, particularly in non-cooperative patients such as neonates and infants.      
### 64.Disentanglement enables cross-domain Hippocampus Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.05650.pdf)
>  Limited amount of labelled training data are a common problem in medical imaging. This makes it difficult to train a well-generalised model and therefore often leads to failure in unknown domains. Hippocampus segmentation from magnetic resonance imaging (MRI) scans is critical for the diagnosis and treatment of neuropsychatric disorders. Domain differences in contrast or shape can significantly affect segmentation. We address this issue by disentangling a T1-weighted MRI image into its content and domain. This separation enables us to perform a domain transfer and thus convert data from new sources into the training domain. This step thus simplifies the segmentation problem, resulting in higher quality segmentations. We achieve the disentanglement with the proposed novel methodology 'Content Domain Disentanglement GAN', and we propose to retrain the UNet on the transformed outputs to deal with GAN-specific artefacts. With these changes, we are able to improve performance on unseen domains by 6-13% and outperform state-of-the-art domain transfer methods.      
### 65.Mitigating Misinformation Spread on Blockchain Enabled Social Media Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.07076.pdf)
>  We construct a blockchain-enabled social media network to mitigate the spread of misinformation. We derive the information transmission-time distribution by modeling the misinformation transmission as double-spend attacks on blockchain. This distribution is then incorporated into the SIR model, which substitutes the single rate parameter in the traditional SIR model. Then, on a multi-community network, we study the propagation of misinformation numerically and show that the proposed blockchain enabled social media network outperforms the baseline network in flattening the curve of the infected population.      
### 66.Temporal Characterization of XR Traffic with Application to Predictive Network Slicing  [ :arrow_down: ](https://arxiv.org/pdf/2201.07043.pdf)
>  Over the past few years, eXtended Reality (XR) has attracted increasing interest thanks to its extensive industrial and commercial applications, and its popularity is expected to rise exponentially over the next decade. However, the stringent Quality of Service (QoS) constraints imposed by XR's interactive nature require Network Slicing (NS) solutions to support its use over wireless connections: in this context, quasi-Constant Bit Rate (CBR) encoding is a promising solution, as it can increase the predictability of the stream, making the network resource allocation easier. However, traffic characterization of XR streams is still a largely unexplored subject, particularly with this encoding. In this work, we characterize XR streams from more than 4 hours of traces captured in a real setup, analyzing their temporal correlation and proposing two prediction models for future frame size. Our results show that even the state-of-the-art H.264 CBR mode can have significant frame size fluctuations, which can impact the NS optimization. Our proposed prediction models can be applied to different traces, and even to different contents, achieving very similar performance. We also show the trade-off between network resource efficiency and XR QoS in a simple NS use case.      
### 67.Computational Rational Engineering and Development: Synergies and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2201.06922.pdf)
>  Research and development in computer technology and computational methods have resulted in a wide variety of valuable tools for Computer-Aided Engineering (CAE) and Industrial Engineering. However, despite the exponential increase in computational capabilities and Artificial Intelligence (AI) methods, many of the visionary perspectives on cybernetic automation of design, engineering, and development have not been successfully pursued or realized yet. While contemporary research trends and movements such as Industry 4.0 primarily target progress by connected automation in manufacturing and production, the objective of this paper is to survey progress and formulate perspectives targeted on the automation and autonomization of engineering development processes. Based on an interdisciplinary mini-review, this work identifies open challenges, synergies, and research opportunities towards the realization of resource-efficient cooperative engineering and development systems. In order to go beyond conventional human-centered, tool-based CAE approaches and realize Computational Intelligence Driven Development processes, it is suggested to extend the framework of Computational Rationality to challenges in design, engineering and development.      
### 68.Hardware-Efficient Deconvolution-Based GAN for Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2201.06878.pdf)
>  Generative Adversarial Networks (GAN) are cutting-edge algorithms for generating new data samples based on the learned data distribution. However, its performance comes at a significant cost in terms of computation and memory requirements. In this paper, we proposed an HW/SW co-design approach for training quantized deconvolution GAN (QDCGAN) implemented on FPGA using a scalable streaming dataflow architecture capable of achieving higher throughput versus resource utilization trade-off. The developed accelerator is based on an efficient deconvolution engine that offers high parallelism with respect to scaling factors for GAN-based edge computing. Furthermore, various precisions, datasets, and network scalability were analyzed for low-power inference on resource-constrained platforms. Lastly, an end-to-end open-source framework is provided for training, implementation, state-space exploration, and scaling the inference using Vivado high-level synthesis for Xilinx SoC-FPGAs, and a comparison testbed with Jetson Nano.      
### 69.Structural Consensus in Networks with Directed Topologies and Its Cryptographic Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2201.06747.pdf)
>  The existing cryptosystem based approaches for privacy-preserving consensus of networked systems are usually limited to those with undirected topologies. This paper proposes a new privacy-preserving algorithm for networked systems with directed topologies to reach confidential consensus. As a prerequisite for applying the algorithm, a structural consensus problem is formulated and the solvability conditions are discussed for an explicitly constructed controller. The controller is then implemented with encryption to achieve consensus while avoiding individual's information leakage to external eavesdroppers and/or malicious internal neighbors.      
### 70.Dynamic Blockage Pre-Avoidance using Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2201.06659.pdf)
>  Internet-of-vehicle (IoV) is a general concept referring to, e.g., autonomous drive based vehicle-to-everything (V2X) communications or moving relays. Here, high rate and reliability demands call for advanced multi-antenna techniques and millimeter-wave (mmw) based communications. However, the sensitivity of the mmw signals to blockage may limit the system performance, especially in highways/rural areas with limited building reflectors/base station deployments and high-speed devices. To avoid the blockage, various techniques have been proposed among which reconfigurable intelligent surface (RIS) is a candidate. RIS, however, has been mainly of interest in stationary/low mobility scenarios, due to the associated channel state information acquisition and beam management overhead as well as imperfect reflection. In this article, we study the potentials and challenges of RIS-assisted dynamic blockage avoidance in IoV networks. Particularly, by designing region-based RIS pre-selection as well as blockage prediction schemes, we show that RIS-assisted communication has the potential to boost the performance of IoV networks. However, there are still issues to be solved before RIS can be practically deployed in IoV networks.      
### 71.Control of port-Hamiltonian differential-algebraic systems and applications  [ :arrow_down: ](https://arxiv.org/pdf/2201.06590.pdf)
>  The modeling framework of port-Hamiltonian descriptor systems and their use in numerical simulation and control are discussed. The structure is ideal for automated network-based modeling since it is invariant under power-conserving interconnection, congruence transformations, and Galerkin projection. Moreover, stability and passivity properties are easily shown. Condensed forms under orthogonal transformations present easy analysis tools for existence, uniqueness, regularity, and numerical methods to check these properties. <br>After recalling the concepts for general linear and nonlinear descriptor systems, we demonstrate that many difficulties that arise in general descriptor systems can be easily overcome within the port-Hamiltonian framework. The properties of port-Hamiltonian descriptor systems are analyzed, time-discretization, and numerical linear algebra techniques are discussed. Structure-preserving regularization procedures for descriptor systems are presented to make them suitable for simulation and control. Model reduction techniques that preserve the structure and stabilization and optimal control techniques are discussed. <br>The properties of port-Hamiltonian descriptor systems and their use in modeling simulation and control methods are illustrated with several examples from different physical domains. The survey concludes with open problems and research topics that deserve further attention.      
### 72.Reliable Beam Tracking with Dynamic Beamwidth Adaptation in Terahertz (THz) Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.06541.pdf)
>  THz communication is regarded as one of the potential key enablers for next-generation wireless systems. While THz frequency bands provide abundant bandwidths and extremely high data rates, the operation at THz bands is mandated by short communication ranges and narrow pencil beams, which are highly susceptible to user mobility and beam misalignment as well as channel blockages. This raises the need for novel beam tracking methods that take into account the tradeoff between enhancing the received signal strength by increasing beam directivity, and increasing the coverage probability by widening the beam. To address these challenges, a multi-objective optimization problem is formulated with the goal of jointly maximizing the ergodic rate and minimizing the outage probability subject to transmit power and average overhead constraints. Then, a novel parameterized beamformer with dynamic beamwidth adaptation is proposed. In addition to the precoder, an event-based beam tracking approach is introduced that enables reacting to outages caused by beam misalignment and dynamic blockage while maintaining a low pilot overhead. Simulation results show that our proposed beamforming scheme improves average rate performance and reduces the amount of communication outages caused by beam misalignment. Moreover, the proposed event-triggered channel estimation approach enables low-overhead yet reliable communication.      
### 73.Structured model order reduction for vibro-acoustic problems using interpolation and balancing methods  [ :arrow_down: ](https://arxiv.org/pdf/2201.06518.pdf)
>  Vibration and dissipation in vibro-acoustic systems can be assessed using frequency response analysis. Evaluating a frequency sweep on a full-order model can be very costly, so model order reduction methods are employed to compute cheap-to-evaluate surrogates. This work compares structure-preserving model reduction methods based on rational interpolation and balanced truncation with a specific focus on their applicability to vibro-acoustic systems. Such models typically exhibit a second-order structure and their material properties as well as their excitation may be depending on the driving frequency. We show and compare the effectiveness of all considered methods by applying them to numerical models of vibro-acoustic systems depicting structural vibration, sound transmission, acoustic scattering, and poroelastic problems.      
### 74.MsEmoTTS: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2201.06460.pdf)
>  Expressive synthetic speech is essential for many human-computer interaction and audio broadcast scenarios, and thus synthesizing expressive speech has attracted much attention in recent years. Previous methods performed the expressive speech synthesis either with explicit labels or with a fixed-length style embedding extracted from reference audio, both of which can only learn an average style and thus ignores the multi-scale nature of speech prosody. In this paper, we propose MsEmoTTS, a multi-scale emotional speech synthesis framework, to model the emotion from different levels. Specifically, the proposed method is a typical attention-based sequence-to-sequence model and with proposed three modules, including global-level emotion presenting module (GM), utterance-level emotion presenting module (UM), and local-level emotion presenting module (LM), to model the global emotion category, utterance-level emotion variation, and syllable-level emotion strength, respectively. In addition to modeling the emotion from different levels, the proposed method also allows us to synthesize emotional speech in different ways, i.e., transferring the emotion from reference audio, predicting the emotion from input text, and controlling the emotion strength manually. Extensive experiments conducted on a Chinese emotional speech corpus demonstrate that the proposed method outperforms the compared reference audio-based and text-based emotional speech synthesis methods on the emotion transfer speech synthesis and text-based emotion prediction speech synthesis respectively. Besides, the experiments also show that the proposed method can control the emotion expressions flexibly. Detailed analysis shows the effectiveness of each module and the good design of the proposed method.      
### 75.On Training Targets and Activation Functions for Deep Representation Learning in Text-Dependent Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2201.06426.pdf)
>  Deep representation learning has gained significant momentum in advancing text-dependent speaker verification (TD-SV) systems. When designing deep neural networks (DNN) for extracting bottleneck features, key considerations include training targets, activation functions, and loss functions. In this paper, we systematically study the impact of these choices on the performance of TD-SV. For training targets, we consider speaker identity, time-contrastive learning (TCL) and auto-regressive prediction coding with the first being supervised and the last two being self-supervised. Furthermore, we study a range of loss functions when speaker identity is used as the training target. With regard to activation functions, we study the widely used sigmoid function, rectified linear unit (ReLU), and Gaussian error linear unit (GELU). We experimentally show that GELU is able to reduce the error rates of TD-SV significantly compared to sigmoid, irrespective of training target. Among the three training targets, TCL performs the best. Among the various loss functions, cross entropy, joint-softmax and focal loss functions outperform the others. Finally, score-level fusion of different systems is also able to reduce the error rates. Experiments are conducted on the RedDots 2016 challenge database for TD-SV using short utterances. For the speaker classifications, the well-known Gaussian mixture model-universal background model (GMM-UBM) and i-vector techniques are used.      
### 76.Improving Performance of Semantic Segmentation CycleGANs by Noise Injection into the Latent Segmentation Space  [ :arrow_down: ](https://arxiv.org/pdf/2201.06415.pdf)
>  In recent years, semantic segmentation has taken benefit from various works in computer vision. Inspired by the very versatile CycleGAN architecture, we combine semantic segmentation with the concept of cycle consistency to enable a multitask training protocol. However, learning is largely prevented by the so-called steganography effect, which expresses itself as watermarks in the latent segmentation domain, making image reconstruction a too easy task. To combat this, we propose a noise injection, based either on quantization noise or on Gaussian noise addition to avoid this disadvantageous information flow in the cycle architecture. We find that noise injection significantly reduces the generation of watermarks and thus allows the recognition of highly relevant classes such as "traffic signs", which are hardly detected by the ERFNet baseline. We report mIoU and PSNR results on the Cityscapes dataset for semantic segmentation and image reconstruction, respectively. The proposed methodology allows to achieve an mIoU improvement on the Cityscapes validation set of 5.7% absolute over the same CycleGAN without noise injection, and still an absolute 4.9% over the ERFNet non-cyclic baseline.      
### 77.An Approach for System Analysis with MBSE and Graph Data Engineering  [ :arrow_down: ](https://arxiv.org/pdf/2201.06363.pdf)
>  Model-Based Systems Engineering aims at creating a model of a system under development, covering the complete system with a level of detail that allows to define and understand its behavior and enables to define any interface and workpackage based on the model. Once such a model is established, further benefits can be reaped, such as the analysis of complex technical correlations within the system. Various insights can be gained by displaying the model as a formal graph and querying it. To enable such queries, a graph schema needs to be designed, which allows to transfer the model into a graph database. In the course of this paper, we discuss the design of a graph schema and MBSE modelling approach, enabling deep going system analysis and anomaly resolution in complex embedded systems. The schema and modelling approach are designed to answer questions such as what happens if there is an electrical short in a component? Which other components are now offline and which data cannot be gathered anymore? Or if a condition cannot be met, which alternative routes can be established to reach a certain state of the system. We build on the use case of qualification and operations of a small spacecraft. Structural and behavioral elements of the MBSE model are transferred to a graph database where analyses are conducted on the system. The schema is implemented by an adapter for MagicDraw to Neo4j. A selection of complex analyses are shown on the example of the MOVE-II space mission.      
### 78.Group Gated Fusion on Attention-based Bidirectional Alignment for Multimodal Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.06309.pdf)
>  Emotion recognition is a challenging and actively-studied research area that plays a critical role in emotion-aware human-computer interaction systems. In a multimodal setting, temporal alignment between different modalities has not been well investigated yet. This paper presents a new model named as Gated Bidirectional Alignment Network (GBAN), which consists of an attention-based bidirectional alignment network over LSTM hidden states to explicitly capture the alignment relationship between speech and text, and a novel group gated fusion (GGF) layer to integrate the representations of different modalities. We empirically show that the attention-aligned representations outperform the last-hidden-states of LSTM significantly, and the proposed GBAN model outperforms existing state-of-the-art multimodal approaches on the IEMOCAP dataset.      
### 79.Hybrid Analog/Digital Precoding for Downlink Massive MIMO LEO Satellite Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.06281.pdf)
>  Massive multiple-input multiple-output (MIMO) is promising for low earth orbit (LEO) satellite communications due to the potential in enhancing the spectral efficiency. However, the conventional fully digital precoding architectures might lead to high implementation complexity and energy consumption. In this paper, hybrid analog/digital precoding solutions are developed for the downlink operation in LEO massive MIMO satellite communications, by exploiting the slow-varying statistical channel state information (CSI) at the transmitter. First, we formulate the hybrid precoder design as an energy efficiency (EE) maximization problem by considering both the continuous and discrete phase shift networks for implementing the analog precoder. The cases of both the fully and the partially connected architectures are considered. Since the EE optimization problem is nonconvex, it is in general difficult to solve. To make the EE maximization problem tractable, we apply a closed-form tight upper bound to approximate the ergodic rate. Then, we develop an efficient algorithm to obtain the fully digital precoders. Based on which, we further develop two different efficient algorithmic solutions to compute the hybrid precoders for the fully and the partially connected architectures, respectively. Simulation results show that the proposed approaches achieve significant EE performance gains over the existing baselines, especially when the discrete phase shift network is employed for analog precoding.      
### 80.Learning-based multiplexed transmission of scattered twisted light through a kilometer-scale standard multimode fiber  [ :arrow_down: ](https://arxiv.org/pdf/2201.06263.pdf)
>  Multiplexing multiple orbital angular momentum (OAM) modes of light has the potential to increase data capacity in optical communication. However, the distribution of such modes over long distances remains challenging. Free-space transmission is strongly influenced by atmospheric turbulence and light scattering, while the wave distortion induced by the mode dispersion in fibers disables OAM demultiplexing in fiber-optic communications. Here, a deep-learning-based approach is developed to recover the data from scattered OAM channels without measuring any phase information. Over a 1-km-long standard multimode fiber, the method is able to identify different OAM modes with an accuracy of more than 99.9% in parallel demultiplexing of 24 scattered OAM channels. To demonstrate the transmission quality, color images are encoded in multiplexed twisted light and our method achieves decoding the transmitted data with an error rate of 0.13%. Our work shows the artificial intelligence algorithm could benefit the use of OAM multiplexing in commercial fiber networks and high-performance optical communication in turbulent environments.      
### 81.Optimisation of Structured Neural Controller Based on Continuous-Time Policy Gradient  [ :arrow_down: ](https://arxiv.org/pdf/2201.06262.pdf)
>  This study presents a policy optimisation framework for structured nonlinear control of continuous-time (deterministic) dynamic systems. The proposed approach prescribes a structure for the controller based on relevant scientific knowledge (such as Lyapunov stability theory or domain experiences) while considering the tunable elements inside the given structure as the point of parametrisation with neural networks. To optimise a cost represented as a function of the neural network weights, the proposed approach utilises the continuous-time policy gradient method based on adjoint sensitivity analysis as a means for correct and performant computation of cost gradient. This enables combining the stability, robustness, and physical interpretability of an analytically-derived structure for the feedback controller with the representational flexibility and optimised resulting performance provided by machine learning techniques. Such a hybrid paradigm for fixed-structure control synthesis is particularly useful for optimising adaptive nonlinear controllers to achieve improved performance in online operation, an area where the existing theory prevails the design of structure while lacking clear analytical understandings about tuning of the gains and the uncertainty model basis functions that govern the performance characteristics. Numerical experiments on aerospace applications illustrate the utility of the structured nonlinear controller optimisation framework.      
### 82.WASABI: Widely-Spaced Array and Beamforming Design for Terahertz Range-Angle Secure Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.06253.pdf)
>  Terahertz (THz) communications have naturally promising physical layer security (PLS) performance in the angular domain due to the high directivity feature. However, if eavesdroppers reside in the beam sector, the directivity fails to work effectively to handle this range-domain security problem. More critically, with an eavesdropper inside the beam sector and nearer to the transmitter than the legitimate receiver, i.e., in close proximity, secure communication is jeopardized. This open challenge motivates this work to study PLS techniques to enhance THz range-angle security. In this paper, a novel widely-spaced array and beamforming (WASABI) design for THz range-angle secure communication is proposed, based on the uniform planar array and hybrid beamforming. Specifically, the WASABI design is theoretically proved to achieve the optimal secrecy rate powered by the non-constrained optimum approaching (NCOA) algorithm with more than one RF chain, i.e., with the hybrid beamforming scheme. Moreover, with a low-complexity and sub-optimal analog beamforming, the WASABI scheme can achieve sub-optimal performance with less than 5% secrecy rate degradation. Simulation results illustrate that our proposed widely-spaced antenna communication scheme can ensure a 6bps/Hz secrecy rate when the transmit power is 10dBm. Finally, a frequency diverse array, as an advocated range security candidate in the literature, is proven to be ineffective to enhance range security.      
### 83.Clustering-based Joint Channel Estimation and Signal Detection for NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2201.06245.pdf)
>  We propose a joint channel estimation and signal detection approach for the uplink non-orthogonal multiple access using unsupervised machine learning. We apply the Gaussian mixture model to cluster the received signals, and accordingly optimize the decision regions to enhance the symbol error rate (SER). We show that, when the received powers of the users are sufficiently different, the proposed clustering-based approach achieves an SER performance on a par with that of the conventional maximum-likelihood detector with full channel state information. However, unlike the proposed approach, the maximum-likelihood detector requires the transmission of a large number of pilot symbols to accurately estimate the channel. The accuracy of the utilized clustering algorithm depends on the number of the data points available at the receiver. Therefore, there exists a tradeoff between accuracy and block length. We provide a comprehensive performance analysis of the proposed approach as well as deriving a theoretical bound on its SER performance as a function of the block length. Our simulation results corroborate the effectiveness of the proposed approach and verify that the calculated theoretical bound can predict the SER performance of the proposed approach well.      
### 84.Comparative Study of Acoustic Echo Cancellation Algorithms for Speech Recognition System in Noisy Environment  [ :arrow_down: ](https://arxiv.org/pdf/2201.06209.pdf)
>  Traditionally, adaptive filters have been deployed to achieve AEC by estimating the acoustic echo response using algorithms such as the Normalized Least-Mean-Square (NLMS) algorithm. Several approaches have been proposed over recent years to improve the performance of the standard NLMS algorithm in various ways for AEC. These include algorithms based on Time Domain, Frequency Domain, Fourier Transform, Wavelet Transform Adaptive Schemes, Proportionate Schemes, Proportionate Adaptive Filters, Combination Schemes, Block Based Combination, Sub band Adaptive Filtering, Uniform Over Sampled DFT Filter Banks, Sub band Over-Sampled DFT Filter Banks, Volterra Filters, Variable Step-Size (VSS) algorithms, Data Reusing Techniques, Partial Update Adaptive Filtering Techniques and Sub band (SAF) Schemes. These approaches aim to address issues in echo cancellation including the performance with noisy input signals, Time-Varying echo paths and computational complexity. In contrast to these approaches, Sparse Adaptive algorithms have been developed specifically to address the performance of adaptive filters in sparse system identification. In this paper we have discussed some AEC algorithms followed by comparative study with respective to step-size, convergence and performance.      
### 85.A Novel Covert Communication Method using Ambient Backscatter Communications  [ :arrow_down: ](https://arxiv.org/pdf/2201.06204.pdf)
>  This paper introduces a novel solution to enable covert communication in wireless systems by using ambient backscatter communication technology. In the considered system, the original message at the transmitter is first divided into two parts: (i) active transmit message and (ii) backscatter message. Then, the active transmit message is transmitted by using the conventional wireless transmission method while the backscatter message is transmitted by backscattering the active transmit signals via an ambient backscatter tag. As the backscatter tag does not generate any active signals, it is intractable for the adversary to detect the backscatter message. Therefore, secret information, e.g., secret key for decryption, can be carried by the backscattered message, making the adversary unable to decode the original message. Simulation results demonstrate that our proposed solution can help to significantly enhance security protection for communication systems.      
### 86.A novel attention model for salient structure detection in seismic volumes  [ :arrow_down: ](https://arxiv.org/pdf/2201.06174.pdf)
>  A new approach to seismic interpretation is proposed to leverage visual perception and human visual system modeling. Specifically, a saliency detection algorithm based on a novel attention model is proposed for identifying subsurface structures within seismic data volumes. The algorithm employs 3D-FFT and a multi-dimensional spectral projection, which decomposes local spectra into three distinct components, each depicting variations along different dimensions of the data. Subsequently, a novel directional center-surround attention model is proposed to incorporate directional comparisons around each voxel for saliency detection within each projected dimension. Next, the resulting saliency maps along each dimension are combined adaptively to yield a consolidated saliency map, which highlights various structures characterized by subtle variations and relative motion with respect to their neighboring sections. A priori information about the seismic data can be either embedded into the proposed attention model in the directional comparisons, or incorporated into the algorithm by specifying a template when combining saliency maps adaptively. Experimental results on two real seismic datasets from the North Sea, Netherlands and Great South Basin, New Zealand demonstrate the effectiveness of the proposed algorithm for detecting salient seismic structures of different natures and appearances in one shot, which differs significantly from traditional seismic interpretation algorithms. The results further demonstrate that the proposed method outperforms comparable state-of-the-art saliency detection algorithms for natural images and videos, which are inadequate for seismic imaging data.      
### 87.On Maximum-a-Posteriori estimation with Plug &amp; Play priors and stochastic gradient descent  [ :arrow_down: ](https://arxiv.org/pdf/2201.06133.pdf)
>  Bayesian methods to solve imaging inverse problems usually combine an explicit data likelihood function with a prior distribution that explicitly models expected properties of the solution. Many kinds of priors have been explored in the literature, from simple ones expressing local properties to more involved ones exploiting image redundancy at a non-local scale. In a departure from explicit modelling, several recent works have proposed and studied the use of implicit priors defined by an image denoising algorithm. This approach, commonly known as Plug &amp; Play (PnP) regularisation, can deliver remarkably accurate results, particularly when combined with state-of-the-art denoisers based on convolutional neural networks. However, the theoretical analysis of PnP Bayesian models and algorithms is difficult and works on the topic often rely on unrealistic assumptions on the properties of the image denoiser. This papers studies maximum-a-posteriori (MAP) estimation for Bayesian models with PnP priors. We first consider questions related to existence, stability and well-posedness, and then present a convergence proof for MAP computation by PnP stochastic gradient descent (PnP-SGD) under realistic assumptions on the denoiser used. We report a range of imaging experiments demonstrating PnP-SGD as well as comparisons with other PnP schemes.      
### 88.Modeling the Repetition-based Recovering of Acoustic and Visual Sources with Dendritic Neurons  [ :arrow_down: ](https://arxiv.org/pdf/2201.06123.pdf)
>  In natural auditory environments, acoustic signals originate from the temporal superimposition of different sound sources. The problem of inferring individual sources from ambiguous mixtures of sounds is known as blind source decomposition. Experiments on humans have demonstrated that the auditory system can identify sound sources as repeating patterns embedded in the acoustic input. Source repetition produces temporal regularities that can be detected and used for segregation. Specifically, listeners can identify sounds occurring more than once across different mixtures, but not sounds heard only in a single mixture. However, whether such a behaviour can be computationally modelled has not yet been explored. Here, we propose a biologically inspired computational model to perform blind source separation on sequences of mixtures of acoustic stimuli. Our method relies on a somatodendritic neuron model trained with a Hebbian-like learning rule which can detect spatio-temporal patterns recurring in synaptic inputs. We show that the segregation capabilities of our model are reminiscent of the features of human performance in a variety of experimental settings involving synthesized sounds with naturalistic properties. Furthermore, we extend the study to investigate the properties of segregation on task settings not yet explored with human subjects, namely natural sounds and images. Overall, our work suggests that somatodendritic neuron models offer a promising neuro-inspired learning strategy to account for the characteristics of the brain segregation capabilities as well as to make predictions on yet untested experimental settings.      
### 89.Bayesian Promised Persuasion: Dynamic Forward-Looking Multiagent Delegation with Informational Burning  [ :arrow_down: ](https://arxiv.org/pdf/2201.06081.pdf)
>  This work studies a dynamic mechanism design problem in which a principal delegates decision makings to a group of privately-informed agents without the monetary transfer or burning. We consider that the principal privately possesses complete knowledge about the state transitions and study how she can use her private observation to support the incentive compatibility of the delegation via informational burning, a process we refer to as the looking-forward persuasion. The delegation mechanism is formulated in which the agents form belief hierarchies due to the persuasion and play a dynamic Bayesian game. We propose a novel randomized mechanism, known as Bayesian promised delegation (BPD), in which the periodic incentive compatibility is guaranteed by persuasions and promises of future delegations. We show that the BPD can achieve the same optimal social welfare as the original mechanism in stationary Markov perfect Bayesian equilibria. A revelation-principle-like design regime is established to show that the persuasion with belief hierarchies can be fully characterized by correlating the randomization of the agents' local BPD mechanisms with the persuasion as a direct recommendation of the future promises.      
### 90.Fully Convolutional Change Detection Framework with Generative Adversarial Network for Unsupervised, Weakly Supervised and Regional Supervised Change Detection  [ :arrow_down: ](https://arxiv.org/pdf/2201.06030.pdf)
>  Deep learning for change detection is one of the current hot topics in the field of remote sensing. However, most end-to-end networks are proposed for supervised change detection, and unsupervised change detection models depend on traditional pre-detection methods. Therefore, we proposed a fully convolutional change detection framework with generative adversarial network, to conclude unsupervised, weakly supervised, regional supervised, and fully supervised change detection tasks into one framework. A basic Unet segmentor is used to obtain change detection map, an image-to-image generator is implemented to model the spectral and spatial variation between multi-temporal images, and a discriminator for changed and unchanged is proposed for modeling the semantic changes in weakly and regional supervised change detection task. The iterative optimization of segmentor and generator can build an end-to-end network for unsupervised change detection, the adversarial process between segmentor and discriminator can provide the solutions for weakly and regional supervised change detection, the segmentor itself can be trained for fully supervised task. The experiments indicate the effectiveness of the propsed framework in unsupervised, weakly supervised and regional supervised change detection. This paper provides theorical definitions for unsupervised, weakly supervised and regional supervised change detection tasks, and shows great potentials in exploring end-to-end network for remote sensing change detection.      
### 91.Discrete Simulation Optimization for Tuning Machine Learning Method Hyperparameters  [ :arrow_down: ](https://arxiv.org/pdf/2201.05978.pdf)
>  Machine learning methods are being increasingly used in most technical areas such as image recognition, product recommendation, financial analysis, medical diagnosis, and predictive maintenance. The key question that arises is: how do we control the learning process according to our requirement for the problem? Hyperparameter tuning is used to choose the optimal set of hyperparameters for controlling the learning process of a model. Selecting the appropriate hyperparameters directly impacts the performance measure a model. We have used simulation optimization using discrete search methods like ranking and selection (R&amp;S) methods such as the KN method and stochastic ruler method and its variations for hyperparameter optimization and also developed the theoretical basis for applying common R&amp;S methods. The KN method finds the best possible system with statistical guarantee and stochastic ruler method asymptotically converges to the optimal solution and is also computationally very efficient. We also benchmarked our results with state of art hyperparameter optimization libraries such as $hyperopt$ and $mango$ and found KN and stochastic ruler to be performing consistently better than $hyperopt~rand$ and stochastic ruler to be equally efficient in comparison with $hyperopt~tpe$ in most cases, even when our computational implementations are not yet optimized in comparison to professional packages.      
### 92.IRHA: An Intelligent RSSI based Home automation System  [ :arrow_down: ](https://arxiv.org/pdf/2201.05975.pdf)
>  Human existence is getting more sophisticated and better in many areas due to remarkable advances in the fields of automation. Automated systems are favored over manual ones in the current environment. Home Automation is becoming more popular in this scenario, as people are drawn to the concept of a home environment that can automatically satisfy users' requirements. The key challenges in an intelligent home are intelligent decision making, location-aware service, and compatibility for all users of different ages and physical conditions. Existing solutions address just one or two of these challenges, but smart home automation that is robust, intelligent, location-aware, and predictive is needed to satisfy the user's demand. This paper presents a location-aware intelligent RSSI-based home automation system (IRHA) that uses Wi-Fi signals to detect the user's location and control the appliances automatically. The fingerprinting method is used to map the Wi-Fi signals for different rooms, and the machine learning method, such as Decision Tree, is used to classify the signals for different rooms. The machine learning models are then implemented in the ESP32 microcontroller board to classify the rooms based on the real-time Wi-Fi signal, and then the result is sent to the main control board through the ESP32 MAC communication protocol to control the appliances automatically. The proposed method has achieved 97% accuracy in classifying the users' location.      
### 93.TriLock: IC Protection with Tunable Corruptibility and Resilience to SAT and Removal Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2201.05943.pdf)
>  Sequential logic locking has been studied over the last decade as a method to protect sequential circuits from reverse engineering. However, most of the existing sequential logic locking techniques are threatened by increasingly more sophisticated SAT-based attacks, efficiently using input queries to a SAT solver to rule out incorrect keys, as well as removal attacks based on structural analysis. In this paper, we propose TriLock, a sequential logic locking method that simultaneously addresses these vulnerabilities. TriLock can achieve high, tunable functional corruptibility while still guaranteeing exponential queries to the SAT solver in a SAT-based attack. Further, it adopts a state re-encoding method to obscure the boundary between the original state registers and those inserted by the locking method, thus making it more difficult to detect and remove the locking-related components.      
### 94.A Unified View on Semantic Information and Communication: A Probabilistic Logic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.05936.pdf)
>  This article aims to provide a unified and technical approach to semantic information, communication, and their interplay through the lens of probabilistic logic. To this end, on top of the existing technical communication (TC) layer, we additionally introduce a semantic communication (SC) layer that exchanges logically meaningful clauses in knowledge bases. To make these SC and TC layers interact, we propose various measures based on the entropy of a clause in a knowledge base. These measures allow us to delineate various technical issues on SC such as a message selection problem for improving the knowledge at a receiver. Extending this, we showcase selected examples in which SC and TC layers interact with each other while taking into account constraints on physical channels.      
### 95.Computing Truncated Joint Approximate Eigenbases for Model Order Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2201.05928.pdf)
>  In this document, some elements of the theory and algorithmics corresponding to the existence and computability of approximate joint eigenpairs for finite collections of matrices with applications to model order reduction, are presented. More specifically, given a finite collection $X_1,\ldots,X_d$ of Hermitian matrices in $\mathbb{C}^{n\times n}$, a positive integer $r\ll n$, and a collection of complex numbers $\hat{x}_{j,k}\in \mathbb{C}$ for $1\leq j\leq d$, $1\leq k\leq r$. First, we study the computability of a set of $r$ vectors $w_1,\ldots,w_r\in \mathbb{C}^{n}$, such that $w_k=\arg\min_{w\in \mathbb{C}^n}\sum_{j=1}^d\|X_jw-\hat{x}_{j,k} w\|^2$ for each $1\leq k \leq r$, then we present a model order reduction procedure based on the truncated joint approximate eigenbases computed with the aforementioned techniques. Some prototypical algorithms together with some numerical examples are presented as well.      
### 96.ConvMixer: Feature Interactive Convolution with Curriculum Learning for Small Footprint and Noisy Far-field Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2201.05863.pdf)
>  Building efficient architecture in neural speech processing is paramount to success in keyword spotting deployment. However, it is very challenging for lightweight models to achieve noise robustness with concise neural operations. In a real-world application, the user environment is typically noisy and may also contain reverberations. We proposed a novel feature interactive convolutional model with merely 100K parameters to tackle this under the noisy far-field condition. The interactive unit is proposed in place of the attention module that promotes the flow of information with more efficient computations. Moreover, curriculum-based multi-condition training is adopted to attain better noise robustness. Our model achieves 98.2% top-1 accuracy on Google Speech Command V2-12 and is competitive against large transformer models under the designed noise condition.      
### 97.HARQ Optimization for Real-Time Remote Estimation in Wireless Networked Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.05838.pdf)
>  This paper analyzes wireless network control for remote estimation of linear time-invariant (LTI) dynamical systems under various Hybrid Automatic Repeat Request (HARQ) based packet retransmission schemes. In conventional HARQ, packet reliability increases gradually with additional packets; however, each retransmission maximally increases the Age of Information (AoI). A slight increase in AoI can cause severe degradation in mean squared error (MSE) performance. We optimize standard HARQ schemes by allowing partial retransmissions to increase the packet reliability gradually and limit the AoI growth. In incremental redundancy HARQ (IR-HARQ), we utilize a shorter time for retransmission, which improves the MSE performance by enabling the early arrival of fresh status updates. In Chase combining HARQ (CC-HARQ), since packet length remains fixed, we propose sending retransmission for an old update and new updates in a single time slot using non-orthogonal signaling. Non-orthogonal retransmissions increase the packet reliability without delaying the fresh updates. Using the Markov decision process formulation, we find the optimal policies of the proposed HARQ based schemes to optimize the MSE performance. We provide static and dynamic policy optimization techniques to improve the MSE performance. The simulation results show that the proposed schemes achieve better long-term average and packet-level MSE performance.      
### 98.Quantum estimation, control and learning: opportunities and challenges  [ :arrow_down: ](https://arxiv.org/pdf/2201.05835.pdf)
>  The development of estimation and control theories for quantum systems is a fundamental task for practical quantum technology. This vision article presents a brief introduction to challenging problems and potential opportunities in the emerging areas of quantum estimation, control and learning. The topics cover quantum state estimation, quantum parameter identification, quantum filtering, quantum open-loop control, quantum feedback control, machine learning for estimation and control of quantum systems, and quantum machine learning.      
### 99.A Novel Multi-Task Learning Method for Symbolic Music Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2201.05782.pdf)
>  Symbolic Music Emotion Recognition(SMER) is to predict music emotion from symbolic data, such as MIDI and MusicXML. Previous work mainly focused on learning better representation via (mask) language model pre-training but ignored the intrinsic structure of the music, which is extremely important to the emotional expression of music. In this paper, we present a simple multi-task framework for SMER, which incorporates the emotion recognition task with other emotion-related auxiliary tasks derived from the intrinsic structure of the music. The results show that our multi-task framework can be adapted to different models. Moreover, the labels of auxiliary tasks are easy to be obtained, which means our multi-task methods do not require manually annotated labels other than emotion. Conducting on two publicly available datasets (EMOPIA and VGMIDI), the experiments show that our methods perform better in SMER task. Specifically, accuracy has been increased by 4.17 absolute point to 67.58 in EMOPIA dataset, and 1.97 absolute point to 55.85 in VGMIDI dataset. Ablation studies also show the effectiveness of multi-task methods designed in this paper.      
### 100.Integrated Sensing and Communication with mmWave Massive MIMO: A Compressed Sampling Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2201.05766.pdf)
>  Integrated sensing and communication (ISAC) has opened up numerous game-changing opportunities for realizing future wireless systems. In this paper, we propose an ISAC processing framework relying on millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems. Specifically, we provide a compressed sampling (CS) perspective to facilitate ISAC processing, which can not only recover the large-scale channel state information or/and radar imaging information, but also significantly reduce pilot overhead. First, an energy-efficient widely spaced array (WSA) architecture is tailored for the radar receiver, which enhances the angular resolution of radar sensing at the cost of angular ambiguity. Then, we propose an ISAC frame structure for time-variant ISAC systems considering different timescales. The pilot waveforms are judiciously designed by taking into account both CS theories and hardware constraints. Next, we design the dedicated dictionary for WSA that serves as a building block for formulating the ISAC processing as sparse signal recovery problems. The orthogonal matching pursuit with support refinement (OMP-SR) algorithm is proposed to effectively solve the problems in the existence of the angular ambiguity. We also provide a framework for estimating and compensating the Doppler frequencies during payload data transmission to guarantee communication performances. Simulation results demonstrate the good performances of both communications and radar sensing under the proposed ISAC framework.      
### 101.Self-Adaptive Binary-Addition-Tree Algorithm-Based Novel Monte Carlo Simulation for Binary-State Network Reliability Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2201.05764.pdf)
>  The Monte Carlo simulation (MCS) is a statistical methodology used in a large number of applications. It uses repeated random sampling to solve problems with a probability interpretation to obtain high-quality numerical results. The MCS is simple and easy to develop, implement, and apply. However, its computational cost and total runtime can be quite high as it requires many samples to obtain an accurate approximation with low variance. In this paper, a novel MCS, called the self-adaptive BAT-MCS, based on the binary-adaption-tree algorithm (BAT) and our proposed self-adaptive simulation-number algorithm is proposed to simply and effectively reduce the run time and variance of the MCS. The proposed self-adaptive BAT-MCS was applied to a simple benchmark problem to demonstrate its application in network reliability. The statistical characteristics, including the expectation, variance, and simulation number, and the time complexity of the proposed self-adaptive BAT-MCS are discussed. Furthermore, its performance is compared to that of the traditional MCS extensively on a large-scale problem.      
### 102.Deep Optimal Transport on SPD Manifolds for Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2201.05745.pdf)
>  The domain adaption (DA) problem on symmetric positive definite (SPD) manifolds has raised interest in the machine learning community because of the growing potential for the SPD-matrix representations across many non-stationary applicable scenarios. This paper generalizes the joint distribution adaption (JDA) to align the source and target domains on SPD manifolds and proposes a deep network architecture, Deep Optimal Transport (DOT), using the generalized JDA and the existing deep network architectures on SPD manifolds. The specific architecture in DOT enables it to learn an approximate optimal transport (OT) solution to the DA problems on SPD manifolds. In the experiments, DOT exhibits a 2.32% and 2.92% increase on the average accuracy in two highly non-stationary cross-session scenarios in brain-computer interfaces (BCIs), respectively. The visualizational results of the source and target domains before and after the transformation also demonstrate the validity of DOT.      
