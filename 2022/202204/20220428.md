# ArXiv eess --Thu, 28 Apr 2022
### 1.Mean-square stability of linear systems over channels with random transmission delays  [ :arrow_down: ](https://arxiv.org/pdf/2204.13083.pdf)
>  This work studies the mean-square stability and stabilization problem for netwroked feedback systems. Data transmission delays in the network channels of the systems are considered. It is assumed that these delays are i.i.d. processes with given PMFs. A necessary and sufficient condition of mean-square (input-output) stability is studied for the networked feedback systems in both time-domain and frequency-domain. Furthermore, according to this condition, mean-square stabization via output feedback is studied for the networked feedback systems in terms of the state-space models.      
### 2.Electrified Autonomous Freight Benefit analysis on Fleet, Infrastructure and Grid Leveraging Grid-Electrified Mobility (GEM) Model  [ :arrow_down: ](https://arxiv.org/pdf/2204.13082.pdf)
>  This paper analyzes the potential benefit of heavy-duty vehicle (HDV) electrification and automation on fleet cost, infrastructure cost, grid, and environmental impact. In this work, we extended the vehicle electrification benefit analysis tool: Grid-Electrified Mobility (GEM) model, which had primarily been used to study light-duty passenger vehicles (LDVs), to analyze the heavy-duty vehicle electrification. The extended model is derived for freight transportation and key results and findings on the impact of freight electrification and automation are presented and discussed.      
### 3.A Uniform Framework for Diagnosis of Discrete-Event Systems with Unreliable Sensors using Linear Temporal Logic  [ :arrow_down: ](https://arxiv.org/pdf/2204.13057.pdf)
>  In this paper, we investigate the diagnosability verification problem of partially-observed discrete-event systems (DES) subject to unreliable sensors. In this setting, upon the occurrence of each event, the sensor reading may be non-deterministic due to measurement noises or possible sensor failures. Existing works on this topic mainly consider specific types of unreliable sensors such as the cases of intermittent sensors failures, permanent sensor failures or their combinations. In this work, we propose a novel \emph{uniform framework} for diagnosability of DES subject to, not only sensor failures, but also a very general class of unreliable sensors. Our approach is to use linear temporal logic (LTL) with semantics on infinite traces to describe the possible behaviors of the sensors. A new notion of $\varphi$-diagnosability is proposed as the necessary and sufficient condition for the existence of a diagnoser when the behaviors of sensors satisfy the LTL formula $\varphi$. Effective approach is provided to verify this notion. We show that, our new notion of $\varphi$-diagnosability subsumes all existing notions of robust diagnosability of DES subject to sensor failures. Furthermore, the proposed framework is user-friendly and flexible since it supports an arbitrary user-defined unreliable sensor type based on the specific scenario of the application. As examples, we provide two new notions of diagnosability, which have never been investigated in the literature, using our uniform framework.      
### 4.Differential Data-Aided Beam Training for RIS-Empowered Multi-Antenna Communications  [ :arrow_down: ](https://arxiv.org/pdf/2204.13029.pdf)
>  The Reconfigurable Intelligent Surface (RIS) constitutes one of the prominent technologies for the next generation of wireless communications. It is envisioned to enhance the signal coverage in cases when the direct link of the communication is weak. Recently, beam training based on codebook selection is proposed to obtain the optimized phase configuration of the RIS, and then, the data is transmitted and received by using the classical coherent demodulation scheme (CDS). This training approach is able to avoid the large overhead required by the channel sounding process, and it also circumvents complex optimization problems. However, the beam training still requires the transmission of some reference signals to test the different phase configurations of the codebook, which reduces the spectral efficiency. The best codeword is chosen according to the received energy of the reference signals. In this paper, the data transmission and reception based on non-CDS (NCDS) is proposed during the beam training process in order to increase the efficiency of the system, and at the same time, enable the energy measurement for the determination of the best beam for the RIS. After choosing the best codebook, NCDS is still more suitable to transmit information for high mobility scenarios as compared to the classical CDS. Analytical expressions for the Signal-to-Interference and Noise Ratio (SINR) for the non-coherent RIS-empowered system are presented. Moreover, a detailed comparison between the NCDS and CDS in terms of efficiency and complexity is also given. The extensive computer simulation results verify the accuracy of the presented analysis and showcase that the proposed system outperforms the existing solutions.      
### 5.Rapid Phase Ambiguity Elimination Methods for DOA Estimator via Hybrid Massive MIMO Receive Array  [ :arrow_down: ](https://arxiv.org/pdf/2204.12991.pdf)
>  For a sub-connected hybrid multiple-input multiple-output (MIMO) receiver with $K$ subarrays and $N$ antennas, there exists a challenging problem of how to rapidly remove phase ambiguity in only single time-slot. First, a DOA estimator of maximizing received power (Max-RP) is proposed to find the maximum value of $K$-subarray output powers, where each subarray is in charge of one sector, and the center angle of the sector corresponding to the maximum output is the estimated true DOA. To make an enhancement on precision, Max-RP plus quadratic interpolation (Max-RP-QI) method is designed. In the proposed Max-RP-QI, a quadratic interpolation scheme is adopted to interpolate the three DOA values corresponding to the largest three receive powers of Max-RP. Finally, to achieve the CRLB, a Root-MUSIC plus Max-RP-QI scheme is developed. Simulation results show that the proposed three methods eliminate the phase ambiguity during one time-slot and also show low-computational-complexities. In particular, the proposed Root-MUSIC plus Max-RP-QI scheme can reach the CRLB, and the proposed Max-RP and Max-RP-QI are still some performance losses $2dB\thicksim4dB$ compared to the CRLB.      
### 6.Energy-Efficient Dynamic Edge Computing with Electromagnetic Field Exposure Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2204.12988.pdf)
>  We present a dynamic resource allocation strategy for energy-efficient and Electromagnetic Field (EMF) exposure aware computation offloading at the wireless network edge. The goal is to maximize the overall system sum-rate of offloaded data, under stability (i.e. finite end-to-end delay), EMF exposure and system power constraints. The latter comprises end devices for uplink transmission and a Mobile Edge Host (MEH) for computation. Our proposed method, based on Lyapunov stochastic optimization, is able to achieve this goal with theoretical guarantees on asymptotic optimality, without any prior knowledge of wireless channel statistics. Although a complex long-term optimization problem is formulated, a per-slot optimization based on instantaneous realizations is derived. Moreover, the solution of the instantaneous problem is provided with closed form expressions and fast iterative procedures. Besides the theoretical analysis, numerical results assess the performance of the proposed strategy in striking the best trade-off between offloading sum-rate, power consumption, EMF exposure, and E2E delay. To the best of our knowledge, this is the first work addressing the problem of energy and exposure aware computation offloading.      
### 7.Multi-Objective Physics-Guided Recurrent Neural Networks for Identifying Non-Autonomous Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.12972.pdf)
>  While trade-offs between modeling effort and model accuracy remain a major concern with system identification, resorting to data-driven methods often leads to a complete disregard for physical plausibility. To address this issue, we propose a physics-guided hybrid approach for modeling non-autonomous systems under control. Starting from a traditional physics-based model, this is extended by a recurrent neural network and trained using a sophisticated multi-objective strategy yielding physically plausible models. While purely data-driven methods fail to produce satisfying results, experiments conducted on real data reveal substantial accuracy improvements by our approach compared to a physics-based model.      
### 8.Blending Data and Physics Against False Data Injection Attack: An Event-Triggered Moving Target Defence Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.12970.pdf)
>  Recently, data-driven detectors and physics-based Moving Target Defences (MTD) have been proposed to detect false data injection (FDI) attacks on power system state estimation. However, the uncontrollable false positive rate of the data-driven detector and the extra cost of frequent MTD usage limit their wide applications. Few works have explored the overlap between these two areas to collaboratively detect FDI attacks. To fill the gap, this paper proposes blending data-driven and physics-based approaches to enhance the detection of FDI attacks. To start, a physics-informed data-driven attack detection and identification algorithm is proposed. Following this, an MTD protocol is triggered by a positive signal from the data-driven detector and is formulated as a bilevel optimisation to robustly guarantee the effectiveness of MTD against the worst-case attack around the identified attack vector while maximising the hiddenness of MTD. To guarantee the feasibility and convergence, the bilevel nonconvex optimisation is separated into two stages, and for each stage, a semidefinite programming is derived through duality and linear matrix inequality. The simulation results verify that by blending data and physics, it can significantly enhance the detection performance while simultaneously reducing the false positive rate of the data-driven detector and the usage of MTD.      
### 9.Computationally efficient neural network classifiers for next generation closed loop neuromodulation therapy, a case study in epilepsy  [ :arrow_down: ](https://arxiv.org/pdf/2204.12938.pdf)
>  This work explores the potential utility of neural network classifiers for real-time classification of field-potential based biomarkers in next-generation responsive neuromodulation systems. Compared to classical filter-based classifiers, neural networks offer an ease of patient-specific parameter tuning, promising to reduce the burden of programming on clinicians. The paper explores a compact, feed-forward neural network architecture of only dozens of units for seizure-state classification in refractory epilepsy. The proposed classifier offers comparable accuracy to filter classifiers on clinician-labelled data, while reducing detection latency. As a trade-off to classical methods, the paper focuses on keeping the complexity of the architecture minimal, to accommodate the on-board computational constraints of implantable pulse generator systems.      
### 10.An open-source simulation package for power electronics education  [ :arrow_down: ](https://arxiv.org/pdf/2204.12924.pdf)
>  Extension of the open-source simulation package GSEIM for power electronics applications is presented. Recent developments in GSEIM, including those oriented specifically towards power electronic circuits, are described. Some examples of electrical element templates, which form a part of the GSEIM library, are discussed. Representative simulation examples in power electronics are presented to bring out important features of the simulator. Advantages of GSEIM for educational purposes are discussed. Finally, plans regarding future developments in GSEIM are presented.      
### 11.Epicardial Adipose Tissue Segmentation from CT Images with A Semi-3D Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2204.12904.pdf)
>  Epicardial adipose tissue is a type of adipose tissue located between the heart wall and a protective layer around the heart called the pericardium. The volume and thickness of epicardial adipose tissue are linked to various cardiovascular diseases. It is shown to be an independent cardiovascular disease risk factor. Fully automatic and reliable measurements of epicardial adipose tissue from CT scans could provide better disease risk assessment and enable the processing of large CT image data sets for a systemic epicardial adipose tissue study. This paper proposes a method for fully automatic semantic segmentation of epicardial adipose tissue from CT images using a deep neural network. The proposed network uses a U-Net-based architecture with slice depth information embedded in the input image to segment a pericardium region of interest, which is used to obtain an epicardial adipose tissue segmentation. Image augmentation is used to increase model robustness. Cross-validation of the proposed method yields a Dice score of 0.86 on the CT scans of 20 patients.      
### 12.Motion Compensated Three-Dimensional Frequency Selective Extrapolation for Improved Error Concealment in Video Communication  [ :arrow_down: ](https://arxiv.org/pdf/2204.12882.pdf)
>  During transmission of video data over error-prone channels the risk of getting severe image distortions due to transmission errors is ubiquitous. To deal with image distortions at decoder side, error concealment is applied. This article presents Motion Compensated Three-Dimensional Frequency Selective Extrapolation, a novel spatio-temporal error concealment algorithm. The algorithm uses fractional-pel motion estimation and compensation as initial step, being followed by the generation of a model of the distorted signal. The model generation is conducted by an enhanced version of Three-Dimensional Frequency Selective Extrapolation, an existing error concealment algorithm. Compared to this existent algorithm, the proposed one yields an improvement in concealment quality of up to 1.64 dB PSNR. Altogether, the incorporation of motion compensation and the improved model generation extends the already high extrapolation quality of the underlying Frequency Selective Extrapolation, resulting in a gain of more than 3 dB compared to other well-known error concealment algorithms.      
### 13.Resampling Images to a Regular Grid from a Non-Regular Subset of Pixel Positions Using Frequency Selective Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2204.12873.pdf)
>  Even though image signals are typically defined on a regular two-dimensional grid, there also exist many scenarios where this is not the case and the amplitude of the image signal only is available for a non-regular subset of pixel positions. In such a case, a resampling of the image to a regular grid has to be carried out. This is necessary since almost all algorithms and technologies for processing, transmitting or displaying image signals rely on the samples being available on a regular grid. Thus, it is of great importance to reconstruct the image on this regular grid so that the reconstruction comes closest to the case that the signal has been originally acquired on the regular grid. In this paper, Frequency Selective Reconstruction is introduced for solving this challenging task. This algorithm reconstructs image signals by exploiting the property that small areas of images can be represented sparsely in the Fourier domain. By further taking into account the basic properties of the Optical Transfer Function of imaging systems, a sparse model of the signal is iteratively generated. In doing so, the proposed algorithm is able to achieve a very high reconstruction quality, in terms of PSNR and SSIM as well as in terms of visual quality. Simulation results show that the proposed algorithm is able to outperform state-of-the-art reconstruction algorithms and gains of more than 1 dB PSNR are possible.      
### 14.Increasing Imaging Resolution by Non-Regular Sampling and Joint Sparse Deconvolution and Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2204.12867.pdf)
>  Increasing the resolution of image sensors has been a never ending struggle since many years. In this paper, we propose a novel image sensor layout which allows for the acquisition of images at a higher resolution and improved quality. For this, the image sensor makes use of non-regular sampling which reduces the impact of aliasing. Therewith, it allows for capturing details which would not be possible with state-of-the-art sensors of the same number of pixels. The non-regular sampling is achieved by rotating prototype pixel cells in a non-regular fashion. As not the whole area of the pixel cell is sensitive to light, a non-regular spatial integration of the incident light is obtained. Based on the sensor output data, a high-resolution image can be reconstructed by performing a deconvolution with respect to the integration area and an extrapolation of the information to the insensitive regions of the pixels. To solve this challenging task, we introduce a novel joint sparse deconvolution and extrapolation algorithm. The union of non-regular sampling and the proposed reconstruction allows for achieving a higher resolution and therewith an improved imaging quality.      
### 15.Conformer and Blind Noisy Students for Improved Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2204.12819.pdf)
>  Generative models for image restoration, enhancement, and generation have significantly improved the quality of the generated images. Surprisingly, these models produce more pleasant images to the human eye than other methods, yet, they may get a lower perceptual quality score using traditional perceptual quality metrics such as PSNR or SSIM. Therefore, it is necessary to develop a quantitative metric to reflect the performance of new algorithms, which should be well-aligned with the person's mean opinion score (MOS). Learning-based approaches for perceptual image quality assessment (IQA) usually require both the distorted and reference image for measuring the perceptual quality accurately. However, commonly only the distorted or generated image is available. In this work, we explore the performance of transformer-based full-reference IQA models. We also propose a method for IQA based on semi-supervised knowledge distillation from full-reference teacher models into blind student models using noisy pseudo-labeled data. Our approaches achieved competitive results on the NTIRE 2022 Perceptual Image Quality Assessment Challenge: our full-reference model was ranked 4th, and our blind noisy student was ranked 3rd among 70 participants, each in their respective track.      
### 16.Semi-Autonomous Electric Vehicles in Platooning Mode and Their Effects on Travel Time: A Framework for Simulation Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2204.12815.pdf)
>  Connected and Automated Vehicles (CAVs) have received a lot of attention in recent years. However, there are still numerous challenges in this field. In this paper, we investigated the effects of dynamic-flexible platooning on travel time by considering real-world trips data. For this purpose we extended the platooning capabilities of the 3DCoAutosim simulation platform, and proposed a dynamic-flexible model that we validated by creating use cases on traffic efficiency. We studied our dynamic-flexible platooning case for three electric vans with an autonomous leader, a semi-autonomous first follower with a driver, and an autonomous last follower. Results showed that the model developed in this study is efficient to investigate the effects of dynamic-flexible platooning on travel time.      
### 17.MCRB-based Performance Analysis of 6G Localization under Hardware Impairments  [ :arrow_down: ](https://arxiv.org/pdf/2204.12788.pdf)
>  Location information is expected to be the key to meeting the needs of communication and context-aware services in 6G systems. User localization is achieved based on delay and/or angle estimation using uplink or downlink pilot signals. However, hardware impairments (HWIs) distort the signals at both the transmitter and receiver sides and thus affect the localization performance. While this impact can be ignored at lower frequencies where HWIs are less severe, modeling and analysis efforts are needed for 6G to evaluate the localization degradation due to HWIs. In this work, we model various types of impairments and conduct a misspecified Cramér-Rao bound analysis to evaluate the HWI-induced performance loss. Simulation results with different types of HWIs show that each HWI leads to a different level of degradation in angle and delay estimation performance.      
### 18.RIS-aided Near-Field Localization under Phase-Dependent Amplitude Variations  [ :arrow_down: ](https://arxiv.org/pdf/2204.12783.pdf)
>  We investigate the problem of reconfigurable intelligent surface (RIS)-aided near-field localization of a user equipment (UE) served by a base station (BS) under phase-dependent amplitude variations at each RIS element. Through a misspecified Cramér-Rao bound (MCRB) analysis and a resulting lower bound (LB) on localization, we show that when the UE is unaware of amplitude variations (i.e., assumes unit-amplitude responses), severe performance penalties can arise, especially at high signal-to-noise ratios (SNRs). Leveraging Jacobi-Anger expansion to decouple range-azimuth-elevation dimensions, we develop a low-complexity approximated mismatched maximum likelihood (AMML) estimator, which is asymptotically tight to the LB. To mitigate performance loss due to model mismatch, we propose to jointly estimate the UE location and the RIS amplitude model parameters. The corresponding Cramér-Rao bound (CRB) is derived, as well as an iterative refinement algorithm, which employs the AMML method as a subroutine and alternatingly updates individual parameters of the RIS amplitude model. Simulation results indicate fast convergence and performance close to the CRB. The proposed method can successfully recover the performance loss of the AMML under a wide range of RIS parameters and effectively calibrate the RIS amplitude model online with the help of a user that has an a-priori unknown location.      
### 19.Ultra Fast Speech Separation Model with Teacher Student Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.12777.pdf)
>  Transformer has been successfully applied to speech separation recently with its strong long-dependency modeling capacity using a self-attention mechanism. However, Transformer tends to have heavy run-time costs due to the deep encoder layers, which hinders its deployment on edge devices. A small Transformer model with fewer encoder layers is preferred for computational efficiency, but it is prone to performance degradation. In this paper, an ultra fast speech separation Transformer model is proposed to achieve both better performance and efficiency with teacher student learning (T-S learning). We introduce layer-wise T-S learning and objective shifting mechanisms to guide the small student model to learn intermediate representations from the large teacher model. Compared with the small Transformer model trained from scratch, the proposed T-S learning method reduces the word error rate (WER) by more than 5% for both multi-channel and single-channel speech separation on LibriCSS dataset. Utilizing more unlabeled speech data, our ultra fast speech separation models achieve more than 10% relative WER reduction.      
### 20.Multi-task Learning-based CSI Feedback Design in Multiple Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2204.12698.pdf)
>  For frequency division duplex systems, the essential downlink channel state information (CSI) feedback includes the links of compression, feedback, decompression and reconstruction to reduce the feedback overhead. One efficient CSI feedback method is the Auto-Encoder (AE) structure based on deep learning, yet facing problems in actual deployments, such as selecting the deployment mode when deploying in a cell with multiple complex scenarios. Rather than designing an AE network with huge complexity to deal with CSI of all scenarios, a more realistic mode is to divide the CSI dataset by region/scenario and use multiple relatively simple AE networks to handle subregions' CSI. However, both require high memory capacity for user equipment (UE) and are not suitable for low-level devices. In this paper, we propose a new user-friendly-designed framework based on the latter multi-tasking mode. Via Multi-Task Learning, our framework, Single-encoder-to-Multiple-decoders (S-to-M), designs the multiple independent AEs into a joint architecture: a shared encoder corresponds to multiple task-specific decoders. We also complete our framework with GateNet as a classifier to enable the base station autonomously select the right task-specific decoder corresponding to the subregion. Experiments on the simulating multi-scenario CSI dataset demonstrate our proposed S-to-M's advantages over the other benchmark modes, i.e., significantly reducing the model complexity and the UE's memory consumption      
### 21.Nondominated-Solution-based Multiobjective-Greedy Sensor Selection for Optimal Design of Experiments  [ :arrow_down: ](https://arxiv.org/pdf/2204.12695.pdf)
>  In this study, a nondominated-solution-based multiobjective-greedy sensor selection method for the optimal design of experiments is proposed and its performance is investigated. The proposed method simultaneously considers D-, A- and E-optimality and applies the idea of Pareto ranking to select the sensor set. With the proposed method, a new sensor is iteratively added to the nondominated solutions of sensor sets, and the multiobjective functions are evaluated for new sets of sensors based on increments in the relevant optimality index. The nondominated solutions are selected from the examined solutions, and the next sensor sets are then considered. With this procedure, the multiobjective optimization of sensor selection can be conducted with reasonable computational costs. The results show that the proposed method not only gives the Pareto-optimal front of the multiobjective optimization problem but also produces sets of sensors in terms of D-, A- and E-optimality, that are superior to the sets selected by pure-greedy methods that consider only a single objective function.      
### 22.Model predictive control of agro-hydrological systems based on a two-layer neural network modeling framework  [ :arrow_down: ](https://arxiv.org/pdf/2204.12694.pdf)
>  Water scarcity is an urgent issue to be resolved and improving irrigation water-use efficiency through closed-loop control is essential. The complex agro-hydrological system dynamics, however, often pose challenges in closed-loop control applications. In this work, we propose a two-layer neural network (NN) framework to approximate the dynamics of the agro-hydrological system. To minimize the prediction error, a linear bias correction is added to the proposed model. The model is employed by a model predictive controller with zone tracking (ZMPC), which aims to keep the root zone soil moisture in the target zone while minimizing the total amount of irrigation. The performance of the proposed approximation model framework is shown to be better compared to a benchmark long-short-term-memory (LSTM) model for both open-loop and closed-loop applications. Significant computational cost reduction of the ZMPC is achieved with the proposed framework. To handle the tracking offset caused by the plant-model-mismatch of the proposed NN framework, a shrinking target zone is proposed for the ZMPC. Different hyper-parameters of the shrinking zone in the presence of noise and weather disturbances are investigated, of which the control performance is compared to a ZMPC with a time-invariant target zone.      
### 23.Study on the Fairness of Speaker Verification Systems on Underrepresented Accents in English  [ :arrow_down: ](https://arxiv.org/pdf/2204.12649.pdf)
>  Speaker verification (SV) systems are currently being used to make sensitive decisions like giving access to bank accounts or deciding whether the voice of a suspect coincides with that of the perpetrator of a crime. Ensuring that these systems are fair and do not disfavor any particular group is crucial. In this work, we analyze the performance of several state-of-the-art SV systems across groups defined by the accent of the speakers when speaking English. To this end, we curated a new dataset based on the VoxCeleb corpus where we carefully selected samples from speakers with accents from different countries. We use this dataset to evaluate system performance for several SV systems trained with VoxCeleb data. We show that, while discrimination performance is reasonably robust across accent groups, calibration performance degrades dramatically on some accents that are not well represented in the training data. Finally, we show that a simple data balancing approach mitigates this undesirable bias, being particularly effective when applied to our recently-proposed discriminative condition-aware backend.      
### 24.Impacts of Variable-Impedance-Based Power Flow Control on Renewable Energy Integration  [ :arrow_down: ](https://arxiv.org/pdf/2204.12642.pdf)
>  The electric power grid has evolved significantly over the past two decades in response to climate change. Increased levels of renewable energy generation, as a prominent feature of this evolution, have led to new congestion patterns in the transmission network. The transmission system is originally designed for conventional energy sources, with predictable flow patterns. Insufficient transfer capability in congested transmission systems results in commitment of more expensive power plants and higher levels of renewable energy curtailment. One way to mitigate congestion is adoption of power flow control through variable-impedance flexible ac transmission system (FACTS) devices. In this paper the impacts of power flow control on generation cost, carbon emissions and renewable energy curtailment are studied under a wide range of scenarios, including generation mix from major US regional transmission organizations, and different load curves, representing seasonal variations. A two-stage stochastic unit commitment, including FACTS adjustment, is used to evaluate the impacts of FACTS devices on various types and penetration levels of renewable energy. The results show that FACTS installation effectively reduces generation cost, carbon emissions, and renewable energy curtailment. Location of renewable energy resources, peak-hour demand and the system's generation mix are among the influential factors.      
### 25.Gaussian Kernel Variance For an Adaptive Learning Method on Signals Over Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2204.12629.pdf)
>  This paper discusses a special kind of a simple yet possibly powerful algorithm, called single-kernel Gradraker (SKG), which is an adaptive learning method predicting unknown nodal values in a network using known nodal values and the network structure. We aim to find out how to configure the special kind of the model in applying the algorithm. To be more specific, we focus on SKG with a Gaussian kernel and specify how to find a suitable variance for the kernel. To do so, we introduce two variables with which we are able to set up requirements on the variance of the Gaussian kernel to achieve (near-) optimal performance and can better understand how SKG works. Our contribution is that we introduce two variables as analysis tools, illustrate how predictions will be affected under different Gaussian kernels, and provide an algorithm finding a suitable Gaussian kernel for SKG with knowledge about the training network. Simulation results on real datasets are provided.      
### 26.Information-theoretic multi-time-scale partially observable systems with relevance to leukemia treatment  [ :arrow_down: ](https://arxiv.org/pdf/2204.12604.pdf)
>  Inspired by a leukemia treatment challenge, we study a partially observable non-linear stochastic system with unknown parameters, where the given time scales of the states and measurements may be distinct. Key words: Stochastic control; Non-linear systems; Partially observable systems; System identification; Biomedical systems.      
### 27.Online multi-resolution fusion of space-borne multispectral images  [ :arrow_down: ](https://arxiv.org/pdf/2204.12566.pdf)
>  Satellite imaging has a central role in monitoring, detecting and estimating the intensity of key natural phenomena. One important feature of satellite images is the trade-off between spatial/spectral resolution and their revisiting time, a consequence of design and physical constraints imposed by satellite orbit among other technical limitations. In this paper, we focus on fusing multi-temporal, multi-spectral images where data acquired from different instruments with different spatial resolutions is used. We leverage the spatial relationship between images at multiple modalities to generate high-resolution image sequences at higher revisiting rates. To achieve this goal, we formulate the fusion method as a recursive state estimation problem and study its performance in filtering and smoothing contexts. The proposed strategy clearly outperforms competing methodologies, which is shown in the paper for real data acquired by the Landsat and MODIS instruments.      
### 28.Learning Eco-Driving Strategies at Signalized Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2204.12561.pdf)
>  Signalized intersections in arterial roads result in persistent vehicle idling and excess accelerations, contributing to fuel consumption and CO2 emissions. There has thus been a line of work studying eco-driving control strategies to reduce fuel consumption and emission levels at intersections. However, methods to devise effective control strategies across a variety of traffic settings remain elusive. In this paper, we propose a reinforcement learning (RL) approach to learn effective eco-driving control strategies. We analyze the potential impact of a learned strategy on fuel consumption, CO2 emission, and travel time and compare with naturalistic driving and model-based baselines. We further demonstrate the generalizability of the learned policies under mixed traffic scenarios. Simulation results indicate that scenarios with 100% penetration of connected autonomous vehicles (CAV) may yield as high as 18% reduction in fuel consumption and 25% reduction in CO2 emission levels while even improving travel speed by 20%. Furthermore, results indicate that even 25% CAV penetration can bring at least 50% of the total fuel and emission reduction benefits.      
### 29.Multi stain graph fusion for multimodal integration in pathology  [ :arrow_down: ](https://arxiv.org/pdf/2204.12541.pdf)
>  In pathology, tissue samples are assessed using multiple staining techniques to enhance contrast in unique histologic features. In this paper, we introduce a multimodal CNN-GNN based graph fusion approach that leverages complementary information from multiple non-registered histopathology images to predict pathologic scores. We demonstrate this approach in nonalcoholic steatohepatitis (NASH) by predicting CRN fibrosis stage and NAFLD Activity Score (NAS). Primary assessment of NASH typically requires liver biopsy evaluation on two histological stains: Trichrome (TC) and hematoxylin and eosin (H&amp;E). Our multimodal approach learns to extract complementary information from TC and H&amp;E graphs corresponding to each stain while simultaneously learning an optimal policy to combine this information. We report up to 20% improvement in predicting fibrosis stage and NAS component grades over single-stain modeling approaches, measured by computing linearly weighted Cohen's kappa between machine-derived vs. pathologist consensus scores. Broadly, this paper demonstrates the value of leveraging diverse pathology images for improved ML-powered histologic assessment.      
### 30.Personalized Driving Behaviors and Fuel Economy over Realistic Commute Traffic: Modeling, Correlation, and Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2204.12540.pdf)
>  Drivers have distinctively diverse behaviors when operating vehicles in natural traffic flow, such as preferred pedal position, car-following distance, preview time headway, etc. These highly personalized behavioral variations are known to impact vehicle fuel economy qualitatively. Nevertheless, the quantitative relationship between driving behaviors and vehicle fuel consumption remains obscure. Addressing this critical missing link will contribute to the improvement of transportation sustainability, as well as understanding drivers' behavioral diversity. This study proposed an integrated microscopic driver behavior and fuel consumption model to assess and predict vehicle fuel economy with naturalistic highway and local commuting traffic data. Through extensive Monte Carlo simulations, significant correlation results are revealed between specific individual driving preferences and fuel economy over drivers' frequent commuting routes. Correlation results indicate that the differences in fuel consumption incurred by various driving behaviors, even in the same traffic conditions, can be as much as 29% for a light-duty truck and 15% for a passenger car. A Gaussian Process Regression model is further trained, validated, and tested under different traffic and vehicle conditions to predict fuel consumption based on drivers' personalized behaviors. Such a quantitative and personalized model can be used to identify and recommend fuel-friendly driving behaviors and routes, demonstrating a strong incentive for relevant stakeholders.      
### 31.Unsupervised Word Segmentation using K Nearest Neighbors  [ :arrow_down: ](https://arxiv.org/pdf/2204.13094.pdf)
>  In this paper, we propose an unsupervised kNN-based approach for word segmentation in speech utterances. Our method relies on self-supervised pre-trained speech representations, and compares each audio segment of a given utterance to its K nearest neighbors within the training set. Our main assumption is that a segment containing more than one word would occur less often than a segment containing a single word. Our method does not require phoneme discovery and is able to operate directly on pre-trained audio representations. This is in contrast to current methods that use a two-stage approach; first detecting the phonemes in the utterance and then detecting word-boundaries according to statistics calculated on phoneme patterns. Experiments on two datasets demonstrate improved results over previous single-stage methods and competitive results on state-of-the-art two-stage methods.      
### 32.Variational Kalman Filtering with Hinf-Based Correction for Robust Bayesian Learning in High Dimensions  [ :arrow_down: ](https://arxiv.org/pdf/2204.13089.pdf)
>  In this paper, we address the problem of convergence of sequential variational inference filter (VIF) through the application of a robust variational objective and Hinf-norm based correction for a linear Gaussian system. As the dimension of state or parameter space grows, performing the full Kalman update with the dense covariance matrix for a large scale system requires increased storage and computational complexity, making it impractical. The VIF approach, based on mean-field Gaussian variational inference, reduces this burden through the variational approximation to the covariance usually in the form of a diagonal covariance approximation. The challenge is to retain convergence and correct for biases introduced by the sequential VIF steps. We desire a framework that improves feasibility while still maintaining reasonable proximity to the optimal Kalman filter as data is assimilated. To accomplish this goal, a Hinf-norm based optimization perturbs the VIF covariance matrix to improve robustness. This yields a novel VIF- Hinf recursion that employs consecutive variational inference and Hinf based optimization steps. We explore the development of this method and investigate a numerical example to illustrate the effectiveness of the proposed filter.      
### 33.Quantum Compressive Sensing: Mathematical Machinery, Quantum Algorithms, and Quantum Circuitry  [ :arrow_down: ](https://arxiv.org/pdf/2204.13035.pdf)
>  Compressive sensing is a sensing protocol that facilitates reconstruction of large signals from relatively few measurements by exploiting known structures of signals of interest, typically manifested as signal sparsity. Compressive sensing's vast repertoire of applications in areas such as communications and image reconstruction stems from the traditional approach of utilizing non-linear optimization to exploit the sparsity assumption by selecting the lowest-weight (i.e. maximum sparsity) signal consistent with all acquired measurements. Recent efforts in the literature consider instead a data-driven approach, training tensor networks to learn the structure of signals of interest. The trained tensor network is updated to "project" its state onto one consistent with the measurements taken, and is then sampled site by site to "guess" the original signal. In this paper, we take advantage of this computing protocol by formulating an alternative "quantum" protocol, in which the state of the tensor network is a quantum state over a set of entangled qubits. Accordingly, we present the associated algorithms and quantum circuits required to implement the training, projection, and sampling steps on a quantum computer. We supplement our theoretical results by simulating the proposed circuits with a small, qualitative model of LIDAR imaging of earth forests. Our results indicate that a quantum, data-driven approach to compressive sensing, may have significant promise as quantum technology continues to make new leaps.      
### 34.Statistically Consistent Inverse Optimal Control for Linear-Quadratic Tracking with Random Time Horizon  [ :arrow_down: ](https://arxiv.org/pdf/2204.13013.pdf)
>  The goal of Inverse Optimal Control (IOC) is to identify the underlying objective function based on observed optimal trajectories. It provides a powerful framework to model expert's behavior, and a data-driven way to design an objective function so that the induced optimal control is adapted to a contextual environment. In this paper, we design an IOC algorithm for linear-quadratic tracking problems with random time horizon, and prove the statistical consistency of the algorithm. More specifically, the proposed estimator is the solution to a convex optimization problem, which means that the estimator does not suffer from local minima. This enables the proven statistical consistency to actually be achieved in practice. The algorithm is also verified on simulated data as well as data from a real world experiment, both in the setting of identifying the objective function of human tracking locomotion. The statistical consistency is illustrated on the synthetic data set, and the experimental results on the real data shows that we can get a good prediction on human tracking locomotion based on estimating the objective function. It shows that the theory and the model have a good performance in real practice. Moreover, the identified model can be used as a control target in personalized rehabilitation robot controller design, since the identified objective function describes personal habit and preferences.      
### 35.Capabilities and Skills in Manufacturing: A Survey Over the Last Decade of ETFA  [ :arrow_down: ](https://arxiv.org/pdf/2204.12908.pdf)
>  Industry 4.0 envisions Cyber-Physical Production Systems (CPPSs) to foster adaptive production of mass-customizable products. Manufacturing approaches based on capabilities and skills aim to support this adaptability by encapsulating machine functions and decoupling them from specific production processes. At the 2022 IEEE conference on Emerging Technologies and Factory Automation (ETFA), a special session on capability- and skill-based manufacturing is hosted for the fourth time. However, an overview on capability- and skill based systems in factory automation and manufacturing systems is missing. This paper aims to provide such an overview and give insights to this particular field of research. We conducted a concise literature survey of papers covering the topics of capabilities and skills in manufacturing from the last ten years of the ETFA conference. We found 247 papers with a notion on capabilities and skills and identified and analyzed 34 relevant papers which met this survey's inclusion criteria. In this paper, we provide (i) an overview of the research field, (ii) an analysis of the characteristics of capabilities and skills, and (iii) a discussion on gaps and opportunities.      
### 36.Low-rank Meets Sparseness: An Integrated Spatial-Spectral Total Variation Approach to Hyperspectral Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2204.12879.pdf)
>  Spatial-Spectral Total Variation (SSTV) can quantify local smoothness of image structures, so it is widely used in hyperspectral image (HSI) processing tasks. Essentially, SSTV assumes a sparse structure of gradient maps calculated along the spatial and spectral directions. In fact, these gradient tensors are not only sparse, but also (approximately) low-rank under FFT, which we have verified by numerical tests and theoretical analysis. Based on this fact, we propose a novel TV regularization to simultaneously characterize the sparsity and low-rank priors of the gradient map (LRSTV). The new regularization not only imposes sparsity on the gradient map itself, but also penalize the rank on the gradient map after Fourier transform along the spectral dimension. It naturally encodes the sparsity and lowrank priors of the gradient map, and thus is expected to reflect the inherent structure of the original image more faithfully. Further, we use LRSTV to replace conventional SSTV and embed it in the HSI processing model to improve its performance. Experimental results on multiple public data-sets with heavy mixed noise show that the proposed model can get 1.5dB improvement of PSNR.      
### 37.Uncertainty-Aware Prediction of Battery Energy Consumption for Hybrid Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2204.12825.pdf)
>  The usability of vehicles is highly dependent on their energy consumption. In particular, one of the main factors hindering the mass adoption of electric (EV), hybrid (HEV), and plug-in hybrid (PHEV) vehicles is range anxiety, which occurs when a driver is uncertain about the availability of energy for a given trip. To tackle this problem, we propose a machine learning approach for modeling the battery energy consumption. By reducing predictive uncertainty, this method can help increase trust in the vehicle's performance and thus boost its usability. Most related work focuses on physical and/or chemical models of the battery that affect the energy consumption. We propose a data-driven approach which relies on real-world datasets including battery related attributes. Our approach showed an improvement in terms of predictive uncertainty as well as in accuracy compared to traditional methods.      
### 38.Autonomous Vehicle Calibration via Linear Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.12818.pdf)
>  In navigation activities, kinematic parameters of a mobile vehicle play a significant role. Odometry is most commonly used for dead reckoning. However, the unrestricted accumulation of errors is a disadvantage using this method. As a result, it is necessary to calibrate odometry parameters to minimize the error accumulation. This paper presents a pipeline based on sequential least square programming to minimize the relative position displacement of an arbitrary landmark in consecutive time steps of a kinematic vehicle model by calibrating the parameters of applied model. Results showed that the developed pipeline produced accurate results with small datasets.      
### 39.Affine Frequency Division Multiplexing for Next Generation Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2204.12798.pdf)
>  Affine Frequency Division Multiplexing (AFDM), a new chirp-based multicarrier waveform for high mobility communications, is introduced here. AFDM is based on discrete affine Fourier transform (DAFT), a generalization of discrete Fourier transform, which is characterized by two parameters that can be adapted to better cope with doubly dispersive channels. First, we derive the explicit input-output relation in the DAFT domain showing the effect of AFDM parameters in the input-output relation. Second, we show how the DAFT parameters underlying AFDM have to be set so that the resulting DAFT domain impulse response conveys a full delay-Doppler representation of the channel. Then, we show analytically that AFDM can achieve full diversity in doubly dispersive channels, where full diversity refers to the number of multipath components separable in either the delay or the Doppler domain, due to its full delay-Doppler representation. Furthermore, we present a low complexity detection method taking advantage of zero-padding. We also propose an embedded pilot-aided channel estimation scheme for AFDM, in which both channel estimation and data detection are performed within the same AFDM frame. Finally, simulations corroborate the validity of our analytical results and show the significant performance gains of AFDM over state-of-the-art multicarrier schemes in high mobility scenarios.      
### 40.3-D generalized analytic signal associated with linear canonical transform in Clifford biquaternion domain  [ :arrow_down: ](https://arxiv.org/pdf/2204.12787.pdf)
>  The analytic signal is a useful mathematical tool. It separates qualitative and quantitative information of a signal in form of the local phase and local amplitude. The Clifford Fourier transform (CFT) plays a vital role in the representation of multidimensional signals. By generalizing the CFT to the Clifford linear canonical transform (CLCT), we present a new type of Clifford biquaternionic analytic signal. Due to the advantages of more freedom, the envelop detection problems of 3D images, with the help of this new analytic signal, can get a better visual appearance. Synthesis examples are presented to demonstrate these advantages.      
### 41.The Sparse Readout RIGEL Application Specific Integrated Circuit for Pixel Silicon Drift Detectors in Soft X-Ray Imaging Space Applications  [ :arrow_down: ](https://arxiv.org/pdf/2204.12778.pdf)
>  An Application Specific Integrated Circuit (ASIC), called RIGEL, designed for the sparse readout of a Silicon Pixel Drift Detector (PixDD) for space applications is presented.The low leakage current (less than 1 pA at +20 °C) and anode capacitance (less than 40 fF) of each pixel (300 um x 300 um) of the detector, combined with a low-noise electronics readout, allow to reach a high spectroscopic resolution performance even at room temperature. The RIGEL ASIC front-end architecture is composed by a 2-D matrix of 128 readout pixel cells (RPCs), arranged to host, in a 300 um-sided square area, a central octagonal pad (for the PixDD anode bump-bonding), and the full-analog processing chain, providing a full-shaped and stretched signal. In the chip periphery, the back-end electronics features 16 integrated 10-bits Wilkinson ADCs, the configuration register and a trigger management circuit. The characterization of a single RPC has been carried out whose features are: eight selectable peaking times from 0.5 us to 5 us, an input charge range equivalent to 30 keV, and a power consumption of less than 550 uW per channel. The RPC has been tested also with a 4x4 prototype PixDD and 167 eV Full Width at Half Maximum (FWHM) at the 5.9 keV line of 55Fe at 0°C and 1.8 us of peaking time has been measured.      
### 42.Masked Spectrogram Prediction For Self-Supervised Audio Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2204.12768.pdf)
>  Transformer-based models attain excellent results and generalize well when trained on sufficient amounts of data. However, constrained by the limited data available in the audio domain, most transformer-based models for audio tasks are finetuned from pre-trained models in other domains (e.g. image), which has a notable gap with the audio domain. Other methods explore the self-supervised learning approaches directly in the audio domain but currently do not perform well in the downstream tasks. In this paper, we present a novel self-supervised learning method for transformer-based audio models, called masked spectrogram prediction (MaskSpec), to learn powerful audio representations from unlabeled audio data (AudioSet used in this paper). Our method masks random patches of the input spectrogram and reconstructs the masked regions with an encoder-decoder architecture. Without using extra model weights or supervision, experimental results on multiple downstream datasets demonstrate MaskSpec achieves a significant performance gain against the supervised methods and outperforms the previous pre-trained models. In particular, our best model reaches the performance of 0.471 (mAP) on AudioSet, 0.854 (mAP) on OpenMIC2018, 0.982 (accuracy) on ESC-50, 0.976 (accuracy) on SCV2, and 0.823 (accuracy) on DCASE2019 Task1A respectively.      
### 43.Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?  [ :arrow_down: ](https://arxiv.org/pdf/2204.12765.pdf)
>  Recently, self-supervised learning (SSL) has demonstrated strong performance in speaker recognition, even if the pre-training objective is designed for speech recognition. In this paper, we study which factor leads to the success of self-supervised learning on speaker-related tasks, e.g. speaker verification (SV), through a series of carefully designed experiments. Our empirical results on the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a combination of mask speech prediction loss, data scale, and model size, while the SSL quantizer has a minor impact. We further employ the integrated gradients attribution method and loss landscape visualization to understand the effectiveness of self-supervised learning for speaker recognition performance.      
### 44.A Multi-Head Convolutional Neural Network With Multi-path Attention improves Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2204.12736.pdf)
>  Recently, convolutional neural networks (CNNs) and attention mechanisms have been widely used in image denoising and achieved satisfactory performance. However, the previous works mostly use a single head to receive the noisy image, limiting the richness of extracted features. Therefore, a novel CNN with multiple heads (MH) named MHCNN is proposed in this paper, whose heads will receive the input images rotated by different rotation angles. MH makes MHCNN simultaneously utilize features of rotated images to remove noise. We also present a novel multi-path attention mechanism (MPA) to integrate these features effectively. Unlike previous attention mechanisms that handle pixel-level, channel-level, and patch-level features, MPA focuses on features at the image level. Experiments show MHCNN surpasses other state-of-the-art CNN models on additive white Gaussian noise (AWGN) denoising and real-world image denoising. Its peak signal-to-noise ratio (PSNR) results are higher than other networks, such as DnCNN, BRDNet, RIDNet, PAN-Net, and CSANN. It is also demonstrated that the proposed MH with MPA mechanism can be used as a pluggable component.      
### 45.Density-preserving Deep Point Cloud Compression  [ :arrow_down: ](https://arxiv.org/pdf/2204.12684.pdf)
>  Local density of point clouds is crucial for representing local details, but has been overlooked by existing point cloud compression methods. To address this, we propose a novel deep point cloud compression method that preserves local density information. Our method works in an auto-encoder fashion: the encoder downsamples the points and learns point-wise features, while the decoder upsamples the points using these features. Specifically, we propose to encode local geometry and density with three embeddings: density embedding, local position embedding and ancestor embedding. During the decoding, we explicitly predict the upsampling factor for each point, and the directions and scales of the upsampled points. To mitigate the clustered points issue in existing methods, we design a novel sub-point convolution layer, and an upsampling block with adaptive scale. Furthermore, our method can also compress point-wise attributes, such as normal. Extensive qualitative and quantitative results on SemanticKITTI and ShapeNet demonstrate that our method achieves the state-of-the-art rate-distortion trade-off.      
### 46.Understanding A Class of Decentralized and Federated Optimization Algorithms: A Multi-Rate Feedback Control Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2204.12663.pdf)
>  Distributed algorithms have been playing an increasingly important role in many applications such as machine learning, signal processing, and control. Significant research efforts have been devoted to developing and analyzing new algorithms for various applications. In this work, we provide a fresh perspective to understand, analyze, and design distributed optimization algorithms. Through the lens of multi-rate feedback control, we show that a wide class of distributed algorithms, including popular decentralized/federated schemes, can be viewed as discretizing a certain continuous-time feedback control system, possibly with multiple sampling rates, such as decentralized gradient descent, gradient tracking, and federated averaging. This key observation not only allows us to develop a generic framework to analyze the convergence of the entire algorithm class. More importantly, it also leads to an interesting way of designing new distributed algorithms. We develop the theory behind our framework and provide examples to highlight how the framework can be used in practice.      
### 47.Discrete-Time Adaptive Control of a Class of Nonlinear Systems Using High-Order Tuners  [ :arrow_down: ](https://arxiv.org/pdf/2204.12634.pdf)
>  This paper concerns the adaptive control of a class of discrete-time nonlinear systems with all states accessible. Recently, a high-order tuner algorithm was developed for the minimization of convex loss functions with time-varying regressors in the context of an identification problem. Based on Nesterov's algorithm, the high-order tuner was shown to guarantee bounded parameter estimation when regressors vary with time, and to lead to accelerated convergence of the tracking error when regressors are constant. In this paper, we apply the high-order tuner to the adaptive control of a particular class of discrete-time nonlinear dynamical systems. First, we show that for plants of this class, the underlying dynamical error model can be causally converted to an algebraic error model. Second, we show that using this algebraic error model, the high-order tuner can be applied to provably stabilize the class of dynamical systems around a reference trajectory.      
### 48.Named Entity Recognition for Audio De-Identification  [ :arrow_down: ](https://arxiv.org/pdf/2204.12622.pdf)
>  Data anonymization is often a task carried out by humans. Automating it would reduce the cost and time required to complete this task. This paper presents a pipeline to automate the anonymization of audio data in French. We propose a pipeline, which takes audio files with their transcriptions and removes the named entities (NEs) present in the audio. Our pipeline is made up of a forced aligner, which aligns words in an audio transcript with speech and a model that performs named entity recognition (NER). Then, the audio segments that correspond to NEs are substituted with silence to anonymize audio. We compared forced aligners and NER models to find the best ones for our scenario. We evaluated our pipeline on a small hand-annotated dataset, achieving an F1 score of 0.769. This result shows that automating this task is feasible.      
### 49.Zero-Touch Network on Industrial IoT: An End-to-End Machine Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.12605.pdf)
>  Industry 4.0-enabled smart factory is expected to realize the next revolution for manufacturers. Although artificial intelligence (AI) technologies have improved productivity, current use cases belong to small-scale and single-task operations. To unbound the potential of smart factory, this paper develops zero-touch network systems for intelligent manufacturing and facilitates distributed AI applications in both training and inferring stages in a large-scale manner. The open radio access network (O-RAN) architecture is first introduced for the zero-touch platform to enable globally controlling communications and computation infrastructure capability in the field. The designed serverless framework allows intelligent and efficient learning assignments and resource allocations. Hence, requested learning tasks can be assigned to appropriate robots, and the underlying infrastructure can be used to support the learning tasks without expert knowledge. Moreover, due to the proposed network system's flexibility, powerful AI-enabled networking algorithms can be utilized to ensure service-level agreements and superior performances for factory workloads. Finally, three open research directions of backward compatibility, end-to-end enhancements, and cybersecurity are discussed for zero-touch smart factory.      
### 50.Building Change Detection using Multi-Temporal Airborne LiDAR Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.12535.pdf)
>  Building change detection is essential for monitoring urbanization, disaster assessment, urban planning and frequently updating the maps. 3D structure information from airborne light detection and ranging (LiDAR) is very effective for detecting urban changes. But the 3D point cloud from airborne LiDAR(ALS) holds an enormous amount of unordered and irregularly sparse information. Handling such data is tricky and consumes large memory for processing. Most of this information is not necessary when we are looking for a particular type of urban change. In this study, we propose an automatic method that reduces the 3D point clouds into a much smaller representation without losing the necessary information required for detecting Building changes. The method utilizes the Deep Learning(DL) model U-Net for segmenting the buildings from the background. Produced segmentation maps are then processed further for detecting changes and the results are refined using morphological methods. For the change detection task, we used multi-temporal airborne LiDAR data. The data is acquired over Stockholm in the years 2017 and 2019. The changes in buildings are classified into four types: 'newly built', 'demolished', 'taller' and 'shorter'. The detected changes are visualized in one map for better interpretation.      
### 51.A Gaussian Process Model for Opponent Prediction in Autonomous Racing  [ :arrow_down: ](https://arxiv.org/pdf/2204.12533.pdf)
>  In head-to-head racing, performing tightly constrained, but highly rewarding maneuvers, such as overtaking, require an accurate model of interactive behavior of the opposing target vehicle (TV). However, such information is not typically made available in competitive scenarios, we therefore propose to construct a prediction and uncertainty model given data of the TV from previous races. In particular, a one-step Gaussian Process (GP) model is trained on closed-loop interaction data to learn the behavior of a TV driven by an unknown policy. Predictions of the nominal trajectory and associated uncertainty are rolled out via a sampling-based approach and are used in a model predictive control (MPC) policy for the ego vehicle in order to intelligently trade-off between safety and performance when attempting overtaking maneuvers against a TV. We demonstrate the GP-based predictor in closed loop with the MPC policy in simulation races and compare its performance against several predictors from literature. In a Monte Carlo study, we observe that the GP-based predictor achieves similar win rates while maintaining safety in up to 3x more races. We finally demonstrate the prediction and control framework in real-time on hardware experiments.      
### 52.Covariance-analytic performance criteria, Hardy-Schatten norms and Wick-like ordering of cascaded systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.12519.pdf)
>  This paper is concerned with linear stochastic systems whose output is a stationary Gaussian random process related by an integral operator to a standard Wiener process at the input. We consider a performance criterion which involves the trace of an analytic function of the spectral density of the output process. This class of "covariance-analytic" cost functionals includes the usual mean square and risk-sensitive criteria as particular cases. Due to the presence of the "cost-shaping" analytic function, the performance criterion is related to higher-order Hardy-Schatten norms of the system transfer function. These norms have links with the asymptotic properties of cumulants of finite-horizon quadratic functionals of the system output and satisfy variational inequalities pertaining to system robustness to statistically uncertain inputs. In the case of strictly proper finite-dimensional systems, governed in state space by linear stochastic differential equations, we develop a method for recursively computing the Hardy-Schatten norms through a recently proposed technique of rearranging cascaded linear systems, which resembles the Wick ordering of annihilation and creation operators in quantum mechanics. The resulting computational procedure involves a recurrence sequence of solutions to algebraic Lyapunov equations and represents the covariance-analytic cost as the squared $\mathcal{H}_2$-norm of an auxiliary cascaded system. These results are also compared with an alternative approach which uses higher-order derivatives of stabilising solutions of parameter-dependent algebraic Riccati equations.      
### 53.Refining Control Barrier Functions through Hamilton-Jacobi Reachability  [ :arrow_down: ](https://arxiv.org/pdf/2204.12507.pdf)
>  Safety filters based on Control Barrier Functions (CBFs) have emerged as a practical tool for the safety-critical control of autonomous systems. These approaches encode safety through a value function and enforce safety by imposing a constraint on the time derivative of this value function. However, synthesizing a valid CBF that is not overly conservative in the presence of input constraints is a notorious challenge. In this work, we propose refining candidate CBFs using formal verification methods to obtain a valid CBF. In particular, we update an expert-synthesized or backup CBF using dynamic programming (DP) based reachability analysis. Our framework guarantees that with every DP iteration the obtained CBF is provably at least as safe as the prior iteration and converges to a valid CBF. Therefore, our proposed method can be used in-the-loop for robotic systems. We demonstrate the practicality of our method to enhance safety and/or reduce conservativeness on a range of nonlinear control-affine systems using various CBF synthesis techniques in simulation.      
### 54.Frequency Plan Design for Multibeam Satellite Constellations Using Linear Programming  [ :arrow_down: ](https://arxiv.org/pdf/2204.12494.pdf)
>  Upcoming large satellite constellations and the advent of tighter steerable beams will offer unprecedented flexibility. This new flexibility will require resource management strategies to be operated in high-dimensional and dynamic environments, as existing satellite operators are unaccustomed to operational flexibility and automation. Frequency assignment policies have the potential to drive constellations' performance in this new context, and are no exception to real-time and scalability requirements. The majority of frequency assignment methods proposed in the literature fail to fulfill these two requirements, or are unable to meet them without falling short on bandwidth and/or power efficiency. In this paper we propose a new frequency assignment method designed to prioritize operational requirements. We present an algorithm based on Integer Linear Programming (ILP) that is able to fully define a frequency plan while respecting key system constraints such as handovers and interference. We are able to encode operators' goals such as bandwidth maximization or power reduction and produce optimal or quasi-optimal plans according to such objectives. In our experiments, we find our method is able to allocate at least 50% more bandwidth and reduce power consumption by 40% compared to previous operational benchmarks. The performance advantage of our method compared to previous solutions increases with the dimensionality of the constellation; in an experiment with a 5,000-beam MEO constellation we find that we can allocate three times more bandwidth.      
