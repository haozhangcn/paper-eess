# ArXiv eess --Tue, 26 Apr 2022
### 1.Performer: A Novel PPG to ECG Reconstruction Transformer For a Digital Biomarker of Cardiovascular Disease Detection  [ :arrow_down: ](https://arxiv.org/pdf/2204.11795.pdf)
>  Cardiovascular diseases (CVDs) have become the top one cause of death; three-quarters of these deaths occur in lower-income communities. Electrocardiography (ECG), an electrical measurement capturing the cardiac activities, is a gold-standard to diagnose CVDs. However, ECG is infeasible for continuous cardiac monitoring due to its requirement for user participation. Meanwhile, photoplethysmography (PPG) is easy to collect, but the limited accuracy constrains its clinical usage. In this research, a novel Transformer-based architecture, Performer, is invented to reconstruct ECG from PPG and to create a novel digital biomarker, PPG along with its reconstructed ECG, as multiple modalities for CVD detection. This architecture, for the first time, performs Transformer sequence to sequence translation on biomedical waveforms, while also utilizing the advantages of the easily accessible PPG and the well-studied base of ECG. Shifted Patch-based Attention (SPA) is created to maximize the signal features by fetching the various sequence lengths as hierarchical stages into the training while also capturing cross-patch connections through the shifted patch mechanism. This architecture generates a state-of-the-art performance of 0.29 RMSE for reconstructing ECG from PPG, achieving an average of 95.9% diagnosis for CVDs on the MIMIC III dataset and 75.9% for diabetes on the PPG-BP dataset. Performer, along with its novel digital biomarker, offers a low-cost and non-invasive solution for continuous cardiac monitoring, only requiring the easily extractable PPG data to reconstruct the not-as-accessible ECG data. As a prove of concept, an earring wearable, named PEARL (prototype), is designed to scale up the point-of-care (POC) healthcare system.      
### 2.Planning and Control of Multi-Robot-Object Systems under Temporal Logic Tasks and Uncertain Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2204.11783.pdf)
>  We develop an algorithm for the motion and task planning of a system comprised of multiple robots and unactuated objects under tasks expressed as Linear Temporal Logic (LTL) constraints. The robots and objects evolve subject to uncertain dynamics in an obstacle-cluttered environment. The key part of the proposed solution is the intelligent construction of a coupled transition system that encodes the motion and tasks of the robots and the objects. We achieve such a construction by designing appropriate adaptive control protocols in the lower level, which guarantee the safe robot navigation/object transportation in the environment while compensating for the dynamic uncertainties. The transition system is efficiently interfaced with the temporal logic specification via a sampling-based algorithm to output a discrete path as a sequence of synchronized actions of the robots; such actions satisfy the robots' as well as the objects' specifications. The robots execute this discrete path by using the derived low level control protocol. Simulation results verify the proposed framework.      
### 3.Blind Equalization and Channel Estimation in Coherent Optical Communications Using Variational Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2204.11776.pdf)
>  We investigate the potential of adaptive blind equalizers based on variational inference for carrier recovery in optical communications. These equalizers are based on a low-complexity approximation of maximum likelihood channel estimation. We generalize the concept of variational autoencoder (VAE) equalizers to higher order modulation formats encompassing probabilistic constellation shaping (PCS), ubiquitous in optical communications, oversampling at the receiver, and dual-polarization transmission. Besides black-box equalizers based on convolutional neural networks, we propose a model-based equalizer based on a linear butterfly filter and train the filter coefficients using the variational inference paradigm. As a byproduct, the VAE also provides a reliable channel estimation. We analyze the VAE in terms of performance and flexibility over a classical additive white Gaussian noise (AWGN) channel with inter-symbol interference (ISI) and over a dispersive linear optical dual-polarization channel. We show that it can extend the application range of blind adaptive equalizers by outperforming the state-of-the-art constant-modulus algorithm (CMA) for PCS for both fixed but also time-varying channels. The evaluation is accompanied with a hyperparameter analysis.      
### 4.Multi-scale reconstruction of undersampled spectral-spatial OCT data for coronary imaging using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.11769.pdf)
>  Coronary artery disease (CAD) is a cardiovascular condition with high morbidity and mortality. Intravascular optical coherence tomography (IVOCT) has been considered as an optimal imagining system for the diagnosis and treatment of CAD. Constrained by Nyquist theorem, dense sampling in IVOCT attains high resolving power to delineate cellular structures/ features. There is a trade-off between high spatial resolution and fast scanning rate for coronary imaging. In this paper, we propose a viable spectral-spatial acquisition method that down-scales the sampling process in both spectral and spatial domain while maintaining high quality in image reconstruction. The down-scaling schedule boosts data acquisition speed without any hardware modifications. Additionally, we propose a unified multi-scale reconstruction framework, namely Multiscale- Spectral-Spatial-Magnification Network (MSSMN), to resolve highly down-scaled (compressed) OCT images with flexible magnification factors. We incorporate the proposed methods into Spectral Domain OCT (SD-OCT) imaging of human coronary samples with clinical features such as stent and calcified lesions. Our experimental results demonstrate that spectral-spatial downscaled data can be better reconstructed than data that is downscaled solely in either spectral or spatial domain. Moreover, we observe better reconstruction performance using MSSMN than using existing reconstruction methods. Our acquisition method and multi-scale reconstruction framework, in combination, may allow faster SD-OCT inspection with high resolution during coronary intervention.      
### 5.CellDefectNet: A Machine-designed Attention Condenser Network for Electroluminescence-based Photovoltaic Cell Defect Inspection  [ :arrow_down: ](https://arxiv.org/pdf/2204.11766.pdf)
>  Photovoltaic cells are electronic devices that convert light energy to electricity, forming the backbone of solar energy harvesting systems. An essential step in the manufacturing process for photovoltaic cells is visual quality inspection using electroluminescence imaging to identify defects such as cracks, finger interruptions, and broken cells. A big challenge faced by industry in photovoltaic cell visual inspection is the fact that it is currently done manually by human inspectors, which is extremely time consuming, laborious, and prone to human error. While deep learning approaches holds great potential to automating this inspection, the hardware resource-constrained manufacturing scenario makes it challenging for deploying complex deep neural network architectures. In this work, we introduce CellDefectNet, a highly efficient attention condenser network designed via machine-driven design exploration specifically for electroluminesence-based photovoltaic cell defect detection on the edge. We demonstrate the efficacy of CellDefectNet on a benchmark dataset comprising of a diversity of photovoltaic cells captured using electroluminescence imagery, achieving an accuracy of ~86.3% while possessing just 410K parameters (~13$\times$ lower than EfficientNet-B0, respectively) and ~115M FLOPs (~12$\times$ lower than EfficientNet-B0) and ~13$\times$ faster on an ARM Cortex A-72 embedded processor when compared to EfficientNet-B0.      
### 6.LightDefectNet: A Highly Compact Deep Anti-Aliased Attention Condenser Neural Network Architecture for Light Guide Plate Surface Defect Detection  [ :arrow_down: ](https://arxiv.org/pdf/2204.11765.pdf)
>  Light guide plates are essential optical components widely used in a diverse range of applications ranging from medical lighting fixtures to back-lit TV displays. An essential step in the manufacturing of light guide plates is the quality inspection of defects such as scratches, bright/dark spots, and impurities. This is mainly done in industry through manual visual inspection for plate pattern irregularities, which is time-consuming and prone to human error and thus act as a significant barrier to high-throughput production. Advances in deep learning-driven computer vision has led to the exploration of automated visual quality inspection of light guide plates to improve inspection consistency, accuracy, and efficiency. However, given the cost constraints in visual inspection scenarios, the widespread adoption of deep learning-driven computer vision methods for inspecting light guide plates has been greatly limited due to high computational requirements. In this study, we explore the utilization of machine-driven design exploration with computational and "best-practices" constraints as well as L$_1$ paired classification discrepancy loss to create LightDefectNet, a highly compact deep anti-aliased attention condenser neural network architecture tailored specifically for light guide plate surface defect detection in resource-constrained scenarios. Experiments show that LightDetectNet achieves a detection accuracy of $\sim$98.2% on the LGPSDD benchmark while having just 770K parameters ($\sim$33$\times$ and $\sim$6.9$\times$ lower than ResNet-50 and EfficientNet-B0, respectively) and $\sim$93M FLOPs ($\sim$88$\times$ and $\sim$8.4$\times$ lower than ResNet-50 and EfficientNet-B0, respectively) and $\sim$8.8$\times$ faster inference speed than EfficientNet-B0 on an embedded ARM processor.      
### 7.Feedback control for distributed ledgers: An attack mitigation policy for DAG-based DLTs  [ :arrow_down: ](https://arxiv.org/pdf/2204.11691.pdf)
>  In this paper we present a feedback approach to the design of an attack mitigation policy for DAG-based Distributed Ledgers. We develop a model to analyse the behaviour of the ledger under the so called Tips Inflation Attack and we design a control strategy to counteract this attack strategy. The efficacy of this approach is showcased through a theoretical analysis, in the form of two theorems about the stability properties of the ledger with and without the controller, and extensive Monte Carlo simulations of an agent-based model of the distributed ledger.      
### 8.Deep-learning-enabled Brain Hemodynamic Mapping Using Resting-state fMRI  [ :arrow_down: ](https://arxiv.org/pdf/2204.11669.pdf)
>  Cerebrovascular disease is a leading cause of death globally. Prevention and early intervention are known to be the most effective forms of its management. Non-invasive imaging methods hold great promises for early stratification, but at present lack the sensitivity for personalized prognosis. Resting-state functional magnetic resonance imaging (rs-fMRI), a powerful tool previously used for mapping neural activity, is available in most hospitals. Here we show that rs-fMRI can be used to map cerebral hemodynamic function and delineate impairment. By exploiting time variations in breathing pattern during rs-fMRI, deep learning enables reproducible mapping of cerebrovascular reactivity (CVR) and bolus arrive time (BAT) of the human brain using resting-state CO2 fluctuations as a natural 'contrast media'. The deep-learning network was trained with CVR and BAT maps obtained with a reference method of CO2-inhalation MRI, which included data from young and older healthy subjects and patients with Moyamoya disease and brain tumors. We demonstrate the performance of deep-learning cerebrovascular mapping in the detection of vascular abnormalities, evaluation of revascularization effects, and vascular alterations in normal aging. In addition, cerebrovascular maps obtained with the proposed method exhibited excellent reproducibility in both healthy volunteers and stroke patients. Deep-learning resting-state vascular imaging has the potential to become a useful tool in clinical cerebrovascular imaging.      
### 9.Adversarial Training-Aided Time-Varying Channel Prediction for TDD/FDD Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.11638.pdf)
>  In this paper, a time-varying channel prediction method based on conditional generative adversarial network (CPcGAN) is proposed for time division duplexing/frequency division duplexing (TDD/FDD) systems. CPcGAN utilizes a discriminator to calculate the divergence between the predicted downlink channel state information (CSI) and the real sample distributions under a conditional constraint that is previous uplink CSI. The generator of CPcGAN learns the function relationship between the conditional constraint and the predicted downlink CSI and reduces the divergence between predicted CSI and real CSI. The capability of CPcGAN fitting data distribution can capture the time-varying and multipath characteristics of the channel well. Considering the propagation characteristics of real channel, we further develop a channel prediction error indicator to determine whether the generator reaches the best state. Simulations show that the CPcGAN can obtain higher prediction accuracy and lower system bit error rate than the existing methods under the same user speeds.      
### 10.A RIS-Based Passive DOA Estimation Method for Integrated Sensing and Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2204.11626.pdf)
>  Integrated sensing and communication (ISAC) system has received growing attention, especially in the context of B5G/6G development. Combining the reconfigurable intelligent surface (RIS) with wireless communication process, a novel passive sensing technique is formulated in this paper to estimate the direction of arrival (DOA) of the targets, where the control matrix of the RIS is used to to realize the multiple measurements with only one full-functional receiving channel. Unlike the existing methods, the interference signals introduced by wireless communication are also considered. To improve the DOA estimation, a novel atomic norm-based method is proposed to remove the interference signals by the sparse reconstruction. The DOA is estimated after the interference removal by a novel Hankel-based multiple signal classification (MUSIC) method. Then, an optimization method is also developed for the measurement matrix to reduce the power interference signals and keep the measurement matrix's randomness, which guarantees the performance of the sparse reconstruction. Finally, we derive the theoretical Cramér-Rao lower bound (CRLB) for the proposed system on the DOA estimation. Simulation results show that the proposed method outperforms the existing methods in the DOA estimation and shows the corresponding CRLB with different distributions of the sensing node. The code about the proposed method is available online <a class="link-external link-https" href="https://github.com/chenpengseu/PassiveDOA-ISAC-RIS.git" rel="external noopener nofollow">this https URL</a>.      
### 11.Translating Clinical Delineation of Diabetic Foot Ulcers into Machine Interpretable Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11618.pdf)
>  Diabetic foot ulcer is a severe condition that requires close monitoring and management. For training machine learning methods to auto-delineate the ulcer, clinical staff must provide ground truth annotations. In this paper, we propose a new diabetic foot ulcers dataset, namely DFUC2022, the largest segmentation dataset where ulcer regions were manually delineated by clinicians. We assess whether the clinical delineations are machine interpretable by deep learning networks or if image processing refined contour should be used. By providing benchmark results using a selection of popular deep learning algorithms, we draw new insights into the limitations of DFU wound delineation and report on the associated issues. This paper provides some observations on baseline models to facilitate DFUC2022 Challenge in conjunction with MICCAI 2022. The leaderboard will be ranked by Dice score, where the best FCN-based method is 0.5708 and DeepLabv3+ achieved the best score of 0.6277. This paper demonstrates that image processing using refined contour as ground truth can provide better agreement with machine predicted results. DFUC2022 will be released on the 27th April 2022.      
### 12.Value of Optimal Trip and Charging Scheduling of Commercial Electric Vehicle Fleets with Vehicle-to-Grid in Future Low Inertia Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.11565.pdf)
>  The electrification of transport is seen as an important step in the global decarbonisation agenda. With such a large expected load on the power system from electric vehicles (EVs), it is important to coordinate charging in order to balance the supply and demand for electricity. Bidirectional charging, enabled through Vehicle-to-Grid (V2G) technology, will unlock significant storage capacity from stationary EVs that are plugged in. To take this concept a step further, this paper quantifies the potential revenues to be gained by a commercial EV fleet operator from simultaneously scheduling its trips on a day-ahead basis, as well as its charging. This allows the fleet to complete its trips (with user defined trip length and distance), while taking advantage of fluctuating energy and ancillary services prices. A mathematical framework for optimal trip scheduling is proposed, formulated as a mixed-integer linear program, and is applied to several relevant scenarios of the present and future British electricity system. It is demonstrated that an optimal journey start time can increase the revenue of commercial fleets by up to 38% in summer and 12% in winter. This means a single EV from the maintenance fleet can make additional annual revenue of up to £729. Flexible trip schedules are more valuable in the summer because keeping EVs plugged in during peak solar output will benefit the grid and the fleet operators the most. It was also found that a fleet of 5,000 EVs would result in the equivalent $\textrm{CO}_2$ of removing one Combined Cycle Gas Turbine from the system. This significant increase in revenue and carbon savings show this approach is worth investigating for potential future application.      
### 13.Leveraging RIS-Enabled Smart Signal Propagation for Solving Infeasible Localization Problems  [ :arrow_down: ](https://arxiv.org/pdf/2204.11538.pdf)
>  Reconfigurable intelligent surfaces (RISs) have tremendous potential for both communication and localization. While communication benefits are now well-understood, the breakthrough nature of the technology may well lie in its capability to provide location estimates when conventional approaches fail, (e.g., due to insufficient available infrastructure). A limited number of example scenarios have been identified, but an overview of possible RIS-enabled localization scenarios is still missing from the literature. In this article, we present such an overview and extend localization to include even user orientation or velocity. In particular, we consider localization scenarios with various numbers of RISs, single- or multi-antenna base stations, narrowband or wideband transmissions, and near- and farfield operation. Furthermore, we provide a short description of the general RIS operation together with radio localization fundamentals, experimental validation of a localization scheme with two RISs, as well as key research directions and open challenges specific to RIS-enabled localization and sensing.      
### 14.On Identifiable Polytope Characterization for Polytopic Matrix Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2204.11534.pdf)
>  Polytopic matrix factorization (PMF) is a recently introduced matrix decomposition method in which the data vectors are modeled as linear transformations of samples from a polytope. The successful recovery of the original factors in the generative PMF model is conditioned on the "identifiability" of the chosen polytope. In this article, we investigate the problem of determining the identifiability of a polytope. The identifiability condition requires the polytope to be permutation-and/or-sign-only invariant. We show how this problem can be efficiently solved by using a graph automorphism algorithm. In particular, we show that checking only the generating set of the linear automorphism group of a polytope, which corresponds to the automorphism group of an edge-colored complete graph, is sufficient. This property prevents checking all the elements of the permutation group, which requires factorial algorithm complexity. We demonstrate the feasibility of the proposed approach through some numerical experiments.      
### 15.Offline and online monitoring of scattered uncertain logs using uncertain linear dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.11505.pdf)
>  Monitoring the correctness of distributed cyber-physical systems is essential. We address the analysis of the log of a black-box cyber-physical system. Detecting possible safety violations can be hard when some samples are uncertain or missing. In this work, the log is made of values known with some uncertainty; in addition, we make use of an over-approximated yet expressive model, given by a non-linear extension of dynamical systems. Given an offline log, our approach is able to monitor the log against safety specifications with a limited number of false alarms. As a second contribution, we show that our approach can be used online to minimize the number of sample triggers, with the aim at energetic efficiency. We apply our approach to two benchmarks, an anesthesia model and an adaptive cruise controller.      
### 16.Brain-Computer Interfaces: Investigating the Transition from Visually Evoked to Purely Imagined Steady-State Potentials  [ :arrow_down: ](https://arxiv.org/pdf/2204.11503.pdf)
>  Brain-Computer Interfaces (BCIs) based on Steady State Visually Evoked Potentials (SSVEPs) have proven effective and provide significant accuracy and information-transfer rates. This family of strategies, however, requires external devices that provide the frequency stimuli required by the technique. This limits the scenarios in which they can be applied, especially when compared to other BCI approaches. In this work, we have investigated the possibility of obtaining frequency responses in the EEG output based on the pure visual imagination of SSVEP-eliciting stimuli. Our results show that not only that EEG signals present frequency-specific peaks related to the frequency the user is focusing on, but also that promising classification accuracy can be achieved, paving the way for a robust and reliable visual imagery BCI modality.      
### 17.Graph Convolutional Network Based Semi-Supervised Learning on Multi-Speaker Meeting Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.11501.pdf)
>  Unsupervised clustering on speakers is becoming increasingly important for its potential uses in semi-supervised learning. In reality, we are often presented with enormous amounts of unlabeled data from multi-party meetings and discussions. An effective unsupervised clustering approach would allow us to significantly increase the amount of training data without additional costs for annotations. Recently, methods based on graph convolutional networks (GCN) have received growing attention for unsupervised clustering, as these methods exploit the connectivity patterns between nodes to improve learning performance. In this work, we present a GCN-based approach for semi-supervised learning. Given a pre-trained embedding extractor, a graph convolutional network is trained on the labeled data and clusters unlabeled data with "pseudo-labels". We present a self-correcting training mechanism that iteratively runs the cluster-train-correct process on pseudo-labels. We show that this proposed approach effectively uses unlabeled data and improves speaker recognition accuracy.      
### 18.Self-supervision versus synthetic datasets: which is the lesser evil in the context of video denoising?  [ :arrow_down: ](https://arxiv.org/pdf/2204.11493.pdf)
>  Supervised training has led to state-of-the-art results in image and video denoising. However, its application to real data is limited since it requires large datasets of noisy-clean pairs that are difficult to obtain. For this reason, networks are often trained on realistic synthetic data. More recently, some self-supervised frameworks have been proposed for training such denoising networks directly on the noisy data without requiring ground truth. On synthetic denoising problems supervised training outperforms self-supervised approaches, however in recent years the gap has become narrower, especially for video. In this paper, we propose a study aiming to determine which is the best approach to train denoising networks for real raw videos: supervision on synthetic realistic data or self-supervision on real data. A complete study with quantitative results in case of natural videos with real motion is impossible since no dataset with clean-noisy pairs exists. We address this issue by considering three independent experiments in which we compare the two frameworks. We found that self-supervision on the real data outperforms supervision on synthetic data, and that in normal illumination conditions the drop in performance is due to the synthetic ground truth generation, not the noise model.      
### 19.Mitigation of Cyberattacks through Battery Storage for Stable Microgrid Operation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11473.pdf)
>  In this paper, we present a mitigation methodology that leverages battery energy storage system (BESS) resources in coordination with microgrid (MG) ancillary services to maintain power system operations during cyberattacks. The control of MG agents is achieved in a distributed fashion, and once a misbehaving agent is detected, the MG's mode supervisory controller (MSC) isolates the compromised agent and initiates self-healing procedures to support the power demand and restore the compromised agent. Our results demonstrate the practicality of the proposed attack mitigation strategy and how grid resilience can be improved using BESS synergies. Simulations are performed on a modified version of the Canadian urban benchmark distribution model.      
### 20.High-Efficiency Lossy Image Coding Through Adaptive Neighborhood Information Aggregation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11448.pdf)
>  Questing for lossy image coding (LIC) with superior efficiency on both compression performance and computation throughput is challenging. The vital factor behind is how to intelligently explore Adaptive Neighborhood Information Aggregation (ANIA) in transform and entropy coding modules. To this aim, Integrated Convolution and Self-Attention (ICSA) unit is first proposed to form content-adaptive transform to dynamically characterize and embed neighborhood information conditioned on the input. Then a Multistage Context Model (MCM) is developed to stagewisely execute context prediction using necessary neighborhood elements for accurate and parallel entropy probability estimation. Both ICSA and MCM are stacked under a Variational Auto-Encoder (VAE) architecture to derive rate-distortion optimized compact representation of input image via end-to-end training. Our method reports the superior compression performance surpassing the VVC Intra with $\approx$15% BD-rate improvement averaged across Kodak, CLIC and Tecnick datasets; and also demonstrates $\approx$10$\times$ speedup of image decoding when compared with other notable learned LIC approaches. All materials are made publicly accessible at <a class="link-external link-https" href="https://njuvision.github.io/TinyLIC" rel="external noopener nofollow">this https URL</a> for reproducible research.      
### 21.BCI: Breast Cancer Immunohistochemical Image Generation through Pyramid Pix2pix  [ :arrow_down: ](https://arxiv.org/pdf/2204.11425.pdf)
>  The evaluation of human epidermal growth factor receptor 2 (HER2) expression is essential to formulate a precise treatment for breast cancer. The routine evaluation of HER2 is conducted with immunohistochemical techniques (IHC), which is very expensive. Therefore, for the first time, we propose a breast cancer immunohistochemical (BCI) benchmark attempting to synthesize IHC data directly with the paired hematoxylin and eosin (HE) stained images. The dataset contains 4870 registered image pairs, covering a variety of HER2 expression levels. Based on BCI, as a minor contribution, we further build a pyramid pix2pix image generation method, which achieves better HE to IHC translation results than the other current popular algorithms. Extensive experiments demonstrate that BCI poses new challenges to the existing image translation research. Besides, BCI also opens the door for future pathology studies in HER2 expression evaluation based on the synthesized IHC images. BCI dataset can be downloaded from <a class="link-external link-https" href="https://bupt-ai-cz.github.io/BCI" rel="external noopener nofollow">this https URL</a>.      
### 22.An Online Stochastic Optimization Approach for Insulin Intensification in Type 2 Diabetes with Attention to Pseudo-Hypoglycemia  [ :arrow_down: ](https://arxiv.org/pdf/2204.11380.pdf)
>  Upon the initiation of insulin treatment for subjects with type 2 diabetes (T2D), symptoms of hypoglycemia can be experienced by the subjects depending on how quick their blood glucose concentrations are reduced. This phenomena is known as relative hypoglycemia or pseudo-hypoglycemia (PHG) and it is different from patient to patient. In this paper, we propose a model free strategy to calculate insulin doses for T2D which mitigates PHG. The strategy makes use of a feedback rating mechanism obtained from the subjects. The proposed strategy tunes the parameters of a proposed control law by using an online stochastic optimization approach for a defined cost function. The strategy uses gradient estimates obtained by a Recursive Least Square (RLS) scheme in an adaptive moment estimation based approach named AdaBelief. The performance of the insulin calculation strategy is demonstrated and compared with current insulin calculation strategies using simulations with three different models.      
### 23.Deep Learning for Medical Image Registration: A Comprehensive Review  [ :arrow_down: ](https://arxiv.org/pdf/2204.11341.pdf)
>  Image registration is a critical component in the applications of various medical image analyses. In recent years, there has been a tremendous surge in the development of deep learning (DL)-based medical image registration models. This paper provides a comprehensive review of medical image registration. Firstly, a discussion is provided for supervised registration categories, for example, fully supervised, dual supervised, and weakly supervised registration. Next, similarity-based as well as generative adversarial network (GAN)-based registration are presented as part of unsupervised registration. Deep iterative registration is then described with emphasis on deep similarity-based and reinforcement learning-based registration. Moreover, the application areas of medical image registration are reviewed. This review focuses on monomodal and multimodal registration and associated imaging, for instance, X-ray, CT scan, ultrasound, and MRI. The existing challenges are highlighted in this review, where it is shown that a major challenge is the absence of a training dataset with known transformations. Finally, a discussion is provided on the promising future research areas in the field of DL-based medical image registration.      
### 24.A Markovian Model for the Spread of the SARS-CoV-2 Virus  [ :arrow_down: ](https://arxiv.org/pdf/2204.11317.pdf)
>  We propose a Markovian stochastic approach to model the spread of a SARS-CoV-2-like infection within a closed group of humans. The model takes the form of a Partially Observable Markov Decision Process (POMDP), whose states are given by the number of subjects in different health conditions. The model also exposes the different parameters that have an impact on the spread of the disease and the various decision variables that can be used to control it (e.g, social distancing, number of tests administered to single out infected subjects). The model describes the stochastic phenomena that underlie the spread of the epidemic and captures, in the form of deterministic parameters, some fundamental limitations in the availability of resources (hospital beds and test swabs). The model lends itself to different uses. For a given control policy, it is possible to verify if it satisfies an analytical property on the stochastic evolution of the state (e.g., to compute probability that the hospital beds will reach a fill level, or that a specified percentage of the population will die). If the control policy is not given, it is possible to apply POMDP techniques to identify an optimal control policy that fulfils some specified probabilistic goals. Whilst the paper primarily aims at the model description, we show with numeric examples some of its potential applications.      
### 25.Colorectal cancer survival prediction using deep distribution based multiple-instance learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.11294.pdf)
>  Several deep learning algorithms have been developed to predict survival of cancer patients using whole slide images (WSIs).However, identification of image phenotypes within the WSIs that are relevant to patient survival and disease progression is difficult for both clinicians, and deep learning algorithms. Most deep learning based Multiple Instance Learning (MIL) algorithms for survival prediction use either top instances (e.g., maxpooling) or top/bottom instances (e.g., MesoNet) to identify image phenotypes. In this study, we hypothesize that wholistic information of the distribution of the patch scores within a WSI can predict the cancer survival better. We developed a distribution based multiple-instance survival learning algorithm (DeepDisMISL) to validate this hypothesis. We designed and executed experiments using two large international colorectal cancer WSIs datasets - MCO CRC and TCGA COAD-READ. Our results suggest that the more information about the distribution of the patch scores for a WSI, the better is the prediction performance. Including multiple neighborhood instances around each selected distribution location (e.g., percentiles) could further improve the prediction. DeepDisMISL demonstrated superior predictive ability compared to other recently published, state-of-the-art algorithms. Furthermore, our algorithm is interpretable and could assist in understanding the relationship between cancer morphological phenotypes and patients cancer survival risk.      
### 26.Improved far-field speech recognition using Joint Variational Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2204.11286.pdf)
>  Automatic Speech Recognition (ASR) systems suffer considerably when source speech is corrupted with noise or room impulse responses (RIR). Typically, speech enhancement is applied in both mismatched and matched scenario training and testing. In matched setting, acoustic model (AM) is trained on dereverberated far-field features while in mismatched setting, AM is fixed. In recent past, mapping speech features from far-field to close-talk using denoising autoencoder (DA) has been explored. In this paper, we focus on matched scenario training and show that the proposed joint VAE based mapping achieves a significant improvement over DA. Specifically, we observe an absolute improvement of 2.5% in word error rate (WER) compared to DA based enhancement and 3.96% compared to AM trained directly on far-field filterbank features.      
### 27.Unsupervised Learning Discriminative MIG Detectors in Nonhomogeneous Clutter  [ :arrow_down: ](https://arxiv.org/pdf/2204.11278.pdf)
>  Principal component analysis (PCA) is a common used pattern analysis method that maps high-dimensional data into a lower-dimensional space maximizing the data variance, that results in the promotion of separability of data. Inspired by the principle of PCA, a novel type of learning discriminative matrix information geometry (MIG) detectors in the unsupervised scenario are developed, and applied to signal detection in nonhomogeneous environments. Hermitian positive-definite (HPD) matrices can be used to model the sample data, while the clutter covariance matrix is estimated by the geometric mean of a set of secondary HPD matrices. We define a projection that maps the HPD matrices in a high-dimensional manifold to a low-dimensional and more discriminative one to increase the degree of separation of HPD matrices by maximizing the data variance. Learning a mapping can be formulated as a two-step mini-max optimization problem in Riemannian manifolds, which can be solved by the Riemannian gradient descent algorithm. Three discriminative MIG detectors are illustrated with respect to different geometric measures, i.e., the Log-Euclidean metric, the Jensen--Bregman LogDet divergence and the symmetrized Kullback--Leibler divergence. Simulation results show that performance improvements of the novel MIG detectors can be achieved compared with the conventional detectors and their state-of-the-art counterparts within nonhomogeneous environments.      
### 28.Optimization-Based Ramping Reserve Allocation in AGC Avoiding Counterproductive Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11270.pdf)
>  It has been observed in practice that Battery Energy Storage Systems (BESSs) may not always contribute to the minimization of Area Control Error (ACE). This phenomenon of "counterproductive regulation" is open to a number of interpretations. For one thing, cost-effective coordination of distributed BESSs has not been well-established and requires much more research efforts. For another, the BESSs could be driven into an inefficient operation due to miscalculated ACE. Moreover, some BESSs may have to operate in the opposite direction than desired in order to recover an energy-neutral state. Leveraging corrected ACE signals, this paper explores a novel scheme termed Optimization-based Ramping Reserve Allocation (ORRA) to coordinate a group of BESSs in Automatic Generation Control (AGC). The underlying methodology is to counteract net-load forecasting errors by providing only ramping reserves rather than capacity reserves to AGC, based on which an online optimization problem is formulated to minimize the associated battery usage cost of all nodes. The corresponding optimization algorithm is purely distributed and can guarantee fair and near-optimal allocation in real-time while avoiding those counterproductive behaviors mentioned above. Simulations on a modified IEEE 14-bus system are performed to illustrate the effectiveness of ORRA in both allocation and AGC enhancement.      
### 29.Improving the Naturalness of Simulated Conversations for End-to-End Neural Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2204.11232.pdf)
>  This paper investigates a method for simulating natural conversation in the model training of end-to-end neural diarization (EEND). Due to the lack of any annotated real conversational dataset, EEND is usually pretrained on a large-scale simulated conversational dataset first and then adapted to the target real dataset. Simulated datasets play an essential role in the training of EEND, but as yet there has been insufficient investigation into an optimal simulation method. We thus propose a method to simulate natural conversational speech. In contrast to conventional methods, which simply combine the speech of multiple speakers, our method takes turn-taking into account. We define four types of speaker transition and sequentially arrange them to simulate natural conversations. The dataset simulated using our method was found to be statistically similar to the real dataset in terms of the silence and overlap ratios. The experimental results on two-speaker diarization using the CALLHOME and CSJ datasets showed that the simulated dataset contributes to improving the performance of EEND.      
### 30.Experimental Platform for Boundary Control of Mechanical Frenkel-Kontorova Model  [ :arrow_down: ](https://arxiv.org/pdf/2204.11230.pdf)
>  In this paper, we present a laboratory mechatronic platform for experimental verification and demonstration of various dynamical and control system phenomena exhibited by the Frenkel-Kontorova (FK) model -- a spatially discretized version of the sine-Gordon equation. The platform consists of an array of torsionally coupled pendulums pivoting around a single shaft that can be controlled through the motors at boundaries while all angles are read electronically. We first introduce and describe the platform, providing details of its mechatronic design and software architecture. All the files are freely shared with the research community under an open-source license through a public repository. The motivation for this sharing is to help reproducibility or research -- the platform can be useful for others as a testbed for control algorithms for this class of dynamical systems, for instance, distributed control or control of flexible structures. In the second part of the paper, we then showcase the platform using two control problem formulations tailored to the FK model -- we discuss the practical motivation for studying these problems, propose (some) methods for solving them and demonstrate the functionality using experiments. In particular, the first control formulation deals with non-collocated control/regulation of a single pendulum through one boundary of the array in the presence of a disturbance sent from the other boundary; the second control problem consists in synchronizing pendulums' angular speeds. Some other motivated problems can certainly be formulated, solved, and demonstrated using the proposed platform.      
### 31.PUERT: Probabilistic Under-sampling and Explicable Reconstruction Network for CS-MRI  [ :arrow_down: ](https://arxiv.org/pdf/2204.11189.pdf)
>  Compressed Sensing MRI (CS-MRI) aims at reconstructing de-aliased images from sub-Nyquist sampling k-space data to accelerate MR Imaging, thus presenting two basic issues, i.e., where to sample and how to reconstruct. To deal with both problems simultaneously, we propose a novel end-to-end Probabilistic Under-sampling and Explicable Reconstruction neTwork, dubbed PUERT, to jointly optimize the sampling pattern and the reconstruction network. Instead of learning a deterministic mask, the proposed sampling subnet explores an optimal probabilistic sub-sampling pattern, which describes independent Bernoulli random variables at each possible sampling point, thus retaining robustness and stochastics for a more reliable CS reconstruction. A dynamic gradient estimation strategy is further introduced to gradually approximate the binarization function in backward propagation, which efficiently preserves the gradient information and further improves the reconstruction quality. Moreover, in our reconstruction subnet, we adopt a model-based network design scheme with high efficiency and interpretability, which is shown to assist in further exploitation for the sampling subnet. Extensive experiments on two widely used MRI datasets demonstrate that our proposed PUERT not only achieves state-of-the-art results in terms of both quantitative metrics and visual quality but also yields a sub-sampling pattern and a reconstruction model that are both customized to training data.      
### 32.Few-Shot Speaker Identification Using Depthwise Separable Convolutional Network with Channel Attention  [ :arrow_down: ](https://arxiv.org/pdf/2204.11180.pdf)
>  Although few-shot learning has attracted much attention from the fields of image and audio classification, few efforts have been made on few-shot speaker identification. In the task of few-shot learning, overfitting is a tough problem mainly due to the mismatch between training and testing conditions. In this paper, we propose a few-shot speaker identification method which can alleviate the overfitting problem. In the proposed method, the model of a depthwise separable convolutional network with channel attention is trained with a prototypical loss function. Experimental datasets are extracted from three public speech corpora: Aishell-2, VoxCeleb1 and TORGO. Experimental results show that the proposed method exceeds state-of-the-art methods for few-shot speaker identification in terms of accuracy and F-score.      
### 33.Virtual Rings on Highways: Traffic Control by Connected Automated Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2204.11177.pdf)
>  This work gives introduction to traffic control by connected automated vehicles. The influence of vehicle control on vehicular traffic and traffic control strategies are discussed and compared. It is highlighted that vehicle-to-everything connectivity allows connected automated vehicles to access the state of the traffic behind them such that feedback can be utilized to mitigate evolving congestions. Numerical simulations demonstrate that such connectivity-based traffic control is beneficial for smoothness and energy efficiency of highway traffic. The dynamics and stability of traffic flow, under the proposed controllers, are analyzed in detail to construct stability charts that guide the selection of stabilizing control gains.      
### 34.Gabor is Enough: Interpretable Deep Denoising with a Gabor Synthesis Dictionary Prior  [ :arrow_down: ](https://arxiv.org/pdf/2204.11146.pdf)
>  Image processing neural networks, natural and artificial, have a long history with orientation-selectivity, often described mathematically as Gabor filters. Gabor-like filters have been observed in the early layers of CNN classifiers and even throughout low-level image processing networks. In this work, we take this observation to the extreme and explicitly constrain the filters of a natural-image denoising CNN to be learned 2D real Gabor filters. Surprisingly, we find that the proposed network (GDLNet) can achieve near state-of-the-art denoising performance amongst popular fully convolutional neural networks, with only a fraction of the learned parameters. We further verify that this parameterization maintains the noise-level generalization (training vs. inference mismatch) characteristics of the base network, and investigate the contribution of individual Gabor filter parameters to the performance of the denoiser. We present positive findings for the interpretation of dictionary learning networks as performing accelerated sparse-coding via the importance of untied learned scale parameters between network layers. Our network's success suggests that representations used by low-level image processing CNNs can be as simple and interpretable as Gabor filterbanks.      
### 35.Multi-sensor Suboptimal Fusion Student's $t$ Filter  [ :arrow_down: ](https://arxiv.org/pdf/2204.11098.pdf)
>  A multi-sensor fusion Student's $t$ filter is proposed for time-series recursive estimation in the presence of heavy-tailed process and measurement noises. Driven from an information-theoretic optimization, the approach extends the single sensor Student's $t$ Kalman filter based on the suboptimal arithmetic average (AA) fusion approach. To ensure computationally efficient, closed-form $t$ density recursion, reasonable approximation has been used in both local-sensor filtering and inter-sensor fusion calculation. The overall framework accommodates any Gaussian-oriented fusion approach such as the covariance intersection (CI). Simulation demonstrates the effectiveness of the proposed multi-sensor AA fusion-based $t$ filter in dealing with outliers as compared with the classic Gaussian estimator, and the advantage of the AA fusion in comparison with the CI approach and the augmented measurement fusion.      
### 36.Intelligent Reflecting Surface Enabled Sensing: Cramér-Rao Lower Bound Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.11071.pdf)
>  This paper investigates intelligent reflecting surface (IRS) enabled non-line-of-sight (NLoS) wireless sensing, in which an IRS is dedicatedly deployed to assist an access point (AP) to sense a target at its NLoS region. It is assumed that the AP is equipped with multiple antennas and the IRS is equipped with a uniform linear array. The AP aims to estimate the target's direction-of-arrival (DoA) with respect to the IRS based on the echo signal from the AP-IRS-target-IRS-AP link. Under this setup, we jointly design the transmit beamforming at the AP and the reflective beamforming at the IRS to minimize the DoA estimation error in terms of Cramér-Rao lower bound (CRLB). Towards this end, we first obtain the closed-form expression of CRLB for DoA estimation. Next, we optimize the joint beamforming design to minimize the obtained CRLB, via alternating optimization, semi-definite relaxation, and successive convex approximation. Finally, numerical results show that the proposed design based on CRLB minimization achieves improved sensing performance in terms of lower estimation mean squared error (MSE), as compared to the traditional schemes with signal-to-noise ratio maximization and separate beamforming designs.      
### 37.Transformation Invariant Cancerous Tissue Classification Using Spatially Transformed DenseNet  [ :arrow_down: ](https://arxiv.org/pdf/2204.11066.pdf)
>  In this work, we introduce a spatially transformed DenseNet architecture for transformation invariant classification of cancer tissue. Our architecture increases the accuracy of the base DenseNet architecture while adding the ability to operate in a transformation invariant way while simultaneously being simpler than other models that try to provide some form of invariance.      
### 38.Class Balanced PixelNet for Neurological Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11048.pdf)
>  In this paper, we propose an automatic brain tumor segmentation approach (e.g., PixelNet) using a pixel-level convolutional neural network (CNN). The model extracts feature from multiple convolutional layers and concatenate them to form a hyper-column where samples a modest number of pixels for optimization. Hyper-column ensures both local and global contextual information for pixel-wise predictors. The model confirms the statistical efficiency by sampling a few pixels in the training phase where spatial redundancy limits the information learning among the neighboring pixels in conventional pixel-level semantic segmentation approaches. Besides, label skewness in training data leads the convolutional model often converge to certain classes which is a common problem in the medical dataset. We deal with this problem by selecting an equal number of pixels for all the classes in sampling time. The proposed model has achieved promising results in brain tumor and ischemic stroke lesion segmentation datasets.      
### 39.Heterogeneous Separation Consistency Training for Adaptation of Unsupervised Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11032.pdf)
>  Recently, supervised speech separation has made great progress. However, limited by the nature of supervised training, most existing separation methods require ground-truth sources and are trained on synthetic datasets. This ground-truth reliance is problematic, because the ground-truth signals are usually unavailable in real conditions. Moreover, in many industry scenarios, the real acoustic characteristics deviate far from the ones in simulated datasets. Therefore, the performance usually degrades significantly when applying the supervised speech separation models to real applications. To address these problems, in this study, we propose a novel separation consistency training, termed SCT, to exploit the real-world unlabeled mixtures for improving cross-domain unsupervised speech separation in an iterative manner, by leveraging upon the complementary information obtained from heterogeneous (structurally distinct but behaviorally complementary) models. SCT follows a framework using two heterogeneous neural networks (HNNs) to produce high confidence pseudo labels of unlabeled real speech mixtures. These labels are then updated, and used to refine the HNNs to produce more reliable consistent separation results for real mixture pseudo-labeling. To maximally utilize the large complementary information between different separation networks, a cross-knowledge adaptation is further proposed. Together with simulated dataset, those real mixtures with high confidence pseudo labels are then used to update the HNN separation models iteratively. In addition, we find that combing the heterogeneous separation outputs by a simple linear fusion can further slightly improve the final system performance.      
### 40.Improving Self-Supervised Learning-based MOS Prediction Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.11030.pdf)
>  MOS (Mean Opinion Score) is a subjective method used for the evaluation of a system's quality. Telecommunications (for voice and video), and speech synthesis systems (for generated speech) are a few of the many applications of the method. While MOS tests are widely accepted, they are time-consuming and costly since human input is required. In addition, since the systems and subjects of the tests differ, the results are not really comparable. On the other hand, a large number of previous tests allow us to train machine learning models that are capable of predicting MOS value. By automatically predicting MOS values, both the aforementioned issues can be resolved. <br>The present work introduces data-, training- and post-training specific improvements to a previous self-supervised learning-based MOS prediction model. We used a wav2vec 2.0 model pre-trained on LibriSpeech, extended with LSTM and non-linear dense layers. We introduced transfer learning, target data preprocessing a two- and three-phase training method with different batch formulations, dropout accumulation (for larger batch sizes) and quantization of the predictions. <br>The methods are evaluated using the shared synthetic speech dataset of the first Voice MOS challenge.      
### 41.Federated Learning with Lossy Distributed Source Coding: Analysis and Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.10985.pdf)
>  Recently, federated learning (FL), which replaces data sharing with model sharing, has emerged as an efficient and privacy-friendly paradigm for machine learning (ML). A main challenge of FL is its huge uplink communication cost. In this paper, we tackle this challenge from an information-theoretic perspective. Specifically, we put forth a distributed source coding (DSC) framework for FL uplink, which unifies the encoding, transmission, and aggregation of the local updates as a lossy DSC problem, thus providing a systematic way to exploit the correlation between local updates to improve the uplink efficiency. Under this DSC-FL framework, we propose an FL uplink scheme based on the modified Berger-Tung coding (MBTC), which supports separate encoding and joint decoding by modifying the achievability scheme of the Berger-Tung inner bound. The achievable region of the MBTC-based uplink scheme is also derived. To unleash the potential of the MBTC-based FL scheme, we carry out a convergence analysis and then formulate a convergence rate maximization problem to optimize the parameters of MBTC. To solve this problem, we develop two algorithms, respectively for small- and large-scale FL systems, based on the majorization-minimization (MM) technique. Numerical results demonstrate the superiority of the MBTC-based FL scheme in terms of aggregation distortion, convergence performance, and communication cost, revealing the great potential of the DSC-FL framework.      
### 42.Deep Reinforcement Learning-based Radio Resource Allocation and Beam Management under Location Uncertainty in 5G mmWave Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.10984.pdf)
>  Millimeter Wave (mmWave) is an important part of 5G new radio (NR), in which highly directional beams are adapted to compensate for the substantial propagation loss based on UE locations. However, the location information may have some errors such as GPS errors. In any case, some uncertainty, and localization error is unavoidable in most settings. Applying these distorted locations for clustering will increase the error of beam management. Meanwhile, the traffic demand may change dynamically in the wireless environment. Therefore, a scheme that can handle both the uncertainty of localization and dynamic radio resource allocation is needed. In this paper, we propose a UK-means-based clustering and deep reinforcement learning-based resource allocation algorithm (UK-DRL) for radio resource allocation and beam management in 5G mmWave networks. We first apply UK-means as the clustering algorithm to mitigate the localization uncertainty, then deep reinforcement learning (DRL) is adopted to dynamically allocate radio resources. Finally, we compare the UK-DRL with K-means-based clustering and DRL-based resource allocation algorithm (K-DRL), the simulations show that our proposed UK-DRL-based method achieves 150% higher throughput and 61.5% lower delay compared with K-DRL when traffic load is 4Mbps.      
### 43.Federated Contrastive Learning for Volumetric Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.10983.pdf)
>  Supervised deep learning needs a large amount of labeled data to achieve high performance. However, in medical imaging analysis, each site may only have a limited amount of data and labels, which makes learning ineffective. Federated learning (FL) can help in this regard by learning a shared model while keeping training data local for privacy. Traditional FL requires fully-labeled data for training, which is inconvenient or sometimes infeasible to obtain due to high labeling cost and the requirement of expertise. Contrastive learning (CL), as a self-supervised learning approach, can effectively learn from unlabeled data to pre-train a neural network encoder, followed by fine-tuning for downstream tasks with limited annotations. However, when adopting CL in FL, the limited data diversity on each client makes federated contrastive learning (FCL) ineffective. In this work, we propose an FCL framework for volumetric medical image segmentation with limited annotations. More specifically, we exchange the features in the FCL pre-training process such that diverse contrastive data are provided to each site for effective local CL while keeping raw data private. Based on the exchanged features, global structural matching further leverages the structural similarity to align local features to the remote ones such that a unified feature space can be learned among different sites. Experiments on a cardiac MRI dataset show the proposed framework substantially improves the segmentation performance compared with state-of-the-art techniques.      
### 44.Evaluation of Multi-Scale Multiple Instance Learning to Improve Thyroid Cancer Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.10942.pdf)
>  Thyroid cancer is currently the fifth most common malignancy diagnosed in women. Since differentiation of cancer sub-types is important for treatment and current, manual methods are time consuming and subjective, automatic computer-aided differentiation of cancer types is crucial. Manual differentiation of thyroid cancer is based on tissue sections, analysed by pathologists using histological features. Due to the enormous size of gigapixel whole slide images, holistic classification using deep learning methods is not feasible. Patch based multiple instance learning approaches, combined with aggregations such as bag-of-words, is a common approach. This work's contribution is to extend a patch based state-of-the-art method by generating and combining feature vectors of three different patch resolutions and analysing three distinct ways of combining them. The results showed improvements in one of the three multi-scale approaches, while the others led to decreased scores. This provides motivation for analysis and discussion of the individual approaches.      
### 45.Parallel Synthesis for Autoregressive Speech Generation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11806.pdf)
>  Autoregressive models have achieved outstanding performance in neural speech synthesis tasks. Though they can generate highly natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance's length, leading to low efficiency. Many works were dedicated to generating the whole speech time sequence in parallel and then proposed GAN-based, flow-based, and score-based models. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is first split into different frequency subbands. The proposed model generates a subband conditioned on the previously generated one. A full band speech can then be reconstructed by using these generated subbands and a synthesis filter bank. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance's length but the number of subbands/bits. The inference efficiency is hence significantly increased. Besides, a post-filter is employed to sample audio signals from output posteriors, and its training objective is designed based on the characteristics of the proposed autoregressive methods. The experimental results show that the proposed model is able to synthesize speech faster than real-time without GPU acceleration. Compared with the baseline autoregressive and non-autoregressive models, the proposed model achieves better MOS and shows its good generalization ability while synthesizing 44 kHz speech or utterances from unseen speakers.      
### 46.SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2204.11792.pdf)
>  The recent progress in non-autoregressive text-to-speech (NAR-TTS) has made fast and high-quality speech synthesis possible. However, current NAR-TTS models usually use phoneme sequence as input and thus cannot understand the tree-structured syntactic information of the input sequence, which hurts the prosody modeling. To this end, we propose SyntaSpeech, a syntax-aware and light-weight NAR-TTS model, which integrates tree-structured syntactic information into the prosody modeling modules in PortaSpeech \cite{ren2021portaspeech}. Specifically, 1) We build a syntactic graph based on the dependency tree of the input sentence, then process the text encoding with a syntactic graph encoder to extract the syntactic information. 2) We incorporate the extracted syntactic encoding with PortaSpeech to improve the prosody prediction. 3) We introduce a multi-length discriminator to replace the flow-based post-net in PortaSpeech, which simplifies the training pipeline and improves the inference speed, while keeping the naturalness of the generated audio. Experiments on three datasets not only show that the tree-structured syntactic information grants SyntaSpeech the ability to synthesize better audio with expressive prosody, but also demonstrate the generalization ability of SyntaSpeech to adapt to multiple languages and multi-speaker text-to-speech. Ablation studies demonstrate the necessity of each component in SyntaSpeech. Source code and audio samples are available at <a class="link-external link-https" href="https://syntaspeech.github.io" rel="external noopener nofollow">this https URL</a>      
### 47.Travel time optimization on multi-AGV routing by reverse annealing  [ :arrow_down: ](https://arxiv.org/pdf/2204.11789.pdf)
>  Quantum annealing has been actively researched since D-Wave Systems produced the first commercial machine in 2011. Controlling a large fleet of automated guided vehicles is one of the real-world applications utilizing quantum annealing. In this study, we propose a formulation to control the traveling routes to minimize the travel time. We validate our formulation through simulation in a virtual plant and authenticate the effectiveness for faster distribution compared to a greedy algorithm that does not consider the overall detour distance. Furthermore, we utilize reverse annealing to maximize the advantage of the D-Wave's quantum annealer. Starting from relatively good solutions obtained by a fast greedy algorithm, reverse annealing searches for better solutions around them. Our reverse annealing method improves the performance compared to standard quantum annealing alone and performs up to 10 times faster than the strong classical solver, Gurobi. This study extends a use of optimization with general problem solvers in the application of multi-AGV systems and reveals the potential of reverse annealing as an optimizer.      
### 48.A quantum Fourier transform (QFT) based note detection algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2204.11775.pdf)
>  In quantum information processing (QIP), the quantum Fourier transform (QFT) has a plethora of applications [1] [2] [3]: Shor's algorithm and phase estimation are just a few well-known examples. Shor's quantum factorization algorithm, one of the most widely quoted quantum algorithms [4] [5] [6] relies heavily on the QFT and efficiently finds integer prime factors of large numbers on quantum computers [4]. This seminal ground-breaking design for quantum algorithms has triggered a cascade of viable alternatives to previously unsolvable problems on a classical computer that are potentially superior and can run in polynomial time. In this work we examine the QFT's structure and implementation for the creation of a quantum music note detection algorithm both on a simulated and a real quantum computer. Though formal approaches [7] [1] [8] [9] exist for the verification of quantum algorithms, in this study we limit ourselves to a simpler, symbolic representation which we validate using the symbolic SymPy [10] [11] package which symbolically replicates quantum computing processes. The algorithm is then implemented as a quantum circuit, using IBM's qiskit [12] library and finally period detection is exemplified on an actual single musical tone using a varying number of qubits.      
### 49.Forecasting Electricity Prices  [ :arrow_down: ](https://arxiv.org/pdf/2204.11735.pdf)
>  Forecasting electricity prices is a challenging task and an active area of research since the 1990s and the deregulation of the traditionally monopolistic and government-controlled power sectors. Although it aims at predicting both spot and forward prices, the vast majority of research is focused on short-term horizons which exhibit dynamics unlike in any other market. The reason is that power system stability calls for a constant balance between production and consumption, while being weather (both demand and supply) and business activity (demand only) dependent. The recent market innovations do not help in this respect. The rapid expansion of intermittent renewable energy sources is not offset by the costly increase of electricity storage capacities and modernization of the grid infrastructure. On the methodological side, this leads to three visible trends in electricity price forecasting research as of 2022. Firstly, there is a slow, but more noticeable with every year, tendency to consider not only point but also probabilistic (interval, density) or even path (also called ensemble) forecasts. Secondly, there is a clear shift from the relatively parsimonious econometric (or statistical) models towards more complex and harder to comprehend, but more versatile and eventually more accurate statistical/machine learning approaches. Thirdly, statistical error measures are nowadays regarded as only the first evaluation step. Since they may not necessarily reflect the economic value of reducing prediction errors, more and more often, they are complemented by case studies comparing profits from scheduling or trading strategies based on price forecasts obtained from different models.      
### 50.4DAC: Learning Attribute Compression for Dynamic Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2204.11723.pdf)
>  With the development of the 3D data acquisition facilities, the increasing scale of acquired 3D point clouds poses a challenge to the existing data compression techniques. Although promising performance has been achieved in static point cloud compression, it remains under-explored and challenging to leverage temporal correlations within a point cloud sequence for effective dynamic point cloud compression. In this paper, we study the attribute (e.g., color) compression of dynamic point clouds and present a learning-based framework, termed 4DAC. To reduce temporal redundancy within data, we first build the 3D motion estimation and motion compensation modules with deep neural networks. Then, the attribute residuals produced by the motion compensation component are encoded by the region adaptive hierarchical transform into residual coefficients. In addition, we also propose a deep conditional entropy model to estimate the probability distribution of the transformed coefficients, by incorporating temporal context from consecutive point clouds and the motion estimation/compensation modules. Finally, the data stream is losslessly entropy coded with the predicted distribution. Extensive experiments on several public datasets demonstrate the superior compression performance of the proposed approach.      
### 51.Optimal security hardening over a probabilistic attack graph: a case study of an industrial control system using the CySecTool tool  [ :arrow_down: ](https://arxiv.org/pdf/2204.11707.pdf)
>  CySecTool is a tool that finds a cost-optimal security controls portfolio in a given budget for a probabilistic attack graph. A portfolio is a set of counter-measures, or controls, against vulnerabilities adopted for a computer system, while an attack graph is a type of a threat scenario model. In an attack graph, nodes are privilege states of the attacker, edges are vulnerabilities escalating privileges, and controls reduce the probabilities of some vulnerabilities being exploited. The tool builds on an optimisation algorithm published by Khouzani et al. (2019), enabling a user to quickly create, edit, and incrementally improve models, analyse results for given portfolios and display the best solutions for all possible budgets in the form of a Pareto frontier. A case study was performed utilising a system graph and suspected attack paths prepared by industrial security engineers based on an industrial source with which they work. The goal of the case study is to model a supervisory control and data acquisition (SCADA) industrial system which, due to having the potential to harm people, necessitates strong protection while not allowing the use of typical penetration tools like vulnerability scanners. Results are analysed to show how a cyber-security analyst would use CySecTool to store cyber-security intelligence and draw further conclusions.      
### 52.Low Complexity Suboptimal ML Detection for OFDM-IM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.11643.pdf)
>  Orthogonal frequency division multiplexing with index modulation (OFDM-IM) is a novel multicarrier scheme, which uses the k out of n subcarriers as active subcarriers to transmit data. For detecting the subcarrier activation pattern (SAP) at the receiver, maximum likelihood (ML) detection cannot be used because of its high computational complexity. Instead, the detector selecting the most likely active k subcarriers is used, which is called a k largest values (klv) detector. However, this method degrades the detection performance especially if the ratio of illegal SAPs to SAPs is high. In this letter, the suboptimal ML detector is proposed, which is a slight modification of the klv detector. However, the proposed detector has a similar detection performance compared to the ML detection, which is suitable for flexible implementation of OFDM-IM systems.      
### 53.Hybrid ISTA: Unfolding ISTA With Convergence Guarantees Using Free-Form Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.11640.pdf)
>  It is promising to solve linear inverse problems by unfolding iterative algorithms (e.g., iterative shrinkage thresholding algorithm (ISTA)) as deep neural networks (DNNs) with learnable parameters. However, existing ISTA-based unfolded algorithms restrict the network architectures for iterative updates with the partial weight coupling structure to guarantee convergence. In this paper, we propose hybrid ISTA to unfold ISTA with both pre-computed and learned parameters by incorporating free-form DNNs (i.e., DNNs with arbitrary feasible and reasonable network architectures), while ensuring theoretical convergence. We first develop HCISTA to improve the efficiency and flexibility of classical ISTA (with pre-computed parameters) without compromising the convergence rate in theory. Furthermore, the DNN-based hybrid algorithm is generalized to popular variants of learned ISTA, dubbed HLISTA, to enable a free architecture of learned parameters with a guarantee of linear convergence. To our best knowledge, this paper is the first to provide a convergence-provable framework that enables free-form DNNs in ISTA-based unfolded algorithms. This framework is general to endow arbitrary DNNs for solving linear inverse problems with convergence guarantees. Extensive experiments demonstrate that hybrid ISTA can reduce the reconstruction error with an improved convergence rate in the tasks of sparse recovery and compressive sensing.      
### 54.Deep CSI Compression for Massive MIMO: A Self-information Model-driven Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2204.11567.pdf)
>  In order to fully exploit the advantages of massive multiple-input multiple-output (mMIMO), it is critical for the transmitter to accurately acquire the channel state information (CSI). Deep learning (DL)-based methods have been proposed for CSI compression and feedback to the transmitter. Although most existing DL-based methods consider the CSI matrix as an image, structural features of the CSI image are rarely exploited in neural network design. As such, we propose a model of self-information that dynamically measures the amount of information contained in each patch of a CSI image from the perspective of structural features. Then, by applying the self-information model, we propose a model-and-data-driven network for CSI compression and feedback, namely IdasNet. The IdasNet includes the design of a module of self-information deletion and selection (IDAS), an encoder of informative feature compression (IFC), and a decoder of informative feature recovery (IFR). In particular, the model-driven module of IDAS pre-compresses the CSI image by removing informative redundancy in terms of the self-information. The encoder of IFC then conducts feature compression to the pre-compressed CSI image and generates a feature codeword which contains two components, i.e., codeword values and position indices of the codeword values. Subsequently, the IFR decoder decouples the codeword values as well as position indices to recover the CSI image. Experimental results verify that the proposed IdasNet noticeably outperforms existing DL-based networks under various compression ratios while it has the number of network parameters reduced by orders-of-magnitude compared with various existing methods.      
### 55.Maximum Mean Discrepancy Distributionally Robust Nonlinear Chance-Constrained Optimization with Finite-Sample Guarantee  [ :arrow_down: ](https://arxiv.org/pdf/2204.11564.pdf)
>  This paper is motivated by addressing open questions in distributionally robust chance-constrained programs (DRCCP) using the popular Wasserstein ambiguity sets. Specifically, the computational techniques for those programs typically place restrictive assumptions on the constraint functions and the size of the Wasserstein ambiguity sets is often set using costly cross-validation (CV) procedures or conservative measure concentration bounds. In contrast, we propose a practical DRCCP algorithm using kernel maximum mean discrepancy (MMD) ambiguity sets, which we term MMD-DRCCP, to treat general nonlinear constraints without using ad-hoc reformulation techniques. MMD-DRCCP can handle general nonlinear and non-convex constraints with a proven finite-sample constraint satisfaction guarantee of a dimension-independent $\mathcal{O}(\frac{1}{\sqrt{N}})$ rate, achievable by a practical algorithm. We further propose an efficient bootstrap scheme for constructing sharp MMD ambiguity sets in practice without resorting to CV. Our algorithm is validated numerically on a portfolio optimization problem and a tube-based distributionally robust model predictive control problem with non-convex constraints.      
### 56.Speech Detection For Child-Clinician Conversations In Danish For Low-Resource In-The-Wild Conditions: A Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2204.11550.pdf)
>  Use of speech models for automatic speech processing tasks can improve efficiency in the screening, analysis, diagnosis and treatment in medicine and psychiatry. However, the performance of pre-processing speech tasks like segmentation and diarization can drop considerably on in-the-wild clinical data, specifically when the target dataset comprises of atypical speech. In this paper we study the performance of a pre-trained speech model on a dataset comprising of child-clinician conversations in Danish with respect to the classification threshold. Since we do not have access to sufficient labelled data, we propose few-instance threshold adaptation, wherein we employ the first minutes of the speech conversation to obtain the optimum classification threshold. Through our work in this paper, we learned that the model with default classification threshold performs worse on children from the patient group. Furthermore, the error rates of the model is directly correlated to the severity of diagnosis in the patients. Lastly, our study on few-instance adaptation shows that three-minutes of clinician-child conversation is sufficient to obtain the optimum classification threshold.      
### 57.Coupling Matrix-based Beamforming for Superdirective Antenna Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2204.11547.pdf)
>  In most multiple-input multiple-output (MIMO) communication systems, e.g., Massive MIMO, the antenna spacing is generally no less than half a wavelength. It helps to reduce the mutual coupling and therefore facilitate the system design. The maximum array gain is the number of antennas in this settings. However, when the antenna spacing is made very small, the array gain of a compact array can be proportional to the square of the number of antennas - a value much larger than the traditional array. To achieve this so-called "superdirectivity" however, the calculation of the excitation coefficients (beamforming vector) is known to be a challenging problem. In this paper, we derive the beamforming vector of superdirective arrays based on a novel coupling matrix-enabled method. We also propose an approach to obtain the coupling matrix, which is derived by the spherical wave expansion method and active element pattern. The full-wave electromagnetic simulations are conducted to validate the effectiveness of our proposed method. Simulation results show that when the beamforming vector obtained by our method is applied, the directivity of the designed dipole antenna array has a good agreement with the theoretical values.      
### 58.Multi-UE Multi-AP Beam Alignment in User-Centric Cell-Free Massive MIMO Systems Operating at mmWave  [ :arrow_down: ](https://arxiv.org/pdf/2204.11524.pdf)
>  This paper considers the problem of beam alignment in a cell-free massive MIMO deployment with multiple access points (APs) and multiple user equipments (UEs) simultaneously operating in the same millimeter wave frequency band. Assuming the availability of a control channel at sub-6 GHz frequencies, a protocol is developed that permits estimating, for each UE, the strongest propagation path from each of the surrounding APs, and to perform user-centric association between the UEs and the APs. Estimation of the strongest paths from nearby APs is realized at the UE in a one-phase procedure, during which all the APs simultaneously transmit on pseudo-randomly selected channels with pseudo-random transmit beamformers. An algorithm for orthogonal channels assignment to the APs is also proposed, with the aim of minimizing the mutual interference between APs that transmit on the same channels. The performance of the proposed strategy is evaluated both in terms of probability of correct detection of the directions of arrival and of departure associated to the strongest beam from nearby APs, and in terms of downlink and uplink signal-to-interference-plus-noise ratio. Numerical results show that the proposed approach is effective and capable of efficiently realizing beam alignment in a multi-UE multi-AP wireless scenario.      
### 59.Randomly Initialized Alternating Least Squares: Fast Convergence for Matrix Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2204.11516.pdf)
>  We consider the problem of reconstructing rank-one matrices from random linear measurements, a task that appears in a variety of problems in signal processing, statistics, and machine learning. In this paper, we focus on the Alternating Least Squares (ALS) method. While this algorithm has been studied in a number of previous works, most of them only show convergence from an initialization close to the true solution and thus require a carefully designed initialization scheme. However, random initialization has often been preferred by practitioners as it is model-agnostic. In this paper, we show that ALS with random initialization converges to the true solution with $\varepsilon$-accuracy in $O(\log n + \log (1/\varepsilon)) $ iterations using only a near-optimal amount of samples, where we assume the measurement matrices to be i.i.d. Gaussian and where by $n$ we denote the ambient dimension. Key to our proof is the observation that the trajectory of the ALS iterates only depends very mildly on certain entries of the random measurement matrices. Numerical experiments corroborate our theoretical predictions.      
### 60.End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network  [ :arrow_down: ](https://arxiv.org/pdf/2204.11479.pdf)
>  While efficient architectures and a plethora of augmentations for end-to-end image classification tasks have been suggested and heavily investigated, state-of-the-art techniques for audio classifications still rely on numerous representations of the audio signal together with large architectures, finetuned from large datasets. By utilizing the inherited lightweight nature of audio and novel audio augmentations, we were able to present an efficient end-to-end1 network with strong generalization ability. Experiments on a variety of sound classification sets demonstrate the effectiveness and robustness of our approach, by achieving state-of-the-art results in various settings. Public code will be available.      
### 61.Deep Reinforcement Learning for Online Routing of Unmanned Aerial Vehicles with Wireless Power Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2204.11477.pdf)
>  The unmanned aerial vehicle (UAV) plays an vital role in various applications such as delivery, military mission, disaster rescue, communication, etc., due to its flexibility and versatility. This paper proposes a deep reinforcement learning method to solve the UAV online routing problem with wireless power transfer, which can charge the UAV remotely without wires, thus extending the capability of the battery-limited UAV. Our study considers the power consumption of the UAV and the wireless charging process. Unlike the previous works, we solve the problem by a designed deep neural network. The model is trained using a deep reinforcement learning method offline, and is used to optimize the UAV routing problem online. On small and large scale instances, the proposed model runs from four times to 500 times faster than Google OR-tools, the state-of-the-art combinatorial optimization solver, with identical solution quality. It also outperforms different types of heuristic and local search methods in terms of both run-time and optimality. In addition, once the model is trained, it can scale to new generated problem instances with arbitrary topology that are not seen during training. The proposed method is practically applicable when the problem scale is large and the response time is crucial.      
### 62.A Time-Triggered Dimension Reduction Algorithm for the Task Assignment Problem  [ :arrow_down: ](https://arxiv.org/pdf/2204.11476.pdf)
>  The task assignment problem is fundamental in combinatorial optimisation, aiming at allocating one or more tasks to a number of agents while minimizing the total cost or maximizing the overall assignment benefit. This problem is known to be computationally hard since it is usually formulated as a mixed-integer programming problem. In this paper, we consider a novel time-triggered dimension reduction algorithm (TTDRA). We propose convexification approaches to convexify both the constraints and the cost function for the general non-convex assignment problem. The computational speed is accelerated via our time-triggered dimension reduction scheme, where the triggering condition is designed based on the optimality tolerance and the convexity of the cost function. Optimality and computational efficiency are verified via numerical simulations on benchmark examples.      
### 63.IMDeception: Grouped Information Distilling Super-Resolution Network  [ :arrow_down: ](https://arxiv.org/pdf/2204.11463.pdf)
>  Single-Image-Super-Resolution (SISR) is a classical computer vision problem that has benefited from the recent advancements in deep learning methods, especially the advancements of convolutional neural networks (CNN). Although state-of-the-art methods improve the performance of SISR on several datasets, direct application of these networks for practical use is still an issue due to heavy computational load. For this purpose, recently, researchers have focused on more efficient and high-performing network structures. Information multi-distilling network (IMDN) is one of the highly efficient SISR networks with high performance and low computational load. IMDN achieves this efficiency with various mechanisms such as Intermediate Information Collection (IIC), working in a global setting, Progressive Refinement Module (PRM), and Contrast Aware Channel Attention (CCA), employed in a local setting. These mechanisms, however, do not equally contribute to the efficiency and performance of IMDN. In this work, we propose the Global Progressive Refinement Module (GPRM) as a less parameter-demanding alternative to the IIC module for feature aggregation. To further decrease the number of parameters and floating point operations persecond (FLOPS), we also propose Grouped Information Distilling Blocks (GIDB). Using the proposed structures, we design an efficient SISR network called IMDeception. Experiments reveal that the proposed network performs on par with state-of-the-art models despite having a limited number of parameters and FLOPS. Furthermore, using grouped convolutions as a building block of GIDB increases room for further optimization during deployment. To show its potential, the proposed model was deployed on NVIDIA Jetson Xavier AGX and it has been shown that it can run in real-time on this edge device      
### 64.Error Performance Analysis of Multi-user Detection in Uplink-NOMA with Adaptive $\mathcal{M}$-QAM  [ :arrow_down: ](https://arxiv.org/pdf/2204.11460.pdf)
>  This work provides a generalized performance analysis for the multi-user uplink-NOMA system with adaptive square quadrature amplitude modulation (QAM) over Rayleigh fading channels. Motivated by the massive IoT connections and unavailability of orthogonal resources for each node, we consider a multi-access scheme where multi-users with single-antenna transmit data to a multiple-antenna base station through the same resource block. By taking advantage of combining diversity paths with the proposed joint maximum-likelihood detector (JMLD), a closed form expression for the upper bound of bit error rate (BER) is obtained. Despite the number of users or the order of modulation, the analytical results endorsed via computer simulations reveal the ability of the MRC-JMLD detector to discard the error floor completely. Moreover, the simulation results show that the MRC-JMLD surpasses its counterparts significantly and ensures a full diversity order.      
### 65.Understanding Audio Features via Trainable Basis Functions  [ :arrow_down: ](https://arxiv.org/pdf/2204.11437.pdf)
>  In this paper we explore the possibility of maximizing the information represented in spectrograms by making the spectrogram basis functions trainable. We experiment with two different tasks, namely keyword spotting (KWS) and automatic speech recognition (ASR). For most neural network models, the architecture and hyperparameters are typically fine-tuned and optimized in experiments. Input features, however, are often treated as fixed. In the case of audio, signals can be mainly expressed in two main ways: raw waveforms (time-domain) or spectrograms (time-frequency-domain). In addition, different spectrogram types are often used and tailored to fit different applications. In our experiments, we allow for this tailoring directly as part of the network. <br>Our experimental results show that using trainable basis functions can boost the accuracy of Keyword Spotting (KWS) by 14.2 percentage points, and lower the Phone Error Rate (PER) by 9.5 percentage points. Although models using trainable basis functions become less effective as the model complexity increases, the trained filter shapes could still provide us with insights on which frequency bins are important for that specific task. From our experiments, we can conclude that trainable basis functions are a useful tool to boost the performance when the model complexity is limited.      
### 66.Audio-Visual Scene Classification Using A Transfer Learning Based Joint Optimization Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2204.11420.pdf)
>  Recently, audio-visual scene classification (AVSC) has attracted increasing attention from multidisciplinary communities. Previous studies tended to adopt a pipeline training strategy, which uses well-trained visual and acoustic encoders to extract high-level representations (embeddings) first, then utilizes them to train the audio-visual classifier. In this way, the extracted embeddings are well suited for uni-modal classifiers, but not necessarily suited for multi-modal ones. In this paper, we propose a joint training framework, using the acoustic features and raw images directly as inputs for the AVSC task. Specifically, we retrieve the bottom layers of pre-trained image models as visual encoder, and jointly optimize the scene classifier and 1D-CNN based acoustic encoder during training. We evaluate the approach on the development dataset of TAU Urban Audio-Visual Scenes 2021. The experimental results show that our proposed approach achieves significant improvement over the conventional pipeline training strategy. Moreover, our best single system outperforms previous state-of-the-art methods, yielding a log loss of 0.1517 and accuracy of 94.59% on the official test fold.      
### 67.Road Traffic Law Adaptive Decision-making for Self-Driving Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2204.11411.pdf)
>  Self-driving vehicles have their own intelligence to drive on open roads. However, vehicle managers, e.g., government or industrial companies, still need a way to tell these self-driving vehicles what behaviors are encouraged or forbidden. Unlike human drivers, current self-driving vehicles cannot understand the traffic laws, thus rely on the programmers manually writing the corresponding principles into the driving systems. It would be less efficient and hard to adapt some temporary traffic laws, especially when the vehicles use data-driven decision-making algorithms. Besides, current self-driving vehicle systems rarely take traffic law modification into consideration. This work aims to design a road traffic law adaptive decision-making method. The decision-making algorithm is designed based on reinforcement learning, in which the traffic rules are usually implicitly coded in deep neural networks. The main idea is to supply the adaptability to traffic laws of self-driving vehicles by a law-adaptive backup policy. In this work, the natural language-based traffic laws are first translated into a logical expression by the Linear Temporal Logic method. Then, the system will try to monitor in advance whether the self-driving vehicle may break the traffic laws by designing a long-term RL action space. Finally, a sample-based planning method will re-plan the trajectory when the vehicle may break the traffic rules. The method is validated in a Beijing Winter Olympic Lane scenario and an overtaking case, built in CARLA simulator. The results show that by adopting this method, the self-driving vehicles can comply with new issued or updated traffic laws effectively. This method helps self-driving vehicles governed by digital traffic laws, which is necessary for the wide adoption of autonomous driving.      
### 68.Dynamic Point Cloud Compression with Cross-Sectional Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.11409.pdf)
>  The recent development of dynamic point clouds has introduced the possibility of mimicking natural reality, and greatly assisting quality of life. However, to broadcast successfully, the dynamic point clouds require higher compression due to their huge volume of data compared to the traditional video. Recently, MPEG finalized a Video-based Point Cloud Compression standard known as V-PCC. However, V-PCC requires huge computational time due to expensive normal calculation and segmentation, sacrifices some points to limit the number of 2D patches, and cannot occupy all spaces in the 2D frame. The proposed method addresses these limitations by using a novel cross-sectional approach. This approach reduces expensive normal estimation and segmentation, retains more points, and utilizes more spaces for 2D frame generation compared to the VPCC. The experimental results using standard video sequences show that the proposed technique can achieve better compression in both geometric and texture data compared to the V-PCC standard.      
### 69.Back-ends Selection for Deep Speaker Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2204.11403.pdf)
>  Probabilistic Linear Discriminant Analysis (PLDA) was the dominant and necessary back-end for early speaker recognition approaches, like i-vector and x-vector. However, with the development of neural networks and margin-based loss functions, we can obtain deep speaker embeddings (DSEs), which have advantages of increased inter-class separation and smaller intra-class distances. In this case, PLDA seems unnecessary or even counterproductive for the discriminative embeddings, and cosine similarity scoring (Cos) achieves better performance than PLDA in some situations. Motivated by this, in this paper, we systematically explore how to select back-ends (Cos or PLDA) for deep speaker embeddings to achieve better performance in different situations. By analyzing PLDA and the properties of DSEs extracted from models with different numbers of segment-level layers, we make the conjecture that Cos is better in same-domain situations and PLDA is better in cross-domain situations. We conduct experiments on VoxCeleb and NIST SRE datasets in four application situations, single-/multi-domain training and same-/cross-domain test, to validate our conjecture and briefly explain why back-ends adaption algorithms work.      
### 70.Real-time Speech Emotion Recognition Based on Syllable-Level Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2204.11382.pdf)
>  Speech emotion recognition systems have high prediction latency because of the high computational requirements for deep learning models and low generalizability mainly because of the poor reliability of emotional measurements across multiple corpora. To solve these problems, we present a speech emotion recognition system based on a reductionist approach of decomposing and analyzing syllable-level features. Mel-spectrogram of an audio stream is decomposed into syllable-level components, which are then analyzed to extract statistical features. The proposed method uses formant attention, noise-gate filtering, and rolling normalization contexts to increase feature processing speed and tolerance to adversity. A set of syllable-level formant features is extracted and fed into a single hidden layer neural network that makes predictions for each syllable as opposed to the conventional approach of using a sophisticated deep learner to make sentence-wide predictions. The syllable level predictions help to achieve the real-time latency and lower the aggregated error in utterance level cross-corpus predictions. The experiments on IEMOCAP (IE), MSP-Improv (MI), and RAVDESS (RA) databases show that the method archives real-time latency while predicting with state-of-the-art cross-corpus unweighted accuracy of 47.6% for IE to MI and 56.2% for MI to IE.      
### 71.Emotion-Aware Transformer Encoder for Empathetic Dialogue Generation  [ :arrow_down: ](https://arxiv.org/pdf/2204.11320.pdf)
>  Modern day conversational agents are trained to emulate the manner in which humans communicate. To emotionally bond with the user, these virtual agents need to be aware of the affective state of the user. Transformers are the recent state of the art in sequence-to-sequence learning that involves training an encoder-decoder model with word embeddings from utterance-response pairs. We propose an emotion-aware transformer encoder for capturing the emotional quotient in the user utterance in order to generate human-like empathetic responses. The contributions of our paper are as follows: 1) An emotion detector module trained on the input utterances determines the affective state of the user in the initial phase 2) A novel transformer encoder is proposed that adds and normalizes the word embedding with emotion embedding thereby integrating the semantic and affective aspects of the input utterance 3) The encoder and decoder stacks belong to the Transformer-XL architecture which is the recent state of the art in language modeling. Experimentation on the benchmark Facebook AI empathetic dialogue dataset confirms the efficacy of our model from the higher BLEU-4 scores achieved for the generated responses as compared to existing methods. Emotionally intelligent virtual agents are now a reality and inclusion of affect as a modality in all human-machine interfaces is foreseen in the immediate future.      
### 72.Dictionary Attacks on Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2204.11304.pdf)
>  In this paper, we propose dictionary attacks against speaker verification - a novel attack vector that aims to match a large fraction of speaker population by chance. We introduce a generic formulation of the attack that can be used with various speech representations and threat models. The attacker uses adversarial optimization to maximize raw similarity of speaker embeddings between a seed speech sample and a proxy population. The resulting master voice successfully matches a non-trivial fraction of people in an unknown population. Adversarial waveforms obtained with our approach can match on average 69% of females and 38% of males enrolled in the target system at a strict decision threshold calibrated to yield false alarm rate of 1%. By using the attack with a black-box voice cloning system, we obtain master voices that are effective in the most challenging conditions and transferable between speaker encoders. We also show that, combined with multiple attempts, this attack opens even more to serious issues on the security of these systems.      
### 73.Semi-Integrated-Sensing-and-Communication (Semi-ISaC): From OMA to NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2204.11245.pdf)
>  The new concept of semi-integrated-sensing-and-communication (Semi-ISaC) is proposed for next-generation cellular networks. We propose a novel Semi-ISaC framework which provides more freedom as it allows that a portion of the bandwidth is exclusively used for either wireless communication or for radar detection, while the rest is for ISaC transmission. To enhance the bandwidth efficiency (BE), we investigate the evolution of Semi-ISaC networks from orthogonal multiple access (OMA) to non-orthogonal multiple access (NOMA). First, we evaluate the performance of an OMA-based Semi-ISaC network. As for the communication signals, we investigate both the outage probability (OP) and the ergodic rate. As for the radar echoes, we characterize the ergodic radar estimation information rate (REIR). Based on these metrics, we derive the analytical and asymptotic expressions of the ergodic RIER. Then, we investigate the performance of a NOMA-based Semi-ISaC network. More specifically, we derive the analytical expressions of both the OP and of the ergodic rate for the communication signals. The asymptotic expressions of OP are also derived for quantifying the diversity gains of the communication signals. As for the radar echoes, we derive the analytical and asymptotic ergodic REIR. The high signal-to-noise ratio (SNR) slopes are also evaluated. The analytical results indicate that: 1) Under a two-user NOMA Semi-ISaC scenario, the diversity order of the near-user equals to the coefficient of the Nakagami-m fading channels ($m$), while that of the far-user is zero; and 2) The high-SNR slope for the ergodic REIR is based on the ratio of the radar signal's duty cycle and the pulse duration. Our simulation results show that: 1) Semi-ISaC has better channel capacity than conventional ISaC; and 2) NOMA-based Semi-ISaC has better channel capacity than OMA-based Semi-ISaC.      
### 74.Bi-objective Optimization of Information Rate and Harvested Power in RIS-aided SWIPT Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.11229.pdf)
>  The problem of simultaneously optimizing the information rate and the harvested power in a reconfigurable intelligent surface (RIS)-aided multiple-input single-output downlink wireless network with simultaneous wireless information and power transfer (SWIPT) is addressed. The beamforming vectors, RIS reflection coefficients, and power split ratios are jointly optimized subject to maximum power constraints, minimum harvested power constraints, and realistic constraints on the RIS reflection coefficients. A practical algorithm is developed through an interplay of alternating optimization, sequential optimization, and pricing-based methods. Numerical results show that the deployment of RISs can significantly improve the information rate and the amount of harvested power.      
### 75.Musical Stylistic Analysis: A Study of Intervallic Transition Graphs via Persistent Homology  [ :arrow_down: ](https://arxiv.org/pdf/2204.11139.pdf)
>  Topological data analysis has been recently applied to investigate stylistic signatures and trends in musical compositions. A useful tool in this area is Persistent Homology. In this paper, we develop a novel method to represent a weighted directed graph as a finite metric space and then use persistent homology to extract useful features. We apply this method to weighted directed graphs obtained from pitch transitions information of a given musical fragment and use these techniques to the study of stylistic trends. In particular, we are interested in using these tools to make quantitative stylistic comparisons. As a first illustration, we analyze a selection of string quartets by Haydn, Mozart and Beethoven and discuss possible implications of our results in terms of different approaches by these composers to stylistic exploration and variety. We observe that Haydn is stylistically the most conservative, followed by Mozart, while Beethoven is the most innovative, expanding and modifying the string quartet as a musical form. Finally we also compare the variability of different genres, namely minuets, allegros, prestos and adagios, by a given composer and conclude that the minuet is the most stable form of the string quartet movements.      
### 76.Koopman-based Policy Iteration for Robust Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.10987.pdf)
>  Classically, the optimal control problem in the presence of an adversary is formulated as a two-player zero-sum differential game or an $H_\infty$ control problem. The solution to these problems can be obtained by solving the Hamilton-Jacobi-Issac equation (HJIE). We provide a novel Koopman-based expression of the HJIE, where the solutions can be obtained through the approximation of the Koopman operator itself. In particular, we developed a data-driven and model based policy iteration algorithm for approximating the optimal value function using a finite-dimensional approximation of the Koopman operator and generator.      
### 77.Generative sampling in tractography using autoencoders (GESTA)  [ :arrow_down: ](https://arxiv.org/pdf/2204.10891.pdf)
>  Current tractography methods use the local orientation information to propagate streamlines from seed locations. Many such seeds provide streamlines that stop prematurely or fail to map the true pathways because some white matter bundles are "harder-to-track" than others. This results in tractography reconstructions with poor white and gray matter spatial coverage. In this work, we propose a generative, autoencoder-based method, named GESTA (Generative Sampling in Tractography using Autoencoders), that produces streamlines with better spatial coverage. Compared to other deep learning methods, our autoencoder-based framework is not constrained by any prior or a fixed set of bundles. GESTA produces new and complete streamlines for any white matter bundle. GESTA is shown to be effective on both synthetic and human brain in vivo data. Our streamline evaluation framework ensures that the streamlines produced by GESTA are anatomically plausible and fit well to the local diffusion signal. The streamline evaluation criteria assess anatomy (white matter coverage), local orientation alignment (direction), geometry features of streamlines, and optionally, gray matter connectivity. The GESTA framework offers considerable gains in bundle coverage using a reduced set of seeding streamlines with a 1.5x improvement for the "Fiber Cup", and 6x for the ISMRM 2015 Tractography Challenge datasets. Similarly, it provides a 4x white matter volume increase on the BIL&amp;GIN callosal homotopic dataset. It also successfully generates new streamlines in poorly populated bundles, such as the fornix and other hard-to-track bundles, on in vivo data. GESTA is thus the first deep tractography generative method that can improve white matter reconstruction of hard-to-track bundles.      
### 78.Quantum Fault Trees  [ :arrow_down: ](https://arxiv.org/pdf/2204.10877.pdf)
>  Fault tree analysis is a technique widely used in risk and reliability analysis of complex engineering systems given its deductive nature and relatively simple interpretation. In a fault tree, events are usually represented by a binary variable that indicates whether an event occurs or not, traditionally associated with the values 1 and 0, respectively. Different events are linked together using logical gates, modelling the dependencies that a subsystem or system may have over its basic components. In this study, quantum computing is leveraged to propose a novel approach to encode a traditional fault tree into a quantum algorithm. This quantum fault tree method uses quantum bits to represent basic events, effectively encoding the original fault tree into a quantum circuit. The execution of the resulting quantum circuit represents a full simulation of the fault tree, and multiple executions can be utilized to compute the failure probability of the whole system. The proposed approach is tested on a case study portraying a dynamic positioning system. Results verify that the quantum-based proposed approach is able to effectively obtain the dynamic positioning failure probability through simulation, opening promising opportunities for future investigations in the area.      
