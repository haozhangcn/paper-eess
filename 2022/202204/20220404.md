# ArXiv eess --Mon, 4 Apr 2022
### 1.Population Games With Erlang Clocks: Convergence to Nash Equilibria For Pairwise Comparison Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2204.00593.pdf)
>  The prevailing methodology for analyzing population games and evolutionary dynamics in the large population limit assumes that a Poisson process (or clock) inherent to each agent determines when the agent can revise its strategy. Hence, such an approach presupposes exponentially distributed inter-revision intervals, and is inadequate for cases where each strategy entails a sequence of sub-tasks (sub-strategies) that must be completed before a new revision time occurs. This article proposes a methodology for such cases under the premise that a sub-strategy's duration is exponentially-distributed, leading to Erlang distributed inter-revision intervals. We assume that a so-called pairwise-comparison protocol captures the agents' revision preferences to render our analysis concrete. The presence of sub-strategies brings on additional dynamics that is incompatible with existing models and results. Our main contributions are twofold, both derived for a deterministic approximation valid for large populations. We prove convergence of the population's state to the Nash equilibrium set when a potential game generates a payoff for the strategies. We use system-theoretic passivity to determine conditions under which this convergence is guaranteed for contractive games.      
### 2.Nonlinear VRFT with LASSO  [ :arrow_down: ](https://arxiv.org/pdf/2204.00590.pdf)
>  Virtual Reference Feedback Tuning (VRFT) is a well known and very successful data-driven control design method. It has been initially conceived for linear plants and this original formulation has been much explored in the literature, besides having already found many practical applications. A nonlinear version of VRFT has been proposed early on, but not much explored later on. In this paper we highlight various issues involved in the application of nonlinear VRFT and propose the inclusion of L1 regularization in its formulation. We illustrate by means of two simple examples the critical role played by two aspects: the L1 regularization and the choice of dictionary used to describe the nonlinearity of the controller.      
### 3.1-D CNN based Acoustic Scene Classification via Reducing Layer-wise Dimensionality  [ :arrow_down: ](https://arxiv.org/pdf/2204.00555.pdf)
>  This paper presents an alternate representation framework to commonly used time-frequency representation for acoustic scene classification (ASC). A raw audio signal is represented using a pre-trained convolutional neural network (CNN) using its various intermediate layers. The study assumes that the representations obtained from the intermediate layers lie in low-dimensions intrinsically. To obtain low-dimensional embeddings, principal component analysis is performed, and the study analyzes that only a few principal components are significant. However, the appropriate number of significant components are not known. To address this, an automatic dictionary learning framework is utilized that approximates the underlying subspace. Further, the low-dimensional embeddings are aggregated in a late-fusion manner in the ensemble framework to incorporate hierarchical information learned at various intermediate layers. The experimental evaluation is performed on publicly available DCASE 2017 and 2018 ASC datasets on a pre-trained 1-D CNN, SoundNet. Empirically, it is observed that deeper layers show more compression ratio than others. At 70% compression ratio across different datasets, the performance is similar to that obtained without performing any dimensionality reduction. The proposed framework outperforms the time-frequency representation based methods.      
### 4.An Approximate MSE Expression for Maximum Likelihood and Other Implicitly Defined Estimators of Non-Random Parameters (extended version)  [ :arrow_down: ](https://arxiv.org/pdf/2204.00532.pdf)
>  An approximate mean square error (MSE) expression for the performance analysis of implicitly defined estimators of non-random parameters is proposed. An implicitly defined estimator (IDE) declares the minimizer/maximizer of a selected cost/reward function as the parameter estimate. The maximum likelihood (ML) and the least squares estimators are among the well known examples of this class. In this paper, an exact MSE expression for implicitly defined estimators with a symmetric and unimodal objective function is given. It is shown that the expression reduces to the Cramer-Rao lower bound (CRLB) and misspecified CRLB in the large sample size regime for ML and misspecified ML estimation, respectively. The expression is shown to yield the Ziv-Zakai bound (without the valley filling function) when it is used in a Bayesian setting, that is, when an a-priori distribution is assigned to the unknown parameter. In addition, extension of the suggested expression to the case of nuisance parameters is studied and some approximations are given to ease the computations for this case. Numerical results indicate that the suggested MSE expression not only predicts the estimator performance in the asymptotic region; but it is also applicable for the threshold region analysis, even for IDEs whose objective functions do not satisfy the symmetry and unimodality assumptions. Advantages of the suggested MSE expression are its conceptual simplicity and its relatively straightforward numerical calculation due to the reduction of the estimation problem to a binary hypothesis testing problem, similar to the usage of Ziv-Zakai bounds in random parameter estimation problems.      
### 5.An Analytical Framework for Control Synthesis of Cyber-Physical Systems with Safety Guarantee  [ :arrow_down: ](https://arxiv.org/pdf/2204.00514.pdf)
>  Cyber-physical systems (CPS) are required to operate safely under fault and malicious attacks. The simplex architecture and the recently proposed cyber resilient architectures, e.g., Byzantine fault tolerant++ (BFT++), provide safety for CPS under faults and malicious cyber attacks, respectively. However, these existing architectures make use of different timing parameters and implementations to provide safety, and are seemingly unrelated. In this paper, we propose an analytical framework to represent the simplex, BFT++ and other practical cyber resilient architectures (CRAs). We construct a hybrid system that models CPS adopting any of these architectures. We derive sufficient conditions via our proposed framework under which a control policy is guaranteed to be safe. We present an algorithm to synthesize the control policy. We validate the proposed framework using a case study on lateral control of a Boeing 747, and demonstrate that our proposed approach ensures safety of the system.      
### 6.An Arduino based heartbeat detection device (ArdMob-ECG) for real-time ECG analysis  [ :arrow_down: ](https://arxiv.org/pdf/2204.00513.pdf)
>  This technical paper provides a tutorial to build a low-cost (10-100 USD) and easy to assemble ECG device (ArdMob-ECG) that can be easily used for a variety of different scientific studies. The advantage of this device is that it automatically stores the data and has a built-in detection algorithm for heartbeats. Compared to a clinical ECG, this device entails a serial interface that can send triggers via USB directly to a computer and software (e.g. Unity, Matlab) with minimal delay due to its architecture. Its software and hardware is open-source and publicly available. The performance of the device regarding sensitivity and specificity is comparable to a professional clinical ECG and is assessed in this paper. Due to the open-source software, a variety of different research questions and individual alterations can be adapted using this ECG. The code as well as the circuit is publicly available and accessible for everyone to promote a better health system in remote areas, Open Science, and to boost scientific progress and the development of new paradigms that ultimately foster innovation.      
### 7.A Compositional Approach to Safety-Critical Resilient Control for Systems with Coupled Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2204.00512.pdf)
>  Complex, interconnected Cyber-physical Systems (CPS) are increasingly common in applications including smart grids and transportation. Ensuring safety of interconnected systems whose dynamics are coupled is challenging because the effects of faults and attacks in one sub-system can propagate to other sub-systems and lead to safety violations. In this paper, we study the problem of safety-critical control for CPS with coupled dynamics when some sub-systems are subject to failure or attack. We first propose resilient-safety indices (RSIs) for the faulty or compromised sub-systems that bound the worst-case impacts of faulty or compromised sub-systems on a set of specified safety constraints. By incorporating the RSIs, we provide a sufficient condition for the synthesis of control policies in each failure- and attack- free sub-systems. The synthesized control policies compensate for the impacts of the faulty or compromised sub-systems to guarantee safety. We formulate sum-of-square optimization programs to compute the RSIs and the safety-ensuring control policies. We present a case study that applies our proposed approach on the temperature regulation of three coupled rooms. The case study demonstrates that control policies obtained using our algorithm guarantee system's safety constraints.      
### 8.A measurement decoupling based fast algorithm for super-resolving point sources with multi-cluster structure  [ :arrow_down: ](https://arxiv.org/pdf/2204.00469.pdf)
>  We consider the problem of resolving closely spaced point sources in one dimension from their Fourier data in a bounded domain. Classical subspace methods (e.g., MUSIC algorithm, Matrix Pencil method, etc.) show great superiority in resolving closely spaced sources, but their computational cost is usually heavy. This is especially the case for point sources with a multi-cluster structure which requires processing large-sized data matrix resulted from highly sampled measurements. To address this issue, we propose a fast algorithm termed D-MUSIC, based on a measurement decoupling strategy. We demonstrate theoretically that for point sources with a known cluster structure, their measurement can be decoupled into local measurements of each of the clusters by solving a system of linear equations that are obtained by using a multipole basis. We further develop a subsampled MUSIC algorithm to detect the cluster structure and utilize it to decouple the global measurement. In the end, the MUSIC algorithm was applied to each local measurement to resolve point sources therein. Compared to the standard MUSIC algorithm, the proposed algorithm has comparable super-resolving capability while having a much lower computational complexity.      
### 9.Deep Neural Convolutive Matrix Factorization for Articulatory Representation Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2204.00465.pdf)
>  Most of the research on data-driven speech representation learning has focused on raw audios in an end-to-end manner, paying little attention to their internal phonological or gestural structure. This work, investigating the speech representations derived from articulatory kinematics signals, uses a neural implementation of convolutive sparse matrix factorization to decompose the articulatory data into interpretable gestures and gestural scores. By applying sparse constraints, the gestural scores leverage the discrete combinatorial properties of phonological gestures. Phoneme recognition experiments were additionally performed to show that gestural scores indeed code phonological information successfully. The proposed work thus makes a bridge between articulatory phonology and deep neural networks to leverage informative, intelligible, interpretable,and efficient speech representations.      
### 10.Atomic Filter: a Weak Form of Shift Operator for Graph Signals  [ :arrow_down: ](https://arxiv.org/pdf/2204.00457.pdf)
>  The shift operation plays a crucial role in the classical signal processing. It is the generator of all the filters and the basic operation for time-frequency analysis, such as windowed Fourier transform and wavelet transform. With the rapid development of internet technology and big data science, a large amount of data are expressed as signals defined on graphs. In order to establish the theory of filtering, windowed Fourier transform and wavelet transform in the setting of graph signals, we need to extend the shift operation of classical signals to graph signals. <br>It is a fundamental problem since the vertex set of a graph is usually not a vector space and the addition operation cannot be defined on the vertex set of the graph. In this paper, based on our understanding on the core role of shift operation in classical signal processing we propose the concept of atomic filters, which can be viewed as a weak form of the shift operator for graph signals. Then, we study the conditions such that an atomic filter is norm-preserving, periodic, or real-preserving. The property of real-preserving holds naturally in the classical signal processing, but no the research has been reported on this topic in the graph signal setting. With these conditions we propose the concept of normal atomic filters for graph signals, which degenerates into the classical shift operator under mild conditions if the graph is circulant. Typical examples of graphs that have or have not normal atomic filters are given. Finally, as an application, atomic filters are utilized to construct time-frequency atoms which constitute a frame of the graph signal space.      
### 11.Optimal Management of a Smart Port with Shore-Connection and Hydrogen Supplying by Stochastic Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.00453.pdf)
>  The paper proposes an optimal management strategy for a Smart Port equipped with renewable generation and composed by an electrified quay, operating Cold-Ironing, and a Hydrogen-based quay, supplying Zero-Emission Ships. One Battery Energy Storage System and one Hydrogen Energy Storage System are used to manage renewable energy sources and to supply electric and hydrogen-fueled ships. A model predictive control based algorithm is designed to define the best economic strategy to be followed during operations. The control algorithm takes into account the uncertainties of renewable energy generation using stochastic optimization. The performance of the approach is tested on a potential future Smart Port equipped with wind and photovoltaic generation.      
### 12.AdaSpeech 4: Adaptive Text to Speech in Zero-Shot Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2204.00436.pdf)
>  Adaptive text to speech (TTS) can synthesize new voices in zero-shot scenarios efficiently, by using a well-trained source TTS model without adapting it on the speech data of new speakers. Considering seen and unseen speakers have diverse characteristics, zero-shot adaptive TTS requires strong generalization ability on speaker characteristics, which brings modeling challenges. In this paper, we develop AdaSpeech 4, a zero-shot adaptive TTS system for high-quality speech synthesis. We model the speaker characteristics systematically to improve the generalization on new speakers. Generally, the modeling of speaker characteristics can be categorized into three steps: extracting speaker representation, taking this speaker representation as condition, and synthesizing speech/mel-spectrogram given this speaker representation. Accordingly, we improve the modeling in three steps: 1) To extract speaker representation with better generalization, we factorize the speaker characteristics into basis vectors and extract speaker representation by weighted combining of these basis vectors through attention. 2) We leverage conditional layer normalization to integrate the extracted speaker representation to TTS model. 3) We propose a novel supervision loss based on the distribution of basis vectors to maintain the corresponding speaker characteristics in generated mel-spectrograms. Without any fine-tuning, AdaSpeech 4 achieves better voice quality and similarity than baselines in multiple datasets.      
### 13.Physics-guided neural networks for feedforward control: From consistent identification to feedforward controller design  [ :arrow_down: ](https://arxiv.org/pdf/2204.00431.pdf)
>  Model-based feedforward control improves tracking performance of motion systems, provided that the model describing the inverse dynamics is of sufficient accuracy. Model sets, such as neural networks (NNs) and physics-guided neural networks (PGNNs) are typically used as flexible parametrizations that enable accurate identification of the inverse system dynamics. Currently, these (PG)NNs are used to identify the inverse dynamics directly. However, direct identification of the inverse dynamics is sensitive to noise that is present in the training data, and thereby results in biased parameter estimates which limit the achievable tracking performance. In order to push performance further, it is therefore crucial to account for noise when performing the identification. To address this problem, this paper proposes the use of a forward system identification using (PG)NNs from noisy data. Afterwards, two methods are proposed for inverting PGNNs to design a feedforward controller for high-precision motion control. The developed methodology is validated on a real-life industrial linear motor, where it showed significant improvements in tracking performance with respect to the direct inverse identification.      
### 14.Comparison of convolutional neural networks for cloudy optical images reconstruction from single or multitemporal joint SAR and optical images  [ :arrow_down: ](https://arxiv.org/pdf/2204.00424.pdf)
>  With the increasing availability of optical and synthetic aperture radar (SAR) images thanks to the Sentinel constellation, and the explosion of deep learning, new methods have emerged in recent years to tackle the reconstruction of optical images that are impacted by clouds. In this paper, we focus on the evaluation of convolutional neural networks that use jointly SAR and optical images to retrieve the missing contents in one single polluted optical image. We propose a simple framework that ease the creation of datasets for the training of deep nets targeting optical image reconstruction, and for the validation of machine learning based or deterministic approaches. These methods are quite different in terms of input images constraints, and comparing them is a problematic task not addressed in the literature. We show how space partitioning data structures help to query samples in terms of cloud coverage, relative acquisition date, pixel validity and relative proximity between SAR and optical images. We generate several datasets to compare the reconstructed images from networks that use a single pair of SAR and optical image, versus networks that use multiple pairs, and a traditional deterministic approach performing interpolation in temporal domain.      
### 15.Learning to Deblur using Light Field Generated and Real Defocus Images  [ :arrow_down: ](https://arxiv.org/pdf/2204.00367.pdf)
>  Defocus deblurring is a challenging task due to the spatially varying nature of defocus blur. While deep learning approach shows great promise in solving image restoration problems, defocus deblurring demands accurate training data that consists of all-in-focus and defocus image pairs, which is difficult to collect. Naive two-shot capturing cannot achieve pixel-wise correspondence between the defocused and all-in-focus image pairs. Synthetic aperture of light fields is suggested to be a more reliable way to generate accurate image pairs. However, the defocus blur generated from light field data is different from that of the images captured with a traditional digital camera. In this paper, we propose a novel deep defocus deblurring network that leverages the strength and overcomes the shortcoming of light fields. We first train the network on a light field-generated dataset for its highly accurate image correspondence. Then, we fine-tune the network using feature loss on another dataset collected by the two-shot method to alleviate the differences between the defocus blur exists in the two domains. This strategy is proved to be highly effective and able to achieve the state-of-the-art performance both quantitatively and qualitatively on multiple test sets. Extensive ablation studies have been conducted to analyze the effect of each network module to the final performance.      
### 16.Algebraic connectivity of layered path graphs under node deletion  [ :arrow_down: ](https://arxiv.org/pdf/2204.00356.pdf)
>  This paper studies the relation between node deletion and the algebraic connectivity for a graph with a hierarchical structure represented by layers. The problem is motivated by a mobile robot formation control guided by a leader. In particular, we consider a scenario in which robots may leave the network resulting in the removal of the nodes and the associated edges. We show that the existence of at least one neighbor in the upper layer is crucial for the algebraic connectivity not to deteriorate by node deletion.      
### 17.Impact of spatiotemporal heterogeneity in heat pump loads on generation and storage requirements  [ :arrow_down: ](https://arxiv.org/pdf/2204.00353.pdf)
>  This paper investigates how spatiotemporal heterogeneity in inflexible residential heat pump loads affects the need for storage and generation in the electricity system under business-as-usual and low-carbon emissions budgets. Homogeneous and heterogeneous heat pump loads are generated using population-weighted average and local temperature, respectively, assuming complete residential heat pump penetration. The results of a storage and generation optimal expansion model with network effects for spatiotemporally homogeneous and heterogeneous load profiles are compared. A case study is performed using a 3-bus network of London, Manchester, and Glasgow in Britain for load and weather data for representative weeks. Using heterogeneous heating demand data changes storage sizing: under a business-as-usual budget, 26% more total storage is built on an energy and power basis, and this storage is distributed among all of the buses in the heterogeneous case. Under a low-carbon budget, total energy storage at all buses increases 2 times on an energy basis and 40% on a power basis. The energy to power ratio of storage at each bus also increases when accounting for heterogeneity; this change suggests that storage will be needed to provide energy support in addition to power support for electric heating in high-renewable power systems. Accounting for heterogeneity also increases modeled systems costs, particularly capital costs, because of the need for higher generation capacity in the largest load center and coincidence of local peak demand at different buses. These results show the importance of accounting for heat pump load heterogeneity in power system planning.      
### 18.Computation of Optical Refractive Index Structure Parameter from its Statistical Definition Using Radiosonde Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.00349.pdf)
>  Knowledge of the optical refractive index structure parameter $C_n^2$ is of interest for Free Space Optics (FSO) and ground-based optical astronomy, as it depicts the strength of the expected scintillation on the received optical waves. Focus is given here to models using meteorological quantities coming from radiosonde measurements as inputs to estimate the $C_n^2$ profile in the atmosphere. A model relying on the $C_n^2$ statistical definition is presented and applied to recent high-density radiosonde profiles at Trappes (France) and Hilo, HI (USA). It is also compared to thermosonde measurements coming from the T-REX campaign. This model enables to obtain site-specific average profiles and to identify isolated turbulent layers using only pressure and temperature measurements, paving the way for optical site selection.      
### 19.Self-triggered MPC robust to bounded packet loss via a min-max approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.00339.pdf)
>  Networked Control Systems typically come with a limited communication bandwidth and thus require special care when designing the underlying control and triggering law. A method that allows to consider hard constraints on the communication traffic as well as on states and inputs is self-triggered model predictive control (MPC). In this scheme, the optimal length of the sampling interval is determined proactively using predictions of the system behavior. However, previous formulations of self-triggered MPC have neglected the widespread phenomenon of packet loss, such that these approaches might fail in practice. In this paper, we present a novel self-triggered MPC scheme which is robust to bounded packet loss by virtue of a min-max optimization problem. We prove recursive feasibility, constraint satisfaction and convergence to the origin for any possible packet loss realization, and demonstrate the advantages of the proposed scheme in a numerical example.      
### 20.Towards gain tuning for numerical KKL observers  [ :arrow_down: ](https://arxiv.org/pdf/2204.00318.pdf)
>  This paper presents a first step towards tuning observers for nonlinear systems. Relying on recent results around Kazantzis-Kravaris/Luenberger (KKL) observers, we propose to design a family of observers parametrized by the cut-off frequency of a linear filter. We use neural networks to learn the mapping between the observer and the nonlinear system as a function of this frequency, and present a novel method to sample the state-space efficiently for nonlinear regression. We then propose a criterion related to noise sensitivity, which can be used to tune the observer by choosing the most appropriate frequency. We illustrate the merits of this approach in numerical simulations.      
### 21.The role of living laboratories in unlocking the potential of low-carbon energy technologies on the journey to net-zero  [ :arrow_down: ](https://arxiv.org/pdf/2204.00293.pdf)
>  We demonstrate the potential role of one of the largest at scale multi-vector Smart Energy Network Demonstrator (SEND).      
### 22.To Explore or Not to Explore: Regret-Based LTL Planning in Partially-Known Environments  [ :arrow_down: ](https://arxiv.org/pdf/2204.00268.pdf)
>  In this paper, we investigate the optimal robot path planning problem for high-level specifications described by co-safe linear temporal logic (LTL) formulae. We consider the scenario where the map geometry of the workspace is partially-known. Specifically, we assume that there are some unknown regions, for which the robot does not know their successor regions a priori unless it reaches these regions physically. In contrast to the standard game-based approach that optimizes the worst-case cost, in the paper, we propose to use regret as a new metric for planning in such a partially-known environment. The regret of a plan under a fixed but unknown environment is the difference between the actual cost incurred and the best-response cost the robot could have achieved if it realizes the actual environment with hindsight. We provide an effective algorithm for finding an optimal plan that satisfies the LTL specification while minimizing its regret. A case study on firefighting robots is provided to illustrate the proposed framework. We argue that the new metric is more suitable for the scenario of partially-known environment since it captures the trade-off between the actual cost spent and the potential benefit one may obtain for exploring an unknown region.      
### 23.MS-HLMO: Multi-scale Histogram of Local Main Orientation for Remote Sensing Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2204.00260.pdf)
>  Multi-source image registration is challenging due to intensity, rotation, and scale differences among the images. Considering the characteristics and differences of multi-source remote sensing images, a feature-based registration algorithm named Multi-scale Histogram of Local Main Orientation (MS-HLMO) is proposed. Harris corner detection is first adopted to generate feature points. The HLMO feature of each Harris feature point is extracted on a Partial Main Orientation Map (PMOM) with a Generalized Gradient Location and Orientation Histogram-like (GGLOH) feature descriptor, which provides high intensity, rotation, and scale invariance. The feature points are matched through a multi-scale matching strategy. Comprehensive experiments on 17 multi-source remote sensing scenes demonstrate that the proposed MS-HLMO and its simplified version MS-HLMO$^+$ outperform other competitive registration algorithms in terms of effectiveness and generalization.      
### 24.Multiple Confidence Gates For Joint Training Of SE And ASR  [ :arrow_down: ](https://arxiv.org/pdf/2204.00226.pdf)
>  Joint training of speech enhancement model (SE) and speech recognition model (ASR) is a common solution for robust ASR in noisy environments. SE focuses on improving the auditory quality of speech, but the enhanced feature distribution is changed, which is uncertain and detrimental to the ASR. To tackle this challenge, an approach with multiple confidence gates for jointly training of SE and ASR is proposed. A speech confidence gates prediction module is designed to replace the former SE module in joint training. The noisy speech is filtered by gates to obtain features that are easier to be fitting by the ASR network. The experimental results show that the proposed method has better performance than the traditional robust speech recognition system on test sets of clean speech, synthesized noisy speech, and real noisy speech.      
### 25.End-to-End Multi-speaker ASR with Independent Vector Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2204.00218.pdf)
>  We develop an end-to-end system for multi-channel, multi-speaker automatic speech recognition. We propose a frontend for joint source separation and dereverberation based on the independent vector analysis (IVA) paradigm. It uses the fast and stable iterative source steering algorithm together with a neural source model. The parameters from the ASR module and the neural source model are optimized jointly from the ASR loss itself. We demonstrate competitive performance with previous systems using neural beamforming frontends. First, we explore the trade-offs when using various number of channels for training and testing. Second, we demonstrate that the proposed IVA frontend performs well on noisy data, even when trained on clean mixtures only. Furthermore, it extends without retraining to the separation of more speakers, which is demonstrated on mixtures of three and four speakers.      
### 26.Optimizing Pilot Spacing in MU-MIMO Systems Operating Over Aging Channels  [ :arrow_down: ](https://arxiv.org/pdf/2204.00213.pdf)
>  In the uplink of multiuser multiple input multiple output (MU-MIMO) systems operating over aging channels, pilot spacing is crucial for acquiring channel state information and achieving high signal-to-interference-plus-noise ratio (SINR). Somewhat surprisingly, very few works examine the impact of pilot spacing on the correlation structure of subsequent channel estimates and the resulting quality of channel state information considering channel aging. In this paper, we consider a fast-fading environment characterized by its exponentially decaying autocorrelation function, and model pilot spacing as a sampling problem to capture the inherent trade-off between the quality of channel state information and the number of symbols available for information carrying data symbols. We first establish a quasi-closed form for the achievable asymptotic deterministic equivalent SINR when the channel estimation algorithm utilizes multiple pilot signals. Next, we establish upper bounds on the achievable SINR and spectral efficiency, as a function of pilot spacing, which helps to find the optimum pilot spacing within a limited search space. Our key insight is that to maximize the achievable SINR and the spectral efficiency of MU-MIMO systems, proper pilot spacing must be applied to control the impact of the aging channel and to tune the trade-off between pilot and data symbols.      
### 27.Spatial Loss for Unsupervised Multi-channel Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2204.00210.pdf)
>  We propose a spatial loss for unsupervised multi-channel source separation. The proposed loss exploits the duality of direction of arrival (DOA) and beamforming: the steering and beamforming vectors should be aligned for the target source, but orthogonal for interfering ones. The spatial loss encourages consistency between the mixing and demixing systems from a classic DOA estimator and a neural separator, respectively. With the proposed loss, we train the neural separators based on minimum variance distortionless response (MVDR) beamforming and independent vector analysis (IVA). We also investigate the effectiveness of combining our spatial loss and a signal loss, which uses the outputs of blind source separation as the reference. We evaluate our proposed method on synthetic and recorded (LibriCSS) mixtures. We find that the spatial loss is most effective to train IVA-based separators. For the neural MVDR beamformer, it performs best when combined with a signal loss. On synthetic mixtures, the proposed unsupervised loss leads to the same performance as a supervised loss in terms of word error rate. On LibriCSS, we obtain close to state-of-the-art performance without any labeled training data.      
### 28.Green Routing Game: Strategic Logistical Planning using Mixed Fleets of ICEVs and EVs  [ :arrow_down: ](https://arxiv.org/pdf/2204.00209.pdf)
>  This paper introduces a "green" routing game between multiple logistic operators (players), each owning a mixed fleet of internal combustion engine vehicle (ICEV) and electric vehicle (EV) trucks. Each player faces the cost of delayed delivery (due to charging requirements of EVs) and a pollution cost levied on the ICEVs. This cost structure models: 1) limited battery capacity of EVs and their charging requirement; 2) shared nature of charging facilities; 3) pollution cost levied by regulatory agency on the use of ICEVs. We characterize Nash equilibria of this game and derive a condition for its uniqueness. We also use the gradient projection method to compute this equilibrium in a distributed manner. Our equilibrium analysis is useful to analyze the trade-off faced by players in incurring higher delay due to congestion at charging locations when the share of EVs increases versus a higher pollution cost when the share of ICEVs increases. A numerical example suggests that to increase marginal pollution cost can dramatically reduce inefficiency of equilibria.      
### 29.Epipolar Focus Spectrum: A Novel Light Field Representation and Application in Dense-view Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2204.00193.pdf)
>  Existing light field representations, such as epipolar plane image (EPI) and sub-aperture images, do not consider the structural characteristics across the views, so they usually require additional disparity and spatial structure cues for follow-up tasks. Besides, they have difficulties dealing with occlusions or larger disparity scenes. To this end, this paper proposes a novel Epipolar Focus Spectrum (EFS) representation by rearranging the EPI spectrum. Different from the classical EPI representation where an EPI line corresponds to a specific depth, there is a one-to-one mapping from the EFS line to the view. Accordingly, compared to a sparsely-sampled light field, a densely-sampled one with the same field of view (FoV) leads to a more compact distribution of such linear structures in the double-cone-shaped region with the identical opening angle in its corresponding EFS. Hence the EFS representation is invariant to the scene depth. To demonstrate its effectiveness, we develop a trainable EFS-based pipeline for light field reconstruction, where a dense light field can be reconstructed by compensating the "missing EFS lines" given a sparse light field, yielding promising results with cross-view consistency, especially in the presence of severe occlusion and large disparity. Experimental results on both synthetic and real-world datasets demonstrate the validity and superiority of the proposed method over SOTA methods.      
### 30.Pose correction scheme for camera-scanning Fourier ptychography based on camera calibration and homography transform  [ :arrow_down: ](https://arxiv.org/pdf/2204.00173.pdf)
>  Fourier ptychography (FP), as a computational imaging method, is a powerful tool to improve imaging resolution. Camera-scanning Fourier ptychography extends the application of FP from micro to macro creatively. Due to the non-ideal scanning of the camera driven by the mechanical translation stage, the pose error of the camera occurs, greatly degrading the reconstruction quality, while a precise translation stage is expensive and not suitable for wide-range imaging. Here, to improve the imaging performance of camera-scanning Fourier ptychography, we propose a pose correction scheme based on camera calibration and homography transform approaches. The scheme realizes the accurate alignment of data set and location error correction in the frequency domain. Simulation and experimental results demonstrate this method can optimize the reconstruction results and realize high-quality imaging effectively. Combined with the feature recognition algorithm, the scheme provides the possibility for applying FP in remote sensing imaging and space imaging.      
### 31.Universal Adaptor: Converting Mel-Spectrograms Between Different Configurations for Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2204.00170.pdf)
>  Most recent TTS systems are composed of a synthesizer and a vocoder. However, the existing synthesizers and vocoders can only be matched to acoustic features extracted with a specific configuration. Hence, we can't combine arbitrary synthesizers and vocoders together to form a complete TTS system, not to mention applying to a newly developed model. In this paper, we proposed a universal adaptor, which takes a Mel-spectogram parametrized by the source configuration and converts it into a Mel-spectrogram parametrized by the target configuration, as long as we feed in the source and the target configurations. Experiments show that the quality of speeches synthesized from our output of the universal adaptor is comparable to those synthesized from ground truth Mel-spectrogram. Moreover, our universal adaptor can be applied in the recent TTS systems and in multi-speaker speech synthesis without dropping quality.      
### 32.Multi-Rate Planning and Control of Uncertain Nonlinear Systems: Model Predictive Control and Control Lyapunov Functions  [ :arrow_down: ](https://arxiv.org/pdf/2204.00152.pdf)
>  Modern control systems must operate in increasingly complex environments subject to safety constraints and input limits, and are often implemented in a hierarchical fashion with different controllers running at multiple time scales. Yet traditional constructive methods for nonlinear controller synthesis typically "flatten" this hierarchy, focusing on a single time scale, and thereby limited the ability to make rigorous guarantees on constraint satisfaction that hold for the entire system. In this work we seek to address the stabilization of constrained nonlinear systems through a \textit{multi-rate} control architecture. This is accomplished by iteratively planning continuous reference trajectories for a nonlinear system using a linearized model and Model Predictive Control (MPC), and tracking said trajectories using the full-order nonlinear model and Control Lyapunov Functions (CLFs). Connecting these two levels of control design in a way that ensures constraint satisfaction is achieved through the use of \textit{Bézier curves}, which enable planning continuous trajectories respecting constraints by planning a sequence of discrete points. Our framework is encoded via convex optimization problems which may be efficiently solved, as demonstrated in simulation.      
### 33.Robust remote estimation over the collision channel in the presence of an intelligent jammer  [ :arrow_down: ](https://arxiv.org/pdf/2204.00148.pdf)
>  We consider a sensor-receiver pair communicating over a wireless channel in the presence of a jammer who may launch a denial-of-service attack. We formulate a zero-sum game between a coordinator that jointly designs the transmission and estimation policies, and the jammer. We consider two cases depending on whether the jammer can sense the channel or not. We characterize a saddle-point equilibrium for the class of symmetric and unimodal probability density functions when the jammer cannot sense the channel. If the jammer can sense if the channel is being used, we provide an efficient algorithm that alternates between iterations of Projected Gradient Ascent and the Convex-Concave Procedure to find approximate First-order Nash-Equilibria. Our numerical results show that in certain cases the jammer may decide to launch a denial-of-service attack with the goal of deceiving the receiver even when the sensor decides not to transmit.      
### 34.Machine Learning Integrated with Model Predictive Control for Imitative Optimal Control of Compression Ignition Engines  [ :arrow_down: ](https://arxiv.org/pdf/2204.00142.pdf)
>  The high thermal efficiency and reliability of the compression-ignition engine makes it the first choice for many applications. For this to continue, a reduction of the pollutant emissions is needed. One solution is the use of machine learning (ML) and model predictive control (MPC) to minimize emissions and fuel consumption, without adding substantial computational cost to the engine controller. ML is developed in this paper for both modeling engine performance and emissions and for imitating the behaviour of an Linear Parameter Varying (LPV) MPC. Using a support vector machine-based linear parameter varying model of the engine performance and emissions, a model predictive controller is implemented for a 4.5 Cummins diesel engine. This online optimized MPC solution offers advantages in minimizing the \nox~emissions and fuel consumption compared to the baseline feedforward production controller. To reduce the computational cost of this MPC, a deep learning scheme is designed to mimic the behavior of the developed controller. The performance in reducing NOx emissions at a constant load by the imitative controller is similar to that of the online optimized MPC compared to the Cummins production controller. In addition, the imitative controller requires 50 times less computation time compared to that of the online MPC optimization.      
### 35.Integration of Deep Learning and Nonlinear Model Predictive Control for Emission Reduction of Compression Ignition Combustion Engines: A Simulation Study  [ :arrow_down: ](https://arxiv.org/pdf/2204.00139.pdf)
>  Machine learning (ML) and nonlinear model predictive control (NMPC) are used in this paper to minimize the emissions and fuel consumption of a compression ignition engine. In this work machine learning is used for two applications. In the first application, ML is to identify a model for implementation in model predictive control optimization problems. In the second application, ML is used as a replacement of a model predictive controller where the ML controller learns the optimal control action by imitating or mimicking the behavior of the model predictive controller. In this study, a deep recurrent neural network including long-short term memory (LSTM) layers are used to model the emission and performance of a 4.5 liter 4-cylinder Cummins compression ignition engine. This model is then used for model predictive controller implementation. Then, a deep learning scheme is deployed to clone the behavior of the developed controller. In the LSTM integration, a novel scheme is used by augmenting hidden and cell states of the network in an NMPC optimization problem. The developed LSTM-NMPC and the imitative NMPC are compared with the calibrated Cummins Engine Control Unit (ECU) model in an experimentally validated engine simulation platform. Results show a significant reduction in Nitrogen Oxides (NOx) emissions and a slight decrease in the injected fuel quantity while maintaining the same load. In addition, the imitative NMPC has a similar performance as the NMPC but with two orders of magnitude less computation time.      
### 36.Perceptual Quality Assessment of UGC Gaming Videos  [ :arrow_down: ](https://arxiv.org/pdf/2204.00128.pdf)
>  In recent years, with the vigorous development of the video game industry, the proportion of gaming videos on major video websites like YouTube has dramatically increased. However, relatively little research has been done on the automatic quality prediction of gaming videos, especially on those that fall in the category of "User-Generated-Content" (UGC). Since current leading general-purpose Video Quality Assessment (VQA) models do not perform well on this type of gaming videos, we have created a new VQA model specifically designed to succeed on UGC gaming videos, which we call the Gaming Video Quality Predictor (GAME-VQP). GAME-VQP successfully predicts the unique statistical characteristics of gaming videos by drawing upon features designed under modified natural scene statistics models, combined with gaming specific features learned by a Convolution Neural Network. We study the performance of GAME-VQP on a very recent large UGC gaming video database called LIVE-YT-Gaming, and find that it both outperforms other mainstream general VQA models as well as VQA models specifically designed for gaming videos. The new model will be made public after paper being accepted.      
### 37.Synthesis of Stabilizing Recurrent Equilibrium Network Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2204.00122.pdf)
>  We propose a parameterization of a nonlinear dynamic controller based on the recurrent equilibrium network, a generalization of the recurrent neural network. We derive constraints on the parameterization under which the controller guarantees exponential stability of a partially observed dynamical system with sector-bounded nonlinearities. Finally, we present a method to synthesize this controller using projected policy gradient methods to maximize a reward function with arbitrary structure. The projection step involves the solution of convex optimization problems. We demonstrate the proposed method with simulated examples of controlling nonlinear plants, including plants modeled with neural networks.      
### 38.Gallium Oxide Heterojunction Diodes for Improved High-Temperature Performance  [ :arrow_down: ](https://arxiv.org/pdf/2204.00112.pdf)
>  ${\beta}$-Ga${_2}$O${_3}$ based semiconductor devices are expected to have significantly improved high-power and high-temperature performance due to its ultra-wide bandgap of close to 5 eV. However, the high-temperature operation of these ultra-wide-bandgap devices is usually limited by the relatively low 1-2 eV built-in potential at the Schottky barrier with most high-work-function metals. Here, we report heterojunction p-NiO/n-${\beta}$-Ga${_2}$O${_3}$ diodes fabrication and optimization for high-temperature device applications, demonstrating a current rectification ratio of more than 10${^6}$ at 410°C. The NiO heterojunction diode can achieve higher turn-on voltage and lower reverse leakage current compared to the Ni-based Schottky diode fabricated on the same single crystal ${\beta}$-Ga${_2}$O${_3}$ substrate, despite charge transport dominated by interfacial recombination. Electrical characterization and device modeling show that these advantages are due to a higher built-in potential and additional band offset. These results suggest that heterojunction p-n diodes based on ${\beta}$-Ga${_2}$O${_3}$ can significantly improve high-temperature electronic device and sensor performance.      
### 39.Tooth Instance Segmentation on Panoramic Dental Radiographs Using U-Nets and Morphological Processing  [ :arrow_down: ](https://arxiv.org/pdf/2204.00095.pdf)
>  Automatic teeth segmentation in panoramic x-ray images is an important research subject of the image analysis in dentistry. In this study, we propose a post-processing stage to obtain a segmentation map in which the objects in the image are separated, and apply this technique to tooth instance segmentation with U-Net network. The post-processing consists of grayscale morphological and filtering operations, which are applied to the sigmoid output of the network before binarization. A dice overlap score of 95.4 - 0.3% is obtained in overall teeth segmentation. The proposed post-processing stages reduce the mean error of tooth count to 6.15%, whereas the error without post-processing is 26.81%. The performances of both segmentation and tooth counting are the highest in the literature, to our knowledge. Moreover, this is achieved by using a relatively small training dataset, which consists of 105 images. Although the aim in this study is to segment tooth instances, the presented method is applicable to similar problems in other domains, such as separating the cell instances      
### 40.On-line Estimation of Stability and Passivity Metrics  [ :arrow_down: ](https://arxiv.org/pdf/2204.00073.pdf)
>  We consider the problem of on-line evaluation of critical characteristic parameters such as the L_2-gain (L2G), input feedforward passivity index (IFP) and output feedback passivity index (OFP) of non-linear systems using their input-output data. Typically, having an accurate measure of such "system indices" enables the application of systematic control design techniques. Moreover, if such system indices can efficiently be evaluated on-line, they can be exploited to device intelligent controller reconfiguration and fault-tolerant control techniques. However, the existing estimation methods of such system indices (i.e., L2G, IFP and OFP) are predominantly off-line, computationally inefficient, and require a large amount of actual or synthetically generated input-output trajectory data under some specific initial/terminal conditions. On the other hand, the existing on-line estimation methods take an averaging-based approach, which may be sub-optimal, computationally inefficient and susceptible to estimate saturation. In this paper, to overcome these challenges (in the on-line estimation of system indices), we establish and exploit several interesting theoretical results on a particular class of fractional function optimization problems. For comparison purposes, the details of an existing averaging-based approach are provided for the same on-line estimation problem. Finally, several numerical examples are discussed to demonstrate the proposed on-line estimation approach and to highlight our contributions.      
### 41.Numerical Solution of the Steady-State Network Flow Equations for a Non-Ideal Gas  [ :arrow_down: ](https://arxiv.org/pdf/2204.00071.pdf)
>  We formulate a steady-state network flow problem for non-ideal gas that relates injection rates and nodal pressures in the network to flows in pipes. For this problem, we present and prove a theorem on uniqueness of generalized solution for a broad class of non-ideal pressure-density relations that satisfy a monotonicity property. Further, we develop a Newton-Raphson algorithm for numerical solution of the steady-state problem, which is made possible by a systematic non-dimensionalization of the equations. The developed algorithm has been extensively tested on benchmark instances and shown to converge robustly to a generalized solution. Previous results indicate that the steady-state network flow equations for an ideal gas are difficult to solve by the Newton-Raphson method because of its extreme sensitivity to the initial guess. In contrast, we find that non-dimensionalization of the steady-state problem is key to robust convergence of the Newton-Raphson method. We identify criteria based on the uniqueness of solutions under which the existence of a non-physical generalized solution found by a non-linear solver implies non-existence of a physical solution, i.e., infeasibility of the problem. Finally, we compare pressure and flow solutions based on ideal and non-ideal equations of state to demonstrate the need to apply the latter in practice. The solver developed in this article is open-source and is made available for both the academic and research communities as well as the industry.      
### 42.Automatic Classification of Alzheimer's Disease using brain MRI data and deep Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.00068.pdf)
>  Alzheimer's disease (AD) is one of the most common public health issues the world is facing today. This disease has a high prevalence primarily in the elderly accompanying memory loss and cognitive decline. AD detection is a challenging task which many authors have developed numerous computerized automatic diagnosis systems utilizing neuroimaging and other clinical data. MRI scans provide high-intensity visible features, making these scans the most widely used brain imaging technique. In recent years deep learning has achieved leading success in medical image analysis. But a relatively little investigation has been done to apply deep learning techniques for the brain MRI classification. This paper explores the construction of several deep learning architectures evaluated on brain MRI images and segmented images. The idea behind segmented images investigates the influence of image segmentation step on deep learning classification. The image processing presented a pipeline consisting of pre-processing to enhance the MRI scans and post-processing consisting of a segmentation method for segmenting the brain tissues. The results show that the processed images achieved a better accuracy in the binary classification of AD vs. CN (Cognitively Normal) across four different architectures. ResNet architecture resulted in the highest prediction accuracy amongst the other architectures (90.83% for the original brain images and 93.50% for the processed images).      
### 43.Importance of Different Temporal Modulations of Speech: A Tale of Two Perspectives  [ :arrow_down: ](https://arxiv.org/pdf/2204.00065.pdf)
>  How important are different temporal speech modulations for speech recognition? We answer this question from two complementary perspectives. Firstly, we quantify the amount of phonetic information in the modulation spectrum of speech by computing the mutual information between temporal modulations with frame-wise phoneme labels. Looking from another perspective, we ask - which speech modulations does an Automatic Speech Recognition (ASR) system prefer for its operation. Data-driven weights are learnt over the modulation spectrum and optimized for an end-to-end ASR task. Both methods unanimously agree that speech information is mostly contained in slow modulation. Maximum mutual information occurs around 3-6 Hz which also happens to be the range of modulations most preferred by the ASR. In addition, we show that incorporation of this knowledge into ASRs significantly reduces its dependency on the amount of training data.      
### 44.Lyapunov based Stochastic Stability of Human-Machine Interaction: A Quantum Decision System Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.00059.pdf)
>  In mathematical psychology, decision makers are modeled using the Lindbladian equations from quantum mechanics to capture important human-centric features such as order effects and violation of the sure thing principle. We consider human-machine interaction involving a quantum decision maker (human) and a controller (machine). Given a sequence of human decisions over time, how can the controller dynamically provide input messages to adapt these decisions so as to converge to a specific decision? We show via novel stochastic Lyapunov arguments how the Lindbladian dynamics of the quantum decision maker can be controlled to converge to a specific decision asymptotically. Our methodology yields a useful mathematical framework for human-sensor decision making. The stochastic Lyapunov results are also of independent interest as they generalize recent results in the literature.      
### 45.Leakage Localization in Water Distribution Networks: A Model-Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.00050.pdf)
>  The paper studies the problem of leakage localization in water distribution networks. For the case of a single pipe that suffers from a single leak, by taking recourse to pressure and flow measurements, and assuming those are noiseless, we provide a closed-form expression for leak localization, leak exponent and leak constant. For the aforementioned setting, but with noisy pressure and flow measurements, an expression for estimating the location of the leak is provided. Finally, assuming the existence of a single leak, for a network comprising of more than one pipe and assuming that the network has a tree structure, we provide a systematic procedure for determining the leak location, the leak exponent, and the leak constant      
### 46.Quantized GAN for Complex Music Generation from Dance Videos  [ :arrow_down: ](https://arxiv.org/pdf/2204.00604.pdf)
>  We present Dance2Music-GAN (D2M-GAN), a novel adversarial multi-modal framework that generates complex musical samples conditioned on dance videos. Our proposed framework takes dance video frames and human body motion as input, and learns to generate music samples that plausibly accompany the corresponding input. Unlike most existing conditional music generation works that generate specific types of mono-instrumental sounds using symbolic audio representations (e.g., MIDI), and that heavily rely on pre-defined musical synthesizers, in this work we generate dance music in complex styles (e.g., pop, breakdancing, etc.) by employing a Vector Quantized (VQ) audio representation, and leverage both its generality and the high abstraction capacity of its symbolic and continuous counterparts. By performing an extensive set of experiments on multiple datasets, and following a comprehensive evaluation protocol, we assess the generative quality of our approach against several alternatives. The quantitative results, which measure the music consistency, beats correspondence, and music diversity, clearly demonstrate the effectiveness of our proposed method. Last but not least, we curate a challenging dance-music dataset of in-the-wild TikTok videos, which we use to further demonstrate the efficacy of our approach in real-world applications - and which we hope to serve as a starting point for relevant future research.      
### 47.Prefix-Free Coding for LQG Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.00588.pdf)
>  In this work, we develop quantization and variable-length source codecs for the feedback links in linear-quadratic-Gaussian (LQG) control systems. We prove that for any fixed control performance, the approaches we propose nearly achieve lower bounds on communication cost that have been established in prior work. In particular, we refine the analysis of a classical achievability approach with an eye towards more practical details. Notably, in the prior literature the source codecs used to demonstrate the (near) achievability of these lower bounds are often implicitly assumed to be time-varying. For single-input single-output (SISO) plants, we prove that it suffices to consider time-invariant quantization and source coding. This result follows from analyzing the long-term stochastic behavior of the system's quantized measurements and reconstruction errors. To our knowledge, this time-invariant achievability result is the first in the literature.      
### 48.Uniform quasi-convex optimisation via Extremum Seeking  [ :arrow_down: ](https://arxiv.org/pdf/2204.00580.pdf)
>  The paper deals with a well-known extremum seeking scheme by proving uniformity properties with respect to the amplitudes of the dither signal and of the cost function. Those properties are then used to show that the scheme guarantees the global minimiser to be semi-global practically stable despite the presence of local saddle points. To achieve these results, we analyse the average system associated with the extremum seeking scheme via arguments based on the Fourier series.      
### 49.Multi-task RNN-T with Semantic Decoder for Streamable Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2204.00558.pdf)
>  End-to-end Spoken Language Understanding (E2E SLU) has attracted increasing interest due to its advantages of joint optimization and low latency when compared to traditionally cascaded pipelines. Existing E2E SLU models usually follow a two-stage configuration where an Automatic Speech Recognition (ASR) network first predicts a transcript which is then passed to a Natural Language Understanding (NLU) module through an interface to infer semantic labels, such as intent and slot tags. This design, however, does not consider the NLU posterior while making transcript predictions, nor correct the NLU prediction error immediately by considering the previously predicted word-pieces. In addition, the NLU model in the two-stage system is not streamable, as it must wait for the audio segments to complete processing, which ultimately impacts the latency of the SLU system. In this work, we propose a streamable multi-task semantic transducer model to address these considerations. Our proposed architecture predicts ASR and NLU labels auto-regressively and uses a semantic decoder to ingest both previously predicted word-pieces and slot tags while aggregating them through a fusion network. Using an industry scale SLU and a public FSC dataset, we show the proposed model outperforms the two-stage E2E SLU model for both ASR and NLU metrics.      
### 50.End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation  [ :arrow_down: ](https://arxiv.org/pdf/2204.00540.pdf)
>  This work presents our end-to-end (E2E) automatic speech recognition (ASR) model targetting at robust speech recognition, called Integraded speech Recognition with enhanced speech Input for Self-supervised learning representation (IRIS). Compared with conventional E2E ASR models, the proposed E2E model integrates two important modules including a speech enhancement (SE) module and a self-supervised learning representation (SSLR) module. The SE module enhances the noisy speech. Then the SSLR module extracts features from enhanced speech to be used for speech recognition (ASR). To train the proposed model, we establish an efficient learning scheme. Evaluation results on the monaural CHiME-4 task show that the IRIS model achieves the best performance reported in the literature for the single-channel CHiME-4 benchmark (2.0% for the real development and 3.9% for the real test) thanks to the powerful pre-trained SSLR module and the fine-tuned SE module.      
### 51.Convergence Rate Bounds for the Mirror Descent Method: IQCs and the Bregman Divergence  [ :arrow_down: ](https://arxiv.org/pdf/2204.00502.pdf)
>  This paper is concerned with convergence analysis for the mirror descent (MD) method, a well-known algorithm in convex optimization. An analysis framework via integral quadratic constraints (IQCs) is constructed to analyze the convergence rate of the MD method with strongly convex objective functions in both continuous-time and discrete-time. We formulate the problem of finding convergence rates of the MD algorithms into feasibility problems of linear matrix inequalities (LMIs) in both schemes. In particular, in continuoustime, we show that the Bregman divergence function, which is commonly used as a Lyapunov function for this algorithm, is a special case of the class of Lyapunov functions associated with the Popov criterion, when the latter is applied to an appropriate reformulation of the problem. Thus, applying the Popov criterion and its combination with other IQCs, can lead to convergence rate bounds with reduced conservatism. We also illustrate via examples that the convergence rate bounds derived can be tight.      
### 52.FrequencyLowCut Pooling -- Plug &amp; Play against Catastrophic Overfitting  [ :arrow_down: ](https://arxiv.org/pdf/2204.00491.pdf)
>  Over the last years, Convolutional Neural Networks (CNNs) have been the dominating neural architecture in a wide range of computer vision tasks. From an image and signal processing point of view, this success might be a bit surprising as the inherent spatial pyramid design of most CNNs is apparently violating basic signal processing laws, i.e. Sampling Theorem in their down-sampling operations. However, since poor sampling appeared not to affect model accuracy, this issue has been broadly neglected until model robustness started to receive more attention. Recent work [17] in the context of adversarial attacks and distribution shifts, showed after all, that there is a strong correlation between the vulnerability of CNNs and aliasing artifacts induced by poor down-sampling operations. This paper builds on these findings and introduces an aliasing free down-sampling operation which can easily be plugged into any CNN architecture: FrequencyLowCut pooling. Our experiments show, that in combination with simple and fast FGSM adversarial training, our hyper-parameter free operator significantly improves model robustness and avoids catastrophic overfitting.      
### 53.Accelerating Federated Edge Learning via Topology Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.00489.pdf)
>  Federated edge learning (FEEL) is envisioned as a promising paradigm to achieve privacy-preserving distributed learning. However, it consumes excessive learning time due to the existence of straggler devices. In this paper, a novel topology-optimized federated edge learning (TOFEL) scheme is proposed to tackle the heterogeneity issue in federated learning and to improve the communication-and-computation efficiency. Specifically, a problem of jointly optimizing the aggregation topology and computing speed is formulated to minimize the weighted summation of energy consumption and latency. To solve the mixed-integer nonlinear problem, we propose a novel solution method of penalty-based successive convex approximation, which converges to a stationary point of the primal problem under mild conditions. To facilitate real-time decision making, an imitation-learning based method is developed, where deep neural networks (DNNs) are trained offline to mimic the penalty-based method, and the trained imitation DNNs are deployed at the edge devices for online inference. Thereby, an efficient imitate-learning based approach is seamlessly integrated into the TOFEL framework. Simulation results demonstrate that the proposed TOFEL scheme accelerates the federated learning process, and achieves a higher energy efficiency. Moreover, we apply the scheme to 3D object detection with multi-vehicle point cloud datasets in the CARLA simulator. The results confirm the superior learning performance of the TOFEL scheme over conventional designs with the same resource and deadline constraints.      
### 54.Autonomous crater detection on asteroids using a fully-convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2204.00477.pdf)
>  This paper shows the application of autonomous Crater Detection using the U-Net, a Fully-Convolutional Neural Network, on Ceres. The U-Net is trained on optical images of the Moon Global Morphology Mosaic based on data collected by the LRO and manual crater catalogues. The Moon-trained network will be tested on Dawn optical images of Ceres: this task is accomplished by means of a Transfer Learning (TL) approach. The trained model has been fine-tuned using 100, 500 and 1000 additional images of Ceres. The test performance was measured on 350 never before seen images, reaching a testing accuracy of 96.24%, 96.95% and 97.19%, respectively. This means that despite the intrinsic differences between the Moon and Ceres, TL works with encouraging results. The output of the U-Net contains predicted craters: it will be post-processed applying global thresholding for image binarization and a template matching algorithm to extract craters positions and radii in the pixel space. Post-processed craters will be counted and compared to the ground truth data in order to compute image segmentation metrics: precision, recall and F1 score. These indices will be computed, and their effect will be discussed for tasks such as automated crater cataloguing and optical navigation.      
### 55.Marginal Contrastive Correspondence for Guided Image Generation  [ :arrow_down: ](https://arxiv.org/pdf/2204.00442.pdf)
>  Exemplar-based image translation establishes dense correspondences between a conditional input and an exemplar (from two different domains) for leveraging detailed exemplar styles to achieve realistic image translation. Existing work builds the cross-domain correspondences implicitly by minimizing feature-wise distances across the two domains. Without explicit exploitation of domain-invariant features, this approach may not reduce the domain gap effectively which often leads to sub-optimal correspondences and image translation. We design a Marginal Contrastive Learning Network (MCL-Net) that explores contrastive learning to learn domain-invariant features for realistic exemplar-based image translation. Specifically, we design an innovative marginal contrastive loss that guides to establish dense correspondences explicitly. Nevertheless, building correspondence with domain-invariant semantics alone may impair the texture patterns and lead to degraded texture generation. We thus design a Self-Correlation Map (SCM) that incorporates scene structures as auxiliary information which improves the built correspondences substantially. Quantitative and qualitative experiments on multifarious image translation tasks show that the proposed method outperforms the state-of-the-art consistently.      
### 56.Inverse Design and Experimental Verification of a Bianisotropic Metasurface Using Optimization and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.00433.pdf)
>  Electromagnetic metasurfaces have attracted significant interest recently due to their low profile and advantageous applications. Practically, many metasurface designs start with a set of constraints for the radiated far-field, such as main-beam direction(s) and side lobe levels, and end with a non-uniform physical structure for the surface. This problem is quite challenging, since the required tangential field transformations are not completely known when only constraints are placed on the scattered fields. Hence, the required surface properties cannot be solved for analytically. Moreover, the translation of the desired surface properties to the physical unit cells can be time-consuming and difficult, as it is often a one-to-many mapping in a large solution space. Here, we divide the inverse design process into two steps: a macroscopic and microscopic design step. In the former, we use an iterative optimization process to find the surface properties that radiate a far-field pattern that complies with specified constraints. This iterative process exploits non-radiating currents to ensure a passive and lossless design. In the microscopic step, these optimized surface properties are realized with physical unit cells using machine learning surrogate models. The effectiveness of this end-to-end synthesis process is demonstrated through measurement results of a beam-splitting prototype.      
### 57.Synthetic Photovoltaic and Wind Power Forecasting Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.00411.pdf)
>  Photovoltaic and wind power forecasts in power systems with a high share of renewable energy are essential in several applications. These include stable grid operation, profitable power trading, and forward-looking system planning. However, there is a lack of publicly available datasets for research on machine learning based prediction methods. This paper provides an openly accessible time series dataset with realistic synthetic power data. Other publicly and non-publicly available datasets often lack precise geographic coordinates, timestamps, or static power plant information, e.g., to protect business secrets. On the opposite, this dataset provides these. The dataset comprises 120 photovoltaic and 273 wind power plants with distinct sides all over Germany from 500 days in hourly resolution. This large number of available sides allows forecasting experiments to include spatial correlations and run experiments in transfer and multi-task learning. It includes side-specific, power source-dependent, non-synthetic input features from the ICON-EU weather model. A simulation of virtual power plants with physical models and actual meteorological measurements provides realistic synthetic power measurement time series. These time series correspond to the power output of virtual power plants at the location of the respective weather measurements. Since the synthetic time series are based exclusively on weather measurements, possible errors in the weather forecast are comparable to those in actual power data. In addition to the data description, we evaluate the quality of weather-prediction-based power forecasts by comparing simplified physical models and a machine learning model. This experiment shows that forecasts errors on the synthetic power data are comparable to real-world historical power measurements.      
### 58.On the Efficiency of Integrating Self-supervised Learning and Meta-learning for User-defined Few-shot Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2204.00352.pdf)
>  User-defined keyword spotting is a task to detect new spoken terms defined by users. This can be viewed as a few-shot learning problem since it is unreasonable for users to define their desired keywords by providing many examples. To solve this problem, previous works try to incorporate self-supervised learning models or apply meta-learning algorithms. But it is unclear whether self-supervised learning and meta-learning are complementary and which combination of the two types of approaches is most effective for few-shot keyword discovery. In this work, we systematically study these questions by utilizing various self-supervised learning models and combining them with a wide variety of meta-learning algorithms. Our result shows that HuBERT combined with Matching network achieves the best result and is robust to the changes of few-shot examples.      
### 59.WavFT: Acoustic model finetuning with labelled and unlabelled data  [ :arrow_down: ](https://arxiv.org/pdf/2204.00348.pdf)
>  Unsupervised and self-supervised learning methods have leveraged unlabelled data to improve the pretrained models. However, these methods need significantly large amount of unlabelled data and the computational cost of training models with such large amount of data can be prohibitively high. We address this issue by using unlabelled data during finetuning, instead of pretraining. We propose acoustic model finetuning (FT) using labelled and unlabelled data. The model is jointly trained to learn representations to classify senones, as well as learn contextual acoustic representations. Our training objective is a combination of cross entropy loss, suitable for classification task, and contrastive loss, suitable to learn acoustic representations. The proposed approach outperforms conventional finetuning with 11.2% and 9.19% word error rate relative (WERR) reduction on Gujarati and Bengali languages respectively.      
### 60.Using segment-based features of jaw movements to recognize foraging activities in grazing cattle  [ :arrow_down: ](https://arxiv.org/pdf/2204.00331.pdf)
>  Precision livestock farming optimizes livestock production through the use of sensor information and communication technologies to support decision making, proactively and near real-time. Among available technologies to monitor foraging behavior, the acoustic method has been highly reliable and repeatable, but can be subject to further computational improvements to increase precision and specificity of recognition of foraging activities. In this study, an algorithm called Jaw Movement segment-based Foraging Activity Recognizer (JMFAR) is proposed. The method is based on the computation and analysis of temporal, statistical and spectral features of jaw movement sounds for detection of rumination and grazing bouts. They are called JM-segment features because they are extracted from a sound segment and expect to capture JM information of the whole segment rather than individual JMs. Two variants of the method are proposed and tested: (i) the temporal and statistical features only JMFAR-ns; and (ii) a feature selection process (JMFAR-sel). The JMFAR was tested on signals registered in a free grazing environment, achieving an average weighted F1-score greater than 95%. Then, it was compared with a state-of-the-art algorithm, showing improved performance for estimation of grazing bouts (+19%). The JMFAR-ns variant reduced the computational cost by 25.4%, but achieved a slightly lower performance than the JMFAR. The good performance and low computational cost of JMFAR-ns supports the feasibility of using this algorithm variant for real-time implementation in low-cost embedded systems.      
### 61.State-feedback Abstractions for Optimal Control of Piecewise-affine Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.00315.pdf)
>  In this manuscript, we investigate symbolic abstractions that capture the behavior of piecewise-affine systems under input constraints and bounded external noise. This is accomplished by considering local affine feedback controllers that are jointly designed with the symbolic model, which ensures that an alternating simulation relation between the system and the abstraction holds. The resulting symbolic system is called a state-feedback abstraction and we show that it can be deterministic even when the original piecewise-affine system is unstable and non-deterministic. One benefit of this approach is the fact that the input space need not be discretized and the symbolic-input space is reduced to a finite set of controllers. When ellipsoidal cells and affine controllers are considered, we present necessary and sufficient conditions written as a semi-definite program for the existence of a transition and a robust upper bound on the transition cost. Two examples illustrate particular aspects of the theory and its applicability.      
### 62.Speaker verification in mismatch training and testing conditions  [ :arrow_down: ](https://arxiv.org/pdf/2204.00311.pdf)
>  This paper presents an exhaustive study about the robustness of several parameterizations, with a new database specially acquired for the purpose of a speaker recognition application. This database includes the following variations: different recording sessions (including telephonic and microphonic recordings), recording rooms, and languages (it has been obtained from a bilingual set of speakers). This study has been performed with covariance matrices in a text independent speaker verification application. It reveals that the combination of several parameterizations can improve the robustness in all the scenarios.      
### 63.Text-To-Speech Data Augmentation for Low Resource Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.00291.pdf)
>  Nowadays, the main problem of deep learning techniques used in the development of automatic speech recognition (ASR) models is the lack of transcribed data. The goal of this research is to propose a new data augmentation method to improve ASR models for agglutinative and low-resource languages. This novel data augmentation method generates both synthetic text and synthetic audio. Some experiments were conducted using the corpus of the Quechua language, which is an agglutinative and low-resource language. In this study, a sequence-to-sequence (seq2seq) model was applied to generate synthetic text, in addition to generating synthetic speech using a text-to-speech (TTS) model for Quechua. The results show that the new data augmentation method works well to improve the ASR model for Quechua. In this research, an 8.73% improvement in the word-error-rate (WER) of the ASR model is obtained using a combination of synthetic text and synthetic speech.      
### 64.Globally Optimal Spectrum- and Energy-Efficient Beamforming for Rate Splitting Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2204.00273.pdf)
>  Rate splitting multiple access (RSMA) is a promising non-orthogonal transmission strategy for next-generation wireless networks. It has been shown to outperform existing multiple access schemes in terms of spectral and energy efficiency when suboptimal beamforming schemes are employed. In this work, we fill the gap between suboptimal and truly optimal beamforming schemes and conclusively establish the superior spectral and energy efficiency of RSMA. To this end, we propose a successive incumbent transcending (SIT) branch and bound (BB) algorithm to find globally optimal beamforming solutions that maximize the weighted sum rate or energy efficiency of RSMA in Gaussian multiple-input single-output (MISO) broadcast channels. Numerical results show that RSMA exhibits an explicit globally optimal spectral and energy efficiency gain over conventional multi-user linear precoding (MU-LP) and power-domain non-orthogonal multiple access (NOMA). Compared to existing globally optimal beamforming algorithms for MU-LP, the proposed SIT BB not only improves the numerical stability but also achieves faster convergence. Moreover, for the first time, we show that the spectral/energy efficiency of RSMA achieved by suboptimal beamforming schemes (including weighted minimum mean squared error (WMMSE) and successive convex approximation) almost coincides with the corresponding globally optimal performance, making it a valid choice for performance comparisons. The globally optimal results provided in this work are imperative to the ongoing research on RSMA as they serve as benchmarks for existing suboptimal beamforming strategies and those to be developed in multi-antenna broadcast channels.      
### 65.Can a Ground-Based Vehicle Hear the Shape of a Room?  [ :arrow_down: ](https://arxiv.org/pdf/2204.00244.pdf)
>  Assume that a ground-based vehicle moves in a room with walls or other planar surfaces. Can the vehicle reconstruct the positions of the walls from the echoes of a single sound event? We assume that the vehicle carries some microphones and that a loudspeaker is either also mounted on the vehicle or placed at a fixed location in the room. We prove that the reconstruction is almost always possible if (1) no echoes are received from floors, ceilings or sloping walls and the vehicle carries at least three non-collinear microphones, or if (2) walls of any inclination may occur, the loudspeaker is fixed in the room and there are four non-coplanar microphones. <br>The difficulty lies in the echo-matching problem: how to determine which echoes come from the same wall. We solve this by using a Cayley-Menger determinant. Our proofs use methods from computational commutative algebra.      
### 66.Turbulence-free computational ghost imaging  [ :arrow_down: ](https://arxiv.org/pdf/2204.00229.pdf)
>  Turbulence-free images cannot be produced by conventional computational ghost imaging because calculated light is not affected by the same atmospheric turbulence as real light. In this article, we first addressed this issue by measuring the photon number fluctuation autocorrelation of the signals generated by a conventional computational ghost imaging device. Our results illustrate how conventional computational ghost imaging without structural changes can be used to produce turbulence-free images.      
### 67.Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.00212.pdf)
>  Large-scale language models (LLMs) such as GPT-2, BERT and RoBERTa have been successfully applied to ASR N-best rescoring. However, whether or how they can benefit competitive, near state-of-the-art ASR systems remains unexplored. In this study, we incorporate LLM rescoring into one of the most competitive ASR baselines: the Conformer-Transducer model. We demonstrate that consistent improvement is achieved by the LLM's bidirectionality, pretraining, in-domain finetuning and context augmentation. Furthermore, our lexical analysis sheds light on how each of these components may be contributing to the ASR performance.      
### 68.Predictive Control Barrier Functions for Online Safety Critical Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.00208.pdf)
>  This paper presents a methodology for constructing Control Barrier Functions (CBFs) that proactively consider the future safety of a system along a nominal trajectory, and effect corrective action before the trajectory leaves a designated safe set. Specifically, this paper presents a systematic approach for propagating a nominal trajectory on a receding horizon, and then encoding the future safety of this trajectory into a CBF. If the trajectory is unsafe, then a controller satisfying the CBF condition will modify the nominal trajectory before the trajectory becomes unsafe. Compared to existing CBF techniques, this strategy is proactive rather than reactive and thus potentially results in smaller modifications to the nominal trajectory. The proposed strategy is shown to be provably safe, and then is demonstrated in simulated scenarios where it would otherwise be difficult to construct a traditional CBF. In simulation, the predictive CBF results in less modification to the nominal trajectory and smaller control inputs than a traditional CBF, and faster computations than a nonlinear model predictive control approach.      
### 69.Wasserstein Two-Sided Chance Constraints with An Application to Optimal Power Flow  [ :arrow_down: ](https://arxiv.org/pdf/2204.00191.pdf)
>  As a natural approach to modeling system safety conditions, chance constraint (CC) seeks to satisfy a set of uncertain inequalities individually or jointly with high probability. Although a joint CC offers stronger reliability certificate, it is oftentimes much more challenging to compute than individual CCs. Motivated by the application of optimal power flow, we study a special joint CC, named two-sided CC. We model the uncertain parameters through a Wasserstein ball centered at a Gaussian distribution and derive a hierarchy of conservative approximations based on second-order conic constraints, which can be efficiently computed by off-the-shelf commercial solvers. In addition, we show the asymptotic consistency of these approximations and derive their approximation guarantee when only a finite hierarchy is adopted. We demonstrate the out-of-sample performance and scalability of the proposed model and approximations in a case study based on the IEEE 118-bus and 3120-bus systems.      
### 70.Comparative Analysis of Interval Reachability for Robust Implicit and Feedforward Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.00187.pdf)
>  We use interval reachability analysis to obtain robustness guarantees for implicit neural networks (INNs). INNs are a class of implicit learning models that use implicit equations as layers and have been shown to exhibit several notable benefits over traditional deep neural networks. We first establish that tight inclusion functions of neural networks, which provide the tightest rectangular over-approximation of an input-output map, lead to sharper robustness guarantees than the well-studied robustness measures of local Lipschitz constants. Like Lipschitz constants, tight inclusions functions are computationally challenging to obtain, and we thus propose using mixed monotonicity and contraction theory to obtain computationally efficient estimates of tight inclusion functions for INNs. We show that our approach performs at least as well as, and generally better than, applying state-of-the-art interval bound propagation methods to INNs. We design a novel optimization problem for training robust INNs and we provide empirical evidence that suitably-trained INNs can be more robust than comparably-trained feedforward networks.      
### 71.Better Intermediates Improve CTC Inference  [ :arrow_down: ](https://arxiv.org/pdf/2204.00176.pdf)
>  This paper proposes a method for improved CTC inference with searched intermediates and multi-pass conditioning. The paper first formulates self-conditioned CTC as a probabilistic model with an intermediate prediction as a latent representation and provides a tractable conditioning framework. We then propose two new conditioning methods based on the new formulation: (1) Searched intermediate conditioning that refines intermediate predictions with beam-search, (2) Multi-pass conditioning that uses predictions of previous inference for conditioning the next inference. These new approaches enable better conditioning than the original self-conditioned CTC during inference and improve the final performance. Experiments with the LibriSpeech dataset show relative 3%/12% performance improvement at the maximum in test clean/other sets compared to the original self-conditioned CTC.      
### 72.Multi-sequence Intermediate Conditioning for CTC-based ASR  [ :arrow_down: ](https://arxiv.org/pdf/2204.00175.pdf)
>  End-to-end automatic speech recognition (ASR) directly maps input speech to a character sequence without using pronunciation lexica. However, in languages with thousands of characters, such as Japanese and Mandarin, modeling all these characters is problematic due to data scarcity. To alleviate the problem, we propose a multi-task learning model with explicit interaction between characters and syllables by utilizing Self-conditioned connectionist temporal classification (CTC) technique. While the original Self-conditioned CTC estimates character-level intermediate predictions by applying auxiliary CTC losses to a set of intermediate layers, the proposed method additionally estimates syllable-level intermediate predictions in another set of intermediate layers. The character-level and syllable-level predictions are alternately used as conditioning features to deal with mutual dependency between characters and syllables. Experimental results on Japanese and Mandarin datasets show that the proposed multi-sequence intermediate conditioning outperformed the conventional multi-task-based and Self-conditioned CTC-based methods.      
### 73.InterAug: Augmenting Noisy Intermediate Predictions for CTC-based ASR  [ :arrow_down: ](https://arxiv.org/pdf/2204.00174.pdf)
>  This paper proposes InterAug: a novel training method for CTC-based ASR using augmented intermediate representations for conditioning. The proposed method exploits the conditioning framework of self-conditioned CTC to train robust models by conditioning with "noisy" intermediate predictions. During the training, intermediate predictions are changed to incorrect intermediate predictions, and fed into the next layer for conditioning. The subsequent layers are trained to correct the incorrect intermediate predictions with the intermediate losses. By repeating the augmentation and the correction, iterative refinements, which generally require a special decoder, can be realized only with the audio encoder. To produce noisy intermediate predictions, we also introduce new augmentation: intermediate feature space augmentation and intermediate token space augmentation that are designed to simulate typical errors. The combination of the proposed InterAug framework with new augmentation allows explicit training of the robust audio encoders. In experiments using augmentations simulating deletion, insertion, and substitution error, we confirmed that the trained model acquires robustness to each error, boosting the speech recognition performance of the strong self-conditioned CTC baseline.      
### 74.Filter-based Discriminative Autoencoders for Children Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.00164.pdf)
>  Children speech recognition is indispensable but challenging due to the diversity of children's speech. In this paper, we propose a filter-based discriminative autoencoder for acoustic modeling. To filter out the influence of various speaker types and pitches, auxiliary information of the speaker and pitch features is input into the encoder together with the acoustic features to generate phonetic embeddings. In the training phase, the decoder uses the auxiliary information and the phonetic embedding extracted by the encoder to reconstruct the input acoustic features. The autoencoder is trained by simultaneously minimizing the ASR loss and feature reconstruction error. The framework can make the phonetic embedding purer, resulting in more accurate senone (triphone-state) scores. Evaluated on the test set of the CMU Kids corpus, our system achieves a 7.8% relative WER reduction compared to the baseline system. In the domain adaptation experiment, our system also outperforms the baseline system on the British-accent PF-STAR task.      
### 75.Distributionally Robust Decision Making Leveraging Conditional Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2204.00138.pdf)
>  Distributionally robust optimization (DRO) is a powerful tool for decision making under uncertainty. It is particularly appealing because of its ability to leverage existing data. However, many practical problems call for decision-making with some auxiliary information, and DRO in the context of conditional distribution is not straightforward. We propose a conditional kernel distributionally robust optimization (CKDRO) method that enables robust decision making under conditional distributions through kernel DRO and the conditional mean operator in the reproducing kernel Hilbert space (RKHS). In particular, we consider problems where there is a correlation between the unknown variable y and an auxiliary observable variable x. Given past data of the two variables and a queried auxiliary variable, CKDRO represents the conditional distribution P(y|x) as the conditional mean operator in the RKHS space and quantifies the ambiguity set in the RKHS as well, which depends on the size of the dataset as well as the query point. To justify the use of RKHS, we demonstrate that the ambiguity set defined in RKHS can be viewed as a ball under a metric that is similar to the Wasserstein metric. The DRO is then dualized and solved via a finite dimensional convex program. The proposed CKDRO approach is applied to a generation scheduling problem and shows that the result of CKDRO is superior to common benchmarks in terms of quality and robustness.      
### 76.Intersection Crossing with Future-Focused Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2204.00127.pdf)
>  In this paper, we introduce a future-focused control barrier function (ff-CBF) approach to an unsignaled four-way intersection crossing problem for a collection of communicating automobiles. Our novel ff-CBF encodes that vehicles take control actions that avoid predicted collisions over an arbitrarily long future time horizon, thereby defining a virtual barrier. We then propose a relaxed-virtual CBF (rv-CBF) that inherits the predictive power of the ff-CBF while allowing relaxations of the virtual barrier away from the physical barrier between vehicles. We study the efficacy of the ff-CBF and rv-CBF based controllers via a series of simulated trials of the intersection scenario and highlight how the rv-CBF based controller empirically outperforms a benchmark controller from the literature by improving intersection throughput while preserving safety and feasibility properties.      
### 77.Perceptive, non-linear Speech Processing and Spiking Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.00094.pdf)
>  Source separation and speech recognition are very difficult in the context of noisy and corrupted speech. Most conventional techniques need huge databases to estimate speech (or noise) density probabilities to perform separation or recognition. We discuss the potential of perceptive speech analysis and processing in combination with biologically plausible neural network processors. We illustrate the potential of such non-linear processing of speech on a source separation system inspired by an Auditory Scene Analysis paradigm. We also discuss a potential application in speech recognition.      
### 78.Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression  [ :arrow_down: ](https://arxiv.org/pdf/2204.00088.pdf)
>  Embedded in any speech signal is a rich combination of cognitive, neuromuscular and physiological information. This richness makes speech a powerful signal in relation to a range of different health conditions, including major depressive disorders (MDD). One pivotal issue in speech-depression research is the assumption that depressive severity is the dominant measurable effect. However, given the heterogeneous clinical profile of MDD, it may actually be the case that speech alterations are more strongly associated with subsets of key depression symptoms. This paper presents strong evidence in support of this argument. First, we present a novel large, cross-sectional, multi-modal dataset collected at Thymia. We then present a set of machine learning experiments that demonstrate that combining speech with features from an n-Back working memory assessment improves classifier performance when predicting the popular eight-item Patient Health Questionnaire depression scale (PHQ-8). Finally, we present a set of experiments that highlight the association between different speech and n-Back markers at the PHQ-8 item level. Specifically, we observe that somatic and psychomotor symptoms are more strongly associated with n-Back performance scores, whilst the other items: anhedonia, depressed mood, change in appetite, feelings of worthlessness and trouble concentrating are more strongly associated with speech changes.      
### 79.Quantitative analysis of diaphragm motion during fluoroscopic sniff test to assist in diagnosis of hemidiaphragm paralysis  [ :arrow_down: ](https://arxiv.org/pdf/2204.00082.pdf)
>  The current imaging gold standard for detecting paradoxical diaphragm motion and diagnosing hemidiaphragm paralysis is to perform the fluoroscopic sniff test. The images are visually examined by an experienced radiologist, and if one hemidiaphragm ascends while the other descends, then it is described as paradoxical motion, which is highly suggestive of hemidiaphragm paralysis. However, diagnosis can be challenging because diaphragm motion during sniffing is fast, paradoxical motion can be subtle, and the analysis is based on a 2-dimensional projection of a 3-dimensional surface. This paper presents a case of chronic left hemidiaphragm elevation that was initially reported as mild paradoxical motion on fluoroscopy. After measuring the elevations of the diaphragms and modeling their temporal correlation using Gaussian process regression, the systematic trend of the hemidiaphragmatic motion along with its stochastic properties was determined. When analyzing the trajectories of the hemidiaphragms, no statistically significant paradoxical motion was detected. This could potentially change the prognosis if the patient was to consider diaphragm plication as treatment. The presented method provides a more objective analysis of hemidiaphragm motions and can potentially improve diagnostic accuracy.      
### 80.4Weed Dataset: Annotated Imagery Weeds Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2204.00080.pdf)
>  Weeds are a major threat to crops and are responsible for reducing crop yield worldwide. To mitigate their negative effect, it is advantageous to accurately identify them early in the season to prevent their spread throughout the field. Traditionally, farmers rely on manually scouting fields and applying herbicides for different weeds. However, it is easy to confuse between crops with weeds during the early growth stages. Recently, deep learning-based weed identification has become popular as deep learning relies on convolutional neural networks that are capable of learning important distinguishable features between weeds and crops. However, training robust deep learning models requires access to large imagery datasets. Therefore, an early-season weeds dataset was acquired under field conditions. The dataset consists of 159 Cocklebur images, 139 Foxtail images, 170 Redroot Pigweed images and 150 Giant Ragweed images corresponding to four common weed species found in corn and soybean production systems.. Bounding box annotations were created for each image to prepare the dataset for training both image classification and object detection deep learning networks capable of accurately locating and identifying weeds within corn and soybean fields. (<a class="link-external link-https" href="https://osf.io/w9v3j/" rel="external noopener nofollow">this https URL</a>)      
### 81.Data-augmented cross-lingual synthesis in a teacher-student framework  [ :arrow_down: ](https://arxiv.org/pdf/2204.00061.pdf)
>  Cross-lingual synthesis can be defined as the task of letting a speaker generate fluent synthetic speech in another language. This is a challenging task, and resulting speech can suffer from reduced naturalness, accented speech, and/or loss of essential voice characteristics. Previous research shows that many models appear to have insufficient generalization capabilities to perform well on every of these cross-lingual aspects. To overcome these generalization problems, we propose to apply the teacher-student paradigm to cross-lingual synthesis. While a teacher model is commonly used to produce teacher forced data, we propose to also use it to produce augmented data of unseen speaker-language pairs, where the aim is to retain essential speaker characteristics. Both sets of data are then used for student model training, which is trained to retain the naturalness and prosodic variation present in the teacher forced data, while learning the speaker identity from the augmented data. Some modifications to the student model are proposed to make the separation of teacher forced and augmented data more straightforward. Results show that the proposed approach improves the retention of speaker characteristics in the speech, while managing to retain high levels of naturalness and prosodic variation.      
### 82.Optimal Resource Scheduling and Allocation under Allowable Over-Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2204.00038.pdf)
>  This paper studies optimal scheduling and resource allocation under allowable over-scheduling. Formulating an optimisation problem where over-scheduling is embedded, we derive an optimal solution that can be implemented by means of a new additive increase multiplicative decrease (AIMD) algorithm. After describing the AIMD-like scheduling mechanism as a switching system, we show convergence of the scheme, based on the joint spectral radius of symmetric matrices, and propose two methods for fitting an optimal AIMD tuning to the optimal solution derived. Finally, we demonstrate the overall optimal design strategy via an illustrative example.      
### 83.A Statistical Decision-Theoretical Perspective on the Two-Stage Approach to Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2204.00036.pdf)
>  One of the most important problems in system identification and statistics is how to estimate the unknown parameters of a given model. Optimization methods and specialized procedures, such as Empirical Minimization (EM) can be used in case the likelihood function can be computed. For situations where one can only simulate from a parametric model, but the likelihood is difficult or impossible to evaluate, a technique known as the Two-Stage (TS) Approach can be applied to obtain reliable parametric estimates. Unfortunately, there is currently a lack of theoretical justification for TS. In this paper, we propose a statistical decision-theoretical derivation of TS, which leads to Bayesian and Minimax estimators. We also show how to apply the TS approach on models for independent and identically distributed samples, by computing quantiles of the data as a first step, and using a linear function as the second stage. The proposed method is illustrated via numerical simulations.      
### 84.Graph-based Active Learning for Semi-supervised Classification of SAR Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.00005.pdf)
>  We present a novel method for classification of Synthetic Aperture Radar (SAR) data by combining ideas from graph-based learning and neural network methods within an active learning framework. Graph-based methods in machine learning are based on a similarity graph constructed from the data. When the data consists of raw images composed of scenes, extraneous information can make the classification task more difficult. In recent years, neural network methods have been shown to provide a promising framework for extracting patterns from SAR images. These methods, however, require ample training data to avoid overfitting. At the same time, such training data are often unavailable for applications of interest, such as automatic target recognition (ATR) and SAR data. We use a Convolutional Neural Network Variational Autoencoder (CNNVAE) to embed SAR data into a feature space, and then construct a similarity graph from the embedded data and apply graph-based semi-supervised learning techniques. The CNNVAE feature embedding and graph construction requires no labeled data, which reduces overfitting and improves the generalization performance of graph learning at low label rates. Furthermore, the method easily incorporates a human-in-the-loop for active learning in the data-labeling process. We present promising results and compare them to other standard machine learning methods on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset for ATR with small amounts of labeled data.      
### 85.Ball 3D localization from a single calibrated image  [ :arrow_down: ](https://arxiv.org/pdf/2204.00003.pdf)
>  Ball 3D localization in team sports has various applications including automatic offside detection in soccer, or shot release localization in basketball. Today, this task is either resolved by using expensive multi-views setups, or by restricting the analysis to ballistic trajectories. In this work, we propose to address the task on a single image from a calibrated monocular camera by estimating ball diameter in pixels and use the knowledge of real ball diameter in meters. This approach is suitable for any game situation where the ball is (even partly) visible. To achieve this, we use a small neural network trained on image patches around candidates generated by a conventional ball detector. Besides predicting ball diameter, our network outputs the confidence of having a ball in the image patch. Validations on 3 basketball datasets reveals that our model gives remarkable predictions on ball 3D localization. In addition, through its confidence output, our model improves the detection rate by filtering the candidates produced by the detector. The contributions of this work are (i) the first model to address 3D ball localization on a single image, (ii) an effective method for ball 3D annotation from single calibrated images, (iii) a high quality 3D ball evaluation dataset annotated from a single viewpoint. In addition, the code to reproduce this research is be made freely available at <a class="link-external link-https" href="https://github.com/gabriel-vanzandycke/deepsport" rel="external noopener nofollow">this https URL</a>.      
