# ArXiv eess --Tue, 12 Apr 2022
### 1.Neglectable effect of brain MRI data prepreprocessing for tumor segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.05278.pdf)
>  Magnetic resonance imaging (MRI) data is heterogeneous due to the differences in device manufacturers, scanning protocols, and inter-subject variability. A conventional way to mitigate MR image heterogeneity is to apply preprocessing transformations, such as anatomy alignment, voxel resampling, signal intensity equalization, image denoising, and localization of regions of interest (ROI). Although preprocessing pipeline standardizes image appearance, its influence on the quality of image segmentation and other downstream tasks on deep neural networks (DNN) has never been rigorously studied. <br>Here we report a comprehensive study of multimodal MRI brain cancer image segmentation on TCIA-GBM open-source dataset. Our results demonstrate that most popular standardization steps add no value to artificial neural network performance; moreover, preprocessing can hamper model performance. We suggest that image intensity normalization approaches do not contribute to model accuracy because of the reduction of signal variance with image standardization. Finally, we show the contribution of scull-stripping in data preprocessing is almost negligible if measured in terms of clinically relevant metrics. <br>We show that the only essential transformation for accurate analysis is the unification of voxel spacing across the dataset. In contrast, anatomy alignment in form of non-rigid atlas registration is not necessary and most intensity equalization steps do not improve model productiveness.      
### 2.Segmentation-Consistent Probabilistic Lesion Counting  [ :arrow_down: ](https://arxiv.org/pdf/2204.05276.pdf)
>  Lesion counts are important indicators of disease severity, patient prognosis, and treatment efficacy, yet counting as a task in medical imaging is often overlooked in favor of segmentation. This work introduces a novel continuously differentiable function that maps lesion segmentation predictions to lesion count probability distributions in a consistent manner. The proposed end-to-end approach--which consists of voxel clustering, lesion-level voxel probability aggregation, and Poisson-binomial counting--is non-parametric and thus offers a robust and consistent way to augment lesion segmentation models with post hoc counting capabilities. Experiments on Gadolinium-enhancing lesion counting demonstrate that our method outputs accurate and well-calibrated count distributions that capture meaningful uncertainty information. They also reveal that our model is suitable for multi-task learning of lesion segmentation, is efficient in low data regimes, and is robust to adversarial attacks.      
### 3.Invariant Smoothing with low process noise  [ :arrow_down: ](https://arxiv.org/pdf/2204.05256.pdf)
>  In this paper we address smoothing-that is, optimisation-based-estimation techniques for localisation problems in the case where motion sensors are very accurate. Our mathematical analysis focuses on the difficult limit case where motion sensors are infinitely precise, resulting in the absence of process noise. Then the formulation degenerates, as the dynamical model that serves as a soft constraint becomes an equality constraint, and conventional smoothing methods are not able to fully respect it. By contrast, once an appropriate Lie group embedding has been found, we prove theoretically that invariant smoothing gracefully accommodates this limit case in that the estimates tend to be consistent with the induced constraints when the noise tends to zero. Simulations on the important problem of initial alignement in inertial navigation show that, in a low noise setting, invariant smoothing may favorably compare to state-of-the-art smoothers when using precise inertial measurements units (IMU).      
### 4.The self-learning AI controller for adaptive power beaming with fiber-array laser transmitter system  [ :arrow_down: ](https://arxiv.org/pdf/2204.05227.pdf)
>  In this study we consider adaptive power beaming with fiber-array laser transmitter system in presence of atmospheric turbulence. For optimization of power transition through the atmosphere fiber-array is traditionally controlled by stochastic parallel gradient descent (SPGD) algorithm where control feedback is provided via radio frequency link by an optical-to-electrical power conversion sensor, attached to a cooperative target. The SPGD algorithm continuously and randomly perturbs voltages applied to fiber-array phase shifters and fiber tip positioners in order to maximize sensor signal, i.e. uses, so-called, "blind" optimization principle. <br>In opposite to this approach a perspective artificially intelligent (AI) control systems for synthesis of optimal control can utilize various pupil- or target-plane data available for the analysis including wavefront sensor data, photo-voltaic array (PVA) data, other optical or atmospheric parameters, and potentially can eliminate well-known drawbacks of SPGD-based controllers. In this study an optimal control is synthesized by a deep neural network (DNN) using target-plane PVA sensor data as its input. A DNN training is occurred online in sync with control system operation and is performed by applying of small perturbations to DNN's outputs. This approach does not require initial DNN's pre-training as well as guarantees optimization of system performance in time. All theoretical results are verified by numerical experiments.      
### 5.Rethinking Machine Learning Model Evaluation in Pathology  [ :arrow_down: ](https://arxiv.org/pdf/2204.05205.pdf)
>  Machine Learning has been applied to pathology images in research and clinical practice with promising outcomes. However, standard ML models often lack the rigorous evaluation required for clinical decisions. Machine learning techniques for natural images are ill-equipped to deal with pathology images that are significantly large and noisy, require expensive labeling, are hard to interpret, and are susceptible to spurious correlations. We propose a set of practical guidelines for ML evaluation in pathology that address the above concerns. The paper includes measures for setting up the evaluation framework, effectively dealing with variability in labels, and a recommended suite of tests to address issues related to domain shift, robustness, and confounding variables. We hope that the proposed framework will bridge the gap between ML researchers and domain experts, leading to wider adoption of ML techniques in pathology and improving patient outcomes.      
### 6.CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.05203.pdf)
>  Federated learning enables building a shared model from multicentre data while storing the training data locally for privacy. In this paper, we present an evaluation (called CXR-FL) of deep learning-based models for chest X-ray image analysis using the federated learning method. We examine the impact of federated learning parameters on the performance of central models. Additionally, we show that classification models perform worse if trained on a region of interest reduced to segmentation of the lung compared to the full image. However, focusing training of the classification model on the lung area may result in improved pathology interpretability during inference. We also find that federated learning helps maintain model generalizability. The pre-trained weights and code are publicly available at (<a class="link-external link-https" href="https://github.com/SanoScience/CXR-FL" rel="external noopener nofollow">this https URL</a>).      
### 7.A Post-Processing Tool and Feasibility Study for Three-Dimensional Imaging with Electrical Impedance Tomography During Deep Brain Stimulation Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2204.05201.pdf)
>  Electrical impedance tomography (EIT) is a promising technique for biomedical imaging. The strength of EIT is its ability to reconstruct images of the body's internal structures through radiation-safe techniques. EIT is regarded as safe for patients' health, and it is currently being actively researched. This paper investigates the application of EIT during deep brain stimulation (DBS) surgery as a means to identify targets during operations. DBS involves a surgical procedure in which a lead or electrode array is implanted in a specific target area in the brain. Electrical stimulations are then used to modulate neural circuits within the target area to reduce disabling neurological symptoms. The main difficulty in performing DBS surgery is to accurately position the lead in the target area before commencing the treatment. Brain tissue shifts during DBS surgery can be as large as the target size when compared with the pre-operative magnetic resonance imaging (MRI) or computed tomography (CT) images. To address this problem, a solution based on open-domain EIT to reconstruct images surrounding the probe during DBS surgery is proposed. Data acquisition and image reconstruction were performed, and artificial intelligence was applied to enhance the resulting images. The results showed that the proposed method is rapid, produces valuable high-quality images, and constitutes a first step towards in-vivo study.      
### 8.MmWave 6D Radio Localization with a Snapshot Observation from a Single BS  [ :arrow_down: ](https://arxiv.org/pdf/2204.05189.pdf)
>  Accurate and ubiquitous localization is crucial for a variety of applications such as logistics, navigation, intelligent transport, monitoring, and control. Exploiting mmWave signals in 5G and Beyond 5G systems can provide accurate localization with limited infrastructure. We consider the single base station localization problem and extend it to 3D position and 3D orientation estimation of an unsynchronized multi-antenna user, using downlink MIMO-OFDM signals. Through a Fisher information analysis, we show that the problem is often identifiable, provided that there is at least one additional multipath component, even if the position of corresponding incidence point is a priori unknown. Subsequently, we pose a maximum likelihood (ML) estimation problem, to jointly estimate the 3D position and 3D orientation of the user as well as several nuisance parameters (the user clock offset and the positions of incidence points corresponding to the multipath). The ML problem is a high-dimensional non-convex optimization problem over a product of Euclidean and Riemannian manifolds. To avoid complex exhaustive search procedures, we propose a geometric initial estimate of all parameters, which reduces the problem to a 1-dimensional search over a finite interval. Numerical results show the efficiency of the proposed ad-hoc estimation, whose gap to the CRB is tightened using ML estimation.      
### 9.The PartialSpoof Database and Countermeasures for the Detection of Short Generated Audio Segments Embedded in a Speech Utterance  [ :arrow_down: ](https://arxiv.org/pdf/2204.05177.pdf)
>  Automatic speaker verification is susceptible to various manipulations and spoofing, such as text-to-speech (TTS) synthesis, voice conversion (VC), replay, tampering, and so on. In this paper, we consider a new spoofing scenario called "Partial Spoof" (PS) in which synthesized or transformed audio segments are embedded into a bona fide speech utterance. While existing countermeasures (CMs) can detect fully spoofed utterances, there is a need for their adaptation or extension to the PS scenario to detect utterances in which only a part of the audio signal is generated and hence only a fraction of an utterance is spoofed. For improved explainability, such new CMs should ideally also be able to detect such short spoofed segments. Our previous study introduced the first version of a speech database suitable for training CMs for the PS scenario and showed that, although it is possible to train CMs to execute the two types of detection described above, there is much room for improvement. In this paper we propose various improvements to construct a significantly more accurate CM that can detect short generated spoofed audio segments at finer temporal resolutions. First, we introduce newly proposed self-supervised pre-trained models as enhanced feature extractors. Second, we extend the PartialSpoof database by adding segment labels for various temporal resolutions, ranging from 20 ms to 640 ms. Third, we propose a new CM and training strategies that enable the simultaneous use of the utterance-level and segment-level labels at different temporal resolutions. We also show that the proposed CM is capable of detecting spoofing at the utterance level with low error rates, not only in the PS scenario but also in a related logical access (LA) scenario. The equal error rates of utterance-level detection on the PartialSpoof and the ASVspoof 2019 LA database were 0.47% and 0.59%, respectively.      
### 10.NeoRS: a neonatal resting state fMRI data preprocessing pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2204.05137.pdf)
>  Resting state fMRI (rsfMRI) has been shown to be a promising tool to study intrinsic functional connectivity and assess its integrity in cerebral development. In neonates, where fMRI is limited to few paradigms, rsfMRI was shown to be a relevant tool to explore regional interactions of brain networks. However, to identify the resting state networks, data needs to be carefully processed. Because of the non-collaborative nature of the neonates, the differences in brain size and the reversed contrast compared to adults, neonates can't be processed with the existing adult pipelines. Therefore, we developed NeoRS. The main processing steps include atlas registration, skull tripping, segmentation, slice timing and head motion correction and confounds regression. To address the specificity of neonatal brain imaging, particular attention was given to registration including neonatal atlas type and parameters, such as brain size variations, and contrast differences compared to adults. Furthermore, head motion was scrutinized and optimized, as it is a major issue when processing neonatal data. The pipeline includes visual quality control assessment checkpoints. To assess its effectiveness, we used the data from the Baby Connectome Project including 10 neonates. NeoRS was designed to work on both multi-band and single-band acquisitions and is applicable on smaller datasets. It also includes popular functional connectivity analysis features such as seed based correlations. Language, default mode, dorsal attention, visual, ventral attention, motor and fronto parietal networks were evaluated. The different analyzed networks were in agreement with previously published studies in the neonate. NeoRS is coded in Matlab, it is open-source and available on <a class="link-external link-https" href="https://github.com/venguix/NeoRS" rel="external noopener nofollow">this https URL</a>. NeoRS allows robust image processing of the neonatal rsfMRI data that can be readily customized to different datasets.      
### 11.Exergetic Port-Hamiltonian Systems: Navier-Stokes-Fourier Fluid  [ :arrow_down: ](https://arxiv.org/pdf/2204.05135.pdf)
>  The Exergetic Port-Hamiltonian Systems modeling language combines a graphical syntax inspired by bond graphs with a port-Hamiltonian semantics akin to the GENERIC formalism. The syntax enables the modular and hierarchical specification of the composition pattern of lumped and distributed-parameter models. The semantics reflects the first and second law of thermodynamics as structural properties. Interconnected and hierarchically defined models of multiphysical thermodynamic systems can thus be expressed in a formal language accessible to humans and computers alike. We discuss a composed model of the Navier-Stokes-Fourier fluid on a fixed spatial domain as an example of an open distributed-parameter system. At the top level, the system comprises five subsystems which model kinetic energy storage, internal energy storage, thermal conduction, bulk viscosity, and shear viscosity.      
### 12.IMLE-Net: An Interpretable Multi-level Multi-channel Model for ECG Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.05116.pdf)
>  Early detection of cardiovascular diseases is crucial for effective treatment and an electrocardiogram (ECG) is pivotal for diagnosis. The accuracy of Deep Learning based methods for ECG signal classification has progressed in recent years to reach cardiologist-level performance. In clinical settings, a cardiologist makes a diagnosis based on the standard 12-channel ECG recording. Automatic analysis of ECG recordings from a multiple-channel perspective has not been given enough attention, so it is essential to analyze an ECG recording from a multiple-channel perspective. We propose a model that leverages the multiple-channel information available in the standard 12-channel ECG recordings and learns patterns at the beat, rhythm, and channel level. The experimental results show that our model achieved a macro-averaged ROC-AUC score of 0.9216, mean accuracy of 88.85\%, and a maximum F1 score of 0.8057 on the PTB-XL dataset. The attention visualization results from the interpretable model are compared against the cardiologist's guidelines to validate the correctness and usability.      
### 13.Blind Orthogonal Least Squares based Compressive Spectrum Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2204.05086.pdf)
>  Compressive spectrum sensing (CSS) has been widely studied in wideband cognitive radios, benefiting from the reduction of sampling rate via compressive sensing (CS) technology. However, the sensing performance of most existing CSS excessively relies on the prior information such as spectrum sparsity or noise variance. Thus, a key challenge in practical CSS is how to work effectively even in the absence of such information. In this paper, we propose a blind orthogonal least squares based CSS algorithm (B-OLS-CSS), which functions properly without the requirement of prior information. Specifically, we develop a novel blind stopping rule for the OLS algorithm based on its probabilistic recovery condition. This innovative rule gets rid of the need of the spectrum sparsity or noise information, but only requires the computational-feasible mutual incoherence property of the given measurement matrix. Our theoretical analysis indicates that the signal-to-noise ratio required by the proposed B-OLS-CSS for achieving a certain sensing accuracy is relaxed than that by the benchmark CSS using the OMP algorithm, which is verified by extensive simulation results.      
### 14.A Novel Channel Identification Architecture for mmWave Systems Based on Eigen Features  [ :arrow_down: ](https://arxiv.org/pdf/2204.05052.pdf)
>  Millimeter wave (mmWave) communication technique has been developed rapidly because of many advantages of high speed, large bandwidth, and ultra-low delay. However, mmWave communications systems suffer from fast fading and frequent blocking. Hence, the ideal communication environment for mmWave is line of sight (LOS) channel. To improve the efficiency and capacity of mmWave system, and to better build the Internet of Everything (IoE) service network, this paper focuses on the channel identification technique in line-of- sight (LOS) and non-LOS (NLOS) environments. Considering the limited computing ability of user equipments (UEs), this paper proposes a novel channel identification architecture based on eigen features, i.e. eigenmatrix and eigenvector (EMEV) of channel state information (CSI). Furthermore, this paper explores clustered delay line (CDL) channel identification with mmWave, which is defined by the 3rd generation partnership project (3GPP). Ther experimental results show that the EMEV based scheme can achieve identification accuracy of 99.88% assuming perfect CSI. In the robustness test, the maximum noise can be tolerated is SNR= 16 dB, with the threshold acc \geq 95%. What is more, the novel architecture based on EMEV feature will reduce the comprehensive overhead by about 90%.      
### 15.From CNNs to Vision Transformers -- A Comprehensive Evaluation of Deep Learning Models for Histopathology  [ :arrow_down: ](https://arxiv.org/pdf/2204.05044.pdf)
>  While machine learning is currently transforming the field of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classification accuracy. In order to fill this gap, we conducted an extensive evaluation by benchmarking a wide range of classification models, including recent vision transformers, convolutional neural networks and hybrid models comprising transformer and convolutional models. We thoroughly tested the models on five widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classification model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of the models' classification strategies that allow for plausibility checks and systematic comparisons. The study resulted in specific model recommendations for practitioners as well as putting forward a general methodology to quantify a model's quality according to complementary requirements that can be transferred to future model architectures.      
### 16.Learning-based Lossless Point Cloud Geometry Coding using Sparse Representations  [ :arrow_down: ](https://arxiv.org/pdf/2204.05043.pdf)
>  Most point cloud compression methods operate in the voxel or octree domain which is not the original representation of point clouds. Those representations either remove the geometric information or require high computational power for processing. In this paper, we propose a context-based lossless point cloud geometry compression that directly processes the point representation. Operating on a point representation allows us to preserve geometry correlation between points and thus to obtain an accurate context model while significantly reduce the computational cost. Specifically, our method uses a sparse convolution neural network to estimate the voxel occupancy sequentially from the x,y,z input data. Experimental results show that our method outperforms the state-of-the-art geometry compression standard from MPEG with average rate savings of 52% on a diverse set of point clouds from four different datasets.      
### 17.Ischemic Stroke Lesion Segmentation Using Adversarial Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.04993.pdf)
>  Ischemic stroke occurs through a blockage of clogged blood vessels supplying blood to the brain. Segmentation of the stroke lesion is vital to improve diagnosis, outcome assessment and treatment planning. In this work, we propose a segmentation model with adversarial learning for ischemic lesion segmentation. We adopt U-Net with skip connection and dropout as segmentation baseline network and a fully connected network (FCN) as discriminator network. Discriminator network consists of 5 convolution layers followed by leaky-ReLU and an upsampling layer to rescale the output to the size of the input map. Training a segmentation network along with an adversarial network can detect and correct higher order inconsistencies between the segmentation maps produced by ground-truth and the Segmentor. We exploit three modalities (CT, DPWI, CBF) of acute computed tomography (CT) perfusion data provided in ISLES 2018 (Ischemic Stroke Lesion Segmentation) for ischemic lesion segmentation. Our model has achieved dice accuracy of 42.10% with the cross-validation of training and 39% with the testing data.      
### 18.Double Nonstationarity: Blind Extraction of Independent Nonstationary Vector/Component from Nonstationary Mixtures -- Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2204.04992.pdf)
>  In this article, nonstationary mixing and source models are combined for developing new fast and accurate algorithms for Independent Component or Vector Extraction (ICE/IVE), one of which stands for a new extension of the well-known FastICA. This model allows for a moving source-of-interest (SOI) whose distribution on short intervals can be (non-)circular (non-)Gaussian. A particular Gaussian source model assuming tridiagonal covariance matrix structures is proposed. It is shown to be beneficial in the frequency-domain speaker extraction problem. The algorithms are verified in simulations. In comparison to the state-of-the-art algorithms, they show superior performance in terms of convergence speed and extraction accuracy.      
### 19.A Dual Sensor Computational Camera for High Quality Dark Videography  [ :arrow_down: ](https://arxiv.org/pdf/2204.04987.pdf)
>  Videos captured under low light conditions suffer from severe noise. A variety of efforts have been devoted to image/video noise suppression and made large progress. However, in extremely dark scenarios, extensive photon starvation would hamper precise noise modeling. Instead, developing an imaging system collecting more photons is a more effective way for high-quality video capture under low illuminations. In this paper, we propose to build a dual-sensor camera to additionally collect the photons in NIR wavelength, and make use of the correlation between RGB and near-infrared (NIR) spectrum to perform high-quality reconstruction from noisy dark video pairs. In hardware, we build a compact dual-sensor camera capturing RGB and NIR videos simultaneously. Computationally, we propose a dual-channel multi-frame attention network (DCMAN) utilizing spatial-temporal-spectral priors to reconstruct the low-light RGB and NIR videos. In addition, we build a high-quality paired RGB and NIR video dataset, based on which the approach can be applied to different sensors easily by training the DCMAN model with simulated noisy input following a physical-process-based CMOS noise model. Both experiments on synthetic and real videos validate the performance of this compact dual-sensor camera design and the corresponding reconstruction algorithm in dark videography.      
### 20.External control of a genetic toggle switch via Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.04972.pdf)
>  We investigate the problem of using a learning-based strategy to stabilize a synthetic toggle switch via an external control approach. To overcome the data efficiency problem that would render the algorithm unfeasible for practical use in synthetic biology, we adopt a sim-to-real paradigm where the policy is learnt via training on a simplified model of the toggle switch and it is then subsequently exploited to control a more realistic model of the switch parameterized from in-vivo experiments. Our in-silico experiments confirm the viability of the approach suggesting its potential use for in-vivo control implementations.      
### 21.A General Compressive Sensing Construct using Density Evolution  [ :arrow_down: ](https://arxiv.org/pdf/2204.04963.pdf)
>  This paper proposes a general framework to design a sparse sensing matrix $\ensuremath{\mathbf{A}}\in \mathbb{R}^{m\times n}$, in a linear measurement system $\ensuremath{\mathbf{y}} = \ensuremath{\mathbf{Ax}}^{\natural} + \ensuremath{\mathbf{w}}$, where $\ensuremath{\mathbf{y}} \in \mathbb{R}^m$, $\ensuremath{\mathbf{x}}^{\natural}\in \RR^n$, and $\ensuremath{\mathbf{w}}$ denote the measurements, the signal with certain structures, and the measurement noise, respectively. By viewing the signal reconstruction from the measurements as a message passing algorithm over a graphical model, we leverage tools from coding theory in the design of low density parity check codes, namely the density evolution, and provide a framework for the design of matrix $\ensuremath{\mathbf{A}}$. Particularly, compared to the previous methods, our proposed framework enjoys the following desirable properties: ($i$) Universality: the design supports both regular sensing and preferential sensing, and incorporates them in a single framework; ($ii$) Flexibility: the framework can easily adapt the design of $\bA$ to a signal $\ensuremath{\mathbf{x}}^{\natural}$ with different underlying structures. As an illustration, we consider the $\ell_1$ regularizer, which correspond to Lasso, for both the regular sensing and preferential sensing scheme. Noteworthy, our framework can reproduce the classical result of Lasso, i.e., $m\geq c_0 k\log(n/k)$ (the regular sensing) with regular design after proper distribution approximation, where $c_0 &gt; 0$ is some fixed constant. We also provide numerical experiments to confirm the analytical results and demonstrate the superiority of our framework whenever a preferential treatment of a sub-block of vector $\bx^{\natural}$ is required.      
### 22.Segmentation Network with Compound Loss Function for Hydatidiform Mole Hydrops Lesion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.04956.pdf)
>  Pathological morphology diagnosis is the standard diagnosis method of hydatidiform mole. As a disease with malignant potential, the hydatidiform mole section of hydrops lesions is an important basis for diagnosis. Due to incomplete lesion development, early hydatidiform mole is difficult to distinguish, resulting in a low accuracy of clinical diagnosis. As a remarkable machine learning technology, image semantic segmentation networks have been used in many medical image recognition tasks. We developed a hydatidiform mole hydrops lesion segmentation model based on a novel loss function and training method. The model consists of different networks that segment the section image at the pixel and lesion levels. Our compound loss function assign weights to the segmentation results of the two levels to calculate the loss. We then propose a stagewise training method to combine the advantages of various loss functions at different levels. We evaluate our method on a hydatidiform mole hydrops dataset. Experiments show that the proposed model with our loss function and training method has good recognition performance under different segmentation metrics.      
### 23.A Semantic Segmentation Network Based Real-Time Computer-Aided Diagnosis System for Hydatidiform Mole Hydrops Lesion Recognition in Microscopic View  [ :arrow_down: ](https://arxiv.org/pdf/2204.04949.pdf)
>  As a disease with malignant potential, hydatidiform mole (HM) is one of the most common gestational trophoblastic diseases. For pathologists, the HM section of hydrops lesions is an important basis for diagnosis. In pathology departments, the diverse microscopic manifestations of HM lesions and the limited view under the microscope mean that physicians with extensive diagnostic experience are required to prevent missed diagnosis and misdiagnosis. Feature extraction can significantly improve the accuracy and speed of the diagnostic process. As a remarkable diagnosis assisting technology, computer-aided diagnosis (CAD) has been widely used in clinical practice. We constructed a deep-learning-based CAD system to identify HM hydrops lesions in the microscopic view in real-time. The system consists of three modules; the image mosaic module and edge extension module process the image to improve the outcome of the hydrops lesion recognition module, which adopts a semantic segmentation network, our novel compound loss function, and a stepwise training function in order to achieve the best performance in identifying hydrops lesions. We evaluated our system using an HM hydrops dataset. Experiments show that our system is able to respond in real-time and correctly display the entire microscopic view with accurately labeled HM hydrops lesions.      
### 24.Listen only to me! How well can target speech extraction handle false alarms?  [ :arrow_down: ](https://arxiv.org/pdf/2204.04811.pdf)
>  Target speech extraction (TSE) extracts the speech of a target speaker in a mixture given auxiliary clues characterizing the speaker, such as an enrollment utterance. TSE addresses thus the challenging problem of simultaneously performing separation and speaker identification. There has been much progress in extraction performance following the recent development of neural networks for speech enhancement and separation. Most studies have focused on processing mixtures where the target speaker is actively speaking. However, the target speaker is sometimes silent in practice, i.e., inactive speaker (IS). A typical TSE system will tend to output a signal in IS cases, causing false alarms. This is a severe problem for the practical deployment of TSE systems. This paper aims at understanding better how well TSE systems can handle IS cases. We consider two approaches to deal with IS, (1) training a system to directly output zero signals or (2) detecting IS with an extra speaker verification module. We perform an extensive experimental comparison of these schemes in terms of extraction performance and IS detection using the LibriMix dataset and reveal their pros and cons.      
### 25.Design and Experimental Verification of a Novel Error-Backpropagation-Based Background Calibration for Time Interleaved ADC in Digital Communication Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2204.04806.pdf)
>  A novel background calibration technique for Time-Interleaved Analog-to-Digital Converters (TI-ADCs) is presented in this paper. This technique is applicable to equalized digital communication receivers. As shown by Tsai et al. [1] and Luna et al. [2], in a digital receiver it is possible to treat the TI-ADC errors as part of the communication channel and take advantage of the adaptive equalizer to compensate them. Therefore calibration becomes an integral part of the channel equalization. No special purpose analog or digital calibration blocks or algorithms are required. However, there is a large class of receivers where the equalization technique cannot be directly applied because other signal processing blocks are located between the TI-ADC and the equalizer. The technique presented here generalizes earlier works to this class of receivers. The error backpropagation algorithm, traditionally used in machine learning, is applied to the error computed at the receiver slicer and used to adapt an auxiliary equalizer adjacent to the TI-ADC, called the Compensation Equalizer (CE). Simulations using a dual polarization optical coherent receiver model demonstrate accurate and robust mismatch compensation across different application scenarios. Several Quadrature Amplitude Modulation (QAM) schemes are tested in simulations and experimentally. Measurements on an emulation platform which includes an 8 bit, 4 GS/s TI-ADC prototype chip fabricated in 130nm CMOS technology, show an almost ideal mitigation of the impact of the mismatches on the receiver performance when 64-QAM and 256-QAM schemes are tested. An absolute improvement in the TI-ADC performance of $\sim$15 dB in both SNDR and SFDR is measured.      
### 26.Image Reconstruction for MRI using Deep CNN Priors Trained without Groundtruth  [ :arrow_down: ](https://arxiv.org/pdf/2204.04771.pdf)
>  We propose a new plug-and-play priors (PnP) based MR image reconstruction method that systematically enforces data consistency while also exploiting deep-learning priors. Our prior is specified through a convolutional neural network (CNN) trained without any artifact-free ground truth to remove undersampling artifacts from MR images. The results on reconstructing free-breathing MRI data into ten respiratory phases show that the method can form high-quality 4D images from severely undersampled measurements corresponding to acquisitions of about 1 and 2 minutes in length. The results also highlight the competitive performance of the method compared to several popular alternatives, including the TGV regularization and traditional UNet3D.      
### 27.Denoiser-based projections for 2-D super-resolution multi-reference alignment  [ :arrow_down: ](https://arxiv.org/pdf/2204.04754.pdf)
>  We study the 2-D super-resolution multi-reference alignment (SR-MRA) problem: estimating an image from its down-sampled, circularly-translated, and noisy copies. The SR-MRA problem serves as a mathematical abstraction of the structure determination problem for biological molecules. Since the SR-MRA problem is ill-posed without prior knowledge, accurate image estimation relies on designing priors that well-describe the statistics of the images of interest. In this work, we build on recent advances in image processing, and harness the power of denoisers as priors of images. In particular, we suggest to use denoisers as projections, and design two computational frameworks to estimate the image: projected expectation-maximization and projected method of moments. We provide an efficient GPU implementation, and demonstrate the effectiveness of these algorithms by extensive numerical experiments on a wide range of parameters and images.      
### 28.Regret Analysis of Online Gradient Descent-based Iterative Learning Control with Model Mismatch  [ :arrow_down: ](https://arxiv.org/pdf/2204.04722.pdf)
>  In Iterative Learning Control (ILC), a sequence of feedforward control actions is generated at each iteration on the basis of partial model knowledge and past measurements with the goal of steering the system toward a desired reference trajectory. This is framed here as an online learning task, where the decision-maker takes sequential decisions by solving a sequence of optimization problems having only partial knowledge of the cost functions. Having established this connection, the performance of an online gradient-descent based scheme using inexact gradient information is analyzed in the setting of dynamic and static regret, standard measures in online learning. Fundamental limitations of the scheme and its integration with adaptation mechanisms are further investigated, followed by numerical simulations on a benchmark ILC problem.      
### 29.Dual-Function Radar-Communication System Aided by Intelligent Reflecting Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2204.04721.pdf)
>  We propose a novel design of a dual-function radar communication (DFRC) system aided by an Intelligent Reflecting Surface (IRS). We consider a scenario with one target and multiple communication receivers, where there is no line-of-sight between the radar and the target. The radar precoding matrix and the IRS weights are optimally designed to maximize the weighted sum of the signal-to-noise ratio (SNR) at the radar receiver and the SNR at the communication receivers subject to power constraints and constant modulus constraints on the IRS weights. The problem is decoupled into two sub-problems, namely, waveform design and IRS weight design, and is solved via alternating optimization. The former subproblem is solved via linear programming, and the latter via manifold optimization with a quartic polynomial objective. The key contribution of this paper lies in solving the IRS weight design sub-problem that is based on the optimization of a quartic objective function in the IRS weights, and is subject to unit modulus-constraint on the IRS weights. Simulation results are provided to show the convergence behavior of the proposed algorithm under different system configurations, and the effectiveness of using IRS to improve radar and communication performance.      
### 30.Spectral Unmixing of Hyperspectral Images Based on Block Sparse Structure  [ :arrow_down: ](https://arxiv.org/pdf/2204.04638.pdf)
>  Spectral unmixing (SU) of hyperspectral images (HSIs) is one of the important areas in remote sensing (RS) that needs to be carefully addressed in different RS applications. Despite the high spectral resolution of the hyperspectral data, the relatively low spatial resolution of the sensors may lead to mixture of different pure materials within the image pixels. In this case, the spectrum of a given pixel recorded by the sensor can be a combination of multiple spectra each belonging to a unique material in that pixel. Spectral unmixing is then used as a technique to extract the spectral characteristics of the different materials within the mixed pixels and to recover the spectrum of each pure spectral signature, called endmember. Block-sparsity exists in hyperspectral images as a result of spectral similarity between neighboring pixels. In block-sparse signals, the nonzero samples occur in clusters and the pattern of the clusters is often supposed to be unavailable as prior information. This paper presents an innovative spectral unmixing approach for HSIs based on block-sparse structure and sparse Bayesian learning (SBL) strategy. To evaluate the performance of the proposed SU algorithm, it is tested on both synthetic and real hyperspectral data and the quantitative results are compared to those of other state-of-the-art methods in terms of abundance angel distance (AAD) and mean square error (MSE). The achieved results show the superiority of the proposed algorithm over the other competing methods by a significant margin.      
### 31.Prognostic classification based on random convolutional kernel  [ :arrow_down: ](https://arxiv.org/pdf/2204.04527.pdf)
>  Assessing the health status (HS) of system/component has long been a challenging task in the prognostic and health management (PHM) study. Differed from other regression based prognostic task such as predicting the remaining useful life, the HS assessment is essentially a multi class classificatIon problem. To address this issue, we introduced the random convolutional kernel-based approach, the RandOm Convolutional KErnel Transforms (ROCKET) and its latest variant MiniROCKET, in the paper. We implement ROCKET and MiniROCKET on the NASA's CMPASS dataset and assess the turbine fan engine's HS with the multi-sensor time-series data. Both methods show great accuracy when tackling the HS assessment task. More importantly, they demonstrate considerably efficiency especially compare with the deep learning-based method. We further reveal that the feature generated by random convolutional kernel can be combined with other classifiers such as support vector machine (SVM) and linear discriminant analysis (LDA). The newly constructed method maintains the high efficiency and outperforms all the other deop neutal network models in classification accuracy.      
### 32.Ultrasound Signal Processing: From Models to Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.04466.pdf)
>  Medical ultrasound imaging relies heavily on high-quality signal processing algorithms to provide reliable and interpretable image reconstructions. Hand-crafted reconstruction methods, often based on approximations of the underlying measurement model, are useful in practice, but notoriously fall behind in terms of image quality. More sophisticated solutions, based on statistical modelling, careful parameter tuning, or through increased model complexity, can be sensitive to different environments. Recently, deep learning based methods have gained popularity, which are optimized in a data-driven fashion. These model-agnostic methods often rely on generic model structures, and require vast training data to converge to a robust solution. A relatively new paradigm combines the power of the two: leveraging data-driven deep learning, as well as exploiting domain knowledge. These model-based solutions yield high robustness, and require less trainable parameters and training data than conventional neural networks. In this work we provide an overview of these methods from the recent literature, and discuss a wide variety of ultrasound applications. We aim to inspire the reader to further research in this area, and to address the opportunities within the field of ultrasound signal processing. We conclude with a future perspective on these model-based deep learning techniques for medical ultrasound applications.      
### 33.Approximation-free control based on the bioinspired reference model for suspension systems with uncertainty and unknown nonlinearity  [ :arrow_down: ](https://arxiv.org/pdf/2204.04456.pdf)
>  Uncertainty and unknown nonlinearity are often inevitable in the suspension systems, which were often solved using fuzzy logic system (FLS) or neural networks (NNs). However, these methods are restricted by the structural complexity of the controller and the huge computing cost. Meanwhile, the estimation error of such approximators is affected by adopted adaptive laws and learning gains. Thus, in view of the above problem, this paper proposes the approximation-free control based on the bioinspired reference model for a class of uncertain suspension systems with unknown nonlinearity. The proposed method integrates the superior vibration suppression of the bioinspired reference model and the structural advantage of the prescribed performance function (PPF) in approximation-free control. Then, the vibration suppression performance is improved, the calculation burden is relieved, and the transient performance is improved, which is analyzed theoretically in this paper. Finally, the simulation results validate the approach, and the comparisons show the advantages of the proposed control method in terms of good vibration suppression, fast convergence, and less calculation burden.      
### 34.CMOS Circuit Implementation of Spiking Neural Network for Pattern Recognition Using On-chip Unsupervised STDP Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.04430.pdf)
>  Computation on a large volume of data at high speed and low power requires energy-efficient computing architectures. Spiking neural network (SNN) with bio-inspired spike-timing-dependent plasticity learning (STDP) is a promising solution for energy-efficient neuromorphic systems than conventional artificial neural network (ANN). Previous works on SNN with STDP learning primarily uses memristive devices which are difficult to fabricate. Some reported works on SNN makes use of memristor macro models, which are software-based and cannot give complete insight into circuit implementation challenges. This article presents for the first time, a full circuit-level implementation of the SNN system featuring on-chip unsupervised STDP learning in standard CMOS technology. It does not involve the use of FPGAs, CPUs or GPUs for training the neural network. We demonstrated the complete circuit-level design, implementation and simulation of SNN with on-chip training and inference for pattern classification using 180 nm CMOS technology. A comprehensive comparison of the proposed SNN circuit with the previous related work is also presented. To demonstrate the versatility of the CMOS synapse circuit for application scenarios requiring rate-based learning, we have tuned the pair-based STDP circuit to obtain Bienenstock-Cooper-Munro (BCM) characteristics and applied it to heart rate classification.      
### 35.Robust Dynamic Average Consensus for a Network of Agents with Time-varying Reference Signals  [ :arrow_down: ](https://arxiv.org/pdf/2204.04415.pdf)
>  This paper presents continuous dynamic average consensus (DAC) algorithms for a group of agents to estimate the average of their time-varying reference signals cooperatively. We propose consensus algorithms that are robust to agents joining and leaving the network, at the same time, avoid the chattering phenomena and guarantee zero steady-state consensus error. Our algorithms are edge-based protocols with smooth functions in their internal structure to avoid the chattering effect. Furthermore, each agent is only capable of performing local computations and can only communicate with its local neighbors. For a balanced and strongly connected underlying communication graph, we provide the convergence analysis to determine the consensus design parameters that guarantee the agents' estimate of their average to asymptotically converge to the average of the time-varying reference signals of the agents. We provide simulation results to validate the proposed consensus algorithms and to perform a performance comparison of the proposed algorithms to existing algorithms in the literature.      
### 36.A Probabilistic Model-Based Robust Waveform Design for MIMO Radar Detection  [ :arrow_down: ](https://arxiv.org/pdf/2204.04408.pdf)
>  This paper addresses robust waveform design for multiple-input-multiple-output (MIMO) radar detection. A probabilistic model is proposed to describe the target uncertainty. Considering that waveform design based on maximizing the probability of detection is intractable, the relative entropy between the distributions of the observations under two hypotheses (viz., the target is present/absent) is employed as the design metric. To tackle the resulting non-convex optimization problem, an efficient algorithm based on minorization-maximization (MM) is derived. Numerical results demonstrate that the waveform synthesized by the proposed algorithm is more robust to model mismatches.      
### 37.A Machine Learning-Based Method for Identifying Critical Distance Relays for Transient Stability Studies  [ :arrow_down: ](https://arxiv.org/pdf/2204.04395.pdf)
>  Modeling protective relays is crucial for performing accurate stability studies as they play a critical role in defining the dynamic responses of power systems during disturbances. Nevertheless, due to the current limitations of stability software and the challenges of keeping track of the changes in the settings information of thousands of protective relays, modeling all the protective relays in bulk power systems is a challenging task. Distance relays are among the critical protection schemes, which are not properly modeled in current practices of stability studies. This paper proposes a machine learning-based method that uses the results of early-terminated stability studies to identify the critical distance relays required to be modeled in those studies. The algorithm used is the random forest (RF) classifier. GE positive sequence load flow analysis (PSLF) software is used to perform stability studies. The model is trained and tested on the Western Electricity Coordinating Council (WECC) system data representing the 2018 summer peak load under different operating conditions and topologies of the system. The results show the great performance of the method in identifying the critical distance relays. The results also show that only modeling the identified critical distance relays suffices to perform accurate stability studies.      
### 38.Dual-Stage Approach Toward Hyperspectral Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2204.04387.pdf)
>  Hyperspectral image produces high spectral resolution at the sacrifice of spatial resolution. Without reducing the spectral resolution, improving the resolution in the spatial domain is a very challenging problem. Motivated by the discovery that hyperspectral image exhibits high similarity between adjacent bands in a large spectral range, in this paper, we explore a new structure for hyperspectral image super-resolution (DualSR), leading to a dual-stage design, i.e., coarse stage and fine stage. In coarse stage, five bands with high similarity in a certain spectral range are divided into three groups, and the current band is guided to study the potential knowledge. Under the action of alternative spectral fusion mechanism, the coarse SR image is super-resolved in band-by-band. In order to build model from a global perspective, an enhanced back-projection method via spectral angle constraint is developed in fine stage to learn the content of spatial-spectral consistency, dramatically improving the performance gain. Extensive experiments demonstrate the effectiveness of the proposed coarse stage and fine stage. Besides, our network produces state-of-the-art results against existing works in terms of spatial reconstruction and spectral fidelity.      
### 39.Learning-based Bounded Synthesis for Semi-MDPs with LTL Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2204.04383.pdf)
>  This letter proposes a learning-based bounded synthesis for a semi-Markov decision process (SMDP) with a linear temporal logic (LTL) specification. In the product of the SMDP and the deterministic $K$-co-Bchi automaton (d$K$cBA) converted from the LTL specification, we learn both the winning region of satisfying the LTL specification and the dynamics therein based on reinforcement learning and Bayesian inference. Then, we synthesize an optimal policy satisfying the following two conditions. (1) It maximizes the probability of reaching the wining region. (2) It minimizes a long-term risk for the dwell time within the winning region. The minimization of the long-term risk is done based on the estimated dynamics and a value iteration. We show that, if the discount factor is sufficiently close to one, the synthesized policy converges to the optimal policy as the number of the data obtained by the exploration goes to the infinity.      
### 40.Small-Gain Theorem for Safety Verification under High-Relative-Degree Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2204.04376.pdf)
>  This paper develops a small-gain technique for the safety analysis and verification of interconnected systems with high-relative-degree safety constraints. In this technique, input-to-state safety (ISSf) is used to characterize how the safety of a subsystem is influenced by the external input, and ISSf-barrier functions (ISSf-BFs) with high relative degree are employed to capture the safety of subsystems. With a coordination transform, the relationship between ISSf-BFs and the existing high-relative-degree (or high-order) barrier functions is established in order to simplify the ISSf analysis. With the help of high-relative-degree ISSf-BFs, a small-gain theorem is proposed for safety verification. It is shown that, under the small-gain condition, i) the interconnection of ISSf subsystems is still ISSf; and ii) the overall interconnected system is input-to-state stable (ISS) with respect to the compositional safe set. The effectiveness of the proposed small-gain theorem is illustrated on the output-constrained decentralized control of two inverted pendulums connected by a spring mounted on two carts.      
### 41.QuiKo: A Quantum Beat Generation Application  [ :arrow_down: ](https://arxiv.org/pdf/2204.04370.pdf)
>  In this chapter a quantum music generation application called QuiKo will be discussed. It combines existing quantum algorithms with data encoding methods from quantum machine learning to build drum and audio sample patterns from a database of audio tracks. QuiKo leverages the physical properties and characteristics of quantum computers to generate what can be referred to as Soft Rules proposed by Alexis Kirke. These rules take advantage of the noise produced by quantum devices to develop flexible rules and grammars for quantum music generation. These properties include qubit decoherence and phase kickback due controlled quantum gates within the quantum circuit. QuiKo builds upon the concept of soft rules in quantum music generation and takes it a step further. It attempts to mimic and react to an external musical inputs, similar to the way that human musicians play and compose with one another. Audio signals are used as inputs into the system. Feature extraction is then performed on the signal to identify the harmonic and percussive elements. This information is then encoded onto the quantum circuit. Measurements of the quantum circuit are then taken providing results in the form of probability distributions for external music applications to use to build the new drum patterns.      
### 42.A Study of Using Cepstrogram for Countermeasure Against Replay Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2204.04333.pdf)
>  In this paper, we investigate the properties of the cepstrogram and demonstrate its effectiveness as a powerful feature for countermeasure against replay attacks. Cepstrum analysis of replay attacks suggests that crucial information for anti-spoofing against replay attacks may retain in the cepstrogram. Experimental results on the ASVspoof 2019 physical access (PA) database demonstrate that, compared with other features, the cepstrogram dominates in both single and fusion systems when building countermeasures against replay attacks. Our LCNN-based single and fusion systems with the cepstrogram feature outperform the corresponding LCNN-based systems without using the cepstrogram feature and several state-of-the-art (SOTA) single and fusion systems in the literature.      
### 43.Risk-Bounded Temporal Logic Control of Continuous-Time Stochastic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.04310.pdf)
>  Motivated by the recent interest in risk-aware control, we study a continuous-time control synthesis problem to bound the risk that a stochastic linear system violates a given specification. We use risk signal temporal logic as a specification formalism in which distributionally robust risk predicates are considered and equipped with the usual Boolean and temporal operators. Our control approach relies on reformulating these risk predicates as deterministic predicates over mean and covariance states of the system. We then obtain a timed sequence of sets of mean and covariance states from the timed automata representation of the specification. To avoid an explosion in the number of automata states, we propose heuristics to find candidate sequences effectively. To execute and check dynamic feasibility of these sequences, we present a sampled-data control technique based on time discretization and constraint tightening that allows to perform timed transitions while satisfying the continuous-time constraints.      
### 44.Unsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2204.04288.pdf)
>  Non-intrusive intelligibility prediction is important for its application in realistic scenarios, where a clean reference signal is difficult to access. The construction of many non-intrusive predictors require either ground truth intelligibility labels or clean reference signals for supervised learning. In this work, we leverage an unsupervised uncertainty estimation method for predicting speech intelligibility, which does not require intelligibility labels or reference signals to train the predictor. Our experiments demonstrate that the uncertainty from state-of-the-art end-to-end automatic speech recognition (ASR) models is highly correlated with speech intelligibility. The proposed method is evaluated on two databases and the results show that the unsupervised uncertainty measures of ASR models are more correlated with speech intelligibility from listening results than the predictions made by widely used intrusive methods.      
### 45.Exploiting Hidden Representations from a DNN-based Speech Recogniser for Speech Intelligibility Prediction in Hearing-impaired Listeners  [ :arrow_down: ](https://arxiv.org/pdf/2204.04287.pdf)
>  An accurate objective speech intelligibility prediction algorithms is of great interest for many applications such as speech enhancement for hearing aids. Most algorithms measures the signal-to-noise ratios or correlations between the acoustic features of clean reference signals and degraded signals. However, these hand-picked acoustic features are usually not explicitly correlated with recognition. Meanwhile, deep neural network (DNN) based automatic speech recogniser (ASR) is approaching human performance in some speech recognition tasks. This work leverages the hidden representations from DNN-based ASR as features for speech intelligibility prediction in hearing-impaired listeners. The experiments based on a hearing aid intelligibility database show that the proposed method could make better prediction than a widely used short-time objective intelligibility (STOI) based binaural measure.      
### 46.Auditory-Based Data Augmentation for End-to-End Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.04284.pdf)
>  End-to-end models have achieved significant improvement on automatic speech recognition. One common method to improve performance of these models is expanding the data-space through data augmentation. Meanwhile, human auditory inspired front-ends have also demonstrated improvement for automatic speech recognisers. In this work, a well-verified auditory-based model, which can simulate various hearing abilities, is investigated for the purpose of data augmentation for end-to-end speech recognition. By introducing the auditory model into the data augmentation process, end-to-end systems are encouraged to ignore variation from the signal that cannot be heard and thereby focus on robust features for speech recognition. Two mechanisms in the auditory model, spectral smearing and loudness recruitment, are studied on the LibriSpeech dataset with a transformer-based end-to-end model. The results show that the proposed augmentation methods can bring statistically significant improvement on the performance of the state-of-the-art SpecAugment.      
### 47.Grid-connected Soft Switching Partial Resonance Inverter for Distributed Generation  [ :arrow_down: ](https://arxiv.org/pdf/2204.04279.pdf)
>  This paper presents current control method for a grid-connected partial resonant soft switching inverter. This inverter does not use an electrolytic capacitor and is capable of boosting and bucking the voltage. Grid-connected inverters are used to integrate distributed energy sources to the grid. Current control is vital in meeting the standards and requirements when connecting to the grid. The closed-loop current regulation for this type of converters is analyzed and design guidelines are provided. The control is implemented in the synchronous frame. In addition active damping techniques using capacitor voltage and inductor voltage feedback is used to mitigate CL filter resonance at the output. The mentioned control strategies are implemented on a 400W lab prototype and the results are presented      
### 48.Towards Reliable and Explainable AI Model for Solid Pulmonary Nodule Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2204.04219.pdf)
>  Lung cancer has the highest mortality rate of deadly cancers in the world. Early detection is essential to treatment of lung cancer. However, detection and accurate diagnosis of pulmonary nodules depend heavily on the experiences of radiologists and can be a heavy workload for them. Computer-aided diagnosis (CAD) systems have been developed to assist radiologists in nodule detection and diagnosis, greatly easing the workload while increasing diagnosis accuracy. Recent development of deep learning, greatly improved the performance of CAD systems. However, lack of model reliability and interpretability remains a major obstacle for its large-scale clinical application. In this work, we proposed a multi-task explainable deep-learning model for pulmonary nodule diagnosis. Our neural model can not only predict lesion malignancy but also identify relevant manifestations. Further, the location of each manifestation can also be visualized for visual interpretability. Our proposed neural model achieved a test AUC of 0.992 on LIDC public dataset and a test AUC of 0.923 on our in-house dataset. Moreover, our experimental results proved that by incorporating manifestation identification tasks into the multi-task model, the accuracy of the malignancy classification can also be improved. This multi-task explainable model may provide a scheme for better interaction with the radiologists in a clinical environment.      
### 49.Multimodal Multi-Head Convolutional Attention with Various Kernel Sizes for Medical Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2204.04218.pdf)
>  Super-resolving medical images can help physicians in providing more accurate diagnostics. In many situations, computed tomography (CT) or magnetic resonance imaging (MRI) techniques output several scans (modes) during a single investigation, which can jointly be used (in a multimodal fashion) to further boost the quality of super-resolution results. To this end, we propose a novel multimodal multi-head convolutional attention module to super-resolve CT and MRI scans. Our attention module uses the convolution operation to perform joint spatial-channel attention on multiple concatenated input tensors, where the kernel (receptive field) size controls the reduction rate of the spatial attention and the number of convolutional filters controls the reduction rate of the channel attention, respectively. We introduce multiple attention heads, each head having a distinct receptive field size corresponding to a particular reduction rate for the spatial attention. We integrate our multimodal multi-head convolutional attention (MMHCA) into two deep neural architectures for super-resolution and conduct experiments on three data sets. Our empirical results show the superiority of our attention module over the state-of-the-art attention mechanisms used in super-resolution. Moreover, we conduct an ablation study to assess the impact of the components involved in our attention module, e.g. the number of inputs or the number of heads.      
### 50.Feature-enhanced Adversarial Semi-supervised Semantic Segmentation Network for Pulmonary Embolism Annotation  [ :arrow_down: ](https://arxiv.org/pdf/2204.04217.pdf)
>  This study established a feature-enhanced adversarial semi-supervised semantic segmentation model to automatically annotate pulmonary embolism lesion areas in computed tomography pulmonary angiogram (CTPA) images. In current studies, all of the PE CTPA image segmentation methods are trained by supervised learning. However, the supervised learning models need to be retrained and the images need to be relabeled when the CTPA images come from different hospitals. This study proposed a semi-supervised learning method to make the model applicable to different datasets by adding a small amount of unlabeled images. By training the model with both labeled and unlabeled images, the accuracy of unlabeled images can be improved and the labeling cost can be reduced. Our semi-supervised segmentation model includes a segmentation network and a discriminator network. We added feature information generated from the encoder of segmentation network to the discriminator so that it can learn the similarity between predicted mask and ground truth mask. This HRNet-based architecture can maintain a higher resolution for convolutional operations so the prediction of small PE lesion areas can be improved. We used the labeled open-source dataset and the unlabeled National Cheng Kung University Hospital (NCKUH) (IRB number: B-ER-108-380) dataset to train the semi-supervised learning model, and the resulting mean intersection over union (mIOU), dice score, and sensitivity achieved 0.3510, 0.4854, and 0.4253, respectively on the NCKUH dataset. Then, we fine-tuned and tested the model with a small amount of unlabeled PE CTPA images from China Medical University Hospital (CMUH) (IRB number: CMUH110-REC3-173) dataset. Comparing the results of our semi-supervised model with the supervised model, the mIOU, dice score, and sensitivity improved from 0.2344, 0.3325, and 0.3151 to 0.3721, 0.5113, and 0.4967, respectively.      
### 51.Learning Trajectory-Aware Transformer for Video Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2204.04216.pdf)
>  Video super-resolution (VSR) aims to restore a sequence of high-resolution (HR) frames from their low-resolution (LR) counterparts. Although some progress has been made, there are grand challenges to effectively utilize temporal dependency in entire video sequences. Existing approaches usually align and aggregate video frames from limited adjacent frames (e.g., 5 or 7 frames), which prevents these approaches from satisfactory results. In this paper, we take one step further to enable effective spatio-temporal learning in videos. We propose a novel Trajectory-aware Transformer for Video Super-Resolution (TTVSR). In particular, we formulate video frames into several pre-aligned trajectories which consist of continuous visual tokens. For a query token, self-attention is only learned on relevant visual tokens along spatio-temporal trajectories. Compared with vanilla vision Transformers, such a design significantly reduces the computational cost and enables Transformers to model long-range features. We further propose a cross-scale feature tokenization module to overcome scale-changing problems that often occur in long-range videos. Experimental results demonstrate the superiority of the proposed TTVSR over state-of-the-art models, by extensive quantitative and qualitative evaluations in four widely-used video super-resolution benchmarks. Both code and pre-trained models can be downloaded at <a class="link-external link-https" href="https://github.com/researchmm/TTVSR" rel="external noopener nofollow">this https URL</a>.      
### 52.Intelligent Sight and Sound: A Chronic Cancer Pain Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2204.04214.pdf)
>  Cancer patients experience high rates of chronic pain throughout the treatment process. Assessing pain for this patient population is a vital component of psychological and functional well-being, as it can cause a rapid deterioration of quality of life. Existing work in facial pain detection often have deficiencies in labeling or methodology that prevent them from being clinically relevant. This paper introduces the first chronic cancer pain dataset, collected as part of the Intelligent Sight and Sound (ISS) clinical trial, guided by clinicians to help ensure that model findings yield clinically relevant results. The data collected to date consists of 29 patients, 509 smartphone videos, 189,999 frames, and self-reported affective and activity pain scores adopted from the Brief Pain Inventory (BPI). Using static images and multi-modal data to predict self-reported pain levels, early models show significant gaps between current methods available to predict pain today, with room for improvement. Due to the especially sensitive nature of the inherent Personally Identifiable Information (PII) of facial images, the dataset will be released under the guidance and control of the National Institutes of Health (NIH).      
### 53.Settling the Sample Complexity of Model-Based Offline Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.05275.pdf)
>  This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications. <br>We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We prove that model-based offline RL yields $\varepsilon$-accuracy with a sample complexity of \[ \begin{cases} \frac{H^{4}SC_{\text{clipped}}^{\star}}{\varepsilon^{2}} &amp; (\text{finite-horizon MDPs}) \frac{SC_{\text{clipped}}^{\star}}{(1-\gamma)^{3}\varepsilon^{2}} &amp; (\text{infinite-horizon MDPs}) \end{cases} \] up to log factor, which is minimax optimal for the entire $\varepsilon$-range. Our algorithms are "pessimistic" variants of value iteration with Bernstein-style penalties, and do not require sophisticated variance reduction.      
### 54.Maximum entropy optimal density control of discrete-time linear systems and Schrdinger bridges  [ :arrow_down: ](https://arxiv.org/pdf/2204.05263.pdf)
>  We consider an entropy-regularized version of optimal density control of deterministic discrete-time linear systems. Entropy regularization, or a maximum entropy (MaxEnt) method for optimal control has attracted much attention especially in reinforcement learning due to its many advantages such as a natural exploration strategy. Despite the merits, high-entropy control policies introduce probabilistic uncertainty into systems, which severely limits the applicability of MaxEnt optimal control to safety-critical systems. To remedy this situation, we impose a Gaussian density constraint at a specified time on the MaxEnt optimal control to directly control state uncertainty. Specifically, we derive the explicit form of the MaxEnt optimal density control. In addition, we also consider the case where a density constraint is replaced by a fixed point constraint. Then, we characterize the associated state process as a pinned process, which is a generalization of the Brownian bridge to linear systems. Finally, we reveal that the MaxEnt optimal density control induces the so-called Schrdinger bridge associated to a discrete-time linear system.      
### 55.Performance analysis of WDM in LoS communications with arbitrary orientation and position  [ :arrow_down: ](https://arxiv.org/pdf/2204.05224.pdf)
>  This letter focuses on the wavenumber-division-multiplexing (WDM) scheme that was recently proposed in [1] for line-of-sight communications between parallel spatially-continuous electromagnetic segments. Our aim is to analyze the performance of WDM, combined with different digital processing architectures, when the electromagnetic segments have an arbitrary orientation and position. To this end, we first show how the general electromagnetic MIMO (multiple-input multiple-output) model from [1] can be particularized to the case of interest and then use numerical results to evaluate the impact of system parameters (e.g., horizontal and vertical distances, azimuth and elevation orientations). It turns out that WDM performs satisfactorily also when the transmit and receive segments are not in boresight direction of each other.      
### 56.Resource Allocation for Multiuser Edge Inference with Batching and Early Exiting (Extended Version)  [ :arrow_down: ](https://arxiv.org/pdf/2204.05223.pdf)
>  The deployment of inference services at the network edge, called edge inference, offloads computation-intensive inference tasks from mobile devices to edge servers, thereby enhancing the former's capabilities and battery lives. In a multiuser system, the joint allocation of communication-and-computation ($\text{C}^\text{2}$) resources (i.e., scheduling and bandwidth allocation) is made challenging by adopting efficient inference techniques, batching and early exiting, and further complicated by the heterogeneity in users' requirements on accuracy and latency. Batching groups multiple tasks into one batch for parallel processing to reduce time-consuming memory access and thereby boosts the throughput (i.e., completed task per second). On the other hand, early exiting allows a task to exit from a deep-neural network without traversing the whole network to support a tradeoff between accuracy and latency. In this work, we study optimal $\text{C}^\text{2}$ resource allocation with batching and early exiting, which is an NP-complete integer program. A set of efficient algorithms are designed under the criterion of maximum throughput by tackling the challenge. Experimental results demonstrate that both optimal and sub-optimal $\text{C}^\text{2}$ resource allocation algorithms can leverage integrated batching and early exiting to achieve 200% throughput gain over conventional schemes.      
### 57.INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2204.05222.pdf)
>  Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams caused by data transmission failures in packet switched networks. This is a common problem, and of increasing importance as end-to-end VoIP telephony and teleconference systems become the default and ever more widely used form of communication in business as well as in personal usage. This paper presents the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an overview of the PLC problem, and introduce some classical approaches to PLC as well as recent work. We then present the open source dataset released as part of this challenge as well as the evaluation methods and metrics used to determine the winner. We also briefly introduce PLCMOS, a novel data-driven metric that can be used to quickly evaluate the performance PLC systems. Finally, we present the results of the INTERSPEECH 2022 Audio Deep PLC Challenge, and provide a summary of important takeaways.      
### 58.Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.05188.pdf)
>  Recent advances in End-to-End (E2E) Spoken Language Understanding (SLU) have been primarily due to effective pretraining of speech representations. One such pretraining paradigm is the distillation of semantic knowledge from state-of-the-art text-based models like BERT to speech encoder neural networks. This work is a step towards doing the same in a much more efficient and fine-grained manner where we align speech embeddings and BERT embeddings on a token-by-token basis. We introduce a simple yet novel technique that uses a cross-modal attention mechanism to extract token-level contextual embeddings from a speech encoder such that these can be directly compared and aligned with BERT based contextual embeddings. This alignment is performed using a novel tokenwise contrastive loss. Fine-tuning such a pretrained model to perform intent recognition using speech directly yields state-of-the-art performance on two widely used SLU datasets. Our model improves further when fine-tuned with additional regularization using SpecAugment especially when speech is noisy, giving an absolute improvement as high as 8% over previous results.      
### 59.Domain Adversarial Graph Convolutional Network Based on RSSI and Crowdsensing for Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2204.05184.pdf)
>  In recent years, due to the wider WiFi coverage and the popularization of mobile communication devices, the technology of indoor positioning using WiFi fingerprints has been rapidly developed. Currently, most supervised methods need to collect a large amount of data to construct fingerprint datasets, which is labor-intensive and time-consuming. To solve the problem, we proposed a novel WiDAGCN model that can be trained with a few labeled site survey data and unlabeled crowdsensing WiFi fingerprints. To comprehensively represent the topology structure of the data, we constructed heterogeneous graphs according to the received signal strength indicators (RSSIs) between the waypoints and WiFi access points (APs). We focus on the graph convolutional network (GCN) method and the representation of graph-level features, which were rarely involved in previous WiFi indoor localization studies. Then, we try to minimize the difference between the source and target domains and make full use of the unlabeled data in the target domain using the domain adversarial training scheme. A public indoor localization dataset containing different buildings was used to evaluate the performance of the model. The experimental results show that our system can achieve a competitive localization accuracy in large buildings such as shopping malls.      
### 60.Building an ASR Error Robust Spoken Virtual Patient System in a Highly Class-Imbalanced Scenario Without Speech Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.05183.pdf)
>  A Virtual Patient (VP) is a powerful tool for training medical students to take patient histories, where responding to a diverse set of spoken questions is essential to simulate natural conversations with a student. The performance of such a Spoken Language Understanding system (SLU) can be adversely affected by both the presence of Automatic Speech Recognition (ASR) errors in the test data and a high degree of class imbalance in the SLU training data. While these two issues have been addressed separately in prior work, we develop a novel two-step training methodology that tackles both these issues effectively in a single dialog agent. As it is difficult to collect spoken data from users without a functioning SLU system, our method does not rely on spoken data for training, rather we use an ASR error predictor to "speechify" the text data. Our method shows significant improvements over strong baselines on the VP intent classification task at various word error rate settings.      
### 61.How to Listen? Rethinking Visual Sound Localization  [ :arrow_down: ](https://arxiv.org/pdf/2204.05156.pdf)
>  Localizing visual sounds consists on locating the position of objects that emit sound within an image. It is a growing research area with potential applications in monitoring natural and urban environments, such as wildlife migration and urban traffic. Previous works are usually evaluated with datasets having mostly a single dominant visible object, and proposed models usually require the introduction of localization modules during training or dedicated sampling strategies, but it remains unclear how these design choices play a role in the adaptability of these methods in more challenging scenarios. In this work, we analyze various model choices for visual sound localization and discuss how their different components affect the model's performance, namely the encoders' architecture, the loss function and the localization strategy. Furthermore, we study the interaction between these decisions, the model performance, and the data, by digging into different evaluation datasets spanning different difficulties and characteristics, and discuss the implications of such decisions in the context of real-world applications. Our code and model weights are open-sourced and made available for further applications.      
### 62.Artificial Intelligence Enabled Spectral Reconfigurable Fiber Laser  [ :arrow_down: ](https://arxiv.org/pdf/2204.05146.pdf)
>  The combinations of artificial intelligence and lasers provide powerful ways to form smart light sources with ground-breaking functions. Here, a Raman fiber laser (RFL) with reconfigurable and programmable spectra in an ultra-wide bandwidth is developed based on spectral-spatial manipulation of light in multimode fiber (MMF). The proposed fiber laser uses nonlinear gain from cascaded stimulated Raman scattering, random distributed feedback from Rayleigh scattering, and point feedback from an MMF-based smart spectral filter. Through wavefront shaping controlled by a genetic algorithm, light of selective wavelength(s) can be focused in the MMF, forming the filter that, together with the active part of the laser, actively shape the output spectrum with a high degree of freedom. We achieved arbitrary spectral shaping of the cascaded RFL (e.g., continuously tunable single-wavelength and multi-wavelength laser with customizable linewidth, mode separation, and power distribution) from the 1st- to the 3rd-order Stokes emission by adjusting the pump power and auto-optimization of the smart filter. Our research uses artificial-intelligence controlled light manipulation in a fiber platform with multi-eigenmodes and nonlinear gain, mapping the spatial control into the spectral domain as well as extending the linear control of light in MMF to active light emission, which is of great significance for applications in optical communication, sensing, and spectroscopy.      
### 63.PetroGAN: A novel GAN-based approach to generate realistic, label-free petrographic datasets  [ :arrow_down: ](https://arxiv.org/pdf/2204.05114.pdf)
>  Deep learning architectures have enriched data analytics in the geosciences, complementing traditional approaches to geological problems. Although deep learning applications in geosciences show encouraging signs, the actual potential remains untapped. This is primarily because geological datasets, particularly petrography, are limited, time-consuming, and expensive to obtain, requiring in-depth knowledge to provide a high-quality labeled dataset. We approached these issues by developing a novel deep learning framework based on generative adversarial networks (GANs) to create the first realistic synthetic petrographic dataset. The StyleGAN2 architecture is selected to allow robust replication of statistical and esthetical characteristics, and improving the internal variance of petrographic data. The training dataset consists of 10070 images of rock thin sections both in plane- and cross-polarized light. The algorithm trained for 264 GPU hours and reached a state-of-the-art Frchet Inception Distance (FID) score of 12.49 for petrographic images. We further observed the FID values vary with lithology type and image resolution. Our survey established that subject matter experts found the generated images were indistinguishable from real images. This study highlights that GANs are a powerful method for generating realistic synthetic data, experimenting with the latent space, and as a future tool for self-labelling, reducing the effort of creating geological datasets.      
### 64.Transformer-Based Self-Supervised Learning for Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.05103.pdf)
>  In order to exploit representations of time-series signals, such as physiological signals, it is essential that these representations capture relevant information from the whole signal. In this work, we propose to use a Transformer-based model to process electrocardiograms (ECG) for emotion recognition. Attention mechanisms of the Transformer can be used to build contextualized representations for a signal, giving more importance to relevant parts. These representations may then be processed with a fully-connected network to predict emotions. To overcome the relatively small size of datasets with emotional labels, we employ self-supervised learning. We gathered several ECG datasets with no labels of emotion to pre-train our model, which we then fine-tuned for emotion recognition on the AMIGOS dataset. We show that our approach reaches state-of-the-art performances for emotion recognition using ECG signals on AMIGOS. More generally, our experiments show that transformers and pre-training are promising strategies for emotion recognition with physiological signals.      
### 65.An approach to improving sound-based vehicle speed estimation  [ :arrow_down: ](https://arxiv.org/pdf/2204.05082.pdf)
>  We consider improving the performance of a recently proposed sound-based vehicle speed estimation method. In the original method, an intermediate feature, referred to as the modified attenuation (MA), has been proposed for both vehicle detection and speed estimation. The MA feature maximizes at the instant of the vehicle's closest point of approach, which represents a training label extracted from video recording of the vehicle's pass by. In this paper, we show that the original labeling approach is suboptimal and propose a method for label correction. The method is tested on the VS10 dataset, which contains 304 audio-video recordings of ten different vehicles. The results show that the proposed label correction method reduces average speed estimation error from 7.39 km/h to 6.92 km/h. If the speed is discretized into 10 km/h classes, the accuracy of correct class prediction is improved from 53.2% to 53.8%, whereas when tolerance of one class offset is allowed, accuracy is improved from 93.4% to 94.3%.      
### 66.End-to-End Speech Translation for Code Switched Speech  [ :arrow_down: ](https://arxiv.org/pdf/2204.05076.pdf)
>  Code switching (CS) refers to the phenomenon of interchangeably using words and phrases from different languages. CS can pose significant accuracy challenges to NLP, due to the often monolingual nature of the underlying systems. In this work, we focus on CS in the context of English/Spanish conversations for the task of speech translation (ST), generating and evaluating both transcript and translation. To evaluate model performance on this task, we create a novel ST corpus derived from existing public data sets. We explore various ST architectures across two dimensions: cascaded (transcribe then translate) vs end-to-end (jointly transcribe and translate) and unidirectional (source -&gt; target) vs bidirectional (source &lt;-&gt; target). We show that our ST architectures, and especially our bidirectional end-to-end architecture, perform well on CS speech, even when no CS training data is used.      
### 67.Fine-grained Noise Control for Multispeaker Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2204.05070.pdf)
>  A text-to-speech (TTS) model typically factorizes speech attributes such as content, speaker and prosody into disentangled representations.Recent works aim to additionally model the acoustic conditions explicitly, in order to disentangle the primary speech factors, i.e. linguistic content, prosody and timbre from any residual factors, such as recording conditions and background noise.This paper proposes unsupervised, interpretable and fine-grained noise and prosody modeling. We incorporate adversarial training, representation bottleneck and utterance-to-frame modeling in order to learn frame-level noise representations. To the same end, we perform fine-grained prosody modeling via a Fully Hierarchical Variational AutoEncoder (FVAE) which additionally results in more expressive speech synthesis.      
### 68.Performance Metrics for Communication Systems with Forward Error Correction  [ :arrow_down: ](https://arxiv.org/pdf/2204.05051.pdf)
>  We revisit performance metrics for optical communication systems with FEC. We illustrate the concept of universality and discuss the most widespread performance thresholds. Finally, we show by example how to include FEC into transmission experiments.      
### 69.gTLO: A Generalized and Non-linear Multi-Objective Deep Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.04988.pdf)
>  In real-world decision optimization, often multiple competing objectives must be taken into account. Following classical reinforcement learning, these objectives have to be combined into a single reward function. In contrast, multi-objective reinforcement learning (MORL) methods learn from vectors of per-objective rewards instead. In the case of multi-policy MORL, sets of decision policies for various preferences regarding the conflicting objectives are optimized. This is especially important when target preferences are not known during training or when preferences change dynamically during application. While it is, in general, straightforward to extend a single-objective reinforcement learning method for MORL based on linear scalarization, solutions that are reachable by these methods are limited to convex regions of the Pareto front. Non-linear MORL methods like Thresholded Lexicographic Ordering (TLO) are designed to overcome this limitation. Generalized MORL methods utilize function approximation to generalize across objective preferences and thereby implicitly learn multiple policies in a data-efficient manner, even for complex decision problems with high-dimensional or continuous state spaces. In this work, we propose \textit{generalized Thresholded Lexicographic Ordering} (gTLO), a novel method that aims to combine non-linear MORL with the advantages of generalized MORL. We introduce a deep reinforcement learning realization of the algorithm and present promising results on a standard benchmark for non-linear MORL and a real-world application from the domain of manufacturing process control.      
### 70.Consistent Estimators for Nonlinear Vessel Models  [ :arrow_down: ](https://arxiv.org/pdf/2204.04973.pdf)
>  In this work, the issue of obtaining consistent parameter estimators for nonlinear regression models where the regressors are second-order modulus functions is explored. It is shown that consistent instrumental variable estimators can be obtained by estimating first and second-order moments of non-additive environmental disturbances' probability distributions as nuisance parameters in parallel to the sought-after model parameters, conducting experiments with a static excitation offset of sufficient amplitude and forcing the instruments to have zero mean. The proposed method is evaluated in a simulation example with a model of a marine surface vessel.      
### 71.Assessing hierarchies by their consistent segmentations  [ :arrow_down: ](https://arxiv.org/pdf/2204.04969.pdf)
>  Recent segmentation approaches start by creating a hierarchy of nested image partitions, and then specify a segmentation from it, usually, by choosing one horizontal cut. Our first contribution is to describe several different ways, some of them new, for specifying segmentations using the hierarchy regions. Then we consider the best hierarchy-induced segmentation, in which the segments are specified by a limited number, k, of hierarchy nodes/regions. The number of hierarchy-induced segmentations grows exponentially with the hierarchy size, implying that exhaustive search is unfeasible. We focus on a common quality measure, the Jaccard index (known also as IoU). Optimizing the Jaccard index is highly nontrivial. Yet, we propose an efficient optimization * This work was done when the first author was with the Math dept. Technion, Israel.      
### 72.Multistream neural architectures for cued-speech recognition using a pre-trained visual feature extractor and constrained CTC decoding  [ :arrow_down: ](https://arxiv.org/pdf/2204.04965.pdf)
>  This paper proposes a simple and effective approach for automatic recognition of Cued Speech (CS), a visual communication tool that helps people with hearing impairment to understand spoken language with the help of hand gestures that can uniquely identify the uttered phonemes in complement to lipreading. The proposed approach is based on a pre-trained hand and lips tracker used for visual feature extraction and a phonetic decoder based on a multistream recurrent neural network trained with connectionist temporal classification loss and combined with a pronunciation lexicon. The proposed system is evaluated on an updated version of the French CS dataset CSF18 for which the phonetic transcription has been manually checked and corrected. With a decoding accuracy at the phonetic level of 70.88%, the proposed system outperforms our previous CNN-HMM decoder and competes with more complex baselines.      
### 73.Low-Complexity Sum-Capacity Maximization for Intelligent Reflecting Surface-Aided MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.04915.pdf)
>  Reducing computational complexity is crucial in optimizing the phase shifts of Intelligent Reflecting Surface (IRS) systems since IRS-assisted communication systems are generally deployed with a large number of reflecting elements (REs). This letter proposes a low-complexity algorithm, designated as Dimension-wise Sinusoidal Maximization (DSM), to obtain the optimal IRS phase shifts that maximize the sum capacity of a MIMO network. The algorithm exploits the fact that the objective function for the optimization problem is sinusoidal w.r.t. the phase shift of each RE. The numerical results show that DSM achieves a near-maximal sum rate and faster convergence speed than two other benchmark methods.      
### 74.Confusing Image Quality Assessment: Towards Better Augmented Reality Experience  [ :arrow_down: ](https://arxiv.org/pdf/2204.04900.pdf)
>  With the development of multimedia technology, Augmented Reality (AR) has become a promising next-generation mobile platform. The primary value of AR is to promote the fusion of digital contents and real-world environments, however, studies on how this fusion will influence the Quality of Experience (QoE) of these two components are lacking. To achieve better QoE of AR, whose two layers are influenced by each other, it is important to evaluate its perceptual quality first. In this paper, we consider AR technology as the superimposition of virtual scenes and real scenes, and introduce visual confusion as its basic theory. A more general problem is first proposed, which is evaluating the perceptual quality of superimposed images, i.e., confusing image quality assessment. A ConFusing Image Quality Assessment (CFIQA) database is established, which includes 600 reference images and 300 distorted images generated by mixing reference images in pairs. Then a subjective quality perception study and an objective model evaluation experiment are conducted towards attaining a better understanding of how humans perceive the confusing images. An objective metric termed CFIQA is also proposed to better evaluate the confusing image quality. Moreover, an extended ARIQA study is further conducted based on the CFIQA study. We establish an ARIQA database to better simulate the real AR application scenarios, which contains 20 AR reference images, 20 background (BG) reference images, and 560 distorted images generated from AR and BG references, as well as the correspondingly collected subjective quality ratings. We also design three types of full-reference (FR) IQA metrics to study whether we should consider the visual confusion when designing corresponding IQA algorithms. An ARIQA metric is finally proposed for better evaluating the perceptual quality of AR images.      
### 75.Why Shape Coding? Asymptotic Analysis of the Entropy Rate for Digital Images  [ :arrow_down: ](https://arxiv.org/pdf/2204.04857.pdf)
>  This paper focuses on the ultimate limit theory of image compression. It proves that for an image source, there exists a coding method with shapes that can achieve the entropy rate under a certain condition where the shape-pixel ratio in the encoder/decoder is $O({1 \over {\log t}})$. Based on the new finding, an image coding framework with shapes is proposed and proved to be asymptotically optimal for stationary and ergodic processes.      
### 76.Fusion of Self-supervised Learned Models for MOS Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2204.04855.pdf)
>  We participated in the mean opinion score (MOS) prediction challenge, 2022. This challenge aims to predict MOS scores of synthetic speech on two tracks, the main track and a more challenging sub-track: out-of-domain (OOD). To improve the accuracy of the predicted scores, we have explored several model fusion-related strategies and proposed a fused framework in which seven pretrained self-supervised learned (SSL) models have been engaged. These pretrained SSL models are derived from three ASR frameworks, including Wav2Vec, Hubert, and WavLM. For the OOD track, we followed the 7 SSL models selected on the main track and adopted a semi-supervised learning method to exploit the unlabeled data. According to the official analysis results, our system has achieved 1st rank in 6 out of 16 metrics and is one of the top 3 systems for 13 out of 16 metrics. Specifically, we have achieved the highest LCC, SRCC, and KTAU scores at the system level on main track, as well as the best performance on the LCC, SRCC, and KTAU evaluation metrics at the utterance level on OOD track. Compared with the basic SSL models, the prediction accuracy of the fused system has been largely improved, especially on OOD sub-track.      
### 77.On the pragmatism of using binary classifiers over data intensive neural network classifiers for detection of COVID-19 from voice  [ :arrow_down: ](https://arxiv.org/pdf/2204.04802.pdf)
>  Lately, there has been a global effort by multiple research groups to detect COVID-19 from voice. Different researchers use different kinds of information from the voice signal to achieve this. Various types of phonated sounds and the sound of cough and breath have all been used with varying degrees of success in automated voice-based COVID-19 detection apps. In this paper, we show that detecting COVID-19 from voice does not require custom-made non-standard features or complicated neural network classifiers rather it can be successfully done with just standard features and simple binary classifiers. In fact, we show that the latter is not only more accurate and interpretable and also more computationally efficient in that they can be run locally on small devices. We demonstrate this from a human-curated dataset collected and calibrated in clinical settings. On this dataset which comprises over 1000 speakers, a simple binary classifier is able to achieve 94% detection accuracy.      
### 78.Risk-aware UAV-UGV Rendezvous with Chance-Constrained Markov Decision Process  [ :arrow_down: ](https://arxiv.org/pdf/2204.04767.pdf)
>  We study a chance-constrained variant of the cooperative aerial-ground vehicle routing problem, in which an Unmanned Aerial Vehicle (UAV) with limited battery capacity and an Unmanned Ground Vehicle (UGV) that can also act as a mobile recharging station need to jointly accomplish a mission such as monitoring a set of points. Due to the limited battery capacity of the UAV, two vehicles sometimes have to deviate from their task to rendezvous and recharge the UAV\@. Unlike prior work that has focused on the deterministic case, we address the challenge of stochastic energy consumption of the UAV\@. We are interested in finding the optimal policy that decides when and where to rendezvous such that the expected travel time of the UAV is minimized and the probability of running out of charge is less than a user-defined tolerance. We formulate this problem as a Chance Constrained Markov Decision Process (CCMDP). To the best knowledge of the authors, this is the first CMDP-based formulation for the UAV-UGV routing problems under power consumption uncertainty. We adopt a Linear Programming (LP) based approach to solve the problem optimally. We demonstrate the effectiveness of our formulation in the context of an Intelligence Surveillance and Reconnaissance (ISR) mission.      
### 79.Configuration and Collection Factors for Side-Channel Disassembly  [ :arrow_down: ](https://arxiv.org/pdf/2204.04766.pdf)
>  Myriad uses, methodologies, and channels have been explored for side-channel analysis. However, specific implementation considerations are often unpublished. This paper explores select test configuration and collection parameters, such as input voltage, shunt resistance, sample rate, and microcontroller clock frequency, along with their impact on side-channel analysis performance. The analysis use case considered is instruction disassembly and classification using the microcontroller power side-channel. An ATmega328P microcontroller and a subset of the AVR instruction set are used in the experiments as the Device Under Test (DUT). A time-series convolutional neural network (CNN) is used to evaluate classification performance at clock-cycle fidelity. We conclude that configuration and collection parameters have a meaningful impact on performance, especially where the instruction-trace's signal to noise ratio (SNR) is impacted. Additionally, data collection and analysis well above the Nyquist rate is required for side-channel disassembly. We also found that 7V input voltage with 1 kiloohm shunt and a sample rate of 250-500 MSa/s provided optimal performance in our application, with diminishing returns or in some cases degradation at higher levels.      
### 80.Towards Evaluation of Autonomously Generated Musical Compositions: A Comprehensive Survey  [ :arrow_down: ](https://arxiv.org/pdf/2204.04756.pdf)
>  There are many applications that aim to create a complete model for an autonomously generated composition; systems are able to generate muzak songs, assist singers in transcribing songs or can imitate long-dead authors. Subjective understanding of creativity or aesthetics differs not only within preferences (popular authors or genres), but also differs on the basis of experienced experience or socio-cultural environment. So, what do we want to achieve with such an adaptation? What is the benefit of the resulting work for the author, who can no longer evaluate this composition? And in what ways should we evaluate such a composition at all?      
### 81.Machine Learning-Based CSI Feedback With Variable Length in FDD Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2204.04723.pdf)
>  To fully unlock the benefits of multiple-input multiple-output (MIMO) networks, downlink channel state information (CSI) is required at the base station (BS). In frequency division duplex (FDD) systems, the CSI is acquired through a feedback signal from the user equipment (UE). However, this may lead to an important overhead in FDD massive MIMO systems. Focusing on these systems, in this study, we propose a novel strategy to design the CSI feedback. Our strategy allows to optimally design the feedback with variable length, while reducing the parameter number at the UE. Specifically, principal component analysis (PCA) is used to compress the channel into a latent space with adaptive dimensionality. To quantize this compressed channel, the feedback bits are smartly allocated to the latent space dimensions by minimizing the normalized mean squared error (NMSE) distortion. Finally, the quantization codebook is determined with k-means clustering. Numerical simulations show that our strategy improves the zeroforcing beamforming sum rate by 26.8%, compared with the popular CsiNet. The number of model parameters is reduced by 24.9 times, thus causing a significantly smaller offloading overhead. At the same time, PCA is characterized by a lightweight unsupervised training, requiring eight times fewer training samples than CsiNet.      
### 82.Generative Adversarial Networks for Image Augmentation in Agriculture: A Systematic Review  [ :arrow_down: ](https://arxiv.org/pdf/2204.04707.pdf)
>  In agricultural image analysis, optimal model performance is keenly pursued for better fulfilling visual recognition tasks (e.g., image classification, segmentation, object detection and localization), in the presence of challenges with biological variability and unstructured environments. Large-scale, balanced and ground-truthed image datasets, however, are often difficult to obtain to fuel the development of advanced, high-performance models. As artificial intelligence through deep learning is impacting analysis and modeling of agricultural images, data augmentation plays a crucial role in boosting model performance while reducing manual efforts for data preparation, by algorithmically expanding training datasets. Beyond traditional data augmentation techniques, generative adversarial network (GAN) invented in 2014 in the computer vision community, provides a suite of novel approaches that can learn good data representations and generate highly realistic samples. Since 2017, there has been a growth of research into GANs for image augmentation or synthesis in agriculture for improved model performance. This paper presents an overview of the evolution of GAN architectures followed by a systematic review of their application to agriculture (<a class="link-external link-https" href="https://github.com/Derekabc/GANs-Agriculture" rel="external noopener nofollow">this https URL</a>), involving various vision tasks for plant health, weeds, fruits, aquaculture, animal farming, plant phenotyping as well as postharvest detection of fruit defects. Challenges and opportunities of GANs are discussed for future research.      
### 83.Deep Conditional Representation Learning for Drum Sample Retrieval by Vocalisation  [ :arrow_down: ](https://arxiv.org/pdf/2204.04651.pdf)
>  Imitating musical instruments with the human voice is an efficient way of communicating ideas between music producers, from sketching melody lines to clarifying desired sonorities. For this reason, there is an increasing interest in building applications that allow artists to efficiently pick target samples from big sound libraries just by imitating them vocally. In this study, we investigated the potential of conditional autoencoder models to learn informative features for Drum Sample Retrieval by Vocalisation (DSRV). We assessed the usefulness of their embeddings using four evaluation metrics, two of them relative to their acoustic properties and two of them relative to their perceptual properties via human listeners' similarity ratings. Results suggest that models conditioned on both sound-type labels (drum vs imitation) and drum-type labels (kick vs snare vs closed hi-hat vs opened hi-hat) learn the most informative embeddings for DSRV. We finally looked into individual differences in vocal imitation style via the Mantel test and found salient differences among participants, highlighting the importance of user information when designing DSRV systems.      
### 84.Deep Embeddings for Robust User-Based Amateur Vocal Percussion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.04646.pdf)
>  Vocal Percussion Transcription (VPT) is concerned with the automatic detection and classification of vocal percussion sound events, allowing music creators and producers to sketch drum lines on the fly. Classifier algorithms in VPT systems learn best from small user-specific datasets, which usually restrict modelling to small input feature sets to avoid data overfitting. This study explores several deep supervised learning strategies to obtain informative feature sets for amateur vocal percussion classification. We evaluated the performance of these sets on regular vocal percussion classification tasks and compared them with several baseline approaches including feature selection methods and a speech recognition engine. These proposed learning models were supervised with several label sets containing information from four different levels of abstraction: instrument-level, syllable-level, phoneme-level, and boxeme-level. Results suggest that convolutional neural networks supervised with syllable-level annotations produced the most informative embeddings for classification, which can be used as input representations to fit classifiers with. Finally, we used back-propagation-based saliency maps to investigate the importance of different spectrogram regions for feature learning.      
### 85.Self-Supervised Audio-and-Text Pre-training with Extremely Low-Resource Parallel Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.04645.pdf)
>  Multimodal pre-training for audio-and-text has recently been proved to be effective and has significantly improved the performance of many downstream speech understanding tasks. However, these state-of-the-art pre-training audio-text models work well only when provided with large amount of parallel audio-and-text data, which brings challenges on many languages that are rich in unimodal corpora but scarce of parallel cross-modal corpus. In this paper, we investigate whether it is possible to pre-train an audio-text multimodal model with extremely low-resource parallel data and extra non-parallel unimodal data. Our pre-training framework consists of the following components: (1) Intra-modal Denoising Auto-Encoding (IDAE), which is able to reconstruct input text (audio) representations from a noisy version of itself. (2) Cross-modal Denoising Auto-Encoding (CDAE), which is pre-trained to reconstruct the input text (audio), given both a noisy version of the input text (audio) and the corresponding translated noisy audio features (text embeddings). (3) Iterative Denoising Process (IDP), which iteratively translates raw audio (text) and the corresponding text embeddings (audio features) translated from previous iteration into the new less-noisy text embeddings (audio features). We adapt a dual cross-modal Transformer as our backbone model which consists of two unimodal encoders for IDAE and two cross-modal encoders for CDAE and IDP. Our method achieves comparable performance on multiple downstream speech understanding tasks compared with the model pre-trained on fully parallel data, demonstrating the great potential of the proposed method. Our code is available at: \url{<a class="link-external link-https" href="https://github.com/KarlYuKang/Low-Resource-Multimodal-Pre-training" rel="external noopener nofollow">this https URL</a>}.      
### 86.A High Capacity Preamble Sequence for Random Access in Beyond 5G Networks: Design and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2204.04604.pdf)
>  The widely used Zadoff-Chu sequence (ZC sequence) for random access preamble in 5G has limitations in terms of the total number of preambles generated, forcing the reuse of preambles. Hence, the probability of collision of preambles of UEs increase, resulting in the failure of random access procedure. To truly qualify beyond 5G networks as green technology, the preamble capacity should be increased without sacrificing energy efficiency. In this paper, we propose a new candidate preamble sequence called $mALL$ sequence using the concept of cover sequences to achieve higher preamble capacity without degrading the power efficiency and hence minimizing device's carbon footprint. We compare the performance of $mALL$ sequence with Zadoff-Chu sequence and other sequences in the literature, such as $mZC$ and $aZC$ sequences. We evaluate the performance of the preamble sequences in terms of periodic correlation, detection probability and the effect of diversity combining. Also, this paper explores the Peak to Average Power Ratio (PAPR) and Cubic Metric(CM) for these sequences, as these are essential parameters to evaluate energy efficiency. We show that the preamble capacity of the proposed $mALL$ sequence is $10^{4}$ times higher than that of legacy ZC sequence without any deterioration in the detection performance.      
### 87.Inferring Pitch from Coarse Spectral Features  [ :arrow_down: ](https://arxiv.org/pdf/2204.04579.pdf)
>  Fundamental frequency (F0) has long been treated as the physical definition of "pitch" in phonetic analysis. But there have been many demonstrations that F0 is at best an approximation to pitch, both in production and in perception: pitch is not F0, and F0 is not pitch. Changes in the pitch involve many articulatory and acoustic covariates; pitch perception often deviates from what F0 analysis predicts; and in fact, quasi-periodic signals from a single voice source are often incompletely characterized by an attempt to define a single time-varying F0. In this paper, we find strong support for the existence of covariates for pitch in aspects of relatively coarse spectra, in which an overtone series is not available. Thus linear regression can predict the pitch of simple vocalizations, produced by an articulatory synthesizer or by human, from single frames of such coarse spectra. Across speakers, and in more complex vocalizations, our experiments indicate that the covariates are not quite so simple, though apparently still available for more sophisticated modeling. On this basis, we propose that the field needs a better way of thinking about speech pitch, just as celestial mechanics requires us to go beyond Newton's point mass approximations to heavenly bodies.      
### 88.Uncertainty-Informed Deep Learning Models Enable High-Confidence Predictions for Digital Histopathology  [ :arrow_down: ](https://arxiv.org/pdf/2204.04516.pdf)
>  A model's ability to express its own predictive uncertainty is an essential attribute for maintaining clinical user confidence as computational biomarkers are deployed into real-world medical settings. In the domain of cancer digital histopathology, we describe a novel, clinically-oriented approach to uncertainty quantification (UQ) for whole-slide images, estimating uncertainty using dropout and calculating thresholds on training data to establish cutoffs for low- and high-confidence predictions. We train models to identify lung adenocarcinoma vs. squamous cell carcinoma and show that high-confidence predictions outperform predictions without UQ, in both cross-validation and testing on two large external datasets spanning multiple institutions. Our testing strategy closely approximates real-world application, with predictions generated on unsupervised, unannotated slides using predetermined thresholds. Furthermore, we show that UQ thresholding remains reliable in the setting of domain shift, with accurate high-confidence predictions of adenocarcinoma vs. squamous cell carcinoma for out-of-distribution, non-lung cancer cohorts.      
### 89.Comparison of EEG based epilepsy diagnosis using neural networks and wavelet transform  [ :arrow_down: ](https://arxiv.org/pdf/2204.04488.pdf)
>  Epilepsy is one of the common neurological disorders characterized by recurrent and uncontrollable seizures, which seriously affect the life of patients. In many cases, electroencephalograms signal can provide important physiological information about the activity of the human brain which can be used to diagnose epilepsy. However, visual inspection of a large number of electroencephalogram signals is very time-consuming and can often lead to inconsistencies in physicians' diagnoses. Quantification of abnormalities in brain signals can indicate brain conditions and pathology so the electroencephalogram (EEG) signal plays a key role in the diagnosis of epilepsy. In this article, an attempt has been made to create a single instruction for diagnosing epilepsy, which consists of two steps. In the first step, a low-pass filter was used to preprocess the data and three separate mid-pass filters for different frequency bands and a multilayer neural network were designed. In the second step, the wavelet transform technique was used to process data. In particular, this paper proposes a multilayer perceptron neural network classifier for the diagnosis of epilepsy, that requires normal data and epilepsy data for education, but this classifier can recognize normal disorders, epilepsy, and even other disorders taught in educational examples. Also, the value of using electroencephalogram signal has been evaluated in two ways: using wavelet transform and non-using wavelet transform. Finally, the evaluation results indicate a relatively uniform impact factor on the use or non-use of wavelet transform on the improvement of epilepsy data functions, but in the end, it was shown that the use of perceptron multilayer neural network can provide a higher accuracy coefficient for experts.      
### 90.Multichannel Speech Separation with Narrow-band Conformer  [ :arrow_down: ](https://arxiv.org/pdf/2204.04464.pdf)
>  This work proposes a multichannel speech separation method with narrow-band Conformer (named NBC). The network is trained to learn to automatically exploit narrow-band speech separation information, such as spatial vector clustering of multiple speakers. Specifically, in the short-time Fourier transform (STFT) domain, the network processes each frequency independently, and is shared by all frequencies. For one frequency, the network inputs the STFT coefficients of multichannel mixture signals, and predicts the STFT coefficients of separated speech signals. Clustering of spatial vectors shares a similar principle with the self-attention mechanism in the sense of computing the similarity of vectors and then aggregating similar vectors. Therefore, Conformer would be especially suitable for the present problem. Experiments show that the proposed narrow-band Conformer achieves better speech separation performance than other state-of-the-art methods by a large margin.      
### 91.A3CLNN: Spatial, Spectral and Multiscale Attention ConvLSTM Neural Network for Multisource Remote Sensing Data Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.04462.pdf)
>  The problem of effectively exploiting the information multiple data sources has become a relevant but challenging research topic in remote sensing. In this paper, we propose a new approach to exploit the complementarity of two data sources: hyperspectral images (HSIs) and light detection and ranging (LiDAR) data. Specifically, we develop a new dual-channel spatial, spectral and multiscale attention convolutional long short-term memory neural network (called dual-channel A3CLNN) for feature extraction and classification of multisource remote sensing data. Spatial, spectral and multiscale attention mechanisms are first designed for HSI and LiDAR data in order to learn spectral- and spatial-enhanced feature representations, and to represent multiscale information for different classes. In the designed fusion network, a novel composite attention learning mechanism (combined with a three-level fusion strategy) is used to fully integrate the features in these two data sources. Finally, inspired by the idea of transfer learning, a novel stepwise training strategy is designed to yield a final classification result. Our experimental results, conducted on several multisource remote sensing data sets, demonstrate that the newly proposed dual-channel A3CLNN exhibits better feature representation ability (leading to more competitive classification performance) than other state-of-the-art methods.      
### 92.Noise-based Enhancement for Foveated Rendering  [ :arrow_down: ](https://arxiv.org/pdf/2204.04455.pdf)
>  Human visual sensitivity to spatial details declines towards the periphery. Novel image synthesis techniques, so-called foveated rendering, exploit this observation and reduce the spatial resolution of synthesized images for the periphery, avoiding the synthesis of high-spatial-frequency details that are costly to generate but not perceived by a viewer. However, contemporary techniques do not make a clear distinction between the range of spatial frequencies that must be reproduced and those that can be omitted. For a given eccentricity, there is a range of frequencies that are detectable but not resolvable. While the accurate reproduction of these frequencies is not required, an observer can detect their absence if completely omitted. We use this observation to improve the performance of existing foveated rendering techniques. We demonstrate that this specific range of frequencies can be efficiently replaced with procedural noise whose parameters are carefully tuned to image content and human perception. Consequently, these frequencies do not have to be synthesized during rendering, allowing more aggressive foveation, and they can be replaced by noise generated in a less expensive post-processing step, leading to improved performance of the rendering system. Our main contribution is a perceptually-inspired technique for deriving the parameters of the noise required for the enhancement and its calibration. The method operates on rendering output and runs at rates exceeding 200FPS at 4K resolution, making it suitable for integration with real-time foveated rendering systems for VR and AR devices. We validate our results and compare them to the existing contrast enhancement technique in user experiments.      
### 93.HSTR-Net: High Spatio-Temporal Resolution Video Generation For Wide Area Surveillance  [ :arrow_down: ](https://arxiv.org/pdf/2204.04435.pdf)
>  Wide area surveillance has many applications and tracking of objects under observation is an important task, which often needs high spatio-temporal resolution (HSTR) video for better precision. This paper presents the usage of multiple video feeds for the generation of HSTR video as an extension of reference based super resolution (RefSR). One feed captures video at high spatial resolution with low frame rate (HSLF) while the other captures low spatial resolution and high frame rate (LSHF) video simultaneously for the same scene. The main purpose is to create an HSTR video from the fusion of HSLF and LSHF videos. In this paper we propose an end-to-end trainable deep network that performs optical flow estimation and frame reconstruction by combining inputs from both video feeds. The proposed architecture provides significant improvement over existing video frame interpolation and RefSR techniques in terms of objective PSNR and SSIM metrics.      
### 94.Leaderless Swarm Formation Control: From Global Specifications to Local Control Laws  [ :arrow_down: ](https://arxiv.org/pdf/2204.04412.pdf)
>  This paper introduces a distributed leaderless swarm formation control framework to address the problem of collectively driving a swarm of robots to track a time-varying formation. The swarm's formation is captured by the trajectory of an abstract shape that circumscribes the convex hull of robots' positions and is independent of the number of robots and their ordering in the swarm. For each robot in the swarm, given global specifications in terms of the trajectory of the abstract shape parameters, the proposed framework synthesizes a control law that steers the swarm to track the desired formation using the information available at the robot's local neighbors. For this purpose, we generate a suitable local reference trajectory that the robot controller tracks by solving the input-output linearization problem. Here, we select the swarm output to be the parameters of the abstract shape. For this purpose, we design a dynamic average consensus estimator to estimate the abstract shape parameters. The abstract shape parameters are used as the swarm state feedback to generate a suitable robot trajectory. We demonstrate the effectiveness and robustness of the proposed control framework by providing the simulation of coordinated collective navigation of a group of car-like robots in the presence of robots and communication link failures.      
### 95.Improve Generalization of Driving Policy at Signalized Intersections with Adversarial Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.04403.pdf)
>  Intersections are quite challenging among various driving scenes wherein the interaction of signal lights and distinct traffic actors poses great difficulty to learn a wise and robust driving policy. Current research rarely considers the diversity of intersections and stochastic behaviors of traffic participants. For practical applications, the randomness usually leads to some devastating events, which should be the focus of autonomous driving. This paper introduces an adversarial learning paradigm to boost the intelligence and robustness of driving policy for signalized intersections with dense traffic flow. Firstly, we design a static path planner which is capable of generating trackable candidate paths for multiple intersections with diversified topology. Next, a constrained optimal control problem (COCP) is built based on these candidate paths wherein the bounded uncertainty of dynamic models is considered to capture the randomness of driving environment. We propose adversarial policy gradient (APG) to solve the COCP wherein the adversarial policy is introduced to provide disturbances by seeking the most severe uncertainty while the driving policy learns to handle this situation by competition. Finally, a comprehensive system is established to conduct training and testing wherein the perception module is introduced and the human experience is incorporated to solve the yellow light dilemma. Experiments indicate that the trained policy can handle the signal lights flexibly meanwhile realizing the smooth and efficient passing with a humanoid paradigm. Besides, APG enables a large-margin improvement of the resistance to the abnormal behaviors and thus ensures a high safety level for the autonomous vehicle.      
### 96.Fundamental Limits on Detection With a Dual-function Radar Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2204.04332.pdf)
>  This paper investigates the fundamental limits on the target detection performance with a dual-function multiple-input-multiple-output (MIMO) radar communication (RadCom) systems. By assuming the presence of a point-like target and a communication receiver, closed-form expressions for the maximum detection probability and the transmit waveforms achieving the optimal performance are derived. Results show that for the considered case, the dual-function system should transmit coherent waveforms to achieve the optimal detection performance. Moreover, the angle separation between the target and communication receiver has a great impact on the achievable detection performance.      
### 97.Understanding the Influence of Receptive Field and Network Complexity in Neural-Network-Guided TEM Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2204.04250.pdf)
>  Trained neural networks are promising tools to analyze the ever-increasing amount of scientific image data, but it is unclear how to best customize these networks for the unique features in transmission electron micrographs. Here, we systematically examine how neural network architecture choices affect how neural networks segment, or pixel-wise separate, crystalline nanoparticles from amorphous background in transmission electron microscopy (TEM) images. We focus on decoupling the influence of receptive field, or the area of the input image that contributes to the output decision, from network complexity, which dictates the number of trainable parameters. We find that for low-resolution TEM images which rely on amplitude contrast to distinguish nanoparticles from background, the receptive field does not significantly influence segmentation performance. On the other hand, for high-resolution TEM images which rely on a combination of amplitude and phase contrast changes to identify nanoparticles, receptive field is a key parameter for increased performance, especially in images with minimal amplitude contrast. Our results provide insight and guidance as to how to adapt neural networks for applications with TEM datasets.      
### 98.Data-Free Quantization with Accurate Activation Clipping and Adaptive Batch Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2204.04215.pdf)
>  Data-free quantization is a task that compresses the neural network to low bit-width without access to original training data. Most existing data-free quantization methods cause severe performance degradation due to inaccurate activation clipping range and quantization error, especially for low bit-width. In this paper, we present a simple yet effective data-free quantization method with accurate activation clipping and adaptive batch normalization. Accurate activation clipping (AAC) improves the model accuracy by exploiting accurate activation information from the full-precision model. Adaptive batch normalization firstly proposes to address the quantization error from distribution changes by updating the batch normalization layer adaptively. Extensive experiments demonstrate that the proposed data-free quantization method can yield surprisingly performance, achieving 64.33% top-1 accuracy of ResNet18 on ImageNet dataset, with 3.7% absolute improvement outperforming the existing state-of-the-art methods.      
