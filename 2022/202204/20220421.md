# ArXiv eess --Thu, 21 Apr 2022
### 1.Parametric Models for DOA Trajectory Localization  [ :arrow_down: ](https://arxiv.org/pdf/2204.09647.pdf)
>  Directions of arrival (DOA) estimation or localization of sources is an important problem in many applications for which numerous algorithms have been proposed. Most localization methods use block-level processing that combines multiple data snapshots to estimate DOA within a block. The DOAs are assumed to be constant within the block duration. However, these assumptions are often violated due to source motion. In this paper, we propose a signal model that captures the linear variations in DOA within a block. We applied conventional beamforming (CBF) algorithm to this model to estimate linear DOA trajectories. Further, we formulate the proposed signal model as a block sparse model and subsequently derive sparse Bayesian learning (SBL) algorithm. Our simulation results show that this linear parametric DOA model and corresponding algorithms capture the DOA trajectories for moving sources more accurately than traditional signal models and methods.      
### 2.Extraction of Unaliased High-Frequency Micro-Doppler Signature using FMCW radar  [ :arrow_down: ](https://arxiv.org/pdf/2204.09621.pdf)
>  Micro-Doppler signature is a potent feature that has been used for target identification and micro-motion parameter estimation. The extraction of high frequency micro-Doppler signature from frequency modulated continuous wave (FMCW) radar along with the target range and velocity is the problem considered in this article. The severe aliasing of the high micro-Doppler frequency spread is circumvented by the fast time processing in the proposed method. The use of range-Doppler (RD) filtering and empirical mode decomposition (EMD) enables effective out-of-band and in-band noise suppression. Simulation studies and experimental results present the effectiveness of the proposed approach.      
### 3.On the Practical Design of Tube-Enhanced Multi-Stage Nonlinear Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.09607.pdf)
>  Tube-enhanced multi-stage nonlinear model predictive control is a robust control scheme that can handle a wide range of uncertainties with reduced conservatism and manageable computational complexity. In this paper, we elaborate on the flexibility of the approach from an application point of view. We discuss the path to making design decisions to implement the novel scheme systematically. We illustrate the critical steps in the design and implementation of the scheme for an industrial example.      
### 4.Federated Learning for Distributed Energy-Efficient Resource Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2204.09602.pdf)
>  In cellular networks, resource allocation is performed in a centralized way, which brings huge computation complexity to the base station (BS) and high transmission overhead. This paper investigates the distributed resource allocation scheme for cellular networks to maximize the energy efficiency of the system in the uplink transmission, while guaranteeing the quality of service (QoS) for cellular users. Particularly, to cope the fast varying channels in wireless communication environment, we propose a robust federated reinforcement learning (FRL_suc) framework to enable local users to perform distributed resource allocation in items of transmit power and channel assignment by the guidance of the local neural network trained at each user. Analysis and numerical results show that the proposed FRL_suc framework can lower the transmission overhead and offload the computation from the central server to the local users, while outperforming the conventional multi-agent reinforcement learning algorithm in terms of EE, and is more robust to channel variations.      
### 5.Risk-Averse Receding Horizon Motion Planning  [ :arrow_down: ](https://arxiv.org/pdf/2204.09596.pdf)
>  This paper studies the problem of risk-averse receding horizon motion planning for agents with uncertain dynamics, in the presence of stochastic, dynamic obstacles. We propose a model predictive control (MPC) scheme that formulates the obstacle avoidance constraint using coherent risk measures. To handle disturbances, or process noise, in the state dynamics, the state constraints are tightened in a risk-aware manner to provide a disturbance feedback policy. We also propose a waypoint following algorithm that uses the proposed MPC scheme for discrete distributions and prove its risk-sensitive recursive feasibility while guaranteeing finite-time task completion. We further investigate some commonly used coherent risk metrics, namely, conditional value-at-risk (CVaR), entropic value-at-risk (EVaR), and g-entropic risk measures, and propose a tractable incorporation within MPC. We illustrate our framework via simulation studies.      
### 6.Restructuring TCAD System: Teaching Traditional TCAD New Tricks  [ :arrow_down: ](https://arxiv.org/pdf/2204.09578.pdf)
>  Traditional TCAD simulation has succeeded in predicting and optimizing the device performance; however, it still faces a massive challenge - a high computational cost. There have been many attempts to replace TCAD with deep learning, but it has not yet been completely replaced. This paper presents a novel algorithm restructuring the traditional TCAD system. The proposed algorithm predicts three-dimensional (3-D) TCAD simulation in real-time while capturing a variance, enables deep learning and TCAD to complement each other, and fully resolves convergence errors.      
### 7.Energy-Efficient Tree-Based EEG Artifact Detection  [ :arrow_down: ](https://arxiv.org/pdf/2204.09577.pdf)
>  In the context of epilepsy monitoring, EEG artifacts are often mistaken for seizures due to their morphological similarity in both amplitude and frequency, making seizure detection systems susceptible to higher false alarm rates. In this work we present the implementation of an artifact detection algorithm based on a minimal number of EEG channels on a parallel ultra-low-power (PULP) embedded platform. The analyses are based on the TUH EEG Artifact Corpus dataset and focus on the temporal electrodes. First, we extract optimal feature models in the frequency domain using an automated machine learning framework, achieving a 93.95% accuracy, with a 0.838 F1 score for a 4 temporal EEG channel setup. The achieved accuracy levels surpass state-of-the-art by nearly 20%. Then, these algorithms are parallelized and optimized for a PULP platform, achieving a 5.21 times improvement of energy-efficient compared to state-of-the-art low-power implementations of artifact detection frameworks. Combining this model with a low-power seizure detection algorithm would allow for 300h of continuous monitoring on a 300 mAh battery in a wearable form factor and power budget. These results pave the way for implementing affordable, wearable, long-term epilepsy monitoring solutions with low false-positive rates and high sensitivity, meeting both patients' and caregivers' requirements.      
### 8.Fast and Robust Femur Segmentation from Computed Tomography Images for Patient-Specific Hip Fracture Risk Screening  [ :arrow_down: ](https://arxiv.org/pdf/2204.09575.pdf)
>  Osteoporosis is a common bone disease that increases the risk of bone fracture. Hip-fracture risk screening methods based on finite element analysis depend on segmented computed tomography (CT) images; however, current femur segmentation methods require manual delineations of large data sets. Here we propose a deep neural network for fully automated, accurate, and fast segmentation of the proximal femur from CT. Evaluation on a set of 1147 proximal femurs with ground truth segmentations demonstrates that our method is apt for hip-fracture risk screening, bringing us one step closer to a clinically viable option for screening at-risk patients for hip-fracture susceptibility.      
### 9.Fetal Brain Tissue Annotation and Segmentation Challenge Results  [ :arrow_down: ](https://arxiv.org/pdf/2204.09573.pdf)
>  In-utero fetal MRI is emerging as an important tool in the diagnosis and analysis of the developing human brain. Automatic segmentation of the developing fetal brain is a vital step in the quantitative analysis of prenatal neurodevelopment both in the research and clinical context. However, manual segmentation of cerebral structures is time-consuming and prone to error and inter-observer variability. Therefore, we organized the Fetal Tissue Annotation (FeTA) Challenge in 2021 in order to encourage the development of automatic segmentation algorithms on an international level. The challenge utilized FeTA Dataset, an open dataset of fetal brain MRI reconstructions segmented into seven different tissues (external cerebrospinal fluid, grey matter, white matter, ventricles, cerebellum, brainstem, deep grey matter). 20 international teams participated in this challenge, submitting a total of 21 algorithms for evaluation. In this paper, we provide a detailed analysis of the results from both a technical and clinical perspective. All participants relied on deep learning methods, mainly U-Nets, with some variability present in the network architecture, optimization, and image pre- and post-processing. The majority of teams used existing medical imaging deep learning frameworks. The main differences between the submissions were the fine tuning done during training, and the specific pre- and post-processing steps performed. The challenge results showed that almost all submissions performed similarly. Four of the top five teams used ensemble learning methods. However, one team's algorithm performed significantly superior to the other submissions, and consisted of an asymmetrical U-Net network architecture. This paper provides a first of its kind benchmark for future automatic multi-tissue segmentation algorithms for the developing human brain in utero.      
### 10.Informative Path Planning in Random Fields via Mixed Integer Programming  [ :arrow_down: ](https://arxiv.org/pdf/2204.09571.pdf)
>  We present a new mixed integer formulation for the discrete informative path planning problem in random fields. The objective is to compute a budget constrained path while collecting measurements whose linear estimate results in minimum error over a finite set of prediction locations. The problem is known to be NP-hard. However, we strive to compute optimal solutions by leveraging advances in mixed integer optimization. Our approach is based on expanding the search space so we optimize not only over the collected measurement subset, but also over the class of all linear estimators. This allows us to formulate a mixed integer quadratic program that is convex in the continuous variables. The formulations are general and are not restricted to any covariance structure of the field. In simulations, we demonstrate the effectiveness of our approach over previous branch and bound algorithms.      
### 11.Simulation of machine learning-based 6G systems in virtual worlds  [ :arrow_down: ](https://arxiv.org/pdf/2204.09518.pdf)
>  Digital representations of the real world are being used in many applications, such as augmented reality. 6G systems will not only support use cases that rely on virtual worlds but also benefit from their rich contextual information to improve performance and reduce communication overhead. This paper focuses on the simulation of 6G systems that rely on a 3D representation of the environment, as captured by cameras and other sensors. We present new strategies for obtaining paired MIMO channels and multimodal data. We also discuss trade-offs between speed and accuracy when generating channels via ray tracing. We finally provide beam selection simulation results to assess the proposed methodology.      
### 12.From Laser Speckle to Particle Size Distribution in drying powders: A Physics-Enhanced AutoCorrelation-based Estimator (PEACE)  [ :arrow_down: ](https://arxiv.org/pdf/2204.09516.pdf)
>  Extracting quantitative information about highly scattering surfaces from an imaging system is challenging because the phase of the scattered light undergoes multiple folds upon propagation, resulting in complex speckle patterns. One specific application is the drying of wet powders in the pharmaceutical industry, where quantifying the particle size distribution (PSD) is of particular interest. A non-invasive and real-time monitoring probe in the drying process is required, but there is no suitable candidate for this purpose. In this report, we develop a theoretical relationship from the PSD to the speckle image and describe a physics-enhanced autocorrelation-based estimator (PEACE) machine learning algorithm for speckle analysis to measure the PSD of a powder surface. This method solves both the forward and inverse problems together and enjoys increased interpretability, since the machine learning approximator is regularized by the physical law.      
### 13.A Reinforcement Learning-based Volt-VAR Control Dataset and Testing Environment  [ :arrow_down: ](https://arxiv.org/pdf/2204.09500.pdf)
>  To facilitate the development of reinforcement learning (RL) based power distribution system Volt-VAR control (VVC), this paper introduces a suite of open-source datasets for RL-based VVC algorithm research that is sample efficient, safe, and robust. The dataset consists of two components: 1. a Gym-like VVC testing environment for the IEEE-13, 123, and 8500-bus test feeders and 2. a historical operational dataset for each of the feeders. Potential users of the dataset and testing environment could first train an sample-efficient off-line (batch) RL algorithm on the historical dataset and then evaluate the performance of the trained RL agent on the testing environments. This dataset serves as a useful testbed to conduct RL-based VVC research mimicking the real-world operational challenges faced by electric utilities. Meanwhile, it allows researchers to conduct fair performance comparisons between different algorithms.      
### 14.Effective Goal-oriented 6G Communications: the Energy-aware Edge Inferencing Case  [ :arrow_down: ](https://arxiv.org/pdf/2204.09447.pdf)
>  Currently, the world experiences an unprecedentedly increasing generation of application data, from sensor measurements to video streams, thanks to the extreme connectivity capability provided by 5G networks. Going beyond 5G technology, such data aim to be ingested by Artificial Intelligence (AI) functions instantiated in the network to facilitate informed decisions, essential for the operation of applications, such as automated driving and factory automation. Nonetheless, while computing platforms hosting Machine Learning (ML) models are ever powerful, their energy footprint is a key impeding factor towards realizing a wireless network as a sustainable intelligent platform. Focusing on a beyond 5G wireless network, overlaid by a Multi-access Edge Computing (MEC) infrastructure with inferencing capabilities, our paper tackles the problem of energy-aware dependable inference by considering inference effectiveness as value of a goal that needs to be accomplished by paying the minimum price in energy consumption. Both MEC-assisted standalone and ensemble inference options are evaluated. It is shown that, for some system scenarios, goal effectiveness above 84% is achieved and sustained even by relaxing communication reliability requirements by one decimal digit, while enjoying a device radio energy consumption reduction of almost 23% at the same time. Also, ensemble inference is shown to improve system-wide energy efficiency and even achieve higher goal effectiveness, as compared to the standalone case for some system parameterizations.      
### 15.Learning Data-Driven PCHD Models for Control Engineering Applications  [ :arrow_down: ](https://arxiv.org/pdf/2204.09436.pdf)
>  The design of control engineering applications usually requires a model that accurately represents the dynamics of the real system. In addition to classical physical modeling, powerful data-driven approaches are increasingly used. However, the resulting models are not necessarily in a form that is advantageous for controller design. In the control engineering domain, it is highly beneficial if the system dynamics is given in PCHD form (Port-Controlled Hamiltonian Systems with Dissipation) because globally stable control laws can be easily realized while physical interpretability is guaranteed. In this work, we exploit the advantages of both strategies and present a new framework to obtain nonlinear high accurate system models in a data-driven way that are directly in PCHD form. We demonstrate the success of our method by model-based application on an academic example, as well as experimentally on a test bed.      
### 16.Two Low-complexity DOA Estimators for Massive/Ultra-massive MIMO Receive Array  [ :arrow_down: ](https://arxiv.org/pdf/2204.09411.pdf)
>  Eigen-decomposition-based direction finding methods of using large-scale/ultra-large-scale fully-digital receive antenna arrays leads to a high or ultra-high complexity. To address the complexity dilemma, in this paper, two low-complexity estimators are proposed: partitioned subarray combining (PSAC) and power iteration max correlation successive convex approximation (PI-Max-CSCA). Compared with the conventional no-partitioned direction finding method like root multiple signal classification (Root-MUSIC), in PSAC method, the total set of antennas are equally partitioned into subsets of antennas, called subarrays, each subarray performs independent DOA estimation, and finally all DOA estimates are coherently combined to give the final estimate. In PI-Max-CSCA method, using a fraction of all subarrays to make an initial coarse direction measurement (ICDM), the power iterative method is adopted to compute the more precise steering vector (SV) by exploiting the total array, and a more accurate DOA value is found using ICDM and SV through the maximum correlation method solved by successive convex approximation.      
### 17.Constructions of Polyphase Golay Complementary Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2204.09372.pdf)
>  Golay complementary matrices (GCM) have recently drawn considerable attentions owing to its potential applications in omnidirectional precoding. In this paper we generalize the GCM to multi-dimensional Golay complementary arrays (GCA) and propose new constructions of GCA pairs and GCA quads. These constructions are facilitated by introducing a set of identities over a commutative ring. We prove that a quaternary GCA pair is feasible if the product of the array sizes in all dimensions is a quaternary Golay number with an additional constraint on the factorization of the product. For the binary GCM quads, we conjecture that the feasible sizes are arbitrary, and verify for sizes within 78 $\times$ 78 and other less densely distributed sizes. For the quaternary GCM quads, all the positive integers within 1000 can be covered for the size in one dimension.      
### 18.Unsupervised Domain Adaptation for Cardiac Segmentation: Towards Structure Mutual Information Maximization  [ :arrow_down: ](https://arxiv.org/pdf/2204.09334.pdf)
>  Unsupervised domain adaptation approaches have recently succeeded in various medical image segmentation tasks. The reported works often tackle the domain shift problem by aligning the domain-invariant features and minimizing the domain-specific discrepancies. That strategy works well when the difference between a specific domain and between different domains is slight. However, the generalization ability of these models on diverse imaging modalities remains a significant challenge. This paper introduces UDA-VAE++, an unsupervised domain adaptation framework for cardiac segmentation with a compact loss function lower bound. To estimate this new lower bound, we develop a novel Structure Mutual Information Estimation (SMIE) block with a global estimator, a local estimator, and a prior information matching estimator to maximize the mutual information between the reconstruction and segmentation tasks. Specifically, we design a novel sequential reparameterization scheme that enables information flow and variance correction from the low-resolution latent space to the high-resolution latent space. Comprehensive experiments on benchmark cardiac segmentation datasets demonstrate that our model outperforms previous state-of-the-art qualitatively and quantitatively. The code is available at <a class="link-external link-https" href="https://github.com/LOUEY233/Toward-Mutual-Information" rel="external noopener nofollow">this https URL</a>}{<a class="link-external link-https" href="https://github.com/LOUEY233/Toward-Mutual-Information" rel="external noopener nofollow">this https URL</a>      
### 19.Congestion Mitigation in Unbalanced Residential Networks with OPF-based Demand Management  [ :arrow_down: ](https://arxiv.org/pdf/2204.09325.pdf)
>  This paper proposes a novel congestion mitigation strategy for low voltage residential feeders in which the rising power demand due to the electrification of the transport and heating systems leads to congestion problems. The strategy is based on requiring residential customers to limit their demand for a certain amount of time in exchange for economic benefits. <br>The main novelty of the method consists of combining a thorough representation of the network physics with advanced constraints that ensure the comfort of residential users, in a scalable manner that suits real systems. The mitigation strategy is presented from a DSO perspective, and takes the form of contracts between users and system operator. The focus on user comfort aims to make the contracts appealing, encouraging users to voluntarily enroll in the proposed mitigation scheme. <br>The presented solution is implemented as a mixed-integer multi-period optimal power flow problem which relies on a linearized three-phase power flow formulation. Calculations on 100 real-life distribution feeders are performed, to analyze the congestion-relieving potential of several possible system operator-user contracts. From a planning perspective, the results can help the system operator define contractual terms that make a specific congestion mitigation scheme effective and viable. From an operational perspective, the same calculations can be used to optimally schedule power reduction on a day-ahead basis.      
### 20.Bone marrow sparing for cervical cancer radiotherapy on multimodality medical images  [ :arrow_down: ](https://arxiv.org/pdf/2204.09278.pdf)
>  Cervical cancer threatens the health of women seriously. Radiotherapy is one of the main therapy methods but with high risk of acute hematologic toxicity. Delineating the bone marrow (BM) for sparing using computer tomography (CT) images to plan before radiotherapy can effectively avoid this risk. Comparing with magnetic resonance (MR) images, CT lacks the ability to express the activity of BM. Thus, in current clinical practice, medical practitioners manually delineate the BM on CT images by corresponding to MR images. However, the time?consuming delineating BM by hand cannot guarantee the accuracy due to the inconsistency of the CT-MR multimodal images. In this study, we propose a multimodal image oriented automatic registration method for pelvic BM sparing, which consists of three-dimensional bone point cloud reconstruction, a local spherical system iteration closest point registration for marking BM on CT images. Experiments on patient dataset reveal that our proposed method can enhance the multimodal image registration accuracy and efficiency for medical practitioners in sparing BM of cervical cancer radiotherapy. The method proposed in this contribution might also provide references for similar studies in other clinical application.      
### 21.Estimating probabilistic dynamic origin-destination demands using multi-day traffic data on computational graphs  [ :arrow_down: ](https://arxiv.org/pdf/2204.09229.pdf)
>  System-level decision making in transportation needs to understand day-to-day variation of network flows, which calls for accurate modeling and estimation of probabilistic dynamic travel demand on networks. Most existing studies estimate deterministic dynamic origin-destination (OD) demand, while the day-to-day variation of demand and flow is overlooked. Estimating probabilistic distributions of dynamic OD demand is challenging due to the complexity of the spatio-temporal networks and the computational intensity of the high-dimensional problems. With the availability of massive traffic data and the emergence of advanced computational methods, this paper develops a data-driven framework that solves the probabilistic dynamic origin-destination demand estimation (PDODE) problem using multi-day data. Different statistical distances (e.g., lp-norm, Wasserstein distance, KL divergence, Bhattacharyya distance) are used and compared to measure the gap between the estimated and the observed traffic conditions, and it is found that 2-Wasserstein distance achieves a balanced accuracy in estimating both mean and standard deviation. The proposed framework is cast into the computational graph and a reparametrization trick is developed to estimate the mean and standard deviation of the probabilistic dynamic OD demand simultaneously. We demonstrate the effectiveness and efficiency of the proposed PDODE framework on both small and real-world networks. In particular, it is demonstrated that the proposed PDODE framework can mitigate the overfitting issues by considering the demand variation. Overall, the developed PDODE framework provides a practical tool for public agencies to understand the sources of demand stochasticity, evaluate day-to-day variation of network flow, and make reliable decisions for intelligent transportation systems.      
### 22.Phase-Shift Design and Channel Modeling for Focused Beams in IRS-Assisted FSO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.09190.pdf)
>  Interest in free-space optics (FSO) is rapidly growing as a potential solution for the backhaul of next-generation mobile or low-orbit satellite communications. Various techniques have been suggested for employing an intelligent reflecting surface (IRS) in FSO systems, such as anomalous reflection, power amplification, and beam splitting. It is possible to deliver more power to the receiver (Rx) by collimating or focusing the reflected beam at the Rx lens. In this study, we propose a phase-shift design of an IRS for beam focusing. In addition, we propose a new pointing error model and an outage performance analysis applicable when the beam width is comparable to or less than the aperture size of the Rx. The analytical results are validated by Monte Carlo simulations. This study provides essential preliminary results for future researches that assume a focused beam in FSO systems.      
### 23.A Scalable Deep Learning Framework for Multi-rate CSI Feedback under Variable Antenna Ports  [ :arrow_down: ](https://arxiv.org/pdf/2204.09169.pdf)
>  Channel state information (CSI) at transmitter is crucial for massive MIMO downlink systems to achieve high spectrum and energy efficiency. Existing works have provided deep learning architectures for CSI feedback and recovery at the eNB/gNB by reducing user feedback overhead and improving recovery accuracy. However, existing DL architectures tend to be inflexible and non-scalable as models are often trained according to a preset number of antennas for a given compression ratio. In this work, we develop a flexible and scalable learning framework based on a divide-and-conquer approach (DCA). This new DCA architecture can flexibly accommodate different numbers of 3GPP antenna ports and dynamic levels of feedback compression. Importantly, it also significantly reduces computational complexity and memory size by allowing UEs to feedback segmented downlink CSI. We further propose a multi-rate successive convolution encoder with fewer than 1000 parameters. Test results demonstrate superior performance, good scalability, and low complexity for both indoor and outdoor channels.      
### 24.Conditional Value at Risk-Sensitive Solar Hosting Capacity Analysis in Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.09096.pdf)
>  Solar hosting capacity analysis (HCA) assesses the ability of a distribution network to host distributed solar generation without seriously violating distribution network constraints. In this paper, we consider risk-sensitive HCA that limits the risk of network constraint violations with a collection of scenarios of solar irradiance and nodal power demands, where risk is modeled via the conditional value at risk (CVaR) measure. First, we consider the question of maximizing aggregate installed solar capacities, subject to risk constraints and solve it as a second-order cone program (SOCP) with a standard conic relaxation of the feasible set with power flow equations. Second, we design an incremental algorithm to decide whether a configuration of solar installations has acceptable risk of constraint violations, modeled via CVaR. The algorithm circumvents explicit risk computation by incrementally constructing inner and outer polyhedral approximations of the set of acceptable solar installation configurations from prior such tests conducted. Our numerical examples study the impact of risk parameters, the number of scenarios and the scalability of our framework.      
### 25.Music Source Separation with Generative Flow  [ :arrow_down: ](https://arxiv.org/pdf/2204.09079.pdf)
>  Music source separation with both paired mixed signals and source signals has obtained substantial progress over the years. However, this setting highly relies on large amounts of paired data. Source-only supervision decouples the process of learning a mapping from a mixture to particular sources into a two stage paradigm: source modeling and separation. Recent systems under source-only supervision either achieve good performance in synthetic toy experiments or limited performance in music separation task. In this paper, we leverage flow-based implicit generators to train music source priors and likelihood based objective to separate music mixtures. Experiments show that in singing voice and music separation tasks, our proposed systems achieve competitive results to one of the full supervision systems. We also demonstrate one variant of our proposed systems is capable of separating new source tracks effortlessly.      
### 26.Near Optimal Per-Clip Lagrangian Multiplier Prediction in HEVC  [ :arrow_down: ](https://arxiv.org/pdf/2204.09056.pdf)
>  The majority of internet traffic is video content. This drives the demand for video compression to deliver high quality video at low target bitrates. Optimising the parameters of a video codec for a specific video clip (per-clip optimisation) has been shown to yield significant bitrate savings. In previous work we have shown that per-clip optimisation of the Lagrangian multiplier leads to up to 24% BD-Rate improvement. A key component of these algorithms is modeling the R-D characteristic across the appropriate bitrate range. This is computationally heavy as it usually involves repeated video encodes of the high resolution material at different parameter settings. This work focuses on reducing this computational load by deploying a NN operating on lower bandwidth features. Our system achieves BD-Rate improvement in approximately 90% of a large corpus with comparable results to previous work in direct optimisation.      
### 27.Per-clip and per-bitrate adaptation of the Lagrangian multiplier in video coding  [ :arrow_down: ](https://arxiv.org/pdf/2204.09055.pdf)
>  In the past ten years there have been significant developments in optimization of transcoding parameters on a per-clip rather than per-genre basis. In our recent work we have presented per-clip optimization for the Lagrangian multiplier in Rate controlled compression, which yielded BD-Rate improvements of approximately 2\% across a corpus of videos using HEVC. However, in a video streaming application, the focus is on optimizing the rate/distortion tradeoff at a particular bitrate and not on average across a range of performance. We observed in previous work that a particular multiplier might give BD rate improvements over a certain range of bitrates, but not the entire range. Using different parameters across the range would improve gains overall. Therefore here we present a framework for choosing the best Lagrangian multiplier on a per-operating point basis across a range of bitrates. In effect, we are trying to find the para-optimal gain across bitrate and distortion for a single clip. In the experiments presented we employ direct optimization techniques to estimate this Lagrangian parameter path approximately 2,000 video clips. The clips are primarily from the YouTube-UGC dataset. We optimize both for bitrate savings as well as distortion metrics (PSNR, SSIM).      
### 28.PR-DAD: Phase Retrieval Using Deep Auto-Decoders  [ :arrow_down: ](https://arxiv.org/pdf/2204.09051.pdf)
>  Phase retrieval is a well known ill-posed inverse problem where one tries to recover images given only the magnitude values of their Fourier transform as input. In recent years, new algorithms based on deep learning have been proposed, providing breakthrough results that surpass the results of the classical methods. In this work we provide a novel deep learning architecture PR-DAD (Phase Retrieval Using Deep Auto- Decoders), whose components are carefully designed based on mathematical modeling of the phase retrieval problem. The architecture provides experimental results that surpass all current results.      
### 29.The MIT Voice Name System  [ :arrow_down: ](https://arxiv.org/pdf/2204.09657.pdf)
>  This RFC white Paper summarizes our progress on the MIT Voice Name System (VNS) and Huey. The VNS, similar in name and function to the DNS, is a system to reserve and use "wake words" to activate Artificial Intelligence (AI) devices. Just like you can say "Hey Siri" to activate Apple's personal assistant, we propose using the VNS in smart speakers and other devices to route wake requests based on commands such as "turn off", "open grocery shopping list" or "271, start flash card review of my computer vision class". We also introduce Huey, an unambiguous Natural Language to interact with AI devices. We aim to standardize voice interactions to a universal reach similar to that of other systems such as phone numbering, with an agreed world-wide approach to assign and use numbers, or the Internet's DNS, with a standard naming system, that has helped flourish popular services including the World-Wide-Web, FTP, and email. Just like these standards are "neutral", we also aim to endow the VNS with "wake neutrality" so that each participant can develop its own digital voice. We focus on voice as a starting point to talk to any IoT object and explain briefly how the VNS may be expanded to other AI technologies enabling person-to-machine conversations (really machine-to-machine), including computer vision or neural interfaces. We also describe briefly considerations for a broader set of standards, MIT Open AI (MOA), including a reference architecture to serve as a starting point for the development of a general conversational commerce infrastructure that has standard "Wake Words", NLP commands such as "Shopping Lists" or "Flash Card Reviews", and personalities such as Pi or 271. Privacy and security are key elements considered because of speech-to-text errors and the amount of personal information contained in a voice sample.      
### 30.Clotho-AQA: A Crowdsourced Dataset for Audio Question Answering  [ :arrow_down: ](https://arxiv.org/pdf/2204.09634.pdf)
>  Audio question answering (AQA) is a multimodal translation task where a system analyzes an audio signal and a natural language question, to generate a desirable natural language answer. In this paper, we introduce Clotho-AQA, a dataset for Audio question answering consisting of 1991 audio files each between 15 to 30 seconds in duration selected from the Clotho dataset [1]. For each audio file, we collect six different questions and corresponding answers by crowdsourcing using Amazon Mechanical Turk. The questions and answers are produced by different annotators. Out of the six questions for each audio, two questions each are designed to have 'yes' and 'no' as answers, while the remaining two questions have other single-word answers. For each question, we collect answers from three different annotators. We also present two baseline experiments to describe the usage of our dataset for the AQA task - an LSTM-based multimodal binary classifier for 'yes' or 'no' type answers and an LSTM-based multimodal multi-class classifier for 828 single-word answers. The binary classifier achieved an accuracy of 62.7% and the multi-class classifier achieved a top-1 accuracy of 54.2% and a top-5 accuracy of 93.7%. Clotho-AQA dataset is freely available online at <a class="link-external link-https" href="https://zenodo.org/record/6473207" rel="external noopener nofollow">this https URL</a>.      
### 31.Dissipative stabilization of linear input delay systems via dynamical state feedback controllers: an optimization based approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.09615.pdf)
>  In this note, we present an effective solution to stabilize linear input delay systems subject to dissipative constraints while all the effect of input delay is compensated by a novel controller. The method is inspired by the recent development in the mathematical treatment of distributed delays and predictor controllers, which are critical for the derivation of the solution. An important conceptual innovation is the use of a parameterized dynamical state feedback controller, where the dimension of the controller equals the dimension of the control input. A sufficient condition for the existence of a dissipative dynamical state feedback controller is obtained via the Krasovskii functional approach, where the condition includes a bilinear matrix inequality (BMI). To solve the BMI, we apply an inner convex approximation algorithm which can be initialized based on an explicit construction of a predictor controller gain. The dynamical state controller in this paper can be considered as an extension of the classical predictor controller, thereby capable of compensating all the effects of the pointwise input delay while satisfying dissipative constraints. A numerical example is given to illustrate the effectiveness of our proposed methodology.      
### 32.Detecting Unintended Memorization in Language-Model-Fused ASR  [ :arrow_down: ](https://arxiv.org/pdf/2204.09606.pdf)
>  End-to-end (E2E) models are often being accompanied by language models (LMs) via shallow fusion for boosting their overall quality as well as recognition of rare words. At the same time, several prior works show that LMs are susceptible to unintentionally memorizing rare or unique sequences in the training data. In this work, we design a framework for detecting memorization of random textual sequences (which we call canaries) in the LM training data when one has only black-box (query) access to LM-fused speech recognizer, as opposed to direct access to the LM. On a production-grade Conformer RNN-T E2E model fused with a Transformer LM, we show that detecting memorization of singly-occurring canaries from the LM training data of 300M examples is possible. Motivated to protect privacy, we also show that such memorization gets significantly reduced by per-example gradient-clipped LM training without compromising overall quality.      
### 33.Radiology Text Analysis System (RadText): Architecture and Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2204.09599.pdf)
>  Analyzing radiology reports is a time-consuming and error-prone task, which raises the need for an efficient automated radiology report analysis system to alleviate the workloads of radiologists and encourage precise diagnosis. In this work, we present RadText, an open-source radiology text analysis system developed by Python. RadText offers an easy-to-use text analysis pipeline, including de-identification, section segmentation, sentence split and word tokenization, named entity recognition, parsing, and negation detection. RadText features a flexible modular design, provides a hybrid text processing schema, and supports raw text processing and local processing, which enables better usability and improved data privacy. RadText adopts BioC as the unified interface, and also standardizes the input / output into a structured representation compatible with Observational Medical Outcomes Partnership (OMOP) Common Data Model (CDM). This allows for a more systematic approach to observational research across multiple, disparate data sources. We evaluated RadText on the MIMIC-CXR dataset, with five new disease labels we annotated for this work. RadText demonstrates highly accurate classification performances, with an average precision of, a recall of 0.94, and an F-1 score of 0.92. We have made our code, documentation, examples, and the test set available at <a class="link-external link-https" href="https://github.com/bionlplab/radtext" rel="external noopener nofollow">this https URL</a> .      
### 34.Exploring Continuous Integrate-and-Fire for Adaptive Simultaneous Speech Translation  [ :arrow_down: ](https://arxiv.org/pdf/2204.09595.pdf)
>  Simultaneous speech translation (SimulST) is a challenging task aiming to translate streaming speech before the complete input is observed. A SimulST system generally includes two components: the pre-decision that aggregates the speech information and the policy that decides to read or write. While recent works had proposed various strategies to improve the pre-decision, they mainly adopt the fixed wait-k policy, leaving the adaptive policies rarely explored. This paper proposes to model the adaptive policy by adapting the Continuous Integrate-and-Fire (CIF). Compared with monotonic multihead attention (MMA), our method has the advantage of simpler computation, superior quality at low latency, and better generalization to long utterances. We conduct experiments on the MuST-C V2 dataset and show the effectiveness of our approach.      
### 35.Cross-view Brain Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2204.09564.pdf)
>  How the brain captures the meaning of linguistic stimuli across multiple views is still a critical open question in neuroscience. Consider three different views of the concept apartment: (1) picture (WP) presented with the target word label, (2) sentence (S) using the target word, and (3) word cloud (WC) containing the target word along with other semantically related words. Unlike previous efforts, which focus only on single view analysis, in this paper, we study the effectiveness of brain decoding in a zero-shot cross-view learning setup. Further, we propose brain decoding in the novel context of cross-view-translation tasks like image captioning (IC), image tagging (IT), keyword extraction (KE), and sentence formation (SF). Using extensive experiments, we demonstrate that cross-view zero-shot brain decoding is practical leading to ~0.68 average pairwise accuracy across view pairs. Also, the decoded representations are sufficiently detailed to enable high accuracy for cross-view-translation tasks with following pairwise accuracy: IC (78.0), IT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different brain networks reveals exciting cognitive insights: (1) A high percentage of visual voxels are involved in image captioning and image tagging tasks, and a high percentage of language voxels are involved in the sentence formation and keyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view and tested on WC view is better than same-view accuracy of the model trained and tested on WC view.      
### 36.Modeling and Executing Production Processes with Capabilities and Skills using Ontologies and BPMN  [ :arrow_down: ](https://arxiv.org/pdf/2204.09472.pdf)
>  Current challenges of the manufacturing industry require modular and changeable manufacturing systems that can be adapted to variable conditions with little effort. At the same time, production recipes typically represent important company know-how that should not be directly tied to changing plant configurations. Thus, there is a need to model general production recipes independent of specific plant layouts. For execution of such a recipe however, a binding to then available production resources needs to be made. In this contribution, select a suitable modeling language to model and execute such recipes. Furthermore, we present an approach to solve the issue of recipe modeling and execution in modular plants using semantically modeled capabilities and skills as well as BPMN. We make use of BPMN to model \emph{capability processes}, i.e. production processes referencing abstract descriptions of resource functions. These capability processes are not bound to a certain plant layout, as there can be multiple resources fulfilling the same capability. For execution, every capability in a capability process is replaced by a skill realizing it, effectively creating a \emph{skill process} consisting of various skill invocations. The presented solution is capable of orchestrating and executing complex processes that integrate production steps with typical IT functionalities such as error handling, user interactions and notifications. Benefits of the approach are demonstrated using a flexible manufacturing system.      
### 37.DAM-GAN : Image Inpainting using Dynamic Attention Map based on Fake Texture Detection  [ :arrow_down: ](https://arxiv.org/pdf/2204.09442.pdf)
>  Deep neural advancements have recently brought remarkable image synthesis performance to the field of image inpainting. The adaptation of generative adversarial networks (GAN) in particular has accelerated significant progress in high-quality image reconstruction. However, although many notable GAN-based networks have been proposed for image inpainting, still pixel artifacts or color inconsistency occur in synthesized images during the generation process, which are usually called fake textures. To reduce pixel inconsistency disorder resulted from fake textures, we introduce a GAN-based model using dynamic attention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake texture and products dynamic attention maps to diminish pixel inconsistency from the feature maps in the generator. Evaluation results on CelebA-HQ and Places2 datasets with other image inpainting approaches show the superiority of our network.      
### 38.Deep subspace encoders for continuous-time state-space identification  [ :arrow_down: ](https://arxiv.org/pdf/2204.09405.pdf)
>  Continuous-time (CT) models have shown an improved sample efficiency during learning and enable ODE analysis methods for enhanced interpretability compared to discrete-time (DT) models. Even with numerous recent developments, the multifaceted CT state-space model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, and latent states. This paper presents a novel estimation method that includes these aspects and that is able to obtain state-of-the-art results on multiple benchmarks where a small fully connected neural network describes the CT dynamics. The novel estimation method called the subspace encoder approach ascertains these results by altering the well-known simulation loss to include short subsections instead, by using an encoder function and a state-derivative normalization term to obtain a computationally feasible and stable optimization problem. This encoder function estimates the initial states of each considered subsection. We prove that the existence of the encoder function has the necessary condition of a Lipschitz continuous state-derivative utilizing established properties of ODEs.      
### 39.Attentive Dual Stream Siamese U-net for Flood Detection on Multi-temporal Sentinel-1 Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.09387.pdf)
>  Due to climate and land-use change, natural disasters such as flooding have been increasing in recent years. Timely and reliable flood detection and mapping can help emergency response and disaster management. In this work, we propose a flood detection network using bi-temporal SAR acquisitions. The proposed segmentation network has an encoder-decoder architecture with two Siamese encoders for pre and post-flood images. The network's feature maps are fused and enhanced using attention blocks to achieve more accurate detection of the flooded areas. Our proposed network is evaluated on publicly available Sen1Flood11 benchmark dataset. The network outperformed the existing state-of-the-art (uni-temporal) flood detection method by 6\% IOU. The experiments highlight that the combination of bi-temporal SAR data with an effective network architecture achieves more accurate flood detection than uni-temporal methods.      
### 40.Safety Verification and Controller Synthesis for Systems with Input Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2204.09386.pdf)
>  In this paper we consider the safety verification and safe controller synthesis problems for nonlinear control systems. The Control Barrier Certificates (CBC) approach is proposed as an extension to the Barrier certificates approach. Our approach can be used to characterize the control invariance of a given set in terms of safety of a general nonlinear control system subject to input constraints. From the point of view of controller design, the proposed method provides an approach to synthesize a safe control law that guarantees that the trajectories of the system starting from a given initial set do not enter an unsafe set. Unlike the related control Barrier functions approach, our formulation only considers the vector field within the tangent cone of the zero level set defined by the certificates, and is shown to be less conservative by means of numerical evidence. For polynomial systems with semi-algebraic initial and safe sets, CBCs and safe control laws can be synthesized using sum-of-squares decomposition and semi-definite programming. Examples demonstrate our method.      
### 41.Exploration strategies for articulatory synthesis of complex syllable onsets  [ :arrow_down: ](https://arxiv.org/pdf/2204.09381.pdf)
>  High-quality articulatory speech synthesis has many potential applications in speech science and technology. However, developing appropriate mappings from linguistic specification to articulatory gestures is difficult and time consuming. In this paper we construct an optimisation-based framework as a first step towards learning these mappings without manual intervention. We demonstrate the production of syllables with complex onsets and discuss the quality of the articulatory gestures with reference to coarticulation.      
### 42.Explicit Solutions for Safety Problems Using Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2204.09380.pdf)
>  The control Barrier function approach has been widely used for safe controller synthesis. By solving an online convex quadratic programming problem, an optimal safe controller can be synthesized implicitly in state-space. Since the solution is unique, the mapping from state-space to control inputs is injective, thus enabling us to evaluate the underlying relationship. In this paper we aim at explicitly synthesizing a safe control law as a function of the state for nonlinear control-affine systems with limited control ability. We propose to transform the online quadratic programming problem into an offline parameterized optimisation problem which considers states as parameters. The obtained explicit safe controller is shown to be a piece-wise Lipschitz continuous function over the partitioned state space if the program is feasible. We address the infeasible cases by solving a parameterized adaptive control Barrier function-based quadratic programming problem. Extensive simulation results show the state-space partition and the controller properties.      
### 43.Placement and Resource Allocation of Wireless-Powered Multiantenna UAV for Energy-Efficient Multiuser NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2204.09350.pdf)
>  This paper investigates a new downlink nonorthogonal multiple access (NOMA) system, where a multiantenna unmanned aerial vehicle (UAV) is powered by wireless power transfer (WPT) and serves as the base station for multiple pairs of ground users (GUs) running NOMA in each pair. An energy efficiency (EE) maximization problem is formulated to jointly optimize the WPT time and the placement for the UAV, and the allocation of the UAV's transmit power between different NOMA user pairs and within each pair. To efficiently solve this nonconvex problem, we decompose the problem into three subproblems using block coordinate descent. For the subproblem of intra-pair power allocation within each NOMA user pair, we construct a supermodular game with confirmed convergence to a Nash equilibrium. Given the intra-pair power allocation, successive convex approximation is applied to convexify and solve the subproblem of WPT time allocation and inter-pair power allocation between the user pairs. Finally, we solve the subproblem of UAV placement by using the Lagrange multiplier method. Simulations show that our approach can substantially outperform its alternatives that do not use NOMA and WPT techniques or that do not optimize the UAV location.      
### 44.Logarithmic Morphological Neural Nets robust to lighting variations  [ :arrow_down: ](https://arxiv.org/pdf/2204.09319.pdf)
>  Morphological neural networks allow to learn the weights of a structuring function knowing the desired output image. However, those networks are not intrinsically robust to lighting variations in images with an optical cause, such as a change of light intensity. In this paper, we introduce a morphological neural network which possesses such a robustness to lighting variations. It is based on the recent framework of Logarithmic Mathematical Morphology (LMM), i.e. Mathematical Morphology defined with the Logarithmic Image Processing (LIP) model. This model has a LIP additive law which simulates in images a variation of the light intensity. We especially learn the structuring function of a LMM operator robust to those variations, namely : the map of LIP-additive Asplund distances. Results in images show that our neural network verifies the required property.      
### 45.Distributed Coverage Control of Multi-Agent Systems in Uncertain Environments using Heat Transfer Equations  [ :arrow_down: ](https://arxiv.org/pdf/2204.09289.pdf)
>  This paper addresses the coverage control problem of multi-agent systems in the uncertain environment. With the aid of Voronoi partition, a distributed coverage control formulation of multi-agent system is proposed to complete the workload in uncertain environments. Driven by the gradient of thermal field, each agent is able to move around for clearing the workload on its own subregion. Theoretical analysis is conducted to ensure the completion of workload in finite time. Finally, numerical simulations are carried out to demonstrate the effectiveness and advantages of the proposed coverage control approach as compared to other existing approaches.      
### 46.Cross-stitched Multi-modal Encoders  [ :arrow_down: ](https://arxiv.org/pdf/2204.09227.pdf)
>  In this paper, we propose a novel architecture for multi-modal speech and text input. We combine pretrained speech and text encoders using multi-headed cross-modal attention and jointly fine-tune on the target problem. The resultant architecture can be used for continuous token-level classification or utterance-level prediction acting on simultaneous text and speech. The resultant encoder efficiently captures both acoustic-prosodic and lexical information. We compare the benefits of multi-headed attention-based fusion for multi-modal utterance-level classification against a simple concatenation of pre-pooled, modality-specific representations. Our model architecture is compact, resource efficient, and can be trained on a single consumer GPU card.      
### 47.Improving Self-Supervised Speech Representations by Disentangling Speakers  [ :arrow_down: ](https://arxiv.org/pdf/2204.09224.pdf)
>  Self-supervised learning in speech involves training a speech representation network on a large-scale unannotated speech corpus, and then applying the learned representations to downstream tasks. Since the majority of the downstream tasks of SSL learning in speech largely focus on the content information in speech, the most desirable speech representations should be able to disentangle unwanted variations, such as speaker variations, from the content. However, disentangling speakers is very challenging, because removing the speaker information could easily result in a loss of content as well, and the damage of the latter usually far outweighs the benefit of the former. In this paper, we propose a new SSL method that can achieve speaker disentanglement without severe loss of content. Our approach is adapted from the HuBERT framework, and incorporates disentangling mechanisms to regularize both the teacher labels and the learned representations. We evaluate the benefit of speaker disentanglement on a set of content-related downstream tasks, and observe a consistent and notable performance advantage of our speaker-disentangled representations.      
### 48.Efficient Progressive High Dynamic Range Image Restoration via Attention and Alignment Network  [ :arrow_down: ](https://arxiv.org/pdf/2204.09213.pdf)
>  HDR is an important part of computational photography technology. In this paper, we propose a lightweight neural network called Efficient Attention-and-alignment-guided Progressive Network (EAPNet) for the challenge NTIRE 2022 HDR Track 1 and Track 2. We introduce a multi-dimensional lightweight encoding module to extract features. Besides, we propose Progressive Dilated U-shape Block (PDUB) that can be a progressive plug-and-play module for dynamically tuning MAccs and PSNR. Finally, we use fast and low-power feature-align module to deal with misalignment problem in place of the time-consuming Deformable Convolutional Network (DCN). The experiments show that our method achieves about 20 times compression on MAccs with better mu-PSNR and PSNR compared to the state-of-the-art method. We got the second place of both two tracks during the testing phase. Figure1. shows the visualized result of NTIRE 2022 HDR challenge.      
### 49.Primary accelerometer calibration with two-axis automatic positioning stage  [ :arrow_down: ](https://arxiv.org/pdf/2204.09212.pdf)
>  In this study, we developed an automated, multipoint primary accelerometer calibration system using a two-axis positioning stage and a heterodyne laser interferometer. The proposed system offers low-cost, convenient, and automated multipoint accelerometer calibration, enabling less calibration lead time. The positioning stage also offers better positioning repeatability of 1 um, which is impossible through manual alignments. We measured the surface deformation of a laser reflection adaptor for a single-ended accelerometer by measuring more than 450 measurement positions. Visualizing the deformation of laser reflection surfaces facilitates understanding the effects of deformation or nonrectilinear motion, which are among the most significant uncertainty components in high-frequency accelerometer calibrations.      
### 50.Lie Algebraic Cost Function Design for Control on Lie Groups  [ :arrow_down: ](https://arxiv.org/pdf/2204.09177.pdf)
>  This paper presents a control framework on Lie groups by designing the control objective in its Lie algebra. Control on Lie groups is challenging due to its nonlinear nature and difficulties in system parameterization. Existing methods to design the control objective on a Lie group and then derive the gradient for controller design are non-trivial and can result in slow convergence in tracking control. We show that with a proper left-invariant metric, setting the gradient of the cost function as the tracking error in the Lie algebra leads to a quadratic Lyapunov function that enables globally exponential convergence. In the PD control case, we show that our controller can maintain an exponential convergence rate even when the initial error is approaching $\pi$ in SO(3). We also show the merit of this proposed framework in trajectory optimization. The proposed cost function enables the iterative Linear Quadratic Regulator (iLQR) to converge much faster than the Differential Dynamic Programming (DDP) with a well-adopted cost function when the initial trajectory is poorly initialized on SO(3).      
