# ArXiv eess --Wed, 27 Apr 2022
### 1.neuro2vec: Masked Fourier Spectrum Prediction for Neurophysiological Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.12440.pdf)
>  Extensive data labeling on neurophysiological signals is often prohibitively expensive or impractical, as it may require particular infrastructure or domain expertise. To address the appetite for data of deep learning methods, we present for the first time a Fourier-based modeling framework for self-supervised pre-training of neurophysiology signals. The intuition behind our approach is simple: frequency and phase distribution of neurophysiology signals reveal the underlying neurophysiological activities of the brain and muscle. Our approach first randomly masks out a portion of the input signal and then predicts the missing information from either spatiotemporal or the Fourier domain. Pre-trained models can be potentially used for downstream tasks such as sleep stage classification using electroencephalogram (EEG) signals and gesture recognition using electromyography (EMG) signals. Unlike contrastive-based methods, which strongly rely on carefully hand-crafted augmentations and siamese structure, our approach works reasonably well with a simple transformer encoder with no augmentation requirements. By evaluating our method on several benchmark datasets, including both EEG and EMG, we show that our modeling approach improves downstream neurophysiological related tasks by a large margin.      
### 2.Interpretable Battery Cycle Life Range Prediction Using Early Degradation Data at Cell Level  [ :arrow_down: ](https://arxiv.org/pdf/2204.12420.pdf)
>  Battery cycle life prediction using early degradation data has many potential applications throughout the battery product life cycle. Various data-driven methods have been proposed for point prediction of battery cycle life with minimum knowledge of the battery degradation mechanisms. However, management of batteries at end-of-life with lower economic and technical risk requires prediction of cycle life with quantified uncertainty, which is still lacking. The interpretability (i.e., the reason for high prediction accuracy) of these advanced data-driven methods is also worthy of investigation. Here, a physics-informed Quantile Regression Forest (QRF) model is introduced to make cycle life range prediction with uncertainty quantified as the length of the prediction interval, in addition to point predictions with high accuracy. The hyperparameters of the QRF model are tuned with a proposed area-based performance evaluation metric so that the coverage probabilities associated with the prediction intervals are calibrated. The interpretability of the final QRF model is explored with two global model-agnostic methods, namely permutation importance, and partial dependence plot. The final QRF model facilitates dual-criteria decision-making to select the high-cycle-life charging protocol with consideration of both point predictions and uncertainty associated with the prediction.      
### 3.UAS Imagery and Computer Vision for Site-Specific Weed Control in Corn  [ :arrow_down: ](https://arxiv.org/pdf/2204.12417.pdf)
>  Currently, weed control in a corn field is performed by a blanket application of herbicides which do not consider spatial distribution information of weeds and also uses an extensive amount of chemical herbicides. In order to reduce the amount of chemicals, we used drone based high-resolution imagery and computer-vision techniwue to perform site-specific weed control in corn.      
### 4.Automatic Monitoring of Fruit Ripening Rooms by UHF RFID Sensor Network and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.12415.pdf)
>  Accelerated ripening through the exposure of fruits to controlled environmental conditions and gases is nowadays one of the most assessed food technologies, especially for climacteric and exotic products. However, a fine granularity control of the process and consequently of the quality of the goods is still missing, so the management of the ripening rooms is mainly based on qualitative estimations only. Following the modern paradigms of Industry 4.0, this contribution proposes a non-destructive RFID-based system for the automatic evaluation of the live ripening of avocados. The system, coupled with a properly trained automatic classification algorithm based on Support Vector Machines (SVMs), can discriminate the stage of ripening with an accuracy greater than 85%.      
### 5.REDCHO: Robust Exact Dynamic Consensus of High Order  [ :arrow_down: ](https://arxiv.org/pdf/2204.12344.pdf)
>  This article addresses the problem of average consensus in a multi-agent system when the desired consensus quantity is a time varying signal. Recently, the EDCHO protocol leveraged high order sliding modes to achieve exact consensus under a constrained set of initial conditions, limiting its applicability to static networks. In this work, we propose REDCHO, an extension of the previous protocol which is robust to mismatch in the initial conditions, making it suitable to use cases in which connection and disconnection of agents is possible. The convergence properties of the protocol are formally explored. Finally, the effectiveness and advantages of our proposal are shown with concrete simulation examples showing the benefits of REDCHO against other methods in the literature.      
### 6.An Algorithm for the Labeling and Interactive Visualization of the Cerebrovascular System of Ischemic Strokes  [ :arrow_down: ](https://arxiv.org/pdf/2204.12333.pdf)
>  During the diagnosis of ischemic strokes, the Circle of Willis and its surrounding vessels are the arteries of interest. Their visualization in case of an acute stroke is often enabled by Computed Tomography Angiography (CTA). Still, the identification and analysis of the cerebral arteries remain time consuming in such scans due to a large number of peripheral vessels which may disturb the visual impression. In previous work we proposed VirtualDSA++, an algorithm designed to segment and label the cerebrovascular tree on CTA scans. Especially with stroke patients, labeling is a delicate procedure, as in the worst case whole hemispheres may not be present due to impeded perfusion. Hence, we extended the labeling mechanism for the cerebral arteries to identify occluded vessels. In the work at hand, we place the algorithm in a clinical context by evaluating the labeling and occlusion detection on stroke patients, where we have achieved labeling sensitivities comparable to other works between 92\,\% and 95\,\%. To the best of our knowledge, ours is the first work to address labeling and occlusion detection at once, whereby a sensitivity of 67\,\% and a specificity of 81\,\% were obtained for the latter. VirtualDSA++ also automatically segments and models the intracranial system, which we further used in a deep learning driven follow up work. We present the generic concept of iterative systematic search for pathways on all nodes of said model, which enables new interactive features. Exemplary, we derive in detail, firstly, the interactive planning of vascular interventions like the mechanical thrombectomy and secondly, the interactive suppression of vessel structures that are not of interest in diagnosing strokes (like veins). We discuss both features as well as further possibilities emerging from the proposed concept.      
### 7.Supervised Attention in Sequence-to-Sequence Models for Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.12308.pdf)
>  Attention mechanism in sequence-to-sequence models is designed to model the alignments between acoustic features and output tokens in speech recognition. However, attention weights produced by models trained end to end do not always correspond well with actual alignments, and several studies have further argued that attention weights might not even correspond well with the relevance attribution of frames. Regardless, visual similarity between attention weights and alignments is widely used during training as an indicator of the models quality. In this paper, we treat the correspondence between attention weights and alignments as a learning problem by imposing a supervised attention loss. Experiments have shown significant improved performance, suggesting that learning the alignments well during training critically determines the performance of sequence-to-sequence models.      
### 8.Linear TDOA-based Measurements for Distributed Estimation and Localized Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2204.12298.pdf)
>  We propose a linear time-difference-of-arrival (TDOA) measurement model to improve \textit{distributed} estimation performance for localized target tracking. We design distributed filters over sparse (possibly large-scale) communication networks using consensus-based data-fusion techniques. The proposed distributed and localized tracking protocols considerably reduce the sensor network's required connectivity and communication rate. We, further, consider $\kappa$-redundant observability and fault-tolerant design in case of losing communication links or sensor nodes. We present the minimal conditions on the remaining sensor network (after link/node removal) such that the distributed observability is still preserved and, thus, the sensor network can track the (single) maneuvering target. The motivation is to reduce the communication load versus the processing load, as the computational units are, in general, less costly than the communication devices. We evaluate the tracking performance via simulations in MATLAB.      
### 9.Low-dimensional representation of infant and adult vocalization acoustics  [ :arrow_down: ](https://arxiv.org/pdf/2204.12279.pdf)
>  During the first years of life, infant vocalizations change considerably, as infants develop the vocalization skills that enable them to produce speech sounds. Characterizations based on specific acoustic features, protophone categories, or phonetic transcription are able to provide a representation of the sounds infants make at different ages and in different contexts but do not fully describe how sounds are perceived by listeners, can be inefficient to obtain at large scales, and are difficult to visualize in two dimensions without additional statistical processing. Machine-learning-based approaches provide the opportunity to complement these characterizations with purely data-driven representations of infant sounds. Here, we use spectral features extraction and unsupervised machine learning, specifically Uniform Manifold Approximation (UMAP), to obtain a novel 2-dimensional spatial representation of infant and caregiver vocalizations extracted from day-long home recordings. UMAP yields a continuous and well-distributed space conducive to certain analyses of infant vocal development. For instance, we found that the dispersion of infant vocalization acoustics within the 2-D space over a day increased from 3 to 9 months, and then decreased from 9 to 18 months. The method also permits analysis of similarity between infant and adult vocalizations, which also shows changes with infant age.      
### 10.Masked Spectrogram Modeling using Masked Autoencoders for Learning General-purpose Audio Representation  [ :arrow_down: ](https://arxiv.org/pdf/2204.12260.pdf)
>  Recent general-purpose audio representations show state-of-the-art performance on various audio tasks. These representations are pre-trained by self-supervised learning methods that create training signals from the input. For example, typical audio contrastive learning uses temporal relationships among input sounds to create training signals, whereas some methods use a difference among input views created by data augmentations. However, these training signals do not provide information derived from the intact input sound, which we think is suboptimal for learning representation that describes the input as it is. <br>In this paper, we seek to learn audio representations from the input itself as supervision using a pretext task of auto-encoding of masked spectrogram patches, Masked Spectrogram Modeling (MSM, a variant of Masked Image Modeling applied to audio spectrogram). To implement MSM, we use Masked Autoencoders (MAE), an image self-supervised learning method. MAE learns to efficiently encode the small number of visible patches into latent representations to carry essential information for reconstructing a large number of masked patches. While training, MAE minimizes the reconstruction error, which uses the input as training signal, consequently achieving our goal. <br>We conducted experiments on our MSM using MAE (MSM-MAE) models under the evaluation benchmark of the HEAR 2021 NeurIPS Challenge. Our MSM-MAE models outperformed the HEAR 2021 Challenge results on seven out of 15 tasks (e.g., accuracies of 73.4% on CREMA-D and 85.8% on LibriCount), while showing top performance on other tasks where specialized models perform better. We also investigate how the design choices of MSM-MAE impact the performance and conduct qualitative analysis of visualization outcomes to gain an understanding of learned representations. We make our code available online.      
### 11.Modeling and Analysis of 2-Tier Heterogeneous Vehicular Networks Leveraging Roadside Units and Vehicle Relays  [ :arrow_down: ](https://arxiv.org/pdf/2204.12243.pdf)
>  While roadside units (RSUs) play an essential role in vehicle-to-everything (V2X) by communicating with users, some users in congestion areas may not be well-served due to data traffic, signal attenuation, and interference. In these cases, vehicle relays can be employed to enhance the network topology to better serve those users. This paper leverages stochastic geometry to propose a novel framework for the performance analysis of heterogeneous vehicular networks with RSUs, vehicle relays, and vehicle users. We present a two-dimensional analytical model where the spatial dependence between RSUs, vehicle relays, vehicle users, and roads is accurately taken into account through a Cox point process structure. Assuming relays are backhauled to RSUs over a reserved wireless resource and users are associated with the closest RSU or relay, we derive the probability that the typical user is associated with either an RSU or a relay. Then, we derive the signal-to-interference ratio (SIR) coverage probability of the typical user. Finally, using the derived formulas, we evaluate the average effective rate of the typical user in the network. This allows us to determine the gain of the average effective rate of users that results from the deployment of relays in the network.      
### 12.A note on load balancing in DC microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2204.12219.pdf)
>  A problem of load balancing in isolated DC microgrids is considered in this paper. Here, a DC load is fed by multiple heterogenous DC sources, each of which is connected to the load via a boost converter. The gains of the DCC's provide for a means to control the division of load current amongst the DC sources. The primary objective of the control scheme is to minimise the total losses in the network, while maintaining the output voltage within a desired range, serving the load current demand and adhering to VI-characteristics of the power sources. Under assumptions of concavity/monotonocity/piece-wise-linearity of the VI-characteristics, the problem is solved using a convex relaxation. It is shown that the solution to the relaxed problem is tight. Thus, the resulting algorithm is guaranteed to reach global optimality in a numerically efficient manner. Simulations are provided for corroboration.      
### 13.Design and Experimental Evaluation of a Bluetooth 5.1 Antenna Array for Angle-of-Arrival Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2204.12188.pdf)
>  On the of the applications in the realm of the Internet-of-Things (IoT) is real-time localization of assets in specific application environments where satellite based global positioning is unviable. Numerous approaches for localization relying on wireless sensor mesh systems have been evaluated, but the recent Bluetooth Low Energy (BLE) 5.1 direction finding features based on Angle-of-Arrival (AoA) promise a low-cost solution for this application. In this paper, we present an implementation of a BLE 5.1 based circular antenna array, and perform two experimental evaluations over the quality of the retrieved data sampled from the array: in an anechoic chamber, and in a real-world environment using a setup composed of four BLE beacons.      
### 14.Razumikhin and Krasovskii Approaches for Safe Stabilization  [ :arrow_down: ](https://arxiv.org/pdf/2204.12106.pdf)
>  This paper studies the stabilization and safety problems of nonlinear time-delay systems. Following both Razumikhin and Krasovskii approaches, we propose novel control Lyapunov functions/functionals for the stabilization problem and novel control barrier functions/functionals for the safety problem. The proposed control Lyapunov and barrier functions/functionals extend the existing ones from the delay-free case to the time-delay case, and allow for designing the stabilizing and safety controllers in closed-form. Since analytical solutions to time-delay optimal control problems are hard to be achieved, a sliding mode control based approach is developed to merge the proposed control Lyapunov and barrier functions/functionals. Based on the sliding surface functional, a feedback control law is established to investigate the stabilization and safety objectives simultaneously. In particular, the properties of the sliding surface functional are analyzed, and further how to construct the sliding surface functional is discussed. Finally, the proposed approaches are illustrated via two numerical examples from the connected cruise control problem of automotive systems and the synchronization problem of multi-agent systems.      
### 15.Mask scalar prediction for improving robust automatic speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.12092.pdf)
>  Using neural network based acoustic frontends for improving robustness of streaming automatic speech recognition (ASR) systems is challenging because of the causality constraints and the resulting distortion that the frontend processing introduces in speech. Time-frequency masking based approaches have been shown to work well, but they need additional hyper-parameters to scale the mask to limit speech distortion. Such mask scalars are typically hand-tuned and chosen conservatively. In this work, we present a technique to predict mask scalars using an ASR-based loss in an end-to-end fashion, with minimal increase in the overall model size and complexity. We evaluate the approach on two robust ASR tasks: multichannel enhancement in the presence of speech and non-speech noise, and acoustic echo cancellation (AEC). Results show that the presented algorithm consistently improves word error rate (WER) without the need for any additional tuning over strong baselines that use hand-tuned hyper-parameters: up to 16% for multichannel enhancement in noisy conditions, and up to 7% for AEC.      
### 16.Gridless Tomographic SAR Imaging Based on Accelerated Atomic Norm Minimization with Efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2204.12091.pdf)
>  Synthetic aperture radar (SAR) tomography (TomoSAR) enables the reconstruction and three-dimensional (3D) localization of targets based on multiple two-dimensional (2D) observations of the same scene. The resolving along the elevation direction can be treated as a line spectrum estimation problem. However, traditional super-resolution spectrum estimation algorithms require multiple snapshots and uncorrelated targets. Meanwhile, as the most popular TomoSAR imaging method in modern years, compressed sensing (CS) based methods suffer from the gridding mismatch effect which markedly degrades the imaging performance. As a gridless CS approach, atomic norm minimization can avoid the gridding effect but requires enormous computing resources. Addressing the above issues, this paper proposes an improved fast ANM algorithm to TomoSAR elevation focusing by introducing the IVDST-ANM algorithm, which reduces the huge computational complexity of the conventional time-consuming semi-positive definite programming (SDP) by the iterative Vandermonde decomposition and shrinkage-thresholding (IVDST) approach, and retains the benefits of ANM in terms of gridless imaging and single snapshot recovery. We conducted experiments using simulated data to evaluate the performance of the proposed method, and reconstruction results of an urban area from the SARMV3D-Imaging 1.0 dataset are also presented.      
### 17.Frequency Hopping Joint Radar-Communications with Hybrid Sub-pulse Frequency and Duration  [ :arrow_down: ](https://arxiv.org/pdf/2204.12090.pdf)
>  Frequency-hopping (FH) joint radar-communications (JRC) can offer excellent security for integrated sensing and communication systems. However, existing JRC schemes mainly embed information using only the sub-pulse frequencies and hence the data rate is limited. In this paper, we propose to use both sub-pulse frequencies and durations for information modulation, leading to higher communication data rates. For information demodulation, we propose a novel scheme by using the time-frequency analysis (TFA) technique and a "you only look once" (YOLO)-based detection system. As such, our system does not require channel estimation, simplifying the transmission signal frame design. Simulation results demonstrate the effectiveness of our scheme, and show that it is robust against the Doppler shift and timing offset between the transceiver and the communication receiver.      
### 18.Acquiring a Dynamic Light Field through a Single-Shot Coded Image  [ :arrow_down: ](https://arxiv.org/pdf/2204.12089.pdf)
>  We propose a method for compressively acquiring a dynamic light field (a 5-D volume) through a single-shot coded image (a 2-D measurement). We designed an imaging model that synchronously applies aperture coding and pixel-wise exposure coding within a single exposure time. This coding scheme enables us to effectively embed the original information into a single observed image. The observed image is then fed to a convolutional neural network (CNN) for light-field reconstruction, which is jointly trained with the camera-side coding patterns. We also developed a hardware prototype to capture a real 3-D scene moving over time. We succeeded in acquiring a dynamic light field with 5x5 viewpoints over 4 temporal sub-frames (100 views in total) from a single observed image. Repeating capture and reconstruction processes over time, we can acquire a dynamic light field at 4x the frame rate of the camera. To our knowledge, our method is the first to achieve a finer temporal resolution than the camera itself in compressive light-field acquisition. Our software is available from our project webpage      
### 19.Cyber-Physical Vulnerability Assessment of P2P Energy Exchanges in Active Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.12081.pdf)
>  Owing to the decreasing costs of distributed energy resources (DERs) as well as decarbonization policies, power systems are undergoing a modernization process. The large deployment of DERs together with internet of things (IoT) devices provide a platform for peer-to-peer (P2P) energy trading in active distribution networks. However, P2P energy trading with IoT devices have driven the grid more vulnerable to cyber-physical threats. To this end, in this paper, a resilience-oriented P2P energy exchange model is developed considering three phase unbalanced distribution systems. In addition, various scenarios for vulnerability assessment of P2P energy exchanges considering adverse prosumers and consumers, who provide false information regarding the price and quantity with the goal of maximum financial benefit and system operation disruption, are considered. Techno-economic survivability analysis against these attacks are investigated on a IEEE 13-node unbalanced distribution test system. Simulation results demonstrate that adverse peers can affect the physical operation of grid, maximize their benefits, and cause financial loss of other agents.      
### 20.AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2204.12077.pdf)
>  Various deep learning methods have been proposed to segment breast lesion from ultrasound images. However, similar intensity distributions, variable tumor morphology and blurred boundaries present challenges for breast lesions segmentation, especially for malignant tumors with irregular shapes. Considering the complexity of ultrasound images, we develop an adaptive attention U-net (AAU-net) to segment breast lesions automatically and stably from ultrasound images. Specifically, we introduce a hybrid adaptive attention module, which mainly consists of a channel self-attention block and a spatial self-attention block, to replace the traditional convolution operation. Compared with the conventional convolution operation, the design of the hybrid adaptive attention module can help us capture more features under different receptive fields. Different from existing attention mechanisms, the hybrid adaptive attention module can guide the network to adaptively select more robust representation in channel and space dimensions to cope with more complex breast lesions segmentation. Extensive experiments with several state-of-the-art deep learning segmentation methods on three public breast ultrasound datasets show that our method has better performance on breast lesion segmentation. Furthermore, robustness analysis and external experiments demonstrate that our proposed AAU-net has better generalization performance on the segmentation of breast lesions. Moreover, the hybrid adaptive attention module can be flexibly applied to existing network frameworks.      
### 21.ATST: Audio Representation Learning with Teacher-Student Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2204.12076.pdf)
>  Self-supervised learning (SSL) learns knowledge from a large amount of unlabeled data, and then transfers the knowledge to a specific problem with a limited number of labeled data. SSL has achieved promising results in various domains. This work addresses the problem of segment-level general audio SSL, and proposes a new transformer-based teacher-student SSL model, named ATST. A transformer encoder is developed on a recently emerged teacher-student baseline scheme, which largely improves the modeling capability of pre-training. In addition, a new strategy for positive pair creation is designed to fully leverage the capability of transformer. Extensive experiments have been conducted, and the proposed model achieves the new state-of-the-art results on almost all of the downstream tasks.      
### 22.Estimating the Resize Parameter in End-to-end Learned Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2204.12022.pdf)
>  We describe a search-free resizing framework that can further improve the rate-distortion tradeoff of recent learned image compression models. Our approach is simple: compose a pair of differentiable downsampling/upsampling layers that sandwich a neural compression model. To determine resize factors for different inputs, we utilize another neural network jointly trained with the compression model, with the end goal of minimizing the rate-distortion objective. Our results suggest that "compression friendly" downsampled representations can be quickly determined during encoding by using an auxiliary network and differentiable image warping. By conducting extensive experimental tests on existing deep image compression models, we show results that our new resizing parameter estimation framework can provide Bjøntegaard-Delta rate (BD-rate) improvement of about 10% against leading perceptual quality engines. We also carried out a subjective quality study, the results of which show that our new approach yields favorable compressed images. To facilitate reproducible research in this direction, the implementation used in this paper is being made freely available online at: <a class="link-external link-https" href="https://github.com/treammm/ResizeCompression" rel="external noopener nofollow">this https URL</a>.      
### 23.Assessing the ability of generative adversarial networks to learn canonical medical image statistics  [ :arrow_down: ](https://arxiv.org/pdf/2204.12007.pdf)
>  In recent years, generative adversarial networks (GANs) have gained tremendous popularity for potential applications in medical imaging, such as medical image synthesis, restoration, reconstruction, translation, as well as objective image quality assessment. Despite the impressive progress in generating high-resolution, perceptually realistic images, it is not clear if modern GANs reliably learn the statistics that are meaningful to a downstream medical imaging application. In this work, the ability of a state-of-the-art GAN to learn the statistics of canonical stochastic image models (SIMs) that are relevant to objective assessment of image quality is investigated. It is shown that although the employed GAN successfully learned several basic first- and second-order statistics of the specific medical SIMs under consideration and generated images with high perceptual quality, it failed to correctly learn several per-image statistics pertinent to the these SIMs, highlighting the urgent need to assess medical image GANs in terms of objective measures of image quality.      
### 24.gLaSDI: Parametric Physics-informed Greedy Latent Space Dynamics Identification  [ :arrow_down: ](https://arxiv.org/pdf/2204.12005.pdf)
>  A parametric adaptive physics-informed greedy Latent Space Dynamics Identification (gLaSDI) method is proposed for accurate, efficient, and robust data-driven reduced-order modeling of high-dimensional nonlinear dynamical systems. In the proposed gLaSDI framework, an autoencoder discovers intrinsic nonlinear latent representations of high-dimensional data, while dynamics identification (DI) models capture local latent-space dynamics. An interactive training algorithm is adopted for the autoencoder and local DI models, which enables identification of simple latent-space dynamics and enhances accuracy and efficiency of data-driven reduced-order modeling. To maximize and accelerate the exploration of the parameter space for the optimal model performance, an adaptive greedy sampling algorithm integrated with a physics-informed residual-based error indicator and random-subset evaluation is introduced to search for the optimal training samples on-the-fly. Further, to exploit local latent-space dynamics captured by the local DI models for an improved modeling accuracy with a minimum number of local DI models in the parameter space, an efficient k-nearest neighbor convex interpolation scheme is employed. The effectiveness of the proposed framework is demonstrated by modeling various nonlinear dynamical problems, including Burgers equations, nonlinear heat conduction, and radial advection. The proposed adaptive greedy sampling outperforms the conventional predefined uniform sampling in terms of accuracy. Compared with the high-fidelity models, gLaSDI achieves 66 to 4,417x speed-up with 1 to 5% relative errors.      
### 25.Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System  [ :arrow_down: ](https://arxiv.org/pdf/2204.11970.pdf)
>  In ophthalmology, intravitreal operative medication therapy (IVOM) is widespread treatment for diseases such as the age-related macular degeneration (AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion (RVO). However, in real-world settings, patients often suffer from loss of vision on time scales of years despite therapy, whereas the prediction of the visual acuity (VA) and the earliest possible detection of deterioration under real-life conditions is challenging due to heterogeneous and incomplete data. In this contribution, we present a workflow for the development of a research-compatible data corpus fusing different IT systems of the department of ophthalmology of a German maximum care hospital. The extensive data corpus allows predictive statements of the expected progression of a patient and his or her VA in each of the three diseases. Within our proposed multistage system, we classify the VA progression into the three groups of therapy "winners", "stabilizers", and "losers" (WSL scheme). Our OCT biomarker classification using an ensemble of deep neural networks results in a classification accuracy (F1-score) of over 98 %, enabling us to complete incomplete OCT documentations while allowing us to exploit them for a more precise VA modelling process. Our VA prediction requires at least four VA examinations and optionally OCT biomarkers from the same time period to predict the VA progression within a forecasted time frame. While achieving a prediction accuracy of up to 69 % (macro average F1-score) when considering all three WSL-based progression groups, this corresponds to an improvement by 11 % in comparison to our ophthalmic expertise (58 %).      
### 26.Cleanformer: A microphone array configuration-invariant, streaming, multichannel neural enhancement frontend for ASR  [ :arrow_down: ](https://arxiv.org/pdf/2204.11933.pdf)
>  This work introduces the Cleanformer, a streaming multichannel neural based enhancement frontend for automatic speech recognition (ASR). This model has a conformer-based architecture which takes as inputs a single channel each of raw and enhanced signals, and uses self-attention to derive a time-frequency mask. The enhanced input is generated by a multichannel adaptive noise cancellation algorithm known as Speech Cleaner, which makes use of noise context to derive its filter taps. The time-frequency mask is applied to the noisy input to produce enhanced output features for ASR. Detailed evaluations are presented with simulated and re-recorded datasets in speech-based and non-speech-based noise that show significant reduction in word error rate (WER) when using a large-scale state-of-the-art ASR model. It also will be shown to significantly outperform enhancement using a beamformer with ideal steering. The enhancement model is agnostic of the number of microphones and array configuration and, therefore, can be used with different microphone arrays without the need for retraining. It is demonstrated that performance improves with more microphones, up to 4, with each additional microphone providing a smaller marginal benefit. Specifically, for an SNR of -6dB, relative WER improvements of about 80\% are shown in both noise conditions.      
### 27.Comparison study of the combination of the SPSA algorithm and the PSO algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2204.11908.pdf)
>  Particle swarm optimization (PSO) is attracting an ever-growing attention and more than ever it has found many application areas for many challenging optimization problems. It is, however, a known fact that PSO has a severe drawback in the update of its global best (gbest) particle, which has a crucial role of guiding the rest of the swarm. In this paper, we propose three efficient solutions to remedy this problem using the SPSA Algorithm. In the first approach, gbest is updated with respect to a global estimation of the gradient and can avoid getting trapped into a local optimum. The second approach is based on the formation of an alternative or artificial global best particle, the so-called aGB, which can replace the native gbest particle for a better guidance, the decision of which is held by a fair competition between the two. The third approach is based on the update of the swarm particle. For this purpose we use simultaneous perturbation stochastic approximation (SPSA) for its low cost. Since SPSA is applied only to the gbest (not to the entire swarm) or to the entire swarm, both approaches result thus in a negligible overhead cost for the entire PSO process. Both approaches are shown to significantly improve the performance of PSO over a wide range of non-linear functions, especially if SPSA and PSO parameters are well selected to fit the problem at hand. As in the basic PSO application, experimental results show that the proposed approaches significantly improved the quality of the Optimization process as measured by a statistic analysis.      
### 28.Memory Efficient Invertible Neural Networks for 3D Photoacoustic Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2204.11850.pdf)
>  Photoacoustic imaging (PAI) can image high-resolution structures of clinical interest such as vascularity in cancerous tumor monitoring. When imaging human subjects, geometric restrictions force limited-view data retrieval causing imaging artifacts. Iterative physical model based approaches reduce artifacts but require prohibitively time consuming PDE solves. Machine learning (ML) has accelerated PAI by combining physical models and learned networks. However, the depth and overall power of ML methods is limited by memory intensive training. We propose using invertible neural networks (INNs) to alleviate memory pressure. We demonstrate INNs can image 3D photoacoustic volumes in the setting of limited-view, noisy, and subsampled data. The frugal constant memory usage of INNs enables us to train an arbitrary depth of learned layers on a consumer GPU with 16GB RAM.      
### 29.Sound Localization by Self-Supervised Time Delay Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2204.12489.pdf)
>  Sounds reach one microphone in a stereo pair sooner than the other, resulting in an interaural time delay that conveys their directions. Estimating a sound's time delay requires finding correspondences between the signals recorded by each microphone. We propose to learn these correspondences through self-supervision, drawing on recent techniques from visual tracking. We adapt the contrastive random walk of Jabri et al. to learn a cycle-consistent representation from unlabeled stereo sounds, resulting in a model that performs on par with supervised methods on "in the wild" internet recordings. We also propose a multimodal contrastive learning model that solves a visually-guided localization task: estimating the time delay for a particular person in a multi-speaker mixture, given a visual representation of their face. Project site: <a class="link-external link-https" href="https://ificl.github.io/stereocrw/" rel="external noopener nofollow">this https URL</a>      
### 30.Measurement uncertainty and unicity of single number quantities describing the spatial decay of speech level in open-plan offices  [ :arrow_down: ](https://arxiv.org/pdf/2204.12486.pdf)
>  The ISO 3382-3 standard (2012) defines single number quantities (SNQs) which evaluate the acoustic quality of open-plan offices, but does not address the issue of measurement uncertainties. This study focusses on the SNQs present in this standard related to spatial decay of speech, i.e. D 2S , L pAS4m and r c. The aim is to provide additional information to the limited literature on the measurement uncertainties of these SNQs by use of both analytical developments and a stochastic approach based on simulations. The accuracy of the analytical developments was studied thanks to simulations of the sound propagation within a series of offices (1 layout, 16 acoustic configurations with different screen heights and different acoustic qualities of screens and ceiling). The SNQs obtained in the simulations cover a wide range: D 2S between 3.4 and 7.5 dB(A), L pAS4m between 40.6 and 51.9 dB(A) and r c between 2.5 and 14.7 m. Therefore, the simulations are representative of a broad set of acoustic qualities. Estimated uncertainties have a magnitude of 0.4 dB(A) for D 2S and vary between 0.4 and 0.7 dB(A) for L pAS4m and between 0.2 and 1.5 m for r c over a measurement path comprising 7 measurement positions. The simulations also raise the question of describing the acoustic quality of an office using a single value for the indicators. The results of the simulations show that in some cases, D 2S values significantly depend on the measurement path, leading to a strong increase of its measurement uncertainty if a unique value is to be considered.      
### 31.A Model-Adaptive Clustering Method for Low-Carbon Energy System Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.12467.pdf)
>  Intermittent renewable energy resources like wind and solar pose great uncertainty of multiple time scales, from minutes to years, on the design and operation of power systems. Energy system optimization models have been developed to find the least-cost solution to matching the uncertainty with flexibility resources. However, input data that capture such multi-time-scale uncertainty are characterized with a long time horizon and bring great difficulty to solving the optimization model. Here we propose an adaptive clustering method based on the decision variables of optimization model to alleviate the computational complexity, in which the energy system is optimized over selected representative time periods instead of the full time horizon. The proposed clustering method is adaptive to various energy system optimization models or settings, because it extracts features from the optimization models. Results show that the proposed clustering method can significantly lower the error in approximating the solution with the full time horizon, compared to traditional clustering methods.      
### 32.Encoding Cardiopulmonary Exercise Testing Time Series as Images for Classification using Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2204.12432.pdf)
>  Exercise testing has been available for more than a half-century and is a remarkably versatile tool for diagnostic and prognostic information of patients for a range of diseases, especially cardiovascular and pulmonary. With rapid advancements in technology, wearables, and learning algorithm in the last decade, its scope has evolved. Specifically, Cardiopulmonary exercise testing (CPX) is one of the most commonly used laboratory tests for objective evaluation of exercise capacity and performance levels in patients. CPX provides a non-invasive, integrative assessment of the pulmonary, cardiovascular, and skeletal muscle systems involving the measurement of gas exchanges. However, its assessment is challenging, requiring the individual to process multiple time series data points, leading to simplification to peak values and slopes. But this simplification can discard the valuable trend information present in these time series. In this work, we encode the time series as images using the Gramian Angular Field and Markov Transition Field and use it with a convolutional neural network and attention pooling approach for the classification of heart failure and metabolic syndrome patients. Using GradCAMs, we highlight the discriminative features identified by the model.      
### 33.Time-triggered Federated Learning over Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.12426.pdf)
>  The newly emerging federated learning (FL) framework offers a new way to train machine learning models in a privacy-preserving manner. However, traditional FL algorithms are based on an event-triggered aggregation, which suffers from stragglers and communication overhead issues. To address these issues, in this paper, we present a time-triggered FL algorithm (TT-Fed) over wireless networks, which is a generalized form of classic synchronous and asynchronous FL. Taking the constrained resource and unreliable nature of wireless communication into account, we jointly study the user selection and bandwidth optimization problem to minimize the FL training loss. To solve this joint optimization problem, we provide a thorough convergence analysis for TT-Fed. Based on the obtained analytical convergence upper bound, the optimization problem is decomposed into tractable sub-problems with respect to each global aggregation round, and finally solved by our proposed online search algorithm. Simulation results show that compared to asynchronous FL (FedAsync) and FL with asynchronous user tiers (FedAT) benchmarks, our proposed TT-Fed algorithm improves the converged test accuracy by up to 12.5% and 5%, respectively, under highly imbalanced and non-IID data, while substantially reducing the communication overhead.      
### 34.Knowledge Transfer in Engineering Fleets: Hierarchical Bayesian Modelling for Multi-Task Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.12404.pdf)
>  We propose a population-level analysis to address issues of data sparsity when building predictive models of engineering infrastructure. By sharing information between similar assets, hierarchical Bayesian modelling is used to improve the survival analysis of a truck fleet (hazard curves) and power prediction in a wind farm (power curves). In each example, a set of correlated functions are learnt over the asset fleet, in a combined inference, to learn a population model. Parameter estimation is improved when sub-fleets of assets are allowed to share correlated information at different levels in the hierarchy. In turn, groups with incomplete data automatically borrow statistical strength from those that are data-rich. The correlations can be inspected to inform which assets share information for which effect (i.e. parameter).      
### 35.On Machine Learning-Driven Surrogates for Sound Transmission Loss Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2204.12290.pdf)
>  Surrogate models are data-based approximations of computationally expensive simulations that enable efficient exploration of the model's design space and informed decision-making in many physical domains. The usage of surrogate models in the vibroacoustic domain, however, is challenging due to the non-smooth, complex behavior of wave phenomena. This paper investigates four Machine Learning (ML) approaches in the modelling of surrogates of Sound Transmission Loss (STL). Feature importance and feature engineering are used to improve the models' accuracy while increasing their interpretability and physical consistency. The transfer of the proposed techniques to other problems in the vibroacoustic domain and possible limitations of the models are discussed.      
### 36.A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment  [ :arrow_down: ](https://arxiv.org/pdf/2204.12283.pdf)
>  Understanding the impact of tumor biology on the composition of nearby cells often requires characterizing the impact of biologically distinct tumor regions. Biomarkers have been developed to label biologically distinct tumor regions, but challenges arise because of differences in the spatial extent and distribution of differentially labeled regions. In this work, we present a framework for systematically investigating the impact of distinct tumor regions on cells near the tumor borders, accounting their cross spatial distributions. We apply the framework to multiplex immunohistochemistry (mIHC) studies of pancreatic cancer and show its efficacy in demonstrating how biologically different tumor regions impact the immune response in the tumor microenvironment. Furthermore, we show that the proposed framework can be extended to largescale whole slide image analysis.      
### 37.Energy Efficient Beamforming Optimization for Integrated Sensing and Communication  [ :arrow_down: ](https://arxiv.org/pdf/2204.12264.pdf)
>  This paper investigates the optimization of beamforming design in a system with integrated sensing and communication (ISAC), where the base station (BS) sends signals for simultaneous multiuser communication and radar sensing. We aim at maximizing the energy efficiency (EE) of the multiuser communication while guaranteeing the sensing requirement in terms of individual radar beampattern gains. The problem is a complicated nonconvex fractional program which is challenging to be solved. By appropriately reformulating the problem and then applying the techniques of successive convex approximation (SCA) and semidefinite relaxation (SDR), we propose an iterative algorithm to address this problem. In theory, we prove that the introduced relaxation of the SDR is rigorously tight. Numerical results validate the effectiveness of the proposed algorithm.      
### 38.A Comparative Study on Approaches to Acoustic Scene Classification using CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2204.12177.pdf)
>  Acoustic scene classification is a process of characterizing and classifying the environments from sound recordings. The first step is to generate features (representations) from the recorded sound and then classify the background environments. However, different kinds of representations have dramatic effects on the accuracy of the classification. In this paper, we explored the three such representations on classification accuracy using neural networks. We investigated the spectrograms, MFCCs, and embeddings representations using different CNN networks and autoencoders. Our dataset consists of sounds from three settings of indoors and outdoors environments - thus the dataset contains sound from six different kinds of environments. We found that the spectrogram representation has the highest classification accuracy while MFCC has the lowest classification accuracy. We reported our findings, insights as well as some guidelines to achieve better accuracy for environment classification using sounds.      
### 39.Motion Planning and Robust Tracking for the Heat Equation using Boundary Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.12144.pdf)
>  Robust output tracking is addressed in this paper for a heat equation with Neumann boundary conditions and anti-collocated boundary input and output. The desired reference tracking is solved using the well-known flatness and Lyapunov approaches. The reference profile is obtained by solving the motion planning problem for the nominal plant. To robustify the closed-loop system in the presence of the disturbances and uncertainties, it is then augmented with PI feedback plus a discontinuous component responsible for rejecting matched disturbances with \textit{a priori} known magnitude bounds. Such control law only requires the information of the system at the same boundary as the control input is located. The resulting dynamic controller globally exponentially stabilizes the error dynamics while also attenuating the influence of Lipschitz-in-time external disturbances and parameter uncertainties. For the case when the motion planning is performed over the uncertain plant, an exponential Input-to-State Stability is obtained, preserving the boundedness of the tracking error norm. The proposed controller relies on a discontinuous term that however passes through an integrator, thereby minimizing the chattering effect in the plant dynamics. The performance of the closed-loop system, thus designed, is illustrated in simulations under different kinds of reference trajectories in the presence of external disturbances and parameter uncertainties.      
### 40.Neural Maximum A Posteriori Estimation on Unpaired Data for Motion Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2204.12139.pdf)
>  Real-world dynamic scene deblurring has long been a challenging task since paired blurry-sharp training data is unavailable. Conventional Maximum A Posteriori estimation and deep learning-based deblurring methods are restricted by handcrafted priors and synthetic blurry-sharp training pairs respectively, thereby failing to generalize to real dynamic blurriness. To this end, we propose a Neural Maximum A Posteriori (NeurMAP) estimation framework for training neural networks to recover blind motion information and sharp content from unpaired data. The proposed NeruMAP consists of a motion estimation network and a deblurring network which are trained jointly to model the (re)blurring process (i.e. likelihood function). Meanwhile, the motion estimation network is trained to explore the motion information in images by applying implicit dynamic motion prior, and in return enforces the deblurring network training (i.e. providing sharp image prior). The proposed NeurMAP is an orthogonal approach to existing deblurring neural networks, and is the first framework that enables training image deblurring networks on unpaired datasets. Experiments demonstrate our superiority on both quantitative metrics and visual quality over state-of-the-art methods. Codes are available on <a class="link-external link-https" href="https://github.com/yjzhang96/NeurMAP-deblur" rel="external noopener nofollow">this https URL</a>.      
### 41.Fast Successive-Cancellation Decoding of Polar Codes with Sequence Nodes  [ :arrow_down: ](https://arxiv.org/pdf/2204.12115.pdf)
>  Due to the sequential nature of the successive-cancellation (SC) algorithm, the decoding of polar codes suffers from significant decoding latencies. Fast SC decoding is able to speed up the SC decoding process, by implementing parallel decoders at the intermediate levels of the SC decoding tree for some special nodes with specific information and frozen bit patterns. To further improve the parallelism of SC decoding, this paper present a new class of special node composed of a sequence of rate one or single-parity-check (SR1/SPC) nodes, which envelops a wide variety of existing special node types. Then, we analyse the parity constraints caused by the frozen bits in each descendant node, such that the decoding performance of the SR1/SPC node can be preserved once the parity constraints are satisfied. Finally, a generalized fast SC decoding algorithm is proposed for the SR1/SPC node, where the corresponding parity constraints are taken into consideration. Simulation results show that compared with the state-of-the-art fast SC decoding algorithms, the proposed decoding algorithm achieves a higher degree of parallelism, especially for high-rate polar codes, without tangibly altering the error-correction performance.      
### 42.Reformulating Speaker Diarization as Community Detection With Emphasis On Topological Structure  [ :arrow_down: ](https://arxiv.org/pdf/2204.12112.pdf)
>  Clustering-based speaker diarization has stood firm as one of the major approaches in reality, despite recent development in end-to-end diarization. However, clustering methods have not been explored extensively for speaker diarization. Commonly-used methods such as k-means, spectral clustering, and agglomerative hierarchical clustering only take into account properties such as proximity and relative densities. In this paper we propose to view clustering-based diarization as a community detection problem. By doing so the topological structure is considered. This work has four major contributions. First it is shown that Leiden community detection algorithm significantly outperforms the previous methods on the clustering of speaker-segments. Second, we propose to use uniform manifold approximation to reduce dimension while retaining global and local topological structure. Third, a masked filtering approach is introduced to extract "clean" speaker embeddings. Finally, the community structure is applied to an end-to-end post-processing network to obtain diarization results. The final system presents a relative DER reduction of up to 70 percent. The breakdown contribution of each component is analyzed.      
### 43.Learning Dual-Pixel Alignment for Defocus Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2204.12105.pdf)
>  It is a challenging task to recover all-in-focus image from a single defocus blurry image in real-world applications. On many modern cameras, dual-pixel (DP) sensors create two-image views, based on which stereo information can be exploited to benefit defocus deblurring. Despite existing DP defocus deblurring methods achieving impressive results, they directly take naive concatenation of DP views as input, while neglecting the disparity between left and right views in the regions out of camera's depth of field (DoF). In this work, we propose a Dual-Pixel Alignment Network (DPANet) for defocus deblurring. Generally, DPANet is an encoder-decoder with skip-connections, where two branches with shared parameters in the encoder are employed to extract and align deep features from left and right views, and one decoder is adopted to fuse aligned features for predicting the all-in-focus image. Due to that DP views suffer from different blur amounts, it is not trivial to align left and right views. To this end, we propose novel encoder alignment module (EAM) and decoder alignment module (DAM). In particular, a correlation layer is suggested in EAM to measure the disparity between DP views, whose deep features can then be accordingly aligned using deformable convolutions. And DAM can further enhance the alignment of skip-connected features from encoder and deep features in decoder. By introducing several EAMs and DAMs, DP views in DPANet can be well aligned for better predicting latent all-in-focus image. Experimental results on real-world datasets show that our DPANet is notably superior to state-of-the-art deblurring methods in reducing defocus blur while recovering visually plausible sharp structures and textures.      
### 44.Centimeter-level Positioning by Instantaneous Lidar-aided GNSS Ambiguity Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2204.12103.pdf)
>  High-precision vehicle positioning is key to the implementation of modern driving systems in urban environments. Global Navigation Satellite System (GNSS) carrier phase measurements can provide millimeter- to centimeter-level positioning, provided that the integer ambiguities are correctly resolved. Abundant code measurements are often used to facilitate integer ambiguity resolution (IAR), however, they suffer from signal blockage and multipath in urban canyons. In this contribution, a lidar-aided instantaneous ambiguity resolution method is proposed. Lidar measurements, in the form of 3D keypoints, are generated by a learning-based point cloud registration method using a pre-built HD map and integrated with GNSS observations in a mixed measurement model to produce precise float solutions, which in turn increase the ambiguity success rate. Closed-form expressions of the ambiguity variance matrix and the associated Ambiguity Dilution of Precision (ADOP) are developed to provide a priori evaluation of such lidar-aided ambiguity resolution performance. Both analytical and experimental results show that the proposed method enables successful instantaneous IAR with limited GNSS satellites and frequencies, leading to centimeter-level vehicle positioning.      
### 45.Learning Weighting Map for Bit-Depth Expansion within a Rational Range  [ :arrow_down: ](https://arxiv.org/pdf/2204.12039.pdf)
>  Bit-depth expansion (BDE) is one of the emerging technologies to display high bit-depth (HBD) image from low bit-depth (LBD) source. Existing BDE methods have no unified solution for various BDE situations, and directly learn a mapping for each pixel from LBD image to the desired value in HBD image, which may change the given high-order bits and lead to a huge deviation from the ground truth. In this paper, we design a bit restoration network (BRNet) to learn a weight for each pixel, which indicates the ratio of the replenished value within a rational range, invoking an accurate solution without modifying the given high-order bit information. To make the network adaptive for any bit-depth degradation, we investigate the issue in an optimization perspective and train the network under progressive training strategy for better performance. Moreover, we employ Wasserstein distance as a visual quality indicator to evaluate the difference of color distribution between restored image and the ground truth. Experimental results show our method can restore colorful images with fewer artifacts and false contours, and outperforms state-of-the-art methods with higher PSNR/SSIM results and lower Wasserstein distance. The source code will be made available at <a class="link-external link-https" href="https://github.com/yuqing-liu-dut/bit-depth-expansion" rel="external noopener nofollow">this https URL</a>      
### 46.Meta-AF: Meta-Learning for Adaptive Filters  [ :arrow_down: ](https://arxiv.org/pdf/2204.11942.pdf)
>  Adaptive filtering algorithms are pervasive throughout modern society and have had a significant impact on a wide variety of domains including audio processing, telecommunications, biomedical sensing, astropyhysics and cosmology, seismology, and many more. Adaptive filters typically operate via specialized online, iterative optimization methods such as least-mean squares or recursive least squares and aim to process signals in unknown or nonstationary environments. Such algorithms, however, can be slow and laborious to develop, require domain expertise to create, and necessitate mathematical insight for improvement. In this work, we seek to go beyond the limits of human-derived adaptive filter algorithms and present a comprehensive framework for learning online, adaptive signal processing algorithms or update rules directly from data. To do so, we frame the development of adaptive filters as a meta-learning problem in the context of deep learning and use a form of self-supervision to learn online iterative update rules for adaptive filters. To demonstrate our approach, we focus on audio applications and systematically develop meta-learned adaptive filters for five canonical audio problems including system identification, acoustic echo cancellation, blind equalization, multi-channel dereverberation, and beamforming. For each application, we compare against common baselines and/or current state-of-the-art methods and show we can learn high-performing adaptive filters that operate in real-time and, in most cases, significantly out perform all past specially developed methods for each task using a single general-purpose configuration of our method.      
### 47.On-demand compute reduction with stochastic wav2vec 2.0  [ :arrow_down: ](https://arxiv.org/pdf/2204.11934.pdf)
>  Squeeze and Efficient Wav2vec (SEW) is a recently proposed architecture that squeezes the input to the transformer encoder for compute efficient pre-training and inference with wav2vec 2.0 (W2V2) models. In this work, we propose stochastic compression for on-demand compute reduction for W2V2 models. As opposed to using a fixed squeeze factor, we sample it uniformly during training. We further introduce query and key-value pooling mechanisms that can be applied to each transformer layer for further compression. Our results for models pre-trained on 960h Librispeech dataset and fine-tuned on 10h of transcribed data show that using the same stochastic model, we get a smooth trade-off between word error rate (WER) and inference time with only marginal WER degradation compared to the W2V2 and SEW models trained for a specific setting. We further show that we can fine-tune the same stochastically pre-trained model to a specific configuration to recover the WER difference resulting in significant computational savings on pre-training models from scratch.      
### 48.Dynamic Ensemble Bayesian Filter for Robust Control of a Human Brain-machine Interface  [ :arrow_down: ](https://arxiv.org/pdf/2204.11840.pdf)
>  Objective: Brain-machine interfaces (BMIs) aim to provide direct brain control of devices such as prostheses and computer cursors, which have demonstrated great potential for mobility restoration. One major limitation of current BMIs lies in the unstable performance in online control due to the variability of neural signals, which seriously hinders the clinical availability of BMIs. Method: To deal with the neural variability in online BMI control, we propose a dynamic ensemble Bayesian filter (DyEnsemble). DyEnsemble extends Bayesian filters with a dynamic measurement model, which adjusts its parameters in time adaptively with neural changes. This is achieved by learning a pool of candidate functions and dynamically weighting and assembling them according to neural signals. In this way, DyEnsemble copes with variability in signals and improves the robustness of online control. Results: Online BMI experiments with a human participant demonstrate that, compared with the velocity Kalman filter, DyEnsemble significantly improves the control accuracy (increases the success rate by 13.9% and reduces the reach time by 13.5% in the random target pursuit task) and robustness (performs more stably over different experiment days). Conclusion: Our results demonstrate the superiority of DyEnsemble in online BMI control. Significance: DyEnsemble frames a novel and flexible framework for robust neural decoding, which is beneficial to different neural decoding applications.      
### 49.Joint Learning of Reward Machines and Policies in Environments with Partially Known Semantics  [ :arrow_down: ](https://arxiv.org/pdf/2204.11833.pdf)
>  We study the problem of reinforcement learning for a task encoded by a reward machine. The task is defined over a set of properties in the environment, called atomic propositions, and represented by Boolean variables. One unrealistic assumption commonly used in the literature is that the truth values of these propositions are accurately known. In real situations, however, these truth values are uncertain since they come from sensors that suffer from imperfections. At the same time, reward machines can be difficult to model explicitly, especially when they encode complicated tasks. We develop a reinforcement-learning algorithm that infers a reward machine that encodes the underlying task while learning how to execute it, despite the uncertainties of the propositions' truth values. In order to address such uncertainties, the algorithm maintains a probabilistic estimate about the truth value of the atomic propositions; it updates this estimate according to new sensory measurements that arrive from the exploration of the environment. Additionally, the algorithm maintains a hypothesis reward machine, which acts as an estimate of the reward machine that encodes the task to be learned. As the agent explores the environment, the algorithm updates the hypothesis reward machine according to the obtained rewards and the estimate of the atomic propositions' truth value. Finally, the algorithm uses a Q-learning procedure for the states of the hypothesis reward machine to determine the policy that accomplishes the task. We prove that the algorithm successfully infers the reward machine and asymptotically learns a policy that accomplishes the respective task.      
