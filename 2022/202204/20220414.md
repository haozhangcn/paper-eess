# ArXiv eess --Thu, 14 Apr 2022
### 1.Sharing the Load: Considering Fairness in De-energization Scheduling to Mitigate Wildfire Ignition Risk using Rolling Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.06543.pdf)
>  Wildfires are a threat to public safety and have increased in both frequency and severity due to climate change. To mitigate wildfire ignition risks, electric power system operators proactively de-energize high-risk power lines during "public safety power shut-off" (PSPS) events. Line de-energizations can cause communities to lose power, which may result in negative economic, health, and safety impacts. Furthermore, the same communities may repeatedly experience power shutoffs over the course of a wildfire season, which compounds these negative impacts. However, there are often many combinations of power lines whose de-energization will result in about the same reduction of wildfire risk, but the associated power loss affects different communities. Therefore, one may raise concerns regarding the fairness of de-energization decisions. Accordingly, this paper proposes a model-predictive-control-inspired framework to select lines to de-energize in order to balance wildfire risk reduction, total load shedding, and fairness considerations. The goal of the framework is to prevent a small fraction of communities from disproportionally being impacted by PSPS events, and to instead more equally share the burden of power outages. For a geolocated test case in the southwestern United States, we use actual California demand data as well as real wildfire risk forecasts to simulate PSPS events during the 2021 wildfire season and compare the performance of various methods for promoting fairness. Our results demonstrate that the proposed formulation can provide significantly more fair outcomes with limited impacts on system-wide performance.      
### 2.Bounding the difference between model predictive control and neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.06486.pdf)
>  There is a growing debate on whether the future of feedback control systems will be dominated by data-driven or model-driven approaches. Each of these two approaches has their own complimentary set of advantages and disadvantages, however, only limited attempts have, so far, been developed to bridge the gap between them. To address this issue, this paper introduces a method to bound the worst-case error between feedback control policies based upon model predictive control (MPC) and neural networks (NNs). This result is leveraged into an approach to automatically synthesize MPC policies minimising the worst-case error with respect to a NN. Numerical examples highlight the application of the bounds, with the goal of the paper being to encourage a more quantitative understanding of the relationship between data-driven and model-driven control.      
### 3.BEHM-GAN: Bandwidth Extension of Historical Music using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.06478.pdf)
>  Audio bandwidth extension aims to expand the spectrum of narrow-band audio signals. Although this topic has been broadly studied during recent years, the particular problem of extending the bandwidth of historical music recordings remains an open challenge. This paper proposes BEHM-GAN, a model based on generative adversarial networks, as a practical solution to this problem. The proposed method works with the complex spectrogram representation of audio and, thanks to a dedicated regularization strategy, can effectively extend the bandwidth of out-of-distribution real historical recordings. The BEHM-GAN is designed to be applied as a second step after denoising the recording to suppress any additive disturbances, such as clicks and background noise. We train and evaluate the method using solo piano classical music. The proposed method outperforms the compared baselines in both objective and subjective experiments. The results of a formal blind listening test show that BEHM-GAN significantly increases the perceptual sound quality in early-20th-century gramophone recordings. For several items, there is a substantial improvement in the mean opinion score after enhancing historical recordings with the proposed bandwidth-extension algorithm. This study represents a relevant step toward data-driven music restoration in real-world scenarios.      
### 4.WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma  [ :arrow_down: ](https://arxiv.org/pdf/2204.06455.pdf)
>  Lung cancer is the leading cause of cancer death worldwide, and adenocarcinoma (LUAD) is the most common subtype. Exploiting the potential value of the histopathology images can promote precision medicine in oncology. Tissue segmentation is the basic upstream task of histopathology image analysis. Existing deep learning models have achieved superior segmentation performance but require sufficient pixel-level annotations, which is time-consuming and expensive. To enrich the label resources of LUAD and to alleviate the annotation efforts, we organize this challenge WSSS4LUAD to call for the outstanding weakly-supervised semantic segmentation (WSSS) techniques for histopathology images of LUAD. Participants have to design the algorithm to segment tumor epithelial, tumor-associated stroma and normal tissue with only patch-level labels. This challenge includes 10,091 patch-level annotations (the training set) and over 130 million labeled pixels (the validation and test sets), from 87 WSIs (67 from GDPH, 20 from TCGA). All the labels were generated by a pathologist-in-the-loop pipeline with the help of AI models and checked by the label review board. Among 532 registrations, 28 teams submitted the results in the test phase with over 1,000 submissions. Finally, the first place team achieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919). According to the technical reports of the top-tier teams, CAM is still the most popular approach in WSSS. Cutmix data augmentation has been widely adopted to generate more reliable samples. With the success of this challenge, we believe that WSSS approaches with patch-level annotations can be a complement to the traditional pixel annotations while reducing the annotation efforts. The entire dataset has been released to encourage more researches on computational pathology in LUAD and more novel WSSS techniques.      
### 5.Sample-based observability of linear discrete-time systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.06451.pdf)
>  In this work, sample-based observability of linear discrete-time systems is studied. That is, we consider the case where the system output measurements are not available at every time instance. It is shown that some discrete-time systems exhibit particular behaviors that lead to pathological sampling. Depending on the characteristics of the system, different sampling schemes are developed that allow the system state to be reconstructed.      
### 6.Electric Motor Design Optimization: A Convex Surrogate Modeling Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.06422.pdf)
>  This paper instantiates a convex electric powertrain design optimization framework, bridging the gap between high-level powertrain sizing and low-level components design. We focus on the electric motor and transmission of electric vehicles, using a scalable convex motor model based on surrogate modeling techniques. Specifically, we first select relevant motor design variables and evaluate high-fidelity samples according to a predefined sampling plan. Second, using the sample data, we identify a convex model of the motor, which predicts its losses as a function of the operating point and the design parameters. We also identify models of the remaining components of the powertrain, namely a battery and a fixed-gear transmission. Third, we frame the minimum-energy consumption design problem over a drive cycle as a second-order conic program that can be efficiently solved with optimality guarantees. Finally, we showcase our framework in a case study for a compact family car and compute the optimal motor design and transmission ratio. We validate the accuracy of our models with a high-fidelity simulation tool and calculate the drift in battery energy consumption. We show that our model can capture the optimal operating line and the error in battery energy consumption is low. Overall, our framework can provide electric motor design experts with useful starting points for further design optimization.      
### 7.Low Voltage Customer Phase Identification Methods Based on Smart Meter Data  [ :arrow_down: ](https://arxiv.org/pdf/2204.06372.pdf)
>  The increased deployment of distributed energy generation and the integration of new, large electric loads such as electric vehicles and heat pumps challenge the correct and reliable operation of low voltage distribution systems. To tackle potential problems, active management solutions are proposed in the literature, which require distribution system models that include the phase connectivity of all the consumers in the network. However, information on the phase connectivity is in practice often unavailable. In this work, several voltage and power measurement-based phase identification methods from the literature are implemented. A consistent comparison of the methods is made across different smart meter accuracy classes and smart meter penetration levels using publicly available data. Furthermore, a novel method is proposed that makes use of ensemble learning and that can combine data from different measurement campaigns. The results indicate that generally better results are obtained with voltage data compared to power data from smart meters of the same accuracy class. If power data is available too, the novel ensemble method can improve the accuracy of the phase identification obtained from voltage data alone.      
### 8.Mixed-Integer Programming for Signal Temporal Logic with Fewer Binary Variables  [ :arrow_down: ](https://arxiv.org/pdf/2204.06367.pdf)
>  Signal Temporal Logic (STL) provides a convenient way of encoding complex control objectives for robotic and cyber-physical systems. The state-of-the-art in trajectory synthesis for STL is based on Mixed-Integer Convex Programming (MICP). The MICP approach is sound and complete, but has limited scalability due to exponential complexity in the number of binary variables. In this letter, we propose a more efficient MICP encoding for STL. Our new encoding is based on the insight that disjunction can be encoded using a logarithmic number of binary variables and conjunction can be encoded without binary variables. We demonstrate in simulation examples that our proposed approach significantly outperforms the state-of-the-art for long and complex specifications. Open-source software is available at <a class="link-external link-https" href="https://stlpy.readthedocs.io" rel="external noopener nofollow">this https URL</a> .      
### 9.Techtile: a Flexible Testbed for Distributed Acoustic Indoor Positioning and Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2204.06352.pdf)
>  The proposed infrastructure, named Techtile, provides a unique R&amp;D facility as features dispersed electronics enables transmission and capturing of a multitude of signals in 3D. Specific available equipment that enhances the design process from smooth prototyping to a commercial product is discussed. The acoustic parameters of the room, particularly the reverberation and ambient noise, are measured to take these into account for future innovative acoustic indoor positioning and sensing systems. This can have a positive influence on the accuracy and precision. The wooden construction represents an acoustically challenging room for audible sound with a maximum measured RT60 value of 1.17s at 5kHz, while for ultrasound it is rather challenging due to the present ambient noise sources. In general, the Techtile room can be compared with a home or quiet office environment, in terms of sound pressure levels (SPLs). In addition to the acoustic properties, possible research and development options are discussed in combination with the associated challenges. Many of the designs described are available through open source.      
### 10.IRS-assisted Multi-cell Multi-band Systems: Practical Reflection Model and Joint Beamforming Design  [ :arrow_down: ](https://arxiv.org/pdf/2204.06351.pdf)
>  Intelligent reflecting surface (IRS) has been regarded as a promising and revolutionary technology for future wireless communication systems owing to its capability of tailoring signal propagation environment in an energy/spectrum/hardware-efficient manner. However, most existing studies on IRS optimizations are based on a simple and ideal reflection model that is impractical in hardware implementation, which thus leads to severe performance loss in realistic wideband/multi-band systems. To deal with this problem, in this paper we first propose a more practical and more tractable IRS reflection model that describes the difference of reflection responses for signals at different frequencies. Then, we investigate the joint transmit beamforming and IRS reflection beamforming design for an IRS-assisted multi-cell multi-band system. Both power minimization and sum-rate maximization problems are solved by exploiting popular second-order cone programming (SOCP), Riemannian manifold, minimization-majorization (MM), weighted minimum mean square error (WMMSE), and block coordinate descent (BCD) methods. Simulation results illustrate the significant performance improvement of our proposed joint transmit beamforming and reflection design algorithms based on the practical reflection model in terms of power saving and rate enhancement.      
### 11.Secure Formation Control via Edge Computing Enabled by Fully Homomorphic Encryption and Mixed Uniform-Logarithmic Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2204.06349.pdf)
>  Recent developments in communication technologies, such as 5G, together with innovative computing paradigms, such as edge computing, provide further possibilities for the implementation of real-time networked control systems. However, privacy and cyber-security concerns arise when sharing private data between sensors, agents and a third-party computing facility. In this paper, a secure version of the distributed formation control is presented, analyzed and simulated, where gradient-based formation control law is implemented in the edge, with sensor and actuator information being secured by fully homomorphic encryption method based on learning with error (FHE-LWE) combined with a proposed mixed uniform-logarithmic quantizer (MULQ). The novel quantizer is shown to be suitable for realizing secure control systems with FHE-LWE where the critical real-time information can be quantized into a prescribed bounded space of plaintext while satisfying a sector bound condition whose lower and upper-bound can be made sufficiently close to an identity. An absolute stability analysis is presented, that shows the asymptotic stability of the closed-loop secure control system.      
### 12.CoDGraD: A Code-based Distributed Gradient Descent Scheme for Decentralized Convex Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.06344.pdf)
>  In this paper, we consider a large network containing many regions such that each region is equipped with a worker with some data processing and communication capability. For such a network, some workers may become stragglers due to the failure or heavy delay on computing or communicating. To resolve the above straggling problem, a coded scheme that introduces certain redundancy for every worker was recently proposed, and a gradient coding paradigm was developed to solve convex optimization problems when the network has a centralized fusion center. In this paper, we propose an iterative distributed algorithm, referred as Code-Based Distributed Gradient Descent algorithm (CoDGraD), to solve convex optimization problems over distributed networks. In each iteration of the proposed algorithm, an active worker shares the coded local gradient and approximated solution of the convex optimization problem with non-straggling workers at the adjacent regions only. In this paper, we also provide the consensus and convergence analysis for the CoDGraD algorithm and we demonstrate its performance via numerical simulations.      
### 13.FiN: A Smart Grid and Power Line Communication Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2204.06336.pdf)
>  The increasing complexity of low-voltage networks poses a growing challenge for the reliable and fail-safe operation of electricity grids. The reasons for this include an increasingly decentralized energy generation (photovoltaic systems, wind power, etc.) and the emergence of new types of consumers (e-mobility, domestic electricity storage, etc.). At the same time, the low-voltage grid is largely unmonitored and local power failures are sometimes hard to detect. To overcome this, power line communication (PLC) has emerged as a potential solution for reliable monitoring of the low-voltage grid. In addition to establishing a communication infrastructure, PLC also offers the possibility of evaluating the cables themselves, as well as the connection quality between individual cable distributors based on their Signal-to-Noise Ratio (SNR). The roll-out of a large-scale PLC infrastructure therefore not only ensures communication, but also introduces a tool for monitoring the entire network. To evaluate the potential of this data, we installed 38 PLC modems in three different areas of a German city with a population of about 150,000 as part of the Fühler-im-Netz project. Over a period of 22 months, an SNR spectrum of each connection between adjacent PLC modems was generated every quarter of an hour. % and the voltage was measured every minute. The availability of this real-world PLC data opens up new possibilities to react to the increasingly complex challenges in future smart grids. This paper provides a detailed analysis of the data generation and describes how the data was collected during normal operation of the electricity grid. In addition, we present common anomalies, effects, and trends that could be observed in the PLC data at daily, weekly, or seasonal levels. Finally, we discuss potential use cases and the remote inspection of a cable section is highlighted as an example.      
### 14.Production federated keyword spotting via distillation, filtering, and joint federated-centralized training  [ :arrow_down: ](https://arxiv.org/pdf/2204.06322.pdf)
>  We trained a keyword spotting model using federated learning on real user devices and observed significant improvements when the model was deployed for inference on phones. To compensate for data domains that are missing from on-device training caches, we employed joint federated-centralized training. And to learn in the absence of curated labels on-device, we formulated a confidence filtering strategy based on user-feedback signals for federated distillation. These techniques created models that significantly improved quality metrics in offline evaluations and user-experience metrics in live A/B experiments.      
### 15.Deep Learning-based Framework for Automatic Cranial Defect Reconstruction and Implant Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2204.06310.pdf)
>  The goal of this work is to propose a robust, fast, and fully automatic method for personalized cranial defect reconstruction and implant modeling. <br>We propose a two-step deep learning-based method using a modified U-Net architecture to perform the defect reconstruction, and a dedicated iterative procedure to improve the implant geometry, followed by automatic generation of models ready for 3-D printing. We propose a cross-case augmentation based on imperfect image registration combining cases from different datasets. We perform ablation studies regarding different augmentation strategies and compare them to other state-of-the-art methods. <br>We evaluate the method on three datasets introduced during the AutoImplant 2021 challenge, organized jointly with the MICCAI conference. We perform the quantitative evaluation using the Dice and boundary Dice coefficients, and the Hausdorff distance. The average Dice coefficient, boundary Dice coefficient, and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm respectively. We perform an additional qualitative evaluation by 3-D printing and visualization in mixed reality to confirm the implant's usefulness. <br>We propose a complete pipeline that enables one to create the cranial implant model ready for 3-D printing. The described method is a greatly extended version of the method that scored 1st place in all AutoImplant 2021 challenge tasks. We freely release the source code, that together with the open datasets, makes the results fully reproducible. The automatic reconstruction of cranial defects may enable manufacturing personalized implants in a significantly shorter time, possibly allowing one to perform the 3-D printing process directly during a given intervention. Moreover, we show the usability of the defect reconstruction in mixed reality that may further reduce the surgery time.      
### 16.Optimal Intermittent Particle Filter  [ :arrow_down: ](https://arxiv.org/pdf/2204.06265.pdf)
>  The problem of the optimal allocation (in the expected mean square error sense) of a measurement budget for particle filtering is addressed. We propose three different optimal intermittent filters, whose optimality criteria depend on the information available at the time of decision making. For the first, the stochastic program filter, the measurement times are given by a policy that determines whether a measurement should be taken based on the measurements already acquired. The second, called the offline filter, determines all measurement times at once by solving a combinatorial optimization program before any measurement acquisition. For the third one, which we call online filter, each time a new measurement is received, the next measurement time is recomputed to take all the information that is then available into account. We prove that in terms of expected mean square error, the stochastic program filter outperforms the online filter, which itself outperforms the offline filter. However, these filters are generally intractable. For this reason, the filter estimate is approximated by a particle filter. Moreover, the mean square error is approximated using a Monte-Carlo approach, and different optimization algorithms are compared to approximately solve the combinatorial programs (a random trial algorithm, greedy forward and backward algorithms, a simulated annealing algorithm, and a genetic algorithm). Finally, the performance of the proposed methods is illustrated on two examples: a tumor motion model and a common benchmark for particle filtering.      
### 17.Simulator-in-the-loop state estimation for vehicle dynamics control: theory and experiments  [ :arrow_down: ](https://arxiv.org/pdf/2204.06259.pdf)
>  In vehicle dynamics control, many variables of interest cannot be directly measured, as sensors might be costly, fragile, or even not available. Therefore, real-time estimation techniques need to be used. The previous approach suffers from two main drawbacks: (i) the approximations due to model mismatch might jeopardize the performance of the final estimation-based control; (ii) each new estimator requires the calibration from scratch of a dedicated model. In this paper, we propose a simulator-in-the-loop scheme, where the ad-hoc model is replaced by an accurate multibody simulator of the vehicle, typically available to vehicles manufacturers and suitable for the estimation of any onboard variable, coupled with a compensator within a closed-loop observer scheme. Given the black-box nature of the simulator, a data-driven methodology for observer tuning is developed, based on Bayesian optimization. The effectiveness of the proposed estimation method for the estimation of vehicle states and forces, as compared to traditional model-based Kalman filtering, is experimentally shown on a dataset collected with a sport car.      
### 18.Wi-Fi Based Passive Human Motion Sensing for In-Home Healthcare Applications  [ :arrow_down: ](https://arxiv.org/pdf/2204.06219.pdf)
>  This paper introduces a Wi-Fi signal based passive wireless sensing system that has the capability to detect diverse indoor human movements, from whole body motions to limb movements and including breathing movements of the chest. The real time signal processing used for human body motion sensing and software defined radio demo system are described and verified in practical experiments scenarios, which include detection of through-wall human body movement, hand gesture or tremor, and even respiration. The experiment results offer potential for promising healthcare applications using Wi-Fi passive sensing in the home to monitor daily activities, to gather health data and detect emergency situations.      
### 19.Safe Stochastic Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2204.06207.pdf)
>  Combining efficient and safe control for safety-critical systems is challenging. Robust methods may be overly conservative, whereas probabilistic controllers require a trade-off between efficiency and safety. In this work, we propose a safety algorithm that is compatible with any stochastic Model Predictive Control method for linear systems with additive uncertainty and polytopic constraints. This safety algorithm allows to use the optimistic control inputs of stochastic Model Predictive Control as long as a safe backup planner can ensure safety with respect to satisfying hard constraints subject to bounded uncertainty. Besides ensuring safe behavior, the proposed stochastic Model Predictive Control algorithm guarantees recursive feasibility and input-to-state stability of the system origin. The benefits of the safe stochastic Model Predictive Control algorithm are demonstrated in a numerical simulation, highlighting the advantages compared to purely robust or stochastic predictive controllers.      
### 20.Optimal Sensor Placement for Hybrid Source Localization Using Fused TOA-RSS-AOA Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2204.06198.pdf)
>  Source localization techniques incorporating hybrid measurements improve the reliability and accuracy of the location estimate. Given a set of hybrid sensors that can collect combined time of arrival (TOA), received signal strength (RSS) and angle of arrival (AOA) measurements, the localization accuracy can be enhanced further by optimally designing the placements of the hybrid sensors. In this paper, we present an optimal sensor placement methodology, which is based on the principle of majorization-minimization (MM), for hybrid localization technique. We first derive the Cramer-Rao lower bound (CRLB) of the hybrid measurement model, and formulate the design problem using the A-optimal criterion. Next, we introduce an auxiliary variable to reformulate the design problem into an equivalent saddle-point problem, and then construct simple surrogate functions (having closed form solutions) over both primal and dual variables. The application of MM in this paper is distinct from the conventional MM (that is usually developed only over the primal variable), and we believe that the MM framework developed in this paper can be employed to solve many optimization problems. The main advantage of our method over most of the existing state-of-the-art algorithms (which are mostly analytical in nature) is its ability to work for both uncorrelated and correlated noise in the measurements. We also discuss the extension of the proposed algorithm for the optimal placement designs based on D and E optimal criteria. Finally, the performance of the proposed method is studied under different noise conditions and different design parameters.      
### 21.A Unified Cascaded Encoder ASR Model for Dynamic Model Sizes  [ :arrow_down: ](https://arxiv.org/pdf/2204.06164.pdf)
>  In this paper, we propose a dynamic cascaded encoder Automatic Speech Recognition (ASR) model, which unifies models for different deployment scenarios. Moreover, the model can significantly reduce model size and power consumption without loss of quality. Namely, with the dynamic cascaded encoder model, we explore three techniques to maximally boost the performance of each model size: 1) Use separate decoders for each sub-model while sharing the encoders; 2) Use funnel-pooling to improve the encoder efficiency; 3) Balance the size of causal and non-causal encoders to improve quality and fit deployment constraints. Overall, the proposed large-medium model has 30% smaller size and reduces power consumption by 33%, compared to the baseline cascaded encoder model. The triple-size model that unifies the large, medium, and small models achieves 37% total size reduction with minimal quality loss, while substantially reducing the engineering efforts of having separate models.      
### 22.Simultaneous Lane-Keeping and Obstacle Avoidance by Combining Model Predictive Control and Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2204.06136.pdf)
>  In this work, we combine {Model Predictive Control} (MPC) and Control Barrier Function (CBF) design {methods} to create a hierarchical control law for simultaneous lane-keeping (LK) and obstacle avoidance (OA): at the low level, MPC performs LK via trajectory tracking during nominal operation; and at the high level, different CBF-based safety filters that ensure both LK and OA are designed and compared across some practical scenarios. In particular, we show that Exponential Safety (ESf) and Prescribed-Time Safety (PTSf) filters, which override the MPC control when necessary, result in feasible Quadratic Programs when safety is prioritized appropriately. We additionally investigate control designs subject to input constraints by using Input-Constrained-CBFs. Finally, we compare the performance of combinations of ESf, PTSf, and their input-constrained counterparts with respect to the LK and OA goals in two simulation studies for early- and late-detected obstacle scenarios.      
### 23.Integrating Distributed Energy Resources: Optimal Prosumer Decisions and Impacts of Net Metering Tariffs  [ :arrow_down: ](https://arxiv.org/pdf/2204.06115.pdf)
>  The rapid growth of the behind-the-meter (BTM) distributed generation has led to initiatives to reform the net energy metering (NEM) policies to address pressing concerns of rising electricity bills, fairness of cost allocation, and the long-term growth of distributed energy resources. This article presents an analytical framework for the optimal prosumer consumption decision using an inclusive NEM X tariff model that covers existing and proposed NEM tariff designs. The structure of the optimal consumption policy lends itself to near closed-form optimal solutions suitable for practical energy management systems that are responsive to stochastic BTM generation and dynamic pricing. The short and long-run performance of NEM and feed-in tariffs (FiT) are considered under a sequential rate-setting decision process. Also presented are numerical results that characterize social welfare distributions, cross-subsidies, and long-run solar adoption performance for selected NEM policy designs.      
### 24.SRMD: Sparse Random Mode Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2204.06108.pdf)
>  Signal decomposition and multiscale signal analysis provide many useful tools for time-frequency analysis. We proposed a random feature method for analyzing time-series data by constructing a sparse approximation to the spectrogram. The randomization is both in the time window locations and the frequency sampling, which lowers the overall sampling and computational cost. The sparsification of the spectrogram leads to a sharp separation between time-frequency clusters which makes it easier to identify intrinsic modes, and thus leads to a new data-driven mode decomposition. The applications include signal representation, outlier removal, and mode decomposition. On the benchmark tests, we show that our approach outperforms other state-of-the-art decomposition methods.      
### 25.A Tutorial on Solution Properties of State Space Models of Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.06104.pdf)
>  The starting point of analysis of state space models is investigating existence, uniqueness and solution properties such as the semigroup property, and various formulas for the solutions. Several concepts such as the state transition matrix, the matrix exponential, the variations of constants formula (the Cauchy formula), the Peano-Baker series, and the Picard iteration are used to characterize solutions. In this note, a tutorial treatment is given where all of these concepts are shown to be various manifestations of a single abstract method, namely solving equations using an operator Neumann series involving the Volterra operator of forward integration. The matrix exponential, the Peano-Baker series, the Picard iteration, and the Cauchy formula can be "discovered" naturally from this Neumann series. The convergence of the series and iterations is a consequence of the key property of asymptotic nilpotence of the Volterra operator. This property is an asymptotic version of the nilpotence property of a strictly-lower-triangular matrix.      
### 26.A Post Auto-regressive GAN Vocoder Focused on Spectrum Fracture  [ :arrow_down: ](https://arxiv.org/pdf/2204.06086.pdf)
>  Generative adversarial networks (GANs) have been indicated their superiority in usage of the real-time speech synthesis. Nevertheless, most of them make use of deep convolutional layers as their backbone, which may cause the absence of previous signal information. However, the generation of speech signals invariably require preceding waveform samples in its reconstruction, as the lack of this can lead to artifacts in generated speech. To address this conflict, in this paper, we propose an improved model: a post auto-regressive (AR) GAN vocoder with a self-attention layer, which merging self-attention in an AR loop. It will not participate in inference, but can assist the generator to learn temporal dependencies within frames in training. Furthermore, an ablation study was done to confirm the contribution of each part. Systematic experiments show that our model leads to a consistent improvement on both objective and subjective evaluation performance.      
### 27.Massive MIMO Beam Management in Sub-6 GHz 5G NR  [ :arrow_down: ](https://arxiv.org/pdf/2204.06064.pdf)
>  Beam codebooks are a new feature of massive multiple-input multiple-output (M-MIMO) in 5G new radio (NR). Codebooks comprised of beamforming vectors are used to transmit reference signals and obtain limited channel state information (CSI) from receivers via the codeword index. This enables large arrays that cannot otherwise obtain sufficient CSI. The performance, however, is limited by the codebook design. In this paper, we show that machine learning can be used to train site-specific codebooks for initial access. We design a neural network based on an autoencoder architecture that uses a beamspace observation in combination with RF environment characteristics to improve the synchronization signal (SS) burst codebook. We test our algorithm using a flexible dataset of channels generated from QuaDRiGa. The results show that our model outperforms the industry standard (DFT beams) and approaches the optimal performance (perfect CSI and singular value decomposition (SVD)-based beamforming), using only a few bits of feedback.      
### 28.Optimal Cybersecurity Investments Using SIS Model: Weakly Connected Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.06035.pdf)
>  We study the problem of minimizing the (time) average security costs in large systems comprising many interdependent subsystems, where the state evolution is captured by a susceptible-infected-susceptible (SIS) model. The security costs reflect security investments, economic losses and recovery costs from infections and failures following successful attacks. However, unlike in existing studies, we assume that the underlying dependence graph is only weakly connected, but not strongly connected. When the dependence graph is not strongly connected, existing approaches to computing optimal security investments cannot be applied. Instead, we show that it is still possible to find a good solution by perturbing the problem and establishing necessary continuity results that then allow us to leverage the existing algorithms.      
### 29.Hybrid Neural Network Augmented Physics-based Models for Nonlinear Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2204.06471.pdf)
>  In this paper we present a hybrid neural network augmented physics-based modeling (APBM) framework for Bayesian nonlinear latent space estimation. The proposed APBM strategy allows for model adaptation when new operation conditions come into play or the physics-based model is insufficient (or incomplete) to properly describe the latent phenomenon. One advantage of the APBMs and our estimation procedure is the capability of maintaining the physical interpretability of estimated states. Furthermore, we propose a constraint filtering approach to control the neural network contributions to the overall model. We also exploit assumed density filtering techniques and cubature integration rules to present a flexible estimation strategy that can easily deal with nonlinear models and high-dimensional latent spaces. Finally, we demonstrate the efficacy of our methodology by leveraging a target tracking scenario with nonlinear and incomplete measurement and acceleration models, respectively.      
### 30.Is Speech Pathology a Biomarker in Automatic Speaker Verification?  [ :arrow_down: ](https://arxiv.org/pdf/2204.06450.pdf)
>  With the advancements in deep learning (DL) and an increasing interest in data-driven speech processing methods, a major challenge for speech data scientists in the healthcare domain is the anonymization of pathological speech, which is a required step to be able to make them accessible as a public training resource. In this paper, we investigate pathological speech data and compare their speaker verifiability with that of healthy individuals. We utilize a large pathological speech corpus of more than 2,000 test subjects with various speech and voice disorders from different ages and apply DL-based automatic speaker verification (ASV) techniques. As a result, we obtained a mean equal error rate (EER) of 0.86% with a standard deviation of 0.16%, which is a factor of three lower than comparable healthy speech databases. We further perform detailed analyses of external influencing factors on ASV such as age, pathology, recording environment, and utterance length, to explore their respective effect. Our findings indicate that speech pathology is a potential biomarker in ASV. This is potentially of high interest for the anonymization of pathological speech data.      
### 31.Receptive Field Analysis of Temporal Convolutional Networks for Monaural Speech Dereverberation  [ :arrow_down: ](https://arxiv.org/pdf/2204.06439.pdf)
>  Speech dereverberation is often an important requirement in robust speech processing tasks. Supervised deep learning (DL) models give state-of-the-art performance for single-channel speech dereverberation. Temporal convolutional networks (TCNs) are commonly used for sequence modelling in speech enhancement tasks. A feature of TCNs is that they have a receptive field (RF) dependant on the specific model configuration which determines the number of input frames that can be observed to produce an individual output frame. It has been shown that TCNs are capable of performing dereverberation of simulated speech data, however a thorough analysis, especially with focus on the RF is yet lacking in the literature. This paper analyses dereverberation performance depending on the model size and the RF of TCNs. Experiments using the WHAMR corpus which is extended to include room impulse responses (RIRs) with larger T60 values demonstrate that a larger RF can have significant improvement in performance when training smaller TCN models. It is also demonstrated that TCNs benefit from a wider RF when dereverberating RIRs with larger RT60 values.      
### 32.Sound Event Triage: Detecting Sound Events Considering Priority of Classes  [ :arrow_down: ](https://arxiv.org/pdf/2204.06402.pdf)
>  We propose a new task for sound event detection (SED): sound event triage (SET). The goal of SET is to detect a high-priority event while allowing misdetections of low-priority events where the extent of priority is given for each event class. In conventional methods of SED for targeting a specific sound event class, only information on types of target sound can be treated. To flexibly control more wealth of information on the target event, the proposed SET exploits not only types of target sound but also the extent to which each target sound is detected with priority. To implement SET, we apply a method that allows the system input of the priority of sound events to be detected, which is based on the class-level loss-conditional training. Results of the experiment using the URBAN--SED dataset reveal that our SET scheme achieves reasonable detection performance in terms of frame-based and intersection-based F-scores. In particular, the proposed method of SET outperforms the conventional SED method by around 10 percentage points for some events.      
### 33.Online greedy identification of linear dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.06375.pdf)
>  This work addresses the problem of exploration in an unknown environment. For linear dynamical systems, we use an experimental design framework and introduce an online greedy policy where the control maximizes the information of the next step. In a setting with a limited number of experimental trials, our algorithm has low complexity and shows experimentally competitive performances compared to more elaborate gradient-based methods.      
### 34.Deep learning based automatic detection of offshore oil slicks using SAR data and contextual information  [ :arrow_down: ](https://arxiv.org/pdf/2204.06371.pdf)
>  Ocean surface monitoring, especially oil slick detection, has become mandatory due to its importance for oil exploration and risk prevention on ecosystems. For years, the detection task has been performed manually by photo-interpreters using Synthetic Aperture Radar (SAR) images with the help of contextual data such as wind. This tedious manual work cannot handle the increasing amount of data collected by the available sensors and thus requires automation. Literature reports conventional and semi-automated detection methods that generally focus either on oil slicks originating from anthropogenic (spills) or natural (seeps) sources on limited data collections. As an extension, this paper presents the automation of offshore oil slicks on an extensive database with both kinds of slicks. It builds upon the slick annotations of specialized photo-interpreters on Sentinel-1 SAR data for 4 years over 3 exploration and monitoring areas worldwide. All the considered SAR images and related annotation relate to real oil slick monitoring scenarios. Further, wind estimation is systematically computed to enrich the data collection. Paper contributions are the following : (i) a performance comparison of two deep learning approaches: semantic segmentation using FC-DenseNet and instance segmentation using Mask-RCNN. (ii) the introduction of meteorological information (wind speed) is deemed valuable for oil slick detection in the performance evaluation. The main results of this study show the effectiveness of slick detection by deep learning approaches, in particular FC-DenseNet, which captures more than 92% of oil instances in our test set. Furthermore, a strong correlation between model performances and contextual information such as slick size and wind speed is demonstrated in the performance evaluation. This work opens perspectives to design models that can fuse SAR and wind information to reduce the false alarm rate.      
### 35.A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic  [ :arrow_down: ](https://arxiv.org/pdf/2204.06362.pdf)
>  The use of Machine Learning (ML) has rapidly spread across several fields, having encountered many applications in Structural Dynamics and Vibroacoustic (SD\&amp;V). The increasing capabilities of ML to unveil insights from data, driven by unprecedented data availability, algorithms advances and computational power, enhance decision making, uncertainty handling, patterns recognition and real-time assessments. Three main applications in SD\&amp;V have taken advantage of these benefits. In Structural Health Monitoring, ML detection and prognosis lead to safe operation and optimized maintenance schedules. System identification and control design are leveraged by ML techniques in Active Noise Control and Active Vibration Control. Finally, the so-called ML-based surrogate models provide fast alternatives to costly simulations, enabling robust and optimized product design. Despite the many works in the area, they have not been reviewed and analyzed. Therefore, to keep track and understand this ongoing integration of fields, this paper presents a survey of ML applications in SD\&amp;V analyses, shedding light on the current state of implementation and emerging opportunities. The main methodologies, advantages, limitations, and recommendations based on scientific knowledge were identified for each of the three applications. Moreover, the paper considers the role of Digital Twins and Physics Guided ML to overcome current challenges and power future research progress. As a result, the survey provides a broad overview of the present landscape of ML applied in SD\&amp;V and guides the reader to an advanced understanding of progress and prospects in the field.      
### 36.HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.06328.pdf)
>  Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT) and wav2vec 2.0, has brought significant improvements in automatic speech recognition (ASR). However, these models usually require an expensive computational cost to achieve outstanding performance, slowing down the inference speed. To improve the model efficiency, we propose an early exit scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference dynamically. In HuBERT-EE, multiple early exit branches are added at the intermediate layers, and each branch is used to decide whether a prediction can be exited early. Experimental results on the LibriSpeech dataset show that HuBERT-EE can accelerate the inference of a large-scale HuBERT model while simultaneously balancing the trade-off between the word error rate (WER) performance and the latency.      
### 37.Call-sign recognition and understanding for noisy air-traffic transcripts using surveillance information  [ :arrow_down: ](https://arxiv.org/pdf/2204.06309.pdf)
>  Air traffic control (ATC) relies on communication via speech between pilot and air-traffic controller (ATCO). The call-sign, as unique identifier for each flight, is used to address a specific pilot by the ATCO. Extracting the call-sign from the communication is a challenge because of the noisy ATC voice channel and the additional noise introduced by the receiver. A low signal-to-noise ratio (SNR) in the speech leads to high word error rate (WER) transcripts. We propose a new call-sign recognition and understanding (CRU) system that addresses this issue. The recognizer is trained to identify call-signs in noisy ATC transcripts and convert them into the standard International Civil Aviation Organization (ICAO) format. By incorporating surveillance information, we can multiply the call-sign accuracy (CSA) up to a factor of four. The introduced data augmentation adds additional performance on high WER transcripts and allows the adaptation of the model to unseen airspaces.      
### 38.Overparameterized Linear Regression under Adversarial Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2204.06274.pdf)
>  As machine learning models start to be used in critical applications, their vulnerabilities and brittleness become a pressing concern. Adversarial attacks are a popular framework for studying these vulnerabilities. In this work, we study the error of linear regression in the face of adversarial attacks. We provide bounds of the error in terms of the traditional risk and the parameter norm and show how these bounds can be leveraged and make it possible to use analysis from non-adversarial setups to study the adversarial risk. The usefulness of these results is illustrated by shedding light on whether or not overparameterized linear models can be adversarially robust. We show that adding features to linear models might be either a source of additional robustness or brittleness. We show that these differences appear due to scaling and how the $\ell_1$ and $\ell_2$ norms of random projections concentrate. We also show how the reformulation we propose allows for solving adversarial training as a convex optimization problem. This is then used as a tool to study how adversarial training and other regularization methods might affect the robustness of the estimated models.      
### 39.Self-critical Sequence Training for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.06260.pdf)
>  Although automatic speech recognition (ASR) task has gained remarkable success by sequence-to-sequence models, there are two main mismatches between its training and testing that might lead to performance degradation: 1) The typically used cross-entropy criterion aims to maximize log-likelihood of the training data, while the performance is evaluated by word error rate (WER), not log-likelihood; 2) The teacher-forcing method leads to the dependence on ground truth during training, which means that model has never been exposed to its own prediction before testing. In this paper, we propose an optimization method called self-critical sequence training (SCST) to make the training procedure much closer to the testing phase. As a reinforcement learning (RL) based method, SCST utilizes a customized reward function to associate the training criterion and WER. Furthermore, it removes the reliance on teacher-forcing and harmonizes the model with respect to its inference procedure. We conducted experiments on both clean and noisy speech datasets, and the results show that the proposed SCST respectively achieves 8.7% and 7.8% relative improvements over the baseline in terms of WER.      
### 40.Physical layer security in large-scale random multiple access wireless sensor networks: a stochastic geometry approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.06257.pdf)
>  This paper investigates physical layer security for a large-scale WSN with random multiple access, where each fusion center in the network randomly schedules a number of sensors to upload their sensed data subject to the overhearing of randomly distributed eavesdroppers. We propose an uncoordinated random jamming scheme in which those unscheduled sensors send jamming signals with a certain probability to defeat the eavesdroppers. With the aid of stochastic geometry theory and order statistics, we derive analytical expressions for the connection outage probability and secrecy outage probability to characterize transmission reliability and secrecy, respectively. Based on the obtained analytical results, we formulate an optimization problem for maximizing the sum secrecy throughput subject to both reliability and secrecy constraints, considering a joint design of the wiretap code rates for each scheduled sensor and the jamming probability for the unscheduled sensors. We provide both optimal and low-complexity sub-optimal algorithms to tackle the above problem, and further reveal various properties on the optimal parameters which are useful to guide practical designs. In particular, we demonstrate that the proposed random jamming scheme is beneficial for improving the sum secrecy throughput, and the optimal jamming probability is the result of trade-off between secrecy and throughput. We also show that the throughput performance of the sub-optimal scheme approaches that of the optimal one when facing a stringent reliability constraint or a loose secrecy constraint.      
### 41.Performance Analysis of Wireless Network Aided by Discrete-Phase-Shifter IRS  [ :arrow_down: ](https://arxiv.org/pdf/2204.06230.pdf)
>  Discrete phase shifters of intelligent reflecting surface (IRS) generates phase quantization error (QE) and degrades the receive performance at the receiver. To make an analysis of the performance loss caused by IRS with phase QE, based on the law of large numbers, the closed-form expressions of signal-to-noise ratio (SNR) performance loss (PL), achievable rate (AR), and bit error rate (BER) are successively derived under line-of-sight (LoS) channels and Rayleigh channels. Moreover, based on the Taylor series expansion, the approximate simple closed form of PL of IRS with approximate QE is also given. The simulation results show that the performance losses of SNR and AR decrease as the number of quantization bits increase, while they gradually increase with the number of IRS phase shifter elements increase. Regardless of LoS channels or Rayleigh channels, when the number of quantization bits is larger than or equal to 3, the performance losses of SNR and AR are less than 0.23dB and 0.08bits/s/Hz, respectively, and the BER performance degradation is trivial. In particular, the performance loss difference between IRS with QE and IRS with approximate QE is negligible when the number of quantization bits is not less than 2.      
### 42.Rate Splitting Multiple Access Aided Mobile Edge Computing in Cognitive Radio Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.06208.pdf)
>  In this paper, we investigate rate splitting multiple access (RSMA) aided mobile edge computing (MEC) in a cognitive radio network. We propose a RSMA scheme that enables the secondary user to offload tasks to the MEC server utilizing dynamic rate splitting without deteriorating the primary user's offloading. The expressions for the optimal rate splitting parameters that maximize the achievable rate for the secondary user and successful computation probability of the proposed RSMA scheme are derived in closed-form. We formulate a problem to maximize successful computation probability by jointly optimizing task offloading ratio and task offloading time and obtain the optimal solutions in closed-form. Simulation results clarify that the proposed RSMA scheme achieves a higher successful computation probability than the existing non-orthogonal multiple access scheme.      
### 43.A Survey of Impedance Measurement Methods in Power Electronics  [ :arrow_down: ](https://arxiv.org/pdf/2204.06095.pdf)
>  Impedance is one of the vital parameters that provides useful information for many power electronics related applications. A lot of impedance measurement methods in power electronics have been reported. However, a comprehensive investigation among these methods in terms of their characteristics, advantages, and limitations has not been found in the literature. In order to bridge this gap, a survey of the impedance measurement methods is conducted in this paper. These methods are introduced, discussed, and then classified into different categories depending on the measurement modes, principles, and instruments. Moreover, recommendations for the future research on the impedance measurement are also presented.      
### 44.Optimal Control with Broken Symmetry of Multi-Agent Systems on Lie Groups  [ :arrow_down: ](https://arxiv.org/pdf/2204.06050.pdf)
>  In this paper we study reduction by symmetry for optimality conditions in optimal control problems of left-invariant affine multi-agent control systems, with partial symmetry breaking cost functions. Our approach emphasizes the role of variational principles. Specifically, we recast the optimal control problem as a constrained variational problem with a partial symmetry breaking Hamiltonian and obtain the reduced optimality conditions from a reduced variational principle via Pontryagin Maximum Principle. We apply the results to a collision avoidance problem for multiple unicycles in the presence of an obstacle.      
### 45.Avoiding Unintended Consequences: How Incentives Aid Information Provisioning in Bayesian Congestion Games  [ :arrow_down: ](https://arxiv.org/pdf/2204.06046.pdf)
>  When users lack specific knowledge of various system parameters, their uncertainty may lead them to make undesirable deviations in their decision making. To alleviate this, an informed system operator may elect to signal information to uninformed users with the hope of persuading them to take more preferable actions. In this work, we study public and truthful signalling mechanisms in the context of Bayesian congestion games on parallel networks. We provide bounds on the possible benefit a signalling policy can provide with and without the concurrent use of monetary incentives. We find that though revealing information can reduce system cost in some settings, it can also be detrimental and cause worse performance than not signalling at all. However, by utilizing both signalling and incentive mechanisms, the system operator can guarantee that revealing information does not worsen performance while offering similar opportunities for improvement. These findings emerge from the closed form bounds we derive on the benefit a signalling policy can provide. We provide a numerical example which illustrates the phenomenon that revealing more information can degrade performance when incentives are not used and improves performance when incentives are used.      
