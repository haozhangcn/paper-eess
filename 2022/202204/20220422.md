# ArXiv eess --Fri, 22 Apr 2022
### 1.Efficient Representations of Radiality Constraints in Optimization of Islanding and De-Energization in Distribution Grids  [ :arrow_down: ](https://arxiv.org/pdf/2204.10276.pdf)
>  Optimization of power distribution system topology is complicated by the requirement that the system be operated in a radial configuration. In this paper, we discuss existing methods for enforcing radiality constraints and introduce two new formulations that enable optimization over partially energized or islanded network topologies. The first builds on methods that use so-called parent-child constraints, but enforces those constraints on an abstracted network which enables an equivalent formulation with significantly less variables and constraints. The second formulation builds on existing approaches which directly generate constraints disallowing loops, and through an iterative approach seeks to limit the number of these constraints which must be enforced.      
### 2.The 2021 NIST Speaker Recognition Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2204.10242.pdf)
>  The 2021 Speaker Recognition Evaluation (SRE21) was the latest cycle of the ongoing evaluation series conducted by the U.S. National Institute of Standards and Technology (NIST) since 1996. It was the second large-scale multimodal speaker/person recognition evaluation organized by NIST (the first one being SRE19). Similar to SRE19, it featured two core evaluation tracks, namely audio and audio-visual, as well as an optional visual track. In addition to offering fixed and open training conditions, it also introduced new challenges for the community, thanks to a new multimodal (i.e., audio, video, and selfie images) and multilingual (i.e., with multilingual speakers) corpus, termed WeCanTalk, collected outside North America by the Linguistic Data Consortium (LDC). These challenges included: 1) trials (target and non-target) with enrollment and test segments originating from different domains (i.e., telephony versus video), and 2) trials (target and non-target) with enrollment and test segments spoken in different languages (i.e., cross-lingual trials). This paper presents an overview of SRE21 including the tasks, performance metric, data, evaluation protocol, results and system performance analyses. A total of 23 organizations (forming 15 teams) from academia and industry participated in SRE21 and submitted 158 valid system outputs. Evaluation results indicate: audio-visual fusion produce substantial gains in performance over audio-only or visual-only systems; top performing speaker and face recognition systems exhibited comparable performance under the matched domain conditions present in this evaluation; and, the use of complex neural network architectures (e.g., ResNet) along with angular losses with margin, data augmentation, as well as long duration fine-tuning contributed to notable performance improvements for the audio-only speaker recognition task.      
### 3.The NIST CTS Speaker Recognition Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2204.10228.pdf)
>  The US National Institute of Standards and Technology (NIST) has been conducting a second iteration of the CTS challenge since August 2020. The current iteration of the CTS Challenge is a leaderboard-style speaker recognition evaluation using telephony data extracted from the unexposed portions of the Call My Net 2 (CMN2) and Multi-Language Speech (MLS) corpora collected by the LDC. The CTS Challenge is currently organized in a similar manner to the SRE19 CTS Challenge, offering only an open training condition using two evaluation subsets, namely Progress and Test. Unlike in the SRE19 Challenge, no training or development set was initially released, and NIST has publicly released the leaderboards on both subsets for the CTS Challenge. Which subset (i.e., Progress or Test) a trial belongs to is unknown to challenge participants, and each system submission needs to contain outputs for all of the trials. The CTS Challenge has also served, and will continue to do so, as a prerequisite for entrance to the regular SREs (such as SRE21). Since August 2020, a total of 53 organizations (forming 33 teams) from academia and industry have participated in the CTS Challenge and submitted more than 4400 valid system outputs. This paper presents an overview of the evaluation and several analyses of system performance for some primary conditions in the CTS Challenge. The CTS Challenge results thus far indicate remarkable improvements in performance due to 1) speaker embeddings extracted using large-scale and complex neural network architectures such as ResNets along with angular margin losses for speaker embedding extraction, 2) extensive data augmentation, 3) the use of large amounts of in-house proprietary data from a large number of labeled speakers, 4) long-duration fine-tuning.      
### 4.OCTOPUS -- optical coherence tomography plaque and stent analysis software  [ :arrow_down: ](https://arxiv.org/pdf/2204.10212.pdf)
>  Compared with other imaging modalities, intravascular optical coherence tomography (IVOCT) has significant advantages for guiding percutaneous coronary interventions. To aid IVOCT research studies, we developed the Optical Coherence TOmography PlaqUe and Stent (OCTOPUS) analysis software. To automate image analysis results, the software includes several important algorithmic steps: pre-processing, deep learning plaque segmentation, machine learning identification of stent struts, and registration of pullbacks. Interactive visualization and manual editing of segmentations were included in the software. Quantifications include stent deployment characteristics (e.g., stent strut malapposition), strut level analysis, calcium angle, and calcium thickness measurements. Interactive visualizations include (x,y) anatomical, en face, and longitudinal views with optional overlays. Underlying plaque segmentation algorithm yielded excellent pixel-wise results (86.2% sensitivity and 0.781 F1 score). Using OCTOPUS on 34 new pullbacks, we determined that following automated segmentation, only 13% and 23% of frames needed any manual touch up for detailed lumen and calcification labeling, respectively. Only up to 3.8% of plaque pixels were modified, leading to an average editing time of only 7.5 seconds/frame, an approximately 80% reduction compared to manual analysis. Regarding stent analysis, sensitivity and precision were both greater than 90%, and each strut was successfully classified as either covered or uncovered with high sensitivity (94%) and specificity (90%). We introduced and evaluated the clinical application of a highly automated software package, OCTOPUS, for quantitative plaque and stent analysis in IVOCT images. The software is currently used as an offline tool for research purposes; however, the software's embedded algorithms may also be useful for real-time treatment planning.      
### 5.Analysis of Ferroelectric Negative Capacitance-Hybrid MEMS Actuator Using Energy-Displacement Landscape  [ :arrow_down: ](https://arxiv.org/pdf/2204.10180.pdf)
>  We propose an energy-based framework to analyze the statics and dynamics of a ferroelectric negative capacitance-hybrid Microelectromechanical System (MEMS) actuator. A mapping function that relates the charge on the ferroelectric to displacement of the movable electrode, is used to obtain the Hamiltonian of the hybrid actuator in terms of displacement. We then use graphical energy-displacement and phase portrait plots to analyze static pull-in, dynamic pull-in and pull-out phenomena of the hybrid actuator. Using these, we illustrate the low-voltage operation of the hybrid actuator to static and step inputs, as compared to the standalone MEMS actuator. The results obtained are in agreement with the analytical predictions and numerical simulations. The proposed framework enables straightforward inclusion of adhesion between the contacting surfaces, modeled using van der Waals force. We show that the pull-in voltage is not affected, while the pull-out voltage is reduced due to adhesion. The proposed framework provides a physics-based tool to design and analyze negative capacitance based low-voltage MEMS actuators.      
### 6.Gated Multimodal Fusion with Contrastive Learning for Turn-taking Prediction in Human-robot Dialogue  [ :arrow_down: ](https://arxiv.org/pdf/2204.10172.pdf)
>  Turn-taking, aiming to decide when the next speaker can start talking, is an essential component in building human-robot spoken dialogue systems. Previous studies indicate that multimodal cues can facilitate this challenging task. However, due to the paucity of public multimodal datasets, current methods are mostly limited to either utilizing unimodal features or simplistic multimodal ensemble models. Besides, the inherent class imbalance in real scenario, e.g. sentence ending with short pause will be mostly regarded as the end of turn, also poses great challenge to the turn-taking decision. In this paper, we first collect a large-scale annotated corpus for turn-taking with over 5,000 real human-robot dialogues in speech and text modalities. Then, a novel gated multimodal fusion mechanism is devised to utilize various information seamlessly for turn-taking prediction. More importantly, to tackle the data imbalance issue, we design a simple yet effective data augmentation method to construct negative instances without supervision and apply contrastive learning to obtain better feature representations. Extensive experiments are conducted and the results demonstrate the superiority and competitiveness of our model over several state-of-the-art baselines.      
### 7.A Bitstream Feature Based Model for Video Decoding Energy Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2204.10151.pdf)
>  In this paper we show that a small amount of bit stream features can be used to accurately estimate the energy consumption of state-of-the-art software and hardware accelerated decoder implementations for four different video codecs. By testing the estimation performance on HEVC, H.264, H.263, and VP9 we show that the proposed model can be used for any hybrid video codec. We test our approach on a high amount of different test sequences to prove the general validity. We show that less than 20 features are sufficient to obtain mean estimation errors that are smaller than 8%. Finally, an example will show the performance trade-offs in terms of rate, distortion, and decoding energy for all tested codecs.      
### 8.Multiple EffNet/ResNet Architectures for Melanoma Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.10142.pdf)
>  Melanoma is the most malignant skin tumor and usually cancerates from normal moles, which is difficult to distinguish benign from malignant in the early stage. Therefore, many machine learning methods are trying to make auxiliary prediction. However, these methods attach more attention to the image data of suspected tumor, and focus on improving the accuracy of image classification, but ignore the significance of patient-level contextual information for disease diagnosis in actual clinical diagnosis. To make more use of patient information and improve the accuracy of diagnosis, we propose a new melanoma classification model based on EffNet and Resnet. Our model not only uses images within the same patient but also consider patient-level contextual information for better cancer prediction. The experimental results demonstrated that the proposed model achieved 0.981 ACC. Furthermore, we note that the overall ROC value of the model is 0.976 which is better than the previous state-of-the-art approaches.      
### 9.Reconfigurable Intelligent Surface for Near Field Communications: Beamforming and Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2204.10114.pdf)
>  Reconfigurable intelligent surface (RIS) can improve the communications between a source and a destination. The surface contains metamaterial that is configured to reflect the incident wave from the source towards the destination, especially when there is a blockage in between. Recently, continuous aperture RIS is proved to have better communication performance than discrete aperture RIS and has received much attention. However, the conventional continuous aperture RIS is designed to convert the incoming planar waves into the outgoing planar waves, which is not the optimal reflecting scheme when the receiver is not a planar array and is located in the near field of the RIS. In this paper, we consider two types of receivers in the radiating near field of the RIS: (1) when the receiver is equipped with a uniform linear array (ULA), we design RIS coefficient to convert planar waves into cylindrical waves; (2) when the receiver is equipped with a single antenna, we design RIS coefficient to convert planar waves into spherical waves. Simulation results demonstrate that the proposed scheme can reduce energy leakage at the receiver and thus enhance the channel capacity compared to the conventional scheme. More interestingly, with cylindrical or spherical wave radiation, the power received by the receiver is a function of its location and attitude, which could be utilized to sense the location and the attitude of the receiver with communication signaling.      
### 10.Generative Compression for Face Video: A Hybrid Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2204.10055.pdf)
>  As the latest video coding standard, versatile video coding (VVC) has shown its ability in retaining pixel quality. To excavate more compression potential for video conference scenarios under ultra-low bitrate, this paper proposes a bitrate adjustable hybrid compression scheme for face video. This hybrid scheme combines the pixel-level precise recovery capability of traditional coding with the generation capability of deep learning based on abridged information, where Pixel wise Bi-Prediction, Low-Bitrate-FOM and Lossless Keypoint Encoder collaborate to achieve PSNR up to 36.23 dB at a low bitrate of 1.47 KB/s. Without introducing any additional bitrate, our method has a clear advantage over VVC under a completely fair comparative experiment, which proves the effectiveness of our proposed scheme. Moreover, our scheme can adapt to any existing encoder / configuration to deal with different encoding requirements, and the bitrate can be dynamically adjusted according to the network condition.      
### 11.Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.10020.pdf)
>  Data augmentation via voice conversion (VC) has been successfully applied to low-resource expressive text-to-speech (TTS) when only neutral data for the target speaker are available. Although the quality of VC is crucial for this approach, it is challenging to learn a stable VC model because the amount of data is limited in low-resource scenarios, and highly expressive speech has large acoustic variety. To address this issue, we propose a novel data augmentation method that combines pitch-shifting and VC techniques. Because pitch-shift data augmentation enables the coverage of a variety of pitch dynamics, it greatly stabilizes training for both VC and TTS models, even when only 1,000 utterances of the target speaker's neutral data are available. Subjective test results showed that a FastSpeech 2-based emotional TTS system with the proposed method improved naturalness and emotional similarity compared with conventional methods.      
### 12.On Learning the Invisible in Photoacoustic Tomography with Flat Directionally Sensitive Detector  [ :arrow_down: ](https://arxiv.org/pdf/2204.10001.pdf)
>  In photoacoustic tomography (PAT) with flat sensor, we routinely encounter two types of limited data. The first is due to using a finite sensor and is especially perceptible if the region of interest is large relatively to the sensor or located farther away from the sensor. In this paper, we focus on the second type caused by a varying sensitivity of the sensor to the incoming wavefront direction which can be modelled as binary i.e. by a cone of sensitivity. Such visibility conditions result, in Fourier domain, in a restriction of both the image and the data to a bowtie, akin to the one corresponding to the range of the forward operator. The visible ranges, in image and data domains, are related by the wavefront direction mapping. We adapt the wedge restricted Curvelet decomposition, we previously proposed for the representation of the full PAT data, to separate the visible and invisible wavefronts in the image. We optimally combine fast approximate operators with tailored deep neural network architectures into efficient learned reconstruction methods which perform reconstruction of the visible coefficients and the invisible coefficients are learned from a training set of similar data.      
### 13.Dynamic Tomography Reconstruction by Projection-Domain Separable Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2204.09935.pdf)
>  In dynamic tomography the object undergoes changes while projections are being acquired sequentially in time. The resulting inconsistent set of projections cannot be used directly to reconstruct an object corresponding to a time instant. Instead, the objective is to reconstruct a spatio-temporal representation of the object, which can be displayed as a movie. We analyze conditions for unique and stable solution of this ill-posed inverse problem, and present a recovery algorithm, validating it experimentally. We compare our approach to one based on the recently proposed GMLR variation on deep prior for video, demonstrating the advantages of the proposed approach.      
### 14.FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2204.09934.pdf)
>  Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hindered their applications to speech synthesis. This paper proposes FastDiff, a fast conditional diffusion model for high-quality speech synthesis. FastDiff employs a stack of time-aware location-variable convolutions of diverse receptive field patterns to efficiently model long-term time dependencies with adaptive conditions. A noise schedule predictor is also adopted to reduce the sampling steps without sacrificing the generation quality. Based on FastDiff, we design an end-to-end text-to-speech synthesizer, FastDiff-TTS, which generates high-fidelity speech waveforms without any intermediate feature (e.g., Mel-spectrogram). Our evaluation of FastDiff demonstrates the state-of-the-art results with higher-quality (MOS 4.28) speech samples. Also, FastDiff enables a sampling speed of 58x faster than real-time on a V100 GPU, making diffusion models practically applicable to speech synthesis deployment for the first time. We further show that FastDiff generalized well to the mel-spectrogram inversion of unseen speakers, and FastDiff-TTS outperformed other competing methods in end-to-end text-to-speech synthesis. Audio samples are available at \url{<a class="link-external link-https" href="https://FastDiff.github.io/" rel="external noopener nofollow">this https URL</a>}.      
### 15.An Efficient End-to-End Deep Neural Network for Interstitial Lung Disease Recognition and Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.09909.pdf)
>  The automated Interstitial Lung Diseases (ILDs) classification technique is essential for assisting clinicians during the diagnosis process. Detecting and classifying ILDs patterns is a challenging problem. This paper introduces an end-to-end deep convolution neural network (CNN) for classifying ILDs patterns. The proposed model comprises four convolutional layers with different kernel sizes and Rectified Linear Unit (ReLU) activation function, followed by batch normalization and max-pooling with a size equal to the final feature map size well as four dense layers. We used the ADAM optimizer to minimize categorical cross-entropy. A dataset consisting of 21328 image patches of 128 CT scans with five classes is taken to train and assess the proposed model. A comparison study showed that the presented model outperformed pre-trained CNNs and five-fold cross-validation on the same dataset. For ILDs pattern classification, the proposed approach achieved the accuracy scores of 99.09% and the average F score of 97.9%, outperforming three pre-trained CNNs. These outcomes show that the proposed model is relatively state-of-the-art in precision, recall, f score, and accuracy.      
### 16.Multi-Tier Platform for Cognizing Massive Electroencephalogram  [ :arrow_down: ](https://arxiv.org/pdf/2204.09840.pdf)
>  An end-to-end platform assembling multiple tiers is built for precisely cognizing brain activities. Being fed massive electroencephalogram (EEG) data, the time-frequency spectrograms are conventionally projected into the episode-wise feature matrices (seen as tier-1). A spiking neural network (SNN) based tier is designed to distill the principle information in terms of spike-streams from the rare features, which maintains the temporal implication in the nature of EEGs. The proposed tier-3 transposes time- and space-domain of spike patterns from the SNN; and feeds the transposed pattern-matrices into an artificial neural network (ANN, Transformer specifically) known as tier-4, where a special spanning topology is proposed to match the two-dimensional input form. In this manner, cognition such as classification is conducted with high accuracy. For proof-of-concept, the sleep stage scoring problem is demonstrated by introducing multiple EEG datasets with the largest comprising 42,560 hours recorded from 5,793 subjects. From experiment results, our platform achieves the general cognition overall accuracy of 87% by leveraging sole EEG, which is 2% superior to the state-of-the-art. Moreover, our developed multi-tier methodology offers visible and graphical interpretations of the temporal characteristics of EEG by identifying the critical episodes, which is demanded in neurodynamics but hardly appears in conventional cognition scenarios.      
### 17.Robust Phase Retrieval via Reverse Kullback-Leibler Divergence and Wirtinger Flow  [ :arrow_down: ](https://arxiv.org/pdf/2204.09791.pdf)
>  Robustness to noise and outliers is a desirable trait in phase retrieval algorithms for many applications in imaging and signal processing. In this paper, we develop a novel robust phase retrieval algorithm based on the minimization of reverse Kullback-Leibler divergence (RKLD) within the Wirtinger Flow (WF) framework. We use RKLD over intensity-only measurements in two distinct ways: i) to design a novel initial estimate based on minimum distortion design of spectral estimates, and ii) as a loss function for iterative refinement based on WF. The RKLD-based loss function offers implicit regularization by processing data at the logarithmic scale and provides the following benefits: suppressing the influence of large magnitude errors and promoting projections orthogonal to noise subspace. We present three algorithms based on RKLD minimization, including two with truncation schemes to enhance the robustness to significant contamination. Our numerical study demonstrates the advantages of our algorithms in terms of sample efficiency, convergence speed, and robustness to outliers over the state-of-the-art techniques using both synthetic and real optical imaging data.      
### 18.MultiPathGAN: Structure Preserving Stain Normalization using Unsupervised Multi-domain Adversarial Network with Perception Loss  [ :arrow_down: ](https://arxiv.org/pdf/2204.09782.pdf)
>  Histopathology relies on the analysis of microscopic tissue images to diagnose disease. A crucial part of tissue preparation is staining whereby a dye is used to make the salient tissue components more distinguishable. However, differences in laboratory protocols and scanning devices result in significant confounding appearance variation in the corresponding images. This variation increases both human error and the inter-rater variability, as well as hinders the performance of automatic or semi-automatic methods. In the present paper we introduce an unsupervised adversarial network to translate (and hence normalize) whole slide images across multiple data acquisition domains. Our key contributions are: (i) an adversarial architecture which learns across multiple domains with a single generator-discriminator network using an information flow branch which optimizes for perceptual loss, and (ii) the inclusion of an additional feature extraction network during training which guides the transformation network to keep all the structural features in the tissue image intact. We: (i) demonstrate the effectiveness of the proposed method firstly on H\&amp;E slides of 120 cases of kidney cancer, as well as (ii) show the benefits of the approach on more general problems, such as flexible illumination based natural image enhancement and light source adaptation.      
### 19.Delamination prediction in composite panels using unsupervised-feature learning methods with wavelet-enhanced guided wave representations  [ :arrow_down: ](https://arxiv.org/pdf/2204.09764.pdf)
>  With the introduction of damage tolerance-based design philosophies, the demand for reliable and robust structural health monitoring (SHM) procedures for aerospace composite structures is increasing rapidly. The performance of supervised learning algorithms for SHM depends on the amount of labeled and balanced datasets. Apart from this, collecting datasets accommodating all possible damage scenarios is cumbersome, costly, and inaccessible for aerospace applications. In this paper, we have proposed two different unsupervised-feature learning approaches where the algorithms are trained only on the baseline scenarios to learn the distribution of baseline signals. The trained unsupervised feature learner is used for delamination prediction with an anomaly detection philosophy. In the first approach, we have combined dimensionality reduction techniques (principal component analysis and independent component analysis) with a one-class support vector machine. In another approach, we have utilized deep learning-based deep convolutional autoencoders (CAE). These state-of-the-art algorithms are applied on three different guided wave-based experimental datasets. The raw guided wave signals present in the datasets are converted into wavelet-enhanced higher-order representations for training unsupervised feature-learning algorithms. We have also compared different techniques, and it is seen that CAE generates better reconstructions with lower mean squared error and can provide higher accuracy on all the datasets.      
### 20.A Generalized Distributed Analysis and Control Synthesis Approach for Networked Systems with Arbitrary Interconnections  [ :arrow_down: ](https://arxiv.org/pdf/2204.09756.pdf)
>  We consider the problem of distributed analysis and control synthesis to verify and ensure properties like stability and dissipativity of a large-scale networked system comprised of linear subsystems interconnected in an arbitrary topology. In particular, we design systematic networked system analysis and control synthesis processes that can be executed in a distributed manner at the subsystem level with minimal information sharing among the subsystems. Compared to a recent work on the same topic, we consider a substantially more generalized problem setup and develop distributed processes to verify and ensure a broader range of networked system properties. We also show that optimizing subsystems' indexing scheme used in such distributed processes can substantially reduce the required information-sharing sessions between subsystems. Moreover, the proposed networked system analysis and control synthesis processes are compositional and thus allow them to conveniently and efficiently handle situations where new subsystems are being added to an existing network. We also provide significant insights into our approach so that it can be quickly adopted to verify and ensure properties beyond the stability and dissipativity of networked systems. Finally, we provide several simulation results to demonstrate the proposed distributed analysis and control synthesis processes.      
### 21.Joint state and parameter estimation based on constrained zonotopes  [ :arrow_down: ](https://arxiv.org/pdf/2204.09740.pdf)
>  This note presents a new method for set-based joint state and parameter estimation of discrete-time systems using constrained zonotopes. This is done by extending previous set-based state estimation methods to include parameter identification in a unified framework. Unlike in interval-based methods, the existing dependencies between states and model parameters are maintained from one time step to the next, thus providing a more accurate estimation scheme. In addition, the enclosure of states and parameters is refined using measurements through generalized intersections, which are properly captured by constrained zonotopes. The advantages of the new approach are highlighted in two numerical examples.      
### 22.Vehicle Models and Optimal Control on a Nonplanar Surface  [ :arrow_down: ](https://arxiv.org/pdf/2204.09720.pdf)
>  We present a 10 DoF dynamic vehicle model for model-based control on nonplanar road surfaces. A parametric surface is used to describe the road surface, allowing the surface parameterization to describe the pose of the vehicle. We use the proposed approach to compute minimum-time vehicle trajectories on nonplanar surfaces and compare planar and nonplanar models.      
### 23.Smart Interference Management xApp using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.09707.pdf)
>  Interference continues to be a key limiting factor in cellular radio access network (RAN) deployments. Effective, data-driven, self-adapting radio resource management (RRM) solutions are essential for tackling interference, and thus achieving the desired performance levels particularly at the cell-edge. In future network architecture, RAN intelligent controller (RIC) running with near-real-time applications, called xApps, is considered as a potential component to enable RRM. In this paper, based on deep reinforcement learning (RL) xApp, a joint sub-band masking and power management is proposed for smart interference management. The sub-band resource masking problem is formulated as a Markov Decision Process (MDP) that can be solved employing deep RL to approximate the policy functions as well as to avoid extremely high computational and storage costs of conventional tabular-based approaches. The developed xApp is scalable in both storage and computation. Simulation results demonstrate advantages of the proposed approach over decentralized baselines in terms of the trade-off between cell-centre and cell-edge user rates, energy efficiency and computational efficiency.      
### 24.Spatially-Preserving Flattening for Location-Aware Classification of Findings in Chest X-Rays  [ :arrow_down: ](https://arxiv.org/pdf/2204.09676.pdf)
>  Chest X-rays have become the focus of vigorous deep learning research in recent years due to the availability of large labeled datasets. While classification of anomalous findings is now possible, ensuring that they are correctly localized still remains challenging, as this requires recognition of anomalies within anatomical regions. Existing deep learning networks for fine-grained anomaly classification learn location-specific findings using architectures where the location and spatial contiguity information is lost during the flattening step before classification. In this paper, we present a new spatially preserving deep learning network that preserves location and shape information through auto-encoding of feature maps during flattening. The feature maps, auto-encoder and classifier are then trained in an end-to-end fashion to enable location aware classification of findings in chest X-rays. Results are shown on a large multi-hospital chest X-ray dataset indicating a significant improvement in the quality of finding classification over state-of-the-art methods.      
### 25.DooDLeNet: Double DeepLab Enhanced Feature Fusion for Thermal-color Semantic Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.10266.pdf)
>  In this paper we present a new approach for feature fusion between RGB and LWIR Thermal images for the task of semantic segmentation for driving perception. We propose DooDLeNet, a double DeepLab architecture with specialized encoder-decoders for thermal and color modalities and a shared decoder for final segmentation. We combine two strategies for feature fusion: confidence weighting and correlation weighting. We report state-of-the-art mean IoU results on the MF dataset.      
### 26.A Real-time Calculus Approach for Integrating Sporadic Events in Time-triggered Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.10264.pdf)
>  In time-triggered systems, where the schedule table is predefined and statically configured at design time, sporadic event-triggered (ET) tasks can only be handled within specially dedicated slots or when time-triggered (TT) tasks finish their execution early. We introduce a new paradigm for synthesizing TT schedules that guarantee the correct temporal behavior of TT tasks and the schedulability of sporadic ET tasks with arbitrary deadlines. The approach first expresses a constraint for the TT task schedule in the form of a maximal affine envelope that guarantees that as long as the schedule generation respects this envelope, all sporadic ET tasks meet their deadline. The second step consists of modeling this envelope as a burst limiting constraint and building the TT schedule via simulating a modified Least-Laxity-First (LLF) scheduler. Using this novel technique, we show that we achieve equal or better schedulability and a faster schedule generation for most use-cases compared to other approaches inspired by, e.g., hierarchical scheduling. Moreover, we present an extension to our method that finds the most favourable schedule for TT tasks with respect to ET schedulability, thus increasing the probability of the computed TT schedule remaining feasible when ET tasks are later added or changed.      
### 27.Flexible and dependable manufacturing beyond xURLLC: A novel framework for communication-control co-design  [ :arrow_down: ](https://arxiv.org/pdf/2204.10197.pdf)
>  Future Industrial 4.0 applications in the 6G era is calling for high dependability that goes far beyond the current ultra-reliable low latency communication (URLLC), and therewith proposed critical challenges to the communication technology. Instead of struggling against the physical and technical limits towards an extreme URLLC (xURLLC), communication-control co-design (CoCoCo) appears a more promising solution. This work proposes a novel framework of CoCoCo, which is not only enhancing the dependability of 6G industrial applications such as remote control, but also exhibiting rich potential in revolutionizing the future industry per openness and flexibility of manufacturing systems.      
### 28.Scale Dependencies and Self-Similarity Through Wavelet Scattering Covariance  [ :arrow_down: ](https://arxiv.org/pdf/2204.10177.pdf)
>  We introduce a scattering covariance matrix which provides non-Gaussian models of time-series having stationary increments. A complex wavelet transform computes signal variations at each scale. Dependencies across scales are captured by the joint covariance across time and scales of complex wavelet coefficients and their modulus. This covariance is nearly diagonalized by a second wavelet transform, which defines the scattering covariance. We show that this set of moments characterizes a wide range of non-Gaussian properties of multi-scale processes. This is analyzed for a variety of processes, including fractional Brownian motions, Poisson, multifractal random walks and Hawkes processes. We prove that self-similar processes have a scattering covariance matrix which is scale invariant. This property can be estimated numerically and defines a class of wide-sense self-similar processes. We build maximum entropy models conditioned by scattering covariance coefficients, and generate new time-series with a microcanonical sampling algorithm. Applications are shown for highly non-Gaussian financial and turbulence time-series.      
### 29.Surface waves prediction based on long-range acoustic backscattering in a mid-frequency range  [ :arrow_down: ](https://arxiv.org/pdf/2204.10153.pdf)
>  New data was obtained for a frequency band that had not been so well-studied for sea surface probing applications before. During the described 2-weeks sea experiment 1-3 kHz tonal pulses were emitted from a platform, located on the northern Black Sea shelf, and Doppler spectrum of reverberation was studied. We believe that this band is worth further studying due the sound propagation range is large enough to meet practical needs in coastal zone while the angle-distance resolution is quite moderate. However it is quite difficult to interpret the obtained data since backscattering spectrum shape is influenced by a series of effects and has a complicated link to wind waves and currents parameters. Backscattering of acoustical signals was received for distances around 2 nautical miles. Significant wave height, dominant wave frequency were estimated as the result of such signals processing with the use of machine learning tools. A decision-tree-based mathematical regression model was trained to solve the inverse problem. Wind waves prediction is in a good agreement with direct measurements, made on the platform, and machine learning results allow physical interpretation.      
### 30.Physical Modeling using Recurrent Neural Networks with Fast Convolutional Layers  [ :arrow_down: ](https://arxiv.org/pdf/2204.10125.pdf)
>  Discrete-time modeling of acoustic, mechanical and electrical systems is a prominent topic in the musical signal processing literature. Such models are mostly derived by discretizing a mathematical model, given in terms of ordinary or partial differential equations, using established techniques. Recent work has applied the techniques of machine-learning to construct such models automatically from data for the case of systems which have lumped states described by scalar values, such as electrical circuits. In this work, we examine how similar techniques are able to construct models of systems which have spatially distributed rather than lumped states. We describe several novel recurrent neural network structures, and show how they can be thought of as an extension of modal techniques. As a proof of concept, we generate synthetic data for three physical systems and show that the proposed network structures can be trained with this data to reproduce the behavior of these systems.      
### 31.Deep Model-Based Super-Resolution with Non-uniform Blur  [ :arrow_down: ](https://arxiv.org/pdf/2204.10109.pdf)
>  We propose a state-of-the-art method for super-resolution with non-uniform blur. Single-image super-resolution methods seek to restore a high-resolution image from blurred, subsampled, and noisy measurements. Despite their impressive performance, existing techniques usually assume a uniform blur kernel. Hence, these techniques do not generalize well to the more general case of non-uniform blur. Instead, in this paper, we address the more realistic and computationally challenging case of spatially-varying blur. To this end, we first propose a fast deep plug-and-play algorithm, based on linearized ADMM splitting techniques, which can solve the super-resolution problem with spatially-varying blur. Second, we unfold our iterative algorithm into a single network and train it end-to-end. In this way, we overcome the intricacy of manually tuning the parameters involved in the optimization scheme. Our algorithm presents remarkable performance and generalizes well after a single training to a large family of spatially-varying blur kernels, noise levels and scale factors.      
### 32.GAF-NAU: Gramian Angular Field encoded Neighborhood Attention U-Net for Pixel-Wise Hyperspectral Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2204.10099.pdf)
>  Hyperspectral image (HSI) classification is the most vibrant area of research in the hyperspectral community due to the rich spectral information contained in HSI can greatly aid in identifying objects of interest. However, inherent non-linearity between materials and the corresponding spectral profiles brings two major challenges in HSI classification: interclass similarity and intraclass variability. Many advanced deep learning methods have attempted to address these issues from the perspective of a region/patch-based approach, instead of a pixel-based alternate. However, the patch-based approaches hypothesize that neighborhood pixels of a target pixel in a fixed spatial window belong to the same class. And this assumption is not always true. To address this problem, we herein propose a new deep learning architecture, namely Gramian Angular Field encoded Neighborhood Attention U-Net (GAF-NAU), for pixel-based HSI classification. The proposed method does not require regions or patches centered around a raw target pixel to perform 2D-CNN based classification, instead, our approach transforms 1D pixel vector in HSI into 2D angular feature space using Gramian Angular Field (GAF) and then embed it to a new neighborhood attention network to suppress irrelevant angular feature while emphasizing on pertinent features useful for HSI classification task. Evaluation results on three publicly available HSI datasets demonstrate the superior performance of the proposed model.      
### 33.Learn from Unpaired Data for Image Restoration: A Variational Bayes Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.10090.pdf)
>  Collecting paired training data is difficult in practice, but the unpaired samples broadly exist. Current approaches aim at generating synthesized training data from the unpaired samples by exploring the relationship between the corrupted and clean data. This work proposes LUD-VAE, a deep generative method to learn the joint probability density function from data sampled from marginal distributions. Our approach is based on a carefully designed probabilistic graphical model in which the clean and corrupted data domains are conditionally independent. Using variational inference, we maximize the evidence lower bound (ELBO) to estimate the joint probability density function. Furthermore, we show that the ELBO is computable without paired samples under the inference invariant assumption. This property provides the mathematical rationale of our approach in the unpaired setting. Finally, we apply our method to real-world image denoising and super-resolution tasks and train the models using the synthetic data generated by the LUD-VAE. Experimental results validate the advantages of our method over other learnable approaches.      
### 34.A two-level machine learning framework for predictive maintenance: comparison of learning formulations  [ :arrow_down: ](https://arxiv.org/pdf/2204.10083.pdf)
>  Predicting incoming failures and scheduling maintenance based on sensors information in industrial machines is increasingly important to avoid downtime and machine failure. Different machine learning formulations can be used to solve the predictive maintenance problem. However, many of the approaches studied in the literature are not directly applicable to real-life scenarios. Indeed, many of those approaches usually either rely on labelled machine malfunctions in the case of classification and fault detection, or rely on finding a monotonic health indicator on which a prediction can be made in the case of regression and remaining useful life estimation, which is not always feasible. Moreover, the decision-making part of the problem is not always studied in conjunction with the prediction phase. This paper aims to design and compare different formulations for predictive maintenance in a two-level framework and design metrics that quantify both the failure detection performance as well as the timing of the maintenance decision. The first level is responsible for building a health indicator by aggregating features using a learning algorithm. The second level consists of a decision-making system that can trigger an alarm based on this health indicator. Three degrees of refinements are compared in the first level of the framework, from simple threshold-based univariate predictive technique to supervised learning methods based on the remaining time before failure. We choose to use the Support Vector Machine (SVM) and its variations as the common algorithm used in all the formulations. We apply and compare the different strategies on a real-world rotating machine case study and observe that while a simple model can already perform well, more sophisticated refinements enhance the predictions for well-chosen parameters.      
### 35.Multi-UAV trajectory planning for 3D visual inspection of complex structures  [ :arrow_down: ](https://arxiv.org/pdf/2204.10070.pdf)
>  This paper presents a new trajectory planning algorithm for 3D autonomous UAV volume coverage and visual inspection. The algorithm is an extension of a state-of-the-art Heat Equation Driven Area Coverage (HEDAC) multi-agent area coverage algorithm for 3D domains. With a given target exploration density field, the algorithm designs a potential field and directs UAVs to the regions of higher potential, i.e., higher values of remaining density. Collisions between the agents and agents with domain boundaries are prevented by implementing the distance field and correcting the agent's directional vector when the distance threshold is reached. A unit cube test case is considered to evaluate this trajectory planning strategy for volume coverage. For visual inspection applications, the algorithm is supplemented with camera direction control. A field containing the nearest distance from any point in the domain to the structure surface is designed. The gradient of this field is calculated to obtain the camera orientation throughout the trajectory. Three different test cases of varying complexities are considered to validate the proposed method for visual inspection. The simplest scenario is a synthetic portal-like structure inspected using three UAVs. The other two inspection scenarios are based on realistic structures where UAVs are commonly utilized: a wind turbine and a bridge. When deployed to a wind turbine inspection, two simulated UAVs traversing smooth spiral trajectories have successfully explored the entire turbine structure while cameras are directed to the curved surfaces of the turbine's blades. In the bridge test case an efficacious visual inspection of a complex structure is demonstrated by employing a single UAV and five UAVs. The proposed methodology is successful, flexible and applicable in real-world UAV inspection tasks.      
### 36.Stability, Linear Convergence, and Robustness of the Wang-Elia Algorithm for Distributed Consensus Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.10030.pdf)
>  We revisit an algorithm for distributed consensus optimization proposed in 2010 by J. Wang and N. Elia. By means of a Lyapunov-based analysis, we prove input-to-state stability of the algorithm relative to a closed invariant set composed of optimal equilibria and with respect to perturbations affecting the algorithm's dynamics. In the absence of perturbations, this result implies linear convergence of the local estimates and Lyapunov stability of the optimal steady state. Moreover, we unveil fundamental connections with the well-known Gradient Tracking and with distributed integral control. Overall, our results suggest that a control theoretic approach can have a considerable impact on (distributed) optimization, especially when robustness is considered.      
### 37.Baseline Systems for the First Spoofing-Aware Speaker Verification Challenge: Score and Embedding Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2204.09976.pdf)
>  Deep learning has brought impressive progress in the study of both automatic speaker verification (ASV) and spoofing countermeasures (CM). Although solutions are mutually dependent, they have typically evolved as standalone sub-systems whereby CM solutions are usually designed for a fixed ASV system. The work reported in this paper aims to gauge the improvements in reliability that can be gained from their closer integration. Results derived using the popular ASVspoof2019 dataset indicate that the equal error rate (EER) of a state-of-the-art ASV system degrades from 1.63% to 23.83% when the evaluation protocol is extended with spoofed trials.%subjected to spoofing attacks. However, even the straightforward integration of ASV and CM systems in the form of score-sum and deep neural network-based fusion strategies reduce the EER to 1.71% and 6.37%, respectively. The new Spoofing-Aware Speaker Verification (SASV) challenge has been formed to encourage greater attention to the integration of ASV and CM systems as well as to provide a means to benchmark different solutions.      
### 38.ChildPredictor: A Child Face Prediction Framework with Disentangled Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.09962.pdf)
>  The appearances of children are inherited from their parents, which makes it feasible to predict them. Predicting realistic children's faces may help settle many social problems, such as age-invariant face recognition, kinship verification, and missing child identification. It can be regarded as an image-to-image translation task. Existing approaches usually assume domain information in the image-to-image translation can be interpreted by "style", i.e., the separation of image content and style. However, such separation is improper for the child face prediction, because the facial contours between children and parents are not the same. To address this issue, we propose a new disentangled learning strategy for children's face prediction. We assume that children's faces are determined by genetic factors (compact family features, e.g., face contour), external factors (facial attributes irrelevant to prediction, such as moustaches and glasses), and variety factors (individual properties for each child). On this basis, we formulate predictions as a mapping from parents' genetic factors to children's genetic factors, and disentangle them from external and variety factors. In order to obtain accurate genetic factors and perform the mapping, we propose a ChildPredictor framework. It transfers human faces to genetic factors by encoders and back by generators. Then, it learns the relationship between the genetic factors of parents and children through a mapping function. To ensure the generated faces are realistic, we collect a large Family Face Database to train ChildPredictor and evaluate it on the FF-Database validation set. Experimental results demonstrate that ChildPredictor is superior to other well-known image-to-image translation methods in predicting realistic and diverse child faces. Implementation codes can be found at <a class="link-external link-https" href="https://github.com/zhaoyuzhi/ChildPredictor" rel="external noopener nofollow">this https URL</a>.      
### 39.RIS-Assisted Vehicular Network with Direct Transmission over Double-Generalized Gamma Fading Channels  [ :arrow_down: ](https://arxiv.org/pdf/2204.09958.pdf)
>  Reconfigurable intelligent surface (RIS) can provide stable connectivity for vehicular communications when direct transmission becomes significantly weaker with dynamic channel conditions between an access point and a moving vehicle. In this paper, we analyze the performance of a RIS-assisted vehicular network by coherently combining received signals reflected by RIS elements and direct transmissions from the source terminal over double generalized Gamma (dGG) fading channels. We present analytical expressions on the outage probability and average bit-error rate (BER) performance of the considered system by deriving exact density and distribution functions for the end-to-end signal-to-noise ratio (SNR) resulted from the finite sum of the direct link and product of channel coefficients each distributed according to the dGG. We also develop asymptotic analysis on the outage probability and average BER to derive diversity order for a better insight into the system performance at high SNR. We validate the derived analytical expressions through numerical and simulation results and demonstrate scaling of the system performance with RIS elements and a comparison to the conventional relaying techniques and direct transmissions considering various practically relevant scenarios.      
### 40.Sonic Interactions in Virtual Environments: the Egocentric Audio Perspective of the Digital Twin  [ :arrow_down: ](https://arxiv.org/pdf/2204.09919.pdf)
>  The relationships between the listener, physical world and virtual environment (VE) should not only inspire the design of natural multimodal interfaces but should be discovered to make sense of the mediating action of VR technologies. This chapter aims to transform an archipelago of studies related to sonic interactions in virtual environments (SIVE) into a research field equipped with a first theoretical framework with an inclusive vision of the challenges to come: the egocentric perspective of the auditory digital twin. In a VE with immersive audio technologies implemented, the role of VR simulations must be enacted by a participatory exploration of sense-making in a network of human and non-human agents, called actors. The guardian of such locus of agency is the auditory digital twin that fosters intra-actions between humans and technology, dynamically and fluidly redefining all those configurations that are crucial for an immersive and coherent experience. The idea of entanglement theory is here mainly declined in an egocentric-spatial perspective related to emerging knowledge of the listener's perceptual capabilities. This is an actively transformative relation with the digital twin potentials to create movement, transparency, and provocative activities in VEs. The chapter contains an original theoretical perspective complemented by several bibliographical references and links to the other book chapters that have contributed significantly to the proposal presented here.      
### 41.SinTra: Learning an inspiration model from a single multi-track music segment  [ :arrow_down: ](https://arxiv.org/pdf/2204.09917.pdf)
>  In this paper, we propose SinTra, an auto-regressive sequential generative model that can learn from a single multi-track music segment, to generate coherent, aesthetic, and variable polyphonic music of multi-instruments with an arbitrary length of bar. For this task, to ensure the relevance of generated samples and training music, we present a novel pitch-group representation. SinTra, consisting of a pyramid of Transformer-XL with a multi-scale training strategy, can learn both the musical structure and the relative positional relationship between notes of the single training music segment. Additionally, for maintaining the inter-track correlation, we use the convolution operation to process multi-track music, and when decoding, the tracks are independent to each other to prevent interference. We evaluate SinTra with both subjective study and objective metrics. The comparison results show that our framework can learn information from a single music segment more sufficiently than Music Transformer. Also the comparison between SinTra and its variant, i.e., the single-stage SinTra with the first stage only, shows that the pyramid structure can effectively suppress overly-fragmented notes.      
### 42.STFT-Domain Neural Speech Enhancement with Very Low Algorithmic Latency  [ :arrow_down: ](https://arxiv.org/pdf/2204.09911.pdf)
>  Deep learning based speech enhancement in the short-term Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window contains more samples and the frequency resolution can be higher for potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed based on the same 32 ms window size. To reduce this inherent latency, we adapt a conventional dual window size approach, where a regular input window size is used for STFT but a shorter output window is used for the overlap-add in the iSTFT, for STFT-domain deep learning based frame-online speech enhancement. Based on this STFT and iSTFT configuration, we employ single- or multi-microphone complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the RI components predicted by the DNN to conduct frame-online beamforming, the results of which are then used as extra features for a second DNN to perform frame-online post-filtering. The frequency-domain beamforming in between the two DNNs can be easily integrated with complex spectral mapping and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation results on a noisy-reverberant speech enhancement task demonstrate the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFT-domain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms.      
### 43.Joint Trajectory Design and User Scheduling of Aerial Cognitive Radio Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.09901.pdf)
>  Unmanned aerial vehicles (UAVs) have been widely employed to enhance the end-to-end performance of wireless communications since the links between UAVs and terrestrial nodes are line-of-sight (LoS) with high probability. However, the broadcast characteristics of signal propagation in LoS links make it vulnerable to being wiretapped by malicious eavesdroppers, which poses a considerable challenge to the security of wireless communications. This paper investigates the security of aerial cognitive radio networks (CRNs). An airborne base station transmits confidential messages to secondary users utilizing the same spectrum as the primary network. An aerial base station transmits jamming signals to suppress the eavesdropper to enhance secrecy performance. The uncertainty of eavesdropping node locations is considered, and the average secrecy rate of the cognitive user is maximized by optimizing multiple users' scheduling, the UAVs' trajectory, and transmit power. To solve the non-convex optimization problem with mixed multiple integers variable problem, we propose an iterative algorithm based on block coordinate descent and successive convex approximation. Numerical results verify the effectiveness of our proposed algorithm and demonstrate that our scheme is beneficial to improving the secrecy performance of aerial CRNs.      
### 44.Layer-wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.09883.pdf)
>  Accent variability has posed a huge challenge to automatic speech recognition~(ASR) modeling. Although one-hot accent vector based adaptation systems are commonly used, they require prior knowledge about the target accent and cannot handle unseen accents. Furthermore, simply concatenating accent embeddings does not make good use of accent knowledge, which has limited improvements. In this work, we aim to tackle these problems with a novel layer-wise adaptation structure injected into the E2E ASR model encoder. The adapter layer encodes an arbitrary accent in the accent space and assists the ASR model in recognizing accented speech. Given an utterance, the adaptation structure extracts the corresponding accent information and transforms the input acoustic feature into an accent-related feature through the linear combination of all accent bases. We further explore the injection position of the adaptation layer, the number of accent bases, and different types of accent bases to achieve better accent adaptation. Experimental results show that the proposed adaptation structure brings 12\% and 10\% relative word error rate~(WER) reduction on the AESRC2020 accent dataset and the Librispeech dataset, respectively, compared to the baseline.      
### 45.Physics vs. Learned Priors: Rethinking Camera and Algorithm Design for Task-Specific Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2204.09871.pdf)
>  Cameras were originally designed using physics-based heuristics to capture aesthetic images. In recent years, there has been a transformation in camera design from being purely physics-driven to increasingly data-driven and task-specific. In this paper, we present a framework to understand the building blocks of this nascent field of end-to-end design of camera hardware and algorithms. As part of this framework, we show how methods that exploit both physics and data have become prevalent in imaging and computer vision, underscoring a key trend that will continue to dominate the future of task-specific camera design. Finally, we share current barriers to progress in end-to-end design, and hypothesize how these barriers can be overcome.      
### 46.Sample-Based Bounds for Coherent Risk Measures: Applications to Policy Synthesis and Verification  [ :arrow_down: ](https://arxiv.org/pdf/2204.09833.pdf)
>  The dramatic increase of autonomous systems subject to variable environments has given rise to the pressing need to consider risk in both the synthesis and verification of policies for these systems. This paper aims to address a few problems regarding risk-aware verification and policy synthesis, by first developing a sample-based method to bound the risk measure evaluation of a random variable whose distribution is unknown. These bounds permit us to generate high-confidence verification statements for a large class of robotic systems. Second, we develop a sample-based method to determine solutions to non-convex optimization problems that outperform a large fraction of the decision space of possible solutions. Both sample-based approaches then permit us to rapidly synthesize risk-aware policies that are guaranteed to achieve a minimum level of system performance. To showcase our approach in simulation, we verify a cooperative multi-agent system and develop a risk-aware controller that outperforms the system's baseline controller. We also mention how our approach can be extended to account for any $g$-entropic risk measure - the subset of coherent risk measures on which we focus.      
### 47.Parametric Level-sets Enhanced To Improve Reconstruction (PaLEnTIR)  [ :arrow_down: ](https://arxiv.org/pdf/2204.09815.pdf)
>  In this paper, we consider the restoration and reconstruction of piecewise constant objects in two and three dimensions using PaLEnTIR, a significantly enhanced Parametric level set (PaLS) model relative to the current state-of-the-art. The primary contribution of this paper is a new PaLS formulation which requires only a single level set function to recover a scene with piecewise constant objects possessing multiple unknown contrasts. Our model offers distinct advantages over current approaches to the multi-contrast, multi-object problem, all of which require multiple level sets and explicit estimation of the contrast magnitudes. Given upper and lower bounds on the contrast, our approach is able to recover objects with any distribution of contrasts and eliminates the need to know either the number of contrasts in a given scene or their values. We provide an iterative process for finding these space-varying contrast limits. Relative to most PaLS methods which employ radial basis functions (RBFs), our model makes use of non-isotropic basis functions, thereby expanding the class of shapes that a PaLS model of a given complexity can approximate. Finally, PaLEnTIR improves the conditioning of the Jacobian matrix required as part of the parameter identification process and consequently accelerates the optimization methods by controlling the magnitude of the PaLS expansion coefficients, fixing the centers of the basis functions, and the uniqueness of parametric to image mappings provided by the new parameterization. We demonstrate the performance of the new approach using both 2D and 3D variants of X-ray computed tomography, diffuse optical tomography (DOT), denoising, deconvolution problems. Application to experimental sparse CT data and simulated data with different types of noise are performed to further validate the proposed method.      
### 48.Exact Formulas for Finite-Time Estimation Errors of Decentralized Temporal Difference Learning with Linear Function Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2204.09801.pdf)
>  In this paper, we consider the policy evaluation problem in multi-agent reinforcement learning (MARL) and derive exact closed-form formulas for the finite-time mean-squared estimation errors of decentralized temporal difference (TD) learning with linear function approximation. Our analysis hinges upon the fact that the decentralized TD learning method can be viewed as a Markov jump linear system (MJLS). Then standard MJLS theory can be applied to quantify the mean and covariance matrix of the estimation error of the decentralized TD method at every time step. Various implications of our exact formulas on the algorithm performance are also discussed. An interesting finding is that under a necessary and sufficient stability condition, the mean-squared TD estimation error will converge to an exact limit at a specific exponential rate.      
### 49.Multi-Scale Features and Parallel Transformers Based Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2204.09779.pdf)
>  With the increase in multimedia content, the type of distortions associated with multimedia is also increasing. This problem of image quality assessment is expanded well in the PIPAL dataset, which is still an open problem to solve for researchers. Although, recently proposed transformers networks have already been used in the literature for image quality assessment. At the same time, we notice that multi-scale feature extraction has proven to be a promising approach for image quality assessment. However, the way transformer networks are used for image quality assessment until now lacks these properties of multi-scale feature extraction. We utilized this fact in our approach and proposed a new architecture by integrating these two promising quality assessment techniques of images. Our experimentation on various datasets, including the PIPAL dataset, demonstrates that the proposed integration technique outperforms existing algorithms. The source code of the proposed algorithm is available online: <a class="link-external link-https" href="https://github.com/KomalPal9610/IQA" rel="external noopener nofollow">this https URL</a>      
### 50.FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow  [ :arrow_down: ](https://arxiv.org/pdf/2204.09679.pdf)
>  Super-resolution suffers from an innate ill-posed problem that a single low-resolution (LR) image can be from multiple high-resolution (HR) images. Recent studies on the flow-based algorithm solve this ill-posedness by learning the super-resolution space and predicting diverse HR outputs. Unfortunately, the diversity of the super-resolution outputs is still unsatisfactory, and the outputs from the flow-based model usually suffer from undesired artifacts which causes low-quality outputs. In this paper, we propose FS-NCSR which produces diverse and high-quality super-resolution outputs using frequency separation and noise conditioning compared to the existing flow-based approaches. As the sharpness and high-quality detail of the image rely on its high-frequency information, FS-NCSR only estimates the high-frequency information of the high-resolution outputs without redundant low-frequency components. Through this, FS-NCSR significantly improves the diversity score without significant image quality degradation compared to the NCSR, the winner of the previous NTIRE 2021 challenge.      
