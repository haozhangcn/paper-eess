# ArXiv eess --Tue, 7 Jun 2022
### 1.On Crossover Distance for Optical Wireless Satellite Networks and Optical Fiber Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.02763.pdf)
>  Optical wireless satellite networks (OWSNs) can provide lower latency data communications compared to optical fiber terrestrial networks (OFTNs). The crossover function enables to calculate the crossover distance for an OWSN and an OFTN. If the distance between two points on Earth is greater than the crossover distance, then switching or crossing over from the OFTN to the OWSN results in lower latency for data communications between these points. In this work, we extend the previously proposed crossover function for a scenario such that intermediate satellites (or hops) are incorporated between ingress and egress satellites in the OWSN for a more realistic calculation of the crossover distance in this scenario. We consider different OWSNs with different satellite altitudes and different OFTNs with different optical fiber refractive indexes, and we study the effect of the number of hops on crossover distance and length of a laser inter-satellite link (LISL). It is observed from numerical results that crossover distance increases with number of hops, and this increase is higher at higher satellite altitudes in OWSNs and lower refractive indexes in OFTNs. Furthermore, an inverse relationship between crossover distance and length of a LISL is observed. With increase in number of hops, the length of a LISL decreases as opposed to the crossover distance.      
### 2.Compound Multi-branch Feature Fusion for Real Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2206.02748.pdf)
>  Image restoration is a challenging and ill-posed problem which also has been a long-standing issue. However, most of learning based restoration methods are proposed to target one degradation type which means they are lack of generalization. In this paper, we proposed a multi-branch restoration model inspired from the Human Visual System (i.e., Retinal Ganglion Cells) which can achieve multiple restoration tasks in a general framework. The experiments show that the proposed multi-branch architecture, called CMFNet, has competitive performance results on four datasets, including image dehazing, deraindrop, and deblurring, which are very common applications for autonomous cars. The source code and pretrained models of three restoration tasks are available at <a class="link-external link-https" href="https://github.com/FanChiMao/CMFNet" rel="external noopener nofollow">this https URL</a>.      
### 3.Human Behavior Recognition Method Based on CEEMD-ES Radar Selection  [ :arrow_down: ](https://arxiv.org/pdf/2206.02705.pdf)
>  In recent years, the millimeter-wave radar to identify human behavior has been widely used in medical,security, and other fields. When multiple radars are performing detection tasks, the validity of the features contained in each radar is difficult to guarantee. In addition, processing multiple radar data also requires a lot of time and computational cost. The Complementary Ensemble Empirical Mode Decomposition-Energy Slice (CEEMD-ES) multistatic radar selection method is proposed to solve these problems. First, this method decomposes and reconstructs the radar signal according to the difference in the reflected echo frequency between the limbs and the trunk of the human body. Then, the radar is selected according to the difference between the ratio of echo energy of limbs and trunk and the theoretical value. The time domain, frequency domain and various entropy features of the selected radar are extracted. Finally, the Extreme Learning Machine (ELM) recognition model of the ReLu core is established. Experiments show that this method can effectively select the radar, and the recognition rate of three kinds of human actions is 98.53%.      
### 4.Continuous-Time Analog Filters for Audio Edge Intelligence: Review and Analysis on Design Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2206.02639.pdf)
>  Silicon cochlea designs capture the functionality of the biological cochlea. Their use has been explored for cochlea prosthesis applications and more recently in edge audio devices which are required to support always-on operation. As their stringent power constraints pose several design challenges, IC designers are forced to look for solutions that use low standby power. One promising bio-inspired approach is to combine the continuous-time analog filter channels of the silicon cochlea with a small memory footprint deep neural network that is trained on edge tasks such as keyword spotting, thereby allowing all blocks to be embedded in an IC. This paper reviews the analog filter circuits used as feature extractors for current edge audio devices, starting with the original biquad filter circuits proposed for the silicon cochlea. Our analysis starts from the interpretation of a basic biquad filter as a two-integrator-loop topology and reviews the progression in the design of second-order low-pass and band-pass filters ranging from OTA-based to source-follower-based architectures. We also derive and analyze the small-signal transfer function and discuss performance aspects of these filters. The analysis of these different filter configurations can be applied to other application domains such as biomedical devices which employ a front-end bandpass filter.      
### 5.A Robust Deep Learning Enabled Semantic Communication System for Text  [ :arrow_down: ](https://arxiv.org/pdf/2206.02596.pdf)
>  With the advent of the 6G era, the concept of semantic communication has attracted increasing attention. Compared with conventional communication systems, semantic communication systems are not only affected by physical noise existing in the wireless communication environment, e.g., additional white Gaussian noise, but also by semantic noise due to the source and the nature of deep learning-based systems. In this paper, we elaborate on the mechanism of semantic noise. In particular, we categorize semantic noise into two categories: literal semantic noise and adversarial semantic noise. The former is caused by written errors or expression ambiguity, while the latter is caused by perturbations or attacks added to the embedding layer via the semantic channel. To prevent semantic noise from influencing semantic communication systems, we present a robust deep learning enabled semantic communication system (R-DeepSC) that leverages a calibrated self-attention mechanism and adversarial training to tackle semantic noise. Compared with baseline models that only consider physical noise for text transmission, the proposed R-DeepSC achieves remarkable performance in dealing with semantic noise under different signal-to-noise ratios.      
### 6.Coding of volumetric content with MIV using VVC subpictures  [ :arrow_down: ](https://arxiv.org/pdf/2206.02588.pdf)
>  Storage and transport of six degrees of freedom (6DoF) dynamic volumetric visual content for immersive applications requires efficient compression. ISO/IEC MPEG has recently been working on a standard that aims to efficiently code and deliver 6DoF immersive visual experiences. This standard is called the MIV. MIV uses regular 2D video codecs to code the visual data. MPEG jointly with ITU-T VCEG, has also specified the VVC standard. VVC introduced recently the concept of subpicture. This tool was specifically designed to provide independent accessibility and decodability of sub-bitstreams for omnidirectional applications. This paper shows the benefit of using subpictures in the MIV use-case. While different ways in which subpictures could be used in MIV are discussed, a particular case study is selected. Namely, subpictures are used for parallel encoding and to reduce the number of decoder instances. Experimental results show that the cost of using subpictures in terms of bitrate overhead is negligible (0.1% to 0.4%), when compared to the overall bitrate. The number of decoder instances on the other hand decreases by a factor of two.      
### 7.Urban Road Safety Prediction: A Satellite Navigation Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2206.02584.pdf)
>  Predicting the safety of urban roads for navigation via global navigation satellite systems (GNSS) signals is considered. To ensure safe driving of automated vehicles, the vehicle must plan its trajectory to avoid navigating on unsafe roads (e.g., icy conditions, construction zones, narrow streets, etc.). Such information can be derived from the roads' physical properties, vehicle's capabilities, and weather conditions. From a GNSS-based navigation perspective, the reliability of GNSS signals in different locales, which is heavily dependent on the road layout within the surrounding environment, is crucial to ensure safe automated driving. An urban road environment surrounded by tall objects can significantly degrade the accuracy and availability of GNSS signals. This article proposes an approach to predict the reliability of GNSS-based navigation to ensure safe urban navigation. Satellite navigation reliability at a given location and time on a road is determined based on the probabilistic position error bound of the vehicle-mounted GNSS receiver. A metric for GNSS reliability for ground vehicles is suggested, and a method to predict the conservative probabilistic error bound of the GNSS navigation solution is proposed. A satellite navigation reliability map is generated for various navigation applications. As a case study, the reliability map is used in the proposed optimization problem formulation for automated ground vehicle safety-constrained path planning.      
### 8.Operative and Procedural Cooperative Training in Marine Ports  [ :arrow_down: ](https://arxiv.org/pdf/2206.02578.pdf)
>  This article faces the problem of operative and procedural cooperative training in marine ports with particular attention to harbour pilots and port traffic controller. The design and development of an advanced system, equipped with dedicated hardware in the loop, for cooperative training of operators involved in the last mile of navigation is presented. Indeed, the article describes the software and hardware development of a distributed and interoperable system composed by two simulators (the bridge ship simulator and control tower simulator). Multiple problems are faced and solved including (i) the motion of the ship at sea that is based on a 6 Degree Of Freedom (DOF) model for surge, sway and yaw and closed form expressions for pitch, roll and heave and its validation; (ii) the development of the 3D geometric models and related virtual environments of a real marine port and vessel (to provide the trainees with the sensation to experience a real port and ship environment); (iii) the design of a bridge ship replica, the bridge hardware integration and the design of the visualization system; (iv) the design and development of the control tower simulator; (v) the integration of the bridge ship simulator and control tower simulator through the IEEE 1516 High Level Architecture standard for distributed simulation.      
### 9.Wigner-Smith Time Delay Matrix for Electromagnetics: Guiding and Periodic Systems with Evanescent Modes  [ :arrow_down: ](https://arxiv.org/pdf/2206.02571.pdf)
>  The Wigner-Smith (WS) time delay matrix relates an electromagnetic system's scattering matrix and its frequency derivative. Previous work showed that the entries of WS time delay matrices of systems excited by propagating waves consist of volume integrals of energy-like field quantities. This paper introduces a generalized WS relationship that applies to systems excited by mixtures of propagating and evanescent fields. Just like its predecessor, the generalized WS relationship allows for the identification of so-called WS modes that interact with the system with well-defined time delays. Furthermore, a technique is developed to compute the WS time delay matrix of a composite system from the WS time delay matrices of its subsystems. Numerical examples demonstrate the usefulness of the generalized WS method when characterizing time delays experienced by fields interacting with guiding and periodic structures that have ports supporting evanescent modes.      
### 10.Complete Performance Analysis of Underwater VLC Diffusion Adaptive Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.02554.pdf)
>  In this paper, we simulated a diffusion adaptive network in the underwater environment. The communication method between the nodes of this network is assumed to be the visible light communication technology (VLC) which in the underwater condition is known as the UVLC. The links between the nodes in this case are contaminated with the optical noise and turbulence. These contaminations are modeled with the proper statistical distributions depending on the underwater conditions. The optical turbulence modeling link coefficients are shown to be following the Log-normal distribution which its mean and variance are directly dependent on the temperature and the salinity of the simulated water and the assumed distance between the diffusion network nodes. The performance of the diffusion network in using UVLC technology is then analyzed both with simulations and theoretical calculations and the results are presented using the steady-state error metrics. Our analysis showed that the diffusion network can be implemented underwater with the VLC technology providing that the distance between the network nodes is less than 10 meters. Also, in order to guarantee the convergence of the adaptive network, the water salinity level and temperature must not exceed the values that are presented in our simulations.      
### 11.UTTS: Unsupervised TTS with Conditional Disentangled Sequential Variational Auto-encoder  [ :arrow_down: ](https://arxiv.org/pdf/2206.02512.pdf)
>  In this paper, we propose a novel unsupervised text-to-speech (UTTS) framework which does not require text-audio pairs for the TTS acoustic modeling (AM). UTTS is a multi-speaker speech synthesizer developed from the perspective of disentangled speech representation learning. The framework offers a flexible choice of a speaker's duration model, timbre feature (identity) and content for TTS inference. We leverage recent advancements in self-supervised speech representation learning as well as speech synthesis front-end techniques for the system development. Specifically, we utilize a lexicon to map input text to the phoneme sequence, which is expanded to the frame-level forced alignment (FA) with a speaker-dependent duration model. Then, we develop an alignment mapping module that converts the FA to the unsupervised alignment (UA). Finally, a Conditional Disentangled Sequential Variational Auto-encoder (C-DSVAE), serving as the self-supervised TTS AM, takes the predicted UA and a target speaker embedding to generate the mel spectrogram, which is ultimately converted to waveform with a neural vocoder. We show how our method enables speech synthesis without using a paired TTS corpus. Experiments demonstrate that UTTS can synthesize speech of high naturalness and intelligibility measured by human and objective evaluations.      
### 12.Multi-model assessment of heat decarbonisation options in the UK using electricity and hydrogen  [ :arrow_down: ](https://arxiv.org/pdf/2206.02483.pdf)
>  Delivering low-carbon heat will require the substitution of natural gas with low-carbon alternatives such as electricity and hydrogen. The objective of this paper is to develop a method to soft-link two advanced, investment-optimising energy system models, RTN (Resource-Technology Network) and WeSIM (Whole-electricity System Investment Model), in order to assess cost-efficient heat decarbonisation pathways for the UK while utilising the respective strengths of the two models. The linking procedure included passing on hourly electricity prices from WeSIM as input to RTN, and returning capacities and locations of hydrogen generation and shares of electricity and hydrogen in heat supply from RTN to WeSIM. The outputs demonstrate that soft-linking can improve the quality of the solution, while providing useful insights into the cost-efficient pathways for zero-carbon heating. Quantitative results point to the cost-effectiveness of using a mix of electricity and hydrogen technologies for delivering zero-carbon heat, also demonstrating a high level of interaction between electricity and hydrogen infrastructure in a zero-carbon system. Hydrogen from gas reforming with carbon capture and storage can play a significant role in the medium term, while remaining a cost-efficient option for supplying peak heat demand in the longer term, with the bulk of heat demand being supplied by electric heat pumps.      
### 13.Model predictive eco-driving control for heavy-duty trucks using Branch and Bound optimization  [ :arrow_down: ](https://arxiv.org/pdf/2206.02447.pdf)
>  Eco-driving (ED) can be used for fuel savings in existing vehicles, requiring only a few hardware modifications. For this technology to be successful in a dynamic environment, ED requires an online real-time implementable policy. In this work, a dedicated Branch and Bound (BnB) model predictive control (MPC) algorithm is proposed to solve the optimization part of an ED optimal control problem. The developed MPC solution for ED is based on the following ingredients. As a prediction model, the velocity dynamics as a function of distance is modeled by a finite number of driving modes and gear positions. Then we formulate an optimization problem that minimizes a cost function with two terms: one penalizing the fuel consumption and one penalizing the trip duration. We exploit contextual elements and use a warm-started solution to make the BnB solver run in real-time. The results are evaluated in numerical simulations on two routes in Israel and France and the long haul cycle of the Vehicle Energy consumption Calculation Tool (VECTO). In comparison with a human driver and a Pontryagin's Minimum Principle (PMP) solution, 25.8% and 12.9% fuel savings, respectively, are achieved on average.      
### 14.Pervasive wireless channel modeling theory and applications to 6G GBSMs for all frequency bands and all scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2206.02442.pdf)
>  In this paper, a pervasive wireless channel modeling theory is first proposed, which uses a unified channel modeling method and a unified equation of channel impulse response (CIR), and can integrate important channel characteristics at different frequency bands and scenarios. Then, we apply the proposed theory to a three dimensional (3D) space-time-frequency (STF) non-stationary geometry-based stochastic model (GBSM) for the sixth generation (6G) wireless communication systems. The proposed 6G pervasive channel model (6GPCM) can characterize statistical properties of channels at all frequency bands from sub-6 GHz to visible light communication (VLC) bands and all scenarios such as unmanned aerial vehicle (UAV), maritime, (ultra-)massive multiple-input multiple-output (MIMO), reconfigurable intelligent surface (RIS), and industry Internet of things (IIoT) scenarios. By adjusting channel model parameters, the 6GPCM can be reduced to various simplified channel models for specific frequency bands and scenarios. Also, it includes standard fifth generation (5G) channel models as special cases. In addition, key statistical properties of the proposed 6GPCM are derived, simulated, and verified by various channel measurement results, which clearly demonstrates its accuracy, pervasiveness, and applicability.      
### 15.Continuous and Distribution-free Probabilistic Wind Power Forecasting: A Conditional Normalizing Flow Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.02433.pdf)
>  We present a data-driven approach for probabilistic wind power forecasting based on conditional normalizing flow (CNF). In contrast with the existing, this approach is distribution-free (as for non-parametric and quantile-based approaches) and can directly yield continuous probability densities, hence avoiding quantile crossing. It relies on a base distribution and a set of bijective mappings. Both the shape parameters of the base distribution and the bijective mappings are approximated with neural networks. Spline-based conditional normalizing flow is considered owing to its non-affine characteristics. Over the training phase, the model sequentially maps input examples onto samples of base distribution, given the conditional contexts, where parameters are estimated through maximum likelihood. To issue probabilistic forecasts, one eventually maps samples of the base distribution into samples of a desired distribution. Case studies based on open datasets validate the effectiveness of the proposed model, and allows us to discuss its advantages and caveats with respect to the state of the art.      
### 16.Online Neural Diarization of Unlimited Numbers of Speakers  [ :arrow_down: ](https://arxiv.org/pdf/2206.02432.pdf)
>  A method to perform offline and online speaker diarization for an unlimited number of speakers is described in this paper. End-to-end neural diarization (EEND) has achieved overlap-aware speaker diarization by formulating it as a multi-label classification problem. It has also been extended for a flexible number of speakers by introducing speaker-wise attractors. However, the output number of speakers of attractor-based EEND is empirically capped; it cannot deal with cases where the number of speakers appearing during inference is higher than that during training because its speaker counting is trained in a fully supervised manner. Our method, EEND-GLA, solves this problem by introducing unsupervised clustering into attractor-based EEND. In the method, the input audio is first divided into short blocks, then attractor-based diarization is performed for each block, and finally the results of each blocks are clustered on the basis of the similarity between locally-calculated attractors. While the number of output speakers is limited within each block, the total number of speakers estimated for the entire input can be higher than the limitation. To use EEND-GLA in an online manner, our method also extends the speaker-tracing buffer, which was originally proposed to enable online inference of conventional EEND. We introduces a block-wise buffer update to make the speaker-tracing buffer compatible with EEND-GLA. Finally, to improve online diarization, our method improves the buffer update method and revisits the variable chunk-size training of EEND. The experimental results demonstrate that EEND-GLA can perform speaker diarization of an unseen number of speakers in both offline and online inferences.      
### 17.mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02425.pdf)
>  Accurate brain tumor segmentation from Magnetic Resonance Imaging (MRI) is desirable to joint learning of multimodal images. However, in clinical practice, it is not always possible to acquire a complete set of MRIs, and the problem of missing modalities causes severe performance degradation in existing multimodal segmentation methods. In this work, we present the first attempt to exploit the Transformer for multimodal brain tumor segmentation that is robust to any combinatorial subset of available modalities. Concretely, we propose a novel multimodal Medical Transformer (mmFormer) for incomplete multimodal learning with three main components: the hybrid modality-specific encoders that bridge a convolutional encoder and an intra-modal Transformer for both local and global context modeling within each modality; an inter-modal Transformer to build and align the long-range correlations across modalities for modality-invariant features with global semantics corresponding to tumor region; a decoder that performs a progressive up-sampling and fusion with the modality-invariant features to generate robust segmentation. Besides, auxiliary regularizers are introduced in both encoder and decoder to further enhance the model's robustness to incomplete modalities. We conduct extensive experiments on the public BraTS $2018$ dataset for brain tumor segmentation. The results demonstrate that the proposed mmFormer outperforms the state-of-the-art methods for incomplete multimodal brain tumor segmentation on almost all subsets of incomplete modalities, especially by an average 19.07% improvement of Dice on tumor segmentation with only one available modality. The code is available at <a class="link-external link-https" href="https://github.com/YaoZhang93/mmFormer" rel="external noopener nofollow">this https URL</a>.      
### 18.Efficient and Scalable High-Order Portfolios Design via Parametric Skew-t Distribution  [ :arrow_down: ](https://arxiv.org/pdf/2206.02412.pdf)
>  Since Markowitz's mean-variance framework, optimizing a portfolio that maximizes the profit and minimizes the risk has been ubiquitous in the financial industry. Initially, profit and risk were measured by the first two moments of the portfolio's return, a.k.a. the mean and variance, which are sufficient to characterize a Gaussian distribution. However, it is broadly believed that the first two moments are not enough to capture the characteristics of the returns' behavior, which have been recognized to be asymmetric and heavy-tailed. Although there is ample evidence that portfolio designs involving the third and fourth moments, i.e., skewness and kurtosis, will outperform the conventional mean-variance framework, they are non-trivial. Specifically, in the classical framework, the memory and computational cost of computing the skewness and kurtosis grow sharply with the number of assets. To alleviate the difficulty in high-dimensional problems, we consider an alternative expression for high-order moments based on parametric representations via a generalized hyperbolic skew-t distribution. Then, we reformulate the high-order portfolio optimization problem as a fixed-point problem and propose a robust fixed-point acceleration algorithm that solves the problem in an efficient and scalable manner. Empirical experiments also demonstrate that our proposed high-order portfolio optimization framework is of low complexity and significantly outperforms the state-of-the-art methods by 2 to 4 orders of magnitude.      
### 19.Interference Management for Over-the-Air Federated Learning in Multi-Cell Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.02398.pdf)
>  Federated learning (FL) over resource-constrained wireless networks has recently attracted much attention. However, most existing studies consider one FL task in single-cell wireless networks and ignore the impact of downlink/uplink inter-cell interference on the learning performance. In this paper, we investigate FL over a multi-cell wireless network, where each cell performs a different FL task and over-the-air computation (AirComp) is adopted to enable fast uplink gradient aggregation. We conduct convergence analysis of AirComp-assisted FL systems, taking into account the inter-cell interference in both the downlink and uplink model/gradient transmissions, which reveals that the distorted model/gradient exchanges induce a gap to hinder the convergence of FL. We characterize the Pareto boundary of the error-induced gap region to quantify the learning performance trade-off among different FL tasks, based on which we formulate an optimization problem to minimize the sum of error-induced gaps in all cells. To tackle the coupling between the downlink and uplink transmissions as well as the coupling among multiple cells, we propose a cooperative multi-cell FL optimization framework to achieve efficient interference management for downlink and uplink transmission design. Results demonstrate that our proposed algorithm achieves much better average learning performance over multiple cells than non-cooperative baseline schemes.      
### 20.Neural-inspired Measurement Observability  [ :arrow_down: ](https://arxiv.org/pdf/2206.02361.pdf)
>  The neural encoding by biological sensors of flying insects, which prefilters stimulus data before sending it to the central nervous system in the form of voltage spikes, enables sensing capabilities that are computationally low-cost while also being highly robust to noise. This process, which can be modeled as the composition of a linear moving average filter and a nonlinear decision function, inspired the work reported here to improve engineered sensing performance by maximizing the observability of particular neural-inspired composite measurement functions. We first present a tool to determine the observability of a linear system with measurement delay (the first element of the composition), then use a Lie algebraic observability approach to study nonlinear autonomous systems with output delay (the second element of the composition). The Lie algebraic tools are then extended to address overall observability of systems with composite outputs as in the neural encoder model we adopt. The analytical outcomes are supported using the empirical observability Gramian, and optimal sensor placement on a bioinspired wing model is performed using metrics based on the empirical Gramian.      
### 21.Implementation of a Modified U-Net for Medical Image Segmentation on Edge Devices  [ :arrow_down: ](https://arxiv.org/pdf/2206.02358.pdf)
>  Deep learning techniques, particularly convolutional neural networks, have shown great potential in computer vision and medical imaging applications. However, deep learning models are computationally demanding as they require enormous computational power and specialized processing hardware for model training. To make these models portable and compatible for prototyping, their implementation on low-power devices is imperative. In this work, we present the implementation of Modified U-Net on Intel Movidius Neural Compute Stick 2 (NCS-2) for the segmentation of medical images. We selected U-Net because, in medical image segmentation, U-Net is a prominent model that provides improved performance for medical image segmentation even if the dataset size is small. The modified U-Net model is evaluated for performance in terms of dice score. Experiments are reported for segmentation task on three medical imaging datasets: BraTs dataset of brain MRI, heart MRI dataset, and Ziehl-Neelsen sputum smear microscopy image (ZNSDB) dataset. For the proposed model, we reduced the number of parameters from 30 million in the U-Net model to 0.49 million in the proposed architecture. Experimental results show that the modified U-Net provides comparable performance while requiring significantly lower resources and provides inference on the NCS-2. The maximum dice scores recorded are 0.96 for the BraTs dataset, 0.94 for the heart MRI dataset, and 0.74 for the ZNSDB dataset.      
### 22.Establishing the Capabilities of the Murchison Widefield Array as a Passive Radar for the Surveillance of Space  [ :arrow_down: ](https://arxiv.org/pdf/2206.02357.pdf)
>  This paper describes the use of the Murchison Widefield Array, a low-frequency radio telescope at a radio-quiet Western Australian site, as a radar receiver forming part of a continent-spanning multistatic radar network for the surveillance of space. This paper details the system geometry employed, the orbit-specific radar signal processing, and the orbit determination algorithms necessary to ensure resident space objects are detected, tracked, and propagated. Finally, the paper includes the results processed after a short collection campaign utilising several FM radio transmitters across the country, up to a maximum baseline distance of over 2500 km. The results demonstrate the Murchison Widefield Array is able to provide widefield and persistent coverage of objects in low Earth orbit.      
### 23.Modeling the Material-Inventory Transportation Problem Using Multi-Objective Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2206.02350.pdf)
>  In the era of industry 4.0, procurement in supply chain management is the key to developing information management systems. It directly affects production planning failure. In this case, it is the process to prepare and confirming the material inventory is in the ordinal stages and be able to produce the products in any production line. In terms of industrial informatics, it can provide information management approaches for leveraging data sharing between factories. The multiobjective optimization will be enabled by integrating material inventory, production planning and monitoring, and transportation planning collaboration. The material-inventory transportation problem is the virtual factory situation when production plan failure occurs. It becomes the cost to transport material between each factory and the distribution to clients. In this study, the question of the material-inventory transportation problem is: How can we transport other materials from one factory into another factory? This study proposed a model to find out about the adjustment of material inventory through transportation. The objective of this model is to minimize the whole production cost and total transportation cost.      
### 24.Underdetermined 2D-DOD and 2D-DOA Estimation for Bistatic Coprime EMVS-MIMO Radar: From the Difference Coarray Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2206.02311.pdf)
>  In this paper, the underdetermined 2D-DOD and 2D-DOA estimation for bistatic coprime EMVS-MIMO radar is considered. Firstly, a 5-D tensor model was constructed by using the multi-dimensional space-time characteristics of the received data. Then, an 8-D tensor has been obtained by using the auto-correlation calculation. To obtain the difference coarrays of transmit and receive EMVS, the de-coupling process between the spatial response of EMVS and the steering vector is inevitable. Thus, a new 6-D tensor can be constructed via the tensor permutation and the generalized tensorization of the canonical polyadic decomposition. {According} to the theory of the Tensor-Matrix Product operation, the duplicated elements in the difference coarrays can be removed by the utilization of two designed selection matrices. Due to the centrosymmetric geometry of the difference coarrays, two DFT beamspace matrices were subsequently designed to convert the complex steering matrices into the real-valued ones, whose advantage is to improve the estimation accuracy of the 2D-DODs and 2D-DOAs. Afterwards, a third-order tensor with the third-way fixed at 36 was constructed and the Parallel Factor algorithm was deployed, which can yield the closed-form automatically paired 2D-DOD and 2D-DOA estimation. The simulation results show that the proposed algorithm can exhibit superior estimation performance for the underdetermined 2D-DOD and 2D-DOA estimation.      
### 25.Reconfigurable intelligent surfaces: Channel characterization and modeling  [ :arrow_down: ](https://arxiv.org/pdf/2206.02308.pdf)
>  Reconfigurable intelligent surfaces (RISs) are two dimensional (2D) metasurfaces which can intelligently manipulate electromagnetic waves by low-cost near passive reflecting elements. RIS is viewed as a potential key technology for the sixth generation (6G) wireless communication systems mainly due to its advantages in tuning wireless signals, thus smartly controlling propagation environments. In this paper, we aim at addressing channel characterization and modeling issues of RIS-assisted wireless communication systems. At first, the concept, principle, and potential applications of RIS are given. An overview of RIS based channel measurements and experiments is presented by classifying frequency bands, scenarios, system configurations, RIS constructions, experiment purposes, and channel observations. Then, RIS based channel characteristics are studied, including reflection and transmission, Doppler effect and multipath fading mitigation, channel reciprocity, channel hardening, rank improvement, far field and near field, etc. RIS based channel modeling works are investigated, including largescale path loss models and small-scale multipath fading models. At last, future research directions related to RIS-assisted channels are also discussed.      
### 26.MICAL: Mutual Information-Based CNN-Aided Learned Factor  [ :arrow_down: ](https://arxiv.org/pdf/2206.02298.pdf)
>  We develop a hybrid model-based data-driven seizure detection algorithm called Mutual Information-based CNNAided Learned factor graphs (MICAL) for detection of eclectic seizures from EEG signals. Our proposed method contains three main components: a neural mutual information (MI) estimator, 1D convolutional neural network (CNN), and factor graph inference. Since during seizure the electrical activity in one or more regions in the brain becomes correlated, we use neural MI estimators to measure inter-channel statistical dependence. We also design a 1D CNN to extract additional features from raw EEG signals. Since the soft estimates obtained as the combined features from the neural MI estimator and the CNN do not capture the temporal correlation between different EEG blocks, we use them not as estimates of the seizure state, but to compute the function nodes of a factor graph. The resulting factor graphs allows structured inference which exploits the temporal correlation for further improving the detection performance. On public CHB-MIT database, We conduct three evaluation approaches using the public CHB-MIT database, including 6-fold leave-four-patients-out cross-validation, all patient training; and per patient training. Our evaluations systematically demonstrate the impact of each element in MICAL through a complete ablation study and measuring six performance metrics. It is shown that the proposed method obtains state-of-the-art performance specifically in 6-fold leave-four-patients-out cross-validation and all patient training, demonstrating a superior generalizability.      
### 27.Integral Line-of-Sight Curved Path Following of Helical Microswimmers Actuated by Rotating Magnetic Dipoles  [ :arrow_down: ](https://arxiv.org/pdf/2206.02287.pdf)
>  This short paper investigates the problem of curved path following for helical microswimmers actuated by rotating magnetic dipoles. The proposed solution, which relies on an integral line-of-sight (ILOS) guidance law, can be utilized in both below and beyond step-out frequency regimes.      
### 28.Autoregressive Model for Multi-Pass SAR Change Detection Based on Image Stacks  [ :arrow_down: ](https://arxiv.org/pdf/2206.02278.pdf)
>  Change detection is an important synthetic aperture radar (SAR) application, usually used to detect changes on the ground scene measurements in different moments in time. Traditionally, change detection algorithm (CDA) is mainly designed for two synthetic aperture radar (SAR) images retrieved at different instants. However, more images can be used to improve the algorithms performance, witch emerges as a research topic on SAR change detection. Image stack information can be treated as a data series over time and can be modeled by autoregressive (AR) models. Thus, we present some initial findings on SAR change detection based on image stack considering AR models. Applying AR model for each pixel position in the image stack, we obtained an estimated image of the ground scene which can be used as a reference image for CDA. The experimental results reveal that ground scene estimates by the AR models is accurate and can be used for change detection applications.      
### 29.Conditions for Oscillator Small-Signal Amplitude-Phase Orthogonality  [ :arrow_down: ](https://arxiv.org/pdf/2206.02244.pdf)
>  The paper explores a previously unknown connection relating the symmetry properties of an oscillator steady-state to the orthogonal representation of amplitude and phase variables in the small-signal regime. It is shown that only circuits producing perfectly symmetric steady-states can produce an orthogonal Floquet decomposition. Considering room temperature operation this scenario implies zero AM-PM noise conversion. This surprising and novel result follows directly from the predictions of a rigorous model framework first described herein. The work presented in this text extend the current state-of-the-art w.r.t. oscillator small-signal/noise characterization.      
### 30.Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography  [ :arrow_down: ](https://arxiv.org/pdf/2206.02225.pdf)
>  Displacement estimation is a critical step of virtually all Ultrasound Elastography (USE) techniques. Two main features make this task unique compared to the general optical flow problem: the high-frequency nature of ultrasound radio-frequency (RF) data and the governing laws of physics on the displacement field. Recently, the architecture of the optical flow networks has been modified to be able to use RF data. Also, semi-supervised and unsupervised techniques have been employed for USE by considering prior knowledge of displacement continuity in the form of the first- and second-derivative regularizers. Despite these attempts, no work has considered the tissue compression pattern, and displacements in axial and lateral directions have been assumed to be independent. However, tissue motion pattern is governed by laws of physics in USE, rendering the axial and the lateral displacements highly correlated. In this paper, we propose Physically Inspired ConsTraint for Unsupervised Regularized Elastography (PICTURE), where we impose constraints on the Poisson's ratio to improve lateral displacement estimates. Experiments on phantom and in vivo data show that PICTURE substantially improves the quality of the lateral displacement estimation.      
### 31.Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2206.02147.pdf)
>  Polyphone disambiguation aims to capture accurate pronunciation knowledge from natural text sequences for reliable Text-to-speech (TTS) systems. However, previous approaches require substantial annotated training data and additional efforts from language experts, making it difficult to extend high-quality neural TTS systems to out-of-domain daily conversations and countless languages worldwide. This paper tackles the polyphone disambiguation problem from a concise and novel perspective: we propose Dict-TTS, a semantic-aware generative text-to-speech model with an online website dictionary (the existing prior information in the natural language). Specifically, we design a semantics-to-pronunciation attention (S2PA) module to match the semantic patterns between the input text sequence and the prior semantics in the dictionary and obtain the corresponding pronunciations; The S2PA module can be easily trained with the end-to-end TTS model without any annotated phoneme labels. Experimental results in three languages show that our model outperforms several strong baseline models in terms of pronunciation accuracy and improves the prosody modeling of TTS systems. Further extensive analyses with different linguistic encoders demonstrate that each design in Dict-TTS is effective. Audio samples are available at \url{<a class="link-external link-https" href="https://dicttts.github.io/DictTTS-Demo/" rel="external noopener nofollow">this https URL</a>}.      
### 32.Geometrically-Motivated Primary-Ambient Decomposition With Center-Channel Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2206.02125.pdf)
>  A geometrically-motivated method for primary-ambient decomposition is proposed and evaluated in an up-mixing application. The method consists of two steps, accommodating a particularly intuitive explanation. The first step consists of signal-adaptive rotations applied on the input stereo scene, which translate the primary sound sources into the center of the rotated scene. The second step applies a center-channel extraction method, based on a simple signal model and optimal in the mean-squared-error sense. The performance is evaluated by using the estimated ambient component to enable surround sound starting from real-world stereo signals. The participants in the reported listening test are asked to adjust the audio scene envelopment and find the audio settings that pleases them the most. The possibility for up-mixing enabled by the proposed method is used extensively, and the user satisfaction is significantly increased compared to the original stereo mix.      
### 33.Sampling Frequency Independent Dialogue Separation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02124.pdf)
>  In some DNNs for audio source separation, the relevant model parameters are independent of the sampling frequency of the audio used for training. Considering the application of dialogue separation, this is shown for two DNN architectures: a U-Net and a fully-convolutional model. The models are trained with audio sampled at 8 kHz. The learned parameters are transferred to models for processing audio at 48 kHz. The separated audio sources are compared with the ones produced by the same model architectures trained with 48 kHz versions of the same training data. A listening test and computational measures show that there is no significant perceptual difference between the models trained with 8 kHz or with 48 kHz. This transferability of the learned parameters allows for a faster and computationally less costly training. It also enables using training datasets available at a lower sampling frequency than the one needed by the application at hand, or using data collections with multiple sampling frequencies.      
### 34.LASSO-Based Multiple-Line Outage Identification In Partially Observable Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.02111.pdf)
>  Phasor measurement units (PMUs) create ample real-time monitoring opportunities for modern power systems. Among them, line outage detection and identification remains a crucial but challenging task. Current works on outage identification succeed in full PMU deployment and single-line outages. Performance however degrades for multiple-line outage with partial system observability. We propose a novel framework of multiple-line outage identification using partial nodal voltage measurements. Using alternating current (AC) power flow model, phase angle signatures of outages are extracted and used to group lines into minimal diagnosable clusters. Identification is then formulated into an underdetermined sparse regression problem solved by lasso. Tested on IEEE 39-bus system with 25% and 50% PMU coverage, the proposed identification method is 93% and 80% accurate for single- and double-line outages. Our study suggests that the AC power flow is better at capturing outage patterns and sacrificing some precision could yield substantial improvement in identification accuracy. These findings could contribute to the development of future control schemes that help power systems resist and recover from outage disruptions in real time.      
### 35.Differentiable Point Scattering Models for Efficient Radar Target Characterization  [ :arrow_down: ](https://arxiv.org/pdf/2206.02075.pdf)
>  Target characterization is an important step in many defense missions, often relying on fitting a known target model to observed data. Optimization of model parameters can be computationally expensive depending on the model complexity, thus having models that both describe the data well and that can be efficiently optimized is critical. This work introduces a class of radar models that can be used to represent the radar scattering response of a target at high frequencies while also enabling the use of gradient-based optimization.      
### 36.Low Power Neuromorphic EMG Gesture Classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.02061.pdf)
>  EMG (Electromyograph) signal based gesture recognition can prove vital for applications such as smart wearables and bio-medical neuro-prosthetic control. Spiking Neural Networks (SNNs) are promising for low-power, real-time EMG gesture recognition, owing to their inherent spike/event driven spatio-temporal dynamics. In literature, there are limited demonstrations of neuromorphic hardware implementation (at full chip/board/system scale) for EMG gesture classification. Moreover, most literature attempts exploit primitive SNNs based on LIF (Leaky Integrate and Fire) neurons. In this work, we address the aforementioned gaps with following key contributions: (1) Low-power, high accuracy demonstration of EMG-signal based gesture recognition using neuromorphic Recurrent Spiking Neural Networks (RSNN). In particular, we propose a multi-time scale recurrent neuromorphic system based on special double-exponential adaptive threshold (DEXAT) neurons. Our network achieves state-of-the-art classification accuracy (90%) while using ~53% lesser neurons than best reported prior art on Roshambo EMG dataset. (2) A new multi-channel spike encoder scheme for efficient processing of real-valued EMG data on neuromorphic systems. (3) Unique multi-compartment methodology to implement complex adaptive neurons on Intel's dedicated neuromorphic Loihi chip is shown. (4) RSNN implementation on Loihi (Nahuku 32) achieves significant energy/latency benefits of ~983X/19X compared to GPU for batch size as 50.      
### 37.A new approach to improve the performance of OFDM signal for 6G communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.01979.pdf)
>  The orthogonal frequency division multiplexing is a very efficient modulation technique that can achieve very high throughput by transmitting many carriers simultaneously and it is spectrally efficient because of the proximity of the subcarriers. OFDM is used in 5G communication for higher data transmission. 6th generation communication also demands OFDM, since it is more spectrally efficient and suitable for high data transmission. The drawback of the OFDM includes peak to average power ratio and sensitivity to carrier offsets and drifts. The usage of a non-linear power amplifier causes the signal spreading and leads to inter-modulation and signal constellation distortion. These two distortions have an impact on the signal-to-noise ratio and hence reduce the efficiency. The methods used to reduce PAPR are clipping and filtering, selective mapping, partial transmit sequence, tone reservation, and injection and non-linear commanding. The drawbacks of the above methods are computational complexity, spectrum inefficiency, increase in bit error rate and PAPR rate. In this work, three effective methods are discussed and compared to improve the performance parameters. These are adaptive peak window method based on harmonize clipping, harmonics kernel adaptive filter and Slepian-based flat-top window techniques are presented to reduce the BER, PAPR, and CCDF to improve the signal-to-noise of the system. This window technique averages out the noise spread out in the spectrum and thus reduces the signal loss by minimizing peak to average power ratio. The results are analyzed and compared with the existing conventional methods. Finally, the reductions in PAPR, BER and CCDF obtained are discussed in the results and comparison section. The proposed work has a higher signal-to-noise ratio than the conventional methods.      
### 38.Scheduling for Ground-Assisted Federated Learning in LEO Satellite Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2206.01952.pdf)
>  Distributed training of machine learning models directly on satellites in low Earth orbit (LEO) is considered. Based on a federated learning (FL) algorithm specifically targeted at the unique challenges of the satellite scenario, we design a scheduler that exploits the predictability of visiting times between ground stations (GS) and satellites to reduce model staleness. Numerical experiments show that this can improve the convergence speed by a factor three.      
### 39.STARSS22: A dataset of spatial recordings of real scenes with spatiotemporal annotations of sound events  [ :arrow_down: ](https://arxiv.org/pdf/2206.01948.pdf)
>  This report presents the Sony-TAu Realistic Spatial Soundscapes 2022 (STARS22) dataset for sound event localization and detection, comprised of spatial recordings of real scenes collected in various interiors of two different sites. The dataset is captured with a high resolution spherical microphone array and delivered in two 4-channel formats, first-order Ambisonics and tetrahedral microphone array. Sound events in the dataset belonging to 13 target sound classes are annotated both temporally and spatially through a combination of human annotation and optical tracking. The dataset serves as the development and evaluation dataset for the Task 3 of the DCASE2022 Challenge on Sound Event Localization and Detection and introduces significant new challenges for the task compared to the previous iterations, which were based on synthetic spatialized sound scene recordings. Dataset specifications are detailed including recording and annotation process, target classes and their presence, and details on the development and evaluation splits. Additionally, the report presents the baseline system that accompanies the dataset in the challenge with emphasis on the differences with the baseline of the previous iterations; namely, introduction of the multi-ACCDOA representation to handle multiple simultaneous occurences of events of the same class, and support for additional improved input features for the microphone array format. Results of the baseline indicate that with a suitable training strategy a reasonable detection and localization performance can be achieved on real sound scene recordings. The dataset is available in <a class="link-external link-https" href="https://zenodo.org/record/6387880" rel="external noopener nofollow">this https URL</a>.      
### 40.On the exponential convergence of input-output signals of nonlinear feedback systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01945.pdf)
>  We show that the integral-constraint-based robust feedback stability theorem for certain Lurye systems exhibits the property that the endogenous input-output signals enjoy an exponential convergence rate for all initial conditions of the linear time-invariant subsystem. More generally, we provide conditions under which a feedback interconnection of possibly open-loop unbounded subsystems to admit such an exponential convergence property, using perturbation analysis and a combination of tools including integral quadratic constraints, directed gap measure, and exponential weightings. As an application, we apply the result to first-order convex optimisation methods. In particular, by making use of the Zames-Falb multipliers, we state conditions for these methods to converge exponentially when applied to strongly convex functions with Lipschitz gradients.      
### 41.DOBC-Based Frequency &amp; Voltage Regulation Strategy for PV-Diesel Hybrid Microgrid During Islanding Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2206.01936.pdf)
>  This paper proposes a disturbance observer-based control (DOBC) method for frequency and voltage regulation of a solar photovoltaic (PV)-diesel generator(DG) based hybrid microgrid during islanding conditions. The DOBC is integrated as a feed-forward control to the synchronous generator based DG, which handles real-time power mismatches and regulates the microgrid frequency and voltage under islanding. To substantiate the operational robustness of the developed controller under real-time uncertainties arising due to variability in PV output and load, the controller has been tested under worstcase uncertainty conditions. The proposed controller has been developed as a MATLAB/Simulink model and the results are validated on the real-time simulator OPAL-RT. The effectiveness of the proposed control scheme has further been validated in the presence of communication delays and noisy load conditions. Results verify the dynamic performance of the controller in regulating the system frequency and voltage for low-inertia microgrids. Finally, the proposed control strategy has been implemented on laboratory scale microgrid setup in which synchronous generator based diesel generator regulates system frequency fast and efficiently under worst case uncertainty scenario.      
### 42.Neural Lyapunov Control of Unknown Nonlinear Systems with Stability Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2206.01913.pdf)
>  Learning for control of dynamical systems with formal guarantees remains a challenging task. This paper proposes a learning framework to simultaneously stabilize an unknown nonlinear system with a neural controller and learn a neural Lyapunov function to certify a region of attraction (ROA) for the closed-loop system. The algorithmic structure consists of two neural networks and a satisfiability modulo theories (SMT) solver. The first neural network is responsible for learning the unknown dynamics. The second neural network aims to identify a valid Lyapunov function and a provably stabilizing nonlinear controller. The SMT solver then verifies that the candidate Lyapunov function indeed satisfies the Lyapunov conditions. We provide theoretical guarantees of the proposed learning framework in terms of the closed-loop stability for the unknown nonlinear system. We illustrate the effectiveness of the approach with a set of numerical experiments.      
### 43.Markovian Decentralized Ensemble Control for Demand Response  [ :arrow_down: ](https://arxiv.org/pdf/2206.01912.pdf)
>  With the advancement in smart grid and smart energy devices, demand response becomes one of the most economic and feasible solutions to ease the load stress of the power grids during peak hours. In this work, we propose a fully decentralized ensemble control framework with consensus for demand response (DR) events and compatible control methods based on random policies. We show that under the consensus that is tailored to DR, our proposed decentralized control method yields the same optimality as the centralized control method in both myopic and multistage settings.      
### 44.Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2206.01903.pdf)
>  This paper proposes to encode the distribution of features learned from a convolutional neural network using a Gaussian Mixture Model. These parametric features, called GMM-CNN, are derived from chest computed tomography and X-ray scans of patients with Coronavirus Disease 2019. We use the proposed GMM-CNN features as input to a robust classifier based on random forests to differentiate between COVID-19 and other pneumonia cases. Our experiments assess the advantage of GMM-CNN features compared to standard CNN classification on test images. Using a random forest classifier (80\% samples for training; 20\% samples for testing), GMM-CNN features encoded with two mixture components provided a significantly better performance than standard CNN classification (p\,$&lt;$\,0.05). Specifically, our method achieved an accuracy in the range of 96.00\,--\,96.70\% and an area under the ROC curve in the range of 99.29\,--\,99.45\%, with the best performance obtained by combining GMM-CNN features from both computed tomography and X-ray images. Our results suggest that the proposed GMM-CNN features could improve the prediction of COVID-19 in chest computed tomography and X-ray scans.      
### 45.Modeling of Textures to Predict Immune Cell Status and Survival of Brain Tumour Patients  [ :arrow_down: ](https://arxiv.org/pdf/2206.01897.pdf)
>  Radiomics has shown a capability for different types of cancers such as glioma to predict the clinical outcome. It can have a non-invasive means of evaluating the immunotherapy response prior to treatment. However, the use of deep convolutional neural networks (CNNs)-based radiomics requires large training image sets. To avoid this problem, we investigate a new imaging features that model distribution with a Gaussian mixture model (GMM) of learned 3D CNN features. Using these deep radiomic features (DRFs), we aim to predict the immune marker status (low versus high) and overall survival for glioma patients. We extract the DRFs by aggregating the activation maps of a pre-trained 3D-CNN within labeled tumor regions of MRI scans that corresponded immune markers of 151 patients. Our experiments are performed to assess the relationship between the proposed DRFs, three immune cell markers (Macrophage M1, Neutrophils and T Cells Follicular Helper), and measure their association with overall survival. Using the random forest (RF) model, DRFs was able to predict the immune marker status with area under the ROC curve (AUC) of 78.67, 83.93 and 75.67\% for Macrophage M1, Neutrophils and T Cells Follicular Helper, respectively. Combined the immune markers with DRFs and clinical variables, Kaplan-Meier estimator and Log-rank test achieved the most significant difference between predicted groups of patients (short-term versus long-term survival) with p\,=\,4.31$\times$10$^{-7}$ compared to p\,=\,0.03 for Immune cell markers, p\,=\,0.07 for clinical variables , and p\,=\,1.45$\times$10$^{-5}$ for DRFs. Our findings indicate that the proposed features (DRFs) used in RF models may significantly consider prognosticating patients with brain tumour prior to surgery through regularly acquired imaging data.      
### 46.8D Parameters Estimation for Bistatic EMVS-MIMO Radar via the nested PARAFAC  [ :arrow_down: ](https://arxiv.org/pdf/2206.01891.pdf)
>  In this letter, a novel nested PARAFAC algorithm was proposed to improve the 8D parameters estimation performance for the bistatic EMVS-MIMO radar. Firstly, the outer part PARAFAC algorithm was carried out to estimate the receive spatial response matrix and its first way factor matrix. For the estimated first way factor matrix, a theory is given to rearrange its data into an new matrix, which is the mode-1 unfolding matrix of a three-way tensor. Then, the inner part PARAFAC algorithm was used to estimate the transmit steering vector matrix, the transmit spatial response matrix and the receive steering vector matrix. Thus, the transmit 4D parameters and receive 4D parameters can be accurately located via the abovementioned process. Compared with the original PARAFAC algorithm, the proposed nested PARAFAC algorithm can avoid additional reconstruction process when estimating the transmit/receive spatial response matrix. Moreover, the proposed algorithm can offer a highly-accurate 8D parameters estimaiton than that of the original PARAFAC algorithm. Simulated results verify the effectiveness of the proposed algorithm.      
### 47.Robust and Kernelized Data-Enabled Predictive Control for Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01866.pdf)
>  This paper presents a robust and kernelized data-enabled predictive control (RoKDeePC) algorithm to perform model-free optimal control for nonlinear systems using only input and output data. The algorithm combines robust predictive control and a non-parametric representation of nonlinear systems enabled by regularized kernel methods. The latter is based on implicitly learning the nonlinear behavior of the system via the representer theorem. Instead of seeking a model and then performing control design, our method goes directly from data to control. This allows us to robustify the control inputs against the uncertainties in data by considering a min-max optimization problem to calculate the optimal control sequence. We show that by incorporating a proper uncertainty set, this min-max problem can be reformulated as a nonconvex but structured minimization problem. By exploiting its structure, we present a projected gradient descent algorithm to effectively solve this problem. Finally, we test the RoKDeePC on two nonlinear example systems - one academic case study and a grid-forming converter feeding a nonlinear load - and compare it with some existing nonlinear data-driven predictive control methods.      
### 48.Image Data collection and implementation of deep learning-based model in detecting Monkeypox disease using modified VGG16  [ :arrow_down: ](https://arxiv.org/pdf/2206.01862.pdf)
>  While the world is still attempting to recover from the damage caused by the broad spread of COVID-19, the Monkeypox virus poses a new threat of becoming a global pandemic. Although the Monkeypox virus itself is not deadly and contagious as COVID-19, still every day, new patients case has been reported from many nations. Therefore, it will be no surprise if the world ever faces another global pandemic due to the lack of proper precautious steps. Recently, Machine learning (ML) has demonstrated huge potential in image-based diagnoses such as cancer detection, tumor cell identification, and COVID-19 patient detection. Therefore, a similar application can be adopted to diagnose the Monkeypox-related disease as it infected the human skin, which image can be acquired and further used in diagnosing the disease. Considering this opportunity, in this work, we introduce a newly developed "Monkeypox2022" dataset that is publicly available to use and can be obtained from our shared GitHub repository. The dataset is created by collecting images from multiple open-source and online portals that do not impose any restrictions on use, even for commercial purposes, hence giving a safer path to use and disseminate such data when constructing and deploying any type of ML model. Further, we propose and evaluate a modified VGG16 model, which includes two distinct studies: Study One and Two. Our exploratory computational results indicate that our suggested model can identify Monkeypox patients with an accuracy of $97\pm1.8\%$ (AUC=97.2) and $88\pm0.8\%$ (AUC=0.867) for Study One and Two, respectively. Additionally, we explain our model's prediction and feature extraction utilizing Local Interpretable Model-Agnostic Explanations (LIME) help to a deeper insight into specific features that characterize the onset of the Monkeypox virus.      
### 49.R2U++: A Multiscale Recurrent Residual U-Net with Dense Skip Connections for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01793.pdf)
>  U-Net is a widely adopted neural network in the domain of medical image segmentation. Despite its quick embracement by the medical imaging community, its performance suffers on complicated datasets. The problem can be ascribed to its simple feature extracting blocks: encoder/decoder, and the semantic gap between encoder and decoder. Variants of U-Net (such as R2U-Net) have been proposed to address the problem of simple feature extracting blocks by making the network deeper, but it does not deal with the semantic gap problem. On the other hand, another variant UNET++ deals with the semantic gap problem by introducing dense skip connections but has simple feature extraction blocks. To overcome these issues, we propose a new U-Net based medical image segmentation architecture R2U++. In the proposed architecture, the adapted changes from vanilla U-Net are: (1) the plain convolutional backbone is replaced by a deeper recurrent residual convolution block. The increased field of view with these blocks aids in extracting crucial features for segmentation which is proven by improvement in the overall performance of the network. (2) The semantic gap between encoder and decoder is reduced by dense skip pathways. These pathways accumulate features coming from multiple scales and apply concatenation accordingly. The modified architecture has embedded multi-depth models, and an ensemble of outputs taken from varying depths improves the performance on foreground objects appearing at various scales in the images. The performance of R2U++ is evaluated on four distinct medical imaging modalities: electron microscopy (EM), X-rays, fundus, and computed tomography (CT). The average gain achieved in IoU score is 1.5+-0.37% and in dice score is 0.9+-0.33% over UNET++, whereas, 4.21+-2.72 in IoU and 3.47+-1.89 in dice score over R2U-Net across different medical imaging segmentation datasets.      
### 50.Monkeypox Image Data collection  [ :arrow_down: ](https://arxiv.org/pdf/2206.01774.pdf)
>  This paper explains the initial Monkeypox Open image data collection procedure. It was created by assembling images collected from websites, newspapers, and online portals and currently contains around 1905 images after data augmentation.      
### 51.Automatic Quantification of Volumes and Biventricular Function in Cardiac Resonance. Validation of a New Artificial Intelligence Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.01746.pdf)
>  Background: Artificial intelligence techniques have shown great potential in cardiology, especially in quantifying cardiac biventricular function, volume, mass, and ejection fraction (EF). However, its use in clinical practice is not straightforward due to its poor reproducibility with cases from daily practice, among other reasons. Objectives: To validate a new artificial intelligence tool in order to quantify the cardiac biventricular function (volume, mass, and EF). To analyze its robustness in the clinical area, and the computational times compared with conventional methods. Methods: A total of 189 patients were analyzed: 89 from a regional center and 100 from a public center. The method proposes two convolutional networks that include anatomical information of the heart to reduce classification errors. Results: A high concordance (Pearson coefficient) was observed between manual quantification and the proposed quantification of cardiac function (0.98, 0.92, 0.96 and 0.8 for volumes and biventricular EF) in about 5 seconds per study. Conclusions: This method quantifies biventricular function and volumes in seconds with an accuracy equivalent to that of a specialist.      
### 52.Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial Intelligence Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2206.01745.pdf)
>  Background: Artificial intelligence techniques have demonstrated great potential in cardiology, especially to detect imperceptible patterns for the human eye. In this sense, these techniques seem to be adequate to identify patterns in the myocardial texture which could lead to characterize and quantify fibrosis. Purpose: The aim of this study was to postulate a new artificial intelligence method to identify fibrosis in cine cardiac magnetic resonance (CMR) imaging. Methods: A retrospective observational study was carried out in a population of 75 subjects from a clinical center of San Carlos de Bariloche. The proposed method analyzes the myocardial texture in cine CMR images using a convolutional neural network to determine local myocardial tissue damage. Results: An accuracy of 89% for quantifying local tissue damage was observed for the validation data set and 70% for the test set. In addition, the qualitative analysis showed a high spatial correlation in lesion location. Conclusions: The postulated method enables to spatially identify fibrosis using only the information from cine nuclear magnetic resonance studies, demonstrating the potential of this technique to quantify myocardial viability in the future or to study the lesions etiology      
### 53.Orthogonal Transform based Generative Adversarial Network for Image Dehazing  [ :arrow_down: ](https://arxiv.org/pdf/2206.01743.pdf)
>  Image dehazing has become one of the crucial preprocessing steps for any computer vision task. Most of the dehazing methods try to estimate the transmission map along with the atmospheric light to get the dehazed image in the image domain. In this paper, we propose a novel end-to-end architecture that directly estimates dehazed image in Krawtchouk transform domain. For this a customized Krawtchouk Convolution Layer (KCL) in the architecture is added. KCL is constructed using Krawtchouk basis functions which converts the image from the spatial domain to the Krawtchouk transform domain. Another convolution layer is added at the end of the architecture named as Inverse Krawtchouk Convolution Layer (IKCL) which converts the image back to the spatial domain from the transform domain. It has been observed that the haze is mainly present in lower frequencies of hazy images, wherein the Krawtchouk transform helps to analyze the high and low frequencies of the images separately. We have divided our architecture into two branches, the upper branch deals with the higher frequencies while the lower branch deals with the lower frequencies of the image. The lower branch is made deeper in terms of the layers as compared to the upper branch to address the haze present in the lower frequencies. Using the proposed Orthogonal Transform based Generative Adversarial Network (OTGAN) architecture for image dehazing, we were able to achieve competitive results when compared to the present state-of-the-art methods.      
### 54.Learning Probabilistic Structural Representation for Biomedical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01742.pdf)
>  Accurate segmentation of various fine-scale structures from biomedical images is a very important yet challenging problem. Existing methods use topological information as an additional training loss, but are ultimately learning a pixel-wise representation. In this paper, we propose the first deep learning method to learn a structural representation. We use discrete Morse theory and persistent homology to construct an one-parameter family of structures as the structural representation space. Furthermore, we learn a probabilistic model that can do inference tasks on such a structural representation space. We empirically demonstrate the strength of our method, i.e., generating true structures rather than pixel-maps with better topological integrity, and facilitating a human-in-the-loop annotation pipeline using the sampling of structures and structure-aware uncertainty.      
### 55.Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01741.pdf)
>  We present a new encoder-decoder Vision Transformer architecture, Patcher, for medical image segmentation. Unlike standard Vision Transformers, it employs Patcher blocks that segment an image into large patches, each of which is further divided into small patches. Transformers are applied to the small patches within a large patch, which constrains the receptive field of each pixel. We intentionally make the large patches overlap to enhance intra-patch communication. The encoder employs a cascade of Patcher blocks with increasing receptive fields to extract features from local to global levels. This design allows Patcher to benefit from both the coarse-to-fine feature extraction common in CNNs and the superior spatial relationship modeling of Transformers. We also propose a new mixture-of-experts (MoE) based decoder, which treats the feature maps from the encoder as experts and selects a suitable set of expert features to predict the label for each pixel. The use of MoE enables better specializations of the expert features and reduces interference between them during inference. Extensive experiments demonstrate that Patcher outperforms state-of-the-art Transformer- and CNN-based approaches significantly on stroke lesion segmentation and polyp segmentation. Code for Patcher will be released with publication to facilitate future research.      
### 56.Denoising Fast X-Ray Fluorescence Raster Scans of Paintings  [ :arrow_down: ](https://arxiv.org/pdf/2206.01740.pdf)
>  Macro x-ray fluorescence (XRF) imaging of cultural heritage objects, while a popular non-invasive technique for providing elemental distribution maps, is a slow acquisition process in acquiring high signal-to-noise ratio XRF volumes. Typically on the order of tenths of a second per pixel, a raster scanning probe counts the number of photons at different energies emitted by the object under x-ray illumination. In an effort to reduce the scan times without sacrificing elemental map and XRF volume quality, we propose using dictionary learning with a Poisson noise model as well as a color image-based prior to restore noisy, rapidly acquired XRF data.      
### 57.Mutual- and Self- Prototype Alignment for Semi-supervised Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01739.pdf)
>  Semi-supervised learning methods have been explored in medical image segmentation tasks due to the scarcity of pixel-level annotation in the real scenario. Proto-type alignment based consistency constraint is an intuitional and plausible solu-tion to explore the useful information in the unlabeled data. In this paper, we propose a mutual- and self- prototype alignment (MSPA) framework to better utilize the unlabeled data. In specific, mutual-prototype alignment enhances the information interaction between labeled and unlabeled data. The mutual-prototype alignment imposes two consistency constraints in reverse directions between the unlabeled and labeled data, which enables the consistent embedding and model discriminability on unlabeled data. The proposed self-prototype alignment learns more stable region-wise features within unlabeled images, which optimizes the classification margin in semi-supervised segmentation by boosting the intra-class compactness and inter-class separation on the feature space. Extensive experimental results on three medical datasets demonstrate that with a small amount of labeled data, MSPA achieves large improvements by leveraging the unlabeled data. Our method also outperforms seven state-of-the-art semi-supervised segmentation methods on all three datasets.      
### 58.RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding  [ :arrow_down: ](https://arxiv.org/pdf/2206.01738.pdf)
>  Lidars are depth measuring sensors widely used in autonomous driving and augmented reality. However, the large volume of data produced by lidars can lead to high costs in data storage and transmission. While lidar data can be represented as two interchangeable representations: 3D point clouds and range images, most previous work focus on compressing the generic 3D point clouds. In this work, we show that directly compressing the range images can leverage the lidar scanning pattern, compared to compressing the unprojected point clouds. We propose a novel data-driven range image compression algorithm, named RIDDLE (Range Image Deep DeLta Encoding). At its core is a deep model that predicts the next pixel value in a raster scanning order, based on contextual laser shots from both the current and past scans (represented as a 4D point cloud of spherical coordinates and time). The deltas between predictions and original values can then be compressed by entropy encoding. Evaluated on the Waymo Open Dataset and KITTI, our method demonstrates significant improvement in the compression rate (under the same distortion) compared to widely used point cloud and range image compression algorithms as well as recent deep methods.      
### 59.MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01737.pdf)
>  Convolutional neural networks (CNNs) have achieved remarkable segmentation accuracy on benchmark datasets where training and test sets are from the same domain, yet their performance can degrade significantly on unseen domains, which hinders the deployment of CNNs in many clinical scenarios. Most existing works improve model out-of-domain (OOD) robustness by collecting multi-domain datasets for training, which is expensive and may not always be feasible due to privacy and logistical issues. In this work, we focus on improving model robustness using a single-domain dataset only. We propose a novel data augmentation framework called MaxStyle, which maximizes the effectiveness of style augmentation for model OOD performance. It attaches an auxiliary style-augmented image decoder to a segmentation network for robust feature learning and data augmentation. Importantly, MaxStyle augments data with improved image style diversity and hardness, by expanding the style space with noise and searching for the worst-case style composition of latent features via adversarial training. With extensive experiments on multiple public cardiac and prostate MR datasets, we demonstrate that MaxStyle leads to significantly improved out-of-distribution robustness against unseen corruptions as well as common distribution shifts across multiple, different, unseen sites and unknown image sequences under both low- and high-training data settings. The code can be found at <a class="link-external link-https" href="https://github.com/cherise215/MaxStyle" rel="external noopener nofollow">this https URL</a>.      
### 60.Adaptive Adversarial Training to Improve Adversarial Robustness of DNNs for Medical Image Segmentation and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.01736.pdf)
>  Recent methods based on Deep Neural Networks (DNNs) have reached high accuracy for medical image analysis, including the three basic tasks: segmentation, landmark detection, and object detection. It is known that DNNs are vulnerable to adversarial attacks, and the adversarial robustness of DNNs could be improved by adding adversarial noises to training data (i.e., adversarial training). In this study, we show that the standard adversarial training (SAT) method has a severe issue that limits its practical use: it generates a fixed level of noise for DNN training, and it is difficult for the user to choose an appropriate noise level, because a high noise level may lead to a large reduction in model performance, and a low noise level may have little effect. To resolve this issue, we have designed a novel adaptive-margin adversarial training (AMAT) method that generates adaptive adversarial noises for DNN training, which are dynamically tailored for each individual training sample. We have applied our AMAT method to state-of-the-art DNNs for the three basic tasks, using five publicly available datasets. The experimental results demonstrate that our AMAT method outperforms the SAT method in adversarial robustness on noisy data and prediction accuracy on clean data. Please contact the author for the source code.      
### 61.Examining the behaviour of state-of-the-art convolutional neural networks for brain tumor detection with and without transfer learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.01735.pdf)
>  Distinguishing normal from malignant and determining the tumor type are critical components of brain tumor diagnosis. Two different kinds of dataset are investigated using state-of-the-art CNN models in this research work. One dataset(binary) has images of normal and tumor types, while another(multi-class) provides all images of tumors classified as glioma, meningioma, or pituitary. The experiments were conducted in these dataset with transfer learning from pre-trained weights from ImageNet as well as initializing the weights randomly. The experimental environment is equivalent for all models in this study in order to make a fair comparison. For both of the dataset, the validation set are same for all the models where train data is 60% while the rest is 40% for validation. With the proposed techniques in this research, the EfficientNet-B5 architecture outperforms all the state-of-the-art models in the binary-classification dataset with the accuracy of 99.75% and 98.61% accuracy for the multi-class dataset. This research also demonstrates the behaviour of convergence of validation loss in different weight initialization techniques.      
### 62.Empirical Study of Quality Image Assessment for Synthesis of Fetal Head Ultrasound Imaging with DCGANs  [ :arrow_down: ](https://arxiv.org/pdf/2206.01731.pdf)
>  In this work, we present an empirical study of DCGANs for synthetic generation of fetal head ultrasound, consisting of hyperparameter heuristics and image quality assessment. We present experiments to show the impact of different image sizes, epochs, data size input, and learning rates for quality image assessment on four metrics: mutual information (MI), frchet inception distance (FID), peak-signal-to-noise ratio (PSNR), and local binary pattern vector (LBPv). The results show that FID and LBPv have stronger relationship with clinical image quality scores. The resources to reproduce this work are available at \url{<a class="link-external link-https" href="https://github.com/xfetus/miua2022" rel="external noopener nofollow">this https URL</a>}.      
### 63.A review of machine learning approaches, challenges and prospects for computational tumor pathology  [ :arrow_down: ](https://arxiv.org/pdf/2206.01728.pdf)
>  Computational pathology is part of precision oncology medicine. The integration of high-throughput data including genomics, transcriptomics, proteomics, metabolomics, pathomics, and radiomics into clinical practice improves cancer treatment plans, treatment cycles, and cure rates, and helps doctors open up innovative approaches to patient prognosis. In the past decade, rapid advances in artificial intelligence, chip design and manufacturing, and mobile computing have facilitated research in computational pathology and have the potential to provide better-integrated solutions for whole-slide images, multi-omics data, and clinical informatics. However, tumor computational pathology now brings some challenges to the application of tumour screening, diagnosis and prognosis in terms of data integration, hardware processing, network sharing bandwidth and machine learning technology. This review investigates image preprocessing methods in computational pathology from a pathological and technical perspective, machine learning-based methods, and applications of computational pathology in breast, colon, prostate, lung, and various tumour disease scenarios. Finally, the challenges and prospects of machine learning in computational pathology applications are discussed.      
### 64.Randomized Synthesis for Diversity and Cost Constraints with Control Improvisation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02775.pdf)
>  In many synthesis problems, it can be essential to generate implementations which not only satisfy functional constraints but are also randomized to improve variety, robustness, or unpredictability. The recently-proposed framework of control improvisation (CI) provides techniques for the correct-by-construction synthesis of randomized systems subject to hard and soft constraints. However, prior work on CI has focused on qualitative specifications, whereas in robotic planning and other areas we often have quantitative quality metrics which can be traded against each other. For example, a designer of a patrolling security robot might want to know by how much the average patrol time needs to be increased in order to ensure that a particular aspect of the robot's route is sufficiently diverse and hence unpredictable. In this paper, we enable this type of application by generalizing the CI problem to support quantitative soft constraints which bound the expected value of a given cost function, and randomness constraints which enforce diversity of the generated traces with respect to a given label function. We establish the basic theory of labelled quantitative CI problems, and develop efficient algorithms for solving them when the specifications are encoded by finite automata. We also provide an approximate improvisation algorithm based on constraint solving for any specifications encodable as Boolean formulas. We demonstrate the utility of our problem formulation and algorithms with experiments applying them to generate diverse near-optimal plans for robotic planning problems.      
### 65.Day-to-Night Image Synthesis for Training Nighttime Neural ISPs  [ :arrow_down: ](https://arxiv.org/pdf/2206.02715.pdf)
>  Many flagship smartphone cameras now use a dedicated neural image signal processor (ISP) to render noisy raw sensor images to the final processed output. Training nightmode ISP networks relies on large-scale datasets of image pairs with: (1) a noisy raw image captured with a short exposure and a high ISO gain; and (2) a ground truth low-noise raw image captured with a long exposure and low ISO that has been rendered through the ISP. Capturing such image pairs is tedious and time-consuming, requiring careful setup to ensure alignment between the image pairs. In addition, ground truth images are often prone to motion blur due to the long exposure. To address this problem, we propose a method that synthesizes nighttime images from daytime images. Daytime images are easy to capture, exhibit low-noise (even on smartphone cameras) and rarely suffer from motion blur. We outline a processing framework to convert daytime raw images to have the appearance of realistic nighttime raw images with different levels of noise. Our procedure allows us to easily produce aligned noisy and clean nighttime image pairs. We show the effectiveness of our synthesis framework by training neural ISPs for nightmode rendering. Furthermore, we demonstrate that using our synthetic nighttime images together with small amounts of real data (e.g., 5% to 10%) yields performance almost on par with training exclusively on real nighttime images. Our dataset and code are available at <a class="link-external link-https" href="https://github.com/SamsungLabs/day-to-night" rel="external noopener nofollow">this https URL</a>.      
### 66.Canonical Cortical Graph Neural Networks and its Application for Speech Enhancement in Future Audio-Visual Hearing Aids  [ :arrow_down: ](https://arxiv.org/pdf/2206.02671.pdf)
>  Despite the recent success of machine learning algorithms, most of these models still face several drawbacks when considering more complex tasks requiring interaction between different sources, such as multimodal input data and logical time sequence. On the other hand, the biological brain is highly sharpened in this sense, empowered to automatically manage and integrate such a stream of information through millions of years of evolution. In this context, this paper finds inspiration from recent discoveries on cortical circuits in the brain to propose a more biologically plausible self-supervised machine learning approach that combines multimodal information using intra-layer modulations together with canonical correlation analysis (CCA), as well as a memory mechanism to keep track of temporal data, the so-called Canonical Cortical Graph Neural networks. The approach outperformed recent state-of-the-art results considering both better clean audio reconstruction and energy efficiency, described by a reduced and smother neuron firing rate distribution, suggesting the model as a suitable approach for speech enhancement in future audio-visual hearing aid devices.      
### 67.Real-World Image Super-Resolution by Exclusionary Dual-Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.02609.pdf)
>  Real-world image super-resolution is a practical image restoration problem that aims to obtain high-quality images from in-the-wild input, has recently received considerable attention with regard to its tremendous application potentials. Although deep learning-based methods have achieved promising restoration quality on real-world image super-resolution datasets, they ignore the relationship between L1- and perceptual- minimization and roughly adopt auxiliary large-scale datasets for pre-training. In this paper, we discuss the image types within a corrupted image and the property of perceptual- and Euclidean- based evaluation protocols. Then we propose a method, Real-World image Super-Resolution by Exclusionary Dual-Learning (RWSR-EDL) to address the feature diversity in perceptual- and L1- based cooperative learning. Moreover, a noise-guidance data collection strategy is developed to address the training time consumption in multiple datasets optimization. When an auxiliary dataset is incorporated, RWSR-EDL achieves promising results and repulses any training time increment by adopting the noise-guidance data collection strategy. Extensive experiments show that RWSR-EDL achieves competitive performance over state-of-the-art methods on four in-the-wild image super-resolution datasets.      
### 68.Machine Learning for Detection of 3D Features using sparse X-ray data  [ :arrow_down: ](https://arxiv.org/pdf/2206.02564.pdf)
>  In many inertial confinement fusion experiments, the neutron yield and other parameters cannot be completely accounted for with one and two dimensional models. This discrepancy suggests that there are three dimensional effects which may be significant. Sources of these effects include defects in the shells and shell interfaces, the fill tube of the capsule, and the joint feature in double shell targets. Due to their ability to penetrate materials, X-rays are used to capture the internal structure of objects. Methods such as Computational Tomography use X-ray radiographs from hundreds of projections in order to reconstruct a three dimensional model of the object. In experimental environments, such as the National Ignition Facility and Omega-60, the availability of these views is scarce and in many cases only consist of a single line of sight. Mathematical reconstruction of a 3D object from sparse views is an ill-posed inverse problem. These types of problems are typically solved by utilizing prior information. Neural networks have been used for the task of 3D reconstruction as they are capable of encoding and leveraging this prior information. We utilize half a dozen different convolutional neural networks to produce different 3D representations of ICF implosions from the experimental data. We utilize deep supervision to train a neural network to produce high resolution reconstructions. We use these representations to track 3D features of the capsules such as the ablator, inner shell, and the joint between shell hemispheres. Machine learning, supplemented by different priors, is a promising method for 3D reconstructions in ICF and X-ray radiography in general.      
### 69.Scan4CFU: Low-cost, open-source bacterial colony tracking over large areas and extended incubation times  [ :arrow_down: ](https://arxiv.org/pdf/2206.02534.pdf)
>  A hallmark of bacterial populations cultured in vitro is their homogeneity of growth, where the majority of cells display identical growth rate, cell size and content. Recent insights, however, have revealed that even cells growing in exponential growth phase can be heterogeneous with respect to variables typically used to measure cell growth. Bacterial heterogeneity has important implications for how bacteria respond to environmental stresses, such as antibiotics. The phenomenon of antimicrobial persistence, for example, has been linked to a small subpopulation of cells that have entered into a state of dormancy where antibiotics are no longer effective. While methods have been developed for identifying individual non-growing cells in bacterial cultures, there has been less attention paid to how these cells may influence growth in colonies on a solid surface. In response, we have developed a low-cost, open-source platform to perform automated image capture and image analysis of bacterial colony growth on multiple nutrient agar plates simultaneously. The descriptions of the hardware and software are included, along with details about the temperature-controlled growth chamber, high-resolution scanner, and graphical interface to extract and plot the colony lag time and growth kinetics. Experiments were conducted using a wild type strain of Escherichia coli K12 to demonstrate the feasibility and operation of our setup. By automated tracking of bacterial growth kinetics in colonies, the system holds the potential to reveal new insights into understanding the impact of microbial heterogeneity on antibiotic resistance and persistence.      
### 70.Single pixel imaging at high pixel resolutions  [ :arrow_down: ](https://arxiv.org/pdf/2206.02510.pdf)
>  The usually reported pixel resolution of single pixel imaging (SPI) varies between $32 \times 32$ and $256 \times 256$ pixels falling far below imaging standards with classical methods. Low resolution results from the trade-off between the acceptable compression ratio, the limited DMD modulation frequency, and reasonable reconstruction time, and has not improved significantly during the decade of intensive research on SPI. In this paper we show that image measurement at the full resolution of the DMD, which lasts only a fraction of a second, is possible for sparse images or in a situation when the field of view is limited but is a priori unknown. We propose the sampling and reconstruction strategies that enable us to reconstruct sparse images at the resolution of $1024 \times 768$ within the time of $0.3~$s. Non-sparse images are reconstructed with less details. The compression ratio is on the order of $0.4 \%$ which corresponds to an acquisition frequency of $7~$Hz. Sampling is differential, binary, and non-adaptive, and includes information on multiple partitioning of the image which later allows us to determine the actual field of view. Reconstruction is based on the differential Fourier domain regularized inversion (D-FDRI). The proposed SPI framework is an alternative to both adaptive SPI, which is challenging to implement in real time, and to classical compressive sensing image recovery methods, which are very slow at high resolutions.      
### 71.Learning to Control under Time-Varying Environment  [ :arrow_down: ](https://arxiv.org/pdf/2206.02507.pdf)
>  This paper investigates the problem of regret minimization in linear time-varying (LTV) dynamical systems. Due to the simultaneous presence of uncertainty and non-stationarity, designing online control algorithms for unknown LTV systems remains a challenging task. At a cost of NP-hard offline planning, prior works have introduced online convex optimization algorithms, although they suffer from nonparametric rate of regret. <br>In this paper, we propose the first computationally tractable online algorithm with regret guarantees that avoids offline planning over the state linear feedback policies. Our algorithm is based on the optimism in the face of uncertainty (OFU) principle in which we optimistically select the best model in a high confidence region. Our algorithm is then more explorative when compared to previous approaches. To overcome non-stationarity, we propose either a restarting strategy (R-OFU) or a sliding window (SW-OFU) strategy. With proper configuration, our algorithm is attains sublinear regret $O(T^{2/3})$. These algorithms utilize data from the current phase for tracking variations on the system dynamics. We corroborate our theoretical findings with numerical experiments, which highlight the effectiveness of our methods. To the best of our knowledge, our study establishes the first model-based online algorithm with regret guarantees under LTV dynamical systems.      
### 72.Universal Photometric Stereo Network using Global Lighting Contexts  [ :arrow_down: ](https://arxiv.org/pdf/2206.02452.pdf)
>  This paper tackles a new photometric stereo task, named universal photometric stereo. Unlike existing tasks that assumed specific physical lighting models; hence, drastically limited their usability, a solution algorithm of this task is supposed to work for objects with diverse shapes and materials under arbitrary lighting variations without assuming any specific models. To solve this extremely challenging task, we present a purely data-driven method, which eliminates the prior assumption of lighting by replacing the recovery of physical lighting parameters with the extraction of the generic lighting representation, named global lighting contexts. We use them like lighting parameters in a calibrated photometric stereo network to recover surface normal vectors pixelwisely. To adapt our network to a wide variety of shapes, materials and lightings, it is trained on a new synthetic dataset which simulates the appearance of objects in the wild. Our method is compared with other state-of-the-art uncalibrated photometric stereo methods on our test data to demonstrate the significance of our method.      
### 73.CARE: Resource Allocation Using Sparse Communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.02410.pdf)
>  We propose a new framework for studying effective resource allocation in a load balancing system under sparse communication, a problem that arises, for instance, in data centers. At the core of our approach is state approximation, where the load balancer first estimates the servers' states via a carefully designed communication protocol, and subsequently feeds the said approximated state into a load balancing algorithm to generate a routing decision. Specifically, we show that by using a novel approximation algorithm and server-side-adaptive communication protocol, the load balancer can obtain good queue-length approximations using a communication frequency that decays quadratically in the maximum approximation error. Furthermore, using a diffusion-scaled analysis, we prove that the load balancer achieves asymptotically optimal performance whenever the approximation error scales at a lower rate than the square-root of the total processing capacity, which includes as a special case constant-error approximations. Using simulations, we find that the proposed policies achieve performance that matches or outperforms the state-of-the-art load balancing algorithms while reducing communication rates by as much as 90%. Taken as a whole, our results demonstrate that it is possible to achieve good performance even under very sparse communication, and provide strong evidence that approximate states serve as a robust and powerful information intermediary for designing communication-efficient load balancing systems.      
### 74.Is More Data All You Need? A Causal Exploration  [ :arrow_down: ](https://arxiv.org/pdf/2206.02409.pdf)
>  Curating a large scale medical imaging dataset for machine learning applications is both time consuming and expensive. Balancing the workload between model development, data collection and annotations is difficult for machine learning practitioners, especially under time constraints. Causal analysis is often used in medicine and economics to gain insights about the effects of actions and policies. In this paper we explore the effect of dataset interventions on the output of image classification models. Through a causal approach we investigate the effects of the quantity and type of data we need to incorporate in a dataset to achieve better performance for specific subtasks. The main goal of this paper is to highlight the potential of causal analysis as a tool for resource optimization for developing medical imaging ML applications. We explore this concept with a synthetic dataset and an exemplary use-case for Diabetic Retinopathy image analysis.      
### 75.Green Interference Based Symbiotic Security in Integrated Satellite-terrestrial Communications  [ :arrow_down: ](https://arxiv.org/pdf/2206.02407.pdf)
>  In this paper, we investigate secure transmissions in integrated satellite-terrestrial communications and the green interference based symbiotic security scheme is proposed. Particularly, the co-channel interference induced by the spectrum sharing between satellite and terrestrial networks and the inter-beam interference due to frequency reuse among satellite multi-beam serve as the green interference to assist the symbiotic secure transmission, where the secure transmissions of both satellite and terrestrial links are guaranteed simultaneously. Specifically, to realize the symbiotic security, we formulate a problem to maximize the sum secrecy rate of satellite users by cooperatively beamforming optimizing and a constraint of secrecy rate of each terrestrial user is guaranteed. Since the formulated problem is non-convex and intractable, the Taylor expansion and semi-definite relaxation (SDR) are adopted to further reformulate this problem, and the successive convex approximation (SCA) algorithm is designed to solve it. Finally, the tightness of the relaxation is proved. In addition, numerical results verify the efficiency of our proposed approach.      
### 76.Robust Image Protection Countering Cropping Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02405.pdf)
>  Image cropping is an inexpensive and effective operation of maliciously altering image contents. Existing cropping detection mechanisms analyze the fundamental traces of image cropping, for example, chromatic aberration and vignetting to uncover cropping attack, yet fragile to common post-processing attacks which deceive forensics by removing such cues. Besides, they ignore the fact that recovering the cropped-out contents can unveil the purpose of the behaved cropping attack. This paper presents a novel robust watermarking scheme for image Cropping Localization and Recovery (CLR-Net). We first protect the original image by introducing imperceptible perturbations. Then, typical image post-processing attacks are simulated to erode the protected image. On the recipient's side, we predict the cropping mask and recover the original image. We propose two plug-and-play networks to improve the real-world robustness of CLR-Net, namely, the Fine-Grained generative JPEG simulator (FG-JPEG) and the Siamese image pre-processing network. To the best of our knowledge, we are the first to address the combined challenge of image cropping localization and entire image recovery from a fragment. Experiments demonstrate that CLR-Net can accurately localize the cropping as well as recover the details of the cropped-out regions with both high quality and fidelity, despite of the presence of image processing attacks of varied types.      
### 77.Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs  [ :arrow_down: ](https://arxiv.org/pdf/2206.02346.pdf)
>  We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we establish sublinear convergence rates up to a function approximation error caused by restricted policy parametrization. We also provide convergence and finite-sample complexity guarantees for two sample-based NPG-PD algorithms. Finally, we use computational experiments to showcase the merits and the effectiveness of our approach.      
### 78.Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02307.pdf)
>  Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a more important question: Can we really handle imbalanced samples to yield better performance? Hence, the key innovation in ACTION is to learn global semantic relationship across the entire dataset and local anatomical features among the neighbouring pixels with minimal additional memory footprint. During the training, we introduce anatomical contrast by actively sampling a sparse set of hard negative pixels, which can generate smoother segmentation boundaries and more accurate predictions. Extensive experiments across two benchmark datasets and different unlabeled settings show that ACTION performs comparable or better than the current state-of-the-art supervised and semi-supervised methods. Our code and models will be publicly available.      
### 79.Tagged-MRI2Audio with Attention Guided Heterogeneous Translator  [ :arrow_down: ](https://arxiv.org/pdf/2206.02284.pdf)
>  Understanding the underlying relationship between tongue and oropharyngeal muscle deformation seen in tagged-MRI and intelligible speech plays an important role in advancing speech motor control theories and treatment of speech related-disorders. Because of their heterogeneous representations, however, direct mapping between the two modalities -- i.e., two-dimensional (mid-sagittal slice) plus time tagged-MRI sequence and its corresponding one-dimensional waveform -- is not straightforward. Instead, we resort to two-dimensional spectrograms as an intermediate representation, which contains both pitch and resonance, from which to develop an end-to-end deep learning framework to translate from a sequence of tagged-MRI to its corresponding audio waveform with limited dataset size. Our framework is based on a novel fully convolutional asymmetry translator with guidance of a self residual attention strategy to specifically exploit the moving muscular structures during speech. In addition, we leverage a pairwise correlation of the samples with the same utterances with a latent space representation disentanglement strategy. Furthermore, we incorporate an adversarial training approach with generative adversarial networks to offer improved realism on our generated spectrograms. Our experimental results, carried out with a total of 63 tagged-MRI sequences alongside speech acoustics, showed that our framework enabled the generation of clear audio waveforms from a sequence of tagged-MRI, surpassing competing methods.      
### 80.Quantized and Distributed Subgradient Optimization Method with Malicious Attack  [ :arrow_down: ](https://arxiv.org/pdf/2206.02272.pdf)
>  This paper considers a distributed optimization problem in a multi-agent system where a fraction of the agents act in an adversarial manner. Specifically, the malicious agents steer the network of agents away from the optimal solution by sending false information to their neighbors and consume significant bandwidth in the communication process. We propose a distributed gradient-based optimization algorithm in which the non-malicious agents exchange quantized information with one another. We prove convergence of the solution to a neighborhood of the optimal solution, and characterize the solutions obtained under the communication-constrained environment and presence of malicious agents. Numerical simulations to illustrate the results are also presented.      
### 81.Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models  [ :arrow_down: ](https://arxiv.org/pdf/2206.02246.pdf)
>  We present a novel way of conditioning a pretrained denoising diffusion speech model to produce speech in the voice of a novel person unseen during training. The method requires a short (~3 seconds) sample from the target person, and generation is steered at inference time, without any training steps. At the heart of the method lies a sampling process that combines the estimation of the denoising model with a low-pass version of the new speaker's sample. The objective and subjective evaluations show that our sampling method can generate a voice similar to that of the target speaker in terms of frequency, with an accuracy comparable to state-of-the-art methods, and without training.      
### 82.Models of human preference for learning reward functions  [ :arrow_down: ](https://arxiv.org/pdf/2206.02231.pdf)
>  The utility of reinforcement learning is limited by the alignment of reward functions with the interests of human stakeholders. One promising method for alignment is to learn the reward function from human-generated preferences between pairs of trajectory segments. These human preferences are typically assumed to be informed solely by partial return, the sum of rewards along each segment. We find this assumption to be flawed and propose modeling preferences instead as arising from a different statistic: each segment's regret, a measure of a segment's deviation from optimal decision-making. Given infinitely many preferences generated according to regret, we prove that we can identify a reward function equivalent to the reward function that generated those preferences. We also prove that the previous partial return model lacks this identifiability property without preference noise that reveals rewards' relative proportions, and we empirically show that our proposed regret preference model outperforms it with finite training data in otherwise the same setting. Additionally, our proposed regret preference model better predicts real human preferences and also learns reward functions from these preferences that lead to policies that are better human-aligned. Overall, this work establishes that the choice of preference model is impactful, and our proposed regret preference model provides an improvement upon a core assumption of recent research.      
### 83.How does a Rational Agent Act in an Epidemic?  [ :arrow_down: ](https://arxiv.org/pdf/2206.02222.pdf)
>  Evolution of disease in a large population is a function of the top-down policy measures from a centralized planner, as well as the self-interested decisions (to be socially active) of individual agents in a large heterogeneous population. This paper is concerned with understanding the latter based on a mean-field type optimal control model. Specifically, the model is used to investigate the role of partial information on an agent's decision-making, and study the impact of such decisions by a large number of agents on the spread of the virus in the population. The motivation comes from the presymptomatic and asymptomatic spread of the COVID-19 virus where an agent unwittingly spreads the virus. We show that even in a setting with fully rational agents, limited information on the viral state can result in an epidemic growth.      
### 84.Variable-rate hierarchical CPC leads to acoustic unit discovery in speech  [ :arrow_down: ](https://arxiv.org/pdf/2206.02211.pdf)
>  The success of deep learning comes from its ability to capture the hierarchical structure of data by learning high-level representations defined in terms of low-level ones. In this paper we explore self-supervised learning of hierarchical representations of speech by applying multiple levels of Contrastive Predictive Coding (CPC). We observe that simply stacking two CPC models does not yield significant improvements over single-level architectures. Inspired by the fact that speech is often described as a sequence of discrete units unevenly distributed in time, we propose a model in which the output of a low-level CPC module is non-uniformly downsampled to directly minimize the loss of a high-level CPC module. The latter is designed to also enforce a prior of separability and discreteness in its representations by enforcing dissimilarity of successive high-level representations through focused negative sampling, and by quantization of the prediction targets. Accounting for the structure of the speech signal improves upon single-level CPC features and enhances the disentanglement of the learned representations, as measured by downstream speech recognition tasks, while resulting in a meaningful segmentation of the signal that closely resembles phone boundaries.      
### 85.M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02187.pdf)
>  Emotion Recognition in Conversations (ERC) is crucial in developing sympathetic human-machine interaction. In conversational videos, emotion can be present in multiple modalities, i.e., audio, video, and transcript. However, due to the inherent characteristics of these modalities, multi-modal ERC has always been considered a challenging undertaking. Existing ERC research focuses mainly on using text information in a discussion, ignoring the other two modalities. We anticipate that emotion recognition accuracy can be improved by employing a multi-modal approach. Thus, in this study, we propose a Multi-modal Fusion Network (M2FNet) that extracts emotion-relevant features from visual, audio, and text modality. It employs a multi-head attention-based fusion mechanism to combine emotion-rich latent representations of the input data. We introduce a new feature extractor to extract latent features from the audio and visual modality. The proposed feature extractor is trained with a novel adaptive margin-based triplet loss function to learn emotion-relevant features from the audio and visual data. In the domain of ERC, the existing methods perform well on one benchmark dataset but not on others. Our results show that the proposed M2FNet architecture outperforms all other methods in terms of weighted average F1 score on well-known MELD and IEMOCAP datasets and sets a new state-of-the-art performance in ERC.      
### 86.Optimizing Sensing Matrices for Spherical Near-Field Antenna Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2206.02181.pdf)
>  In this article, we address the problem of reducing the number of required samples for Spherical Near-Field Antenna Measurements (SNF) by using Compressed Sensing (CS). A condition to ensure the numerical performance of sparse recovery algorithms is the design of a sensing matrix with low mutual coherence. Without fixing any part of the sampling pattern, we propose sampling points that minimize the mutual coherence of the respective sensing matrix by using augmented Lagrangian method. Numerical experiments show that the proposed sampling scheme yields a higher recovery success in terms of phase transition diagram when compared to other known sampling patterns, such as the spiral and Hammersley sampling schemes. Furthermore, we also demonstrate that the application of CS with an optimized sensing matrix requires fewer samples than classical approaches to reconstruct the Spherical Mode Coefficients (SMCs) and far-field pattern.      
### 87.A Survey on Deep Learning based Channel Estimation in Doubly Dispersive Environments  [ :arrow_down: ](https://arxiv.org/pdf/2206.02165.pdf)
>  Wireless communications systems are impacted by multi-path fading and Doppler shift in dynamic environments, where the channel becomes doubly-dispersive and its estimation becomes an arduous task. Only a few pilots are used for channel estimation in conventional approaches to preserve high data rate transmission. Consequently, such estimators experience a significant performance degradation in high mobility scenarios. Recently, deep learning has been employed for doubly-dispersive channel estimation due to its low-complexity, robustness, and good generalization ability. Against this backdrop, the current paper presents a comprehensive survey on channel estimation techniques based on deep learning by deeply investigating different methods. The study also provides extensive experimental simulations followed by a computational complexity analysis. After considering different parameters such as modulation order, mobility, frame length, and deep learning architecture, the performance of the studied estimators is evaluated in several mobility scenarios. In addition, the source codes are made available online in order to make the results reproducible.      
### 88.Recurrent Video Restoration Transformer with Guided Deformable Attention  [ :arrow_down: ](https://arxiv.org/pdf/2206.02146.pdf)
>  Video restoration aims at restoring multiple high-quality frames from multiple low-quality frames. Existing video restoration methods generally fall into two extreme cases, i.e., they either restore all frames in parallel or restore the video frame by frame in a recurrent way, which would result in different merits and drawbacks. Typically, the former has the advantage of temporal information fusion. However, it suffers from large model size and intensive memory consumption; the latter has a relatively small model size as it shares parameters across frames; however, it lacks long-range dependency modeling ability and parallelizability. In this paper, we attempt to integrate the advantages of the two cases by proposing a recurrent video restoration transformer, namely RVRT. RVRT processes local neighboring frames in parallel within a globally recurrent framework which can achieve a good trade-off between model size, effectiveness, and efficiency. Specifically, RVRT divides the video into multiple clips and uses the previously inferred clip feature to estimate the subsequent clip feature. Within each clip, different frame features are jointly updated with implicit feature aggregation. Across different clips, the guided deformable attention is designed for clip-to-clip alignment, which predicts multiple relevant locations from the whole inferred clip and aggregates their features by the attention mechanism. Extensive experiments on video super-resolution, deblurring, and denoising show that the proposed RVRT achieves state-of-the-art performance on benchmark datasets with balanced model size, testing memory and runtime.      
### 89.Toward Sustainable Transportation: Accelerating Vehicle Electrification with Dynamic Charging Deployment  [ :arrow_down: ](https://arxiv.org/pdf/2206.02134.pdf)
>  Electric vehicles (EVs) are being actively adopted as a solution to sustainable transportation. However, a bottleneck remains with charging, where two of the main problems are the long charging time and the range anxiety of EV drivers. In this research, we investigate the deployment of dynamic charging systems, i.e., electrified roads that wirelessly charge EVs on the go, with a view to accelerating EVs adoption rate. We propose a traffic-based deployment strategy, statistically quantify its impact, and apply the strategy to two case studies of real traffic in New York City (USA) and Xi'an (China). We find that our analytical estimates not only closely match the real data, but they also suggest that dynamic charging considerably extends the driving range of popular EV models in urban mobility. For example, when only 5% of the existing roads in New York City are equipped with this technology, an EV model such as the Nissan Leaf will approximately maintain its battery level without stopping to recharge. If the percentage of charging roads is increased to 10%, then the Leaf will gain nearly 10% of its battery after every 40 kilometers of driving. Our framework provides a solution to public and private organizations that support and facilitate vehicle electrification through charging infrastructure.      
### 90.Computer Vision-based Characterization of Large-scale Jet Flames using a Synthetic Infrared Image Generation Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.02110.pdf)
>  Among the different kinds of fire accidents that can occur during industrial activities that involve hazardous materials, jet fires are one of the lesser-known types. This is because they are often involved in a process that generates a sequence of other accidents of greater magnitude, known as domino effect. Flame impingement usually causes domino effects, and jet fires present specific features that can significantly increase the probability of this happening. These features become relevant from a risk analysis perspective, making their proper characterization a crucial task. Deep Learning approaches have become extensively used for tasks such as jet fire characterization; however, these methods are heavily dependent on the amount of data and the quality of the labels. Data acquisition of jet fires involve expensive experiments, especially so if infrared imagery is used. Therefore, this paper proposes the use of Generative Adversarial Networks to produce plausible infrared images from visible ones, making experiments less expensive and allowing for other potential applications. The results suggest that it is possible to realistically replicate the results for experiments carried out using both visible and infrared cameras. The obtained results are compared with some previous experiments, and it is shown that similar results were obtained.      
### 91.Delay Alignment Modulation: Manipulating Channel Delay Spread for Efficient Single- and Multi-Carrier Communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.02109.pdf)
>  The evolution of mobile communication networks has always been accompanied by the advancement of inter-symbol interference (ISI) mitigation techniques, from equalization in 2G, spread spectrum and RAKE receiver in 3G, to OFDM in 4G and 5G. Looking forward towards 6G, by exploiting the extremely large spatial dimension brought by large antenna arrays and multi-path sparsity of millimeter wave (mmWave)/Terahertz channels, we propose a novel ISI mitigation technique, termed delay alignment modulation (DAM). The key ideas of DAM are path delay pre-compensation and path-based beamforming, i.e., by deliberately introducing symbol delays to compensate respective multi-path delays of the channel, so that with appropriate per-path-based beamforming, the multi-path signal components will arrive at the receiver simultaneously and constructively. To gain some insights, we first show that perfect delay alignment can be achieved to transform the time-dispersive channel to time non-dispersive channel, without sophisticated channel equalization or multi-carrier processing. This thus enables efficient equalization-free single-carrier transmission or CP-free OFDM transmission. When perfect DAM is infeasible or undesirable, we propose the generic DAM technique to significantly reduce the channel delay spread. This thus provides a new DoF to combat channel time dispersion for more efficient single- or multi-carrier signal transmissions. As an illustration, we propose the novel DAM-OFDM technique, which may save the CP overhead or mitigate the PAPR and CFO issues suffered by conventional OFDM. We show that DAM-OFDM involves joint frequency-domain and time-domain beamforming optimization, for which a closed-form solution is derived. Simulation results show that the proposed DAM-OFDM achieves significant performance gains over the conventional OFDM, in terms of spectral efficiency, BER and PAPR.      
### 92.Performance Analysis of SPAD-Based Optical Wireless Communication with OFDM  [ :arrow_down: ](https://arxiv.org/pdf/2206.02062.pdf)
>  In recent years, there has been a growing interest in the use of single-photon avalanche diode (SPAD) in optical wireless communication (OWC). SPAD operates in the Geiger mode and can act as a photon counting receiver obviating the need for a transimpedance amplifier (TIA). Although a SPAD receiver can provide higher sensitivity compared to the traditional linear photodetectors, it suffers from the dead-time-induced nonlinearity. To improve the data rates of SPAD-based OWC systems, optical orthogonal frequency division multiplexing (OFDM) can be employed. This paper provides a comprehensive theoretical analysis of the SPAD-based OWC systems using OFDM signalling considering the effects of signal clipping, SPAD nonlinearity, and signal-dependent shot noise. An equivalent additive Gaussian noise channel model is proposed to describe the performance of the SPAD-based OFDM system. The statistics of the proposed channel model and the analytical expressions of the signal-to-noise ratio (SNR) and bit error rate (BER) are derived in closed forms. By means of extensive numerical results, the impact of the unique receiver nonlinearity on the system performance is investigated. The results demonstrate new insights into different optical power regimes of reliable operation for SPAD-based OFDM systems even well beyond SPAD saturation level.      
### 93.Learning Speaker-specific Lip-to-Speech Generation  [ :arrow_down: ](https://arxiv.org/pdf/2206.02050.pdf)
>  Understanding the lip movement and inferring the speech from it is notoriously difficult for the common person. The task of accurate lip-reading gets help from various cues of the speaker and its contextual or environmental setting. Every speaker has a different accent and speaking style, which can be inferred from their visual and speech features. This work aims to understand the correlation/mapping between speech and the sequence of lip movement of individual speakers in an unconstrained and large vocabulary. We model the frame sequence as a prior to the transformer in an auto-encoder setting and learned a joint embedding that exploits temporal properties of both audio and video. We learn temporal synchronization using deep metric learning, which guides the decoder to generate speech in sync with input lip movements. The predictive posterior thus gives us the generated speech in speaker speaking style. We have trained our model on the Grid and Lip2Wav Chemistry lecture dataset to evaluate single speaker natural speech generation tasks from lip movement in an unconstrained natural setting. Extensive evaluation using various qualitative and quantitative metrics with human evaluation also shows that our method outperforms the Lip2Wav Chemistry dataset(large vocabulary in an unconstrained setting) by a good margin across almost all evaluation metrics and marginally outperforms the state-of-the-art on GRID dataset.      
### 94.A Control Theoretic Framework for Adaptive Gradient Optimizers in Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.02034.pdf)
>  Adaptive gradient methods have become popular in optimizing deep neural networks; recent examples include AdaGrad and Adam. Although Adam usually converges faster, variations of Adam, for instance, the AdaBelief algorithm, have been proposed to enhance Adam's poor generalization ability compared to the classical stochastic gradient method. This paper develops a generic framework for adaptive gradient methods that solve non-convex optimization problems. We first model the adaptive gradient methods in a state-space framework, which allows us to present simpler convergence proofs of adaptive optimizers such as AdaGrad, Adam, and AdaBelief. We then utilize the transfer function paradigm from classical control theory to propose a new variant of Adam, coined AdamSSM. We add an appropriate pole-zero pair in the transfer function from squared gradients to the second moment estimate. We prove the convergence of the proposed AdamSSM algorithm. Applications on benchmark machine learning tasks of image classification using CNN architectures and language modeling using LSTM architecture demonstrate that the AdamSSM algorithm improves the gap between generalization accuracy and faster convergence than the recent adaptive gradient methods.      
### 95.Implicit Neural Representation for Mesh-Free Inverse Obstacle Scattering  [ :arrow_down: ](https://arxiv.org/pdf/2206.02027.pdf)
>  Implicit representation of shapes as level sets of multilayer perceptrons has recently flourished in different shape analysis, compression, and reconstruction tasks. In this paper, we introduce an implicit neural representation-based framework for solving the inverse obstacle scattering problem in a mesh-free fashion. We efficiently express the obstacle shape as the zero-level set of a signed distance function which is implicitly determined by a small number of network parameters. To solve the direct scattering problem, we implement the implicit boundary integral method. It uses projections of the grid points in the tubular neighborhood onto the boundary to compute the PDE solution instead of a grid-size-dependent extraction method of surface points such as Marching Cubes. The implicit representation conveniently handles the shape perturbation in the optimization process. To update the shape, we use PyTorch's automatic differentiation to backpropagate the loss function w.r.t. the network parameters, allowing us to avoid complex and error-prone manual derivation of the shape derivative. The proposed framework makes the inverse scattering problem more tractable with fewer parameters to optimize in comparison to the memory-inefficient grid-based approaches and outputs high-quality reconstruction results.      
### 96.Geodesic Properties of a Generalized Wasserstein Embedding for Time Series Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2206.01984.pdf)
>  Transport-based metrics and related embeddings (transforms) have recently been used to model signal classes where nonlinear structures or variations are present. In this paper, we study the geodesic properties of time series data with a generalized Wasserstein metric and the geometry related to their signed cumulative distribution transforms in the embedding space. Moreover, we show how understanding such geometric characteristics can provide added interpretability to certain time series classifiers, and be an inspiration for more robust classifiers.      
### 97.Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image  [ :arrow_down: ](https://arxiv.org/pdf/2206.01856.pdf)
>  Image enhancement approaches often assume that the noise is signal independent, and approximate the degradation model as zero-mean additive Gaussian noise. However, this assumption does not hold for biomedical imaging systems where sensor-based sources of noise are proportional to signal strengths, and the noise is better represented as a Poisson process. In this work, we explore a sparsity and dictionary learning-based approach and present a novel self-supervised learning method for single-image denoising where the noise is approximated as a Poisson process, requiring no clean ground-truth data. Specifically, we approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network which enforces sparsity with respect to the weights of the network. Since the sparse representations are based on the underlying image, it is able to suppress the spurious components (noise) in the image patches, thereby introducing implicit regularization for denoising task through the network structure. Experiments on two bio-imaging datasets demonstrate that our method outperforms the state-of-the-art approaches in terms of PSNR and SSIM. Our qualitative results demonstrate that, in addition to higher performance on standard quantitative metrics, we are able to recover much more subtle details than other compared approaches.      
### 98.Leveraging Heterogeneous Capabilities in Multi-Agent Systems for Environmental Conflict Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2206.01833.pdf)
>  In this paper, we introduce a high-level controller synthesis framework that enables teams of heterogeneous agents to assist each other in resolving environmental conflicts that appear at runtime. This conflict resolution method is built upon temporal-logic-based reactive synthesis to guarantee safety and task completion under specific environment assumptions. In heterogeneous multi-agent systems, every agent is expected to complete its own tasks in service of a global team objective. However, at runtime, an agent may encounter un-modeled obstacles (e.g., doors or walls) that prevent it from achieving its own task. To address this problem, we take advantage of the capability of other heterogeneous agents to resolve the obstacle. A controller framework is proposed to redirect agents with the capability of resolving the appropriate obstacles to the required target when such a situation is detected. A set of case studies involving a bipedal robot Digit and a quadcopter are used to evaluate the controller performance in action. Additionally, we implement the proposed framework on a physical multi-agent robotic system to demonstrate its viability for real world applications.      
### 99.The Gamma Generalized Normal Distribution: A Descriptor of SAR Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2206.01826.pdf)
>  We propose a new four-parameter distribution for modeling synthetic aperture radar (SAR) imagery named the gamma generalized normal (GGN) by combining the gamma and generalized normal distributions. A mathematical characterization of the new distribution is provided by identifying the limit behavior and by calculating the density and moment expansions. The GGN model performance is evaluated on both synthetic and actual data and, for that, maximum likelihood estimation and random number generation are discussed. The proposed distribution is compared with the beta generalized normal distribution (BGN), which has already shown to appropriately represent SAR imagery. The performance of these two distributions are measured by means of statistics which provide evidence that the GGN can outperform the BGN distribution in some contexts.      
### 100.Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata  [ :arrow_down: ](https://arxiv.org/pdf/2206.01813.pdf)
>  Most camera images are rendered and saved in the standard RGB (sRGB) format by the camera's hardware. Due to the in-camera photo-finishing routines, nonlinear sRGB images are undesirable for computer vision tasks that assume a direct relationship between pixel values and scene radiance. For such applications, linear raw-RGB sensor images are preferred. Saving images in their raw-RGB format is still uncommon due to the large storage requirement and lack of support by many imaging applications. Several "raw reconstruction" methods have been proposed that utilize specialized metadata sampled from the raw-RGB image at capture time and embedded in the sRGB image. This metadata is used to parameterize a mapping function to de-render the sRGB image back to its original raw-RGB format when needed. Existing raw reconstruction methods rely on simple sampling strategies and global mapping to perform the de-rendering. This paper shows how to improve the de-rendering results by jointly learning sampling and reconstruction. Our experiments show that our learned sampling can adapt to the image content to produce better raw reconstructions than existing methods. We also describe an online fine-tuning strategy for the reconstruction network to improve results further.      
### 101.Optimal Competitive-Ratio Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.01782.pdf)
>  Inspired by competitive policy designs approaches in online learning, new control paradigms such as competitive-ratio and regret-optimal control have been recently proposed as alternatives to the classical $\mathcal{H}_2$ and $\mathcal{H}_\infty$ approaches. These competitive metrics compare the control cost of the designed controller against the cost of a clairvoyant controller, which has access to past, present, and future disturbances in terms of ratio and difference, respectively. While prior work provided the optimal solution for the regret-optimal control problem, in competitive-ratio control, the solution is only provided for the sub-optimal problem. In this work, we derive the optimal solution to the competitive-ratio control problem. We show that the optimal competitive ratio formula can be computed as the maximal eigenvalue of a simple matrix, and provide a state-space controller that achieves the optimal competitive ratio. We conduct an extensive numerical study to verify this analytical solution, and demonstrate that the optimal competitive-ratio controller outperforms other controllers on several large scale practical systems. The key techniques that underpin our explicit solution is a reduction of the control problem to a Nehari problem, along with a novel factorization of the clairvoyant controller's cost. We reveal an interesting relation between the explicit solutions that now exist for both competitive control paradigms by formulating a regret-optimal control framework with weight functions that can also be utilized for practical purposes.      
### 102.Real-Time Super-Resolution for Real-World Images on Mobile Devices  [ :arrow_down: ](https://arxiv.org/pdf/2206.01777.pdf)
>  Image Super-Resolution (ISR), which aims at recovering High-Resolution (HR) images from the corresponding Low-Resolution (LR) counterparts. Although recent progress in ISR has been remarkable. However, they are way too computationally intensive to be deployed on edge devices, since most of the recent approaches are deep learning-based. Besides, these methods always fail in real-world scenes, since most of them adopt a simple fixed "ideal" bicubic downsampling kernel from high-quality images to construct LR/HR training pairs which may lose track of frequency-related details. In this work, an approach for real-time ISR on mobile devices is presented, which is able to deal with a wide range of degradations in real-world scenarios. Extensive experiments on traditional super-resolution datasets (Set5, Set14, BSD100, Urban100, Manga109, DIV2K) and real-world images with a variety of degradations demonstrate that our method outperforms the state-of-art methods, resulting in higher PSNR and SSIM, lower noise and better visual quality. Most importantly, our method achieves real-time performance on mobile or edge devices.      
### 103.Using UAS Imagery and Computer Vision to Support Site-Specific Weed Control in Corn  [ :arrow_down: ](https://arxiv.org/pdf/2206.01734.pdf)
>  Currently, weed control in a corn field is performed by a blanket application of herbicides that do not consider spatial distribution information of weeds and also uses an extensive amount of chemical herbicides. To reduce the amount of chemicals, we used drone-based high-resolution imagery and computer-vision techniques to perform site-specific weed control in corn.      
### 104.Adversarial RAW: Image-Scaling Attack Against Imaging Pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2206.01733.pdf)
>  Deep learning technologies have become the backbone for the development of computer vision. With further explorations, deep neural networks have been found vulnerable to well-designed adversarial attacks. Most of the vision devices are equipped with image signal processing (ISP) pipeline to implement RAW-to-RGB transformations and embedded into data preprocessing module for efficient image processing. Actually, ISP pipeline can introduce adversarial behaviors to post-capture images while data preprocessing may destroy attack patterns. However, none of the existing adversarial attacks takes into account the impacts of both ISP pipeline and data preprocessing. In this paper, we develop an image-scaling attack targeting on ISP pipeline, where the crafted adversarial RAW can be transformed into attack image that presents entirely different appearance once being scaled to a specific-size image. We first consider the gradient-available ISP pipeline, i.e., the gradient information can be directly used in the generation process of adversarial RAW to launch the attack. To make the adversarial attack more applicable, we further consider the gradient-unavailable ISP pipeline, in which a proxy model that well learns the RAW-to-RGB transformations is proposed as the gradient oracles. Extensive experiments show that the proposed adversarial attacks can craft adversarial RAW data against the target ISP pipelines with high attack rates.      
