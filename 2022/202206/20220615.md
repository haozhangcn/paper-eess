# ArXiv eess --Wed, 15 Jun 2022
### 1.A deterministic view on explicit data-driven (M)PC  [ :arrow_down: ](https://arxiv.org/pdf/2206.07025.pdf)
>  We show that the explicit realization of data-driven predictive control (DPC) for linear deterministic systems is more tractable than previously thought. To this end, we compare the optimal control problems (OCP) corresponding to deterministic DPC and classical model predictive control (MPC), specify its close relation, and systematically eliminate ambiguity inherent in DPC. As a central result, we find that the explicit solutions to these types of DPC and MPC are of exactly the same complexity. We illustrate our results with two numerical examples highlighting features of our approach.      
### 2.A Note on Low-Pass Filter Conditioning for Current-Mode Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.06997.pdf)
>  The low-pass filter is a classic control conditioning approach for high-frequency current-mode control. However, no existing literature discusses the large-signal stability criterion for the current-mode control with low-pass filters. This paper provides a mathematically rigorous large-signal stability criterion. The result can directly benefit the practical engineering implementation of the low-pass filter in high-frequency current-mode control.      
### 3.K-Space Transformer for Fast MRI Reconstruction with Implicit Representation  [ :arrow_down: ](https://arxiv.org/pdf/2206.06947.pdf)
>  This paper considers the problem of fast MRI reconstruction. We propose a novel Transformer-based framework for directly processing the sparsely sampled signals in k-space, going beyond the limitation of regular grids as ConvNets do. We adopt an implicit representation of spectrogram, treating spatial coordinates as inputs, and dynamically query the partially observed measurements to complete the spectrogram, i.e. learning the inductive bias in k-space. To strive a balance between computational cost and reconstruction quality, we build an hierarchical structure with low-resolution and high-resolution decoders respectively. To validate the necessity of our proposed modules, we have conducted extensive experiments on two public datasets, and demonstrate superior or comparable performance over state-of-the-art approaches.      
### 4.Creating and Operating Areas With Reduced Electromagnetic Field Exposure Thanks to Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2206.06880.pdf)
>  Mobile network operators must provide a target quality of service within a target coverage area. Each generation of networks from the 2nd to the 5th has reached higher quality-of-service (QoS) targets and coverage area sizes. However, the deployment of new networks is sometimes challenged by Electromagnetic Field (EMF) exposure constraints. In this paper, to take into account these constraints, we assess the novel and recent concept of Reduced EMF Exposure Area. Such an area would be created and operated by a mobile network operator upon the request of its customers. In such an area, customers keep enjoying high data rate internet access while getting a reduced EMF exposure. To ensure this EMF exposure reduction, we propose to deploy Reconfigurable Intelligent Surfaces (RIS) connected to the mobile network and exploit a joint RIS-M-MIMO uplink beamforming scheme. We use our ray-tracing-based simulation tool to visualize and characterize the Reduced EMF Exposure Area in a challenging environment in terms of propagation. Our simulations show that an operator can create and operate such an area under the condition that it carefully places the RIS in the environment.      
### 5.A Novel RIS-Aided EMF Exposure Aware Approach using an Angularly Equalized Virtual Propagation Channel  [ :arrow_down: ](https://arxiv.org/pdf/2206.06870.pdf)
>  Massive Multiple-Input Multiple-Output systems with beamforming are key components of the 5th and the future 6th generation of networks. However, in some cases, where the BS serves the same user for a long period, and in some propagation conditions, such systems reduce their transmit power to avoid creating unwanted regions of electromagnetic field exposure exceeding the regulatory threshold, beyond the circle around the BS that limits the distance between people and the BS antenna. Such power reduction strongly degrades the received power at the target user. Recently, exposition aware beamforming schemes aided by self-tuning reconfigurable intelligent surfaces derived from maximum ratio transmission beamforming, have been proposed: truncated beamforming. However, such scheme is highly complex. In this paper, we propose a novel and low complexity reconfigurable intelligent surface aided beamforming scheme called Equalized beamforming, which applies maximum ratio transmission to an angularly equalized virtual propagation channel. Our simulations show that our proposed scheme outperforms the reduced beamforming scheme, whilst complying with the exposition regulation.      
### 6.Combining Image Space and q-Space PDEs for Lossless Compression of Diffusion MR Images  [ :arrow_down: ](https://arxiv.org/pdf/2206.06846.pdf)
>  Diffusion MRI is a modern neuroimaging modality with a unique ability to acquire microstructural information by measuring water self-diffusion at the voxel level. However, it generates huge amounts of data, resulting from a large number of repeated 3D scans. Each volume samples a location in q-space, indicating the direction and strength of a diffusion sensitizing gradient during the measurement. This captures detailed information about the self-diffusion, and the tissue microstructure that restricts it. Lossless compression with GZIP is widely used to reduce the memory requirements. We introduce a novel lossless codec for diffusion MRI data. It reduces file sizes by more than 30% compared to GZIP, and also beats lossless codecs from the JPEG family. Our codec builds on recent work on lossless PDE-based compression of 3D medical images, but additionally exploits smoothness in q-space. We demonstrate that, compared to using only image space PDEs, q-space PDEs further improve compression rates. Moreover, implementing them with Finite Element Methods and a custom acceleration significantly reduces computational expense. Finally, we show that our codec clearly benefits from integrating subject motion correction, and slightly from optimizing the order in which the 3D volumes are coded.      
### 7.Fully Automated Assessment of Cardiac Coverage in Cine Cardiovascular Magnetic Resonance Images using an Explainable Deep Visual Salient Region Detection Model  [ :arrow_down: ](https://arxiv.org/pdf/2206.06844.pdf)
>  Cardiovascular magnetic resonance (CMR) imaging has become a modality with superior power for the diagnosis and prognosis of cardiovascular diseases. One of the essential basic quality controls of CMR images is to investigate the complete cardiac coverage, which is necessary for the volumetric and functional assessment. This study examines the full cardiac coverage using a 3D convolutional model and then reduces the number of false predictions using an innovative salient region detection model. Salient regions are extracted from the short-axis cine CMR stacks using a three-step proposed algorithm. Combining the 3D CNN baseline model with the proposed salient region detection model provides a cascade detector that can reduce the number of false negatives of the baseline model. The results obtained on the images of over 6,200 participants of the UK Biobank population cohort study show the superiority of the proposed model over the previous state-of-the-art studies. The dataset is the largest regarding the number of participants to control the cardiac coverage. The accuracy of the baseline model in identifying the presence/absence of basal/apical slices is 96.25\% and 94.51\%, respectively, which increases to 96.88\% and 95.72\% after improving using the proposed salient region detection model. Using the salient region detection model by forcing the baseline model to focus on the most informative areas of the images can help the model correct misclassified samples' predictions. The proposed fully automated model's performance indicates that this model can be used in image quality control in population cohort datasets and also real-time post-imaging quality assessments.      
### 8.Distributed Coordination of Charging Stations Considering Aggregate EV Power Flexibility  [ :arrow_down: ](https://arxiv.org/pdf/2206.06834.pdf)
>  In recent years, electric vehicle (EV) charging stations have witnessed a rapid growth. However, effective management of charging stations is challenging due to individual EV owners' privacy concerns, competing interests of different stations, and the coupling distribution network constraints. To cope with this challenge, this paper proposes a two-stage scheme. In the first stage, the aggregate EV power flexibility region is derived by solving an optimization problem. We prove that any trajectory within the obtained region corresponds to at least one feasible EV dispatch strategy. By submitting this flexibility region instead of the detailed EV data to the charging station operator, EV owners' privacy can be preserved and the computational burden can be reduced. In the second stage, a distributed coordination mechanism with a clear physical interpretation is developed with consideration of AC power flow constraints. We prove that the proposed mechanism is guaranteed to converge to the centralized optimum. Case studies validate the theoretical results. Comprehensive performance comparisons are carried out to demonstrate the advantages of the proposed scheme.      
### 9.Tailored max-out networks for learning convex PWQ functions  [ :arrow_down: ](https://arxiv.org/pdf/2206.06826.pdf)
>  Convex piecewise quadratic (PWQ) functions frequently appear in control and elsewhere. For instance, it is well-known that the optimal value function (OVF) as well as Q-functions for linear MPC are convex PWQ functions. Now, in learning-based control, these functions are often represented with the help of artificial neural networks (NN). In this context, a recurring question is how to choose the topology of the NN in terms of depth, width, and activations in order to enable efficient learning. An elegant answer to that question could be a topology that, in principle, allows to exactly describe the function to be learned. Such solutions are already available for related problems. In fact, suitable topologies are known for piecewise affine (PWA) functions that can, for example, reflect the optimal control law in linear MPC. Following this direction, we show in this paper that convex PWQ functions can be exactly described by max-out-NN with only one hidden layer and two neurons.      
### 10.Design of optical voltage sensor based on electric field regulation and rotating isomerism electrode  [ :arrow_down: ](https://arxiv.org/pdf/2206.06822.pdf)
>  Temperature drift, stress birefringence and low frequency vibration lead to the randomness and fluctuation of the output of optical voltage sensor(OVS). In order to solve the problem, this study adopts the lock-in amplifier technology with the aid of a high-speed rotating electrode to realize electric field modulation. This technology could shift the measured signal frequency band from near 50 Hz moved to several kilometer Hz, so as to make the output signal avoid the interference from low-frequency temperature drift, stress birefringence and vibration, leading to higher stability and reliability. The electro-optic coupling wave theory and static electric field finite element method are utilized to investigate the shape of modulation wave. The simulation results proves that lock-in technology is able to prevent the measured voltage signal from the large step signal interference and restore the perfect original signal. While the sample rate is decreased to the value of the modulation frequency.      
### 11.Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites  [ :arrow_down: ](https://arxiv.org/pdf/2206.06813.pdf)
>  In clinical practice, a segmentation network is often required to continually learn on a sequential data stream from multiple sites rather than a consolidated set, due to the storage cost and privacy restriction. However, during the continual learning process, existing methods are usually restricted in either network memorizability on previous sites or generalizability on unseen sites. This paper aims to tackle the challenging problem of Synchronous Memorizability and Generalizability (SMG) and to simultaneously improve performance on both previous and unseen sites, with a novel proposed SMG-learning framework. First, we propose a Synchronous Gradient Alignment (SGA) objective, which \emph{not only} promotes the network memorizability by enforcing coordinated optimization for a small exemplar set from previous sites (called replay buffer), \emph{but also} enhances the generalizability by facilitating site-invariance under simulated domain shift. Second, to simplify the optimization of SGA objective, we design a Dual-Meta algorithm that approximates the SGA objective as dual meta-objectives for optimization without expensive computation overhead. Third, for efficient rehearsal, we configure the replay buffer comprehensively considering additional inter-site diversity to reduce redundancy. Experiments on prostate MRI data sequentially acquired from six institutes demonstrate that our method can simultaneously achieve higher memorizability and generalizability over state-of-the-art methods. Code is available at <a class="link-external link-https" href="https://github.com/jingyzhang/SMG-Learning" rel="external noopener nofollow">this https URL</a>.      
### 12.Adversarial Audio Synthesis with Complex-valued Polynomial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.06811.pdf)
>  Time-frequency (TF) representations in audio synthesis have been increasingly modeled with real-valued networks. However, overlooking the complex-valued nature of TF representations can result in suboptimal performance and require additional modules (e.g., for modeling the phase). To this end, we introduce complex-valued polynomial networks, called APOLLO, that integrate such complex-valued representations in a natural way. Concretely, APOLLO captures high-order correlations of the input elements using high-order tensors as scaling parameters. By leveraging standard tensor decompositions, we derive different architectures and enable modeling richer correlations. We outline such architectures and showcase their performance in audio generation across four benchmarks. As a highlight, APOLLO results in $17.5\%$ improvement over adversarial methods and $8.2\%$ over the state-of-the-art diffusion models on SC09 dataset in audio generation. Our models can encourage the systematic design of other efficient architectures on the complex field.      
### 13.RIS Assisted Device Activity Detection with Statistical Channel State Information  [ :arrow_down: ](https://arxiv.org/pdf/2206.06805.pdf)
>  This paper studies reconfigurable intelligent surface (RIS) assisted device activity detection for grant-free (GF) uplink transmission in wireless communication networks. In particular, we consider mobile devices located in an area where the direct link to an access point (AP) is blocked. Thus, the devices try to connect to the AP via a reflected link provided by an RIS. Therefore, a RIS phase-shift design is desired that covers the entire blocked area with a wide reflection beam because the exact locations and times of activity of the devices are unknown in GF transmission. In order to study the impact of the phase-shift design on the device activity detection, we derive a generalized likelihood ratio test (GLRT) based detector and present an analytical expression for the probability of detection. Assuming knowledge of statistical CSI, we formulate an optimization problem for the phase-shift design for maximization of the guaranteed probability of detection for all locations within a given coverage area. To tackle the non-convexity of the problem, we propose two different approximations of the objective function. The first approximation leads to a design that aims to reduce the variations of the end-to-end channel while taking system parameters such as transmit power, noise power, and probability of false alarm into account. The second approximation can be adopted for versatile RIS deployments because it only depends on the line-of-sight component of the end-to-end channel and is not affected by system parameters. For comparison, we also consider a phase-shift design maximizing the average channel gain and a baseline analytical phase-shift design for large blocked areas. Our performance evaluation shows that the proposed approximations result in phase-shift designs that guarantee high probability of detection across the coverage area and outperform the baseline designs.      
### 14.Artificial Neural Network For Transient Stability Assessment: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2206.06800.pdf)
>  Integration of large-scale renewable energy sources and increasing uncertainty has drastically changed the dynamics of power system and has consequently brought various challenges. Rapid transient stability assessment of modern power system is a vital requirement for accurate power system planning and operation. The conventional methods are unable to fulfil this requirement. Therefore, novel approaches are required in this regard. Machine leaning approaches such as artificial neural network can play a significant role in this regard. Therefore, this paper aims to review the application of artificial neural network for transient stability assessment of power systems. It is believed that this work will provide a solid foundation for researchers in the domain of machine learning applications to power system security and stability.      
### 15.PhML-DyR: A Physics-Informed ML framework for Dynamic Reconfiguration in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.06789.pdf)
>  A transformation of the US electricity sector is underway with aggressive targets to achieve 100% carbon pollution-free electricity by 2035. To achieve this objective while maintaining a safe and reliable power grid, new operating paradigms are needed, of computationally fast and accurate decision making in a dynamic and uncertain environment. We propose a novel physics-informed machine learning framework for the decision of dynamic grid reconfiguration (PhML-DyR), a key task in power systems. Dynamic reconfiguration (DyR) is a process by which switch-states are dynamically set so as to lead to an optimal grid topology that minimizes line losses. To address the underlying computational complexities of NP-hardness due to the mixed nature of the decision variables, we propose the use of physics-informed ML (PhML) which integrates both operating constraints and topological and connectivity constraints into a neural network framework. Our PhML approach learns to simultaneously optimize grid topology and generator dispatch to meet loads, increase efficiency, and remain within safe operating limits. We demonstrate the effectiveness of PhML-DyR on a canonical grid, showing a reduction in electricity loss by 23%, and improved voltage profiles. We also show a reduction in constraint violations by an order of magnitude as well as in training time using PhML-DyR.      
### 16.Physics-driven Deep Learning for PET/MRI  [ :arrow_down: ](https://arxiv.org/pdf/2206.06788.pdf)
>  In this paper, we review physics- and data-driven reconstruction techniques for simultaneous positron emission tomography (PET) / magnetic resonance imaging (MRI) systems, which have significant advantages for clinical imaging of cancer, neurological disorders, and heart disease. These reconstruction approaches utilize priors, either structural or statistical, together with a physics-based description of the PET system response. However, due to the nested representation of the forward problem, direct PET/MRI reconstruction is a nonlinear problem. We elucidate how a multi-faceted approach accommodates hybrid data- and physics-driven machine learning for reconstruction of 3D PET/MRI, summarizing important deep learning developments made in the last 5 years to address attenuation correction, scattering, low photon counts, and data consistency. We also describe how applications of these multi-modality approaches extend beyond PET/MRI to improving accuracy in radiation therapy planning. We conclude by discussing opportunities for extending the current state-of-the-art following the latest trends in physics- and deep learning-based computational imaging and next-generation detector hardware.      
### 17.Stochastic Event-triggered Variational Bayesian Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2206.06784.pdf)
>  This paper proposes an event-triggered variational Bayesian filter for remote state estimation with unknown and time-varying noise covariances. After presetting multiple nominal process noise covariances and an initial measurement noise covariance, a variational Bayesian method and a fixed-point iteration method are utilized to jointly estimate the posterior state vector and the unknown noise covariances under a stochastic event-triggered mechanism. The proposed algorithm ensures low communication loads and excellent estimation performances for a wide range of unknown noise covariances. Finally, the performance of the proposed algorithm is demonstrated by tracking simulations of a vehicle.      
### 18.Time Optimization of Constrained Control for a Thermoelectric Solid System with a Peltier Element  [ :arrow_down: ](https://arxiv.org/pdf/2206.06745.pdf)
>  A solid system consisting of two heat conducting cylinders with a thermoelectric converter (Peltier element) between them is considered. A nonlinear model, which was previously verified by authors, is used to design a constrained control law that allows us to achieve a steady-state distribution of the temperature in one of the cylinders in much less time than the characteristic time of transient processes. The initial-boundary value problem is exactly linearized over temperature by means of feedback linearization. Although the resulting system is nonlinear in a control function, it is possible to construct a finite-dimensional approximation based on analytical solution of the corresponding eigenproblem for a constant control signal. The time-optimal control problem is studied numerically by using this eigenfunction decomposition. To construct admissible control laws, an auxiliary unconstrained optimization problem is introduced. Its cost functional represents a weighted sum of temperature deviation from the desired zero distribution and a penalty for violating an electric power constraint. The control time interval is split into several parts, and on each subinterval the control signal is taken constant. The optimal piecewise constant feedforward control is found numerically by applying the gradient descent method. We analyze the proposed control law with respect to the shortest admissible time of the process.      
### 19.Automated Precision Localization of Peripherally Inserted Central Catheter Tip through Model-Agnostic Multi-Stage Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.06730.pdf)
>  Peripherally inserted central catheters (PICCs) have been widely used as one of the representative central venous lines (CVCs) due to their long-term intravascular access with low infectivity. However, PICCs have a fatal drawback of a high frequency of tip mispositions, increasing the risk of puncture, embolism, and complications such as cardiac arrhythmias. To automatically and precisely detect it, various attempts have been made by using the latest deep learning (DL) technologies. However, even with these approaches, it is still practically difficult to determine the tip location because the multiple fragments phenomenon (MFP) occurs in the process of predicting and extracting the PICC line required before predicting the tip. This study aimed to develop a system generally applied to existing models and to restore the PICC line more exactly by removing the MFs of the model output, thereby precisely localizing the actual tip position for detecting its disposition. To achieve this, we proposed a multi-stage DL-based framework post-processing the PICC line extraction result of the existing technology. The performance was compared by each root mean squared error (RMSE) and MFP incidence rate according to whether or not MFCN is applied to five conventional models. In internal validation, when MFCN was applied to the existing single model, MFP was improved by an average of 45%. The RMSE was improved by over 63% from an average of 26.85mm (17.16 to 35.80mm) to 9.72mm (9.37 to 10.98mm). In external validation, when MFCN was applied, the MFP incidence rate decreased by an average of 32% and the RMSE decreased by an average of 65\%. Therefore, by applying the proposed MFCN, we observed the significant/consistent detection performance improvement of PICC tip location compared to the existing model.      
### 20.Automated SSIM Regression for Detection and Quantification of Motion Artefacts in Brain MR Images  [ :arrow_down: ](https://arxiv.org/pdf/2206.06725.pdf)
>  Motion artefacts in magnetic resonance brain images are a crucial issue. The assessment of MR image quality is fundamental before proceeding with the clinical diagnosis. If the motion artefacts alter a correct delineation of structure and substructures of the brain, lesions, tumours and so on, the patients need to be re-scanned. Otherwise, neuro-radiologists could report an inaccurate or incorrect diagnosis. The first step right after scanning a patient is the "\textit{image quality assessment}" in order to decide if the acquired images are diagnostically acceptable. An automated image quality assessment based on the structural similarity index (SSIM) regression through a residual neural network has been proposed here, with the possibility to perform also the classification in different groups - by subdividing with SSIM ranges. This method predicts SSIM values of an input image in the absence of a reference ground truth image. The networks were able to detect motion artefacts, and the best performance for the regression and classification task has always been achieved with ResNet-18 with contrast augmentation. Mean and standard deviation of residuals' distribution were $\mu=-0.0009$ and $\sigma=0.0139$, respectively. Whilst for the classification task in 3, 5 and 10 classes, the best accuracies were 97, 95 and 89\%, respectively. The obtained results show that the proposed method could be a tool in supporting neuro-radiologists and radiographers in evaluating the image quality before the diagnosis.      
### 21.A Neural Network-Based Energy Management System for PV-Battery Based Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2206.06716.pdf)
>  A neural network-based energy management system (NN-EMS) has been proposed in this paper for islanded ac microgrids fed by multiple PV-battery based distributed generators (DG). The stochastic and unequal irradiation results in unequal PV output, which causes an unequal state-of-charge (SoC) among the batteries of the DGs. This effect may cause the difference in the SoCs to increase considerably over time, leading to some batteries reaching their SoC limits. These batteries would no longer be able to control the dc-link of the hybrid grid forming DG. The proposed NN-EMS ensures SoC balancing by learning an optimal state-action mapping using the outputs of an optimal power flow (OPF). The training dataset has been generated by executing a mixed-integer linear programming based OPF for droop-based island microgrids considering a practical generation-load profile. The resultant NN-EMS controller inherits the information of optimal states and the network behaviour. Compared to traditional time-ahead centralized methods, the proposed strategy does not require accurate generation-load forecasting. Further, it can also respond to the variations in the PV power in near-real-time without resorting to solving an OPF. The proposed NN-EMS controller has been validated by case studies on a CIGRE LV microgrid containing PV-battery hybrid DGs. The proposed concept can also be extended to synthesize decentralized controllers that can cooperate among themselves to achieve a global objective without communication.      
### 22.CNN-based Classification Framework for Tissues of Lung with Additional Information  [ :arrow_down: ](https://arxiv.org/pdf/2206.06701.pdf)
>  Interstitial lung diseases are a large group of heterogeneous diseases characterized by different degrees of alveolitis and pulmonary fibrosis. Accurately diagnosing these diseases has significant guiding value for formulating treatment plans. Although previous work has produced impressive results in classifying interstitial lung diseases, there is still room for improving the accuracy of these techniques, mainly to enhance automated decision-making. In order to improve the classification precision, our study proposes a convolutional neural networks-based framework with additional information. Firstly, ILD images are added with their medical information by re-scaling the original image in Hounsfield Units. Secondly, a modified CNN model is used to produce a vector of classification probability for each tissue. Thirdly, location information of the input image, consisting of the occurrence frequencies of different diseases in the CT scans on certain locations, is used to calculate a location weight vector. Finally, the Hadamard product between two vectors is used to produce a decision vector for the prediction. Compared to the state-of-the-art methods, the results using a publicly available ILD database show the potential of predicting these using different additional information.      
### 23.The Open Kidney Ultrasound Data Set  [ :arrow_down: ](https://arxiv.org/pdf/2206.06657.pdf)
>  Ultrasound use is because of its low cost, non-ionizing, and non-invasive characteristics, and has established itself as a cornerstone radiological examination. Research on ultrasound applications has also expanded, especially with image analysis with machine learning. However, ultrasound data are frequently restricted to closed data sets, with only a few openly available. Despite being a frequently examined organ, the kidney lacks a publicly available ultrasonography data set. The proposed Open Kidney Ultrasound Data Set is the first publicly available set of kidney B-mode ultrasound data that includes annotations for multi-class semantic segmentation. It is based on data retrospectively collected in a 5-year period from over 500 patients with a mean age of 53.2 +/- 14.7 years, body mass index of 27.0 +/- 5.4 kg/m2, and most common primary diseases being diabetes mellitus, IgA nephropathy, and hypertension. There are labels for the view and fine-grained manual annotations from two expert sonographers. Notably, this data includes native and transplanted kidneys. Initial benchmarking measurements are performed, demonstrating a state-of-the-art algorithm achieving a Dice Sorenson Coefficient of 0.74 for the kidney capsule. This data set is a high-quality data set, including two sets of expert annotations, with a larger breadth of images than previously available. In increasing access to kidney ultrasound data, future researchers may be able to create novel image analysis techniques for tissue characterization, disease detection, and prognostication.      
### 24.The Kidneys Are Not All Normal: Investigating the Speckle Distributions of Transplanted Kidneys  [ :arrow_down: ](https://arxiv.org/pdf/2206.06654.pdf)
>  Modelling ultrasound speckle has generated considerable interest for its ability to characterize tissue properties. As speckle is dependent on the underlying tissue architecture, modelling it may aid in tasks like segmentation or disease detection. However, for the transplanted kidney where ultrasound is commonly used to investigate dysfunction, it is currently unknown which statistical distribution best characterises such speckle. This is especially true for the regions of the transplanted kidney: the cortex, the medulla and the central echogenic complex. Furthermore, it is unclear how these distributions vary by patient variables such as age, sex, body mass index, primary disease, or donor type. These traits may influence speckle modelling given their influence on kidney anatomy. We are the first to investigate these two aims. N=821 kidney transplant recipient B-mode images were automatically segmented into the cortex, medulla, and central echogenic complex using a neural network. Seven distinct probability distributions were fitted to each region. The Rayleigh and Nakagami distributions had model parameters that differed significantly between the three regions (p &lt;= 0.05). While both had excellent goodness of fit, the Nakagami had higher Kullbeck-Leibler divergence. Recipient age correlated weakly with scale in the cortex (Omega: rho = 0.11, p = 0.004), while body mass index correlated weakly with shape in the medulla (m: rho = 0.08, p = 0.04). Neither sex, primary disease, nor donor type demonstrated any correlation. We propose the Nakagami distribution be used to characterize transplanted kidneys regionally independent of disease etiology and most patient characteristics based on our findings.      
### 25.ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2206.06623.pdf)
>  Neoadjuvant therapy (NAT) for breast cancer is a common treatment option in clinical practice. Tumor cellularity (TC), which represents the percentage of invasive tumors in the tumor bed, has been widely used to quantify the response of breast cancer to NAT. Therefore, automatic TC estimation is significant in clinical practice. However, existing state-of-the-art methods usually take it as a TC score regression problem, which ignores the ambiguity of TC labels caused by subjective assessment or multiple raters. In this paper, to efficiently leverage the label ambiguities, we proposed an Uncertainty-aware Label disTRibution leArning (ULTRA) framework for automatic TC estimation. The proposed ULTRA first converted the single-value TC labels to discrete label distributions, which effectively models the ambiguity among all possible TC labels. Furthermore, the network learned TC label distributions by minimizing the Kullback-Leibler (KL) divergence between the predicted and ground-truth TC label distributions, which better supervised the model to leverage the ambiguity of TC labels. Moreover, the ULTRA mimicked the multi-rater fusion process in clinical practice with a multi-branch feature fusion module to further explore the uncertainties of TC labels. We evaluated the ULTRA on the public BreastPathQ dataset. The experimental results demonstrate that the ULTRA outperformed the regression-based methods for a large margin and achieved state-of-the-art results. The code will be available from <a class="link-external link-https" href="https://github.com/PerceptionComputingLab/ULTRA" rel="external noopener nofollow">this https URL</a>      
### 26.CorticalFlow$^{++}$: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability  [ :arrow_down: ](https://arxiv.org/pdf/2206.06598.pdf)
>  The problem of Cortical Surface Reconstruction from magnetic resonance imaging has been traditionally addressed using lengthy pipelines of image processing techniques like FreeSurfer, CAT, or CIVET. These frameworks require very long runtimes deemed unfeasible for real-time applications and unpractical for large-scale studies. Recently, supervised deep learning approaches have been introduced to speed up this task cutting down the reconstruction time from hours to seconds. Using the state-of-the-art CorticalFlow model as a blueprint, this paper proposes three modifications to improve its accuracy and interoperability with existing surface analysis tools, while not sacrificing its fast inference time and low GPU memory consumption. First, we employ a more accurate ODE solver to reduce the diffeomorphic mapping approximation error. Second, we devise a routine to produce smoother template meshes avoiding mesh artifacts caused by sharp edges in CorticalFlow's convex-hull based template. Last, we recast pial surface prediction as the deformation of the predicted white surface leading to a one-to-one mapping between white and pial surface vertices. This mapping is essential to many existing surface analysis tools for cortical morphometry. We name the resulting method CorticalFlow$^{++}$. Using large-scale datasets, we demonstrate the proposed changes provide more geometric accuracy and surface regularity while keeping the reconstruction time and GPU memory requirements almost unchanged.      
### 27.Med-DANet: Dynamic Architecture Network for Efficient Medical Volumetric Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.06575.pdf)
>  For 3D medical image (e.g. CT and MRI) segmentation, the difficulty of segmenting each slice in a clinical case varies greatly. Previous research on volumetric medical image segmentation in a slice-by-slice manner conventionally use the identical 2D deep neural network to segment all the slices of the same case, ignoring the data heterogeneity among image slices. In this paper, we focus on multi-modal 3D MRI brain tumor segmentation and propose a dynamic architecture network named Med-DANet based on adaptive model selection to achieve effective accuracy and efficiency trade-off. For each slice of the input 3D MRI volume, our proposed method learns a slice-specific decision by the Decision Network to dynamically select a suitable model from the predefined Model Bank for the subsequent 2D segmentation task. Extensive experimental results on both BraTS 2019 and 2020 datasets show that our proposed method achieves comparable or better results than previous state-of-the-art methods for 3D MRI brain tumor segmentation with much less model complexity. Compared with the state-of-the-art 3D method TransBTS, the proposed framework improves the model efficiency by up to 3.5x without sacrificing the accuracy. Our code will be publicly available soon.      
### 28.Pixel-by-pixel Mean Opinion Score (pMOS) for No-Reference Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2206.06541.pdf)
>  Deep-learning based techniques have contributed to the remarkable progress in the field of automatic image quality assessment (IQA). Existing IQA methods are designed to measure the quality of an image in terms of Mean Opinion Score (MOS) at the image-level (i.e. the whole image) or at the patch-level (dividing the image into multiple units and measuring quality of each patch). Some applications may require assessing the quality at the pixel-level (i.e. MOS value for each pixel), however, this is not possible in case of existing techniques as the spatial information is lost owing to their network structures. This paper proposes an IQA algorithm that can measure the MOS at the pixel-level, in addition to the image-level MOS. The proposed algorithm consists of three core parts, namely: i) Local IQA; ii) Region of Interest (ROI) prediction; iii) High-level feature embedding. The Local IQA part outputs the MOS at the pixel-level, or pixel-by-pixel MOS - we term it 'pMOS'. The ROI prediction part outputs weights that characterize the relative importance of region when calculating the image-level IQA. The high-level feature embedding part extracts high-level image features which are then embedded into the Local IQA part. In other words, the proposed algorithm yields three outputs: the pMOS which represents MOS for each pixel, the weights from the ROI indicating the relative importance of region, and finally the image-level MOS that is obtained by the weighted sum of pMOS and ROI values. The image-level MOS thus obtained by utilizing pMOS and ROI weights shows superior performance compared to the existing popular IQA techniques. In addition, visualization results indicate that predicted pMOS and ROI outputs are reasonably aligned with the general principles of the human visual system (HVS).      
### 29.Assessing Privacy Leakage in Synthetic 3-D PET Imaging using Transversal GAN  [ :arrow_down: ](https://arxiv.org/pdf/2206.06448.pdf)
>  Training computer-vision related algorithms on medical images for disease diagnosis or image segmentation is difficult in large part due to privacy concerns. For this reason, generative image models are highly sought after to facilitate data sharing. However, 3-D generative models are understudied, and investigation of their privacy leakage is needed. We introduce our 3-D generative model, Transversal GAN (TrGAN), using head &amp; neck PET images which are conditioned on tumour masks as a case study. We define quantitative measures of image fidelity, utility and privacy for our model. These metrics are evaluated in the course of training to identify ideal fidelity, utility and privacy trade-offs and establish the relationships between these parameters. We show that the discriminator of the TrGAN is vulnerable to attack, and that an attacker can identify which samples were used in training with almost perfect accuracy (AUC = 0.99). We also show that an attacker with access to only the generator cannot reliably classify whether a sample had been used for training (AUC = 0.51). This suggests that TrGAN generators, but not discriminators, may be used for sharing synthetic 3-D PET data with minimal privacy risk while maintaining good utility and fidelity.      
### 30.Multiband Massive IoT: A Learning Approach to Infrastructure Deployment  [ :arrow_down: ](https://arxiv.org/pdf/2206.06446.pdf)
>  We consider a novel ultra-narrowband (UNB) low-power wide-area network (LPWAN) architecture design for uplink transmission of a massive number of Internet of Things (IoT) devices over multiple multiplexing bands. An IoT device can randomly choose any of the multiplexing bands to transmit its packet. Due to hardware constraints, a base station (BS) is able to listen to only one multiplexing band. Our main objective is to maximize the packet decoding probability (PDP) by optimizing the placement of the BSs and frequency assignment of BSs to multiplexing bands. We develop two online approaches that adapt to the environment based on the statistics of (un)successful packets at the BSs. The first approach is based on a predefined model of the environment, while the second approach is measurement-based model-free approach, which is applicable to any environment. The benefit of the model-based approach is a lower training complexity, at the risk of a poor fit in a model-incompatible environment. The simulation results show that our proposed approaches to band assignment and BS placement offer significant improvement in PDP over baseline random approaches and perform closely to the theoretical upper bound.      
### 31.Fitting Segmentation Networks on Varying Image Resolutions using Splatting  [ :arrow_down: ](https://arxiv.org/pdf/2206.06445.pdf)
>  Data used in image segmentation are not always defined on the same grid. This is particularly true for medical images, where the resolution, field-of-view and orientation can differ across channels and subjects. Images and labels are therefore commonly resampled onto the same grid, as a pre-processing step. However, the resampling operation introduces partial volume effects and blurring, thereby changing the effective resolution and reducing the contrast between structures. In this paper we propose a splat layer, which automatically handles resolution mismatches in the input data. This layer pushes each image onto a mean space where the forward pass is performed. As the splat operator is the adjoint to the resampling operator, the mean-space prediction can be pulled back to the native label space, where the loss function is computed. Thus, the need for explicit resolution adjustment using interpolation is removed. We show on two publicly available datasets, with simulated and real multi-modal magnetic resonance images, that this model improves segmentation results compared to resampling as a pre-processing step.      
### 32.Acceleration of cerebral blood flow and arterial transit time maps estimation from multiple post-labeling delay arterial spin-labeled MRI via deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.06372.pdf)
>  Purpose: Arterial spin labeling (ASL) perfusion imaging indicates direct and absolute measurement of cerebral blood flow (CBF). Arterial transit time (ATT) is a related physiological parameter reflecting the duration for the labeled spins to reach the brain region of interest. Multiple post-labeling delay (PLDs) can provide robust measures of both CBF and ATT, allowing for optimization of regional CBF modeling based on ATT. The prolonged acquisition time can potentially reduce the quality and accuracy of the CBF and ATT estimation. We proposed a novel network to significantly reduce the number of PLDs with higher signal-to-noise ratio (SNR). Method: CBF and ATT estimations were performed for one PLD and two PLDs sepa-rately. Each model was trained independently to learn the nonlinear transformation from perfusion weighted image (PWI) to CBF and ATT images. Results: Both one-PLD and two-PLD models outperformed the conventional method visually on CBF and two-PLD model showed more accurate structure on ATT estima-tion. The proposed method significantly reduces the number of PLDs from 6 to 2 on ATT and even to single PLD on CBF without sacrificing the SNR. Conclusion: It is feasible to generate CBF and ATT maps with reduced PLDs using deep learning with high quality.      
### 33.Constellation Design for Deep Joint Source-Channel Coding  [ :arrow_down: ](https://arxiv.org/pdf/2206.07008.pdf)
>  Deep learning-based joint source-channel coding (JSCC) has shown excellent performance in image and feature transmission. However, the output values of the JSCC encoder are continuous, which makes the constellation of modulation complex and dense. It is hard and expensive to design radio frequency chains for transmitting such full-resolution constellation points. In this paper, two methods of mapping the full-resolution constellation to finite constellation are proposed for real system implementation. The constellation mapping results of the proposed methods correspond to regular constellation and irregular constellation, respectively. We apply the methods to existing deep JSCC models and evaluate them on AWGN channels with different signal-to-noise ratios (SNRs). Experimental results show that the proposed methods outperform the traditional uniform quadrature amplitude modulation (QAM) constellation mapping method by only adding a few additional parameters.      
### 34.Polarization Diversity-enabled LOS/NLOS Identification via Carrier Phase Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2206.07007.pdf)
>  Provision of accurate localization is an increasingly important feature of wireless networks. To this end, reliable distinction between line-of-sight (LOS) and non-LOS (NLOS) radio links is necessary to avoid degenerative localization estimation biases. Interestingly, LOS and NLOS transmissions affect differently the polarization of receive signals. In this work, we leverage this phenomenon to propose a threshold-based LOS/NLOS classifier exploiting weighted differential carrier phase measurements over a single link with different polarization configurations. Operation in full and limited polarization diversity systems are both possible. We develop a framework for assessing the performance of the proposed classifier, and show through simulations the performance impact of the reflecting materials in NLOS scenarios. For instance, the classifier is far more efficient in NLOS scenarios with wooden reflectors than in those with metallic reflectors. Numerical results evince the potential performance gains from exploiting full polarization diversity, properly weighting the differential carrier phase measurements, and using multi-signal/tone transmissions. Finally, we show that the optimum decision threshold is inversely proportional to the path power gain in dB, while it does not depend significantly on the material of potential NLOS reflectors.      
### 35.Beyond-Cell Communications via HAPS-RIS  [ :arrow_down: ](https://arxiv.org/pdf/2206.07005.pdf)
>  The ever-increasing number of users and new services in urban regions can lead terrestrial base stations (BSs) to become overloaded and, consequently, some users to go unserved. Compounding this, users in urban areas can face severe shadowing and blockages, which means that some users do not receive a desired quality of service (QoS). Motivated by the energy and cost benefits of reconfigurable intelligent surfaces (RIS) and the advantages of high altitude platform stations (HAPS), including their wide footprint and strong line-of-sight (LoS) links, we propose a solution to service the stranded users using the RISaided HAPS. More specifically, we propose to service the stranded users by a dedicated control station (CS) via a HAPS equipped with RIS (HAPS-RIS). Through this approach, users are not restricted from being serviced by the cell they belong to; hence, we refer to this approach as beyond-cell communication. As we demonstrate in this paper, beyond-cell communication works in tandem with legacy terrestrial networks to support uncovered or unserved users. Optimal transmit power and RIS unit assignment strategies for the users based on different network objectives are introduced. Numerical results demonstrate the benefits of the proposed beyond-cell communication approach. Moreover, the results provide insights into the different optimization objectives and their interplay with minimum quality-of-service (QoS) and network resources, such as transmit power and the number of reflectors.      
### 36.Edge Graph Neural Networks for Massive MIMO Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.06979.pdf)
>  Massive Multiple-Input Multiple-Out (MIMO) detection is an important problem in modern wireless communication systems. While traditional Belief Propagation (BP) detectors perform poorly on loopy graphs, the recent Graph Neural Networks (GNNs)-based method can overcome the drawbacks of BP and achieve superior performance. Nevertheless, direct use of GNN ignores the importance of edge attributes and suffers from high computation overhead using a fully connected graph structure. In this paper, we propose an efficient GNN-inspired algorithm, called the Edge Graph Neural Network (EGNN), to detect MIMO signals. We first compute graph edge weights through channel correlation and then leverage the obtained weights as a metric to evaluate the importance of neighbors of each node. Moreover, we design an adaptive Edge Drop (ED) scheme to sparsify the graph such that computational cost can be significantly reduced. Experimental results demonstrate that our proposed EGNN achieves better or comparable performance to popular MIMO detection methods for different modulation schemes and costs the least detection time compared to GNN-based approaches.      
### 37.Low-Latency MAC Design for Pairwise Random Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.06978.pdf)
>  Feasibility of using unlicensed spectrum for ultra reliable low latency communications (URLLC) is still a question for beyond 5G wireless networks. Low latency access to the channel and efficiently sharing spectrum among the multiple users are the main requirements for exploiting unlicensed spectrum for URLLC. Listen before talk and back-off procedures implemented to avoid the collisions in channel access hinder the low latency communication. In this paper, we propose a novel low-latency medium access control (MAC) scheme based on the collision resolution for a pairwise random wireless network. We use geometric sequence decomposition for collision resolution among the competing users. This enables the system to tackle collisions and thus removing the need for carrier sensing and back-off procedures. This saves time in obtaining access to the channel and improves the efficiency of the system. We implement our approach in the synchronized time slotted system and show that it yields significant improvement over existing MAC schemes.      
### 38.Worst-case Design for RIS-aided Over-the-air Computation with Imperfect CSI  [ :arrow_down: ](https://arxiv.org/pdf/2206.06936.pdf)
>  Over-the-air computation (AirComp) enables fast wireless data aggregation at the receiver through concurrent transmission by sensors in the application of Internet-of-Things (IoT). To further improve the performance of AirComp under unfavorable propagation channel conditions, we consider the problem of computation distortion minimization in a reconfigurable intelligent surface (RIS)-aided AirComp system. In particular, we take into account an additive bounded uncertainty of the channel state information (CSI) and the total power constraint, and jointly optimize the transceiver (Tx-Rx) and the RIS phase design from the perspective of worst-case robustness by minimizing the mean squared error (MSE) of the computation. To solve this intractable nonconvex problem, we develop an efficient alternating algorithm where both solutions to the robust sub-problem and to the joint design of Tx-Rx and RIS are obtained in closed forms. Simulation results demonstrate the effectiveness of the proposed method.      
### 39.LPCSE: Neural Speech Enhancement through Linear Predictive Coding  [ :arrow_down: ](https://arxiv.org/pdf/2206.06908.pdf)
>  The increasingly stringent requirement on quality-of-experience in 5G/B5G communication systems has led to the emerging neural speech enhancement techniques, which however have been developed in isolation from the existing expert-rule based models of speech pronunciation and distortion, such as the classic Linear Predictive Coding (LPC) speech model because it is difficult to integrate the models with auto-differentiable machine learning frameworks. In this paper, to improve the efficiency of neural speech enhancement, we introduce an LPC-based speech enhancement (LPCSE) architecture, which leverages the strong inductive biases in the LPC speech model in conjunction with the expressive power of neural networks. Differentiable end-to-end learning is achieved in LPCSE via two novel blocks: a block that utilizes the expert rules to reduce the computational overhead when integrating the LPC speech model into neural networks, and a block that ensures the stability of the model and avoids exploding gradients in end-to-end training by mapping the Linear prediction coefficients to the filter poles. The experimental results show that LPCSE successfully restores the formants of the speeches distorted by transmission loss, and outperforms two existing neural speech enhancement methods of comparable neural network sizes in terms of the Perceptual evaluation of speech quality (PESQ) and Short-Time Objective Intelligibility (STOI) on the LJ Speech corpus.      
### 40.Evaluating histopathology transfer learning with ChampKit  [ :arrow_down: ](https://arxiv.org/pdf/2206.06862.pdf)
>  Histopathology remains the gold standard for diagnosis of various cancers. Recent advances in computer vision, specifically deep learning, have facilitated the analysis of histopathology images for various tasks, including immune cell detection and microsatellite instability classification. The state-of-the-art for each task often employs base architectures that have been pretrained for image classification on ImageNet. The standard approach to develop classifiers in histopathology tends to focus narrowly on optimizing models for a single task, not considering the aspects of modeling innovations that improve generalization across tasks. Here we present ChampKit (Comprehensive Histopathology Assessment of Model Predictions toolKit): an extensible, fully reproducible benchmarking toolkit that consists of a broad collection of patch-level image classification tasks across different cancers. ChampKit enables a way to systematically document the performance impact of proposed improvements in models and methodology. ChampKit source code and data are freely accessible at <a class="link-external link-https" href="https://github.com/kaczmarj/champkit" rel="external noopener nofollow">this https URL</a> .      
### 41.Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal  [ :arrow_down: ](https://arxiv.org/pdf/2206.06803.pdf)
>  This work studies the joint rain and haze removal problem. In real-life scenarios, rain and haze, two often co-occurring common weather phenomena, can greatly degrade the clarity and quality of the scene images, leading to a performance drop in the visual applications, such as autonomous driving. However, jointly removing the rain and haze in scene images is ill-posed and challenging, where the existence of haze and rain and the change of atmosphere light, can both degrade the scene information. Current methods focus on the contamination removal part, thus ignoring the restoration of the scene information affected by the change of atmospheric light. We propose a novel deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address the aforementioned challenge. The ADU-Net produces both the contamination residual and the scene residual to efficiently remove the rain and haze while preserving the fidelity of the scene information. Extensive experiments show our work outperforms the existing state-of-the-art methods by a considerable margin in both synthetic data and real-world data benchmarks, including RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data, respectively. <br>Codes will be made available freely to the research community.      
### 42.Quantitative Imaging Principles Improves Medical Image Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.06663.pdf)
>  Fundamental differences between natural and medical images have recently favored the use of self-supervised learning (SSL) over ImageNet transfer learning for medical image applications. Differences between image types are primarily due to the imaging modality and medical images utilize a wide range of physics based techniques while natural images are captured using only visible light. While many have demonstrated that SSL on medical images has resulted in better downstream task performance, our work suggests that more performance can be gained. The scientific principles which are used to acquire medical images are not often considered when constructing learning problems. For this reason, we propose incorporating quantitative imaging principles during generative SSL to improve image quality and quantitative biological accuracy. We show that this training schema results in better starting states for downstream supervised training on limited data. Our model also generates images that validate on clinical quantitative analysis software.      
### 43.Bayesian Channel Estimation for Intelligent Reflecting Surface-Aided mmWave Massive MIMO Systems With Semi-Passive Elements  [ :arrow_down: ](https://arxiv.org/pdf/2206.06605.pdf)
>  In this paper, we propose a Bayesian channel estimator for intelligent reflecting surface-aided (IRS-aided) millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) systems with semi-passive elements that can receive the signal in the active sensing mode. Ultimately, our goal is to minimize the channel estimation error using the received signal at the base station and additional information acquired from a small number of active sensors at the IRS. Unlike recent works on channel estimation with semi-passive elements that require both uplink and downlink training signals to estimate the UE-IRS and IRS-BS links, we only use uplink training signals to estimate all the links. To compute the minimum mean squared error (MMSE) estimates of all the links, we propose a novel variational inference-sparse Bayesian learning (VI-SBL) channel estimator that performs approximate posterior inference on the channel using VI with the mean-field approximation under the SBL framework. The simulation results show that VI-SBL outperforms the state-of-the-art baselines for IRS with passive reflecting elements in terms of the channel estimation accuracy, training overhead, and spectral efficiency. Furthermore, VI-SBL with semi-passive elements is shown to be more energy-efficient than the baselines with passive reflecting elements while employing a small number of low-cost active sensors.      
### 44.WHIS: Hearing impairment simulator based on the gammachirp auditory filterbank  [ :arrow_down: ](https://arxiv.org/pdf/2206.06604.pdf)
>  A new version of a hearing impairment simulator (WHIS) was implemented based on a revised version of the gammachirp filterbank (GCFB), which incorporates fast frame-based processing, absolute threshold (AT), an audiogram of a hearing-impaired (HI) listener, and a parameter to control the cochlear input-output (IO) function. The parameter referred to as the compression health $\alpha$ controlled the slope of the IO function to range from normal hearing (NH) listeners to HI listeners, without largely changing the total hearing loss (HL). The new WHIS was designed provide an NH listener the same EPs as those of a target HI listener.The analysis part of WHIS was almost the same as that of the revised GCFB, except that the IO function was used instead of the gain function. We proposed two synthesis methods: a direct time-varying filter for perceptually small distortion and a filterbank analysis-synthesis for further HI simulations including temporal smearing. We evaluated the WHIS family and a Cambridge version of the HL simulator (CamHLS) in terms of differences in the IO function and spectral distance. The IO functions were simulated fairly well at $\alpha$ less than 0.5 but not at $\alpha$ equal to 1. Thus, it is difficult to simulate the HL when the IO function is sufficiently healthy. This is a fundamental limit of any existing HL simulator as well as WHIS. The new WHIS yielded a smaller spectral distortion than CamHLS and was fairly compatible with the previous version.      
### 45.Downlink Power Allocation in Massive MIMO via Deep Learning: Adversarial Attacks and Training  [ :arrow_down: ](https://arxiv.org/pdf/2206.06592.pdf)
>  The successful emergence of deep learning (DL) in wireless system applications has raised concerns about new security-related challenges. One such security challenge is adversarial attacks. Although there has been much work demonstrating the susceptibility of DL-based classification tasks to adversarial attacks, regression-based problems in the context of a wireless system have not been studied so far from an attack perspective. The aim of this paper is twofold: (i) we consider a regression problem in a wireless setting and show that adversarial attacks can break the DL-based approach and (ii) we analyze the effectiveness of adversarial training as a defensive technique in adversarial settings and show that the robustness of DL-based wireless system against attacks improves significantly. Specifically, the wireless application considered in this paper is the DL-based power allocation in the downlink of a multicell massive multi-input-multi-output system, where the goal of the attack is to yield an infeasible solution by the DL model. We extend the gradient-based adversarial attacks: fast gradient sign method (FGSM), momentum iterative FGSM, and projected gradient descent method to analyze the susceptibility of the considered wireless application with and without adversarial training. We analyze the deep neural network (DNN) models performance against these attacks, where the adversarial perturbations are crafted using both the white-box and black-box attacks.      
### 46.Speech intelligibility of simulated hearing loss sounds and its prediction using the Gammachirp Envelope Similarity Index (GESI)  [ :arrow_down: ](https://arxiv.org/pdf/2206.06573.pdf)
>  In the present study, speech intelligibility (SI) experiments were performed using simulated hearing loss (HL) sounds in laboratory and remote environments to clarify the effects of peripheral dysfunction. Noisy speech sounds were processed to simulate the average HL of 70- and 80-year-olds using Wadai Hearing Impairment Simulator (WHIS). These sounds were presented to normal hearing (NH) listeners whose cognitive function could be assumed to be normal. The results showed that the divergence was larger in the remote experiments than in the laboratory ones. However, the remote results could be equalized to the laboratory ones, mostly through data screening using the results of tone pip tests prepared on the experimental web page. In addition, a newly proposed objective intelligibility measure (OIM) called the Gammachirp Envelope Similarity Index (GESI) explained the psychometric functions in the laboratory and remote experiments fairly well. GESI has the potential to explain the SI of HI listeners by properly setting HL parameters.      
### 47.Safe Output Feedback Motion Planning from Images via Learned Perception Modules and Contraction Theory  [ :arrow_down: ](https://arxiv.org/pdf/2206.06553.pdf)
>  We present a motion planning algorithm for a class of uncertain control-affine nonlinear systems which guarantees runtime safety and goal reachability when using high-dimensional sensor measurements (e.g., RGB-D images) and a learned perception module in the feedback control loop. First, given a dataset of states and observations, we train a perception system that seeks to invert a subset of the state from an observation, and estimate an upper bound on the perception error which is valid with high probability in a trusted domain near the data. Next, we use contraction theory to design a stabilizing state feedback controller and a convergent dynamic state observer which uses the learned perception system to update its state estimate. We derive a bound on the trajectory tracking error when this controller is subjected to errors in the dynamics and incorrect state estimates. Finally, we integrate this bound into a sampling-based motion planner, guiding it to return trajectories that can be safely tracked at runtime using sensor data. We demonstrate our approach in simulation on a 4D car, a 6D planar quadrotor, and a 17D manipulation task with RGB(-D) sensor measurements, demonstrating that our method safely and reliably steers the system to the goal, while baselines that fail to consider the trusted domain or state estimation errors can be unsafe.      
### 48.3D scene reconstruction from monocular spherical video with motion parallax  [ :arrow_down: ](https://arxiv.org/pdf/2206.06533.pdf)
>  In this paper, we describe a method to capture nearly entirely spherical (360 degree) depth information using two adjacent frames from a single spherical video with motion parallax. After illustrating a spherical depth information retrieval using two spherical cameras, we demonstrate monocular spherical stereo by using stabilized first-person video footage. Experiments demonstrated that the depth information was retrieved on up to 97% of the entire sphere in solid angle. At a speed of 30 km/h, we were able to estimate the depth of an object located over 30 m from the camera. We also reconstructed the 3D structures (point cloud) using the obtained depth data and confirmed the structures can be clearly observed. We can apply this method to 3D structure retrieval of surrounding environments such as 1) previsualization, location hunting/planning of a film, 2) real scene/computer graphics synthesis and 3) motion capture. Thanks to its simplicity, this method can be applied to various videos. As there is no pre-condition other than to be a 360 video with motion parallax, we can use any 360 videos including those on the Internet to reconstruct the surrounding environments. The cameras can be lightweight enough to be mounted on a drone. We also demonstrated such applications.      
### 49.Multi-Hop Quantum Key Distribution with Passive Relays over Underwater Turbulence Channels  [ :arrow_down: ](https://arxiv.org/pdf/2206.06514.pdf)
>  Absorption, scattering, and turbulence experienced in underwater channels severely limit the range of quantum communications. In this paper, to overcome range limitations, we investigate a multi-hop underwater quantum key distribution (QKD) where intermediate nodes help the key distribution between the source and destination nodes. We consider deployment of passive-relays which simply redirect the qubits to the next relay node or receiver without any measurement. Based on near-field analysis, we present the performance of relay-assisted QKD scheme in clear ocean under different atmospheric conditions. We further investigate the effect of system parameters (aperture size and detector field-of-view) on the achievable distance.      
### 50.Stable Relationships  [ :arrow_down: ](https://arxiv.org/pdf/2206.06468.pdf)
>  We study a dynamic model of the relationship between two people where the states depend on the "power" in the relationship. We perform a comprehensive analysis of stability of the system, and determine a set of conditions under which stable relationships are possible. In particular, stable relationships can occur if both people are dominant, but the sum of dominances is below a bound determined by the model's parameters. Stable relationships can also occur if one person is dominant and the other is submissive, provided the level of dominance exceeds the level of submissiveness but not beyond a threshold. We also conclude that a stable relationship is not possible if both people are submissive. While our model is motivated by a social or romantic relationship, it can also be applied to professional or business relationships as well as diplomatic relationships between nations.      
### 51.GoAutoBash: Golang-based Multi-Thread Automatic Pull-Execute Framework with GitHub Webhooks And Queuing Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2206.06401.pdf)
>  Recently, more and more server tasks are done using full automation, including grading tasks for students in the college courses, integrating tasks for programmers in big projects and server-based transactions, and visualization tasks for researchers in a data-dense topic. Using automation on servers provides a great possibility for reducing the burden on manual tasks. Although server tools like CI/CD for continuous integration and Hexo for automated blog deployment have been developed, they're highly dedicated to certain functionalities and thus lack general usage. In this paper, we introduce a Golang-based automation framework that reacts to the events happening on GitHub in a multi-thread approach. This framework utilizes a queue to arrange the tasks submitted and execute each task with a thread in a preemptive manner. We then use the project GoAutoGrader to illustrate a specific implementation of this framework and its value in implementing high-freedom server applications. As Golang is developing in a rapid way because of its incredible parallel programming efficiency and a super-easy way to learn on the basis of C-like programming languages, we decide to develop this system in Golang.      
### 52.SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency  [ :arrow_down: ](https://arxiv.org/pdf/2206.03820.pdf)
>  Intra-voxel incoherent motion (IVIM) analysis of fetal lungs Diffusion-Weighted MRI (DWI) data shows potential in providing quantitative imaging bio-markers that reflect, indirectly, diffusion and pseudo-diffusion for non-invasive fetal lung maturation assessment. However, long acquisition times, due to the large number of different "b-value" images required for IVIM analysis, precluded clinical feasibility. We introduce SUPER-IVIM-DC a deep-neural-networks (DNN) approach which couples supervised loss with a data-consistency term to enable IVIM analysis of DWI data acquired with a limited number of b-values. We demonstrated the added-value of SUPER-IVIM-DC over both classical and recent DNN approaches for IVIM analysis through numerical simulations, healthy volunteer study, and IVIM analysis of fetal lung maturation from fetal DWI data. % add results Our numerical simulations and healthy volunteer study show that SUPER-IVIM-DC estimates of the IVIM model parameters from limited DWI data had lower normalized root mean-squared error compared to previous DNN-based approaches. Further, SUPER-IVIM-DC estimates of the pseudo-diffusion fraction parameter from limited DWI data of fetal lungs correlate better with gestational age compared to both to classical and DNN-based approaches (0.242 vs. -0.079 and 0.239). SUPER-IVIM-DC has the potential to reduce the long acquisition times associated with IVIM analysis of DWI data and to provide clinically feasible bio-markers for non-invasive fetal lung maturity assessment.      
