# ArXiv eess --Thu, 2 Jun 2022
### 1.The elements of flexibility for task-performing systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.00582.pdf)
>  What makes living systems flexible so that they can react quickly and adapt easily to changingenvironments? In this paper, we consider how the design of living systems promotes flexibility andpresent a research perspective how these insights can contribute to the realization of a "batch size one" production regime based on cyber-physical production systems. To this end, we introduce the concept of task-performing systems and a unifying formalism to formulate and study flexibilityproblems for task-performing systems appearing in production engineering, machine learning or the life sciences. Moreover, we give an overview of the elements of flexibility which are six design features that promote flexibility on many levels of biological organization in living systems.      
### 2.The Fully Convolutional Transformer for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00566.pdf)
>  We propose a novel transformer model, capable of segmenting medical images of varying modalities. Challenges posed by the fine grained nature of medical image analysis mean that the adaptation of the transformer for their analysis is still at nascent stages. The overwhelming success of the UNet lay in its ability to appreciate the fine-grained nature of the segmentation task, an ability which existing transformer based models do not currently posses. To address this shortcoming, we propose The Fully Convolutional Transformer (FCT), which builds on the proven ability of Convolutional Neural Networks to learn effective image representations, and combines them with the ability of Transformers to effectively capture long-term dependencies in its inputs. The FCT is the first fully convolutional Transformer model in medical imaging literature. It processes its input in two stages, where first, it learns to extract long range semantic dependencies from the input image, and then learns to capture hierarchical global attributes from the features. FCT is compact, accurate and robust. Our results show that it outperforms all existing transformer architectures by large margins across multiple medical image segmentation datasets of varying data modalities without the need for any pre-training. FCT outperforms its immediate competitor on the ACDC dataset by 1.3%, on the Synapse dataset by 4.4%, on the Spleen dataset by 1.2% and on ISIC 2017 dataset by 1.1% on the dice metric, with up to five times fewer parameters. Our code, environments and models will be available via GitHub.      
### 3.Impact of loss function in Deep Learning methods for accurate retinal vessel segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00536.pdf)
>  The retinal vessel network studied through fundus images contributes to the diagnosis of multiple diseases not only found in the eye. The segmentation of this system may help the specialized task of analyzing these images by assisting in the quantification of morphological characteristics. Due to its relevance, several Deep Learning-based architectures have been tested for tackling this problem automatically. However, the impact of loss function selection on the segmentation of the intricate retinal blood vessel system hasn't been systematically evaluated. In this work, we present the comparison of the loss functions Binary Cross Entropy, Dice, Tversky, and Combo loss using the deep learning architectures (i.e. U-Net, Attention U-Net, and Nested UNet) with the DRIVE dataset. Their performance is assessed using four metrics: the AUC, the mean squared error, the dice score, and the Hausdorff distance. The models were trained with the same number of parameters and epochs. Using dice score and AUC, the best combination was SA-UNet with Combo loss, which had an average of 0.9442 and 0.809 respectively. The best average of Hausdorff distance and mean square error were obtained using the Nested U-Net with the Dice loss function, which had an average of 6.32 and 0.0241 respectively. The results showed that there is a significant difference in the selection of loss function      
### 4.Frequency-domain analysis for reset systems using pulse-based model  [ :arrow_down: ](https://arxiv.org/pdf/2206.00523.pdf)
>  In the precision motion industry, the demand for machines with higher speed, accuracy, and stability is rapidly increasing. Traditional linear controllers struggle to meet these requirements due to fundamental limitations such as Bode's gain-phase relationship and waterbed effects. Hence, non-linear controllers that can address these constraints are needed. Reset controllers are potential alternatives with the advantage of phase lead in comparison to their linear counterparts. However, reset controllers are incompatible with the loop-shaping analysis technique, since loop-shaping relies on the simple first-order element, whereas reset controllers include complex higher-order harmonics. Thus, the analysis and design of reset controllers remain bottlenecks in their way of widespread application. In this paper, a new analytical technique for reset controllers is proposed to break the limitations. A novel pulse-based model for analyzing general reset control systems both in open-loop and closed-loop is developed, for the first time enabling loop-shaping implemented into the analysis of reset control systems accurately. Corresponding sensitivity functions are subsequently illustrated. Finally, through intuitive examples, the accuracy of the proposed model is evaluated.      
### 5.Networked Sensing in 6G Cellular Networks: Opportunities and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2206.00493.pdf)
>  Radar and wireless communication are widely acknowledged as the two most successful applications of the radio technology over the past decades. Recently, there is a trend in both academia and industry to achieve integrated sensing and communication (ISAC) in one system via utilizing a common radio spectrum and the same hardware platform. This article will discuss about the possibility of exploiting the future sixth-generation (6G) cellular network to realize ISAC. Our vision is that the cellular base stations (BSs) deployed all over the world can be transformed into a powerful sensor to provide highresolution localization services. Specifically, motivated by the joint encoding/decoding gain in multi-cell coordinated communication, we advocate the adoption of the networked sensing technique in 6G network to achieve the above goal, where the BSs can share the sensing information with each other for jointly estimating the locations and velocities of the targets. Several opportunities and challenges to realize networked sensing in the 6G era will be revealed in this article. Moreover, the future research directions for this promising trend will be outlined as well.      
### 6.Weak consistency of P-time event graphs  [ :arrow_down: ](https://arxiv.org/pdf/2206.00478.pdf)
>  P-time event graphs (P-TEGs) are event graphs where the residence time of tokens in places is bounded by specified time windows. In this paper, we define a new property of PTEGs, called weak consistency. In weakly consistent P-TEGs, the amount of times a transition can fire before the first violation of a time constraint can be made as large as desired. We show the practical implications of this property and, based on previous results in graph theory, we formulate an algorithm of strongly polynomial time complexity that verifies it. From this algorithm, it is possible to determine, in pseudo-polynomial time, the maximum number of firings before the first constraint violation in a P-TEG.      
### 7.Spiking Neural Network Equalization on Neuromorphic Hardware for IM/DD Optical Communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.00401.pdf)
>  A spiking neural network (SNN) non-linear equalizer model is implemented on the mixed-signal neuromorphic hardware system BrainScaleS-2 and evaluated for an IM/DD link. The BER 2e-3 is achieved with a hardware penalty less than 1 dB, outperforming numeric linear equalization.      
### 8.A comparative study between vision transformers and CNNs in digital pathology  [ :arrow_down: ](https://arxiv.org/pdf/2206.00389.pdf)
>  Recently, vision transformers were shown to be capable of outperforming convolutional neural networks when pretrained on sufficient amounts of data. In comparison to convolutional neural networks, vision transformers have a weaker inductive bias and therefore allow a more flexible feature detection. Due to their promising feature detection, this work explores vision transformers for tumor detection in digital pathology whole slide images in four tissue types, and for tissue type identification. We compared the patch-wise classification performance of the vision transformer DeiT-Tiny to the state-of-the-art convolutional neural network ResNet18. Due to the sparse availability of annotated whole slide images, we further compared both models pretrained on large amounts of unlabeled whole-slide images using state-of-the-art self-supervised approaches. The results show that the vision transformer performed slightly better than the ResNet18 for three of four tissue types for tumor detection while the ResNet18 performed slightly better for the remaining tasks. The aggregated predictions of both models on slide level were correlated, indicating that the models captured similar imaging features. All together, the vision transformer models performed on par with the ResNet18 while requiring more effort to train. In order to surpass the performance of convolutional neural networks, vision transformers might require more challenging tasks to benefit from their weak inductive bias.      
### 9.Graph Signal Sampling Under Stochastic Priors  [ :arrow_down: ](https://arxiv.org/pdf/2206.00382.pdf)
>  We propose a generalized sampling framework for stochastic graph signals. Stochastic graph signals are characterized by graph wide sense stationarity (GWSS) which is an extension of wide sense stationarity (WSS) for standard time-domain signals. In this paper, graph signals are assumed to satisfy the GWSS conditions and we study their sampling as well as recovery procedures. In generalized sampling, a correction filter is inserted between sampling and reconstruction operators to compensate for non-ideal measurements. We propose a design method for the correction filters to reduce the mean-squared error (MSE) between original and reconstructed graph signals. We derive the correction filters for two cases: The reconstruction filter is arbitrarily chosen or predefined. The proposed framework allows for arbitrary sampling methods, i.e., sampling in the vertex or graph frequency domain. We also show that the graph spectral response of the resulting correction filter parallels that for generalized sampling for WSS signals if sampling is performed in the graph frequency domain. The effectiveness of our approach is validated via experiments by comparing its MSE with existing approaches.      
### 10.A Survey on Deep Learning for Skin Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00356.pdf)
>  Skin cancer is a major public health problem that could benefit from computer-aided diagnosis to reduce the burden of this common disease. Skin lesion segmentation from images is an important step toward achieving this goal. However, the presence of natural and artificial artifacts (e.g., hair and air bubbles), intrinsic factors (e.g., lesion shape and contrast), and variations in image acquisition conditions make skin lesion segmentation a challenging task. Recently, various researchers have explored the applicability of deep learning models to skin lesion segmentation. In this survey, we cross-examine 134 research papers that deal with deep learning based segmentation of skin lesions. We analyze these works along several dimensions, including input data (datasets, preprocessing, and synthetic data generation), model design (architecture, modules, and losses), and evaluation aspects (data annotation requirements and segmentation performance). We discuss these dimensions both from the viewpoint of select seminal works, and from a systematic viewpoint, examining how those choices have influenced current trends, and how their limitations should be addressed. We summarize all examined works in a comprehensive table to facilitate comparisons.      
### 11.Synthesizing Safety Controllers for Uncertain Linear Systems: A Direct Data-driven Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.00354.pdf)
>  In this paper, we provide a direct data-driven approach to synthesize safety controllers for unknown linear systems affected by unknown-but-bounded disturbances, in which identifying the unknown model is not required. First, we propose a notion of $\gamma$-robust safety invariant ($\gamma$-RSI) sets and their associated state-feedback controllers, which can be applied to enforce invariance properties. Then, we formulate a data-driven computation of these sets in terms of convex optimization problems with linear matrix inequalities (LMI) as constraints, which can be solved based on a finite number of data collected from a single input-state trajectory of the system. To show the effectiveness of the proposed approach, we apply our results to a 4-dimensional inverted pendulum.      
### 12.CellCentroidFormer: Combining Self-attention and Convolution for Cell Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.00338.pdf)
>  Cell detection in microscopy images is important to study how cells move and interact with their environment. Most recent deep learning-based methods for cell detection use convolutional neural networks (CNNs). However, inspired by the success in other computer vision applications, vision transformers (ViTs) are also used for this purpose. We propose a novel hybrid CNN-ViT model for cell detection in microscopy images to exploit the advantages of both types of deep learning models. We employ an efficient CNN, that was pre-trained on the ImageNet dataset, to extract image features and utilize transfer learning to reduce the amount of required training data. Extracted image features are further processed by a combination of convolutional and transformer layers, so that the convolutional layers can focus on local information and the transformer layers on global information. Our centroid-based cell detection method represents cells as ellipses and is end-to-end trainable. Furthermore, we show that our proposed model can outperform a fully convolutional baseline model on four different 2D microscopy datasets. Code is available at: <a class="link-external link-https" href="https://github.com/roydenwa/cell-centroid-former" rel="external noopener nofollow">this https URL</a>      
### 13.Smart Channel State Information Pre-processing for Joint Authentication and Secret Key Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00332.pdf)
>  While the literature on RF fingerprinting-based authentication and key distillation is vast, the two topics have customarily been studied separately. In this paper, starting from the observation that the wireless channel is a composite, deterministic / stochastic process, we propose a power domain decomposition that allows performing the two tasks simultaneously. We devise intelligent pre-processing schemes to decompose channel state information (CSI) observation vectors into "predictable" and "unpredictable" components. The former, primarily due to large-scale fading, can be used for node authentication through RF fingerprinting. The latter, primarily due to small-scale fading, could be used for semantically secure secret key generation (SKG). To perform the decomposition, we propose: (i) a fingerprint "separability" criterion, expressed through the maximisation of the total variation distance between the empirical fingerprint measures; (ii) a statistical independence metric for observations collected at different users, expressed through a normalised version of the $d$-dimensional Hilbert Schmidt independence criterion (dHSIC) test statistic. We propose both explicit implementations, using principal component analysis (PCA) and kernel PCA and black-box, unsupervised learning, using autoencoders. Our experiments on synthetic and real CSI datasets showcase that the incorporation of RF fingerprinting and SKG, with explicit security guarantees, is tangible in future generations of wireless.      
### 14.Supervised Denoising of Diffusion-Weighted Magnetic Resonance Images Using a Convolutional Neural Network and Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.00305.pdf)
>  In this paper, we propose a method for denoising diffusion-weighted images (DWI) of the brain using a convolutional neural network trained on realistic, synthetic MR data. We compare our results to averaging of repeated scans, a widespread method used in clinics to improve signal-to-noise ratio of MR images. To obtain training data for transfer learning, we model, in a data-driven fashion, the effects of echo-planar imaging (EPI): Nyquist ghosting and ramp sampling. We introduce these effects to the digital phantom of brain anatomy (BrainWeb). Instead of simulating pseudo-random noise with a defined probability distribution, we perform noise scans with a brain-DWI-designed protocol to obtain realistic noise maps. We combine them with the simulated, noise-free EPI images. We also measure the Point Spread Function in a DW image of an AJR-approved geometrical phantom and inter-scan movement in a brain scan of a healthy volunteer. Their influence on image denoising and averaging of repeated images is investigated at different signal-to-noise ratio levels. Denoising performance is evaluated quantitatively using the simulated EPI images and qualitatively in real EPI DWI of the brain. We show that the application of our method allows for a significant reduction in scan time by lowering the number of repeated scans. Visual comparisons made in the acquired brain images indicate that the denoised single-repetition images are less noisy than multi-repetition averaged images. We also analyse the convolutional neural network denoiser and point out the challenges accompanying this denoising method.      
### 15.An adaptive fuzzy dead-zone compensation scheme for nonlinear systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.00276.pdf)
>  The dead-zone nonlinearity is frequently encountered in many industrial automation equipment and its presence can severely compromise control system performance. Due to the possibility to express human experience in an algorithmic manner, fuzzy logic has been largely employed in the last decades to both control and identification of uncertain dynamical systems. In spite of the simplicity of this heuristic approach, in some situations a more rigorous mathematical treatment of the problem is required. In this work, an adaptive fuzzy controller is proposed for nonlinear systems subject to dead-zone input. The convergence properties of the tracking error will be proven using Lyapunov stability theory and Barbalat's lemma. An application of this adaptive fuzzy scheme to a Van der Pol oscillator is introduced to illustrate the controller design method. Numerical results are also presented in order to demonstrate the control system performance.      
### 16.Structured Neural-PI Control for Networked Systems: Stability and Steady-State Optimality Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2206.00261.pdf)
>  We study the control of networked systems with the goal of optimizing both transient and steady-state performances while providing stability guarantees. Linear Proportional-Integral (PI) controllers are almost always used in practice, but the linear parameterization of the controller fundamentally limits its performance. Learning-based approaches are becoming popular in designing nonlinear controllers, but the lack of stability guarantees makes the learned controllers difficult to apply in practical applications. This paper bridges the gap between neural network-based controller design and the need for stability guarantees. Using equilibrium-independent passivity, a property present in a wide range of physical systems, we propose structured neural-PI controllers that have provable guarantees on stability and zero steady-state output tracking error. If communication between neighbours is available, we further extend the controller to distributedly achieve optimal resource allocation at the steady state. We explicitly characterize the stability conditions and engineer neural networks that satisfy them by design. Experiments on traffic and power networks demonstrate that the proposed approach can improve both transient and steady-state performances compared to existing state-of-the-art, while unstructured neural networks lead to unstable behaviors.      
### 17.A Unified Multi-Task Semantic Communication System with Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00254.pdf)
>  The task-oriented semantic communication systems have achieved significant performance gain, however, the paradigm that employs a model for a specific task might be limited, since the system has to be updated once the task is changed or multiple models are stored for serving various tasks. To address this issue, we firstly propose a unified deep learning enabled semantic communication system (U-DeepSC), where a unified model is developed to serve various transmission tasks. To jointly serve these tasks in one model with fixed parameters, we employ domain adaptation in the training procedure to specify the task-specific features for each task. Thus, the system only needs to transmit the task-specific features, rather than all the features, to reduce the transmission overhead. Moreover, since each task is of different difficulty and requires different number of layers to achieve satisfactory performance, we develop the multi-exit architecture to provide early-exit results for relatively simple tasks. In the experiments, we employ a proposed U-DeepSC to serve five tasks with multi-modalities. Simulation results demonstrate that our proposed U-DeepSC achieves comparable performance to the task-oriented semantic communication system designed for a specific task with significant transmission overhead reduction and much less number of model parameters.      
### 18.Distributed Estimation for Interconnected Systems with Arbitrary Coupling Structures  [ :arrow_down: ](https://arxiv.org/pdf/2206.00221.pdf)
>  This paper is concerned with the problem of distributed estimation for time-varying interconnected dynamic systems with arbitrary coupling structures. To guarantee the robustness of the designed estimators, novel distributed stability conditions are proposed with only local information and the information from neighbors. Then, simplified stability conditions which do not require timely exchange of neighbors' estimator gain information is further developed for systems with delayed communication. By merging these subsystem-level stability conditions and the optimization-based estimator gain design, the distributed, stable and optimal estimators are proposed. Quite notably, these optimization solutions can be easily obtained by standard software packages, and it is also shown that the designed estimators are scalable in the sense of adding or subtracting subsystems. Finally, an illustrative example is employed to show the effectiveness of the proposed methods.      
### 19.Resilience in Industrial Internet of Things Systems: A Communication Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2206.00217.pdf)
>  Industrial Internet of Things is an ultra-large-scale system that is much more sophisticated and fragile than conventional industrial platforms. The effective management of such a system relies heavily on the resilience of the network, especially the communication part. Imperative as resilient communication is, there is not enough attention from literature and a standardized framework is still missing. In awareness of these, this paper intends to provide a systematic overview of resilience in IIoT with a communication perspective, aiming to answer the questions of why we need it, what it is, how to enhance it, and where it can be applied. Specifically, we emphasize the urgency of resilience studies via examining existing literature and analyzing malfunction data from a real satellite communication system. Resilience-related concepts and metrics, together with standardization efforts are then summarized and discussed, presenting a basic framework for analyzing the resilience of the system before, during, and after disruptive events. On the basis of the framework, key resilience concerns associated with the design, deployment, and operation of IIoT are briefly described to shed light on the methods for resilience enhancement. Promising resilient applications in different IIoT sectors are also introduced to highlight the opportunities and challenges in practical implementations.      
### 20.How Much Demand Flexibility Could Have Spared Texas from the 2021 Outage?  [ :arrow_down: ](https://arxiv.org/pdf/2206.00184.pdf)
>  The February 2021 Texas winter power outage has led to hundreds of deaths and billions of dollars in economic losses, largely due to the generation failure and record-breaking electric demand. In this paper, we study the scaling-up of demand flexibility as a means to avoid load shedding during such an extreme weather event. The three mechanisms considered are interruptible load, residential load rationing, and incentive-based demand response. By simulating on a synthetic but realistic large-scale Texas grid model along with demand flexibility modeling and electricity outage data, we identify portfolios of mixing mechanisms that exactly avoid outages, which a single mechanism may fail due to decaying marginal effects. We also reveal a complementary relationship between interruptible load and residential load rationing and find nonlinear impacts of incentive-based demand response on the efficacy of other mechanisms.      
### 21.On a Control Architecture for Future Electric Energy Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.00160.pdf)
>  This paper presents considerations towards a control architecture for future electric energy systems driven by massive changes resulting from the societal goals of decarbonization and electrification. A historical perspective is provided on the role that architecture and abstractions have played in other technological systems such as the Internet, serial computation, and communication systems. For power systems, we present a viewpoint of architecture as the organization of multiple control loops aligned with the entities involved, as well as taking advantage of time-scale and spatial scale separations. New requirements and challenges in designing the set of control loops required for future electric energy systems are substantiated from a temporal and spatial scale perspective. Finally, we articulate key desirable control loops that can enable decarbonization of the electricity sector. We thereby argue that the present architecture of electric power grids designed in a different era is indeed extensible to allow the incorporation of increased renewables.      
### 22.Low-complexity Three-dimensional Discrete Hartley Transform Approximations for Medical Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2206.00124.pdf)
>  The discrete Hartley transform (DHT) is a useful tool for medical image coding. The three-dimensional DHT (3D DHT) can be employed to compress medical image data, such as magnetic resonance and X-ray angiography. However, the computation of the 3D DHT involves several multiplications by irrational quantities, which require floating-point arithmetic and inherent truncation errors. In recent years, a significant progress in wireless and implantable biomedical devices has been achieved. Such devices present critical power and hardware limitations. The multiplication operation demands higher hardware, power, and time consumption than other arithmetic operations, such as addition and bit-shifts. In this work, we present a set of multiplierless DHT approximations, which can be implemented with fixed-point arithmetic. We derive 3D DHT approximations by employing tensor formalism. Such proposed methods present prominent computational savings compared to the usual 3D DHT approach, being appropriate for devices with limited resources. The proposed transforms are applied in a lossy 3D DHT-based medical image compression algorithm, presenting practically the same level of visual quality ($&gt;98\%$ in terms of SSIM) at a considerable reduction in computational effort ($100 \%$ multiplicative complexity reduction). Furthermore, we implemented the proposed 3D transforms in an ARM Cortex-M0+ processor employing the low-cost Raspberry Pi Pico board. The execution time was reduced by $\sim$70% compared to the usual 3D DHT and $\sim$90% compared to 3D DCT.      
### 23.A Class of Low-complexity DCT-like Transforms for Image and Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2206.00122.pdf)
>  The discrete cosine transform (DCT) is a relevant tool in signal processing applications, mainly known for its good decorrelation properties. Current image and video coding standards -- such as JPEG and HEVC -- adopt the DCT as a fundamental building block for compression. Recent works have introduced low-complexity approximations for the DCT, which become paramount in applications demanding real-time computation and low-power consumption. The design of DCT approximations involves a trade-off between computational complexity and performance. This paper introduces a new multiparametric transform class encompassing the round-off DCT (RDCT) and the modified RDCT (MRDCT), two relevant multiplierless 8-point approximate DCTs. The associated fast algorithm is provided. Four novel orthogonal low-complexity 8-point DCT approximations are obtained by solving a multicriteria optimization problem. The optimal 8-point transforms are scaled to lengths 16 and 32 while keeping the arithmetic complexity low. The proposed methods are assessed by proximity and coding measures with respect to the exact DCT. Image and video coding experiments hardware realization are performed. The novel transforms perform close to or outperform the current state-of-the-art DCT approximations.      
### 24.Deep learning pipeline for image classification on mobile phones  [ :arrow_down: ](https://arxiv.org/pdf/2206.00105.pdf)
>  This article proposes and documents a machine-learning framework and tutorial for classifying images using mobile phones. Compared to computers, the performance of deep learning model performance degrades when deployed on a mobile phone and requires a systematic approach to find a model that performs optimally on both computers and mobile phones. By following the proposed pipeline, which consists of various computational tools, simple procedural recipes, and technical considerations, one can bring the power of deep learning medical image classification to mobile devices, potentially unlocking new domains of applications. The pipeline is demonstrated on four different publicly available datasets: COVID X-rays, COVID CT scans, leaves, and colorectal cancer. We used two application development frameworks: TensorFlow Lite (real-time testing) and Flutter (digital image testing) to test the proposed pipeline. We found that transferring deep learning models to a mobile phone is limited by hardware and classification accuracy drops. To address this issue, we proposed this pipeline to find an optimized model for mobile phones. Finally, we discuss additional applications and computational concerns related to deploying deep-learning models on phones, including real-time analysis and image preprocessing. We believe the associated documentation and code can help physicians and medical experts develop medical image classification applications for distribution.      
### 25.Characterization of 3D Printers and X-Ray Computerized Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2206.00041.pdf)
>  The 3D printing process flow requires several inputs for the best printing quality. These settings may vary from sample to sample, printer to printer, and depend upon users' previous experience. The involved operational parameters for 3D Printing are varied to test the optimality. Thirty-eight samples are printed using four commercially available 3D printers, namely: (a) Ultimaker 2 Extended+, (b) Delta Wasp, (c) Raise E2, and (d) ProJet MJP. The sample profiles contain uniform and non-uniform distribution of the assorted size of cubes and spheres with a known amount of porosity. These samples are scanned using X-Ray Computed Tomography system. Functional Imaging analysis is performed using AI-based segmentation codes to (a) characterize these 3D printers and (b) find Three-dimensional surface roughness of three teeth and one sandstone pebble (from riverbed) with naturally deposited layers is also compared with printed sample values. Teeth has best quality. It is found that ProJet MJP gives the best quality of printed samples with the least amount of surface roughness and almost near to the actual porosity value. As expected, 100% infill density value, best spatial resolution for printing or Layer height, and minimum nozzle speed give the best quality of 3D printing.      
### 26.Calibrated Bagging Deep Learning for Image Semantic Segmentation: A Case Study on COVID-19 Chest X-ray Image  [ :arrow_down: ](https://arxiv.org/pdf/2206.00002.pdf)
>  Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR) and computed tomography (CT) can provide useful information to clinical staff for facilitating a diagnosis of COVID-19 in a more efficient and comprehensive manner. As a breakthrough of artificial intelligence (AI), deep learning has been applied to perform COVID-19 infection region segmentation and disease classification by analyzing CXR and CT data. However, prediction uncertainty of deep learning models for these tasks, which is very important to safety-critical applications like medical image processing, has not been comprehensively investigated. In this work, we propose a novel ensemble deep learning model through integrating bagging deep learning and model calibration to not only enhance segmentation performance, but also reduce prediction uncertainty. The proposed method has been validated on a large dataset that is associated with CXR image segmentation. Experimental results demonstrate that the proposed method can improve the segmentation performance, as well as decrease prediction uncertainties.      
### 27.Speech Artifact Removal from EEG Recordings of Spoken Word Production with Tensor Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2206.00635.pdf)
>  Research about brain activities involving spoken word production is considerably underdeveloped because of the undiscovered characteristics of speech artifacts, which contaminate electroencephalogram (EEG) signals and prevent the inspection of the underlying cognitive processes. To fuel further EEG research with speech production, a method using three-mode tensor decomposition (time x space x frequency) is proposed to perform speech artifact removal. Tensor decomposition enables simultaneous inspection of multiple modes, which suits the multi-way nature of EEG data. In a picture-naming task, we collected raw data with speech artifacts by placing two electrodes near the mouth to record lip EMG. Based on our evaluation, which calculated the correlation values between grand-averaged speech artifacts and the lip EMG, tensor decomposition outperformed the former methods that were based on independent component analysis (ICA) and blind source separation (BSS), both in detecting speech artifact (0.985) and producing clean data (0.101). Our proposed method correctly preserved the components unrelated to speech, which was validated by computing the correlation value between the grand-averaged raw data without EOG and cleaned data before the speech onset (0.92-0.94).      
### 28.Passive Beamforming Design for Reconfigurable Intelligent Surface Enabled Integrated Sensing and Communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.00525.pdf)
>  To exploit the potential of the reconfigurable intelligent surface (RIS) in supporting the future integrated sensing and communication (ISAC), this paper proposes a novel passive beamforming strategy for the RIS-enabled ISAC (RIS-ISAC) system in consideration of the target size. To this end, the detection probability for target sensing is derived in closed-form based on the illumination power on an approximated scattering surface area (SSA) of the target, and a new concept of ultimate detection resolution (UDR) is defined for the first time to measure the capability of the target detection. Subsequently, an optimization problem is formulated to maximize the signal-to-noise ratio (SNR) at the user-equipment (UE) under a minimum detection probability constraint. To solve this problem, a novel convexification process is performed to convexify the detection probability constraint with matrix operations and a real-valued first-order Taylor approximation. The semidefinite relaxation (SDR) is then adopted to relax the problem. A successive convex approximation (SCA) based algorithm is finally designed to yield a phase-shift solution, followed by a detailed analysis on the problem feasibility condition as well as the algorithm convergence. Our results reveal the inherent trade-offs between the sensing and the communication performances, and between the UDR and the duration of a sensing time slot. In comparison with two existing approaches, the proposed strategy is validated to be superior when detecting targets with practical sizes.      
### 29.Landslide4Sense: Reference Benchmark Data and Deep Learning Models for Landslide Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.00515.pdf)
>  This study introduces \textit{Landslide4Sense}, a reference benchmark for landslide detection from remote sensing. The repository features 3,799 image patches fusing optical layers from Sentinel-2 sensors with the digital elevation model and slope layer derived from ALOS PALSAR. The added topographical information facilitates an accurate detection of landslide borders, which recent researches have shown to be challenging using optical data alone. The extensive data set supports deep learning (DL) studies in landslide detection and the development and validation of methods for the systematic update of landslide inventories. The benchmark data set has been collected at four different times and geographical locations: Iburi (September 2018), Kodagu (August 2018), Gorkha (April 2015), and Taiwan (August 2009). Each image pixel is labelled as belonging to a landslide or not, incorporating various sources and thorough manual annotation. We then evaluate the landslide detection performance of 11 state-of-the-art DL segmentation models: U-Net, ResU-Net, PSPNet, ContextNet, DeepLab-v2, DeepLab-v3+, FCN-8s, LinkNet, FRRN-A, FRRN-B, and SQNet. All models were trained from scratch on patches from one quarter of each study area and tested on independent patches from the other three quarters. Our experiments demonstrate that ResU-Net outperformed the other models for the landslide detection task. We make the multi-source landslide benchmark data (Landslide4Sense) and the tested DL models publicly available at \url{<a class="link-external link-http" href="http://www.landslide4sense.org" rel="external noopener nofollow">this http URL</a>}, establishing an important resource for remote sensing, computer vision, and machine learning communities in studies of image classification in general and applications to landslide detection in particular.      
### 30.helyOS: A customized off-the-shelf solution for autonomous driving applications in delimited areas  [ :arrow_down: ](https://arxiv.org/pdf/2206.00504.pdf)
>  Microservice Architectures (MSA), known to successfully handle complex software systems, are emerging as the new paradigm for automotive software. The design of an MSA requires correct subdivision of the software system and implementation of the communication between components. These tasks demand both software expertise and domain knowledge. In this context, we developed an MSA framework pre-tailored to meet the requirements of autonomous driving applications in delimited areas - the helyOS framework. The framework decomposes complex applications in predefined microservice domains and provides a communication backbone for event messages and data. This paper demonstrates how such a tailored MSA framework can accelerate the development by prompting a quick start for the integration of motion planning algorithms, device controllers, vehicles simulators and web-browser interfaces.      
### 31.Physics-based neural network for non-invasive control of coherent light in scattering media  [ :arrow_down: ](https://arxiv.org/pdf/2206.00487.pdf)
>  Optical imaging through complex media, such as biological tissues or fog, is challenging due to light scattering. In the multiple scattering regime, wavefront shaping provides an effective method to retrieve information; it relies on measuring how the propagation of different optical wavefronts are impacted by scattering. Based on this principle, several wavefront shaping techniques were successfully developed, but most of them are highly invasive and limited to proof-of-principle experiments. Here, we propose to use a neural network approach to non-invasively characterize and control light scattering inside the medium and also to retrieve information of hidden objects buried within it. Unlike most of the recently-proposed approaches, the architecture of our neural network with its layers, connected nodes and activation functions has a true physical meaning as it mimics the propagation of light in our optical system. It is trained with an experimentally-measured input/output dataset built from a series of incident light patterns and corresponding camera snapshots. We apply our physics-based neural network to a fluorescence microscope in epi-configuration and demonstrate its performance through numerical simulations and experiments. This flexible method can include physical priors and we show that it can be applied to other systems as, for example, non-linear or coherent contrast mechanisms.      
### 32.A Logistic Regression Approach to Field Estimation Using Binary Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2206.00394.pdf)
>  In this letter, we consider the problem of field estimation using binary measurements. Previous work has formulated the problem as a parameter estimation problem, with the parameter estimation carried out in an online manner using sequential Monte Carlo techniques. In the current work, we consider an alternative approach to the parameter estimation based on online logistic regression. The developed algorithm is less computationally intensive than the sequential Monte Carlo approach, while having more reliable estimation performance.      
### 33.Towards Generalisable Audio Representations for Audio-Visual Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00393.pdf)
>  In audio-visual navigation (AVN), an intelligent agent needs to navigate to a constantly sound-making object in complex 3D environments based on its audio and visual perceptions. While existing methods attempt to improve the navigation performance with preciously designed path planning or intricate task settings, none has improved the model generalisation on unheard sounds with task settings unchanged. We thus propose a contrastive learning-based method to tackle this challenge by regularising the audio encoder, where the sound-agnostic goal-driven latent representations can be learnt from various audio signals of different classes. In addition, we consider two data augmentation strategies to enrich the training sounds. We demonstrate that our designs can be easily equipped to existing AVN frameworks to obtain an immediate performance gain (13.4%$\uparrow$ in SPL on Replica and 12.2%$\uparrow$ in SPL on MP3D). Our project is available at <a class="link-external link-https" href="https://AV-GeN.github.io/" rel="external noopener nofollow">this https URL</a>.      
### 34.Attention-embedded Quadratic Network (Qttention) for Effective and Interpretable Bearing Fault Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2206.00390.pdf)
>  Bearing fault diagnosis is of great importance to decrease the damage risk of rotating machines and further improve economic profits. Recently, machine learning, represented by deep learning, has made great progress in bearing fault diagnosis. However, applying deep learning to such a task still faces two major problems. On the one hand, deep learning loses its effectiveness when bearing data are noisy or big data are unavailable, making deep learning hard to implement in industrial fields. On the other hand, a deep network is notoriously a black box. It is difficult to know how a model classifies faulty signals from the normal and the physics principle behind the classification. To solve the effectiveness and interpretability issues, we prototype a convolutional network with recently-invented quadratic neurons. This quadratic neuron empowered network can qualify the noisy and small bearing data due to the strong feature representation ability of quadratic neurons. Moreover, we independently derive the attention mechanism from a quadratic neuron, referred to as qttention, by factorizing the learned quadratic function in analogue to the attention, making the model with quadratic neurons inherently interpretable. Experiments on the public and our datasets demonstrate that the proposed network can facilitate effective and interpretable bearing fault diagnosis.      
### 35.NOMA for Integrating Sensing and Communications towards 6G: A Multiple Access Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2206.00377.pdf)
>  This article focuses on the development of integrated sensing and communications (ISAC) from a multiple access (MA) perspective, where the idea of non-orthogonal multiple access (NOMA) is exploited for harmoniously accommodating the sensing and communication functionalities. We first reveal that the developing trend of ISAC is from \emph{orthogonality} to \emph{non-orthogonality}, and introduce the fundamental models of the downlink and uplink ISAC while identifying the design challenges from the MA perspective. (1) For the downlink ISAC, we propose two novel designs, namely \emph{NOMA-empowered} downlink ISAC and \emph{NOMA-inspired} downlink ISAC to effectively coordinate the inter-user interference and the sensing-to-communication interference, respectively. (2) For the uplink ISAC, we first propose a \emph{pure-NOMA-based} uplink ISAC design, where a fixed communication-to-sensing successive interference cancellation order is employed for distinguishing the mixed sensing-communication signal received over the fully shared radio resources. Then, we propose a general \emph{semi-NOMA-based} uplink ISAC design, which includes the conventional orthogonal multiple access-based and pure-NOMA-based uplink ISAC as special cases, thus being capable of providing flexible resource allocation strategies between sensing and communication. Along each proposed NOMA-ISAC design, numerical results are provided for showing the superiority over conventional ISAC designs.      
### 36.Temporal Characterization of VR Traffic for Network Slicing Requirement Definition  [ :arrow_down: ](https://arxiv.org/pdf/2206.00317.pdf)
>  Over the past few years, the concept of VR has attracted increasing interest thanks to its extensive industrial and commercial applications. Currently, the 3D models of the virtual scenes are generally stored in the VR visor itself, which operates as a standalone device. However, applications that entail multi-party interactions will likely require the scene to be processed by an external server and then streamed to the visors. However, the stringent Quality of Service (QoS) constraints imposed by VR's interactive nature require Network Slicing (NS) solutions, for which profiling the traffic generated by the VR application is crucial. To this end, we collected more than 4 hours of traces in a real setup and analyzed their temporal correlation. More specifically, we focused on the CBR encoding mode, which should generate more predictable traffic streams. From the collected data, we then distilled two prediction models for future frame size, which can be instrumental in the design of dynamic resource allocation algorithms. Our results show that even the state-of-the-art H.264 CBR mode can have significant fluctuations, which can impact the NS optimization. We then exploited the proposed models to dynamically determine the Service Level Agreement (SLA) parameters in an NS scenario, providing service with the required QoS while minimizing resource usage.      
### 37.Efficient Multi-Purpose Cross-Attention Based Image Alignment Block for Edge Devices  [ :arrow_down: ](https://arxiv.org/pdf/2206.00291.pdf)
>  Image alignment, also known as image registration, is a critical block used in many computer vision problems. One of the key factors in alignment is efficiency, as inefficient aligners can cause significant overhead to the overall problem. In the literature, there are some blocks that appear to do the alignment operation, although most do not focus on efficiency. Therefore, an image alignment block which can both work in time and/or space and can work on edge devices would be beneficial for almost all networks dealing with multiple images. Given its wide usage and importance, we propose an efficient, cross-attention-based, multi-purpose image alignment block (XABA) suitable to work within edge devices. Using cross-attention, we exploit the relationships between features extracted from images. To make cross-attention feasible for real-time image alignment problems and handle large motions, we provide a pyramidal block based cross-attention scheme. This also captures local relationships besides reducing memory requirements and number of operations. Efficient XABA models achieve real-time requirements of running above 20 FPS performance on NVIDIA Jetson Xavier with 30W power consumption compared to other powerful computers. Used as a sub-block in a larger network, XABA also improves multi-image super-resolution network performance in comparison to other alignment methods.      
### 38.Transcranial photoacoustic computed tomography of human brain function  [ :arrow_down: ](https://arxiv.org/pdf/2206.00248.pdf)
>  Herein we report the first in-human transcranial imaging of brain function using photoacoustic computed tomography. Functional responses to benchmark motor tasks were imaged on both the skull-less and the skull-intact hemispheres of a hemicraniectomy patient. The observed brain responses in these preliminary results demonstrate the potential of photoacoustic computed tomography for achieving transcranial functional imaging.      
### 39.AdaVITS: Tiny VITS for Low Computing Resource Speaker Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00208.pdf)
>  Speaker adaptation in text-to-speech synthesis (TTS) is to finetune a pre-trained TTS model to adapt to new target speakers with limited data. While much effort has been conducted towards this task, seldom work has been performed for low computational resource scenarios due to the challenges raised by the requirement of the lightweight model and less computational complexity. In this paper, a tiny VITS-based TTS model, named AdaVITS, for low computing resource speaker adaptation is proposed. To effectively reduce parameters and computational complexity of VITS, an iSTFT-based wave construction decoder is proposed to replace the upsampling-based decoder which is resource-consuming in the original VITS. Besides, NanoFlow is introduced to share the density estimate across flow blocks to reduce the parameters of the prior encoder. Furthermore, to reduce the computational complexity of the textual encoder, scaled-dot attention is replaced with linear attention. To deal with the instability caused by the simplified model, instead of using the original text encoder, phonetic posteriorgram (PPG) is utilized as linguistic feature via a text-to-PPG module, which is then used as input for the encoder. Experiment shows that AdaVITS can generate stable and natural speech in speaker adaptation with 8.97M model parameters and 0.72GFlops computational complexity.      
### 40.Learning Sparse Nonlinear Dynamics via Mixed-Integer Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2206.00176.pdf)
>  Discovering governing equations of complex dynamical systems directly from data is a central problem in scientific machine learning. In recent years, the sparse identification of nonlinear dynamics (SINDy) framework, powered by heuristic sparse regression methods, has become a dominant tool for learning parsimonious models. We propose an exact formulation of the SINDy problem using mixed-integer optimization (MIO) to solve the sparsity constrained regression problem to provable optimality in seconds. On a large number of canonical ordinary and partial differential equations, we illustrate the dramatic improvement of our approach in accurate model discovery while being more sample efficient, robust to noise, and flexible in accommodating physical constraints.      
### 41.PAGER: Progressive Attribute-Guided Extendable Robust Image Generation  [ :arrow_down: ](https://arxiv.org/pdf/2206.00162.pdf)
>  This work presents a generative modeling approach based on successive subspace learning (SSL). Unlike most generative models in the literature, our method does not utilize neural networks to analyze the underlying source distribution and synthesize images. The resulting method, called the progressive attribute-guided extendable robust image generative (PAGER) model, has advantages in mathematical transparency, progressive content generation, lower training time, robust performance with fewer training samples, and extendibility to conditional image generation. PAGER consists of three modules: core generator, resolution enhancer, and quality booster. The core generator learns the distribution of low-resolution images and performs unconditional image generation. The resolution enhancer increases image resolution via conditional generation. Finally, the quality booster adds finer details to generated images. Extensive experiments on MNIST, Fashion-MNIST, and CelebA datasets are conducted to demonstrate generative performance of PAGER.      
### 42.Real-time motion planning and decision-making for a group of differential drive robots under connectivity constraints using robust MPC and mixed-integer programming  [ :arrow_down: ](https://arxiv.org/pdf/2206.00097.pdf)
>  This work is concerned with the problem of planning trajectories and assigning tasks for a Multi-Agent System (MAS) comprised of differential drive robots. We propose a multirate hierarchical control structure that employs a planner based on robust Model Predictive Control (MPC) with mixed-integer programming (MIP) encoding. The planner computes trajectories and assigns tasks for each element of the group in real-time, while also guaranteeing the communication network of the MAS to be robustly connected at all times. Additionally, we provide a data-based methodology to estimate the disturbances sets required by the robust MPC formulation. The results are demonstrated with experiments in two obstacle-filled scenarios      
