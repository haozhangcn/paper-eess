# ArXiv eess --Mon, 6 Jun 2022
### 1.Composite Adaptive Control for Time-varying Systems with Dual Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01700.pdf)
>  This paper proposes a composite adaptive control architecture using dual adaptation scheme for dynamical systems comprising time-varying uncertain parameters. While majority of the adaptive control schemes in literature address the case of constant parameters, recent research has conceptualized improved adaptive control techniques for time-varying systems with rigorous stability proofs. The proposed work is an effort towards a similar direction, where a novel dual adaptation mechanism is introduced to efficiently tackle the time-varying nature of the parameters. Projection and $\sigma$-modification algorithms are strategically combined using congelation of variables to claim a global result for the tracking error space. While the classical adaptive systems demand a restrictive condition of persistence of excitation (PE) for accurate parameter estimation, the proposed work relies on a milder condition, called initial excitation (IE) for the same. A rigorous Lyapunov stability analysis is carried out to establish uniformly ultimately bounded (UUB) stability of the closed-loop system. Further it is analytically shown that the proposed work can recover the performance of previously designed IE-based adaptive controller in case of time invariant systems.      
### 2.Fundamentals of RIS-Aided Localization in the Far-Field  [ :arrow_down: ](https://arxiv.org/pdf/2206.01652.pdf)
>  This paper develops fundamental bounds for localization in wireless systems aided by reconfigurable intelligent surfaces (RISs). Specifically, we start from the assumption that the position and orientation of a RIS can be viewed as prior information for RIS-aided localization in wireless systems and derive Bayesian bounds for the localization of a user equipment (UE). To do this, we first derive the Bayesian Fisher information matrix (FIM) for geometric channel parameters in order to derive the Bayesian localization bounds. Subsequently, we show through the equivalent Fisher information matrix (EFIM) that all the information provided by the RIS-related geometric channel parameters is completely lost when the complex path gains are unknown. More specifically, in the absence of channel knowledge, the EFIM of the RIS-related geometric channel parameters is a zero matrix. This observation is crucial to parametric channel estimation. It mandates that any parametric channel estimator must estimate the complex path gains before estimating the RIS-related geometric channel parameters. Furthermore, we note that because these RIS-related geometric parameters are needed for localization, prior information about the complex path gains must be available for feasible UE localization. We also show that this FIM is decomposable into i) information provided by the transmitter, ii) information provided by the RIS, and iii) information provided by the receiver. We then transform the Bayesian EFIM for geometric channel parameters to the Bayesian FIM for the UE position and orientation parameters and examine its specific structure under a particular class of RIS reflection coefficients. Finally, we show the effect of having a set of RISs with perturbed position/orientation on localization performance through numerical results.      
### 3.An adaptive fuzzy sliding mode controller applied to a chaotic pendulum  [ :arrow_down: ](https://arxiv.org/pdf/2206.01617.pdf)
>  In this work, an intelligent controller is employed to the chaos control problem in a nonlinear pendulum. The adopted approach is based on the sliding mode control strategy and enhanced by an adaptive fuzzy algorithm to cope with modeling inaccuracies. The convergence properties of the closed-loop system are analytically proven using Lyapunov's direct method and Barbalat's lemma. Numerical results are also presented in order to demonstrate the control system performance.      
### 4.A Deep-Learning Usability Expansion Model of Ocean Observations  [ :arrow_down: ](https://arxiv.org/pdf/2206.01599.pdf)
>  Today's ocean numerical prediction skills depend on the availability of in-situ and remote ocean observations at the time of the predictions only. Because observations are scarce and discontinuous in time and space, numerical models are often unable to accurately model and predict real ocean dynamics, leading to a lack of fulfillment of a range of services that require reliable predictions at various temporal and spatial scales. The process of constraining free numerical models with observations is known as data assimilation. The primary objective is to minimize the misfit of model states with the observations while respecting the rules of physics. The caveat of this approach is that measurements are used only once, at the time of the prediction. The information contained in the history of the measurements and its role in the determinism of the prediction is, therefore, not accounted for. Consequently, historical measurement cannot be used in real-time forecasting systems. The research presented in this paper provides a novel approach rooted in artificial intelligence to expand the usability of observations made before the time of the prediction. Our approach is based on the re-purpose of an existing deep learning model, called U-Net, designed specifically for image segmentation analysis in the biomedical field. U-Net is used here to create a Transform Model that retains the temporal and spatial evolution of the differences between model and observations to produce a correction in the form of regression weights that evolves spatially and temporally with the model both forward and backward in time, beyond the observation period. Using virtual observations, we show that the usability of the observation can be extended up to a one year prior or post observations.      
### 5.Functional Connectivity Methods for EEG-based Biometrics on a Large, Heterogeneous Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2206.01475.pdf)
>  This study examines the utility of functional connectivity (FC) and graph-based (GB) measures with a support vector machine classifier for use in electroencephalogram (EEG) based biometrics. Although FC-based features have been used in biometric applications, studies assessing the identification algorithms on heterogeneous and large datasets are scarce. This work investigates the performance of FC and GB metrics on a dataset of 184 subjects formed by pooling three datasets recorded under different protocols and acquisition systems. The results demonstrate the higher discriminatory power of FC than GB metrics. The identification accuracy increases with higher frequency EEG bands, indicating the enhanced uniqueness of the neural signatures in beta and gamma bands. Using all the 56 EEG channels common to the three databases, the best identification accuracy of 97.4% is obtained using phase-locking value (PLV) based measures extracted from the gamma frequency band. Further, we investigate the effect of the length of the analysis epoch to determine the data acquisition time required to obtain satisfactory identification accuracy. When the number of channels is reduced to 21 from 56, there is a marginal reduction of 2.4% only in the identification accuracy using PLV features in the gamma band. Additional experiments have been conducted to study the effect of the cognitive state of the subject and mismatched train/test conditions on the performance of the system.      
### 6.PAC Statistical Model Checking of Mean Payoff in Discrete- and Continuous-Time MDP  [ :arrow_down: ](https://arxiv.org/pdf/2206.01465.pdf)
>  Markov decision processes (MDP) and continuous-time MDP (CTMDP) are the fundamental models for non-deterministic systems with probabilistic uncertainty. Mean payoff (a.k.a. long-run average reward) is one of the most classic objectives considered in their context. We provide the first algorithm to compute mean payoff probably approximately correctly in unknown MDP; further, we extend it to unknown CTMDP. We do not require any knowledge of the state space, only a lower bound on the minimum transition probability, which has been advocated in literature. In addition to providing probably approximately correct (PAC) bounds for our algorithm, we also demonstrate its practical nature by running experiments on standard benchmarks.      
### 7.Safety Certification for Stochastic Systems via Neural Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2206.01463.pdf)
>  Providing non-trivial certificates of safety for non-linear stochastic systems is an important open problem that limits the wider adoption of autonomous systems in safety-critical applications. One promising solution to address this problem is barrier functions. The composition of a barrier function with a stochastic system forms a supermartingale, thus enabling the computation of the probability that the system stays in a safe set over a finite time horizon via martingale inequalities. However, existing approaches to find barrier functions for stochastic systems generally rely on convex optimization programs that restrict the search of a barrier to a small class of functions such as low degree SoS polynomials and can be computationally expensive. In this paper, we parameterize a barrier function as a neural network and show that techniques for robust training of neural networks can be successfully employed to find neural barrier functions. Specifically, we leverage bound propagation techniques to certify that a neural network satisfies the conditions to be a barrier function via linear programming and then employ the resulting bounds at training time to enforce the satisfaction of these conditions. We also present a branch-and-bound scheme that makes the certification framework scalable. We show that our approach outperforms existing methods in several case studies and often returns certificates of safety that are orders of magnitude larger.      
### 8.Optimal Control for Unmanned Systems with One-way Broadcast Communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.01448.pdf)
>  Unmanned systems (USs) including unmanned aerial vehicles, unmanned underwater vehicles, and unmanned ground vehicles have great application prospects in military and civil fields, among which the process of finding feasible and optimal paths for the agents in USs is a kernel problem. Traditional path finding algorithms are hard to adequately obtain optimal paths in real-time under fast time-varying and poor communication environments. We propose an online optimal control algorithm for USs based on a one-way broadcast communication mode under the assumption of a poor communication environment, mobile targets, radars (or sonar), and missiles (or torpedoes). With the principle of receding horizon control, optimal (or suboptimal) paths are then generated by the approximation theory of neural networks and gradient optimization techniques, with low computation requirements. Also, we give a convergence analysis for our algorithm, and show that each agent can reach its target in finite time under some conditions on agents, targets and radar-missiles. Moreover, simulations demonstrate that the agents in USs can generate optimal (or suboptimal) paths in real time using our algorithm while effectively avoiding collision with other agents or detection by enemy radars.      
### 9.Dual-Port Dynamically Reconfigurable Battery with Semi-Controlled and Fully-Controlled Outputs  [ :arrow_down: ](https://arxiv.org/pdf/2206.01435.pdf)
>  Modular multilevel converters (MMC) and cascaded H-bridge (CHB) converters are an established concept in ultra-high voltage systems. In combination with batteries, these circuits allow dynamically changing the series or parallel configuration of subportions of the battery as so-called modular battery integrated converters or reconfigurable batteries, and are being discussed for grid-storage and electromobility applications. A large body of research focuses on such circuits for supplying a single load, such as a motor for electric drives. Modularity, failure tolerance, less dependence on the weakest element of a battery pack, higher controllability, and better efficiency are the main incentives behind this pursuit. However, most studies neglect the auxiliary loads which require isolation from the high-voltage battery. This paper proposes a simple topology and controller that can fork off a second (galvanically isolated) output of a reconfigurable dc battery. The proposed system provides a nonisolated semicontrolled port for the dc link to maintain the operating point of the main inverter(s) close to optimal, while fully controlling an isolated output for the auxiliaries per the safety regulations. The proposed system does not require additional active switches for the auxiliary port and can operate with a wide range of voltages. Simulation and experiments verify the developed analysis.      
### 10.Receding Horizon Games with Coupling Constraints for Demand-Side Management  [ :arrow_down: ](https://arxiv.org/pdf/2206.01431.pdf)
>  Distributed energy storage and flexible loads are essential tools for ensuring stable and robust operation of the power grid in spite of the challenges arising from the integration of volatile renewable energy generation and increasing peak loads due to widespread electrification. This paper proposes a novel demand-side management policy to coordinate self-interested energy prosumers based on receding horizon games, i.e., a closed-loop receding-horizon implementation of game-theoretic day-ahead planning. Practical stability and recursive constraint satisfaction of the proposed feedback control policy is proven under symmetric pricing assumptions using tools from game theory and economic model predictive control. Our numerical studies show that the proposed approach is superior to standard open-loop day-head implementations in terms of peak-shaving, disturbance rejection, and control performance.      
### 11.LenslessPiCam: A Hardware and Software Platform for Lensless Computational Imaging with a Raspberry Pi  [ :arrow_down: ](https://arxiv.org/pdf/2206.01430.pdf)
>  Lensless imaging seeks to replace/remove the lens in a conventional imaging system. The earliest cameras were in fact lensless, relying on long exposure times to form images on the other end of a small aperture in a darkened room/container (camera obscura). The introduction of a lens allowed for more light throughput and therefore shorter exposure times, while retaining sharp focus. The incorporation of digital sensors readily enabled the use of computational imaging techniques to post-process and enhance raw images (e.g. via deblurring, inpainting, denoising, sharpening). Recently, imaging scientists have started leveraging computational imaging as an integral part of lensless imaging systems, allowing them to form viewable images from the highly multiplexed raw measurements of lensless cameras (see [5] and references therein for a comprehensive treatment of lensless imaging). This represents a real paradigm shift in camera system design as there is more flexibility to cater the hardware to the application at hand (e.g. lightweight or flat designs). This increased flexibility comes however at the price of a more demanding post-processing of the raw digital recordings and a tighter integration of sensing and computation, often difficult to achieve in practice due to inefficient interactions between the various communities of scientists involved. With LenslessPiCam, we provide an easily accessible hardware and software framework to enable researchers, hobbyists, and students to implement and explore practical and computational aspects of lensless imaging. We also provide detailed guides and exercises so that LenslessPiCam can be used as an educational resource, and point to results from our graduate-level signal processing course.      
### 12.Vision-Aided Frame-Capture-Based CSI Recomposition for WiFi Sensing: A Multimodal Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.01414.pdf)
>  Recompositing channel state information (CSI) from the beamforming feedback matrix (BFM), which is a compressed version of CSI and can be captured because of its lack of encryption, is an alternative way of implementing firmware-agnostic WiFi sensing. In this study, we propose the use of camera images toward the accuracy enhancement of CSI recomposition from BFM. The key motivation for this vision-aided CSI recomposition is to draw a first-hand insight that the BFM does not fully involve spatial information to recomposite CSI and that this could be compensated by camera images. To leverage the camera images, we use multimodal deep learning, where the two modalities, i.e., images and BFMs, are integrated to recomposite the CSI. We conducted experiments using IEEE 802.11ac devices. The experimental results confirmed that the recomposition accuracy of the proposed multimodal framework is improved compared to the single-modal framework only using images or BFMs.      
### 13.Data Encryption based on 9D Complex Chaotic System with Quaternion for Smart Grid  [ :arrow_down: ](https://arxiv.org/pdf/2206.01402.pdf)
>  With the development of smart grid, the operation and control of power system is realized through power communication network, especially the power production and enterprise management business involve a large amount of sensitive information, and the requirements for data security and real-time transmission are gradually improved. In this paper, a new 9D complex chaotic system with quaternion is proposed for the encryption of smart grid data. Firstly, a new 9D complex chaotic system with quaternion is proposed, and its attractors, bifurcation diagram, complexity, and 0-1 test are analyzed. Secondly, the pseudo-random sequences are generated by the new chaotic system to encrypt power data. Finally, the proposed encryption algorithm is verifed with power data and images in the smart grid, which can ensure the encryption security and real-time. The verifcation results show that the proposed encryption scheme is technically feasible and available for power data and image encryption in smart grid.      
### 14.Completion Time Minimization of Fog-RAN-Assisted Federated Learning With Rate-Splitting Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2206.01373.pdf)
>  This work studies federated learning (FL) over a fog radio access network, in which multiple internet-of-things (IoT) devices cooperatively learn a shared machine learning model by communicating with a cloud server (CS) through distributed access points (APs). Under the assumption that the fronthaul links connecting APs to CS have finite capacity, a rate-splitting transmission at IoT devices (IDs) is proposed which enables hybrid edge and cloud decoding of split uplink messages. The problem of completion time minimization for FL is tackled by optimizing the rate-splitting transmission and fronthaul quantization strategies along with training hyperparameters such as precision and iteration numbers. Numerical results show that the proposed rate-splitting transmission achieves notable gains over benchmark schemes which rely solely on edge or cloud decoding.      
### 15.Detecting Pulmonary Embolism from Computed Tomography Using Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2206.01344.pdf)
>  The clinical symptoms of pulmonary embolism (PE) are very diverse and non-specific, which makes it difficult to diagnose. In addition, pulmonary embolism has multiple triggers and is one of the major causes of vascular death. Therefore, if it can be detected and treated quickly, it can significantly reduce the risk of death in hospitalized patients. In the detection process, the cost of computed tomography pulmonary angiography (CTPA) is high, and angiography requires the injection of contrast agents, which increase the risk of damage to the patient. Therefore, this study will use a deep learning approach to detect pulmonary embolism in all patients who take a CT image of the chest using a convolutional neural network. With the proposed pulmonary embolism detection system, we can detect the possibility of pulmonary embolism at the same time as the patient's first CT image, and schedule the CTPA test immediately, saving more than a week of CT image screening time and providing timely diagnosis and treatment to the patient.      
### 16.Characterizing Within-Driver Variability in Driving Dynamics During Obstacle Avoidance Maneuvers  [ :arrow_down: ](https://arxiv.org/pdf/2206.01331.pdf)
>  Variability in human response creates non-trivial challenges for modeling and control of human-automation systems. As autonomy becomes pervasive, methods that can accommodate human variability will become paramount, to ensure efficiency, safety, and high levels of performance. We propose an easily computable modeling framework which takes advantage of a metric to assess variability in individual human response in a dynamic task that subjects repeat over several trials. Our approach is based in a transformation of observed trajectories to a reproducing kernel Hilbert space, which captures variability in human response as a distribution embedded within the Hilbert space. We evaluate the similarity across responses via the maximum mean discrepancy, which measures the distance between distributions within the Hilbert space. We apply this metric to a difficult driving task designed to elucidate differences across subjects. We conducted a pilot study with 6 subjects in an advanced driving simulator, in which subjects were tasked with collision avoidance of an obstacle in the middle of the road, around a blind corner, in a nighttime scenario, while steering only with the non-dominant hand.      
### 17.Data-Driven Linear Koopman Embedding for Model-Predictive Power System Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.01272.pdf)
>  This paper presents a linear Koopman embedding for model predictive emergency voltage regulation in power systems, by way of a data-driven lifting of the system dynamics into a higher dimensional linear space over which the MPC (model predictive control) is exercised, thereby scaling as well as expediting the MPC computation for its real-time implementation for practical systems. We develop a {\em Koopman-inspired deep neural network} (KDNN) architecture for the linear embedding of the voltage dynamics subjected to reactive controls. The training of the KDNN for the purposes of linear embedding is done using the simulated voltage trajectories under a variety of applied control inputs and load conditions. The proposed framework learns the underlying system dynamics from the input/output data in the form of a triple of transforms: A Neural Network (NN)-based lifting to a higher dimension, a linear dynamics within that higher dynamics, and an NN-based projection to original space. This approach alleviates the burden of an ad-hoc selection of the basis functions for the purposes of lifting to higher dimensional linear space. The MPC is computed over the linear dynamics, making the control computation scalable and also real-time.      
### 18.Impact of Frequency Support by Wind Turbines on Small-Signal Stability of Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01237.pdf)
>  Rising wind energy integration, accompanied by a decreasing level of system inertia, requires additional sources of ancillary services. Wind turbines based on doubly fed induction generators (DFIG) can provide inertial and primary frequency support, when equipped with specific controls. This paper investigates the effect of frequency support provision by DFIGs on the small-signal stability of power systems. To this end, a modified version of the Kundur two-area test system is employed to analyze different scenarios. Wind energy generation is either added to the existing system or displaces part of the synchronous generation. Simulations show that primary frequency support tends to improve the damping of electromechanical oscillations and deteriorate it for converter control-based ones. On the other hand, inertial response may be either beneficial, detrimental or negligible to damping, depending on the tuning of control parameters.      
### 19.Snow Mountain: Dataset of Audio Recordings of The Bible in Low Resource Languages  [ :arrow_down: ](https://arxiv.org/pdf/2206.01205.pdf)
>  Automatic Speech Recognition (ASR) has increasing utility in the modern world. There are a many ASR models available for languages with large amounts of training data like English. However, low-resource languages are poorly represented. In response we create and release an open-licensed and formatted dataset of audio recordings of the Bible in low-resource northern Indian languages. We setup multiple experimental splits and train and analyze two competitive ASR models to serve as the baseline for future research using this data.      
### 20.KCRL: Krasovskii-Constrained Reinforcement Learning with Guaranteed Stability in Nonlinear Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01704.pdf)
>  Learning a dynamical system requires stabilizing the unknown dynamics to avoid state blow-ups. However, current reinforcement learning (RL) methods lack stabilization guarantees, which limits their applicability for the control of safety-critical systems. We propose a model-based RL framework with formal stability guarantees, Krasovskii Constrained RL (KCRL), that adopts Krasovskii's family of Lyapunov functions as a stability constraint. The proposed method learns the system dynamics up to a confidence interval using feature representation, e.g. Random Fourier Features. It then solves a constrained policy optimization problem with a stability constraint based on Krasovskii's method using a primal-dual approach to recover a stabilizing policy. We show that KCRL is guaranteed to learn a stabilizing policy in a finite number of interactions with the underlying unknown system. We also derive the sample complexity upper bound for stabilization of unknown nonlinear dynamical systems via the KCRL framework.      
### 21.A unifying framework for tangential interpolation of structured bilinear control systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01657.pdf)
>  In this paper, we consider the structure-preserving model order reduction problem for multi-input/multi-output bilinear control systems by tangential interpolation. We propose a new type of tangential interpolation problem for structured bilinear systems, for which we develop a new structure-preserving interpolation framework. This new framework extends and generalizes different formulations of tangential interpolation for bilinear systems from the literature and also provides a unifying framework. We then derive explicit conditions on the projection spaces to enforce tangential interpolation in different settings, including conditions for tangential Hermite interpolation. The analysis is illustrated by means of three numerical examples.      
### 22.Structure-Preserving Model Order Reduction for Index One Port-Hamiltonian Descriptor Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01608.pdf)
>  We develop optimization-based structure-preserving model order reduction (MOR) methods for port-Hamiltonian (pH) descriptor systems of differentiation index one. Descriptor systems in pH form permit energy-based modeling and intuitive coupling of physical systems across different physical domains, scales, and accuracies. This makes pH models well-suited building-blocks for component-wise modeling of large system networks. In this context, it is often necessary to preserve the pH structure during MOR. We discuss current projection-based and structure-preserving MOR algorithms for pH systems and present a new optimization-based framework for that task. The benefits of our method include a simplified treatment of algebraic constraints and often a higher accuracy of the resulting reduced-order model, which is demonstrated by several numerical examples.      
### 23.Energy-Efficient Resource Allocation for Aggregated RF/VLC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.01567.pdf)
>  Visible light communication (VLC) is envisioned as a core component of future wireless communication networks due to, among others, the huge unlicensed bandwidth it offers and the fact that it does not cause any interference to existing radio frequency (RF) communication systems. Most research on RF and VLC coexistence has focused on hybrid designs where data transmission to any user could originate from either an RF or a VLC access point (AP). However, hybrid RF/VLC systems fail to exploit the distinct transmission characteristics of RF and VLC systems to fully reap the benefits they can offer. Aggregated RF/VLC systems, in which any user can be served simultaneously by both RF and VLC APs, have recently emerged as a more promising and robust design for the coexistence of RF and VLC systems. To this end, this paper, for the first time, investigates AP assignment, subchannel allocation (SA), and transmit power allocation (PA) to optimize the energy efficiency (EE) of aggregated RF/VLC systems while considering the effects of interference and VLC line-of-sight link blockages. A novel and challenging EE optimization problem is formulated for which an efficient joint solution based on alternating optimization is developed. More particularly, an energy-efficient AP assignment algorithm based on matching theory is proposed. Then, a low-complexity SA scheme that allocates subchannels to users based on their channel conditions is developed. Finally, an effective PA algorithm is presented by utilizing the quadratic transform approach and a multi-objective optimization framework. Extensive simulation results reveal that: 1) the proposed joint AP assignment, SA, and PA solution obtains significant EE, sum-rate, and outage performance gains with low complexity, and 2) the aggregated RF/VLC system provides considerable performance improvement compared to hybrid RF/VLC systems.      
### 24.Detecting the Severity of Major Depressive Disorder from Speech: A Novel HARD-Training Methodology  [ :arrow_down: ](https://arxiv.org/pdf/2206.01542.pdf)
>  Major Depressive Disorder (MDD) is a common worldwide mental health issue with high associated socioeconomic costs. The prediction and automatic detection of MDD can, therefore, make a huge impact on society. Speech, as a non-invasive, easy to collect signal, is a promising marker to aid the diagnosis and assessment of MDD. In this regard, speech samples were collected as part of the Remote Assessment of Disease and Relapse in Major Depressive Disorder (RADAR-MDD) research programme. RADAR-MDD was an observational cohort study in which speech and other digital biomarkers were collected from a cohort of individuals with a history of MDD in Spain, United Kingdom and the Netherlands. In this paper, the RADAR-MDD speech corpus was taken as an experimental framework to test the efficacy of a Sequence-to-Sequence model with a local attention mechanism in a two-class depression severity classification paradigm. Additionally, a novel training method, HARD-Training, is proposed. It is a methodology based on the selection of more ambiguous samples for the model training, and inspired by the curriculum learning paradigm. HARD-Training was found to consistently improve - with an average increment of 8.6% - the performance of our classifiers for both of two speech elicitation tasks used and each collection site of the RADAR-MDD speech corpus. With this novel methodology, our Sequence-to-Sequence model was able to effectively detect MDD severity regardless of language. Finally, recognising the need for greater awareness of potential algorithmic bias, we conduct an additional analysis of our results separately for each gender.      
### 25.Constraining Gaussian processes for physics-informed acoustic emission mapping  [ :arrow_down: ](https://arxiv.org/pdf/2206.01495.pdf)
>  The automated localisation of damage in structures is a challenging but critical ingredient in the path towards predictive or condition-based maintenance of high value structures. The use of acoustic emission time of arrival mapping is a promising approach to this challenge, but is severely hindered by the need to collect a dense set of artificial acoustic emission measurements across the structure, resulting in a lengthy and often impractical data acquisition process. In this paper, we consider the use of physics-informed Gaussian processes for learning these maps to alleviate this problem. In the approach, the Gaussian process is constrained to the physical domain such that information relating to the geometry and boundary conditions of the structure are embedded directly into the learning process, returning a model that guarantees that any predictions made satisfy physically-consistent behaviour at the boundary. A number of scenarios that arise when training measurement acquisition is limited, including where training data are sparse, and also of limited coverage over the structure of interest. Using a complex plate-like structure as an experimental case study, we show that our approach significantly reduces the burden of data collection, where it is seen that incorporation of boundary condition knowledge significantly improves predictive accuracy as training observations are reduced, particularly when training measurements are not available across all parts of the structure.      
### 26.Distributional loss for convolutional neural network regression and application to GNSS multi-path estimation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01473.pdf)
>  Convolutional Neural Network (CNN) have been widely used in image classification. Over the years, they have also benefited from various enhancements and they are now considered as state of the art techniques for image like data. However, when they are used for regression to estimate some function value from images, fewer recommendations are available. In this study, a novel CNN regression model is proposed. It combines convolutional neural layers to extract high level features representations from images with a soft labelling technique. More specifically, as the deep regression task is challenging, the idea is to account for some uncertainty in the targets that are seen as distributions around their mean. The estimations are carried out by the model in the form of distributions. Building from earlier work, a specific histogram loss function based on the Kullback-Leibler (KL) divergence is applied during training. The model takes advantage of the CNN feature representation and is able to carry out estimation from multi-channel input images. To assess and illustrate the technique, the model is applied to Global Navigation Satellite System (GNSS) multi-path estimation where multi-path signal parameters have to be estimated from correlator output images from the I and Q channels. The multi-path signal delay, magnitude, Doppler shift frequency and phase parameters are estimated from synthetically generated datasets of satellite signals. Experiments are conducted under various receiving conditions and various input images resolutions to test the estimation performances quality and robustness. The results show that the proposed soft labelling CNN technique using distributional loss outperforms classical CNN regression under all conditions. Furthermore, the extra learning performance achieved by the model allows the reduction of input image resolution from 80x80 down to 40x40 or sometimes 20x20.      
### 27.Learning Distributed and Fair Policies for Network Load Balancing as Markov Potentia Game  [ :arrow_down: ](https://arxiv.org/pdf/2206.01451.pdf)
>  This paper investigates the network load balancing problem in data centers (DCs) where multiple load balancers (LBs) are deployed, using the multi-agent reinforcement learning (MARL) framework. The challenges of this problem consist of the heterogeneous processing architecture and dynamic environments, as well as limited and partial observability of each LB agent in distributed networking systems, which can largely degrade the performance of in-production load balancing algorithms in real-world setups. Centralised-training-decentralised-execution (CTDE) RL scheme has been proposed to improve MARL performance, yet it incurs -- especially in distributed networking systems, which prefer distributed and plug-and-play design scheme -- additional communication and management overhead among agents. We formulate the multi-agent load balancing problem as a Markov potential game, with a carefully and properly designed workload distribution fairness as the potential function. A fully distributed MARL algorithm is proposed to approximate the Nash equilibrium of the game. Experimental evaluations involve both an event-driven simulator and real-world system, where the proposed MARL load balancing algorithm shows close-to-optimal performance in simulations, and superior results over in-production LBs in the real-world system.      
### 28.Quantum coherent feedback control with photons  [ :arrow_down: ](https://arxiv.org/pdf/2206.01445.pdf)
>  The purpose of this paper is to study two-photon dynamics induced by the coherent feedback control of a cavity quantum electrodynamics (cavity QED) system coupled to a waveguide with a terminal mirror. In this set-up, the two-level system in the cavity can work as a photon source, and the photon emitted into the waveguide can re-interact with the cavity-QED system many times after perfectly reflected by the terminal mirror of the waveguide, during which the feedback can tune the number of the photons in the waveguide and cavity. We analyze the dynamics of two-photon processes in this coherent feedback network in two scenarios: the continuous mode coupling scheme and the discrete periodic mode coupling scheme between the waveguide and cavity. The difference of these coupling schemes is due to the transmission of fields between the waveguide and cavity, and the their relative scales. Specifically, in the continuous mode coupling scheme, the generation of two-photon states is influenced by the length of the feedback loop of the waveguide and the coupling strength between the waveguide and the cavity QED system. By tuning the length of the waveguide and the coupling strength, we are able to generate two-photon states efficiently. In the discrete periodic mode coupling scheme, the Rabi oscillation in the cavity can be stabilized and there are no notable two-photon states in the waveguide.      
### 29.Learning rich optical embeddings for privacy-preserving lensless image classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.01429.pdf)
>  By replacing the lens with a thin optical element, lensless imaging enables new applications and solutions beyond those supported by traditional camera design and post-processing, e.g. compact and lightweight form factors and visual privacy. The latter arises from the highly multiplexed measurements of lensless cameras, which require knowledge of the imaging system to recover a recognizable image. In this work, we exploit this unique multiplexing property: casting the optics as an encoder that produces learned embeddings directly at the camera sensor. We do so in the context of image classification, where we jointly optimize the encoder's parameters and those of an image classifier in an end-to-end fashion. Our experiments show that jointly learning the lensless optical encoder and the digital processing allows for lower resolution embeddings at the sensor, and hence better privacy as it is much harder to recover meaningful images from these measurements. Additional experiments show that such an optimization allows for lensless measurements that are more robust to typical real-world image transformations. While this work focuses on classification, the proposed programmable lensless camera and end-to-end optimization can be applied to other computational imaging tasks.      
### 30.MetaLR: Layer-wise Learning Rate based on Meta-Learning for Adaptively Fine-tuning Medical Pre-trained Models  [ :arrow_down: ](https://arxiv.org/pdf/2206.01408.pdf)
>  When applying transfer learning for medical image analysis, downstream tasks often have significant gaps with the pre-training tasks. Previous methods mainly focus on improving the transferabilities of the pre-trained models to bridge the gaps. In fact, model fine-tuning can also play a very important role in tackling this problem. A conventional fine-tuning method is updating all deep neural networks (DNNs) layers by a single learning rate (LR), which ignores the unique transferabilities of different layers. In this work, we explore the behaviors of different layers in the fine-tuning stage. More precisely, we first hypothesize that lower-level layers are more domain-specific while higher-level layers are more task-specific, which is verified by a simple bi-directional fine-tuning scheme. It is harder for the pre-trained specific layers to transfer to new tasks than general layers. On this basis, to make different layers better co-adapt to the downstream tasks according to their transferabilities, a meta-learning-based LR learner, namely MetaLR, is proposed to assign LRs for each layer automatically. Extensive experiments on various medical applications (i.e., POCUS, BUSI, Chest X-ray, and LiTS) well confirm our hypothesis and show the superior performance of the proposed methods to previous state-of-the-art fine-tuning methods.      
### 31.Dynamic Structured Illumination Microscopy with a Neural Space-time Model  [ :arrow_down: ](https://arxiv.org/pdf/2206.01397.pdf)
>  Structured illumination microscopy (SIM) reconstructs a super-resolved image from multiple raw images; hence, acquisition speed is limited, making it unsuitable for dynamic scenes. We propose a new method, Speckle Flow SIM, that models sample motion during the data capture in order to reconstruct dynamic scenes with super-resolution. Speckle Flow SIM uses fixed speckle illumination and relies on sample motion to capture a sequence of raw images. Then, the spatio-temporal relationship of the dynamic scene is modeled using a neural space-time model with coordinate-based multi-layer perceptrons (MLPs), and the motion dynamics and the super-resolved scene are jointly recovered. We validated Speckle Flow SIM in simulation and built a simple, inexpensive experimental setup with off-the-shelf components. We demonstrated that Speckle Flow SIM can reconstruct a dynamic scene with deformable motion and 1.88x the diffraction-limited resolution in experiment.      
### 32.Feedback Stabilization of Tank-Liquid System with Robustness to Wall Friction  [ :arrow_down: ](https://arxiv.org/pdf/2206.01385.pdf)
>  We solve the feedback stabilization problem for a tank, with friction, containing a liquid modeled by the viscous Saint-Venant system of Partial Differential Equations. A spill-free exponential stabilization is achieved, with robustness to the wall friction forces. A Control Lyapunov Functional (CLF) methodology with two different Lyapunov functionals is employed. These functionals determine specific parameterized sets which approximate the state space. The feedback law is designed based only on one of the two functionals (which is the CLF) while the other functional is used for the derivation of estimates of the sup-norm of the velocity. The feedback law does not require the knowledge of the exact relation of the friction coefficient. Two main results are provided: the first deals with the special case of a velocity-independent friction coefficient, while the second deals with the general case. The obtained results are new even in the frictionless case.      
### 33.Incremental Learning Meets Transfer Learning: Application to Multi-site Prostate MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.01369.pdf)
>  Many medical datasets have recently been created for medical image segmentation tasks, and it is natural to question whether we can use them to sequentially train a single model that (1) performs better on all these datasets, and (2) generalizes well and transfers better to the unknown target site domain. Prior works have achieved this goal by jointly training one model on multi-site datasets, which achieve competitive performance on average but such methods rely on the assumption about the availability of all training data, thus limiting its effectiveness in practical deployment. In this paper, we propose a novel multi-site segmentation framework called incremental-transfer learning (ITL), which learns a model from multi-site datasets in an end-to-end sequential fashion. Specifically, "incremental" refers to training sequentially constructed datasets, and "transfer" is achieved by leveraging useful information from the linear combination of embedding features on each dataset. In addition, we introduce our ITL framework, where we train the network including a site-agnostic encoder with pre-trained weights and at most two segmentation decoder heads. We also design a novel site-level incremental loss in order to generalize well on the target domain. Second, we show for the first time that leveraging our ITL training scheme is able to alleviate challenging catastrophic forgetting problems in incremental learning. We conduct experiments using five challenging benchmark datasets to validate the effectiveness of our incremental-transfer learning approach. Our approach makes minimal assumptions on computation resources and domain-specific expertise, and hence constitutes a strong starting point in multi-site medical image segmentation.      
### 34.Adversarial Attacks on Human Vision  [ :arrow_down: ](https://arxiv.org/pdf/2206.01365.pdf)
>  This article presents an introduction to visual attention retargeting, its connection to visual saliency, the challenges associated with it, and ideas for how it can be approached. The difficulty of attention retargeting as a saliency inversion problem lies in the lack of one-to-one mapping between saliency and the image domain, in addition to the possible negative impact of saliency alterations on image aesthetics. A few approaches from recent literature to solve this challenging problem are reviewed, and several suggestions for future development are presented.      
### 35.Beta Generalized Normal Distribution with an Application for SAR Image Processing  [ :arrow_down: ](https://arxiv.org/pdf/2206.01357.pdf)
>  We introduce the beta generalized normal distribution which is obtained by compounding the beta and generalized normal [Nadarajah, S., A generalized normal distribution, \emph{Journal of Applied Statistics}. 32, 685--694, 2005] distributions. The new model includes as sub-models the beta normal, beta Laplace, normal, and Laplace distributions. The shape of the new distribution is quite flexible, specially the skewness and the tail weights, due to two additional parameters. We obtain general expansions for the moments. The estimation of the parameters is investigated by maximum likelihood. We also proposed a random number generator for the new distribution. Actual synthetic aperture radar were analyzed and modeled after the new distribution. Results could outperform the $\mathcal{G}^0$, $\mathcal{K}$, and $\Gamma$ distributions in several scenarios.      
### 36.Equipping Black-Box Policies with Model-Based Advice for Stable Nonlinear Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.01341.pdf)
>  Machine-learned black-box policies are ubiquitous for nonlinear control problems. Meanwhile, crude model information is often available for these problems from, e.g., linear approximations of nonlinear dynamics. We study the problem of equipping a black-box control policy with model-based advice for nonlinear control on a single trajectory. We first show a general negative result that a naive convex combination of a black-box policy and a linear model-based policy can lead to instability, even if the two policies are both stabilizing. We then propose an adaptive $\lambda$-confident policy, with a coefficient $\lambda$ indicating the confidence in a black-box policy, and prove its stability. With bounded nonlinearity, in addition, we show that the adaptive $\lambda$-confident policy achieves a bounded competitive ratio when a black-box policy is near-optimal. Finally, we propose an online learning approach to implement the adaptive $\lambda$-confident policy and verify its efficacy in case studies about the CartPole problem and a real-world electric vehicle (EV) charging problem with data bias due to COVID-19.      
### 37.SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG  [ :arrow_down: ](https://arxiv.org/pdf/2206.01323.pdf)
>  Electroencephalography (EEG) provides access to neuronal dynamics non-invasively with millisecond resolution, rendering it a viable method in neuroscience and healthcare. However, its utility is limited as current EEG technology does not generalize well across domains (i.e., sessions and subjects) without expensive supervised re-calibration. Contemporary methods cast this transfer learning (TL) problem as a multi-source/-target unsupervised domain adaptation (UDA) problem and address it with deep learning or shallow, Riemannian geometry aware alignment methods. Both directions have, so far, failed to consistently close the performance gap to state-of-the-art domain-specific methods based on tangent space mapping (TSM) on the symmetric positive definite (SPD) manifold. Here, we propose a theory-based machine learning framework that enables, for the first time, learning domain-invariant TSM models in an end-to-end fashion. To achieve this, we propose a new building block for geometric deep learning, which we denote SPD domain-specific momentum batch normalization (SPDDSMBN). A SPDDSMBN layer can transform domain-specific SPD inputs into domain-invariant SPD outputs, and can be readily applied to multi-source/-target and online UDA scenarios. In extensive experiments with 6 diverse EEG brain-computer interface (BCI) datasets, we obtain state-of-the-art performance in inter-session and -subject TL with a simple, intrinsically interpretable network architecture, which we denote TSMNet.      
### 38.Optimization of Energy-Constrained IRS-NOMA Using a Complex Circle Manifold Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.01312.pdf)
>  This work investigates the performance of intelligent reflective surfaces (IRSs) assisted uplink non-orthogonal multiple access (NOMA) in energy-constrained networks. Specifically, we formulate and solve two optimization problems, one for minimizing the users' sum transmit power and another for maximizing the energy efficiency (EE) of the system. The two problems are solved by jointly optimizing the users' transmit powers and the passive beamforming coefficients at the IRS reflectors subject to the users' individual uplink rate constraints. A novel algorithm is developed to optimize the IRS passive beamforming coefficients by optimizing the objective function over the \textit{complex circle manifold} (CCM), exploiting the manifold optimization technique. The proposed manifold optimization-based solution is bench-marked against the rather \textit{standard} semi-definite relaxation method (SDR). The results show that the manifold optimization-based algorithm achieves significantly better performance for both transmit power minimization and EE maximization problems at a computational complexity lower than the SDR approach. The results also reveal that IRS-NOMA is superior to the orthogonal multiple access (OMA) counterpart only when the users' target achievable rate requirements are relatively high.      
### 39.The Musical Arrow of Time -- The Role of Temporal Asymmetry in Music and Its Organicist Implications  [ :arrow_down: ](https://arxiv.org/pdf/2206.01305.pdf)
>  Adopting a performer-centric perspective, we frequently encounter two statements: "music flows", and "music is life-like". This dissertation builds on top of the two statements above, resulting in an exploration of the role of temporal asymmetry in music (generalizing "music flows") and its relation to the idea of organicism (generalizing "music is life-like"). We focus on two aspects of temporal asymmetry. The first aspect concerns the vastly different epistemic mechanisms with which we obtain knowledge of the past and the future. A particular musical consequence follows: recurrence. The epistemic difference between the past and the future shapes our experience and interpretation of recurring events in music. The second aspect concerns the arrow of time: the unambiguous ordering imposed on temporal events gives rise to the a priori pointedness of time, rendering time asymmetrical and irreversible. A discussion on thermodynamics informs us musically: the arrow of time effectuates itself in musical forms by delaying the placement of the climax. <br>Organicism serves as a mediating topic, engaging with the concept of life as in organisms. On the one hand, organicism is related to temporal asymmetry in science via a thermodynamical interpretation of life as entropy-reducing entities. On the other hand, organicism is a topic native to music via the universally acknowledged artistic idea that music should be interpreted as a vital force possessing volitional power. With organicism as a mediator, we better understand the role of temporal asymmetry in music. In particular, we view musical form as a process of expansion and elaboration analogous to organic growth. Finally, we present an organicist interpretation of delaying the climax: viewing musical form as the result of organic growth, the arrow of time translates to a preference for prepending structure over appending structure.      
### 40.Lossless Compression of Point Cloud Sequences Using Sequence Optimized CNN Models  [ :arrow_down: ](https://arxiv.org/pdf/2206.01297.pdf)
>  We propose a new paradigm for encoding the geometry of point cloud sequences, where the convolutional neural network (CNN) which estimates the encoding distributions is optimized on several frames of the sequence to be compressed. We adopt lightweight CNN structures, we perform training as part of the encoding process, and the CNN parameters are transmitted as part of the bitstream. The newly proposed encoding scheme operates on the octree representation for each point cloud, encoding consecutively each octree resolution layer. At every octree resolution layer, the voxel grid is traversed section-by-section (each section being perpendicular to a selected coordinate axis) and in each section the occupancies of groups of two-by-two voxels are encoded at once, in a single arithmetic coding operation. A context for the conditional encoding distribution is defined for each two-by-two group of voxels, based on the information available about the occupancy of neighbor voxels in the current and lower resolution layers of the octree. The CNN estimates the probability distributions of occupancy patterns of all voxel groups from one section in four phases. In each new phase the contexts are updated with the occupancies encoded in the previous phase, and each phase estimates the probabilities in parallel, providing a reasonable trade-off between the parallelism of processing and the informativeness of the contexts. The CNN training time is comparable to the time spent in the remaining encoding steps, leading to competitive overall encoding times. Bitrates and encoding-decoding times compare favorably with those of recently published compression schemes.      
### 41.Multifocus microscopy with optically sectioned axial superresolution  [ :arrow_down: ](https://arxiv.org/pdf/2206.01257.pdf)
>  Multifocus microscopy enables recording of entire volumes in a single camera exposure. In dense samples, multifocus microscopy is severely hampered by background haze. Here, we introduce a scalable multifocus method that incorporates optical sectioning and offers axial superresolution capabilities. In our method, a dithered oblique light-sheet scans the sample volume during a single exposure, while generated fluorescence is linearised onto the camera with a multifocus optical element. A synchronised rolling shutter readout realised optical sectioning. We describe the technique theoretically and verify its optical sectioning and superresolution capabilities. We demonstrate a prototype system with a multifocus beam splitter cascade and record monolayers of endothelial cells at 35 volumes per second. We furthermore image uncleared engineered human heart tissue and visualise the distribution of mitochondria at axial superresolution. Our method manages to capture sub-diffraction sized mitochondria-derived vesicles up to 30 um deep into the tissue.      
### 42.Real-Time Portrait Stylization on the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2206.01244.pdf)
>  In this work we demonstrate real-time portrait stylization, specifically, translating self-portrait into cartoon or anime style on mobile devices. We propose a latency-driven differentiable architecture search method, maintaining realistic generative quality. With our framework, we obtain $10\times$ computation reduction on the generative model and achieve real-time video stylization on off-the-shelf smartphone using mobile GPUs.      
### 43.Constellation Shared Multiple Access -- A NOMA scheme for increased user capacity in 5G MMTC  [ :arrow_down: ](https://arxiv.org/pdf/2206.01228.pdf)
>  While the legacy cyclic prefix orthogonal frequency division multiple access is retained as the preferred multiple access scheme for 5G enhanced mobile broadband the research is now focussed on the multiple access schemes for massive machine type communication (mMTC) and ultra-reliable low latency communication .Though orthogonal multiple access schemes provide simple reception, they limit number of simultaneous user equipment as against the primary requirement of mMTC. On the other hand, the various non-orthogonal multiple access schemes which have been proposed so far as the likely solution, need complex successive interference cancellation receivers. So a simplified scheme named constellation shared multiple access is proposed here which substantially increases the number of simultaneous users to be served within a single resource block (RB) in LTE or 5G New Radio, thus aiding the massive connectivity requirement of mMTC. This is achieved by differentiating among the users in constellation domain. Moreover, the simple architecture compatible with 5G eMBB makes it a strong contender multiple access contender for 5G mMTC.      
