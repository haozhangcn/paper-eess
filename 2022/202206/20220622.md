# ArXiv eess --Wed, 22 Jun 2022
### 1.Controllability of Coarsely Measured Networked Linear Dynamical Systems (Extended Version)  [ :arrow_down: ](https://arxiv.org/pdf/2206.10569.pdf)
>  We consider the controllability of large-scale linear networked dynamical systems when complete knowledge of network structure is unavailable and knowledge is limited to coarse summaries. We provide conditions under which average controllability of the fine-scale system can be well approximated by average controllability of the (synthesized, reduced-order) coarse-scale system. To this end, we require knowledge of some inherent parametric structure of the fine-scale network that makes this type of approximation possible. Therefore, we assume that the underlying fine-scale network is generated by the stochastic block model (SBM) -- often studied in community detection. We then provide an algorithm that directly estimates the average controllability of the fine-scale system using a coarse summary of SBM. Our analysis indicates the necessity of underlying structure (e.g., in-built communities) to be able to quantify accurately the controllability from coarsely characterized networked dynamics. We also compare our method to that of the reduced-order method and highlight the regimes where both can outperform each other. Finally, we provide simulations to confirm our theoretical results for different scalings of network size and density, and the parameter that captures how much community-structure is retained in the coarse summary.      
### 2.Faster Diffusion Cardiac MRI with Deep Learning-based breath hold reduction  [ :arrow_down: ](https://arxiv.org/pdf/2206.10543.pdf)
>  Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) enables us to probe the microstructural arrangement of cardiomyocytes within the myocardium in vivo and non-invasively, which no other imaging modality allows. This innovative technology could revolutionise the ability to perform cardiac clinical diagnosis, risk stratification, prognosis and therapy follow-up. However, DT-CMR is currently inefficient with over six minutes needed to acquire a single 2D static image. Therefore, DT-CMR is currently confined to research but not used clinically. We propose to reduce the number of repetitions needed to produce DT-CMR datasets and subsequently de-noise them, decreasing the acquisition time by a linear factor while maintaining acceptable image quality. Our proposed approach, based on Generative Adversarial Networks, Vision Transformers, and Ensemble Learning, performs significantly and considerably better than previous proposed approaches, bringing single breath-hold DT-CMR closer to reality.      
### 3.Overcoming High Frequency Limitations of Current-Mode Control Using a Control Conditioning Approach -- Part II: Implementation and Hardware  [ :arrow_down: ](https://arxiv.org/pdf/2206.10523.pdf)
>  This article is the second part of a paper series about interference in extremum (i.e., peak or valley) current-mode control, which applies to both fixed and variable switching frequency power converters. Specifically, this part presents three control conditioning methods that mitigate the adverse effect of interference. These methods are new ways to use: (i) slope compensation; (ii) low-pass filtering; and (iii) the phenomenon of comparator-overdrive-delay, for control conditioning. The stability criterion, closed-loop dynamics, and transient performance are derived with mathematical rigor for each method. The design tradeoffs are illustrated, discussed, and compared. The effectiveness of all three methods are demonstrated and validated in hardware using a power converter operating at multi-MHz switching frequencies.      
### 4.Overcoming High Frequency Limitations of Current-Mode Control Using a Control Conditioning Approach -- Part I: Modeling and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2206.10518.pdf)
>  Current-mode control is one of the most popular controller strategies for power converters. With the advent of wide bandgap devices including GaN and SiC, higher switching frequencies have become more viable at higher power because of lower switching losses. However, the advantage of higher switching frequency for faster, higher bandwidth control is squandered because of current sensor interference. We present a framework for characterizing and analyzing this interference as uncertainties to the controller model. These uncertainties introduce additional dynamics and nonlinearity that can result in instability and poor transient performance of the current control loop. In this paper, we provide a model framework based on a new control conditioning approach that guarantees global stability and a strategy for optimizing transient performance. In Part II of this paper series, we present the analysis, design, and hardware validation of three effective solutions.      
### 5.A Learning Aided Gradient Descent for MISO Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2206.10499.pdf)
>  This paper proposes a learning aided gradient descent (LAGD) algorithm to solve the weighted sum rate (WSR) maximization problem for multiple-input single-output (MISO) beamforming. The proposed LAGD algorithm directly optimizes the transmit precoder through implicit gradient descent based iterations, at each of which the optimization strategy is determined by a neural network, and thus, is dynamic and adaptive. At each instance of the problem, this network is initialized randomly, and updated throughout the iterative solution process. Therefore, the LAGD algorithm can be implemented at any signal-to-noise ratio (SNR) and for arbitrary antenna/user numbers, does not require labelled data or training prior to deployment. Numerical results show that the LAGD algorithm can outperform of the well-known WMMSE algorithm as well as other learning-based solutions with a modest computational complexity. Our code is available at <a class="link-external link-https" href="https://github.com/XiaGroup/LAGD" rel="external noopener nofollow">this https URL</a>.      
### 6.Automated Coronary Calcium Scoring using U-Net Models through Semi-supervised Learning on Non-Gated CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2206.10455.pdf)
>  Every year, thousands of innocent people die due to heart attacks. Often undiagnosed heart attacks can hit people by surprise since many current medical plans don't cover the costs to require the searching of calcification on these scans. Only if someone is suspected to have a heart problem, a gated CT scan is taken, otherwise, there's no way for the patient to be aware of a possible heart attack/disease. While nongated CT scans are more periodically taken, it is harder to detect calcification and is usually taken for a purpose other than locating calcification in arteries. In fact, in real time coronary artery calcification scores are only calculated on gated CT scans, not nongated CT scans. After training a unet model on the Coronary Calcium and chest CT's gated scans, it received a DICE coefficient of 0.95 on its untouched test set. This model was used to predict on nongated CT scans, performing with a mean absolute error (MAE) of 674.19 and bucket classification accuracy of 41% (5 classes). Through the analysis of the images and the information stored in the images, mathematical equations were derived and used to automatically crop the images around the location of the heart. By performing semi-supervised learning the new cropped nongated scans were able to closely resemble gated CT scans, improving the performance by 91% in MAE (62.38) and 23% in accuracy.      
### 7.An Augmented Lagrangian Based Parallelizable Nonconvex Solver for Bilinear Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.10425.pdf)
>  Nonlinear model predictive control is widely adopted to manipulate bilinear systems, and bilinear models are ubiquitous in chemical process, mechanical system and quantum physics, to name a few. Running an MPC controller in real-time requires solving a non-convex optimization problem at step. In this work, we propose a novel parallel augmented Lagrangian based bilinear MPC solver via a novel horizon splitting scheme. The resulting algorithm converts the non-convex MPC control problem into a set parallelizable multi-parametric quadratic programming (mpQP) and an equality constrained QP problem. The mpQP solution can be pre-computed offline to enable efficient online compuation. The proposed algorithm is validated on a building simulation and is deployed on a TI C2000 LaunchPad to emulate the bilinear DC motor speed control.      
### 8.Approximate Equivariance SO(3) Needlet Convolution  [ :arrow_down: ](https://arxiv.org/pdf/2206.10385.pdf)
>  This paper develops a rotation-invariant needlet convolution for rotation group SO(3) to distill multiscale information of spherical signals. The spherical needlet transform is generalized from $\mathbb{S}^2$ onto the SO(3) group, which decomposes a spherical signal to approximate and detailed spectral coefficients by a set of tight framelet operators. The spherical signal during the decomposition and reconstruction achieves rotation invariance. Based on needlet transforms, we form a Needlet approximate Equivariance Spherical CNN (NES) with multiple SO(3) needlet convolutional layers. The network establishes a powerful tool to extract geometric-invariant features of spherical signals. The model allows sufficient network scalability with multi-resolution representation. A robust signal embedding is learned with wavelet shrinkage activation function, which filters out redundant high-pass representation while maintaining approximate rotation invariance. The NES achieves state-of-the-art performance for quantum chemistry regression and Cosmic Microwave Background (CMB) delensing reconstruction, which shows great potential for solving scientific challenges with high-resolution and multi-scale spherical signal representation.      
### 9.Confidence-Guided Unsupervised Domain Adaptation for Cerebellum Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.10357.pdf)
>  The lack of a comprehensive high-resolution atlas of the cerebellum has hampered studies of cerebellar involvement in normal brain function and disease. A good representation of the tightly foliated aspect of the cerebellar cortex is difficult to achieve because of the highly convoluted surface and the time it would take for manual delineation. The quality of manual segmentation is influenced by human expert judgment, and automatic labelling is constrained by the limited robustness of existing segmentation algorithms. The 20umisotropic BigBrain dataset provides an unprecedented high resolution framework for semantic segmentation compared to the 1000um(1mm) resolution afforded by magnetic resonance imaging. To dispense with the manual annotation requirement, we propose to train a model to adaptively transfer the annotation from the cerebellum on the Allen Brain Human Brain Atlas to the BigBrain in an unsupervised manner, taking into account the different staining and spacing between sections. The distinct visual discrepancy between the Allen Brain and BigBrain prevents existing approaches to provide meaningful segmentation masks, and artifacts caused by sectioning and histological slice preparation in the BigBrain data pose an extra challenge. To address these problems, we propose a two-stage framework where we first transfer the Allen Brain cerebellum to a space sharing visual similarity with the BigBrain. We then introduce a self-training strategy with a confidence map to guide the model learning from the noisy pseudo labels iteratively. Qualitative results validate the effectiveness of our approach, and quantitative experiments reveal that our method can achieve over 2.6% loss reduction compared with other approaches.      
### 10.Using the Polar Transform for Efficient Deep Learning-Based Aorta Segmentation in CTA Images  [ :arrow_down: ](https://arxiv.org/pdf/2206.10294.pdf)
>  Medical image segmentation often requires segmenting multiple elliptical objects on a single image. This includes, among other tasks, segmenting vessels such as the aorta in axial CTA slices. In this paper, we present a general approach to improving the semantic segmentation performance of neural networks in these tasks and validate our approach on the task of aorta segmentation. We use a cascade of two neural networks, where one performs a rough segmentation based on the U-Net architecture and the other performs the final segmentation on polar image transformations of the input. Connected component analysis of the rough segmentation is used to construct the polar transformations, and predictions on multiple transformations of the same image are fused using hysteresis thresholding. We show that this method improves aorta segmentation performance without requiring complex neural network architectures. In addition, we show that our approach improves robustness and pixel-level recall while achieving segmentation performance in line with the state of the art.      
### 11.Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.10286.pdf)
>  The morphological changes in knee cartilage (especially femoral and tibial cartilages) are closely related to the progression of knee osteoarthritis, which is expressed by magnetic resonance (MR) images and assessed on the cartilage segmentation results. Thus, it is necessary to propose an effective automatic cartilage segmentation model for longitudinal research on osteoarthritis. In this research, to relieve the problem of inaccurate discontinuous segmentation caused by the limited receptive field in convolutional neural networks, we proposed a novel position-prior clustering-based self-attention module (PCAM). In PCAM, long-range dependency between each class center and feature point is captured by self-attention allowing contextual information re-allocated to strengthen the relative features and ensure the continuity of segmentation result. The clutsering-based method is used to estimate class centers, which fosters intra-class consistency and further improves the accuracy of segmentation results. The position-prior excludes the false positives from side-output and makes center estimation more precise. Sufficient experiments are conducted on OAI-ZIB dataset. The experimental results show that the segmentation performance of combination of segmentation network and PCAM obtains an evident improvement compared to original model, which proves the potential application of PCAM in medical segmentation tasks. The source code is publicly available from link: <a class="link-external link-https" href="https://github.com/LeongDong/PCAMNet" rel="external noopener nofollow">this https URL</a>      
### 12.Analog Self-Interference Cancellation with Practical RF Components for Full-Duplex Radios  [ :arrow_down: ](https://arxiv.org/pdf/2206.10284.pdf)
>  One of the main obstacles in full-duplex radios is analog-to-digital converter (ADC) saturation on a receiver due to the strong self-interference (SI). To solve this issue, researchers have proposed two different types of analog self-interference cancellation (SIC) methods -- i) passive suppression and ii) regeneration-and-subtraction of SI. For the latter case, the tunable RF component, such as a multi-tap circuit, reproduces and subtracts the SI. The resolutions of such RF components constitute the key factor of the analog SIC. Indeed, they are directly related to how well the SI is imitated. Another major issue in analog SIC is the inaccurate estimation of the SI channel due to the nonlinear distortions, which mainly come from the power amplifier (PA). In this paper, we derive a closed-form expression for the SIC performance of the multi-tap circuit; we consider how the RF components must overcome such practical impairments as digitally-controlled attenuators, phase shifters, and PA. For a realistic performance analysis, we exploit the measured PA characteristics and carry out a 3D ray-tracing-based, system-level throughput analysis. Our results confirm that the non-idealities of the RF components significantly affect the analog SIC performance. We believe our study provides insight into the design of the practical full-duplex system.      
### 13.Steady-state nonlinearity of open-loop reset systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.10275.pdf)
>  In this paper, we introduce a new representation for open-loop reset systems. We show that at steady-state a reset integrator can be modelled as a parallel interconnection of the base-linear system and piece-wise constant nonlinearity. For sinusoidal input signals, this nonlinearity takes a form of a square wave. Subsequently, we show how the behaviour of a general open-loop reset system is related to the nonlinearity of a reset integrator. The proposed approach simplifies the analysis of reset elements in the frequency domain and provides new insights into the behaviour of reset control systems.      
### 14.GNN-PMB: A Simple but Effective Online 3D Multi-Object Tracker without Bells and Whistles  [ :arrow_down: ](https://arxiv.org/pdf/2206.10255.pdf)
>  Multi-object tracking (MOT) is among crucial applications in modern advanced driver assistance systems (ADAS) and autonomous driving (AD) systems. Most solutions to MOT are based on random vector Bayesian filters like global nearest neighbor (GNN) plus rule-based heuristical track maintenance. With the development of random finite set (RFS) theory, the RFS Bayesian filters have been applied in MOT tasks for ADAS and AD systems recently. However, their usefulness in the real traffic is open to doubt due to computational cost and implementation complexity. In this paper, it is revealed that GNN with rule-based heuristic track maintenance is insufficient for LiDAR-based MOT tasks in ADAS and AD systems. This judgement is illustrated by systematically comparing several different multi-point object filter-based tracking frameworks, including traditional random vector Bayesian filters with rule-based heuristical track maintenance and RFS Bayesian filters. Moreover, a simple and effective tracker, namely Poisson multi-Bernoulli filter using global nearest neighbor (GNN-PMB) tracker, is proposed for LiDAR-based MOT tasks. The proposed GNN-PMB tracker achieves competitive results in nuScenes test dataset, and shows superior tracking performance over other state-of-the-art LiDAR only trackers and LiDAR and camera fusion-based trackers.      
### 15.Digital twin of a MWh-scale grid battery system for efficiency and degradation analysis  [ :arrow_down: ](https://arxiv.org/pdf/2206.10245.pdf)
>  Large-scale grid-connected lithium-ion batteries are increasingly being deployed to support renewable energy roll-out on the power grid. These battery systems consist of thousands of individual cells and various ancillary systems for monitoring and control. Although many studies have focused on the behaviour of single lithium-ion cells, the impact of system design choices and ancillary system controls on long-term degradation and efficiency of system, containing thousands of cells, has rarely been considered in detail. Here, we simulate a 1 MWh grid battery system consisting of 18900 individual cells, each represented by a separate electrochemical model, as well as the thermal management system and power electronic converters. Simulations of the impact of cell-to-cell variability, thermal effects, and degradation effects were run for up to 10000 cycles and 10 years. It is shown that electrical contact resistances and cell-to-cell variations in initial capacity and resistance have a smaller effect on performance than previously thought. Instead, the variation in degradation rate of individual cells dominates the system behaviour over the lifetime. The importance of careful thermal management system control is demonstrated, with proportional control improving overall efficiency by 5 %-pts over on-off methods, also increasing the total usable energy of the battery by 5 %-pts after 10 years.      
### 16.Propagation of Measurement and Model Uncertainties through Multiline TRL Calibration  [ :arrow_down: ](https://arxiv.org/pdf/2206.10209.pdf)
>  In this work, we present a linear uncertainty (LU) propagation treatment of measurement and model uncertainties in multiline thru-reflect-line (TRL) calibration. The proposed method is in accordance with the ISO Guide to the Expression of Uncertainty in Measurement (GUM). We demonstrate with numerical simulation based on the Monte Carlo (MC) method that our proposed LU method delivers identical uncertainty as the MC method but in a more efficient way.      
### 17.covEcho Resource constrained lung ultrasound image analysis tool for faster triaging and active learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.10183.pdf)
>  Lung ultrasound (LUS) is possibly the only medical imaging modality which could be used for continuous and periodic monitoring of the lung. This is extremely useful in tracking the lung manifestations either during the onset of lung infection or to track the effect of vaccination on lung as in pandemics such as COVID-19. There have been many attempts in automating the classification of severity of lung into various classes or automatic segmentation of various LUS landmarks and manifestations. However, all these approaches are based on training static machine learning models which require a significantly clinically annotated large dataset and are computationally heavy and most of the time non-real time. In this work, a real-time light weight active learning-based approach is presented for faster triaging in COVID-19 subjects in resource constrained settings. The tool, based on the you look only once (YOLO) network, has the capability of providing the quality of images based on the identification of various LUS landmarks, artefacts and manifestations, prediction of severity of lung infection, possibility of active learning based on the feedback from clinicians or on the image quality and a summarization of the significant frames which are having high severity of infection and high image quality for further analysis. The results show that the proposed tool has a mean average precision (mAP) of 66% at an Intersection over Union (IoU) threshold of 0.5 for the prediction of LUS landmarks. The 14MB lightweight YOLOv5s network achieves 123 FPS while running in a Quadro P4000 GPU. The tool is available for usage and analysis upon request from the authors.      
### 18.Fast image reverse filters through fixed point and gradient descent acceleration  [ :arrow_down: ](https://arxiv.org/pdf/2206.10124.pdf)
>  In this paper, we study the problem of reverse image filtering. An image filter denoted g(.), which is available as a black box, produces an observation b = g(x) when provided with an input x. The problem is to estimate the original input signal x from the black box filter g(.) and the observation b. We study and re-develop state-of-the-art methods from two points of view, fixed point iteration and gradient descent. We also explore the application of acceleration techniques for the two types of iterations. Through extensive experiments and comparison, we show that acceleration methods for both fixed point iteration and gradient descent help to speed up the convergence of state-of-the-art methods.      
### 19.DECAL: DEployable Clinical Active Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.10120.pdf)
>  Conventional machine learning systems that operate on natural images assume the presence of attributes within the images that lead to some decision. However, decisions in medical domain are a resultant of attributes within medical diagnostic scans and electronic medical records (EMR). Hence, active learning techniques that are developed for natural images are insufficient for handling medical data. We focus on reducing this insufficiency by designing a deployable clinical active learning (DECAL) framework within a bi-modal interface so as to add practicality to the paradigm. Our approach is a "plug-in" method that makes natural image based active learning algorithms generalize better and faster. We find that on two medical datasets on three architectures and five learning strategies, DECAL increases generalization across 20 rounds by approximately 4.81%. DECAL leads to a 5.59% and 7.02% increase in average accuracy as an initialization strategy for optical coherence tomography (OCT) and X-Ray respectively. Our active learning results were achieved using 3000 (5%) and 2000 (38%) samples of OCT and X-Ray data respectively.      
### 20.Optimization simulation of reflow welding based on prediction of regional center temperature field  [ :arrow_down: ](https://arxiv.org/pdf/2206.10119.pdf)
>  Before reflow soldering of integrated electronic products, the numerical simulation of temperature control curve of reflow furnace is crucial for selecting proper parameters and improving the overall efficiency of reflow soldering process and product quality. According to the heat conduction law and the specific heat capacity formula, the first-order ordinary differential equation of the central temperature curve of the welding area with respect to the temperature distribution function in the furnace on the conveyor belt displacement is obtained. For the gap with small temperature difference, the sigmoid function is used to obtain a smooth interval temperature transition curve; For the gap with large temperature difference, the linear combination of exponential function and primary function is used to approach the actual concave function, so as to obtain the complete temperature distribution function in the furnace. The welding parameters are obtained by solving the ordinary differential equation, and a set of optimal process parameters consistent with the process boundary are obtained by calculating the mean square error between the predicted temperature field and the real temperature distribution. At the same time, a set of reflow optimization strategies are designed for speed interval prediction strategy, minimum parameter interval prediction strategy, and the most symmetrical parameter interval prediction of solder paste melting reflow area. The simulation results show that the temperature field prediction results obtained by this method are highly consistent with the actual sensor data, and have strong correlation. This method can help to select appropriate process parameters, optimize the production process, reduce equipment commissioning practice and optimize the solder joint quality of production products.      
### 21.Electrochemical Parameter Identification for Lithium-ion Batteries on Separated Time-scales  [ :arrow_down: ](https://arxiv.org/pdf/2206.10099.pdf)
>  Lithium-ion batteries (LIBs) play an essential role in the energy sector and have been widely deployed in recent years. Generally, LIBs are managed in model-driven manners, leading to the need for parameter identification. However, as an electrochemical system, the battery contains various parameters while the measurements are mainly the current and voltage, inducing an inherent ill-conditioned identification problem. A flexible and lightweight parameter identification framework, including the test, model, and algorithm, is proposed in this work. Electrochemical parameters are grouped by time-variant features and identified on separated time-scales. Parameters with slow dynamics are identified in a hybrid data-driven and model-driven approach based on the data of a quasi-static test covering the partial SOC range. Parameters with fast dynamics are identified using a specifically designed sensitivity-oriented stepwise optimization algorithm based on the data of a dynamic test consisting of a series of constant current (CC) pulses. The mixed impacts of different parameters can be decoupled and the time costs of the test and computation can be reduced. Specifically, it takes a few hours to identify slow dynamic parameters and a few minutes to identify fast dynamic parameters. Numerical experiments on a typical \ce{LiNCM} battery at different life stages are conducted. The results show that the identification accuracy of crucial parameters can reach over 95\%, and the battery model error is reduced below 2$\times$10$^{-3}$V.      
### 22.Memory-Efficient Learned Image Compression with Pruned Hyperprior Module  [ :arrow_down: ](https://arxiv.org/pdf/2206.10083.pdf)
>  Learned Image Compression (LIC) gradually became more and more famous in these years. The hyperprior-module-based LIC models have achieved remarkable rate-distortion performance. However, the memory cost of these LIC models is too large to actually apply them to various devices, especially to portable or edge devices. The parameter scale is directly linked with memory cost. In our research, we found the hyperprior module is not only highly over-parameterized, but also its latent representation contains redundant information. Therefore, we propose a novel pruning method named ERHP in this paper to efficiently reduce the memory cost of hyperprior module, while improving the network performance. The experiments show our method is effective, reducing at least 22.6% parameters in the whole model while achieving better rate-distortion performance.      
### 23.Assessment of the Center of Inertia and Regional Inertia with Load Contribution via a Fully Data-Driven Method  [ :arrow_down: ](https://arxiv.org/pdf/2206.10036.pdf)
>  This paper proposes a new comprehensive and fully data-driven methodology to estimate the center of inertia (COI) and the regional inertia, considering the displacement of the COI due to disturbances and load inertial contributions. The strategy uses the typicality-based data analysis (TDA) technique to detect the right pilot-bus that represents the COI. In the TDA, a compound of correlation and cosine similarities is implemented to approximate the actual distribution of the data and find the point (bus) closest to the mean which is elected as the pilot-bus. Then, the frequency response at the pilot-bus and the active power deviations are embedded into an autoregressive moving average exogenous input (ARMAX)-based approach to determine the regional inertia represented by an equivalent machine, whose inertia constant corresponds to the inertial contribution in the Region. % the This is done in order to perform the inertial estimation of the region using the frequency response of the pilot-bus and the active power of the region interconnections to model the region as and equivalent machine whose inertial constant is estimated using ARMAX identification technique. The methodology is tested using the IEEE 68-bus benchmark test system and an adapted version with aggregated dynamical loads, corroborating the method effectiveness.      
### 24.An interval-valued recursive estimation framework for linearly parameterized systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.10015.pdf)
>  This paper proposes a recursive interval-valued estimation framework for identifying the parameters of linearly parameterized systems which may be slowly time-varying. It is assumed that the model error (which may consist in measurement noise or model mismatch or both) is unknown but lies at each time instant in a known interval. In this context, the proposed method relies on bounding the error generated by a given reference point-valued recursive estimator, for example, the well-known recursive least squares algorithm. We discuss the trade-off between computational complexity and tightness of the estimated parametric interval.      
### 25.Model-Free Optimal Control of Inverter for Dynamic Voltage Support  [ :arrow_down: ](https://arxiv.org/pdf/2206.09962.pdf)
>  Inverter-based resources (IBRs) are required to provide dynamic voltage support (DVS) during voltage dips to enhance the low-voltage ride-through capability. In this paper, we develop a model-free control method to achieve the optimal DVS (ODVS) without relying on the knowledge of grid parameters. Delving into the optimum trajectory of the ODVS problem, it is found that either the current constraint and the maximum active power constraint of IBRs are binding or one of the constraints is binding. This inspires us to search for the optimum in a closed-loop way by a perturb-and-observe (P&amp;O)-based optimum seeking (OS) controller with either the power factor angle or the reactive current being the manipulated (perturbed) variable. The system is guaranteed to converge asymptotically to the optimum provided the stepsize sequence is diminishing and non-summable. The proposed model-free optimal control is finally implemented within a single-stage photovoltaic (PV) system, where dynamic simulations demonstrate the optimal and fast DVS performance      
### 26.Sparse Representations of Dynamical Networks: A Coprime Factorization Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.09945.pdf)
>  We study a general class of dynamical networks modeled by linear and time-invariant systems, described by state-space realizations. For these networks, we investigate the relations between various types of factorizations which preserve the structure of their component subsystems' interconnection. In doing so, we provide tractable means of shifting between different types of sparsity-preserving representations and we show how to employ these factorizations to obtain distributed implementations for stabilizing and possibly stable controllers. By formulating all the aforementioned results for both discrete- and continuous-time systems, we develop specialized distributed implementations that, up to this point, were only available for networks modeled as discrete-time systems.      
### 27.WiFi-based Spatiotemporal Human Action Perception  [ :arrow_down: ](https://arxiv.org/pdf/2206.09867.pdf)
>  WiFi-based sensing for human activity recognition (HAR) has recently become a hot topic as it brings great benefits when compared with video-based HAR, such as eliminating the demands of line-of-sight (LOS) and preserving privacy. Making the WiFi signals to 'see' the action, however, is quite coarse and thus still in its infancy. An end-to-end spatiotemporal WiFi signal neural network (STWNN) is proposed to enable WiFi-only sensing in both line-of-sight and non-line-of-sight scenarios. Especially, the 3D convolution module is able to explore the spatiotemporal continuity of WiFi signals, and the feature self-attention module can explicitly maintain dominant features. In addition, a novel 3D representation for WiFi signals is designed to preserve multi-scale spatiotemporal information. Furthermore, a small wireless-vision dataset (WVAR) is synchronously collected to extend the potential of STWNN to 'see' through occlusions. Quantitative and qualitative results on WVAR and the other three public benchmark datasets demonstrate the effectiveness of our approach on both accuracy and shift consistency.      
### 28.Work-loop techniques for optimising nonlinear forced oscillators  [ :arrow_down: ](https://arxiv.org/pdf/2206.09859.pdf)
>  Linear and nonlinear resonant states can be restrictive: they exist at particular discrete states in frequency and/or elasticity, under particular (e.g., simple-harmonic) waveforms. In forced oscillators, this restrictiveness is an obstacle to system design and control modulation: altering the system elasticity, or modulating the response, would both appear to necessarily incur a penalty to efficiency. In this work, we describe an approach for bypassing this obstacle. Using novel work-loop techniques, we prove and illustrate how certain classes of resonant optimisation problem lead to non-unique solutions. In a structural optimisation context, several categories of energetically-optimal elasticity are non-unique. In an optimal control context, several categories of energetically-optimal frequency are non-unique. For these classes of non-unique optimum, we can derive simple bounds defining the optimal region. These novel theoretical results have practical implications for the design and control of a range of biomimetic propulsion systems, including flapping-wing micro-air-vehicles: using these results, we can generate efficient forms of wingbeat modulation for flight control.      
### 29.On the benefit of parameter-driven approaches for the modeling and the prediction of Satisfied User Ratio for compressed video  [ :arrow_down: ](https://arxiv.org/pdf/2206.09854.pdf)
>  The human eye cannot perceive small pixel changes in images or videos until a certain threshold of distortion. In the context of video compression, Just Noticeable Difference (JND) is the smallest distortion level from which the human eye can perceive the difference between reference video and the distorted/compressed one. Satisfied-User-Ratio (SUR) curve is the complementary cumulative distribution function of the individual JNDs of a viewer group. However, most of the previous works predict each point in SUR curve by using features both from source video and from compressed videos with assumption that the group-based JND annotations follow Gaussian distribution, which is neither practical nor accurate. In this work, we firstly compared various common functions for SUR curve modeling. Afterwards, we proposed a novel parameter-driven method to predict the video-wise SUR from video features. Besides, we compared the prediction results of source-only features based (SRC-based) models and source plus compressed videos features (SRC+PVS-based) models.      
### 30.On a Scalable Path for Multimode MIMO-DSP  [ :arrow_down: ](https://arxiv.org/pdf/2206.09846.pdf)
>  A novel MIMO-DSP for space-division multiplexing over multimode fibres is proposed. A principal modes approach is shown to provide two-fold benefits: over 13 times channel memory reduction while minimising the number of optical front-ends needed to detect a subset of the spatial domain.      
### 31.Deep representation of EEG data from Spatio-Spectral Feature Images  [ :arrow_down: ](https://arxiv.org/pdf/2206.09807.pdf)
>  Unlike conventional data such as natural images, audio and speech, raw multi-channel Electroencephalogram (EEG) data are difficult to interpret. Modern deep neural networks have shown promising results in EEG studies, however finding robust invariant representations of EEG data across subjects remains a challenge, due to differences in brain folding structures. Thus, invariant representations of EEG data would be desirable to improve our understanding of the brain activity and to use them effectively during transfer learning. In this paper, we propose an approach to learn deep representations of EEG data by exploiting spatial relationships between recording electrodes and encoding them in a Spatio-Spectral Feature Images. We use multi-channel EEG signals from the PhyAAt dataset for auditory tasks and train a Convolutional Neural Network (CNN) on 25 subjects individually. Afterwards, we generate the input patterns that activate deep neurons across all the subjects. The generated pattern can be seen as a map of the brain activity in different spatial regions. Our analysis reveals the existence of specific brain regions related to different tasks. Low-level features focusing on larger regions and high-level features focusing on a smaller and very specific cluster of regions are also identified. Interestingly, similar patterns are found across different subjects although the activities appear in different regions. Our analysis also reveals common brain regions across subjects, which can be used as generalized representations. Our proposed approach allows us to find more interpretable representations of EEG data, which can further be used for effective transfer learning.      
### 32.Consensus ADMM-Based Distributed Simultaneous Imaging &amp; Communication  [ :arrow_down: ](https://arxiv.org/pdf/2206.09793.pdf)
>  This paper takes the first steps toward enabling wireless networks to perform both imaging and communication in a distributed manner. We propose Distributed Simultaneous Imaging and Symbol Detection (DSISD), a provably convergent distributed simultaneous imaging and communication scheme based on the alternating direction method of multipliers. We show that DSISD achieves similar imaging and communication performance as centralized schemes, with order-wise reduction in computational complexity. We evaluate the performance of DSISD via 2.4 GHz Wi-Fi simulations.      
### 33.Boosting Cross-Domain Speech Recognition with Self-Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2206.09783.pdf)
>  The cross-domain performance of automatic speech recognition (ASR) could be severely hampered due to the mismatch between training and testing distributions. Since the target domain usually lacks labeled data, and domain shifts exist at acoustic and linguistic levels, it is challenging to perform unsupervised domain adaptation (UDA) for ASR. Previous work has shown that self-supervised learning (SSL) or pseudo-labeling (PL) is effective in UDA by exploiting the self-supervisions of unlabeled data. However, these self-supervisions also face performance degradation in mismatched domain distributions, which previous work fails to address. This work presents a systematic UDA framework to fully utilize the unlabeled data with self-supervision in the pre-training and fine-tuning paradigm. On the one hand, we apply continued pre-training and data replay techniques to mitigate the domain mismatch of the SSL pre-trained model. On the other hand, we propose a domain-adaptive fine-tuning approach based on the PL technique with three unique modifications: Firstly, we design a dual-branch PL method to decrease the sensitivity to the erroneous pseudo-labels; Secondly, we devise an uncertainty-aware confidence filtering strategy to improve pseudo-label correctness; Thirdly, we introduce a two-step PL approach to incorporate target domain linguistic knowledge, thus generating more accurate target domain pseudo-labels. Experimental results on various cross-domain scenarios demonstrate that the proposed approach could effectively boost the cross-domain performance and significantly outperform previous approaches.      
### 34.Improving Triplet-Based Channel Charting on Distributed Massive MIMO Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2206.09774.pdf)
>  The objective of channel charting is to learn a virtual map of the radio environment from high-dimensional CSI that is acquired by a multi-antenna wireless system. Since, in static environments, CSI is a function of the transmitter location, a mapping from CSI to channel chart coordinates can be learned in a self-supervised manner using dimensionality reduction techniques. The state-of-the-art triplet-based approach is evaluated on multiple datasets measured by a distributed massive MIMO channel sounder, with both co-located and distributed antenna setups. The importance of suitable triplet selection is investigated by comparing results to channel charts learned from a genie-aided triplet generator and learned from triplets on simulated trajectories through measured data. Finally, the transferability of learned forward charting functions to similar, but different radio environments is explored.      
### 35.Quantitative CT texture-based method to predict diagnosis and prognosis of fibrosing interstitial lung disease patterns  [ :arrow_down: ](https://arxiv.org/pdf/2206.09766.pdf)
>  Purpose: To utilize high-resolution quantitative CT (QCT) imaging features for prediction of diagnosis and prognosis in fibrosing interstitial lung diseases (ILD). Approach: 40 ILD patients (20 usual interstitial pneumonia (UIP), 20 non-UIP pattern ILD) were classified by expert consensus of 2 radiologists and followed for 7 years. Clinical variables were recorded. Following segmentation of the lung field, a total of 26 texture features were extracted using a lattice-based approach (TM model). The TM model was compared with previously histogram-based model (HM) for their abilities to classify UIP vs non-UIP. For prognostic assessment, survival analysis was performed comparing the expert diagnostic labels versus TM metrics. Results: In the classification analysis, the TM model outperformed the HM method with AUC of 0.70. While survival curves of UIP vs non-UIP expert labels in Cox regression analysis were not statistically different, TM QCT features allowed statistically significant partition of the cohort. Conclusions: TM model outperformed HM model in distinguishing UIP from non-UIP patterns. Most importantly, TM allows for partitioning of the cohort into distinct survival groups, whereas expert UIP vs non-UIP labeling does not. QCT TM models may improve diagnosis of ILD and offer more accurate prognostication, better guiding patient management.      
### 36.Multiband Delay Estimation for Localization Using a Two-Stage Global Estimation Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2206.09751.pdf)
>  The time of arrival (TOA)-based localization techniques, which need to estimate the delay of the line-of-sight (LoS) path, have been widely employed in location-aware networks. To achieve a high-accuracy delay estimation, a number of multiband-based algorithms have been proposed recently, which exploit the channel state information (CSI) measurements over multiple non-contiguous frequency bands. However, to the best of our knowledge, there still lacks an efficient scheme that fully exploits the multiband gains when the phase distortion factors caused by hardware imperfections are considered, due to that the associated multi-parameter estimation problem contains many local optimums and the existing algorithms can easily get stuck in a "bad" local optimum. To address these issues, we propose a novel two-stage global estimation (TSGE) scheme for multiband delay estimation. In the coarse stage, we exploit the group sparsity structure of the multiband channel and propose a Turbo Bayesian inference (Turbo-BI) algorithm to achieve a good initial delay estimation based on a coarse signal model, which is transformed from the original multiband signal model by absorbing the carrier frequency terms. The estimation problem derived from the coarse signal model contains less local optimums and thus a more stable estimation can be achieved than directly using the original signal model. Then in the refined stage, with the help of coarse estimation results to narrow down the search range, we perform a global delay estimation using a particle swarm optimization-least square (PSO-LS) algorithm based on a refined multiband signal model to exploit the multiband gains to further improve the estimation accuracy. Simulation results show that the proposed TSGE significantly outperforms the benchmarks with comparative computational complexity.      
### 37.Efficient Joint DOA and TOA Estimation for Indoor Positioning with 5G Picocell Base Stations  [ :arrow_down: ](https://arxiv.org/pdf/2206.09748.pdf)
>  The ubiquity, large bandwidth, and spatial diversity of the fifth generation (5G) cellular signal render it a promising candidate for accurate positioning in indoor environments where the global navigation satellite system (GNSS) signal is absent. In this paper, a joint angle and delay estimation (JADE) scheme is designed for 5G picocell base stations (gNBs) which addresses two crucial issues to make it both effective and efficient in realistic indoor environments. Firstly, the direction-dependence of the array modeling error for picocell gNB as well as its impact on JADE is revealed. This error is mitigated by fitting the array response measurements to a vector-valued function and pre-calibrating the ideal steering-vector with the fitted function. Secondly, based on the deployment reality that 5G picocell gNBs only have a small-scale antenna array but have a large signal bandwidth, the proposed scheme decouples the estimation of time-of-arrival (TOA) and direction-of-arrival (DOA) to reduce the huge complexity induced by two-dimensional joint processing. It employs the iterative-adaptive-approach (IAA) to resolve multipath signals in the TOA domain, followed by a conventional beamformer (CBF) to retrieve the desired line-of-sight DOA. By further exploiting a dimension-reducing pre-processing module and accelerating spectrum computing by fast Fourier transforms, an efficient implementation is achieved for real-time JADE. Numerical simulations demonstrate the superiority of the proposed method in terms of DOA estimation accuracy. Field tests show that a triangulation positioning error of 0.44 m is achieved for 90% cases using only DOAs estimated at two separated receiving points.      
### 38.Data Fusion for Radio Frequency SLAM with Robust Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2206.09746.pdf)
>  Precise indoor localization remains a challenging problem for a variety of essential applications. A promising approach to address this problem is to exchange radio signals between mobile agents and static physical anchors (PAs) that bounce off flat surfaces in the indoor environment. Radio frequency simultaneous localization and mapping (RF-SLAM) methods can be used to jointly estimates the time-varying location of agents as well as the static locations of the flat surfaces. Recent work on RF-SLAM methods has shown that each surface can be efficiently represented by a single master virtual anchor (MVA). The measurement model related to this MVA-based RF-SLAM method is highly nonlinear. Thus, Bayesian estimation relies on sampling-based techniques. The original MVA-based RF-SLAM method employs conventional "bootstrap" sampling. In challenging scenarios it was observed that the original method might converge to incorrect MVA positions corresponding to local maxima. In this paper, we introduce MVA-based RF-SLAM with an improved sampling technique that succeeds in the aforementioned challenging scenarios. Our simulation results demonstrate significant performance advantages.      
### 39.A Safe Control Architecture Based on Robust Model Predictive Control for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2206.09735.pdf)
>  This paper proposes a Robust Safe Control Architecture (RSCA) for safe-decision making. The system to be controlled is a vehicle in the presence of bounded disturbances. The RSCA consists of two parts: a Supervisor MPC and a Controller MPC. Both the Supervisor and the Controller are tube MPCs (TMPCs). The Supervisor MPC provides a safety certificate for an operating controller and a backup control input in every step. After an unsafe action by the operating controller is predicted, the Controller MPC takes over the system. In this paper, a method for the computation of a terminal set is proposed, which is robust against changes in road curvature and forces the vehicle to reach a safe reference. Moreover, two important proofs are provided in this paper. First, it is shown that the backup control input is safe to be applied to the system to lead the vehicle to a safe state. Next, the recursive feasibility of the RSCA is proven. By simulating some obstacle avoidance scenarios, the effectiveness of the proposed RSCA is confirmed.      
### 40.Multi-channel end-to-end neural network for speech enhancement, source localization, and voice activity detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.09728.pdf)
>  Speech enhancement and source localization has been active research for several decades with a wide range of real-world applications. Recently, the Deep Complex Convolution Recurrent network (DCCRN) has yielded impressive enhancement performance for single-channel systems. In this study, a neural beamformer consisting of a beamformer and a novel multi-channel DCCRN is proposed for speech enhancement and source localization. Complex-valued filters estimated by the multi-channel DCCRN serve as the weights of beamformer. In addition, a one-stage learning-based procedure is employed for speech enhancement and source localization. The proposed network composed of the multi-channel DCCRN and the auxiliary network models the sound field, while minimizing the distortionless response loss function. Simulation results show that the proposed neural beamformer is effective in enhancing speech signals, with speech quality well preserved. The proposed neural beamformer also provides source localization and voice activity detection (VAD) functions.      
### 41.SJ-HD^2R: Selective Joint High Dynamic Range and Denoising Imaging for Dynamic Scenes  [ :arrow_down: ](https://arxiv.org/pdf/2206.09611.pdf)
>  Ghosting artifacts, motion blur, and low fidelity in highlight are the main challenges in High Dynamic Range (HDR) imaging from multiple Low Dynamic Range (LDR) images. These issues come from using the medium-exposed image as the reference frame in previous methods. To deal with them, we propose to use the under-exposed image as the reference to avoid these issues. However, the heavy noise in dark regions of the under-exposed image becomes a new problem. Therefore, we propose a joint HDR and denoising pipeline, containing two sub-networks: (i) a pre-denoising network (PreDNNet) to adaptively denoise input LDRs by exploiting exposure priors; (ii) a pyramid cascading fusion network (PCFNet), introducing an attention mechanism and cascading structure in a multi-scale manner. To further leverage these two paradigms, we propose a selective and joint HDR and denoising (SJ-HD$^2$R) imaging framework, utilizing scenario-specific priors to conduct the path selection with an accuracy of more than 93.3$\%$. We create the first joint HDR and denoising benchmark dataset, which contains a variety of challenging HDR and denoising scenes and supports the switching of the reference image. Extensive experiment results show that our method achieves superior performance to previous methods.      
### 42.Reconstruction and segmentation from sparse sequential X-ray measurements of wood logs  [ :arrow_down: ](https://arxiv.org/pdf/2206.09595.pdf)
>  In industrial applications it is common to scan objects on a moving conveyor belt. If slice-wise 2D computed tomography (CT) measurements of the moving object are obtained we call it a sequential scanning geometry. In this case, each slice on its own does not carry sufficient information to reconstruct a useful tomographic image. Thus, here we propose the use of a Dimension reduced Kalman Filter to accumulate information between slices and allow for sufficiently accurate reconstructions for further assessment of the object. Additionally, we propose to use an unsupervised clustering approach known as Density Peak Advanced, to perform a segmentation and spot density anomalies in the internal structure of the reconstructed objects. We evaluate the method in a proof of concept study for the application of wood log scanning for the industrial sawing process, where the goal is to spot anomalies within the wood log to allow for optimal sawing patterns. Reconstruction and segmentation quality is evaluated from experimental measurement data for various scenarios of severely undersampled X-measurements. Results show clearly that an improvement of reconstruction quality can be obtained by employing the Dimension reduced Kalman Filter allowing to robustly obtain the segmented logs.      
### 43.Analysis of Electric Vehicle Charging Station Usage and Profitability in Germany based on Empirical Data  [ :arrow_down: ](https://arxiv.org/pdf/2206.09582.pdf)
>  Electric vehicles are booming and with them the required public charging stations. Knowing how charging stations are used is crucial for operators of the charging stations themselves, navigation systems, electricity grids, and many more. Given that there are now 2.5 as many vehicles per charging station compared to 2017, the system needs to allocate charging points intelligently and efficiently. This paper presents representative data on energy consumption, arrival times, occupation, and profitability of charging stations in Germany by combining usage data of 27,800 installations. Charging happens mainly during the day and on weekdays for AC charging stations while DC fast-charging stations are more popular on the weekend. Fast-chargers service approximately 3 times as many vehicles per connection point while also being substantially more profitable due to higher achieved margins. For AC chargers, up to 20 kWh of energy are charged in an average charge event while DC fast-chargers supply approximately 40 kWh.      
### 44.An Empirical Analysis on the Vulnerabilities of End-to-End Speech Segregation Models  [ :arrow_down: ](https://arxiv.org/pdf/2206.09556.pdf)
>  End-to-end learning models have demonstrated a remarkable capability in performing speech segregation. Despite their wide-scope of real-world applications, little is known about the mechanisms they employ to group and consequently segregate individual speakers. Knowing that harmonicity is a critical cue for these networks to group sources, in this work, we perform a thorough investigation on ConvTasnet and DPT-Net to analyze how they perform a harmonic analysis of the input mixture. We perform ablation studies where we apply low-pass, high-pass, and band-stop filters of varying pass-bands to empirically analyze the harmonics most critical for segregation. We also investigate how these networks decide which output channel to assign to an estimated source by introducing discontinuities in synthetic mixtures. We find that end-to-end networks are highly unstable, and perform poorly when confronted with deformations which are imperceptible to humans. Replacing the encoder in these networks with a spectrogram leads to lower overall performance, but much higher stability. This work helps us to understand what information these network rely on for speech segregation, and exposes two sources of generalization-errors. It also pinpoints the encoder as the part of the network responsible for these errors, allowing for a redesign with expert knowledge or transfer learning.      
### 45.Hands-on Wireless Sensing with Wi-Fi: A Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2206.09532.pdf)
>  With the rapid development of wireless communication technology, wireless access points (AP) and internet of things (IoT) devices have been widely deployed in our surroundings. Various types of wireless signals (e.g., Wi-Fi, LoRa, LTE) are filling out our living and working spaces. Previous researches reveal the fact that radio waves are modulated by the spatial structure during the propagation process (e.g., reflection, diffraction, and scattering) and superimposed on the receiver. This observation allows us to reconstruct the surrounding environment based on received wireless signals, called "wireless sensing". Wireless sensing is an emerging technology that enables a wide range of applications, such as gesture recognition for human-computer interaction, vital signs monitoring for health care, and intrusion detection for security management. Compared with other sensing paradigms, such as vision-based and IMU-based sensing, wireless sensing solutions have unique advantages such as high coverage, pervasiveness, low cost, and robustness under adverse light and texture scenarios. Besides, wireless sensing solutions are generally lightweight in terms of both computation overhead and device size. This tutorial takes Wi-Fi sensing as an example. It introduces both the theoretical principles and the code implementation of data collection, signal processing, features extraction, and model design. In addition, this tutorial highlights state-of-the-art deep learning models (e.g., CNN, RNN, and adversarial learning models) and their applications in wireless sensing systems. We hope this tutorial will help people in other research fields to break into wireless sensing research and learn more about its theories, designs, and implementation skills, promoting prosperity in the wireless sensing research field.      
### 46.A Step Towards Preserving Speakers' Identity While Detecting Depression Via Speaker Disentanglement  [ :arrow_down: ](https://arxiv.org/pdf/2206.09530.pdf)
>  Preserving a patient's identity is a challenge for automatic, speech-based diagnosis of mental health disorders. In this paper, we address this issue by proposing adversarial disentanglement of depression characteristics and speaker identity. The model used for depression classification is trained in a speaker-identity-invariant manner by minimizing depression prediction loss and maximizing speaker prediction loss during training. The effectiveness of the proposed method is demonstrated on two datasets - DAIC-WOZ (English) and CONVERGE (Mandarin), with three feature sets (Mel-spectrograms, raw-audio signals, and the last-hidden-state of wav2vec2.0), using a modified DepAudioNet model. With adversarial training, depression classification improves for every feature when compared to the baseline. Wav2vec2.0 features with adversarial learning resulted in the best performance (F1-score of 69.2% for DAIC-WOZ and 91.5% for CONVERGE). Analysis of the class-separability measure (J-ratio) of the hidden states of the DepAudioNet model shows that when adversarial learning is applied, the backend model loses some speaker-discriminability while it improves depression-discriminability. These results indicate that there are some components of speaker identity that may not be useful for depression detection and minimizing their effects provides a more accurate diagnosis of the underlying disorder and can safeguard a speaker's identity.      
### 47.Towards Trustworthy Edge Intelligence: Insights from Voice-Activated Services  [ :arrow_down: ](https://arxiv.org/pdf/2206.09523.pdf)
>  In an age of surveillance capitalism, anchoring the design of emerging smart services in trustworthiness is urgent and important. Edge Intelligence, which brings together the fields of AI and Edge computing, is a key enabling technology for smart services. Trustworthy Edge Intelligence should thus be a priority research concern. However, determining what makes Edge Intelligence trustworthy is not straight forward. This paper examines requirements for trustworthy Edge Intelligence in a concrete application scenario of voice-activated services. We contribute to deepening the understanding of trustworthiness in the emerging Edge Intelligence domain in three ways: firstly, we propose a unified framing for trustworthy Edge Intelligence that jointly considers trustworthiness attributes of AI and the IoT. Secondly, we present research outputs of a tangible case study in voice-activated services that demonstrates interdependencies between three important trustworthiness attributes: privacy, security and fairness. Thirdly, based on the empirical and analytical findings, we highlight challenges and open questions that present important future research areas for trustworthy Edge Intelligence.      
### 48.Resource-Efficient Separation Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2206.09507.pdf)
>  Transformers have recently achieved state-of-the-art performance in speech separation. These models, however, are computationally-demanding and require a lot of learnable parameters. This paper explores Transformer-based speech separation with a reduced computational cost. Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer), a self-attention-based architecture that reduces the computational burden in two ways. First, it uses non-overlapping blocks in the latent space. Second, it operates on compact latent summaries calculated from each chunk. The RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and WHAM! datasets in both causal and non-causal settings. Remarkably, it scales significantly better than the previous Transformer and RNN-based architectures in terms of memory and inference-time, making it more suitable for processing long mixtures.      
### 49.Two-Hop Age of Information Scheduling for Multi-UAV Assisted Mobile Edge Computing: FRL vs MADDPG  [ :arrow_down: ](https://arxiv.org/pdf/2206.09488.pdf)
>  In this work, we adopt the emerging technology of mobile edge computing (MEC) in the Unmanned aerial vehicles (UAVs) for communication-computing systems, to optimize the age of information (AoI) in the network. We assume that tasks are processed jointly on UAVs and BS to enhance edge performance with limited connectivity and computing. Using UAVs and BS jointly with MEC can reduce AoI on the network. To maintain the freshness of the tasks, we formulate the AoI minimization in two-hop communication framework, the first hop at the UAVs and the second hop at the BS. To approach the challenge, we optimize the problem using a deep reinforcement learning (DRL) framework, called federated reinforcement learning (FRL). In our network we have two types of agents with different states and actions but with the same policy. Our FRL enables us to handle the two-step AoI minimization and UAV trajectory problems. In addition, we compare our proposed algorithm, which has a centralized processing unit to update the weights, with fully decentralized multi-agent deep deterministic policy gradient (MADDPG), which enhances the agent's performance. As a result, the suggested algorithm outperforms the MADDPG by about 38\%      
### 50.Data-Driven Synthesis of Symbolic Abstractions with Guaranteed Confidence  [ :arrow_down: ](https://arxiv.org/pdf/2206.09397.pdf)
>  In this work, we propose a data-driven approach for the construction of finite abstractions (a.k.a., symbolic models) for discrete-time deterministic control systems with unknown dynamics. We leverage notions of so-called alternating bisimulation functions (ABF), as a relation between each unknown system and its symbolic model, to quantify the mismatch between state behaviors of two systems. Accordingly, one can employ our proposed results to perform formal verification and synthesis over symbolic models and then carry the results back over unknown original systems. In our data-driven setting, we first cast the required conditions for constructing ABF as a robust optimization program (ROP). Solving the provided ROP is not tractable due to the existence of unknown models in the constraints of ROP. To tackle this difficulty, we collect finite numbers of data from trajectories of unknown systems and propose a scenario optimization program (SOP) corresponding to the original ROP. By establishing a probabilistic relation between optimal values of SOP and ROP, we formally construct ABF between unknown systems and their symbolic models based on the number of data and a required confidence level. We verify the effectiveness of our data-driven results over two physical case studies with unknown models including (i) a DC motor and (ii) a nonlinear jet engine compressor. We construct symbolic models from data as appropriate substitutes of original systems and synthesize policies maintaining states of unknown systems in a safe set within infinite time horizons with some guaranteed confidence levels.      
### 51.Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping  [ :arrow_down: ](https://arxiv.org/pdf/2206.09396.pdf)
>  Automatic Speech Recognition (ASR) systems are known to exhibit difficulties when transcribing children's speech. This can mainly be attributed to the absence of large children's speech corpora to train robust ASR models and the resulting domain mismatch when decoding children's speech with systems trained on adult data. In this paper, we propose multiple enhancements to alleviate these issues. First, we propose a data augmentation technique based on the source-filter model of speech to close the domain gap between adult and children's speech. This enables us to leverage the data availability of adult speech corpora by making these samples perceptually similar to children's speech. Second, using this augmentation strategy, we apply transfer learning on a Transformer model pre-trained on adult data. This model follows the recently introduced XLS-R architecture, a wav2vec 2.0 model pre-trained on several cross-lingual adult speech corpora to learn general and robust acoustic frame-level representations. Adopting this model for the ASR task using adult data augmented with the proposed source-filter warping strategy and a limited amount of in-domain children's speech significantly outperforms previous state-of-the-art results on the PF-STAR British English Children's Speech corpus with a 4.86% WER on the official test set.      
### 52.Graph Neural Network Aided MU-MIMO Detectors  [ :arrow_down: ](https://arxiv.org/pdf/2206.09381.pdf)
>  Multi-user multiple-input multiple-output (MU-MIMO) systems can be used to meet high throughput requirements of 5G and beyond networks. A base station serves many users in an uplink MU-MIMO system, leading to a substantial multi-user interference (MUI). Designing a high-performance detector for dealing with a strong MUI is challenging. This paper analyses the performance degradation caused by the posterior distribution approximation used in the state-of-the-art message passing (MP) detectors in the presence of high MUI. We develop a graph neural network based framework to fine-tune the MP detectors' cavity distributions and thus improve the posterior distribution approximation in the MP detectors. We then propose two novel neural network based detectors which rely on the expectation propagation (EP) and Bayesian parallel interference cancellation (BPIC), referred to as the GEPNet and GPICNet detectors, respectively. The GEPNet detector maximizes detection performance, while GPICNet detector balances the performance and complexity. We provide proof of the permutation equivariance property, allowing the detectors to be trained only once, even in the systems with dynamic changes of the number of users. The simulation results show that the proposed GEPNet detector performance approaches maximum likelihood performance in various configurations and GPICNet detector doubles the multiplexing gain of BPIC detector.      
### 53.A Note on Comparator-Overdrive-Delay Conditioning for Current-Mode Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.09340.pdf)
>  Comparator-overdrive-delay conditioning is a new control conditioning approach for high-frequency current-mode control. No existing literature rigorously studies the effect of the comparator overdrive delay on the current-mode control. The results in this paper provide insights into the mechanism of comparator-overdrive-delay conditioning.      
### 54.Multi-period Optimal Control for Mobile Agents Considering State Unpredictability  [ :arrow_down: ](https://arxiv.org/pdf/2206.09330.pdf)
>  The optimal control for mobile agents is an important and challenging research issue. Recent work shows that using randomized mechanism in agents' control can make the state unpredictable, and thus ensure the security of agents. However, the unpredictable design is only considered in single period, which can lead to intolerable control performance in long time horizon. This paper aims at the trade-off between the control performance and state unpredictability of mobile agents in long time horizon. Utilizing random perturbations consistent with uniform distributions to maximize the attackers' prediction errors of future states, we formulate the problem as a multi-period convex stochastic optimization problem and solve it through dynamic programming. Specifically, we design the optimal control strategy considering both unconstrained and input constrained systems. The analytical iterative expressions of the control are further provided. Simulation illustrates that the algorithm increases the prediction errors under Kalman filter while achieving the control performance requirements successfully.      
### 55.Evaluation of RF Fingerprinting-Aided RSS-Based Target Localization for Emergency Response  [ :arrow_down: ](https://arxiv.org/pdf/2206.09312.pdf)
>  Target localization is essential for emergency dispatching situations. Maximum likelihood estimation (MLE) methods are widely used to estimate the target position based on the received signal strength measurements. However, the performance of MLE solvers is significantly affected by the initialization (i.e., initial guess of the solution or solution search space). To address this, a previous study proposed the semidefinite programming (SDP)-based MLE initialization. However, the performance of the SDP-based initialization technique is largely affected by the shadowing variance and geometric diversity between the target and receivers. In this study, a radio frequency (RF) fingerprinting-based MLE initialization is proposed. Further, a maximum likelihood problem for target localization combining RF fingerprinting is formulated. In the three test environments of open space, urban, and indoor, the proposed RF fingerprinting-aided target localization method showed a performance improvement of up to 63.31% and an average of 39.13%, compared to the MLE algorithm initialized with SDP. Furthermore, unlike the SDP-MLE method, the proposed method was not significantly affected by the poor geometry between the target and receivers in our experiments.      
### 56.TBraTS: Trusted Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.09309.pdf)
>  Despite recent improvements in the accuracy of brain tumor segmentation, the results still exhibit low levels of confidence and robustness. Uncertainty estimation is one effective way to change this situation, as it provides a measure of confidence in the segmentation results. In this paper, we propose a trusted brain tumor segmentation network which can generate robust segmentation results and reliable uncertainty estimations without excessive computational burden and modification of the backbone network. In our method, uncertainty is modeled explicitly using subjective logic theory, which treats the predictions of backbone neural network as subjective opinions by parameterizing the class probabilities of the segmentation as a Dirichlet distribution. Meanwhile, the trusted segmentation framework learns the function that gathers reliable evidence from the feature leading to the final segmentation results. Overall, our unified trusted segmentation framework endows the model with reliability and robustness to out-of-distribution samples. To evaluate the effectiveness of our model in robustness and reliability, qualitative and quantitative experiments are conducted on the BraTS 2019 dataset.      
### 57.Automatic Self-Adaptive Local Voltage Control Under Limited Reactive Power  [ :arrow_down: ](https://arxiv.org/pdf/2206.09269.pdf)
>  Increasing proliferation of distributed energy resources has posed new challenges to Volt/VAr control problems in distribution networks. To this end, this paper proposes an automatic self-adaptive local voltage control (ASALVC) by locally controlling VAr outputs of distributed energy resources. In this ASALVC strategy, each bus agent can locally and dynamically adjust its voltage droop function in accordance with time-varying system changes. The voltage droop function is associated with the bus-specific time-varying slope and intercept, which can be locally updated, merely based on local voltage measurements, without requiring communications. Stability, convergence and optimality properties of this local voltage control are analytically established. Numerical test cases are performed to validate and demonstrate the effectiveness and superiority of ASALVC.      
### 58.Multi-Modality Image Inpainting using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.09210.pdf)
>  Deep learning techniques, especially Generative Adversarial Networks (GANs) have significantly improved image inpainting and image-to-image translation tasks over the past few years. To the best of our knowledge, the problem of combining the image inpainting task with the multi-modality image-to-image translation remains intact. In this paper, we propose a model to address this problem. The model will be evaluated on combined night-to-day image translation and inpainting, along with promising qualitative and quantitative results.      
### 59.Scanning Inside Volcanoes by Synthetic Aperture Radar Echography Tomographic Doppler Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2206.09200.pdf)
>  A problem with synthetic aperture radar (SAR) is that due to the poor penetrating action of electromagnetic waves within solid bodies, the ability to observe through distributed targets is precluded. In this context, indeed, imaging is only possible on targets distribute on the scene surface. This work describes an imaging method based on the analysis of micro-motions present on volcanoes and generated by the underground Earth's heat. <br>Processing the coherent vibrational information embedded on the single SAR image, in the single-look-complex configuration, the sound information is exploited, penetrating tomographic imaging over a depth of about 3 km from the Earth's surface. Measurement results are calculated by processing a SLC image from the COSMO-SkyMed Second Generation satellite constellation of the Vesuvius. <br>Tomographic maps reveal the presence of the magma chamber, together with the main and the secondary volcanic conduits. This technique certainly paves the way for completely new exploitation of SAR images to scan inside the Earth's surface.      
### 60.Multi-Modality Image Super-Resolution using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.09193.pdf)
>  Over the past few years deep learning-based techniques such as Generative Adversarial Networks (GANs) have significantly improved solutions to image super-resolution and image-to-image translation problems. In this paper, we propose a solution to the joint problem of image super-resolution and multi-modality image-to-image translation. The problem can be stated as the recovery of a high-resolution image in a modality, given a low-resolution observation of the same image in an alternative modality. Our paper offers two models to address this problem and will be evaluated on the recovery of high-resolution day images given low-resolution night images of the same scene. Promising qualitative and quantitative results will be presented for each model.      
### 61.Perceptual Optimization of a Biologically-Inspired Tone Mapping Operator  [ :arrow_down: ](https://arxiv.org/pdf/2206.09146.pdf)
>  With the increasing popularity and accessibility of high dynamic range (HDR) photography, tone mapping operators (TMOs) for dynamic range compression and medium presentation are practically demanding. In this paper, we develop a two-stage neural network-based HDR image TMO that is biologically-inspired, computationally efficient, and perceptually optimized. In Stage one, motivated by the physiology of the early stages of the human visual system (HVS), we first decompose an HDR image into a normalized Laplacian pyramid. We then use two lightweight deep neural networks (DNNs) that take this normalized representation as input and estimate the Laplacian pyramid of the corresponding LDR image. We optimize the tone mapping network by minimizing the normalized Laplacian pyramid distance (NLPD), a perceptual metric calibrated against human judgments of tone-mapped image quality. In Stage two, we generate a pseudo-multi-exposure image stack with different color saturation and detail visibility by inputting an HDR image ``calibrated'' with different maximum luminances to the learned tone mapping network. We then train another lightweight DNN to fuse the LDR image stack into a desired LDR image by maximizing a variant of MEF-SSIM, another perceptually calibrated metric for image fusion. By doing so, the proposed TMO is fully automatic to tone map uncalibrated HDR images. Across an independent set of HDR images, we find that our method produces images with consistently better visual quality, and is among the fastest local TMOs.      
### 62.A Combined PCA-MLP Network for Early Breast Cancer Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.09128.pdf)
>  Breast cancer is the second most responsible for all cancer types and has been the cause of numerous deaths over the years, especially among women. Any improvisation of the existing diagnosis system for the detection of cancer can contribute to minimizing the death ratio. Moreover, cancer detection at an early stage has recently been a prime research area in the scientific community to enhance the survival rate. Proper choice of machine learning tools can ensure early-stage prognosis with high accuracy. In this paper, we have studied different machine learning algorithms to detect whether a patient is likely to face breast cancer or not. Due to the implicit behavior of early-stage features, we have implemented a multilayer perception model with the integration of PCA and suggested it to be more viable than other detection algorithms. Our 4 layers MLP-PCA network has obtained the best accuracy of 100% with a mean of 90.48% accuracy on the BCCD dataset.      
### 63.Identifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.09103.pdf)
>  An automatic speaker verification system aims to verify the speaker identity of a speech signal. However, a voice conversion system manipulates the original person's speech signal to make it sound like the target speaker's voice and deceive the speaker verification system. Most countermeasures for voice conversion-based spoofing attacks are designed to discriminate bona fide speech from spoofed speech for speaker verification systems. In this paper, we investigate the problem of source speaker identification -- inferring the identity of the source speaker given the voice converted speech. To perform source speaker identification, we simply add voice-converted speech data with the label of source speaker identity to the genuine speech dataset during speaker embedding network training. Experimental results show the feasibility of source speaker identification when training and testing with converted speeches from the same voice conversion model(s). When testing on converted speeches from an unseen voice conversion algorithm, the performance of source speaker identification improves when more voice conversion models are used during training.      
### 64.Decoupled Federated Learning for ASR with Non-IID Data  [ :arrow_down: ](https://arxiv.org/pdf/2206.09102.pdf)
>  Automatic speech recognition (ASR) with federated learning (FL) makes it possible to leverage data from multiple clients without compromising privacy. The quality of FL-based ASR could be measured by recognition performance, communication and computation costs. When data among different clients are not independently and identically distributed (non-IID), the performance could degrade significantly. In this work, we tackle the non-IID issue in FL-based ASR with personalized FL, which learns personalized models for each client. Concretely, we propose two types of personalized FL approaches for ASR. Firstly, we adapt the personalization layer based FL for ASR, which keeps some layers locally to learn personalization models. Secondly, to reduce the communication and computation costs, we propose decoupled federated learning (DecoupleFL). On one hand, DecoupleFL moves the computation burden to the server, thus decreasing the computation on clients. On the other hand, DecoupleFL communicates secure high-level features instead of model parameters, thus reducing communication cost when models are large. Experiments demonstrate two proposed personalized FL-based ASR approaches could reduce WER by 2.3% - 3.4% compared with FedAvg. Among them, DecoupleFL has only 11.4% communication and 75% computation cost compared with FedAvg, which is also significantly less than the personalization layer based FL.      
### 65.Semi-supervised Time Domain Target Speaker Extraction with Attention  [ :arrow_down: ](https://arxiv.org/pdf/2206.09072.pdf)
>  In this work, we propose Exformer, a time-domain architecture for target speaker extraction. It consists of a pre-trained speaker embedder network and a separator network based on transformer encoder blocks. We study multiple methods to combine speaker information with the input mixture, and the resulting Exformer architecture obtains superior extraction performance compared to prior time-domain networks. Furthermore, we investigate a two-stage procedure to train the model using mixtures without reference signals upon a pre-trained supervised model. Experimental results show that the proposed semi-supervised learning procedure improves the performance of the supervised baselines.      
### 66.Free-form Lesion Synthesis Using a Partial Convolution Generative Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.09065.pdf)
>  Automatic deep learning segmentation models has been shown to improve both the segmentation efficiency and the accuracy. However, training a robust segmentation model requires considerably large labeled training samples, which may be impractical. This study aimed to develop a deep learning framework for generating synthetic lesions that can be used to enhance network training. The lesion synthesis network is a modified generative adversarial network (GAN). Specifically, we innovated a partial convolution strategy to construct an Unet-like generator. The discriminator is designed using Wasserstein GAN with gradient penalty and spectral normalization. A mask generation method based on principal component analysis was developed to model various lesion shapes. The generated masks are then converted into liver lesions through a lesion synthesis network. The lesion synthesis framework was evaluated for lesion textures, and the synthetic lesions were used to train a lesion segmentation network to further validate the effectiveness of this framework. All the networks are trained and tested on the public dataset from LITS. The synthetic lesions generated by the proposed approach have very similar histogram distributions compared to the real lesions for the two employed texture parameters, GLCM-energy and GLCM-correlation. The Kullback-Leibler divergence of GLCM-energy and GLCM-correlation were 0.01 and 0.10, respectively. Including the synthetic lesions in the tumor segmentation network improved the segmentation dice performance of U-Net significantly from 67.3% to 71.4% (p&lt;0.05). Meanwhile, the volume precision and sensitivity improve from 74.6% to 76.0% (p=0.23) and 66.1% to 70.9% (p&lt;0.01), respectively. The synthetic data significantly improves the segmentation performance.      
### 67.NASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling  [ :arrow_down: ](https://arxiv.org/pdf/2206.09058.pdf)
>  For deep learning-based speech enhancement (SE) systems, the training-test acoustic mismatch can cause notable performance degradation. To address the mismatch issue, numerous noise adaptation strategies have been derived. In this paper, we propose a novel method, called noise adaptive speech enhancement with target-conditional resampling (NASTAR), which reduces mismatches with only one sample (one-shot) of noisy speech in the target environment. NASTAR uses a feedback mechanism to simulate adaptive training data via a noise extractor and a retrieval model. The noise extractor estimates the target noise from the noisy speech, called pseudo-noise. The noise retrieval model retrieves relevant noise samples from a pool of noise signals according to the noisy speech, called relevant-cohort. The pseudo-noise and the relevant-cohort set are jointly sampled and mixed with the source speech corpus to prepare simulated training data for noise adaptation. Experimental results show that NASTAR can effectively use one noisy speech sample to adapt an SE model to a target condition. Moreover, both the noise extractor and the noise retrieval model contribute to model adaptation. To our best knowledge, NASTAR is the first work to perform one-shot noise adaptation through noise extraction and retrieval.      
### 68.Report: EPOC Emotiv EEG Basics  [ :arrow_down: ](https://arxiv.org/pdf/2206.09051.pdf)
>  This document provides some basic guidance to start working with the EPOC Emotiv neuroheadset device and describes how to use it to perform basic Brain-Computer Interface (BCI) research. A brief tutorial on how to set up the device, from its electrophysiological point of view, as well as a description and practical code to perform some basic analysis, is explained. A basic experiment is introduced to detect one of the oldest and, indeed, quite still valuable electrophysiological correlate, visual occipital alpha waves, or Berger Rhythm. An additional experiment is expounded where the power spectrum of alpha waves is reduced when a subject is affected by background cognitive disturbances. This document also briefs about the extraction of information by using the EPOC Emotiv library and also with python Emokit package. This report presents a basic guide on how to use EEGLAB and MATLAB, as well as python stack to perform the neurophysiological analysis. Finally, a basic analysis on different feature extraction and classification methods is provided.      
### 69.Enabling Undergrounding of Long-Distance Transmission Lines with Low Frequency AC Technology  [ :arrow_down: ](https://arxiv.org/pdf/2206.09045.pdf)
>  With increasing prevalence of severe natural hazards and the ignition of wildfires by power lines, many power system planners are considering converting overhead lines to underground cables to mitigate risks related to these events. Systems with a large proportion of underground cables can bring challenges due to the capacitance and losses of the cables, factors which may limit the potential for hardening in critical areas. Low frequency AC (LFAC) transmission solves these problems, as lowering the frequency decreases both the effects of capacitance and the losses. This paper presents a tractable frequency-dependent model for underground cables and incorporates it into the open source optimal power flow tool VariableFrequencyOPF.jl. The model and implementation are used to demonstrate the benefits of LFAC in a case study involving two multi-terminal cable upgrades with LFAC, including a comparison with HVDC. The results demonstrate the value of LFAC systems for both power flow control and reduction of losses.      
### 70.Orthogonal Rational Approximation of Transfer Functions for High-Frequency Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2206.09008.pdf)
>  Rational function approximations find applications in many areas including macro-modeling of high-frequency circuits, model order reduction for controller design, interpolation and extrapolation of system responses, surrogate models for high-energy physics, and approximation of elementary mathematical functions. The unknown denominator polynomial of the model results in a non-linear problem, which can be replaced with successive solutions of linearized problems following the Sanathanan-Koerner (SK) iteration. An orthogonal basis can be obtained based on Arnoldi resulting in a stabilized SK iteration. We present an extension of the stabilized SK, called Orthogonal Rational Approximation (ORA), which ensures real polynomial coefficients and stable poles for realizability of electrical networks. We also introduce an efficient implementation of ORA for multi-port networks based on a block QR decomposition.      
### 71.TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.08985.pdf)
>  Colorectal cancer (CRC) is one of the most common causes of cancer and cancer-related mortality worldwide. Performing colon cancer screening in a timely fashion is the key to early detection. Colonoscopy is the primary modality used to diagnose colon cancer. However, the miss rate of polyps, adenomas and advanced adenomas remains significantly high. Early detection of polyps at the precancerous stage can help reduce the mortality rate and the economic burden associated with colorectal cancer. Deep learning-based computer-aided diagnosis (CADx) system may help gastroenterologists to identify polyps that may otherwise be missed, thereby improving the polyp detection rate. Additionally, CADx system could prove to be a cost-effective system that improves long-term colorectal cancer prevention. In this study, we proposed a deep learning-based architecture for automatic polyp segmentation, called Transformer ResU-Net (TransResU-Net). Our proposed architecture is built upon residual blocks with ResNet-50 as the backbone and takes the advantage of transformer self-attention mechanism as well as dilated convolution(s). Our experimental results on two publicly available polyp segmentation benchmark datasets showed that TransResU-Net obtained a highly promising dice score and a real-time speed. With high efficacy in our performance metrics, we concluded that TransResU-Net could be a strong benchmark for building a real-time polyp detection system for the early diagnosis, treatment, and prevention of colorectal cancer. The source code of the proposed TransResU-Net is publicly available at <a class="link-external link-https" href="https://github.com/nikhilroxtomar/TransResUNet" rel="external noopener nofollow">this https URL</a>.      
### 72.Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness  [ :arrow_down: ](https://arxiv.org/pdf/2206.08984.pdf)
>  Magnetic Resonance Spectroscopic Imaging (MRSI) is a valuable tool for studying metabolic activities in the human body, but the current applications are limited to low spatial resolutions. The existing deep learning-based MRSI super-resolution methods require training a separate network for each upscaling factor, which is time-consuming and memory inefficient. We tackle this multi-scale super-resolution problem using a Filter Scaling strategy that modulates the convolution filters based on the upscaling factor, such that a single network can be used for various upscaling factors. Observing that each metabolite has distinct spatial characteristics, we also modulate the network based on the specific metabolite. Furthermore, our network is conditioned on the weight of adversarial loss so that the perceptual sharpness of the super-resolved metabolic maps can be adjusted within a single network. We incorporate these network conditionings using a novel Multi-Conditional Module. The experiments were carried out on a 1H-MRSI dataset from 15 high-grade glioma patients. Results indicate that the proposed network achieves the best performance among several multi-scale super-resolution methods and can provide super-resolved metabolic maps with adjustable sharpness.      
### 73.Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency  [ :arrow_down: ](https://arxiv.org/pdf/2206.08936.pdf)
>  Segmenting both bone surface and the corresponding acoustic shadow are fundamental tasks in ultrasound (US) guided orthopedic procedures. However, these tasks are challenging due to minimal and blurred bone surface response in US images, cross-machine discrepancy, imaging artifacts, and low signal-to-noise ratio. Notably, bone shadows are caused by a significant acoustic impedance mismatch between the soft tissue and bone surfaces. To leverage this mutual information between these highly related tasks, we propose a single end-to-end network with a shared transformer-based encoder and task independent decoders for simultaneous bone and shadow segmentation. To share complementary features, we propose a cross task feature transfer block which learns to transfer meaningful features from decoder of shadow segmentation to that of bone segmentation and vice-versa. We also introduce a correspondence consistency loss which makes sure that network utilizes the inter-dependency between the bone surface and its corresponding shadow to refine the segmentation. Validation against expert annotations shows that the method outperforms the previous state-of-the-art for both bone surface and shadow segmentation.      
### 74.Gradient-Enhanced Physics-Informed Neural Networks for Power Systems Operational Support  [ :arrow_down: ](https://arxiv.org/pdf/2206.10579.pdf)
>  The application of deep learning methods to speed up the resolution of challenging power flow problems has recently shown very encouraging results. However, power system dynamics are not snap-shot, steady-state operations. These dynamics must be considered to ensure that the optimal solutions provided by these models adhere to practical dynamical constraints, avoiding frequency fluctuations and grid instabilities. Unfortunately, dynamic system models based on ordinary or partial differential equations are frequently unsuitable for direct application in control or state estimates due to their high computational costs. To address these challenges, this paper introduces a machine learning method to approximate the behavior of power systems dynamics in near real time. The proposed framework is based on gradient-enhanced physics-informed neural networks (gPINNs) and encodes the underlying physical laws governing power systems. A key characteristic of the proposed gPINN is its ability to train without the need of generating expensive training data. The paper illustrates the potential of the proposed approach in both forward and inverse problems in a single-machine infinite bus system for predicting rotor angles and frequency, and uncertain parameters such as inertia and damping to showcase its potential for a range of power systems applications.      
### 75.Multi-UAV Planning for Cooperative Wildfire Coverage and Tracking with Quality-of-Service Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2206.10544.pdf)
>  In recent years, teams of robot and Unmanned Aerial Vehicles (UAVs) have been commissioned by researchers to enable accurate, online wildfire coverage and tracking. While the majority of prior work focuses on the coordination and control of such multi-robot systems, to date, these UAV teams have not been given the ability to reason about a fire's track (i.e., location and propagation dynamics) to provide performance guarantee over a time horizon. Motivated by the problem of aerial wildfire monitoring, we propose a predictive framework which enables cooperation in multi-UAV teams towards collaborative field coverage and fire tracking with probabilistic performance guarantee. Our approach enables UAVs to infer the latent fire propagation dynamics for time-extended coordination in safety-critical conditions. We derive a set of novel, analytical temporal, and tracking-error bounds to enable the UAV-team to distribute their limited resources and cover the entire fire area according to the case-specific estimated states and provide a probabilistic performance guarantee. Our results are not limited to the aerial wildfire monitoring case-study and are generally applicable to problems, such as search-and-rescue, target tracking and border patrol. We evaluate our approach in simulation and provide demonstrations of the proposed framework on a physical multi-robot testbed to account for real robot dynamics and restrictions. Our quantitative evaluations validate the performance of our method accumulating 7.5x and 9.0x smaller tracking-error than state-of-the-art model-based and reinforcement learning benchmarks, respectively.      
### 76.Lyapunov Density Models: Constraining Distribution Shift in Learning-Based Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.10524.pdf)
>  Learned models and policies can generalize effectively when evaluated within the distribution of the training data, but can produce unpredictable and erroneous outputs on out-of-distribution inputs. In order to avoid distribution shift when deploying learning-based control algorithms, we seek a mechanism to constrain the agent to states and actions that resemble those that it was trained on. In control theory, Lyapunov stability and control-invariant sets allow us to make guarantees about controllers that stabilize the system around specific states, while in machine learning, density models allow us to estimate the training data distribution. Can we combine these two concepts, producing learning-based control algorithms that constrain the system to in-distribution states using only in-distribution actions? In this work, we propose to do this by combining concepts from Lyapunov stability and density estimation, introducing Lyapunov density models: a generalization of control Lyapunov functions and density models that provides guarantees on an agent's ability to stay in-distribution over its entire trajectory.      
### 77.Rethinking Audio-visual Synchronization for Active Speaker Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.10421.pdf)
>  Active speaker detection (ASD) systems are important modules for analyzing multi-talker conversations. They aim to detect which speakers or none are talking in a visual scene at any given time. Existing research on ASD does not agree on the definition of active speakers. We clarify the definition in this work and require synchronization between the audio and visual speaking activities. This clarification of definition is motivated by our extensive experiments, through which we discover that existing ASD methods fail in modeling the audio-visual synchronization and often classify unsynchronized videos as active speaking. To address this problem, we propose a cross-modal contrastive learning strategy and apply positional encoding in attention modules for supervised ASD models to leverage the synchronization cue. Experimental results suggest that our model can successfully detect unsynchronized speaking as not speaking, addressing the limitation of current models.      
### 78.Audio-video fusion strategies for active speaker detection in meetings  [ :arrow_down: ](https://arxiv.org/pdf/2206.10411.pdf)
>  Meetings are a common activity in professional contexts, and it remains challenging to endow vocal assistants with advanced functionalities to facilitate meeting management. In this context, a task like active speaker detection can provide useful insights to model interaction between meeting participants. Motivated by our application context related to advanced meeting assistant, we want to combine audio and visual information to achieve the best possible performance. In this paper, we propose two different types of fusion for the detection of the active speaker, combining two visual modalities and an audio modality through neural networks. For comparison purpose, classical unsupervised approaches for audio feature extraction are also used. We expect visual data centered on the face of each participant to be very appropriate for detecting voice activity, based on the detection of lip and facial gestures. Thus, our baseline system uses visual data and we chose a 3D Convolutional Neural Network architecture, which is effective for simultaneously encoding appearance and movement. To improve this system, we supplemented the visual information by processing the audio stream with a CNN or an unsupervised speaker diarization system. We have further improved this system by adding visual modality information using motion through optical flow. We evaluated our proposal with a public and state-of-the-art benchmark: the AMI corpus. We analysed the contribution of each system to the merger carried out in order to determine if a given participant is currently speaking. We also discussed the results we obtained. Besides, we have shown that, for our application context, adding motion information greatly improves performance. Finally, we have shown that attention-based fusion improves performance while reducing the standard deviation.      
### 79.Neural Moving Horizon Estimation for Robust Flight Control  [ :arrow_down: ](https://arxiv.org/pdf/2206.10397.pdf)
>  Estimating and reacting to external disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive real-world data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the MHE parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradient of the MHE estimates with respect to the tunable parameters, enabling a seamless embedding of MHE as a layer into the neural network for highly effective learning. Most interestingly, we show that the gradient can be solved efficiently from a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the trajectory tracking error without the need for the ground-truth disturbance. The effectiveness of NeuroMHE is verified extensively via both simulations and physical experiments on a quadrotor in various challenging flights. Notably, NeuroMHE outperforms the state-of-the-art estimator with force estimation error reductions of up to 49.4% by using only a 2.5% amount of parameters. The proposed method is general and can be applied to robust adaptive control for other robotic systems.      
### 80.Can we trust our energy measurements? A study on the Odroid-XU4  [ :arrow_down: ](https://arxiv.org/pdf/2206.10377.pdf)
>  IoT devices, edge devices and embedded devices, in general, are ubiquitous. The energy consumption of such devices is important both due to the total number of devices deployed and because such devices are often battery-powered. Hence, improving the energy efficiency of such high-performance embedded systems is crucial. The first step to decreasing energy consumption is to accurately measure it, as we base our conclusions and decisions on the measurements. Given the importance of the measurements, it surprised us that most publications dedicate little space and effort to the description of their experimental setup. <br>One variable of importance of the measurement system is the sampling frequency, e.g. how often the continuous signal's voltage and current are measured per second. In this paper, we systematically explore the impact of the sampling frequency on the accuracy of the measurement system. We measure the energy consumption of a Hardkernel Odroid-XU4 board executing nine Rodinia benchmarks with a wide range of runtimes and options at 4kHz, which is the standard sampling frequency of our measurement system. We show that one needs to measure at least at 350Hz to achieve equivalent results in comparison to the original power traces. Sampling at 1Hz (e.g. Hardkernel SmartPower2) results in a maximum error of 80%.      
### 81.Joint Analysis of Acoustic Scenes and Sound Events Based on Multitask Learning with Dynamic Weight Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2206.10349.pdf)
>  Acoustic scene classification (ASC) and sound event detection (SED) are major topics in environmental sound analysis. Considering that acoustic scenes and sound events are closely related to each other, the joint analysis of acoustic scenes and sound events using multitask learning (MTL)-based neural networks was proposed in some previous works. Conventional methods train MTL-based models using a linear combination of ASC and SED loss functions with constant weights. However, the performance of conventional MTL-based methods depends strongly on the weights of the ASC and SED losses, and it is difficult to determine the appropriate balance between the constant weights of the losses of MTL of ASC and SED. In this paper, we thus propose dynamic weight adaptation methods for MTL of ASC and SED based on dynamic weight average and multi--focal loss to adjust the learning weights automatically. Evaluation experiments using parts of the TUT Acoustic Scenes 2016/2017 and TUT Sound Events 2016/2017 are conducted, and we show that the proposed methods improve the scene classification and event detection performance characteristics compared with the conventional MTL-based method. We then investigate how the learning weights of ASC and SED tasks dynamically adapt as the model training progresses.      
### 82.Human-in-the-loop Speaker Adaptation for DNN-based Multi-speaker TTS  [ :arrow_down: ](https://arxiv.org/pdf/2206.10256.pdf)
>  This paper proposes a human-in-the-loop speaker-adaptation method for multi-speaker text-to-speech. With a conventional speaker-adaptation method, a target speaker's embedding vector is extracted from his/her reference speech using a speaker encoder trained on a speaker-discriminative task. However, this method cannot obtain an embedding vector for the target speaker when the reference speech is unavailable. Our method is based on a human-in-the-loop optimization framework, which incorporates a user to explore the speaker-embedding space to find the target speaker's embedding. The proposed method uses a sequential line search algorithm that repeatedly asks a user to select a point on a line segment in the embedding space. To efficiently choose the best speech sample from multiple stimuli, we also developed a system in which a user can switch between multiple speakers' voices for each phoneme while looping an utterance. Experimental results indicate that the proposed method can achieve comparable performance to the conventional one in objective and subjective evaluations even if reference speech is not used as the input of a speaker encoder directly.      
### 83.Incorporating Voice Instructions in Model-Based Reinforcement Learning for Self-Driving Cars  [ :arrow_down: ](https://arxiv.org/pdf/2206.10249.pdf)
>  This paper presents a novel approach that supports natural language voice instructions to guide deep reinforcement learning (DRL) algorithms when training self-driving cars. DRL methods are popular approaches for autonomous vehicle (AV) agents. However, most existing methods are sample- and time-inefficient and lack a natural communication channel with the human expert. In this paper, how new human drivers learn from human coaches motivates us to study new ways of human-in-the-loop learning and a more natural and approachable training interface for the agents. We propose incorporating natural language voice instructions (NLI) in model-based deep reinforcement learning to train self-driving cars. We evaluate the proposed method together with a few state-of-the-art DRL methods in the CARLA simulator. The results show that NLI can help ease the training process and significantly boost the agents' learning speed.      
### 84.A Hierarchical HAZOP-Like Safety Analysis for Learning-Enabled Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.10216.pdf)
>  Hazard and Operability Analysis (HAZOP) is a powerful safety analysis technique with a long history in industrial process control domain. With the increasing use of Machine Learning (ML) components in cyber physical systems--so called Learning-Enabled Systems (LESs), there is a recent trend of applying HAZOP-like analysis to LESs. While it shows a great potential to reserve the capability of doing sufficient and systematic safety analysis, there are new technical challenges raised by the novel characteristics of ML that require retrofit of the conventional HAZOP technique. In this regard, we present a new Hierarchical HAZOP-Like method for LESs (HILLS). To deal with the complexity of LESs, HILLS first does "divide and conquer" by stratifying the whole system into three levels, and then proceeds HAZOP on each level to identify (latent-)hazards, causes, security threats and mitigation (with new nodes and guide words). Finally, HILLS attempts at linking and propagating the causal relationship among those identified elements within and across the three levels via both qualitative and quantitative methods. We examine and illustrate the utility of HILLS by a case study on Autonomous Underwater Vehicles, with discussions on assumptions and extensions to real-world applications. HILLS, as a first HAZOP-like attempt on LESs that explicitly considers ML internal behaviours and its interactions with other components, not only uncovers the inherent difficulties of doing safety analysis for LESs, but also demonstrates a good potential to tackle them.      
### 85.Analysis of Self-Supervised Learning and Dimensionality Reduction Methods in Clustering-Based Active Learning for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2206.10188.pdf)
>  When domain experts are needed to perform data annotation for complex machine-learning tasks, reducing annotation effort is crucial in order to cut down time and expenses. For cases when there are no annotations available, one approach is to utilize the structure of the feature space for clustering-based active learning (AL) methods. However, these methods are heavily dependent on how the samples are organized in the feature space and what distance metric is used. Unsupervised methods such as contrastive predictive coding (CPC) can potentially be used to learn organized feature spaces, but these methods typically create high-dimensional features which might be challenging for estimating data density. In this paper, we combine CPC and multiple dimensionality reduction methods in search of functioning practices for clustering-based AL. Our experiments for simulating speech emotion recognition system deployment show that both the local and global topology of the feature space can be successfully used for AL, and that CPC can be used to improve clustering-based AL performance over traditional signal features. Additionally, we observe that compressing data dimensionality does not harm AL performance substantially, and that 2-D feature representations achieved similar AL performance as higher-dimensional representations when the number of annotations is not very low.      
### 86.A Multi-grained based Attention Network for Semi-supervised Sound Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.10175.pdf)
>  Sound event detection (SED) is an interesting but challenging task due to the scarcity of data and diverse sound events in real life. This paper presents a multi-grained based attention network (MGA-Net) for semi-supervised sound event detection. To obtain the feature representations related to sound events, a residual hybrid convolution (RH-Conv) block is designed to boost the vanilla convolution's ability to extract the time-frequency features. Moreover, a multi-grained attention (MGA) module is designed to learn temporal resolution features from coarse-level to fine-level. With the MGA module,the network could capture the characteristics of target events with short- or long-duration, resulting in more accurately determining the onset and offset of sound events. Furthermore, to effectively boost the performance of the Mean Teacher (MT) method, a spatial shift (SS) module as a data perturbation mechanism is introduced to increase the diversity of data. Experimental results show that the MGA-Net outperforms the published state-of-the-art competitors, achieving 53.27% and 56.96% event-based macro F1 (EB-F1) score, 0.709 and 0.739 polyphonic sound detection score (PSDS) on the validation and public set respectively.      
### 87.An Integrated Representation &amp; Compression Scheme Based on Convolutional Autoencoders with 4D DCT Perceptual Encoding for High Dynamic Range Light Fields  [ :arrow_down: ](https://arxiv.org/pdf/2206.10131.pdf)
>  The emerging and existing light field displays are highly capable of realistic presentation of 3D scenes on auto-stereoscopic glasses-free platforms. The light field size is a major drawback while utilising 3D displays and streaming purposes. When a light field is of high dynamic range, the size increases drastically. In this paper, we propose a novel compression algorithm for a high dynamic range light field which yields a perceptually lossless compression. The algorithm exploits the inter and intra view correlations of the HDR light field by interpreting it to be a four-dimension volume. The HDR light field compression is based on a novel 4DDCT-UCS (4D-DCT Uniform Colour Space) algorithm. Additional encoding of 4DDCT-UCS acquired images by HEVC eliminates intra-frame, inter-frame and intrinsic redundancies in HDR light field data. Comparison with state-of-the-art coders like JPEG-XL and HDR video coding algorithm exhibits superior compression performance of the proposed scheme for real-world light fields.      
### 88.Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training  [ :arrow_down: ](https://arxiv.org/pdf/2206.10125.pdf)
>  Recently, masked prediction pre-training has seen remarkable progress in self-supervised learning (SSL) for speech recognition. It usually requires a codebook obtained in an unsupervised way, making it less accurate and difficult to interpret. We propose two supervision-guided codebook generation approaches to improve automatic speech recognition (ASR) performance and also the pre-training efficiency, either through decoding with a hybrid ASR system to generate phoneme-level alignments (named PBERT), or performing clustering on the supervised speech features extracted from an end-to-end CTC model (named CTC clustering). Both the hybrid and CTC models are trained on the same small amount of labeled speech as used in fine-tuning. Experiments demonstrate significant superiority of our methods to various SSL and self-training baselines, with up to 17.0% relative WER reduction. Our pre-trained models also show good transferability in a non-ASR speech task.      
### 89.Transformers Improve Breast Cancer Diagnosis from Unregistered Multi-View Mammograms  [ :arrow_down: ](https://arxiv.org/pdf/2206.10096.pdf)
>  Deep convolutional neural networks (CNNs) have been widely used in various medical imaging tasks. However, due to the intrinsic locality of convolution operation, CNNs generally cannot model long-range dependencies well, which are important for accurately identifying or mapping corresponding breast lesion features computed from unregistered multiple mammograms. This motivates us to leverage the architecture of Multi-view Vision Transformers to capture long-range relationships of multiple mammograms from the same patient in one examination. For this purpose, we employ local Transformer blocks to separately learn patch relationships within four mammograms acquired from two-view (CC/MLO) of two-side (right/left) breasts. The outputs from different views and sides are concatenated and fed into global Transformer blocks, to jointly learn patch relationships between four images representing two different views of the left and right breasts. To evaluate the proposed model, we retrospectively assembled a dataset involving 949 sets of mammograms, which include 470 malignant cases and 479 normal or benign cases. We trained and evaluated the model using a five-fold cross-validation method. Without any arduous preprocessing steps (e.g., optimal window cropping, chest wall or pectoral muscle removal, two-view image registration, etc.), our four-image (two-view-two-side) Transformer-based model achieves case classification performance with an area under ROC curve (AUC = 0.818), which significantly outperforms AUC = 0.784 achieved by the state-of-the-art multi-view CNNs (p = 0.009). It also outperforms two one-view-two-side models that achieve AUC of 0.724 (CC view) and 0.769 (MLO view), respectively. The study demonstrates the potential of using Transformers to develop high-performing computer-aided diagnosis schemes that combine four mammograms.      
### 90.Bio-inspired Neural Network-based Optimal Path Planning for UUVs under the Effect of Ocean Currents  [ :arrow_down: ](https://arxiv.org/pdf/2206.10087.pdf)
>  To eliminate the effect of ocean currents when addressing the optimal path in the underwater environment, an intelligent algorithm designed for the unmanned underwater vehicle (UUV) is proposed in this paper. The algorithm consists of two parts: a neural network-based algorithm that deducts the shortest path and avoids all possible collisions; and an adjusting component that balances off the deviation brought by the effect of ocean currents. The optimization results of the proposed algorithm are presented in detail, and compared with the path planning algorithm that does not consider the effect of currents. Results of the comparison prove the effectiveness of the path planning method when encountering currents of different directions and velocities.      
### 91.Optimally Controllable Perceptual Lossy Compression  [ :arrow_down: ](https://arxiv.org/pdf/2206.10082.pdf)
>  Recent studies in lossy compression show that distortion and perceptual quality are at odds with each other, which put forward the tradeoff between distortion and perception (D-P). Intuitively, to attain different perceptual quality, different decoders have to be trained. In this paper, we present a nontrivial finding that only two decoders are sufficient for optimally achieving arbitrary (an infinite number of different) D-P tradeoff. We prove that arbitrary points of the D-P tradeoff bound can be achieved by a simple linear interpolation between the outputs of a minimum MSE decoder and a specifically constructed perfect perceptual decoder. Meanwhile, the perceptual quality (in terms of the squared Wasserstein-2 distance metric) can be quantitatively controlled by the interpolation factor. Furthermore, to construct a perfect perceptual decoder, we propose two theoretically optimal training frameworks. The new frameworks are different from the distortion-plus-adversarial loss based heuristic framework widely used in existing methods, which are not only theoretically optimal but also can yield state-of-the-art performance in practical perceptual decoding. Finally, we validate our theoretical finding and demonstrate the superiority of our frameworks via experiments. Code is available at: <a class="link-external link-https" href="https://github.com/ZeyuYan/Controllable-Perceptual-Compression" rel="external noopener nofollow">this https URL</a>      
### 92.The Manifold Scattering Transform for High-Dimensional Point Cloud Data  [ :arrow_down: ](https://arxiv.org/pdf/2206.10078.pdf)
>  The manifold scattering transform is a deep feature extractor for data defined on a Riemannian manifold. It is one of the first examples of extending convolutional neural network-like operators to general manifolds. The initial work on this model focused primarily on its theoretical stability and invariance properties but did not provide methods for its numerical implementation except in the case of two-dimensional surfaces with predefined meshes. In this work, we present practical schemes, based on the theory of diffusion maps, for implementing the manifold scattering transform to datasets arising in naturalistic systems, such as single cell genetics, where the data is a high-dimensional point cloud modeled as lying on a low-dimensional manifold. We show that our methods are effective for signal classification and manifold classification tasks.      
### 93.Thompson Sampling Efficiently Learns to Control Diffusion Processes  [ :arrow_down: ](https://arxiv.org/pdf/2206.09977.pdf)
>  Diffusion processes that evolve according to linear stochastic differential equations are an important family of continuous-time dynamic decision-making models. Optimal policies are well-studied for them, under full certainty about the drift matrices. However, little is known about data-driven control of diffusion processes with uncertain drift matrices as conventional discrete-time analysis techniques are not applicable. In addition, while the task can be viewed as a reinforcement learning problem involving exploration and exploitation trade-off, ensuring system stability is a fundamental component of designing optimal policies. We establish that the popular Thompson sampling algorithm learns optimal actions fast, incurring only a square-root of time regret, and also stabilizes the system in a short time period. To the best of our knowledge, this is the first such result for Thompson sampling in a diffusion process control problem. We validate our theoretical results through empirical simulations with real parameter matrices from two settings of airplane and blood glucose control. Moreover, we observe that Thompson sampling significantly improves (worst-case) regret, compared to the state-of-the-art algorithms, suggesting Thompson sampling explores in a more guarded fashion. Our theoretical analysis involves characterization of a certain optimality manifold that ties the local geometry of the drift parameters to the optimal control of the diffusion process. We expect this technique to be of broader interest.      
### 94.Sample Average Approximation for Stochastic Programming with Equality Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2206.09963.pdf)
>  We revisit the sample average approximation (SAA) approach for general non-convex stochastic programming. We show that applying the SAA approach to problems with expected value equality constraints does not necessarily result in asymptotic optimality guarantees as the number of samples increases. To address this issue, we relax the equality constraints. Then, we prove the asymptotic optimality of the modified SAA approach under mild smoothness and boundedness conditions on the equality constraint functions. Our analysis uses random set theory and concentration inequalities to characterize the approximation error from the sampling procedure. We apply our approach to the problem of stochastic optimal control for nonlinear dynamical systems subject to external disturbances modeled by a Wiener process. We verify our approach on a rocket-powered descent problem and show that our computed solutions allow for significant uncertainty reduction.      
### 95.Towards 1000-mode Optical Fibres  [ :arrow_down: ](https://arxiv.org/pdf/2206.09855.pdf)
>  We report on the design of multimode-mode fibres guiding up to 870 spatial and polarization modes for low differential mode delay over the C-band.      
### 96.Multi-criteria optimization and automated network restructuring to mitigate construction projects delays on-the-run  [ :arrow_down: ](https://arxiv.org/pdf/2206.09823.pdf)
>  Construction project management requires dynamic mitigation control ensuring the project's timely completion by a best fit for common purpose strategy for all stakeholders. Current mitigation approaches are usually performed by an iterative Monte Carlo (MC) analysis focussing on lowest-cost strategies which do not include (1) the project manager's goal-oriented behaviour, (2) automated network restructuring potential, and (3) multi-dimensional optimization criteria for best fitting mitigation strategies-criteria. Therefore, the development statement within this paper is to design a method and implementation tool that properly dissolves all the aforementioned shortcomings ensuring the project's completion date by finding the most effective and efficient mitigation strategy. To fulfill the purpose of this paper, the Mitigation Controller (MitC) has been developed using an integrative approach of non-linear optimization techniques, probabilistic Monte Carlo simulation, and preference function modeling. Compared to the conventional way of mitigating project delays. The developed MitC allows mitigating potential delays with the least negative consequences on several project criteria, such as cost, environmental impact, etc. The application of the model to the demonstrative case study shows the ability of the model to significantly increase the probability of completing the project in the given target duration. Embedding the multi-criteria evaluation in the optimization model ensures that other interests are also represented in finding the optimal strategy for project delays.      
### 97.The Makerere Radio Speech Corpus: A Luganda Radio Corpus for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2206.09790.pdf)
>  Building a usable radio monitoring automatic speech recognition (ASR) system is a challenging task for under-resourced languages and yet this is paramount in societies where radio is the main medium of public communication and discussions. Initial efforts by the United Nations in Uganda have proved how understanding the perceptions of rural people who are excluded from social media is important in national planning. However, these efforts are being challenged by the absence of transcribed speech datasets. In this paper, The Makerere Artificial Intelligence research lab releases a Luganda radio speech corpus of 155 hours. To our knowledge, this is the first publicly available radio dataset in sub-Saharan Africa. The paper describes the development of the voice corpus and presents baseline Luganda ASR performance results using Coqui STT toolkit, an open source speech recognition toolkit.      
### 98.Time Gated Convolutional Neural Networks for Crop Classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.09756.pdf)
>  This paper presented a state-of-the-art framework, Time Gated Convolutional Neural Network (TGCNN) that takes advantage of temporal information and gating mechanisms for the crop classification problem. Besides, several vegetation indices were constructed to expand dimensions of input data to take advantage of spectral information. Both spatial (channel-wise) and temporal (step-wise) correlation are considered in TGCNN. Specifically, our preliminary analysis indicates that step-wise information is of greater importance in this data set. Lastly, the gating mechanism helps capture high-order relationship. Our TGCNN solution achieves $0.973$ F1 score, $0.977$ AUC ROC and $0.948$ IoU, respectively. In addition, it outperforms three other benchmarks in different local tasks (Kenya, Brazil and Togo). Overall, our experiments demonstrate that TGCNN is advantageous in this earth observation time series classification task.      
### 99.Guided Safe Shooting: model based reinforcement learning with safety constraints  [ :arrow_down: ](https://arxiv.org/pdf/2206.09743.pdf)
>  In the last decade, reinforcement learning successfully solved complex control tasks and decision-making problems, like the Go board game. Yet, there are few success stories when it comes to deploying those algorithms to real-world scenarios. One of the reasons is the lack of guarantees when dealing with and avoiding unsafe states, a fundamental requirement in critical control engineering systems. In this paper, we introduce Guided Safe Shooting (GuSS), a model-based RL approach that can learn to control systems with minimal violations of the safety constraints. The model is learned on the data collected during the operation of the system in an iterated batch fashion, and is then used to plan for the best action to perform at each time step. We propose three different safe planners, one based on a simple random shooting strategy and two based on MAP-Elites, a more advanced divergent-search algorithm. Experiments show that these planners help the learning agent avoid unsafe situations while maximally exploring the state space, a necessary aspect when learning an accurate model of the system. Furthermore, compared to model-free approaches, learning a model allows GuSS reducing the number of interactions with the real-system while still reaching high rewards, a fundamental requirement when handling engineering systems.      
### 100.Geo-NI: Geometry-aware Neural Interpolation for Light Field Rendering  [ :arrow_down: ](https://arxiv.org/pdf/2206.09736.pdf)
>  In this paper, we present a Geometry-aware Neural Interpolation (Geo-NI) framework for light field rendering. Previous learning-based approaches either rely on the capability of neural networks to perform direct interpolation, which we dubbed Neural Interpolation (NI), or explore scene geometry for novel view synthesis, also known as Depth Image-Based Rendering (DIBR). Instead, we incorporate the ideas behind these two kinds of approaches by launching the NI with a novel DIBR pipeline. Specifically, the proposed Geo-NI first performs NI using input light field sheared by a set of depth hypotheses. Then the DIBR is implemented by assigning the sheared light fields with a novel reconstruction cost volume according to the reconstruction quality under different depth hypotheses. The reconstruction cost is interpreted as a blending weight to render the final output light field by blending the reconstructed light fields along the dimension of depth hypothesis. By combining the superiorities of NI and DIBR, the proposed Geo-NI is able to render views with large disparity with the help of scene geometry while also reconstruct non-Lambertian effect when depth is prone to be ambiguous. Extensive experiments on various datasets demonstrate the superior performance of the proposed geometry-aware light field rendering framework.      
### 101.Semantic Labeling of High Resolution Images Using EfficientUNets and Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2206.09731.pdf)
>  Semantic segmentation necessitates approaches that learn high-level characteristics while dealing with enormous amounts of data. Convolutional neural networks (CNNs) can learn unique and adaptive features to achieve this aim. However, due to the large size and high spatial resolution of remote sensing images, these networks cannot analyze an entire scene efficiently. Recently, deep transformers have proven their capability to record global interactions between different objects in the image. In this paper, we propose a new segmentation model that combines convolutional neural networks with transformers, and show that this mixture of local and global feature extraction techniques provides significant advantages in remote sensing segmentation. In addition, the proposed model includes two fusion layers that are designed to represent multi-modal inputs and output of the network efficiently. The input fusion layer extracts feature maps summarizing the relationship between image content and elevation maps (DSM). The output fusion layer uses a novel multi-task segmentation strategy where class labels are identified using class-specific feature extraction layers and loss functions. Finally, a fast-marching method is used to convert all unidentified class labels to their closest known neighbors. Our results demonstrate that the proposed methodology improves segmentation accuracy compared to state-of-the-art techniques.      
### 102.Asymptotic Nash Equilibrium for the $M$-ary Sequential Adversarial Hypothesis Testing Game  [ :arrow_down: ](https://arxiv.org/pdf/2206.09620.pdf)
>  In this paper, we consider a novel $M$-ary sequential hypothesis testing problem in which an adversary is present and perturbs the distributions of the samples before the decision maker observes them. This problem is formulated as a sequential adversarial hypothesis testing game played between the decision maker and the adversary. This game is a zero-sum and strategic one. We assume the adversary is active under \emph{all} hypotheses and knows the underlying distribution of observed samples. We adopt this framework as it is the worst-case scenario from the perspective of the decision maker. The goal of the decision maker is to minimize the expectation of the stopping time to ensure that the test is as efficient as possible; the adversary's goal is, instead, to maximize the stopping time. We derive a pair of strategies under which the asymptotic Nash equilibrium of the game is attained. We also consider the case in which the adversary is not aware of the underlying hypothesis and hence is constrained to apply the same strategy regardless of which hypothesis is in effect. Numerical results corroborate our theoretical findings.      
### 103.Performance-Oriented Design for Intelligent Reflecting Surface Assisted Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.09578.pdf)
>  To efficiently exploit the massive raw data that is pervading generated at mobile edge networks, federated learning (FL) has emerged as a promising distributed learning technique that was regarded as a substitute for centralized learning operations. By collaboratively training a shared learning model at edge devices, the raw data transmission and storage are bypassed via the local computed parameters/gradients exchange in FL. Hence, FL can overcome high communication latency and privacy issues. While the high dimensionality in iterative updates (millions of parameters/gradients may be included in the model training) still conflicts with the scarcity of communication resources. Over-the-air computation (AirComp) has come into the spotlight recently which profitably leverages the inherent superposition property of wireless channels to perform efficient model aggeration. However, the model aggregation accuracy is still severely damaged by the unfavorable wireless propagation channels. In this paper, we harness the intelligent reflecting surface (IRS) to program the wireless channel, thus acquiring a satisfying learning performance. Specifically, a performance-oriented design scheme that directly minimizes the optimality gap of the loss function is proposed to accelerate the convergence of AirComp based FL. Firstly, we analyze the convergence behavior of the FL procedure. Then, both offline and online design approaches are proposed based on the obtained optimality gap. We adopt the block coordinate descent (BCD) method to tackle the highly-intractable problem. Simulation results demonstrate that such a performance-oriented design strategy can achieve higher test accuracy than the conventional isolated mean square error (MSE) minimization approach in FL.      
### 104.StudioGAN: A Taxonomy and Benchmark of GANs for Image Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2206.09479.pdf)
>  Generative Adversarial Network (GAN) is one of the state-of-the-art generative models for realistic image synthesis. While training and evaluating GAN becomes increasingly important, the current GAN research ecosystem does not provide reliable benchmarks for which the evaluation is conducted consistently and fairly. Furthermore, because there are few validated GAN implementations, researchers devote considerable time to reproducing baselines. We study the taxonomy of GAN approaches and present a new open-source library named StudioGAN. StudioGAN supports 7 GAN architectures, 9 conditioning methods, 4 adversarial losses, 13 regularization modules, 3 differentiable augmentations, 7 evaluation metrics, and 5 evaluation backbones. With our training and evaluation protocol, we present a large-scale benchmark using various datasets (CIFAR10, ImageNet, AFHQv2, FFHQ, and Baby/Papa/Granpa-ImageNet) and 3 different evaluation backbones (InceptionV3, SwAV, and Swin Transformer). Unlike other benchmarks used in the GAN community, we train representative GANs, including BigGAN, StyleGAN2, and StyleGAN3, in a unified training pipeline and quantify generation performance with 7 evaluation metrics. The benchmark evaluates other cutting-edge generative models(e.g., StyleGAN-XL, ADM, MaskGIT, and RQ-Transformer). StudioGAN provides GAN implementations, training, and evaluation scripts with the pre-trained weights. StudioGAN is available at <a class="link-external link-https" href="https://github.com/POSTECH-CVLab/PyTorch-StudioGAN" rel="external noopener nofollow">this https URL</a>.      
### 105.All you need is feedback: Communication with block attention feedback codes  [ :arrow_down: ](https://arxiv.org/pdf/2206.09457.pdf)
>  Deep learning based channel code designs have recently gained interest as an alternative to conventional coding algorithms, particularly for channels for which existing codes do not provide effective solutions. Communication over a feedback channel is one such problem, for which promising results have recently been obtained by employing various deep learning architectures. In this paper, we introduce a novel learning-aided code design for feedback channels, called generalized block attention feedback (GBAF) codes, which i) employs a modular architecture that can be implemented using different neural network architectures; ii) provides order-of-magnitude improvements in the probability of error compared to existing designs; and iii) can transmit at desired code rates.      
### 106.QuDASH: Quantum-inspired rate adaptation approach for DASH video streaming  [ :arrow_down: ](https://arxiv.org/pdf/2206.09427.pdf)
>  Internet traffic is dramatically increasing with the development of network technologies. Within the total traffic, video streaming traffic accounts for a large amount, which reveals the importance to guarantee the quality of content delivery service. Based on the network conditions, adaptive bitrate (ABR) control is utilized as a common technique which can choose the proper bitrate to ensure the video streaming quality. In this paper, a new bitrate control method, QuDASH is proposed by taking advantage of the emerging quantum technology. In QuDASH, the adaptive control model is developed using the quadratic unconstrained binary optimization (QUBO), which aims at increasing the average bitrate and decreasing the video rebuffering events to maximize the user quality of experience (QoE). Then, the control model is solved by Digital Annealer, which is a quantum-Inspired computing technology. The evaluation of the proposed method is carried out by simulation with the measured throughput traces in real world. Experiment results demonstrated that the proposed QuDASH method has better performance in terms of QoE compared with other advanced ABR methods. In 68.2% of the examined cases, QuDASH achieves the highest QoE results, which shows the superiority of the QuDASH over conventional methods.      
### 107.Extended field-of-view speckle-correlation imaging by estimating autocorrelation  [ :arrow_down: ](https://arxiv.org/pdf/2206.09417.pdf)
>  Imaging through scattering media is a longstanding issue in a wide range of applications, including biomedicine, security, and astronomy. Speckle-correlation imaging is promising for non-invasively seeing through scattering media by assuming shift-invariance of the scattering process called the memory effect. However, the memory effect is known to be severely limited when the medium is thick. Under such a scattering condition, speckle-correlation imaging is not practical because the correlation of the speckle decays, reducing the field of view. To address this problem, we present a method for expanding the field of view of single-shot speckle-correlation imaging through scattering media with a limited memory effect. We derive the imaging model under this scattering condition and its inversion for reconstructing the object. Our method simultaneously estimates both the object and the decay of the speckle correlation based on the gradient descent method. We numerically and experimentally demonstrate the proposed method by reconstructing point sources behind scattering media with a limited memory effect. In the demonstrations, our speckle-correlation imaging method with a minimal lensless optical setup realized a larger field of view compared with the conventional one. This study will make techniques for imaging through scattering media more practical in various fields.      
### 108.JPEG Compression-Resistant Low-Mid Adversarial Perturbation against Unauthorized Face Recognition System  [ :arrow_down: ](https://arxiv.org/pdf/2206.09410.pdf)
>  It has been observed that the unauthorized use of face recognition system raises privacy problems. Using adversarial perturbations provides one possible solution to address this issue. A critical issue to exploit adversarial perturbation against unauthorized face recognition system is that: The images uploaded to the web need to be processed by JPEG compression, which weakens the effectiveness of adversarial perturbation. Existing JPEG compression-resistant methods fails to achieve a balance among compression resistance, transferability, and attack effectiveness. To this end, we propose a more natural solution called low frequency adversarial perturbation (LFAP). Instead of restricting the adversarial perturbations, we turn to regularize the source model to employing more low-frequency features by adversarial training. Moreover, to better influence model in different frequency components, we proposed the refined low-mid frequency adversarial perturbation (LMFAP) considering the mid frequency components as the productive complement. We designed a variety of settings in this study to simulate the real-world application scenario, including cross backbones, supervisory heads, training datasets and testing datasets. Quantitative and qualitative experimental results validate the effectivenss of proposed solutions.      
### 109.Channel Estimation for Delay Alignment Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2206.09339.pdf)
>  Delay alignment modulation (DAM) is a promising technology to eliminate inter-symbol interference (ISI) without relying on sophisticated equalization or multi-carrier transmissions. The key ideas of DAM are delay pre-compensation and path based beamforming, so that the multi-path signal components will arrive at the receiver simultaneously and constructively, rather than causing the detrimental ISI. However, the practical implementation of DAM requires channel state information (CSI) at the transmitter side. Therefore, in this letter, we propose an efficient channel estimation method for DAM based on block orthogonal matching pursuit (BOMP) algorithm, by exploiting the block sparsity of the channel impulse response (CIR) vector. Based on the imperfectly estimated CSI, the delay pre-compensations and path-based beamforming are designed for DAM, and the resulting performance is studied. Simulation results demonstrate that with the proposed channel estimation method, the CSI can be effectively acquired with low training overhead, and the performance of DAM based on estimated CSI is comparable to the ideal case with perfect CSI.      
### 110.Toward Agile and Robust Supply Chains: A Lesson from Stochastic Job-Shop Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2206.09326.pdf)
>  Motivated by the presence of uncertainties as well as combinatorial complexity within the links of supply chains, this paper addresses the outstanding and timely challenge illustrated through a case study of stochastic job-shop scheduling problems arising within low-volume high-variety manufacturing. These problems have been classically formulated as integer linear programs (ILPs), which are known to be NP-hard, and are computationally intractable. Yet, optimal or near-optimal solutions must be obtained within strict computational time requirements. While the deterministic cases have been efficiently solved by state-of-the-art methods such as branch-and-cut (B&amp;C), uncertainties may compromise the entire schedule thereby potentially affecting the entire supply chain downstream, thus, uncertainties must be explicitly captured to ensure the feasibility of operations. The stochastic nature of the resulting problem adds a layer of computational difficulty on top of an already intractable problem, as evidenced by the presented case studies with some cases taking hours without being able to find a "near-optimal" schedule. To efficiently solve the stochastic JSS problem, a recent Surrogate "Level-Based" Lagrangian Relaxation is used to reduce computational effort while efficiently exploiting geometric convergence potential inherent to Polyak's step-sizing formula thereby leading to fast convergence. Computational results demonstrate that the new method is more than two orders of magnitude faster compared to B&amp;C. Moreover, insights based on a small intuitive example are provided through simulations demonstrating an advantage of scholastic scheduling.      
### 111.Delay-aware Multiple Access Design for Intelligent Reflecting Surface Aided Uplink Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2206.09302.pdf)
>  In this paper, we develop a novel multiple access (MA) protocol for an intelligent reflecting (IRS) aided uplink transmission network by incorporating the IRS-aided time-division MA (I-TDMA) protocol and the IRS-aided non-orthogonal MA protocol (I-NOMA) protocol as special cases. Two typical communication scenarios, namely the transmit power limited case and the transmit energy limited case are considered, where the device's rearranged order, time and power allocation, as well as dynamic IRS beamforming patterns over time are jointly optimized to minimize the sum transmission delay. To shed light on the superiority of the proposed IRS-aided hybrid MA (I-HMA) protocol over conventional protocols, the conditions under which I-HMA outperforms I-TDMA and I-NOMA are revealed by characterizing their corresponding optimal solution. Then, a computationally efficient algorithm is proposed to obtain the high-quality solution to the corresponding optimization problems. Simulation results validate our theoretical findings, demonstrate the superiority of the proposed design, and draw some useful insights. Specifically, it is found that the proposed protocol can significantly reduce the sum delay by combining the additional gain of dynamic IRS beamforming with the high spectral efficiency of NOMA, which thus reveals that integrating IRS into the proposed HMA protocol is an effective solution for delay-aware optimization. Furthermore, it reveals that the proposed design reduces the time consumption not only from the system-centric view, but also from the device-centric view.      
### 112.GMM based multi-stage Wiener filtering for low SNR speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2206.09298.pdf)
>  This paper proposes a single-channel speech enhancement method to reduce the noise and enhance speech at low signal-to-noise ratio (SNR) levels and non-stationary noise conditions. Specifically, we focus on modeling the noise using a Gaussian mixture model (GMM) based on a multi-stage process with a parametric Wiener filter. The proposed noise model estimates a more accurate noise power spectral density (PSD), and allows for better generalization under various noise conditions compared to traditional Wiener filtering methods. Simulations show that the proposed approach can achieve better performance in terms of speech quality (PESQ) and intelligibility (STOI) at low SNR levels.      
### 113.Q-linear Convergence of Distributed Optimization with Barzilai-Borwein Step Sizes  [ :arrow_down: ](https://arxiv.org/pdf/2206.09285.pdf)
>  The growth in sizes of large-scale systems and data in machine learning have made distributed optimization a naturally appealing technique to solve decision problems in different contexts. In such methods, each agent iteratively carries out computations on its local objective using information received from its neighbors, and shares relevant information with neighboring agents. Though gradient-based methods are widely used because of their simplicity, they are known to have slow convergence rates. On the other hand, though Newton-type methods have better convergence properties, they are not as applicable because of the enormous computation and memory requirements. In this work, we introduce a distributed quasi-Newton method with Barzilai-Borwein step-sizes. We prove a Q-linear convergence to the optimal solution, present conditions under which the algorithm is superlinearly convergent and validate our results via numerical simulations.      
### 114.Structured Light with Redundancy Codes  [ :arrow_down: ](https://arxiv.org/pdf/2206.09243.pdf)
>  Structured light (SL) systems acquire high-fidelity 3D geometry with active illumination projection. Conventional systems exhibit challenges when working in environments with strong ambient illumination, global illumination and cross-device interference. This paper proposes a general-purposed technique to improve the robustness of SL by projecting redundant optical signals in addition to the native SL patterns. In this way, projected signals become more distinguishable from errors. Thus the geometry information can be more easily recovered using simple signal processing and the ``coding gain" in performance is obtained. We propose three applications using our redundancy codes: (1) Self error-correction for SL imaging under strong ambient light, (2) Error detection for adaptive reconstruction under global illumination, and (3) Interference filtering with device-specific projection sequence encoding, especially for event camera-based SL and light curtain devices. We systematically analyze the design rules and signal processing algorithms in these applications. Corresponding hardware prototypes are built for evaluations on real-world complex scenes. Experimental results on the synthetic and real data demonstrate the significant performance improvements in SL systems with our redundancy codes.      
### 115.Seismic Wavefield Reconstruction based on Compressed Sensing using Data-Driven Reduced-Order Model  [ :arrow_down: ](https://arxiv.org/pdf/2206.09231.pdf)
>  A seismic wavefield reconstruction framework based on compressed sensing using the data-driven reduced-order model (ROM) is proposed and its characteristics are investigated through numerical experiments. The data-driven ROM is generated from the dataset of the wavefield using the singular value decomposition. The spatially continuous seismic wavefield is reconstructed from the sparse and discrete observation and the data-driven ROM. The observation sites used for reconstruction are effectively selected by the sensor optimization method for linear inverse problems based on a greedy algorithm. The validity of the proposed method was confirmed by the reconstruction based on the noise-free observation. Since the ROM of the wavefield is used as prior information, the reconstruction error is reduced to an approximately lower error bound of the present framework, even though the number of sensors used for reconstruction is limited and randomly selected. In addition, the reconstruction error obtained by the proposed framework is much smaller than that obtained by the Gaussian process regression. For the numerical experiment with noise-contaminated observation, the reconstructed wavefield is degraded due to the observation noise, but the reconstruction error obtained by the present framework with all available observation sites is close to a lower error bound, even though the reconstructed wavefield using the Gaussian process regression is fully collapsed. Although the reconstruction error is larger than that obtained using all observation sites, the number of observation sites used for reconstruction can be reduced while minimizing the deterioration and scatter of the reconstructed data by combining it with the sensor optimization method.      
### 116.Bioinspired random projections for robust, sparse classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.09222.pdf)
>  Inspired by the use of random projections in biological sensing systems, we present a new algorithm for processing data in classification problems. This is based on observations of the human brain and the fruit fly's olfactory system and involves randomly projecting data into a space of greatly increased dimension before applying a cap operation to truncate the smaller entries. This leads to an algorithm that achieves a sparse representation with minimal loss in classification accuracy and is also more robust in the sense that classification accuracy is improved when noise is added to the data. This is demonstrated with numerical experiments, which supplement theoretical results demonstrating that the resulting signal transform is continuous and invertible, in an appropriate sense.      
### 117.The Frenet Frame as a Generalization of the Park Transform  [ :arrow_down: ](https://arxiv.org/pdf/2206.09209.pdf)
>  The paper proposes a generalization of the Park transform based on the Frenet frame, which is a special set of coordinates defined in differential geometry for space curves. The proposed geometric transform is first discussed for three dimensions, which correspond to the common three-phase circuits. Then, the expression of the time derivative of the proposed transform is discussed and the Frenet-Serret formulas and the Darboux vector are introduced. The change of reference frame and its differentiation based on Cartan's moving frames and attitude matrices are also described. Finally, the extension to circuits with more than three phases is presented. The features of the Frenet frame are illustrated through a variety of examples, including a case study based on the IEEE 39-bus system.      
### 118.Off-Network Communications For Future Railway Mobile Communication Systems: Challenges and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2206.09157.pdf)
>  GSM-R is predicted to be obsoleted by 2030, and a suitable successor is needed. Defined by the International Union of Railways (UIC), the Future Railway Mobile Communication System (FRMCS) contains many future use cases with strict requirements. These use cases should ensure regular communication not only in network coverage but also uncovered scenarios. There is still a lack of standards on off-network communication in FRMCS, so this article focuses on off-network communication and intends to provide reference and direction for standardization. We first provide a comprehensive summary and analysis of off-network use cases in FRMCS. Then we give an overview of existing technologies (GSM-R, TETRA, DMR, LTE-V2X, and NR-V2X) that may support off-network communication. In addition, we simulate and evaluate the performance of existing technologies. Simulation results show that it is possible to satisfy the off-network communication requirements in FRMCS with enhancements based on LTE-V2X or NR-V2X. Finally, we give some future research directions to provide insights for industry and academia.      
### 119.Redundancy Reduction Twins Network: A Training framework for Multi-output Emotion Regression  [ :arrow_down: ](https://arxiv.org/pdf/2206.09142.pdf)
>  In this paper, we propose the Redundancy Reduction Twins Network (RRTN), a redundancy reduction training framework that minimizes redundancy by measuring the cross-correlation matrix between the outputs of the same network fed with distorted versions of a sample and bringing it as close to the identity matrix as possible. RRTN also applies a new loss function, the Barlow Twins loss function, to help maximize the similarity of representations obtained from different distorted versions of a sample. However, as the distribution of losses can cause performance fluctuations in the network, we also propose the use of a Restrained Uncertainty Weight Loss (RUWL) or joint training to identify the best weights for the loss function. Our best approach on CNN14 with the proposed methodology obtains a CCC over emotion regression of 0.678 on the ExVo Multi-task dev set, a 4.8% increase over a vanilla CNN 14 CCC of 0.647, which achieves a significant difference at the 95% confidence interval (2-tailed).      
### 120.Efficacy of Asynchronous GPS Spoofing Against High Volume Consumer GNSS Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2206.09133.pdf)
>  The vulnerability of the Global Positioning System (GPS) against spoofing is known for quite some time. Also, the positioning and navigation of most semi-autonomous and autonomous drones are dependent on Global Navigation Satellite System (GNSS) signals. In prior work, simplistic or asynchronous GPS spoofing was found to be a simple, efficient, and effective cyber attack against L1 GPS or GNSS dependent commercial drones. In this paper, first we make some important observations on asynchronous GPS spoofing attacks on drones presented in prior research literature. Then, we design an asynchronous GPS spoofing attack plan. Next, we test the effectiveness of this attack against GNSS receivers (high volume consumer devices based on Android mobile phones) of different capabilities and a commercial drone (DJI Mavic 2 Pro) under various conditions. Finally, we present several novel insights based on the results of the tests.      
### 121.Tackling Spoofing-Aware Speaker Verification with Multi-Model Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2206.09131.pdf)
>  Recent years have witnessed the extraordinary development of automatic speaker verification (ASV). However, previous works show that state-of-the-art ASV models are seriously vulnerable to voice spoofing attacks, and the recently proposed high-performance spoofing countermeasure (CM) models only focus solely on the standalone anti-spoofing tasks, and ignore the subsequent speaker verification process. How to integrate the CM and ASV together remains an open question. A spoofing aware speaker verification (SASV) challenge has recently taken place with the argument that better performance can be delivered when both CM and ASV subsystems are optimized jointly. Under the challenge's scenario, the integrated systems proposed by the participants are required to reject both impostor speakers and spoofing attacks from target speakers, which intuitively and effectively matches the expectation of a reliable, spoofing-robust ASV system. This work focuses on fusion-based SASV solutions and proposes a multi-model fusion framework to leverage the power of multiple state-of-the-art ASV and CM models. The proposed framework vastly improves the SASV-EER from 8.75% to 1.17\%, which is 86% relative improvement compared to the best baseline system in the SASV challenge.      
### 122.Quantifying the value of transient voltage sources  [ :arrow_down: ](https://arxiv.org/pdf/2206.09126.pdf)
>  Some voltage sources are transient, lasting only for a moment of time, such as the voltage generated by converting a human motion into electricity. Such sources moreover tend to have a degree of randomness as well as internal resistance. We investigate how to put a number to how valuable a given transient source is. We derive several candidate measures via a systematic approach. We establish an inter-convertibility hierarchy between such sources, where inter-conversion means adding passive interface circuits to the sources. Resistors at the ambient temperature are at the bottom of this hierarchy and sources with low internal resistance and high internal voltages are at the top. We provide three possible measures for a given source that assign a number to the source respecting this hierarchy. One measure captures how much "UnitDC" the source contains, meaning 1V DC with 1$\Omega$ internal resistance for 1s. Another measure relates to the signal-to-noise ratio of the voltage time-series whereas a third is based on the relative entropy between the voltage probability distribution and a thermal noise resistor. We argue that the UnitDc measure is particularly useful by virtue of its operational interpretation in terms of the number of UnitDc sources that one needs to combine to create the source or that can be distilled from the source.      
### 123.Fast and Provable Tensor Robust Principal Component Analysis via Scaled Gradient Descent  [ :arrow_down: ](https://arxiv.org/pdf/2206.09109.pdf)
>  An increasing number of data science and machine learning problems rely on computation with tensors, which better capture the multi-way relationships and interactions of data than matrices. When tapping into this critical advantage, a key challenge is to develop computationally efficient and provably correct algorithms for extracting useful information from tensor data that are simultaneously robust to corruptions and ill-conditioning. This paper tackles tensor robust principal component analysis (RPCA), which aims to recover a low-rank tensor from its observations contaminated by sparse corruptions, under the Tucker decomposition. To minimize the computation and memory footprints, we propose to directly recover the low-dimensional tensor factors -- starting from a tailored spectral initialization -- via scaled gradient descent (ScaledGD), coupled with an iteration-varying thresholding operation to adaptively remove the impact of corruptions. Theoretically, we establish that the proposed algorithm converges linearly to the true low-rank tensor at a constant rate that is independent with its condition number, as long as the level of corruptions is not too large. Empirically, we demonstrate that the proposed algorithm achieves better and more scalable performance than state-of-the-art matrix and tensor RPCA algorithms through synthetic experiments and real-world applications.      
### 124.Weakly Supervised Classification of Vital Sign Alerts as Real or Artifact  [ :arrow_down: ](https://arxiv.org/pdf/2206.09074.pdf)
>  A significant proportion of clinical physiologic monitoring alarms are false. This often leads to alarm fatigue in clinical personnel, inevitably compromising patient safety. To combat this issue, researchers have attempted to build Machine Learning (ML) models capable of accurately adjudicating Vital Sign (VS) alerts raised at the bedside of hemodynamically monitored patients as real or artifact. Previous studies have utilized supervised ML techniques that require substantial amounts of hand-labeled data. However, manually harvesting such data can be costly, time-consuming, and mundane, and is a key factor limiting the widespread adoption of ML in healthcare (HC). Instead, we explore the use of multiple, individually imperfect heuristics to automatically assign probabilistic labels to unlabeled training data using weak supervision. Our weakly supervised models perform competitively with traditional supervised techniques and require less involvement from domain experts, demonstrating their use as efficient and practical alternatives to supervised learning in HC applications of ML.      
### 125.Analysis &amp; Computational Complexity Reduction of Monocular and Stereo Depth Estimation Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2206.09071.pdf)
>  Accurate depth estimation with lowest compute and energy cost is a crucial requirement for unmanned and battery operated autonomous systems. Robotic applications require real time depth estimation for navigation and decision making under rapidly changing 3D surroundings. A high accuracy algorithm may provide the best depth estimation but may consume tremendous compute and energy resources. A general trade-off is to choose less accurate methods for initial depth estimate and a more accurate yet compute intensive method when needed. Previous work has shown this trade-off can be improved by developing a state-of-the-art method (AnyNet) to improve stereo depth estimation. <br>We studied both the monocular and stereo vision depth estimation methods and investigated methods to reduce computational complexity of these methods. This was our baseline. Consequently, our experiments show reduction of monocular depth estimation model size by ~75% reduces accuracy by less than 2% (SSIM metric). Our experiments with the novel stereo vision method (AnyNet) show that accuracy of depth estimation does not degrade more than 3% (three pixel error metric) in spite of reduction in model size by ~20%. We have shown that smaller models can indeed perform competitively.      
### 126.Intelligent Blockchain-based Edge Computing via Deep Reinforcement Learning: Solutions and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2206.09009.pdf)
>  The convergence of mobile edge computing (MEC) and blockchain is transforming the current computing services in wireless Internet-of-Things networks, by enabling task offloading with security enhancement based on blockchain mining. Yet the existing approaches for these enabling technologies are isolated, providing only tailored solutions for specific services and scenarios. To fill this gap, we propose a novel cooperative task offloading and blockchain mining (TOBM) scheme for a blockchain-based MEC system, where each edge device not only handles computation tasks but also deals with block mining for improving system utility. To address the latency issues caused by the blockchain operation in MEC, we develop a new Proof-of-Reputation consensus mechanism based on a lightweight block verification strategy. To accommodate the highly dynamic environment and high-dimensional system state space, we apply a novel distributed deep reinforcement learning-based approach by using a multi-agent deep deterministic policy gradient algorithm. Experimental results demonstrate the superior performance of the proposed TOBM scheme in terms of enhanced system reward, improved offloading utility with lower blockchain mining latency, and better system utility, compared to the existing cooperative and non-cooperative schemes. The paper concludes with key technical challenges and possible directions for future blockchain-based MEC research.      
