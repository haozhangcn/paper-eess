# ArXiv eess --Mon, 13 Jun 2022
### 1.Multi-faceted Graph Attention Network for Radar Target Recognition in Heterogeneous Radar Network  [ :arrow_down: ](https://arxiv.org/pdf/2206.05168.pdf)
>  Radar target recognition (RTR), as a key technology of intelligent radar systems, has been well investigated. Accurate RTR at low signal-to-noise ratios (SNRs) still remains an open challenge. Most existing methods are based on a single radar or the homogeneous radar network, which do not fully exploit frequency-dimensional information. In this paper, a two-stream semantic feature fusion model, termed Multi-faceted Graph Attention Network (MF-GAT), is proposed to greatly improve the accuracy in the low SNR region of the heterogeneous radar network. By fusing the features extracted from the source domain and transform domain via a graph attention network model, the MF-GAT model distills higher-level semantic features before classification in a unified framework. Extensive experiments are presented to demonstrate that the proposed model can greatly improve the RTR performance at low SNRs.      
### 2.Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.05148.pdf)
>  Deep learning models have shown their potential for several applications. However, most of the models are opaque and difficult to trust due to their complex reasoning - commonly known as the black-box problem. Some fields, such as medicine, require a high degree of transparency to accept and adopt such technologies. Consequently, creating explainable/interpretable models or applying post-hoc methods on classifiers to build trust in deep learning models are required. Moreover, deep learning methods can be used for segmentation tasks, which typically require hard-to-obtain, time-consuming manually-annotated segmentation labels for training. This paper introduces three inherently-explainable classifiers to tackle both of these problems as one. The localisation heatmaps provided by the networks -- representing the models' focus areas and being used in classification decision-making -- can be directly interpreted, without requiring any post-hoc methods to derive information for model explanation. The models are trained by using the input image and only the classification labels as ground-truth in a supervised fashion - without using any information about the location of the region of interest (i.e. the segmentation labels), making the segmentation training of the models weakly-supervised through classification labels. The final segmentation is obtained by thresholding these heatmaps. The models were employed for the task of multi-class brain tumour classification using two different datasets, resulting in the best F1-score of 0.93 for the supervised classification task while securing a median Dice score of 0.67$\pm$0.08 for the weakly-supervised segmentation task. Furthermore, the obtained accuracy on a subset of tumour-only images outperformed the state-of-the-art glioma tumour grading binary classifiers with the best model achieving 98.7\% accuracy.      
### 3.A Comprehensive Review on Power System Risk-Based Transient Stability  [ :arrow_down: ](https://arxiv.org/pdf/2206.05113.pdf)
>  Power systems are getting more complex than ever and are consequently operating close to their limit of stability. Moreover, with the increasing demand of renewable wind generation, and the requirement to maintain a secure power system, the importance of transient stability cannot be overestimated. Considering its significance in power system security, it is important to suggest a different methodology for enhancing the transient stability, considering uncertainties. Current deterministic industry practices of transient stability assessment ignore the probabilistic nature of variables (fault type, fault location, fault clearing time, etc.). These approaches typically provide a cautious principle and can result in high-priced expansion projects or operational limits. With the increasing system uncertainties and widespread electricity market deregulation, there is a strong inevitability to incorporate risk in the traditional transient stability analysis. Accurate assessment of transient stability in a modern power network is becoming a strict requirement both in planning and in real-time operation, due to the increasingly intricate dynamics of a power system. Further, increasing sources of uncertainty in forecast state and in the reaction to faults highly implies the implementation of risk-based approach in assessing transient stability. Thus, this paper aims to provide a comprehensive review of risk-based transient stability in power networks and the accompanying research. It is believed that this review can be an inception for researchers in the domain of power system planning and security.      
### 4.The Z3RO Family of Precoders Cancelling Nonlinear Power Amplification Distortion in Large Array Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.05112.pdf)
>  Large array systems use a massive number of antenna elements and clever precoder designs to achieve an array gain at the user location. These precoders require linear front-ends, and more specifically linear power amplifiers (PAs), to avoid distortion. This reduces the energy efficiency since PAs are most efficient close to saturation, where they generate most nonlinear distortion. Moreover, the use of conventional precoders can induce a coherent combining of distortion at the user locations, degrading the signal quality. In this work, novel linear precoders, simple to compute and to implement, are proposed that allow working close to saturation, while cancelling the third-order nonlinearity of the PA without prior knowledge of the signal statistics and PA model. Their design consists in saturating a single or a few antennas on purpose together with an negative gain with respect to all other antennas to compensate for the overall nonlinear distortion at the user location. The performance gains of the designs are significant for PAs working close to saturation, as compared to maximum ratio transmission (MRT) precoding and perfect per-antenna digital pre-distortion (DPD) compensation.      
### 5.Learning self-calibrated optic disc and cup segmentation from multi-rater annotations  [ :arrow_down: ](https://arxiv.org/pdf/2206.05092.pdf)
>  The segmentation of optic disc(OD) and optic cup(OC) from fundus images is an important fundamental task for glaucoma diagnosis. In the clinical practice, it is often necessary to collect opinions from multiple experts to obtain the final OD/OC annotation. This clinical routine helps to mitigate the individual bias. But when data is multiply annotated, standard deep learning models will be inapplicable. In this paper, we propose a novel neural network framework to learn OD/OC segmentation from multi-rater annotations. The segmentation results are self-calibrated through the iterative optimization of multi-rater expertness estimation and calibrated OD/OC segmentation. In this way, the proposed method can realize a mutual improvement of both tasks and finally obtain a refined segmentation result. Specifically, we propose Diverging Model(DivM) and Converging Model(ConM) to process the two tasks respectively. ConM segments the raw image based on the multi-rater expertness map provided by DivM. DivM generates multi-rater expertness map from the segmentation mask provided by ConM. The experiment results show that by recurrently running ConM and DivM, the results can be self-calibrated so as to outperform a range of state-of-the-art(SOTA) multi-rater segmentation methods.      
### 6.Policy Gradient Reinforcement Learning for Uncertain Polytopic LPV Systems based on MHE-MPC  [ :arrow_down: ](https://arxiv.org/pdf/2206.05089.pdf)
>  In this paper, we propose a learning-based Model Predictive Control (MPC) approach for the polytopic Linear Parameter-Varying (LPV) systems with inexact scheduling parameters (as exogenous signals with inexact bounds), where the Linear Time Invariant (LTI) models (vertices) captured by combinations of the scheduling parameters becomes wrong. We first propose to adopt a Moving Horizon Estimation (MHE) scheme to simultaneously estimate the convex combination vector and unmeasured states based on the observations and model matching error. To tackle the wrong LTI models used in both the MPC and MHE schemes, we then adopt a Policy Gradient (PG) Reinforcement Learning (RL) to learn both the estimator (MHE) and controller (MPC) so that the best closed-loop performance is achieved. The effectiveness of the proposed RL-based MHE/MPC design is demonstrated using an illustrative example.      
### 7.A No-reference Quality Assessment Metric for Point Cloud Based on Captured Video Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2206.05054.pdf)
>  Point cloud is one of the most widely used digital formats of 3D models, the visual quality of which is quite sensitive to distortions such as downsampling, noise, and compression. To tackle the challenge of point cloud quality assessment (PCQA) in scenarios where reference is not available, we propose a no-reference quality assessment metric for colored point cloud based on captured video sequences. Specifically, three video sequences are obtained by rotating the camera around the point cloud through three specific orbits. The video sequences not only contain the static views but also include the multi-frame temporal information, which greatly helps understand the human perception of the point clouds. Then we modify the ResNet3D as the feature extraction model to learn the correlation between the capture videos and corresponding subjective quality scores. The experimental results show that our method outperforms most of the state-of-the-art full-reference and no-reference PCQA metrics, which validates the effectiveness of the proposed method.      
### 8.Denoising Generalized Expectation-Consistent Approximation for MRI Image Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2206.05049.pdf)
>  To solve inverse problems, plug-and-play (PnP) methods have been developed that replace the proximal step in a convex optimization algorithm with a call to an application-specific denoiser, often implemented using a deep neural network (DNN). Although such methods have been successful, they can be improved. For example, denoisers are usually designed/trained to remove white Gaussian noise, but the denoiser input error in PnP algorithms is usually far from white or Gaussian. Approximate message passing (AMP) methods provide white and Gaussian denoiser input error, but only when the forward operator is a large random matrix. In this work, for Fourier-based forward operators, we propose a PnP algorithm based on generalized expectation-consistent (GEC) approximation -- a close cousin of AMP -- that offers predictable error statistics at each iteration, as well as a new DNN denoiser that leverages those statistics. We apply our approach to magnetic resonance imaging (MRI) image recovery and demonstrate its advantages over existing PnP and AMP methods.      
### 9.Equilibrium and stiffness study of clustered tensegrity structures with the consideration of pulley sizes  [ :arrow_down: ](https://arxiv.org/pdf/2206.05048.pdf)
>  This paper presents the equilibrium and stiffness study of clustered tensegrity structures (CTS) considering pulley sizes. We first derive the geometric relationship between clustered strings and pulleys, where the nodal vector is chosen as the generalized coordinate. Then, the equilibrium equations of the clustered tensegrity structure with pulleys based on the Lagrangian method are given. Since the stiffness of a structure is usually weakened when using clustering strings, we formulate the tangent stiffness matrix equations for analysis. It is also shown that as pulley sizes go to zero, the governing equations of the clustered tensegrity system with pulleys yield to the classical clustered tensegrity structure without pulleys, which is consistent with the existing literature. Three examples are demonstrated to validate the given theory. The proposed method allows one to conduct equilibrium, stiffness, and robustness studies of cluster tensegrity structures with pulleys. Nevertheless, the approach developed in this paper is not limited to the tensegrity structures. It can also be applied to a wide range of applications with pulley-rope systems, such as drilling rigs, ocean platform anchors, and cargo cranes.      
### 10.A GPU-Accelerated Light-field Super-resolution Framework Based on Mixed Noise Model and Weighted Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2206.05047.pdf)
>  This paper presents a GPU-accelerated computational framework for reconstructing high resolution (HR) LF images under a mixed Gaussian-Impulse noise condition. The main focus is on developing a high-performance approach considering processing speed and reconstruction quality. From a statistical perspective, we derive a joint $\ell^1$-$\ell^2$ data fidelity term for penalizing the HR reconstruction error taking into account the mixed noise situation. For regularization, we employ the weighted non-local total variation approach, which allows us to effectively realize LF image prior through a proper weighting scheme. We show that the alternating direction method of multipliers algorithm (ADMM) can be used to simplify the computation complexity and results in a high-performance parallel computation on the GPU Platform. An extensive experiment is conducted on both synthetic 4D LF dataset and natural image dataset to validate the proposed SR model's robustness and evaluate the accelerated optimizer's performance. The experimental results show that our approach achieves better reconstruction quality under severe mixed-noise conditions as compared to the state-of-the-art approaches. In addition, the proposed approach overcomes the limitation of the previous work in handling large-scale SR tasks. While fitting within a single off-the-shelf GPU, the proposed accelerator provides an average speedup of 2.46$\times$ and 1.57$\times$ for $\times 2$ and $\times 3$ SR tasks, respectively. In addition, a speedup of $77\times$ is achieved as compared to CPU execution.      
### 11.Deep Learning-based Massive MIMO CSI Acquisition for 5G Evolution and 6G  [ :arrow_down: ](https://arxiv.org/pdf/2206.04967.pdf)
>  Recently, inspired by successful applications in many fields, deep learning (DL) technologies for CSI acquisition have received considerable research interest from both academia and industry. Considering the practical feedback mechanism of 5th generation (5G) New radio (NR) networks, we propose two implementation schemes for artificial intelligence for CSI (AI4CSI), the DL-based receiver and end-to-end design, respectively. The proposed AI4CSI schemes were evaluated in 5G NR networks in terms of spectrum efficiency (SE), feedback overhead, and computational complexity, and compared with legacy schemes. To demonstrate whether these schemes can be used in real-life scenarios, both the modeled-based channel data and practically measured channels were used in our investigations. When DL-based CSI acquisition is applied to the receiver only, which has little air interface impact, it provides approximately 25\% SE gain at a moderate feedback overhead level. It is feasible to deploy it in current 5G networks during 5G evolutions. For the end-to-end DL-based CSI enhancements, the evaluations also demonstrated their additional performance gain on SE, which is 6% -- 26% compared with DL-based receivers and 33% -- 58% compared with legacy CSI schemes. Considering its large impact on air-interface design, it will be a candidate technology for 6th generation (6G) networks, in which an air interface designed by artificial intelligence can be used.      
### 12.A Holistic Robust Motion Controller Framework for Autonomous Platooning  [ :arrow_down: ](https://arxiv.org/pdf/2206.04948.pdf)
>  Safety is the foremost concern for autonomous platooning. The vehicle-to-vehicle (V2V) communication delay and the sudden appearance of obstacles will trigger the safety of the intended functionality (SOTIF) issues for autonomous platooning. This research proposes a holistic robust motion controller framework (MCF) for an intelligent and connected vehicle platoon system. The MCF utilizes a hierarchical structure to resolve the longitudinal string stability and the lateral control problem under the complex driving environment and time-varying communication delay. Firstly, the H-infinity feedback controller is developed to ensure the robustness of the platoon under time-varying communication delay in the upper-level coordination layer (UCL). The output from UCL will be delivered to the lower-level motion-planning layer (LML) as reference signals. Secondly, the model predictive control (MPC) algorithm is implemented in the LML to achieve multi-objective control, which comprehensively considers the reference signals, the artificial potential field, and multiple vehicle dynamics constraints. Furthermore, three critical scenarios are co-simulated for case studies, including platooning under time-varying communication delay, merging, and obstacle avoidance scenarios. The simulation results indicate that, compared with single-structure MPC, the proposed MCF can offer a better suppression on position error propagation, and get improvements on maximum position error in the three scenarios by $19.2\%$, $59.8\%$, and $15.3\%$, respectively. Last, the practicability and effectiveness of the proposed MCF are verified via hardware-in-the-loop experiment. The average conducting time of the proposed method on Speedgoat real-time target machine is 1.1 milliseconds, which meets the real-time requirements.      
### 13.Efficient Per-Shot Convex Hull Prediction By Recurrent Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.04877.pdf)
>  Adaptive video streaming relies on the construction of efficient bitrate ladders to deliver the best possible visual quality to viewers under bandwidth constraints. The traditional method of content dependent bitrate ladder selection requires a video shot to be pre-encoded with multiple encoding parameters to find the optimal operating points given by the convex hull of the resulting rate-quality curves. However, this pre-encoding step is equivalent to an exhaustive search process over the space of possible encoding parameters, which causes significant overhead in terms of both computation and time expenditure. To reduce this overhead, we propose a deep learning based method of content aware convex hull prediction. We employ a recurrent convolutional network (RCN) to implicitly analyze the spatiotemporal complexity of video shots in order to predict their convex hulls. A two-step transfer learning scheme is adopted to train our proposed RCN-Hull model, which ensures sufficient content diversity to analyze scene complexity, while also making it possible capture the scene statistics of pristine source videos. Our experimental results reveal that our proposed model yields better approximations of the optimal convex hulls, and offers competitive time savings as compared to existing approaches. On average, the pre-encoding time was reduced by 58.0% by our method, while the average Bjontegaard delta bitrate (BD-rate) of the predicted convex hulls against ground truth was 0.08%, while the mean absolute deviation of the BD-rate distribution was 0.44%      
### 14.Feature-informed Embedding Space Regularization For Audio Classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.04850.pdf)
>  Feature representations derived from models pre-trained on large-scale datasets have shown their generalizability on a variety of audio analysis tasks. Despite this generalizability, however, task-specific features can outperform if sufficient training data is available, as specific task-relevant properties can be learned. Furthermore, the complex pre-trained models bring considerable computational burdens during inference. We propose to leverage both detailed task-specific features from spectrogram input and generic pre-trained features by introducing two regularization methods that integrate the information of both feature classes. The workload is kept low during inference as the pre-trained features are only necessary for training. In experiments with the pre-trained features VGGish, OpenL3, and a combination of both, we show that the proposed methods not only outperform baseline methods, but also can improve state-of-the-art models on several audio classification tasks. The results also suggest that using the mixture of features performs better than using individual features.      
### 15.Optimization for Infrastructure Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.04794.pdf)
>  Cyber-physical systems (CPS) are systems where a decision making (cyber/control) component is tightly integrated with a physical system (with sensing/actuation) to enable real-time monitoring and control. Recently, there has been significant research effort in viewing and optimizing physical infrastructure in built environments as CPS, even if the control action is not in real-time. Some examples of infrastructure CPS include electrical power grids; water distribution networks; transportation and logistics networks; heating, ventilation, and air conditioning (HVAC) in buildings; etc. Complexity arises in infrastructure CPS from the large scale of operations; heterogeneity of system components; dynamic and uncertain operating conditions; and goal-driven decision making and control with time-bounded task completion guarantees. For control optimization, an infrastructure CPS is typically viewed as a system of semi-autonomous sub-systems with a network of sensors and uses distributed control optimization to achieve system-wide objectives that are typically measured and quantified by better, cheaper, or faster system performance. In this article, we first illustrate the scope for control optimization in common infrastructure CPS. Next, we present a brief overview of current optimization techniques. Finally, we share our research position with a description of specific optimization approaches and their challenges for infrastructure CPS of the future.      
### 16.Learning Reduced Nonlinear State-Space Models: an Output-Error Based Canonical Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.04791.pdf)
>  The identification of a nonlinear dynamic model is an open topic in control theory, especially from sparse input-output measurements. A fundamental challenge of this problem is that very few to zero prior knowledge is available on both the state and the nonlinear system model. To cope with this challenge, we investigate the effectiveness of deep learning in the modeling of dynamic systems with nonlinear behavior by advocating an approach which relies on three main ingredients: (i) we show that under some structural conditions on the to-be-identified model, the state can be expressed in function of a sequence of the past inputs and outputs; (ii) this relation which we call the state map can be modelled by resorting to the well-documented approximation power of deep neural networks; (iii) taking then advantage of existing learning schemes, a state-space model can be finally identified. After the formulation and analysis of the approach, we show its ability to identify three different nonlinear systems. The performances are evaluated in terms of open-loop prediction on test data generated in simulation as well as a real world data-set of unmanned aerial vehicle flight measurements.      
### 17.Dilated POCS: Minimax Convex Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2206.04759.pdf)
>  Alternating projection onto convex sets (POCS) provides an iterative procedure to find a signal that satisfies two or more convex constraints when the sets intersect. For nonintersecting constraints, the method of simultaneous projections produces a minimum mean square error (MSE) solution. In certain cases, a minimax solution is more desirable. Generating a minimax solution is possible using dilated POCS (D-POCS). The minimax solution uses morphological dilation of nonintersecting signal convex constraints. The sets are progressively dilated to the point where there is intersection at a minimax solution. Examples are given contrasting the MSE and minimax solutions in problem of tomographic reconstruction of images. Lastly, morphological erosion of signal sets is suggested as a method to shrink the overlap when sets intersect at more than one point.      
### 18.AI-MIA: COVID-19 Detection &amp; Severity Analysis through Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2206.04732.pdf)
>  This paper presents the baseline approach for the organized 2nd Covid-19 Competition, occurring in the framework of the AIMIA Workshop in the European Conference on Computer Vision (ECCV 2022). It presents the COV19-CT-DB database which is annotated for COVID-19 detction, consisting of about 7,700 3-D CT scans. Part of the database consisting of Covid-19 cases is further annotated in terms of four Covid-19 severity conditions. We have split the database and the latter part of it in training, validation and test datasets. The former two datasets are used for training and validation of machine learning models, while the latter will be used for evaluation of the developed models. The baseline approach consists of a deep learning approach, based on a CNN-RNN network and report its performance on the COVID19-CT-DB database.      
### 19.AI-based Clinical Assessment of Optic Nerve Head Robustness Superseding Biomechanical Testing  [ :arrow_down: ](https://arxiv.org/pdf/2206.04689.pdf)
>  $\mathbf{Purpose}$: To use artificial intelligence (AI) to: (1) exploit biomechanical knowledge of the optic nerve head (ONH) from a relatively large population; (2) assess ONH robustness from a single optical coherence tomography (OCT) scan of the ONH; (3) identify what critical three-dimensional (3D) structural features make a given ONH robust. <br>$\mathbf{Design}$: Retrospective cross-sectional study. <br>$\mathbf{Methods}$: 316 subjects had their ONHs imaged with OCT before and after acute intraocular pressure (IOP) elevation through ophthalmo-dynamometry. IOP-induced lamina-cribrosa deformations were then mapped in 3D and used to classify ONHs. Those with LC deformations superior to 4% were considered fragile, while those with deformations inferior to 4% robust. Learning from these data, we compared three AI algorithms to predict ONH robustness strictly from a baseline (undeformed) OCT volume: (1) a random forest classifier; (2) an autoencoder; and (3) a dynamic graph CNN (DGCNN). The latter algorithm also allowed us to identify what critical 3D structural features make a given ONH robust. <br>$\mathbf{Results}$: All 3 methods were able to predict ONH robustness from 3D structural information alone and without the need to perform biomechanical testing. The DGCNN (area under the receiver operating curve [AUC]: 0.76 $\pm$ 0.08) outperformed the autoencoder (AUC: 0.70 $\pm$ 0.07) and the random forest classifier (AUC: 0.69 $\pm$ 0.05). Interestingly, to assess ONH robustness, the DGCNN mainly used information from the scleral canal and the LC insertion sites. <br>$\mathbf{Conclusions}$: We propose an AI-driven approach that can assess the robustness of a given ONH solely from a single OCT scan of the ONH, and without the need to perform biomechanical testing. Longitudinal studies should establish whether ONH robustness could help us identify fast visual field loss progressors.      
### 20.Structure-consistent Restoration Network for Cataract Fundus Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2206.04684.pdf)
>  Fundus photography is a routine examination in clinics to diagnose and monitor ocular diseases. However, for cataract patients, the fundus image always suffers quality degradation caused by the clouding lens. The degradation prevents reliable diagnosis by ophthalmologists or computer-aided systems. To improve the certainty in clinical diagnosis, restoration algorithms have been proposed to enhance the quality of fundus images. Unfortunately, challenges remain in the deployment of these algorithms, such as collecting sufficient training data and preserving retinal structures. In this paper, to circumvent the strict deployment requirement, a structure-consistent restoration network (SCR-Net) for cataract fundus images is developed from synthesized data that shares an identical structure. A cataract simulation model is firstly designed to collect synthesized cataract sets (SCS) formed by cataract fundus images sharing identical structures. Then high-frequency components (HFCs) are extracted from the SCS to constrain structure consistency such that the structure preservation in SCR-Net is enforced. The experiments demonstrate the effectiveness of SCR-Net in the comparison with state-of-the-art methods and the follow-up clinical applications. The code is available at <a class="link-external link-https" href="https://github.com/liamheng/ArcNet-Medical-Image-Enhancement" rel="external noopener nofollow">this https URL</a>.      
### 21.RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.04682.pdf)
>  Accurately segmenting temporal frames of cine magnetic resonance imaging (MRI) is a crucial step in various real-time MRI guided cardiac interventions. To achieve fast and accurate visual assistance, there are strict requirements on the maximum latency and minimum throughput of the segmentation framework. State-of-the-art neural networks on this task are mostly hand-crafted to satisfy these constraints while achieving high accuracy. On the other hand, while existing literature have demonstrated the power of neural architecture search (NAS) in automatically identifying the best neural architectures for various medical applications, they are mostly guided by accuracy, sometimes with computation complexity, and the importance of real-time constraints are overlooked. A major challenge is that such constraints are non-differentiable and are thus not compatible with the widely used differentiable NAS frameworks. In this paper, we present a strategy that directly handles real-time constraints in a differentiable NAS framework named RT-DNAS. Experiments on extended 2017 MICCAI ACDC dataset show that compared with state-of-the-art manually and automatically designed architectures, RT-DNAS is able to identify ones with better accuracy while satisfying the real-time constraints.      
### 22.Gaussian Fourier Pyramid for Local Laplacian Filter  [ :arrow_down: ](https://arxiv.org/pdf/2206.04681.pdf)
>  Multi-scale processing is essential in image processing and computer graphics. Halos are a central issue in multi-scale processing. Several edge-preserving decompositions resolve halos, e.g., local Laplacian filtering (LLF), by extending the Laplacian pyramid to have an edge-preserving property. Its processing is costly; thus, an approximated acceleration of fast LLF was proposed to linearly interpolate multiple Laplacian pyramids. This paper further improves the accuracy by Fourier series expansion, named Fourier LLF. Our results showed that Fourier LLF has a higher accuracy for the same number of pyramids. Moreover, Fourier LLF exhibits parameter-adaptive property for content-adaptive filtering. The code is available at: <a class="link-external link-https" href="https://norishigefukushima.github.io/GaussianFourierPyramid/" rel="external noopener nofollow">this https URL</a>.      
### 23.Multi-dimensional dual-blind deconvolution approach toward joint radar-communications  [ :arrow_down: ](https://arxiv.org/pdf/2206.05166.pdf)
>  We consider a joint multiple-antenna radar-communications system in a co-existence scenario. Contrary to conventional applications, wherein at least the radar waveform and communications channel are known or estimated \textit{a priori}, we investigate the case when the channels and transmit signals of both systems are unknown. In radar applications, this problem arises in multistatic or passive systems, where transmit signal is not known. Similarly, highly dynamic vehicular or mobile communications may render prior estimates of wireless channel unhelpful. In particular, the radar signal reflected-off multiple targets is overlaid with the multi-carrier communications signal. In order to extract the unknown continuous-valued target parameters (range, Doppler velocity, and direction-of-arrival) and communications messages, we formulate the problem as a sparse dual-blind deconvolution and solve it using atomic norm minimization. Numerical experiments validate our proposed approach and show that precise estimation of continuous-valued channel parameters, radar waveform, and communications messages is possible up to scaling ambiguities.      
### 24.A Proof of the Tree of Shapes in n-D  [ :arrow_down: ](https://arxiv.org/pdf/2206.05109.pdf)
>  In this paper, we prove that the self-dual morphological hierarchical structure computed on a n-D gray-level wellcomposed image u by the algorithm of G{Ã©}raud et al. [1] is exactly the mathematical structure defined to be the tree of shape of u in Najman et al [2]. We recall that this algorithm is in quasi-linear time and thus considered to be optimal. The tree of shapes leads to many applications in mathematical morphology and in image processing like grain filtering, shapings, image segmentation, and so on.      
### 25.Tensor Train for Global Optimization Problems in Robotics  [ :arrow_down: ](https://arxiv.org/pdf/2206.05077.pdf)
>  The convergence of many numerical optimization techniques is highly sensitive to the initial guess provided to the solver. We propose an approach based on tensor methods to initialize the existing optimization solvers close to global optima. The approach uses only the definition of the cost function and does not need access to any database of good solutions. We first transform the cost function, which is a function of task parameters and optimization variables, into a probability density function. Unlike existing approaches that set the task parameters as constant, we consider them as another set of random variables and approximate the joint probability distribution of the task parameters and the optimization variables using a surrogate probability model. For a given task, we then generate samples from the conditional distribution with respect to the given task parameter and use them as initialization for the optimization solver. As conditioning and sampling from an arbitrary density function are challenging, we use Tensor Train decomposition to obtain a surrogate probability model from which we can efficiently obtain the conditional model and the samples. The method can produce multiple solutions coming from different modes (when they exist) for a given task. We first evaluate the approach by applying it to various challenging benchmark functions for numerical optimization that are difficult to solve using gradient-based optimization solvers with a naive initialization, showing that the proposed method can produce samples close to the global optima and coming from multiple modes. We then demonstrate the generality of the framework and its relevance to robotics by applying the proposed method to inverse kinematics and motion planning problems with a 7-DoF manipulator.      
### 26.Coswara: A website application enabling COVID-19 screening by analysing respiratory sound samples and health symptoms  [ :arrow_down: ](https://arxiv.org/pdf/2206.05053.pdf)
>  The COVID-19 pandemic has accelerated research on design of alternative, quick and effective COVID-19 diagnosis approaches. In this paper, we describe the Coswara tool, a website application designed to enable COVID-19 detection by analysing respiratory sound samples and health symptoms. A user using this service can log into a website using any device connected to the internet, provide there current health symptom information and record few sound sampled corresponding to breathing, cough, and speech. Within a minute of analysis of this information on a cloud server the website tool will output a COVID-19 probability score to the user. As the COVID-19 pandemic continues to demand massive and scalable population level testing, we hypothesize that the proposed tool provides a potential solution towards this.      
### 27.Meta-data Study in Autism Spectrum Disorder Classification Based on Structural MRI  [ :arrow_down: ](https://arxiv.org/pdf/2206.05052.pdf)
>  Accurate diagnosis of autism spectrum disorder (ASD) based on neuroimaging data has significant implications, as extracting useful information from neuroimaging data for ASD detection is challenging. Even though machine learning techniques have been leveraged to improve the information extraction from neuroimaging data, the varying data quality caused by different meta-data conditions (i.e., data collection strategies) limits the effective information that can be extracted, thus leading to data-dependent predictive accuracies in ASD detection, which can be worse than random guess in some cases. In this work, we systematically investigate the impact of three kinds of meta-data on the predictive accuracy of classifying ASD based on structural MRI collected from 20 different sites, where meta-data conditions vary.      
### 28.Going Beyond the Cookie Theft Picture Test: Detecting Cognitive Impairments using Acoustic Features  [ :arrow_down: ](https://arxiv.org/pdf/2206.05018.pdf)
>  Standardized tests play a crucial role in the detection of cognitive impairment. Previous work demonstrated that automatic detection of cognitive impairment is possible using audio data from a standardized picture description task. The presented study goes beyond that, evaluating our methods on data taken from two standardized neuropsychological tests, namely the German SKT and a German version of the CERAD-NB, and a semi-structured clinical interview between a patient and a psychologist. For the tests, we focus on speech recordings of three sub-tests: reading numbers (SKT 3), interference (SKT 7), and verbal fluency (CERAD-NB 1). We show that acoustic features from standardized tests can be used to reliably discriminate cognitively impaired individuals from non-impaired ones. Furthermore, we provide evidence that even features extracted from random speech samples of the interview can be a discriminator of cognitive impairment. In our baseline experiments, we use OpenSMILE features and Support Vector Machine classifiers. In an improved setup, we show that using wav2vec 2.0 features instead, we can achieve an accuracy of up to 85%.      
### 29.An Open Framework to Model Diffraction by Dynamic Blockers in Millimeter Wave Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2206.05000.pdf)
>  The millimeter wave (mmWave) band will be exploited to address the growing demand for high data rates and low latency. The higher frequencies, however, are prone to limitations on the propagation of the signal in the environment. Thus, highly directional beamforming is needed to increase the antenna gain. Another crucial problem of the mmWave frequencies is their vulnerability to blockage by physical obstacles. To this aim, we studied the problem of modeling the impact of second-order effects on mmWave channels, specifically the susceptibility of the mmWave signals to physical blockers. With respect to existing works on this topic, our project focuses on scenarios where mmWaves interact with multiple, dynamic blockers. Our open source software includes diffraction-based blockage models and interfaces directly with an open source Radio Frequency (RF) ray-tracing software.      
### 30.Artificial Intelligence Enabled NOMA Towards Next Generation Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2206.04992.pdf)
>  This article focuses on the application of artificial intelligence (AI) in non-orthogonal multiple-access (NOMA), which aims to achieve automated, adaptive, and high-efficiency multi-user communications towards next generation multiple access (NGMA). First, the limitations of current scenario-specific multi-antenna NOMA schemes are discussed, and the importance of AI for NGMA is highlighted. Then, to achieve the vision of NGMA, a novel cluster-free NOMA framework is proposed for providing scenario-adaptive NOMA communications, and several promising machine learning solutions are identified. To elaborate further, novel centralized and distributed machine learning paradigms are conceived for efficiently employing the proposed cluster-free NOMA framework in single-cell and multi-cell networks, where numerical results are provided to demonstrate the effectiveness. Furthermore, the interplays between the proposed cluster-free NOMA and emerging wireless techniques are presented. Finally, several open research issues of AI enabled NGMA are discussed.      
### 31.Zero-Shot Audio Classification using Image Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2206.04984.pdf)
>  Supervised learning methods can solve the given problem in the presence of a large set of labeled data. However, the acquisition of a dataset covering all the target classes typically requires manual labeling which is expensive and time-consuming. Zero-shot learning models are capable of classifying the unseen concepts by utilizing their semantic information. The present study introduces image embeddings as side information on zero-shot audio classification by using a nonlinear acoustic-semantic projection. We extract the semantic image representations from the Open Images dataset and evaluate the performance of the models on an audio subset of AudioSet using semantic information in different domains; image, audio, and textual. We demonstrate that the image embeddings can be used as semantic information to perform zero-shot audio classification. The experimental results show that the image and textual embeddings display similar performance both individually and together. We additionally calculate the semantic acoustic embeddings from the test samples to provide an upper limit to the performance. The results show that the classification performance is highly sensitive to the semantic relation between test and training classes and textual and image embeddings can reach up to the semantic acoustic embeddings when the seen and unseen classes are semantically similar.      
### 32.Feature Learning and Ensemble Pre-Tasks Based Self-Supervised Speech Denoising and Dereverberation  [ :arrow_down: ](https://arxiv.org/pdf/2206.04962.pdf)
>  Self-supervised learning (SSL) achieves great success in monaural speech enhancement, while the accuracy of the target speech estimation, particularly for unseen speakers, remains inadequate with existing pre-tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, and spoken content, the latent representation for speech enhancement becomes a tough task. In this paper, we study the effectiveness of each feature which is commonly used in speech enhancement and exploit the feature combination in the SSL case. Besides, we propose an ensemble training strategy. The latent representation of the clean speech signal is learned, meanwhile, the dereverberated mask and the estimated ratio mask are exploited to denoise and dereverberate the mixture. The latent representation learning and the masks estimation are considered as two pre-tasks in the training stage. In addition, to study the effectiveness between the pre-tasks, we compare different training routines to train the model and further refine the performance. The NOISEX and DAPS corpora are used to evaluate the efficacy of the proposed method, which also outperforms the state-of-the-art methods.      
### 33.Semantic Technology based Usage Control for Decentralized Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.04947.pdf)
>  The sharing of data and digital assets in a decentralized settling is associated with various legislative challenges, including, but not limited to, the need to adhere to legal requirements with respect to privacy (e.g. data protection legislation) and copyright (e.g. copyright legislation). In order to enable software platform providers to manage data and digital assets appropriately and to provide more control to data and digital asset owners, usage control technologies could be used to make sure that consumers handle data according to privacy preferences, licenses, regulatory requirements, among others. In this research proposal, we explore the application of usage control in decentralized environments. In particular, we address the challenges related to the specification of usage control policies, the enforcement of the respective policies, and the usability of the tools that are used to administer them.      
### 34.A Novel Chinese Dialect TTS Frontend with Non-Autoregressive Neural Machine Translation  [ :arrow_down: ](https://arxiv.org/pdf/2206.04922.pdf)
>  Chinese dialect text-to-speech(TTS) system usually can only be utilized by native linguists, because the written form of Chinese dialects has different characters, idioms, grammar and usage from Mandarin, and even the local speaker cannot input a correct sentence. For Mandarin text inputs, Chinese dialect TTS can only generate partly-meaningful speech with relatively poor prosody and naturalness. To lower the bar of use and make it more practical in commercial, we propose a novel Chinese dialect TTS frontend with a translation module. It helps to convert Mandarin text into idiomatic expressions with correct orthography and grammar, so that the intelligibility and naturalness of the synthesized speech can be improved. A non-autoregressive neural machine translation model with a glancing sampling strategy is proposed for the translation task. It is the first known work to incorporate translation with TTS frontend. Our experiments on Cantonese approve that the proposed frontend can help Cantonese TTS system achieve a 0.27 improvement in MOS with Mandarin inputs.      
### 35.Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and Attention-based Graph Neural Network for Human Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2206.04855.pdf)
>  Human activity recognition (HAR) through wearable devices has received much interest due to its numerous applications in fitness tracking, wellness screening, and supported living. As a result, we have seen a great deal of work in this field. Traditional deep learning (DL) has set a state of the art performance for HAR domain. However, it ignores the data's structure and the association between consecutive time stamps. To address this constraint, we offer an approach based on Graph Neural Networks (GNNs) for structuring the input representation and exploiting the relations among the samples. However, even when using a simple graph convolution network to eliminate this shortage, there are still several limiting factors, such as inter-class activities issues, skewed class distribution, and a lack of consideration for sensor data priority, all of which harm the HAR model's performance. To improve the current HAR model's performance, we investigate novel possibilities within the framework of graph structure to achieve highly discriminated and rich activity features. We propose a model for (1) time-series-graph module that converts raw data from HAR dataset into graphs; (2) Graph Convolutional Neural Networks (GCNs) to discover local dependencies and correlations between neighboring nodes; and (3) self-attention GNN encoder to identify sensors interactions and data priorities. To the best of our knowledge, this is the first work for HAR, which introduces a GNN-based approach that incorporates both the GCN and the attention mechanism. By employing a uniform evaluation method, our framework significantly improves the performance on hospital patient's activities dataset comparatively considered other state of the art baseline methods.      
### 36.Robot Control for Simultaneous Impact Tasks through Time-Invariant Reference Spreading  [ :arrow_down: ](https://arxiv.org/pdf/2206.04852.pdf)
>  With the goal of enabling the exploitation of impacts in robotic manipulation, a new framework is presented for control of robotic manipulators that are tasked to execute nominally simultaneous impacts. In this framework, we employ tracking of time-invariant reference vector fields corresponding to the ante- and post-impact motion, increasing its applicability over similar conventional tracking control approaches. The ante- and post-impact references are coupled through a rigid impact map, and are extended to overlap around the area where the impact is expected to take place, such that the reference corresponding to the actual contact state of the robot can always be followed. As a sequence of impacts at the different contact points will typically occur, resulting in uncertainty of the contact mode and unreliable velocity measurements, a new interim control mode catered towards time-invariant references is formulated. In this mode, a position feedback signal is derived from the ante-impact velocity reference, which is used to enforce sustained contact in all contact points without using velocity feedback. With an eye towards real implementation, the approach is formulated using a QP control framework, and is validated using numerical simulations both on a rigid robot with a hard inelastic contact model and on a realistic robot model with flexible joints and compliant partially elastic contact model.      
### 37.Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022  [ :arrow_down: ](https://arxiv.org/pdf/2206.04805.pdf)
>  We build a classification model for the BirdCLEF 2022 challenge using unsupervised methods. We implement an unsupervised representation of the training dataset using a triplet loss on spectrogram representation of audio motifs. Our best model performs with a score of 0.48 on the public leaderboard.      
### 38.Scale up your In-Memory Accelerator: Leveraging Wireless-on-Chip Communication for AIMC-based CNN Inference  [ :arrow_down: ](https://arxiv.org/pdf/2206.04796.pdf)
>  Analog In-Memory Computing (AIMC) is emerging as a disruptive paradigm for heterogeneous computing, potentially delivering orders of magnitude better peak performance and efficiency over traditional digital signal processing architectures on Matrix-Vector multiplication. However, to sustain this throughput in real-world applications, AIMC tiles must be supplied with data at very high bandwidth and low latency; this poses an unprecedented pressure on the on-chip communication infrastructure, which becomes the system's performance and efficiency bottleneck. In this context, the performance and plasticity of emerging on-chip wireless communication paradigms provide the required breakthrough to up-scale on-chip communication in large AIMC devices. This work presents a many-tile AIMC architecture with inter-tile wireless communication that integrates multiple heterogeneous computing clusters, embedding a mix of parallel RISC-V cores and AIMC tiles. We perform an extensive design space exploration of the proposed architecture and discuss the benefits of exploiting emerging on-chip communication technologies such as wireless transceivers in the millimeter-wave and terahertz bands.      
### 39.Speak Like a Dog: Human to Non-human creature Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2206.04780.pdf)
>  This paper proposes a new voice conversion (VC) task from human speech to dog-like speech while preserving linguistic information as an example of human to non-human creature voice conversion (H2NH-VC) tasks. Although most VC studies deal with human to human VC, H2NH-VC aims to convert human speech into non-human creature-like speech. Non-parallel VC allows us to develop H2NH-VC, because we cannot collect a parallel dataset that non-human creatures speak human language. In this study, we propose to use dogs as an example of a non-human creature target domain and define the "speak like a dog" task. To clarify the possibilities and characteristics of the "speak like a dog" task, we conducted a comparative experiment using existing representative non-parallel VC methods in acoustic features (Mel-cepstral coefficients and Mel-spectrograms), network architectures (five different kernel-size settings), and training criteria (variational autoencoder (VAE)- based and generative adversarial network-based). Finally, the converted voices were evaluated using mean opinion scores: dog-likeness, sound quality and intelligibility, and character error rate (CER). The experiment showed that the employment of the Mel-spectrogram improved the dog-likeness of the converted speech, while it is challenging to preserve linguistic information. Challenges and limitations of the current VC methods for H2NH-VC are highlighted.      
### 40.CLAP: Learning Audio Concepts From Natural Language Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2206.04769.pdf)
>  Mainstream Audio Analytics models are trained to learn under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the flexibility of models because they require labeled audio for training and can only predict the predefined categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which learns to connect language and audio by using two encoders and a contrastive learning to bring audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 8 domains, such as Sound Event Classification, Music tasks, and Speech-related tasks. Although CLAP was trained with significantly less pairs than similar computer vision models, it establishes SoTA for Zero-Shot performance. Additionally, we evaluated CLAP in a supervised learning setup and achieve SoTA in 5 tasks. Hence, CLAP's Zero-Shot capability removes the need of training with class labels, enables flexible class prediction at inference time, and generalizes to multiple downstream tasks.      
### 41.On Low-Complexity Quickest Intervention of Mutated Diffusion Processes Through Local Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2206.04733.pdf)
>  We consider the problem of controlling a mutated diffusion process with an unknown mutation time. The problem is formulated as the quickest intervention problem with the mutation modeled by a change-point, which is a generalization of the quickest change-point detection (QCD). Our goal is to intervene in the mutated process as soon as possible while maintaining a low intervention cost with optimally chosen intervention actions. This model and the proposed algorithms can be applied to pandemic prevention (such as Covid-19) or misinformation containment. We formulate the problem as a partially observed Markov decision process (POMDP) and convert it to an MDP through the belief state of the change-point. We first propose a grid approximation approach to calculate the optimal intervention policy, whose computational complexity could be very high when the number of grids is large. In order to reduce the computational complexity, we further propose a low-complexity threshold-based policy through the analysis of the first-order approximation of the value functions in the ``local intervention'' regime. Simulation results show the low-complexity algorithm has a similar performance as the grid approximation and both perform much better than the QCD-based algorithms.      
