# ArXiv eess --Wed, 1 Jun 2022
### 1.A Multi-Head Convolutional Neural Network Based Non-Intrusive Load Monitoring Algorithm Under Dynamic Grid Voltage Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2205.15994.pdf)
>  In recent times, non-intrusive load monitoring (NILM) has emerged as an important tool for distribution-level energy management systems owing to its potential for energy conservation and management. However, load monitoring in smart building environments is challenging due to high variability of real-time load and varied load composition. Furthermore, as the volume and dimensionality of smart meters data increases, accuracy and computational time are key concerning factors. In view of these challenges, this paper proposes an improved NILM technique using multi-head (Mh-Net) convolutional neural network (CNN) under dynamic grid voltage conditions. An attention layer is introduced into the proposed CNN model, which helps in improving estimation accuracy of appliance power consumption. The performance of the developed model has been verified on an experimental laboratory setup for multiple appliance sets with varied power consumption levels, under dynamic grid voltages. Moreover, the effectiveness of the proposed model has been verified on widely used UK-DALE data, and its performance has been compared with existing NILM techniques. Results depict that the proposed model accurately identifies appliances, power consumptions and their time-of-use even during practical dynamic grid voltage conditions.      
### 2.Memory-efficient Segmentation of High-resolution Volumetric MicroCT Images  [ :arrow_down: ](https://arxiv.org/pdf/2205.15941.pdf)
>  In recent years, 3D convolutional neural networks have become the dominant approach for volumetric medical image segmentation. However, compared to their 2D counterparts, 3D networks introduce substantially more training parameters and higher requirement for the GPU memory. This has become a major limiting factor for designing and training 3D networks for high-resolution volumetric images. In this work, we propose a novel memory-efficient network architecture for 3D high-resolution image segmentation. The network incorporates both global and local features via a two-stage U-net-based cascaded framework and at the first stage, a memory-efficient U-net (meU-net) is developed. The features learnt at the two stages are connected via post-concatenation, which further improves the information flow. The proposed segmentation method is evaluated on an ultra high-resolution microCT dataset with typically 250 million voxels per volume. Experiments show that it outperforms state-of-the-art 3D segmentation methods in terms of both segmentation accuracy and memory efficiency.      
### 3.Analog Compressed Sensing for Sparse Frequency Shift Keying Modulation Schemes  [ :arrow_down: ](https://arxiv.org/pdf/2205.15933.pdf)
>  There is a growing interest in signaling schemes that operate in the wideband regime due to the crowded frequency spectrum. However, a downside of the wideband regime is that obtaining channel state information is costly, and the capacity of previously used modulation schemes such as code division multiple access and orthogonal frequency division multiplexing begins to diverge from the capacity bound without channel state information. Impulsive frequency shift keying and wideband time frequency coding have been shown to perform well in the wideband regime without channel state information, thus avoiding the costs and challenges associated with obtaining channel state information. However, the maximum likelihood receiver is a bank of frequency-selective filters, which is very costly to implement due to the large number of filters. In this work, we aim to simplify the receiver by using an analog compressed sensing receiver with chipping sequences as correlating signals to detect the sparse signals. Our results show that using a compressed sensing receiver allows for the simplification of the analog receiver with the trade off of a slight degradation in recovery performance. For a fixed frequency separation, symbol time, and peak SNR, the performance loss remains the same for a fixed ratio of number of correlating signals to the number of frequencies.      
### 4.PhD Thesis. Computer-Aided Assessment of Tuberculosis with Radiological Imaging: From rule-based methods to Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.15909.pdf)
>  Tuberculosis (TB) is an infectious disease caused by Mycobacterium tuberculosis (Mtb.) that produces pulmonary damage due to its airborne nature. This fact facilitates the disease fast-spreading, which, according to the World Health Organization (WHO), in 2021 caused 1.2 million deaths and 9.9 million new cases. Fortunately, X-Ray Computed Tomography (CT) images enable capturing specific manifestations of TB that are undetectable using regular diagnostic tests. <br>However, this procedure is unfeasible to process the thousands of volume images belonging to the different TB animal models and humans required for a suitable (pre-)clinical trial. To achieve suitable results, automatization of different image analysis processes is a must to quantify TB. Thus, in this thesis, we introduce a set of novel methods based on the state of the art Artificial Intelligence (AI) and Computer Vision (CV). Initially, we present an algorithm to assess Pathological Lung Segmentation (PLS). Next, a Gaussian Mixture Model ruled by an Expectation-Maximization (EM) algorithm is employed to automatically. <br>Chapter 3 introduces a model to automate the identification of TB lesions and the characterization of disease progression. <br>Chapter 4 extends the classification of TB lesions. Namely, we introduce a computational model to infer TB manifestations present in each lung lobe of CT scans by employing the associated radiologist reports as ground truth. In Chapter 5, we present a DL model capable of extracting disentangled information from images of different animal models, as well as information of the mechanisms that generate the CT volumes. <br>To sum up, the thesis presents a collection of valuable tools to automate the quantification of pathological lungs. Chapter 6 elaborates on these conclusions.      
### 5.Inferring 3D change detection from bitemporal optical images  [ :arrow_down: ](https://arxiv.org/pdf/2205.15903.pdf)
>  Change detection is one of the most active research areas in Remote Sensing (RS). Most of the recently developed change detection methods are based on deep learning (DL) algorithms. This kind of algorithms is generally focused on generating two-dimensional (2D) change maps, thus only identifying planimetric changes in land use/land cover (LULC) and not considering nor returning any information on the corresponding elevation changes. Our work goes one step further, proposing two novel networks, able to solve simultaneously the 2D and 3D CD tasks, and the 3DCD dataset, a novel and freely available dataset precisely designed for this multitask. Particularly, the aim of this work is to lay the foundations for the development of DL algorithms able to automatically infer an elevation (3D) CD map -- together with a standard 2D CD map --, starting only from a pair of bitemporal optical images. The proposed architectures, to perform the task described before, consist of a transformer-based network, the MultiTask Bitemporal Images Transformer (MTBIT), and a deep convolutional network, the Siamese ResUNet (SUNet). Particularly, MTBIT is a transformer-based architecture, based on a semantic tokenizer. SUNet instead combines, in a siamese encoder, skip connections and residual layers to learn rich features, capable to solve efficiently the proposed task. These models are, thus, able to obtain 3D CD maps from two optical images taken at different time instants, without the need to rely directly on elevation data during the inference step. Encouraging results, obtained on the novel 3DCD dataset, are shown. The code and the 3DCD dataset are available at \url{<a class="link-external link-https" href="https://sites.google.com/uniroma1.it/3dchangedetection/home-page" rel="external noopener nofollow">this https URL</a>}.      
### 6.The hybrid approach -- Convolutional Neural Networks and Expectation Maximization Algorithm -- for Tomographic Reconstruction of Hyperspectral Images  [ :arrow_down: ](https://arxiv.org/pdf/2205.15772.pdf)
>  We present a simple but novel hybrid approach to hyperspectral data cube reconstruction from computed tomography imaging spectrometry (CTIS) images that sequentially combines neural networks and the iterative Expectation Maximization (EM) algorithm. We train and test the ability of the method to reconstruct data cubes of $100\times100\times25$ and $100\times100\times100$ voxels, corresponding to 25 and 100 spectral channels, from simulated CTIS images generated by our CTIS simulator. The hybrid approach utilizes the inherent strength of the Convolutional Neural Network (CNN) with regard to noise and its ability to yield consistent reconstructions and make use of the EM algorithm's ability to generalize to spectral images of any object without training. The hybrid approach achieves better performance than both the CNNs and EM alone for seen (included in CNN training) and unseen (excluded from CNN training) cubes for both the 25- and 100-channel cases. For the 25 spectral channels, the improvements from CNN to the hybrid model (CNN + EM) in terms of the mean-squared errors are between 14-26%. For 100 spectral channels, the improvements between 19-40% are attained with the largest improvement of 40% for the unseen data, to which the CNNs are not exposed during the training.      
### 7.Adversarial synthesis based data-augmentation for code-switched spoken language identification  [ :arrow_down: ](https://arxiv.org/pdf/2205.15747.pdf)
>  Spoken Language Identification (LID) is an important sub-task of Automatic Speech Recognition(ASR) that is used to classify the language(s) in an audio segment. Automatic LID plays an useful role in multilingual countries. In various countries, identifying a language becomes hard, due to the multilingual scenario where two or more than two languages are mixed together during conversation. Such phenomenon of speech is called as code-mixing or code-switching. This nature is followed not only in India but also in many Asian countries. Such code-mixed data is hard to find, which further reduces the capabilities of the spoken LID. Due to the lack of avalibility of this code-mixed data, it becomes a minority class in LID task. Hence, this work primarily addresses this problem using data augmentation as a solution on the minority code-switched class. This study focuses on Indic language code-mixed with English. Spoken LID is performed on Hindi, code-mixed with English. This research proposes Generative Adversarial Network (GAN) based data augmentation technique performed using Mel spectrograms for audio data. GANs have already been proven to be accurate in representing the real data distribution in the image domain. Proposed research exploits these capabilities of GANs in speech domains such as speech classification, automatic speech recognition,etc. GANs are trained to generate Mel spectrograms of the minority code-mixed class which are then used to augment data for the classifier. Utilizing GANs give an overall improvement on Unweighted Average Recall by an amount of 3.5\% as compared to a Convolutional Recurrent Neural Network (CRNN) classifier used as the baseline reference.      
### 8.A Compensation Mechanism for EV Flexibility Services using Discrete Utility Functions  [ :arrow_down: ](https://arxiv.org/pdf/2205.15737.pdf)
>  Compensation mechanisms are used to counterbalance the discomfort suffered by users due to quality service issues. Such mechanisms are currently used for different purposes in the electrical power and energy sector, e.g., power quality and reliability. This paper proposes a compensation mechanism using EV flexibility management of a set of charging sessions managed by a charging point operator (CPO). Users' preferences and bilateral agreements with the CPO are modelled via discrete utility functions for the energy not served. A mathematical proof of the proposed compensation mechanism is given and applied to a test scenario using historical data from an office building with a parking lot in the Netherlands. Synthetic data for 400 charging sessions was generated using multivariate elliptical copulas to capture the complex dependency structures in EV charging data. Numerical results validate the usefulness of the proposed compensation mechanism as an attractive measure both for the CPO and the users in case of energy not served.      
### 9.Progressive Multi-scale Consistent Network for Multi-class Fundus Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2205.15720.pdf)
>  Effectively integrating multi-scale information is of considerable significance for the challenging multi-class segmentation of fundus lesions because different lesions vary significantly in scales and shapes. Several methods have been proposed to successfully handle the multi-scale object segmentation. However, two issues are not considered in previous studies. The first is the lack of interaction between adjacent feature levels, and this will lead to the deviation of high-level features from low-level features and the loss of detailed cues. The second is the conflict between the low-level and high-level features, this occurs because they learn different scales of features, thereby confusing the model and decreasing the accuracy of the final prediction. In this paper, we propose a progressive multi-scale consistent network (PMCNet) that integrates the proposed progressive feature fusion (PFF) block and dynamic attention block (DAB) to address the aforementioned issues. Specifically, PFF block progressively integrates multi-scale features from adjacent encoding layers, facilitating feature learning of each layer by aggregating fine-grained details and high-level semantics. As features at different scales should be consistent, DAB is designed to dynamically learn the attentive cues from the fused features at different scales, thus aiming to smooth the essential conflicts existing in multi-scale features. The two proposed PFF and DAB blocks can be integrated with the off-the-shelf backbone networks to address the two issues of multi-scale and feature inconsistency in the multi-class segmentation of fundus lesions, which will produce better feature representation in the feature space. Experimental results on three public datasets indicate that the proposed method is more effective than recent state-of-the-art methods.      
### 10.Lessons Learned from Data-Driven Building Control Experiments: Contrasting Gaussian Process-based MPC, Bilevel DeePC, and Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.15703.pdf)
>  This manuscript offers the perspective of experimentalists on a number of modern data-driven techniques: model predictive control relying on Gaussian processes, adaptive data-driven control based on behavioral theory, and deep reinforcement learning. These techniques are compared in terms of data requirements, ease of use, computational burden, and robustness in the context of real-world applications. Our remarks and observations stem from a number of experimental investigations carried out in the field of building control in diverse environments, from lecture halls and apartment spaces to a hospital surgery center. The final goal is to support others in identifying what technique is best suited to tackle their own problems.      
### 11.Conversational Speech Separation: an Evaluation Study for Streaming Applications  [ :arrow_down: ](https://arxiv.org/pdf/2205.15700.pdf)
>  Continuous speech separation (CSS) is a recently proposed framework which aims at separating each speaker from an input mixture signal in a streaming fashion. Hereafter we perform an evaluation study on practical design considerations for a CSS system, addressing important aspects which have been neglected in recent works. In particular, we focus on the trade-off between separation performance, computational requirements and output latency showing how an offline separation algorithm can be used to perform CSS with a desired latency. We carry out an extensive analysis on the choice of CSS processing window size and hop size on sparsely overlapped data. We find out that the best trade-off between computational burden and performance is obtained for a window of 5 s.      
### 12.Data-driven Reference Trajectory Optimization for Precision Motion Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.15694.pdf)
>  We propose an optimization-based method to improve contour tracking performance on precision motion stages by modifying the reference trajectory, without changing the built-in low-level controller. The position of the precision motion stage is predicted with data-driven models. First, a linear low-fidelity model is used to optimize traversal time, by changing the path velocity and acceleration profiles. Second, a non-linear high-fidelity model is used to refine the previously found time-optimal solution. We experimentally demonstrate that the method is capable of improving the productivity vs. accuracy trade-off for a high precision motion stage. Given the data-based nature of the models used, we claim that the method can easily be adapted to a wide family of precision motion systems.      
### 13.On the Steady-State Behavior of Finite-Control-Set MPC with an Application to High-Precision Power Amplifiers  [ :arrow_down: ](https://arxiv.org/pdf/2205.15668.pdf)
>  Motivated by increasing precision requirements for switched power amplifiers, this paper addresses the problem of model predictive control (MPC) design for discrete-time linear systems with a finite control set (FCS). Typically, existing solutions for FCS-MPC penalize the output tracking error and the control input rate of change, which can lead to arbitrary switching among the available discrete control inputs and unpredictable steady-state behavior. To improve the steady-state behavior of FCS-MPC, in this paper we design a cost function that penalizes the tracking error with respect to a state and input steady-state limit cycle. We prove that if a suitable terminal cost is added to the FCS-MPC algorithm convergence to the limit cycle is ensured. The developed methodology is validated in direct switching control of a power amplifier for high-precision motion systems, where it significantly improves the steady-state output current ripple.      
### 14.Adaptive fuzzy control of electrohydraulic servosystems  [ :arrow_down: ](https://arxiv.org/pdf/2205.15639.pdf)
>  Electrohydraulic servosystems are widely employed in industrial applications such as robotic manipulators, active suspensions, precision machine tools and aerospace systems. They provide many advantages over electric motors, including high force to weight ratio, fast response time and compact size. However, precise control of electrohydraulic actuated systems, due to their inherent nonlinear characteristics, cannot be easily obtained with conventional linear controllers. Most flow control valves can also exhibit some hard nonlinearities such as dead-zone due to valve spool overlap. This work describes the development of an adaptive fuzzy controller for electrohydraulic actuated systems with unknown dead-zone. The stability properties of the closed-loop systems was proven using Lyapunov stability theory and Barbalat's lemma. Numerical results are presented in order to demonstrate the control system performance.      
### 15.Privacy Leakage in Discrete Time Updating Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.15630.pdf)
>  A source generates time-stamped update packets that are sent to a server and then forwarded to a monitor. This occurs in the presence of an adversary that can infer information about the source by observing the output process of the server. The server wishes to release updates in a timely way to the monitor but also wishes to minimize the information leaked to the adversary. We analyze the trade-off between the age of information (AoI) and the maximal leakage for systems in which the source generates updates as a Bernoulli process. For a time slotted system in which sending an update requires one slot, we consider three server policies: (1) Memoryless with Bernoulli Thinning (MBT): arriving updates are queued with some probability and head-of-line update is released after a geometric holding time; (2) Deterministic Accumulate-and-Dump (DAD): the most recently generated update (if any) is released after a fixed time; (3) Random Accumulate-and-Dump (RAD): the most recently generated update (if any) is released after a geometric waiting time. We show that for the same maximal leakage rate, the DAD policy achieves lower age compared to the other two policies but is restricted to discrete age-leakage operating points.      
### 16.Generative Aging of Brain Images with Diffeomorphic Registration  [ :arrow_down: ](https://arxiv.org/pdf/2205.15607.pdf)
>  Analyzing and predicting brain aging is essential for early prognosis and accurate diagnosis of cognitive diseases. The technique of neuroimaging, such as Magnetic Resonance Imaging (MRI), provides a noninvasive means of observing the aging process within the brain. With longitudinal image data collection, data-intensive Artificial Intelligence (AI) algorithms have been used to examine brain aging. However, existing state-of-the-art algorithms tend to be restricted to group-level predictions and suffer from unreal predictions. This paper proposes a methodology for generating longitudinal MRI scans that capture subject-specific neurodegeneration and retain anatomical plausibility in aging. The proposed methodology is developed within the framework of diffeomorphic registration and relies on three key novel technological advances to generate subject-level anatomically plausible predictions: i) a computationally efficient and individualized generative framework based on registration; ii) an aging generative module based on biological linear aging progression; iii) a quality control module to fit registration for generation task. Our methodology was evaluated on 2662 T1-weighted (T1-w) MRI scans from 796 participants from three different cohorts. First, we applied 6 commonly used criteria to demonstrate the aging simulation ability of the proposed methodology; Secondly, we evaluated the quality of the synthetic images using quantitative measurements and qualitative assessment by a neuroradiologist. Overall, the experimental results show that the proposed method can produce anatomically plausible predictions that can be used to enhance longitudinal datasets, in turn enabling data-hungry AI-driven healthcare tools.      
### 17.Enabling NLoS LEO Satellite Communications with Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2205.15528.pdf)
>  Low Earth Orbit (LEO) satellite communications (SatCom) are considered a promising solution to provide uninterrupted services in cellular networks. Line-of-sight (LoS) links between the LEO satellites and the ground users are, however, easily blocked in urban scenarios. In this paper, we propose to enable LEO SatCom in non-line-of-sight (NLoS) channels, as those corresponding to links to users in urban canyons, with the aid of reconfigurable intelligent surfaces (RISs). First, we derive the near field signal model for the satellite-RIS-user link. Then, we propose two deployments to improve the coverage of a RIS-aided link: down tilting the RIS located on the top of a building, and considering a deployment with RISs located on the top of opposite buildings. Simulation results show the effectiveness of using RISs in LEO SatCom to overcome blockages in urban canyons. Insights about the optimal tilt angle and the coverage extension provided by the deployment of an additional RIS are also provided.      
### 18.Optimizing the Deployment of Reconfigurable Intelligent Surfaces in MmWave Vehicular Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.15520.pdf)
>  Millimeter wave (MmWave) systems are vulnerable to blockages, which cause signal drop and link outage. One solution is to deploy reconfigurable intelligent surfaces (RISs) to add a strong non-line-of-sight path from the transmitter to receiver. To achieve the best performance, the location of the deployed RIS should be optimized for a given site, considering the distribution of potential users and possible blockers. In this paper, we find the optimal location, height and downtilt of RIS working in a realistic vehicular scenario. Because of the proximity between the RIS and the vehicles, and the large electrical size of the RIS, we consider a 3D geometry including the elevation angle and near-field beamforming. We provide results on RIS configuration in terms of both coverage ratio and area-averaged rate. We find that the optimized RIS improves the average averaged rate fifty percent over the case without a RIS, as well as further improvements in the coverage ratio.      
### 19.Quantum Speedup for Higher-Order Unconstrained Binary Optimization and MIMO Maximum Likelihood Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.15478.pdf)
>  In this paper, we propose a quantum algorithm that supports a real-valued higher-order unconstrained binary optimization (HUBO) problem. This algorithm is based on the Grover adaptive search that originally supported HUBO with integer coefficients. Next, as an application example, we formulate multiple-input multiple-output maximum likelihood detection as a HUBO problem with real-valued coefficients, where we use the Gray-coded bit-to-symbol mapping specified in the 5G standard. The proposed approach allows us to construct a specific quantum circuit for the detection problem and to analyze specific numbers of required qubits and quantum gates, whereas other conventional studies have assumed that such a circuit is feasible as a quantum oracle. To further accelerate the convergence, we also derive a probability distribution of the objective function value and determine a unique threshold to sample better states for the quantum algorithm. Assuming a future fault-tolerant quantum computer, we demonstrate that the proposed algorithm is capable of reducing the query complexity in the classical domain and providing a quadratic speedup in the quantum domain.      
### 20.StyleTTS: A Style-Based Generative Model for Natural and Diverse Text-to-Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2205.15439.pdf)
>  Text-to-Speech (TTS) has recently seen great progress in synthesizing high-quality speech owing to the rapid development of parallel TTS systems, but producing speech with naturalistic prosodic variations, speaking styles and emotional tones remains challenging. Moreover, since duration and speech are generated separately, parallel TTS models still have problems finding the best monotonic alignments that are crucial for naturalistic speech synthesis. Here, we propose StyleTTS, a style-based generative model for parallel TTS that can synthesize diverse speech with natural prosody from a reference speech utterance. With novel Transferable Monotonic Aligner (TMA) and duration-invariant data augmentation schemes, our method significantly outperforms state-of-the-art models on both single and multi-speaker datasets in subjective tests of speech naturalness and speaker similarity. Through self-supervised learning of the speaking styles, our model can synthesize speech with the same prosodic and emotional tone as any given reference speech without the need for explicitly labeling these categories.      
### 21.Doppler-Enabled Single-Antenna Localization and Mapping Without Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2205.15427.pdf)
>  Radio localization is a key enabler for joint communication and sensing in the fifth/sixth generation (5G/6G) communication systems. With the help of multipath components (MPCs), localization and mapping tasks can be done with a single base station (BS) and single unsynchronized user equipment (UE) if both of them are equipped with an antenna array. However, the antenna array at the UE side increases the hardware and computational cost, preventing localization functionality. In this work, we show that with Doppler estimation and MPCs, localization and mapping tasks can be performed even with a single-antenna mobile UE. Furthermore, we show that the localization and mapping performance will improve and then saturate at a certain level with an increased UE speed. Both theoretical Cramér-Rao bound analysis and simulation results show the potential of localization under mobility and the effectiveness of the proposed localization algorithm.      
### 22.Channel Model Mismatch Analysis for XL-MIMO Systems from a Localization Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2205.15417.pdf)
>  Radio localization is applied in high-frequency (e.g., mmWave and THz) systems to support communication and to provide location-based services without extra infrastructure. {For solving localization problems, a simplified, stationary, narrowband far-field channel model is widely used due to its compact formulation.} However, with increased array size in extra-large MIMO systems and increased bandwidth at upper mmWave bands, the effect of channel spatial non-stationarity (SNS), spherical wave model (SWM), and beam squint effect (BSE) cannot be ignored. In this case, localization performance will be affected when an inaccurate channel model deviating from the true model is adopted. In this work, we employ the MCRB (misspecified Cramér-Rao lower bound) to lower bound the localization error using a simplified mismatched model while the observed data is governed by a more complex true model. The simulation results show that among all the model impairments, the SNS has the least contribution, the SWM dominates when the distance is small compared to the array size, and the BSE has a more significant effect when the distance is much larger than the array size.      
### 23.PolypConnect: Image inpainting for generating realistic gastrointestinal tract images with polyps  [ :arrow_down: ](https://arxiv.org/pdf/2205.15413.pdf)
>  Early identification of a polyp in the lower gastrointestinal (GI) tract can lead to prevention of life-threatening colorectal cancer. Developing computer-aided diagnosis (CAD) systems to detect polyps can improve detection accuracy and efficiency and save the time of the domain experts called endoscopists. Lack of annotated data is a common challenge when building CAD systems. Generating synthetic medical data is an active research area to overcome the problem of having relatively few true positive cases in the medical domain. To be able to efficiently train machine learning (ML) models, which are the core of CAD systems, a considerable amount of data should be used. In this respect, we propose the PolypConnect pipeline, which can convert non-polyp images into polyp images to increase the size of training datasets for training. We present the whole pipeline with quantitative and qualitative evaluations involving endoscopists. The polyp segmentation model trained using synthetic data, and real data shows a 5.1% improvement of mean intersection over union (mIOU), compared to the model trained only using real data. The codes of all the experiments are available on GitHub to reproduce the results.      
### 24.Characterization of integral input-to-state stability for nonlinear time-varying systems of infinite dimension  [ :arrow_down: ](https://arxiv.org/pdf/2205.15993.pdf)
>  For large classes of infinite-dimensional time-varying control systems, the equivalence between integral input-to-state stability (iISS) and the combination of global uniform asymptotic stability under zero input (0-GUAS) and uniformly bounded-energy input/bounded state (UBEBS) is established under a reasonable assumption of continuity of the trajectories with respect to the input, at the zero input. By particularizing to specific instances of infinite-dimensional systems, such as time-delay, or semilinear over Banach spaces, sufficient conditions are given in terms of the functions defining the dynamics. In addition, it is also shown that for semilinear systems whose nonlinear term satisfies an affine-in-the-state norm bound, it holds that iISS becomes equivalent to just 0-GUAS, a fact known to hold for bilinear systems. An additional important aspect is that the iISS notion considered is more general than the standard one.      
### 25.Private Federated Submodel Learning with Sparsification  [ :arrow_down: ](https://arxiv.org/pdf/2205.15992.pdf)
>  We investigate the problem of private read update write (PRUW) in federated submodel learning (FSL) with sparsification. In FSL, a machine learning model is divided into multiple submodels, where each user updates only the submodel that is relevant to the user's local data. PRUW is the process of privately performing FSL by reading from and writing to the required submodel without revealing the submodel index, or the values of updates to the databases. Sparsification is a widely used concept in learning, where the users update only a small fraction of parameters to reduce the communication cost. Revealing the coordinates of these selected (sparse) updates leaks privacy of the user. We show how PRUW in FSL can be performed with sparsification. We propose a novel scheme which privately reads from and writes to arbitrary parameters of any given submodel, without revealing the submodel index, values of updates, or the coordinates of the sparse updates, to databases. The proposed scheme achieves significantly lower reading and writing costs compared to what is achieved without sparsification.      
### 26.CropMix: Sampling a Rich Input Distribution via Multi-Scale Cropping  [ :arrow_down: ](https://arxiv.org/pdf/2205.15955.pdf)
>  We present a simple method, CropMix, for the purpose of producing a rich input distribution from the original dataset distribution. Unlike single random cropping, which may inadvertently capture only limited information, or irrelevant information, like pure background, unrelated objects, etc, we crop an image multiple times using distinct crop scales, thereby ensuring that multi-scale information is captured. The new input distribution, serving as training data, useful for a number of vision tasks, is then formed by simply mixing multiple cropped views. We first demonstrate that CropMix can be seamlessly applied to virtually any training recipe and neural network architecture performing classification tasks. CropMix is shown to improve the performance of image classifiers on several benchmark tasks across-the-board without sacrificing computational simplicity and efficiency. Moreover, we show that CropMix is of benefit to both contrastive learning and masked image modeling towards more powerful representations, where preferable results are achieved when learned representations are transferred to downstream tasks. Code is available at GitHub.      
### 27.SAR Despeckling Using Overcomplete Convolutional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.15906.pdf)
>  Synthetic Aperture Radar (SAR) despeckling is an important problem in remote sensing as speckle degrades SAR images, affecting downstream tasks like detection and segmentation. Recent studies show that convolutional neural networks(CNNs) outperform classical despeckling methods. Traditional CNNs try to increase the receptive field size as the network goes deeper, thus extracting global features. However,speckle is relatively small, and increasing receptive field does not help in extracting speckle features. This study employs an overcomplete CNN architecture to focus on learning low-level features by restricting the receptive field. The proposed network consists of an overcomplete branch to focus on the local structures and an undercomplete branch that focuses on the global structures. We show that the proposed network improves despeckling performance compared to recent despeckling methods on synthetic and real SAR images.      
### 28.Learning brain MRI quality control: a multi-factorial generalization problem  [ :arrow_down: ](https://arxiv.org/pdf/2205.15898.pdf)
>  Due to the growing number of MRI data, automated quality control (QC) has become essential, especially for larger scale analysis. Several attempts have been made in order to develop reliable and scalable QC pipelines. However, the generalization of these methods on new data independent of those used for learning is a difficult problem because of the biases inherent in MRI data. This work aimed at evaluating the performances of the MRIQC pipeline on various large-scale datasets (ABIDE, N = 1102 and CATI derived datasets, N = 9037) used for both training and evaluation purposes. We focused our analysis on the MRIQC preprocessing steps and tested the pipeline with and without them. We further analyzed the site-wise and study-wise predicted classification probability distributions of the models without preprocessing trained on ABIDE and CATI data. Our main results were that a model using features extracted from MRIQC without preprocessing yielded the best results when trained and evaluated on large multi-center datasets with a heterogeneous population (an improvement of the ROC-AUC score on unseen data of 0.10 for the model trained on a subset of the CATI dataset). We concluded that a model trained with data from a heterogeneous population, such as the CATI dataset, provides the best scores on unseen data. In spite of the performance improvement, the generalization abilities of the models remain questionable when looking at the site-wise/study-wise probability predictions and the optimal classification threshold derived from them.      
### 29.Median Pixel Difference Convolutional Network for Robust Face Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2205.15867.pdf)
>  Face recognition is one of the most active tasks in computer vision and has been widely used in the real world. With great advances made in convolutional neural networks (CNN), lots of face recognition algorithms have achieved high accuracy on various face datasets. However, existing face recognition algorithms based on CNNs are vulnerable to noise. Noise corrupted image patterns could lead to false activations, significantly decreasing face recognition accuracy in noisy situations. To equip CNNs with built-in robustness to noise of different levels, we proposed a Median Pixel Difference Convolutional Network (MeDiNet) by replacing some traditional convolutional layers with the proposed novel Median Pixel Difference Convolutional Layer (MeDiConv) layer. The proposed MeDiNet integrates the idea of traditional multiscale median filtering with deep CNNs. The MeDiNet is tested on the four face datasets (LFW, CA-LFW, CP-LFW, and YTF) with versatile settings on blur kernels, noise intensities, scales, and JPEG quality factors. Extensive experiments show that our MeDiNet can effectively remove noisy pixels in the feature map and suppress the negative impact of noise, leading to achieving limited accuracy loss under these practical noises compared with the standard CNN under clean conditions.      
### 30.Multi-agent Multi-target Path Planning in Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2205.15841.pdf)
>  Missions for teams of autonomous systems often require agents to visit multiple targets in complex and dynamic operating conditions. We consider the problem of visiting a set of targets in minimum time by a team of non-communicating agents in a stochastic environment modeled as a Markov decision process. We first consider the single-agent problem, and show that it is at least NP-complete by reducing it to a Hamiltonian path problem. Using Bellman's optimality equation, we discuss an optimal algorithm that is exponential in the number of target states. Then, we tradeoff optimality for time complexity by presenting a suboptimal algorithm that is polynomial at each time step. We prove that the proposed algorithm generates optimal policies for certain classes of Markov decision processes. We extend this algorithm to the multi-agent case by proposing a heuristic partitioning algorithm of assigning targets to agents. Our algorithm approximately minimizes the expected time to visit the target states. We prove that the heuristic procedure generates optimal partitions for environments where the targets are naturally clustered. We present the performance of our algorithms on random Markov decision processes, as well as a gridworld environment inspired by autonomous underwater vehicles operating in an ocean. We show that our algorithms are much faster than the optimal procedure and more optimal than the currently available heuristic.      
### 31.Wideband Time Frequency Coding  [ :arrow_down: ](https://arxiv.org/pdf/2205.15831.pdf)
>  In the wideband regime, the performance of many of the popular modulation schemes such as code division multiple access and orthogonal frequency division multiplexing falls quickly without channel state information. Obtaining the amount of channel information required for these techniques to work is costly and difficult, which suggests the need for schemes which can perform well without channel state information. In this work, we present one such scheme, called wideband time frequency coding, which achieves rates on the order of the additive white Gaussian noise capacity without requiring any channel state information. Wideband time frequency coding combines impulsive frequency shift keying with pulse position modulation, which allows for information to be encoded in both the transmitted frequency and the transmission time period. On the detection side, we propose a non-coherent decoder based on a square-law detector, akin to the optimal decoder for frequency shift keying based signals. The impacts of various parameters on the symbol error probability and capacity of wideband time frequency coding are investigated, and the results show that it is robust to shadowing and highly fading channels. When compared to other modulation schemes such as code division multiple access, orthogonal frequency division multiplexing, pulse position modulation, and impulsive frequency shift keying without channel state information, wideband time frequency coding achieves higher rates in the wideband regime, and performs comparably in smaller bandwidths.      
### 32.Predicting non-native speech perception using the Perceptual Assimilation Model and state-of-the-art acoustic models  [ :arrow_down: ](https://arxiv.org/pdf/2205.15823.pdf)
>  Our native language influences the way we perceive speech sounds, affecting our ability to discriminate non-native sounds. We compare two ideas about the influence of the native language on speech perception: the Perceptual Assimilation Model, which appeals to a mental classification of sounds into native phoneme categories, versus the idea that rich, fine-grained phonetic representations tuned to the statistics of the native language, are sufficient. We operationalize this idea using representations from two state-of-the-art speech models, a Dirichlet process Gaussian mixture model and the more recent wav2vec 2.0 model. We present a new, open dataset of French- and English-speaking participants' speech perception behaviour for 61 vowel sounds from six languages. We show that phoneme assimilation is a better predictor than fine-grained phonetic modelling, both for the discrimination behaviour as a whole, and for predicting differences in discriminability associated with differences in native language background. We also show that wav2vec 2.0, while not good at capturing the effects of native language on speech perception, is complementary to information about native phoneme assimilation, and provides a good model of low-level phonetic representations, supporting the idea that both categorical and fine-grained perception are used during speech perception.      
### 33.Do self-supervised speech models develop human-like perception biases?  [ :arrow_down: ](https://arxiv.org/pdf/2205.15819.pdf)
>  Self-supervised models for speech processing form representational spaces without using any external labels. Increasingly, they appear to be a feasible way of at least partially eliminating costly manual annotations, a problem of particular concern for low-resource languages. But what kind of representational spaces do these models construct? Human perception specializes to the sounds of listeners' native languages. Does the same thing happen in self-supervised models? We examine the representational spaces of three kinds of state-of-the-art self-supervised models: wav2vec 2.0, HuBERT and contrastive predictive coding (CPC), and compare them with the perceptual spaces of French-speaking and English-speaking human listeners, both globally and taking account of the behavioural differences between the two language groups. We show that the CPC model shows a small native language effect, but that wav2vec 2.0 and HuBERT seem to develop a universal speech perception space which is not language specific. A comparison against the predictions of supervised phone recognisers suggests that all three self-supervised models capture relatively fine-grained perceptual phenomena, while supervised models are better at capturing coarser, phone-level, effects of listeners' native language, on perception.      
### 34.Collaborative Sensing in Perceptive Mobile Networks: Opportunities and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2205.15805.pdf)
>  With the development of innovative applications that demand accurate environment information, e.g., autonomous driving, sensing becomes an important requirement for future wireless networks. To this end, integrated sensing and communication (ISAC) provides a promising platform to exploit the synergy between sensing and communication, where perceptive mobile networks (PMNs) were proposed to add accurate sensing capability to existing wireless networks. The well-developed cellular networks offer exciting opportunities for sensing, including large coverage, strong computation and communication power, and most importantly networked sensing, where the perspectives from multiple sensing nodes can be collaboratively utilized for sensing the same target. However, PMNs also face big challenges such as the inherent interference between sensing and communication, the complex sensing environment, and the tracking of high-speed targets by cellular networks. This paper provides a comprehensive review on the design of PMNs, covering the popular network architectures, sensing protocols, standing research problems, and available solutions. Several future research directions that are critical for the development of PMNs are also discussed.      
### 35.Coverage Probability of STAR-RIS assisted Massive MIMO systems with Correlation and Phase Errors  [ :arrow_down: ](https://arxiv.org/pdf/2205.15710.pdf)
>  In this paper, we investigate a simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisting a massive multiple-input multiple-output (mMIMO) system. In particular, we derive a closed-form expression for the coverage probability of a STAR-RIS assisted mMIMO system while accounting for correlated fading and phase-shift errors. Notably, the phase configuration takes place at every several coherence intervals by optimizing the coverage probability since the latter depends on statistical channel state information (CSI) in terms of large-scale statistics. As a result, we achieve a reduced complexity and overhead for the optimization of passive beamforming, which are increased in the case of STAR-RIS networks with instantaneous CSI. Numerical results corroborate our analysis, shed light on interesting properties such as the impact of the number of RIS elements and the effect of phase errors, along with affirming the superiority of STAR-RIS against reflective-only RIS.      
### 36.Generalised Implicit Neural Representations  [ :arrow_down: ](https://arxiv.org/pdf/2205.15674.pdf)
>  We consider the problem of learning implicit neural representations (INRs) for signals on non-Euclidean domains. In the Euclidean case, INRs are trained on a discrete sampling of a signal over a regular lattice. Here, we assume that the continuous signal exists on some unknown topological space from which we sample a discrete graph. In the absence of a coordinate system to identify the sampled nodes, we propose approximating their location with a spectral embedding of the graph. This allows us to train INRs without knowing the underlying continuous domain, which is the case for most graph signals in nature, while also making the INRs equivariant under the symmetry group of the domain. We show experiments with our method on various real-world signals on non-Euclidean domains.      
### 37.Dynamic interventions with limited knowledge in network games  [ :arrow_down: ](https://arxiv.org/pdf/2205.15673.pdf)
>  This paper studies the problem of intervention design for steering the actions of noncooperative players in quadratic network games to the social optimum. The players choose their actions with the aim of maximizing their individual payoff functions, while a central regulator uses interventions to modify their marginal returns and maximize the social welfare function. This work builds on the key observation that the solution to the steering problem depends on the knowledge of the regulator on the players' parameters and the underlying network. We, therefore, consider different scenarios based on limited knowledge and propose suitable static, dynamic and adaptive intervention protocols. We formally prove convergence to the social optimum under the proposed mechanisms. We demonstrate our theoretical findings on a case study of Cournot competition with differentiated goods.      
### 38.Optimizing Intermediate Representations of Generative Models for Phase Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2205.15617.pdf)
>  Phase retrieval is the problem of reconstructing images from magnitude-only measurements. In many real-world applications the problem is underdetermined. When training data is available, generative models are a new idea to constrain the solution set. However, not all possible solutions are within the range of the generator. Instead, they are represented with some error. To reduce this representation error in the context of phase retrieval, we first leverage a novel variation of intermediate layer optimization (ILO) to extend the range of the generator while still producing images consistent with the training data. Second, we introduce new initialization schemes that further improve the quality of the reconstruction. With extensive experiments on Fourier and Gaussian phase retrieval problems and thorough ablation studies, we can show the benefits of our modified ILO and the new initialization schemes.      
### 39.Fundamental CRB-Rate Tradeoff in Multi-antenna Multicast Channel with ISAC  [ :arrow_down: ](https://arxiv.org/pdf/2205.15615.pdf)
>  This paper studies the multi-antenna multicast channel with integrated sensing and communication (ISAC), in which a multi-antenna base station (BS) sends common messages to a set of single-antenna communication users (CUs) and simultaneously estimates the parameters of an extended target via radar sensing. We investigate the fundamental performance limits of this ISAC system, in terms of the achievable rate for communication and the estimation Cramér-Rao bound (CRB) for sensing. First, we derive the optimal transmit covariance in semi-closed form to balance the CRB-rate (C-R) tradeoff, and accordingly characterize the outer bound of a so-called C-R region. It is shown that the optimal transmit covariance should be of full rank, consisting of both information-carrying and dedicated sensing signals in general. Next, we consider a practical joint information and sensing beamforming design, and propose an efficient approach to optimize the joint beamforming for balancing the C-R tradeoff. Numerical results are presented to show the C-R region achieved by the optimal transmit covariance and the joint beamforming, as compared to other benchmark schemes.      
### 40.Communication-Efficient Distributionally Robust Decentralized Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.15614.pdf)
>  Decentralized learning algorithms empower interconnected edge devices to share data and computational resources to collaboratively train a machine learning model without the aid of a central coordinator (e.g. an orchestrating basestation). In the case of heterogeneous data distributions at the network devices, collaboration can yield predictors with unsatisfactory performance for a subset of the devices. For this reason, in this work we consider the formulation of a distributionally robust decentralized learning task and we propose a decentralized single loop gradient descent/ascent algorithm (AD-GDA) to solve the underlying minimax optimization problem. We render our algorithm communication efficient by employing a compressed consensus scheme and we provide convergence guarantees for smooth convex and non-convex loss functions. Finally, we corroborate the theoretical findings with empirical evidence of the ability of the proposed algorithm in providing unbiased predictors over a network of collaborating devices with highly heterogeneous data distributions.      
### 41.MontageGAN: Generation and Assembly of Multiple Components by GANs  [ :arrow_down: ](https://arxiv.org/pdf/2205.15577.pdf)
>  A multi-layer image is more valuable than a single-layer image from a graphic designer's perspective. However, most of the proposed image generation methods so far focus on single-layer images. In this paper, we propose MontageGAN, which is a Generative Adversarial Networks (GAN) framework for generating multi-layer images. Our method utilized a two-step approach consisting of local GANs and global GAN. Each local GAN learns to generate a specific image layer, and the global GAN learns the placement of each generated image layer. Through our experiments, we show the ability of our method to generate multi-layer images and estimate the placement of the generated image layers.      
### 42.Sub-Image Histogram Equalization using Coot Optimization Algorithm for Segmentation and Parameter Selection  [ :arrow_down: ](https://arxiv.org/pdf/2205.15565.pdf)
>  Contrast enhancement is very important in terms of assessing images in an objective way. Contrast enhancement is also significant for various algorithms including supervised and unsupervised algorithms for accurate classification of samples. Some contrast enhancement algorithms solve this problem by addressing the low contrast issue. Mean and variance based sub-image histogram equalization (MVSIHE) algorithm is one of these contrast enhancements methods proposed in the literature. It has different parameters which need to be tuned in order to achieve optimum results. With this motivation, in this study, we employed one of the most recent optimization algorithms, namely, coot optimization algorithm (COA) for selecting appropriate parameters for the MVSIHE algorithm. Blind/referenceless image spatial quality evaluator (BRISQUE) and natural image quality evaluator (NIQE) metrics are used for evaluating fitness of the coot swarm population. The results show that the proposed method can be used in the field of biomedical image processing.      
### 43.Optimal Multicast Service Chain Control: Packet Processing, Routing, and Duplication  [ :arrow_down: ](https://arxiv.org/pdf/2205.15557.pdf)
>  Distributed computing (cloud) networks, e.g., mobile edge computing (MEC), are playing an increasingly important role in the efficient hosting, running, and delivery of real-time stream-processing applications such as industrial automation, immersive video, and augmented reality. While such applications require timely processing of real-time streams that are simultaneously useful for multiple users/devices, existing technologies lack efficient mechanisms to handle their increasingly multicast nature, leading to unnecessary traffic redundancy and associated network congestion. In this paper, we address the design of distributed packet processing, routing, and duplication policies for optimal control of multicast stream-processing services. We present a characterization of the enlarged capacity region that results from efficient packet duplication, and design the first fully distributed multicast traffic management policy that stabilizes any input rate in the interior of the capacity region while minimizing overall operational cost. Numerical results demonstrate the effectiveness of the proposed policy to achieve throughput- and cost-optimal delivery of stream-processing services over distributed computing networks.      
### 44.Optimal Cloud Network Control with Strict Latency Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2205.15556.pdf)
>  The timely delivery of resource-intensive and latency-sensitive services (e.g., industrial automation, augmented reality) over distributed computing networks (e.g., mobile edge computing) is drawing increasing attention. Motivated by the insufficiency of average delay performance guarantees provided by existing studies, we focus on the critical goal of delivering next generation real-time services ahead of corresponding deadlines on a per-packet basis, while minimizing overall cloud network resource cost. We introduce a novel queuing system that is able to track data packets' lifetime and formalize the optimal cloud network control problem with strict deadline constraints. After illustrating the main challenges in delivering packets to their destinations before getting dropped due to lifetime expiry, we construct an equivalent formulation, where relaxed flow conservation allows leveraging Lyapunov optimization to derive a provably near-optimal fully distributed algorithm for the original problem. Numerical results validate the theoretical analysis and show the superior performance of the proposed control policy compared with state-of-the-art cloud network control.      
### 45.AI-based automated Meibomian gland segmentation, classification and reflection correction in infrared Meibography  [ :arrow_down: ](https://arxiv.org/pdf/2205.15543.pdf)
>  Purpose: Develop a deep learning-based automated method to segment meibomian glands (MG) and eyelids, quantitatively analyze the MG area and MG ratio, estimate the meiboscore, and remove specular reflections from infrared images. Methods: A total of 1600 meibography images were captured in a clinical setting. 1000 images were precisely annotated with multiple revisions by investigators and graded 6 times by meibomian gland dysfunction (MGD) experts. Two deep learning (DL) models were trained separately to segment areas of the MG and eyelid. Those segmentation were used to estimate MG ratio and meiboscores using a classification-based DL model. A generative adversarial network was implemented to remove specular reflections from original images. Results: The mean ratio of MG calculated by investigator annotation and DL segmentation was consistent 26.23% vs 25.12% in the upper eyelids and 32.34% vs. 32.29% in the lower eyelids, respectively. Our DL model achieved 73.01% accuracy for meiboscore classification on validation set and 59.17% accuracy when tested on images from independent center, compared to 53.44% validation accuracy by MGD experts. The DL-based approach successfully removes reflection from the original MG images without affecting meiboscore grading. Conclusions: DL with infrared meibography provides a fully automated, fast quantitative evaluation of MG morphology (MG Segmentation, MG area, MG ratio, and meiboscore) which are sufficiently accurate for diagnosing dry eye disease. Also, the DL removes specular reflection from images to be used by ophthalmologists for distraction-free assessment.      
### 46.DeepDefacer: Automatic Removal of Facial Features via U-Net Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2205.15536.pdf)
>  Recent advancements in the field of magnetic resonance imaging (MRI) have enabled large-scale collaboration among clinicians and researchers for neuroimaging tasks. However, researchers are often forced to use outdated and slow software to anonymize MRI images for publication. These programs specifically perform expensive mathematical operations over 3D images that rapidly slow down anonymization speed as an image's volume increases in size. In this paper, we introduce DeepDefacer, an application of deep learning to MRI anonymization that uses a streamlined 3D U-Net network to mask facial regions in MRI images with a significant increase in speed over traditional de-identification software. We train DeepDefacer on MRI images from the Brain Development Organization (IXI) and International Consortium for Brain Mapping (ICBM) and quantitatively evaluate our model against a baseline 3D U-Net model with regards to Dice, recall, and precision scores. We also evaluate DeepDefacer against Pydeface, a traditional defacing application, with regards to speed on a range of CPU and GPU devices and qualitatively evaluate our model's defaced output versus the ground truth images produced by Pydeface. We provide a link to a PyPi program at the end of this manuscript to encourage further research into the application of deep learning to MRI anonymization.      
### 47.On Forward Kinematics of a 3SPR Parallel Manipulator  [ :arrow_down: ](https://arxiv.org/pdf/2205.15518.pdf)
>  In this paper, a new numerical method to solve the forward kinematics (FK) of a parallel manipulator with three-limb spherical-prismatic-revolute (3SPR) structure is presented. Unlike the existing numerical approaches that rely on computation of the manipulator's Jacobian matrix and its inverse at each iteration, the proposed algorithm requires much less computations to estimate the FK parameters. A cost function is introduced that measures the difference of the estimates from the actual FK values. At each iteration, the problem is decomposed into two steps. First, the estimates of the platform orientation from the heave estimates are obtained. Then, heave estimates are updated by moving in the gradient direction of the proposed cost function. To validate the performance of the proposed algorithm, it is compared against a Jacobian-based (JB) approach for a 3SPR parallel manipulator.      
### 48.Rethinking Graph Neural Networks for Anomaly Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.15508.pdf)
>  Graph Neural Networks (GNNs) are widely applied for graph anomaly detection. As one of the key components for GNN design is to select a tailored spectral filter, we take the first step towards analyzing anomalies via the lens of the graph spectrum. Our crucial observation is the existence of anomalies will lead to the `right-shift' phenomenon, that is, the spectral energy distribution concentrates less on low frequencies and more on high frequencies. This fact motivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed, BWGNN has spectral and spatial localized band-pass filters to better handle the `right-shift' phenomenon in anomalies. We demonstrate the effectiveness of BWGNN on four large-scale anomaly detection datasets. Our code and data are released at <a class="link-external link-https" href="https://github.com/squareRoot3/Rethinking-Anomaly-Detection" rel="external noopener nofollow">this https URL</a>      
### 49.A hybrid approach to seismic deblending: when physics meets self-supervision  [ :arrow_down: ](https://arxiv.org/pdf/2205.15395.pdf)
>  To limit the time, cost, and environmental impact associated with the acquisition of seismic data, in recent decades considerable effort has been put into so-called simultaneous shooting acquisitions, where seismic sources are fired at short time intervals between each other. As a consequence, waves originating from consecutive shots are entangled within the seismic recordings, yielding so-called blended data. For processing and imaging purposes, the data generated by each individual shot must be retrieved. This process, called deblending, is achieved by solving an inverse problem which is heavily underdetermined. Conventional approaches rely on transformations that render the blending noise into burst-like noise, whilst preserving the signal of interest. Compressed sensing type regularization is then applied, where sparsity in some domain is assumed for the signal of interest. The domain of choice depends on the geometry of the acquisition and the properties of seismic data within the chosen domain. In this work, we introduce a new concept that consists of embedding a self-supervised denoising network into the Plug-and-Play (PnP) framework. A novel network is introduced whose design extends the blind-spot network architecture of [28 ] for partially coherent noise (i.e., correlated in time). The network is then trained directly on the noisy input data at each step of the PnP algorithm. By leveraging both the underlying physics of the problem and the great denoising capabilities of our blind-spot network, the proposed algorithm is shown to outperform an industry-standard method whilst being comparable in terms of computational cost. Moreover, being independent on the acquisition geometry, our method can be easily applied to both marine and land data without any significant modification.      
### 50.Truly Deterministic Policy Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2205.15379.pdf)
>  In this paper, we present a policy gradient method that avoids exploratory noise injection and performs policy search over the deterministic landscape. By avoiding noise injection all sources of estimation variance can be eliminated in systems with deterministic dynamics (up to the initial state distribution). Since deterministic policy regularization is impossible using traditional non-metric measures such as the KL divergence, we derive a Wasserstein-based quadratic model for our purposes. We state conditions on the system model under which it is possible to establish a monotonic policy improvement guarantee, propose a surrogate function for policy gradient estimation, and show that it is possible to compute exact advantage estimates if both the state transition model and the policy are deterministic. Finally, we describe two novel robotic control environments -- one with non-local rewards in the frequency domain and the other with a long horizon (8000 time-steps) -- for which our policy gradient method (TDPO) significantly outperforms existing methods (PPO, TRPO, DDPG, and TD3). Our implementation with all the experimental settings is available at <a class="link-external link-https" href="https://github.com/ehsansaleh/code_tdpo" rel="external noopener nofollow">this https URL</a>      
### 51.Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data  [ :arrow_down: ](https://arxiv.org/pdf/2205.15370.pdf)
>  We propose Guided-TTS 2, a diffusion-based generative model for high-quality adaptive TTS using untranscribed data. Guided-TTS 2 combines a speaker-conditional diffusion model with a speaker-dependent phoneme classifier for adaptive text-to-speech. We train the speaker-conditional diffusion model on large-scale untranscribed datasets for a classifier-free guidance method and further fine-tune the diffusion model on the reference speech of the target speaker for adaptation, which only takes 40 seconds. We demonstrate that Guided-TTS 2 shows comparable performance to high-quality single-speaker TTS baselines in terms of speech quality and speaker similarity with only a ten-second untranscribed data. We further show that Guided-TTS 2 outperforms adaptive TTS baselines on multi-speaker datasets even with a zero-shot adaptation setting. Guided-TTS 2 can adapt to a wide range of voices only using untranscribed speech, which enables adaptive TTS with the voice of non-human characters such as Gollum in \textit{"The Lord of the Rings"}.      
### 52.Revisiting Audio Pattern Recognition for Asthma Medication Adherence: Evaluation with the RDA Benchmark Suite  [ :arrow_down: ](https://arxiv.org/pdf/2205.15360.pdf)
>  Asthma is a common, usually long-term respiratory disease with negative impact on society and the economy worldwide. Treatment involves using medical devices (inhalers) that distribute medication to the airways, and its efficiency depends on the precision of the inhalation technique. Health monitoring systems equipped with sensors and embedded with sound signal detection enable the recognition of drug actuation and could be powerful tools for reliable audio content analysis. This paper revisits audio pattern recognition and machine learning techniques for asthma medication adherence assessment and presents the Respiratory and Drug Actuation (RDA) Suite(<a class="link-external link-https" href="https://gitlab.com/vvr/monitoring-medication-adherence/rda-benchmark" rel="external noopener nofollow">this https URL</a>) for benchmarking and further research. The RDA Suite includes a set of tools for audio processing, feature extraction and classification and is provided along with a dataset consisting of respiratory and drug actuation sounds. The classification models in RDA are implemented based on conventional and advanced machine learning and deep network architectures. This study provides a comparative evaluation of the implemented approaches, examines potential improvements and discusses challenges and future tendencies.      
### 53.CTR: Checkpoint, Transfer, and Restore for Secure Enclaves  [ :arrow_down: ](https://arxiv.org/pdf/2205.15359.pdf)
>  Hardware-based Trusted Execution Environments (TEEs) are becoming increasingly prevalent in cloud computing, forming the basis for confidential computing. However, the security goals of TEEs sometimes conflict with existing cloud functionality, such as VM or process migration, because TEE memory cannot be read by the hypervisor, OS, or other software on the platform. Whilst some newer TEE architectures support migration of entire protected VMs, there is currently no practical solution for migrating individual processes containing in-process TEEs. The inability to migrate such processes leads to operational inefficiencies or even data loss if the host platform must be urgently restarted. <br>We present CTR, a software-only design to retrofit migration functionality into existing TEE architectures, whilst maintaining their expected security guarantees. Our design allows TEEs to be interrupted and migrated at arbitrary points in their execution, thus maintaining compatibility with existing VM and process migration techniques. By cooperatively involving the TEE in the migration process, our design also allows application developers to specify stateful migration-related policies, such as limiting the number of times a particular TEE may be migrated. Our prototype implementation for Intel SGX demonstrates that migration latency increases linearly with the size of the TEE memory and is dominated by TEE system operations.      
