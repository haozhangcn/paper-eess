# ArXiv eess --Tue, 14 Jun 2022
### 1.Unsupervised inter-frame motion correction for whole-body dynamic PET using convolutional long short-term memory in a convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2206.06341.pdf)
>  Subject motion in whole-body dynamic PET introduces inter-frame mismatch and seriously impacts parametric imaging. Traditional non-rigid registration methods are generally computationally intense and time-consuming. Deep learning approaches are promising in achieving high accuracy with fast speed, but have yet been investigated with consideration for tracer distribution changes or in the whole-body scope. In this work, we developed an unsupervised automatic deep learning-based framework to correct inter-frame body motion. The motion estimation network is a convolutional neural network with a combined convolutional long short-term memory layer, fully utilizing dynamic temporal features and spatial information. Our dataset contains 27 subjects each under a 90-min FDG whole-body dynamic PET scan. With 9-fold cross-validation, compared with both traditional and deep learning baselines, we demonstrated that the proposed network obtained superior performance in enhanced qualitative and quantitative spatial alignment between parametric $K_{i}$ and $V_{b}$ images and in significantly reduced parametric fitting error. We also showed the potential of the proposed motion correction method for impacting downstream analysis of the estimated parametric images, improving the ability to distinguish malignant from benign hypermetabolic regions of interest. Once trained, the motion estimation inference time of our proposed network was around 460 times faster than the conventional registration baseline, showing its potential to be easily applied in clinical settings.      
### 2.Robust Trajectory Tracking for Underactuated Quadrotors with Prescribed Performance  [ :arrow_down: ](https://arxiv.org/pdf/2206.06275.pdf)
>  We propose a control protocol based on the prescribed performance control (PPC) methodology for a quadrotor unmanned aerial vehicle (UAV). Quadrotor systems belong to the class of underactuated systems for which the original PPC methodology cannot be directly applied. We introduce the necessary design modifications to stabilize the considered system with prescribed performance. The proposed control protocol does not use any information of dynamic model parameters or exogenous disturbances. Furthermore, the stability analysis guarantees that the tracking errors remain inside of designer-specified time-varying functions, achieving prescribed performance independent from the control gains' selection. Finally, simulation results verify the theoretical results.      
### 3.MMMNA-Net for Overall Survival Time Prediction of Brain Tumor Patients  [ :arrow_down: ](https://arxiv.org/pdf/2206.06267.pdf)
>  Overall survival (OS) time is one of the most important evaluation indices for gliomas situations. Multimodal Magnetic Resonance Imaging (MRI) scans play an important role in the study of glioma prognosis OS time. Several deep learning-based methods are proposed for the OS time prediction on multi-modal MRI problems. However, these methods usually fuse multi-modal information at the beginning or at the end of the deep learning networks and lack the fusion of features from different scales. In addition, the fusion at the end of networks always adapts global with global (eg. fully connected after concatenation of global average pooling output) or local with local (eg. bilinear pooling), which loses the information of local with global. In this paper, we propose a novel method for multi-modal OS time prediction of brain tumor patients, which contains an improved nonlocal features fusion module introduced on different scales. Our method obtains a relative 8.76% improvement over the current state-of-art method (0.6989 vs. 0.6426 on accuracy). Extensive testing demonstrates that our method could adapt to situations with missing modalities. The code is available at <a class="link-external link-https" href="https://github.com/TangWen920812/mmmna-net" rel="external noopener nofollow">this https URL</a>.      
### 4.Enhancement of Rural Connectivity by Recycling TV Towers with Massive MIMO Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2206.06266.pdf)
>  Nowadays, the digital divide is one of the major issues facing the global community. Around 3 billion people worldwide are still not-connected or under-connected. In this article, we investigate the use of TV towers with multi user (MU) massive multiple input multiple output (mMIMO) techniques to offer connectivity in rural areas. Specifically, the coverage range is assessed for a MU mMIMO base station (BS) mounted on a high tower as a TV tower, and compared with a legacy mMIMO BS. The obtained results show that one high tower BS can cover an area at least 25 times larger than the area covered by a legacy BS. This is of high interest as recycling TV towers can enhance the rural connectivity with low expenditures. We apply the proposed solution to a realistic case study in an Ethiopian rural area, based on population densities and locations of current BS and TV towers. Our study shows that a high number of people can be covered by existing TV towers. Additional possible solutions to enhance rural connectivity are discussed in the last section.      
### 5.Automatic Polyp Segmentation with Multiple Kernel Dilated Convolution Network  [ :arrow_down: ](https://arxiv.org/pdf/2206.06264.pdf)
>  The detection and removal of precancerous polyps through colonoscopy is the primary technique for the prevention of colorectal cancer worldwide. However, the miss rate of colorectal polyp varies significantly among the endoscopists. It is well known that a computer-aided diagnosis (CAD) system can assist endoscopists in detecting colon polyps and minimize the variation among endoscopists. In this study, we introduce a novel deep learning architecture, named {\textbf{MKDCNet}}, for automatic polyp segmentation robust to significant changes in polyp data distribution. MKDCNet is simply an encoder-decoder neural network that uses the pre-trained \textit{ResNet50} as the encoder and novel \textit{multiple kernel dilated convolution (MKDC)} block that expands the field of view to learn more robust and heterogeneous representation. Extensive experiments on four publicly available polyp datasets and cell nuclei dataset show that the proposed MKDCNet outperforms the state-of-the-art methods when trained and tested on the same dataset as well when tested on unseen polyp datasets from different distributions. With rich results, we demonstrated the robustness of the proposed architecture. From an efficiency perspective, our algorithm can process at ($\approx45$) frames per second on RTX 3090 GPU. MKDCNet can be a strong benchmark for building real-time systems for clinical colonoscopies. The code of the proposed MKDCNet is available at \url{<a class="link-external link-https" href="https://github.com/nikhilroxtomar/MKDCNet" rel="external noopener nofollow">this https URL</a>}.      
### 6.Realistic Gramophone Noise Synthesis using a Diffusion Model  [ :arrow_down: ](https://arxiv.org/pdf/2206.06259.pdf)
>  This paper introduces a novel data-driven strategy for synthesizing gramophone noise textures. A diffusion probabilistic model is applied to generate highly realistic quasiperiodic noises. The proposed model is designed to generate samples of length equal to one disk revolution, but a method to generate plausible periodic variations between revolutions is also proposed. A guided approach is also applied as a conditioning method, where an audio signal generated with manually-tuned signal processing is refined via reverse diffusion to appear more realistically sounding. The method has been evaluated in a subjective listening test, in which the participants were often unable to recognize the synthesized signals from the real ones. The synthetic noises produced with the best proposed unconditional method are statistically indistinguishable from real noise recordings. This work shows the potential of diffusion models for highly realistic audio synthesis tasks.      
### 7.RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2206.06253.pdf)
>  In clinical practice, anisotropic volumetric medical images with low through-plane resolution are commonly used due to short acquisition time and lower storage cost. Nevertheless, the coarse resolution may lead to difficulties in medical diagnosis by either physicians or computer-aided diagnosis algorithms. Deep learning-based volumetric super-resolution (SR) methods are feasible ways to improve resolution, with convolutional neural networks (CNN) at their core. Despite recent progress, these methods are limited by inherent properties of convolution operators, which ignore content relevance and cannot effectively model long-range dependencies. In addition, most of the existing methods use pseudo-paired volumes for training and evaluation, where pseudo low-resolution (LR) volumes are generated by a simple degradation of their high-resolution (HR) counterparts. However, the domain gap between pseudo- and real-LR volumes leads to the poor performance of these methods in practice. In this paper, we build the first public real-paired dataset RPLHR-CT as a benchmark for volumetric SR, and provide baseline results by re-implementing four state-of-the-art CNN-based methods. Considering the inherent shortcoming of CNN, we also propose a transformer volumetric super-resolution network (TVSRN) based on attention mechanisms, dispensing with convolutions entirely. This is the first research to use a pure transformer for CT volumetric SR. The experimental results show that TVSRN significantly outperforms all baselines on both PSNR and SSIM. Moreover, the TVSRN method achieves a better trade-off between the image quality, the number of parameters, and the running time. Data and code are available at <a class="link-external link-https" href="https://github.com/smilenaxx/RPLHR-CT" rel="external noopener nofollow">this https URL</a>.      
### 8.Prostate Cancer Malignancy Detection and localization from mpMRI using auto-Deep Learning: One Step Closer to Clinical Utilization  [ :arrow_down: ](https://arxiv.org/pdf/2206.06235.pdf)
>  Automatic diagnosis of malignant prostate cancer patients from mpMRI has been studied heavily in the past years. Model interpretation and domain drift have been the main road blocks for clinical utilization. As an extension from our previous work where we trained a customized convolutional neural network on a public cohort with 201 patients and the cropped 2D patches around the region of interest were used as the input, the cropped 2.5D slices of the prostate glands were used as the input, and the optimal model were searched in the model space using autoKeras. Something different was peripheral zone (PZ) and central gland (CG) were trained and tested separately, the PZ detector and CG detector were demonstrated effectively in highlighting the most suspicious slices out of a sequence, hopefully to greatly ease the workload for the physicians.      
### 9.Automated Evaluation of Standardized Dementia Screening Tests  [ :arrow_down: ](https://arxiv.org/pdf/2206.06208.pdf)
>  For dementia screening and monitoring, standardized tests play a key role in clinical routine since they aim at minimizing subjectivity by measuring performance on a variety of cognitive tasks. In this paper, we report on a study that consists of a semi-standardized history taking followed by two standardized neuropsychological tests, namely the SKT and the CERAD-NB. The tests include basic tasks such as naming objects, learning word lists, but also widely used tools such as the MMSE. Most of the tasks are performed verbally and should thus be suitable for automated scoring based on transcripts. For the first batch of 30 patients, we analyze the correlation between expert manual evaluations and automatic evaluations based on manual and automatic transcriptions. For both SKT and CERAD-NB, we observe high to perfect correlations using manual transcripts; for certain tasks with lower correlation, the automatic scoring is stricter than the human reference since it is limited to the audio. Using automatic transcriptions, correlations drop as expected and are related to recognition accuracy; however, we still observe high correlations of up to 0.98 (SKT) and 0.85 (CERAD-NB). We show that using word alternatives helps to mitigate recognition errors and subsequently improves correlation with expert scores.      
### 10.Toward Zero Oracle Word Error Rate on the Switchboard Benchmark  [ :arrow_down: ](https://arxiv.org/pdf/2206.06192.pdf)
>  The "Switchboard benchmark" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5\% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.      
### 11.AmbiSep: Ambisonic-to-Ambisonic Reverberant Speech Separation Using Transformer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.06184.pdf)
>  Consider a multichannel Ambisonic recording containing a mixture of several reverberant speech signals. Retreiving the reverberant Ambisonic signals corresponding to the individual speech sources blindly from the mixture is a challenging task as it requires to estimate multiple signal channels for each source. In this work, we propose AmbiSep, a deep neural network-based plane-wave domain masking approach to solve this task. The masking network uses learned feature representations and transformers in a triple-path processing configuration. We train and evaluate the proposed network architecture on a spatialized WSJ0-2mix dataset, and show that the method achieves a multichannel scale-invariant signal-to-distortion ratio improvement of 17.7 dB on the blind test set, while preserving the spatial characteristics of the separated sounds.      
### 12.LiVeR: Lightweight Vehicle Detection and Classification in Real-Time  [ :arrow_down: ](https://arxiv.org/pdf/2206.06173.pdf)
>  Detection and classification of vehicles are very significant components in an Intelligent-Transportation System. Existing solutions not only use heavy-weight and costly equipment, but also largely depend on constant cloud (Internet) connectivity, as well as adequate uninterrupted power-supply. Such dependencies make these solutions fundamentally impractical considering the possible adversities of outdoor environment as well as requirement of correlated wide-area operation. For practical use, apart from being technically sound and accurate, a solution has to be lightweight, cost-effective, easy-to-install, flexible as well as supporting efficient time-correlated coverage over large area. In this work we propose an IoT-assisted strategy to fulfil all these goals together. We adopt a top-down approach where we first introduce a lightweight framework for time-correlated low-cost wide-area measurement and then reuse the concept for developing the individual measurement units. Our extensive outdoor measurement studies and trace-based simulation on the empirical data show about 98% accuracy in vehicle detection and upto 93% of accuracy in classification of the vehicles over moderately busy urban roads.      
### 13.RIS-ADMM: An ADMM-Based Passive and Sparse Sensing Method with Interference Removal  [ :arrow_down: ](https://arxiv.org/pdf/2206.06172.pdf)
>  The reconfigurable intelligent surface (RIS) has been a potential technology for future radar and wireless communication applications. In this letter, the passive sensing problem using wireless communications signal and RIS is addressed in the scenario with the interference from the wireless access point (AP). An atomic norm minimization (ANM) method is formulated to exploit the target sparsity in the spatial domain and estimate the direction of arrival (DOA), but the conventional semidefinite programming (SDP)-based method to solve the ANM problem is complex and cannot be realized efficiently. Therefore, we proposed a RIS-ADMM method as an alternating direction method of multipliers (ADMM)-based iterative method. The closed-form expressions are derived, and the interference signal is also suppressed. Simulation results show that the proposed RIS-ADMM method outperforms the compared methods in the DOA estimation performance with low computational complexity. The code about the proposed method is avaliable online \url{<a class="link-external link-https" href="https://github.com/chenpengseu/RIS-ADMM.git" rel="external noopener nofollow">this https URL</a>}.      
### 14.A Semi Empirical Approach to a Physically Based Aging Model for Home Energy Management Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.06158.pdf)
>  A growing interest in the study of aging related phenomena in lithium-ion batteries is propelled by the increasing utilization of energy storage systems in electric vehicles and in buildings as stationery energy accumulators paired with renewable energy sources. This paper proposes a mixed-degradation model approach that combines the benefits of a semi-empirical approach with that of a physics-based model. This enables easy calibration for different battery chemistries, the ability to extrapolate when necessary, and is computationally efficient enough to be coupled with real-time running control systems. To demonstrate the effectiveness of the proposed approach, the effect of two different control strategies in a smart home energy management system is demonstrated on the aging of a Lithium iron phosphate (LFP) battery.      
### 15.SyntheX: Scaling Up Learning-based X-ray Image Analysis Through In Silico Experiments  [ :arrow_down: ](https://arxiv.org/pdf/2206.06127.pdf)
>  Artificial intelligence (AI) now enables automated interpretation of medical images for clinical use. However, AI's potential use for interventional images (versus those involved in triage or diagnosis), such as for guidance during surgery, remains largely untapped. This is because surgical AI systems are currently trained using post hoc analysis of data collected during live surgeries, which has fundamental and practical limitations, including ethical considerations, expense, scalability, data integrity, and a lack of ground truth. Here, we demonstrate that creating realistic simulated images from human models is a viable alternative and complement to large-scale in situ data collection. We show that training AI image analysis models on realistically synthesized data, combined with contemporary domain generalization or adaptation techniques, results in models that on real data perform comparably to models trained on a precisely matched real data training set. Because synthetic generation of training data from human-based models scales easily, we find that our model transfer paradigm for X-ray image analysis, which we refer to as SyntheX, can even outperform real data-trained models due to the effectiveness of training on a larger dataset. We demonstrate the potential of SyntheX on three clinical tasks: Hip image analysis, surgical robotic tool detection, and COVID-19 lung lesion segmentation. SyntheX provides an opportunity to drastically accelerate the conception, design, and evaluation of intelligent systems for X-ray-based medicine. In addition, simulated image environments provide the opportunity to test novel instrumentation, design complementary surgical approaches, and envision novel techniques that improve outcomes, save time, or mitigate human error, freed from the ethical and practical considerations of live human data collection.      
### 16.DCASE 2022 Challenge Task 6B: Language-Based Audio Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2206.06108.pdf)
>  In this report, we introduce the task setup and the baseline system for the sub-task B of the DCASE 2022 Challenge Task 6: language-based audio retrieval subtask. For this subtask, the Clotho v2 dataset is utilized as the development dataset, and an additional dataset consisting of 1,000 audio-caption pairs as the evaluation dataset. We train the baseline system with the development dataset, and evaluate it on the evaluation dataset to provide some initial results for this subtask.      
### 17.Physics-informed EDFA Gain Model Based on Active Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.06077.pdf)
>  We propose a physics-informed EDFA gain model based on the active learning method. Experimental results show that the proposed modelling method can reach a higher optimal accuracy and reduce ~90% training data to achieve the same performance compared with the conventional method.      
### 18.On Connections between Opacity and Security in Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.06074.pdf)
>  Opacity and attack detectability are important properties for any system as they allow the states to remain private and malicious attacks to be detected, respectively. In this paper, we show that a fundamental trade-off exists between these properties for a linear dynamical system, in the sense that if an opaque system is subjected to attacks, all attacks cannot be detected. We first characterize the opacity conditions for the system in terms of its weakly unobservable subspace (WUS) and show that the number of opaque states is proportional to the size of the WUS. Further, we establish conditions under which increasing the opaque sets also increases the set of undetectable attacks. This highlights a fundamental trade-off between security and privacy. We demonstrate application of our results on a remotely controlled automotive system.      
### 19.Annular Computational Imaging: Capture Clear Panoramic Images through Simple Lens  [ :arrow_down: ](https://arxiv.org/pdf/2206.06070.pdf)
>  Panoramic Annular Lens (PAL), composed of few lenses, has great potential in panoramic surrounding sensing tasks for mobile and wearable devices because of its tiny size and large Field of View (FoV). However, the image quality of tiny-volume PAL confines to optical limit due to the lack of lenses for aberration correction. In this paper, we propose an Annular Computational Imaging (ACI) framework to break the optical limit of light-weight PAL design. To facilitate learning-based image restoration, we introduce a wave-based simulation pipeline for panoramic imaging and tackle the synthetic-to-real gap through multiple data distributions. The proposed pipeline can be easily adapted to any PAL with design parameters and is suitable for loose-tolerance designs. Furthermore, we design the Physics Informed Image Restoration Network (PI2RNet), considering the physical priors of panoramic imaging and physics-informed learning. At the dataset level, we create the DIVPano dataset and the extensive experiments on it illustrate that our proposed network sets the new state of the art in the panoramic image restoration under spatially-variant degradation. In addition, the evaluation of the proposed ACI on a simple PAL with only 3 spherical lenses reveals the delicate balance between high-quality panoramic imaging and compact design. To the best of our knowledge, we are the first to explore Computational Imaging (CI) in PAL. Code and datasets will be made publicly available at <a class="link-external link-https" href="https://github.com/zju-jiangqi/ACI-PI2RNet" rel="external noopener nofollow">this https URL</a>.      
### 20.Deep ensemble learning for segmenting tuberculosis-consistent manifestations in chest radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2206.06065.pdf)
>  Automated segmentation of tuberculosis (TB)-consistent lesions in chest X-rays (CXRs) using deep learning (DL) methods can help reduce radiologist effort, supplement clinical decision-making, and potentially result in improved patient treatment. The majority of works in the literature discuss training automatic segmentation models using coarse bounding box annotations. However, the granularity of the bounding box annotation could result in the inclusion of a considerable fraction of false positives and negatives at the pixel level that may adversely impact overall semantic segmentation performance. This study (i) evaluates the benefits of using fine-grained annotations of TB-consistent lesions and (ii) trains and constructs ensembles of the variants of U-Net models for semantically segmenting TB-consistent lesions in both original and bone-suppressed frontal CXRs. We evaluated segmentation performance using several ensemble methods such as bitwise AND, bitwise-OR, bitwise-MAX, and stacking. We observed that the stacking ensemble demonstrated superior segmentation performance (Dice score: 0.5743, 95% confidence interval: (0.4055,0.7431)) compared to the individual constituent models and other ensemble methods. To the best of our knowledge, this is the first study to apply ensemble learning to improve fine-grained TB-consistent lesion segmentation performance.      
### 21.Sequential Convex Programming for Optimal Line of Sight Steering in Agile Missions  [ :arrow_down: ](https://arxiv.org/pdf/2206.06061.pdf)
>  The trend toward onboard autonomy and spacecraft minimization present significant potential for advances in efficient Line of Sight management by making optimal use of the limited torque resources available. At SENER Aeroespacial, we are implementing AOCS algorithms capable of providing agility in different observation scenarios in which the exploitation of the resources is fundamental for the mission success. In this contribution, we present an in-house optimization toolbox for onboard guidance, the SENER Optimization Toolbox, and we propose its use for online attitude guidance of an agile space-craft using Control Moment Gyroscopes. We propose different optimization schemes based on Sequential Convex Programming aiming for reducing the computational burden for real-time implementations. The results highlight the potential for performance improvements when making use of embedded optimal control for fast slew maneuvers: with proposed schemes, we find solutions that implicitly manage singularities and are up to 12.2% faster than classical bang-bang maneuvers while increasing the smoothness of the trajectories to minimize the excitation of flexible modes.      
### 22.Energy-Efficient Wake-Up Signalling for Machine-Type Devices Based on Traffic-Aware Long-Short Term Memory Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2206.06058.pdf)
>  Reducing energy consumption is a pressing issue in low-power machine-type communication (MTC) networks. In this regard, the Wake-up Signal (WuS) technology, which aims to minimize the energy consumed by the radio interface of the machine-type devices (MTDs), stands as a promising solution. However, state-of-the-art WuS mechanisms use static operational parameters, so they cannot efficiently adapt to the system dynamics. To overcome this, we design a simple but efficient neural network to predict MTC traffic patterns and configure WuS accordingly. Our proposed forecasting WuS (FWuS) leverages an accurate long-short term memory (LSTM)- based traffic prediction that allows extending the sleep time of MTDs by avoiding frequent page monitoring occasions in idle state. Simulation results show the effectiveness of our approach. The traffic prediction errors are shown to be below 4%, being false alarm and miss-detection probabilities respectively below 8.8% and 1.3%. In terms of energy consumption reduction, FWuS can outperform the best benchmark mechanism in up to 32%. Finally, we certify the ability of FWuS to dynamically adapt to traffic density changes, promoting low-power MTC scalability      
### 23.Robust Adaptive Beamforming via Worst-Case SINR Maximization with Nonconvex Uncertainty Sets  [ :arrow_down: ](https://arxiv.org/pdf/2206.06001.pdf)
>  This paper considers a formulation of the robust adaptive beamforming (RAB) problem based on worst-case signal-to-interference-plus-noise ratio (SINR) maximization with a nonconvex uncertainty set for the steering vectors. The uncertainty set consists of a similarity constraint and a (nonconvex) double-sided ball constraint. The worst-case SINR maximization problem is turned into a quadratic matrix inequality (QMI) problem using the strong duality of semidefinite programming. Then a linear matrix inequality (LMI) relaxation for the QMI problem is proposed, with an additional valid linear constraint. Necessary and sufficient conditions for the tightened LMI relaxation problem to have a rank-one solution are established. When the tightened LMI relaxation problem still has a high-rank solution, the LMI relaxation problem is further restricted to become a bilinear matrix inequality (BLMI) problem. We then propose an iterative algorithm to solve the BLMI problem that finds an optimal/suboptimal solution for the original RAB problem by solving the BLMI formulations. To validate our results, simulation examples are presented to demonstrate the improved array output SINR of the proposed robust beamformer.      
### 24.GPU-Accelerated Machine Learning in Non-Orthogonal Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2206.05998.pdf)
>  Non-orthogonal multiple access (NOMA) is an interesting technology that enables massive connectivity as required in future 5G and 6G networks. While purely linear processing already achieves good performance in NOMA systems, in certain scenarios, non-linear processing is mandatory to ensure acceptable performance. In this paper, we propose a neural network architecture that combines the advantages of both linear and non-linear processing. Its real-time detection performance is demonstrated by a highly efficient implementation on a graphics processing unit (GPU). Using real measurements in a laboratory environment, we show the superiority of our approach over conventional methods.      
### 25.Discretization and Stabilization of Energy-Based Controller for Period Switching Control and Flexible Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2206.05994.pdf)
>  Emerging advanced control applications, with increased complexity in software but limited computing resources, suggest that real-time controllers should have adaptable designs. These control strategies also should be designed with consideration of the run-time behavior of the system. One of such research attempts is to design the controller along with the task scheduler, known as control-scheduling co-design, for more predictable timing behavior as well as surviving system overloads. Unlike traditional controller designs, which have equal-distance sampling periods, the co-design approach increases the system flexibility and resilience by explicitly considering timing properties, for example using an event-based controller or with multiple sampling times (non-uniform sampling and control). Within this context, we introduce the first work on the discretization of an energy-based controller that can switch arbitrarily between multiple periods and adjust the control parameters accordingly without destabilizing the system. A digital controller design based on this paradigm for a DC motor with an elastic load as an example is introduced and the stability condition is given based on the proposed Lyapunov function. The method is evaluated with various computer-based simulations which demonstrate its effectiveness.      
### 26.Channel Sounder with Over-the-Air Antenna Synchronization: Absolute Phase and Timing Calibration Using Known Transmitter Locations  [ :arrow_down: ](https://arxiv.org/pdf/2206.05984.pdf)
>  Synchronization of transceiver chains is a major challenge in the practical realization of massive MIMO and especially distributed massive MIMO. While frequency synchronization is comparatively easy to achieve, estimating the carrier phase and sampling time offsets of individual transceivers is challenging. However, under the assumption of phase and time offsets that are constant over some duration and knowing the positions of several transmit and receive antennas, it is possible to estimate and compensate for these offsets even in scattering environments with multipath propagation components. The resulting phase and time calibration is a prerequisite for applying classical antenna array processing methods to massive MIMO arrays and for transferring machine learning models either between simulation and deployment or from one radio environment to another. Algorithms for phase and time offset estimation are presented and several investigations on large datasets generated by an over-the-air-synchronized channel sounder are carried out.      
### 27.Reduction and Observer Design for a Grey-Box Model in Continuous Pharmaceutical Manufacturing  [ :arrow_down: ](https://arxiv.org/pdf/2206.05983.pdf)
>  In this contribution, a novel Reduced Order Model (ROM) formulation of the grey-box model proposed in Elkhashap et al. (2020a) for the pharmaceutical continuous vibrated fluid bed dryer (VFBD) is presented. The ROM exploits the $\mathcal{H}_2$-norm projection-based model order reduction method after a special solution formulation of the model's infinite-dimensional part. This is mainly by introducing a vector field mapping between the model parts casting the semi-discretized PDE into a bilinear form. The ROM produced is then integrated into an nonlinear Kalman Filtering-based observer design also handling the estimation of the model's algebraic variables. Evaluations of the FOM, ROM, ROM-based observer variants, and the FOM-based observer are performed using Monte-Carlo simulations as well as simulations based on experimental data of the real system. It is shown that the ROM could reproduce the FOM states accurately with a relative mean square error below $0.3\,\%$ for the experimental data simulation. This is while reaching a computational-time reduction up to a factor of $40$. The ROM-based observer with algebraic states correction is shown (using Monte-Carlo simulations) to be able to converge to the true values for all cases regardless of initialization. Moreover, it is also shown that the performance degradation of the observer due to reduction is practically insignificant. This is while the computational speedup of the observer due to reduction reached a factor of more than third order of magnitude.      
### 28.Techno Economic Modeling for Agrivoltaics: Can Agrivoltaics be more profitable than Ground mounted PV?  [ :arrow_down: ](https://arxiv.org/pdf/2206.05964.pdf)
>  Agrivoltaics (AV) is a dual land-use approach to collocate solar energy generation with agriculture for preserving the terrestrial ecosystem and enabling food-energy-water synergies. Here, we present a systematic approach to model the economic performance of AV relative to standalone ground-mounted PV (GMPV) and explore how the module design configuration can affect the dual food-energy economic performance. A remarkably simple criterion for economic feasibility is quantified that relates the land preservation cost to dual food-energy profit. We explore case studies including both high and low value crops under fixed tilt bifacial modules oriented either along the conventional North/South (N/S) facings or vertical East/West (E/W) facings. For each module configuration, the array density is varied to explore an economically feasible design space relative to GMPV for a range of module to land cost ratio (M_L) - a location-specific indicator relating the module technology (hardware and installation) costs to the soft (land acquisition, tax, overheads, etc.) costs. To offset a typically higher AV module cost needed to preserve the cropland, both E/W and N/S orientated modules favor high value crops, reduced (&lt;60%) module density, and higher M_L (&gt;25). In contrast, higher module density and an increased feed-in-tariff (FIT) relative to GMPV are desirable at lower M_L. The economic trends vary sharply for M_L&lt; 10 but tend to saturate for M_L&gt; 20. For low value crops, ~15% additional FIT can enable economic equivalence to GMPV at standard module density. The proposed modeling framework can provide a valuable tool for AV stakeholders to assess, predict, and optimize the techno-economic design for AV      
### 29.Data-Driven Denoising of Accelerometer Signals  [ :arrow_down: ](https://arxiv.org/pdf/2206.05937.pdf)
>  Modern navigation solutions are largely dependent on the performances of the standalone inertial sensors, especially at times when no external sources are available. During these outages, the inertial navigation solution is likely to degrade over time due to instrumental noises sources, particularly when using consumer low-cost inertial sensors. Conventionally, model-based estimation algorithms are employed to reduce noise levels and enhance meaningful information, thus improving the navigation solution directly. However, guaranteeing their optimality often proves to be challenging as sensors performance differ in manufacturing quality, process noise modeling, and calibration precision. In the literature, most inertial denoising models are model-based when recently several data-driven approaches were suggested primarily for gyroscope measurements denoising. Data-driven approaches for accelerometer denoising task are more challenging due to the unknown gravity projection on the accelerometer axes. To fill this gap, we propose several learning-based approaches and compare their performances with prominent denoising algorithms, in terms of pure noise removal, followed by stationary coarse alignment procedure. Based on the benchmarking results, obtained in field experiments, we show that: (i) learning-based models perform better than traditional signal processing filtering; (ii) non-parametric kNN algorithm outperforms all state of the art deep learning models examined in this study; (iii) denoising can be fruitful for pure inertial signal reconstruction, but moreover for navigation-related tasks, as both errors are shown to be reduced up to one order of magnitude.      
### 30.Fluorescence angiography classification in colorectal surgery -- A preliminary report  [ :arrow_down: ](https://arxiv.org/pdf/2206.05935.pdf)
>  Background: Fluorescence angiography has shown very promising results in reducing anastomotic leaks by allowing the surgeon to select optimally perfused tissue. However, subjective interpretation of the fluorescent signal still hinders broad application of the technique, as significant variation between different surgeons exists. Our aim is to develop an artificial intelligence algorithm to classify colonic tissue as 'perfused' or 'not perfused' based on intraoperative fluorescence angiography data. <br>Methods: A classification model with a Resnet architecture was trained on a dataset of fluorescence angiography videos of colorectal resections at a tertiary referral centre. Frames corresponding to fluorescent and non-fluorescent segments of colon were used to train a classification algorithm. Validation using frames from patients not used in the training set was performed, including both data collected using the same equipment and data collected using a different camera. Performance metrics were calculated, and saliency maps used to further analyse the output. A decision boundary was identified based on the tissue classification. <br>Results: A convolutional neural network was successfully trained on 1790 frames from 7 patients and validated in 24 frames from 14 patients. The accuracy on the training set was 100%, on the validation set was 80%. Recall and precision were respectively 100% and 100% on the training set and 68.8% and 91.7% on the validation set. <br>Conclusion: Automated classification of intraoperative fluorescence angiography with a high degree of accuracy is possible and allows automated decision boundary identification. This will enable surgeons to standardise the technique of fluorescence angiography. A web based app was made available to deploy the algorithm.      
### 31.Intra Encoding Complexity Control with a Time-Cost Model for Versatile Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2206.05889.pdf)
>  For the latest video coding standard Versatile Video Coding (VVC), the encoding complexity is much higher than previous video coding standards to achieve a better coding efficiency, especially for intra coding. The complexity becomes a major barrier of its deployment and use. Even with many fast encoding algorithms, it is still practically important to control the encoding complexity to a given level. Inspired by rate control algorithms, we propose a scheme to precisely control the intra encoding complexity of VVC. In the proposed scheme, a Time-PlanarCost (viz. Time-Cost, or T-C) model is utilized for CTU encoding time estimation. By combining a set of predefined parameters and the T-C model, CTU-level complexity can be roughly controlled. Then to achieve a precise picture-level complexity control, a framework is constructed including uneven complexity pre-allocation, preset selection and feedback. Experimental results show that, for the challenging intra coding scenario, the complexity error quickly converges to under 3.21%, while keeping a reasonable time saving and rate-distortion (RD) performance. This proves the efficiency of the proposed methods.      
### 32.Adaptive Multi-robot Implicit Control of Heterogeneous Herds  [ :arrow_down: ](https://arxiv.org/pdf/2206.05888.pdf)
>  This paper presents a novel control strategy to herd groups of non-cooperative evaders by means of a team of robotic herders. In herding problems, the motion of the evaders is typically determined by strongly nonlinear and heterogeneous reactive dynamics, which makes the development of flexible control solutions a challenging problem. In this context, we propose Implicit Control, an approach that leverages numerical analysis theory to find suitable herding inputs even when the nonlinearities in the evaders' dynamics yield to implicit equations. The intuition behind this methodology consists in driving the input, rather than computing it, towards the unknown value that achieves the desired dynamic behavior of the herd. The same idea is exploited to develop an adaptation law, with stability guarantees, that copes with uncertainties in the herd's models. Moreover, our solution is completed with a novel caging technique based on uncertainty models and Control Barrier Functions (CBFs), together with a distributed estimator to overcome the need of complete perfect measurements. Different simulations and experiments validate the generality and flexibility of the proposal.      
### 33.Revisiting Whole-Slide Image Pyramids for Cancer Prognosis via Dual-Stream Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.05782.pdf)
>  The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a challenging task. Most existing approaches focus solely on single-resolution images. The multi-resolution schemes, utilizing image pyramids to enhance WSI visual representations, have not yet been paid enough attention to. In order to explore a multi-resolution solution for improving cancer prognosis accuracy, this paper proposes a dual-stream architecture to model WSIs by an image pyramid strategy. This architecture consists of two sub-streams: one is for low-resolution WSIs, and the other is especially for high-resolution ones. Compared to other approaches, our scheme has three highlights: (i) there exists a one-to-one relation between stream and resolution; (ii) a square pooling layer is added to align the patches from two resolution streams, largely reducing computation cost and enabling a natural stream feature fusion; (iii) a cross-attention-based method is proposed to pool high-resolution patches spatially under the guidance of low-resolution ones. We validate our scheme on three publicly-available datasets, a total number of 3,101 WSIs from 1,911 patients. Experimental results verify that (1) hierarchical dual-stream representation is more effective than single-stream ones for cancer prognosis, gaining an average C-Index rise of 5.0% and 1.8% on a single low-resolution and high-resolution stream, respectively; (2) our dual-stream scheme could outperform current state-of-the-art ones, by a 5.1% average improvement of C-Index; (3) the cancer diseases with observable survival differences could have different preferences for model complexity. Our scheme could serve as an alternative tool for further facilitating WSI prognosis research.      
### 34.Concurrent Learning Based Adaptive Control of Euler Lagrange Systems with Guaranteed Parameter Convergence  [ :arrow_down: ](https://arxiv.org/pdf/2206.05753.pdf)
>  This work presents a solution to the adaptive tracking control of Euler Lagrange systems with guaranteed tracking and parameter estimation error convergence. Specifically a concurrent learning based update rule fused by the filtered version of the desired system dynamics in conjunction with a desired state based regression matrix has been utilized to ensure that both the position tracking error and parameter estimation error terms converge to origin exponentially. As the regression matrix used in proposed controller makes use of the desired versions of the system states, an initial, sufficiently exciting memory stack can be formed from the knowledge of the desired system trajectory a priori, thus removing the initial excitation condition required for the previously proposed concurrent learning based controllers in the literature. The output feedback versions of the proposed method where only the position measurements are available for the controller design, (for both gradient and composite type adaptions) are also presented in order to illustrate the modularity of the proposed method. The stability and boundedness of the closed loop signals for all the proposed controllers are ensured via Lyapunov based analysis. %Trajectory tracking control of a class of fully actuated Euler Lagrange systems is considered in this work. System dynamics is considered to be subject to parametric uncertainties and on--line identification uncertain model parameters is also aimed. When compared with the relevant past research, via a novel approach, desired states are proposed to be used in forming the regression matrix and a desired compensation based concurrent learning type adaptive update rule is designed. Via utilizing novel Lyapunov analysis, semi--global exponential convergence of both tracking and parameter identification error to the origin is ensured.      
### 35.Mismatched Estimation in the Distance Geometry Problem  [ :arrow_down: ](https://arxiv.org/pdf/2206.05727.pdf)
>  We investigate mismatched estimation in the context of the distance geometry problem (DGP). In the DGP, for a set of points, we are given noisy measurements of pairwise distances between the points, and our objective is to determine the geometric locations of the points. A common approach to deal with noisy measurements of pairwise distances is to compute least-squares estimates of the locations of the points. However, these least-squares estimates are likely to be suboptimal, because they do not necessarily maximize the correct likelihood function. In this paper, we argue that more accurate estimates can be obtained when an estimation procedure using the correct likelihood function of noisy measurements is performed. Our numerical results demonstrate that least-squares estimates can be suboptimal by several dB.      
### 36.Scheduling Delays and Curtailment for Household Appliances with Deterministic Load Profiles using MPC  [ :arrow_down: ](https://arxiv.org/pdf/2206.05697.pdf)
>  Smart home appliances can time-shift and curtail their power demand to assist demand side management or allow operation with limited power, as in an off-grid application. This paper proposes a scheduling process to start appliances with time-varying deterministic load profiles. Self-triggered model predictive control is used to limit the household net power demand below a given threshold. Meanwhile, deterministic load profiles are more difficult to schedule compared to variable charging or thermal loads because system failure will occur once power demand is not satisfied. The proposed scheme formulates the decision of the load shifting time as a continuous optimization problem, and an inhomogeneous time grid system is introduced to handle the optimization of different appliances and their consensus at this resolution. The efficacy of the proposed scheme is studied by numerical comparison with a mixed-integer MPC controller and by a case study of three home appliances and an interruptible washing machine.      
### 37.PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model  [ :arrow_down: ](https://arxiv.org/pdf/2206.05695.pdf)
>  Early prediction of pathological complete response (pCR) following neoadjuvant chemotherapy (NAC) for breast cancer plays a critical role in surgical planning and optimizing treatment strategies. Recently, machine and deep-learning based methods were suggested for early pCR prediction from multi-parametric MRI (mp-MRI) data including dynamic contrast-enhanced MRI and diffusion-weighted MRI (DWI) with moderate success. We introduce PD-DWI, a physiologically decomposed DWI machine-learning model to predict pCR from DWI and clinical data. Our model first decomposes the raw DWI data into the various physiological cues that are influencing the DWI signal and then uses the decomposed data, in addition to clinical variables, as the input features of a radiomics-based XGBoost model. We demonstrated the added-value of our PD-DWI model over conventional machine-learning approaches for pCR prediction from mp-MRI data using the publicly available Breast Multi-parametric MRI for prediction of NAC Response (BMMR2) challenge. Our model substantially improves the area under the curve (AUC), compared to the current best result on the leaderboard (0.8849 vs. 0.8397) for the challenge test set. PD-DWI has the potential to improve prediction of pCR following NAC for breast cancer, reduce overall mp-MRI acquisition times and eliminate the need for contrast-agent injection.      
### 38.Resilience for Distributed Consensus with Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2206.05662.pdf)
>  This paper proposes a new approach that enables multi-agent systems to achieve resilient constrained consensus in the presence of Byzantine attacks, in contrast to existing literature that is only applicable for unconstrained resilient consensus problems. The key enabler for our approach is a new device called a $(\gamma_i,\alpha_i)$-resilient convex combination, which allows normal agents in the network to utilize their locally available information to automatically isolate the impact of the Byzantine agents. Such a resilient convex combination is computable through linear programming, whose complexity scales well with the size of the overall system. By employing this new device to multi-agent systems, we introduce redundancy conditions under which resilient constrained consensus can be achieved with an exponential convergence rate. We also provide insights on the design of a network such that the redundancy conditions are satisfied. We validate all the proposed results through theoretical proofs. Finally, numerical simulations and an application example of safe multi-agent learning are provided to demonstrate the effectiveness of the proposed results.      
### 39.Preprocessing Enhanced Image Compression for Machine Vision  [ :arrow_down: ](https://arxiv.org/pdf/2206.05650.pdf)
>  Recently, more and more images are compressed and sent to the back-end devices for the machine analysis tasks~(\textit{e.g.,} object detection) instead of being purely watched by humans. However, most traditional or learned image codecs are designed to minimize the distortion of the human visual system without considering the increased demand from machine vision systems. In this work, we propose a preprocessing enhanced image compression method for machine vision tasks to address this challenge. Instead of relying on the learned image codecs for end-to-end optimization, our framework is built upon the traditional non-differential codecs, which means it is standard compatible and can be easily deployed in practical applications. Specifically, we propose a neural preprocessing module before the encoder to maintain the useful semantic information for the downstream tasks and suppress the irrelevant information for bitrate saving. Furthermore, our neural preprocessing module is quantization adaptive and can be used in different compression ratios. More importantly, to jointly optimize the preprocessing module with the downstream machine vision tasks, we introduce the proxy network for the traditional non-differential codecs in the back-propagation stage. We provide extensive experiments by evaluating our compression method for two representative downstream tasks with different backbone networks. Experimental results show our method achieves a better trade-off between the coding bitrate and the performance of the downstream machine vision tasks by saving about 20% bitrate.      
### 40.A Fast Alternating Minimization Algorithm for Coded Aperture Snapshot Spectral Imaging Based on Sparsity and Deep Image Priors  [ :arrow_down: ](https://arxiv.org/pdf/2206.05647.pdf)
>  Coded aperture snapshot spectral imaging (CASSI) is a technique used to reconstruct three-dimensional hyperspectral images (HSIs) from one or several two-dimensional projection measurements. However, fewer projection measurements or more spectral channels leads to a severly ill-posed problem, in which case regularization methods have to be applied. In order to significantly improve the accuracy of reconstruction, this paper proposes a fast alternating minimization algorithm based on the sparsity and deep image priors (Fama-SDIP) of natural images. By integrating deep image prior (DIP) into the principle of compressive sensing (CS) reconstruction, the proposed algorithm can achieve state-of-the-art results without any training dataset. Extensive experiments show that Fama-SDIP method significantly outperforms prevailing leading methods on simulation and real HSI datasets.      
### 41.Machine learning approaches for COVID-19 detection from chest X-ray imaging: A Systematic Review  [ :arrow_down: ](https://arxiv.org/pdf/2206.05615.pdf)
>  There is a necessity to develop affordable, and reliable diagnostic tools, which allow containing the COVID-19 spreading. Machine Learning (ML) algorithms have been proposed to design support decision-making systems to assess chest X-ray images, which have proven to be useful to detect and evaluate disease progression. Many research articles are published around this subject, which makes it difficult to identify the best approaches for future work. This paper presents a systematic review of ML applied to COVID-19 detection using chest X-ray images, aiming to offer a baseline for researchers in terms of methods, architectures, databases, and current limitations.      
### 42.Signal-informed DNN-based DOA Estimation combining an External Microphone and GCC-PHAT Features  [ :arrow_down: ](https://arxiv.org/pdf/2206.05606.pdf)
>  Aiming at estimating the direction of arrival (DOA) of a desired speaker in a multi-talker environment using a microphone array, in this paper we propose a signal-informed method exploiting the availability of an external microphone attached to the desired speaker. The proposed method applies a binary mask to the GCC-PHAT input features of a convolutional neural network, where the binary mask is computed based on the power distribution of the external microphone signal. Experimental results for a reverberant scenario with up to four interfering speakers demonstrate that the signal-informed masking improves the localization accuracy, without requiring any knowledge about the interfering speakers.      
### 43.Convex quantization preserves logconcavity  [ :arrow_down: ](https://arxiv.org/pdf/2206.05598.pdf)
>  Much like convexity is key to variational optimization, a logconcave distribution is key to amenable statistical inference. Quantization is often disregarded when writing likelihood models: ignoring the limitations of physical detectors. This begs the questions: would including quantization preclude logconcavity, and, are the true data likelihoods logconcave? We show that the same simple assumption that leads to logconcave continuous data likelihoods also leads to logconcave quantized data likelihoods, provided that convex quantization regions are used.      
### 44.Neural Network-based Flight Control Systems: Present and Future  [ :arrow_down: ](https://arxiv.org/pdf/2206.05596.pdf)
>  As the first review in this field, this paper presents an in-depth mathematical view of Intelligent Flight Control Systems (IFCSs), particularly those based on artificial neural networks. The rapid evolution of IFCSs in the last two decades in both the methodological and technical aspects necessitates a comprehensive view of them to better demonstrate the current stage and the crucial remaining steps towards developing a truly intelligent flight management unit. To this end, in this paper, we will provide a detailed mathematical view of Neural Network (NN)-based flight control systems and the challenging problems that still remain. The paper will cover both the model-based and model-free IFCSs. The model-based methods consist of the basic feedback error learning scheme, the pseudocontrol strategy, and the neural backstepping method. Besides, different approaches to analyze the closed-loop stability in IFCSs, their requirements, and their limitations will be discussed in detail. Various supplementary features, which can be integrated with a basic IFCS such as the fault-tolerance capability, the consideration of system constraints, and the combination of NNs with other robust and adaptive elements like disturbance observers, would be covered, as well. On the other hand, concerning model-free flight controllers, both the indirect and direct adaptive control systems including indirect adaptive control using NN-based system identification, the approximate dynamic programming using NN, and the reinforcement learning-based adaptive optimal control will be carefully addressed. Finally, by demonstrating a well-organized view of the current stage in the development of IFCSs, the challenging issues, which are critical to be addressed in the future, are thoroughly identified.      
### 45.Optimal Solutions for Joint Beamforming and Antenna Selection: From Branch and Bound to Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.05576.pdf)
>  This work revisits the joint beamforming (BF) and antenna selection (AS) problem, as well as its robust beamforming (RBF) version under imperfect channel state information (CSI). Such problems arise in scenarios where the number of the radio frequency (RF) chains is smaller than that of the antenna elements at the transmitter, which has become a critical consideration in the era of large-scale arrays. The joint (R)BF\&amp;AS problem is a mixed integer and nonlinear program, and thus finding {\it optimal solutions} is often costly, if not outright impossible. The vast majority of the prior works tackled these problems using continuous optimization-based approximations -- yet these approximations do not ensure optimality or even feasibility of the solutions. The main contribution of this work is threefold. First, an effective {\it branch and bound} (B\&amp;B) framework for solving the problems of interest is proposed. Leveraging existing BF and RBF solvers, it is shown that the B\&amp;B framework guarantees global optimality of the considered problems. Second, to expedite the potentially costly B\&amp;B algorithm, a machine learning (ML)-based scheme is proposed to help skip intermediate states of the B\&amp;B search tree. The learning model features a {\it graph neural network} (GNN)-based design that is resilient to a commonly encountered challenge in wireless communications, namely, the change of problem size (e.g., the number of users) across the training and test stages. Third, comprehensive performance characterizations are presented, showing that the GNN-based method retains the global optimality of B\&amp;B with provably reduced complexity, under reasonable conditions. Numerical simulations also show that the ML-based acceleration can often achieve an order-of-magnitude speedup relative to B\&amp;B.      
### 46.MammoDL: Mammographic Breast Density Estimation using Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.05575.pdf)
>  Assessing breast cancer risk from imaging remains a subjective process, in which radiologists employ computer aided detection (CAD) systems or qualitative visual assessment to estimate breast percent density (PD). More advanced machine learning (ML) models have become the most promising way to quantify breast cancer risk for early, accurate, and equitable diagnoses, but training such models in medical research is often restricted to small, single-institution data. Since patient demographics and imaging characteristics may vary considerably across imaging sites, models trained on single-institution data tend not to generalize well. In response to this problem, MammoDL is proposed, an open-source software tool that leverages UNet architecture to accurately estimate breast PD and complexity from digital mammography (DM). With the Open Federated Learning (OpenFL) library, this solution enables secure training on datasets across multiple institutions. MammoDL is a leaner, more flexible model than its predecessors, boasting improved generalization due to federation-enabled training on larger, more representative datasets.      
### 47.Strategies to Maintain Voltage on Long, Lightly Loaded Feeders with Widespread Residential Level 2 Plug-in Electric Vehicle Charging  [ :arrow_down: ](https://arxiv.org/pdf/2206.05552.pdf)
>  Long, lightly loaded feeders serving residential loads may begin to experience voltage excursions as plug-in electric vehicle (PEV) penetration increases. Residential PEV charging tends to occur during peak-load hours on residential feeders, leading to increased peak loads and potential voltage excursions. To avoid voltage excursions, two PEV charging control strategies were investigated using the IEEE 34-bus feeder. The first strategy shifts PEV charging energy from peak hours to off-peak hours; the other strategy allows PEVs to provide reactive power support. Undervoltage excursions seen in a simulation of uncontrolled charging of 200 PEVs were improved dramatically when these two control strategies were used. The minimum voltage on the feeder improved from 0.855 pu when PEV charging was uncontrolled to 0.959 pu when both control strategies were applied together.      
### 48.Deep Learning-Based MR Image Re-parameterization  [ :arrow_down: ](https://arxiv.org/pdf/2206.05516.pdf)
>  Magnetic resonance (MR) image re-parameterization refers to the process of generating via simulations of an MR image with a new set of MRI scanning parameters. Different parameter values generate distinct contrast between different tissues, helping identify pathologic tissue. Typically, more than one scan is required for diagnosis; however, acquiring repeated scans can be costly, time-consuming, and difficult for patients. Thus, using MR image re-parameterization to predict and estimate the contrast in these imaging scans can be an effective alternative. In this work, we propose a novel deep learning (DL) based convolutional model for MRI re-parameterization. Based on our preliminary results, DL-based techniques hold the potential to learn the non-linearities that govern the re-parameterization.      
### 49.An Algorithm for Exact Numerical Age-of-Information Evaluation in Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2206.05510.pdf)
>  We present an algorithm for the numerical evaluation of the state-space distribution of an Age-of-Information network. Given enough computational resources, the evaluation can be performed to an arbitrary high precision. An Age-of-Information network is described by a vector of natural numbers, that track how outdated status information from various agents is. Our algorithm yields the means to determine any moment of the corresponding stochastic process. This can be extremely valuable for cases in which the network consists of controllers that communicate with one another, as it potentially allows for less conservative control behavior. It also enables the comparison of different policies regarding their performance (minimizing the average Age-of-Information) to a much more accurate degree than was possible before. This is illustrated using the conventional MaxWeight policy and the optimal policy. We also validate and compare the algorithm with Monte-Carlo-Simulations.      
### 50.Integration of Physics-Based and Data-Driven Models for Hyperspectral Image Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2206.05508.pdf)
>  Spectral unmixing is one of the most important quantitative analysis tasks in hyperspectral data processing. Conventional physics-based models are characterized by clear interpretation. However, due to the complex mixture mechanism and limited nonlinearity modeling capacity, these models may not be accurate, especially, in analyzing scenes with unknown physical characteristics. Data-driven methods have developed rapidly in recent years, in particular deep learning methods as they possess superior capability in modeling complex and nonlinear systems. Simply transferring these methods as black-boxes to conduct unmixing may lead to low physical interpretability and generalization ability. Consequently, several contributions have been dedicated to integrating advantages of both physics-based models and data-driven methods. In this article, we present an overview of recent advances on this topic from several aspects, including deep neural network (DNN) structures design, prior capturing and loss design, and summarise these methods in a common mathematical optimization framework. In addition, relevant remarks and discussions are conducted made for providing further understanding and prospective improvement of the methods. The related source codes and data are collected and made available at <a class="link-external link-http" href="http://github.com/xiuheng-wang/awesome-hyperspectral-image-unmixing" rel="external noopener nofollow">this http URL</a>.      
### 51.Differentiable Projection from Optical Coherence Tomography B-Scan without Retinal Layer Segmentation Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2206.05472.pdf)
>  Projection map (PM) from optical coherence tomography (OCT) B-scan is an important tool to diagnose retinal diseases, which typically requires retinal layer segmentation. In this study, we present a novel end-to-end framework to predict PMs from B-scans. Instead of segmenting retinal layers explicitly, we represent them implicitly as predicted coordinates. By pixel interpolation on uniformly sampled coordinates between retinal layers, the corresponding PMs could be easily obtained with pooling. Notably, all the operators are differentiable; therefore, this Differentiable Projection Module (DPM) enables end-to-end training with the ground truth of PMs rather than retinal layer segmentation. Our framework produces high-quality PMs, significantly outperforming baselines, including a vanilla CNN without DPM and an optimization-based DPM without a deep prior. Furthermore, the proposed DPM, as a novel neural representation of areas/volumes between curves/surfaces, could be of independent interest for geometric deep learning.      
### 52.Svadhyaya system for the Second Diagnosing COVID-19 using Acoustics Challenge 2021  [ :arrow_down: ](https://arxiv.org/pdf/2206.05462.pdf)
>  This report describes the system used for detecting COVID-19 positives using three different acoustic modalities, namely speech, breathing, and cough in the second DiCOVA challenge. The proposed system is based on the combination of 4 different approaches, each focusing more on one aspect of the problem, and reaches the blind test AUCs of 86.41, 77.60, and 84.55, in the breathing, cough, and speech tracks, respectively, and the AUC of 85.37 in the fusion of these three tracks.      
### 53.Effect of Strong Time-Varying Transmission Distance on LEO Satellite-Terrestrial Deliveries  [ :arrow_down: ](https://arxiv.org/pdf/2206.05428.pdf)
>  In this paper, we investigate the effect of the strong time-varying transmission distance on the performance of the low-earth orbit (LEO) satellite-terrestrial transmission (STT) system. We propose a new analytical framework using finite-state Markov channel (FSMC) model and time discretization method. Moreover, to demonstrate the applications of the proposed framework, the performances of two adaptive transmissions, rate-adaptive transmission (RAT) and power-adaptive transmission (PAT) schemes, are evaluated for the cases when the transmit power or the transmission rate at the LEO satellite is fixed. Closed-form expressions for the throughput, energy efficiency (EE), and delay outage rate (DOR) of the considered systems are derived and verified, which are capable of addressing the capacity, energy efficiency, and outage rate performance of the considered LEO STT scenarios with the proposed analytical framework.      
### 54.Reconfigurable Intelligent Surface-Aided 6G Massive Access: Coupled Tensor Modeling and Sparse Bayesian Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.05427.pdf)
>  This paper investigates a reconfigurable intelligent surface (RIS)-aided unsourced random access (URA) scheme for the sixth-generation (6G) wireless networks with massive sporadic traffic devices. First of all, this paper proposes a novel joint active device separation (the message recovery of active device) and channel estimation architecture for the RIS-aided URA. Specifically, the RIS passive reflection is optimized before the successful device separation. Then, by associating the data sequences to multiple rank-one tensors and exploiting the angular sparsity of the RIS-BS channel, the detection problem is cast as a high-order coupled tensor decomposition problem without the need of exploiting pilot sequences. However, the inherent coupling among multiple sparse device-RIS channels, together with the unknown number of active devices make the detection problem at hand deviate from the widely-used coupled tensor decomposition format. To overcome this challenge, this paper judiciously devises a probabilistic model that captures both the element-wise sparsity from the angular channel model and the low-rank property due to the sporadic nature of URA. Then, based on such a probabilistic model, a iterative detection algorithm is developed under the framework of sparse variational inference, where each update iteration is obtained in a closed-form and the number of active devices can be automatically estimated for effectively avoiding the overfitting of noise. Extensive simulation results confirm the excellence of the proposed URA algorithm, especially for the case of a large number of reflecting elements for accommodating a significantly large number of devices.      
### 55.Reducing the Control Overhead of Intelligent Reconfigurable Surfaces Via a Tensor-Based Low-Rank Factorization Approach  [ :arrow_down: ](https://arxiv.org/pdf/2206.05341.pdf)
>  Passive intelligent reconfigurable surfaces (IRS) are becoming an attractive component of cellular networks due to their ability of shaping the propagation environment and thereby improving the coverage. While passive IRS nodes incorporate a great number of phase-shifting elements and a controller entity, the phase-shifts are typically determined by the cellular base station (BS) due to its computational capability. Since the fine granularity control of the large number of phase-shifters may become prohibitive in practice, it is important to reduce the control overhead between the BS and the IRS controller. To this end, in this paper we propose a low-rank approximation of the near-optimal phase-shifts, which would incur prohibitively high communication overhead on the BS-IRS controller links. The key idea is to represent the potentially large IRS phase-shift vector using a low-rank tensor model. This is achieved by factorizing a tensorized version of the IRS phase-shift vector, where each component is modeled as the Kronecker product of a predefined number of factors of smaller sizes, which can be obtained via tensor decomposition algorithms. We show that the proposed low-rank models drastically reduce the required feedback requirements associated with the BS-IRS control links. Our simulation results indicate that the proposed method is especially attractive in scenarios with a strong line of sight component, in which case nearly the same spectral efficiency is reached as in the cases with near-optimal phase-shifts, but with a drastically reduced communication overhead.      
### 56.IRS for Multi-Access Edge Computing in 6G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.05290.pdf)
>  Computation offloading in multi-access edge computing (MEC) is an effective paradigm for enabling resource-intensive smart applications. However, when the wireless channel utilized for offloading computing activities is hostile, the proper advantages of MEC may not be completely realized. Intelligent reflecting surface (IRS) is a new technology that has recently attracted significant interest can optimize the wireless transmission environment in a programmable way and improving the connectivity between user equipment (UE) and base station (BS). In this paper, the performance of MEC architecture is analyzed considering both IRS-assisted and without IRS communication scenarios in the context of the urban micro cellular scenarios. The research obtained that the deployment of IRS can reduce the spectrum and energy consumption significantly.      
### 57.Localized adversarial artifacts for compressed sensing MRI  [ :arrow_down: ](https://arxiv.org/pdf/2206.05289.pdf)
>  As interest in deep neural networks (DNNs) for image reconstruction tasks grows, their reliability has been called into question (Antun et al., 2020; Gottschling et al., 2020). However, recent work has shown that compared to total variation (TV) minimization, they show similar robustness to adversarial noise in terms of $\ell^2$-reconstruction error (Genzel et al., 2022). We consider a different notion of robustness, using the $\ell^\infty$-norm, and argue that localized reconstruction artifacts are a more relevant defect than the $\ell^2$-error. We create adversarial perturbations to undersampled MRI measurements which induce severe localized artifacts in the TV-regularized reconstruction. The same attack method is not as effective against DNN based reconstruction. Finally, we show that this phenomenon is inherent to reconstruction methods for which exact recovery can be guaranteed, as with compressed sensing reconstructions with $\ell^1$- or TV-minimization.      
### 58.From Labels to Priors in Capsule Endoscopy: A Prior Guided Approach for Improving Generalization with Few Labels  [ :arrow_down: ](https://arxiv.org/pdf/2206.05288.pdf)
>  The lack of generalizability of deep learning approaches for the automated diagnosis of pathologies in Wireless Capsule Endoscopy (WCE) has prevented any significant advantages from trickling down to real clinical practices. As a result, disease management using WCE continues to depend on exhaustive manual investigations by medical experts. This explains its limited use despite several advantages. Prior works have considered using higher quality and quantity of labels as a way of tackling the lack of generalization, however this is hardly scalable considering pathology diversity not to mention that labeling large datasets encumbers the medical staff additionally. We propose using freely available domain knowledge as priors to learn more robust and generalizable representations. We experimentally show that domain priors can benefit representations by acting in proxy of labels, thereby significantly reducing the labeling requirement while still enabling fully unsupervised yet pathology-aware learning. We use the contrastive objective along with prior-guided views during pretraining, where the view choices inspire sensitivity to pathological information. Extensive experiments on three datasets show that our method performs better than (or closes gap with) the state-of-the-art in the domain, establishing a new benchmark in pathology classification and cross-dataset generalization, as well as scaling to unseen pathology categories.      
### 59.Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2206.05284.pdf)
>  Distributed learning has shown great potential in medical image analysis. It allows to use multi-center training data with privacy protection. However, data distributions in local centers can vary from each other due to different imaging vendors, and annotation protocols. Such variation degrades the performance of learning-based methods. To mitigate the influence, two groups of methods have been proposed for different aims, i.e., the global methods and the personalized methods. The former are aimed to improve the performance of a single global model for all test data from unseen centers (known as generic data); while the latter target multiple models for each center (denoted as local data). However, little has been researched to achieve both goals simultaneously. In this work, we propose a new framework of distributed learning that bridges the gap between two groups, and improves the performance for both generic and local data. Specifically, our method decouples the predictions for generic data and local data, via distribution-conditioned adaptation matrices. Results on multi-center left atrial (LA) MRI segmentation showed that our method demonstrated superior performance over existing methods on both generic and local data. Our code is available at <a class="link-external link-https" href="https://github.com/key1589745/decouple_predict" rel="external noopener nofollow">this https URL</a>      
### 60.Poissonian Blurred Image Deconvolution by Framelet based Local Minimal Prior  [ :arrow_down: ](https://arxiv.org/pdf/2206.05283.pdf)
>  Image production tools do not always create a clear image, noisy and blurry images are sometimes created. Among these cases, Poissonian noise is one of the most famous noises that appear in medical images and images taken in astronomy. Blurred image with Poissonian noise obscures important details that are of great importance in medicine or astronomy. Therefore, studying and increasing the quality of images that are affected by this type of noise is always considered by researchers. In this paper, in the first step, based on framelet transform, a local minimal prior is introduced, and in the next step, this tool together with fractional calculation is used for Poissonian blurred image deconvolution. In the following, the model is generalized to the blind case. To evaluate the performance of the presented model, several images such as real images have been investigated.      
### 61.PILC: Practical Image Lossless Compression with an End-to-end GPU Oriented Neural Framework  [ :arrow_down: ](https://arxiv.org/pdf/2206.05279.pdf)
>  Generative model based image lossless compression algorithms have seen a great success in improving compression ratio. However, the throughput for most of them is less than 1 MB/s even with the most advanced AI accelerated chips, preventing them from most real-world applications, which often require 100 MB/s. In this paper, we propose PILC, an end-to-end image lossless compression framework that achieves 200 MB/s for both compression and decompression with a single NVIDIA Tesla V100 GPU, 10 times faster than the most efficient one before. To obtain this result, we first develop an AI codec that combines auto-regressive model and VQ-VAE which performs well in lightweight setting, then we design a low complexity entropy coder that works well with our codec. Experiments show that our framework compresses better than PNG by a margin of 30% in multiple datasets. We believe this is an important step to bring AI compression forward to commercial use.      
### 62.Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT  [ :arrow_down: ](https://arxiv.org/pdf/2206.05278.pdf)
>  Single-photon emission computed tomography (SPECT) is a widely applied imaging approach for diagnosis of coronary artery diseases. Attenuation maps (u-maps) derived from computed tomography (CT) are utilized for attenuation correction (AC) to improve diagnostic accuracy of cardiac SPECT. However, SPECT and CT are obtained sequentially in clinical practice, which potentially induces misregistration between the two scans. Convolutional neural networks (CNN) are powerful tools for medical image registration. Previous CNN-based methods for cross-modality registration either directly concatenated two input modalities as an early feature fusion or extracted image features using two separate CNN modules for a late fusion. These methods do not fully extract or fuse the cross-modality information. Besides, deep-learning-based rigid registration of cardiac SPECT and CT-derived u-maps has not been investigated before. In this paper, we propose a Dual-Branch Squeeze-Fusion-Excitation (DuSFE) module for the registration of cardiac SPECT and CT-derived u-maps. DuSFE fuses the knowledge from multiple modalities to recalibrate both channel-wise and spatial features for each modality. DuSFE can be embedded at multiple convolutional layers to enable feature fusion at different spatial dimensions. Our studies using clinical data demonstrated that a network embedded with DuSFE generated substantial lower registration errors and therefore more accurate AC SPECT images than previous methods.      
### 63.Superresolution and Segmentation of OCT scans using Multi-Stage adversarial Guided Attention Training  [ :arrow_down: ](https://arxiv.org/pdf/2206.05277.pdf)
>  Optical coherence tomography (OCT) is one of the non-invasive and easy-to-acquire biomarkers (the thickness of the retinal layers, which is detectable within OCT scans) being investigated to diagnose Alzheimer's disease (AD). This work aims to segment the OCT images automatically; however, it is a challenging task due to various issues such as the speckle noise, small target region, and unfavorable imaging conditions. In our previous work, we have proposed the multi-stage &amp; multi-discriminatory generative adversarial network (MultiSDGAN) to translate OCT scans in high-resolution segmentation labels. In this investigation, we aim to evaluate and compare various combinations of channel and spatial attention to the MultiSDGAN architecture to extract more powerful feature maps by capturing rich contextual relationships to improve segmentation performance. Moreover, we developed and evaluated a guided mutli-stage attention framework where we incorporated a guided attention mechanism by forcing an L-1 loss between a specifically designed binary mask and the generated attention maps. Our ablation study results on the WVU-OCT data-set in five-fold cross-validation (5-CV) suggest that the proposed MultiSDGAN with a serial attention module provides the most competitive performance, and guiding the spatial attention feature maps by binary masks further improves the performance in our proposed network. Comparing the baseline model with adding the guided-attention, our results demonstrated relative improvements of 21.44% and 19.45% on the Dice coefficient and SSIM, respectively.      
### 64.A Versatile Pseudo-Rigid Body Modeling Method  [ :arrow_down: ](https://arxiv.org/pdf/2206.06237.pdf)
>  A novel semi-analytical method is proposed to develop the pseudo-rigid-body~(PRB) model of robots made of highly flexible members (HFM), such as flexures and continuum robots, with no limit on the degrees of freedom of the PRB model. The proposed method has a simple formulation yet high precision. Furthermore, it can describe HFMs with variable curvature and stiffness along their length. The method offers a semi-analytical solution for the highly coupled nonlinear constrained optimization problem of PRB modeling and can be extended to variable-length robots comprised of HFM, such as catheter and concentric tube robots. We also show that this method can obtain a PRB model of uniformly stiff HFMs, with only three parameters. The versatility of the method is investigated in various applications of HFM in continuum robots. Simulations demonstrate substantial improvement in the precision of the PRB model in general and a reduction in the complexity of the formulation.      
### 65.Learning a Degradation-Adaptive Network for Light Field Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2206.06214.pdf)
>  Recent years have witnessed the great advances of deep neural networks (DNNs) in light field (LF) image super-resolution (SR). However, existing DNN-based LF image SR methods are developed on a single fixed degradation (e.g., bicubic downsampling), and thus cannot be applied to super-resolve real LF images with diverse degradations. In this paper, we propose the first method to handle LF image SR with multiple degradations. In our method, a practical LF degradation model that considers blur and noise is developed to approximate the degradation process of real LF images. Then, a degradation-adaptive network (LF-DAnet) is designed to incorporate the degradation prior into the SR process. By training on LF images with multiple synthetic degradations, our method can learn to adapt to different degradations while incorporating the spatial and angular information. Extensive experiments on both synthetically degraded and real-world LFs demonstrate the effectiveness of our method. Compared with existing state-of-the-art single and LF image SR methods, our method achieves superior SR performance under a wide range of degradations, and generalizes better to real LF images. Codes and models are available at <a class="link-external link-https" href="https://github.com/YingqianWang/LF-DAnet" rel="external noopener nofollow">this https URL</a>.      
### 66.A DSEL for High Throughput and Low Latency Software-Defined Radio on Multicore CPUs  [ :arrow_down: ](https://arxiv.org/pdf/2206.06147.pdf)
>  This article presents a new Domain Specific Embedded Language (DSEL) dedicated to Software-Defined Radio (SDR). From a set of carefully designed components, it enables to build efficient software digital communication systems, able to take advantage of the parallelism of modern processor architectures, in a straightforward and safe manner for the programmer. In particular, proposed DSEL enables the combination of pipelining and sequence duplication techniques to extract both temporal and spatial parallelism from digital communication systems. We leverage the DSEL capabilities on a real use case: a fully digital transceiver for the widely used DVB-S2 standard designed entirely in software. Through evaluation, we show how proposed software DVB-S2 transceiver is able to get the most from modern, high-end multicore CPU targets.      
### 67.Identification of cancer-keeping genes as therapeutic targets by finding network control hubs  [ :arrow_down: ](https://arxiv.org/pdf/2206.06145.pdf)
>  Finding cancer driver genes has been a focal theme of cancer research and clinical studies. One of the recent approaches is based on network structural controllability that focuses on finding a control scheme and driver genes that can steer the cell from an arbitrary state to a designated state. While theoretically sound, this approach is impractical for many reasons, e.g., the control scheme is often not unique and half of the nodes may be driver genes for the cell. We developed a novel approach that transcends structural controllability. Instead of considering driver genes for one control scheme, we considered control hub genes that reside in the middle of a control path of every control scheme. Control hubs are the most vulnerable spots for controlling the cell and exogenous stimuli on them may render the cell uncontrollable. We adopted control hubs as cancer-keep genes (CKGs) and applied them to a gene regulatory network of bladder cancer (BLCA). All the genes on the cell cycle and p53 singling pathways in BLCA are CKGs, confirming the importance of these genes and the two pathways in cancer. A smaller set of 35 sensitive CKGs (sCKGs) for BLCA was identified by removing network links. Six sCKGs (RPS6KA3, FGFR3, N-cadherin (CDH2), EP300, caspase-1, and FN1) were subjected to small-interferencing-RNA knockdown in four cell lines to validate their effects on the proliferation or migration of cancer cells. Knocking down RPS6KA3 in a mouse model of BLCA significantly inhibited the growth of tumor xenografts in the mouse model. Combined, our results demonstrated the value of CKGs as therapeutic targets for cancer therapy and the potential of CKGs as an effective means for studying and characterizing cancer etiology.      
### 68.Robust Time Series Denoising with Learnable Wavelet Packet Transform  [ :arrow_down: ](https://arxiv.org/pdf/2206.06126.pdf)
>  In many applications, signal denoising is often the first pre-processing step before any subsequent analysis or learning task. In this paper, we propose to apply a deep learning denoising model inspired by a signal processing, a learnable version of wavelet packet transform. The proposed algorithm has signficant learning capabilities with few interpretable parameters and has an intuitive initialisation. We propose a post-learning modification of the parameters to adapt the denoising to different noise levels. We evaluate the performance of the proposed methodology on two case studies and compare it to other state of the art approaches, including wavelet schrinkage denoising, convolutional neural network, autoencoder and U-net deep models. The first case study is based on designed functions that have typically been used to study denoising properties of the algorithms. The second case study is an audio background removal task. We demonstrate how the proposed algorithm relates to the universality of signal processing methods and the learning capabilities of deep learning approaches. In particular, we evaluate the obtained denoising performances on structured noisy signals inside and outside the classes used for training. In addition to having good performance in denoising signals inside and outside to the training class, our method shows to be particularly robust when different noise levels, noise types and artifacts are added.      
### 69.Optimizing musical chord inversions using the cartesian coordinate system  [ :arrow_down: ](https://arxiv.org/pdf/2206.06117.pdf)
>  In classical music and in any genre of contemporary music, the tonal elements or notes used for playing are the same. The numerous possibilities of chords for a given instance in a piece make the playing, in general, very intricate, and advanced. The theory sounds quite trivial, yet the application has vast options, each leading to inarguably different outcomes, characterized by scientific and musical principles. Chords and their importance are self-explanatory. A chord is a bunch of notes played together. As far as scientists are concerned, it is a set of tonal frequencies ringing together resulting in a consonant/dissonant sound. It is well-known that the notes of a chord can be rearranged to come up with various voicings (1) of the same chord which enables a composer/player to choose the most optimal one to convey the emotion they wish to convey. Though there are numerous possibilities, it is scientific to think that there is just one appropriate voicing for a particular situation of tonal movements. In this study, we attempt to find the optimal voicings by considering chords to be points in a 3-dimensional cartesian coordinate system and further the fundamental understanding of mathematics in music theory.      
### 70.AR-NeRF: Unsupervised Learning of Depth and Defocus Effects from Natural Images with Aperture Rendering Neural Radiance Fields  [ :arrow_down: ](https://arxiv.org/pdf/2206.06100.pdf)
>  Fully unsupervised 3D representation learning has gained attention owing to its advantages in data collection. A successful approach involves a viewpoint-aware approach that learns an image distribution based on generative models (e.g., generative adversarial networks (GANs)) while generating various view images based on 3D-aware models (e.g., neural radiance fields (NeRFs)). However, they require images with various views for training, and consequently, their application to datasets with few or limited viewpoints remains a challenge. As a complementary approach, an aperture rendering GAN (AR-GAN) that employs a defocus cue was proposed. However, an AR-GAN is a CNN-based model and represents a defocus independently from a viewpoint change despite its high correlation, which is one of the reasons for its performance. As an alternative to an AR-GAN, we propose an aperture rendering NeRF (AR-NeRF), which can utilize viewpoint and defocus cues in a unified manner by representing both factors in a common ray-tracing framework. Moreover, to learn defocus-aware and defocus-independent representations in a disentangled manner, we propose aperture randomized training, for which we learn to generate images while randomizing the aperture size and latent codes independently. During our experiments, we applied AR-NeRF to various natural image datasets, including flower, bird, and face images, the results of which demonstrate the utility of AR-NeRF for unsupervised learning of the depth and defocus effects.      
### 71.Low-complexity deep learning frameworks for acoustic scene classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.06057.pdf)
>  In this report, we presents low-complexity deep learning frameworks for acoustic scene classification (ASC). The proposed frameworks can be separated into four main steps: Front-end spectrogram extraction, online data augmentation, back-end classification, and late fusion of predicted probabilities. In particular, we initially transform audio recordings into Mel, Gammatone, and CQT spectrograms. Next, data augmentation methods of Random Cropping, Specaugment, and Mixup are then applied to generate augmented spectrograms before being fed into deep learning based classifiers. Finally, to achieve the best performance, we fuse probabilities which obtained from three individual classifiers, which are independently-trained with three type of spectrograms. Our experiments conducted on DCASE 2022 Task 1 Development dataset have fullfiled the requirement of low-complexity and achieved the best classification accuracy of 60.1%, improving DCASE baseline by 17.2%.      
### 72.Combining BMC and Fuzzing Techniques for Finding Software Vulnerabilities in Concurrent Programs  [ :arrow_down: ](https://arxiv.org/pdf/2206.06043.pdf)
>  Finding software vulnerabilities in concurrent programs is a challenging task due to the size of the state-space exploration, as the number of interleavings grows exponentially with the number of program threads and statements. We propose and evaluate EBF (Ensembles of Bounded Model Checking with Fuzzing) -- a technique that combines Bounded Model Checking (BMC) and Gray-Box Fuzzing (GBF) to find software vulnerabilities in concurrent programs. Since there are no publicly-available GBF tools for concurrent code, we first propose a novel concurrency-aware gray-box fuzzer that explores different thread schedules by instrumenting the code under test with random delays controlled by the fuzzing engine. Then, we build an ensemble of one BMC and one GBF tool in the following way. On the one hand, when the BMC tool in the ensemble returns a counterexample, we use it as a seed for our GBF tool, thus increasing the likelihood of executing paths guarded by complex mathematical expressions. On the other hand, we aggregate the outcomes of the BMC and GBF tools in the ensemble using a decision matrix, thus improving the accuracy of EBF. We evaluate EBF against state-of-the-art pure BMC tools and show that it can generate up to 14.9% more correct verification witnesses than BMC alone. Furthermore, we demonstrate the efficacy of our concurrency-aware GBF by showing that it can find 21.4% of the vulnerabilities in our evaluation suite, while non-concurrency-aware GBF tools can only find 0.55%. Finally, thanks to our concurrency-aware GBF tool, EBF detects a data race in the open-source wolfMqtt library, which demonstrates its effectiveness in finding vulnerabilities in real-world software.      
### 73.Automatic Contact Tracing using Bluetooth Low Energy Signals and IMU Sensor Readings  [ :arrow_down: ](https://arxiv.org/pdf/2206.06033.pdf)
>  In this report, we present our solution to the challenge provided by the SFI Centre for Machine Learning (ML-Labs) in which the distance between two phones needs to be estimated. It is a modified version of the NIST Too Close For Too Long (TC4TL) Challenge, as the time aspect is excluded. We propose a feature-based approach based on Bluetooth RSSI and IMU sensory data, that outperforms the previous state of the art by a significant margin, reducing the error down to 0.071. We perform an ablation study of our model that reveals interesting insights about the relationship between the distance and the Bluetooth RSSI readings.      
### 74.Rethinking: Deep-learning-based Demodulation and Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2206.06025.pdf)
>  In this paper, we focus on the demodulation/decoding of the complex modulations/codes that approach the Shannon capacity. Theoretically, the maximum likelihood (ML) algorithm can achieve the optimal error performance whereas it has $\mathcal{O}(2^k)$ demodulation/decoding complexity with $k$ denoting the number of information bits. Recent progress in deep learning provides a new direction to tackle the demodulation and the decoding. The purpose of this paper is to analyze the feasibility of the neural network to demodulate/decode the complex modulations/codes close to the Shannon capacity and characterize the error performance and the complexity of the neural network. Regarding the neural network demodulator, we use the golden angle modulation (GAM), a promising modulation format that can offer the Shannon capacity approaching performance, to evaluate the demodulator. It is observed that the neural network demodulator can get a close performance to the ML-based method while it suffers from the lower complexity order in the low-order GAM. Regarding the neural network decoder, we use the Gaussian codebook, achieving the Shannon capacity, to evaluate the decoder. We also observe that the neural network decoder achieves the error performance close to the ML decoder with a much lower complexity order in the small Gaussian codebook. Limited by the current training resources, we cannot evaluate the performance of the high-order modulation and the long codeword. But, based on the results of the low-order GAM and the small Gaussian codebook, we boldly give our conjecture: the neural network demodulator/decoder is a strong candidate approach for demodulating/decoding the complex modulations/codes close to the Shannon capacity owing to the error performance of the near-ML algorithm and the lower complexity.      
### 75.Reinforcement Learning-based Placement of Charging Stations in Urban Road Networks  [ :arrow_down: ](https://arxiv.org/pdf/2206.06011.pdf)
>  The transition from conventional mobility to electromobility largely depends on charging infrastructure availability and optimal placement.This paper examines the optimal placement of charging stations in urban areas. We maximise the charging infrastructure supply over the area and minimise waiting, travel, and charging times while setting budget constraints. Moreover, we include the possibility of charging vehicles at home to obtain a more refined estimation of the actual charging demand throughout the urban area. We formulate the Placement of Charging Stations problem as a non-linear integer optimisation problem that seeks the optimal positions for charging stations and the optimal number of charging piles of different charging types. We design a novel Deep Reinforcement Learning approach to solve the charging station placement problem (PCRL). Extensive experiments on real-world datasets show how the PCRL reduces the waiting and travel time while increasing the benefit of the charging plan compared to five baselines. Compared to the existing infrastructure, we can reduce the waiting time by up to 97% and increase the benefit up to 497%.      
### 76.One Size Fits All: Hypernetwork for Tunable Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2206.05970.pdf)
>  We introduce a novel approach for tunable image restoration that achieves the accuracy of multiple models, each optimized for a different level of degradation, with exactly the same number of parameters as a single model. Our model can be optimized to restore as many degradation levels as required with a constant number of parameters and for various image restoration tasks. Experiments on real-world datasets show that our approach achieves state-of-the art results in denoising, DeJPEG and super-resolution with respect to existing tunable models, allowing smoother and more accurate fitting over a wider range of degradation levels.      
### 77.Toward Ambient Intelligence: Federated Edge Learning with Task-Oriented Sensing, Computation, and Communication Integration  [ :arrow_down: ](https://arxiv.org/pdf/2206.05949.pdf)
>  In this paper, we address the problem of joint sensing, computation, and communication (SC$^{2}$) resource allocation for federated edge learning (FEEL) via a concrete case study of human motion recognition based on wireless sensing in ambient intelligence. First, by analyzing the wireless sensing process in human motion recognition, we find that there exists a thresholding value for the sensing transmit power, exceeding which yields sensing data samples with approximately the same satisfactory quality. Then, the joint SC$^{2}$ resource allocation problem is cast to maximize the convergence speed of FEEL, under the constraints on training time, energy supply, and sensing quality of each edge device. Solving this problem entails solving two subproblems in order: the first one reduces to determine the joint sensing and communication resource allocation that maximizes the total number of samples that can be sensed during the entire training process; the second one concerns the partition of the attained total number of sensed samples over all the communication rounds to determine the batch size at each round for convergence speed maximization. The first subproblem on joint sensing and communication resource allocation is converted to a single-variable optimization problem by exploiting the derived relation between different control variables (resources), which thus allows an efficient solution via one-dimensional grid search. For the second subproblem, it is found that the number of samples to be sensed (or batch size) at each round is a decreasing function of the loss function value attained at the round. Based on this relationship, the approximate optimal batch size at each communication round is derived in closed-form as a function of the round index. Finally, extensive simulation results are provided to validate the superiority of the proposed joint SC$^{2}$ resource allocation scheme.      
### 78.Improvement of Serial Approach to Anomalous Sound Detection by Incorporating Two Binary Cross-Entropies for Outlier Exposure  [ :arrow_down: ](https://arxiv.org/pdf/2206.05929.pdf)
>  Anomalous sound detection systems must detect unknown, atypical sounds using only normal audio data. Conventional methods use the serial method, a combination of outlier exposure (OE), which classifies normal and pseudo-anomalous data and obtains embedding, and inlier modeling (IM), which models the probability distribution of the embedding. Although the serial method shows high performance due to the powerful feature extraction of OE and the robustness of IM, OE still has a problem that doesn't work well when the normal and pseudo-anomalous data are too similar or too different. To explicitly distinguish these data, the proposed method uses multi-task learning of two binary cross-entropies when training OE. The first is a loss that classifies the sound of the target machine to which product it is emitted from, which deals with the case where the normal data and the pseudo-anomalous data are too similar. The second is a loss that identifies whether the sound is emitted from the target machine or not, which deals with the case where the normal data and the pseudo-anomalous data are too different. We perform our experiments with DCASE 2021 Task~2 dataset. Our proposed single-model method outperforms the top-ranked method, which combines multiple models, by 2.1% in AUC.      
### 79.Compressive Clustering with an Optical Processing Unit  [ :arrow_down: ](https://arxiv.org/pdf/2206.05928.pdf)
>  We explore the use of Optical Processing Units (OPU) to compute random Fourier features for sketching, and adapt the overall compressive clustering pipeline to this setting. We also propose some tools to help tuning a critical hyper-parameter of compressive clustering.      
### 80.GradICON: Approximate Diffeomorphisms via Gradient Inverse Consistency  [ :arrow_down: ](https://arxiv.org/pdf/2206.05897.pdf)
>  Many registration approaches exist with early work focusing on optimization-based approaches for image pairs. Recent work focuses on deep registration networks to predict spatial transformations. In both cases, commonly used non-parametric registration models, which estimate transformation functions instead of low-dimensional transformation parameters, require choosing a suitable regularizer (to encourage smooth transformations) and its parameters. This makes models difficult to tune and restricts deformations to the deformation space permissible by the chosen regularizer. While deep-learning models for optical flow exist that do not regularize transformations and instead entirely rely on the data these might not yield diffeomorphic transformations which are desirable for medical image registration. In this work, we therefore develop GradICON building upon the unsupervised ICON deep-learning registration approach, which only uses inverse-consistency for regularization. However, in contrast to ICON, we prove and empirically verify that using a gradient inverse-consistency loss not only significantly improves convergence, but also results in a similar implicit regularization of the resulting transformation map. Synthetic experiments and experiments on magnetic resonance (MR) knee images and computed tomography (CT) lung images show the excellent performance of GradICON. We achieve state-of-the-art (SOTA) accuracy while retaining a simple registration formulation, which is practically important.      
### 81.Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2206.05876.pdf)
>  We present the task description of the Detection and Classification of Acoustic Scenes and Events (DCASE) 2022 Challenge Task 2: "Unsupervised anomalous sound detection (ASD) for machine condition monitoring applying domain generalization techniques". Domain shifts are a critical problem for the application of ASD systems. Because domain shifts can change the acoustic characteristics of data, a model trained in a source domain performs poorly for a target domain. In DCASE 2021 Challenge Task 2, we organized an ASD task for handling domain shifts. In this task, it was assumed that the occurrences of domain shifts are known. However, in practice, the domain of each sample may not be given, and the domain shifts can occur implicitly. In 2022 Task 2, we focus on domain generalization techniques that detects anomalies regardless of the domain shifts. Specifically, the domain of each sample is not given in the test data and only one threshold is allowed for all domains. We will add challenge results and analysis of the submissions after the challenge submission deadline.      
### 82.Resource Allocation and 3D Trajectory Design for Power-Efficient IRS-Assisted UAV-NOMA Communications  [ :arrow_down: ](https://arxiv.org/pdf/2206.05872.pdf)
>  In this paper, an intelligent reflecting surface (IRS) is introduced to assist an unmanned aerial vehicle (UAV) communication system based on non-orthogonal multiple access (NOMA) for serving multiple ground users. We aim to minimize the average total system energy consumption by jointly designing the resource allocation strategy, the three dimensional (3D) trajectory of the UAV, as well as the phase control at the IRS. The design is formulated as a non-convex optimization problem taking into account the maximum tolerable outage probability constraint and the individual minimum data rate requirement. To circumvent the intractability of the design problem due to the altitude-dependent Rician fading in UAV-to-user links, we adopt the deep neural network (DNN) approach to accurately approximate the corresponding effective channel gains, which facilitates the development of a low-complexity suboptimal iterative algorithm via dividing the formulated problem into two subproblems and address them alternatingly. Numerical results demonstrate that the proposed algorithm can converge to an effective solution within a small number of iterations and illustrate some interesting insights: (1) IRS enables a highly flexible UAV's 3D trajectory design via recycling the dissipated radio signal for improving the achievable system data rate and reducing the flight power consumption of the UAV; (2) IRS provides a rich array gain through passive beamforming in the reflection link, which can substantially reduce the required communication power for guaranteeing the required quality-of-service (QoS); (3) Optimizing the altitude of UAV's trajectory can effectively exploit the outage-guaranteed effective channel gain to save the total required communication power enabling power-efficient UAV communications.      
### 83.Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Conservative Natural Policy Gradient Primal-Dual Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2206.05850.pdf)
>  We consider the problem of constrained Markov decision process (CMDP) in continuous state-actions spaces where the goal is to maximize the expected cumulative reward subject to some constraints. We propose a novel Conservative Natural Policy Gradient Primal-Dual Algorithm (C-NPG-PD) to achieve zero constraint violation while achieving state of the art convergence results for the objective value function. For general policy parametrization, we prove convergence of value function to global optimal upto an approximation error due to restricted policy class. We even improve the sample complexity of existing constrained NPG-PD algorithm \cite{Ding2020} from $\mathcal{O}(1/\epsilon^6)$ to $\mathcal{O}(1/\epsilon^4)$. To the best of our knowledge, this is the first work to establish zero constraint violation with Natural policy gradient style algorithms for infinite horizon discounted CMDPs. We demonstrate the merits of proposed algorithm via experimental evaluations.      
### 84.FisheyeEX: Polar Outpainting for Extending the FoV of Fisheye Lens  [ :arrow_down: ](https://arxiv.org/pdf/2206.05844.pdf)
>  Fisheye lens gains increasing applications in computational photography and assisted driving because of its wide field of view (FoV). However, the fisheye image generally contains invalid black regions induced by its imaging model. In this paper, we present a FisheyeEX method that extends the FoV of the fisheye lens by outpainting the invalid regions, improving the integrity of captured scenes. Compared with the rectangle and undistorted image, there are two challenges for fisheye image outpainting: irregular painting regions and distortion synthesis. Observing the radial symmetry of the fisheye image, we first propose a polar outpainting strategy to extrapolate the coherent semantics from the center to the outside region. Such an outpainting manner considers the distribution pattern of radial distortion and the circle boundary, boosting a more reasonable completion direction. For the distortion synthesis, we propose a spiral distortion-aware perception module, in which the learning path keeps consistent with the distortion prior of the fisheye image. Subsequently, a scene revision module rearranges the generated pixels with the estimated distortion to match the fisheye image, thus extending the FoV. In the experiment, we evaluate the proposed FisheyeEX on three popular outdoor datasets: Cityscapes, BDD100k, and KITTI, and one real-world fisheye image dataset. The results demonstrate that our approach significantly outperforms the state-of-the-art methods, gaining around 27% more content beyond the original fisheye image.      
### 85.An Industry 4.0 example: real-time quality control for steel-based mass production using Machine Learning on non-invasive sensor data  [ :arrow_down: ](https://arxiv.org/pdf/2206.05818.pdf)
>  Insufficient steel quality in mass production can cause extremely costly damage to tooling, production downtimes and low quality products. Automatic, fast and cheap strategies to estimate essential material properties for quality control, risk mitigation and the prediction of faults are highly desirable. In this work we analyse a high throughput production line of steel-based products. Currently, the material quality is checked using manual destructive testing, which is slow, wasteful and covers only a tiny fraction of the material. To achieve complete testing coverage our industrial collaborator developed a contactless, non-invasive, electromagnetic sensor to measure all material during production in real-time. Our contribution is three-fold: 1) We show in a controlled experiment that the sensor can distinguish steel with deliberately altered properties. 2) 48 steel coils were fully measured non-invasively and additional destructive tests were conducted on samples to serve as ground truth. A linear model is fitted to predict from the non-invasive measurements two key material properties (yield strength and tensile strength) that normally are obtained by destructive tests. The performance is evaluated in leave-one-coil-out cross-validation. 3) The resulting model is used to analyse the material properties and the relationship with logged product faults on real production data of ~108 km of processed material measured with the non-invasive sensor. The model achieves an excellent performance (F3-score of 0.95) predicting material running out of specifications for the tensile strength. The combination of model predictions and logged product faults shows that if a significant percentage of estimated yield stress values is out of specification, the risk of product faults is high. Our analysis demonstrates promising directions for real-time quality control, risk monitoring and fault detection.      
### 86.The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task  [ :arrow_down: ](https://arxiv.org/pdf/2206.05777.pdf)
>  This paper describes the submission of our end-to-end YiTrans speech translation system for the IWSLT 2022 offline task, which translates from English audio to German, Chinese, and Japanese. The YiTrans system is built on large-scale pre-trained encoder-decoder models. More specifically, we first design a multi-stage pre-training strategy to build a multi-modality model with a large amount of labeled and unlabeled data. We then fine-tune the corresponding components of the model for the downstream speech translation tasks. Moreover, we make various efforts to improve performance, such as data filtering, data augmentation, speech segmentation, model ensemble, and so on. Experimental results show that our YiTrans system obtains a significant improvement than the strong baseline on three translation directions, and it achieves +5.2 BLEU improvements over last year's optimal end-to-end system on tst2021 English-German. Our final submissions rank first on English-German and English-Chinese end-to-end systems in terms of the automatic evaluation metric. We make our code and models publicly available.      
### 87.Learning to Detect with Constant False Alarm Rate  [ :arrow_down: ](https://arxiv.org/pdf/2206.05747.pdf)
>  We consider the use of machine learning for hypothesis testing with an emphasis on target detection. Classical model-based solutions rely on comparing likelihoods. These are sensitive to imperfect models and are often computationally expensive. In contrast, data-driven machine learning is often more robust and yields classifiers with fixed computational complexity. Learned detectors usually provide high accuracy with low complexity but do not have a constant false alarm rate (CFAR) as required in many applications. To close this gap, we propose to add a term to the loss function that promotes similar distributions of the detector under any null hypothesis scenario. Experiments show that our approach leads to near CFAR detectors with similar accuracy as their competitors.      
### 88.Arena-Bench: A Benchmarking Suite for Obstacle Avoidance Approaches in Highly Dynamic Environments  [ :arrow_down: ](https://arxiv.org/pdf/2206.05728.pdf)
>  The ability to autonomously navigate safely, especially within dynamic environments, is paramount for mobile robotics. In recent years, DRL approaches have shown superior performance in dynamic obstacle avoidance. However, these learning-based approaches are often developed in specially designed simulation environments and are hard to test against conventional planning approaches. Furthermore, the integration and deployment of these approaches into real robotic platforms are not yet completely solved. In this paper, we present Arena-bench, a benchmark suite to train, test, and evaluate navigation planners on different robotic platforms within 3D environments. It provides tools to design and generate highly dynamic evaluation worlds, scenarios, and tasks for autonomous navigation and is fully integrated into the robot operating system. To demonstrate the functionalities of our suite, we trained a DRL agent on our platform and compared it against a variety of existing different model-based and learning-based navigation approaches on a variety of relevant metrics. Finally, we deployed the approaches towards real robots and demonstrated the reproducibility of the results. The code is publicly available at <a class="link-external link-http" href="http://github.com/ignc-research/arena-bench" rel="external noopener nofollow">this http URL</a>.      
### 89.DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2206.05687.pdf)
>  Remote photoplethysmography (rPPG) based physiological measurement has great application values in affective computing, non-contact health monitoring, telehealth monitoring, etc, which has become increasingly important especially during the COVID-19 pandemic. Existing methods are generally divided into two groups. The first focuses on mining the subtle blood volume pulse (BVP) signals from face videos, but seldom explicitly models the noises that dominate face video content. They are susceptible to the noises and may suffer from poor generalization ability in unseen scenarios. The second focuses on modeling noisy data directly, resulting in suboptimal performance due to the lack of regularity of these severe random noises. In this paper, we propose a Decomposition and Reconstruction Network (DRNet) focusing on the modeling of physiological features rather than noisy data. A novel cycle loss is proposed to constrain the periodicity of physiological information. Besides, a plug-and-play Spatial Attention Block (SAB) is proposed to enhance features along with the spatial location information. Furthermore, an efficient Patch Cropping (PC) augmentation strategy is proposed to synthesize augmented samples with different noise and features. Extensive experiments on different public datasets as well as the cross-database testing demonstrate the effectiveness of our approach.      
### 90.Dealing with Sparse Rewards in Continuous Control Robotics via Heavy-Tailed Policies  [ :arrow_down: ](https://arxiv.org/pdf/2206.05652.pdf)
>  In this paper, we present a novel Heavy-Tailed Stochastic Policy Gradient (HT-PSG) algorithm to deal with the challenges of sparse rewards in continuous control problems. Sparse reward is common in continuous control robotics tasks such as manipulation and navigation, and makes the learning problem hard due to non-trivial estimation of value functions over the state space. This demands either reward shaping or expert demonstrations for the sparse reward environment. However, obtaining high-quality demonstrations is quite expensive and sometimes even impossible. We propose a heavy-tailed policy parametrization along with a modified momentum-based policy gradient tracking scheme (HT-SPG) to induce a stable exploratory behavior to the algorithm. The proposed algorithm does not require access to expert demonstrations. We test the performance of HT-SPG on various benchmark tasks of continuous control with sparse rewards such as 1D Mario, Pathological Mountain Car, Sparse Pendulum in OpenAI Gym, and Sparse MuJoCo environments (Hopper-v2). We show consistent performance improvement across all tasks in terms of high average cumulative reward. HT-SPG also demonstrates improved convergence speed with minimum samples, thereby emphasizing the sample efficiency of our proposed algorithm.      
### 91.An Unsupervised Deep-Learning Method for Bone Age Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2206.05641.pdf)
>  The bone age, reflecting the degree of development of the bones, can be used to predict the adult height and detect endocrine diseases of children. Both examinations of radiologists and variability of operators have a significant impact on bone age assessment. To decrease human intervention , machine learning algorithms are used to assess the bone age automatically. However, conventional supervised deep-learning methods need pre-labeled data. In this paper, based on the convolutional auto-encoder with constraints (CCAE), an unsupervised deep-learning model proposed in the classification of the fingerprint, we propose this model for the classification of the bone age and baptize it BA-CCAE. In the proposed BA-CCAE model, the key regions of the raw X-ray images of the bone age are encoded, yielding the latent vectors. The K-means clustering algorithm is used to obtain the final classifications by grouping the latent vectors of the bone images. A set of experiments on the Radiological Society of North America pediatric bone age dataset (RSNA) show that the accuracy of classifications at 48-month intervals is 76.15%. Although the accuracy now is lower than most of the existing supervised models, the proposed BA-CCAE model can establish the classification of bone age without any pre-labeled data, and to the best of our knowledge, the proposed BA-CCAE is one of the few trails using the unsupervised deep-learning method for the bone age assessment.      
### 92.Random Access-based Multiuser Computation Offloading for Devices in IoT Applications  [ :arrow_down: ](https://arxiv.org/pdf/2206.05634.pdf)
>  In various Internet-of-Things (IoT) applications, a number of devices and sensors are used to collect data sets. As devices become more capable and smarter, they can not only collect data sets, but also process them locally. However, since most devices would be limited in terms of computing power and energy, they can take advantage of offloading so that their tasks can be carried out at mobile edge computing (MEC) servers. In this paper, we discuss computation offloading for devices in IoT applications. In particular, we consider users or devices with sporadic tasks, where optimizing resource allocation between offloading devices and coordinating for multiuser offloading becomes inefficient. Thus, we propose a two-stage offloading approach that is friendly to devices with sporadic tasks as it employs multichannel random access for offloading requests with low signaling overhead. The stability of the two-stage offloading approach is considered with methods to stabilize the system. We also analyze the latency outage probability as a performance index from a device perspective.      
### 93.A Two-Dimensional FFT Precoded Filter Bank Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2206.05570.pdf)
>  This work proposes a new precoded filter bank (FB) system via a two-dimensional (2D) fast Fourier transform (2D-FFT). Its structure is similar to Orthogonal Time Frequency Space (OTFS) systems, where the OFDM transmitter is changed to a filter bank multi-carrier (FBMC) one, thus obtaining a lower out-of-band emission. The complex orthogonality of the FBMC transmission is guaranteed by using precoding based on a discrete Fourier transform, which is also used to implement the two-dimensional fast Fourier transform. Through the use of a global transmission matrix, we propose a hybrid receiver for the new system. First, a frequency domain equalization is performed, followed by an interference cancellation on the delay-Doppler domain. The simulation results show that the proposed system obtains an error performance similar to other OTFS systems, and superior performance as compared to other precoded FBMC systems.      
### 94.Multiple RISs-Aided Networks: Performance Analysis and Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2206.05524.pdf)
>  This paper analyzes the performance of multiple reconfigurable intelligent surfaces (RISs)-aided networks. The paper also provides some optimization results on the number of reflecting elements on RISs and the optimal placement of RISs. We first derive accurate closed-form approximations for RIS channels' distributions assuming independent non-identically distributed (i.ni.d.) Nakagami-\emph{m} fading environment. Then, the approximate expressions for outage probability (OP) and average symbol error probability are derived in closed-form. Furthermore, to get more insights into the system performance, we derive the asymptotic OP at the high signal-to-noise ratio regime and provide closed-form expressions for the system diversity order and coding gain. Finally, the accuracy of our theoretical analysis is validated through Monte-Carlo simulations. The obtained results show that the considered RIS scenario can provide a diversity order of $\frac{a}{2}K$, where $a$ is a function of the Nakagami fading parameter $m$ and the number of meta-surface elements $N$, and $K$ is the number of RISs.      
### 95.A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal  [ :arrow_down: ](https://arxiv.org/pdf/2206.05520.pdf)
>  There are several previous methods based on neural network can have great performance in denoising salt and pepper noise. However, those methods are based on a hypothesis that the value of salt and pepper noise is exactly 0 and 255. It is not true in the real world. The result of those methods deviate sharply when the value is different from 0 and 255. To overcome this weakness, our method aims at designing a convolutional neural network to detect the noise pixels in a wider range of value and then a filter is used to modify pixel value to 0, which is beneficial for further filtering. Additionally, another convolutional neural network is used to conduct the denoising and restoration work.      
### 96.Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2206.05518.pdf)
>  Self-supervised learning (SSL) based models have been shown to generate powerful representations that can be used to improve the performance of downstream speech tasks. Several state-of-the-art SSL models are available, and each of these models optimizes a different loss which gives rise to the possibility of their features being complementary. This paper proposes using an ensemble of such SSL representations and models, which exploits the complementary nature of the features extracted by the various pretrained models. We hypothesize that this results in a richer feature representation and shows results for the ASR downstream task. To this end, we use three SSL models that have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and WaveLM. We explore the ensemble of models fine-tuned for the ASR task and the ensemble of features using the embeddings obtained from the pre-trained models for a downstream ASR task. We get improved performance over individual models and pre-trained features using Librispeech(100h) and WSJ dataset for the downstream tasks.      
### 97.Channel Estimation for Massive MIMO systems using Tensor Cores in GPU  [ :arrow_down: ](https://arxiv.org/pdf/2206.05506.pdf)
>  For efficient use of Massive MIMO systems, fast and accurate channel estimation is very important. But the Large-scale antenna array presence requires high pilot overhead for high accuracy of estimation. Also, when used with software-based processing systems like CPUs and GPUs, high processing latency becomes a major issue. To reduce Pilot overhead, a Pilot transmission scheme in combination with PN Sequence correlation based channel estimation scheme is implemented. Then, to deal with the issue of high processing latency, Tensor Cores in Nvidia GPUs are used for computing the channel estimation. Experiments are performed by using Nvidia V100 GPU in the ORBIT Testbed to show the performance of the Pilot transmission scheme. By varying factors like PN sequence length, Channel Impulse Response length, number of multiplexed transmitters, and scale of MIMO, the accuracy and processing latency of Tensor Core implementation of the Channel Estimation is evaluated.      
### 98.Hierarchical Conditional Variational Autoencoder Based Acoustic Anomaly Detection  [ :arrow_down: ](https://arxiv.org/pdf/2206.05460.pdf)
>  This paper aims to develop an acoustic signal-based unsupervised anomaly detection method for automatic machine monitoring. Existing approaches such as deep autoencoder (DAE), variational autoencoder (VAE), conditional variational autoencoder (CVAE) etc. have limited representation capabilities in the latent space and, hence, poor anomaly detection performance. Different models have to be trained for each different kind of machines to accurately perform the anomaly detection task. To solve this issue, we propose a new method named as hierarchical conditional variational autoencoder (HCVAE). This method utilizes available taxonomic hierarchical knowledge about industrial facility to refine the latent space representation. This knowledge helps model to improve the anomaly detection performance as well. We demonstrated the generalization capability of a single HCVAE model for different types of machines by using appropriate conditions. Additionally, to show the practicability of the proposed approach, (i) we evaluated HCVAE model on different domain and (ii) we checked the effect of partial hierarchical knowledge. Our results show that HCVAE method validates both of these points, and it outperforms the baseline system on anomaly detection task by utmost 15 % on the AUC score metric.      
### 99.Robust full-pose-parameter estimation for the LED array in Fourier ptychographic microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2206.05451.pdf)
>  Fourier ptychographic microscopy (FPM) can achieve quantitative phase imaging with a large space-bandwidth product by synthesizing a set of low-resolution intensity images captured under angularly varying illuminations. Determining accurate illumination angles is critical because the consistency between actual systematic parameters and those used in the recovery algorithm is essential for high-quality imaging. This paper presents a full-pose-parameter and physics-based method for calibrating illumination angles. Using a physics-based model constructed with general knowledge of the employed microscope and the brightfield-to-darkfield boundaries inside captured images, we can solve for the full-pose parameters of misplaced LED array, which consist of the distance between the sample and the LED array, two orthogonal lateral shifts, one in-plane rotation angle, and two tilt angles, to correct illumination angles precisely. The feasibility and effectiveness of the proposed method for recovering random or remarkable pose parameters have been demonstrated by both qualitative and quantitative experiments. Due to the completeness of the pose parameters, the clarity of the physical model, and the high robustness for arbitrary misalignments, our method can significantly facilitate the design, implementation, and application of concise and robust FPM platforms.      
### 100.Luminance-Guided Chrominance Image Enhancement for HEVC Intra Coding  [ :arrow_down: ](https://arxiv.org/pdf/2206.05432.pdf)
>  In this paper, we propose a luminance-guided chrominance image enhancement convolutional neural network for HEVC intra coding. Specifically, we firstly develop a gated recursive asymmetric-convolution block to restore each degraded chrominance image, which generates an intermediate output. Then, guided by the luminance image, the quality of this intermediate output is further improved, which finally produces the high-quality chrominance image. When our proposed method is adopted in the compression of color images with HEVC intra coding, it achieves 28.96% and 16.74% BD-rate gains over HEVC for the U and V images, respectively, which accordingly demonstrate its superiority.      
### 101.Access Control of Semantic Segmentation Models Using Encrypted Feature Maps  [ :arrow_down: ](https://arxiv.org/pdf/2206.05422.pdf)
>  In this paper, we propose an access control method with a secret key for semantic segmentation models for the first time so that unauthorized users without a secret key cannot benefit from the performance of trained models. The method enables us not only to provide a high segmentation performance to authorized users but to also degrade the performance for unauthorized users. We first point out that, for the application of semantic segmentation, conventional access control methods which use encrypted images for classification tasks are not directly applicable due to performance degradation. Accordingly, in this paper, selected feature maps are encrypted with a secret key for training and testing models, instead of input images. In an experiment, the protected models allowed authorized users to obtain almost the same performance as that of non-protected models but also with robustness against unauthorized access without a key.      
### 102.Downlink Power Minimization in Intelligent Reconfigurable Surface-Aided Security Classification Wireless Communications System  [ :arrow_down: ](https://arxiv.org/pdf/2206.05414.pdf)
>  User privacy protection is considered a critical issue in wireless networks, which drives the demand for various secure information interaction techniques. In this paper, we introduce an intelligent reflecting surface (IRS)-aided security classification wireless communication system, which reduces the transmit power of the base station (BS) by classifying users with different security requirements. Specifically, we divide the users into confidential subscribers with secure communication requirements and general communication users with simple communication requirements. During the communication period, we guarantee the secure rate of the confidential subscribers while ensuring the service quality of the general communication users, thereby reducing the transmit power of the BS. To realize such a secure and green information transmission, the BS implements a beamforming design on the transmitted signal superimposed with artificial noise (AN) and then broadcasts it to users with the assistance of the IRS's reflection. We develop an alternating optimization framework to minimize the BS downlink power with respect to the active beamformers of the BS, the AN vector at the BS, and the reflection phase shifts of the IRS. A successive convex approximation (SCA) method is proposed so that the nonconvex beamforming problems can be converted to tractable convex forms. The simulation results demonstrate that the proposed algorithm is convergent and can reduce the transmit power by 20\% compared to the best benchmark scheme.      
### 103.Multi-instrument Music Synthesis with Spectrogram Diffusion  [ :arrow_down: ](https://arxiv.org/pdf/2206.05408.pdf)
>  An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on all of music but with minimal control and slow generation. <br>In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. <br>We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fréchet distance metrics. <br>Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.      
### 104.Bispectrum-based Cross-frequency Functional Connectivity: Classification of Alzheimer's disease  [ :arrow_down: ](https://arxiv.org/pdf/2206.05354.pdf)
>  Alzheimer's disease (AD) is a neurodegenerative disease known to affect brain functional connectivity (FC). Linear FC measures have been applied to study the differences in AD by splitting neurophysiological signals such as electroencephalography (EEG) recordings into discrete frequency bands and analysing them in isolation. We address this limitation by quantifying cross-frequency FC in addition to the traditional within-band approach. Cross-bispectrum, a higher-order spectral analysis, is used to measure the nonlinear FC and is compared with the cross-spectrum, which only measures the linear FC within bands. Each frequency coupling is then used to construct an FC network, which is in turn vectorised and used to train a classifier. We show that fusing features from networks improves classification accuracy. Although both within-frequency and cross-frequency networks can be used to predict AD with high accuracy, our results show that bispectrum-based FC outperforms cross-spectrum suggesting an important role of cross-frequency FC.      
### 105.AHD ConvNet for Speech Emotion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2206.05286.pdf)
>  Accomplishments in the field of artificial intelligence are utilized in the advancement of computing and making of intelligent machines for facilitating mankind and improving user experience. Emotions are rudimentary for people, affecting thinking and ordinary exercises like correspondence, learning and direction. Speech emotion recognition is domain of interest in this regard and in this work, we propose a novel mel spectrogram learning approach in which our model uses the datapoints to learn emotions from the given wav form voice notes in the popular CREMA-D dataset. Our model uses log mel-spectrogram as feature with number of mels = 64. It took less training time compared to other approaches used to address the problem of emotion speech recognition.      
### 106.Game-Theoretic Neyman-Pearson Detection to Combat Strategic Evasion  [ :arrow_down: ](https://arxiv.org/pdf/2206.05276.pdf)
>  The security in networked systems depends greatly on recognizing and identifying adversarial behaviors. Traditional detection methods focus on specific categories of attacks and have become inadequate for increasingly stealthy and deceptive attacks that are designed to bypass detection strategically. This work aims to develop a holistic theory to countermeasure such evasive attacks. We focus on extending a fundamental class of statistical-based detection methods based on Neyman-Pearson's (NP) hypothesis testing formulation. We propose game-theoretic frameworks to capture the conflicting relationship between a strategic evasive attacker and an evasion-aware NP detector. By analyzing both the equilibrium behaviors of the attacker and the NP detector, we characterize their performance using Equilibrium Receiver-Operational-Characteristic (EROC) curves. We show that the evasion-aware NP detectors outperform the passive ones in the way that the former can act strategically against the attacker's behavior and adaptively modify their decision rules based on the received messages. In addition, we extend our framework to a sequential setting where the user sends out identically distributed messages. We corroborate the analytical results with a case study of anomaly detection.      
### 107.Sharp-MAML: Sharpness-Aware Model-Agnostic Meta Learning  [ :arrow_down: ](https://arxiv.org/pdf/2206.03996.pdf)
>  Model-agnostic meta learning (MAML) is currently one of the dominating approaches for few-shot meta-learning. Albeit its effectiveness, the optimization of MAML can be challenging due to the innate bilevel problem structure. Specifically, the loss landscape of MAML is much more complex with possibly more saddle points and local minimizers than its empirical risk minimization counterpart. To address this challenge, we leverage the recently invented sharpness-aware minimization and develop a sharpness-aware MAML approach that we term Sharp-MAML. We empirically demonstrate that Sharp-MAML and its computation-efficient variant can outperform popular existing MAML baselines (e.g., $+12\%$ accuracy on Mini-Imagenet). We complement the empirical study with the convergence rate analysis and the generalization bound of Sharp-MAML. To the best of our knowledge, this is the first empirical and theoretical study on sharpness-aware minimization in the context of bilevel learning. The code is available at <a class="link-external link-https" href="https://github.com/mominabbass/Sharp-MAML" rel="external noopener nofollow">this https URL</a>.      
