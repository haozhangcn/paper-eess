# ArXiv eess --Fri, 27 May 2022
### 1.Minimization of THD in Nine Level Cascaded H-Bridge Inverter Using Artificial Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2205.13366.pdf)
>  Multilevel inverter converts different level DC voltage to AC voltage. It has wide interest in power industry especially in high power applications. In power electronic equipment the major drawback is the harmonics. Several control strategies are available to reduce the harmonic content and the most widely used measure of Total Harmonic Distortion (THD). In this project, the comparison has been made for the open loop and closed loop PI controller and neural network that predict the switching angle in order to reduce the harmonics. The mapping between Modulation Index and Switching angles are plotted for the forward neural network. After the prediction of switching angles the neural network topologies are executed for better result. This technique is applied for any type of multilevel inverter, Cascaded H-Bridge multilevel inverter is chosen. A nine level Cascaded H-Bridge multilevel inverter power circuit is simulated in MATLAB 8.3 simulink with sinusoidal PWM technique. The comparison results reveal that the THD is reduced to about 3% with neural network control compared to open loop control. The results are presented and analyzed.      
### 2.Sliding mode control with a neural network compensation scheme for electro-hydraulic systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.13343.pdf)
>  Electro-hydraulic servo-systems are widely employed in industrial applications such as robotic manipulators, active suspensions, precision machine tools and aerospace systems. They provide many advantages over electric motors, including high force to weight ratio, fast response time and compact size. However, precise control of electro-hydraulic systems, due to their inherent nonlinear characteristics, cannot be easily obtained with conventional linear controllers. Most flow control valves can also exhibit some hard nonlinearities such as dead-zone due to valve spool overlap. This work describes the development of a sliding mode controller with a neural network compensation scheme for electro-hydraulic systems subject to an unknown dead-zone input. The boundedness and convergence properties of the closed-loop signals are proven using Lyapunov stability theory. Numerical results are presented in order to demonstrate the control system performance.      
### 3.DeepTechnome: Mitigating Unknown Bias in Deep Learning Based Assessment of CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2205.13297.pdf)
>  Reliably detecting diseases using relevant biological information is crucial for real-world applicability of deep learning techniques in medical imaging. We debias deep learning models during training against unknown bias - without preprocessing/filtering the input beforehand or assuming specific knowledge about its distribution or precise nature in the dataset. We use control regions as surrogates that carry information regarding the bias, employ the classifier model to extract features, and suppress biased intermediate features with our custom, modular DecorreLayer. We evaluate our method on a dataset of 952 lung computed tomography scans by introducing simulated biases w.r.t. reconstruction kernel and noise level and propose including an adversarial test set in evaluations of bias reduction techniques. In a moderately sized model architecture, applying the proposed method to learn from data exhibiting a strong bias, it near-perfectly recovers the classification performance observed when training with corresponding unbiased data.      
### 4.Joint Training of Speech Enhancement and Self-supervised Model for Noise-robust ASR  [ :arrow_down: ](https://arxiv.org/pdf/2205.13293.pdf)
>  Speech enhancement (SE) is usually required as a front end to improve the speech quality in noisy environments, while the enhanced speech might not be optimal for automatic speech recognition (ASR) systems due to speech distortion. On the other hand, it was shown that self-supervised pre-training enables the utilization of a large amount of unlabeled noisy data, which is rather beneficial for the noise robustness of ASR. However, the potential of the (optimal) integration of SE and self-supervised pre-training still remains unclear. In order to find an appropriate combination and reduce the impact of speech distortion caused by SE, in this paper we therefore propose a joint pre-training approach for the SE module and the self-supervised model. First, in the pre-training phase the original noisy waveform or the waveform obtained by SE is fed into the self-supervised model to learn the contextual representation, where the quantified clean speech acts as the target. Second, we propose a dual-attention fusion method to fuse the features of noisy and enhanced speeches, which can compensate the information loss caused by separately using individual modules. Due to the flexible exploitation of clean/noisy/enhanced branches, the proposed method turns out to be a generalization of some existing noise-robust ASR models, e.g., enhanced wav2vec2.0. Finally, experimental results on both synthetic and real noisy datasets show that the proposed joint training approach can improve the ASR performance under various noisy settings, leading to a stronger noise robustness.      
### 5.An Event-Driven Compressive Neuromorphic System for Cardiac Arrhythmia Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.13292.pdf)
>  Wearable electrocardiograph (ECG) recording and processing systems have been developed to detect cardiac arrhythmia to help prevent heart attacks. Conventional wearable systems, however, suffer from high energy consumption at both circuit and system levels. To overcome the design challenges, this paper proposes an event-driven compressive ECG recording and neuromorphic processing system for cardiac arrhythmia detection. The proposed system achieves low power consumption and high arrhythmia detection accuracy via system level co-design with spike-based information representation. Event-driven level-crossing ADC (LC-ADC) is exploited in the recording system, which utilizes the sparsity of ECG signal to enable compressive recording and save ADC energy during the silent signal period. Meanwhile, the proposed spiking convolutional neural network (SCNN) based neuromorphic arrhythmia detection method is inherently compatible with the spike-based output of LC-ADC, hence realizing accurate detection and low energy consumption at system level. Simulation results show that the proposed system with 5-bit LC-ADC achieves 88.6\% reduction of sampled data points compared with Nyquist sampling in the MIT-BIH dataset, and 93.59\% arrhythmia detection accuracy with SCNN, demonstrating the compression ability of LC-ADC and the effectiveness of system level co-design with SCNN.      
### 6.Audio Data Augmentation for Acoustic-to-articulatory Speech Inversion using Bidirectional Gated RNNs  [ :arrow_down: ](https://arxiv.org/pdf/2205.13086.pdf)
>  Data augmentation has proven to be a promising prospect in improving the performance of deep learning models by adding variability to training data. In previous work with developing a noise robust acoustic-to-articulatory speech inversion system, we have shown the importance of noise augmentation to improve the performance of speech inversion in noisy speech. In this work, we compare and contrast different ways of doing data augmentation and show how this technique improves the performance of articulatory speech inversion not only on noisy speech, but also on clean speech data. We also propose a Bidirectional Gated Recurrent Neural Network as the speech inversion system instead of the previously used feed forward neural network. The inversion system uses mel-frequency cepstral coefficients (MFCCs) as the input acoustic features and six vocal tract-variables (TVs) as the output articulatory features. The Performance of the system was measured by computing the correlation between estimated and actual TVs on the U. Wisc. X-ray Microbeam database. The proposed speech inversion system shows a 5% relative improvement in correlation over the baseline noise robust system for clean speech data. The pre-trained model, when adapted to each unseen speaker in the test set, improves the average correlation by another 6%.      
### 7.Online Deep Equilibrium Learning for Regularization by Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2205.13051.pdf)
>  Plug-and-Play Priors (PnP) and Regularization by Denoising (RED) are widely-used frameworks for solving imaging inverse problems by computing fixed-points of operators combining physical measurement models and learned image priors. While traditional PnP/RED formulations have focused on priors specified using image denoisers, there is a growing interest in learning PnP/RED priors that are end-to-end optimal. The recent Deep Equilibrium Models (DEQ) framework has enabled memory-efficient end-to-end learning of PnP/RED priors by implicitly differentiating through the fixed-point equations without storing intermediate activation values. However, the dependence of the computational/memory complexity of the measurement models in PnP/RED on the total number of measurements leaves DEQ impractical for many imaging applications. We propose ODER as a new strategy for improving the efficiency of DEQ through stochastic approximations of the measurement models. We theoretically analyze ODER giving insights into its convergence and ability to approximate the traditional DEQ approach. Our numerical results suggest the potential improvements in training/testing complexity due to ODER on three distinct imaging applications.      
### 8.Measuring Perceptual Color Differences of Smartphone Photography  [ :arrow_down: ](https://arxiv.org/pdf/2205.13489.pdf)
>  Measuring perceptual color differences (CDs) is of great importance in modern smartphone photography. Despite the long history, most CD measures have been constrained by psychophysical data of homogeneous color patches or a limited number of simplistic natural images. It is thus questionable whether existing CD measures generalize in the age of smartphone photography characterized by greater content complexities and learning-based image signal processors. In this paper, we put together so far the largest image dataset for perceptual CD assessment, in which the natural images are 1) captured by six flagship smartphones, 2) altered by Photoshop, 3) post-processed by built-in filters of the smartphones, and 4) reproduced with incorrect color profiles. We then conduct a large-scale psychophysical experiment to gather perceptual CDs of 30,000 image pairs in a carefully controlled laboratory environment. Based on the newly established dataset, we make one of the first attempts to construct an end-to-end learnable CD formula based on a lightweight neural network, as a generalization of several previous metrics. Extensive experiments demonstrate that the optimized formula outperforms 28 existing CD measures by a large margin, offers reasonable local CD maps without the use of dense supervision, generalizes well to color patch data, and empirically behaves as a proper metric in the mathematical sense.      
### 9.Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2205.13476.pdf)
>  Reinforcement learning in partially observed Markov decision processes (POMDPs) faces two challenges. (i) It often takes the full history to predict the future, which induces a sample complexity that scales exponentially with the horizon. (ii) The observation and state spaces are often continuous, which induces a sample complexity that scales exponentially with the extrinsic dimension. Addressing such challenges requires learning a minimal but sufficient representation of the observation and state histories by exploiting the structure of the POMDP. <br>To this end, we propose a reinforcement learning algorithm named Embed to Control (ETC), which learns the representation at two levels while optimizing the policy.~(i) For each step, ETC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, ETC learns to represent the full history with a low-dimensional embedding, which assembles the per-step feature. We integrate (i) and (ii) in a unified framework that allows a variety of estimators (including maximum likelihood estimators and generative adversarial networks). For a class of POMDPs with a low-rank structure in the transition kernel, ETC attains an $O(1/\epsilon^2)$ sample complexity that scales polynomially with the horizon and the intrinsic dimension (that is, the rank). Here $\epsilon$ is the optimality gap. To our best knowledge, ETC is the first sample-efficient algorithm that bridges representation learning and policy optimization in POMDPs with infinite observation and state spaces.      
### 10.Sub-Rate Linear Network Coding  [ :arrow_down: ](https://arxiv.org/pdf/2205.13431.pdf)
>  Increasing network utilization is often considered as the holy grail of communications. In this article, the concept of sub-rate coding and decoding in the framework of linear network coding (LNC) is discussed for single-source multiple-sinks finite acyclic networks. Sub-rate coding offers an add-on to existing LNC. It allows sinks whose max-flow is smaller than the source message-rate, termed \emph{sub-rate sinks}, to decode a portion of the transmitted message without degrading the maximum achievable rate of LNC sinks whose max-flow is equal (or greater) than the rate of the source node. The article studies theoretical aspects of sub-rate coding by formulating the conditions a node (and indeed the network) must fulfill so as to qualify as a legitimate sub-rate sink.      
### 11.Machine Learning Models Are Not Necessarily Biased When Constructed Properly: Evidence from Neuroimaging Studies  [ :arrow_down: ](https://arxiv.org/pdf/2205.13421.pdf)
>  Despite the great promise that machine learning has offered in many fields of medicine, it has also raised concerns about potential biases and poor generalization across genders, age distributions, races and ethnicities, hospitals, and data acquisition equipment and protocols. In the current study, and in the context of three brain diseases, we provide experimental data which support that when properly trained, machine learning models can generalize well across diverse conditions and do not suffer from biases. Specifically, by using multi-study magnetic resonance imaging consortia for diagnosing Alzheimer's disease, schizophrenia, and autism spectrum disorder, we find that, the accuracy of well-trained models is consistent across different subgroups pertaining to attributes such as gender, age, and racial groups, as also different clinical studies. We find that models that incorporate multi-source data from demographic, clinical, genetic factors and cognitive scores are also unbiased. These models have better predictive accuracy across subgroups than those trained only with structural measures in some cases but there are also situations when these additional features do not help.      
### 12.A Physical-World Adversarial Attack Against 3D Face Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2205.13412.pdf)
>  3D face recognition systems have been widely employed in intelligent terminals, among which structured light imaging is a common method to measure the 3D shape. However, this method could be easily attacked, leading to inaccurate 3D face recognition. In this paper, we propose a novel, physically-achievable attack on the fringe structured light system, named structured light attack. The attack utilizes a projector to project optical adversarial fringes on faces to generate point clouds with well-designed noises. We firstly propose a 3D transform-invariant loss function to enhance the robustness of 3D adversarial examples in the physical-world attack. Then we reverse the 3D adversarial examples to the projector's input to place noises on phase-shift images, which models the process of structured light imaging. A real-world structured light system is constructed for the attack and several state-of-the-art 3D face recognition neural networks are tested. Experiments show that our method can attack the physical system successfully and only needs minor modifications of projected images.      
### 13.On stochastic stabilization via non-smooth control Lyapunov functions  [ :arrow_down: ](https://arxiv.org/pdf/2205.13409.pdf)
>  Control Lyapunov function is a central tool in stabilization. It generalizes an abstract energy function -- a Lyapunov function -- to the case of controlled systems. It is a known fact that most control Lyapunov functions are non-smooth -- so is the case in non-holonomic systems, like wheeled robots and cars. Frameworks for stabilization using non-smooth control Lyapunov functions exist, like Dini aiming and steepest descent. This work generalizes the related results to the stochastic case. As the groundwork, sampled control scheme is chosen in which control actions are computed at discrete moments in time using discrete measurements of the system state. In such a setup, special attention should be paid to the sample-to-sample behavior of the control Lyapunov function. A particular challenge here is a random noise acting on the system. The central result of this work is a theorem that states, roughly, that if there is a, generally non-smooth, control Lyapunov function, the given stochastic dynamical system can be practically stabilized in the sample-and-hold mode meaning that the control actions are held constant within sampling time steps. A particular control method chosen is based on Moreau-Yosida regularization, in other words, inf-convolution of the control Lyapunov function, but the overall framework is extendable to further control schemes. It is assumed that the system noise be bounded almost surely, although the case of unbounded noise is briefly addressed.      
### 14.Dynamic Interventions for Networked Contagions  [ :arrow_down: ](https://arxiv.org/pdf/2205.13394.pdf)
>  We study the problem of designing dynamic intervention policies for minimizing networked defaults in financial networks. Formally, we consider a dynamic version of the celebrated Eisenberg-Noe model of financial network liabilities, and use this to study the design of external intervention policies. Our controller has a fixed resource budget in each round, and can use this to minimize the effect of demand/supply shocks in the network. We formulate the optimal intervention problem as a Markov Decision Process, and show how we can leverage the problem structure to efficiently compute optimal intervention policies with continuous interventions, and constant-factor approximations with discrete interventions. Going beyond financial networks, we argue that our model captures dynamic network intervention in a much broader class of dynamic demand/supply settings with networked inter-dependencies. To demonstrate this, we apply our intervention algorithms to a wide variety of application domains, including ridesharing, online transaction platforms, and financial networks with agent mobility; in each case, we study the relationship between node centrality and intervention strength, as well as fairness properties of the optimal interventions.      
### 15.New methods of removing debris and high-throughput counting of cyst nematode eggs extracted from field soil  [ :arrow_down: ](https://arxiv.org/pdf/2205.13363.pdf)
>  The soybean cyst nematode (SCN), Heterodera glycines, is the most damaging pathogen of soybeans in the United States. To assess the severity of nematode infestations in the field, SCN egg population densities are determined. Cysts (dead females) of the nematode must be extracted from soil samples and then ground to extract the eggs within. Sucrose centrifugation commonly is used to separate debris from suspensions of extracted nematode eggs. We present a method using OptiPrep as a density gradient medium with improved separation and recovery of extracted eggs compared to the sucrose centrifugation technique. Also, computerized methods were developed to automate the identification and counting of nematode eggs from the processed samples. In one approach, a high-resolution scanner was used to take static images of extracted eggs and debris on filter papers, and a deep learning network was trained to identify and count the eggs among the debris. In the second approach, a lensless imaging setup was developed using off-the-shelf components, and the processed egg samples were passed through a microfluidic flow chip made from double-sided adhesive tape. Holographic videos were recorded of the passing eggs and debris, and the videos were reconstructed and processed by custom software program to obtain egg counts. The performance of the software programs for egg counting was characterized with SCN-infested soil collected from two farms, and the results using these methods were compared with those obtained through manual counting.      
### 16.Multi-fidelity power flow solver  [ :arrow_down: ](https://arxiv.org/pdf/2205.13362.pdf)
>  We propose a multi-fidelity neural network (MFNN) tailored for rapid high-dimensional grid power flow simulations and contingency analysis with scarce high-fidelity contingency data. The proposed model comprises two networks -- the first one trained on DC approximation as low-fidelity data and coupled to a high-fidelity neural net trained on both low- and high-fidelity power flow data. Each network features a latent module which parametrizes the model by a discrete grid topology vector for generalization (e.g., $n$ power lines with $k$ disconnections or contingencies, if any), and the targeted high-fidelity output is a weighted sum of linear and nonlinear functions. We tested the model on 14- and 118-bus test cases and evaluated its performance based on the $n-k$ power flow prediction accuracy with respect to imbalanced contingency data and high-to-low-fidelity sample ratio. The results presented herein demonstrate MFNN's potential and its limits with up to two orders of magnitude faster and more accurate power flow solutions than DC approximation.      
### 17.A neural network based controller for underwater robotic vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2205.13344.pdf)
>  Due to the enormous technological improvements obtained in the last decades it is possible to use robotic vehicles for underwater exploration. This work describes the development of a dynamic positioning system for remotely operated underwater vehicles based. The adopted approach is developed using Lyapunov Stability Theory and enhanced by a neural network based algorithm for uncertainty and disturbance compensation. The performance of the proposed control scheme is evaluated by means of numerical simulations.      
### 18.SARS-CoV-2 Result Interpretation based on Image Analysis of Lateral Flow Devices  [ :arrow_down: ](https://arxiv.org/pdf/2205.13311.pdf)
>  The widely used gene quantisation technique, Lateral Flow Device (LFD), is now commonly used to detect the presence of SARS-CoV-2. It is enabling the control and prevention of the spread of the virus. Depending on the viral load, LFD have different sensitivity and self-test for normal user present additional challenge to interpret the result. With the evolution of machine learning algorithms, image processing and analysis has seen unprecedented growth. In this interdisciplinary study, we employ novel image analysis methods of computer vision and machine learning field to study visual features of the control region of LFD. Here, we automatically derive results for any image containing LFD into positive, negative or inconclusive. This will reduce the burden of human involvement of health workers and perception bias.      
### 19.Analytical Interpretation of Latent Codes in InfoGAN with SAR Images  [ :arrow_down: ](https://arxiv.org/pdf/2205.13294.pdf)
>  Generative Adversarial Networks (GANs) can synthesize abundant photo-realistic synthetic aperture radar (SAR) images. Some recent GANs (e.g., InfoGAN), are even able to edit specific properties of the synthesized images by introducing latent codes. It is crucial for SAR image synthesis since the targets in real SAR images are with different properties due to the imaging mechanism. Despite the success of InfoGAN in manipulating properties, there still lacks a clear explanation of how these latent codes affect synthesized properties, thus editing specific properties usually relies on empirical trials, unreliable and time-consuming. In this paper, we show that latent codes are disentangled to affect the properties of SAR images in a non-linear manner. By introducing some property estimators for latent codes, we are able to provide a completely analytical nonlinear model to decompose the entangled causality between latent codes and different properties. The qualitative and quantitative experimental results further reveal that the properties can be calculated by latent codes, inversely, the satisfying latent codes can be estimated given desired properties. In this case, properties can be manipulated by latent codes as we expect.      
### 20.Acute Lymphoblastic Leukemia Detection Using Hypercomplex-Valued Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.13273.pdf)
>  This paper features convolutional neural networks defined on hypercomplex algebras applied to classify lymphocytes in blood smear digital microscopic images. Such classification is helpful for the diagnosis of acute lymphoblast leukemia (ALL), a type of blood cancer. We perform the classification task using eight hypercomplex-valued convolutional neural networks (HvCNNs) along with real-valued convolutional networks. Our results show that HvCNNs perform better than the real-valued model, showcasing higher accuracy with a much smaller number of parameters. Moreover, we found that HvCNNs based on Clifford algebras processing HSV-encoded images attained the highest observed accuracies. Precisely, our HvCNN yielded an average accuracy rate of 96.6% using the ALL-IDB2 dataset with a 50% train-test split, a value extremely close to the state-of-the-art models but using a much simpler architecture with significantly fewer parameters.      
### 21.DT-SV: A Transformer-based Time-domain Approach for Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2205.13249.pdf)
>  Speaker verification (SV) aims to determine whether the speaker's identity of a test utterance is the same as the reference speech. In the past few years, extracting speaker embeddings using deep neural networks for SV systems has gone mainstream. Recently, different attention mechanisms and Transformer networks have been explored widely in SV fields. However, utilizing the original Transformer in SV directly may have frame-level information waste on output features, which could lead to restrictions on capacity and discrimination of speaker embeddings. Therefore, we propose an approach to derive utterance-level speaker embeddings via a Transformer architecture that uses a novel loss function named diffluence loss to integrate the feature information of different Transformer layers. Therein, the diffluence loss aims to aggregate frame-level features into an utterance-level representation, and it could be integrated into the Transformer expediently. Besides, we also introduce a learnable mel-fbank energy feature extractor named time-domain feature extractor that computes the mel-fbank features more precisely and efficiently than the standard mel-fbank extractor. Combining Diffluence loss and Time-domain feature extractor, we propose a novel Transformer-based time-domain SV model (DT-SV) with faster training speed and higher accuracy. Experiments indicate that our proposed model can achieve better performance in comparison with other models.      
### 22.Light Field Raindrop Removal via 4D Re-sampling  [ :arrow_down: ](https://arxiv.org/pdf/2205.13165.pdf)
>  The Light Field Raindrop Removal (LFRR) aims to restore the background areas obscured by raindrops in the Light Field (LF). Compared with single image, the LF provides more abundant information by regularly and densely sampling the scene. Since raindrops have larger disparities than the background in the LF, the majority of texture details occluded by raindrops are visible in other views. In this paper, we propose a novel LFRR network by directly utilizing the complementary pixel information of raindrop-free areas in the input raindrop LF, which consists of the re-sampling module and the refinement module. Specifically, the re-sampling module generates a new LF which is less polluted by raindrops through re-sampling position predictions and the proposed 4D interpolation. The refinement module improves the restoration of the completely occluded background areas and corrects the pixel error caused by 4D interpolation. Furthermore, we carefully build the first real scene LFRR dataset for model training and validation. Experiments demonstrate that the proposed method can effectively remove raindrops and achieves state-of-the-art performance in both background restoration and view consistency maintenance.      
### 23.Integration of Blockchain and Edge Computing in Internet of Things: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2205.13160.pdf)
>  As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the integration of blockchain and edge computing. In particular, we first give an overview of blockchain and edge computing. We then present a general architecture of an integration of blockchain and edge computing system. We next study how to utilize blockchain to benefit edge computing, as well as how to use edge computing to benefit blockchain. We also discuss the issues brought by the integration of blockchain and edge computing system and solutions from perspectives of resource management, joint optimization, data management, computation offloading and security mechanism. Finally, we analyze and summarize the existing challenges posed by the integration of blockchain and edge computing system and the potential solutions in the future.      
### 24.Coverage Probability Analysis of RIS-Assisted High-Speed Train Communications  [ :arrow_down: ](https://arxiv.org/pdf/2205.13133.pdf)
>  Reconfigurable intelligent surface (RIS) has received increasing attention due to its capability of extending cell coverage by reflecting signals toward receivers. This paper considers a RIS-assisted high-speed train (HST) communication system to improve the coverage probability. We derive the closed-form expression of coverage probability. Moreover, we analyze impacts of some key system parameters, including transmission power, signal-to-noise ratio threshold, and horizontal distance between base station and RIS. Simulation results verify the efficiency of RIS-assisted HST communications in terms of coverage probability.      
### 25.GraphPMU: Event Clustering via Graph Representation Learning Using Locationally-Scarce Distribution-Level Fundamental and Harmonic PMU Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2205.13116.pdf)
>  This paper is concerned with the complex task of identifying the type and cause of the events that are captured by distribution-level phasor measurement units (D-PMUs) in order to enhance situational awareness in power distribution systems. Our goal is to address two fundamental challenges in this field: a) scarcity in measurement locations due to the high cost of purchasing, installing, and streaming data from D-PMUs; b) limited prior knowledge about the event signatures due to the fact that the events are diverse, infrequent, and inherently unscheduled. To tackle these challenges, we propose an unsupervised graph-representation learning method, called GraphPMU, to significantly improve the performance in event clustering under locationally-scarce data availability by proposing the following two new directions: 1) using the topological information about the relative location of the few available phasor measurement units on the graph of the power distribution network; 2) utilizing not only the commonly used fundamental phasor measurements, bus also the less explored harmonic phasor measurements in the process of analyzing the signatures of various events. Through a detailed analysis of several case studies, we show that GraphPMU can highly outperform the prevalent methods in the literature.      
### 26.Hybrid Spherical- and Planar-Wave Channel Modeling and Estimation for Terahertz Integrated UM-MIMO and IRS Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.13113.pdf)
>  Integrated ultra-massive multiple-input multiple-output (UM-MIMO) and intelligent reflecting surface (IRS) systems are promising for 6G and beyond Terahertz (0.1-10 THz) communications, to effectively bypass the barriers of limited coverage and line-of-sight blockage. However, excessive dimensions of UM-MIMO and IRS enlarge the near-field region, while strong THz channel sparsity in far-field is detrimental to spatial multiplexing. Moreover, channel estimation (CE) requires recovering the large-scale channel from severely compressed observations due to limited RF-chains. To tackle these challenges, a hybrid spherical- and planar-wave channel model (HSPM) is developed for the cascaded channel of the integrated system. The spatial multiplexing gains under near-field and far-field regions are analyzed, which are found to be limited by the segmented channel with a lower rank. Furthermore, a compressive sensing-based CE framework is developed, including a sparse channel representation method, a separate-side estimation (SSE) and a dictionary-shrinkage estimation (DSE) algorithms. Numerical results verify the effectiveness of the HSPM, the capacity of which is only $5\times10^{-4}$ bits/s/Hz deviated from that obtained by the ground-truth spherical-wave-model, with 256 elements. While the SSE achieves improved accuracy for CE than benchmark algorithms, the DSE is more attractive in noisy environments, with 0.8 dB lower normalized-mean-square-error than SSE.      
### 27.Learning to segment with limited annotations: Self-supervised pretraining with regression and contrastive loss in MRI  [ :arrow_down: ](https://arxiv.org/pdf/2205.13109.pdf)
>  Obtaining manual annotations for large datasets for supervised training of deep learning (DL) models is challenging. The availability of large unlabeled datasets compared to labeled ones motivate the use of self-supervised pretraining to initialize DL models for subsequent segmentation tasks. In this work, we consider two pre-training approaches for driving a DL model to learn different representations using: a) regression loss that exploits spatial dependencies within an image and b) contrastive loss that exploits semantic similarity between pairs of images. The effect of pretraining techniques is evaluated in two downstream segmentation applications using Magnetic Resonance (MR) images: a) liver segmentation in abdominal T2-weighted MR images and b) prostate segmentation in T2-weighted MR images of the prostate. We observed that DL models pretrained using self-supervision can be finetuned for comparable performance with fewer labeled datasets. Additionally, we also observed that initializing the DL model using contrastive loss based pretraining performed better than the regression loss.      
### 28.Urban Rhapsody: Large-scale exploration of urban soundscapes  [ :arrow_down: ](https://arxiv.org/pdf/2205.13064.pdf)
>  Noise is one of the primary quality-of-life issues in urban environments. In addition to annoyance, noise negatively impacts public health and educational performance. While low-cost sensors can be deployed to monitor ambient noise levels at high temporal resolutions, the amount of data they produce and the complexity of these data pose significant analytical challenges. One way to address these challenges is through machine listening techniques, which are used to extract features in attempts to classify the source of noise and understand temporal patterns of a city's noise situation. However, the overwhelming number of noise sources in the urban environment and the scarcity of labeled data makes it nearly impossible to create classification models with large enough vocabularies that capture the true dynamism of urban soundscapes In this paper, we first identify a set of requirements in the yet unexplored domain of urban soundscape exploration. To satisfy the requirements and tackle the identified challenges, we propose Urban Rhapsody, a framework that combines state-of-the-art audio representation, machine learning, and visual analytics to allow users to interactively create classification models, understand noise patterns of a city, and quickly retrieve and label audio excerpts in order to create a large high-precision annotated database of urban sound recordings. We demonstrate the tool's utility through case studies performed by domain experts using data generated over the five-year deployment of a one-of-a-kind sensor network in New York City.      
### 29.Preference Dynamics Under Personalized Recommendations  [ :arrow_down: ](https://arxiv.org/pdf/2205.13026.pdf)
>  Many projects (both practical and academic) have designed algorithms to match users to content they will enjoy under the assumption that user's preferences and opinions do not change with the content they see. Evidence suggests that individuals' preferences are directly shaped by what content they see -- radicalization, rabbit holes, polarization, and boredom are all example phenomena of preferences affected by content. Polarization in particular can occur even in ecosystems with "mass media," where no personalization takes place, as recently explored in a natural model of preference dynamics by~\citet{hkazla2019geometric} and~\citet{gaitonde2021polarization}. If all users' preferences are drawn towards content they already like, or are repelled from content they already dislike, uniform consumption of media leads to a population of heterogeneous preferences converging towards only two poles. <br>In this work, we explore whether some phenomenon akin to polarization occurs when users receive \emph{personalized} content recommendations. We use a similar model of preference dynamics, where an individual's preferences move towards content the consume and enjoy, and away from content they consume and dislike. We show that standard user reward maximization is an almost trivial goal in such an environment (a large class of simple algorithms will achieve only constant regret). A more interesting objective, then, is to understand under what conditions a recommendation algorithm can ensure stationarity of user's preferences. We show how to design a content recommendations which can achieve approximate stationarity, under mild conditions on the set of available content, when a user's preferences are known, and how one can learn enough about a user's preferences to implement such a strategy even when user preferences are initially unknown.      
