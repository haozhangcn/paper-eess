# ArXiv eess --Thu, 12 May 2022
### 1.Joint OAM Radar-Communication Systems: Target Recognition and Beam Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2205.05651.pdf)
>  Orbital angular momentum (OAM) radars are able to estimate the azimuth angle and the rotation velocity of multiple targets without relative motion or beam scanning. Moreover, OAM wireless communications can achieve high spectral efficiency (SE) by utilizing a set of information-bearing modes on the same frequency channel. Benefitting from the above advantages, in this paper, we design a novel radar-centric joint OAM radar-communication (RadCom) scheme based on uniform circular arrays (UCAs), which modulates information signals on the existing OAM radar waveform. In details, we first propose an OAM-based three-dimensional (3-D) super-resolution position estimation and rotation velocity detection method, which can accurately estimate the 3-D position and rotation velocity of multiple targets. Then, we derive the posterior Cramer-Rao bound (PCRB) of the OAM-based estimates and, finally, we analyze the transmission rate of the integrated communication system. To achieve the best trade-off between imaging and communication, the transmitted integrated OAM beams are optimized by means of an exhaustive search method. Both mathematical analysis and simulation results show that the proposed radar-centric joint OAM RadCom scheme can accurately estimate the 3-D position and rotation velocity of multiple targets while ensuring the transmission rate of the communication receiver, which can be regarded as an effective supplement to existing joint RadCom schemes.      
### 2.Parametrization of High-Rank Line-of-Sight MIMO Channels with Reflected Paths  [ :arrow_down: ](https://arxiv.org/pdf/2205.05640.pdf)
>  High-rank line-of-sight (LOS) MIMO systems have attracted considerable attention for millimeter wave and THz communications. The small wavelengths in these frequencies enable spatial multiplexing with massive data rates at long distances. Such systems are also being considered for multi-path non-LOS (NLOS) environments. In these scenarios, standard channels models based on plane waves cannot capture the curvature of each wave front necessary to model spatial multiplexing. This work presents a novel and simple multi-path wireless channel parametrization where each path is replaced by a LOS path with a reflected image source. The model fully captures the spherical nature of each wave front and uses only two additional parameters relative to the standard plane wave model. Moreover, the parameters can be easily captured in standard ray tracing. The accuracy of the approach is demonstrated on detailed ray tracing simulations at 28GHz and 140GHz in a dense urban area.      
### 3.Benefits of Feedforward for Model Predictive Airpath Control of Diesel Engines  [ :arrow_down: ](https://arxiv.org/pdf/2205.05630.pdf)
>  This paper investigates options to complement a diesel engine airpath feedback controller with a feedforward. The control objective is to track the intake manifold pressure and exhaust gas recirculation (EGR) rate targets by manipulating the EGR valve and variable geometry turbine (VGT) while satisfying state and input constraints. The feedback controller is based on rate-based Model Predictive Control (MPC) that provides integral action for tracking. Two options for the feedforward are considered one based on a look-up table that specifies the feedforward as a function of engine speed and fuel injection rate, and another one based on a (non-rate-based) MPC that generates dynamic feedforward trajectories. The controllers are designed and verified using a high-fidelity engine model in GT-Power and exploit a low-order rate-based linear parameter-varying (LPV) model for prediction which is identified from transient response data generated by the GT-Power model. It is shown that the combination of feedforward and feedback MPC has the potential to improve the performance and robustness of the control design. In particular, the feedback MPC without feedforward can lose stability at low engine speeds, while MPC-based feedforward results in the best transient response. Mechanisms by which feedforward is able to assist in stabilization and improve performance are discussed.      
### 4.Computing control invariant sets of nonlinear systems: decomposition and distributed computing  [ :arrow_down: ](https://arxiv.org/pdf/2205.05622.pdf)
>  In this work, we present a distributed framework based on the graph algorithm for computing control invariant set for nonlinear cascade systems. The proposed algorithm exploits the structure of the interconnections within a process network. First, the overall system is decomposed into several subsystems with overlapping states. Second, the control invariant set for the subsystems are computed in a distributed manner. Finally, an approximation of the control invariant set for the overall system is reconstructed from the subsystem solutions and validated. We demonstrate the efficacy and convergence of the proposed method to the centralized graph-based algorithm using several numerical examples including a six dimensional continuous stirred tank reactor system.      
### 5.Wasserstein Image Local Analysis: Histogram of Orientations, Smoothing and Edge Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.05606.pdf)
>  The Histogram of Oriented Gradient is a widely used image feature, which describes local image directionality based on numerical differentiation. Due to its ill-posed nature, small noise may lead to large errors. Conventional HOG may fail to produce meaningful directionality results in the presence of noise, which is common in medical radiographic imaging. We approach the directionality problem from a novel perspective by the use of the optimal transport map of a local image patch to a uni-color patch of its mean. We decompose the transport map into sub-work costs in different directions. We evaluated the ability of the optimal transport to quantify tumor heterogeneity from brain MRI images of patients with glioblastoma multiforme from the TCIA. By considering the entropy difference of the extracted local directionality within tumor regions, we found that patients with higher entropy in their images, had statistically significant worse overall survival (p $=0.008$), which indicates that tumors exhibiting flows in many directions may be more malignant, perhaps reflecting high tumor histologic grade, a reflection of histologic disorganization. We also explored the possibility of solving classical image processing problems such as smoothing and edge detection via optimal transport. By looking for a 2-color patch with minimum transport distance to a local patch, we derive a nonlinear shock filter, which preserves edges. Moreover, we found that the color difference of the computed 2-color patch indicates whether there is a large change in color, i.e., an edge in the given patch. In summary, we expand the usefulness of optimal transport as an image local analysis tool, to extract robust measures of imaging tumor heterogeneity for outcomes prediction as well as image pre-processing. Because of its robust nature, we find it offers several advantages over the classical approaches.      
### 6.An Overview of Advances in Signal Processing Techniques for Classical and Quantum Wideband Synthetic Apertures  [ :arrow_down: ](https://arxiv.org/pdf/2205.05602.pdf)
>  Rapid developments in synthetic aperture (SA) systems, which generate a larger aperture with greater angular resolution than is inherently possible from the physical dimensions of a single sensor alone, are leading to novel research avenues in several signal processing applications. The SAs may either use a mechanical positioner to move an antenna through space or deploy a distributed network of sensors. With the advent of new hardware technologies, the SAs tend to be denser nowadays. The recent opening of higher frequency bands has led to wide SA bandwidths. In general, new techniques and setups are required to harness the potential of wide SAs in space and bandwidth. Herein, we provide a brief overview of emerging signal processing trends in such spatially and spectrally wideband SA systems. This guide is intended to aid newcomers in navigating the most critical issues in SA analysis and further supports the development of new theories in the field. In particular, we cover the theoretical framework and practical underpinnings of wideband SA radar, channel sounding, sonar, radiometry, and optical applications. Apart from the classical SA applications, we also discuss the quantum electric-field-sensing probes in SAs that are currently undergoing active research but remain at nascent stages of development.      
### 7.End-to-End Multi-Person Audio/Visual Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2205.05586.pdf)
>  Traditionally, audio-visual automatic speech recognition has been studied under the assumption that the speaking face on the visual signal is the face matching the audio. However, in a more realistic setting, when multiple faces are potentially on screen one needs to decide which face to feed to the A/V ASR system. The present work takes the recent progress of A/V ASR one step further and considers the scenario where multiple people are simultaneously on screen (multi-person A/V ASR). We propose a fully differentiable A/V ASR model that is able to handle multiple face tracks in a video. Instead of relying on two separate models for speaker face selection and audio-visual ASR on a single face track, we introduce an attention layer to the ASR encoder that is able to soft-select the appropriate face video track. Experiments carried out on an A/V system trained on over 30k hours of YouTube videos illustrate that the proposed approach can automatically select the proper face tracks with minor WER degradation compared to an oracle selection of the speaking face while still showing benefits of employing the visual signal instead of the audio alone.      
### 8.A Memory-Efficient Dynamic Image Reconstruction Method using Neural Fields  [ :arrow_down: ](https://arxiv.org/pdf/2205.05585.pdf)
>  Dynamic imaging is essential for analyzing various biological systems and behaviors but faces two main challenges: data incompleteness and computational burden. For many imaging systems, high frame rates and short acquisition times require severe undersampling, which leads to data incompleteness. Multiple images may then be compatible with the data, thus requiring special techniques (regularization) to ensure the uniqueness of the reconstruction. Computational and memory requirements are particularly burdensome for three-dimensional dynamic imaging applications requiring high resolution in both space and time. Exploiting redundancies in the object's spatiotemporal features is key to addressing both challenges. This contribution investigates neural fields, or implicit neural representations, to model the sought-after dynamic object. Neural fields are a particular class of neural networks that represent the dynamic object as a continuous function of space and time, thus avoiding the burden of storing a full resolution image at each time frame. Neural field representation thus reduces the image reconstruction problem to estimating the network parameters via a nonlinear optimization problem (training). Once trained, the neural field can be evaluated at arbitrary locations in space and time, allowing for high-resolution rendering of the object. Key advantages of the proposed approach are that neural fields automatically learn and exploit redundancies in the sought-after object to both regularize the reconstruction and significantly reduce memory storage requirements. The feasibility of the proposed framework is illustrated with an application to dynamic image reconstruction from severely undersampled circular Radon transform data.      
### 9.A deep representation learning speech enhancement method using $β$-VAE  [ :arrow_down: ](https://arxiv.org/pdf/2205.05581.pdf)
>  In previous work, we proposed a variational autoencoder-based (VAE) Bayesian permutation training speech enhancement (SE) method (PVAE) which indicated that the SE performance of the traditional deep neural network-based (DNN) method could be improved by deep representation learning (DRL). Based on our previous work, we in this paper propose to use $\beta$-VAE to further improve PVAE's ability of representation learning. More specifically, our $\beta$-VAE can improve PVAE's capacity of disentangling different latent variables from the observed signal without the trade-off problem between disentanglement and signal reconstruction. This trade-off problem widely exists in previous $\beta$-VAE algorithms. Unlike the previous $\beta$-VAE algorithms, the proposed $\beta$-VAE strategy can also be used to optimize the DNN's structure. This means that the proposed method can not only improve PVAE's SE performance but also reduce the number of PVAE training parameters. The experimental results show that the proposed method can acquire better speech and noise latent representation than PVAE. Meanwhile, it also obtains a higher scale-invariant signal-to-distortion ratio, speech quality, and speech intelligibility.      
### 10.Performance of a deep learning system for detection of referable diabetic retinopathy in real clinical settings  [ :arrow_down: ](https://arxiv.org/pdf/2205.05554.pdf)
>  Background: To determine the ability of a commercially available deep learning system, RetCAD v.1.3.1 (Thirona, Nijmegen, The Netherlands) for the automatic detection of referable diabetic retinopathy (DR) on a dataset of colour fundus images acquired during routine clinical practice in a tertiary hospital screening program, analyzing the reduction of workload that can be released incorporating this artificial intelligence-based technology. Methods: Evaluation of the software was performed on a dataset of 7195 nonmydriatic fundus images from 6325 eyes of 3189 diabetic patients attending our screening program between February to December of 2019. The software generated a DR severity score for each colour fundus image which was combined into an eye-level score. This score was then compared with a reference standard as set by a human expert using receiver operating characteristic (ROC) curve analysis. Results: The artificial intelligence (AI) software achieved an area under the ROC curve (AUC) value of 0.988 [0.981:0.993] for the detection of referable DR. At the proposed operating point, the sensitivity of the RetCAD software for DR is 90.53% and specificity is 97.13%. A workload reduction of 96% could be achieved at the cost of only 6 false negatives. Conclusions: The AI software correctly identified the vast majority of referable DR cases, with a workload reduction of 96% of the cases that would need to be checked, while missing almost no true cases, so it may therefore be used as an instrument for triage.      
### 11.CNN-LSTM Based Multimodal MRI and Clinical Data Fusion for Predicting Functional Outcome in Stroke Patients  [ :arrow_down: ](https://arxiv.org/pdf/2205.05545.pdf)
>  Clinical outcome prediction plays an important role in stroke patient management. From a machine learning point-of-view, one of the main challenges is dealing with heterogeneous data at patient admission, i.e. the image data which are multidimensional and the clinical data which are scalars. In this paper, a multimodal convolutional neural network - long short-term memory (CNN-LSTM) based ensemble model is proposed. For each MR image module, a dedicated network provides preliminary prediction of the clinical outcome using the modified Rankin scale (mRS). The final mRS score is obtained by merging the preliminary probabilities of each module dedicated to a specific type of MR image weighted by the clinical metadata, here age or the National Institutes of Health Stroke Scale (NIHSS). The experimental results demonstrate that the proposed model surpasses the baselines and offers an original way to automatically encode the spatio-temporal context of MR images in a deep learning architecture. The highest AUC (0.77) was achieved for the proposed model with NIHSS.      
### 12.Beyond Griffin-Lim: Improved Iterative Phase Retrieval for Speech  [ :arrow_down: ](https://arxiv.org/pdf/2205.05496.pdf)
>  Phase retrieval is a problem encountered not only in speech and audio processing, but in many other fields such as optics. Iterative algorithms based on non-convex set projections are effective and frequently used for retrieving the phase when only STFT magnitudes are available. While the basic Griffin-Lim algorithm and its variants have been the prevalent method for decades, more recent advances, e.g. in optics, raise the question: Can we do better than Griffin-Lim for speech signals, using the same principle of iterative projection? <br>In this paper we compare the classical algorithms in the speech domain with two modern methods from optics with respect to reconstruction quality and convergence rate. Based on this study, we propose to combine Griffin-Lim with the Difference Map algorithm in a hybrid approach which shows superior results, in terms of both convergence and quality of the final reconstruction.      
### 13.Design and manufacturing of an optimized retro reflective marker for photogrammetric pose estimation in ITER  [ :arrow_down: ](https://arxiv.org/pdf/2205.05486.pdf)
>  Retro reflective markers can remarkably aid photogrammetry tasks in challenging visual environments. They have been demonstrated to be key enablers of pose estimation for remote handling in ITER. However, the strict requirements of the ITER environment have previously markedly constrained the design of such elements and limited their performance. In this work, we identify several retro reflector designs based on the cat's eye principle that are applicable to the ITER usecase and propose a methodology for optimizing their performance. We circumvent some of the environmental constraints by changing the curvature radius and distance to the reflective surface. We model, manufacture and test a marker that fulfils all the application requirements while achieving a gain of around 100\% in performance over the previous solution in the targeted working range.      
### 14.DeepFilterNet2: Towards Real-Time Speech Enhancement on Embedded Devices for Full-Band Audio  [ :arrow_down: ](https://arxiv.org/pdf/2205.05474.pdf)
>  Deep learning-based speech enhancement has seen huge improvements and recently also expanded to full band audio (48 kHz). However, many approaches have a rather high computational complexity and require big temporal buffers for real time usage e.g. due to temporal convolutions or attention. Both make those approaches not feasible on embedded devices. This work further extends DeepFilterNet, which exploits harmonic structure of speech allowing for efficient speech enhancement (SE). Several optimizations in the training procedure, data augmentation, and network structure result in state-of-the-art SE performance while reducing the real-time factor to 0.04 on a notebook Core-i5 CPU. This makes the algorithm applicable to run on embedded devices in real-time. The DeepFilterNet framework can be obtained under an open source license.      
### 15.Finite-Time Analysis of Constant Step-Size Q-Learning : Switching System Approach Revisited  [ :arrow_down: ](https://arxiv.org/pdf/2205.05455.pdf)
>  This technical note revisits the novel switching system framework in [1] for analyzing the finite-time convergence of Q-learning, where the dynamics of asynchronous Q-learning with a constant step-size is formulated as a discrete-time stochastic switching system model, and a bound on the average iteration is established based on Lyapunov functions. We improve the analysis in the previous paper by replacing the average iteration with the final iteration, which is simpler and more common in the literature. The proposed analysis relies on propagations of the autocorrelation matrix of the state instead of the Lyapunov function analysis. Moreover, we provide comparative analysis of the proposed method and existing approaches, and prove that the proposed approach improves the previous sample complexities in terms of the effective horizon. Besides, the proposed analysis offers additional insights on analysis of Q-learning and reinforcement learning, and complements existing approaches.      
### 16.Experimental Validation of Sequence-Wise Predistorter for Evaluation of Geometrically Shaped 128-QAM  [ :arrow_down: ](https://arxiv.org/pdf/2205.05440.pdf)
>  A predistorter for transmitter nonlinearities is applied to the evaluation of a geometrically shaped constellation, such that constellation points are transmitted correctly during the evaluation of the geometrically shaped constellation.      
### 17.Data-Driven Optimal Sensor Placement for High-Dimensional System Using Annealing Machine  [ :arrow_down: ](https://arxiv.org/pdf/2205.05430.pdf)
>  We propose a novel method for solving optimal sensor placement problem for high-dimensional system using an annealing machine. The sensor points are calculated as a maximum clique problem of the graph, the edge weight of which is determined by the proper orthogonal decomposition (POD) mode obtained from data based on the fact that a high-dimensional system usually has a low-dimensional representation. Since the maximum clique problem is equivalent to the independent set problem of the complement graph, the independent set problem is solved using Fujitsu Digital Annealer. As a demonstration of the proposed method, the pressure distribution induced by the Kármán vortex street behind a square cylinder is reconstructed based on the pressure data at the calculated sensor points. The pressure distribution is measured by pressure-sensitive paint (PSP) technique, which is an optical flow diagnose method. The root mean square errors (RMSEs) between the pressure measured by pressure transducer and the reconstructed pressures (calculated from the proposed method and an existing greedy method) at the same place are compared. As the result, the similar RMSE is achieved by the proposed method using approximately 1/5 number of sensor points obtained by the existing method. This method is of great importance as a novel approach for optimal sensor placement problem and a new engineering application of an annealing machine.      
### 18.Learning a Better Control Barrier Function  [ :arrow_down: ](https://arxiv.org/pdf/2205.05429.pdf)
>  Control barrier functions (CBF) are widely used in safety-critical controllers. However, the construction of valid CBFs is well known to be challenging, especially for nonlinear or non-convex constraints and high relative degree systems. On the other hand, finding a conservative CBF that only recovers a portion of the true safe set is usually possible. In this work, starting from a "conservative" handcrafted control barrier function (HCBF), we develop a method to find a control barrier function that recovers a reasonably larger portion of the safe set. Using a different approach, by incorporating the hard constraints into an optimal control problem, e.g., MPC, we can safely generate solutions within the true safe set. Nevertheless, such an approach is usually computationally expensive and may not lend itself to real-time implementations. We propose to combine the two methods. During training, we utilize MPC to collect safe trajectory data. Thereafter, we train a neural network to estimate the difference between the HCBF and the CBF that recovers a closer solution to the true safe set. Using the proposed approach, we can generate a safe controller that is less conservative and computationally efficient. We validate our approach on three systems: a second-order integrator, ball-on-beam, and unicycle.      
### 19.Slab Track Condition Monitoring Based on Learned Sparse Features from Acoustic and Acceleration Signals  [ :arrow_down: ](https://arxiv.org/pdf/2205.05365.pdf)
>  The implementation of concrete slab track solutions has been recently increasing particularly for high-speed lines. While it is typically associated with low periodic maintenance, there is a significant need to detect the state of slab tracks in an efficient way. Data-driven detection methods are promising. However, collecting large amounts of labeled data is particularly challenging since abnormal states are rare for such safety-critical infrastructure. To imitate different healthy and unhealthy states of slab tracks, this study uses three types of slab track supporting conditions in a railway test line. Acceleration sensors (contact) and acoustic sensors (contactless), are installed next to the three types of slab track to collect the acceleration and acoustic signals as a train passes by with different speeds. We use a deep learning framework based on the recently proposed Denoising Sparse Wavelet Network (DeSpaWN) to automatically learn meaningful and sparse representations of raw high-frequency signals. A comparative study is conducted among the feature learning / extraction methods, and between acceleration signals and acoustic signals, by evaluating the detection effectiveness using a multi-class support vector machine. It is found that the classification accuracy using acceleration signals can reach almost 100%, irrespective which feature learning / extraction method is adopted. Due to the more severe noise interference in acoustic signals, the performance of using acoustic signals is worse than of using acceleration signals. However, it can be significantly improved by leaning meaningful features with DeSpaWN.      
### 20.Variational Autoencoder Leveraged MMSE Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2205.05345.pdf)
>  We propose to utilize a variational autoencoder (VAE) for data-driven channel estimation. The underlying true and unknown channel distribution is modeled by the VAE as a conditional Gaussian distribution in a novel way, parameterized by the respective first and second order conditional moments. As a result, it can be observed that the linear minimum mean square error (LMMSE) estimator in its variant conditioned on the latent sample of the VAE approximates an optimal MSE estimator. Furthermore, we argue how a VAE-based channel estimator can approximate the MMSE channel estimator. We propose three variants of VAE estimators that differ in the data used during training and estimation. First, we show that given perfectly known channel state information at the input of the VAE during estimation, which is impractical, we obtain an estimator that can serve as a benchmark result for an estimation scenario. We then propose practically feasible approaches, where perfectly known channel state information is only necessary in the training phase or is not needed at all. Simulation results on 3GPP and QuaDRiGa channel data attest a small performance loss of the practical approaches and the superiority of our VAE approaches in comparison to other related channel estimation methods.      
### 21.Empirical Fading Model and Bayesian Calibration for Multipath-Enhanced Device-Free Localization  [ :arrow_down: ](https://arxiv.org/pdf/2205.05331.pdf)
>  The performance of multipath-enhanced device-free localization severely depends on the information about the propagation paths within the network. While known for the line-of-sight, the propagation paths have yet to be determined for multipath components. This work provides a novel Bayesian calibration approach for determining the propagation paths by estimating reflection points. Therefore, first a statistical fading model is presented, that describes user-induced changes in the received signal of multipath components. The model is derived and validated empirically using an extensive set of wideband and ultra-wideband measurement data. Second, the Bayesian approach is presented, which, based on the derived empirical fading model, relates measured changes in the power of a multipath component to the location of the reflection point. Exploiting the geometric properties of multipath components caused by single-bounce reflections, the solution space of possible locations of reflection points is constrained to the delay ellipse. Thus, a one-dimensional elliptic estimation problem can be formulated, which is solved using a point mass filter. The applicability of the proposed approach is demonstrated and evaluated based on measurement data. Independent of the underlying measurement system, the Bayesian calibration approach is shown to robustly estimate the locations of the reflection points in different environments.      
### 22.A Unified Theory of Adaptive Subspace Detection. Part II: Numerical Examples  [ :arrow_down: ](https://arxiv.org/pdf/2205.05305.pdf)
>  This paper is devoted to the performance analysis of the detectors proposed in the companion paper where a comprehensive design framework is presented for the adaptive detection of subspace signals. The framework addresses four variations on subspace detection: the subspace may be known or known only by its dimension; consecutive visits to the subspace may be unconstrained or they may be constrained by a prior probability distribution. In this paper, Monte Carlo simulations are used to compare the generalized likelihood ratio (GLR) detectors derived in [1] with estimate-and-plug (EP) approximations of the GLR detectors. Remarkably, the EP approximations appear here for the first time (at least to the best of the authors' knowledge). The numerical examples indicate that GLR detectors are effective for the detection of partially-known signals affected by inherent uncertainties due to the system or the operating environment. In particular, if the signal subspace is known, GLR detectors tend to outperform EP detectors. If, instead, the signal subspace is known only by its dimension, the performance of GLR and EP detectors is very similar.      
### 23.Strong Sign Controllability of Diffusively-Coupled Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.05275.pdf)
>  This paper presents several conditions to determine strong sign controllability for diffusively-coupled undirected networks. The strong sign controllability is determined by the sign patterns (positive, negative, zero) of the edges. We first provide the necessary and sufficient conditions for strong sign controllability of basic components, such as path, cycle, and tree. Next, we propose a merging process to extend the basic componenets to a larger graph based on the conditions of the strong sign controllability. Furthermore, we develop an algorithm of polynomial complexity to find the minimum number of external input nodes while maintaining the strong sign controllability of a network.      
### 24.Intelligent Reflecting Surface Configurations for Smart Radio Using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.05269.pdf)
>  Intelligent reflecting surface (IRS) is envisioned to change the paradigm of wireless communications from "adapting to wireless channels" to "changing wireless channels". However, current IRS configuration schemes, consisting of sub-channel estimation and passive beamforming in sequence, conform to the conventional model-based design philosophies and are difficult to be realized practically in the complex radio environment. To create the smart radio environment, we propose a model-free design of IRS control that is independent of the sub-channel channel state information (CSI) and requires the minimum interaction between IRS and the wireless communication system. We firstly model the control of IRS as a Markov decision process (MDP) and apply deep reinforcement learning (DRL) to perform real-time coarse phase control of IRS. Then, we apply extremum seeking control (ESC) as the fine phase control of IRS. Finally, by updating the frame structure, we integrate DRL and ESC in the model-free control of IRS to improve its adaptivity to different channel dynamics. Numerical results show the superiority of our proposed joint DRL and ESC scheme and verify its effectiveness in model-free IRS control without sub-channel CSI.      
### 25.Towards Improved Zero-shot Voice Conversion with Conditional DSVAE  [ :arrow_down: ](https://arxiv.org/pdf/2205.05227.pdf)
>  Disentangling content and speaking style information is essential for zero-shot non-parallel voice conversion (VC). Our previous study investigated a novel framework with disentangled sequential variational autoencoder (DSVAE) as the backbone for information decomposition. We have demonstrated that simultaneous disentangling content embedding and speaker embedding from one utterance is feasible for zero-shot VC. In this study, we continue the direction by raising one concern about the prior distribution of content branch in the DSVAE baseline. We find the random initialized prior distribution will force the content embedding to reduce the phonetic-structure information during the learning process, which is not a desired property. Here, we seek to achieve a better content embedding with more phonetic information preserved. We propose conditional DSVAE, a new model that enables content bias as a condition to the prior modeling and reshapes the content embedding sampled from the posterior distribution. In our experiment on the VCTK dataset, we demonstrate that content embeddings derived from the conditional DSVAE overcome the randomness and achieve a much better phoneme classification accuracy, a stabilized vocalization and a better zero-shot VC performance compared with the competitive DSVAE baseline.      
### 26.Best of Both Worlds: Multi-task Audio-Visual Automatic Speech Recognition and Active Speaker Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.05206.pdf)
>  Under noisy conditions, automatic speech recognition (ASR) can greatly benefit from the addition of visual signals coming from a video of the speaker's face. However, when multiple candidate speakers are visible this traditionally requires solving a separate problem, namely active speaker detection (ASD), which entails selecting at each moment in time which of the visible faces corresponds to the audio. Recent work has shown that we can solve both problems simultaneously by employing an attention mechanism over the competing video tracks of the speakers' faces, at the cost of sacrificing some accuracy on active speaker detection. This work closes this gap in active speaker detection accuracy by presenting a single model that can be jointly trained with a multi-task loss. By combining the two tasks during training we reduce the ASD classification accuracy by approximately 25%, while simultaneously improving the ASR performance when compared to the multi-person baseline trained exclusively for ASR.      
### 27.Separator-Transducer-Segmenter: Streaming Recognition and Segmentation of Multi-party Speech  [ :arrow_down: ](https://arxiv.org/pdf/2205.05199.pdf)
>  Streaming recognition and segmentation of multi-party conversations with overlapping speech is crucial for the next generation of voice assistant applications. In this work we address its challenges discovered in the previous work on multi-turn recurrent neural network transducer (MT-RNN-T) with a novel approach, separator-transducer-segmenter (STS), that enables tighter integration of speech separation, recognition and segmentation in a single model. First, we propose a new segmentation modeling strategy through start-of-turn and end-of-turn tokens that improves segmentation without recognition accuracy degradation. Second, we further improve both speech recognition and segmentation accuracy through an emission regularization method, FastEmit, and multi-task training with speech activity information as an additional training signal. Third, we experiment with end-of-turn emission latency penalty to improve end-point detection for each speaker turn. Finally, we establish a novel framework for segmentation analysis of multi-party conversations through emission latency metrics. With our best model, we report 4.6% abs. turn counting accuracy improvement and 17% rel. word error rate (WER) improvement on LibriCSS dataset compared to the previously published work.      
### 28.Massively Digitized Power Grid: Opportunities and Challenges of Use-inspired AI  [ :arrow_down: ](https://arxiv.org/pdf/2205.05180.pdf)
>  This article presents a use-inspired perspective of the opportunities and challenges in a massively digitized power grid. It argues that the intricate interplay of data availability, computing capability, and artificial intelligence (AI) algorithm development are the three key factors driving the adoption of digitized solutions in the power grid. The impact of these three factors on critical functions of power system operation and planning practices are reviewed and illustrated with industrial practice case studies. Open challenges and research opportunities for data, computing, and AI algorithms are articulated within the context of the power industry's tremendous decarbonization efforts.      
### 29.Frequency-domain digital predistortion for Massive MU-MIMO-OFDM Downlink  [ :arrow_down: ](https://arxiv.org/pdf/2205.05158.pdf)
>  Digital predistortion (DPD) is a method commonly used to compensate for the nonlinear effects of power amplifiers (PAs). However, the computational complexity of most DPD algorithms becomes an issue in the downlink of massive multi-user (MU) multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM), where potentially up to several hundreds of PAs in the base station (BS) require linearization. In this paper, we propose a convolutional neural network (CNN)-based DPD in the frequency domain, taking place before the precoding, where the dimensionality of the signal space depends on the number of users, instead of the number of BS antennas. Simulation results on generalized memory polynomial (GMP)-based PAs show that the proposed CNN-based DPD can lead to very large complexity savings as the number of BS antenna increases at the expense of a small increase in power to achieve the same symbol error rate (SER).      
### 30.Sparsity Based Non-Contact Vital Signs Monitoring of Multiple People Via FMCW Radar  [ :arrow_down: ](https://arxiv.org/pdf/2205.05152.pdf)
>  Non-contact technology for monitoring multiple people's vital signs, such as respiration and heartbeat, has been investigated in recent years due to the rising cardiopulmonary morbidity, the risk of transmitting diseases, and the heavy burden on the medical staff. Frequency modulated continuous wave (FMCW) radars have shown great promise in meeting these needs. However, contemporary techniques for non-contact vital signs monitoring (NCVSM) via FMCW radars, are based on simplistic models, and present difficulties coping with noisy environments containing multiple objects. In this work, we develop an extended model of FMCW radar signals in a noisy setting containing multiple people and clutter. By utilizing the sparse nature of the modeled signals in conjunction with human-typical cardiopulmonary features, we can accurately localize humans and reliably monitor their vital signs, using only a single channel and a single-input-single-output setup. To this end, we first show that spatial sparsity allows for both accurate detection of multiple people and computationally efficient extraction of their Doppler samples, using a joint sparse recovery approach. Given the extracted samples, we develop a method named Vital Signs based Dictionary Recovery (VSDR), which uses a dictionary-based approach to search for the desired rates of respiration and heartbeat over high-resolution grids corresponding to normal cardiopulmonary activity. The advantages of the proposed method are illustrated through examples that combine the proposed model with real data of $30$ monitored individuals. We demonstrate accurate human localization in a clutter-rich scenario that includes both static and vibrating objects, and show that our VSDR approach outperforms existing techniques, based on several statistical metrics. The findings support the widespread use of FMCW radars with the proposed algorithms in healthcare.      
### 31.Limited-memory BFGS Optimisation of Phase-Only Computer-Generated Hologram for Fraunhofer Diffraction  [ :arrow_down: ](https://arxiv.org/pdf/2205.05144.pdf)
>  We implement a novel limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) optimisation algorithm with cross entropy (CE) loss function, to produce phase-only computer-generated hologram (CGH) for holographic displays, with validation on a binary-phase modulation holographic projector.      
### 32.Deep fusion of gray level co-occurrence matrices for lung nodule classification  [ :arrow_down: ](https://arxiv.org/pdf/2205.05123.pdf)
>  Lung cancer is a severe menace to human health, due to which millions of people die because of late diagnoses of cancer; thus, it is vital to detect the disease as early as possible. The Computerized chest analysis Tomography of scan is assumed to be one of the efficient solutions for detecting and classifying lung nodules. The necessity of high accuracy of analyzing C.T. scan images of the lung is considered as one of the crucial challenges in detecting and classifying lung cancer. A new long-short-term-memory (LSTM) based deep fusion structure, is introduced, where, the texture features computed from lung nodules through new volumetric grey-level-co-occurrence-matrices (GLCM) computations are applied to classify the nodules into: benign, malignant and ambiguous. An improved Otsu segmentation method combined with the water strider optimization algorithm (WSA) is proposed to detect the lung nodules. Otsu-WSA thresholding can overcome the restrictions present in previous thresholding methods. Extended experiments are run to assess this fusion structure by considering 2D-GLCM computations based 2D-slices fusion, and an approximation of this 3D-GLCM with volumetric 2.5D-GLCM computations-based LSTM fusion structure. The proposed methods are trained and assessed through the LIDC-IDRI dataset, where 94.4%, 91.6%, and 95.8% Accuracy, sensitivity, and specificity are obtained, respectively for 2D-GLCM fusion and 97.33%, 96%, and 98%, accuracy, sensitivity, and specificity, respectively, for 2.5D-GLCM fusion. The yield of the same are 98.7%, 98%, and 99%, for the 3D-GLCM fusion. The obtained results and analysis indicate that the WSA-Otsu method requires less execution time and yields a more accurate thresholding process. It is found that 3D-GLCM based LSTM outperforms its counterparts.      
### 33.Robust Data-Driven Output Feedback Control via Bootstrapped Multiplicative Noise  [ :arrow_down: ](https://arxiv.org/pdf/2205.05119.pdf)
>  We propose a robust data-driven output feedback control algorithm that explicitly incorporates inherent finite-sample model estimate uncertainties into the control design. The algorithm has three components: (1) a subspace identification nominal model estimator; (2) a bootstrap resampling method that quantifies non-asymptotic variance of the nominal model estimate; and (3) a non-conventional robust control design method comprising a coupled optimal dynamic output feedback filter and controller with multiplicative noise. A key advantage of the proposed approach is that the system identification and robust control design procedures both use stochastic uncertainty representations, so that the actual inherent statistical estimation uncertainty directly aligns with the uncertainty the robust controller is being designed against. Moreover, the control design method accommodates a highly structured uncertainty representation that can capture uncertainty shape more effectively than existing approaches. We show through numerical experiments that the proposed robust data-driven output feedback controller can significantly outperform a certainty equivalent controller on various measures of sample complexity and stability robustness.      
### 34.Vibration-Based Bridge Health Monitoring using Telecommunication Cables  [ :arrow_down: ](https://arxiv.org/pdf/2205.05114.pdf)
>  Bridge Health Monitoring (BHM) enables early damage detection of bridges and is thus critical for avoiding more severe damages that might result in major financial and human losses. However, conventional BHM systems require dedicated sensors on bridges, which is costly to install and maintain and hard to scale up. To overcome this challenge, we introduce a new system that uses existing telecommunication cables for Distributed Acoustic Sensing (DAS) to collect bridge dynamic strain responses. In addition, we develop a two-module physics-guided system identification method to extract bridge damage-sensitive information (e.g., natural frequencies and mode shapes) from noisy DAS data by constraining strain and displacement mode shapes by bridge dynamics. This approach does not require installation and maintenance of dedicated sensors on bridges. We evaluate our system with field experiments on a concrete bridge with fiber cable running in a conduit under the deck. Our system successfully identified modal frequencies and reconstructed meter-scale mode shapes.      
### 35.NTIRE 2022 Challenge on Efficient Super-Resolution: Methods and Results  [ :arrow_down: ](https://arxiv.org/pdf/2205.05675.pdf)
>  This paper reviews the NTIRE 2022 challenge on efficient single image super-resolution with focus on the proposed solutions and results. The task of the challenge was to super-resolve an input image with a magnification factor of $\times$4 based on pairs of low and corresponding high resolution images. The aim was to design a network for single image super-resolution that achieved improvement of efficiency measured according to several metrics including runtime, parameters, FLOPs, activations, and memory consumption while at least maintaining the PSNR of 29.00dB on DIV2K validation set. IMDN is set as the baseline for efficiency measurement. The challenge had 3 tracks including the main track (runtime), sub-track one (model complexity), and sub-track two (overall performance). In the main track, the practical runtime performance of the submissions was evaluated. The rank of the teams were determined directly by the absolute value of the average runtime on the validation set and test set. In sub-track one, the number of parameters and FLOPs were considered. And the individual rankings of the two metrics were summed up to determine a final ranking in this track. In sub-track two, all of the five metrics mentioned in the description of the challenge including runtime, parameter count, FLOPs, activations, and memory consumption were considered. Similar to sub-track one, the rankings of five metrics were summed up to determine a final ranking. The challenge had 303 registered participants, and 43 teams made valid submissions. They gauge the state-of-the-art in efficient single image super-resolution.      
### 36.RepSR: Training Efficient VGG-style Super-Resolution Networks with Structural Re-Parameterization and Batch Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2205.05671.pdf)
>  This paper explores training efficient VGG-style super-resolution (SR) networks with the structural re-parameterization technique. The general pipeline of re-parameterization is to train networks with multi-branch topology first, and then merge them into standard 3x3 convolutions for efficient inference. In this work, we revisit those primary designs and investigate essential components for re-parameterizing SR networks. First of all, we find that batch normalization (BN) is important to bring training non-linearity and improve the final performance. However, BN is typically ignored in SR, as it usually degrades the performance and introduces unpleasant artifacts. We carefully analyze the cause of BN issue and then propose a straightforward yet effective solution. In particular, we first train SR networks with mini-batch statistics as usual, and then switch to using population statistics at the later training period. While we have successfully re-introduced BN into SR, we further design a new re-parameterizable block tailored for SR, namely RepSR. It consists of a clean residual path and two expand-and-squeeze convolution paths with the modified BN. Extensive experiments demonstrate that our simple RepSR is capable of achieving superior performance to previous SR re-parameterization methods among different model sizes. In addition, our RepSR can achieve a better trade-off between performance and actual running time (throughput) than previous SR methods. Codes will be available at <a class="link-external link-https" href="https://github.com/TencentARC/RepSR" rel="external noopener nofollow">this https URL</a>.      
### 37.High-Speed Imaging Receiver Design for 6G Optical Wireless Communications: A Rate-FOV Trade-Off  [ :arrow_down: ](https://arxiv.org/pdf/2205.05626.pdf)
>  The design of a compact high-speed and wide field of view (FOV) receiver is challenging due to the presence of two well-known trade-offs. The first one is the area-bandwidth trade-off of photodetectors (PDs) and the second one is the gain-FOV trade-off due to the use of optics. The combined effects of these two trade-offs imply that the achievable data rate of an imaging optical receiver is limited by its FOV, i.e., a rate-FOV trade-off. To control the area-bandwidth trade-off, an array of small PDs can be used instead of a single PD. Moreover, in practice, a large-area lens is required to ensure sufficient power collection, which in turn limits the receiver FOV (i.e., gain-FOV trade-off). We propose an imaging receiver design in the form of an array of arrays. To achieve a reasonable receiver FOV, we use individual focusing lens for each PD array rather than a single collection lens for the whole receiver. The proposed array of arrays structure provides an effective method to control both gain-FOV trade-off (via an array of lenses) and area-bandwidth trade-off (via arrays of PDs). We first derive a tractable analytical model for the SNR of an array of PDs where the maximum ratio combining has been employed. Then, we extend the model for the proposed array of arrays structure and the accuracy of the analytical model is verified based on several Optic Studio-based simulations. Next, we formulate an optimization problem to maximize the achievable data rate of the imaging receiver subject to a minimum required FOV. The optimization problem is solved for two commonly used modulation techniques, namely, OOK and direct current biased optical orthogonal frequency division multiplexing with variable rate quadrature amplitude modulation. It is demonstrated that a data rate of ~ 24 Gbps with a FOV of 15 is achievable using OOK with a total receiver size of 2 cm by 2 cm.      
### 38.The Fluctuating Two-Ray Fading Model with Independent Specular Components  [ :arrow_down: ](https://arxiv.org/pdf/2205.05604.pdf)
>  We introduce and characterize the independent fluctuating two-ray (IFTR) fading model, a class of fading models consisting of two specular components which fluctuate independently, plus a diffuse component modeled as a complex Gaussian random variable. The IFTR model complements the popular fluctuating two-ray (FTR) model, on which the specular components are fully correlated and fluctuate jointly. The chief probability functions of the received SNR in IFTR fading, including the PDF, CDF and MGF, are expressed in closed-form, having a functional form similar to other state-of-the-art fading models. Then, the IFTR model is empirically validated using multiple channels measured in rather diverse scenarios, including line of sight (LOS) millimeter-wave, land mobile satellite (LMS) and underwater acoustic communication (UAC), showing a better fit than the original FTR model and other models previously used in these environments. Additionally, the performance of wireless communication systems operating under IFTR fading is evaluated in closed-form in two scenarios: (i) exact and asymptotic bit error rate for a family of coherent modulations; and (ii) exact and asymptotic outage probability.      
### 39.Studying Scientific Data Lifecycle in On-demand Distributed Storage Caches  [ :arrow_down: ](https://arxiv.org/pdf/2205.05598.pdf)
>  The XRootD system is used to transfer, store, and cache large datasets from high-energy physics (HEP). In this study we focus on its capability as distributed on-demand storage cache. Through exploring a large set of daily log files between 2020 and 2021, we seek to understand the data access patterns that might inform future cache design. Our study begins with a set of summary statistics regarding file read operations, file lifetimes, and file transfers. We observe that the number of read operations on each file remains nearly constant, while the average size of a read operation grows over time. Furthermore, files tend to have a consistent length of time during which they remain open and are in use. Based on this comprehensive study of the cache access statistics, we developed a cache simulator to explore the behavior of caches of different sizes. Within a certain size range, we find that increasing the XRootD cache size improves the cache hit rate, yielding faster overall file access. In particular, we find that increase the cache size from 40TB to 56TB could increase the hit rate from 0.62 to 0.89, which is a significant increase in cache effectiveness for modest cost.      
### 40.A neural prosody encoder for end-ro-end dialogue act classification  [ :arrow_down: ](https://arxiv.org/pdf/2205.05590.pdf)
>  Dialogue act classification (DAC) is a critical task for spoken language understanding in dialogue systems. Prosodic features such as energy and pitch have been shown to be useful for DAC. Despite their importance, little research has explored neural approaches to integrate prosodic features into end-to-end (E2E) DAC models which infer dialogue acts directly from audio signals. In this work, we propose an E2E neural architecture that takes into account the need for characterizing prosodic phenomena co-occurring at different levels inside an utterance. A novel part of this architecture is a learnable gating mechanism that assesses the importance of prosodic features and selectively retains core information necessary for E2E DAC. Our proposed model improves DAC accuracy by 1.07% absolute across three publicly available benchmark datasets.      
### 41.Choice of training label matters: how to best use deep learning for quantitative MRI parameter estimation  [ :arrow_down: ](https://arxiv.org/pdf/2205.05587.pdf)
>  Deep learning (DL) is gaining popularity as a parameter estimation method for quantitative MRI. A range of competing implementations have been proposed, relying on either supervised or self-supervised learning. Self-supervised approaches, sometimes referred to as unsupervised, have been loosely based on auto-encoders, whereas supervised methods have, to date, been trained on groundtruth labels. These two learning paradigms have been shown to have distinct strengths. Notably, self-supervised approaches have offered lower-bias parameter estimates than their supervised alternatives. This result is counterintuitive - incorporating prior knowledge with supervised labels should, in theory, lead to improved accuracy. In this work, we show that this apparent limitation of supervised approaches stems from the naive choice of groundtruth training labels. By training on labels which are deliberately not groundtruth, we show that the low-bias parameter estimation previously associated with self-supervised methods can be replicated - and improved on - within a supervised learning framework. This approach sets the stage for a single, unifying, deep learning parameter estimation framework, based on supervised learning, where trade-offs between bias and variance are made by careful adjustment of training label.      
### 42.Scream Detection in Heavy Metal Music  [ :arrow_down: ](https://arxiv.org/pdf/2205.05580.pdf)
>  Harsh vocal effects such as screams or growls are far more common in heavy metal vocals than the traditionally sung vocal. This paper explores the problem of detection and classification of extreme vocal techniques in heavy metal music, specifically the identification of different scream techniques. We investigate the suitability of various feature representations, including cepstral, spectral, and temporal features as input representations for classification. The main contributions of this work are (i) a manually annotated dataset comprised of over 280 minutes of heavy metal songs of various genres with a statistical analysis of occurrences of different extreme vocal techniques in heavy metal music, and (ii) a systematic study of different input feature representations for the classification of heavy metal vocals      
### 43.Channel Estimation in RIS-assisted Downlink Massive MIMO: A Learning-Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2205.05577.pdf)
>  For downlink massive multiple-input multiple-output (MIMO) operating in time-division duplex protocol, users can decode the signals effectively by only utilizing the channel statistics as long as channel hardening holds. However, in a reconfigurable intelligent surface (RIS)-assisted massive MIMO system, the propagation channels may be less hardened due to the extra random fluctuations of the effective channel gains. To address this issue, we propose a learning-based method that trains a neural network to learn a mapping between the received downlink signal and the effective channel gains. The proposed method does not require any downlink pilots and statistical information of interfering users. Numerical results show that, in terms of mean-square error of the channel estimation, our proposed learning-based method outperforms the state-of-the-art methods, especially when the light-of-sight (LoS) paths are dominated by non-LoS paths with a low level of channel hardening, e.g., in the cases of small numbers of RIS elements and/or base station antennas.      
### 44.Review on Panoramic Imaging and Its Applications in Scene Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2205.05570.pdf)
>  With the rapid development of high-speed communication and artificial intelligence technologies, human perception of real-world scenes is no longer limited to the use of small Field of View (FoV) and low-dimensional scene detection devices. Panoramic imaging emerges as the next generation of innovative intelligent instruments for environmental perception and measurement. However, while satisfying the need for large-FoV photographic imaging, panoramic imaging instruments are expected to have high resolution, no blind area, miniaturization, and multi-dimensional intelligent perception, and can be combined with artificial intelligence methods towards the next generation of intelligent instruments, enabling deeper understanding and more holistic perception of 360-degree real-world surrounding environments. Fortunately, recent advances in freeform surfaces, thin-plate optics, and metasurfaces provide innovative approaches to address human perception of the environment, offering promising ideas beyond conventional optical imaging. In this review, we begin with introducing the basic principles of panoramic imaging systems, and then describe the architectures, features, and functions of various panoramic imaging systems. Afterwards, we discuss in detail the broad application prospects and great design potential of freeform surfaces, thin-plate optics, and metasurfaces in panoramic imaging. We then provide a detailed analysis on how these techniques can help enhance the performance of panoramic imaging systems. We further offer a detailed analysis of applications of panoramic imaging in scene understanding for autonomous driving and robotics, spanning panoramic semantic image segmentation, panoramic depth estimation, panoramic visual localization, and so on. Finally, we cast a perspective on future potential and research directions for panoramic imaging instruments.      
### 45.Design, Modeling and Control for a Tilt-rotor VTOL UAV in the Presence of Actuator Failure  [ :arrow_down: ](https://arxiv.org/pdf/2205.05533.pdf)
>  Providing both the vertical take-off and landing capabilities and the ability to fly long distances to aircraft opens the door to a wide range of new real-world aircraft applications while improving many existing applications. Tiltrotor vertical take-off and landing (VTOL) unmanned aerial vehicles (UAVs) are a better choice than fixed-wing and multirotor aircraft for such applications. Prior work on these aircraft has addressed the aerodynamic performance, design, modeling, and control. However, a less explored area is the study of their potential fault tolerance due to their inherent redundancy, which allows them to sustain some degree of actuator failure. This work introduces tolerance to several types of actuator failures in a tiltrotor VTOL aircraft. We discuss the design and model of a custom tiltrotor VTOL UAV, which is a combination of a fixed-wing aircraft and a quadrotor with tilting rotors, where the four propellers can be rotated individually. Then, we analyze the feasible wrench space the vehicle can generate and design the dynamic control allocation so that the system can adapt to actuator failure, benefiting from the configuration redundancy. The proposed approach is lightweight and is implemented as an extension to an already existing flight control stack. Extensive experiments are performed to validate that the system can maintain the controlled flight under different actuator failures. To the best of our knowledge, this work is the first study of the tiltrotor VTOL's fault-tolerance that exploits the configuration redundancy.      
### 46.Automatic Tuberculosis and COVID-19 cough classification using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.05480.pdf)
>  We present a deep learning based automatic cough classifier which can discriminate tuberculosis (TB) coughs from COVID-19 coughs and healthy coughs. Both TB and COVID-19 are respiratory disease, have cough as a predominant symptom and claim thousands of lives each year. The cough audio recordings were collected at both indoor and outdoor settings and also uploaded using smartphones from subjects around the globe, thus contain various levels of noise. This cough data include 1.68 hours of TB coughs, 18.54 minutes of COVID-19 coughs and 1.69 hours of healthy coughs from 47 TB patients, 229 COVID-19 patients and 1498 healthy patients and were used to train and evaluate a CNN, LSTM and Resnet50. These three deep architectures were also pre-trained on 2.14 hours of sneeze, 2.91 hours of speech and 2.79 hours of noise for improved performance. The class-imbalance in our dataset was addressed by using SMOTE data balancing technique and using performance metrics such as F1-score and AUC. Our study shows that the highest F1-scores of 0.9259 and 0.8631 have been achieved from a pre-trained Resnet50 for two-class (TB vs COVID-19) and three-class (TB vs COVID-19 vs healthy) cough classification tasks, respectively. The application of deep transfer learning has improved the classifiers' performance and makes them more robust as they generalise better over the cross-validation folds. Their performances exceed the TB triage test requirements set by the world health organisation (WHO). The features producing the best performance contain higher order of MFCCs suggesting that the differences between TB and COVID-19 coughs are not perceivable by the human ear. This type of cough audio classification is non-contact, cost-effective and can easily be deployed on a smartphone, thus it can be an excellent tool for both TB and COVID-19 screening.      
### 47.Comparison of PAM-6 Modulations for Short-Reach Fiber-Optic Links with Intensity Modulation and Direct Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.05460.pdf)
>  PAM-6 transmission is considered for short-reach fiber-optic links with intensity modulation and direct detection. Experiments show that probabilistically-shaped PAM-6 and a framed-cross QAM-32 constellation outperform conventional cross QAM-32 under a peak power constraint.      
### 48.Experiments on Bipolar Transmission with Direct Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.05453.pdf)
>  Achievable information rates of bipolar 4- and 8-ary constellations are experimentally compared to those of intensity modulation (IM) when using an oversampled direct detection receiver. The bipolar constellations gain up to 1.8 dB over their IM counterparts.      
### 49.Symphony Generation with Permutation Invariant Language Model  [ :arrow_down: ](https://arxiv.org/pdf/2205.05448.pdf)
>  In this work, we present a symbolic symphony music generation solution, SymphonyNet, based on a permutation invariant language model. To bridge the gap between text generation and symphony generation task, we propose a novel Multi-track Multi-instrument Repeatable (MMR) representation with particular 3-D positional embedding and a modified Byte Pair Encoding algorithm (Music BPE) for music tokens. A novel linear transformer decoder architecture is introduced as a backbone for modeling extra-long sequences of symphony tokens. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Our empirical results show that our proposed approach can generate coherent, novel, complex and harmonious symphony compared to human composition, which is the pioneer solution for multi-track multi-instrument symbolic music generation.      
### 50.Machine Learning to Support Triage of Children at Risk for Epileptic Seizures in the Pediatric Intensive Care Unit  [ :arrow_down: ](https://arxiv.org/pdf/2205.05389.pdf)
>  Objective: Epileptic seizures are relatively common in critically-ill children admitted to the pediatric intensive care unit (PICU) and thus serve as an important target for identification and treatment. Most of these seizures have no discernible clinical manifestation but still have a significant impact on morbidity and mortality. Children that are deemed at risk for seizures within the PICU are monitored using continuous-electroencephalogram (cEEG). cEEG monitoring cost is considerable and as the number of available machines is always limited, clinicians need to resort to triaging patients according to perceived risk in order to allocate resources. This research aims to develop a computer aided tool to improve seizures risk assessment in critically-ill children, using an ubiquitously recorded signal in the PICU, namely the electrocardiogram (ECG). Approach: A novel data-driven model was developed at a patient-level approach, based on features extracted from the first hour of ECG recording and the clinical data of the patient. Main results: The most predictive features were the age of the patient, the brain injury as coma etiology and the QRS area. For patients without any prior clinical data, using one hour of ECG recording, the classification performance of the random forest classifier reached an area under the receiver operating characteristic curve (AUROC) score of 0.84. When combining ECG features with the patients clinical history, the AUROC reached 0.87. Significance: Taking a real clinical scenario, we estimated that our clinical decision support triage tool can improve the positive predictive value by more than 59% over the clinical standard.      
### 51.A Comprehensive Survey of Automated Audio Captioning  [ :arrow_down: ](https://arxiv.org/pdf/2205.05357.pdf)
>  Automated audio captioning, a task that mimics human perception as well as innovatively links audio processing and natural language processing, has overseen much progress over the last few years. Audio captioning requires recognizing the acoustic scene, primary audio events and sometimes the spatial and temporal relationship between events in an audio clip. It also requires describing these elements by a fluent and vivid sentence. Deep learning-based approaches are widely adopted to tackle this problem. This current paper situates itself as a comprehensive review covering the benchmark datasets, existing deep learning techniques and the evaluation metrics in automated audio captioning.      
### 52.Generalized Fast Multichannel Nonnegative Matrix Factorization Based on Gaussian Scale Mixtures for Blind Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2205.05330.pdf)
>  This paper describes heavy-tailed extensions of a state-of-the-art versatile blind source separation method called fast multichannel nonnegative matrix factorization (FastMNMF) from a unified point of view. The common way of deriving such an extension is to replace the multivariate complex Gaussian distribution in the likelihood function with its heavy-tailed generalization, e.g., the multivariate complex Student's t and leptokurtic generalized Gaussian distributions, and tailor-make the corresponding parameter optimization algorithm. Using a wider class of heavy-tailed distributions called a Gaussian scale mixture (GSM), i.e., a mixture of Gaussian distributions whose variances are perturbed by positive random scalars called impulse variables, we propose GSM-FastMNMF and develop an expectationmaximization algorithm that works even when the probability density function of the impulse variables have no analytical expressions. We show that existing heavy-tailed FastMNMF extensions are instances of GSM-FastMNMF and derive a new instance based on the generalized hyperbolic distribution that include the normal-inverse Gaussian, Student's t, and Gaussian distributions as the special cases. Our experiments show that the normalinverse Gaussian FastMNMF outperforms the state-of-the-art FastMNMF extensions and ILRMA model in speech enhancement and separation in terms of the signal-to-distortion ratio.      
### 53.Error Rate Analysis for Grant-free Massive Random Access with Short-Packet Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2205.05304.pdf)
>  Grant-free massive random access (RA) is a promising protocol to support the massive machine-type communications (mMTC) scenario in 5G and beyond networks. In this paper, we focus on the error rate analysis in grant-free massive RA, which is critical for practical deployment but has not been well studied. We consider a two-phase frame structure, with a pilot transmission phase for activity detection and channel estimation, followed by a data transmission phase with coded data symbols. Considering the characteristics of short-packet transmission, we analyze the block error rate (BLER) in the finite blocklength regime to characterize the data transmission performance. The analysis involves characterizing the activity detection and channel estimation errors as well as applying the random matrix theory (RMT) to analyze the distribution of the post-processing signal-to-noise ratio (SNR). As a case study, the derived BLER expression is further simplified to optimize the pilot length. Simulation results verify our analysis and demonstrate its effectiveness in pilot length optimization.      
### 54.Invisible-to-Visible: Privacy-Aware Human Segmentation using Airborne Ultrasound via Collaborative Learning Probabilistic U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2205.05293.pdf)
>  Color images are easy to understand visually and can acquire a great deal of information, such as color and texture. They are highly and widely used in tasks such as segmentation. On the other hand, in indoor person segmentation, it is necessary to collect person data considering privacy. We propose a new task for human segmentation from invisible information, especially airborne ultrasound. We first convert ultrasound waves to reflected ultrasound directional images (ultrasound images) to perform segmentation from invisible information. Although ultrasound images can roughly identify a person's location, the detailed shape is ambiguous. To address this problem, we propose a collaborative learning probabilistic U-Net that uses ultrasound and segmentation images simultaneously during training, closing the probabilistic distributions between ultrasound and segmentation images by comparing the parameters of the latent spaces. In inference, only ultrasound images can be used to obtain segmentation results. As a result of performance verification, the proposed method could estimate human segmentations more accurately than conventional probabilistic U-Net and other variational autoencoder models.      
### 55.Spatial-temporal associations representation and application for process monitoring using graph convolution neural network  [ :arrow_down: ](https://arxiv.org/pdf/2205.05250.pdf)
>  Industrial process data reflects the dynamic changes of operation conditions, which mainly refer to the irregular changes in the dynamic associations between different variables in different time. And this related associations knowledge for process monitoring is often implicit in these dynamic monitoring data which always have richer operation condition information and have not been paid enough attention in current research. To this end, a new process monitoring method based on spatial-based graph convolution neural network (SGCN) is proposed to describe the characteristics of the dynamic associations which can be used to represent the operation status over time. Spatia-temporal graphs are firstly defined, which can be used to represent the characteristics of node attributes (dynamic edge features) dynamically changing with time. Then, the associations between monitoring variables at a certain time can be considered as the node attributes to define a snapshot of the static graph network at the certain time. Finally, the snapshot containing graph structure and node attributes is used as model inputs which are processed to implement graph classification by spatial-based convolution graph neural network with aggregate and readout steps. The feasibility and applicability of this proposed method are demonstrated by our experimental results of benchmark and practical case application.      
### 56.Deep Learning-based Channel Estimation for Wideband Hybrid MmWave Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2205.05202.pdf)
>  Hybrid analog-digital (HAD) architecture is widely adopted in practical millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) systems to reduce hardware cost and energy consumption. However, channel estimation in the context of HAD is challenging due to only limited radio frequency (RF) chains at transceivers. Although various compressive sensing (CS) algorithms have been developed to solve this problem by exploiting inherent channel sparsity and sparsity structures, practical effects, such as power leakage and beam squint, can still make the real channel features deviate from the assumed models and result in performance degradation. Also, the high complexity of CS algorithms caused by a large number of iterations hinders their applications in practice. To tackle these issues, we develop a deep learning (DL)-based channel estimation approach where the sparse Bayesian learning (SBL) algorithm is unfolded into a deep neural network (DNN). In each SBL layer, Gaussian variance parameters of the sparse angular domain channel are updated by a tailored DNN, which is able to effectively capture complicated channel sparsity structures in various domains. Besides, the measurement matrix is jointly optimized for performance improvement. Then, the proposed approach is extended to the multi-block case where channel correlation in time is further exploited to adaptively predict the measurement matrix and facilitate the update of Gaussian variance parameters. Based on simulation results, the proposed approaches significantly outperform existing approaches but with reduced complexity.      
### 57.Design and Implementation of a Secure RISC-V Microprocessor  [ :arrow_down: ](https://arxiv.org/pdf/2205.05095.pdf)
>  Secret keys can be extracted from the power consumption or electromagnetic emanations of unprotected devices. Traditional counter-measures have limited scope of protection, and impose several restrictions on how sensitive data must be manipulated. We demonstrate a bit-serial RISC-V microprocessor implementation with no plain-text data. All values are protected using Boolean masking. Software can run with little to no counter-measures, reducing code size and performance overheads. Unlike previous literature, our methodology is fully automated and can be applied to designs of arbitrary size or complexity. We also provide details on other key components such as clock randomizer, memory protection, and random number generator. The microprocessor was implemented in 65 nm CMOS technology. Its implementation was evaluated using NIST tests as well as side channel attacks. Random numbers generated with our RNG pass on all NIST tests. Side-channel analysis on the baseline implementation extracted the AES key using only 375 traces, while our secure microprocessor was able to withstand attacks using 20 M traces.      
