# ArXiv eess --Mon, 2 May 2022
### 1.Recommendations on test datasets for evaluating AI solutions in pathology  [ :arrow_down: ](https://arxiv.org/pdf/2204.14226.pdf)
>  Artificial intelligence (AI) solutions that automatically extract information from digital histology images have shown great promise for improving pathological diagnosis. Prior to routine use, it is important to evaluate their predictive performance and obtain regulatory approval. This assessment requires appropriate test datasets. However, compiling such datasets is challenging and specific recommendations are missing. <br>A committee of various stakeholders, including commercial AI developers, pathologists, and researchers, discussed key aspects and conducted extensive literature reviews on test datasets in pathology. Here, we summarize the results and derive general recommendations for the collection of test datasets. <br>We address several questions: Which and how many images are needed? How to deal with low-prevalence subsets? How can potential bias be detected? How should datasets be reported? What are the regulatory requirements in different countries? <br>The recommendations are intended to help AI developers demonstrate the utility of their products and to help regulatory agencies and end users verify reported performance measures. Further research is needed to formulate criteria for sufficiently representative test datasets so that AI solutions can operate with less user intervention and better support diagnostic workflows in the future.      
### 2.Coordinated Charging Station Search in Stochastic Environments: A Multi-Agent Approach  [ :arrow_down: ](https://arxiv.org/pdf/2204.14219.pdf)
>  Range and charge anxiety remain essential barriers to a faster electric vehicle market diffusion. To this end, quickly and reliably finding suitable charging stations may foster an electric vehicle uptake by mitigating drivers' anxieties. Here, existing commercial services help drivers to find available stations based on real-time availability data but struggle with data inaccuracy, e.g., due to conventional vehicles blocking the access to public charging stations. In this context, recent works have studied stochastic search methods to account for availability uncertainty in order to minimize a driver's detour until reaching an available charging station. So far, both practical and theoretical approaches ignore driver coordination enabled by charging requests centralization or sharing of data, e.g., sharing observations of charging stations' availability or visit intentions between drivers. Against this background, we study coordinated stochastic search algorithms, which help to reduce station visit conflicts and improve the drivers' charging experience. We model a multi-agent stochastic charging station search problem as a finite-horizon Markov decision process and introduce an online solution framework applicable to static and dynamic policies. In contrast to static policies, dynamic policies account for information updates during policy planning and execution. We present a hierarchical implementation of a single-agent heuristic for decentralized decision making and a rollout algorithm for centralized decision making. Extensive numerical studies show that compared to an uncoordinated setting, a decentralized setting with visit-intentions sharing decreases the system cost by 26%, which is nearly as good as the 28% cost decrease achieved in a centralized setting, and saves up to 23% of a driver's search time while increasing her search reliability.      
### 3.Preoperative brain tumor imaging: models and software for segmentation and standardized reporting  [ :arrow_down: ](https://arxiv.org/pdf/2204.14199.pdf)
>  For patients suffering from brain tumor, prognosis estimation and treatment decisions are made by a multidisciplinary team based on a set of preoperative MR scans. Currently, the lack of standardized and automatic methods for tumor detection and generation of clinical reports represents a major hurdle. In this study, we investigate glioblastomas, lower grade gliomas, meningiomas, and metastases, through four cohorts of up to 4000 patients. Tumor segmentation models were trained using the AGU-Net architecture with different preprocessing steps and protocols. Segmentation performances were assessed in-depth using a wide-range of voxel and patient-wise metrics covering volume, distance, and probabilistic aspects. Finally, two software solutions have been developed, enabling an easy use of the trained models and standardized generation of clinical reports: Raidionics and Raidionics-Slicer. Segmentation performances were quite homogeneous across the four different brain tumor types, with an average true positive Dice ranging between 80% and 90%, patient-wise recall between 88% and 98%, and patient-wise precision around 95%. With our Raidionics software, running on a desktop computer with CPU support, tumor segmentation can be performed in 16 to 54 seconds depending on the dimensions of the MRI volume. For the generation of a standardized clinical report, including the tumor segmentation and features computation, 5 to 15 minutes are necessary. All trained models have been made open-access together with the source code for both software solutions and validation metrics computation. In the future, an automatic classification of the brain tumor type would be necessary to replace manual user input. Finally, the inclusion of post-operative segmentation in both software solutions will be key for generating complete post-operative standardized clinical reports.      
### 4.A Fast Algorithm for Selective Signal Extrapolation with Arbitrary Basis Functions  [ :arrow_down: ](https://arxiv.org/pdf/2204.14194.pdf)
>  Signal extrapolation is an important task in digital signal processing for extending known signals into unknown areas. The Selective Extrapolation is a very effective algorithm to achieve this. Thereby, the extrapolation is obtained by generating a model of the signal to be extrapolated as weighted superposition of basis functions. Unfortunately, this algorithm is computationally very expensive and, up to now, efficient implementations exist only for basis function sets that emanate from discrete transforms. Within the scope of this contribution, a novel efficient solution for Selective Extrapolation is presented for utilization with arbitrary basis functions. The proposed algorithm mathematically behaves identically to the original Selective Extrapolation, but is several decades faster. Furthermore, it is able to outperform existent fast transform domain algorithms which are limited to basis function sets that belong to the corresponding transform. With that, the novel algorithm allows for an efficient use of arbitrary basis functions, even if they are only numerically defined.      
### 5.Complex-Valued Frequency Selective Extrapolation for Fast Image and Video Signal Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2204.14193.pdf)
>  Signal extrapolation tasks arise in miscellaneous manners in the field of image and video signal processing. But, due to the widespread use of low-power and mobile devices, the computational complexity of an algorithm plays a crucial role in selecting an algorithm for a given problem. Within the scope of this contribution, we introduce the complex-valued Frequency Selective Extrapolation for fast image and video signal extrapolation. This algorithm iteratively generates a generic complex-valued model of the signal to be extrapolated as weighted superposition of Fourier basis functions. We further show that this algorithm is up to 10 times faster than the existent real-valued Frequency Selective Extrapolation that takes the real-valued nature of the input signals into account during the model generation. At the same time, the quality which is achievable by the complex-valued model generation is similar to the quality of the real-valued model generation.      
### 6.Segmentation of kidney stones in endoscopic video feeds  [ :arrow_down: ](https://arxiv.org/pdf/2204.14175.pdf)
>  Image segmentation has been increasingly applied in medical settings as recent developments have skyrocketed the potential applications of deep learning. Urology, specifically, is one field of medicine that is primed for the adoption of a real-time image segmentation system with the long-term aim of automating endoscopic stone treatment. In this project, we explored supervised deep learning models to annotate kidney stones in surgical endoscopic video feeds. In this paper, we describe how we built a dataset from the raw videos and how we developed a pipeline to automate as much of the process as possible. For the segmentation task, we adapted and analyzed three baseline deep learning models -- U-Net, U-Net++, and DenseNet -- to predict annotations on the frames of the endoscopic videos with the highest accuracy above 90\%. To show clinical potential for real-time use, we also confirmed that our best trained model can accurately annotate new videos at 30 frames per second. Our results demonstrate that the proposed method justifies continued development and study of image segmentation to annotate ureteroscopic video feeds.      
### 7.Autonomous Crosslink Radionavigation for a Lunar CubeSat Mission  [ :arrow_down: ](https://arxiv.org/pdf/2204.14155.pdf)
>  This study presents an autonomous orbit determination system based on crosslink radiometric measurements applied to a future lunar CubeSat mission to clearly highlight its advantages with respect to existing ground-based navigation strategies. This work is based on the Linked Autonomous Interplanetary Satellite Orbit Navigation (LiAISON) method which provides an autonomous navigation solution solely using satellite-to-satellite measurements, such as range and/or range-rate, to estimate absolute spacecraft states when at least one of the involved spacecraft has an orbit with a unique size, shape, and orientation. The lunar vicinity is a perfect candidate for this type of application due to the asymmetrical gravity field: the selected lunar mission, an Earth-Moon L2 (EML2) Halo orbiter, has an inter-satellite link between a lunar elliptical frozen orbiter. Simulation results show that, even in case of high-measurement errors (in the order of 100 m, 1 sigma, the navigation filter estimates the true states of spacecraft at EML2 with an error in the order of 500 m for position, and 2 mm/s for velocity, respectively and the elliptical lunar frozen orbiter states can be estimated in the order of 100 m for position and 1 cm/s for velocity, respectively. This study shows that range-only measurements provide better state estimation than range-rate-only measurements for this specific situation. Different bias handling strategies are also investigated. It has been found that even a less accurate ranging method, such as data-aided ranging, provides a sufficient orbit determination solution. This would simplify the communication system design for the selected CubeSat mission. The most observable states are found to be position states of the lunar orbiter via the observability analysis. In addition, the best tracking windows are also investigated for the selected mission scenario.      
### 8.Performance Analysis of Crosslink Radiometric Measurement based Autonomous Orbit Determination for Cislunar Small Satellite Formations  [ :arrow_down: ](https://arxiv.org/pdf/2204.14152.pdf)
>  Recent advances in space technology provide an opportunity for small satellites to be launched in cislunar space. However, tracking these small satellites still depends on ground-based operations. Autonomous navigation could be a possible solution considering challenges presented by costly ground operations and limited onboard power available for small satellites. There have been various studies on autonomous navigation methods for cislunar missions. One of them, LiAISON, provides an autonomous orbit determination solution solely using inter-satellite measurements. This study aims at providing a detailed performance analysis of crosslink radiometric measurements based on autonomous orbit determination for cislunar small satellite formations considering the effects of measurement type, measurement accuracy, bias, formation geometry, and network topology. This study shows that range observations provide better state estimation accuracy than range-rate observations for the autonomous navigation system in cislunar space. Line-of-sight angle measurements derived from radiometric measurements do not improve the overall system performance. In addition, less precise crosslink measurement methods could be an option for formations in the high observable orbital configurations. It was found that measurement biases and measurements with high intervals reduce the overall system performance. In case there are more than two spacecraft in the formation, the navigation system in the mesh topology provided better overall state estimation than the centralized topology.      
### 9.Adversarial Distortion Learning for Medical Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2204.14100.pdf)
>  We present a novel adversarial distortion learning (ADL) for denoising two- and three-dimensional (2D/3D) biomedical image data. The proposed ADL consists of two auto-encoders: a denoiser and a discriminator. The denoiser removes noise from input data and the discriminator compares the denoised result to its noise-free counterpart. This process is repeated until the discriminator cannot differentiate the denoised data from the reference. Both the denoiser and the discriminator are built upon a proposed auto-encoder called Efficient-Unet. Efficient-Unet has a light architecture that uses the residual blocks and a novel pyramidal approach in the backbone to efficiently extract and re-use feature maps. During training, the textural information and contrast are controlled by two novel loss functions. The architecture of Efficient-Unet allows generalizing the proposed method to any sort of biomedical data. The 2D version of our network was trained on ImageNet and tested on biomedical datasets whose distribution is completely different from ImageNet; so, there is no need for re-training. Experimental results carried out on magnetic resonance imaging (MRI), dermatoscopy, electron microscopy and X-ray datasets show that the proposed method achieved the best on each benchmark. Our implementation and pre-trained models are available at <a class="link-external link-https" href="https://github.com/mogvision/ADL" rel="external noopener nofollow">this https URL</a>.      
### 10.A Sampling Theorem for Exact Identification of Continuous-time Nonlinear Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.14021.pdf)
>  Low sampling frequency challenges the exact identification of the continuous-time (CT) dynamical system from sampled data, even when its model is identifiable. The necessary and sufficient condition is proposed-- which is built from Koopman operator-- to the exact identification of the CT system from sampled data. The condition gives a Nyquist-Shannon-like critical frequency for exact identification of CT nonlinear dynamical systems with Koopman invariant subspaces: 1) it establishes a sufficient condition for a sampling frequency that permits a discretized sequence of samples to discover the underlying system and 2) it also establishes a necessary condition for a sampling frequency that leads to system aliasing that the underlying system is indistinguishable; and 3) the original CT signal does not have to be band-limited as required in the Nyquist-Shannon Theorem. The theoretical criterion has been demonstrated on a number of simulated examples, including linear systems, nonlinear systems with equilibria, and limit cycles.      
### 11.A flexible propelled arm: Mechanical considerations for the use in UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2204.13987.pdf)
>  This paper presents a soft propelled arm, 3D-printed in TPU 70A, designed to be used in flexible UAVs, an advantageous concept since it reduces risk in the event of a collision thanks to a high energy absorption. The proposed design also allows the possibility to adapt to the environment and land on pipelines. The flexibility of the arm can be controlled during the additive manufacturing process by adjusting the infill rate or internal density. In this work, a mechanical design based on simulations with an experimentally adjusted 5- parameter Mooney-Rivlin non-linear model is proposed. Thrust efficiency is also maximized through CFD simulations.      
### 12.Participatory Sensing for Localization of a GNSS Jammer  [ :arrow_down: ](https://arxiv.org/pdf/2204.13974.pdf)
>  GNSS receivers are vulnerable to jamming and spoofing attacks, and numerous such incidents have been reported worldwide in the last decade. It is important to detect attacks fast and localize attackers, which can be hard if not impossible without dedicated sensing infrastructure. The notion of participatory sensing, or crowdsensing, is that a large ensemble of voluntary contributors provides the measurements, rather than relying on dedicated sensing infrastructure. This work considers embedded GNSS receivers to provide measurements for participatory jamming detection and localization. Specifically, this work proposes a novel jamming localization algorithm, based on participatory sensing, that exploits AGC and C/N_0 estimates from commercial GNSS receivers. The proposed algorithm does not require knowledge of the jamming power nor of the channels, but automatically estimates all parameters. The algorithm is shown to outperform similar state-of-the-art localization algorithms in relevant scenarios.      
### 13.Size Generalization for Resource Allocation with Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2204.13972.pdf)
>  Size generalization is important for learning wireless policies, which are often with dynamic sizes, say caused by time-varying number of users. Recent works of learning to optimize resource allocation empirically demonstrate that graph neural networks (GNNs) can generalize to different problem scales. However, GNNs are not guaranteed to generalize across input sizes. In this paper, we strive to analyze the size generalization mechanism of GNNs when learning permutation equivariant (PE) policies. We find that the aggregation function and activation functions of a GNN play a key role on its size generalization ability. We take the GNN with mean aggregator, called mean-GNN, as an example to demonstrate a size generalization condition, and interpret why several GNNs in the literature of wireless communications can generalize well to problem scales. To illustrate how to design GNNs with size generalizability according to our finding, we consider power and bandwidth allocation, and suggest to select or pre-train activation function in the output layer of mean-GNN for learning the PE policies. Simulation results show that the proposed GNN can generalize well to the number of users, which validate our analysis for the size generalization condition of GNNs when learning the PE policies.      
### 14.Learned Gradient of a Regularizer for Plug-and-Play Gradient Descent  [ :arrow_down: ](https://arxiv.org/pdf/2204.13940.pdf)
>  The Plug-and-Play (PnP) framework allows integrating advanced image denoising priors into optimization algorithms, to efficiently solve a variety of image restoration tasks. The Plug-and-Play alternating direction method of multipliers (ADMM) and the Regularization by Denoising (RED) algorithms are two examples of such methods that made a breakthrough in image restoration. However, while the former method only applies to proximal algorithms, it has recently been shown that there exists no regularization that explains the RED algorithm when the denoisers lack Jacobian symmetry, which happen to be the case of most practical denoisers. To the best of our knowledge, there exists no method for training a network that directly represents the gradient of a regularizer, which can be directly used in Plug-and-Play gradient-based algorithms. We show that it is possible to train a denoiser along with a network that corresponds to the gradient of its regularizer. We use this gradient of the regularizer in gradient-based optimization methods and obtain better results comparing to other generic Plug-and-Play approaches. We also show that the regularizer can be used as a pre-trained network for unrolled gradient descent. Lastly, we show that the resulting denoiser allows for a quick convergence of the Plug-and-Play ADMM.      
### 15.Control-Theoretic Modeling of Multi-Species Water Quality Dynamics in Drinking Water Networks: Survey, Methods, and Test Cases  [ :arrow_down: ](https://arxiv.org/pdf/2204.13911.pdf)
>  Chlorine is a widely used disinfectant and proxy for water quality (WQ) monitoring in water distribution networks (WDN). Chlorine-based WQ regulation and control aims to maintain pathogen-free water. Chlorine residual evolution within WDN is commonly modeled using the typical single-species decay and reaction dynamics that account for network-wide, spatiotemporal chlorine concentrations only. Prior studies have proposed more advanced and accurate descriptions via multi-species dynamics. This paper presents a host of novel state-space, control-theoretic representations of multi-species water quality dynamics. These representations describe decay, reaction, and transport of chlorine and a fictitious reactive substance to reflect realistic complex scenarios in WDN. Such dynamics are simulated over space- and time-discretized grids of the transport partial differential equation and the nonlinear reaction ordinary differential equation. To that end, this paper (i) provides a full description on how to formulate a high fidelity model-driven state-space representation of the multi-species water quality dynamics and (ii) investigates the applicability and performance of different Eulerian-based schemes (Lax-Wendroff, backward Euler, and Crank- Nicolson) and Lagrangian-based schemes (method of characteristics) in contrast with EPANET and its EPANET-MSX extension. Numerical case studies reveal that the Lax-Wendroff scheme and method of characteristics outperform other schemes with reliable results under reasonable assumptions and limitations.      
### 16.Differentially Private Load Restoration for Microgrids with Distributed Energy Storage  [ :arrow_down: ](https://arxiv.org/pdf/2204.13897.pdf)
>  Distributed energy storage systems (ESSs) can be efficiently leveraged for load restoration (LR) for a microgrid (MG) in island mode. When the ESSs are owned by third parties rather than the MG operator (MGO), the ESS operating setpoints may be considered as private information of their respective owners. Therefore, efforts must be put forth to avoid the disclosure through adversarial analysis of load setpoints. In his paper, we consider a scenario where LR takes place in a MG by determining load and ESS power injections through the solution of an AC optimal power flow (AC-OPF) problem. Since the charge/discharge mode at any given time is assumed to be private, we develop a differentially-private mechanism which restores load while maintaining privacy of ESS mode data. The performance of the proposed mechanism is demonstrated for a 33-bus MG.      
### 17.High-Fidelity Model of Stand-Alone Diesel Electric Generator with Hybrid Turbine-Governor Configuration for Microgrid Studies  [ :arrow_down: ](https://arxiv.org/pdf/2204.13894.pdf)
>  Diesel electric generators are an inherent part of remote hybrid microgrids found in remote regions of the world that provide primary frequency response (PFR) to restore system frequency during load or generation changes. However, with inverter-based resources (IBR) integration into microgrids, the IBR control provides a fast frequency response (FFR) to restore the system frequency. Hence, supplementing PFR with FFR requires a sophisticated control system and a high fidelity diesel electric generator model to design these control systems. In this work, a high-fidelity model of a diesel electric generator is developed. Its parameters are tuned using a surrogate optimization algorithm by emulating its response during a load change to a 400 kVA Caterpillar C-15 diesel generator, similar to those found in remote microgrids. The diesel electric generator model consists of a synchronous machine, DC4B excitation with V/Hz limiter, and a proposed modified IEEE GGOV1 engine-governor model (GGOV1D). The performance of the GGOV1D is compared with simple, Woodward DEGOV, and a standard IEEE GGOV1 engine-governor model. Results show that error in the diesel electric generator's response to load changes using the GGOV1D model is lower with an improved frequency response during the arresting and rebound period than the other engine-governor models.      
### 18.Deployment of an IoT System for Adaptive In-Situ Soundscape Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2204.13890.pdf)
>  Soundscape augmentation is an emerging approach for noise mitigation by introducing additional sounds known as "maskers" to increase acoustic comfort. Traditionally, the choice of maskers is often predicated on expert guidance or post-hoc analysis which can be time-consuming and sometimes arbitrary. Moreover, this often results in a static set of maskers that are inflexible to the dynamic nature of real-world acoustic environments. Overcoming the inflexibility of traditional soundscape augmentation is twofold. First, given a snapshot of a soundscape, the system must be able to select an optimal masker without human supervision. Second, the system must also be able to react to changes in the acoustic environment with near real-time latency. In this work, we harness the combined prowess of cloud computing and the Internet of Things (IoT) to allow in-situ listening and playback using microcontrollers while delegating computationally expensive inference tasks to the cloud. In particular, a serverless cloud architecture was used for inference, ensuring near real-time latency and scalability without the need to provision computing resources. A working prototype of the system is currently being deployed in a public area experiencing high traffic noise, as well as undergoing public evaluation for future improvements.      
### 19.Autonomous In-Situ Soundscape Augmentation via Joint Selection of Masker and Gain  [ :arrow_down: ](https://arxiv.org/pdf/2204.13883.pdf)
>  The selection of maskers and playback gain levels in a soundscape augmentation system is crucial to its effectiveness in improving the overall acoustic comfort of a given environment. Traditionally, the selection of appropriate maskers and gain levels has been informed by expert opinion, which may not representative of the target population, or by listening tests, which can be time-consuming and labour-intensive. Furthermore, the resulting static choices of masker and gain are often inflexible to the dynamic nature of real-world soundscapes. In this work, we utilized a deep learning model to perform joint selection of the optimal masker and its gain level for a given soundscape. The proposed model was designed with highly modular building blocks, allowing for an optimized inference process that can quickly search through a large number of masker and gain combinations. In addition, we introduced the use of feature-domain soundscape augmentation conditioned on the digital gain level, eliminating the computationally expensive waveform-domain mixing process during inference time, as well as the tedious pre-calibration process required for new maskers. The proposed system was validated on a large-scale dataset of subjective responses to augmented soundscapes with more than 440 participants, ensuring the ability of the model to predict combined effect of the masker and its gain level on the perceptual pleasantness level.      
### 20.Indoor 3-Dimensional Visible Light Positioning: Error Metric and LED Layout Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2204.13863.pdf)
>  We consider 3-dimensional (3D) visible light positioning (VLP) based on smartphone camera in an indoor scenario. Based on the positioning model in the quantized pixel-domain, we characterize the 3D normalized positioning error metric (NPEM) through the partial derivative of the positioning function, and evaluate the NPEM for horizontal and non-horizontal receiver camera positions. Moreover, under horizontal receiver terminal position, we explore the relationship between the NPEM and the light-emitting diode (LED) cell layout, approximate the relationship between the NPEM and the number of LEDs captured by the camera, and evaluate the approximation accuracy according to the simulated positioning error. Based on the approximation results, we optimize the LED transmitter cell layout to minimize NPEM assuming structured square cell layouts with certain distance parameters.      
### 21.COVID-Net US-X: Enhanced Deep Neural Network for Detection of COVID-19 Patient Cases from Convex Ultrasound Imaging Through Extended Linear-Convex Ultrasound Augmentation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.13851.pdf)
>  As the global population continues to face significant negative impact by the on-going COVID-19 pandemic, there has been an increasing usage of point-of-care ultrasound (POCUS) imaging as a low-cost and effective imaging modality of choice in the COVID-19 clinical workflow. A major barrier with widespread adoption of POCUS in the COVID-19 clinical workflow is the scarcity of expert clinicians that can interpret POCUS examinations, leading to considerable interest in deep learning-driven clinical decision support systems to tackle this challenge. A major challenge to building deep neural networks for COVID-19 screening using POCUS is the heterogeneity in the types of probes used to capture ultrasound images (e.g., convex vs. linear probes), which can lead to very different visual appearances. In this study, we explore the impact of leveraging extended linear-convex ultrasound augmentation learning on producing enhanced deep neural networks for COVID-19 assessment, where we conduct data augmentation on convex probe data alongside linear probe data that have been transformed to better resemble convex probe data. Experimental results using an efficient deep columnar anti-aliased convolutional neural network designed via a machined-driven design exploration strategy (which we name COVID-Net US-X) show that the proposed extended linear-convex ultrasound augmentation learning significantly increases performance, with a gain of 5.1% in test accuracy and 13.6% in AUC.      
### 22.Noise-reducing attention cross fusion learning transformer for histological image classification of osteosarcoma  [ :arrow_down: ](https://arxiv.org/pdf/2204.13838.pdf)
>  The degree of malignancy of osteosarcoma and its tendency to metastasize/spread mainly depend on the pathological grade (determined by observing the morphology of the tumor under a microscope). The purpose of this study is to use artificial intelligence to classify osteosarcoma histological images and to assess tumor survival and necrosis, which will help doctors reduce their workload, improve the accuracy of osteosarcoma cancer detection, and make a better prognosis for patients. The study proposes a typical transformer image classification framework by integrating noise reduction convolutional autoencoder and feature cross fusion learning (NRCA-FCFL) to classify osteosarcoma histological images. Noise reduction convolutional autoencoder could well denoise histological images of osteosarcoma, resulting in more pure images for osteosarcoma classification. Moreover, we introduce feature cross fusion learning, which integrates two scale image patches, to sufficiently explore their interactions by using additional classification tokens. As a result, a refined fusion feature is generated, which is fed to the residual neural network for label predictions. We conduct extensive experiments to evaluate the performance of the proposed approach. The experimental results demonstrate that our method outperforms the traditional and deep learning approaches on various evaluation metrics, with an accuracy of 99.17% to support osteosarcoma diagnosis.      
### 23.Analysing the Influence of Attack Configurations on the Reconstruction of Medical Images in Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2204.13808.pdf)
>  The idea of federated learning is to train deep neural network models collaboratively and share them with multiple participants without exposing their private training data to each other. This is highly attractive in the medical domain due to patients' privacy records. However, a recently proposed method called Deep Leakage from Gradients enables attackers to reconstruct data from shared gradients. This study shows how easy it is to reconstruct images for different data initialization schemes and distance measures. We show how data and model architecture influence the optimal choice of initialization scheme and distance measure configurations when working with single images. We demonstrate that the choice of initialization scheme and distance measure can significantly increase convergence speed and quality. Furthermore, we find that the optimal attack configuration depends largely on the nature of the target image distribution and the complexity of the model architecture.      
### 24.Distributed Auto-Learning GNN for Multi-Cell Cluster-Free NOMA Communications  [ :arrow_down: ](https://arxiv.org/pdf/2204.13766.pdf)
>  A multi-cell cluster-free NOMA framework is proposed, where both intra-cell and inter-cell interference are jointly mitigated via flexible cluster-free successive interference cancellation (SIC) and coordinated beamforming design, respectively. The joint design problem is formulated to maximize the system sum rate while satisfying the SIC decoding requirements and users' data rate constraints. To address this highly complex and coupling non-convex mixed integer nonlinear programming (MINLP), a novel distributed auto-learning graph neural network (AutoGNN) architecture is proposed to alleviate the overwhelming information exchange burdens among base stations (BSs). The proposed AutoGNN can train the GNN model weights whilst automatically learning the optimal GNN architecture, namely the GNN network depth and message embedding sizes, to achieve communication-efficient distributed scheduling. Based on the proposed architecture, a bi-level AutoGNN learning algorithm is further developed to efficiently approximate the hypergradient in model training. It is theoretically proved that the proposed bi-level AutoGNN learning algorithm can converge to a stationary point. Numerical results reveal that: 1) the proposed cluster-free NOMA framework outperforms the conventional cluster-based NOMA framework in the multi-cell scenario; and 2) the proposed AutoGNN architecture significantly reduces the computation and communication overheads compared to the conventional convex optimization-based methods and the conventional GNN with a fixed architecture.      
### 25.On the Arithmetic and Geometric Fusion of Beliefs for Distributed Inference  [ :arrow_down: ](https://arxiv.org/pdf/2204.13741.pdf)
>  We study the asymptotic learning rates under linear and log-linear combination rules of belief vectors in a distributed hypothesis testing problem. We show that under both combination strategies, agents are able to learn the truth exponentially fast, with a faster rate under log-linear fusion. We examine the gap between the rates in terms of network connectivity and information diversity. We also provide closed-form expressions for special cases involving federated architectures and exchangeable networks.      
### 26.One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation  [ :arrow_down: ](https://arxiv.org/pdf/2204.13738.pdf)
>  Multi-contrast magnetic resonance imaging (MRI) is widely used in clinical practice as each contrast provides complementary information. However, the availability of each contrast may vary amongst patients in reality. This poses challenges to both radiologists and automated image analysis algorithms. A general approach for tackling this problem is missing data imputation, which aims to synthesize the missing contrasts from existing ones. While several convolutional neural network (CNN) based algorithms have been proposed, they suffer from the fundamental limitations of CNN models, such as requirement for fixed numbers of input and output channels, inability to capture long-range dependencies, and lack of interpretability. In this paper, we formulate missing data imputation as a sequence-to-sequence learning problem and propose a multi-contrast multi-scale Transformer (MMT), which can take any subset of input contrasts and synthesize those that are missing. MMT consists of a multi-scale Transformer encoder that builds hierarchical representations of inputs combined with a multi-scale Transformer decoder that generates the outputs in a coarse-to-fine fashion. Thanks to the proposed multi-contrast Swin Transformer blocks, it can efficiently capture intra- and inter-contrast dependencies for accurate image synthesis. Moreover, MMT is inherently interpretable. It allows us to understand the importance of each input contrast in different regions by analyzing the in-built attention maps of Transformer blocks in the decoder. Extensive experiments on two large-scale multi-contrast MRI datasets demonstrate that MMT outperforms the state-of-the-art methods quantitatively and qualitatively.      
### 27.End-to-end Spoken Conversational Question Answering: Task, Dataset and Model  [ :arrow_down: ](https://arxiv.org/pdf/2204.14272.pdf)
>  In spoken question answering, the systems are designed to answer questions from contiguous text spans within the related speech transcripts. However, the most natural way that human seek or test their knowledge is via human conversations. Therefore, we propose a new Spoken Conversational Question Answering task (SCQA), aiming at enabling the systems to model complex dialogue flows given the speech documents. In this task, our main objective is to build the system to deal with conversational questions based on the audio recordings, and to explore the plausibility of providing more cues from different modalities with systems in information gathering. To this end, instead of directly adopting automatically generated speech transcripts with highly noisy data, we propose a novel unified data distillation approach, DDNet, which effectively ingests cross-modal information to achieve fine-grained representations of the speech and language modalities. Moreover, we propose a simple and novel mechanism, termed Dual Attention, by encouraging better alignments between audio and text to ease the process of knowledge transfer. To evaluate the capacity of SCQA systems in a dialogue-style interaction, we assemble a Spoken Conversational Question Answering (Spoken-CoQA) dataset with more than 40k question-answer pairs from 4k conversations. The performance of the existing state-of-the-art methods significantly degrade on our dataset, hence demonstrating the necessity of cross-modal information integration. Our experimental results demonstrate that our proposed method achieves superior performance in spoken conversational question answering tasks.      
### 28.Collision Risk and Operational Impact of Speed Change Advisories as Aircraft Collision Avoidance Maneuvers  [ :arrow_down: ](https://arxiv.org/pdf/2204.14250.pdf)
>  Aircraft collision avoidance systems have long been a key factor in keeping our airspace safe. Over the past decade, the FAA has supported the development of a new family of collision avoidance systems called the Airborne Collision Avoidance System X (ACAS X), which model the collision avoidance problem as a Markov decision process (MDP). Variants of ACAS X have been created for both manned (ACAS Xa) and unmanned aircraft (ACAS Xu and ACAS sXu). The variants primarily differ in the types of collision avoidance maneuvers they issue. For example, ACAS Xa issues vertical collision avoidance advisories, while ACAS Xu and ACAS sXu allow for horizontal advisories due to reduced aircraft performance capabilities. Currently, a new variant of ACAS X, called ACAS Xr, is being developed to provide collision avoidance capability to rotorcraft and Advanced Air Mobility (AAM) vehicles. Due to the desire to minimize deviation from the prescribed flight path of these aircraft, speed adjustments have been proposed as a potential collision avoidance maneuver for aircraft using ACAS Xr. In this work, we investigate the effect of speed change advisories on the safety and operational efficiency of collision avoidance systems. We develop an MDP-based collision avoidance logic that issues speed advisories and compare its performance to that of horizontal and vertical logics through Monte Carlo simulation on existing airspace encounter models. Our results show that while speed advisories are able to reduce collision risk, they are neither as safe nor as efficient as their horizontal and vertical counterparts.      
### 29.Application of machine learning methods to detect and classify Core images using GAN and texture recognition  [ :arrow_down: ](https://arxiv.org/pdf/2204.14224.pdf)
>  During exploration campaigns, oil companies rely heavily on drill core samples as they provide valuable geological information that helps them find important oil deposits. Traditional core logging techniques are laborious and subjective. Core imaging, a new technique in the oil industry, is used to supplement analysis by rapidly characterising large quantities of drill cores in a nondestructive and noninvasive manner. In this paper, we will present the problem of core detection and classification. The first problem is detecting the cores and segmenting the holes in images by using Faster RCNN and Mask RCNN models respectively. The second problem is filling the hole in the core image by applying the Generative adversarial network(GAN) technique and using Contextual Residual Aggregation(CRA) which creates high frequency residual for missing contents in images. And finally applying Texture recognition models for the classification of core images.      
### 30.Outage Performance of Uplink Rate Splitting Multiple Access with Randomly Deployed Users  [ :arrow_down: ](https://arxiv.org/pdf/2204.14154.pdf)
>  Rate splitting multiple access (RSMA) is a promising solution to improve spectral efficiency and provide better fairness for the upcoming sixth-generation (6G) networks. In this paper, the outage performance of uplink RSMA transmission with randomly deployed users is investigated, taking both user scheduling schemes and power allocation strategies into consideration. Specifically, the greedy user scheduling (GUS) and cumulative distribution function (CDF) based user scheduling (CUS) schemes are considered, which could maximize the rate performance and guarantee access fairness, respectively. Meanwhile, we re-investigate cognitive power allocation (CPA) strategy, and propose a new rate-fairness oriented power allocation (FPA) strategy to enhance the scheduled users rate fairness. By employing order statistics and stochastic geometry, an analytical expression of the outage probability for each scheduling scheme combining power allocation is derived to characterize the performance. To get more insights, the achieved diversity order of each scheme is also derived. Theoretical results demonstrate that both GUS and CUS schemes applying CPA or FPA strategy can achieve full diversity orders, and the application of CPA strategy in RSMA can effectively eliminate the secondary user's diversity order constraint from the primary user. Simulation results corroborate the accuracy of the analytical expressions, and show that the proposed FPA strategy can achieve excellent rate fairness performance in high signal-to-noise ratio region.      
### 31.Efficient solution of robust optimal control problems using local reduction  [ :arrow_down: ](https://arxiv.org/pdf/2204.14145.pdf)
>  Robust optimal control enables ensuring correct operation of a system subject to uncertainty. Existing methods for nonlinear robust control often use scenario-based approaches to formulate the control problem as nonlinear optimisation problems. Solving the resulting optimisation problems is a challenge due to the size of the problem. Mitigating the size of the problem by reducing the number of scenarios requires knowledge about how the uncertainty affects the system. This paper draws from local reduction methods used in semi-infinite optimisation to propose a new method to solve robust optimal control problems without a priori knowledge of the system or the uncertainty. By iteratively adding interim worst-case scenarios to the problem, we ensure robustness as well as provide control over the total number of scenarios. The method is validated in two case studies: temperature control in a residential building and flow control in a centrifugal compressor. In the building case study, with 590 uncertain parameters, the usual approach based on boundary values for each parameter would give $2^{590}$ scenarios. Our method only needed to include 68 scenarios in order to provide constraint satisfaction when tested for random realisations of uncertainties. In the compressor case study, the number of scenarios was reduced from 512 to nine, facilitating the numerical solution of the nonlinear optimisation problem while satisfying strict safety constraints.      
### 32.Improving the estimation of directional area scattering factor (DASF) from canopy reflectance: theoretical basis and validation  [ :arrow_down: ](https://arxiv.org/pdf/2204.14059.pdf)
>  Directional area scattering factor (DASF) is a critical canopy structural parameter for vegetation monitoring. It provides an efficient tool for decoupling of canopy structure and leaf optics from canopy reflectance. Current standard approach to estimate DASF from canopy bidirectional reflectance factor (BRF) is based on the assumption that in the weakly absorbing 710 to 790 nm spectral interval, leaf scattering does not change much with the concentration of dry matter and thus its variation can be neglected. This results in biased estimates of DASF and consequently leads to uncertainty in DASF-related applications. This study proposes a new approach to account for variations in concentrations of this biochemical constituent, which additionally uses the canopy BRF at 2260 nm. In silico analysis of the proposed approach suggests significant increase in accuracy over the standard technique by a relative root mean square error (rRMSE) of 49% and 34% for one- and three dimensional scenes, respectively. When compared with indoor multi-angular hyperspectral measurements reported in literature, the mean absolute error has reduced by 68% for needle leaf and 20% for broadleaf canopies. Thus, the proposed DASF estimation approach outperforms the current one and can be used more reliably in DASF-related applications, such as vegetation monitoring of functional traits, dynamics, and radiation budget.      
### 33.Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast  [ :arrow_down: ](https://arxiv.org/pdf/2204.14057.pdf)
>  We present an approach to learn voice-face representations from the talking face videos, without any identity labels. Previous works employ cross-modal instance discrimination tasks to establish the correlation of voice and face. These methods neglect the semantic content of different videos, introducing false-negative pairs as training noise. Furthermore, the positive pairs are constructed based on the natural correlation between audio clips and visual frames. However, this correlation might be weak or inaccurate in a large amount of real-world data, which leads to deviating positives into the contrastive paradigm. To address these issues, we propose the cross-modal prototype contrastive learning (CMPC), which takes advantage of contrastive methods and resists adverse effects of false negatives and deviate positives. On one hand, CMPC could learn the intra-class invariance by constructing semantic-wise positives via unsupervised clustering in different modalities. On the other hand, by comparing the similarities of cross-modal instances from that of cross-modal prototypes, we dynamically recalibrate the unlearnable instances' contribution to overall loss. Experiments show that the proposed approach outperforms state-of-the-art unsupervised methods on various voice-face association evaluation protocols. Additionally, in the low-shot supervision setting, our method also has a significant improvement compared to previous instance-wise contrastive learning.      
### 34.A Deep Learning based No-reference Quality Assessment Model for UGC Videos  [ :arrow_down: ](https://arxiv.org/pdf/2204.14047.pdf)
>  Quality assessment for User Generated Content (UGC) videos plays an important role in ensuring the viewing experience of end-users. Previous UGC video quality assessment (VQA) studies either use the image recognition model or the image quality assessment (IQA) models to extract frame-level features of UGC videos for quality regression, which are regarded as the sub-optimal solutions because of the domain shifts between these tasks and the UGC VQA task. In this paper, we propose a very simple but effective UGC VQA model, which tries to address this problem by training an end-to-end spatial feature extraction network to directly learn the quality-aware spatial feature representation from raw pixels of the video frames. We also extract the motion features to measure the temporal-related distortions that the spatial features cannot model. The proposed model utilizes very sparse frames to extract spatial features and dense frames (i.e. the video chunk) with a very low spatial resolution to extract motion features, which thereby has low computational complexity. With the better quality-aware features, we only use the simple multilayer perception layer (MLP) network to regress them into the chunk-level quality scores, and then the temporal average pooling strategy is adopted to obtain the video-level quality score. We further introduce a multi-scale quality fusion strategy to solve the problem of VQA across different spatial resolutions, where the multi-scale weights are obtained from the contrast sensitivity function of the human visual system. The experimental results show that the proposed model achieves the best performance on five popular UGC VQA databases, which demonstrates the effectiveness of the proposed model. The code will be publicly available.      
### 35.Quantum Computing for Power Flow Algorithms: Testing on real Quantum Computers  [ :arrow_down: ](https://arxiv.org/pdf/2204.14028.pdf)
>  Quantum computing has the potential to solve many computational problems exponentially faster than classical computers. The high shares of renewables and the wide deployment of converter-interfaced resources require new tools that shall drastically accelerate power system computations, including optimization and security assessment, which can benefit from quantum computing. To the best of our knowledge, this is the first paper that goes beyond quantum computing simulations and performs an experimental application of Quantum Computing for power systems on a real quantum computer. We use four different quantum computers, apply the HHL quantum algorithm, and examine the impact of current noisy quantum hardware on the accuracy and speed of an AC power flow algorithm. Using the insights from our real experiments on a 3-bus system, we perform the same studies on a 5-bus system with a simulated quantum computer to identify challenges and open research questions related with the scalability of these algorithms.      
### 36.Semi-Assisted Signal Authentication based on Galileo ACAS  [ :arrow_down: ](https://arxiv.org/pdf/2204.14026.pdf)
>  A GNSS signal authentication concept named semi-assisted authentication is proposed. It is based on the re-encryption and publication of keystream sequences of some milliseconds from an already existing encrypted signal. Some seconds after the keystreams are transmitted in the signal-in-space, the signal broadcasts the key allowing to decrypt the sequences and the a-posteriori correlation at the receiver. The concept is particularized as Galileo Assisted Commercial Authentication Service, or ACAS, for Galileo E1-B, with OSNMA used for the decryption keys, and E6C, assumed to be encrypted in the near future. This work proposes the ACAS cryptographic operations and a model for signal processing and authentication verification. Semi-assisted authentication can be provided without any modification to the signal plan of an existing GNSS, without the disclosure of signal encryption keys, and for several days of receiver autonomy, depending on its storage capabilities.      
### 37.Machine Learning-Based GPS Multipath Detection Method Using Dual Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2204.14001.pdf)
>  In urban areas, global navigation satellite system (GNSS) signals are often reflected or blocked by buildings, thus resulting in large positioning errors. In this study, we proposed a machine learning approach for global positioning system (GPS) multipath detection that uses dual antennas. A machine learning model that could classify GPS signal reception conditions was trained with several GPS measurements selected as suggested features. We applied five features for machine learning, including a feature obtained from the dual antennas, and evaluated the classification performance of the model, after applying four machine learning algorithms: gradient boosting decision tree (GBDT), random forest, decision tree, and K-nearest neighbor (KNN). It was found that a classification accuracy of 82%-96% was achieved when the test data set was collected at the same locations as those of the training data set. However, when the test data set was collected at locations different from those of the training data, a classification accuracy of 44%-77% was obtained.      
### 38.Leveraging triplet loss and nonlinear dimensionality reduction for on-the-fly channel charting  [ :arrow_down: ](https://arxiv.org/pdf/2204.13996.pdf)
>  Channel charting is an unsupervised learning method that aims at mapping wireless channels to a so-called chart, preserving as much as possible spatial neighborhoods. In this paper, a model-based deep learning approach to this problem is proposed. It builds on a physically motivated distance measure to structure and initialize a neural network that is subsequently trained using a triplet loss function. The proposed structure exhibits a low number of parameters and clever initialization leads to fast training. These two features make the proposed approach amenable to on-the-fly channel charting. The method is empirically assessed on realistic synthetic channels, yielding encouraging results.      
### 39.Deep Geometry Post-Processing for Decompressed Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2204.13952.pdf)
>  Point cloud compression plays a crucial role in reducing the huge cost of data storage and transmission. However, distortions can be introduced into the decompressed point clouds due to quantization. In this paper, we propose a novel learning-based post-processing method to enhance the decompressed point clouds. Specifically, a voxelized point cloud is first divided into small cubes. Then, a 3D convolutional network is proposed to predict the occupancy probability for each location of a cube. We leverage both local and global contexts by generating multi-scale probabilities. These probabilities are progressively summed to predict the results in a coarse-to-fine manner. Finally, we obtain the geometry-refined point clouds based on the predicted probabilities. Different from previous methods, we deal with decompressed point clouds with huge variety of distortions using a single model. Experimental results show that the proposed method can significantly improve the quality of the decompressed point clouds, achieving 9.30dB BDPSNR gain on three representative datasets on average.      
### 40.Multiple Degradation and Reconstruction Network for Single Image Denoising via Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2204.13873.pdf)
>  Single image denoising (SID) has achieved significant breakthroughs with the development of deep learning. However, the proposed methods are often accompanied by plenty of parameters, which greatly limits their application scenarios. Different from previous works that blindly increase the depth of the network, we explore the degradation mechanism of the noisy image and propose a lightweight Multiple Degradation and Reconstruction Network (MDRN) to progressively remove noise. Meanwhile, we propose two novel Heterogeneous Knowledge Distillation Strategies (HMDS) to enable MDRN to learn richer and more accurate features from heterogeneous models, which make it possible to reconstruct higher-quality denoised images under extreme conditions. Extensive experiments show that our MDRN achieves favorable performance against other SID models with fewer parameters. Meanwhile, plenty of ablation studies demonstrate that the introduced HMDS can improve the performance of tiny models or the model under high noise levels, which is extremely useful for related applications.      
### 41.Sparse-Group Log-Sum Penalized Graphical Model Learning For Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2204.13824.pdf)
>  We consider the problem of inferring the conditional independence graph (CIG) of a high-dimensional stationary multivariate Gaussian time series. A sparse-group lasso based frequency-domain formulation of the problem has been considered in the literature where the objective is to estimate the sparse inverse power spectral density (PSD) of the data. The CIG is then inferred from the estimated inverse PSD. In this paper we investigate use of a sparse-group log-sum penalty (LSP) instead of sparse-group lasso penalty. An alternating direction method of multipliers (ADMM) approach for iterative optimization of the non-convex problem is presented. We provide sufficient conditions for local convergence in the Frobenius norm of the inverse PSD estimators to the true value. This results also yields a rate of convergence. We illustrate our approach using numerical examples utilizing both synthetic and real data.      
### 42.Automatic Machine Learning for Multi-Receiver CNN Technology Classifiers  [ :arrow_down: ](https://arxiv.org/pdf/2204.13819.pdf)
>  Convolutional Neural Networks (CNNs) are one of the most studied family of deep learning models for signal classification, including modulation, technology, detection, and identification. In this work, we focus on technology classification based on raw I/Q samples collected from multiple synchronized receivers. As an example use case, we study protocol identification of Wi-Fi, LTE-LAA, and 5G NR-U technologies that coexist over the 5 GHz Unlicensed National Information Infrastructure (U-NII) bands. Designing and training accurate CNN classifiers involve significant time and effort that goes into fine-tuning a model's architectural settings and determining the appropriate hyperparameter configurations, such as learning rate and batch size. We tackle the former by defining architectural settings themselves as hyperparameters. We attempt to automatically optimize these architectural parameters, along with other preprocessing (e.g., number of I/Q samples within each classifier input) and learning hyperparameters, by forming a Hyperparameter Optimization (HyperOpt) problem, which we solve in a near-optimal fashion using the Hyperband algorithm. The resulting near-optimal CNN (OCNN) classifier is then used to study classification accuracy for OTA as well as simulations datasets, considering various SNR values. We show that the number of receivers to construct multi-channel inputs for CNNs should be defined as a preprocessing hyperparameter to be optimized via Hyperband. OTA results reveal that our OCNN classifiers improve classification accuracy by 24.58% compared to manually tuned CNNs. We also study the effect of min-max normalization of I/Q samples within each classifier's input on generalization accuracy over simulated datasets with SNRs other than training set's SNR and show an average of 108.05% improvement when I/Q samples are normalized.      
### 43.Deep Learning-based Automatic Player Identification and Logging in American Football Videos  [ :arrow_down: ](https://arxiv.org/pdf/2204.13809.pdf)
>  American football games attract significant worldwide attention every year. Game analysis systems generate crucial information that can help analyze the games by providing fans and coaches with a convenient means to track and evaluate player performance. Identifying participating players in each play is also important for the video indexing of player participation per play. Processing football game video presents challenges such as crowded setting, distorted objects, and imbalanced data for identifying players, especially jersey numbers. In this work, we propose a deep learning-based football video analysis system to automatically track players and index their participation per play. It is a multi-stage network design to highlight area of interest and identify jersey number information with high accuracy. First, we utilize an object detection network, a detection transformer, to tackle the player detection problem in crowded context. Second, we identify players using jersey number recognition with a secondary convolutional neural network, then synchronize it with a game clock subsystem. Finally, the system outputs a complete log in a database for play indexing. We demonstrate the effectiveness and reliability of player identification and the logging system by analyzing the qualitative and quantitative results on football videos. The proposed system shows great potential for implementation in and analysis of football broadcast video.      
### 44.Multicarrier-Division Duplex for Solving the Channel Aging Problem in Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2204.13785.pdf)
>  The separation of training and data transmission as well as the frequent uplink/downlink (UL/DL) switching make time-division duplex (TDD)-based massive multiple-input multiple-output (mMIMO) systems less competent in fast time-varying scenarios due to the resulted severe channel aging. To this end, a multicarrier-division duplex (MDD) mMIMO scheme associated with two types of well-designed frame structures are introduced for combating channel aging when communicating over fast time-varying channels. To compare with TDD, the corresponding frame structures related to 3GPP standards and their variant forms are presented. The MDD-specific general Wiener predictor and decision-directed Wiener predictor are introduced to predict the channel state information, respectively, in the time domain based on UL pilots and in the frequency domain based on the detected UL data, considering the impact of residual self-interference (SI). Moreover, by applying the zero-forcing precoding and maximum ratio combining, the closed-form approximations for the lower bounded rate achieved by TDD and MDD systems over time-varying channels are derived. Our main conclusion from this study is that the MDD, endowed with the capability of full-duplex but less demand on SI cancellation than in-band full-duplex (IBFD), outperforms both the conventional TDD and IBFD in combating channel aging.      
### 45.An Intriguing Property of Geophysics Inversion  [ :arrow_down: ](https://arxiv.org/pdf/2204.13731.pdf)
>  Inversion techniques are widely used to reconstruct subsurface physical properties (e.g., velocity, conductivity, and others) from surface-based geophysical measurements (e.g., seismic, electric/magnetic (EM) data). The problems are governed by partial differential equations~(PDEs) like the wave or Maxwell's equations. Solving geophysical inversion problems is challenging due to the ill-posedness and high computational cost. To alleviate those issues, recent studies leverage deep neural networks to learn the inversion mappings from geophysical measurements to the geophysical property directly. <br>In this paper, we show that such a mapping can be well modeled by a \textit{very shallow}~(but not wide) network with only five layers. This is achieved based on our new finding of an intriguing property: \textit{a near-linear relationship between the input and output, after applying integral transform in high dimensional space.} In particular, when dealing with the inversion from seismic data to subsurface velocity governed by a wave equation, the integral results of velocity with Gaussian kernels are linearly correlated to the integral of seismic data with sine kernels. Furthermore, this property can be easily turned into a light-weight encoder-decoder network for inversion. The encoder contains the integration of seismic data and the linear transformation without need for fine-tuning. The decoder only consists of a single transformer block to reverse the integral of velocity. <br>Experiments show that this interesting property holds for two geophysics inversion problems over four different datasets. Compared to much deeper InversionNet~\cite{wu2019inversionnet}, our method achieves comparable accuracy, but consumes significantly fewer parameters.      
### 46.Direct Air-to-Underwater Optical Wireless Communication: Statistical Characterization and Outage Performance  [ :arrow_down: ](https://arxiv.org/pdf/2204.13730.pdf)
>  In general, a buoy relay is used to connect the underwater communication to the terrestrial network over a radio or optical wireless communication (OWC) link. The use of relay deployment may pose security and deployment issues. This paper investigates the feasibility of direct air-to-underwater (A2UW) communication from an over-the-sea OWC system to an underwater submarine without deploying a relaying node. We analyze the statistical performance of the direct transmission over the combined channel fading effect of atmospheric turbulence, random fog, air-to-water interface, oceanic turbulence, and pointing errors. We develop novel analytical expressions for the probability density function (PDF) and cumulative distribution function (CDF) of the resultant signal-to-noise ratio (SNR) in terms of bivariate Meijer-G and Fox-H functions. We use the derived statistical results to analyze the system performance by providing exact and asymptotic results of the outage probability in terms of system parameters. We use computer simulations to demonstrate the performance of direct A2UW transmissions compared to the relay-assisted system.      
### 47.A Unified and Modular Model Predictive Control Framework for Soft Continuum Manipulators under Internal and External Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2204.13710.pdf)
>  The emerging field of fluidically actuated soft robotic control has promising capabilities such as inherent compliance and user safety. However, these are counterbalanced by issues not common to rigid robots, like nonlinear actuation dynamics, motion constraints, workspace limitations, and variable shape stiffness. In this work, we have adapted Model Predictive Control (MPC), that has recently seen an exponential rise in popularity and fields of applications, to a soft robotic arm called SoPrA. We have addressed the problems that current control methods are facing, trying to propose a unique environment to handle them in a modular way. This work shows, both with simulation and experimental results, that Task-Space MPC can be successfully implemented for dynamic soft robotic control, while past research has usually focused on Joint-Space references. We have provided a way to couple the Piece-wise Constant Curvature and the Augmented Rigid Body Model assumptions with internal and external constraints and actuation dynamics, delivering an algorithm that can manage all these information and optimize over them. We believe that an MPC implementation based on our approach could be the way to address most of model-based soft robotics control issues within a unified and modular framework, while allowing to include improvements that usually belong to other control domains such as learning techniques.      
### 48.Federated Learning: Balancing the Thin Line Between Data Intelligence and Privacy  [ :arrow_down: ](https://arxiv.org/pdf/2204.13697.pdf)
>  Federated learning holds great promise in learning from fragmented sensitive data and has revolutionized how machine learning models are trained. This article provides a systematic overview and detailed taxonomy of federated learning. We investigate the existing security challenges in federated learning and provide a comprehensive overview of established defense techniques for data poisoning, inference attacks, and model poisoning attacks. The work also presents an overview of current training challenges for federated learning, focusing on handling non-i.i.d. data, high dimensionality issues, and heterogeneous architecture, and discusses several solutions for the associated challenges. Finally, we discuss the remaining challenges in managing federated learning training and suggest focused research directions to address the open questions. Potential candidate areas for federated learning, including IoT ecosystem, healthcare applications, are discussed with a particular focus on banking and financial domains.      
