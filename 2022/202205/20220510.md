# ArXiv eess --Tue, 10 May 2022
### 1.Activating More Pixels in Image Super-Resolution Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2205.04437.pdf)
>  Transformer-based methods have shown impressive performance in low-level vision tasks, such as image super-resolution. However, we find that these networks can only utilize a limited spatial range of input information through attribution analysis. This implies that the potential of Transformer is still not fully exploited in existing networks. In order to activate more input pixels for reconstruction, we propose a novel Hybrid Attention Transformer (HAT). It combines channel attention and self-attention schemes, thus making use of their complementary advantages. Moreover, to better aggregate the cross-window information, we introduce an overlapping cross-attention module to enhance the interaction between neighboring window features. In the training stage, we additionally propose a same-task pre-training strategy to bring further improvement. Extensive experiments show the effectiveness of the proposed modules, and the overall method significantly outperforms the state-of-the-art methods by more than 1dB. Codes and models will be available at <a class="link-external link-https" href="https://github.com/chxy95/HAT" rel="external noopener nofollow">this https URL</a>.      
### 2.Speaker Reinforcement Using Target Source Extraction for Robust Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2205.04433.pdf)
>  Improving the accuracy of single-channel automatic speech recognition (ASR) in noisy conditions is challenging. Strong speech enhancement front-ends are available, however, they typically require that the ASR model is retrained to cope with the processing artifacts. In this paper we explore a speaker reinforcement strategy for improving recognition performance without retraining the acoustic model (AM). This is achieved by remixing the enhanced signal with the unprocessed input to alleviate the processing artifacts. We evaluate the proposed approach using a DNN speaker extraction based speech denoiser trained with a perceptually motivated loss function. Results show that (without AM retraining) our method yields about 23% and 25% relative accuracy gains compared with the unprocessed for the monoaural simulated and real CHiME-4 evaluation sets, respectively, and outperforms a state-of-the-art reference method.      
### 3.NaturalSpeech: End-to-End Text to Speech Synthesis with Human-Level Quality  [ :arrow_down: ](https://arxiv.org/pdf/2205.04421.pdf)
>  Text to speech (TTS) has made rapid progress in both academia and industry in recent years. Some questions naturally arise that whether a TTS system can achieve human-level quality, how to define/judge human-level quality and how to achieve it. In this paper, we answer these questions by first defining human-level quality based on statistical significance of measurement and describing the guidelines to judge it, and then proposing a TTS system called NaturalSpeech that achieves human-level quality on a benchmark dataset. Specifically, we leverage a variational autoencoder (VAE) for end-to-end text to waveform generation, with several key designs to enhance the capacity of prior from text and reduce the complexity of posterior from speech, including phoneme pre-training, differentiable duration modeling, bidirectional prior/posterior modeling, and memory mechanism in VAE. Experiment evaluations on popular LJSpeech dataset show that our proposed NaturalSpeech achieves -0.01 CMOS (comparative mean opinion score) to human recordings on sentence level, with Wilcoxon signed rank test at p-level p&gt;&gt;0.05, which demonstrates no statistically significant difference from human recordings for the first time on this dataset.      
### 4.Towards Measuring Domain Shift in Histopathological Stain Translation in an Unsupervised Manner  [ :arrow_down: ](https://arxiv.org/pdf/2205.04368.pdf)
>  Domain shift in digital histopathology can occur when different stains or scanners are used, during stain translation, etc. A deep neural network trained on source data may not generalise well to data that has undergone some domain shift. An important step towards being robust to domain shift is the ability to detect and measure it. This article demonstrates that the PixelCNN and domain shift metric can be used to detect and quantify domain shift in digital histopathology, and they demonstrate a strong correlation with generalisation performance. These findings pave the way for a mechanism to infer the average performance of a model (trained on source data) on unseen and unlabelled target data.      
### 5.Site Generalization: Stroke Lesion Segmentation on Magnetic Resonance Images from Unseen Sites  [ :arrow_down: ](https://arxiv.org/pdf/2205.04329.pdf)
>  There are considerable interests in automatic stroke lesion segmentation on magnetic resonance (MR) images in the medical imaging field, as strokes are the main cause of various cerebrovascular diseases. Although deep learning-based models have been proposed for this task, generalizing these models to unseen sites is difficult due to not only the large intersite discrepancy among different scanners, imaging protocols, and populations but also the variations in stroke lesion shape, size, and location. Thus, we propose a U-net--based segmentation network termed SG-Net to improve unseen site generalization for stroke lesion segmentation on MR images. Specifically, we first propose masked adaptive instance normalization (MAIN) to minimize intersite discrepancies, standardizing input MR images from different sites into a site-unrelated style by dynamically learning affine parameters from the input. Then, we leverage a gradient reversal layer to force the U-net encoder to learn site-invariant representation, which further improves the model generalization in conjunction with MAIN. Finally, inspired by the "pseudosymmetry" of the human brain, we introduce a simple, yet effective data augmentation technique that can be embedded within SG-Net to double the sample size while halving memory consumption. As a result, stroke lesions from the whole brain can be easily identified within a hemisphere, improving the simplicity of training. Experimental results on the benchmark Anatomical Tracings of Lesions After Stroke (ATLAS) dataset, which includes MR images from 9 different sites, demonstrate that under the "leave-one-site-out" setting, the proposed SG-Net substantially outperforms recently published methods in terms of quantitative metrics and qualitative comparisons.      
### 6.TGANet: Text-guided attention for improved polyp segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2205.04280.pdf)
>  Colonoscopy is a gold standard procedure but is highly operator-dependent. Automated polyp segmentation, a precancerous precursor, can minimize missed rates and timely treatment of colon cancer at an early stage. Even though there are deep learning methods developed for this task, variability in polyp size can impact model training, thereby limiting it to the size attribute of the majority of samples in the training dataset that may provide sub-optimal results to differently sized polyps. In this work, we exploit size-related and polyp number-related features in the form of text attention during training. We introduce an auxiliary classification task to weight the text-based embedding that allows network to learn additional feature representations that can distinctly adapt to differently sized polyps and can adapt to cases with multiple polyps. Our experimental results demonstrate that these added text embeddings improve the overall performance of the model compared to state-of-the-art segmentation methods. We explore four different datasets and provide insights for size-specific improvements. Our proposed text-guided attention network (TGANet) can generalize well to variable-sized polyps in different datasets.      
### 7.Bandwidth-Scalable Fully Mask-Based Deep FCRN Acoustic Echo Cancellation and Postfiltering  [ :arrow_down: ](https://arxiv.org/pdf/2205.04276.pdf)
>  Although today's speech communication systems support various bandwidths from narrowband to super-wideband and beyond, state-of-the art DNN methods for acoustic echo cancellation (AEC) are lacking modularity and bandwidth scalability. Our proposed DNN model builds upon a fully convolutional recurrent network (FCRN) and introduces scalability over various bandwidths up to a fullband (FB) system (48 kHz sampling rate). This modular approach allows joint wideband (WB) pre-training of mask-based AEC and postfilter stages with dedicated losses, followed by a separate training of them on FB data. A third lightweight blind bandwidth extension stage is separately trained on FB data, flexibly allowing to extend the WB postfilter output towards higher bandwidths until reaching FB. Thereby, higher frequency noise and echo are reliably suppressed. On the ICASSP 2022 Acoustic Echo Cancellation Challenge blind test set we report a competitive performance, showing robustness even under highly delayed echo and dynamic echo path changes.      
### 8.Spiking Neural Network Equalization for IM/DD Optical Communication  [ :arrow_down: ](https://arxiv.org/pdf/2205.04263.pdf)
>  A spiking neural network (SNN) equalizer model suitable for electronic neuromorphic hardware is designed for an IM/DD link. The SNN achieves the same bit-error-rate as an artificial neural network, outperforming linear equalization.      
### 9.Distributed and Joint Optimization of Precoding and Power for User-Centric Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2205.04239.pdf)
>  In the cell-free massive multiple-input multiple-output (CF mMIMO) system, the centralized transmission scheme is widely adopted to manage the inter-user interference. Unfortunately, its implementation is limited by the extensive signaling overhead between the central process unit (CPU) and the access points (APs). In this letter, we study the downlink transmission scheme in a distributed approach. First, we propose a reduced channel state information (CSI) exchange mechanism, where only the CSI of a portion of users is shared among neighboring APs. Base on this, the dual decomposition method is adopted to jointly optimize the precoder and power control. The precoding vector can be independently calculated by each AP cluster with closed-form expression. With very few iterations, the proposed distributed scheme achieves the same performance as the centralized one. Moreover, it significantly reduces the information exchange to the CPU.      
### 10.RCMNet: A deep learning model assists CAR-T therapy for leukemia  [ :arrow_down: ](https://arxiv.org/pdf/2205.04230.pdf)
>  Acute leukemia is a type of blood cancer with a high mortality rate. Current therapeutic methods include bone marrow transplantation, supportive therapy, and chemotherapy. Although a satisfactory remission of the disease can be achieved, the risk of recurrence is still high. Therefore, novel treatments are demanding. Chimeric antigen receptor-T (CAR-T) therapy has emerged as a promising approach to treat and cure acute leukemia. To harness the therapeutic potential of CAR-T cell therapy for blood diseases, reliable cell morphological identification is crucial. Nevertheless, the identification of CAR-T cells is a big challenge posed by their phenotypic similarity with other blood cells. To address this substantial clinical challenge, herein we first construct a CAR-T dataset with 500 original microscopy images after staining. Following that, we create a novel integrated model called RCMNet (ResNet18 with CBAM and MHSA) that combines the convolutional neural network (CNN) and Transformer. The model shows 99.63% top-1 accuracy on the public dataset. Compared with previous reports, our model obtains satisfactory results for image classification. Although testing on the CAR-T cells dataset, a decent performance is observed, which is attributed to the limited size of the dataset. Transfer learning is adapted for RCMNet and a maximum of 83.36% accuracy has been achieved, which is higher than other SOTA models. The study evaluates the effectiveness of RCMNet on a big public dataset and translates it to a clinical dataset for diagnostic applications.      
### 11.Mixed-UNet: Refined Class Activation Mapping for Weakly-Supervised Semantic Segmentation with Multi-scale Inference  [ :arrow_down: ](https://arxiv.org/pdf/2205.04227.pdf)
>  Deep learning techniques have shown great potential in medical image processing, particularly through accurate and reliable image segmentation on magnetic resonance imaging (MRI) scans or computed tomography (CT) scans, which allow the localization and diagnosis of lesions. However, training these segmentation models requires a large number of manually annotated pixel-level labels, which are time-consuming and labor-intensive, in contrast to image-level labels that are easier to obtain. It is imperative to resolve this problem through weakly-supervised semantic segmentation models using image-level labels as supervision since it can significantly reduce human annotation efforts. Most of the advanced solutions exploit class activation mapping (CAM). However, the original CAMs rarely capture the precise boundaries of lesions. In this study, we propose the strategy of multi-scale inference to refine CAMs by reducing the detail loss in single-scale reasoning. For segmentation, we develop a novel model named Mixed-UNet, which has two parallel branches in the decoding phase. The results can be obtained after fusing the extracted features from two branches. We evaluate the designed Mixed-UNet against several prevalent deep learning-based segmentation approaches on our dataset collected from the local hospital and public datasets. The validation results demonstrate that our model surpasses available methods under the same supervision level in the segmentation of various lesions from brain imaging.      
### 12.TransEM:Residual Swin-Transformer based regularized PET image reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2205.04204.pdf)
>  Positron emission tomography(PET) image reconstruction is an ill-posed inverse problem and suffers from high level of noise due to limited counts received. Recently deep neural networks especially convolutional neural networks(CNN) have been successfully applied to PET image reconstruction. However, the local characteristics of the convolution operator potentially limit the image quality obtained by current CNN-based PET image reconstruction methods. In this paper, we propose a residual swin-transformer based regularizer(RSTR) to incorporate regularization into the iterative reconstruction framework. Specifically, a convolution layer is firstly adopted to extract shallow features, then the deep feature extraction is accomplished by the swin-transformer layer. At last, both deep and shallow features are fused with a residual operation and another convolution layer. Validations on the realistic 3D brain simulated low-count data show that our proposed method outperforms the state-of-the-art methods in both qualitative and quantitative measures.      
### 13.LSTM-Based Distributed Conditional Generative Adversarial Network For Data-Driven 5G-Enabled Maritime UAV Communications  [ :arrow_down: ](https://arxiv.org/pdf/2205.04196.pdf)
>  5G enabled maritime unmanned aerial vehicle (UAV) communication is one of the important applications of 5G wireless network which requires minimum latency and higher reliability to support mission-critical applications. Therefore, lossless reliable communication with a high data rate is the key requirement in modern wireless communication systems. These all factors highly depend upon channel conditions. In this work, a channel model is proposed for air-to-surface link exploiting millimeter wave (mmWave) for 5G enabled maritime unmanned aerial vehicle (UAV) communication. Firstly, we will present the formulated channel estimation method which directly aims to adopt channel state information (CSI) of mmWave from the channel model inculcated by UAV operating within the Long Short Term Memory (LSTM)-Distributed Conditional generative adversarial network (DCGAN) i.e. (LSTM-DCGAN) for each beamforming direction. Secondly, to enhance the applications for the proposed trained channel model for the spatial domain, we have designed an LSTM-DCGAN based UAV network, where each one will learn mmWave CSI for all the distributions. Lastly, we have categorized the most favorable LSTM-DCGAN training method and emanated certain conditions for our UAV network to increase the channel model learning rate. Simulation results have shown that the proposed LSTM-DCGAN based network is vigorous to the error generated through local training. A detailed comparison has been done with the other available state-of-the-art CGAN network architectures i.e. stand-alone CGAN (without CSI sharing), Simple CGAN (with CSI sharing), multi-discriminator CGAN, federated learning CGAN and DCGAN. Simulation results have shown that the proposed LSTM-DCGAN structure demonstrates higher accuracy during the learning process and attained more data rate for downlink transmission as compared to the previous state of artworks.      
### 14.Performance assessment of medical and non-medical CPAP interfaces used during the COVID-19 pandemic  [ :arrow_down: ](https://arxiv.org/pdf/2205.04167.pdf)
>  Background: At the beginning of 2020, a high number of COVID-19 cases affected Italy in a short period, causing a difficult supply of medical equipment. To deal with the problem, many healthcare operators readapted different masks to medical devices, but no experiment was conducted to evaluate their performance. The aims of our study were: to assess the performances of three masks and a CPAP helmet in their original configuration and after modifications, in the maintenance of mean pressure and half-amplitude variations using different PEEP valves and to analyse the impact of antibacterial (AB) or antibacterial-viral (ABV) pre-valve PEEP filters on the effective PEEP delivered to the patients. Four pressure ports were installed on each mask (three on helmet), mean values and half amplitudes of pressure were recorded. Tests were performed with any, AB, ABV filter before the PEEP valve. CPAP helmet was the most efficient interface producing more stable mean pressure and less prominent half-amplitude variations but the non-medical masks, especially after the modifications, maintained a stable mean pressure value with only a moderate increase of half-amplitude. The use of AB and ABV filters, produced respectively an increase of 2,23% and 16.5% in mean pressure, compared to no filter condition. CPAP helmet is the most reliable interface in terms of detected performance, but readapted masks can assure almost a similar performance. The use of ABV filters before the PEEP valve significantly increases the detected mean pressure while the AB filters have almost a neutral effect.      
### 15.Randomized Group-Greedy Method for Data-Driven Sensor Selection  [ :arrow_down: ](https://arxiv.org/pdf/2205.04161.pdf)
>  Randomized group-greedy methods for sensor selection problems are proposed. The randomized greedy sensor selection algorithm is straightforwardly applied to the group-greedy method, and a customized method is also considered. In the customized method, a part of the shrunken sensor candidates is selected to be the oversampled sensors by the common greedy method, and this strategy compensates for the deterioration of the solution due to shrunken sensor candidates. The proposed methods are implemented based on the D- and E-optimal design of experiments, and a numerical experiment is conducted using a randomly generated dataset. The proposed method can provide better optimization results than those obtained by the original group-greedy method when a similar computational cost is spent as for the original group-greedy method. This is because the group size for the group-greedy method can be increased as a result of the reduced sensor candidates by the randomized algorithm.      
### 16.Towards a median signal detector through the total Bregman divergence and its robust analysis  [ :arrow_down: ](https://arxiv.org/pdf/2205.04156.pdf)
>  A novel family of geometric signal detectors are proposed through medians of the total Bregman divergence (TBD), which are shown advantageous over the conventional methods and their mean counterparts. By interpreting the observation data as Hermitian positive-definite (HPD) matrices, their mean or median play an essential role in signal detection. As is difficult to be solved analytically, we propose numerical solutions through Riemannian gradient descent algorithms or fixed-point algorithms. <br>Beside detection performance, robustness of a detector to outliers is also of vital importance, which can often be analyzed via the influence functions. Introducing an orthogonal basis for Hermitian matrices, we are able to compute the corresponding influence functions analytically and exactly by solving a linear system, which is transformed from the governing matrix equation. Numerical simulations show that the TBD medians are more robust than their mean counterparts.      
### 17.Towards Self-Adaptive Peer-to-Peer Monitoring for Fog Environments  [ :arrow_down: ](https://arxiv.org/pdf/2205.04142.pdf)
>  Monitoring is a critical component in fog environments: it promptly provides insights about the behavior of systems, reveals Service Level Agreements (SLAs) violations, enables the autonomous orchestration of services and platforms, calls for the intervention of operators, and triggers self-healing actions. <br>In such environments, monitoring solutions have to cope with the heterogeneity of the devices and platforms present in the Fog, the limited resources available at the edge of the network, and the high dynamism of the whole Cloud-to-Thing continuum. <br>This paper addresses the challenge of accurately and efficiently monitoring the Fog with a self-adaptive peer-to-peer (P2P) monitoring solution that can opportunistically adjust its behavior according to the collected data exploiting a lightweight rule-based expert system. <br>Empirical results show that adaptation can improve monitoring accuracy, while reducing network and power consumption at the cost of higher memory consumption.      
### 18.An Efficient Architecture and High-Throughput Implementation of CCSDS-123.0-B-2 Hybrid Entropy Coder Targeting Space-Grade SRAM FPGA Technology  [ :arrow_down: ](https://arxiv.org/pdf/2205.04123.pdf)
>  Nowadays, hyperspectral imaging is recognized as cornerstone remote sensing technology. The explosive growth in image data volume and instrument data rates, compete with limited on-board storage resources and downlink bandwidth, making hyperspectral image data compression a mission critical on-board processing task. The Consultative Committee for Space Data Systems (CCSDS) extended the previous issue of the CCSDS-123.0 Recommended Standard for multi- and hyperspectral image compression to provide with Near-Lossless compression functionality. A key feature of the CCSDS-123.0-B-2 is the improved Hybrid Entropy Coder, which at low bit rates, provides substantially better compression performance than the Issue 1 entropy coders. In this paper, we introduce a high-throughput hardware implementation of the CCSDS-123.0-B-2 Hybrid Entropy Coder. The introduced architecture exploits the systolic design pattern to provide modularity and latency insensitivity in a deep and elastic pipeline achieving a constant throughput of 1 sample/cycle with a small FPGA resource footprint. This architecture is described in portable VHDL RTL and is implemented, validated and demonstrated on a commercially available Xilinx KCU105 development board hosting a Xilinx Kintex Ultrascale XCKU040 SRAM FPGA, and thus, is directly transferable to Xilinx Radiation Tolerant Kintex UltraScale XQRKU060 space-grade devices for space deployments. Moreover, state-of-the-art SpaceFibre (ECSS-E-ST-50-11C) serial link interface and test equipment were used in the validation platform to emulate an on-board deployment. The introduced CCSDS-123.0-B-2 Hybrid Entropy Encoder achieves a constant throughput performance of 305 MSamples/s. To the best of our knowledge, this is the first published fully-compliant architecture and high-throughput implementation of the CCSDS-123.0-B-2 Hybrid Entropy Coder, targeting space-grade FPGA technology.      
### 19.Development of Charging, Discharging Scheduling Algorithm for Economical and Energy Efficient Operation of Multi EV Charging Station  [ :arrow_down: ](https://arxiv.org/pdf/2205.04116.pdf)
>  As the number of electric vehicles (EVs) significantly increases, the excessive charging demand of parked EVs in the charging station may incur an instability problem to the electricity network during peak hours. For the charging station to take a microgrid (MG) structure, an economical and energy-efficient power management scheme is required for the power provision of EVs while considering the local load demand of the MG. For these purposes, this study presents the power management scheme of interdependent MG and EV fleets aided by a novel EV charg-ing/discharging scheduling algorithm. In this algorithm, the maximum amount of discharging power from parked EVs is determined based on the difference between local load demand and photovoltaic (PV) power production to alleviate imbalances occurred between them. For the power management of the MG with charging/discharging scheduling of parked EVs in the PV-based charging station, multi-objective optimization is performed to minimize the operating cost and grid dependency. In addition, the proposed scheme maximizes the utilization of EV charging/discharging while satisfying the charging requirements of parked EVs. Moreover, a more economical and energy-efficient PV-based charging station is established using the future trends of local load demand and PV power production predicted by a gated recurrent unit (GRU) network. With the proposed EV charging/discharging scheduling algorithm, the operating cost of PV-based charging station is decreased by 167.71% and 28.85% compared with the EV charg-ing scheduling algorithm and the conventional EV charging/discharging scheduling algorithm, respectively. It is obvious that the economical and energy-efficient operation of PV-based charging station can be accomplished by applying the power management scheme with the proposed EV charging/discharging scheduling strategy.      
### 20.Damage Maximization for Combat Network with Limited Costs  [ :arrow_down: ](https://arxiv.org/pdf/2205.04113.pdf)
>  Maximizing the damage of combat network plays a vital role in identifying the important nodes in combat system-of-system (SOS). In order to protect or destroy the critical part of combat network more efficiently with less costs, here we report a more realistic model to study the combat network damage problems. As a first step, the cost and effect of damage are redefined based on the network topology and the functional characteristics of nodes according to practical significance, respectively. Then, the damage maximization model for combat network with limited costs is constructed. To obtain the optimal solution of the mathematical model, an improved genetic algorithm (IPGA) based on prior information is proposed. As a result, our proposed method has a significant advantage in the feasibility and effectiveness compared with other algorithms shown in the simulated experiments. Furthermore, the attack law of combat network is explored.      
### 21.ReCAB-VAE: Gumbel-Softmax Variational Inference Based on Analytic Divergence  [ :arrow_down: ](https://arxiv.org/pdf/2205.04104.pdf)
>  The Gumbel-softmax distribution, or Concrete distribution, is often used to relax the discrete characteristics of a categorical distribution and enable back-propagation through differentiable reparameterization. Although it reliably yields low variance gradients, it still relies on a stochastic sampling process for optimization. In this work, we present a relaxed categorical analytic bound (ReCAB), a novel divergence-like metric which corresponds to the upper bound of the Kullback-Leibler divergence (KLD) of a relaxed categorical distribution. The proposed metric is easy to implement because it has a closed form solution, and empirical results show that it is close to the actual KLD. Along with this new metric, we propose a relaxed categorical analytic bound variational autoencoder (ReCAB-VAE) that successfully models both continuous and relaxed discrete latent representations. We implement an emotional text-to-speech synthesis system based on the proposed framework, and show that the proposed system flexibly and stably controls emotion expressions with better speech quality compared to baselines that use stochastic estimation or categorical distribution approximation.      
### 22.Cascading failure model and robustness of heterogeneous interdependent combat network  [ :arrow_down: ](https://arxiv.org/pdf/2205.04099.pdf)
>  The networked combat system-of-system (CSOS) is the trend of combat development with the innovation of technology. The achievement of combat effectiveness requires CSOS to have a good ability to deal with external interference. From the perspective of complex networks, the modeling of CSOS is carried out, and the robustness of the combat network is explored based on this. Firstly, a more realistic double-layer heterogeneous interdependent combat network model is established. Then, the conditional group dependency situation is considered to design failure rules for dependent failure, and the coupling relation between the double-layer subnets is analysed for cascading failure. Furthermore, the initial load and capacity of the node are defined, respectively, as well as the load redistribution strategy and the status judgment rules for the cascading failure model. Simulation experiments are carried out by changing the attack modes and different parameters, and the results show that the robustness of the combat network can be effectively improved by improving the tolerance limit of one-way dependency of the functional net, the node capacity of the functional net and the tolerance of the overload state. The conclusions of this paper can provide a useful reference for the network structure optimization and network security protection in military field.      
### 23.Robust phase retrieval with complexity-guidance for coherent X-ray imaging  [ :arrow_down: ](https://arxiv.org/pdf/2205.04094.pdf)
>  Reconstruction of a stable and reliable solution from noisy incomplete Fourier intensity data recorded in a coherent X-ray imaging (CXI) experiment is a challenging problem. The Relaxed Averaged Alternating Reflections (RAAR) algorithm that is concluded with a number of Error Reduction (ER) iterations is a popular choice. The RAAR-ER algorithm is usually employed for several hundreds of times starting with independent random guesses to obtain trial solutions that are then averaged to obtain the phase retrieval transfer function (PRTF). In this paper, we examine the phase retrieval solution obtained using the RAAR-ER methodology from perspective of the complexity parameter that was introduced by us in recent works. We observe that a single run of the RAAR-ER algorithm produces a solution with higher complexity compared to what is expected based on the complexity parameter as manifested by spurious high frequency grainy artifacts in the solution that do not seem to go away completely even after a number of trial solutions are averaged. We then describe a CG-RAAR (Complexity Guided RAAR) phase retrieval method that can effectively address this inconsistency problem and provides artifact-free solutions. The CG-RAAR methodology is first illustrated with simulated unblocked noisy Fourier intensity data and later applied to centrally-blocked noisy cyanobacterium data which is available from the CXIDB database. Our simulation and experimental results using CG-RAAR suggest two important improvements over the popular RAAR-ER algorithm. The CG-RAAR solutions after the averaging procedure is more reliable in the sense that it contains smallest features consistent with the resolution estimated by the PRTF curve. Secondly, since the single run of the CG-RAAR solution does not have grainy artifacts, the number of trial solutions needed for the averaging process is reduced.      
### 24.PS-Net: Deep Partially Separable Modelling for Dynamic Magnetic Resonance Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2205.04073.pdf)
>  Deep learning methods driven by the low-rank regularization have achieved attractive performance in dynamic magnetic resonance (MR) imaging. However, most of these methods represent low-rank prior by hand-crafted nuclear norm, which cannot accurately approximate the low-rank prior over the entire dataset through a fixed regularization parameter. In this paper, we propose a learned low-rank method for dynamic MR imaging. In particular, we unrolled the semi-quadratic splitting method (HQS) algorithm for the partially separable (PS) model to a network, in which the low-rank is adaptively characterized by a learnable null-space transform. Experiments on the cardiac cine dataset show that the proposed model outperforms the state-of-the-art compressed sensing (CS) methods and existing deep learning methods both quantitatively and qualitatively.      
### 25.Masked Co-attentional Transformer reconstructs 100x ultra-fast/low-dose whole-body PET from longitudinal images and anatomically guided MRI  [ :arrow_down: ](https://arxiv.org/pdf/2205.04044.pdf)
>  Despite its tremendous value for the diagnosis, treatment monitoring and surveillance of children with cancer, whole body staging with positron emission tomography (PET) is time consuming and associated with considerable radiation exposure. 100x (1% of the standard clinical dosage) ultra-low-dose/ultra-fast whole-body PET reconstruction has the potential for cancer imaging with unprecedented speed and improved safety, but it cannot be achieved by the naive use of machine learning techniques. In this study, we utilize the global similarity between baseline and follow-up PET and magnetic resonance (MR) images to develop Masked-LMCTrans, a longitudinal multi-modality co-attentional CNN-Transformer that provides interaction and joint reasoning between serial PET/MRs of the same patient. We mask the tumor area in the referenced baseline PET and reconstruct the follow-up PET scans. In this manner, Masked-LMCTrans reconstructs 100x almost-zero radio-exposure whole-body PET that was not possible before. The technique also opens a new pathway for longitudinal radiology imaging reconstruction, a significantly under-explored area to date. Our model was trained and tested with Stanford PET/MRI scans of pediatric lymphoma patients and evaluated externally on PET/MRI images from Tübingen University. The high image quality of the reconstructed 100x whole-body PET images resulting from the application of Masked-LMCTrans will substantially advance the development of safer imaging approaches and shorter exam-durations for pediatric patients, as well as expand the possibilities for frequent longitudinal monitoring of these patients by PET.      
### 26.A Contraction-constrained Model Predictive Control for Nonlinear Processes using Disturbance Forecasts  [ :arrow_down: ](https://arxiv.org/pdf/2205.04033.pdf)
>  Model predictive control (MPC) has become the most widely used advanced control method in process industry. In many cases, forecasts of the disturbances are available, e.g., predicted renewable power generation based on weather forecast. While the predictions of disturbances may not be accurate, utilizing the information can significantly improve the control performance in response to the disturbances. By exploiting process and disturbance models, future system behaviour can be predicted and used to optimise control actions via minimisation of an economical cost function which incorporates these predictions. However, stability guarantee of the resulting closed-loop system is often difficult in this approach when the processes are nonlinear. Proposed in the following article is a contraction-constrained predictive controller which optimises process economy whilst ensuring stabilisation to operating targets subject to disturbance measurements and forecasts.      
### 27.Wiener filters on graphs and distributed polynomial approximation algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2205.04019.pdf)
>  In this paper, we consider Wiener filters to reconstruct deterministic and (wide-band) stationary graph signals from their observations corrupted by random noises, and we propose distributed algorithms to implement Wiener filters and inverse filters on networks in which agents are equipped with a data processing subsystem for limited data storage and computation power, and with a one-hop communication subsystem for direct data exchange only with their adjacent agents. The proposed distributed polynomial approximation algorithm is an exponential convergent quasi-Newton method based on Jacobi polynomial approximation and Chebyshev interpolation polynomial approximation to analytic functions on a cube. Our numerical simulations show that Wiener filtering procedure performs better on denoising (wide-band) stationary signals than the Tikhonov regularization approach does, and that the proposed polynomial approximation algorithms converge faster than the Chebyshev polynomial approximation algorithm and gradient decent algorithm do in the implementation of an inverse filtering procedure associated with a polynomial filter of commutative graph shifts.      
### 28.Personalized QoE Enhancement for Adaptive Video Streaming: A Digital Twin-Assisted Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2205.04014.pdf)
>  In this paper, we present a digital twin (DT)-assisted adaptive video streaming scheme to enhance personalized quality-of-experience (PQoE). Since PQoE models are user-specific and time-varying, existing schemes based on universal and time-invariant PQoE models may suffer from performance degradation. To address this issue, we first propose a DT-assisted PQoE model construction method to obtain accurate user-specific PQoE models. Specifically, user DTs (UDTs) are respectively constructed for individual users, which can acquire and utilize users' data to accurately tune PQoE model parameters in real time. Next, given the obtained PQoE models, we formulate a resource management problem to maximize the overall long-term PQoE by taking the dynamics of user' locations, video content requests, and buffer statuses into account. To solve this problem, a deep reinforcement learning algorithm is developed to jointly determine segment version selection, and communication and computing resource allocation. Simulation results on the real-world dataset demonstrate that the proposed scheme can effectively enhance PQoE compared with benchmark schemes.      
### 29.SELF-CARE: Selective Fusion with Context-Aware Low-Power Edge Computing for Stress Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.03974.pdf)
>  Detecting human stress levels and emotional states with physiological body-worn sensors is a complex task, but one with many health-related benefits. Robustness to sensor measurement noise and energy efficiency of low-power devices remain key challenges in stress detection. We propose SELFCARE, a fully wrist-based method for stress detection that employs context-aware selective sensor fusion that dynamically adapts based on data from the sensors. Our method uses motion to determine the context of the system and learns to adjust the fused sensors accordingly, improving performance while maintaining energy efficiency. SELF-CARE obtains state-of-the-art performance across the publicly available WESAD dataset, achieving 86.34% and 94.12% accuracy for the 3-class and 2-class classification problems, respectively. Evaluation on real hardware shows that our approach achieves up to 2.2x (3-class) and 2.7x (2-class) energy efficiency compared to traditional sensor fusion.      
### 30.An Interactive Annotation Tool for Perceptual Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2205.03969.pdf)
>  Human perception is at the core of lossy video compression and yet, it is challenging to collect data that is sufficiently dense to drive compression. In perceptual quality assessment, human feedback is typically collected as a single scalar quality score indicating preference of one distorted video over another. In reality, some videos may be better in some parts but not in others. We propose an approach to collecting finer-grained feedback by asking users to use an interactive tool to directly optimize for perceptual quality given a fixed bitrate. To this end, we built a novel web-tool which allows users to paint these spatio-temporal importance maps over videos. The tool allows for interactive successive refinement: we iteratively re-encode the original video according to the painted importance maps, while maintaining the same bitrate, thus allowing the user to visually see the trade-off of assigning higher importance to one spatio-temporal part of the video at the cost of others. We use this tool to collect data in-the-wild (10 videos, 17 users) and utilize the obtained importance maps in the context of x264 coding to demonstrate that the tool can indeed be used to generate videos which, at the same bitrate, look perceptually better through a subjective study - and are 1.9 times more likely to be preferred by viewers. The code for the tool and dataset can be found at <a class="link-external link-https" href="https://github.com/jenyap/video-annotation-tool.git" rel="external noopener nofollow">this https URL</a>      
### 31.A New Array Synthesizer Based on Slepian Functions  [ :arrow_down: ](https://arxiv.org/pdf/2205.03901.pdf)
>  This study introduces a new multi-antenna array synthesizer based on Slepian functions. The synthesizer concentrates beamforming (BF) gain within a spatial region (i.e., an angular sector), optimizing the spectral efficiency for the given region, which is suitable for codebook-based analog BF. Starting with the spectral efficiency formula incorporating the effect of BF, Jensen's inequality was used to set upper and lower bounds of the spectral efficiency. Then, a novel method was introduced to combine the bounds into an approximation called Hesham's approximation. Finally, Hesham's approximation was formulated into a solvable Slepian optimization problem that yielded the weights of the synthesizer. The properties of the synthesizer were listed, including a discussion of how it behaves by changing the width of the targeted region. The steering method was derived, and simulation results were presented. A useful finding from the thesis that derived a closed-form solution to the integral of the square of a radiation pattern was appended.      
### 32.Preservation of High Frequency Content for Deep Learning-Based Medical Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2205.03898.pdf)
>  Chest radiographs are used for the diagnosis of multiple critical illnesses (e.g., Pneumonia, heart failure, lung cancer), for this reason, systems for the automatic or semi-automatic analysis of these data are of particular interest. An efficient analysis of large amounts of chest radiographs can aid physicians and radiologists, ultimately allowing for better medical care of lung-, heart- and chest-related conditions. We propose a novel Discrete Wavelet Transform (DWT)-based method for the efficient identification and encoding of visual information that is typically lost in the down-sampling of high-resolution radiographs, a common step in computer-aided diagnostic pipelines. Our proposed approach requires only slight modifications to the input of existing state-of-the-art Convolutional Neural Networks (CNNs), making it easily applicable to existing image classification frameworks. We show that the extra high-frequency components offered by our method increased the classification performance of several CNNs in benchmarks employing the NIH Chest-8 and ImageNet-2017 datasets. Based on our results we hypothesize that providing frequency-specific coefficients allows the CNNs to specialize in the identification of structures that are particular to a frequency band, ultimately increasing classification performance, without an increase in computational load. The implementation of our work is available at <a class="link-external link-http" href="http://github.com/DeclanMcIntosh/LeGallCuda" rel="external noopener nofollow">this http URL</a>.      
### 33.Demo: Real-Time Semantic Communications with a Vision Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2205.03886.pdf)
>  Semantic communications are expected to enable the more effective delivery of meaning rather than a precise transfer of symbols. In this paper, we propose an end-to-end deep neural network-based architecture for image transmission and demonstrate its feasibility in a real-time wireless channel by implementing a prototype based on a field-programmable gate array (FPGA). We demonstrate that this system outperforms the traditional 256-quadrature amplitude modulation system in the low signal-to-noise ratio regime with the popular CIFAR-10 dataset. To the best of our knowledge, this is the first work that implements and investigates real-time semantic communications with a vision transformer.      
### 34.WKGM: Weight-K-space Generative Model for Parallel Imaging Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2205.03883.pdf)
>  Parallel Imaging (PI) is one of the most im-portant and successful developments in accelerating magnetic resonance imaging (MRI). Recently deep learning PI has emerged as an effective technique to accelerate MRI. Nevertheless, most approaches have so far been based image domain. In this work, we propose to explore the k-space domain via robust generative modeling for flexible PI reconstruction, coined weight-k-space generative model (WKGM). Specifically, WKGM is a generalized k-space domain model, where the k-space weighting technology and high-dimensional space strategy are efficiently incorporated for score-based generative model training, resulting in good and robust reconstruction. In addition, WKGM is flexible and thus can synergistically combine various traditional k-space PI models, generating learning-based priors to produce high-fidelity reconstructions. Experimental results on datasets with varying sampling patterns and acceleration factors demonstrate that WKGM can attain state-of-the-art reconstruction results under the well-learned k-space generative prior.      
### 35.Majorization-Minimization based Hybrid Localization Method for High Precision Localization in Wireless Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.03881.pdf)
>  This paper investigates the hybrid source localization problem using the four radio measurements - time of arrival (TOA), time difference of arrival (TDOA), received signal strength (RSS) and angle of arrival (AOA). First, after invoking tractable approximations in the RSS and AOA models, the maximum likelihood estimation (MLE) problem for the hybrid TOA-TDOA-RSS-AOA data model is derived. Then, in the MLE, which has the least-squares objective, weights determined using the range-based characteristics of the four heterogeneous measurements, are introduced. The resultant weighted least-squares problem obtained, which is non-smooth and non-convex, is solved using the principle of the majorization-minimization (MM), leading to an iterative algorithm that has a guaranteed convergence. The key feature of the proposed method is that it provides a unified framework where localization using any possible merger out of these four measurements can be implemented as per the requirement/application. Extensive numerical simulations are conducted to study the estimation efficiency of the proposed method. The proposed method employing all four measurements is compared against a conventionally used method and also against the proposed method employing only limited combinations of the four measurements. The results obtained indicate that the hybrid localization model improves the localization accuracy compared to the heterogeneous measurements. The integration of different measurements also yields good accuracy in the presence of non-line of sight (NLOS) errors.      
### 36.Hierarchical Dirichlet Process Based Gamma Mixture Modelling for Terahertz Band Wireless Communication Channels  [ :arrow_down: ](https://arxiv.org/pdf/2205.03812.pdf)
>  Due to the unique channel characteristics of Terahertz (THz), comprehensive propagation channel modeling is essential to understand the spectrum and to develop reliable communication systems in these bands. Ray tracing and traditional statistical modeling are insufficient to construct a suitable channel model due to the wide bandwidth and rapid changes in the characteristics of THz channels. In this work, we propose the utilization of hierarchical Dirichlet Process Gamma Mixture Model (DPGMM) to characterize THz channels statistically in the absence of any prior knowledge. DPGMM provides mixture component parameters and the required number of components. A revised expectation-maximization (EM) algorithm is also proposed as a pre-step for DPGMM. Kullback-Leibler Divergence (KL-divergence) is utilized as an error metric to examine the amount of inaccuracy of the EM algorithm and DPGMM when modeling the experimental probability density functions (PDFs). DPGMM and EM algorithm are implemented over the measurements taken at frequencies between 240 GHz and 300 GHz. By comparing the results of the DPGMM and EM algorithms for the measurement datasets, we demonstrate how well the DPGMM fits the target distribution. It is shown that the proposed DPGMM can accurately describe the various THz channels as good as the EM algorithm, and its flexibility allows it to represent more complex distributions better than the EM algorithm. We also demonstrated that DPGMM can be used to model any wireless channel due to its versatility.      
### 37.Fast and Structured Block-Term Tensor Decomposition For Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2205.03798.pdf)
>  The block-term tensor decomposition model with multilinear rank-$(L_r,L_r,1)$ terms (or, the "LL1 tensor decomposition" in short) offers a valuable alternative for hyperspectral unmixing (HU) under the linear mixture model. Particularly, the LL1 decomposition ensures the endmember/abundance identifiability in scenarios where such guarantees are not supported by the classic matrix factorization (MF) approaches. However, existing LL1-based HU algorithms use a three-factor parameterization of the tensor (i.e., the hyperspectral image cube), which leads to a number of challenges including high per-iteration complexity, slow convergence, and difficulties in incorporating structural prior information. This work puts forth an LL1 tensor decomposition-based HU algorithm that uses a constrained two-factor re-parameterization of the tensor data. As a consequence, a two-block alternating gradient projection (GP)-based LL1 algorithm is proposed for HU. With carefully designed projection solvers, the GP algorithm enjoys a relatively low per-iteration complexity. Like in MF-based HU, the factors under our parameterization correspond to the endmembers and abundances. Thus, the proposed framework is natural to incorporate physics-motivated priors that arise in HU. The proposed algorithm often attains orders-of-magnitude speedup and substantial HU performance gains compared to the existing three-factor parameterization-based HU algorithms.      
### 38.Learning Regionally Decentralized AC Optimal Power Flows with ADMM  [ :arrow_down: ](https://arxiv.org/pdf/2205.03787.pdf)
>  One potential future for the next generation of smart grids is the use of decentralized optimization algorithms and secured communications for coordinating renewable generation (e.g., wind/solar), dispatchable devices (e.g., coal/gas/nuclear generations), demand response, battery &amp; storage facilities, and topology optimization. The Alternating Direction Method of Multipliers (ADMM) has been widely used in the community to address such decentralized optimization problems and, in particular, the AC Optimal Power Flow (AC-OPF). This paper studies how machine learning may help in speeding up the convergence of ADMM for solving AC-OPF. It proposes a novel decentralized machine-learning approach, namely ML-ADMM, where each agent uses deep learning to learn the consensus parameters on the coupling branches. The paper also explores the idea of learning only from ADMM runs that exhibit high-quality convergence properties, and proposes filtering mechanisms to select these runs. Experimental results on test cases based on the French system demonstrate the potential of the approach in speeding up the convergence of ADMM significantly.      
### 39.Decoupled-and-Coupled Networks: Self-Supervised Hyperspectral Image Super-Resolution with Subpixel Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2205.03742.pdf)
>  Enormous efforts have been recently made to super-resolve hyperspectral (HS) images with the aid of high spatial resolution multispectral (MS) images. Most prior works usually perform the fusion task by means of multifarious pixel-level priors. Yet the intrinsic effects of a large distribution gap between HS-MS data due to differences in the spatial and spectral resolution are less investigated. The gap might be caused by unknown sensor-specific properties or highly-mixed spectral information within one pixel (due to low spatial resolution). To this end, we propose a subpixel-level HS super-resolution framework by devising a novel decoupled-and-coupled network, called DC-Net, to progressively fuse HS-MS information from the pixel- to subpixel-level, from the image- to feature-level. As the name suggests, DC-Net first decouples the input into common (or cross-sensor) and sensor-specific components to eliminate the gap between HS-MS images before further fusion, and then fully blends them by a model-guided coupled spectral unmixing (CSU) net. More significantly, we append a self-supervised learning module behind the CSU net by guaranteeing the material consistency to enhance the detailed appearances of the restored HS product. Extensive experimental results show the superiority of our method both visually and quantitatively and achieve a significant improvement in comparison with the state-of-the-arts. Furthermore, the codes and datasets will be available at <a class="link-external link-https" href="https://sites.google.com/view/danfeng-hong" rel="external noopener nofollow">this https URL</a> for the sake of reproducibility.      
### 40.Optimal Lighting Control in Greenhouses Using Bayesian Neural Networks for Sunlight Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2205.03733.pdf)
>  Controlling the environmental parameters, including light in greenhouses, increases the crop yield; however, the electricity cost of supplemental lighting can be high. Therefore, the importance of applying cost-effective lighting methods arises. In this paper, an optimal supplemental lighting control approach is developed considering a variational inference Bayesian Neural Network (BNN) model for sunlight prediction. The predictive model is validated through testing the model on the historical solar data of a site located at North Carolina ($R^{2}$=0.9971, RMSE=1.8%). The proposed lighting approach is shown to minimize electricity cost by considering the BNN-based sunlight prediction, plant light needs, and variable electricity pricing when solving the underlying optimization problem. For evaluation, the new strategy is compared to: 1) a Markov-based prediction method, which solves the same optimization problem, assuming a Markov model for sunlight prediction; 2) a heuristic method which aims to supply a fixed amount of light. Simulation studies are conducted to examine the electricity cost improvements of the BNN-based approach. The results show that the BNN-based approach reduces cost by (on average) 2.27% and 43.91% compared to the Markov prediction-based method and the heuristic method, respectively, throughout a year.      
### 41.Block Modulating Video Compression: An Ultra Low Complexity Image Compression Encoder for Resource Limited Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2205.03677.pdf)
>  We consider the image and video compression on resource limited platforms. An ultra low-cost image encoder, named Block Modulating Video Compression (BMVC) with an encoding complexity ${\cal O}(1)$ is proposed to be implemented on mobile platforms with low consumption of power and computation resources. We also develop two types of BMVC decoders, implemented by deep neural networks. The first BMVC decoder is based on the Plug-and-Play (PnP) algorithm, which is flexible to different compression ratios. And the second decoder is a memory efficient end-to-end convolutional neural network, which aims for real-time decoding. Extensive results on the high definition images and videos demonstrate the superior performance of the proposed codec and the robustness against bit quantization.      
### 42.A co-design method of online learning SMC law via an input-mappping strategy  [ :arrow_down: ](https://arxiv.org/pdf/2205.03652.pdf)
>  The research on sliding mode control strategy is generally based on the robust approach. The larger parameter space consideration will inevitably sacrifice part of the performance. Recently, the data-driven sliding mode control method attracts much attention and shows excellent benefits in the fact that data is introduced to compensate the controller. Nevertheless, most of the research on data-driven sliding mode control relied on identification techniques, which limits its online applications due to the special requirements of the data. In this paper, an input-mapping technique is inserted into the design framework of sliding mode control to compensate for the influence generated by the unknown dynamic of the system. The major novelty of the proposed input-mapping sliding mode control strategy lies in that the sliding mode surface and the sliding mode controller are co-designed through online learning from historical input-output data to minimize an objective function. Then, the convergence rate of the system is improved significantly based on the method designed in this work. Finally, some simulations are provided to show the effectiveness and superiority of the proposed methods.      
### 43.Label Adversarial Learning for Skeleton-level to Pixel-level Adjustable Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2205.03646.pdf)
>  You can have your cake and eat it too. Microvessel segmentation in optical coherence tomography angiography (OCTA) images remains challenging. Skeleton-level segmentation shows clear topology but without diameter information, while pixel-level segmentation shows a clear caliber but low topology. To close this gap, we propose a novel label adversarial learning (LAL) for skeleton-level to pixel-level adjustable vessel segmentation. LAL mainly consists of two designs: a label adversarial loss and an embeddable adjustment layer. The label adversarial loss establishes an adversarial relationship between the two label supervisions, while the adjustment layer adjusts the network parameters to match the different adversarial weights. Such a design can efficiently capture the variation between the two supervisions, making the segmentation continuous and tunable. This continuous process allows us to recommend high-quality vessel segmentation with clear caliber and topology. Experimental results show that our results outperform manual annotations of current public datasets and conventional filtering effects. Furthermore, such a continuous process can also be used to generate an uncertainty map representing weak vessel boundaries and noise.      
### 44.Deep Reinforcement Learning-Based Adaptive IRS Control with Limited Feedback Codebooks  [ :arrow_down: ](https://arxiv.org/pdf/2205.03636.pdf)
>  Intelligent reflecting surfaces (IRS) consist of configurable meta-atoms, which can alter the wireless propagation environment through design of their reflection coefficients. We consider adaptive IRS control in the practical setting where (i) the IRS reflection coefficients are attained by adjusting tunable elements embedded in the meta-atoms, (ii) the IRS reflection coefficients are affected by the incident angles of the incoming signals, (iii) the IRS is deployed in multi-path, time-varying channels, and (iv) the feedback link from the base station (BS) to the IRS has a low data rate. Conventional optimization-based IRS control protocols, which rely on channel estimation and conveying the optimized variables to the IRS, are not practical in this setting due to the difficulty of channel estimation and the low data rate of the feedback channel. To address these challenges, we develop a novel adaptive codebook-based limited feedback protocol to control the IRS. We propose two solutions for adaptive IRS codebook design: (i) random adjacency (RA), which utilizes correlations across the channel realizations, and (ii) deep neural network policy-based IRS control (DPIC), which is based on a deep reinforcement learning. Numerical evaluations show that the data rate and average data rate over one coherence time are improved substantially by the proposed schemes.      
### 45.Deep Quality Assessment of Compressed Videos: A Subjective and Objective Study  [ :arrow_down: ](https://arxiv.org/pdf/2205.03630.pdf)
>  In the video coding process, the perceived quality of a compressed video is evaluated by full-reference quality evaluation metrics. However, it is difficult to obtain reference videos with perfect quality. To solve this problem, it is critical to design no-reference compressed video quality assessment algorithms, which assists in measuring the quality of experience on the server side and resource allocation on the network side. Convolutional Neural Network (CNN) has shown its advantage in Video Quality Assessment (VQA) with promising successes in recent years. A large-scale quality database is very important for learning accurate and powerful compressed video quality metrics. In this work, a semi-automatic labeling method is adopted to build a large-scale compressed video quality database, which allows us to label a large number of compressed videos with manageable human workload. The resulting Compressed Video quality database with Semi-Automatic Ratings (CVSAR), so far the largest of compressed video quality database. We train a no-reference compressed video quality assessment model with a 3D CNN for SpatioTemporal Feature Extraction and Evaluation (STFEE). Experimental results demonstrate that the proposed method outperforms state-of-the-art metrics and achieves promising generalization performance in cross-database tests. The CVSAR database and STFEE model will be made publicly available to facilitate reproducible research.      
### 46.A Probabilistic Framework for Power System Large-Disturbance Global Instability Risk Assessment in the Presence of Renewable Wind Generation  [ :arrow_down: ](https://arxiv.org/pdf/2205.03629.pdf)
>  The increasing demand of large scale wind integration in the conventional power system brings a lot of challenges. One of them is the stability of the power system when subjected to a large disturbance, such as a fault. This paper proposes a probabilistic risk-based framework for computing a global instability index, incorporating angle, voltage, and frequency stability, for a large disturbance. Moreover, the impact of high wind penetration on this index is also observed. Case studies and associated simulations are conducted on the IEEE 39-bus test system using DIgSILENT PowerFactory software. The results show that higher penetration of wind generation enhances the global stability of the power system. Moreover, the impact of changing system generation and load is studied on the global instability index.      
### 47.BrainIB: Interpretable Brain Network-based Psychiatric Diagnosis with Graph Information Bottleneck  [ :arrow_down: ](https://arxiv.org/pdf/2205.03612.pdf)
>  Developing a new diagnostic models based on the underlying biological mechanisms rather than subjective symptoms for psychiatric disorders is an emerging consensus. Recently, machine learning-based classifiers using functional connectivity (FC) for psychiatric disorders and healthy controls are developed to identify brain markers. However, existing machine learningbased diagnostic models are prone to over-fitting (due to insufficient training samples) and perform poorly in new test environment. Furthermore, it is difficult to obtain explainable and reliable brain biomarkers elucidating the underlying diagnostic decisions. These issues hinder their possible clinical applications. In this work, we propose BrainIB, a new graph neural network (GNN) framework to analyze functional magnetic resonance images (fMRI), by leveraging the famed Information Bottleneck (IB) principle. BrainIB is able to identify the most informative regions in the brain (i.e., subgraph) and generalizes well to unseen data. We evaluate the performance of BrainIB against 6 popular brain network classification methods on two multi-site, largescale datasets and observe that our BrainIB always achieves the highest diagnosis accuracy. It also discovers the subgraph biomarkers which are consistent to clinical and neuroimaging findings.      
### 48.A Hybrid Trim Strategy for Coaxial Compound Helicopter  [ :arrow_down: ](https://arxiv.org/pdf/2205.03609.pdf)
>  Interest in the coaxial compound helicopter (CCH) has been increasing in the civil aviation and engineering community for its high-speed and high-maneuverability features, and is likely to continue to do so for the foreseeable future. Since the control in CCH is totally different from the conventional helicopter, the redundant control strategy design is one of the biggest challenges. In this study, the CCH model based on XH-59A is built to investigate the impact of the propeller and the elevator on the flight performance. Four trim strategies with different objectives are proposed and then compared to find the optimal control allocation. A heuristic descent search method is applied to search the optimal velocity at which the propeller and the elevator are engaged. A significant improvement of power required at medium and high speed with acceptable rotor airloads increment was found by using the Hybrid Trim strategy in the speed range of 0-100m/s, with regard to a pre-configured pitch angle schedule. The corresponding control variables obtained locate in a reasonable control range, with a maximum power reduced of 13% at 100m/s, which showcases the potential of the Hybrid Trim strategy.      
### 49.Multi-View Video Coding with GAN Latent Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.03599.pdf)
>  The introduction of multiple viewpoints inevitably increases the bitrates to store and transmit video scenes. To reduce the compressed bitrates, researchers have developed to skip intermediate viewpoints during compression and delivery, and finally reconstruct them with Side Information (SI). Generally, the depth maps can be utilized to construct SI; however, it shows inferior performance with inaccurate reconstruction or high bitrates. In this paper, we propose a multi-view video coding based on SI of Generative Adversarial Network (GAN). At the encoder, we construct a spatio-temporal Epipolar Plane Image (EPI) and further utilize convolutional network to extract the latent code of GAN as SI; while at the decoder side, we combine the SI and adjacent viewpoints to reconstruct intermediate views by the generator of GAN. In particular, we set a joint encoder constraint of reconstruction cost and SI entropy, in order to achieve an optimal tradeoff between reconstruction quality and bitrate overhead. Experiments show a significantly improved Rate-Distortion (RD) performance compared with the state-of-the-art methods.      
### 50.Acoustic echo suppression using a learning-based multi-frame minimum variance distortionless response filter  [ :arrow_down: ](https://arxiv.org/pdf/2205.03594.pdf)
>  Distortion resulting from acoustic echo suppression (AES) is a common issue in full-duplex communication. To address the distortion problem, a multi-frame minimum variance distortionless response (MFMVDR) filtering technique is proposed. The MFMVDR filter with parameter estimation which was used in speech enhancement problems is extended in this study from a deep learning perspective. To alleviate numerical instability of the MFMVDR filter, we propose to directly estimate the inverse of the correlation matrix. The AES system is advantageous in that no double-talk detection is required. The negative scale-invariant signal-to-distortion ratio is employed as the loss function in training the network at the output of the MFMVDR filter. Simulation results have demonstrated the efficacy of the proposed learning-based AES system in double-talk, background noise, and nonlinear distortion conditions.      
### 51.Efficient VVC Intra Prediction Based on Deep Feature Fusion and Probability Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2205.03587.pdf)
>  The ever-growing multimedia traffic has underscored the importance of effective multimedia codecs. Among them, the up-to-date lossy video coding standard, Versatile Video Coding (VVC), has been attracting attentions of video coding community. However, the gain of VVC is achieved at the cost of significant encoding complexity, which brings the need to realize fast encoder with comparable Rate Distortion (RD) performance. In this paper, we propose to optimize the VVC complexity at intra-frame prediction, with a two-stage framework of deep feature fusion and probability estimation. At the first stage, we employ the deep convolutional network to extract the spatialtemporal neighboring coding features. Then we fuse all reference features obtained by different convolutional kernels to determine an optimal intra coding depth. At the second stage, we employ a probability-based model and the spatial-temporal coherence to select the candidate partition modes within the optimal coding depth. Finally, these selected depths and partitions are executed whilst unnecessary computations are excluded. Experimental results on standard database demonstrate the superiority of proposed method, especially for High Definition (HD) and Ultra-HD (UHD) video sequences.      
### 52.SPQE: Structure-and-Perception-Based Quality Evaluation for Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2205.03584.pdf)
>  The image Super-Resolution (SR) technique has greatly improved the visual quality of images by enhancing their resolutions. It also calls for an efficient SR Image Quality Assessment (SR-IQA) to evaluate those algorithms or their generated images. In this paper, we focus on the SR-IQA under deep learning and propose a Structure-and-Perception-based Quality Evaluation (SPQE). In emerging deep-learning-based SR, a generated high-quality, visually pleasing image may have different structures from its corresponding low-quality image. In such case, how to balance the quality scores between no-reference perceptual quality and referenced structural similarity is a critical issue. To help ease this problem, we give a theoretical analysis on this tradeoff and further calculate adaptive weights for the two types of quality scores. We also propose two deep-learning-based regressors to model the no-reference and referenced scores. By combining the quality scores and their weights, we propose a unified SPQE metric for SR-IQA. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts in different datasets.      
### 53.Mask-based Neural Beamforming for Moving Speakers with Self-Attention-based Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2205.03568.pdf)
>  Beamforming is a powerful tool designed to enhance speech signals from the direction of a target source. Computing the beamforming filter requires estimating spatial covariance matrices (SCMs) of the source and noise signals. Time-frequency masks are often used to compute these SCMs. Most studies of mask-based beamforming have assumed that the sources do not move. However, sources often move in practice, which causes performance degradation. In this paper, we address the problem of mask-based beamforming for moving sources. We first review classical approaches to tracking a moving source, which perform online or blockwise computation of the SCMs. We show that these approaches can be interpreted as computing a sum of instantaneous SCMs weighted by attention weights. These weights indicate which time frames of the signal to consider in the SCM computation. Online or blockwise computation assumes a heuristic and deterministic way of computing these attention weights that, although simple, may not result in optimal performance. We thus introduce a learning-based framework that computes optimal attention weights for beamforming. We achieve this using a neural network implemented with self-attention layers. We show experimentally that our proposed framework can greatly improve beamforming performance in moving source situations while maintaining high performance in non-moving situations, thus enabling the development of mask-based beamformers robust to source movements.      
### 54.Realizing Ultra-Fast and Energy-Efficient Baseband Processing Using Analogue Resistive Switching Memory  [ :arrow_down: ](https://arxiv.org/pdf/2205.03561.pdf)
>  To support emerging applications ranging from holographic communications to extended reality, next-generation mobile wireless communication systems require ultra-fast and energy-efficient (UFEE) baseband processors. Traditional complementary metal-oxide-semiconductor (CMOS)-based baseband processors face two challenges in transistor scaling and the von Neumann bottleneck. To address these challenges, in-memory computing-based baseband processors using resistive random-access memory (RRAM) present an attractive solution. In this paper, we propose and demonstrate RRAM-based in-memory baseband processing for the widely adopted multiple-input-multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) air interface. Its key feature is to execute the key operations, including discrete Fourier transform (DFT) and MIMO detection using linear minimum mean square error (L-MMSE) and zero forcing (ZF), in one-step. In addition, RRAM-based channel estimation as well as mapper/demapper modules are proposed. By prototyping and simulations, we demonstrate that the RRAM-based full-fledged communication system can significantly outperform its CMOS-based counterpart in terms of speed and energy efficiency by $10^3$ and $10^6$ times, respectively. The results pave a potential pathway for RRAM-based in-memory computing to be implemented in the era of the sixth generation (6G) mobile communications.      
### 55.I Can Read Your Mind: Control Mechanism Secrecy of Networked Dynamical Systems under Inference Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2205.03556.pdf)
>  Recent years have witnessed the fast advance of security research for networked dynamical system (NDS). Considering the latest inference attacks that enable stealthy and precise attacks into NDSs with observation-based learning, this article focuses on a new security aspect, i.e., how to protect control mechanism secrets from inference attacks, including state information, interaction structure and control laws. We call this security property as control mechanism secrecy, which provides protection of the vulnerabilities in the control process and fills the defense gap that traditional cyber security cannot handle. Since the knowledge of control mechanism defines the capabilities to implement attacks, ensuring control mechanism secrecy needs to go beyond the conventional data privacy to cover both transmissible data and intrinsic models in NDSs. The prime goal of this article is to summarize recent results of both inference attacks on control mechanism secrets and countermeasures. We first introduce the basic inference attack methods on the state and structure of NDSs, respectively, along with their inference performance bounds. Then, the corresponding countermeasures and performance metrics are given to illustrate how to preserve the control mechanism secrecy. Necessary conditions are derived to guide the secrecy design. Finally, thorough discussions on the control laws and open issues are presented, beckoning future investigation on reliable countermeasure design and tradeoffs between the secrecy and control performance.      
### 56.Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2205.03524.pdf)
>  Due to the sophisticated imaging process, an identical scene captured by different cameras could exhibit distinct imaging patterns, introducing distinct proficiency among the super-resolution (SR) models trained on images from different devices. In this paper, we investigate a novel and practical task coded cross-device SR, which strives to adapt a real-world SR model trained on the paired images captured by one camera to low-resolution (LR) images captured by arbitrary target devices. The proposed task is highly challenging due to the absence of paired data from various imaging devices. To address this issue, we propose an unsupervised domain adaptation mechanism for real-world SR, named Dual ADversarial Adaptation (DADA), which only requires LR images in the target domain with available real paired data from a source camera. DADA employs the Domain-Invariant Attention (DIA) module to establish the basis of target model training even without HR supervision. Furthermore, the dual framework of DADA facilitates an Inter-domain Adversarial Adaptation (InterAA) in one branch for two LR input images from two domains, and an Intra-domain Adversarial Adaptation (IntraAA) in two branches for an LR input image. InterAA and IntraAA together improve the model transferability from the source domain to the target. We empirically conduct experiments under six Real to Real adaptation settings among three different cameras, and achieve superior performance compared with existing state-of-the-art approaches. We also evaluate the proposed DADA to address the adaptation to the video camera, which presents a promising research topic to promote the wide applications of real-world super-resolution. Our source code is publicly available at <a class="link-external link-https" href="https://github.com/lonelyhope/DADA.git" rel="external noopener nofollow">this https URL</a>.      
### 57.Unsupervised Deep Unrolled Reconstruction Using Regularization by Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2205.03519.pdf)
>  Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable unsupervised learning for MR image reconstruction by combining an unsupervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of unsupervised learning by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the proposed method requires a reduced amount of training data to achieve high reconstruction quality.      
### 58.Digital Twin Framework for Time to Failure Forecasting of Wind Turbine Gearbox: A Concept  [ :arrow_down: ](https://arxiv.org/pdf/2205.03513.pdf)
>  Wind turbine is a complex machine with its rotating and non-rotating equipment being sensitive to faults. Due to increased wear and tear, the maintenance aspect of a wind turbine is of critical importance. Unexpected failure of wind turbine components can lead to increased O\&amp;M costs which ultimately reduces effective power capture of a wind farm. Fault detection in wind turbines is often supplemented with SCADA data available from wind farm operators in the form of time-series format with a 10-minute sample interval. Moreover, time-series analysis and data representation has become a powerful tool to get a deeper understating of the dynamic processes in complex machinery like wind turbine. Wind turbine SCADA data is usually available in form of a multivariate time-series with variables like gearbox oil temperature, gearbox bearing temperature, nacelle temperature, rotor speed and active power produced. In this preprint, we discuss the concept of a digital twin for time to failure forecasting of the wind turbine gearbox where a predictive module continuously gets updated with real-time SCADA data and generates meaningful insights for the wind farm operator.      
### 59.Reinforcement Learning Approach to Estimation in Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.03504.pdf)
>  This paper addresses two important estimation problems for linear systems, namely system identification and model-free state estimation. Our focus is on ARMAX models with unknown parameters. We first provide a reinforcement learning algorithm for system identification with guaranteed consistency. This algorithm is then used to provide a novel solution to model-free state estimation. These results are then applied to solving the model-free LQG control problem in the reinforcement learning setting.      
### 60.A Conformer-based Waveform-domain Neural Acoustic Echo Canceller Optimized for ASR Accuracy  [ :arrow_down: ](https://arxiv.org/pdf/2205.03481.pdf)
>  Acoustic Echo Cancellation (AEC) is essential for accurate recognition of queries spoken to a smart speaker that is playing out audio. Previous work has shown that a neural AEC model operating on log-mel spectral features (denoted "logmel" hereafter) can greatly improve Automatic Speech Recognition (ASR) accuracy when optimized with an auxiliary loss utilizing a pre-trained ASR model encoder. In this paper, we develop a conformer-based waveform-domain neural AEC model inspired by the "TasNet" architecture. The model is trained by jointly optimizing Negative Scale-Invariant SNR (SISNR) and ASR losses on a large speech dataset. On a realistic rerecorded test set, we find that cascading a linear adaptive AEC and a waveform-domain neural AEC is very effective, giving 56-59% word error rate (WER) reduction over the linear AEC alone. On this test set, the 1.6M parameter waveform-domain neural AEC also improves over a larger 6.5M parameter logmel-domain neural AEC model by 20-29% in easy to moderate conditions. By operating on smaller frames, the waveform neural model is able to perform better at smaller sizes and is better suited for applications where memory is limited.      
### 61.Energy-efficient Connected Cruise Control with Lean Penetration of Connected Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2205.03473.pdf)
>  This paper focuses on energy-efficient longitudinal controller design for a connected automated truck that travels in mixed traffic consisting of connected and non-connected vehicles. The truck has access to information about connected vehicles beyond line of sight using vehicle-to-vehicle (V2V) communication. A novel connected cruise control design is proposed which incorporates additional delays into the control law when responding to distant connected vehicles to account for the finite propagation of traffic waves. The speeds of non-connected vehicles are modeled as stochastic processes. A fundamental theorem is proven which links the spectral properties of the motion signals to the average energy consumption. This enables us to tune controller parameters and maximize energy efficiency. Simulations with synthetic data and real traffic data are used to demonstrate the energy efficiency of the control design. It is demonstrated that even with lean penetration of connected vehicles, our controller can bring significant energy savings.      
### 62.Power Control of Grid-Forming Converters Based on Full-State Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2205.03465.pdf)
>  The active and reactive power controllers of grid-forming converters are traditionally designed separately, which relies on the assumption of loop decoupling. This paper proposes a full-state feedback control for the power loops of grid-forming converters. First, the power loops are modeled considering their natural coupling, which, therefore, can apply to all kinds of line impedance, i.e., resistive, inductive, or complex. Then a full-state feedback control design is used. By this way, the eigenvalues of the system can be arbitrarily placed to any positions in the timescale of power loops. Therefore, the parameters can be directly chosen by the predefined specifications. A step-by-step parameters design procedure is also given in this paper. Experimental results verify the proposed method.      
### 63.Global Multi-modal 2D/3D Registration via Local Descriptors Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.03439.pdf)
>  Multi-modal registration is a required step for many image-guided procedures, especially ultrasound-guided interventions that require anatomical context. While a number of such registration algorithms are already available, they all require a good initialization to succeed due to the challenging appearance of ultrasound images and the arbitrary coordinate system they are acquired in. In this paper, we present a novel approach to solve the problem of registration of an ultrasound sweep to a pre-operative image. We learn dense keypoint descriptors from which we then estimate the registration. We show that our method overcomes the challenges inherent to registration tasks with freehand ultrasound sweeps, namely, the multi-modality and multidimensionality of the data in addition to lack of precise ground truth and low amounts of training examples. We derive a registration method that is fast, generic, fully automatic, does not require any initialization and can naturally generate visualizations aiding interpretability and explainability. Our approach is evaluated on a clinical dataset of paired MR volumes and ultrasound sequences.      
### 64.VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2205.03409.pdf)
>  Most of the existing video face super-resolution (VFSR) methods are trained and evaluated on VoxCeleb1, which is designed specifically for speaker identification and the frames in this dataset are of low quality. As a consequence, the VFSR models trained on this dataset can not output visual-pleasing results. In this paper, we develop an automatic and scalable pipeline to collect a high-quality video face dataset (VFHQ), which contains over $16,000$ high-fidelity clips of diverse interview scenarios. To verify the necessity of VFHQ, we further conduct experiments and demonstrate that VFSR models trained on our VFHQ dataset can generate results with sharper edges and finer textures than those trained on VoxCeleb1. In addition, we show that the temporal information plays a pivotal role in eliminating video consistency issues as well as further improving visual performance. Based on VFHQ, by analyzing the benchmarking study of several state-of-the-art algorithms under bicubic and blind settings. See our project page: <a class="link-external link-https" href="https://liangbinxie.github.io/projects/vfhq" rel="external noopener nofollow">this https URL</a>      
### 65.A High-Resolution Chest CT-Scan Image Dataset for COVID-19 Diagnosis and Differentiation  [ :arrow_down: ](https://arxiv.org/pdf/2205.03408.pdf)
>  During the COVID-19 pandemic, computed tomography (CT) is a good way to diagnose COVID-19 patients. HRCT (High-Resolution Computed Tomography) is a form of computed tomography that uses advanced methods to improve image resolution. Publicly accessible COVID-19 CT image datasets are very difficult to come by due to privacy concerns, which impedes the study and development of AI-powered COVID-19 diagnostic algorithms based on CT images. To address this problem, we have introduced HRCTv1-COVID-19, a new COVID-19 high resolution chest CT Scan image dataset that includes not only COVID-19 cases of Ground Glass Opacity (GGO), Crazy Paving, and Air Space Consolidation, but also CT images of cases with negative COVID-19. The HRCTv1-COVID-19 dataset, which includes slice-level, and patient-level labels, has the potential to aid COVID-19 research, especially for diagnosis and differentiation using artificial intelligence algorithms, machine learning and deep learning methods. This dataset is accessible through web at: <a class="link-external link-http" href="http://databiox.com" rel="external noopener nofollow">this http URL</a> and includes 181,106 chest HRCT images from 395 patients with four labels: GGO, Crazy Paving, Air Space Consolidation and Negative. <br>Keywords- Dataset, COVID-19, CT-Scan, Computed Tomography, Medical Imaging, Chest Image.      
### 66.Fatigue Prediction in Outdoor Running Conditions using Audio Data  [ :arrow_down: ](https://arxiv.org/pdf/2205.04343.pdf)
>  Although running is a common leisure activity and a core training regiment for several athletes, between $29\%$ and $79\%$ of runners sustain an overuse injury each year. These injuries are linked to excessive fatigue, which alters how someone runs. In this work, we explore the feasibility of modelling the Borg received perception of exertion (RPE) scale (range: $[6-20]$), a well-validated subjective measure of fatigue, using audio data captured in realistic outdoor environments via smartphones attached to the runners' arms. Using convolutional neural networks (CNNs) on log-Mel spectrograms, we obtain a mean absolute error of $2.35$ in subject-dependent experiments, demonstrating that audio can be effectively used to model fatigue, while being more easily and non-invasively acquired than by signals from other sensors.      
### 67.Competition and Cooperation of Autonomous Ridepooling Services: Game-Based Simulation of a Broker Concept  [ :arrow_down: ](https://arxiv.org/pdf/2205.04319.pdf)
>  Autonomous mobility on demand services have the potential to disrupt the future mobility system landscape. Ridepooling services in particular can decrease land consumption and increase transportation efficiency by increasing the average vehicle occupancy. Nevertheless, because ridepooling services require a sufficient user base for pooling to take effect, their performance can suffer if multiple operators offer such a service and must split the demand. This study presents a simulation framework for evaluating the impact of competition and cooperation among multiple ridepooling providers. Two different kinds of interaction via a broker platform are compared with the base cases of a single monopolistic operator and two independent operators with divided demand. In the first, the broker presents trip offers from all operators to customers (similar to a mobility-as-a-service platform), who can then freely choose an operator. In the second, a regulated broker platform can manipulate operator offers with the goal of shifting the customer-operator assignment from a user equilibrium towards a system optimum. To model adoptions of the service design depending on the different interaction scenario, a game setting is introduced. Within alternating turns between operators, operators can adapt parameters of their service (fleet size and objective function) to maximize profit. Results for a case study based on Manhattan taxi data, show that operators generate the highest profit in the broker setting while operating the largest fleet. Additionally, pooling efficiency can nearly be maintained compared to a single operator. With the resulting increased service rate, the regulated competition benefits not only operators (profit) and cities (increased pooling efficiency), but also customers. Contrarily, when users can decide freely, the lowest pooling efficiency and operator profit is observed.      
### 68.On Designing Data Models for Energy Feature Stores  [ :arrow_down: ](https://arxiv.org/pdf/2205.04267.pdf)
>  The digitization of the energy infrastructure enables new, data driven, applications often supported by machine learning models. However, domain specific data transformations, pre-processing and management in modern data driven pipelines is yet to be addressed. In this paper we perform a first time study on data models, energy feature engineering and feature management solutions for developing ML-based energy applications. We first propose a taxonomy for designing data models suitable for energy applications, analyze feature engineering techniques able to transform the data model into features suitable for ML model training and finally also analyze available designs for feature stores. Using a short-term forecasting dataset, we show the benefits of designing richer data models and engineering the features on the performance of the resulting models. Finally, we benchmark three complementary feature management solutions, including an open-source feature store.      
### 69.SwinIQA: Learned Swin Distance for Compressed Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2205.04264.pdf)
>  Image compression has raised widespread interest recently due to its significant importance for multimedia storage and transmission. Meanwhile, a reliable image quality assessment (IQA) for compressed images can not only help to verify the performance of various compression algorithms but also help to guide the compression optimization in turn. In this paper, we design a full-reference image quality assessment metric SwinIQA to measure the perceptual quality of compressed images in a learned Swin distance space. It is known that the compression artifacts are usually non-uniformly distributed with diverse distortion types and degrees. To warp the compressed images into the shared representation space while maintaining the complex distortion information, we extract the hierarchical feature representations from each stage of the Swin Transformer. Besides, we utilize cross attention operation to map the extracted feature representations into a learned Swin distance space. Experimental results show that the proposed metric achieves higher consistency with human's perceptual judgment compared with both traditional methods and learning-based methods on CLIC datasets.      
### 70.Cross-Utterance Conditioned VAE for Non-Autoregressive Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2205.04120.pdf)
>  Modelling prosody variation is critical for synthesizing natural and expressive speech in end-to-end text-to-speech (TTS) systems. In this paper, a cross-utterance conditional VAE (CUC-VAE) is proposed to estimate a posterior probability distribution of the latent prosody features for each phoneme by conditioning on acoustic features, speaker information, and text features obtained from both past and future sentences. At inference time, instead of the standard Gaussian distribution used by VAE, CUC-VAE allows sampling from an utterance-specific prior distribution conditioned on cross-utterance information, which allows the prosody features generated by the TTS system to be related to the context and is more similar to how humans naturally produce prosody. The performance of CUC-VAE is evaluated via a qualitative listening test for naturalness, intelligibility and quantitative measurements, including word error rates and the standard deviation of prosody attributes. Experimental results on LJ-Speech and LibriTTS data show that the proposed CUC-VAE TTS system improves naturalness and prosody diversity with clear margins.      
### 71.Linear quantum systems: a tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2205.04080.pdf)
>  The purpose of this tutorial is to give a brief introduction to linear quantum control systems. The mathematical model of linear quantum control systems is presented first, then some fundamental control-theoretic notions such as stability, controllability and observability are given, which are closely related to several important problems in quantum information technology such as decoherence-free subsystems, quantum non-demolition variables, and back-action evasion measurements. After that, quantum Gaussian states are introduced, in particular, an information-theoretic uncertainty relation is presented which often gives a better bound for mixed Gaussian states than the well-known Heisenberg uncertainty relation. The quantum Kalman filter is presented for quantum linear systems, which is the quantum analogy of the Kalman filter for classical (namely, non-quantum-mechanical) linear systems. The quantum Kalman canonical decomposition for quantum linear systems is recorded, and its application is illustrated by means of a recent experiment. As single- and multi-photon states are useful resources in quantum information technology, the response of quantum systems to these types of input is presented. Finally, feedback control of quantum linear systems is briefly introduced, and a recent experiment is used to demonstrate the effectiveness of quantum linear systems and networks theory.      
### 72.Exploiting Digital Surface Models for Inferring Super-Resolution for Remotely Sensed Images  [ :arrow_down: ](https://arxiv.org/pdf/2205.04056.pdf)
>  Despite the plethora of successful Super-Resolution Reconstruction (SRR) models applied to natural images, their application to remote sensing imagery tends to produce poor results. Remote sensing imagery is often more complicated than natural images and has its peculiarities such as being of lower resolution, it contains noise, and often depicting large textured surfaces. As a result, applying non-specialized SRR models on remote sensing imagery results in artifacts and poor reconstructions. To address these problems, this paper proposes an architecture inspired by previous research work, introducing a novel approach for forcing an SRR model to output realistic remote sensing images: instead of relying on feature-space similarities as a perceptual loss, the model considers pixel-level information inferred from the normalized Digital Surface Model (nDSM) of the image. This strategy allows the application of better-informed updates during the training of the model which sources from a task (elevation map inference) that is closely related to remote sensing. Nonetheless, the nDSM auxiliary information is not required during production and thus the model infers a super-resolution image without any additional data besides its low-resolution pairs. We assess our model on two remotely sensed datasets of different spatial resolutions that also contain the DSM pairs of the images: the DFC2018 dataset and the dataset containing the national Lidar fly-by of Luxembourg. Based on visual inspection, the inferred super-resolution images exhibit particularly superior quality. In particular, the results for the high-resolution DFC2018 dataset are realistic and almost indistinguishable from the ground truth images.      
### 73.Realization of Lattice Formation in Nonlinear Two-dimensional Potential by Mobile Robots  [ :arrow_down: ](https://arxiv.org/pdf/2205.04052.pdf)
>  Formation control in multi-agent system has earned significant research interests in both theorical aspect and applications over the past two decades. However, the study on how the external environment shapes swarm formation dynamics, and the design of formation control algorithm for multi-agent system in nonlinear external potential have not been rigorously investigated. In this paper, we present a formation control algorithm for mobile robots travelling in nonlinear external potential. Experiments are performed on real mobile robots to verify the algorithm, and the effectiveness of Dynamic Mode Decomposition in robot's velocity prediction in unknown environment is demonstrated.      
### 74.Quasi-Real Time Multi-Frequency 3D Shear Wave Absolute Vibro-Elastography (S-WAVE) System for Prostate  [ :arrow_down: ](https://arxiv.org/pdf/2205.04038.pdf)
>  This article describes a novel quasi-real time system for quantitative and volumetric measurement of tissue elasticity in the prostate. Tissue elasticity is computed by using a local frequency estimator to measure the three dimensional local wavelengths of a steady-state shear wave within the prostate gland. The shear wave is created using a mechanical voice coil shaker which transmits multi-frequency vibrations transperineally. Radio frequency data is streamed directly from a BK Medical 8848 trans-rectal ultrasound transducer to an external computer where tissue displacement due to the excitation is measured using a speckle tracking algorithm. Bandpass sampling is used that eliminates the need for an ultra fast frame rate to track the tissue motion and allows for accurate reconstruction at a sampling frequency that is below the Nyquist rate. A roll motor with computer control is used to rotate the sagittal array of the transducer and obtain the 3D data. Two CIRS phantoms were used to validate both the accuracy of the elasticity measurement as well as the functional feasibility of using the system for in vivo prostate imaging. The system has been used in two separate clinical studies as a method for cancer identification. The results, presented here, show numerical and visual correlations between our stiffness measurements and cancer likelihood as determined from pathology results. Initial published results using this system include an area under the receiver operating characteristic curve of 0.82+/-0.01 with regards to prostate cancer identification in the peripheral zone.      
### 75.AI Based Digital Twin Model for Cattle Caring  [ :arrow_down: ](https://arxiv.org/pdf/2205.04034.pdf)
>  In this paper, we developed innovative digital twins of cattle status that are powered by artificial intelligence (AI). The work was built on a farm IoT system that remotely monitors and tracks the state of cattle. A digital twin model of cattle health based on Deep Learning (DL) was generated using the sensor data acquired from the farm IoT system. The health and physiological cycle of cattle can be monitored in real time, and the state of the next physiological cycle of cattle can be anticipated using this model. The basis of this work is the vast amount of data which is required to validate the legitimacy of the digital twins model. In terms of behavioural state, it was found that the cattle treated with a combination of topical anaesthetic and meloxicam exhibits the least pain reaction. The digital twins model developed in this work can be used to monitor the health of cattle      
### 76.Muskits: an End-to-End Music Processing Toolkit for Singing Voice Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2205.04029.pdf)
>  This paper introduces a new open-source platform named Muskits for end-to-end music processing, which mainly focuses on end-to-end singing voice synthesis (E2E-SVS). Muskits supports state-of-the-art SVS models, including RNN SVS, transformer SVS, and XiaoiceSing. The design of Muskits follows the style of widely-used speech processing toolkits, ESPnet and Kaldi, for data prepossessing, training, and recipe pipelines. To the best of our knowledge, this toolkit is the first platform that allows a fair and highly-reproducible comparison between several published works in SVS. In addition, we also demonstrate several advanced usages based on the toolkit functionalities, including multilingual training and transfer learning. This paper describes the major framework of Muskits, its functionalities, and experimental results in single-singer, multi-singer, multilingual, and transfer learning scenarios. The toolkit is publicly available at <a class="link-external link-https" href="https://github.com/SJTMusicTeam/Muskits" rel="external noopener nofollow">this https URL</a>.      
### 77.The Degrees-of-Freedom in Monostatic ISAC Channels: NLoS Exploitation vs. Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2205.04010.pdf)
>  The degrees of freedom (DoFs) attained in monostatic integrated sensing and communications (ISAC) are analyzed. Specifically, monostatic sensing aims for extracting target-orientation information from the line of sight (LoS) channel between the transmitter and the target, since the Non-LoS (NLoS) paths only contain clutter or interference. By contrast, in wireless communications, typically, both the LoS and NLoS paths are exploited for achieving diversity or multiplexing gains. Hence, we shed light on the NLoS exploitation vs. reduction tradeoffs in a monostatic ISAC scenario. In particular, we optimize the transmit power of each signal path to maximize the communication rate, while guaranteeing the sensing performance for the target. The non-convex problem formulated is firstly solved in closed form for a single-NLoS-link scenario, then we harness the popular successive convex approximation (SCA) method for a general multiple-NLoS-link scenario. Our simulation results characterize the fundamental performance tradeoffs between sensing and communication, demonstrating that the available DoFs in the ISAC channel should be efficiently exploited in a way that is distinctly different from that of communication-only scenarios.      
### 78.Row-wise Accelerator for Vision Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2205.03998.pdf)
>  Following the success of the natural language processing, the transformer for vision applications has attracted significant attention in recent years due to its excellent performance. However, existing deep learning hardware accelerators for vision cannot execute this structure efficiently due to significant model architecture differences. As a result, this paper proposes the hardware accelerator for vision transformers with row-wise scheduling, which decomposes major operations in vision transformers as a single dot product primitive for a unified and efficient execution. Furthermore, by sharing weights in columns, we can reuse the data and reduce the usage of memory. The implementation with TSMC 40nm CMOS technology only requires 262K gate count and 149KB SRAM buffer for 403.2 GOPS throughput at 600MHz clock frequency.      
### 79.A Real Time Super Resolution Accelerator with Tilted Layer Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2205.03997.pdf)
>  Deep learning based superresolution achieves high-quality results, but its heavy computational workload, large buffer, and high external memory bandwidth inhibit its usage in mobile devices. To solve the above issues, this paper proposes a real-time hardware accelerator with the tilted layer fusion method that reduces the external DRAM bandwidth by 92\% and just needs 102KB on-chip memory. The design implemented with a 40nm CMOS process achieves 1920x1080@60fps throughput with 544.3K gate count when running at 600MHz; it has higher throughput and lower area cost than previous designs.      
### 80.Hardware-Robust In-RRAM-Computing for Object Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.03996.pdf)
>  In-memory computing is becoming a popular architecture for deep-learning hardware accelerators recently due to its highly parallel computing, low power, and low area cost. However, in-RRAM computing (IRC) suffered from large device variation and numerous nonideal effects in hardware. Although previous approaches including these effects in model training successfully improved variation tolerance, they only considered part of the nonideal effects and relatively simple classification tasks. This paper proposes a joint hardware and software optimization strategy to design a hardware-robust IRC macro for object detection. We lower the cell current by using a low word-line voltage to enable a complete convolution calculation in one operation that minimizes the impact of nonlinear addition. We also implement ternary weight mapping and remove batch normalization for better tolerance against device variation, sense amplifier variation, and IR drop problem. An extra bias is included to overcome the limitation of the current sensing range. The proposed approach has been successfully applied to a complex object detection task with only 3.85\% mAP drop, whereas a naive design suffers catastrophic failure under these nonideal effects.      
### 81.Rebellion and Disobedience as Useful Tools in Human-Robot Interaction Research -- The Handheld Robotics Case  [ :arrow_down: ](https://arxiv.org/pdf/2205.03968.pdf)
>  This position paper argues on the utility of rebellion and disobedience (RaD) in human-robot interaction (HRI). In general, we see two main opportunities in the use of controlled and well designed rebellion and disobedience: i) illuminate insight into the effectiveness of the collaboration (or lack of) and ii) prevent mistakes and correct user actions when in the user's own interest. Through the use of a close interaction modality, that of handheld robots, we discuss use cases for utility of rebellion and disobedience that can be applicable to other instances of HRI.      
### 82.High-Resolution UAV Image Generation for Sorghum Panicle Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.03947.pdf)
>  The number of panicles (or heads) of Sorghum plants is an important phenotypic trait for plant development and grain yield estimation. The use of Unmanned Aerial Vehicles (UAVs) enables the capability of collecting and analyzing Sorghum images on a large scale. Deep learning can provide methods for estimating phenotypic traits from UAV images but requires a large amount of labeled data. The lack of training data due to the labor-intensive ground truthing of UAV images causes a major bottleneck in developing methods for Sorghum panicle detection and counting. In this paper, we present an approach that uses synthetic training images from generative adversarial networks (GANs) for data augmentation to enhance the performance of Sorghum panicle detection and counting. Our method can generate synthetic high-resolution UAV RGB images with panicle labels by using image-to-image translation GANs with a limited ground truth dataset of real UAV RGB images. The results show the improvements in panicle detection and counting using our data augmentation approach.      
### 83.Low-pass filter with ultra-wide stopband for quantum computing applications  [ :arrow_down: ](https://arxiv.org/pdf/2205.03941.pdf)
>  A new type of low-pass filter based on a leaky coaxial waveguide is presented. The filter has minimal insertion loss in the pass band, while at the same time high attenuation in the stop band is achieved. Thanks to its arrangement, the filter does not present parasitic leakage paths, so that, unlike conventional resonant filters, the stop band extends to very high frequencies. It is shown that a particular stop-band attenuation can be obtained by adding or removing leaking sections. Coupling between the center coaxial structure and the leaking holes is investigated. A prototype is manufactured and scattering parameters are measured up to 145 GHz. The prototype shows an insertion loss of less than 0.15 dB up to 10 GHz and an attenuation in excess of 60 dB above 70 GHz. The proposed filter is suitable for superconducting quantum computing applications where qubits are sensitive to radiation with energy high enough to break Cooper-pairs, thereby destroying superconductivity.      
### 84.Integrate Bioprocess Mechanisms into Modeling, Analytics, and Control Strategies to Advance Biopharmaceutical Manufacturing and Delivery Processes  [ :arrow_down: ](https://arxiv.org/pdf/2205.03920.pdf)
>  The existing stochastic simulation methodologies tend to ignore ordinary differential equations or (ODE) or partial differential equations or (PDE) mechanistic models that typically represent the scientific understanding of underlying process dynamics and mechanisms. For emerging manufacturing and delivery processes of biopharmaceuticals (such as cell, gene, RNA, protein, peptide therapies and vaccines), this can limit sample efficiency, reliability, and interpretability of critical decision making. It also affects mechanism learning. Therefore, in this tutorial paper, we present the recent studies that integrate bioprocess mechanisms into simulation modeling, risk/sensitivity/predictive analytics, process design and control strategies. They can overcome the key challenges, including high complexity, high uncertainty, and very limited data, and facilitate design and development of productive, robust, and automated end-to-end biopharmaceutical manufacturing and delivery processes.      
### 85.Equitable Optimization of U.S. Airline Route Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.03900.pdf)
>  The civil aviation industry bears much of the responsibility for anthropogenic climate change. Due to the difficulties in developing clean fuels for aviation, restructuring route networks remains the most feasible alternative for reducing emissions. While reducing emissions is crucial, it should not come at the expense of passenger accessibility. As flights connecting underserved communities generally have a significantly higher rate of emissions per passenger serviced, there exists a fundamental trade-off between increasing accessibility and reducing emissions. In this paper, we define a metric to quantify air transportation accessibility for each U.S. census tract. When combined with the system-wide emissions estimation methodology developed in our previous work, the accessibility metric facilitates the creation of an optimization model that seeks to create a sustainable and equitable airline route network.      
### 86.Decentralized Stochastic Optimization with Inherent Privacy Protection  [ :arrow_down: ](https://arxiv.org/pdf/2205.03884.pdf)
>  Decentralized stochastic optimization is the basic building block of modern collaborative machine learning, distributed estimation and control, and large-scale sensing. Since involved data usually contain sensitive information like user locations, healthcare records and financial transactions, privacy protection has become an increasingly pressing need in the implementation of decentralized stochastic optimization algorithms. In this paper, we propose a decentralized stochastic gradient descent algorithm which is embedded with inherent privacy protection for every participating agent against other participating agents and external eavesdroppers. This proposed algorithm builds in a dynamics based gradient-obfuscation mechanism to enable privacy protection without compromising optimization accuracy, which is in significant difference from differential-privacy based privacy solutions for decentralized optimization that have to trade optimization accuracy for privacy. The dynamics based privacy approach is encryption-free, and hence avoids incurring heavy communication or computation overhead, which is a common problem with encryption based privacy solutions for decentralized stochastic optimization. Besides rigorously characterizing the convergence performance of the proposed decentralized stochastic gradient descent algorithm under both convex objective functions and non-convex objective functions, we also provide rigorous information-theoretic analysis of its strength of privacy protection. Simulation results for a distributed estimation problem as well as numerical experiments for decentralized learning on a benchmark machine learning dataset confirm the effectiveness of the proposed approach.      
### 87.SeqNet: An Efficient Neural Network for Automatic Malware Detection  [ :arrow_down: ](https://arxiv.org/pdf/2205.03850.pdf)
>  Malware continues to evolve rapidly, and more than 450,000 new samples are captured every day, which makes manual malware analysis impractical. However, existing deep learning detection models need manual feature engineering or require high computational overhead for long training processes, which might be laborious to select feature space and difficult to retrain for mitigating model aging. Therefore, a crucial requirement for a detector is to realize automatic and efficient detection. In this paper, we propose a lightweight malware detection model called SeqNet which could be trained at high speed with low memory required on the raw binaries. By avoiding contextual confusion and reducing semantic loss, SeqNet maintains the detection accuracy when reducing the number of parameters to only 136K. We demonstrate the effectiveness of our methods and the low training cost requirement of SeqNet in our experiments. Besides, we make our datasets and codes public to stimulate further academic research.      
### 88.Characterizing the Energy-Efficiency Region of Symbiotic Radio Communications  [ :arrow_down: ](https://arxiv.org/pdf/2205.03826.pdf)
>  Symbiotic radio (SR) communication is a promising technology to achieve spectrum- and energy-efficient wireless communication, by enabling passive backscatter devices (BDs) reuse not only the spectrum, but also the power of active primary transmitters (PTs). In this paper, we aim to characterize the energy-efficiency (EE) region of multiple-input single-output (MISO) SR systems, which is defined as all the achievable EE pairs by the active PT and passive BD. To this end, we first derive the maximum individual EE of the PT and BD, respectively, and show that there exists a non-trivial trade-off between these two EEs. To characterize such a trade-off, an optimization problem is formulated to find the Pareto boundary of the EE region by optimizing the transmit beamforming and power allocation. The formulated problem is non-convex and difficult to be directly solved. An efficient algorithm based on successive convex approximation (SCA) is proposed to find a Karush-Kuhn-Tucker (KKT) solution. Simulation results are provided to show that the proposed algorithm is able to effectively characterize the EE region of SR communication systems.      
### 89.A Closer Look at Few-shot Image Generation  [ :arrow_down: ](https://arxiv.org/pdf/2205.03805.pdf)
>  Modern GANs excel at generating high quality and diverse images. However, when transferring the pretrained GANs on small target data (e.g., 10-shot), the generator tends to replicate the training samples. <br>Several methods have been proposed to address this few-shot image generation task, but there is a lack of effort to analyze them under a unified framework. <br>As our first contribution, we propose a framework to analyze existing methods during the adaptation. Our analysis discovers that while some methods have disproportionate focus on diversity preserving which impede quality improvement, all methods achieve similar quality after convergence. <br>Therefore, the better methods are those that can slow down diversity degradation. Furthermore, our analysis reveals that there is still plenty of room to further slow down diversity degradation. <br>Informed by our analysis and to slow down the diversity degradation of the target generator during adaptation, our second contribution proposes to apply mutual information (MI) maximization to retain the source domain's rich multi-level diversity information in the target domain generator. <br>We propose to perform MI maximization by contrastive loss (CL), leverage the generator and discriminator as two feature encoders to extract different multi-level features for computing CL. We refer to our method as Dual Contrastive Learning (DCL). <br>Extensive experiments on several public datasets show that, while leading to a slower diversity-degrading generator during adaptation, our proposed DCL brings visually pleasant quality and state-of-the-art quantitative performance.      
### 90.Transformer-Empowered 6G Intelligent Networks: From Massive MIMO Processing to Semantic Communication  [ :arrow_down: ](https://arxiv.org/pdf/2205.03770.pdf)
>  6G wireless networks are foreseen to speed up the convergence of the physical and cyber worlds and to enable a paradigm-shift in the way we deploy and exploit communication networks. Machine learning, in particular deep learning (DL), is going to be one of the key technological enablers of 6G by offering a new paradigm for the design and optimization of networks with a high level of intelligence. In this article, we introduce an emerging DL architecture, known as the transformer, and discuss its potential impact on 6G network design. We first discuss the differences between the transformer and classical DL architectures, and emphasize the transformer's self-attention mechanism and strong representation capabilities, which make it particularly appealing in tackling various challenges in wireless network design. Specifically, we propose transformer-based solutions for massive multiple-input multiple-output (MIMO) systems and various semantic communication problems in 6G networks. Finally, we discuss key challenges and open issues in transformer-based solutions, and identify future research directions for their deployment in intelligent 6G networks.      
### 91.Silence is Sweeter Than Speech: Self-Supervised Model Using Silence to Store Speaker Information  [ :arrow_down: ](https://arxiv.org/pdf/2205.03759.pdf)
>  Self-Supervised Learning (SSL) has made great strides recently. SSL speech models achieve decent performance on a wide range of downstream tasks, suggesting that they extract different aspects of information from speech. However, how SSL models store various information in hidden representations without interfering is still poorly understood. Taking the recently successful SSL model, HuBERT, as an example, we explore how the SSL model processes and stores speaker information in the representation. We found that HuBERT stores speaker information in representations whose positions correspond to silences in a waveform. There are several pieces of evidence. (1) We find that the utterances with more silent parts in the waveforms have better Speaker Identification (SID) accuracy. (2) If we use the whole utterances for SID, the silence part always contributes more to the SID task. (3) If we only use the representation of a part of the utterance for SID, the silenced part has higher accuracy than the other parts. Our findings not only contribute to a better understanding of SSL models but also improve performance. By simply adding silence to the original waveform, HuBERT improved its accuracy on SID by nearly 2%.      
### 92.Quantifying and Extrapolating Data Needs in Radio Frequency Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.03703.pdf)
>  Understanding the relationship between training data and a model's performance once deployed is a fundamental component in the application of machine learning. While the model's deployed performance is dependent on numerous variables within the scope of machine learning, beyond that of the training data itself, the effect of the dataset is isolated in this work to better understand the role training data plays in the problem. This work examines a modulation classification problem in the Radio Frequency domain space, attempting to answer the question of how much training data is required to achieve a desired level of performance, but the procedure readily applies to classification problems across modalities. By repurposing the metrics of transfer potential developed within transfer learning an approach to bound data quantity needs developed given a training approach and machine learning architecture; this approach is presented as a means to estimate data quantity requirements to achieve a target performance. While this approach will require an initial dataset that is germane to the problem space to act as a target dataset on which metrics are extracted, the goal is to allow for the initial data to be orders of magnitude smaller than what is required for delivering a system that achieves the desired performance. An additional benefit of the techniques presented here is that the quality of different datasets can be numerically evaluated and tied together with the quantity of data, and the performance of the system.      
### 93.Variational Sparse Coding with Learned Thresholding  [ :arrow_down: ](https://arxiv.org/pdf/2205.03665.pdf)
>  Sparse coding strategies have been lauded for their parsimonious representations of data that leverage low dimensional structure. However, inference of these codes typically relies on an optimization procedure with poor computational scaling in high-dimensional problems. For example, sparse inference in the representations learned in the high-dimensional intermediary layers of deep neural networks (DNNs) requires an iterative minimization to be performed at each training step. As such, recent, quick methods in variational inference have been proposed to infer sparse codes by learning a distribution over the codes with a DNN. In this work, we propose a new approach to variational sparse coding that allows us to learn sparse distributions by thresholding samples, avoiding the use of problematic relaxations. We first evaluate and analyze our method by training a linear generator, showing that it has superior performance, statistical efficiency, and gradient estimation compared to other sparse distributions. We then compare to a standard variational autoencoder using a DNN generator on the Fashion MNIST and CelebA datasets      
### 94.Double-Side Near-Field Channel Estimation for Extremely Large-Scale MIMO System  [ :arrow_down: ](https://arxiv.org/pdf/2205.03615.pdf)
>  Accurate channel estimation is essential to empower extremely large-scale MIMO (XL-MIMO) in 6G networks with ultra-high spectral efficiency. Unfortunately, most of the existing channel estimation methods designed for XL-MIMO fail to consider a double-side near-field scenario, where both transmitter and receiver are equipped with extremely large-scale antenna arrays. The existing channel estimation schemes cannot be directly applied to the double-side near-field scenario. In this paper, based on this scenario, we first derive double-side near-field Rayleigh distance (DS-RD) and effective double-side near-field Rayleigh distance (EDS-RD) to determine the range of the double-side near-field region. Then, a double-side near-field channel model is proposed to match this scenario, where the distance of the transmitter from the receiver is smaller than EDS-RD. In the proposed channel model, the line of sight (LoS) path component is modeled by the geometric free assumption while non-line of sight (NLoS) path components are modeled by the near-field array response vectors. Finally, a double-side near-field channel estimation algorithm is proposed to solve the channel estimation problem in this scenario, where the LoS path component and NLoS path components are estimated separately. Numerical simulation results demonstrate that, the proposed channel estimation algorithm is able to outperform the existing methods.      
### 95.Utility-Oriented Underwater Image Quality Assessment Based on Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.03574.pdf)
>  The widespread image applications have greatly promoted the vision-based tasks, in which the Image Quality Assessment (IQA) technique has become an increasingly significant issue. For user enjoyment in multimedia systems, the IQA exploits image fidelity and aesthetics to characterize user experience; while for other tasks such as popular object recognition, there exists a low correlation between utilities and perceptions. In such cases, the fidelity-based and aesthetics-based IQA methods cannot be directly applied. To address this issue, this paper proposes a utility-oriented IQA in object recognition. In particular, we initialize our research in the scenario of underwater fish detection, which is a critical task that has not yet been perfectly addressed. Based on this task, we build an Underwater Image Utility Database (UIUD) and a learning-based Underwater Image Utility Measure (UIUM). Inspired by the top-down design of fidelity-based IQA, we exploit the deep models of object recognition and transfer their features to our UIUM. Experiments validate that the proposed transfer-learning-based UIUM achieves promising performance in the recognition task. We envision our research provides insights to bridge the researches of IQA and computer vision.      
### 96.From Heavy Rain Removal to Detail Restoration: A Faster and Better Network  [ :arrow_down: ](https://arxiv.org/pdf/2205.03553.pdf)
>  The dense rain accumulation in heavy rain can significantly wash out images and thus destroy the background details of images. Although existing deep rain removal models lead to improved performance for heavy rain removal, we find that most of them ignore the detail reconstruction accuracy of rain-free images. In this paper, we propose a dual-stage progressive enhancement network (DPENet) to achieve effective deraining with structure-accurate rain-free images. Two main modules are included in our framework, namely a rain streaks removal network (R$^2$Net) and a detail reconstruction network (DRNet). The former aims to achieve accurate rain removal, and the latter is designed to recover the details of rain-free images. We introduce two main strategies within our networks to achieve trade-off between the effectiveness of deraining and the detail restoration of rain-free images. Firstly, a dilated dense residual block (DDRB) within the rain streaks removal network is presented to aggregate high/low level features of heavy rain. Secondly, an enhanced residual pixel-wise attention block (ERPAB) within the detail reconstruction network is designed for context information aggregation. We also propose a comprehensive loss function to highlight the marginal and regional accuracy of rain-free images. Extensive experiments on benchmark public datasets show both efficiency and effectiveness of the proposed method in achieving structure-preserving rain-free images for heavy rain removal. The source code and pre-trained models can be found at \url{<a class="link-external link-https" href="https://github.com/wybchd/DPENet" rel="external noopener nofollow">this https URL</a>}.      
### 97.Low-Complexity Distributed Precoding in User-Centric Cell-Free mmWave MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.03538.pdf)
>  User-centric (UC) based cell-free (CF) structures can provide the benefits of coverage enhancement for millimeter wave (mmWave) multiple input multiple output (MIMO) systems, which is regarded as the key technology of the reliable and high-rate services. In this paper, we propose a new beam selection scheme and precoding algorithm for the UC CF mmWave MIMO system, where a weighted sum-rate maximization problem is formulated. Since the joint design of beam selection and precoding is non-convex and tractable with high complexity, this paper designs the beam selection and precoding separately. Particularly, the proposed beam selection aims at reducing the inter-cluster inter-beam interference, then we also propose a precoding algorithm based on the weighted sum mean-square error (WSMSE) framework, where the precoding matrix can be updated in a distributed manner. We further employ the low-rank decomposition and Neumann series expansion (NSE) to reduce the computational complexity of the precoding. Simulations and complexity analysis verify the effectiveness of the proposed algorithm with a considerable reduction in computational complexity.      
### 98.Training Enhancement of Deep Learning Models for Massive MIMO CSI Feedback with Small Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2205.03533.pdf)
>  Accurate downlink channel state information (CSI) is vital to achieving high spectrum efficiency in massive MIMO systems. Existing works on the deep learning (DL) model for CSI feedback have shown efficient compression and recovery in frequency division duplex (FDD) systems. However, practical DL networks require sizeable wireless CSI datasets during training to achieve high model accuracy. To address this labor-intensive problem, this work develops an efficient training enhancement solution of DL-based feedback architecture based on a modest dataset by exploiting the complex CSI features, and augmenting CSI dataset based on domain knowledge. We first propose a spherical CSI feedback network, SPTM2-ISTANet+, which employs the spherical normalization framework to mitigate the effect of path loss variation. We exploit the trainable measurement matrix and residual recovery structure to improve the encoding efficiency and recovery accuracy. For limited CSI measurements, we propose a model-driven lightweight and universal augmentation strategy based on decoupling CSI magnitude and phase information, applying the circular shift in angular-delay domain, and randomizing the CSI phase to approximate phase distribution. Test results demonstrate the efficacy and efficiency of the proposed training strategy and feedback architecture for accurate CSI feedback under limited measurements.      
### 99.Comparative Analysis of Non-Blind Deblurring Methods for Noisy Blurred Images  [ :arrow_down: ](https://arxiv.org/pdf/2205.03464.pdf)
>  Image blurring refers to the degradation of an image wherein the image's overall sharpness decreases. Image blurring is caused by several factors. Additionally, during the image acquisition process, noise may get added to the image. Such a noisy and blurred image can be represented as the image resulting from the convolution of the original image with the associated point spread function, along with additive noise. However, the blurred image often contains inadequate information to uniquely determine the plausible original image. Based on the availability of blurring information, image deblurring methods can be classified as blind and non-blind. In non-blind image deblurring, some prior information is known regarding the corresponding point spread function and the added noise. The objective of this study is to determine the effectiveness of non-blind image deblurring methods with respect to the identification and elimination of noise present in blurred images. In this study, three non-blind image deblurring methods, namely Wiener deconvolution, Lucy-Richardson deconvolution, and regularized deconvolution were comparatively analyzed for noisy images featuring salt-and-pepper noise. Two types of blurring effects were simulated, namely motion blurring and Gaussian blurring. The said three non-blind deblurring methods were applied under two scenarios: direct deblurring of noisy blurred images and deblurring of images after denoising through the application of the adaptive median filter. The obtained results were then compared for each scenario to determine the best approach for deblurring noisy images.      
### 100.Structure Learning in Graphical Models from Indirect Observations  [ :arrow_down: ](https://arxiv.org/pdf/2205.03454.pdf)
>  This paper considers learning of the graphical structure of a $p$-dimensional random vector $X \in R^p$ using both parametric and non-parametric methods. Unlike the previous works which observe $x$ directly, we consider the indirect observation scenario in which samples $y$ are collected via a sensing matrix $A \in R^{d\times p}$, and corrupted with some additive noise $w$, i.e, $Y = AX + W$. For the parametric method, we assume $X$ to be Gaussian, i.e., $x\in R^p\sim N(\mu, \Sigma)$ and $\Sigma \in R^{p\times p}$. For the first time, we show that the correct graphical structure can be correctly recovered under the indefinite sensing system ($d &lt; p$) using insufficient samples ($n &lt; p$). In particular, we show that for the exact recovery, we require dimension $d = \Omega(p^{0.8})$ and sample number $n = \Omega(p^{0.8}\log^3 p)$. For the nonparametric method, we assume a nonparanormal distribution for $X$ rather than Gaussian. Under mild conditions, we show that our graph-structure estimator can obtain the correct structure. We derive the minimum sample number $n$ and dimension $d$ as $n\gtrsim (deg)^4 \log^4 n$ and $d \gtrsim p + (deg\cdot\log(d-p))^{\beta/4}$, respectively, where deg is the maximum Markov blanket in the graphical model and $\beta &gt; 0$ is some fixed positive constant. Additionally, we obtain a non-asymptotic uniform bound on the estimation error of the CDF of $X$ from indirect observations with inexact knowledge of the noise distribution. To the best of our knowledge, this bound is derived for the first time and may serve as an independent interest. Numerical experiments on both real-world and synthetic data are provided confirm the theoretical results.      
### 101.Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2205.03433.pdf)
>  Recognizing human non-speech vocalizations is an important task and has broad applications such as automatic sound transcription and health condition monitoring. However, existing datasets have a relatively small number of vocal sound samples or noisy labels. As a consequence, state-of-the-art audio event classification models may not perform well in detecting human vocal sounds. To support research on building robust and accurate vocal sound recognition, we have created a VocalSound dataset consisting of over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. Experiments show that the vocal sound recognition performance of a model can be significantly improved by 41.9% by adding VocalSound dataset to an existing dataset as training material. In addition, different from previous datasets, the VocalSound dataset contains meta information such as speaker age, gender, native language, country, and health condition.      
### 102.Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2205.03432.pdf)
>  Automatic pronunciation assessment is an important technology to help self-directed language learners. While pronunciation quality has multiple aspects including accuracy, fluency, completeness, and prosody, previous efforts typically only model one aspect (e.g., accuracy) at one granularity (e.g., at the phoneme-level). In this work, we explore modeling multi-aspect pronunciation assessment at multiple granularities. Specifically, we train a Goodness Of Pronunciation feature-based Transformer (GOPT) with multi-task learning. Experiments show that GOPT achieves the best results on speechocean762 with a public automatic speech recognition (ASR) acoustic model trained on Librispeech.      
