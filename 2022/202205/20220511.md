# ArXiv eess --Wed, 11 May 2022
### 1.Wideband physical layer cognitive radio using photonic blind source separation  [ :arrow_down: ](https://arxiv.org/pdf/2205.05046.pdf)
>  The expansion of communication transmissions incurs increasingly severe crosstalk and interference, and a physical layer cognitive method, called blind source separation (BSS), can effectively address these issues. The BSS requires minimal prior knowledge in recovering signals from their mixtures, showing obliviousness of carrier frequency, signal formats, and channel conditions. However, previous electronic implemented BSS did not fulfill this agility due to the inherently narrow bandwidth of radio-frequency (RF) components, the high energy consumption of the digital signal processor (DSP), and their shared weaknesses in scalability. Here, we report a photonic BSS approach that inherits the merits of optical devices that can fully entitle the advantageous "blindness" feature. Using a microring weightbank built on a photonic chip, besides energy-efficient tuning and WDM-compatible scalability, we demonstrate a broad bandwidth across 13.8 GHz, which covers many standard frequency bands. Our setup also has high accuracy (9-bit) in performing the signal demixing thanks to a recently developed dithering control method, resulting in higher separability even for the ill-conditioned mixtures.      
### 2.Hybrid Reinforcement Learning for STAR-RISs: A Coupled Phase-Shift Model Based Beamformer  [ :arrow_down: ](https://arxiv.org/pdf/2205.05029.pdf)
>  A simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) assisted multi-user downlink multiple-input single-output (MISO) communication system is investigated. In contrast to the existing ideal STAR-RIS model assuming an independent transmission and reflection phase-shift control, a practical coupled phase-shift model is considered. Then, a joint active and passive beamforming optimization problem is formulated for minimizing the long-term transmission power consumption, subject to the coupled phase-shift constraint and the minimum data rate constraint. Despite the coupled nature of the phase-shift model, the formulated problem is solved by invoking a hybrid continuous and discrete phase-shift control policy. Inspired by this observation, a pair of hybrid reinforcement learning (RL) algorithms, namely the hybrid deep deterministic policy gradient (hybrid DDPG) algorithm and the joint DDPG &amp; deep-Q network (DDPG-DQN) based algorithm are proposed. The hybrid DDPG algorithm controls the associated high-dimensional continuous and discrete actions by relying on the hybrid action mapping. By contrast, the joint DDPG-DQN algorithm constructs two Markov decision processes (MDPs) relying on an inner and an outer environment, thereby amalgamating the two agents to accomplish a joint hybrid control. Simulation results demonstrate that the STAR-RIS has superiority over other conventional RISs in terms of its energy consumption. Furthermore, both the proposed algorithms outperform the baseline DDPG algorithm, and the joint DDPG-DQN algorithm achieves a superior performance, albeit at an increased computational complexity.      
### 3.Neuromimetic Linear Systems -- Resilience and Learning  [ :arrow_down: ](https://arxiv.org/pdf/2205.05013.pdf)
>  Building on our recent work on {\em neuromimetic control theory}, new results on resilience and neuro-inspired quantization are reported. The term neuromimetic refers to the models having features that are characteristic of the neurobiology of biological motor control. As in previous work, the focus is on what we call {\em overcomplete} linear systems that are characterized by larger numbers of input and output channels than the dimensions of the state. The specific contributions of the present paper include a proposed {\em resilient} observer whose operation tolerates output channel intermittency and even complete dropouts. Tying these ideas together with our previous work on resilient stability, a resilient separation principle is established. We also propose a {\em principled quantization} in which control signals are encoded as simple discrete inputs which act collectively through the many channels of input that are the hallmarks of the overcomplete models. Aligned with the neuromimetic paradigm, an {\em emulation} problem is proposed and this in turn defines an optimal quantization problem. Several possible solutions are discussed including direct combinatorial optimization, a Hebbian-like iterative learning algorithm, and a deep Q-learning (DQN) approach. For the problems being considered, machine learning approaches to optimization provide valuable insights regarding comparisons between optimal and nearby suboptimal solutions. These are useful in understanding the kinds of resilience to intermittency and channel dropouts that were earlier demonstrated for continuous systems.      
### 4.Using Deep Learning-based Features Extracted from CT scans to Predict Outcomes in COVID-19 Patients  [ :arrow_down: ](https://arxiv.org/pdf/2205.05009.pdf)
>  The COVID-19 pandemic has had a considerable impact on day-to-day life. Tackling the disease by providing the necessary resources to the affected is of paramount importance. However, estimation of the required resources is not a trivial task given the number of factors which determine the requirement. This issue can be addressed by predicting the probability that an infected patient requires Intensive Care Unit (ICU) support and the importance of each of the factors that influence it. Moreover, to assist the doctors in determining the patients at high risk of fatality, the probability of death is also calculated. For determining both the patient outcomes (ICU admission and death), a novel methodology is proposed by combining multi-modal features, extracted from Computed Tomography (CT) scans and Electronic Health Record (EHR) data. Deep learning models are leveraged to extract quantitative features from CT scans. These features combined with those directly read from the EHR database are fed into machine learning models to eventually output the probabilities of patient outcomes. This work demonstrates both the ability to apply a broad set of deep learning methods for general quantification of Chest CT scans and the ability to link these quantitative metrics to patient outcomes. The effectiveness of the proposed method is shown by testing it on an internally curated dataset, achieving a mean area under Receiver operating characteristic curve (AUC) of 0.77 on ICU admission prediction and a mean AUC of 0.73 on death prediction using the best performing classifiers.      
### 5.Disentangling A Single MR Modality  [ :arrow_down: ](https://arxiv.org/pdf/2205.04982.pdf)
>  Disentangling anatomical and contrast information from medical images has gained attention recently, demonstrating benefits for various image analysis tasks. Current methods learn disentangled representations using either paired multi-modal images with the same underlying anatomy or auxiliary labels (e.g., manual delineations) to provide inductive bias for disentanglement. However, these requirements could significantly increase the time and cost in data collection and limit the applicability of these methods when such data are not available. Moreover, these methods generally do not guarantee disentanglement. In this paper, we present a novel framework that learns theoretically and practically superior disentanglement from single modality magnetic resonance images. Moreover, we propose a new information-based metric to quantitatively evaluate disentanglement. Comparisons over existing disentangling methods demonstrate that the proposed method achieves superior performance in both disentanglement and cross-domain image-to-image translation tasks.      
### 6.Hybrid Far- and Near-Field Channel Estimation for THz Ultra-Massive MIMO via Fixed Point Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.04944.pdf)
>  Terahertz ultra-massive multiple-input multiple-output (THz UM-MIMO) is envisioned as one of the key enablers of 6G wireless systems. Due to the joint effect of its large array aperture and small wavelength, the near-field region of THz UM-MIMO systems is greatly enlarged. The high-dimensional channel of such systems thus consists of a stochastic mixture of far and near fields, which renders channel estimation extremely challenging. Previous works based on uni-field assumptions cannot capture the hybrid far- and near-field features, and will suffer significant performance loss. This motivates us to consider hybrid-field channel estimation. We draw inspirations from fixed point theory to develop an efficient deep learning based channel estimator with adaptive complexity and linear convergence guarantee. Built upon classic orthogonal approximate message passing, we transform each iteration into a contractive mapping, comprising a closed-form linear estimator and a neural network based non-linear estimator. A major algorithmic innovation involves applying fixed point iteration to compute the channel estimate while modeling neural networks with arbitrary depth and adapting to the hybrid-field channel conditions. Simulation results will verify our theoretical analysis and show significant performance gains over state-of-the-art approaches in the estimation accuracy and convergence rate.      
### 7.Understanding the Capability of PD Control for Uncertain Stochastic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.04914.pdf)
>  In this article, we focus on the global stabilizability problem for a class of second order uncertain stochastic control systems, where both the drift term and the diffusion term are nonlinear functions of the state variables and the control variables. We will show that the widely applied proportional-derivative(PD) control in engineering practice has the ability to globally stabilize such systems in the mean square sense, provided that the upper bounds of the partial derivatives of the nonlinear functions satisfy a certain algebraic inequality. It will also be proved that the stabilizing PD parameters can only be selected from a two dimensional bounded convex set, which is a significant difference from the existing literature on PD controlled uncertain stochastic systems. Moreover, a particular polynomial on these bounds is introduced, which can be used to determine under what conditions the system is not stabilizable by the PD control, and thus demonstrating the fundamental limitations of PD control.      
### 8.A Closer Look at Blind Super-Resolution: Degradation Models, Baselines, and Performance Upper Bounds  [ :arrow_down: ](https://arxiv.org/pdf/2205.04910.pdf)
>  Degradation models play an important role in Blind super-resolution (SR). The classical degradation model, which mainly involves blur degradation, is too simple to simulate real-world scenarios. The recently proposed practical degradation model includes a full spectrum of degradation types, but only considers complex cases that use all degradation types in the degradation process, while ignoring many important corner cases that are common in the real world. To address this problem, we propose a unified gated degradation model to generate a broad set of degradation cases using a random gate controller. Based on the gated degradation model, we propose simple baseline networks that can effectively handle non-blind, classical, practical degradation cases as well as many other corner cases. To fairly evaluate the performance of our baseline networks against state-of-the-art methods and understand their limits, we introduce the performance upper bound of an SR network for every degradation type. Our empirical analysis shows that with the unified gated degradation model, the proposed baselines can achieve much better performance than existing methods in quantitative and qualitative results, which are close to the performance upper bounds.      
### 9.MNet: Rethinking 2D/3D Networks for Anisotropic Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2205.04846.pdf)
>  The nature of thick-slice scanning causes severe inter-slice discontinuities of 3D medical images, and the vanilla 2D/3D convolutional neural networks (CNNs) fail to represent sparse inter-slice information and dense intra-slice information in a balanced way, leading to severe underfitting to inter-slice features (for vanilla 2D CNNs) and overfitting to noise from long-range slices (for vanilla 3D CNNs). In this work, a novel mesh network (MNet) is proposed to balance the spatial representation inter axes via learning. 1) Our MNet latently fuses plenty of representation processes by embedding multi-dimensional convolutions deeply into basic modules, making the selections of representation processes flexible, thus balancing representation for sparse inter-slice information and dense intra-slice information adaptively. 2) Our MNet latently fuses multi-dimensional features inside each basic module, simultaneously taking the advantages of 2D (high segmentation accuracy of the easily recognized regions in 2D view) and 3D (high smoothness of 3D organ contour) representations, thus obtaining more accurate modeling for target regions. Comprehensive experiments are performed on four public datasets (CT\&amp;MR), the results consistently demonstrate the proposed MNet outperforms the other methods. The code and datasets are available at: <a class="link-external link-https" href="https://github.com/zfdong-code/MNet" rel="external noopener nofollow">this https URL</a>      
### 10.Self-supervised regression learning using domain knowledge: Applications to improving self-supervised denoising in imaging  [ :arrow_down: ](https://arxiv.org/pdf/2205.04821.pdf)
>  Regression that predicts continuous quantity is a central part of applications using computational imaging and computer vision technologies. Yet, studying and understanding self-supervised learning for regression tasks - except for a particular regression task, image denoising - have lagged behind. This paper proposes a general self-supervised regression learning (SSRL) framework that enables learning regression neural networks with only input data (but without ground-truth target data), by using a designable pseudo-predictor that encapsulates domain knowledge of a specific application. The paper underlines the importance of using domain knowledge by showing that under different settings, the better pseudo-predictor can lead properties of SSRL closer to those of ordinary supervised learning. Numerical experiments for low-dose computational tomography denoising and camera image denoising demonstrate that proposed SSRL significantly improves the denoising quality over several existing self-supervised denoising methods.      
### 11.Explainable Deep Learning Methods in Medical Diagnosis: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2205.04766.pdf)
>  The remarkable success of deep learning has prompted interest in its application to medical diagnosis. Even tough state-of-the-art deep learning models have achieved human-level accuracy on the classification of different types of medical data, these models are hardly adopted in clinical workflows, mainly due to their lack of interpretability. The black-box-ness of deep learning models has raised the need for devising strategies to explain the decision process of these models, leading to the creation of the topic of eXplainable Artificial Intelligence (XAI). In this context, we provide a thorough survey of XAI applied to medical diagnosis, including visual, textual, and example-based explanation methods. Moreover, this work reviews the existing medical imaging datasets and the existing metrics for evaluating the quality of the explanations . Complementary to most existing surveys, we include a performance comparison among a set of report generation-based methods. Finally, the major challenges in applying XAI to medical imaging are also discussed.      
### 12.Characterization of electric consumers through an automated clustering pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2205.04737.pdf)
>  Clustering analysis of daily load profiles represents an effective technique to classify and aggregate electric users based on their actual consumption patterns. Among other purposes, it may be exploited as a preliminary stage for load forecasting, which is applied in the same way to consumers in the same cluster. Several clustering algorithms have been proposed and developed in the literature, and the choice of the most appropriate set of clustering parameters is crucial for ensuring reliable results. In this paper, an automated service, suited for repeated clustering analysis, is presented. The pipeline is able to process a generic time series dataset and is easily adjustable to test other clustering input parameters; therefore, it may be utilized to find the best set of parameters with the specific dataset. Moreover, it facilitates repeated characterization on real-time load profiles with the aim of detecting sudden changes of consumers behaviors and variable external conditions, which influence the real power forecasting activity on a short temporal scale.      
### 13.Nonlinear damping quantification from phase-resonant tests under base excitation  [ :arrow_down: ](https://arxiv.org/pdf/2205.04735.pdf)
>  The present work addresses the experimental identification of amplitude-dependent modal parameters (modal frequency, damping ratio, Fourier coefficients of periodic modal oscillation). Phase-resonant testing has emerged as an important method for this task, as it substantially reduces the amount of data required for the identification compared to conventional frequency-response testing at different excitation/response levels. In the case of shaker-stinger excitation, the applied excitation force is commonly measured in order to quantify the amplitude-dependent modal damping ratio from the phase-resonant test data. In the case of base excitation, however, the applied excitation force is challenging or impossible to measure. In this work we develop an original method for damping quantification from phase-resonant tests. It relies solely on response measurement; it avoids the need to resort to force measurement. The key idea is to estimate the power provided by the distributed inertia force imposed by the base motion. We develop both a model-free and a model-based variant of the method. We validate the developed method first in virtual experiments of a friction-damped and a geometrically nonlinear system, and then in a physical experiment involving a thin beam clamped at both ends via bolted joints. We conclude that the method is highly robust and provides high accuracy already for a reasonable number of sensors.      
### 14.Preliminary assessment of a cost-effective headphone calibration procedure for soundscape evaluations  [ :arrow_down: ](https://arxiv.org/pdf/2205.04728.pdf)
>  The introduction of ISO 12913-2:2018 has provided a framework for standardized data collection and reporting procedures for soundscape practitioners. A strong emphasis was placed on the use of calibrated head and torso simulators (HATS) for binaural audio capture to obtain an accurate subjective impression and acoustic measure of the soundscape under evaluation. To auralise the binaural recordings as recorded or at set levels, the audio stimuli and the headphone setup are usually calibrated with a HATS. However, calibrated HATS are too financially prohibitive for most research teams, inevitably diminishing the availability of the soundscape standard. With the increasing availability of soundscape binaural recording datasets, and the importance of cross-cultural validation of the soundscape ISO standards, e.g.\ via the Soundscape Attributes Translation Project (SATP), it is imperative to assess the suitability of cost-effective headphone calibration methods to maximise availability without severely compromising on accuracy. Hence, this study objectively examines an open-circuit voltage (OCV) calibration method in comparison to a calibrated HATS on various soundcard and headphone combinations. Preliminary experiments found that calibration with the OCV method differed significantly from the reference binaural recordings in sound pressure levels, whereas negligible differences in levels were observed with the HATS calibration.      
### 15.Robust Medical Image Classification from Noisy Labeled Data with Global and Local Representation Guided Co-training  [ :arrow_down: ](https://arxiv.org/pdf/2205.04723.pdf)
>  Deep neural networks have achieved remarkable success in a wide variety of natural image and medical image computing tasks. However, these achievements indispensably rely on accurately annotated training data. If encountering some noisy-labeled images, the network training procedure would suffer from difficulties, leading to a sub-optimal classifier. This problem is even more severe in the medical image analysis field, as the annotation quality of medical images heavily relies on the expertise and experience of annotators. In this paper, we propose a novel collaborative training paradigm with global and local representation learning for robust medical image classification from noisy-labeled data to combat the lack of high quality annotated medical data. Specifically, we employ the self-ensemble model with a noisy label filter to efficiently select the clean and noisy samples. Then, the clean samples are trained by a collaborative training strategy to eliminate the disturbance from imperfect labeled samples. Notably, we further design a novel global and local representation learning scheme to implicitly regularize the networks to utilize noisy samples in a self-supervised manner. We evaluated our proposed robust learning strategy on four public medical image classification datasets with three types of label noise,ie,random noise, computer-generated label noise, and inter-observer variability noise. Our method outperforms other learning from noisy label methods and we also conducted extensive experiments to analyze each component of our method.      
### 16.Efficient Burst Raw Denoising with Variance Stabilization and Multi-frequency Denoising Network  [ :arrow_down: ](https://arxiv.org/pdf/2205.04721.pdf)
>  With the growing popularity of smartphones, capturing high-quality images is of vital importance to smartphones. The cameras of smartphones have small apertures and small sensor cells, which lead to the noisy images in low light environment. Denoising based on a burst of multiple frames generally outperforms single frame denoising but with the larger compututional cost. In this paper, we propose an efficient yet effective burst denoising system. We adopt a three-stage design: noise prior integration, multi-frame alignment and multi-frame denoising. First, we integrate noise prior by pre-processing raw signals into a variance-stabilization space, which allows using a small-scale network to achieve competitive performance. Second, we observe that it is essential to adopt an explicit alignment for burst denoising, but it is not necessary to integrate a learning-based method to perform multi-frame alignment. Instead, we resort to a conventional and efficient alignment method and combine it with our multi-frame denoising network. At last, we propose a denoising strategy that processes multiple frames sequentially. Sequential denoising avoids filtering a large number of frames by decomposing multiple frames denoising into several efficient sub-network denoising. As for each sub-network, we propose an efficient multi-frequency denoising network to remove noise of different frequencies. Our three-stage design is efficient and shows strong performance on burst denoising. Experiments on synthetic and real raw datasets demonstrate that our method outperforms state-of-the-art methods, with less computational cost. Furthermore, the low complexity and high-quality performance make deployment on smartphones possible.      
### 17.Comparision of Traditional and Fuzzy Failure Mode and Effects Analysis for Smart Grid Electrical Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.04720.pdf)
>  Reliability Assessment is an indispensable technology for identifying, interpreting, and lessening the potential failures in safety-critical systems like smart grids. Failure modes and effects analysis (FMEA) is one of the well documented techniques for risk analysis to study the impact of failure modes on safety critical systems like smart grid. In traditional FMEA failure modes are prioritized based on a numeric assessment known as risk priority number. Risk priority number (RPN) is defined as the product of three risk factors namely severity (S), occurrence (O), and detection (D). These risk factors are generally attained by extensive team efforts and judgments which can lead to errors and inconsistencies. To address the shortcomings of the traditional FMEA, a fuzzy-based FMEA approach is proposed to generate reliable risk priority rankings. In this study, traditional and fuzzy-based FMEA risk priority rankings for smart grid electrical distribution systems are compared and recommendations are made based on the analysis. Results prove the efficiency of the proposed fuzzy-FMEA method.      
### 18.InfraRisk: An Open-Source Simulation Platform for Asset-Level Resilience Analysis in Interconnected Infrastructure Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.04717.pdf)
>  Integrated simulation models are emerging as an alternative for analyzing large-scale interdependent infrastructure networks due to their modeling advantages over traditional interdependency models. This paper presents an open-source integrated simulation package for the asset-level analysis of interdependent infrastructure systems. The simulation platform, named 'InfraRisk' and developed in Python, can simulate disaster-induced infrastructure failures and subsequent post-disaster restoration in interconnected water-, power-, and road networks. InfraRisk consists of an infrastructure module, a hazard module, a recovery module, a simulation module, and a resilience quantification module. The infrastructure module integrates existing infrastructure network packages (wntr for water networks, pandapower for power systems, and a static traffic assignment model for transportation networks) through an interface that facilitates the network-level simulation of interdependent failures. The hazard module generates infrastructure component failure sequences based on various disaster characteristics. The recovery module determines repair sequences and assigns repair crews based on predefined heuristics-based recovery strategies or model predictive control (MPC) based optimization. Based on the schedule, the simulation module implements the network-wide simulation of the consequences of the disaster impacts and the recovery actions. The resilience quantification module offers system-level and consumer-level metrics to quantify both the risks and resilience of the integrated infrastructure networks against disaster events. InfraRisk provides a virtual platform for decision-makers to experiment and develop region-specific pre-disaster and post-disaster policies to enhance the overall resilience of interdependent urban infrastructure networks.      
### 19.Automatic Detection of Microaneurysms in OCT Images Using Bag of Features  [ :arrow_down: ](https://arxiv.org/pdf/2205.04695.pdf)
>  Diabetic Retinopathy (DR) caused by diabetes occurs as a result of changes in the retinal vessels and causes visual impairment. Microaneurysms (MAs) are the early clinical signs of DR, whose timely diagnosis can help detecting DR in the early stages of its development. It has been observed that MAs are more common in the inner retinal layers compared to the outer retinal layers in eyes suffering from DR. Optical Coherence Tomography (OCT) is a noninvasive imaging technique that provides a cross-sectional view of the retina and it has been used in recent years to diagnose many eye diseases. As a result, in this paper has attempted to identify areas with MA from normal areas of the retina using OCT images. This work is done using the dataset collected from FA and OCT images of 20 patients with DR. In this regard, firstly Fluorescein Angiography (FA) and OCT images were registered. Then the MA and normal areas were separated and the features of each of these areas were extracted using the Bag of Features (BOF) approach with Speeded-Up Robust Feature (SURF) descriptor. Finally, the classification process was performed using a multilayer perceptron network. For each of the criteria of accuracy, sensitivity, specificity, and precision, the obtained results were 96.33%, 97.33%, 95.4%, and 95.28%, respectively. Utilizing OCT images to detect MAsautomatically is a new idea and the results obtained as preliminary research in this field are promising .      
### 20.Distributed Transmit Beamforming: Analyzing the Maximum Communication Range  [ :arrow_down: ](https://arxiv.org/pdf/2205.04679.pdf)
>  Distributed transmit beamforming is a technique that adjusts the signals from cooperating radios to combine coherently at a destination radio. To achieve coherent combining, the radios can exchange preambles with the destination for frequency synchronization and signal phase adjustment. At the destination, coherent combining leads to a beamforming (BF) gain. The BF gain can extend the communication range by countering the path loss that increases with the distance from the destination. While ideally the maximum range can be trivially calculated from the BF gain, in reality, the BF gain depends on the distance because, at a larger distance, lower SNR of the exchanged preambles causes higher synchronization and phase estimation errors, which in turn degrades the BF gain. In this paper, considering the BF gain degradation for a destination-led BF protocol, we calculate the maximum communication range to realize a desired post-BF SNR by analyzing the relation between the pre-BF SNR and the BF gain. We show that increasing the preamble lengths or increasing the destination power can significantly increase the maximum range while just increasing the number of radios gives diminishing range extension.      
### 21.Balanced control between performance and saturation for constrained nonlinear systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.04674.pdf)
>  This paper addresses the balanced control between performance and saturation for a class of constrained nonlinear systems, including the branches: balanced command filtered backstepping (BCFB) and balanced performance control (BPC). To balance the interconnection and conflict between performance and saturation constraints, define a performance safety evaluation (PSE) function, which evaluates the system safety under the destabilizing effect variables (DEVs) like saturation quantity and filter errors, then the cumulative effects of DEVs are fully utilized and compensated for the performance recovery. Specifically, there exists some degree of tolerance for the DEVs in the safety region, and the compensation operation works when the evaluation of the system goes dangerous. The advantages of the proposed methodology are illustrated in the numerical simulation.      
### 22.Deep Learning Enabled Semantic Communications with Speech Recognition and Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2205.04603.pdf)
>  In this paper, we develop a deep learning based semantic communication system for speech transmission, named DeepSC-ST. We take the speech recognition and speech synthesis as the transmission tasks of the communication system, respectively. First, the speech recognition-related semantic features are extracted for transmission by a joint semantic-channel encoder and the text is recovered at the receiver based on the received semantic features, which significantly reduces the required amount of data transmission without performance degradation. Then, we perform speech synthesis at the receiver, which dedicates to re-generate the speech signals by feeding the recognized text transcription into a neural network based speech synthesis module. To enable the DeepSC-ST adaptive to dynamic channel environments, we identify a robust model to cope with different channel conditions. According to the simulation results, the proposed DeepSC-ST significantly outperforms conventional communication systems, especially in the low signal-to-noise ratio (SNR) regime. A demonstration is further developed as a proof-of-concept of the DeepSC-ST.      
### 23.A Verification Framework for Certifying Learning-Based Safety-Critical Aviation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.04590.pdf)
>  We present a safety verification framework for design-time and run-time assurance of learning-based components in aviation systems. Our proposed framework integrates two novel methodologies. From the design-time assurance perspective, we propose offline mixed-fidelity verification tools that incorporate knowledge from different levels of granularity in simulated environments. From the run-time assurance perspective, we propose reachability- and statistics-based online monitoring and safety guards for a learning-based decision-making model to complement the offline verification methods. This framework is designed to be loosely coupled among modules, allowing the individual modules to be developed using independent methodologies and techniques, under varying circumstances and with different tool access. The proposed framework offers feasible solutions for meeting system safety requirements at different stages throughout the system development and deployment cycle, enabling the continuous learning and assessment of the system product.      
### 24.Underactuated Source Seeking by Surge Force Tuning: Theory and Boat Experiments  [ :arrow_down: ](https://arxiv.org/pdf/2205.04589.pdf)
>  We extend source seeking algorithms, in the absence of position and velocity measurements, and with tuning of the surge input, from velocity-actuated (unicycle) kinematic models to force-actuated generic Euler-Lagrange dynamic underactuated models. In the design and analysis, we employ a symmetric product approximation, averaging, passivity, and partial-state stability theory. The proposed control law requires only real-time measurement of the source signal at the current position of the vehicle and ensures semi-global practical uniform asymptotic stability (SPUAS) with respect to the linear motion coordinates for the closed-loop system. The performance of our source seeker with surge force tuning is illustrated with both numerical simulations and experiments of an underactuated boat.      
### 25.Implicit Particle Filtering via a Bank of Nonlinear Kalman Filters  [ :arrow_down: ](https://arxiv.org/pdf/2205.04521.pdf)
>  The implicit particle filter seeks to mitigate particle degeneracy by identifying particles in the target distribution's high-probability regions. This study is motivated by the need to enhance computational tractability in implementing this approach. We investigate the connection of the particle update step in the implicit particle filter with that of the Kalman filter and then formulate a novel realization of the implicit particle filter based on a bank of nonlinear Kalman filters. This realization is more amenable and efficient computationally.      
### 26.Nonlinear Model Predictive Control Based on Constraint-Aware Particle Filtering/Smoothing  [ :arrow_down: ](https://arxiv.org/pdf/2205.04497.pdf)
>  Nonlinear model predictive control (NMPC) has gained widespread use in many applications. Its formulation traditionally involves repetitively solving a nonlinear constrained optimization problem online. In this paper, we investigate NMPC through the lens of Bayesian estimation and highlight that the Monte Carlo sampling method can offer a favorable way to implement NMPC. We develop a constraint-aware particle filtering/smoothing method and exploit it to implement NMPC. The new sampling-based NMPC algorithm can be executed easily and efficiently even for complex nonlinear systems, while potentially mitigating the issues of computational complexity and local minima faced by numerical optimization in conventional studies. The effectiveness of the proposed algorithm is evaluated through a simulation study.      
### 27.Skin disease diagnosis using image analysis and natural language processing  [ :arrow_down: ](https://arxiv.org/pdf/2205.04468.pdf)
>  In Zambia, there is a serious shortage of medical staff where each practitioner attends to about 17000 patients in a given district while still, other patients travel over 10 km to access the basic medical services. In this research, we implement a deep learning model that can perform the clinical diagnosis process. The study will prove whether image analysis is capable of performing clinical diagnosis. It will also enable us to understand if we can use image analysis to lessen the workload on medical practitioners by delegating some tasks to an AI. The success of this study has the potential to increase the accessibility of medical services to Zambians, which is one of the national goals of Vision 2030.      
### 28.A Contraction-constrained Model Predictive Control for Multi-timescale Nonlinear Processes  [ :arrow_down: ](https://arxiv.org/pdf/2205.04465.pdf)
>  Many chemical processes exhibit diverse timescale dynamics with a strong coupling between timescale sensitive variables. Model predictive control with a non-uniformly spaced optimisation horizon is an effective approach to multi-timescale control and offers opportunities for reduced computational complexity. In such an approach the fast, moderate and slow dynamics can be included in the optimisation problem by implementing smaller time intervals earlier in the prediction horizon and increasingly larger intervals towards the end of the prediction. In this paper, a reference-flexible condition is developed based on the contraction theory to provide a stability guarantee for a nonlinear system under non-uniform prediction horizons.      
### 29.Learning Visual Styles from Audio-Visual Associations  [ :arrow_down: ](https://arxiv.org/pdf/2205.05072.pdf)
>  From the patter of rain to the crunch of snow, the sounds we hear often convey the visual textures that appear within a scene. In this paper, we present a method for learning visual styles from unlabeled audio-visual data. Our model learns to manipulate the texture of a scene to match a sound, a problem we term audio-driven image stylization. Given a dataset of paired audio-visual data, we learn to modify input images such that, after manipulation, they are more likely to co-occur with a given input sound. In quantitative and qualitative evaluations, our sound-based model outperforms label-based approaches. We also show that audio can be an intuitive representation for manipulating images, as adjusting a sound's volume or mixing two sounds together results in predictable changes to visual style. Project webpage: https://tinglok.netlify.app/files/avstyle      
### 30.Secure and Private Source Coding with Private Key and Decoder Side Information  [ :arrow_down: ](https://arxiv.org/pdf/2205.05068.pdf)
>  The problem of secure source coding with multiple terminals is extended by considering a remote source whose noisy measurements are the correlated random variables used for secure source reconstruction. The main additions to the problem include 1) all terminals noncausally observe a noisy measurement of the remote source; 2) a private key is available to all legitimate terminals; 3) the public communication link between the encoder and decoder is rate-limited; 4) the secrecy leakage to the eavesdropper is measured with respect to the encoder input, whereas the privacy leakage is measured with respect to the remote source. Exact rate regions are characterized for a lossy source coding problem with a private key, remote source, and decoder side information under security, privacy, communication, and distortion constraints. By replacing the distortion constraint with a reliability constraint, we obtain the exact rate region also for the lossless case. Furthermore, the lossy rate region for scalar discrete-time Gaussian sources and measurement channels is established.      
### 31.Seeing Around Obstacles with Terahertz Waves  [ :arrow_down: ](https://arxiv.org/pdf/2205.05066.pdf)
>  Traditional imaging systems, such as the eye or cameras, image scenes that lie in the direct line-of-sight (LoS). Most objects are opaque in the optical and infrared regimes and can limit dramatically the field of view (FoV). Current approaches to see around occlusions exploit the multireflection propagation of signals from neighboring surfaces either in the microwave or the optical bands. Using lower frequency signals anatomical information is limited and images suffer from clutter while optical systems encounter diffuse scattering from most surfaces and suffer from path loss, thus limiting the imaging distance. In this work, we show that terahertz (THz) waves can be used to extend visibility to non-line-of-sight (NLoS) while combining the advantages of both spectra. The material properties and roughness of most building surfaces allow for a unique combination of both diffuse and strong specular scattering. As a result, most building surfaces behave as lossy mirrors that enable propagation paths between a THz camera and the NLoS scenes. We propose a mirror folding algorithm that tracks the multireflection propagation of THz waves to 1) correct the image from cluttering and 2) see around occlusions without a priori knowledge of the scene geometry and material properties. To validate the feasibility of the proposed NLoS imaging approach, we carried out a numerical analysis and developed two THz imaging systems to demonstrate real-world NLoS imaging experiments in sub-THz bands (270-300 GHz). The results show the capability of THz radar imaging systems to recover both the geometry and pose of LoS and NLoS objects with centimeter-scale resolution in various multipath propagation scenarios. THz NLoS imaging can operate in low visibility conditions (e.g., night, strong ambient light, smoke) and uses computationally inexpensive image reconstruction algorithms.      
### 32.Metric Learning based Interactive Modulation for Real-World Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2205.05065.pdf)
>  Interactive image restoration aims to restore images by adjusting several controlling coefficients, which determine the restoration strength. Existing methods are restricted in learning the controllable functions under the supervision of known degradation types and levels. They usually suffer from a severe performance drop when the real degradation is different from their assumptions. Such a limitation is due to the complexity of real-world degradations, which can not provide explicit supervision to the interactive modulation during training. However, how to realize the interactive modulation in real-world super-resolution has not yet been studied. In this work, we present a Metric Learning based Interactive Modulation for Real-World Super-Resolution (MM-RealSR). Specifically, we propose an unsupervised degradation estimation strategy to estimate the degradation level in real-world scenarios. Instead of using known degradation levels as explicit supervision to the interactive mechanism, we propose a metric learning strategy to map the unquantifiable degradation levels in real-world scenarios to a metric space, which is trained in an unsupervised manner. Moreover, we introduce an anchor point strategy in the metric learning process to normalize the distribution of metric space. Extensive experiments demonstrate that the proposed MM-RealSR achieves excellent modulation and restoration performance in real-world super-resolution. Codes are available at <a class="link-external link-https" href="https://github.com/TencentARC/MM-RealSR" rel="external noopener nofollow">this https URL</a>.      
### 33.Multi-agent Reinforcement Learning for Dynamic Resource Management in 6G in-X Subnetworks  [ :arrow_down: ](https://arxiv.org/pdf/2205.05036.pdf)
>  The 6G network enables a subnetwork-wide evolution, resulting in a "network of subnetworks". However, due to the dynamic mobility of wireless subnetworks, the data transmission of intra-subnetwork and inter-subnetwork will inevitably interfere with each other, which poses a great challenge to radio resource management. Moreover, most of the existing approaches require the instantaneous channel gain between subnetworks, which are usually difficult to be collected. To tackle these issues, in this paper we propose a novel effective intelligent radio resource management method using multi-agent deep reinforcement learning (MARL), which only needs the sum of received power, named received signal strength indicator (RSSI), on each channel instead of channel gains. However, to directly separate individual interference from RSSI is an almost impossible thing. To this end, we further propose a novel MARL architecture, named GA-Net, which integrates a hard attention layer to model the importance distribution of inter-subnetwork relationships based on RSSI and exclude the impact of unrelated subnetworks, and employs a graph attention network with a multi-head attention layer to exact the features and calculate their weights that will impact individual throughput. Experimental results prove that our proposed framework significantly outperforms both traditional and MARL-based methods in various aspects.      
### 34.Bike Share's Impact on COVID-19 Transmission and Bike Share's Responses to COVID-19: A case study of Washington DC  [ :arrow_down: ](https://arxiv.org/pdf/2205.05011.pdf)
>  Due to the wide-ranging travel restrictions and lockdowns applied to limit the diffusion of the SARS-CoV2 virus, the coronavirus disease of 2019 (COVID-19) pandemic has had an immediate and significant effect on human mobility at the global, national, and local levels. At the local level, bike-sharing played a significant role in urban transport during the pandemic since riders could travel outdoors with reduced infection risk. However, based on different data resources, this non-motorized mode of transportation was still negatively affected by the pandemic (i.e., relative reduction in ridership). This study has two objectives: 1) to investigate the impact of the COVID-19 pandemic on the numbers and duration of trips conducted through a bike-sharing system -- the Capital Bikeshare in Washington, DC, USA; and 2) to explore whether land use and household income in the nation's capital influence the spatial variation of ridership during the pandemic. Towards realizing these objectives, this research looks at the relationship between bike sharing and COVID-19 transmission as a two-directional relationship rather than a one-directional causal relationship. Accordingly, this study models i) the impact of COVID-19 infection numbers and rates on the use of the Capital Bikeshare system and ii) the risk of COVID-19 transmission among individual bike-sharing users. In other words, we examine i) the cyclist's behavior as a function of the COVID-19 transmission evolution in an urban environment and ii) the possible relationship between the bike share usage and the COVID-19 transmission through adopting a probabilistic contagion model. The findings show the risk of using a bike-sharing system during the pandemic and whether bike sharing remains a healthier alternative mode of transportation in terms of infection risk.      
### 35.Proactive Traffic Offloading in Dynamic Integrated Multi-Satellite Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2205.04940.pdf)
>  The integration between the satellite network and the terrestrial network will play a key role in the upcoming sixth-generation (6G) of mobile cellular networks thanks to the wide coverage and bandwidth offered by satellite networks. To leverage this integration, we propose a proactive traffic offloading scheme in an integrated multi-satellite terrestrial network (IMSTN) that considers the future networks' heterogeneity and predicts their variability. Our proposed offloading scheme hinges on traffic prediction to answer the stringent requirements of data-rate, latency and reliability imposed by heterogeneous and coexisting services and traffic namely enhanced mobile broadband (eMBB), massive machine-type communications (mMTC) and ultra-reliable low latency communication (URLLC). However, the fulfilment of these requirements during offloading in dynamic IMSTN comes at the expense of significant energy consumption and introduces inherently supplementary latency. Therefore, our offloading scheme aims to balance the fundamental trade-offs first between energy consumption and the achievable data-rate and second between energy consumption and latency while meeting the respective needs of the present traffic. Our findings prove the importance of the cooperation between the multi-satellite network and the terrestrial network conditioned by traffic prediction to enhance the performance of IMTSN in terms of latency and energy consumption.      
### 36.Gamified Speaker Comparison by Listening  [ :arrow_down: ](https://arxiv.org/pdf/2205.04923.pdf)
>  We address speaker comparison by listening in a game-like environment, hypothesized to make the task more motivating for naive listeners. We present the same 30 trials selected with the help of an x-vector speaker recognition system from VoxCeleb to a total of 150 crowdworkers recruited through Amazon's Mechanical Turk. They are divided into cohorts of 50, each using one of three alternative interface designs: (i) a traditional (nongamified) design; (ii) a gamified design with feedback on decisions, along with points, game level indications, and possibility for interface customization; (iii) another gamified design with an additional constraint of maximum of 5 'lives' consumed by wrong answers. We analyze the impact of these interface designs to listener error rates (both misses and false alarms), probability calibration, time of quitting, along with survey questionnaire. The results indicate improved performance from (i) to (ii) and (iii), particularly in terms of balancing the two types of detection errors.      
### 37.Safety-guaranteed trajectory planning and control based on GP estimation for unmanned surface vessels  [ :arrow_down: ](https://arxiv.org/pdf/2205.04859.pdf)
>  We propose a safety-guaranteed planning and control framework for unmanned surface vessels (USVs), using Gaussian processes (GPs) to learn uncertainties. The uncertainties encountered by USVs, including external disturbances and model mismatches, are potentially state-dependent, time-varying, and hard to capture with constant models. GP is a powerful learning-based tool that can be integrated with a model-based planning and control framework, which employs a Hamilton-Jacobi differential game formulation. Such a combination yields less conservative trajectories and safety-guaranteeing control strategies. We demonstrate the proposed framework in simulations and experiments on a CLEARPATH Heron USV.      
### 38.The Impact of Partial Occlusion on Pedestrian Detectability  [ :arrow_down: ](https://arxiv.org/pdf/2205.04812.pdf)
>  Robust detection of vulnerable road users is a safety critical requirement for the deployment of autonomous vehicles in heterogeneous traffic. One of the most complex outstanding challenges is that of partial occlusion where a target object is only partially available to the sensor due to obstruction by another foreground object. A number of leading pedestrian detection benchmarks provide annotation for partial occlusion, however each benchmark varies greatly in their definition of the occurrence and severity of occlusion. Recent research demonstrates that a high degree of subjectivity is used to classify occlusion level in these cases and occlusion is typically categorized into 2 to 3 broad categories such as partially and heavily occluded. This can lead to inaccurate or inconsistent reporting of pedestrian detection model performance depending on which benchmark is used. This research introduces a novel, objective benchmark for partially occluded pedestrian detection to facilitate the objective characterization of pedestrian detection models. Characterization is carried out on seven popular pedestrian detection models for a range of occlusion levels from 0-99%. Results demonstrate that pedestrian detection performance degrades, and the number of false negative detections increase as pedestrian occlusion level increases. Of the seven popular pedestrian detection routines characterized, CenterNet has the greatest overall performance, followed by SSDlite. RetinaNet has the lowest overall detection performance across the range of occlusion levels.      
### 39.Hybrid RIS and DMA Assisted Multiuser MIMO Uplink Transmission With Electromagnetic Exposure Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2205.04765.pdf)
>  In the fifth-generation and beyond era, reconfigurable intelligent surface (RIS) and dynamic metasurface antennas (DMAs) are emerging metamaterials keeping up with the demand for high-quality wireless communication services, which promote the diversification of portable wireless terminals. However, along with the rapid expansion of wireless devices, the electromagnetic (EM) radiation increases unceasingly and inevitably affects public health, which requires a limited exposure level in the transmission design. To reduce the EM radiation and preserve the quality of communication service, we investigate the spectral efficiency (SE) maximization with EM constraints for uplink transmission in hybrid RIS and DMA assisted multiuser multiple-input multiple-output systems. Specifically, alternating optimization is adopted to optimize the transmit covariance, RIS phase shift, and DMA weight matrices. We first figure out the water-filling solutions of transmit covariance matrices with given RIS and DMA parameters. Then, the RIS phase shift matrix is optimized via the weighted minimum mean square error, block coordinate descent and minorization-maximization methods. Furthermore, we solve the unconstrainted DMA weight matrix optimization problem in closed form and then design the DMA weight matrix to approach this performance under DMA constraints. Numerical results confirm the effectiveness of the EM aware SE maximization transmission scheme over the conventional baselines.      
### 40.The Road to Industry 4.0 and Beyond: A Communications-, Information-, and Operation Technology Collaboration Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2205.04741.pdf)
>  The fourth industrial revolution, i.e., Industry 4.0, is evolving all around the globe. In this article, we introduce the landscape of Industry 4.0 and beyond empowered by the seamless collaboration of communication technology (CT), information technology (IT), and operation technology (OT), i.e., CIOT collaboration. Specifically, CIOT collaboration is regarded as a main improvement of Industry 4.0 compared to the previous industrial revolutions. We commence by reviewing the previous three industrial revolutions and we argue that the key feature of Industry 4.0 is the CIOT collaboration. More particularly, CT domain supports ubiquitous connectivity of the industrial elements and further bridges the physical world and the cyber world, which is a pivotal prerequisite. Then, we present the potential impacts of CIOT collaboration on typical industrial use cases with the objective of creating a more intelligent and human-friendly industry. Furthermore, the technical challenges of paving the way for the CIOT collaboration with an emphasis on the CT domain are discussed. Finally, we shed light on a roadmap for Industry 4.0 and beyond. The salient steps to be taken in the future CIOT collaboration are highlighted, which may be expected to expedite the paradigm shift towards the next industrial revolution.      
### 41.Integrating Parcel Deliveries into a Ride-Pooling Service -- An Agent-Based Simulation Study  [ :arrow_down: ](https://arxiv.org/pdf/2205.04718.pdf)
>  This paper examines the integration of freight delivery into the passenger transport of an on-demand ride-pooling service. The goal of this research is to use existing passenger trips for logistics services and thus reduce additional vehicle kilometers for freight delivery and the total number of vehicles on the road network. This is achieved by merging the need for two separate fleets into a single one by combining the services. To evaluate the potential of such a mobility-on-demand service, this paper uses an agent-based simulation framework and integrates three heuristic parcel assignment strategies into a ride-pooling fleet control algorithm. Two integration scenarios (moderate and full) are set up. While in both scenarios passengers and parcels share rides in one vehicle, in the moderate scenario no stops for parcel pick-up and delivery are allowed during a passenger ride to decrease customer inconvenience. Using real-world demand data for a case study of Munich, Germany, the two integration scenarios together with the three assignment strategies are compared to the status quo, which uses two separate vehicle fleets for passenger and logistics transport. The results indicate that the integration of logistics services into a ride-pooling service is possible and can exploit unused system capacities without deteriorating passenger transport. Depending on the assignment strategies nearly all parcels can be served until a parcel to passenger demand ratio of 1:10 while the overall fleet kilometers can be deceased compared to the status quo.      
### 42.An asynchronous event-based algorithm for periodic signals  [ :arrow_down: ](https://arxiv.org/pdf/2205.04691.pdf)
>  In this paper, we present a simple event-oriented algorithm for detection of pixel-size signals with a known frequency, by the novel technology of an event camera. In addition, we analyze the ability of the algorithm to filter out the desired periodic signals from random fluctuations. We demonstrate this ability and show how the algorithm can distinguish, during twilight, between the signals of a streetlight that flicker with frequency of 100 Hz, and sun glitter originating from windows in far-away buildings in the field of view.      
### 43.Rate-Convergence Tradeoff of Federated Learning over Wireless Channel  [ :arrow_down: ](https://arxiv.org/pdf/2205.04672.pdf)
>  In this paper, we consider a federated learning problem over wireless channel that takes into account the coding rate and packet transmission errors. Communication channels are modelled as packet erasure channels (PEC), where the erasure probability is determined by the block length, code rate, and signal-to-noise ratio (SNR). To lessen the effect of packet erasure on the FL performance, we propose two schemes in which the central node (CN) reuses either the past local updates or the previous global parameters in case of packet erasure. We investigate the impact of coding rate on the convergence of federated learning (FL) for both short packet and long packet communications considering erroneous transmissions. Our simulation results shows that even one unit of memory has considerable impact on the performance of FL in erroneous communication.      
### 44.Real-Time Wearable Gait Phase Segmentation For Running And Walking  [ :arrow_down: ](https://arxiv.org/pdf/2205.04668.pdf)
>  Previous gait phase detection as convolutional neural network (CNN) based classification task requires cumbersome manual setting of time delay or heavy overlapped sliding windows to accurately classify each phase under different test cases, which is not suitable for streaming Inertial-Measurement-Unit (IMU) sensor data and fails to adapt to different scenarios. This paper presents a segmentation based gait phase detection with only a single six-axis IMU sensor, which can easily adapt to both walking and running at various speeds. The proposed segmentation uses CNN with gait phase aware receptive field setting and IMU oriented processing order, which can fit to high sampling rate of IMU up to 1000Hz for high accuracy and low sampling rate down to 20Hz for real time calculation. The proposed model on the 20Hz sampling rate data can achieve average error of 8.86 ms in swing time, 9.12 ms in stance time and 96.44\% accuracy of gait phase detection and 99.97\% accuracy of stride detection. Its real-time implementation on mobile phone only takes 36 ms for 1 second length of sensor data.      
### 45.Deep Gait Tracking With Inertial Measurement Unit  [ :arrow_down: ](https://arxiv.org/pdf/2205.04666.pdf)
>  This paper presents a convolutional neural network based foot motion tracking with only six-axis Inertial-Measurement-Unit (IMU) sensor data. The presented approach can adapt to various walking conditions by adopting differential and window based input. The training data are further augmented by sliding and random window samplings on IMU sensor data to increase data diversity for better performance. The proposed approach fuses predictions of three dimensional output into one model. The proposed fused model can achieve average error of 2.30+-2.23 cm in X-axis, 0.91+-0.95 cm in Y-axis and 0.58+-0.52 cm in Z-axis.      
### 46.A 14uJ/Decision Keyword Spotting Accelerator with In-SRAM-Computing and On Chip Learning for Customization  [ :arrow_down: ](https://arxiv.org/pdf/2205.04665.pdf)
>  Keyword spotting has gained popularity as a natural way to interact with consumer devices in recent years. However, because of its always-on nature and the variety of speech, it necessitates a low-power design as well as user customization. This paper describes a low-power, energy-efficient keyword spotting accelerator with SRAM based in-memory computing (IMC) and on-chip learning for user customization. However, IMC is constrained by macro size, limited precision, and non-ideal effects. To address the issues mentioned above, this paper proposes bias compensation and fine-tuning using an IMC-aware model design. Furthermore, because learning with low-precision edge devices results in zero error and gradient values due to quantization, this paper proposes error scaling and small gradient accumulation to achieve the same accuracy as ideal model training. The simulation results show that with user customization, we can recover the accuracy loss from 51.08\% to 89.76\% with compensation and fine-tuning and further improve to 96.71\% with customization. The chip implementation can successfully run the model with only 14$uJ$ per decision. When compared to the state-of-the-art works, the presented design has higher energy efficiency with additional on-chip model customization capabilities for higher accuracy.      
### 47.Composite IG/FTR Channel Performance in Wireless Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2205.04578.pdf)
>  We present a composite wireless fading model encompassing multipath fading and shadowing based on fluctuating two-ray (FTR) fading and inverse gamma (IG) shadowing. We first determine an alternative framework for the statistical characterization and performance evaluation of the FTR fading model, which is based on the fact that the FTR fading distribution can be described as an underlying Rician Shadowed (RS) distribution with continuously varying parameter Kr (ratio of specular to diffuse components). We demonstrate that this new formulation permits to obtain a closed-form expression of the generalized moment generating function (GMGF) of the FTR model, from which the PDF and CDF of the composite IG/FTR model can be obtained in closed-form. The exact and asymptotic outage probability of the IG/FTR model are analyzed and verified by Monte Carlo simulations.      
### 48.A Realistic Cyclist Model for SUMO Based on the SimRa Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2205.04538.pdf)
>  Increasing the modal share of bicycle traffic to reduce carbon emissions, reduce urban car traffic, and to improve the health of citizens, requires a shift away from car-centric city planning. For this, traffic planners often rely on simulation tools such as SUMO which allow them to study the effects of construction changes before implementing them. Similarly, studies of vulnerable road users, here cyclists, also use such models to assess the performance of communication-based road traffic safety systems. The cyclist model in SUMO, however, is very imprecise as SUMO cyclists behave either like slow cars or fast pedestrians, thus, casting doubt on simulation results for bicycle traffic. In this paper, we analyze acceleration, velocity, and intersection left-turn behavior of cyclists in a large dataset of real world cycle tracks. We use the results to derive an improved cyclist model and implement it in SUMO.      
### 49.Surreal-GAN:Semi-Supervised Representation Learning via GAN for uncovering heterogeneous disease-related imaging patterns  [ :arrow_down: ](https://arxiv.org/pdf/2205.04523.pdf)
>  A plethora of machine learning methods have been applied to imaging data, enabling the construction of clinically relevant imaging signatures of neurological and neuropsychiatric diseases. Oftentimes, such methods don't explicitly model the heterogeneity of disease effects, or approach it via nonlinear models that are not interpretable. Moreover, unsupervised methods may parse heterogeneity that is driven by nuisance confounding factors that affect brain structure or function, rather than heterogeneity relevant to a pathology of interest. On the other hand, semi-supervised clustering methods seek to derive a dichotomous subtype membership, ignoring the truth that disease heterogeneity spatially and temporally extends along a continuum. To address the aforementioned limitations, herein, we propose a novel method, termed Surreal-GAN (Semi-SUpeRvised ReprEsentAtion Learning via GAN). Using cross-sectional imaging data, Surreal-GAN dissects underlying disease-related heterogeneity under the principle of semi-supervised clustering (cluster mappings from normal control to patient), proposes a continuously dimensional representation, and infers the disease severity of patients at individual level along each dimension. The model first learns a transformation function from normal control (CN) domain to the patient (PT) domain with latent variables controlling transformation directions. An inverse mapping function together with regularization on function continuity, pattern orthogonality and monotonicity was also imposed to make sure that the transformation function captures necessarily meaningful imaging patterns with clinical significance. We first validated the model through extensive semi-synthetic experiments, and then demonstrate its potential in capturing biologically plausible imaging patterns in Alzheimer's disease (AD).      
### 50.Sampling-Based Nonlinear MPC of Neural Network Dynamics with Application to Autonomous Vehicle Motion Planning  [ :arrow_down: ](https://arxiv.org/pdf/2205.04506.pdf)
>  Control of machine learning models has emerged as an important paradigm for a broad range of robotics applications. In this paper, we present a sampling-based nonlinear model predictive control (NMPC) approach for control of neural network dynamics. We show its design in two parts: 1) formulating conventional optimization-based NMPC as a Bayesian state estimation problem, and 2) using particle filtering/smoothing to achieve the estimation. Through a principled sampling-based implementation, this approach can potentially make effective searches in the control action space for optimal control and also facilitate computation toward overcoming the challenges caused by neural network dynamics. We apply the proposed NMPC approach to motion planning for autonomous vehicles. The specific problem considers nonlinear unknown vehicle dynamics modeled as neural networks as well as dynamic on-road driving scenarios. The approach shows significant effectiveness in successful motion planning in case studies.      
### 51.Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization  [ :arrow_down: ](https://arxiv.org/pdf/2205.04464.pdf)
>  We propose a new microscopy simulation system that can depict atomistic models in a micrograph visual style, similar to results of physical electron microscopy imaging. This system is scalable, able to represent simulation of electron microscopy of tens of viral particles and synthesizes the image faster than previous methods. On top of that, the simulator is differentiable, both its deterministic as well as stochastic stages that form signal and noise representations in the micrograph. This notable property has the capability for solving inverse problems by means of optimization and thus allows for generation of microscopy simulations using the parameter settings estimated from real data. We demonstrate this learning capability through two applications: (1) estimating the parameters of the modulation transfer function defining the detector properties of the simulated and real micrographs, and (2) denoising the real data based on parameters trained from the simulated examples. While current simulators do not support any parameter estimation due to their forward design, we show that the results obtained using estimated parameters are very similar to the results of real micrographs. Additionally, we evaluate the denoising capabilities of our approach and show that the results showed an improvement over state-of-the-art methods. Denoised micrographs exhibit less noise in the tilt-series tomography reconstructions, ultimately reducing the visual dominance of noise in direct volume rendering of microscopy tomograms.      
