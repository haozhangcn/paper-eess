# ArXiv eess --Wed, 9 Feb 2022
### 1.Analysis of Voltage Stability in Terms of Interactions of Q(U)-Characteristic Control in Distribution Grids  [ :arrow_down: ](https://arxiv.org/pdf/2202.03986.pdf)
>  As the amount of volatile, renewable energy sources in power distribution grids is increasing, the stability of the latter is a vital aspect for grid operators. Within the STABEEL project, the authors develop rules on how to parametrize the reactive power control of distributed energy resources to increase the performance while guaranteeing stability. The work focuses on distribution grids with a high penetration of distributed energy resources equipped with Q(U)-characteristic. This contribution is based on the stability assessment of previous work and introduces a new approach utilizing the circle criterion. With the aim of extending existing technical guidelines, stability assessment methods are applied to various distribution grids - including those from the SimBench project. Herein, distributed energy resources can be modelled as detailed control loops or as approximations, derived from technical guidelines.      
### 2.Communication-Control Co-design in Wireless Edge Industrial Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.03976.pdf)
>  We consider the problem of controlling a series of industrial systems, such as industrial robotics, in a factory environment over a shared wireless channel leveraging edge computing capabilities. The wireless control system model supports the offloading of computational intensive functions, such as perception workloads, to an edge server. However, wireless communications is prone to packet loss and latency and can lead to instability or task failure if the link is not kept sufficiently reliable. Because maintaining high reliability and low latency at all times prohibits scalability due to resource limitations, we propose a communication-control co-design paradigm that varies the network quality of service (QoS) and resulting control actions to the dynamic needs of each plant. We further propose a modular learning framework to solve the complex learning task without knowledge of plant or communication models in a series of learning steps and demonstrate its effectiveness in learning resource-efficient co-design policies in a robotic conveyor belt task.      
### 3.Edge-based fever screening system over private 5G  [ :arrow_down: ](https://arxiv.org/pdf/2202.03917.pdf)
>  Edge computing and 5G have made it possible to perform analytics closer to the source of data and achieve super-low latency response times, which is not possible with centralized cloud deployment. In this paper, we present a novel fever-screening system, which uses edge machine learning techniques and leverages private 5G to accurately identify and screen individuals with fever in real-time. Particularly, we present deep-learning based novel techniques for fusion and alignment of cross-spectral visual and thermal data streams at the edge. Our novel Cross-Spectral Generative Adversarial Network (CS-GAN) synthesizes visual images that have the key, representative object level features required to uniquely associate objects across visual and thermal spectrum. Two key features of CS-GAN are a novel, feature-preserving loss function that results in high-quality pairing of corresponding cross-spectral objects, and dual bottleneck residual layers with skip connections (a new, network enhancement) to not only accelerate real-time inference, but to also speed up convergence during model training at the edge. To the best of our knowledge, this is the first technique that leverages 5G networks and limited edge resources to enable real-time feature-level association of objects in visual and thermal streams (30 ms per full HD frame on an Intel Core i7-8650 4-core, 1.9GHz mobile processor). To the best of our knowledge, this is also the first system to achieve real-time operation, which has enabled fever screening of employees and guests in arenas, theme parks, airports and other critical facilities. By leveraging edge computing and 5G, our fever screening system is able to achieve 98.5% accuracy and is able to process about 5X more people when compared to a centralized cloud deployment.      
### 4.Unsupervised Source Separation via Self-Supervised Training  [ :arrow_down: ](https://arxiv.org/pdf/2202.03875.pdf)
>  We introduce two novel unsupervised (blind) source separation methods, which involve self-supervised training from single-channel two-source speech mixtures without any access to the ground truth source signals. Our first method employs permutation invariant training (PIT) to separate artificially-generated mixtures of the original mixtures back into the original mixtures, which we named mixture permutation invariant training (MixPIT). We found this challenging objective to be a valid proxy task for learning to separate the underlying sources. We improve upon this first method by creating mixtures of source estimates and employing PIT to separate these new mixtures in a cyclic fashion. We named this second method cyclic mixture permutation invariant training (MixCycle), where cyclic refers to the fact that we use the same model to produce artificial mixtures and to learn from them continuously. We show that MixPIT outperforms a common baseline (MixIT) on our small dataset (SC09Mix), and they have comparable performance on a standard dataset (LibriMix). Strikingly, we also show that MixCycle surpasses the performance of supervised PIT by being data-efficient, thanks to its inherent data augmentation mechanism. To the best of our knowledge, no other purely unsupervised method is able to match or exceed the performance of supervised training.      
### 5.A covariant, discrete time-frequency representation tailored for zero-based signal detection  [ :arrow_down: ](https://arxiv.org/pdf/2202.03835.pdf)
>  Recent work in time-frequency analysis proposed to switch the focus from the maxima of the spectrogram toward its zeros. The zeros of signals in white Gaussian noise indeed form a random point pattern with a very stable structure. Using modern spatial statistics tools on the pattern of zeros of a spectrogram has led to component disentanglement and signal detection procedures. The major bottlenecks of this approach are the discretization of the Short-Time Fourier Transform and the necessarily bounded observation window in the time-frequency plane. Both impact the estimation of summary statistics of the zeros, which are then used in standard statistical tests. To circumvent these limitations, we propose a generalized time-frequency representation, which we call the Kravchuk transform. It naturally applies to finite signals, i.e., finite-dimensional vectors. The corresponding phase space, instead of the whole time-frequency plane, is compact, and particularly amenable to spatial statistics. On top of this, the Kravchuk transform has several natural properties for signal processing, among which covariance under the action of SO(3), invertibility and symmetry with respect to complex conjugation. We further show that the point process of the zeros of the Kravchuk transform of discrete white Gaussian noise coincides in law with the zeros of the spherical Gaussian Analytic Function. This implies that the law of the zeros is invariant under isometries of the sphere. Elaborating on this theorem, we develop a procedure for signal detection based on the spatial statistics of the zeros of the Kravchuk spectrogram. The statistical power of this procedure is assessed by intensive numerical simulation, and compares favorably with respect to state-of-the-art zeros-based detection procedures. Furthermore it appears to be particularly robust to both low signal-to-noise ratio and small number of samples.      
### 6.On the Pitfalls of Using the Residual Error as Anomaly Score  [ :arrow_down: ](https://arxiv.org/pdf/2202.03826.pdf)
>  Many current state-of-the-art methods for anomaly localization in medical images rely on calculating a residual image between a potentially anomalous input image and its "healthy" reconstruction. As the reconstruction of the unseen anomalous region should be erroneous, this yields large residuals as a score to detect anomalies in medical images. However, this assumption does not take into account residuals resulting from imperfect reconstructions of the machine learning models used. Such errors can easily overshadow residuals of interest and therefore strongly question the use of residual images as scoring function. Our work explores this fundamental problem of residual images in detail. We theoretically define the problem and thoroughly evaluate the influence of intensity and texture of anomalies against the effect of imperfect reconstructions in a series of experiments. Code and experiments are available under <a class="link-external link-https" href="https://github.com/FeliMe/residual-score-pitfalls" rel="external noopener nofollow">this https URL</a>      
### 7.Design and Implementation of Electronic Infrastructure For Academic Establishment  [ :arrow_down: ](https://arxiv.org/pdf/2202.03801.pdf)
>  Most establishments including academic institutions under goes the lengthy process of study-based document handling such as direct mailing, indexing and tracking. This daily task is time consuming and resource-intensive. Using a private network dedicated for such document management would benefit the establishment increasing operational efficiency. In this study, the Information and Communication Engineering (ICE) Department was used as a model to determine the requirements needed to build the intranet network. A packet tracer simulator was used to build a virtual intranet architecture.Then the simulation report was examined to ensure optimum functionality. Upon establishing a stable behavior an intranet infrastructure building commenced using the available hardware components and software.The system architecture was based on Windows 2012R2 server to manage 3 separated sub-networks connected to three switches and one router. Running the intranet for one semester proved its success in providing a fast, cheap and simplified service for all department needs. The accomplished system is a step forward to achieve a full electronic department in scientific establishments.      
### 8.Coordination of resources at the edge of the electricity grid: systematic review and taxonomy  [ :arrow_down: ](https://arxiv.org/pdf/2202.03786.pdf)
>  This paper proposes a novel taxonomy of coordination strategies for distributed energy resources at the edge of the electricity grid, based on a systematic analysis of key literature trends. The coordination of distributed energy resources such as decentralised generation and flexibility sources is critical for decarbonising electricity and achieving climate goals. The literature on the topic is growing exponentially; however, there is ambiguity in the terminology used to date. We seek to resolve this lack of clarity by synthesising the categories of coordination strategies in a novel exhaustive, mutually exclusive taxonomy based on agency, information and game type. The relevance of these concepts in the literature is illustrated through a systematic literature review of 84,741 publications using a structured topic search query. Then 93 selected coordination strategies are analysed in more detail and mapped onto this framework. Clarity on structural assumptions is key for selecting appropriate coordination strategies for differing contexts within energy systems. We argue that a plurality of complementary strategies is needed to coordinate energy systems' different components and achieve deep decarbonisation.      
### 9.Energy Management Based on Multi-Agent Deep Reinforcement Learning for A Multi-Energy Industrial Park  [ :arrow_down: ](https://arxiv.org/pdf/2202.03771.pdf)
>  Owing to large industrial energy consumption, industrial production has brought a huge burden to the grid in terms of renewable energy access and power supply. Due to the coupling of multiple energy sources and the uncertainty of renewable energy and demand, centralized methods require large calculation and coordination overhead. Thus, this paper proposes a multi-energy management framework achieved by decentralized execution and centralized training for an industrial park. The energy management problem is formulated as a partially-observable Markov decision process, which is intractable by dynamic programming due to the lack of the prior knowledge of the underlying stochastic process. The objective is to minimize long-term energy costs while ensuring the demand of users. To solve this issue and improve the calculation speed, a novel multi-agent deep reinforcement learning algorithm is proposed, which contains the following key points: counterfactual baseline for facilitating contributing agents to learn better policies, soft actor-critic for improving robustness and exploring optimal solutions. A novel reward is designed by Lagrange multiplier method to ensure the capacity constraints of energy storage. In addition, considering that the increase in the number of agents leads to performance degradation due to large observation spaces, an attention mechanism is introduced to enhance the stability of policy and enable agents to focus on important energy-related information, which improves the exploration efficiency of soft actor-critic. Numerical results based on actual data verify the performance of the proposed algorithm with high scalability, indicating that the industrial park can minimize energy costs under different demands.      
### 10.InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training  [ :arrow_down: ](https://arxiv.org/pdf/2202.03751.pdf)
>  Denoising diffusion probabilistic models (diffusion models for short) require a large number of iterations in inference to achieve the generation quality that matches or surpasses the state-of-the-art generative models, which invariably results in slow inference speed. Previous approaches aim to optimize the choice of inference schedule over a few iterations to speed up inference. However, this results in reduced generation quality, mainly because the inference process is optimized separately, without jointly optimizing with the training process. In this paper, we propose InferGrad, a diffusion model for vocoder that incorporates inference process into training, to reduce the inference iterations while maintaining high generation quality. More specifically, during training, we generate data from random noise through a reverse process under inference schedules with a few iterations, and impose a loss to minimize the gap between the generated and ground-truth data samples. Then, unlike existing approaches, the training of InferGrad considers the inference process. The advantages of InferGrad are demonstrated through experiments on the LJSpeech dataset showing that InferGrad achieves better voice quality than the baseline WaveGrad under same conditions while maintaining the same voice quality as the baseline but with $3$x speedup ($2$ iterations for InferGrad vs $6$ iterations for WaveGrad).      
### 11.A Survey of Breast Cancer Screening Techniques: Thermography and Electrical Impedance Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2202.03737.pdf)
>  Breast cancer is a disease that threatens many women's life, thus, early and accurate detection plays a key role in reducing the mortality rate. Mammography stands as the reference technique for breast cancer screening; nevertheless, many countries still lack access to mammograms due to economic, social, and cultural issues. Last advances in computational tools, infrared cameras, and devices for bio-impedance quantification allowed the development of parallel techniques like thermography, infrared imaging, and electrical impedance tomography, these being faster, reliable and cheaper. In the last decades, these have been considered as complement procedures for breast cancer diagnosis, where many studies concluded that false positive and false negative rates are greatly reduced. This work aims to review the last breakthroughs about the three above-mentioned techniques describing the benefits of mixing several computational skills to obtain a better global performance. In addition, we provide a comparison between several machine learning techniques applied to breast cancer diagnosis going from logistic regression, decision trees, and random forest to artificial, deep, and convolutional neural networks. Finally, it is mentioned several recommendations for 3D breast simulations, pre-processing techniques, biomedical devices in the research field, prediction of tumor location and size.      
### 12.Exact Decomposition of Multifrequency Discrete Real and Complex Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.03722.pdf)
>  'The spectral leakage from windowing and the picket fence effect from discretization' have been among the standard contents in textbooks for many decades. The spectral leakage and picket fence effect would cause the distortions in amplitude, frequency, and phase of signals, which have always been of concern, and attempts have been made to solve them. This paper proposes two novel decomposition theorems that can totally eliminate the spectral leakage and picket fence effect, and could broaden the knowledge of signal processing. First, two generalized eigenvalue equations are constructed for multifrequency discrete real signals and complex signals. The two decomposition theorems are then proved. On these bases, exact decomposition methods for real and complex signals are proposed. For a noise-free multifrequency real signal with m sinusoidal components, the frequency, amplitude, and phase of each component can be exactly calculated by using just 4m-1 discrete values and its second-order derivatives. For a multifrequency complex signal, only 2m-1 discrete values and its first-order derivatives are needed. The numerical experiments show that the proposed methods have very high resolution, and the sampling rate does not necessarily obey the Nyquist sampling theorem. With noisy signals, the proposed methods have extraordinary accuracy.      
### 13.CAD-RADS Scoring using Deep Learning and Task-Specific Centerline Labeling  [ :arrow_down: ](https://arxiv.org/pdf/2202.03671.pdf)
>  With coronary artery disease (CAD) persisting to be one of the leading causes of death worldwide, interest in supporting physicians with algorithms to speed up and improve diagnosis is high. In clinical practice, the severeness of CAD is often assessed with a coronary CT angiography (CCTA) scan and manually graded with the CAD-Reporting and Data System (CAD-RADS) score. The clinical questions this score assesses are whether patients have CAD or not (rule-out) and whether they have severe CAD or not (hold-out). In this work, we reach new state-of-the-art performance for automatic CAD-RADS scoring. We propose using severity-based label encoding, test time augmentation (TTA) and model ensembling for a task-specific deep learning architecture. Furthermore, we introduce a novel task- and model-specific, heuristic coronary segment labeling, which subdivides coronary trees into consistent parts across patients. It is fast, robust, and easy to implement. We were able to raise the previously reported area under the receiver operating characteristic curve (AUC) from 0.914 to 0.942 in the rule-out and from 0.921 to 0.950 in the hold-out task respectively.      
### 14.Energy Efficiency and Delay Tradeoff in an MEC-Enabled Mobile IoT Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.03648.pdf)
>  Mobile Edge Computing (MEC) has recently emerged as a promising technology in the 5G era. It is deemed an effective paradigm to support computation-intensive and delay critical applications even at energy-constrained and computation-limited Internet of Things (IoT) devices. To effectively exploit the performance benefits enabled by MEC, it is imperative to jointly allocate radio and computational resources by considering non-stationary computation demands, user mobility, and wireless fading channels. This paper aims to study the tradeoff between energy efficiency (EE) and service delay for multi-user multi-server MEC-enabled IoT systems when provisioning offloading services in a user mobility scenario. Particularly, we formulate a stochastic optimization problem with the objective of minimizing the long-term average network EE with the constraints of the task queue stability, peak transmit power, maximum CPU-cycle frequency, and maximum user number. To tackle the problem, we propose an online offloading and resource allocation algorithm by transforming the original problem into several individual subproblems in each time slot based on Lyapunov optimization theory, which are then solved by convex decomposition and submodular methods. Theoretical analysis proves that the proposed algorithm can achieve a $[O(1/V), O(V)]$ tradeoff between EE and service delay. Simulation results verify the theoretical analysis and demonstrate our proposed algorithm can offer much better EE-delay performance in task offloading challenges, compared to several baselines.      
### 15.An Exact Method for the Daily Package Shipment Problem with Outsourcing  [ :arrow_down: ](https://arxiv.org/pdf/2202.03614.pdf)
>  The package shipment problem requires to optimally co-design paths for both packages and a heterogeneous fleet in a transit center network (TCN). Instances arising from the package delivery industry in China usually involve more than ten thousand origin-destination (OD) pairs and have to be solved daily within an hour. Motivated by the fact that there is no interaction among different origin centers due to their competitive relationship, we propose a novel two-layer localized package shipment on a TCN (LPS-TCN) model that exploits outsourcing for cost saving. Consequently, the original problem breaks into a set of much smaller shipment problems, each of which has hundreds of OD pairs and is subsequently modelled as a mixed integer program (MIP). Since the LPS-TCN model is proved to be Strongly NP-hard and contains tens of thousands of feasible paths, an off-the-shelf MIP solver cannot produce a reliable solution in a practically acceptable amount of time. We develop a column generation based algorithm that iteratively adds "profitable" paths and further enhance it by problem-specific cutting planes and variable bound tightening techniques. Computational experiments on realistic instances from a major Chinese package express company demonstrate that the LPS-TCN model can yield solutions that bring daily economic cost reduction up to 1 million CNY for the whole TCN. In addition, our proposed algorithm solves the LPS-TCN model substantially faster than CPLEX, one of the state-of-the-art commercial MIP solvers.      
### 16.Multiple Correlated Jammers Nullification using LSTM-based Deep Dueling Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.03600.pdf)
>  Suppressing the deliberate interference for wireless networks is critical to guarantee a reliable communication link. However, nullifying the jamming signals can be problematic when the correlations between transmitted jamming signals are deliberately varied over time. Specifically, recent studies reveal that by deliberately varying the correlations among jamming signals, attackers can effectively vary the jamming channels and thus their nullspace, even when the physical channels remain unchanged. That makes the beam-forming matrix derived from the nullspace of the jamming channels unable to suppress the jamming signals. Most existing solutions only consider unchanged correlations or heuristically adapt to the time-varying correlation problem by continuously monitoring the residual jamming signals before updating the beam-forming matrix. In this paper, we systematically formulate the optimization problem of the nullspace estimation and data transmission phases. Even ignoring the unknown strategy of the jammers and the challenging nullspace estimation process, the resulting problem is an integer programming problem, hence intractable to obtain its optimal solution. To tackle it and address the unknown strategy of the jammer, we reformulate the problem using a partially observable semi-Markov decision process (POSMDP) and then design a deep dueling Q-learning based framework to tune the duration of the nullspace estimation and data transmission phases. Extensive simulations demonstrate that the proposed techniques effectively deal with jamming signals whose correlations vary over time, and the range of correlations is unknown. Especially, our techniques do not require continuous monitoring of the residual jamming signals (after the nullification process) before updating the beam-forming matrix. As such, the system is more spectral-efficient and has a lower outage probability.      
### 17.Model and predict age and sex in healthy subjects using brain white matter features: A deep learning approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.03595.pdf)
>  The human brain's white matter (WM) structure is of immense interest to the scientific community. Diffusion MRI gives a powerful tool to describe the brain WM structure noninvasively. To potentially enable monitoring of age-related changes and investigation of sex-related brain structure differences on the mapping between the brain connectome and healthy subjects' age and sex, we extract fiber-cluster-based diffusion features and predict sex and age with a novel ensembled neural network classifier. We conduct experiments on the Human Connectome Project (HCP) young adult dataset and show that our model achieves 94.82% accuracy in sex prediction and 2.51 years MAE in age prediction. We also show that the fractional anisotropy (FA) is the most predictive of sex, while the number of fibers is the most predictive of age and the combination of different features can improve the model performance.      
### 18.Representative Scenarios to Capture Renewable Generation Stochasticity and Cross-Correlations  [ :arrow_down: ](https://arxiv.org/pdf/2202.03588.pdf)
>  Generating representative scenarios for power system planning in which the stochasticity of renewable generation and cross-correlations between renewables and load are fully captured, is a challenging problem. Traditional methods for scenario generation often fail to generate diverse scenarios that include both seasonal (frequently occurring) and atypical (extreme) days required for planning purposes. This paper presents a methodical approach to generate representative scenarios. It also proposes new metrics that are more relevant for evaluating the generated scenarios from an applications perspective. When applied to historical data from a power utility, the proposed approach resulted in scenarios that included a good mix of seasonal and atypical days. The results also demonstrated pertinence of the proposed cluster validation metrics. Finally, the paper presents a trade-off for determining optimal number of scenarios for a given application.      
### 19.CALM: Contrastive Aligned Audio-Language Multirate and Multimodal Representations  [ :arrow_down: ](https://arxiv.org/pdf/2202.03587.pdf)
>  Deriving multimodal representations of audio and lexical inputs is a central problem in Natural Language Understanding (NLU). In this paper, we present Contrastive Aligned Audio-Language Multirate and Multimodal Representations (CALM), an approach for learning multimodal representations using contrastive and multirate information inherent in audio and lexical inputs. The proposed model aligns acoustic and lexical information in the input embedding space of a pretrained language-only contextual embedding model. By aligning audio representations to pretrained language representations and utilizing contrastive information between acoustic inputs, CALM is able to bootstrap audio embedding competitive with existing audio representation models in only a few hours of training time. Operationally, audio spectrograms are processed using linearized patches through a Spectral Transformer (SpecTran) which is trained using a Contrastive Audio-Language Pretraining objective to align audio and language from similar queries. Subsequently, the derived acoustic and lexical tokens representations are input into a multimodal transformer to incorporate utterance level context and derive the proposed CALM representations. We show that these pretrained embeddings can subsequently be used in multimodal supervised tasks and demonstrate the benefits of the proposed pretraining steps in terms of the alignment of the two embedding spaces and the multirate nature of the pretraining. Our system shows 10-25\% improvement over existing emotion recognition systems including state-of-the-art three-modality systems under various evaluation objectives.      
### 20.Multi-Label Classification of Thoracic Diseases using Dense Convolutional Network on Chest Radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2202.03583.pdf)
>  Chest X-ray images are one of the most common medical diagnosis techniques to identify different thoracic diseases. However, identification of pathologies in X-ray images requires skilled manpower and are often cited as a time-consuming task with varied level of interpretation, particularly in cases where the identification of disease only by images is difficult for human eyes. With recent achievements of deep learning in image classification, its application in disease diagnosis has been widely explored. This research project presents a multi-label disease diagnosis model of chest x-rays. Using Dense Convolutional Neural Network (DenseNet), the diagnosis system was able to obtain high classification predictions. The model obtained the highest AUC score of 0.896 for condition Cardiomegaly and the lowest AUC score for Nodule, 0.655. The model also localized the parts of the chest radiograph that indicated the presence of each pathology using GRADCAM, thus contributing to the model interpretability of a deep learning algorithm.      
### 21.Metal Artifact Reduction with Intra-Oral Scan Data for 3D Low Dose Maxillofacial CBCT Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2202.03571.pdf)
>  Low-dose dental cone beam computed tomography (CBCT) has been increasingly used for maxillofacial modeling. However, the presence of metallic inserts, such as implants, crowns, and dental filling, causes severe streaking and shading artifacts in a CBCT image and loss of the morphological structures of the teeth, which consequently prevents accurate segmentation of bones. A two-stage metal artifact reduction method is proposed for accurate 3D low-dose maxillofacial CBCT modeling, where a key idea is to utilize explicit tooth shape prior information from intra-oral scan data whose acquisition does not require any extra radiation exposure. In the first stage, an image-to-image deep learning network is employed to mitigate metal-related artifacts. To improve the learning ability, the proposed network is designed to take advantage of the intra-oral scan data as side-inputs and perform multi-task learning of auxiliary tooth segmentation. In the second stage, a 3D maxillofacial model is constructed by segmenting the bones from the dental CBCT image corrected in the first stage. For accurate bone segmentation, weighted thresholding is applied, wherein the weighting region is determined depending on the geometry of the intra-oral scan data. Because acquiring a paired training dataset of metal-artifact-free and metal artifact-affected dental CBCT images is challenging in clinical practice, an automatic method of generating a realistic dataset according to the CBCT physics model is introduced. Numerical simulations and clinical experiments show the feasibility of the proposed method, which takes advantage of tooth surface information from intra-oral scan data in 3D low dose maxillofacial CBCT modeling.      
### 22.Phase-Stretch Adaptive Gradient-Field Extractor (PAGE)  [ :arrow_down: ](https://arxiv.org/pdf/2202.03570.pdf)
>  Phase-Stretch Adaptive Gradient-Field Extractor (PAGE) is an edge detection algorithm that is inspired by physics of electromagnetic diffraction and dispersion. A computational imaging algorithm, it identifies edges, their orientations and sharpness in a digital image where the image brightness changes abruptly. Edge detection is a basic operation performed by the eye and is crucial to visual perception. PAGE embeds an original image into a set of feature maps that can be used for object representation and classification. The algorithm performs exceptionally well as an edge and texture extractor in low light level and low contrast images. This manuscript is prepared to support the open-source code which is being simultaneously made available within the GitHub repository <a class="link-external link-https" href="https://github.com/JalaliLabUCLA" rel="external noopener nofollow">this https URL</a>.      
### 23.Accurate super-resolution low-field brain MRI  [ :arrow_down: ](https://arxiv.org/pdf/2202.03564.pdf)
>  The recent introduction of portable, low-field MRI (LF-MRI) into the clinical setting has the potential to transform neuroimaging. However, LF-MRI is limited by lower resolution and signal-to-noise ratio, leading to incomplete characterization of brain regions. To address this challenge, recent advances in machine learning facilitate the synthesis of higher resolution images derived from one or multiple lower resolution scans. Here, we report the extension of a machine learning super-resolution (SR) algorithm to synthesize 1 mm isotropic MPRAGE-like scans from LF-MRI T1-weighted and T2-weighted sequences. Our initial results on a paired dataset of LF and high-field (HF, 1.5T-3T) clinical scans show that: (i) application of available automated segmentation tools directly to LF-MRI images falters; but (ii) segmentation tools succeed when applied to SR images with high correlation to gold standard measurements from HF-MRI (e.g., r = 0.85 for hippocampal volume, r = 0.84 for the thalamus, r = 0.92 for the whole cerebrum). This work demonstrates proof-of-principle post-processing image enhancement from lower resolution LF-MRI sequences. These results lay the foundation for future work to enhance the detection of normal and abnormal image findings at LF and ultimately improve the diagnostic performance of LF-MRI. Our tools are publicly available on FreeSurfer (<a class="link-external link-http" href="http://surfer.nmr.mgh.harvard.edu/" rel="external noopener nofollow">this http URL</a>).      
### 24.Aladdin: Joint Atlas Building and Diffeomorphic Registration Learning with Pairwise Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2202.03563.pdf)
>  Atlas building and image registration are important tasks for medical image analysis. Once one or multiple atlases from an image population have been constructed, commonly (1) images are warped into an atlas space to study intra-subject or inter-subject variations or (2) a possibly probabilistic atlas is warped into image space to assign anatomical labels. Atlas estimation and nonparametric transformations are computationally expensive as they usually require numerical optimization. Additionally, previous approaches for atlas building often define similarity measures between a fuzzy atlas and each individual image, which may cause alignment difficulties because a fuzzy atlas does not exhibit clear anatomical structures in contrast to the individual images. This work explores using a convolutional neural network (CNN) to jointly predict the atlas and a stationary velocity field (SVF) parameterization for diffeomorphic image registration with respect to the atlas. Our approach does not require affine pre-registrations and utilizes pairwise image alignment losses to increase registration accuracy. We evaluate our model on 3D knee magnetic resonance images (MRI) from the OAI-ZIB dataset. Our results show that the proposed framework achieves better performance than other state-of-the-art image registration algorithms, allows for end-to-end training, and for fast inference at test time.      
### 25.Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2202.03543.pdf)
>  In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.      
### 26.Multi-modal data generation with a deep metric variational autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2202.03434.pdf)
>  We present a deep metric variational autoencoder for multi-modal data generation. The variational autoencoder employs triplet loss in the latent space, which allows for conditional data generation by sampling in the latent space within each class cluster. The approach is evaluated on a multi-modal dataset consisting of otoscopy images of the tympanic membrane with corresponding wideband tympanometry measurements. The modalities in this dataset are correlated, as they represent different aspects of the state of the middle ear, but they do not present a direct pixel-to-pixel correlation. The approach shows promising results for the conditional generation of pairs of images and tympanograms, and will allow for efficient data augmentation of data from multi-modal sources.      
### 27.A Coarse-to-fine Morphological Approach With Knowledge-based Rules and Self-adapting Correction for Lung Nodules Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.03433.pdf)
>  The segmentation module which precisely outlines the nodules is a crucial step in a computer-aided diagnosis(CAD) system. The most challenging part of such a module is how to achieve high accuracy of the segmentation, especially for the juxtapleural, non-solid and small nodules. In this research, we present a coarse-to-fine methodology that greatly improves the thresholding method performance with a novel self-adapting correction algorithm and effectively removes noisy pixels with well-defined knowledge-based principles. Compared with recent strong morphological baselines, our algorithm, by combining dataset features, achieves state-of-the-art performance on both the public LIDC-IDRI dataset (DSC 0.699) and our private LC015 dataset (DSC 0.760) which closely approaches the SOTA deep learning-based models' performances. Furthermore, unlike most available morphological methods that can only segment the isolated and well-circumscribed nodules accurately, the precision of our method is totally independent of the nodule type or diameter, proving its applicability and generality.      
### 28.Inference of captions from histopathological patches  [ :arrow_down: ](https://arxiv.org/pdf/2202.03432.pdf)
>  Computational histopathology has made significant strides in the past few years, slowly getting closer to clinical adoption. One area of benefit would be the automatic generation of diagnostic reports from H\&amp;E-stained whole slide images which would further increase the efficiency of the pathologists' routine diagnostic workflows. In this study, we compiled a dataset (PatchGastricADC22) of histopathological captions of stomach adenocarcinoma endoscopic biopsy specimens, which we extracted from diagnostic reports and paired with patches extracted from the associated whole slide images. The dataset contains a variety of gastric adenocarcinoma subtypes. We trained a baseline attention-based model to predict the captions from features extracted from the patches and obtained promising results. We make the captioned dataset of 262K patches publicly available.      
### 29.A Topology-Attention ConvLSTM Network and Its Application to EM Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.03430.pdf)
>  Structural accuracy of segmentation is important for finescale structures in biomedical images. We propose a novel TopologyAttention ConvLSTM Network (TACNet) for 3D image segmentation in order to achieve high structural accuracy for 3D segmentation tasks. Specifically, we propose a Spatial Topology-Attention (STA) module to process a 3D image as a stack of 2D image slices and adopt ConvLSTM to leverage contextual structure information from adjacent slices. In order to effectively transfer topology-critical information across slices, we propose an Iterative-Topology Attention (ITA) module that provides a more stable topology-critical map for segmentation. Quantitative and qualitative results show that our proposed method outperforms various baselines in terms of topology-aware evaluation metrics.      
### 30.Performance Evaluation of Infrared Image Enhancement Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2202.03427.pdf)
>  Infrared (IR) images are widely used in many fields such as medical imaging, object tracking, astronomy and military purposes for securing borders. Infrared images can be captured day or night based on the type of capturing device. The capturing devices use electromagnetic radiation with longer wavelengths. There are several types of IR radiation based on the range of wavelength and corresponding frequency. Due to noising and other artifacts, IR images are not clearly visible. In this paper, we present a complete up-todate survey on IR imaging enhancement techniques. The survey includes IR radiation types and devices and existing IR datasets. The survey covers spatial enhancement techniques, frequency-domain based enhancement techniques and Deep learning-based techniques.      
### 31.Age of Information in the Presence of an Adversary  [ :arrow_down: ](https://arxiv.org/pdf/2202.04050.pdf)
>  We consider a communication system where a base station serves $N$ users, one user at a time, over a wireless channel. We consider the timeliness of the communication of each user via the age of information metric. A constrained adversary can block at most a given fraction, $\alpha$, of the time slots over a horizon of $T$ slots, i.e., it can block at most $\alpha T$ slots. We show that an optimum adversary blocks $\alpha T$ consecutive time slots of a randomly selected user. The interesting consecutive property of the blocked time slots is due to the cumulative nature of the age metric.      
### 32.A 120dB Programmable-Range On-Chip Pulse Generator for Characterizing Ferroelectric Devices  [ :arrow_down: ](https://arxiv.org/pdf/2202.04049.pdf)
>  Novel non-volatile memory devices based on ferroelectric thin films represent a promising emerging technology that is ideally suited for neuromorphic applications. The physical switching mechanism in such films is the nucleation and growth of ferroelectric domains. Since this has a strong dependence on both pulse width and voltage amplitude, it is important to use precise pulsing schemes for a thorough characterization of their behaviour. In this work, we present an on-chip 120 dB programmable range pulse generator, that can generate pulse widths ranging from 10ns to 10ms $\pm$2.5% which eliminates the RLC bottleneck in the device characterisation setup. We describe the pulse generator design and show how the pulse width can be tuned with high accuracy, using Digital to Analog converters. Finally, we present experimental results measured from the circuit, fabricated using a standard 180nm CMOS technology.      
### 33.Self-supervised Contrastive Learning for Volcanic Unrest Detection  [ :arrow_down: ](https://arxiv.org/pdf/2202.04030.pdf)
>  Ground deformation measured from Interferometric Synthetic Aperture Radar (InSAR) data is considered a sign of volcanic unrest, statistically linked to a volcanic eruption. Recent studies have shown the potential of using Sentinel-1 InSAR data and supervised deep learning (DL) methods for the detection of volcanic deformation signals, towards global volcanic hazard mitigation. However, detection accuracy is compromised from the lack of labelled data and class imbalance. To overcome this, synthetic data are typically used for finetuning DL models pre-trained on the ImageNet dataset. This approach suffers from poor generalisation on real InSAR data. This letter proposes the use of self-supervised contrastive learning to learn quality visual representations hidden in unlabeled InSAR data. Our approach, based on the SimCLR framework, provides a solution that does not require a specialized architecture nor a large labelled or synthetic dataset. We show that our self-supervised pipeline achieves higher accuracy with respect to the state-of-the-art methods, and shows excellent generalisation even for out-of-distribution test data. Finally, we showcase the effectiveness of our approach for detecting the unrest episodes preceding the recent Icelandic Fagradalsfjall volcanic eruption.      
### 34.Hearing Loss, Cognitive Load and Dementia: An Overview of Interrelation, Detection and Monitoring Challenges with Wearable Non-invasive Microwave Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2202.03973.pdf)
>  This paper provides an overview of hearing loss effects on neurological function and progressive diseases; and explores the role of cognitive load monitoring to detect dementia. It also investigates the prospects of utilizing hearing aid technology to reverse cognitive decline and delay the onset of dementia, for the old age population. The interrelation between hearing loss, cognitive load and dementia is discussed. Future considerations for improvement with respect to robust diagnosis, user centricity, device accuracy and privacy for wider clinical practice is also explored. The review concludes by discussing the future scope and potential of designing practical wearable microwave technologies and evaluating their use in smart care homes setting.      
### 35.vol2Brain: A new online Pipeline for whole Brain MRI analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.03920.pdf)
>  Automatic and reliable quantitative tools for MR brain image analysis are a very valuable resources for both clinical and research environments. In the last years, this field has experienced many advances with successful techniques based on label fusion and more recently deep learning. However, few of them have been specifically designed to provide a dense anatomical labelling at multiscale level and to deal with brain anatomical alterations such as white matter lesions. In this work, we present a fully automatic pipeline (vol2Brain) for whole brain segmentation and analysis which densely labels (N&gt;100) the brain while being robust to the presence of white matter lesions. This new pipeline is an evolution of our previous volBrain pipeline that extends significantly the number of regions that can be analyzed. Our proposed method is based on a fast multiscale multi-atlas label fusion technology with systematic error correction able to provide accurate volumetric information in few minutes. We have deployed our new pipeline within our platform volBrain (<a class="link-external link-http" href="http://www.volbrain.upv.es" rel="external noopener nofollow">this http URL</a>) which has been already demonstrated to be an efficient and effective manner to share our technology with users worldwide      
### 36.Covert backscatter communication with directional MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2202.03899.pdf)
>  We study a backscatter communication protocol over a AWGN channel, where a transmitter illuminates a tag with a directional multi-antenna. The tag performs load modulation on the signal while hiding its physical presence from a warden. We show that, if the transmitter-to-tag channel is inaccessible to the warden, then $\Theta(n)$ reliable and covert bits can be transmitted over $n$ channel usages. This overcomes the square-root law for covert communication. This paper provides the first evidence for practical implementation of covert backscatter communication, with potential applications in IoT security.      
### 37.Speech Emotion Recognition using Self-Supervised Features  [ :arrow_down: ](https://arxiv.org/pdf/2202.03896.pdf)
>  Self-supervised pre-trained features have consistently delivered state-of-art results in the field of natural language processing (NLP); however, their merits in the field of speech emotion recognition (SER) still need further investigation. In this paper we introduce a modular End-to- End (E2E) SER system based on an Upstream + Downstream architecture paradigm, which allows easy use/integration of a large variety of self-supervised features. Several SER experiments for predicting categorical emotion classes from the IEMOCAP dataset are performed. These experiments investigate interactions among fine-tuning of self-supervised feature models, aggregation of frame-level features into utterance-level features and back-end classification networks. The proposed monomodal speechonly based system not only achieves SOTA results, but also brings light to the possibility of powerful and well finetuned self-supervised acoustic features that reach results similar to the results achieved by SOTA multimodal systems using both Speech and Text modalities.      
### 38.BIQ2021: A Large-Scale Blind Image Quality Assessment Database  [ :arrow_down: ](https://arxiv.org/pdf/2202.03879.pdf)
>  The assessment of the perceptual quality of digital images is becoming increasingly important as a result of the widespread use of digital multimedia devices. Smartphones and high-speed internet are just two examples of technologies that have multiplied the amount of multimedia content available. Thus, obtaining a representative dataset, which is required for objective quality assessment training, is a significant challenge. The Blind Image Quality Assessment Database, BIQ2021, is presented in this article. By selecting images with naturally occurring distortions and reliable labeling, the dataset addresses the challenge of obtaining representative images for no-reference image quality assessment. The dataset consists of three sets of images: those taken without the intention of using them for image quality assessment, those taken with intentionally introduced natural distortions, and those taken from an open-source image-sharing platform. It is attempted to maintain a diverse collection of images from various devices, containing a variety of different types of objects and varying degrees of foreground and background information. To obtain reliable scores, these images are subjectively scored in a laboratory environment using a single stimulus method. The database contains information about subjective scoring, human subject statistics, and the standard deviation of each image. The dataset's Mean Opinion Scores (MOS) make it useful for assessing visual quality. Additionally, the proposed database is used to evaluate existing blind image quality assessment approaches, and the scores are analyzed using Pearson and Spearman's correlation coefficients. The image database and MOS are freely available for use and benchmarking.      
### 39.FSM: FBS Set Management, An energy efficient multi-drone 3D trajectory approach in cellular networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.03834.pdf)
>  In this paper, we consider a cellular network demand in an urban area. We aim to cover users and serve their required data rate in a period of time using a 5G cellular network. The type of considered UAV in this scenario is The Scout B- 330 UAV helicopter which can fly up to 3 km height. In these scenarios, to find the most proper trajectory of UAVs, we first must find the best positions of UAVs in different snapshots. We consider orthogonal frequency reuse to avoid interference between UAVs in the network. We also consider the number of communication channels constraint in intra cellular network. To find the optimum position of UAVs in each snapshot. We consider Non-Line of Sight (NLoS) path loss in these scenarios and aim to cover all users in each snapshot. To find the optimum trajectory of UAVs, we propose a mathematical model based on transportation problem to minimize the total distance tracked by UAVs. In each step we solve the proposed mathematical model for transiting UAVs between two snapshots. We also consider that users can be placed in different altitudes an their positions follows the Poison Point Process distribution and their mobility follows the random way point. The UAVs battery and flight limitations are also considered. To tackle the energy problem we introduce the Drone Cell Off (DCO) approach to avoid losing energy in idle hover mode.      
### 40.Joint position and trajectory optimization of flying base station in 5G cellular networks, based on users' current and predicted location  [ :arrow_down: ](https://arxiv.org/pdf/2202.03832.pdf)
>  Nowadays, Unmanned Aerial Vehicles (UAVs) have been significantly improved, and one of their most important applications is to provide temporary coverage for cellular users. Static Base Station cannot service all users due to temporary crashes because of temporary events such as ground BS breakdowns, bad weather conditions, natural disasters, transmission errors, etc., drones equipped with small cellular BS. The Drone Base Station is immediately sent to the target location and establishes the necessary communication links without requiring any predetermined infrastructure and covers that area. Finding the optimal location and the appropriate number (DBS) of drone-BS in this area is a challenge. Therefore, in this paper, the optimal location and optimal number of DBSs are distributed in the current state of the users and the subsequent user states determined by the prediction. Finally, the DBS transition is optimized from the current state to the predicted future locations. The simulation results show that the proposed method can provide acceptable coverage on the network.      
### 41.Predictive Beamforming for Integrated Sensing and Communication in Vehicular Networks: A Deep Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.03811.pdf)
>  The implementation of integrated sensing and communication (ISAC) highly depends on the effective beamforming design exploiting accurate instantaneous channel state information (ICSI). However, channel tracking in ISAC requires large amount of training overhead and prohibitively large computational complexity. To address this problem, in this paper, we focus on ISAC-assisted vehicular networks and exploit a deep learning approach to implicitly learn the features of historical channels and directly predict the beamforming matrix for the next time slot to maximize the average achievable sum-rate of system, thus bypassing the need of explicit channel tracking for reducing the system signaling overhead. To this end, a general sum-rate maximization problem with Cramer-Rao lower bounds-based sensing constraints is first formulated for the considered ISAC system. Then, a historical channels-based convolutional long short-term memory network is designed for predictive beamforming that can exploit the spatial and temporal dependencies of communication channels to further improve the learning performance. Finally, simulation results show that the proposed method can satisfy the requirement of sensing performance, while its achievable sum-rate can approach the upper bound obtained by a genie-aided scheme with perfect ICSI available.      
### 42.A two-step approach to leverage contextual data: speech recognition in air-traffic communications  [ :arrow_down: ](https://arxiv.org/pdf/2202.03725.pdf)
>  Automatic Speech Recognition (ASR), as the assistance of speech communication between pilots and air-traffic controllers, can significantly reduce the complexity of the task and increase the reliability of transmitted information. ASR application can lead to a lower number of incidents caused by misunderstanding and improve air traffic management (ATM) efficiency. Evidently, high accuracy predictions, especially, of key information, i.e., callsigns and commands, are required to minimize the risk of errors. We prove that combining the benefits of ASR and Natural Language Processing (NLP) methods to make use of surveillance data (i.e. additional modality) helps to considerably improve the recognition of callsigns (named entity). In this paper, we investigate a two-step callsign boosting approach: (1) at the 1 step (ASR), weights of probable callsign n-grams are reduced in G.fst and/or in the decoding FST (lattices), (2) at the 2 step (NLP), callsigns extracted from the improved recognition outputs with Named Entity Recognition (NER) are correlated with the surveillance data to select the most suitable one. Boosting callsign n-grams with the combination of ASR and NLP methods eventually leads up to 53.7% of an absolute, or 60.4% of a relative, improvement in callsign recognition.      
### 43.Summary On The ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription Grand Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.03647.pdf)
>  The ICASSP 2022 Multi-channel Multi-party Meeting Transcription Grand Challenge (M2MeT) focuses on one of the most valuable and the most challenging scenarios of speech technologies. The M2MeT challenge has particularly set up two tracks, speaker diarization (track 1) and multi-speaker automatic speech recognition (ASR) (track 2). Along with the challenge, we released 120 hours of real-recorded Mandarin meeting speech data with manual annotation, including far-field data collected by 8-channel microphone array as well as near-field data collected by each participants' headset microphone. We briefly describe the released dataset, track setups, baselines and summarize the challenge results and major techniques used in the submissions.      
### 44.Optimal Control with Learning on the Fly: System with Unknown Drift  [ :arrow_down: ](https://arxiv.org/pdf/2202.03620.pdf)
>  This paper derives an optimal control strategy for a simple stochastic dynamical system with constant drift and an additive control input. Motivated by the example of a physical system with an unexpected change in its dynamics, we take the drift parameter to be unknown, so that it must be learned while controlling the system. The state of the system is observed through a linear observation model with Gaussian noise. In contrast to most previous work, which focuses on a controller's asymptotic performance over an infinite time horizon, we minimize a quadratic cost function over a finite time horizon. The performance of our control strategy is quantified by comparing its cost with the cost incurred by an optimal controller that has full knowledge of the parameters. This approach gives rise to several notions of "regret." We derive a set of control strategies that provably minimize the worst-case regret; these arise from Bayesian strategies that assume a specific fixed prior on the drift parameter. This work suggests that examining Bayesian strategies may lead to optimal or near-optimal control strategies for a much larger class of realistic dynamical models with unknown parameters.      
### 45.Codebook Design for Composite Beamforming in Next-generation mmWave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.03610.pdf)
>  In pursuance of the unused spectrum in higher frequencies, millimeter wave (mmWave) bands have a pivotal role. However, the high path-loss and poor scattering associated with mmWave communications highlight the necessity of employing effective beamforming techniques. In order to efficiently search for the beam to serve a user and to jointly serve multiple users it is often required to use a composite beam which consists of multiple disjoint lobes. A composite beam covers multiple desired angular coverage intervals (ACIs) and ideally has maximum and uniform gain (smoothness) within each desired ACI, negligible gain (leakage) outside the desired ACIs, and sharp edges. We propose an algorithm for designing such ideal composite codebook by providing an analytical closed-form solution with low computational complexity. There is a fundamental trade-off between the gain, leakage and smoothness of the beams. Our design allows to achieve different values in such trade-off based on changing the design parameters. We highlight the shortcomings of the uniform linear arrays (ULAs) in building arbitrary composite beams. Consequently, we use a recently introduced twin-ULA (TULA) antenna structure to effectively resolve these inefficiencies. Numerical results are used to validate the theoretical findings.      
### 46.Maximizing Audio Event Detection Model Performance on Small Datasets Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation Study  [ :arrow_down: ](https://arxiv.org/pdf/2202.03514.pdf)
>  An Xception model reaches state-of-the-art (SOTA) accuracy on the ESC-50 dataset for audio event detection through knowledge transfer from ImageNet weights, pretraining on AudioSet, and an on-the-fly data augmentation pipeline. This paper presents an ablation study that analyzes which components contribute to the boost in performance and training time. A smaller Xception model is also presented which nears SOTA performance with almost a third of the parameters.      
### 47.Random Ferns for Semantic Segmentation of PolSAR Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.03498.pdf)
>  Random Ferns -- as a less known example of Ensemble Learning -- have been successfully applied in many Computer Vision applications ranging from keypoint matching to object detection. This paper extends the Random Fern framework to the semantic segmentation of polarimetric synthetic aperture radar images. By using internal projections that are defined over the space of Hermitian matrices, the proposed classifier can be directly applied to the polarimetric covariance matrices without the need to explicitly compute predefined image features. Furthermore, two distinct optimization strategies are proposed: The first based on pre-selection and grouping of internal binary features before the creation of the classifier; and the second based on iteratively improving the properties of a given Random Fern. Both strategies are able to boost the performance by filtering features that are either redundant or have a low information content and by grouping correlated features to best fulfill the independence assumptions made by the Random Fern classifier. Experiments show that results can be achieved that are similar to a more complex Random Forest model and competitive to a deep learning baseline.      
### 48.New Bounds on the Size of Binary Codes with Large Minimum Distance  [ :arrow_down: ](https://arxiv.org/pdf/2202.03472.pdf)
>  Let A(n, d) denote the maximum number of codewords in a binary code of length n and minimum Hamming distance d. Deriving upper and lower bounds on A(n, d) have been a subject for extensive research in coding theory. In this paper, we examine upper and lower bounds on A(n, d) in the high-minimum distance regime, in particular, when $d = n/2 - \Theta(\sqrt{n})$. We will first provide a lower bound based on a cyclic construction for codes of length $n= 2^m -1$ and show that $A(n, d= n/2 - 2^{c-1}\sqrt{n}) \geq n^c$, where c is an integer with $1 \leq c \leq m/2-1$. With a Fourier-analytic view of Delsarte's linear program, novel upper bounds on $A(n, n/2 - \sqrt{n})$ and $A(n, n/2 - 2 \sqrt{n})$ are obtained, and, to the best of the authors' knowledge, are the first upper bounds scaling polynomially in n for the regime with $d = n/2 - \Theta(\sqrt{n})$.      
### 49.On learning Whittle index policy for restless bandits with scalable regret  [ :arrow_down: ](https://arxiv.org/pdf/2202.03463.pdf)
>  Reinforcement learning is an attractive approach to learn good resource allocation and scheduling policies based on data when the system model is unknown. However, the cumulative regret of most RL algorithms scales as $\tilde O(\mathsf{S} \sqrt{\mathsf{A} T})$, where $\mathsf{S}$ is the size of the state space, $\mathsf{A}$ is the size of the action space, $T$ is the horizon, and the $\tilde{O}(\cdot)$ notation hides logarithmic terms. Due to the linear dependence on the size of the state space, these regret bounds are prohibitively large for resource allocation and scheduling problems. In this paper, we present a model-based RL algorithm for such problem which has scalable regret. In particular, we consider a restless bandit model, and propose a Thompson-sampling based learning algorithm which is tuned to the underlying structure of the model. We present two characterizations of the regret of the proposed algorithm with respect to the Whittle index policy. First, we show that for a restless bandit with $n$ arms and at most $m$ activations at each time, the regret scales either as $\tilde{O}(mn\sqrt{T})$ or $\tilde{O}(n^2 \sqrt{T})$ depending on the reward model. Second, under an additional technical assumption, we show that the regret scales as $\tilde{O}(n^{1.5} \sqrt{T})$. We present numerical examples to illustrate the salient features of the algorithm.      
### 50.Reinforcement learning for multi-item retrieval in the puzzle-based storage system  [ :arrow_down: ](https://arxiv.org/pdf/2202.03424.pdf)
>  Nowadays, fast delivery services have created the need for high-density warehouses. The puzzle-based storage system is a practical way to enhance the storage density, however, facing difficulties in the retrieval process. In this work, a deep reinforcement learning algorithm, specifically the Double&amp;Dueling Deep Q Network, is developed to solve the multi-item retrieval problem in the system with general settings, where multiple desired items, escorts, and I/O points are placed randomly. Additionally, we propose a general compact integer programming model to evaluate the solution quality. Extensive numerical experiments demonstrate that the reinforcement learning approach can yield high-quality solutions and outperforms three related state-of-the-art heuristic algorithms. Furthermore, a conversion algorithm and a decomposition framework are proposed to handle simultaneous movement and large-scale instances respectively, thus improving the applicability of the PBS system.      
