# ArXiv eess --Wed, 2 Feb 2022
### 1.Similarity Learning based Few Shot Learning for ECG Time Series Classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.00612.pdf)
>  Using deep learning models to classify time series data generated from the Internet of Things (IoT) devices requires a large amount of labeled data. However, due to constrained resources available in IoT devices, it is often difficult to accommodate training using large data sets. This paper proposes and demonstrates a Similarity Learning-based Few Shot Learning for ECG arrhythmia classification using Siamese Convolutional Neural Networks. Few shot learning resolves the data scarcity issue by identifying novel classes from very few labeled examples. Few Shot Learning relies first on pretraining the model on a related relatively large database, and then the learning is used for further adaptation towards few examples available per class. Our experiments evaluate the performance accuracy with respect to K (number of instances per class) for ECG time series data classification. The accuracy with 5- shot learning is 92.25% which marginally improves with further increase in K. We also compare the performance of our method against other well-established similarity learning techniques such as Dynamic Time Warping (DTW), Euclidean Distance (ED), and a deep learning model - Long Short Term Memory Fully Convolutional Network (LSTM-FCN) with the same amount of data and conclude that our method outperforms them for a limited dataset size. For K=5, the accuracies obtained are 57%, 54%, 33%, and 92% approximately for ED, DTW, LSTM-FCN, and SCNN, respectively.      
### 2.Signal Quality Assessment of Photoplethysmogram Signals using Quantum Pattern Recognition and lightweight CNN Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2202.00606.pdf)
>  Photoplethysmography (PPG) signal comprises physiological information related to cardiorespiratory health. However, while recording, these PPG signals are easily corrupted by motion artifacts and body movements, leading to noise enriched, poor quality signals. Therefore ensuring high-quality signals is necessary to extract cardiorespiratory information accurately. Although there exists several rule-based and Machine-Learning (ML) - based approaches for PPG signal quality estimation, those algorithms' efficacy is questionable. Thus, this work proposes a lightweight CNN architecture for signal quality assessment employing a novel Quantum pattern recognition (QPR) technique. The proposed algorithm is validated on manually annotated data obtained from the University of Queensland database. A total of 28366, 5s signal segments are preprocessed and transformed into image files of 20 x 500 pixels. The image files are treated as an input to the 2D CNN architecture. The developed model classifies the PPG signal as `good' or `bad' with an accuracy of 98.3% with 99.3% sensitivity, 94.5% specificity and 98.9% F1-score. Finally, the performance of the proposed framework is validated against the noisy `Welltory app' collected PPG database. Even in a noisy environment, the proposed architecture proved its competence. Experimental analysis concludes that a slim architecture along with a novel Spatio-temporal pattern recognition technique improve the system's performance. Hence, the proposed approach can be useful to classify good and bad PPG signals for a resource-constrained wearable implementation.      
### 3.BEA-Base: A Benchmark for ASR of Spontaneous Hungarian  [ :arrow_down: ](https://arxiv.org/pdf/2202.00601.pdf)
>  Hungarian is spoken by 15 million people, still, easily accessible Automatic Speech Recognition (ASR) benchmark datasets - especially for spontaneous speech - have been practically unavailable. In this paper, we introduce BEA-Base, a subset of the BEA spoken Hungarian database comprising mostly spontaneous speech of 140 speakers. It is built specifically to assess ASR, primarily for conversational AI applications. After defining the speech recognition subsets and task, several baselines - including classic HMM-DNN hybrid and end-to-end approaches augmented by cross-language transfer learning - are developed using open-source toolkits. The best results obtained are based on multilingual self-supervised pretraining, achieving a 45% recognition error rate reduction as compared to the classical approach - without the application of an external language model or additional supervised data. The results show the feasibility of using BEA-Base for training and evaluation of Hungarian speech recognition systems.      
### 4.A least squares support vector regression for anisotropic diffusion filtering  [ :arrow_down: ](https://arxiv.org/pdf/2202.00595.pdf)
>  Anisotropic diffusion filtering for signal smoothing as a low-pass filter has the advantage of the edge-preserving, i.e., it does not affect the edges that contain more critical data than the other parts of the signal. In this paper, we present a numerical algorithm based on least squares support vector regression by using Legendre orthogonal kernel with the discretization of the nonlinear diffusion problem in time by the Crank-Nicolson method. This method transforms the signal smoothing process into solving an optimization problem that can be solved by efficient numerical algorithms. In the final analysis, we have reported some numerical experiments to show the effectiveness of the proposed machine learning based approach for signal smoothing.      
### 5.Blind ECG Restoration by Operational Cycle-GANs  [ :arrow_down: ](https://arxiv.org/pdf/2202.00589.pdf)
>  Continuous long-term monitoring of electrocardiography (ECG) signals is crucial for the early detection of cardiac abnormalities such as arrhythmia. Non-clinical ECG recordings acquired by Holter and wearable ECG sensors often suffer from severe artifacts such as baseline wander, signal cuts, motion artifacts, variations on QRS amplitude, noise, and other interferences. Usually, a set of such artifacts occur on the same ECG signal with varying severity and duration, and this makes an accurate diagnosis by machines or medical doctors extremely difficult. Despite numerous studies that have attempted ECG denoising, they naturally fail to restore the actual ECG signal corrupted with such artifacts due to their simple and naive noise model. In this study, we propose a novel approach for blind ECG restoration using cycle-consistent generative adversarial networks (Cycle-GANs) where the quality of the signal can be improved to a clinical level ECG regardless of the type and severity of the artifacts corrupting the signal. To further boost the restoration performance, we propose 1D operational Cycle-GANs with the generative neuron model. The proposed approach has been evaluated extensively using one of the largest benchmark ECG datasets from the China Physiological Signal Challenge (CPSC-2020) with more than one million beats. Besides the quantitative and qualitative evaluations, a group of cardiologists performed medical evaluations to validate the quality and usability of the restored ECG, especially for an accurate arrhythmia diagnosis.      
### 6.A novel ECG signal denoising filter selection algorithm based on conventional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.00579.pdf)
>  We propose a novel deep learning based denoising filter selection algorithm for noisy Electrocardiograph (ECG) signal preprocessing. ECG signals measured under clinical conditions, such as those acquired using skin contact devices in hospitals, often contain baseline signal disturbances and unwanted artefacts; indeed for signals obtained outside of a clinical environment, such as heart rate signatures recorded using non-contact radar systems, the measurements contain greater levels of noise than those acquired under clinical conditions. In this paper, we focus on heart rate signals acquired using non-contact radar systems for use in assisted living environments. Such signals contain more noise than those measured under clinical conditions and thus require a novel signal noise removal method capable of adaptive determining filters. Currently, the most common method of removing noise from such a waveform is through the use of filters; the most popular filtering method amongst which is the wavelet filter. There are, however, circumstances in which using a different filtering method may result in higher signal-to-noise-ratios (SNR) for a waveform; in this paper, we investigate the wavelet and elliptical filtering methods for the task of reducing noise in ECG signals acquired using assistive technologies. Our proposed convolutional neural network architecture classifies (with 92.8% accuracy) the optimum filtering method for noisy signal based on its expected SNR value.      
### 7.Closing the sim-to-real gap in guided wave damage detection with adversarial training of variational auto-encoders  [ :arrow_down: ](https://arxiv.org/pdf/2202.00570.pdf)
>  Guided wave testing is a popular approach for monitoring the structural integrity of infrastructures. We focus on the primary task of damage detection, where signal processing techniques are commonly employed. The detection performance is affected by a mismatch between the wave propagation model and experimental wave data. External variations, such as temperature, which are difficult to model, also affect the performance. While deep learning models can be an alternative detection method, there is often a lack of real-world training datasets. In this work, we counter this challenge by training an ensemble of variational autoencoders only on simulation data with a wave physics-guided adversarial component. We set up an experiment with non-uniform temperature variations to test the robustness of the methods. We compare our scheme with existing deep learning detection schemes and observe superior performance on experimental data.      
### 8.Arrhythmia Classification using CGAN-augmented ECG Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.00569.pdf)
>  One of the easiest ways to diagnose cardiovascular conditions is Electrocardiogram (ECG) analysis. ECG databases usually have highly imbalanced distributions due to the abundance of Normal ECG and scarcity of abnormal cases which are equally, if not more, important for arrhythmia detection. As such, DL classifiers trained on these datasets usually perform poorly, especially on minor classes. One solution to address the imbalance is to generate realistic synthetic ECG signals mostly using Generative Adversarial Networks (GAN) to augment and the datasets. In this study, we designed an experiment to investigate the impact of data augmentation on arrhythmia classification. Using the MIT-BIH Arrhythmia dataset, we employed two ways for ECG beats generation: (i) an unconditional GAN, i.e., Wasserstein GAN with gradient penalty (WGAN-GP) is trained on each class individually; (ii) a conditional GAN model, i.e., Auxiliary Classifier Wasserstein GAN with gradient penalty (AC-WGAN-GP) is trained on all the available classes to train one single generator. Two scenarios are defined for each case: i) unscreened where all the generated synthetic beats were used directly without any post-processing, and ii) screened where a portion of generated beats are selected based on their Dynamic Time Warping (DTW) distance with a designated template. A ResNet classifier is trained on each of the four augmented datasets and the performance metrics of precision, recall and F1-Score as well as the confusion matrices were compared with the reference case, i.e., when the classifier is trained on the imbalanced original dataset. The results show that in all four cases augmentation achieves impressive improvements in metrics particularly on minor classes (typically from 0 or 0.27 to 0.99). The quality of the generated beats is also evaluated using DTW distance function compared with real data.      
### 9.Stochastic 2D Signal Generative Model with Wavelet Packets Basis Regarded as a Random Variable and Bayes Optimal Processing  [ :arrow_down: ](https://arxiv.org/pdf/2202.00568.pdf)
>  This study deals with two-dimensional (2D) signal processing using the wavelet packet transform. When the basis is unknown the candidate of basis increases in exponential order with respect to the signal size. Previous studies do not consider the basis as a random vaiables. Therefore, the cost function needs to be used to select a basis. However, this method is often a heuristic and a greedy search because it is impossible to search all the candidates for a huge number of bases. Therefore, it is difficult to evaluate the entire signal processing under a criterion and also it does not always gurantee the optimality of the entire signal processing. In this study, we propose a stochastic generative model in which the basis is regarded as a random variable. This makes it possible to evaluate entire signal processing under a unified criterion i.e. Bayes criterion. Moreover we can derive an optimal signal processing scheme that achieves the theoretical limit. This derived scheme shows that all the bases should be combined according to the posterior in stead of selecting a single basis. Although exponential order calculations is required for this scheme, we have derived a recursive algorithm for this scheme, which successfully reduces the computational complexity from the exponential order to the polynomial order.      
### 10.Optimal Transport based Data Augmentation for Heart Disease Diagnosis and Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2202.00567.pdf)
>  In this paper, we focus on a new method of data augmentation to solve the data imbalance problem within imbalanced ECG datasets to improve the robustness and accuracy of heart disease detection. By using Optimal Transport, we augment the ECG disease data from normal ECG beats to balance the data among different categories. We build a Multi-Feature Transformer (MF-Transformer) as our classification model, where different features are extracted from both time and frequency domains to diagnose various heart conditions. Learning from 12-lead ECG signals, our model is able to distinguish five categories of cardiac conditions. Our results demonstrate 1) the classification models' ability to make competitive predictions on five ECG categories; 2) improvements in accuracy and robustness reflecting the effectiveness of our data augmentation method.      
### 11.Full-Duplex Wideband mmWave Integrated Access and Backhaul with Low Resolution ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2202.00566.pdf)
>  We consider a wideband integrated access and backhaul system operating in full-duplex mode between the New Radio gNB donor and single user equipment. Due to high power consumption in millimeter wave systems, we use low-resolution analog-to-digital converters (ADCs) in the receivers. Our contributions include (1) hybrid beamformer to maximize sum spectral efficiency of the access and backhaul links by canceling self-interference and maximizing received power; (2) all-digital beamformer and upper bound on sum spectral efficiency; and (3) simulations to compare full vs. half duplex, finite vs. infinite ADC resolution, hybrid vs. all-digital beamforming, and the upper bound in spectral efficiency.      
### 12.Study of filtered-x logarithmic recursive least $p$-power algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2202.00560.pdf)
>  For active impulsive noise control, a filtered-x recursive least $p$-power (FxRLP) algorithm is proposed by minimizing the weighted summation of the $p$-power of the \emph{a posteriori} errors. Since the characteristic of the target noise is investigated, the FxRLP algorithm achieves good performance and robustness. To obtain a better performance, we develop a filtered-x logarithmic recursive least $p$-power (FxlogRLP) algorithm which integrates the $p$-order moment with the logarithmic-order moment. Simulation results demonstrate that the FxlogRLP algorithm is superior to the existing algorithms in terms of convergence rate and noise reduction.      
### 13.Runtime Monitoring and Statistical Approaches for Correlation Analysis of ECG and PPG  [ :arrow_down: ](https://arxiv.org/pdf/2202.00559.pdf)
>  Biophysical signals such as Electrocardiogram (ECG) and Photoplethysmogram (PPG) are key to the sensing of vital parameters for wellbeing. Coincidentally, ECG and PPG are signals, which provide a "different window" into the same phenomena, namely the cardiac cycle. While they are used separately, there are no studies regarding the exact correction of the different ECG and PPG events. Such correlation would be helpful in many fronts such as sensor fusion for improved accuracy using cheaper sensors and attack detection and mitigation methods using multiple signals to enhance the robustness, for example. Considering this, we present the first approach in formally establishing the key relationships between ECG and PPG signals. We combine formal run-time monitoring with statistical analysis and regression analysis for our results.      
### 14.Evaluation of Ultra Wide Band Technology as an Enhancement for BLE Based Location Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2202.00558.pdf)
>  In the scope of this paper, the precise positioning of objects with the help of Ultra Wide Band technology is evaluated. To achieve this, a prototype module for the use with the Decawave transceiver DW1000 was implemented and added to FruityMesh, an Internet of Things (IoT) mesh network of the company M-Way Solutions. The focus of the network is low-power communication based on Bluetooth Low Energy (BLE). This includes, for example, Building Automation, Lighting Management and Asset Tracking. Especially the last stated point benefits from a position tracking as precise as possible and a long battery life. Therefore, FruityMesh is used to perform a signal-strength based localization with the existing BLE messages. Due to absorption, interference, and diffraction, these measurements tend to fluctuate and allow a positioning accuracy within 1.5m. With the integration of Ultra Wide Band and the Time of Arrival (ToA) method, a centimeter-precise localization was made possible in the course of this work, which at the same time causes only small additional costs in general. Since the connection of extra hardware is associated with decreased energy efficiency, an algorithm for optimizing the control was first developed and then tested against the created scenarios. In addition to the motion-based control of the hardware, various configurations and adjustments were analyzed to reduce the power consumption by Ultra-Wideband (UWB) transmission. Finally, the developed prototype was compared with a realistic reproduction of the existing Asset Tracking to evaluate the benefits for the use in the productive application.      
### 15.Intelligent Reflecting Surface Assisted Integrated Sensing and Communications for mmWave Channels  [ :arrow_down: ](https://arxiv.org/pdf/2202.00552.pdf)
>  This paper proposes an intelligent reflecting surface (IRS) assisted integrated sensing and communication (ISAC) system operating at the millimeter-wave (mmWave) band. Specifically, the ISAC system combines communication and radar operations and performs, detecting and communicating simultaneously with multiple targets and users. The IRS dynamically controls the amplitude or phase of the radio signal via reflecting elements to reconfigure the radio propagation environment and enhance the transmission rate of the ISAC system. By jointly designing the radar signal covariance (RSC) matrix, the beamforming vector of the communication system, and the IRS phase shift, the ISAC system transmission rate can be improved while matching the desired waveform for radar. The problem is non-convex due to multivariate coupling, and thus we decompose it into two separate subproblems. First, a closed-form solution of the RSC matrix is derived from the desired radar waveform. Next, the quadratic transformation (QT) technique is applied to the subproblem, and then alternating optimization (AO) is employed to determine the communication beamforming vector and the IRS phase shift. For computing the IRS phase shift, we adopt both the majorization minimization (MM) and the manifold optimization (MO). Also, we derive a closed-form solution for the formulated problem, effectively decreasing computational complexity. Furthermore, a trade-off factor is introduced to balance the performance of communication and sensing. Finally, the simulations verify the effectiveness of the algorithm and demonstrate that the IRS can improve the performance of the ISAC system.      
### 16.Octonion Spectrum of 3D Short time LCT Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.00551.pdf)
>  This work is devoted to the development of the octonion linear canonical transform (OLCT) theory proposed by Gao and Li in 2021 that has been designated as an emerging tool in the scenario of signal processing. The purpose of this work is to introduce octonion linear canonical transform of real-valued functions. Further more keeping in mind the varying frequencies, we used the proposed transform to generate a new transform called short-time octonion linear canonical transform (STOLCT). The results of this article focus on the properties like linearity, reconstruction formula and relation with 3D short-time linear canonical transform (3D-STLCT). The crux of this paper lie in establishing well known uncertainty inequalities and convolution theorem for the proposed transform.      
### 17.Record Capacity-Reach of C band IM/DD Optical Systems over Dispersion-Uncompensated Links  [ :arrow_down: ](https://arxiv.org/pdf/2202.00550.pdf)
>  We experimentally demonstrate a C band 100Gbit/s intensity modulation and direct detection entropy-loaded multi-rate Nyquist-subcarrier modulation signal over 100km dispersion-uncompensated link. A record capacity-reach of 10Tbit/s$\times$km is achieved.      
### 18.A Data-Efficient Deep Learning Training Strategy for Biomedical Ultrasound Imaging: Zone Training  [ :arrow_down: ](https://arxiv.org/pdf/2202.00547.pdf)
>  Deep learning (DL) powered biomedical ultrasound imaging is an emerging research field where researchers adapt the image analysis capabilities of DL algorithms to biomedical ultrasound imaging settings. A major roadblock to wider adoption of DL powered biomedical ultrasound imaging is that acquiring large and diverse datasets is expensive in clinical settings, which is a requirement for successful DL implementation. Hence, there is a constant need for developing data-efficient DL techniques to turn DL powered biomedical ultrasound imaging into reality. In this work, we develop a data-efficient deep learning training strategy, which we named \textit{Zone Training}. In \textit{Zone Training}, we propose to divide the complete field of view of an ultrasound image into multiple zones associated with different regions of a diffraction pattern and then, train separate DL networks for each zone. The main advantage of \textit{Zone Training} is that it requires less training data to achieve high accuracy. In this work, three different tissue-mimicking phantoms were classified by a DL network. The results demonstrated that \textit{Zone Training} required a factor of 2-5 less training data to achieve similar classification accuracies compared to a conventional training strategy.      
### 19.Coordinated Frequency Control through Safe Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.00530.pdf)
>  With widespread deployment of renewables, the electric power grids are experiencing increasing dynamics and uncertainties, with its secure operation being threatened. Existing frequency control schemes based on day-ahead offline analysis and minute-level online sensitivity calculations are difficult to adapt to rapidly changing system states. In particular, they are unable to facilitate coordinated control of system frequency and power flows. A refined approach and tools are urgently needed to assist system operators to make timely decisions. This paper proposes a novel model-free coordinated frequency control framework based on safe reinforcement learning, with multiple control objectives considered. The load frequency control problem is modeled as a constrained Markov decision process, which can be solved by an AI agent continuously interacting with the grid to achieve sub-second decision making. Extensive numerical experiments conducted at East China Power Grid demonstrate the effectiveness and promise of the proposed method.      
### 20.An FPGA-based System for Generalised Electron Devices Testing  [ :arrow_down: ](https://arxiv.org/pdf/2202.00499.pdf)
>  Electronic systems are becoming more and more ubiquitous as our world digitises. Simultaneously, even basic components are experiencing a wave of improvements with new transistors, memristors, voltage/current references, data converters, etc, being designed every year by hundreds of R&amp;D groups world-wide. To date, the workhorse for testing all these designs has been a suite of lab instruments including oscilloscopes and signal generators, to mention the most popular. However, as components become more complex and pin numbers soar, the need for more parallel and versatile testing tools also becomes more pressing. In this work, we describe and benchmark an FPGA system developed that addresses this need. This general purpose testing system features a 64-channel source-meter unit (SMU), and 2x banks of 32 digital pins for digital I/O. We demonstrate that this bench-top system can obtain $170 pA$ current noise floor, $40 ns$ pulse delivery at $\pm13.5 V$ and $12 mA$ maximum current drive/channel. We then showcase the instrument's use in performing a selection of three characteristic measurement tasks: a) current-voltage (IV) characterisation of a diode and a transistor, b) fully parallel read-out of a memristor crossbar array and c) an integral non-linearity (INL) test on a DAC. This work introduces a down-scaled electronics laboratory packaged in a single instrument which provides a shift towards more affordable, reliable, compact and multi-functional instrumentation for emerging electronic technologies.      
### 21.Analysis of Linear Time-Varying &amp; Periodic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.00498.pdf)
>  This thesis applies Floquet theory to analyze linear periodic time-varying (LPTV) systems, represented by a system of ordinary differential equations (ODEs) that depend on a time variable t and have a matrix of coefficients with period T&gt;0. The transition matrix of an LPTV system represented by a square periodic-function matrix A(t)=A(t+T) can be expressed as the product of a square periodic function matrix P(t)=P(t+T) and an exponentiated square matrix of the form Rt, where R is a constant matrix (independent of t). Despite the validity of Floquet theory, it is difficult to find an analytical closed form for the matrices P(t) and R when the transition matrix {\Phi}_A (t,t_0 ) is unknown. In essence, it is difficult to find an analytical solution for an LPTV system (i.e., a closed form for its transition matrix). The research results show that for a given family of periodic matrices A(t), we can compare the powers of {\omega} that multiply the harmonics (i.e., {\omega} is part of the coefficients multiplying the cosine [sine] factors in even [odd] representations or of the exponential factors in complex representations) to determine the matrices P(t) and R. In addition, the results lead to relations between LPTV systems at frequency {\omega} and the associated linear time-invariant system, which is defined by having zero frequency ({\omega}=0).      
### 22.When RIS Meets GEO Satellite Communications: A New Optimization Framework in 6G  [ :arrow_down: ](https://arxiv.org/pdf/2202.00497.pdf)
>  Reflecting intelligent surfaces (RIS) is a low-cost and energy-efficient solution to achieve high spectral efficiency in sixth-generation (6G) networks. The basic idea of RIS is to smartly reconfigure the signal propagation by using passive reflecting elements. On the other side, the demand of high throughput geostationary (GEO) satellite communications (SatCom) is rapidly growing to deliver broadband services in inaccessible/insufficient covered areas of terrestrial networks. This paper proposes a GEO SatCom network, where a satellite transmits the signal to a ground mobile terminal using multicarrier communications. To enhance the effective gain, the signal delivery from satellite to the ground mobile terminal is also assisted by RIS which smartly shift the phase of the signal towards ground terminal. We consider that RIS is mounted on a high building and equipped with multiple re-configurable passive elements along with smart controller. We jointly optimize the power allocation and phase shift design to maximize the channel capacity of the system. The joint optimization problem is formulated as nonconvex due to coupled variables which is hard to solve through traditional convex optimization methods. Thus, we propose a new $\epsilon-$optimal algorithm which is based on Mesh Adaptive Direct Search to obtain an efficient solution. Simulation results unveil the benefits of RIS-assisted SatCom in terms of system channel capacity.      
### 23.A generalizable approach based on U-Net model for automatic Intra retinal cyst segmentation in SD-OCT images  [ :arrow_down: ](https://arxiv.org/pdf/2202.00465.pdf)
>  Intra retinal fluids or Cysts are one of the important symptoms of macular pathologies that are efficiently visualized in OCT images. Automatic segmentation of these abnormalities has been widely investigated in medical image processing studies. In this paper, we propose a new U-Net-based approach for Intra retinal cyst segmentation across different vendors that improves some of the challenges faced by previous deep-based techniques. The proposed method has two main steps: 1- prior information embedding and input data adjustment, and 2- IRC segmentation model. In the first step, we inject the information into the network in a way that overcomes some of the network limitations in receiving data and learning important contextual knowledge. And in the next step, we introduced a connection module between encoder and decoder parts of the standard U-Net architecture that transfers information more effectively from the encoder to the decoder part. Two public datasets namely OPTIMA and KERMANY were employed to evaluate the proposed method. Results showed that the proposed method is an efficient vendor-independent approach for IRC segmentation with mean Dice values of 0.78 and 0.81 on the OPTIMA and KERMANY datasets, respectively.      
### 24.Sinogram Enhancement with Generative Adversarial Networks using Shape Priors  [ :arrow_down: ](https://arxiv.org/pdf/2202.00419.pdf)
>  Compensating scarce measurements by inferring them from computational models is a way to address ill-posed inverse problems. We tackle Limited Angle Tomography by completing the set of acquisitions using a generative model and prior-knowledge about the scanned object. Using a Generative Adversarial Network as model and Computer-Assisted Design data as shape prior, we demonstrate a quantitative and qualitative advantage of our technique over other state-of-the-art methods. Inferring a substantial number of consecutive missing measurements, we offer an alternative to other image inpainting techniques that fall short of providing a satisfying answer to our research question: can X-Ray exposition be reduced by using generative models to infer lacking measurements?      
### 25.CAESR: Conditional Autoencoder and Super-Resolution for Learned Spatial Scalability  [ :arrow_down: ](https://arxiv.org/pdf/2202.00416.pdf)
>  In this paper, we present CAESR, an hybrid learning-based coding approach for spatial scalability based on the versatile video coding (VVC) standard. Our framework considers a low-resolution signal encoded with VVC intra-mode as a base-layer (BL), and a deep conditional autoencoder with hyperprior (AE-HP) as an enhancement-layer (EL) model. The EL encoder takes as inputs both the upscaled BL reconstruction and the original image. Our approach relies on conditional coding that learns the optimal mixture of the source and the upscaled BL image, enabling better performance than residual coding. On the decoder side, a super-resolution (SR) module is used to recover high-resolution details and invert the conditional coding process. Experimental results have shown that our solution is competitive with the VVC full-resolution intra coding while being scalable.      
### 26.A Privacy-Preserving Image Retrieval Scheme with a Mixture of Plain and EtC Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.00382.pdf)
>  In this paper, we propose a novel content-based image-retrieval scheme that allows us to use a mixture of plain images and compressible encrypted ones called "encryption-then-compression (EtC) images." In the proposed scheme, extended SIMPLE descriptors are extracted from EtC images as well as from plain ones, so the mixed use of plain and encrypted images is available for image retrieval. In an experiment, the proposed scheme was demonstrated to have almost the same retrieval performance as that for plain images, even with a mixture of plain and encrypted images.      
### 27.EPHS: A Port-Hamiltonian Modelling Language  [ :arrow_down: ](https://arxiv.org/pdf/2202.00377.pdf)
>  A prevalent theme throughout science and engineering is the ongoing paradigm shift away from isolated systems towards open and interconnected systems. % Port-Hamiltonian theory developed as a synthesis of geometric mechanics and network theory. The possibility to model complex multiphysical systems via interconnection of simpler components is often advertised as one of its most attractive features. The development of a port-Hamiltonian modelling language however remains a topic which has not been sufficiently addressed. % We report on recent progress towards the formalization and implementation of a modelling language for exergetic port-Hamiltonian systems. Its diagrammatic syntax inspired by bond graphs and its functorial semantics together enable a modular and hierarchical approach to model specification.      
### 28.Underwater Differential Game: Finite-Time Target Hunting Task with Communication Delay  [ :arrow_down: ](https://arxiv.org/pdf/2202.00329.pdf)
>  This work considers designing an unmanned target hunting system for a swarm of unmanned underwater vehicles (UUVs) to hunt a target with high maneuverability. Differential game theory is used to analyze combat policies of UUVs and the target within finite time. The challenge lies in UUVs must conduct their control policies in consideration of not only the consistency of the hunting team but also escaping behaviors of the target. To obtain stable feedback control policies satisfying Nash equilibrium, we construct the Hamiltonian function with Leibniz's formula. For further taken underwater disturbances and communication delay into consideration, modified deep reinforcement learning (DRL) is provided to investigate the underwater target hunting task in an unknown dynamic environment. Simulations show that underwater disturbances have a large impact on the system considering communication delay. Moreover, consistency tests show that UUVs perform better consistency with a relatively small range of disturbances.      
### 29.Position-Dependent Snap Feedforward: A Gaussian Process Framework  [ :arrow_down: ](https://arxiv.org/pdf/2202.00257.pdf)
>  Mechatronic systems have increasingly high performance requirements for motion control. The low-frequency contribution of the flexible dynamics, i.e. the compliance, should be compensated for by means of snap feedforward to achieve high accuracy. Position-dependent compliance, which often occurs in motion systems, requires the snap feedforward parameter to be modeled as a function of position. Position-dependent compliance is compensated for by using a Gaussian process to model the snap feedforward parameter as a continuous function of position. A simulation of a flexible beam shows that a significant performance increase is achieved when using the Gaussian process snap feedforward parameter to compensate for position-dependent compliance.      
### 30.ISNet: Costless and Implicit Image Segmentation for Deep Classifiers, with Application in COVID-19 Detection  [ :arrow_down: ](https://arxiv.org/pdf/2202.00232.pdf)
>  In this work we propose a novel deep neural network (DNN) architecture, ISNet, to solve the task of image segmentation followed by classification, substituting the common pipeline of two networks by a single model. We designed the ISNet for high flexibility and performance: it allows virtually any classification neural network architecture to analyze a common image as if it had been previously segmented. Furthermore, in relation to the original classifier, the ISNet does not cause any increment in computational cost or architectural changes at run-time. To accomplish this, we introduce the concept of optimizing DNNs for relevance segmentation in heatmaps created by Layer-wise Relevance Propagation (LRP), which proves to be equivalent to the classification of previously segmented images. We apply an ISNet based on a DenseNet121 classifier to solve the task of COVID-19 detection in chest X-rays. We compare the model to a U-net (performing lung segmentation) followed by a DenseNet121, and to a standalone DenseNet121. Due to the implicit segmentation, the ISNet precisely ignored the X-ray regions outside of the lungs; it achieved 94.5 +/-4.1% mean accuracy with an external database, showing strong generalization capability and surpassing the other models' performances by 6 to 7.9%. ISNet presents a fast and light methodology to perform classification preceded by segmentation, while also being more accurate than standard pipelines.      
### 31.Recognition-Aware Learned Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2202.00198.pdf)
>  Learned image compression methods generally optimize a rate-distortion loss, trading off improvements in visual distortion for added bitrate. Increasingly, however, compressed imagery is used as an input to deep learning networks for various tasks such as classification, object detection, and superresolution. We propose a recognition-aware learned compression method, which optimizes a rate-distortion loss alongside a task-specific loss, jointly learning compression and recognition networks. We augment a hierarchical autoencoder-based compression network with an EfficientNet recognition model and use two hyperparameters to trade off between distortion, bitrate, and recognition performance. We characterize the classification accuracy of our proposed method as a function of bitrate and find that for low bitrates our method achieves as much as 26% higher recognition accuracy at equivalent bitrates compared to traditional methods such as Better Portable Graphics (BPG).      
### 32.Blind Image Deconvolution Using Variational Deep Image Prior  [ :arrow_down: ](https://arxiv.org/pdf/2202.00179.pdf)
>  Conventional deconvolution methods utilize hand-crafted image priors to constrain the optimization. While deep-learning-based methods have simplified the optimization by end-to-end training, they fail to generalize well to blurs unseen in the training dataset. Thus, training image-specific models is important for higher generalization. Deep image prior (DIP) provides an approach to optimize the weights of a randomly initialized network with a single degraded image by maximum a posteriori (MAP), which shows that the architecture of a network can serve as the hand-crafted image prior. Different from the conventional hand-crafted image priors that are statistically obtained, it is hard to find a proper network architecture because the relationship between images and their corresponding network architectures is unclear. As a result, the network architecture cannot provide enough constraint for the latent sharp image. This paper proposes a new variational deep image prior (VDIP) for blind image deconvolution, which exploits additive hand-crafted image priors on latent sharp images and approximates a distribution for each pixel to avoid suboptimal solutions. Our mathematical analysis shows that the proposed method can better constrain the optimization. The experimental results further demonstrate that the generated images have better quality than that of the original DIP on benchmark datasets. The source code of our VDIP is available at <a class="link-external link-https" href="https://github.com/Dong-Huo/VDIP-Deconvolution" rel="external noopener nofollow">this https URL</a>.      
### 33.Fractional Motion Estimation for Point Cloud Compression  [ :arrow_down: ](https://arxiv.org/pdf/2202.00172.pdf)
>  Motivated by the success of fractional pixel motion in video coding, we explore the design of motion estimation with fractional-voxel resolution for compression of color attributes of dynamic 3D point clouds. Our proposed block-based fractional-voxel motion estimation scheme takes into account the fundamental differences between point clouds and videos, i.e., the irregularity of the distribution of voxels within a frame and across frames. We show that motion compensation can benefit from the higher resolution reference and more accurate displacements provided by fractional precision. Our proposed scheme significantly outperforms comparable methods that only use integer motion. The proposed scheme can be combined with and add sizeable gains to state-of-the-art systems that use transforms such as Region Adaptive Graph Fourier Transform and Region Adaptive Haar Transform.      
### 34.A self-organizing multi-agent system for distributed voltage regulation  [ :arrow_down: ](https://arxiv.org/pdf/2202.00170.pdf)
>  This paper presents a distributed voltage regulation method based on multi-agent system control and network self-organization for a large distribution network. The network autonomously organizes itself into small subnetworks through the epsilon decomposition of the sensitivity matrix, and agents group themselves into these subnetworks with the communication links being autonomously determined. Each subnetwork controls its voltage by locating the closest local distributed generation and optimizing their outputs. This simplifies and reduces the size of the optimization problem and the interaction requirements. This approach also facilitates adaptive grouping of the network by self-reorganizing to maintain a stable state in response to time-varying network requirements and changes. The effectiveness of the proposed approach is validated through simulations on a model of a real heavily-meshed secondary distribution network. Simulation results and comparisons with other methods demonstrate the ability of the subnetworks to autonomously and independently regulate the voltage and to adapt to unpredictable network conditions over time, thereby enabling autonomous and flexible distribution networks.      
### 35.Design Constraints of Disturbance Observer-based Motion Control Systems are Stricter in the Discrete-Time Domain  [ :arrow_down: ](https://arxiv.org/pdf/2202.00165.pdf)
>  This paper shows that the design constraints of the Disturbance Observer (DOb) based robust motion control systems become stricter when they are implemented using computers or microcontrollers. The stricter design constraints put new upper bounds on the plant-model mismatch and the bandwidth of the DOb, thus limiting the achievable robustness against disturbances and the phase-lead effect in the inner-loop. Violating the design constraints may yield severe stability and performance issues in practice; therefore, they should be considered in tuning the control parameters of the robust motion controller. This paper also shows that continuous-time analysis methods fall-short in deriving the fundamental design constraints on the nominal plant model and the bandwidth of the digital DOb. Therefore, we may observe unexpected stability and performance issues when tuning the control parameters of the digital robust motion controllers in the continuous-time domain. To improve the robust stability and performance of the motion controllers, this paper explains the fundamental design constraints of the DOb by employing the generalised continuous and discrete Bode Integral Theorems in a unified framework. Simulation and experimental results are given to verify the proposed analysis method.      
### 36.Reply to comment on "Failure of the simultaneous block diagonalization technique applied to complete and cluster synchronization of random networks"  [ :arrow_down: ](https://arxiv.org/pdf/2202.00128.pdf)
>  We respond briefly to a comment [1, <a class="link-https" data-arxiv-id="2110.15493" href="https://arxiv.org/abs/2110.15493">arXiv:2110.15493</a>] recently posted online on our paper [2, <a class="link-https" data-arxiv-id="2108.07893" href="https://arxiv.org/abs/2108.07893">arXiv:2108.07893</a>]. Complete and cluster synchronization of random networks is undoubtedly a topic of interest in the Physics, Engineering, and Nonlinear Dynamics literature. In [3] we study both complete and cluster synchronization of networks and introduce indices that measure success (or failure) of application of the SBD technique in decoupling the stability problem into problems of lower dimensionality. Our usage of the word `failure' indicates that the technique does not produce a decomposition which results in a system which is easier to analyze, not that the technique fails in correctly decoupling these problems.      
### 37.A Metal Artifact Reduction Scheme For Accurate Iterative Dual-Energy CT Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2202.00116.pdf)
>  CT images have been used to generate radiation therapy treatment plans for more than two decades. Dual-energy CT (DECT) has shown high accuracy in estimating electronic density or proton stopping-power maps used in treatment planning. However, the presence of metal implants introduces severe streaking artifacts in the reconstructed images, affecting the diagnostic accuracy and treatment performance. In order to reduce the metal artifacts in DECT, we introduce a metal-artifact reduction scheme for iterative DECT algorithms. An estimate is substituted for the corrupt data in each iteration. We utilize normalized metal-artifact reduction (NMAR) composed with image-domain decomposition to initialize the algorithm and speed up the convergence. A fully 3D joint statistical DECT algorithm, dual-energy alternating minimization (DEAM), with the proposed scheme is tested on experimental and clinical helical data acquired on a Philips Brilliance Big Bore scanner. We compared DEAM with the proposed method to the original DEAM and vendor reconstructions with and without metal-artifact reduction for orthopedic implants (O-MAR). The visualization and quantitative analysis show that DEAM with the proposed method has the best performance in reducing streaking artifacts caused by metallic objects.      
### 38.Holistic Fine-grained GGS Characterization: From Detection to Unbalanced Classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.00087.pdf)
>  Recent studies have demonstrated the diagnostic and prognostic values of global glomerulosclerosis (GGS) in IgA nephropathy, aging, and end-stage renal disease. However, the fine-grained quantitative analysis of multiple GGS subtypes (e.g., obsolescent, solidified, and disappearing glomerulosclerosis) is typically a resource extensive manual process. Very few automatic methods, if any, have been developed to bridge this gap for such analytics. In this paper, we present a holistic pipeline to quantify GGS (with both detection and classification) from a whole slide image in a fully automatic manner. In addition, we conduct the fine-grained classification for the sub-types of GGS. Our study releases the open-source quantitative analytical tool for fine-grained GGS characterization while tackling the technical challenges in unbalanced classification and integrating detection and classification.      
### 39.AutoGeoLabel: Automated Label Generation for Geospatial Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.00067.pdf)
>  A key challenge of supervised learning is the availability of human-labeled data. We evaluate a big data processing pipeline to auto-generate labels for remote sensing data. It is based on rasterized statistical features extracted from surveys such as e.g. LiDAR measurements. Using simple combinations of the rasterized statistical layers, it is demonstrated that multiple classes can be generated at accuracies of ~0.9. As proof of concept, we utilize the big geo-data platform IBM PAIRS to dynamically generate such labels in dense urban areas with multiple land cover classes. The general method proposed here is platform independent, and it can be adapted to generate labels for other satellite modalities in order to enable machine learning on overhead imagery for land use classification and object detection.      
### 40.Cardiac and respiratory motion extraction for MRI using Pilot Tone-a patient study  [ :arrow_down: ](https://arxiv.org/pdf/2202.00055.pdf)
>  Background:The Pilot Tone (PT) technology allows contactless monitoring of physiological motion during the MRI scan. Several studies have shown that both respiratory and cardiac motion can be extracted from the PT signal successfully. However, most of these studies were performed in healthy volunteers. In this study, we seek to evaluate the accuracy and reliability of the cardiac and respiratory signals extracted from PT in patients clinically referred for cardiovascular MRI (CMR). Methods: Twenty-three patients were included in this study, each scanned under free-breathing conditions using a balanced steady-state free-precession real-time (RT) cine sequence on a 1.5T scanner. The PT signal was generated by a built-in PT transmitter integrated within the body array coil. For comparison, ECG and BioMatrix (BM) respiratory sensor signals were also synchronously recorded. To assess the performances of PT, ECG, and BM, cardiac and respiratory signals extracted from the RT cine images were used as the ground truth. Results: The respiratory motion extracted from PT correlated positively with the image-derived respiratory signal in all cases and showed a stronger correlation (absolute coefficient: 0.95-0.09) than BM (0.72-0.24). For the cardiac signal, the precision of PT-based triggers (standard deviation of PT trigger locations relative to ECG triggers) ranged from 6.6 to 81.2 ms (median 19.5 ms). Overall, the performance of PT-based trigger extraction was comparable to that of ECG. Conclusions: This study demonstrates the potential of PT to monitor both respiratory and cardiac motion in patients clinically referred for CMR.      
### 41.Leveraging Bitstream Metadata for Fast and Accurate Video Compression Correction  [ :arrow_down: ](https://arxiv.org/pdf/2202.00011.pdf)
>  Video compression is a central feature of the modern internet powering technologies from social media to video conferencing. While video compression continues to mature, for many, and particularly for extreme, compression settings, quality loss is still noticeable. These extreme settings nevertheless have important applications to the efficient transmission of videos over bandwidth constrained or otherwise unstable connections. In this work, we develop a deep learning architecture capable of restoring detail to compressed videos which leverages the underlying structure and motion information embedded in the video bitstream. We show that this improves restoration accuracy compared to prior compression correction methods and is competitive when compared with recent deep-learning-based video compression methods on rate-distortion while achieving higher throughput.      
### 42.BREAK: Bronchi Reconstruction by gEodesic transformation And sKeleton embedding  [ :arrow_down: ](https://arxiv.org/pdf/2202.00002.pdf)
>  Airway segmentation is critical for virtual bronchoscopy and computer-aided pulmonary disease analysis. In recent years, convolutional neural networks (CNNs) have been widely used to delineate the bronchial tree. However, the segmentation results of the CNN-based methods usually include many discontinuous branches, which need manual repair in clinical use. A major reason for the breakages is that the appearance of the airway wall can be affected by the lung disease as well as the adjacency of the vessels, while the network tends to overfit to these special patterns in the training set. To learn robust features for these areas, we design a multi-branch framework that adopts the geodesic distance transform to capture the intensity changes between airway lumen and wall. Another reason for the breakages is the intra-class imbalance. Since the volume of the peripheral bronchi may be much smaller than the large branches in an input patch, the common segmentation loss is not sensitive to the breakages among the distal branches. Therefore, in this paper, a breakage-sensitive regularization term is designed and can be easily combined with other loss functions. Extensive experiments are conducted on publicly available datasets. Compared with state-of-the-art methods, our framework can detect more branches while maintaining competitive segmentation performance.      
### 43.Timely Gossiping with File Slicing and Network Coding  [ :arrow_down: ](https://arxiv.org/pdf/2202.00649.pdf)
>  We consider a system consisting of a large network of $n$ users and a library of files, wherein inter-user communication is established based upon gossip mechanisms. Each file is initially present at exactly one node, which is designated as the file \emph{source}. The source gets updated with newer versions of the file according to an arbitrary distribution in real time, and the other users in the network wish to acquire the latest possible version of the file. We present a class of gossip protocols that achieve $O(1)$ age at a typical node in a single-file system and $O(n)$ age at a typical node for a given file in an $n$-file system. We show that file slicing and network coding based protocols fall under the presented class of protocols. Numerical evaluation results are presented to confirm the aforementioned bounds.      
### 44.The impact of removing head movements on audio-visual speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.00538.pdf)
>  This paper investigates the impact of head movements on audio-visual speech enhancement (AVSE). Although being a common conversational feature, head movements have been ignored by past and recent studies: they challenge today's learning-based methods as they often degrade the performance of models that are trained on clean, frontal, and steady face images. To alleviate this problem, we propose to use robust face frontalization (RFF) in combination with an AVSE method based on a variational auto-encoder (VAE) model. We briefly describe the basic ingredients of the proposed pipeline and we perform experiments with a recently released audio-visual dataset. In the light of these experiments, and based on three standard metrics, namely STOI, PESQ and SI-SDR, we conclude that RFF improves the performance of AVSE by a considerable margin.      
### 45.Multi-cell Non-coherent Over-the-Air Computation for Federated Edge Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.00506.pdf)
>  In this paper, we propose a framework where over-the-air computation (OAC) occurs in both uplink (UL) and downlink (DL), sequentially, in a multi-cell environment to address the latency and the scalability issues of federated edge learning (FEEL). To eliminate the channel state information (CSI) at the edge devices (EDs) and edge servers (ESs) and relax the time-synchronization requirement for the OAC, we use a non-coherent computation scheme, i.e., frequency-shift keying (FSK)-based majority vote (MV) (FSK-MV). With the proposed framework, multiple ESs function as the aggregation nodes in the UL and each ES determines the MVs independently. After the ESs broadcast the detected MVs, the EDs determine the sign of the gradient through another OAC in the DL. Hence, inter-cell interference is exploited for the OAC. In this study, we prove the convergence of the non-convex optimization problem for the FEEL with the proposed OAC framework. We also numerically evaluate the efficacy of the proposed method by comparing the test accuracy in both multi-cell and single-cell scenarios for both homogeneous and heterogeneous data distributions.      
### 46.Language Dependencies in Adversarial Attacks on Speech Recognition Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.00399.pdf)
>  Automatic speech recognition (ASR) systems are ubiquitously present in our daily devices. They are vulnerable to adversarial attacks, where manipulated input samples fool the ASR system's recognition. While adversarial examples for various English ASR systems have already been analyzed, there exists no inter-language comparative vulnerability analysis. We compare the attackability of a German and an English ASR system, taking Deepspeech as an example. We investigate if one of the language models is more susceptible to manipulations than the other. The results of our experiments suggest statistically significant differences between English and German in terms of computational effort necessary for the successful generation of adversarial examples. This result encourages further research in language-dependent characteristics in the robustness analysis of ASR.      
### 47.NTU VIRAL: A Visual-Inertial-Ranging-Lidar Dataset, From an Aerial Vehicle Viewpoint  [ :arrow_down: ](https://arxiv.org/pdf/2202.00379.pdf)
>  In recent years, autonomous robots have become ubiquitous in research and daily life. Among many factors, public datasets play an important role in the progress of this field, as they waive the tall order of initial investment in hardware and manpower. However, for research on autonomous aerial systems, there appears to be a relative lack of public datasets on par with those used for autonomous driving and ground robots. Thus, to fill in this gap, we conduct a data collection exercise on an aerial platform equipped with an extensive and unique set of sensors: two 3D lidars, two hardware-synchronized global-shutter cameras, multiple Inertial Measurement Units (IMUs), and especially, multiple Ultra-wideband (UWB) ranging units. The comprehensive sensor suite resembles that of an autonomous driving car, but features distinct and challenging characteristics of aerial operations. We record multiple datasets in several challenging indoor and outdoor conditions. Calibration results and ground truth from a high-accuracy laser tracker are also included in each package. All resources can be accessed via our webpage <a class="link-external link-https" href="https://ntu-aris.github.io/ntu_viral_dataset" rel="external noopener nofollow">this https URL</a>.      
### 48.Finite-time enclosing control for multiple moving targets: a continuous estimator approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.00347.pdf)
>  This work addresses the finite-time enclosing control problem where a set of followers are deployed to encircle and rotate around multiple moving targets with a predefined spacing pattern in finite time. A novel distributed and continuous estimator is firstly proposed to track the geometric center of targets in finite time using only local information for every follower. Then a pair of decentralized control laws for both the relative distance and included angle, respectively, are designed to achieve the desired spacing pattern in finite time based on the output of the proposed estimator. Through both theoretical analysis and simulation validation, we show that the proposed estimator is continuous and therefore can avoid dithering control output while still inheriting the merit of finite-time convergence. The steady errors of the estimator and the enclosing controller are guaranteed to converge to some bounded and adjustable regions around zero.      
### 49.Measurement-Based Validation of Z3RO Precoder to Prevent Nonlinear Amplifier Distortion in Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.00286.pdf)
>  In multiple input multiple output (MIMO) systems, precoding allows the base station to spatially focus and multiplex signals towards each user. However, distortion introduced by power amplifier nonlinearities coherently combines in the same spatial directions when using a conventional precoder such as maximum ratio transmission (MRT). This can strongly limit the user performance and moreover create unauthorized out-of-band (OOB) emissions. In order to overcome this problem, the zero third-order distortion (Z3RO) precoder was recently introduced. This precoder constraints the third-order distortion at the user location to be zero. In this work, the performance of the Z3RO precoder is validated based on real-world channel measurement data. The results illustrate the reduction in distortion power at the UE locations: an average distortion reduction of 6.03 dB in the worst-case single-user scenario and 3.54 dB in the 2-user case at a back-off rate of -3 dB.      
### 50.Relative Transformation Estimation Based on Fusion of Odometry and UWB Ranging Data  [ :arrow_down: ](https://arxiv.org/pdf/2202.00279.pdf)
>  In this work, the problem of 4 degree-of-freedom (3D position and heading) robot-to-robot relative frame transformation estimation using onboard odometry and inter-robot distance measurements is studied. Firstly, we present a theoretical analysis of the problem, namely the derivation and interpretation of the Cramer-Rao Lower Bound (CRLB), the Fisher Information Matrix (FIM) and its determinant. Secondly, we propose optimization-based methods to solve the problem, including a quadratically constrained quadratic programming (QCQP) and the corresponding semidefinite programming (SDP) relaxation. Moreover, we address practical issues that are ignored in previous works, such as accounting for spatial-temporal offsets between the ultra-wideband (UWB) and odometry sensors, rejecting UWB outliers and checking for singular configurations before commencing operation. Lastly, extensive simulations and real-life experiments with aerial robots show that the proposed QCQP and SDP methods outperform state-of-the-art methods, especially in geometrically poor or large measurement noise conditions. In general, the QCQP method provides the best results at the expense of computational time, while the SDP method runs much faster and is sufficiently accurate in most cases.      
### 51.Adversarial Imitation Learning from Video using a State Observer  [ :arrow_down: ](https://arxiv.org/pdf/2202.00243.pdf)
>  The imitation learning research community has recently made significant progress towards the goal of enabling artificial agents to imitate behaviors from video demonstrations alone. However, current state-of-the-art approaches developed for this problem exhibit high sample complexity due, in part, to the high-dimensional nature of video observations. Towards addressing this issue, we introduce here a new algorithm called Visual Generative Adversarial Imitation from Observation using a State Observer VGAIfO-SO. At its core, VGAIfO-SO seeks to address sample inefficiency using a novel, self-supervised state observer, which provides estimates of lower-dimensional proprioceptive state representations from high-dimensional images. We show experimentally in several continuous control environments that VGAIfO-SO is more sample efficient than other IfO algorithms at learning from video-only demonstrations and can sometimes even achieve performance close to the Generative Adversarial Imitation from Observation (GAIfO) algorithm that has privileged access to the demonstrator's proprioceptive state information.      
### 52.A pilot study of the Earable device to measure facial muscle and eye movement tasks among healthy volunteers  [ :arrow_down: ](https://arxiv.org/pdf/2202.00206.pdf)
>  Many neuromuscular disorders impair function of cranial nerve enervated muscles. Clinical assessment of cranial muscle function has several limitations. Clinician rating of symptoms suffers from inter-rater variation, qualitative or semi-quantitative scoring, and limited ability to capture infrequent or fluctuating symptoms. Patient-reported outcomes are limited by recall bias and poor precision. Current tools to measure orofacial and oculomotor function are cumbersome, difficult to implement, and non-portable. Here, we show how Earable, a wearable device, can discriminate certain cranial muscle activities such as chewing, talking, and swallowing. We demonstrate using data from a pilot study of 10 healthy participants how Earable can be used to measure features from EMG, EEG, and EOG waveforms from subjects performing mock Performance Outcome Assessments (mock-PerfOs), utilized widely in clinical research. Our analysis pipeline provides a framework for how to computationally process and statistically rank features from the Earable device. Finally, we demonstrate that Earable data may be used to classify these activities. Our results, conducted in a pilot study of healthy participants, enable a more comprehensive strategy for the design, development, and analysis of wearable sensor data for investigating clinical populations. Additionally, the results from this study support further evaluation of Earable or similar devices as tools to objectively measure cranial muscle activity in the context of a clinical research setting. Future work will be conducted in clinical disease populations, with a focus on detecting disease signatures, as well as monitoring intra-subject treatment responses. Readily available quantitative metrics from wearable sensor devices like Earable support strategies for the development of novel digital endpoints, a hallmark goal of clinical research.      
### 53.Differentiable Digital Signal Processing Mixture Model for Synthesis Parameter Extraction from Mixture of Harmonic Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2202.00200.pdf)
>  A differentiable digital signal processing (DDSP) autoencoder is a musical sound synthesizer that combines a deep neural network (DNN) and spectral modeling synthesis. It allows us to flexibly edit sounds by changing the fundamental frequency, timbre feature, and loudness (synthesis parameters) extracted from an input sound. However, it is designed for a monophonic harmonic sound and cannot handle mixtures of harmonic sounds. In this paper, we propose a model (DDSP mixture model) that represents a mixture as the sum of the outputs of multiple pretrained DDSP autoencoders. By fitting the output of the proposed model to the observed mixture, we can directly estimate the synthesis parameters of each source. Through synthesis parameter extraction experiments, we show that the proposed method has high and stable performance compared with a straightforward method that applies the DDSP autoencoder to the signals separated by an audio source separation method.      
### 54.A Unified Robust Motion Controller Synthesis for Compliant Robots Driven by Series Elastic Actuators  [ :arrow_down: ](https://arxiv.org/pdf/2202.00168.pdf)
>  This paper proposes a unified robust motion controller for the position and force control problems of compliant robot manipulators driven by Series Elastic Actuators (SEAs). It is shown that the dynamic model of the compliant robot includes not only matched but also mismatched disturbances that act on the system through a different channel from the control input. To tackle this complex robust control problem, the unified robust motion controller is synthesised by employing a second-order Disturbance Observer (DOb), which allows us to estimate not only disturbances but also their first and second order derivatives, and a novel controller design approach in state space. By using the Brunovsky canonical form transformation and the estimations of disturbances and their first and second order derivatives, the dynamic model of the robot is reconstructed so that a new system model that includes only matched disturbances is obtained for compliant robots driven by SEAs. The robust position and force controllers are simply designed by eliminating the matched disturbances of the reconstructed system model via the conventional DOb-based robust control method. The stability and performance of the proposed robust motion controllers are verified by simulations.      
### 55.Teaching Predictive Control Using Specification-based Summative Assessments  [ :arrow_down: ](https://arxiv.org/pdf/2202.00157.pdf)
>  Including Model Predictive Control (MPC) in the undergraduate/graduate control curriculum is becoming vitally important due to the growing adoption of MPC in many industrial areas. In this paper, we present an overview of the predictive control course taught by the authors at Imperial College London between 2018 and 2021. We discuss how the course evolved from focusing solely on the linear MPC formulation to covering nonlinear MPC and some of its extensions. We also present a novel specification-based summative assessment framework, written in MATLAB, that was developed to assess the knowledge and understanding of the students in the course by tasking them with designing a controller for a real-world problem. The MATLAB assessment framework was designed to provide the students with the freedom to design and implement any MPC controller they wanted. The submitted controllers were then assessed against over 30 variations of the real-world problem to gauge student understanding of design robustness and the MPC topics from the course.      
### 56.Continuous Forecasting via Neural Eigen Decomposition of Stochastic Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2202.00117.pdf)
>  Motivated by a real-world problem of blood coagulation control in Heparin-treated patients, we use Stochastic Differential Equations (SDEs) to formulate a new class of sequential prediction problems -- with an unknown latent space, unknown non-linear dynamics, and irregular sparse observations. We introduce the Neural Eigen-SDE (NESDE) algorithm for sequential prediction with sparse observations and adaptive dynamics. NESDE applies eigen-decomposition to the dynamics model to allow efficient frequent predictions given sparse observations. In addition, NESDE uses a learning mechanism for adaptive dynamics model, which handles changes in the dynamics both between sequences and within sequences. We demonstrate the accuracy and efficacy of NESDE for both synthetic problems and real-world data. In particular, to the best of our knowledge, we are the first to provide a patient-adapted prediction for blood coagulation following Heparin dosing in the MIMIC-IV dataset. Finally, we publish a simulated gym environment based on our prediction model, for experimentation in algorithms for blood coagulation control.      
### 57.A Lightweight Workload-Aware Microservices Autoscaling with QoS Assurance  [ :arrow_down: ](https://arxiv.org/pdf/2202.00057.pdf)
>  Cloud applications are increasingly moving away from monolithic services to agile microservices-based deployments. However, efficient resource management for microservices poses a significant hurdle due to the sheer number of loosely coupled and interacting components. The interdependencies between various microservices make existing cloud resource autoscaling techniques ineffective. Meanwhile, machine learning (ML) based approaches that try to capture the complex relationships in microservices require extensive training data and cause intentional SLO violations. Moreover, these ML-heavy approaches are slow in adapting to dynamically changing microservice operating environments. In this paper, we propose PEMA (Practical Efficient Microservice Autoscaling), a lightweight microservice resource manager that finds efficient resource allocation through opportunistic resource reduction. PEMA's lightweight design enables novel workload-aware and adaptive resource management. Using three prototype microservice implementations, we show that PEMA can find close to optimum resource allocation and save up to 33% resource compared to the commercial rule-based resource allocations.      
