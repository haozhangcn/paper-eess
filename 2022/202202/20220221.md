# ArXiv eess --Mon, 21 Feb 2022
### 1.A reliability measure for smart surveillance systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.09339.pdf)
>  We present a reliability measure for smart surveillance systems, taking into account the adversarial nature of intrusion. Our approach is based on percolation theory and is a generalisation of Hamedmoghadam et al.'s reliability measure. Specifically, our approach incorporates a customisable cost function to allow modelling a diverse range of situations, such as access restrictions, monitoring, and failures. We demonstrate our approach by applying it to a digital twin of a smart building, albeit challenges remain in estimating and modelling the key parameters needed.      
### 2.Distributed Transient Safety Verification via Robust Control Invariant Sets: A Microgrid Application  [ :arrow_down: ](https://arxiv.org/pdf/2202.09320.pdf)
>  Modern safety-critical energy infrastructures are increasingly operated in a hierarchical and modular control framework which allows for limited data exchange between the modules. In this context, it is important for each module to synthesize and communicate constraints on the values of exchanged information in order to assure system-wide safety. To ensure transient safety in inverter-based microgrids, we develop a set invariance-based distributed safety verification algorithm for each inverter module. Applying Nagumo's invariance condition, we construct a robust polynomial optimization problem to jointly search for safety-admissible set of control set-points and design parameters, under allowable disturbances from neighbors. We use sum-of-squares (SOS) programming to solve the verification problem and we perform numerical simulations using grid-forming inverters to illustrate the algorithm.      
### 3.tinyMAN: Lightweight Energy Manager using Reinforcement Learning for Energy Harvesting Wearable IoT Devices  [ :arrow_down: ](https://arxiv.org/pdf/2202.09297.pdf)
>  Advances in low-power electronics and machine learning techniques lead to many novel wearable IoT devices. These devices have limited battery capacity and computational power. Thus, energy harvesting from ambient sources is a promising solution to power these low-energy wearable devices. They need to manage the harvested energy optimally to achieve energy-neutral operation, which eliminates recharging requirements. Optimal energy management is a challenging task due to the dynamic nature of the harvested energy and the battery energy constraints of the target device. To address this challenge, we present a reinforcement learning-based energy management framework, tinyMAN, for resource-constrained wearable IoT devices. The framework maximizes the utilization of the target device under dynamic energy harvesting patterns and battery constraints. Moreover, tinyMAN does not rely on forecasts of the harvested energy which makes it a prediction-free approach. We deployed tinyMAN on a wearable device prototype using TensorFlow Lite for Micro thanks to its small memory footprint of less than 100 KB. Our evaluations show that tinyMAN achieves less than 2.36 ms and 27.75 $\mu$J while maintaining up to 45% higher utility compared to prior approaches.      
### 4.System Safety and Artificial Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2202.09292.pdf)
>  This chapter formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social and institutional components of a system. This chapter honors system safety pioneer Nancy Leveson, by situating her core lessons for today's AI system safety challenges. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.      
### 5.Soft Actor-Critic Deep Reinforcement Learning for Fault Tolerant Flight Control  [ :arrow_down: ](https://arxiv.org/pdf/2202.09262.pdf)
>  Fault-tolerant flight control faces challenges, as developing a model-based controller for each unexpected failure is unrealistic, and online learning methods can handle limited system complexity due to their low sample efficiency. In this research, a model-free coupled-dynamics flight controller for a jet aircraft able to withstand multiple failure types is proposed. An offline trained cascaded Soft Actor-Critic Deep Reinforcement Learning controller is successful on highly coupled maneuvers, including a coordinated 40 degree bank climbing turn with a normalized Mean Absolute Error of 2.64%. The controller is robust to six failure cases, including the rudder jammed at -15 deg, the aileron effectiveness reduced by 70%, a structural failure, icing and a backward c.g. shift as the response is stable and the climbing turn is completed successfully. Robustness to biased sensor noise, atmospheric disturbances, and to varying initial flight conditions and reference signal shapes is also demonstrated.      
### 6.Reduced-Order Modeling of Thermal Dynamics in District Energy Networks using Spectral Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2202.09259.pdf)
>  Simulation of thermal dynamics in city-scale district energy grids often becomes computationally prohibitive for long simulation runs. Current model order reduction methods offer limited interpretability with regards to the non-reduced system, and are not in general applicable for e.g., varying flow rates, multiple producers, or changing flow directions. This article presents a novel method based on graph theory that approximates the solution of an optimization problem that minimizes the local truncation error for heat transport in the grid. It is shown that the method can be used to reduce the thermal dynamic model of a city-scale energy grid, resulting in a coarser temporal and spatial resolution. The relative root mean square error was 2.3\% for the temperature in the evaluation scenario, comparing the reduced-order system with the non-reduced system at the instances of the coarser time-step.      
### 7.Autoencoding Low-Resolution MRI for Semantically Smooth Interpolation of Anisotropic MRI  [ :arrow_down: ](https://arxiv.org/pdf/2202.09258.pdf)
>  High-resolution medical images are beneficial for analysis but their acquisition may not always be feasible. Alternatively, high-resolution images can be created from low-resolution acquisitions using conventional upsampling methods, but such methods cannot exploit high-level contextual information contained in the images. Recently, better performing deep-learning based super-resolution methods have been introduced. However, these methods are limited by their supervised character, i.e. they require high-resolution examples for training. Instead, we propose an unsupervised deep learning semantic interpolation approach that synthesizes new intermediate slices from encoded low-resolution examples. To achieve semantically smooth interpolation in through-plane direction, the method exploits the latent space generated by autoencoders. To generate new intermediate slices, latent space encodings of two spatially adjacent slices are combined using their convex combination. Subsequently, the combined encoding is decoded to an intermediate slice. To constrain the model, a notion of semantic similarity is defined for a given dataset. For this, a new loss is introduced that exploits the spatial relationship between slices of the same volume. During training, an existing in-between slice is generated using a convex combination of its neighboring slice encodings. The method was trained and evaluated using publicly available cardiac cine, neonatal brain and adult brain MRI scans. In all evaluations, the new method produces significantly better results in terms of Structural Similarity Index Measure and Peak Signal-to-Noise Ratio (p&lt; 0.001 using one-sided Wilcoxon signed-rank test) than a cubic B-spline interpolation approach. Given the unsupervised nature of the method, high-resolution training data is not required and hence, the method can be readily applied in clinical settings.      
### 8.Cell-Free Massive MIMO in Virtualized CRAN: How to Minimize the Total Network Power?  [ :arrow_down: ](https://arxiv.org/pdf/2202.09254.pdf)
>  Previous works on cell-free massive MIMO mostly consider physical-layer and fronthaul transport aspects. How to deploy cell-free massive MIMO functionality in a practical wireless system is an open problem. This paper proposes a new cell-free architecture that can be implemented on top of a virtualized cloud radio access network (V-CRAN). We aim to minimize the end-to-end power consumption by jointly considering the radio, optical fronthaul, virtualized cloud processing resources, and spectral efficiency requirements of the user equipments. The considered optimization problem is cast in a mixed binary second-order cone programming form and, thus, the global optimum can be found using a branch-and-bound algorithm. The optimal power-efficient solution of our proposed cell-free system is compared with conventional small-cell implemented using V-CRAN, to determine the benefits of cell-free networking. The numerical results demonstrate that cell-free massive MIMO increases the maximum rate substantially, which can be provided with almost the same energy per bit. We show that it is more power-efficient to activate cell-free massive MIMO already at low spectral efficiencies (above 1 bit/s/Hz).      
### 9.History Data Driven Distributed Consensus in Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.09223.pdf)
>  The association of weights in a distributed consensus protocol quantify the trust that an agent has on its neighbors in a network. An important problem in such networked systems is the uncertainty in the estimation of trust between neighboring agents, coupled with the losses arising from mistakenly associating wrong amounts of trust with different neighboring agents. We introduce a probabilistic approach which uses the historical data collected in the network, to determine the level of trust between each agent. Specifically, using the finite history of the shared data between neighbors, we obtain a configuration which represents the confidence estimate of every neighboring agent's trustworthiness. Finally, we propose a History-Data-Driven (HDD) distributed consensus protocol which translates the computed configuration data into weights to be used in the consensus update. The approach using the historical data in the context of a distributed consensus setting marks the novel contribution of our paper.      
### 10.OKVIS2: Realtime Scalable Visual-Inertial SLAM with Loop Closure  [ :arrow_down: ](https://arxiv.org/pdf/2202.09199.pdf)
>  Robust and accurate state estimation remains a challenge in robotics, Augmented, and Virtual Reality (AR/VR), even as Visual-Inertial Simultaneous Localisation and Mapping (VI-SLAM) getting commoditised. Here, a full VI-SLAM system is introduced that particularly addresses challenges around long as well as repeated loop-closures. A series of experiments reveals that it achieves and in part outperforms what state-of-the-art open-source systems achieve. At the core of the algorithm sits the creation of pose-graph edges through marginalisation of common observations, which can fluidly be turned back into landmarks and observations upon loop-closure. The scheme contains a realtime estimator optimising a bounded-size factor graph consisting of observations, IMU pre-integral error terms, and pose-graph edges -- and it allows for optimisation of larger loops re-using the same factor-graph asynchronously when needed.      
### 11.Task-oriented Scheduling for Networked Control Systems: An Age of Information-Aware Implementation on Software-defined Radios  [ :arrow_down: ](https://arxiv.org/pdf/2202.09189.pdf)
>  Networked control systems (NCSs) are feedback control loops that are closed over a communication network. Emerging applications, such as telerobotics, drones and autonomous driving are the most prominent examples of such systems. Regular and timely information sharing between the components of NCSs is essential, as stale information can lead to performance degradation or even physical damage. In this work, we consider multiple heterogeneous NCSs that transmit their system state over a shared physical wireless channel towards a gateway node. We conduct a comprehensive experimental study on selected MAC protocols using software-defined radios with state-of-the-art (SotA) solutions that have been designed to increase information freshness and control performance. As a significant improvement over the SotA, we propose a contention-free algorithm that is able to outperform the existing solutions by combining their strengths in one protocol. In addition, we propose a new metric called normalized mean squared error and demonstrate its effectiveness as utility for scheduling in a case study with multiple inverted pendulums. From our experimental study and results, we observe that a control-aware prioritization of the sub-systems contributes to minimizing the negative effects of information staleness on control performance. In particular, as the number of devices increases, the benefit of control-awareness to the quality of control stands out when compared to protocols that focus solely on maximizing information freshness.      
### 12.Domain Adaptation of low-resource Target-Domain models using well-trained ASR Conformer Models  [ :arrow_down: ](https://arxiv.org/pdf/2202.09167.pdf)
>  In this paper, we investigate domain adaptation for low-resource Automatic Speech Recognition (ASR) of target-domain data, when a well-trained ASR model trained with a large dataset is available. We argue that in the encoder-decoder framework, the decoder of the well-trained ASR model is largely tuned towards the source-domain, hurting the performance of target-domain models in vanilla transfer-learning. On the other hand, the encoder layers of the well-trained ASR model mostly capture the acoustic characteristics. We, therefore, propose to use the embeddings tapped from these encoder layers as features for a downstream Conformer target-domain model and show that they provide significant improvements. We do ablation studies on which encoder layer is optimal to tap the embeddings, as well as the effect of freezing or updating the well-trained ASR model's encoder layers. We further show that applying Spectral Augmentation (SpecAug) on the proposed features (this is in addition to default SpecAug on input spectral features) provides a further improvement on the target-domain performance. For the LibriSpeech-100-clean data as target-domain and SPGI-5000 as a well-trained model, we get 30% relative improvement over baseline. Similarly, with WSJ data as target-domain and LibriSpeech-960 as a well-trained model, we get 50% relative improvement over baseline.      
### 13.Tackling A Class of Hard Subset-Sum Problems: Integration of Lattice Attacks with Disaggregation Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2202.09157.pdf)
>  Subset-sum problems belong to the NP class and play an important role in both complexity theory and knapsack-based cryptosystems, which have been proved in the literature to become hardest when the so-called density approaches one. Lattice attacks, which are acknowledged in the literature as the most effective methods, fail occasionally even when the number of unknown variables is of medium size. In this paper we propose a modular disaggregation technique and a simplified lattice formulation based on which two lattice attack algorithms are further designed. We introduce the new concept "jump points" in our disaggregation technique, and derive inequality conditions to identify superior jump points which can more easily cut-off non-desirable short integer solutions. Empirical tests have been conducted to show that integrating the disaggregation technique with lattice attacks can effectively raise success ratios to 100% for randomly generated problems with density one and of dimensions up to 100. Finally, statistical regressions are conducted to test significant features, thus revealing reasonable factors behind the empirical success of our algorithms and techniques proposed in this paper.      
### 14.Multi-view and Multi-modal Event Detection Utilizing Transformer-based Multi-sensor fusion  [ :arrow_down: ](https://arxiv.org/pdf/2202.09124.pdf)
>  We tackle a challenging task: multi-view and multi-modal event detection that detects events in a wide-range real environment by utilizing data from distributed cameras and microphones and their weak labels. In this task, distributed sensors are utilized complementarily to capture events that are difficult to capture with a single sensor, such as a series of actions of people moving in an intricate room, or communication between people located far apart in a room. For sensors to cooperate effectively in such a situation, the system should be able to exchange information among sensors and combines information that is useful for identifying events in a complementary manner. For such a mechanism, we propose a Transformer-based multi-sensor fusion (MultiTrans) which combines multi-sensor data on the basis of the relationships between features of different viewpoints and modalities. In the experiments using a dataset newly collected for this task, our proposed method using MultiTrans improved the event detection performance and outperformed comparatives.      
### 15.Echo-aware Adaptation of Sound Event Localization and Detection in Unknown Environments  [ :arrow_down: ](https://arxiv.org/pdf/2202.09121.pdf)
>  Our goal is to develop a sound event localization and detection (SELD) system that works robustly in unknown environments. A SELD system trained on known environment data is degraded in an unknown environment due to environmental effects such as reverberation and noise not contained in the training data. Previous studies on related tasks have shown that domain adaptation methods are effective when data on the environment in which the system will be used is available even without labels. However adaptation to unknown environments remains a difficult task. In this study, we propose echo-aware feature refinement (EAR) for SELD, which suppresses environmental effects at the feature level by using additional spatial cues of the unknown environment obtained through measuring acoustic echoes. FOA-MEIR, an impulse response dataset containing over 100 environments, was recorded to validate the proposed method. Experiments on FOA-MEIR show that the EAR effectively improves SELD performance in unknown environments.      
### 16.Truck Platoon Formation at Hubs: An Optimal Release Time Rule  [ :arrow_down: ](https://arxiv.org/pdf/2202.09119.pdf)
>  We consider a hub-based platoon coordination problem in which vehicles arrive at a hub according to an independent and identically distributed stochastic arrival process. The vehicles wait at the hub, and a platoon coordinator, at each time-step, decides whether to release the vehicles from the hub in the form of a platoon or wait for more vehicles to arrive. The platoon release time problem is modeled as a stopping rule problem wherein the objective is to maximize the average platooning benefit of the vehicles located at the hub and there is a cost of having vehicles waiting at the hub. We show that the stopping rule problem is monotone and the optimal platoon release time policy will therefore be in the form of a one time-step look-ahead rule. The performance of the optimal release rule is numerically compared with (i) a periodic release time rule and (ii) a non-causal release time rule where the coordinator knows all the future realizations of the arrival process. Our numerical results show that the optimal release time rule achieves a close performance to that of the non-causal rule and outperforms the periodic rule, especially when the arrival rate is low.      
### 17.Event-Triggered Distributed Model Predictive Control for Platoon Coordination at Hubs in a Transport System  [ :arrow_down: ](https://arxiv.org/pdf/2202.09105.pdf)
>  This paper considers the problem of hub-based platoon coordination for a large-scale transport system, where trucks have individual utility functions to optimize. An event-triggered distributed model predictive control method is proposed to solve the optimal scheduling of waiting times at hubs for individual trucks. In this distributed framework, trucks are allowed to decide their waiting times independently and only limited information is shared between trucks. Both the predicted reward gained from platooning and the predicted cost for waiting at hubs are included in each truck's utility function. The performance of the coordination method is demonstrated in a simulation with one hundred trucks over the Swedish road network.      
### 18.Toward a Smart Resource Allocation Policy via Artificial Intelligence in 6G Networks: Centralized or Decentralized?  [ :arrow_down: ](https://arxiv.org/pdf/2202.09093.pdf)
>  In this paper, we design a new smart softwaredefined radio access network (RAN) architecture with important properties like flexibility and traffic awareness for sixth generation (6G) wireless networks. In particular, we consider a hierarchical resource allocation framework for the proposed smart soft-RAN model, where the software-defined network (SDN) controller is the first and foremost layer of the framework. This unit dynamically monitors the network to select a network operation type on the basis of distributed or centralized resource allocation architectures to perform decision-making intelligently. In this paper, our aim is to make the network more scalable and more flexible in terms of achievable data rate, overhead, and complexity indicators. To this end, we introduce a new metric, throughput overhead complexity (TOC), for the proposed machine learning-based algorithm, which makes a trade-off between these performance indicators. In particular, the decision making based on TOC is solved via deep reinforcement learning (DRL), which determines an appropriate resource allocation policy. Furthermore, for the selected algorithm, we employ the soft actor-critic method, which is more accurate, scalable, and robust than other learning methods. Simulation results demonstrate that the proposed smart network achieves better performance in terms of TOC compared to fixed centralized or distributed resource management schemes that lack dynamism. Moreover, our proposed algorithm outperforms conventional learning methods employed in other state-of-the-art network designs.      
### 19.Speaker Identity Preservation in Dysarthric Speech Reconstruction by Adversarial Speaker Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2202.09082.pdf)
>  Dysarthric speech reconstruction (DSR), which aims to improve the quality of dysarthric speech, remains a challenge, not only because we need to restore the speech to be normal, but also must preserve the speaker's identity. The speaker representation extracted by the speaker encoder (SE) optimized for speaker verification has been explored to control the speaker identity. However, the SE may not be able to fully capture the characteristics of dysarthric speakers that are previously unseen. To address this research problem, we propose a novel multi-task learning strategy, i.e., adversarial speaker adaptation (ASA). The primary task of ASA fine-tunes the SE with the speech of the target dysarthric speaker to effectively capture identity-related information, and the secondary task applies adversarial training to avoid the incorporation of abnormal speaking patterns into the reconstructed speech, by regularizing the distribution of reconstructed speech to be close to that of reference speech with high quality. Experiments show that the proposed approach can achieve enhanced speaker similarity and comparable speech naturalness with a strong baseline approach. Compared with dysarthric speech, the reconstructed speech achieves 22.3% and 31.5% absolute word error rate reduction for speakers with moderate and moderate-severe dysarthria respectively. Our demo page is released here: <a class="link-external link-https" href="https://wendison.github.io/ASA-DSR-demo/" rel="external noopener nofollow">this https URL</a>      
### 20.VCVTS: Multi-speaker Video-to-Speech synthesis via cross-modal knowledge transfer from voice conversion  [ :arrow_down: ](https://arxiv.org/pdf/2202.09081.pdf)
>  Though significant progress has been made for speaker-dependent Video-to-Speech (VTS) synthesis, little attention is devoted to multi-speaker VTS that can map silent video to speech, while allowing flexible control of speaker identity, all in a single system. This paper proposes a novel multi-speaker VTS system based on cross-modal knowledge transfer from voice conversion (VC), where vector quantization with contrastive predictive coding (VQCPC) is used for the content encoder of VC to derive discrete phoneme-like acoustic units, which are transferred to a Lip-to-Index (Lip2Ind) network to infer the index sequence of acoustic units. The Lip2Ind network can then substitute the content encoder of VC to form a multi-speaker VTS system to convert silent video to acoustic units for reconstructing accurate spoken content. The VTS system also inherits the advantages of VC by using a speaker encoder to produce speaker representations to effectively control the speaker identity of generated speech. Extensive evaluations verify the effectiveness of proposed approach, which can be applied in both constrained vocabulary and open vocabulary conditions, achieving state-of-the-art performance in generating high-quality speech with high naturalness, intelligibility and speaker similarity. Our demo page is released here: <a class="link-external link-https" href="https://wendison.github.io/VCVTS-demo/" rel="external noopener nofollow">this https URL</a>      
### 21.Controllability of Networked Sampled-data Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.09060.pdf)
>  The controllability of networked sampled-data systems with zero-order samplers on the control and transmission channels is explored, where single- and multi-rate sampling patterns are considered, respectively. The effects of sampling on controllability of networked systems are analyzed, with some sufficient and/or necessary controllability conditions derived. Different from the sampling control of single systems, the pathological sampling of node systems could be eliminated by the network structure and inner couplings under some conditions. While for singular topology matrix, the pathological sampling of single nodes will cause the entire system to lose controllability. Moreover, controllability of networked systems with specific node dynamics will not be affected by any periodic sampling. All the results indicate that whether a networked system is under pathological sampling or not is jointly determined by mutually coupled factors.      
### 22.Towards better understanding and better generalization of few-shot classification in histology images with contrastive learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.09059.pdf)
>  Few-shot learning is an established topic in natural images for years, but few work is attended to histology images, which is of high clinical value since well-labeled datasets and rare abnormal samples are expensive to collect. Here, we facilitate the study of few-shot learning in histology images by setting up three cross-domain tasks that simulate real clinics problems. To enable label-efficient learning and better generalizability, we propose to incorporate contrastive learning (CL) with latent augmentation (LA) to build a few-shot system. CL learns useful representations without manual labels, while LA transfers semantic variations of the base dataset in an unsupervised way. These two components fully exploit unlabeled training data and can scale gracefully to other label-hungry problems. In experiments, we find i) models learned by CL generalize better than supervised learning for histology images in unseen classes, and ii) LA brings consistent gains over baselines. Prior studies of self-supervised learning mainly focus on ImageNet-like images, which only present a dominant object in their centers. Recent attention has been paid to images with multi-objects and multi-textures. Histology images are a natural choice for such a study. We show the superiority of CL over supervised learning in terms of generalization for such data and provide our empirical understanding for this observation. The findings in this work could contribute to understanding how the model generalizes in the context of both representation learning and histological image analysis. Code is available.      
### 23.An Analog Signal Processing EIC-PIC Solution for Coherent Data Center Interconnects  [ :arrow_down: ](https://arxiv.org/pdf/2202.09040.pdf)
>  Data center interconnects (DCIs) will have to support throughputs of 400 Gbps or more per wavelength in the near future. To achieve such high data rates, coherent modulation and detection is used, which conventionally requires high-speed data conversion and signal processing in the digital domain. Alternatively, high-speed signal conditioning and processing could be carried out in co-designed photonic and electronic integrated circuits, in the optical and electrical analog domains, respectively, to achieve reduced power consumption, latency, form factor, and cost. A few demonstrations of analog domain processing electronic integrated circuits (EICs), including those of equalizer and carrier phase recovery (CPR) modules showcase progress in this direction in the literature. In this brief, for the first time, we present integration of a silicon photonic integrated coherent receiver (ICR) module with a CPR module, as a part of a complete coherent receiver solution. A phase shifter in the ICR (fabricated in a 220 nm silicon-on-insulator technology) receives feedback from a CPR EIC, and the combination compensates for the time varying phase offset between the modulated signal and the unmodulated carrier in the closed loop configuration. In this proof-of-concept demonstration, we present experimental results obtained from the stand-alone silicon photonic ICR along with its system level integration with CPR chip, for QPSK signals. The technique can be extended to a higher-order modulation format, such as 16-QAM, for data rate scaling. The proposed scheme is suitable for homodyne systems, such as polarization multiplexed carrier based self-homodyne links.      
### 24.REFUGE2 Challenge: Treasure for Multi-Domain Learning in Glaucoma Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2202.08994.pdf)
>  Glaucoma is the second leading cause of blindness and is the leading cause of irreversible blindness disease in the world. Early screening for glaucoma in the population is significant. Color fundus photography is the most cost effective imaging modality to screen for ocular diseases. Deep learning network is often used in color fundus image analysis due to its powful feature extraction capability. However, the model training of deep learning method needs a large amount of data, and the distribution of data should be abundant for the robustness of model performance. To promote the research of deep learning in color fundus photography and help researchers further explore the clinical application signification of AI technology, we held a REFUGE2 challenge. This challenge released 2,000 color fundus images of four models, including Zeiss, Canon, Kowa and Topcon, which can validate the stabilization and generalization of algorithms on multi-domain. Moreover, three sub-tasks were designed in the challenge, including glaucoma classification, cup/optic disc segmentation, and macular fovea localization. These sub-tasks technically cover the three main problems of computer vision and clinicly cover the main researchs of glaucoma diagnosis. Over 1,300 international competitors joined the REFUGE2 challenge, 134 teams submitted more than 3,000 valid preliminary results, and 22 teams reached the final. This article summarizes the methods of some of the finalists and analyzes their results. In particular, we observed that the teams using domain adaptation strategies had high and robust performance on the dataset with multi-domain. This indicates that UDA and other multi-domain related researches will be the trend of deep learning field in the future, and our REFUGE2 datasets will play an important role in these researches.      
### 25.Machine Learning for Touch Localization on Ultrasonic Wave Touchscreen  [ :arrow_down: ](https://arxiv.org/pdf/2202.08947.pdf)
>  Classification and regression employing a simple Deep Neural Network (DNN) are investigated to perform touch localization on a tactile surface using ultrasonic guided waves. A model is trained using data captured using a robotic finger, and validated with experiments conducted with human fingers. The proposed method provides satisfactory localization results for most human-machine interactions, with a mean error of 0.47 cm and standard deviation of 0.18 cm. The classification approach is also adapted to identify touches on an access control keypad layout, which leads to an accuracy of 97%. These results demonstrate that DNN-based methods are a viable alternative to signal processing-based approaches for accurate and robust touch localization using ultrasonic guided waves.      
### 26.Prior image-based medical image reconstruction using a style-based generative adversarial network  [ :arrow_down: ](https://arxiv.org/pdf/2202.08936.pdf)
>  Computed medical imaging systems require a computational reconstruction procedure for image formation. In order to recover a useful estimate of the object to-be-imaged when the recorded measurements are incomplete, prior knowledge about the nature of object must be utilized. In order to improve the conditioning of an ill-posed imaging inverse problem, deep learning approaches are being actively investigated for better representing object priors and constraints. This work proposes to use a style-based generative adversarial network (StyleGAN) to constrain an image reconstruction problem in the case where additional information in the form of a prior image of the sought-after object is available. An optimization problem is formulated in the intermediate latent-space of a StyleGAN, that is disentangled with respect to meaningful image attributes or "styles", such as the contrast used in magnetic resonance imaging (MRI). Discrepancy between the sought-after and prior images is measured in the disentangled latent-space, and is used to regularize the inverse problem in the form of constraints on specific styles of the disentangled latent-space. A stylized numerical study inspired by MR imaging is designed, where the sought-after and the prior image are structurally similar, but belong to different contrast mechanisms. The presented numerical studies demonstrate the superiority of the proposed approach as compared to classical approaches in the form of traditional metrics.      
### 27.Quantisation-aware Precoding for MU-MIMO with Limited-capacity Fronthaul  [ :arrow_down: ](https://arxiv.org/pdf/2202.08925.pdf)
>  Base stations in 5G and beyond use advanced antenna systems (AASs), where multiple passive antenna elements and radio units are integrated into a single box. A critical bottleneck of such a system is the digital fronthaul between the AAS and baseband unit (BBU), which has limited capacity. In this paper, we study an AAS used for precoded downlink transmission over a multi-user multiple-input multiple-output (MU-MIMO) channel. First, we present the baseline quantization-unaware precoding scheme created when a precoder is computed at the BBU and then quantized to be sent over the fronthaul. We propose a new precoding design that is aware of the fronthaul quantization. We formulate an optimization problem to minimize the mean squared error at the receiver side. We rewrite the problem to utilize mixed-integer programming to solve it. The numerical results manifest that our proposed precoding greatly outperforms quantization-unaware precoding in terms of sum rate.      
### 28.Graph Convolutional Networks for Multi-modality Medical Imaging: Methods, Architectures, and Clinical Applications  [ :arrow_down: ](https://arxiv.org/pdf/2202.08916.pdf)
>  Image-based characterization and disease understanding involve integrative analysis of morphological, spatial, and topological information across biological scales. The development of graph convolutional networks (GCNs) has created the opportunity to address this information complexity via graph-driven architectures, since GCNs can perform feature aggregation, interaction, and reasoning with remarkable flexibility and efficiency. These GCNs capabilities have spawned a new wave of research in medical imaging analysis with the overarching goal of improving quantitative disease understanding, monitoring, and diagnosis. Yet daunting challenges remain for designing the important image-to-graph transformation for multi-modality medical imaging and gaining insights into model interpretation and enhanced clinical decision support. In this review, we present recent GCNs developments in the context of medical image analysis including imaging data from radiology and histopathology. We discuss the fast-growing use of graph network architectures in medical image analysis to improve disease diagnosis and patient outcomes in clinical practice. To foster cross-disciplinary research, we present GCNs technical advancements, emerging medical applications, identify common challenges in the use of image-based GCNs and their extensions in model interpretation, large-scale benchmarks that promise to transform the scope of medical image studies and related graph-driven medical research.      
### 29.Curriculum optimization for low-resource speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.08883.pdf)
>  Modern end-to-end speech recognition models show astonishing results in transcribing audio signals into written text. However, conventional data feeding pipelines may be sub-optimal for low-resource speech recognition, which still remains a challenging task. We propose an automated curriculum learning approach to optimize the sequence of training examples based on both the progress of the model while training and prior knowledge about the difficulty of the training examples. We introduce a new difficulty measure called compression ratio that can be used as a scoring function for raw audio in various noise conditions. The proposed method improves speech recognition Word Error Rate performance by up to 33% relative over the baseline system      
### 30.Ray-transfer functions for camera simulation of 3D scenes with hidden lens design  [ :arrow_down: ](https://arxiv.org/pdf/2202.08880.pdf)
>  Combining image sensor simulation tools (e.g., ISETCam) with physically based ray tracing (e.g., PBRT) offers possibilities for designing and evaluating novel imaging systems as well as for synthesizing physically accurate, labeled images for machine learning. One practical limitation has been simulating the optics precisely: Lens manufacturers generally prefer to keep lens design confidential. We present a pragmatic solution to this problem using a black box lens model in Zemax; such models provide necessary optical information while preserving the lens designer's intellectual property. First, we describe and provide software to construct a polynomial ray transfer function that characterizes how rays entering the lens at any position and angle subsequently exit the lens. We implement the ray-transfer calculation as a camera model in PBRT and confirm that the PBRT ray-transfer calculations match the Zemax lens calculations for edge spread functions and relative illumination.      
### 31.Assessment of Cyber-Physical Intrusion Detection and Classification for Industrial Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.09352.pdf)
>  The increasing interaction of industrial control systems (ICSs) with public networks and digital devices introduces new cyber threats to power systems and other critical infrastructure. Recent cyber-physical attacks such as Stuxnet and Irongate revealed unexpected ICS vulnerabilities and a need for improved security measures. Intrusion detection systems constitute a key security technology, which typically monitor network data for detecting malicious activities. However, a central characteristic of modern ICSs is the increasing interdependency of physical and cyber network processes. Thus, the integration of network and physical process data is seen as a promising approach to improve predictability in intrusion detection for ICSs by accounting for physical constraints and underlying process patterns. This work systematically assesses real-time cyber-physical intrusion detection and multiclass classification, based on a comparison to its purely network data-based counterpart and evaluation of misclassifications and detection delay. Multiple supervised machine learning models are applied on a recent cyber-physical dataset, describing various cyber attacks and physical faults on a generic ICS. A key finding is that integration of physical process data improves detection and classification of all attack types. In addition, it enables simultaneous processing of attacks and faults, paving the way for holistic cross-domain cause analysis.      
### 32.Signal Decomposition Using Masked Proximal Operators  [ :arrow_down: ](https://arxiv.org/pdf/2202.09338.pdf)
>  We consider the well-studied problem of decomposing a vector time series signal into components with different characteristics, such as smooth, periodic, nonnegative, or sparse. We propose a simple and general framework in which the components are defined by loss functions (which include constraints), and the signal decomposition is carried out by minimizing the sum of losses of the components (subject to the constraints). When each loss function is the negative log-likelihood of a density for the signal component, our method coincides with maximum a posteriori probability (MAP) estimation; but it also includes many other interesting cases. We give two distributed optimization methods for computing the decomposition, which find the optimal decomposition when the component class loss functions are convex, and are good heuristics when they are not. Both methods require only the masked proximal operator of each of the component loss functions, a generalization of the well-known proximal operator that handles missing entries in its argument. Both methods are distributed, i.e., handle each component separately. We derive tractable methods for evaluating the masked proximal operators of some loss functions that, to our knowledge, have not appeared in the literature.      
### 33.Informativity conditions for data-driven control based on input-state data and polyhedral cross-covariance noise bounds  [ :arrow_down: ](https://arxiv.org/pdf/2202.09266.pdf)
>  Modeling and control of dynamical systems rely on measured data, which contains information about the system. Finite data measurements typically lead to a set of system models that are unfalsified, i.e., that explain the data. The problem of data-informativity for stabilization or control with quadratic performance is concerned with the existence of a controller that stabilizes all unfalsified systems or achieves a desired quadratic performance. Recent results in the literature provide informativity conditions for control based on input-state data and ellipsoidal noise bounds, such as energy or magnitude bounds. In this paper, we consider informativity of input-state data for control where noise bounds are defined through the cross-covariance of the noise with respect to an instrumental variable; bounds that were introduced originally as a noise characterization in parameter bounding identification. The considered cross-covariance bounds are defined by a finite number of hyperplanes, which induce a (possibly unbounded) polyhedral set of unfalsified systems. We provide informativity conditions for input-state data with polyhedral cross-covariance bounds for stabilization and $\mathcal{H}_2$/$\mathcal{H}_\infty$ control through vertex/half-space representations of the polyhedral set of unfalsified systems.      
### 34.Nonstationary multi-output Gaussian processes via harmonizable spectral mixtures  [ :arrow_down: ](https://arxiv.org/pdf/2202.09233.pdf)
>  Kernel design for Multi-output Gaussian Processes (MOGP) has received increased attention recently. In particular, the Multi-Output Spectral Mixture kernel (MOSM) <a class="link-https" data-arxiv-id="1709.01298" href="https://arxiv.org/abs/1709.01298">arXiv:1709.01298</a> approach has been praised as a general model in the sense that it extends other approaches such as Linear Model of Corregionalization, Intrinsic Corregionalization Model and Cross-Spectral Mixture. MOSM relies on CramÃ©r's theorem to parametrise the power spectral densities (PSD) as a Gaussian mixture, thus, having a structural restriction: by assuming the existence of a PSD, the method is only suited for multi-output stationary applications. We develop a nonstationary extension of MOSM by proposing the family of harmonizable kernels for MOGPs, a class of kernels that contains both stationary and a vast majority of non-stationary processes. A main contribution of the proposed harmonizable kernels is that they automatically identify a possible nonstationary behaviour meaning that practitioners do not need to choose between stationary or non-stationary kernels. The proposed method is first validated on synthetic data with the purpose of illustrating the key properties of our approach, and then compared to existing MOGP methods on two real-world settings from finance and electroencephalography.      
### 35.Deep-Learning Architectures for Multi-Pitch Estimation: Towards Reliable Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2202.09198.pdf)
>  Extracting pitch information from music recordings is a challenging but important problem in music signal processing. Frame-wise transcription or multi-pitch estimation aims for detecting the simultaneous activity of pitches in polyphonic music recordings and has recently seen major improvements thanks to deep-learning techniques, with a variety of proposed network architectures. In this paper, we realize different architectures based on CNNs, the U-net structure, and self-attention components. We propose several modifications to these architectures including self-attention modules for skip connections, recurrent layers to replace the self-attention, and a multi-task strategy with simultaneous prediction of the degree of polyphony. We compare variants of these architectures in different sizes for multi-pitch estimation, focusing on Western classical music beyond the piano-solo scenario using the MusicNet and Schubert Winterreise datasets. Our experiments indicate that most architectures yield competitive results and that larger model variants seem to be beneficial. However, we find that these results substantially depend on randomization effects and the particular choice of the training-test split, which questions the claim of superiority for particular architectures given only small improvements. We therefore investigate the influence of dataset splits in the presence of several movements of a work cycle (cross-version evaluation) and propose a best-practice splitting strategy for MusicNet, which weakens the influence of individual test tracks and suppresses overfitting to specific works and recording conditions. A final evaluation on a mixed dataset suggests that improvements on one specific dataset do not necessarily generalize to other scenarios, thus emphasizing the need for further high-quality multi-pitch datasets in order to reliably measure progress in music transcription tasks.      
### 36.Energy Efficient Dual-Functional Radar-Communication: Rate-Splitting Multiple Access, Low-Resolution DACs, and RF Chain Selection  [ :arrow_down: ](https://arxiv.org/pdf/2202.09128.pdf)
>  Dual-Functional Radar-Communication systems enhance the benefits of communications and radar sensing by jointly implementing these on the same hardware platform and using the common RF resources. An important and latest concern to be addressed in designing such Dual-Functional Radar-Communication systems is maximizing the energy-efficiency. In this paper, we consider a Dual-Functional Radar-Communication system performing simultaneous multi-user communications and radar sensing, and investigate the energy-efficiency behaviour with respect to active transmission elements. Specifically, we formulate a problem to find the optimal precoders and the number of active RF chains for maximum energy-efficiency by taking into consideration the power consumption of low-resolution Digital-to-Analog Converters on each RF chain under communications and radar performance constraints. We consider Rate-Splitting Multiple Access to perform multi-user communications with perfect and imperfect Channel State Information at Transmitter. The formulated non-convex optimization problem is solved by means of a novel algorithm. We demonstrate by numerical results that Rate Splitting Multiple Access achieves an improved energy-efficiency by employing a smaller number of RF chains compared to Space Division Multiple Access, owing to its generalized structure and improved interference management capabilities.      
### 37.Large-Scale Acoustic Characterization of Singaporean Children's English Pronunciation  [ :arrow_down: ](https://arxiv.org/pdf/2202.09108.pdf)
>  In this work, we investigate pronunciation differences in English spoken by Singaporean children in relation to their American and British counterparts by conducting Kmeans clustering and Archetypal analysis on selected vowel pairs and approximants. Given that Singapore adopts British English as the institutional standard due to historical reasons, one might expect Singaporean children to follow British pronunciation patterns. Indeed, Singaporean and British children are more similar in their production of syllable-final /r/ -- they do not lower their third formant nearly as much as American children do, suggesting a lack of rhoticity. Interestingly, Singaporean children also present similar patterns to American children when it comes to their fronting of vowels as demonstrated across various vowels including TRAP-BATH split vowels. Singaporean children's English also demonstrated characteristics that do not resemble any of the other two populations. We observe that Singaporean children's vowel height characteristics are distinct from both that of American and British children. In tense and lax vowel pairs, we also consistently observe that the distinction is less conspicuous for Singaporean children compared to the other speaker groups. Further, while American and British children demonstrate lowering of F1 and F2 formants in transitions into syllable-final /l/s, a wide gap between F2 and F3 formants, and small difference between F1 and F2 formants, all of these are not exhibited in Singaporean children's pronunciation. These findings point towards potential sociolinguistic implications of how Singapore English might be evolving to embody more than British pronunciation characteristics. Furthermore, these findings also suggest that Singapore English could be have been influenced by languages beyond American and British English, potentially due to Singapore's multilingual environment.      
### 38.Predicting Sex and Stroke Success -- Computer-aided Player Grunt Analysis in Tennis Matches  [ :arrow_down: ](https://arxiv.org/pdf/2202.09102.pdf)
>  Professional athletes increasingly use automated analysis of meta- and signal data to improve their training and game performance. As in other related human-to-human research fields, signal data, in particular, contain important performance- and mood-specific indicators for automated analysis. In this paper, we introduce the novel data set SCORE! to investigate the performance of several features and machine learning paradigms in the prediction of the sex and immediate stroke success in tennis matches, based only on vocal expression through players' grunts. The data was gathered from YouTube, labelled under the exact same definition, and the audio processed for modelling. We extract several widely used basic, expert-knowledge, and deep acoustic features of the audio samples and evaluate their effectiveness in combination with various machine learning approaches. In a binary setting, the best system, using spectrograms and a Convolutional Recurrent Neural Network, achieves an unweighted average recall (UAR) of 84.0 % for the player sex prediction task, and 60.3 % predicting stroke success, based only on acoustic cues in players' grunts of both sexes. Further, we achieve a UAR of 58.3 %, and 61.3 %, when the models are exclusively trained on female or male grunts, respectively.      
### 39.A Comprehensive Survey with Quantitative Comparison of Image Analysis Methods for Microorganism Biovolume Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2202.09020.pdf)
>  With the acceleration of urbanization and living standards, microorganisms play increasingly important roles in industrial production, bio-technique, and food safety testing. Microorganism biovolume measurements are one of the essential parts of microbial analysis. However, traditional manual measurement methods are time-consuming and challenging to measure the characteristics precisely. With the development of digital image processing techniques, the characteristics of the microbial population can be detected and quantified. The changing trend can be adjusted in time and provided a basis for the improvement. The applications of the microorganism biovolume measurement method have developed since the 1980s. More than 60 articles are reviewed in this study, and the articles are grouped by digital image segmentation methods with periods. This study has high research significance and application value, which can be referred to microbial researchers to have a comprehensive understanding of microorganism biovolume measurements using digital image analysis methods and potential applications.      
### 40.End-to-end contextual asr based on posterior distribution adaptation for hybrid ctc/attention system  [ :arrow_down: ](https://arxiv.org/pdf/2202.09003.pdf)
>  End-to-end (E2E) speech recognition architectures assemble all components of traditional speech recognition system into a single model. Although it simplifies ASR system, it introduces contextual ASR drawback: the E2E model has worse performance on utterances containing infrequent proper nouns. In this work, we propose to add a contextual bias attention (CBA) module to attention based encoder decoder (AED) model to improve its ability of recognizing the contextual phrases. Specifically, CBA utilizes the context vector of source attention in decoder to attend to a specific bias embedding. Jointly learned with the basic AED parameters, CBA can tell the model when and where to bias its output probability distribution. At inference stage, a list of bias phrases is preloaded and we adapt the posterior distributions of both CTC and attention decoder according to the attended bias phrase of CBA. We evaluate the proposed method on GigaSpeech and achieve a consistent relative improvement on recall rate of bias phrases ranging from 15% to 28% compared to the baseline model. Meanwhile, our method shows a strong anti-bias ability as the performance on general tests only degrades 1.7% even 2,000 bias phrases are present.      
### 41.A Summary of the ComParE COVID-19 Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2202.08981.pdf)
>  The COVID-19 pandemic has caused massive humanitarian and economic damage. Teams of scientists from a broad range of disciplines have searched for methods to help governments and communities combat the disease. One avenue from the machine learning field which has been explored is the prospect of a digital mass test which can detect COVID-19 from infected individuals' respiratory sounds. We present a summary of the results from the INTERSPEECH 2021 Computational Paralinguistics Challenges: COVID-19 Cough, (CCS) and COVID-19 Speech, (CSS).      
### 42.Multimodal Emotion Recognition using Transfer Learning from Speaker Recognition and BERT-based models  [ :arrow_down: ](https://arxiv.org/pdf/2202.08974.pdf)
>  Automatic emotion recognition plays a key role in computer-human interaction as it has the potential to enrich the next-generation artificial intelligence with emotional intelligence. It finds applications in customer and/or representative behavior analysis in call centers, gaming, personal assistants, and social robots, to mention a few. Therefore, there has been an increasing demand to develop robust automatic methods to analyze and recognize the various emotions. In this paper, we propose a neural network-based emotion recognition framework that uses a late fusion of transfer-learned and fine-tuned models from speech and text modalities. More specifically, we i) adapt a residual network (ResNet) based model trained on a large-scale speaker recognition task using transfer learning along with a spectrogram augmentation approach to recognize emotions from speech, and ii) use a fine-tuned bidirectional encoder representations from transformers (BERT) based model to represent and recognize emotions from the text. The proposed system then combines the ResNet and BERT-based model scores using a late fusion strategy to further improve the emotion recognition performance. The proposed multimodal solution addresses the data scarcity limitation in emotion recognition using transfer learning, data augmentation, and fine-tuning, thereby improving the generalization performance of the emotion recognition models. We evaluate the effectiveness of our proposed multimodal approach on the interactive emotional dyadic motion capture (IEMOCAP) dataset. Experimental results indicate that both audio and text-based models improve the emotion recognition performance and that the proposed multimodal solution achieves state-of-the-art results on the IEMOCAP benchmark.      
### 43.A Formal Safety Characterization of Advanced Driver Assist Systems in the Car-Following Regime with Scenario-Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2202.08935.pdf)
>  The capability to follow a lead-vehicle and avoid rear-end collisions is one of the most important functionalities for human drivers and various Advanced Driver Assist Systems (ADAS). Existing safety performance justification of the car-following systems either relies on simple concrete scenarios with biased surrogate metrics or requires a significantly long driving distance for risk observation and inference. In this paper, we propose a guaranteed unbiased and sampling efficient scenario-based safety evaluation framework inspired by the previous work on $\epsilon\delta$-almost safe set quantification. The proposal characterizes the complete safety performance of the test subject in the car-following regime. The performance of the proposed method is also demonstrated in challenging cases including some widely adopted car-following decision-making modules and the commercially available Openpilot driving stack by CommaAI.      
### 44.A Distributed Algorithm for Measure-valued Optimization with Additive Objective  [ :arrow_down: ](https://arxiv.org/pdf/2202.08930.pdf)
>  We propose a distributed nonparametric algorithm for solving measure-valued optimization problems with additive objectives. Such problems arise in several contexts in stochastic learning and control including Langevin sampling from an unnormalized prior, mean field neural network learning and Wasserstein gradient flows. The proposed algorithm comprises a two-layer alternating direction method of multipliers (ADMM). The outer-layer ADMM generalizes the Euclidean consensus ADMM to the Wasserstein consensus ADMM, and to its entropy-regularized version Sinkhorn consensus ADMM. The inner-layer ADMM turns out to be a specific instance of the standard Euclidean ADMM. The overall algorithm realizes operator splitting for gradient flows in the manifold of probability measures.      
### 45.Machine learning models and facial regions videos for estimating heart rate: a review on Patents, Datasets and Literature  [ :arrow_down: ](https://arxiv.org/pdf/2202.08913.pdf)
>  Estimating heart rate is important for monitoring users in various situations. Estimates based on facial videos are increasingly being researched because it makes it possible to monitor cardiac information in a non-invasive way and because the devices are simpler, requiring only cameras that capture the user's face. From these videos of the user's face, machine learning is able to estimate heart rate. This study investigates the benefits and challenges of using machine learning models to estimate heart rate from facial videos, through patents, datasets, and articles review. We searched Derwent Innovation, IEEE Xplore, Scopus, and Web of Science knowledge bases and identified 7 patent filings, 11 datasets, and 20 articles on heart rate, photoplethysmography, or electrocardiogram data. In terms of patents, we note the advantages of inventions related to heart rate estimation, as described by the authors. In terms of datasets, we discovered that most of them are for academic purposes and with different signs and annotations that allow coverage for subjects other than heartbeat estimation. In terms of articles, we discovered techniques, such as extracting regions of interest for heart rate reading and using Video Magnification for small motion extraction, and models such as EVM-CNN and VGG-16, that extract the observed individual's heart rate, the best regions of interest for signal extraction and ways to process them.      
### 46.Attributable Watermarking of Speech Generative Models  [ :arrow_down: ](https://arxiv.org/pdf/2202.08900.pdf)
>  Generative models are now capable of synthesizing images, speeches, and videos that are hardly distinguishable from authentic contents. Such capabilities cause concerns such as malicious impersonation and IP theft. This paper investigates a solution for model attribution, i.e., the classification of synthetic contents by their source models via watermarks embedded in the contents. Building on past success of model attribution in the image domain, we discuss algorithmic improvements for generating user-end speech models that empirically achieve high attribution accuracy, while maintaining high generation quality. We show the trade off between attributability and generation quality under a variety of attacks on generated speech signals attempting to remove the watermarks, and the feasibility of learning robust watermarks against these attacks.      
### 47.Word Embeddings for Automatic Equalization in Audio Mixing  [ :arrow_down: ](https://arxiv.org/pdf/2202.08898.pdf)
>  In recent years, machine learning has been widely adopted to automate the audio mixing process. Automatic mixing systems have been applied to various audio effects such as gain-adjustment, stereo panning, equalization, and reverberation. These systems can be controlled through visual interfaces, providing audio examples, using knobs, and semantic descriptors. Using semantic descriptors or textual information to control these systems is an effective way for artists to communicate their creative goals. Furthermore, sometimes artists use non-technical words that may not be understood by the mixing system, or even a mixing engineer. In this paper, we explore the novel idea of using word embeddings to represent semantic descriptors. Word embeddings are generally obtained by training neural networks on large corpora of written text. These embeddings serve as the input layer of the neural network to create a translation from words to EQ settings. Using this technique, the machine learning model can also generate EQ settings for semantic descriptors that it has not seen before. We perform experiments to demonstrate the feasibility of this idea. In addition, we compare the EQ settings of humans with the predictions of the neural network to evaluate the quality of predictions. The results showed that the embedding layer enables the neural network to understand semantic descriptors. We observed that the models with embedding layers perform better those without embedding layers, but not as good as human labels.      
### 48.RemixIT: Continual self-training of speech enhancement models via bootstrapped remixing  [ :arrow_down: ](https://arxiv.org/pdf/2202.08862.pdf)
>  We present RemixIT, a simple yet effective self-supervised method for training speech enhancement without the need of a single isolated in-domain speech nor a noise waveform. Our approach overcomes limitations of previous methods which make them dependent on clean in-domain target signals and thus, sensitive to any domain mismatch between train and test samples. RemixIT is based on a continuous self-training scheme in which a pre-trained teacher model on out-of-domain data infers estimated pseudo-target signals for in-domain mixtures. Then, by permuting the estimated clean and noise signals and remixing them together, we generate a new set of bootstrapped mixtures and corresponding pseudo-targets which are used to train the student network. Vice-versa, the teacher periodically refines its estimates using the updated parameters of the latest student models. Experimental results on multiple speech enhancement datasets and tasks not only show the superiority of our method over prior approaches but also showcase that RemixIT can be combined with any separation model as well as be applied towards any semi-supervised and unsupervised domain adaptation task. Our analysis, paired with empirical evidence, sheds light on the inside functioning of our self-training scheme wherein the student model keeps obtaining better performance while observing severely degraded pseudo-targets.      
