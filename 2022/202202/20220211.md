# ArXiv eess --Fri, 11 Feb 2022
### 1.Conditional Diffusion Probabilistic Model for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.05256.pdf)
>  Speech enhancement is a critical component of many user-oriented audio applications, yet current systems still suffer from distorted and unnatural outputs. While generative models have shown strong potential in speech synthesis, they are still lagging behind in speech enhancement. This work leverages recent advances in diffusion probabilistic models, and proposes a novel speech enhancement algorithm that incorporates characteristics of the observed noisy speech signal into the diffusion and reverse processes. More specifically, we propose a generalized formulation of the diffusion probabilistic model named conditional diffusion probabilistic model that, in its reverse process, can adapt to non-Gaussian real noises in the estimated speech signal. In our experiments, we demonstrate strong performance of the proposed approach compared to representative generative models, and investigate the generalization capability of our models to other datasets with noise characteristics unseen during training.      
### 2.A New Fusion Strategy for Spoofing Aware Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2202.05253.pdf)
>  The performance of automatic speaker verification (ASV) systems could be degraded by voice spoofing attacks. Most existing works aimed to develop standalone spoofing countermeasure (CM) systems. Relatively little work aimed to develop an integrated spoofing aware speaker verification (SASV) system. With the recent SASV challenge aiming to encourage the development of such integration, official protocols and baselines have been released by the organizers. Building on these baselines, we propose a score scaling and multiplication strategy for inference and an SASV training strategy. Surprisingly, these strategies significantly improve the SASV equal error rate (EER) from 19.31\% of the best baseline to 1.58\% on the official evaluation trials of the SASV challenge. We verify the effectiveness of our proposed components through ablation studies and provide insights with score distribution analyses.      
### 3.SA-HMTS: A Secure and Adaptive Hierarchical Multi-timescale Framework for Resilient Load Restoration Using A Community Microgrid  [ :arrow_down: ](https://arxiv.org/pdf/2202.05252.pdf)
>  Distribution system integrated community microgrids (CMGs) can partake in restoring loads during extended duration outages. At such times, the CMG is challenged with limited resource availability, absence of robust grid support, and heightened demand-supply uncertainty. This paper proposes a secure and adaptive three-stage hierarchical multi-timescale framework for scheduling and real-time (RT) dispatch of CMGs with hybrid PV systems to address these challenges. The framework enables the CMG to dynamically expand its boundary to support the neighboring grid sections and is adaptive to the changing forecast error impacts. The first stage solves a stochastic extended duration scheduling (EDS) problem to obtain referral plans for optimal resource rationing. The intermediate near-real-time (NRT) scheduling stage updates the EDS schedule closer to the dispatch time using newly obtained forecasts, followed by the RT dispatch stage. To make the dispatch decisions more secure and robust against forecast errors, a novel concept called delayed recourse is proposed. The methodology is evaluated via numerical simulations on a modified IEEE 123-bus system and validated using OpenDSS/hardware-in-loop simulations. The results show superior performance in maximizing load supply and continuous secure CMG operation under numerous operating scenarios.      
### 4.A Novel Four-DOF Lagrangian Approach to Attitude Tracking for Rigid Spacecraft  [ :arrow_down: ](https://arxiv.org/pdf/2202.05227.pdf)
>  This paper presents a novel Lagrangian approach to attitude tracking for rigid spacecraft using unit quaternions, where the motion equations of a spacecraft are described by a four degrees of freedom Lagrangian dynamics subject to a holonomic constraint imposed by the norm of a unit quaternion. The basic energy-conservation property as well as some additional useful properties of the Lagrangian dynamics are explored, enabling to develop quaternion-based attitude tracking controllers by taking full advantage of a broad class of tracking control designs for mechanical systems based on energy-shaping methodology. Global tracking of a desired attitude on the unit sphere is achieved by designing control laws that render the tracking error on the four-dimensional Euclidean space to converge to the origin. The topological constraints for globally exponentially tracking by a quaternion-based continuous controller and singularities in controller designs based on any three-parameter representation of the attitude are then avoided. Using this approach, a full-state feedback controller is first developed, and then several important issues, such as robustness to noise in quaternion measurements, unknown on-orbit torque disturbances, uncertainty in the inertial matrix, and lack of angular-velocity measurements are addressed progressively, by designing a hybrid state-feedback controller, an adaptive hybrid state-feedback controller, and an adaptive hybrid attitude-feedback controller. Global asymptotic stability is established for each controller. Simulations are included to illustrate the theoretical results.      
### 5.Behavior-Semantic Scenery Description (BSSD) of Road Networks for Automated Driving  [ :arrow_down: ](https://arxiv.org/pdf/2202.05211.pdf)
>  The safety approval of Highly Automated Vehicles (HAV) is economically infeasible with current approaches. For verification and validation, it is essential to describe the intended behavior of an HAV in the development process in order to prove safety. The demand for this behavior comes from the traffic rules which are instantiated by the present scenery around the vehicle (e.g. traffic signs or road markings). The Operational Design Domain (ODD) specifies the scenery in which an HAV may operate, but current descriptions fail to explicitly represent the associated behavioral demand of the scenery. We propose a new approach for a Behavior-Semantic Scenery Description (BSSD) in order to describe the behavior space of a present scenery. A behavior space represents the delimitation of the legally possible behavior. The BSSD explicitly links the scenery with the behavioral demand for HAV. Based on identified goals and challenges for such an approach, we derive requirements for a generic structure of the description for complete road networks. All required elements to represent the behavior space of the scenery are identified. Within real world examples, we present an instance of the BSSD integrated into the HD-map framework Lanelet2 to prove the applicability of the description. The presented approach supports development, test and operation of HAV by closing the knowledge gap of where a vehicle has to behave in which limits within an ODD.      
### 6.Automated Atrial Fibrillation Classification Based on Denoising Stacked Autoencoder and Optimized Deep Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.05177.pdf)
>  The incidences of atrial fibrillation (AFib) are increasing at a daunting rate worldwide. For the early detection of the risk of AFib, we have developed an automatic detection system based on deep neural networks. For achieving better classification, it is mandatory to have good pre-processing of physiological signals. Keeping this in mind, we have proposed a two-fold study. First, an end-to-end model is proposed to denoise the electrocardiogram signals using denoising autoencoders (DAE). To achieve denoising, we have used three networks including, convolutional neural network (CNN), dense neural network (DNN), and recurrent neural networks (RNN). Compared the three models and CNN based DAE performance is found to be better than the other two. Therefore, the signals denoised by the CNN based DAE were used to train the deep neural networks for classification. Three neural networks' performance has been evaluated using accuracy, specificity, sensitivity, and signal to noise ratio (SNR) as the evaluation criteria. <br>The proposed end-to-end deep learning model for detecting atrial fibrillation in this study has achieved an accuracy rate of 99.20%, a specificity of 99.50%, a sensitivity of 99.50%, and a true positive rate of 99.00%. The average accuracy of the algorithms we compared is 96.26%, and our algorithm's accuracy is 3.2% higher than this average of the other algorithms. The CNN classification network performed better as compared to the other two. Additionally, the model is computationally efficient for real-time applications, and it takes approx 1.3 seconds to process 24 hours ECG signal. The proposed model was also tested on unseen dataset with different proportions of arrhythmias to examine the model's robustness, which resulted in 99.10% of recall and 98.50% of precision.      
### 7.Efficacy of Transformer Networks for Classification of Raw EEG Data  [ :arrow_down: ](https://arxiv.org/pdf/2202.05170.pdf)
>  With the unprecedented success of transformer networks in natural language processing (NLP), recently, they have been successfully adapted to areas like computer vision, generative adversarial networks (GAN), and reinforcement learning. Classifying electroencephalogram (EEG) data has been challenging and researchers have been overly dependent on pre-processing and hand-crafted feature extraction. Despite having achieved automated feature extraction in several other domains, deep learning has not yet been accomplished for EEG. In this paper, the efficacy of the transformer network for the classification of raw EEG data (cleaned and pre-processed) is explored. The performance of transformer networks was evaluated on a local (age and gender data) and a public dataset (STEW). First, a classifier using a transformer network is built to classify the age and gender of a person with raw resting-state EEG data. Second, the classifier is tuned for mental workload classification with open access raw multi-tasking mental workload EEG data (STEW). The network achieves an accuracy comparable to state-of-the-art accuracy on both the local (Age and Gender dataset; 94.53% (gender) and 87.79% (age)) and the public (STEW dataset; 95.28% (two workload levels) and 88.72% (three workload levels)) dataset. The accuracy values have been achieved using raw EEG data without feature extraction. Results indicate that the transformer-based deep learning models can successfully abate the need for heavy feature-extraction of EEG data for successful classification.      
### 8.Radar-based Materials Classification Using Deep Wavelet Scattering Transform: A Comparison of Centimeter vs. Millimeter Wave Units  [ :arrow_down: ](https://arxiv.org/pdf/2202.05169.pdf)
>  Radar-based materials detection received significant attention in recent years for its potential inclusion in consumer and industrial applications like object recognition for grasping and manufacturing quality assurance and control. Several radar publications were developed for material classification under controlled settings with specific materials' properties and shapes. Recent literature has challenged the earlier findings on radars-based materials classification claiming that earlier solutions are not easily scaled to industrial applications due to a variety of real-world issues. Published experiments on the impact of these factors on the robustness of the extracted radar-based traditional features have already demonstrated that the application of deep neural networks can mitigate, to some extent, the impact to produce a viable solution. However, previous studies lacked an investigation of the usefulness of lower frequency radar units, specifically &lt;10GHz, against the higher range units around and above 60GHz. This research considers two radar units with different frequency ranges: Walabot-3D (6.3-8 GHz) cm-wave and IMAGEVK-74 (62-69 GHz) mm-wave imaging units by Vayyar Imaging. A comparison is presented on the applicability of each unit for material classification. This work extends upon previous efforts, by applying deep wavelet scattering transform for the identification of different materials based on the reflected signals. In the wavelet scattering feature extractor, data is propagated through a series of wavelet transforms, nonlinearities, and averaging to produce low-variance representations of the reflected radar signals. This work is unique in comparison of the radar units and algorithms in material classification and includes real-time demonstrations that show strong performance by both units, with increased robustness offered by the cm-wave radar unit.      
### 9.Class Distance Weighted Cross-Entropy Loss for Ulcerative Colitis Severity Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2202.05167.pdf)
>  Endoscopic Mayo score and Ulcerative Colitis Endoscopic Index of Severity are commonly used scoring systems for the assessment of endoscopic severity of ulcerative colitis. They are based on assigning a score in relation to the disease activity, which creates a rank among the levels, making it an ordinal regression problem. On the other hand, most studies use categorical cross-entropy loss function, which is not optimal for the ordinal regression problem, to train the deep learning models. In this study, we propose a novel loss function called class distance weighted cross-entropy (CDW-CE) that respects the order of the classes and takes the distance of the classes into account in calculation of cost. Experimental evaluations show that CDW-CE outperforms the conventional categorical cross-entropy and CORN framework, which is designed for the ordinal regression problems. In addition, CDW-CE does not require any modifications at the output layer and is compatible with the class activation map visualization techniques.      
### 10.Design of Flexible Meander Line Antenna for Healthcare for Wireless Medical Body Area Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.05166.pdf)
>  A flexible meander line monopole antenna (MMA) is presented in this paper. The antenna can be worn for on-and off-body applications. The overall dimension of the MMA is 37 mm x 50 mm x2.37 mm3. The MMA was manufactured and measured, and the results matched with simulation results. The MMA design shows a bandwidth of up to 1282.4 (450.5) MHz and provides gains of 3.03 (4.85) dBi in the lower and upper operating bands, respectively, showing omnidirectional radiation patterns in free space. While worn on the chest or arm, bandwidths as high as 688.9 (500.9) MHz and 1261.7 (524.2) MHz, and the gains of 3.80 (4.67) dBi and 3.00 (4.55) dBi were observed. The experimental measurements of the read range confirmed the results of the coverage range of up to 11 meters.      
### 11.SUMO: Advanced sleep spindle identification with neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.05158.pdf)
>  Sleep spindles are neurophysiological phenomena that appear to be linked to memory formation and other functions of the central nervous system, and that can be observed in electroencephalographic recordings (EEG) during sleep. Manually identified spindle annotations in EEG recordings suffer from substantial intra- and inter-rater variability, even if raters have been highly trained, which reduces the reliability of spindle measures as a research and diagnostic tool. The Massive Online Data Annotation (MODA) project has recently addressed this problem by forming a consensus from multiple such rating experts, thus providing a corpus of spindle annotations of enhanced quality. Based on this dataset, we present a U-Net-type deep neural network model to automatically detect sleep spindles. Our model's performance exceeds that of the state-of-the-art detector and of most experts in the MODA dataset. We observed improved detection accuracy in subjects of all ages, including older individuals whose spindles are particularly challenging to detect reliably. Our results underline the potential of automated methods to do repetitive cumbersome tasks with super-human performance.      
### 12.Effective classification of ecg signals using enhanced convolutional neural network in iot  [ :arrow_down: ](https://arxiv.org/pdf/2202.05154.pdf)
>  In this paper, a novel ECG monitoring approach based on IoT technology is suggested. This paper proposes a routing system for IoT healthcare platforms based on Dynamic Source Routing (DSR) and Routing by Energy and Link Quality (REL). In addition, the Artificial Neural Network (ANN), Support Vector Machine (SVM), and Convolution Neural Networks (CNNs)-based approaches for ECG signal categorization were tested in this study. Deep-ECG will employ a deep CNN to extract important characteristics, which will then be compared using simple and fast distance functions in order to classify cardiac problems efficiently. This work has suggested algorithms for the categorization of ECG data acquired from mobile watch users in order to identify aberrant data. The Massachusetts Institute of Technology (MIT) and Beth Israel Hospital (MIT/BIH) Arrhythmia Database have been used for experimental verification of the suggested approaches. The results show that the proposed strategy outperforms others in terms of classification accuracy.      
### 13.AI-based Robust Resource Allocation in End-to-End Network Slicing under Demand and CSI Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2202.05131.pdf)
>  Network slicing (NwS) is one of the main technologies in the fifth-generation of mobile communication and beyond (5G+). One of the important challenges in the NwS is information uncertainty which mainly involves demand and channel state information (CSI). Demand uncertainty is divided into three types: number of users requests, amount of bandwidth, and requested virtual network functions workloads. Moreover, the CSI uncertainty is modeled by three methods: worst-case, probabilistic, and hybrid. In this paper, our goal is to maximize the utility of the infrastructure provider by exploiting deep reinforcement learning algorithms in end-to-end NwS resource allocation under demand and CSI uncertainties. The proposed formulation is a nonconvex mixed-integer non-linear programming problem. To perform robust resource allocation in problems that involve uncertainty, we need a history of previous information. To this end, we use a recurrent deterministic policy gradient (RDPG) algorithm, a recurrent and memory-based approach in deep reinforcement learning. Then, we compare the RDPG method in different scenarios with soft actor-critic (SAC), deep deterministic policy gradient (DDPG), distributed, and greedy algorithms. The simulation results show that the SAC method is better than the DDPG, distributed, and greedy methods, respectively. Moreover, the RDPG method out performs the SAC approach on average by 70%.      
### 14.Deep Learning for Computational Cytology: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2202.05126.pdf)
>  Computational cytology is a critical, rapid-developing, yet challenging topic in the field of medical image computing which analyzes the digitized cytology image by computer-aided technologies for cancer screening. Recently, an increasing number of deep learning (DL) algorithms have made significant progress in medical image analysis, leading to the boosting publications of cytological studies. To investigate the advanced methods and comprehensive applications, we survey more than 120 publications of DL-based cytology image analysis in this article. We first introduce various deep learning methods, including fully supervised, weakly supervised, unsupervised, and transfer learning. Then, we systematically summarize the public datasets, evaluation metrics, versatile cytology image analysis applications including classification, detection, segmentation, and other related tasks. Finally, we discuss current challenges and potential research directions of computational cytology.      
### 15.Cross-speaker style transfer for text-to-speech using data augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.05083.pdf)
>  We address the problem of cross-speaker style transfer for text-to-speech (TTS) using data augmentation via voice conversion. We assume to have a corpus of neutral non-expressive data from a target speaker and supporting conversational expressive data from different speakers. Our goal is to build a TTS system that is expressive, while retaining the target speaker's identity. The proposed approach relies on voice conversion to first generate high-quality data from the set of supporting expressive speakers. The voice converted data is then pooled with natural data from the target speaker and used to train a single-speaker multi-style TTS system. We provide evidence that this approach is efficient, flexible, and scalable. The method is evaluated using one or more supporting speakers, as well as a variable amount of supporting data. We further provide evidence that this approach allows some controllability of speaking style, when using multiple supporting speakers. We conclude by scaling our proposed technology to a set of 14 speakers across 7 languages. Results indicate that our technology consistently improves synthetic samples in terms of style similarity, while retaining the target speaker's identity.      
### 16.Optimised operation of low-emission offshore oil and gas platform integrated energy systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.05072.pdf)
>  This paper considers the operation of offshore oil and gas platform energy systems with energy supply from wind turbines to reduce local CO2 emissions. A new integrated energy system model for operational planning and simulation has been developed and implemented in an open-source software tool (Oogeso). This model and tool is first presented, and then applied on a relevant North Sea case with different energy supply alternatives to quantify and compare CO2 emission reductions and other key indicators.      
### 17.Decomposition Problem in Process of Selective Identification and Localization of Voltage Fluctuations Sources in Power Grids  [ :arrow_down: ](https://arxiv.org/pdf/2202.05020.pdf)
>  Voltage fluctuations are common disturbances in power grids, therefore the effective and selective process of identification and localization of individual voltage fluctuations sources is necessary for the minimization of such disturbances. Selectivity in the process of identification and localization disturbing loads is possible by the use cascade of blocks: demodulation, decomposition and propagation assessment. The effectiveness of this approach is closely related to the used method of decomposition. The paper presents the problem of decomposition process for the selected method of selective identification and localization of voltage fluctuation sources, in which the algorithm of enhanced empirical wavelet transform (EEWT) is used as the decomposition method. The paper presents selected research results from the real power grid, for which the result of selected approach causes mistakes in the process of identification and localization of voltage fluctuations sources. The potential causes of such mistakes related to the decomposition process are discussed on the basis of obtained research results.      
### 18.IEC Flickermeter Measurement Results for Distorted Modulating Signal while Supplied with Distorted Voltage  [ :arrow_down: ](https://arxiv.org/pdf/2202.05001.pdf)
>  The paper presents IEC flickermeter measurement results for voltage fluctuations modelled by amplitude modulation of distorted supply voltage. The supply voltage distortion caused by electronic and power electronic devices in the "clipped cosine" form is assumed. This type of supply voltage distortion is a common disturbance in low voltage networks. Several arbitrary distorted waveforms of the modulating signal with different modulation depth and modulating frequency up to approx. 1 kHz are selected to determine the dependence of severity of voltage fluctuation on their shape. The paper mainly presents the dependence of voltage fluctuation severity with a frequency greater than 3fc, where fc is the power frequency. The voltage fluctuation severity and the dependencies associated with it have been determined on the basis of numerical simulation studies and experimental laboratory tests.      
### 19.Monotonically Convergent Regularization by Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2202.04961.pdf)
>  Regularization by denoising (RED) is a widely-used framework for solving inverse problems by leveraging image denoisers as image priors. Recent work has reported the state-of-the-art performance of RED in a number of imaging applications using pre-trained deep neural nets as denoisers. Despite the recent progress, the stable convergence of RED algorithms remains an open problem. The existing RED theory only guarantees stability for convex data-fidelity terms and nonexpansive denoisers. This work addresses this issue by developing a new monotone RED (MRED) algorithm, whose convergence does not require nonexpansiveness of the deep denoising prior. Simulations on image deblurring and compressive sensing recovery from random matrices show the stability of MRED even when the traditional RED algorithm diverges.      
### 20.The USTC-Ximalaya system for the ICASSP 2022 multi-channel multi-party meeting transcription (M2MeT) challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.04855.pdf)
>  We propose two improvements to target-speaker voice activity detection (TS-VAD), the core component in our proposed speaker diarization system that was submitted to the 2022 Multi-Channel Multi-Party Meeting Transcription (M2MeT) challenge. These techniques are designed to handle multi-speaker conversations in real-world meeting scenarios with high speaker-overlap ratios and under heavy reverberant and noisy condition. First, for data preparation and augmentation in training TS-VAD models, speech data containing both real meetings and simulated indoor conversations are used. Second, in refining results obtained after TS-VAD based decoding, we perform a series of post-processing steps to improve the VAD results needed to reduce diarization error rates (DERs). Tested on the ALIMEETING corpus, the newly released Mandarin meeting dataset used in M2MeT, we demonstrate that our proposed system can decrease the DER by up to 66.55/60.59% relatively when compared with classical clustering based diarization on the Eval/Test set.      
### 21.Spatial active noise control based on individual kernel interpolation of primary and secondary sound fields  [ :arrow_down: ](https://arxiv.org/pdf/2202.04807.pdf)
>  A spatial active noise control (ANC) method based on the individual kernel interpolation of primary and secondary sound fields is proposed. Spatial ANC is aimed at cancelling unwanted primary noise within a continuous region by using multiple secondary sources and microphones. A method based on the kernel interpolation of a sound field makes it possible to attenuate noise over the target region with flexible array geometry. Furthermore, by using the kernel function with directional weighting, prior information on primary noise source directions can be taken into consideration. However, whereas the sound field to be interpolated is a superposition of primary and secondary sound fields, the directional weight for the primary noise source was applied to the total sound field in previous work; therefore, the performance improvement was limited. We propose a method of individually interpolating the primary and secondary sound fields and formulate a normalized least-mean-square algorithm based on this interpolation method. Experimental results indicate that the proposed method outperforms the method based on total kernel interpolation.      
### 22.Should Storage-Centric Tariffs be Extended to Commercial Flexible Demand?  [ :arrow_down: ](https://arxiv.org/pdf/2202.04802.pdf)
>  Further electrification of the economy is expected to sharpen ramp rates and increase peak loads. Flexibility from the demand side, which new technologies might facilitate, can help these operational challenges. Electric utilities have begun implementing new tariffs and other mechanisms to encourage the deployment of energy storage. This paper examines whether making these new tariffs technology agnostic and extending them to flexible demand would significantly improve the procurement of operational flexibility. In particular, we consider how a commercial consumer might adjust its flexible demand when subject to Pacific Gas and Electric Company's storage-centric electric tariff. We show that extending this tariff to consumers with flexible demand would reduce the utility's net demand ramp rates during peak hours. If consumers have a high level of demand flexibility, this tariff also reduces the net demand during peak hours and decreases total electric bills when compared to the base tariff.      
### 23.Multiclass histogram-based thresholding using kernel density estimation and scale-space representations  [ :arrow_down: ](https://arxiv.org/pdf/2202.04785.pdf)
>  We present a new method for multiclass thresholding of a histogram which is based on the nonparametric Kernel Density (KD) estimation, where the unknown parameters of the KD estimate are defined using the Expectation-Maximization (EM) iterations. The method compares the number of extracted minima of the KD estimate with the number of the requested clusters minus one. If these numbers match, the algorithm returns positions of the minima as the threshold values, otherwise, the method gradually decreases/increases the kernel bandwidth until the numbers match. We verify the method using synthetic histograms with known threshold values and using the histogram of real X-ray computed tomography images. After thresholding of the real histogram, we estimated the porosity of the sample and compare it with the direct experimental measurements. The comparison shows the meaningfulness of the thresholding.      
### 24.Wireless Transmission of Images With The Assistance of Multi-level Semantic Information  [ :arrow_down: ](https://arxiv.org/pdf/2202.04754.pdf)
>  Semantic-oriented communication has been considered as a promising to boost the bandwidth efficiency by only transmitting the semantics of the data. In this paper, we propose a multi-level semantic aware communication system for wireless image transmission, named MLSC-image, which is based on the deep learning techniques and trained in an end to end manner. In particular, the proposed model includes a multilevel semantic feature extractor, that extracts both the highlevel semantic information, such as the text semantics and the segmentation semantics, and the low-level semantic information, such as local spatial details of the images. We employ a pretrained image caption to capture the text semantics and a pretrained image segmentation model to obtain the segmentation semantics. These high-level and low-level semantic features are then combined and encoded by a joint semantic and channel encoder into symbols to transmit over the physical channel. The numerical results validate the effectiveness and efficiency of the proposed semantic communication system, especially under the limited bandwidth condition, which indicates the advantages of the high-level semantics in the compression of images.      
### 25.Semantic Segmentation of Anaemic RBCs Using Multilevel Deep Convolutional Encoder-Decoder Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.04650.pdf)
>  Pixel-level analysis of blood images plays a pivotal role in diagnosing blood-related diseases, especially Anaemia. These analyses mainly rely on an accurate diagnosis of morphological deformities like shape, size, and precise pixel counting. In traditional segmentation approaches, instance or object-based approaches have been adopted that are not feasible for pixel-level analysis. The convolutional neural network (CNN) model required a large dataset with detailed pixel-level information for the semantic segmentation of red blood cells in the deep learning domain. In current research work, we address these problems by proposing a multi-level deep convolutional encoder-decoder network along with two state-of-the-art healthy and Anaemic-RBC datasets. The proposed multi-level CNN model preserved pixel-level semantic information extracted in one layer and then passed to the next layer to choose relevant features. This phenomenon helps to precise pixel-level counting of healthy and anaemic-RBC elements along with morphological analysis. For experimental purposes, we proposed two state-of-the-art RBC datasets, i.e., Healthy-RBCs and Anaemic-RBCs dataset. Each dataset contains 1000 images, ground truth masks, relevant, complete blood count (CBC), and morphology reports for performance evaluation. The proposed model results were evaluated using crossmatch analysis with ground truth mask by finding IoU, individual training, validation, testing accuracies, and global accuracies using a 05-fold training procedure. This model got training, validation, and testing accuracies as 0.9856, 0.9760, and 0.9720 on the Healthy-RBC dataset and 0.9736, 0.9696, and 0.9591 on an Anaemic-RBC dataset. The IoU and BFScore of the proposed model were 0.9311, 0.9138, and 0.9032, 0.8978 on healthy and anaemic datasets, respectively.      
### 26.Multi-modal unsupervised brain image registration using edge maps  [ :arrow_down: ](https://arxiv.org/pdf/2202.04647.pdf)
>  Diffeomorphic deformable multi-modal image registration is a challenging task which aims to bring images acquired by different modalities to the same coordinate space and at the same time to preserve the topology and the invertibility of the transformation. Recent research has focused on leveraging deep learning approaches for this task as these have been shown to achieve competitive registration accuracy while being computationally more efficient than traditional iterative registration methods. In this work, we propose a simple yet effective unsupervised deep learning-based {\em multi-modal} image registration approach that benefits from auxiliary information coming from the gradient magnitude of the image, i.e. the image edges, during the training. The intuition behind this is that image locations with a strong gradient are assumed to denote a transition of tissues, which are locations of high information value able to act as a geometry constraint. The task is similar to using segmentation maps to drive the training, but the edge maps are easier and faster to acquire and do not require annotations. We evaluate our approach in the context of registering multi-modal (T1w to T2w) magnetic resonance (MR) brain images of different subjects using three different loss functions that are said to assist multi-modal registration, showing that in all cases the auxiliary information leads to better results without compromising the runtime.      
### 27.FCM-DNN: diagnosing coronary artery disease by deep accuracy Fuzzy C-Means clustering model  [ :arrow_down: ](https://arxiv.org/pdf/2202.04645.pdf)
>  Cardiovascular disease is one of the most challenging diseases in middle-aged and older people, which causes high mortality. Coronary artery disease (CAD) is known as a common cardiovascular disease. A standard clinical tool for diagnosing CAD is angiography. The main challenges are dangerous side effects and high angiography costs. Today, the development of artificial intelligence-based methods is a valuable achievement for diagnosing disease. Hence, in this paper, artificial intelligence methods such as neural network (NN), deep neural network (DNN), and Fuzzy C-Means clustering combined with deep neural network (FCM-DNN) are developed for diagnosing CAD on a cardiac magnetic resonance imaging (CMRI) dataset. The original dataset is used in two different approaches. First, the labeled dataset is applied to the NN and DNN to create the NN and DNN models. Second, the labels are removed, and the unlabeled dataset is clustered via the FCM method, and then, the clustered dataset is fed to the DNN to create the FCM-DNN model. By utilizing the second clustering and modeling, the training process is improved, and consequently, the accuracy is increased. As a result, the proposed FCM-DNN model achieves the best performance with a 99.91% accuracy specifying 10 clusters, i.e., 5 clusters for healthy subjects and 5 clusters for sick subjects, through the 10-fold cross-validation technique compared to the NN and DNN models reaching the accuracies of 92.18% and 99.63%, respectively. To the best of our knowledge, no study has been conducted for CAD diagnosis on the CMRI dataset using artificial intelligence methods. The results confirm that the proposed FCM-DNN model can be helpful for scientific and research centers.      
### 28.On-the-fly 3D metrology of volumetric additive manufacturing  [ :arrow_down: ](https://arxiv.org/pdf/2202.04644.pdf)
>  Additive manufacturing techniques are revolutionizing product development by enabling fast turnaround from design to fabrication. However, the throughput of the rapid prototyping pipeline remains constrained by print optimization, requiring multiple iterations of fabrication and ex-situ metrology. Despite the need for a suitable technology, robust in-situ shape measurement of an entire print is not currently available with any additive manufacturing modality. Here, we address this shortcoming by demonstrating fully simultaneous 3D metrology and printing. We exploit the dramatic increase in light scattering by a photoresin during gelation for real-time 3D imaging of prints during tomographic volumetric additive manufacturing. Tomographic imaging of the light scattering density in the build volume yields quantitative, artifact-free 3D + time models of cured objects that are accurate to below 1% of the size of the print. By integrating shape measurement into the printing process, our work paves the way for next-generation rapid prototyping with real-time defect detection and correction.      
### 29.Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2202.05265.pdf)
>  Image-to-image regression is an important learning task, used frequently in biological imaging. Current algorithms, however, do not generally offer statistical guarantees that protect against a model's mistakes and hallucinations. To address this, we develop uncertainty quantification techniques with rigorous statistical guarantees for image-to-image regression problems. In particular, we show how to derive uncertainty intervals around each pixel that are guaranteed to contain the true value with a user-specified confidence probability. Our methods work in conjunction with any base machine learning model, such as a neural network, and endow it with formal mathematical guarantees -- regardless of the true unknown data distribution or choice of model. Furthermore, they are simple to implement and computationally inexpensive. We evaluate our procedure on three image-to-image regression tasks: quantitative phase microscopy, accelerated magnetic resonance imaging, and super-resolution transmission electron microscopy of a Drosophila melanogaster brain.      
### 30.Learnable Nonlinear Compression for Robust Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2202.05236.pdf)
>  In this study, we focus on nonlinear compression methods in spectral features for speaker verification based on deep neural network. We consider different kinds of channel-dependent (CD) nonlinear compression methods optimized in a data-driven manner. Our methods are based on power nonlinearities and dynamic range compression (DRC). We also propose multi-regime (MR) design on the nonlinearities, at improving robustness. Results on VoxCeleb1 and VoxMovies data demonstrate improvements brought by proposed compression methods over both the commonly-used logarithm and their static counterparts, especially for ones based on power function. While CD generalization improves performance on VoxCeleb1, MR provides more robustness on VoxMovies, with a maximum relative equal error rate reduction of 21.6%.      
### 31.Game Theoretic Analysis of an Adversarial Status Updating System  [ :arrow_down: ](https://arxiv.org/pdf/2202.05233.pdf)
>  We investigate the game theoretic equilibrium points of a status updating system with an adversary that jams the updates in the downlink. We consider the system models with and without diversity. The adversary can jam up to $\alpha$ proportion of the entire communication window. In the model without diversity, in each time slot, the base station schedules a user from $N$ users according to a stationary distribution. The adversary blocks (jams) $\alpha T$ time slots of its choosing out of the total $T$ time slots. For this system, we show that a Nash equilibrium does not exist, however, a Stackelberg equilibrium exists when the scheduling algorithm of the base station acts as the leader and the adversary acts as the follower. In the model with diversity, in each time slot, the base station schedules a user from $N$ users and chooses a sub-carrier from $N_{sub}$ sub-carriers to transmit update packets to the scheduled user according to a stationary distribution. The adversary blocks $\alpha T$ time slots of its choosing out of $T$ time slots at the sub-carriers of its choosing. For this system, we show that a Nash equilibrium exists and identify the Nash equilibrium.      
### 32.Improving Automatic Speech Recognition for Non-Native English with Transfer Learning and Language Model Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2202.05209.pdf)
>  ASR systems designed for native English (L1) usually underperform on non-native English (L2). To address this performance gap, \textbf{(i)} we extend our previous work to investigate fine-tuning of a pre-trained wav2vec 2.0 model \cite{baevski2020wav2vec,xu2021self} under a rich set of L1 and L2 training conditions. We further \textbf{(ii)} incorporate language model decoding in the ASR system, along with the fine-tuning method. Quantifying gains acquired from each of these two approaches separately and an error analysis allows us to identify different sources of improvement within our models. We find that while the large self-trained wav2vec 2.0 may be internalizing sufficient decoding knowledge for clean L1 speech \cite{xu2021self}, this does not hold for L2 speech and accounts for the utility of employing language model decoding on L2 data.      
### 33.Image classification using collective optical modes of an array of nanolasers  [ :arrow_down: ](https://arxiv.org/pdf/2202.05171.pdf)
>  Recent advancements in nanolaser design and manufacturing open up unprecedented perspectives in terms of high integration densities and ultra-low power consumption, making these devices ideal for high-performance optical computing systems. In this work we exploit the symmetry properties of the collective modes of a nanolaser array for binary image classification. The implementation is based on a 8x8 array, and relies on the activation of a collective optical mode of the array, the so-called "zero mode", under spatially modulated pump patterns. We demonstrate that a simple training strategy allows us to achieve an overall success rate of 98% in binary image recognition.      
### 34.On the Acquisition of Stationary Signals Using Uniform ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2202.05143.pdf)
>  In this work, we consider the acquisition of stationary signals using uniform analog-to-digital converters (ADCs), i.e., employing uniform sampling and scalar uniform quantization. We jointly optimize the pre-sampling and reconstruction filters to minimize the time-averaged mean-squared error (TMSE) in recovering the continuous-time input signal for a fixed sampling rate and quantizer resolution and obtain closed-form expressions for the minimal achievable TMSE. We show that the TMSE-minimizing pre-sampling filter omits aliasing and discards weak frequency components to resolve the remaining ones with higher resolution when the rate budget is small. In our numerical study, we validate our results and show that sub-Nyquist sampling often minimizes the TMSE under tight rate budgets at the output of the ADC.      
### 35.Machine Learning-based Urban Canyon Path Loss Prediction using 28 GHz Manhattan Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2202.05107.pdf)
>  Large bandwidth at mm-wave is crucial for 5G and beyond but the high path loss (PL) requires highly accurate PL prediction for network planning and optimization. Statistical models with slope-intercept fit fall short in capturing large variations seen in urban canyons, whereas ray-tracing, capable of characterizing site-specific features, faces challenges in describing foliage and street clutter and associated reflection/diffraction ray calculation. Machine learning (ML) is promising but faces three key challenges in PL prediction: 1) insufficient measurement data; 2) lack of extrapolation to new streets; 3) overwhelmingly complex features/models. We propose an ML-based urban canyon PL prediction model based on extensive 28 GHz measurements from Manhattan where street clutters are modeled via a LiDAR point cloud dataset and buildings by a mesh-grid building dataset. We extract expert knowledge-driven street clutter features from the point cloud and aggressively compress 3D-building information using convolutional-autoencoder. Using a new street-by-street training and testing procedure to improve generalizability, the proposed model using both clutter and building features achieves a prediction error (RMSE) of $4.8 \pm 1.1$ dB compared to $10.6 \pm 4.4$ dB and $6.5 \pm 2.0$ dB for 3GPP LOS and slope-intercept prediction, respectively, where the standard deviation indicates street-by-street variation. By only using four most influential clutter features, RMSE of $5.5\pm 1.1$ dB is achieved.      
### 36.Hardware calibrated learning to compensate heterogeneity in analog RRAM-based Spiking Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.05094.pdf)
>  Spiking Neural Networks (SNNs) can unleash the full power of analog Resistive Random Access Memories (RRAMs) based circuits for low power signal processing. Their inherent computational sparsity naturally results in energy efficiency benefits. The main challenge implementing robust SNNs is the intrinsic variability (heterogeneity) of both analog CMOS circuits and RRAM technology. In this work, we assessed the performance and variability of RRAM-based neuromorphic circuits that were designed and fabricated using a 130\,nm technology node. Based on these results, we propose a Neuromorphic Hardware Calibrated (NHC) SNN, where the learning circuits are calibrated on the measured data. We show that by taking into account the measured heterogeneity characteristics in the off-chip learning phase, the NHC SNN self-corrects its hardware non-idealities and learns to solve benchmark tasks with high accuracy. This work demonstrates how to cope with the heterogeneity of neurons and synapses for increasing classification accuracy in temporal tasks.      
### 37.Two-Stage Deep Anomaly Detection with Heterogeneous Time Series Data  [ :arrow_down: ](https://arxiv.org/pdf/2202.05093.pdf)
>  We introduce a data-driven anomaly detection framework using a manufacturing dataset collected from a factory assembly line. Given heterogeneous time series data consisting of operation cycle signals and sensor signals, we aim at discovering abnormal events. Motivated by our empirical findings that conventional single-stage benchmark approaches may not exhibit satisfactory performance under our challenging circumstances, we propose a two-stage deep anomaly detection (TDAD) framework in which two different unsupervised learning models are adopted depending on types of signals. In Stage I, we select anomaly candidates by using a model trained by operation cycle signals; in Stage II, we finally detect abnormal events out of the candidates by using another model, which is suitable for taking advantage of temporal continuity, trained by sensor signals. A distinguishable feature of our framework is that operation cycle signals are exploited first to find likely anomalous points, whereas sensor signals are leveraged to filter out unlikely anomalous points afterward. Our experiments comprehensively demonstrate the superiority over single-stage benchmark approaches, the model-agnostic property, and the robustness to difficult situations.      
### 38.Equivariance Regularization for Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2202.05062.pdf)
>  In this work, we propose Regularization-by-Equivariance (REV), a novel structure-adaptive regularization scheme for solving imaging inverse problems under incomplete measurements. Our regularization scheme utilizes the equivariant structure in the physics of the measurements -- which is prevalent in many inverse problems such as tomographic image reconstruction -- to mitigate the ill-poseness of the inverse problem. Our proposed scheme can be applied in a plug-and-play manner alongside with any classic first-order optimization algorithm such as the accelerated gradient descent/FISTA for simplicity and fast convergence. Our numerical experiments in sparse-view X-ray CT image reconstruction tasks demonstrate the effectiveness of our approach.      
### 39.TV-based Spline Reconstruction with Fourier Measurements: Uniqueness and Convergence of Grid-Based Methods  [ :arrow_down: ](https://arxiv.org/pdf/2202.05059.pdf)
>  We study the problem of recovering piecewise-polynomial periodic functions from their low-frequency information. This means that we only have access to possibly corrupted versions of the Fourier samples of the ground truth up to a maximum cutoff frequency $K_c$. The reconstruction task is specified as an optimization problem with total-variation (TV) regularization (in the sense of measures) involving the $M$-th order derivative regularization operator $\mathrm{L} = \mathrm{D}^M$. The order $M \geq 1$ determines the degree of the reconstructed piecewise polynomial spline, whereas the TV regularization norm, which is known to promote sparsity, guarantees a small number of pieces. We show that the solution of our optimization problem is always unique, which, to the best of our knowledge, is a first for TV-based problems. Moreover, we show that this solution is a periodic spline matched to the regularization operator $\mathrm{L}$ whose number of knots is upper-bounded by $2 K_c$. We then consider the grid-based discretization of our optimization problem in the space of uniform $\mathrm{L}$-splines. On the theoretical side, we show that any sequence of solutions of the discretized problem converges uniformly to the unique solution of the gridless problem as the grid size vanishes. Finally, on the algorithmic side, we propose a B-spline-based algorithm to solve the grid-based problem, and we demonstrate its numerical feasibility experimentally. On both of these aspects, we leverage the uniqueness of the solution of the original problem.      
### 40.CMOS Circuits for Shape-Based Analog Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.05022.pdf)
>  While analog computing is attractive for implementing machine learning (ML) processors, the paradigm requires chip-in-the-loop training for every processor to alleviate artifacts due to device mismatch and device non-linearity. Speeding up chip-in-the-loop training requires re-biasing the circuits in a manner that the analog functions remain invariant across training and inference. In this paper, we present an analog computational paradigm and circuits using "shape" functions that remain invariant to transistor biasing (weak, moderate, and strong inversion) and ambient temperature variation. We show that a core Shape-based Analog Compute (S-AC) circuit could be re-biased and reused to implement: (a) non-linear functions; (b) inner-product building blocks; and (c) a mixed-signal logarithmic memory, all of which are integral towards designing an ML inference processor. Measured results using a prototype fabricated in a 180nm standard CMOS process demonstrate bias invariance and hence the resulting analog designs can be scaled for power and speed like digital logic circuits. We also demonstrate a regression task using these CMOS building blocks.      
### 41.Intelligent Resource Allocations for IRS-Assisted OFDM Communications: A Hybrid MDQN-DDPG Approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.05017.pdf)
>  In this paper, we study the resource allocation problem for an intelligent reflecting surface (IRS)-assisted OFDM system. The system sum rate maximization framework is formulated by jointly optimizing subcarrier allocation, base station transmit beamforming and IRS phase shift. Considering the continuous and discrete hybrid action space characteristics of the optimization variables, we propose an efficient resource allocation algorithm combining multiple deep Q networks (MDQN) and deep deterministic policy-gradient (DDPG) to deal with this issue. In our algorithm, MDQN are employed to solve the problem of large discrete action space, while DDPG is introduced to tackle the continuous action allocation. Compared with the traditional approaches, our proposed MDQN-DDPG based algorithm has the advantage of continuous behavior improvement through learning from the environment. Simulation results demonstrate superior performance of our design in terms of system sum rate compared with the benchmark schemes.      
### 42.Coverage Probability and Spectral Efficiency Analysis of Multi-Gateway Downlink LoRa Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.05014.pdf)
>  The system-level performance of multi-gateway downlink long-range (LoRa) networks is investigated in the present paper. <br>Specifically, we first compute the active probability of a channel and the selection probability of an active end-device (ED) in the closed-form expressions. We then derive the coverage probability (Pcov) and the area spectral efficiency (ASE) under the impact of the capture effects and different spreading factor (SF) allocation schemes. <br>Our findings show that both the Pcov and the ASE of the considered networks can be enhanced significantly by increasing both the duty cycle and the transmit power. <br>Finally, Monte-Carlo simulations are provided to verify the accuracy of the proposed mathematical frameworks.      
### 43.Semi-Supervised Convolutive NMF for Automatic Music Transcription  [ :arrow_down: ](https://arxiv.org/pdf/2202.04989.pdf)
>  Automatic Music Transcription, which consists in transforming an audio recording of a musical performance into symbolic format, remains a difficult Music Information Retrieval task. In this work, we propose a semi-supervised approach using low-rank matrix factorization techniques, in particular Convolutive Nonnegative Matrix Factorization. In the semi-supervised setting, only a single recording of each individual notes is required. <br>We show on the MAPS dataset that the proposed semi-supervised CNMF method performs better than state-of-the-art low-rank factorization techniques and a little worse than supervised deep learning state-of-the-art methods, while however suffering from generalization issues.      
### 44.Barwise Compression Schemes for Audio-Based Music Structure Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.04981.pdf)
>  Music Structure Analysis (MSA) consists in segmenting a music piece in several distinct sections. We approach MSA within a compression framework, under the hypothesis that the structure is more easily revealed by a simplified representation of the original content of the song. <br>More specifically, under the hypothesis that MSA is correlated with similarities occurring at the bar scale, linear and non-linear compression schemes can be applied to barwise audio signals. Compressed representations capture the most salient components of the different bars in the song and are then used to infer the song structure using a dynamic programming algorithm. <br>This work explores both low-rank approximation models such as Principal Component Analysis or Nonnegative Matrix Factorization and "piece-specific" Auto-Encoding Neural Networks, with the objective to learn latent representations specific to a given song. Such approaches do not rely on supervision nor annotations, which are well-known to be tedious to collect and possibly ambiguous in MSA description. <br>In our experiments, several unsupervised compression schemes achieve a level of performance comparable to that of state-of-the-art supervised methods (for 3s tolerance) on the RWC-Pop dataset, showcasing the importance of the barwise compression processing for MSA.      
### 45.ASRPU: A Programmable Accelerator for Low-Power Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.04971.pdf)
>  The outstanding accuracy achieved by modern Automatic Speech Recognition (ASR) systems is enabling them to quickly become a mainstream technology. ASR is essential for many applications, such as speech-based assistants, dictation systems and real-time language translation. However, highly accurate ASR systems are computationally expensive, requiring on the order of billions of arithmetic operations to decode each second of audio, which conflicts with a growing interest in deploying ASR on edge devices. On these devices, hardware acceleration is key for achieving acceptable performance. However, ASR is a rich and fast-changing field, and thus, any overly specialized hardware accelerator may quickly become obsolete. <br>In this paper, we tackle those challenges by proposing ASRPU, a programmable accelerator for on-edge ASR. ASRPU contains a pool of general-purpose cores that execute small pieces of parallel code. Each of these programs computes one part of the overall decoder (e.g. a layer in a neural network). The accelerator automates some carefully chosen parts of the decoder to simplify the programming without sacrificing generality. We provide an analysis of a modern ASR system implemented on ASRPU and show that this architecture can achieve real-time decoding with a very low power budget.      
### 46.Sound masking degrades perception of self-location during stepping: A case for sound-transparent spacesuits for Mars  [ :arrow_down: ](https://arxiv.org/pdf/2202.04958.pdf)
>  Most efforts to improve spacesuits have been directed towards adding haptic feedback. However, sound transparency can also improve situational awareness at a relatively low cost. The extent of the improvement is unknown. We use the Fukuda-Unterberger stepping test to measure the accuracy of one's perception of self-location. We compare accuracy outcomes in two scenarios: one where hearing is impaired with sound masking with white noise and one where it is not. These scenarios are acoustic proxies for a sound muffling space suit and a sound transparent space suit respectively. The results show that when sound masking is applied, the error in self-location increases by 14.5cm, 95% CI [4.04 28.22]. Suggestions to apply the findings to Mars spacesuit designs are discussed. A cost-benefit analysis is also provided.      
### 47.OWL (Observe, Watch, Listen): Localizing Actions in Egocentric Video via Audiovisual Temporal Context  [ :arrow_down: ](https://arxiv.org/pdf/2202.04947.pdf)
>  Temporal action localization (TAL) is an important task extensively explored and improved for third-person videos in recent years. Recent efforts have been made to perform fine-grained temporal localization on first-person videos. However, current TAL methods only use visual signals, neglecting the audio modality that exists in most videos and that shows meaningful action information in egocentric videos. In this work, we take a deep look into the effectiveness of audio in detecting actions in egocentric videos and introduce a simple-yet-effective approach via Observing, Watching, and Listening (OWL) to leverage audio-visual information and context for egocentric TAL. For doing that, we: 1) compare and study different strategies for where and how to fuse the two modalities; 2) propose a transformer-based model to incorporate temporal audio-visual context. Our experiments show that our approach achieves state-of-the-art performance on EPIC-KITCHENS-100.      
### 48.Case-based reasoning for rare events prediction on strategic sites  [ :arrow_down: ](https://arxiv.org/pdf/2202.04891.pdf)
>  Satellite imagery is now widely used in the defense sector for monitoring locations of interest. Although the increasing amount of data enables pattern identification and therefore prediction, carrying this task manually is hardly feasible. We hereby propose a cased-based reasoning approach for automatic prediction of rare events on strategic sites. This method allows direct incorporation of expert knowledge, and is adapted to irregular time series and small-size datasets. Experiments are carried out on two use-cases using real satellite images: the prediction of submarines arrivals and departures from a naval base, and the forecasting of imminent rocket launches on two space bases. The proposed method significantly outperforms a random selection of reference cases on these challenging applications, showing its strong potential.      
### 49.Auditory Model based Phase-Aware Bayesian Spectral Amplitude Estimator for Single-Channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.04882.pdf)
>  Bayesian estimation of short-time spectral amplitude is one of the most predominant approaches for the enhancement of the noise corrupted speech. The performance of these estimators are usually significantly improved when any perceptually relevant cost function is considered. On the other hand, the recent progress in the phase-based speech signal processing have shown that the phase-only enhancement based on spectral phase estimation methods can also provide joint improvement in the perceived speech quality and intelligibility, even in low SNR conditions. In this paper, to take advantage of both the perceptually motivated cost function involving STSAs of estimated and true clean speech and utilizing the prior spectral phase information, we have derived a phase-aware Bayesian STSA estimator. The parameters of the cost function are chosen based on the characteristics of the human auditory system, namely, the dynamic compressive nonlinearity of the cochlea, the perceived loudness theory and the simultaneous masking properties of the ear. This type of parameter selection scheme results in more noise reduction while limiting the speech distortion. The derived STSA estimator is optimal in the MMSE sense if the prior phase information is available. In practice, however, typically only an estimate of the clean speech phase can be obtained via employing different types of spectral phase estimation techniques which have been developed throughout the last few years. In a blind setup, we have evaluated the proposed Bayesian STSA estimator with different types of standard phase estimation methods available in the literature. Experimental results have shown that the proposed estimator can achieve substantial improvement in performance than the traditional phase-blind approaches.      
### 50.Space-Time Adaptive Processing Using Random Matrix Theory Under Limited Training Samples  [ :arrow_down: ](https://arxiv.org/pdf/2202.04878.pdf)
>  Space-time adaptive processing (STAP) is one of the most effective approaches to suppressing ground clutters in airborne radar systems. It basically takes two forms, i.e., full-dimension STAP (FD-STAP) and reduced-dimension STAP (RD-STAP). When the numbers of clutter training samples are less than two times their respective system degrees-of-freedom (DOF), the performances of both FD-STAP and RD-STAP degrade severely due to inaccurate clutter estimation. To enhance STAP performance under the limited training samples, this paper develops a STAP theory with random matrix theory (RMT). By minimizing the output clutter-plus-noise power, the estimate of the inversion of clutter plus noise covariance matrix (CNCM) can be obtained through optimally manipulating its eigenvalues, and thus producing the optimal STAP weight vector. Two STAP algorithms, FD-STAP using RMT (RMT-FD-STAP) and RD-STAP using RMT (RMT-RD-STAP), are proposed. It is found that both RMT-FD-STAP and RMT-RD-STAP greatly outperform other-related STAP algorithms when the numbers of training samples are larger than their respective clutter DOFs, which are much less than the corresponding system DOFs. Theoretical analyses and simulation demonstrate the effectiveness and the performance advantages of the proposed STAP algorithms.      
### 51.Decreasing Annotation Burden of Pairwise Comparisons with Human-in-the-Loop Sorting: Application in Medical Image Artifact Rating  [ :arrow_down: ](https://arxiv.org/pdf/2202.04823.pdf)
>  Ranking by pairwise comparisons has shown improved reliability over ordinal classification. However, as the annotations of pairwise comparisons scale quadratically, this becomes less practical when the dataset is large. We propose a method for reducing the number of pairwise comparisons required to rank by a quantitative metric, demonstrating the effectiveness of the approach in ranking medical images by image quality in this proof of concept study. Using the medical image annotation software that we developed, we actively subsample pairwise comparisons using a sorting algorithm with a human rater in the loop. We find that this method substantially reduces the number of comparisons required for a full ordinal ranking without compromising inter-rater reliability when compared to pairwise comparisons without sorting.      
### 52.Royalflush Speaker Diarization System for ICASSP 2022 Multi-channel Multi-party Meeting Transcription Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.04814.pdf)
>  This paper describes the Royalflush speaker diarization system submitted to the Multi-channel Multi-party Meeting Transcription Challenge. Our system comprises speech enhancement, overlapped speech detection, speaker embedding extraction, speaker clustering, speech separation and system fusion. In this system, we made three contributions. First, we propose an architecture of combining the multi-channel and U-Net-based models, aiming at utilizing the benefits of these two individual architectures, for far-field overlapped speech detection. Second, in order to use overlapped speech detection model to help speaker diarization, a speech separation based overlapped speech handling approach, in which the speaker verification technique is further applied, is proposed. Third, we explore three speaker embedding methods, and obtained the state-of-the-art performance on the CNCeleb-E test set. With these proposals, our best individual system significantly reduces DER from 15.25% to 6.40%, and the fusion of four systems finally achieves a DER of 6.30% on the far-field Alimeeting evaluation set.      
### 53.SHAS: Approaching optimal Segmentation for End-to-End Speech Translation  [ :arrow_down: ](https://arxiv.org/pdf/2202.04774.pdf)
>  Speech translation models are unable to directly process long audios, like TED talks, which have to be split into shorter segments. Speech translation datasets provide manual segmentations of the audios, which are not available in real-world scenarios, and existing segmentation methods usually significantly reduce translation quality at inference time. To bridge the gap between the manual segmentation of training and the automatic one at inference, we propose Supervised Hybrid Audio Segmentation (SHAS), a method that can effectively learn the optimal segmentation from any manually segmented speech corpus. First, we train a classifier to identify the included frames in a segmentation, using speech representations from a pre-trained wav2vec 2.0. The optimal splitting points are then found by a probabilistic Divide-and-Conquer algorithm that progressively splits at the frame of lowest probability until all segments are below a pre-specified length. Experiments on MuST-C and mTEDx show that the translation of the segments produced by our method approaches the quality of the manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of the manual segmentation's BLEU score, compared to the 87-93% of the best existing methods. Our method is additionally generalizable to different domains and achieves high zero-shot performance in unseen languages.      
### 54.Terrain parameter estimation from proprioceptive sensing of the suspension dynamics in offroad vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2202.04727.pdf)
>  Offroad vehicle movement has to contend with uneven and uncertain terrain which present challenges to path planning and motion control for both manned and unmanned ground vehicles. Knowledge of terrain properties can allow a vehicle to adapt its control and motion planning algorithms. Terrain properties, however, can change on time scales of days or even hours, necessitating their online estimation. The kinematics and, in particular the oscillations experienced by an offroad vehicle carry a signature of the terrain properties. These terrain properties can thus be estimated from proprioceptive sensing of the vehicle dynamics with an appropriate model and estimation algorithm. In this paper, we show that knowledge of the vertical dynamics of a vehicle due to its suspension can enable faster and more accurate estimation of terrain parameters. The paper considers a five degree of freedom model that combines the well known half-car and bicycle models. We show through simulation that the sinkage exponent, a parameter that can significantly influence the wheel forces from the terrain and thus greatly impact the vehicle trajectory, can be estimated from measurements of the vehicle's linear acceleration and rotational velocity, which can be readily obtained from an onboard IMU. We show that modelling the vertical vehicle dynamics can lead to significant improvement in both the estimation of terrain parameters and the prediction of the vehicle trajectory.      
