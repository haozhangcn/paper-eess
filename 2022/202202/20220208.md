# ArXiv eess --Tue, 8 Feb 2022
### 1.AirNN: Neural Networks with Over-the-Air Convolution via Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2202.03399.pdf)
>  Over-the-air analog computation allows offloading computation to the wireless environment through carefully constructed transmitted signals. In this paper, we design and implement the first-of-its-kind over-the-air convolution and demonstrate it for inference tasks in a convolutional neural network (CNN). We engineer the ambient wireless propagation environment through reconfigurable intelligent surfaces (RIS) to design such an architecture, which we call 'AirNN'. AirNN leverages the physics of wave reflection to represent a digital convolution, an essential part of a CNN architecture, in the analog domain. In contrast to classical communication, where the receiver must react to the channel-induced transformation, generally represented as finite impulse response (FIR) filter, AirNN proactively creates the signal reflections to emulate specific FIR filters through RIS. AirNN involves two steps: first, the weights of the neurons in the CNN are drawn from a finite set of channel impulse responses (CIR) that correspond to realizable FIR filters. Second, each CIR is engineered through RIS, and reflected signals combine at the receiver to determine the output of the convolution. This paper presents a proof-of-concept of AirNN by experimentally demonstrating over-the-air convolutions. We then validate the entire resulting CNN model accuracy via simulations for an example task of modulation classification.      
### 2.LEDNet: Joint Low-light Enhancement and Deblurring in the Dark  [ :arrow_down: ](https://arxiv.org/pdf/2202.03373.pdf)
>  Night photography typically suffers from both low light and blurring issues due to the dim environment and the common use of long exposure. While existing light enhancement and deblurring methods could deal with each problem individually, a cascade of such methods cannot work harmoniously to cope well with joint degradation of visibility and textures. Training an end-to-end network is also infeasible as no paired data is available to characterize the coexistence of low light and blurs. We address the problem by introducing a novel data synthesis pipeline that models realistic low-light blurring degradations. With the pipeline, we present the first large-scale dataset for joint low-light enhancement and deblurring. The dataset, LOL-Blur, contains 12,000 low-blur/normal-sharp pairs with diverse darkness and motion blurs in different scenarios. We further present an effective network, named LEDNet, to perform joint low-light enhancement and deblurring. Our network is unique as it is specially designed to consider the synergy between the two inter-connected tasks. Both the proposed dataset and network provide a foundation for this challenging joint task. Extensive experiments demonstrate the effectiveness of our method on both synthetic and real-world datasets.      
### 3.Topological Analysis of Vector-Field Guided Path Following on Manifolds  [ :arrow_down: ](https://arxiv.org/pdf/2202.03343.pdf)
>  A path-following control algorithm enables a system's trajectories under its guidance to converge to and evolve along a given geometric desired path. There exist various such algorithms, but many of them can only guarantee local convergence to the desired path in its neighborhood. In contrast, the control algorithms using a well-designed guiding vector field can ensure almost global convergence of trajectories to the desired path; here, "almost" means that in some cases, a measure-zero set of trajectories converge to the singular set where the vector field becomes zero (with all other trajectories converging to the desired path). In this paper, we first generalize the guiding vector field from the Euclidean space to a general smooth Riemannian manifold. This generalization can deal with path-following in some abstract configuration space (such as robot arm joint space). Then we show several theoretical results from a topological viewpoint. Specifically, we are motivated by the observation that singular points of the guiding vector field exist in many examples where the desired path is homeomorphic to the unit circle, but it is unknown whether the existence of singular points always holds in general (i.e., is inherent in the topology of the desired path). In the $n$-dimensional Euclidean space, we provide an affirmative answer, and conclude that it is not possible to guarantee global convergence to desired paths that are homeomorphic to the unit circle. Furthermore, we show that there always exist \emph{non-path-converging trajectories} (i.e., trajectories that do not converge to the desired path) starting from the boundary of a ball containing the desired path in an $n$-dimensional Euclidean space where $n \ge 3$. Examples are provided to illustrate the theoretical results.      
### 4.A Review of Landcover Classification with Very-High Resolution Remotely Sensed Optical Images-Analysis Unit,Model Scalability and Transferability  [ :arrow_down: ](https://arxiv.org/pdf/2202.03342.pdf)
>  As an important application in remote sensing, landcover classification remains one of the most challenging tasks in very-high-resolution (VHR) image analysis. As the rapidly increasing number of Deep Learning (DL) based landcover methods and training strategies are claimed to be the state-of-the-art, the already fragmented technical landscape of landcover mapping methods has been further complicated. Although there exists a plethora of literature review work attempting to guide researchers in making an informed choice of landcover mapping methods, the articles either focus on the review of applications in a specific area or revolve around general deep learning models, which lack a systematic view of the ever advancing landcover mapping methods. In addition, issues related to training samples and model transferability have become more critical than ever in an era dominated by data-driven approaches, but these issues were addressed to a lesser extent in previous review articles regarding remote sensing classification. Therefore, in this paper, we present a systematic overview of existing methods by starting from learning methods and varying basic analysis units for landcover mapping tasks, to challenges and solutions on three aspects of scalability and transferability with a remote sensing classification focus including (1) sparsity and imbalance of data; (2) domain gaps across different geographical regions; and (3) multi-source and multi-view fusion. We discuss in detail each of these categorical methods and draw concluding remarks in these developments and recommend potential directions for the continued endeavor.      
### 5.Robust Semantic Communications Against Semantic Noise  [ :arrow_down: ](https://arxiv.org/pdf/2202.03338.pdf)
>  Although the semantic communications have exhibited satisfactory performance in a large number of tasks, the impact of semantic noise and the robustness of the systems have not been well investigated. Semantic noise is a particular kind of noise in semantic communication systems, which refers to the misleading between the intended semantic symbols and received ones. In this paper, we first propose a framework for the robust end-to-end semantic communication systems to combat the semantic noise. Particularly, we analyze the causes of semantic noise and propose a practical method to generate it. To remove the effect of semantic noise, adversarial training is proposed to incorporate the samples with semantic noise in the training dataset. Then, the masked autoencoder is designed as the architecture of a robust semantic communication system, where a portion of the input is masked. To further improve the robustness of semantic communication systems, we design a discrete codebook shared by the transmitter and the receiver for encoded feature representation. Thus, the transmitter simply needs to transmit the indices of these features in the codebook. Simulation results show that our proposed method significantly improves the robustness of semantic communication systems against semantic noise with significant reduction on the transmission overhead.      
### 6.Motion-Plane-Adaptive Inter Prediction in 360-Degree Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2202.03323.pdf)
>  Inter prediction is one of the key technologies enabling the high compression efficiency of modern video coding standards. 360-degree video needs to be mapped to the 2D image plane prior to coding in order to allow compression using existing video coding standards. The distortions that inevitably occur when mapping spherical data onto the 2D image plane, however, impair the performance of classical inter prediction techniques. In this paper, we propose a motion-plane-adaptive inter prediction technique (MPA) for 360-degree video that takes the spherical characteristics of 360-degree video into account. Based on the known projection format of the video, MPA allows to perform inter prediction on different motion planes in 3D space instead of having to work on the - in theory arbitrarily mapped - 2D image representation directly. We furthermore derive a motion-plane-adaptive motion vector prediction technique (MPA-MVP) that allows to translate motion information between different motion planes and motion models. Our proposed integration of MPA together with MPA-MVP into the state-of-the-art H.266/VVC video coding standard shows significant Bjontegaard Delta rate savings of 1.72% with a peak of 3.97% based on PSNR and 1.56% with a peak of 3.40% based on WS-PSNR compared to the VTM-14.2 baseline on average.      
### 7.Efficient coverage planning for full-area C-ITS communications based on radio propagation simulation and measurement tools  [ :arrow_down: ](https://arxiv.org/pdf/2202.03282.pdf)
>  Intelligent infrastructure, currently often consisting of C-ITS stations and prospectively supplemented by 5G, is a key-enabler for application-oriented and area-wide realization of highly automated and connected driving. For this, radio coverage along the routes must be ensured, leading to high demands on location- and radio-specific planning and parameterization of roadside units (RSU). Hence, this paper presents efficient planning, measurement and evaluation methods for RSU coverage outlining, allowing economically efficient and technically secured planning of intelligent infrastructure. Necessary scientific technical steps are showcased along a 3.5 km testbed for automated and connected driving in rural environments. First, a radio propagation simulation based on a 3D environment model and its electro-magnetic properties is performed, allowing the examination and optimization of RSU quantity as well as site and antenna selection. Additionally, the necessary calibration of simulation results based on continuous wave (CW) and C-ITS service measurements in both lab-based and real-world scenarios is presented.      
### 8.Detecting Soil Moisture Levels Using Battery-Free Wi-Fi Tag  [ :arrow_down: ](https://arxiv.org/pdf/2202.03275.pdf)
>  Soil sensing plays an important role in increasing agricultural output and protecting soil sites. Existing soil sensing methods failed to achieve both high accuracy and low cost. In this paper, we design and implement a high-accuracy and low cost chipless soil moisture sensing system called SoilTAG. We propose a general chipless sensor design methodology which can allow us to customize the signal feature for sensing soil moisture, instead of blindly capturing the disturbance law of the soil. Based on this principle, we design a battery-free passive tag which can respond to different soil-moisture. Further, we optimize hardware and algorithm design of SoilTAG to locate the passive tag and extract its reflection signal feature to identify soil-moisture using WiFi signals. Extensive experimental results reveal that it can identify 2% absolute soil water content with a sensing distance up to 3m in open field. When the sensing distance is up to 13 m, it can also achieve 5% absolute soil-moisture sensing resolution.      
### 9.Human Activity Recognition Using Tools of Convolutional Neural Networks: A State of the Art Review, Data Sets, Challenges and Future Prospects  [ :arrow_down: ](https://arxiv.org/pdf/2202.03274.pdf)
>  Human Activity Recognition (HAR) plays a significant role in the everyday life of people because of its ability to learn extensive high-level information about human activity from wearable or stationary devices. A substantial amount of research has been conducted on HAR and numerous approaches based on deep learning and machine learning have been exploited by the research community to classify human activities. The main goal of this review is to summarize recent works based on a wide range of deep neural networks architecture, namely convolutional neural networks (CNNs) for human activity recognition. The reviewed systems are clustered into four categories depending on the use of input devices like multimodal sensing devices, smartphones, radar, and vision devices. This review describes the performances, strengths, weaknesses, and the used hyperparameters of CNN architectures for each reviewed system with an overview of available public data sources. In addition, a discussion with the current challenges to CNN-based HAR systems is presented. Finally, this review is concluded with some potential future directions that would be of great assistance for the researchers who would like to contribute to this field.      
### 10.Wave-Controlled Metasurface-Based Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2202.03273.pdf)
>  Reconfigurable Intelligent Surfaces (RISs) are programmable metasurfaces that can adaptively steer received electromagnetic energy in desired directions by employing controllable phase shifting cells. Among other uses, an RIS can modify the propagation environment in order to provide wireless access to user locations that are not otherwise reachable by a base station. Alternatively, an RIS can steer the waves away from particular locations in space, to eliminate interference and allow for co-existence of the wireless network with other types of fixed wireless services (e.g., radars, unlicensed radio bands, etc.). The novel approach in this work is a wave-controlled architecture that properly accounts for the maximum possible change in the local reflection phase that can be achieved by adjacent RIS elements. It obviates the need for dense wiring and signal paths that would be required for individual control of every RIS element, and thus offers a substantial reduction in the required hardware. We specify this wave-controlled RIS architecture in detail and discuss signal processing and machine learning methods that exploit it in both point-to-point and multicell MIMO systems. Such implementations can lead to a dramatic improvement in next-generation wireless, radar, and navigation systems where RIS finds wide applications. They have the potential to improve the efficiency of spectrum utilization and coexistence by orders of magnitude.      
### 11.Spectro Temporal EEG Biomarkers For Binary Emotion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.03271.pdf)
>  Electroencephalogram (EEG) is one of the most reliable physiological signal for emotion detection. Being non-stationary in nature, EEGs are better analysed by spectro temporal representations. Standard features like Discrete Wavelet Transformation (DWT) can represent temporal changes in spectral dynamics of an EEG, but is insufficient to extract information other way around, i.e. spectral changes in temporal dynamics. On the other hand, Empirical mode decomposition (EMD) based features can be useful to bridge the above mentioned gap. Towards this direction, we extract two novel features on top of EMD, namely, (a) marginal hilbert spectrum (MHS) and (b) Holo-Hilbert spectral analysis (HHSA) based on EMD, to better represent emotions in 2D arousal-valence (A-V) space. The usefulness of these features for EEG emotion classification is investigated through extensive experiments using state-of-the-art classifiers. In addition, experiments conducted on DEAP dataset for binary emotion classification in both A-V space, reveal the efficacy of the proposed features over the standard set of temporal and spectral features.      
### 12.Radio Map Estimation: A Data-Driven Approach to Spectrum Cartography  [ :arrow_down: ](https://arxiv.org/pdf/2202.03269.pdf)
>  Radio maps can be utilized to characterize a parameter of interest in a communication channel, such as the received signal strength, at every point of a certain geographical region. This article presents an introductory tutorial to radio map estimation, where radio maps are constructed using spatially distributed measurements. After describing the applications of this kind of maps, this article delves into estimation approaches. Starting by simple regression techniques, gradually more sophisticated algorithms are introduced until reaching state-of-the-art estimators. The presentation of this versatile toolkit is accompanied with toy examples to build up intuition and gain insight into the foundations of radio map estimation. As a secondary objective, this article attempts to reconcile the sometimes conflicting terminology in the literature and to connect multiple bodies of literature and sub-communities that have been working separately in this context.      
### 13.Cyber-resilience for marine navigation by information fusion and change detection  [ :arrow_down: ](https://arxiv.org/pdf/2202.03268.pdf)
>  Cyber-resilience is an increasing concern in developing autonomous navigation solutions for marine vessels. This paper scrutinizes cyber-resilience properties of marine navigation through a prism with three edges: multiple sensor information fusion, diagnosis of not-normal behaviours, and change detection. It proposes a two-stage estimator for diagnosis and mitigation of sensor signals used for coastal navigation. Developing a Likelihood Field approach, a first stage extracts shoreline features from radar and matches them to the electronic navigation chart. A second stage associates buoy and beacon features from the radar with chart information. Using real data logged at sea tests combined with simulated spoofing, the paper verifies the ability to timely diagnose and isolate an attempt to compromise position measurements. A new approach is suggested for high level processing of received data to evaluate their consistency, that is agnostic to the underlying technology of the individual sensory input. A combined parametric Gaussian modelling and Kernel Density Estimation is suggested and compared with a generalized likelihood ratio change detector that uses sliding windows. The paper shows how deviations from nominal behaviour and isolation of the components is possible when under attack or when defects in sensors occur.      
### 14.Team Cogitat at NeurIPS 2021: Benchmarks for EEG Transfer Learning Competition  [ :arrow_down: ](https://arxiv.org/pdf/2202.03267.pdf)
>  Building subject-independent deep learning models for EEG decoding faces the challenge of strong covariate-shift across different datasets, subjects and recording sessions. Our approach to address this difficulty is to explicitly align feature distributions at various layers of the deep learning model, using both simple statistical techniques as well as trainable methods with more representational capacity. This follows in a similar vein as covariance-based alignment methods, often used in a Riemannian manifold context. The methodology proposed herein won first place in the 2021 Benchmarks in EEG Transfer Learning (BEETL) competition, hosted at the NeurIPS conference. The first task of the competition consisted of sleep stage classification, which required the transfer of models trained on younger subjects to perform inference on multiple subjects of older age groups without personalized calibration data, requiring subject-independent models. The second task required to transfer models trained on the subjects of one or more source motor imagery datasets to perform inference on two target datasets, providing a small set of personalized calibration data for multiple test subjects.      
### 15.Paper-based printed CPW-fed antenna for Wi-Fi applications  [ :arrow_down: ](https://arxiv.org/pdf/2202.03266.pdf)
>  A paper-based co-planar waveguide (CPW) fed monopole antenna for Wi-Fi applications is proposed. The antenna is fabricated by printing a commercial silver nanoparticle (Ag NP) based ink on photo paper substrate. The antenna is designed as a single layer for the ease of fabrication, and it is designed to radiate at two frequencies, 2.4 and 5.8 GHz, which are suitable for Wi-Fi applications. The printed film exhibits good electrical conductivity, with a low sheet resistance of 114 m{\Omega}/sq comparable with commonly used conductive metal lines. The fabricated antenna demonstrates good radiative properties at both flat and bent conditions. This confirms the good flexible properties of the antenna making it compatible with mounting on curved surfaces. The performance of the fabricated antenna is also compared with a commercial rigid antenna by interfacing with a USB dongle. The printed antenna demonstrates better performance with respect to signal strength at specific distances when compared with the commercial antenna. This work demonstrates that rigid and long commercial antennas can be replaced with paper-based flexible and cheap antennas and incorporated with wearable technologies. Additionally, replacing Ag NPs with nanowires provides transparency without compromising on the electrical properties.      
### 16.Image-based eeg classification of brain responses to song recordings  [ :arrow_down: ](https://arxiv.org/pdf/2202.03265.pdf)
>  Classifying EEG responses to naturalistic acoustic stimuli is of theoretical and practical importance, but standard approaches are limited by processing individual channels separately on very short sound segments (a few seconds or less). Recent developments have shown classification for music stimuli (~2 mins) by extracting spectral components from EEG and using convolutional neural networks (CNNs). This paper proposes an efficient method to map raw EEG signals to individual songs listened for end-to-end classification. EEG channels are treated as a dimension of a [Channel x Sample] image tile, and images are classified using CNNs. Our experimental results (88.7%) compete with state-of-the-art methods (85.0%), yet our classification task is more challenging by processing longer stimuli that were similar to each other in perceptual quality, and were unfamiliar to participants. We also adopt a transfer learning scheme using a pre-trained ResNet-50, confirming the effectiveness of transfer learning despite image domains unrelated from each other.      
### 17.Short-term Multi-horizon Residential Electric Load Forecasting using Deep Learning and Signal Decomposition Methods  [ :arrow_down: ](https://arxiv.org/pdf/2202.03264.pdf)
>  With the booming growth of advanced digital technologies, it has become possible for users as well as distributors of energy to obtain detailed and timely information about the electricity consumption of households. These technologies can also be used to forecast the household's electricity consumption (a.k.a. the load). In this paper, we investigate the use of Variational Mode Decomposition and deep learning techniques to improve the accuracy of the load forecasting problem. Although this problem has been studied in the literature, selecting an appropriate decomposition level and a deep learning technique providing better forecasting performance have garnered comparatively less attention. This study bridges this gap by studying the effect of six decomposition levels and five distinct deep learning networks. The raw load profiles are first decomposed into intrinsic mode functions using the Variational Mode Decomposition in order to mitigate their non-stationary aspect. Then, day, hour, and past electricity consumption data are fed as a three-dimensional input sequence to a four-level Wavelet Decomposition Network model. Finally, the forecast sequences related to the different intrinsic mode functions are combined to form the aggregate forecast sequence. The proposed method was assessed using load profiles of five Moroccan households from the Moroccan buildings' electricity consumption dataset (MORED) and was benchmarked against state-of-the-art time-series models and a baseline persistence model.      
### 18.Weakly Supervised Indoor Localization via Manifold Matching  [ :arrow_down: ](https://arxiv.org/pdf/2202.03239.pdf)
>  Inferring the location of a mobile device in an indoor setting is an open problem of utmost significance. A leading approach that does not require the deployment of expensive infrastructure is fingerprinting, where a classifier is trained to predict the location of a device based on its captured signal. The main caveat of this approach is that acquiring a sufficiently large and accurate training set may be prohibitively expensive. Here, we propose a weakly supervised method that only requires the location of a small number of devices. The localization is done by matching a low-dimensional spectral representation of the signals to a given sketch of the indoor environment. We test our approach on simulated and real data and show that it yields an accuracy of a few meters, which is on par with fully supervised approaches. The simplicity of our method and its accuracy with minimal supervision makes it ideal for implementation in indoor localization systems.      
### 19.Passive learning to address nonstationarity in virtual flow metering applications  [ :arrow_down: ](https://arxiv.org/pdf/2202.03236.pdf)
>  Steady-state process models are common in virtual flow meter applications due to low computational complexity, and low model development and maintenance cost. Nevertheless, the prediction performance of steady-state models typically degrades with time due to the inherent nonstationarity of the underlying process being modeled. Few studies have investigated how learning methods can be applied to sustain the prediction accuracy of steady-state virtual flow meters. This paper explores passive learning, where the model is frequently calibrated to new data, as a way to address nonstationarity and improve long-term performance. An advantage with passive learning is that it is compatible with models used in the industry. Two passive learning methods, periodic batch learning and online learning, are applied with varying calibration frequency to train virtual flow meters. Six different model types, ranging from data-driven to first-principles, are trained on historical production data from 10 petroleum wells. The results are two-fold: first, in the presence of frequently arriving measurements, frequent model updating sustains an excellent prediction performance over time; second, in the presence of intermittent and infrequently arriving measurements, frequent updating in addition to the utilization of expert knowledge is essential to increase the performance accuracy. The investigation may be of interest to experts developing soft-sensors for nonstationary processes, such as virtual flow meters.      
### 20.HERMES: Hybrid Error-corrector Model with inclusion of External Signals for nonstationary fashion time series  [ :arrow_down: ](https://arxiv.org/pdf/2202.03224.pdf)
>  Developing models and algorithms to draw causal inference for time series is a long standing statistical problem. It is crucial for many applications, in particular for fashion or retail industries, to make optimal inventory decisions and avoid massive wastes. By tracking thousands of fashion trends on social media with state-of-the-art computer vision approaches, we propose a new model for fashion time series forecasting. Our contribution is twofold. We first provide publicly the first fashion dataset gathering 10000 weekly fashion time series. As influence dynamics are the key of emerging trend detection, we associate with each time series an external weak signal representing behaviors of influencers. Secondly, to leverage such a complex and rich dataset, we propose a new hybrid forecasting model. Our approach combines per-time-series parametric models with seasonal components and a global recurrent neural network to include sporadic external signals. This hybrid model provides state-of-the-art results on the proposed fashion dataset, on the weekly time series of the M4 competition \cite{makridakis2018m4}, and illustrates the benefit of the contribution of external weak signals.      
### 21.Semantic-aware Speech to Text Transmission with Redundancy Removal  [ :arrow_down: ](https://arxiv.org/pdf/2202.03211.pdf)
>  Deep learning (DL) based semantic communication methods have been explored for the efficient transmission of images, text, and speech in recent years. In contrast to traditional wireless communication methods that focus on the transmission of abstract symbols, semantic communication approaches attempt to achieve better transmission efficiency by only sending the semantic-related information of the source data. In this paper, we consider semantic-oriented speech to text transmission. We propose a novel end-to-end DL-based transceiver, which includes an attention-based soft alignment module and a redundancy removal module to compress the transmitted data. In particular, the former extracts only the text-related semantic features, and the latter further drops the semantically redundant content, greatly reducing the amount of semantic redundancy compared to existing methods. We also propose a two-stage training scheme, which speeds up the training of the proposed DL model. The simulation results indicate that our proposed method outperforms current methods in terms of the accuracy of the received text and transmission efficiency. Moreover, the proposed method also has a smaller model size and shorter end-to-end runtime.      
### 22.Quasi-Monostatic Antenna Displacement in Radar Target Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2202.03210.pdf)
>  Radar target simulators (RTS) have recently drawn much attention in research and commercial development, as they are capable of performing over-the-air validation tests under laboratory conditions by generating virtual radar echoes that are perceived as targets by a radar under test (RuT). The estimated angle of arrival (AoA) of such a virtual target is determined by the physical position of the particular RTS channel that creates it, which must therefore be considered when planning the setup. A single channel employs two antennas, one for the reception and the other for the re-transmission of the incoming radar signal. The antennas are positioned close together, but still spatially separated, thus an RTS channel can be considered quasi-monostatic, which causes non-negligible inaccuracies in the angle simulation. In this paper, the authors examine the analytical implications of this systemic deficiency on the angle estimation, which provides support for the design and setup of angle-simulating RTS systems. The mathematical derivations developed are verified by measurement.      
### 23.T-NGA: Temporal Network Grafting Algorithm for Learning to Process Spiking Audio Sensor Events  [ :arrow_down: ](https://arxiv.org/pdf/2202.03204.pdf)
>  Spiking silicon cochlea sensors encode sound as an asynchronous stream of spikes from different frequency channels. The lack of labeled training datasets for spiking cochleas makes it difficult to train deep neural networks on the outputs of these sensors. This work proposes a self-supervised method called Temporal Network Grafting Algorithm (T-NGA), which grafts a recurrent network pretrained on spectrogram features so that the network works with the cochlea event features. T-NGA training requires only temporally aligned audio spectrograms and event features. Our experiments show that the accuracy of the grafted network was similar to the accuracy of a supervised network trained from scratch on a speech recognition task using events from a software spiking cochlea model. Despite the circuit non-idealities of the spiking silicon cochlea, the grafted network accuracy on the silicon cochlea spike recordings was only about 5% lower than the supervised network accuracy using the N-TIDIGITS18 dataset. T-NGA can train networks to process spiking audio sensor events in the absence of large labeled spike datasets.      
### 24.Two-Dimensional Arbitrary Angle of Arrival in Radar Target Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2202.03203.pdf)
>  Automotive radar sensors play a key role in the current development of advanced driver assistance systems (ADAS). Their ability to detect objects even under adverse weather conditions makes them indispensable for environment-sensing tasks in autonomous vehicles. Since an operational failure presents a potential risk to human life, thorough and practical validation testing must be performed, requiring an integrative test solution. Radar target simulators (RTS) are capable of performing over-the-air validation tests by generating virtual radar echoes that are perceived as targets by the radar under test (RuT). Since the authenticity and credibility of these targets is based on the accuracy with which they are created, their simulated position must be arbitrarily adjustable. In this work, an existing approach to synthesize virtual radar targets at an arbitrary angle of arrival (AoA) is extended to cover both, the azimuth and elevation domain. The concept is based on the superposition of the returning signals from four neighboring RTS channels. A theoretical model describing the basic principle and its constraints is developed. In addition, a measurement campaign is conducted to verify the practical functionality of the proposed approach.      
### 25.On discretization of continuous-time LPV control solutions  [ :arrow_down: ](https://arxiv.org/pdf/2202.03177.pdf)
>  In recent years, the Linear Parameter-Varying (LPV) framework has become increasingly useful for analysis and control of time-varying systems. Generally, LPV control synthesis is performed in the continuous-time (CT) domain due to significantly less complex stability and performance requirements, see \cite{zhou1998essentials}. The main complication of CT synthesis approaches is implementation of the CT LPV control solutions on physical hardware. In literature, several discretization methods have been explored for LPV systems, see \cite{Tothbook}. However, these approaches necessitate for heavy nonlinear operations introduced by the discretization of these time-varying matrices, thereby severely limiting implementation capabilities of the CT LPV control solutions. Alternatively, the $w'$ discretization approach illustrates to be a promising research direction for LPV systems, since it allows for preservation of the CT system matrices in the LTI case, see \cite{whitbeck1978digital}. Moreover, this manuscript aims at extending the $w'$ discretization approach towards LPV systems, such that implementation of CT LPV control solutions on physical hardware is simplified.      
### 26.Neural Network based Inter bi-prediction Blending  [ :arrow_down: ](https://arxiv.org/pdf/2202.03149.pdf)
>  This paper presents a learning-based method to improve bi-prediction in video coding. In conventional video coding solutions, the motion compensation of blocks from already decoded reference pictures stands out as the principal tool used to predict the current frame. Especially, the bi-prediction, in which a block is obtained by averaging two different motion-compensated prediction blocks, significantly improves the final temporal prediction accuracy. In this context, we introduce a simple neural network that further improves the blending operation. A complexity balance, both in terms of network size and encoder mode selection, is carried out. Extensive tests on top of the recently standardized VVC codec are performed and show a BD-rate improvement of -1.4% in random access configuration for a network size of fewer than 10k parameters. We also propose a simple CPU-based implementation and direct network quantization to assess the complexity/gains tradeoff in a conventional codec framework.      
### 27.Building Synthetic Speaker Profiles in Text-to-Speech Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.03125.pdf)
>  The diversity of speaker profiles in multi-speaker TTS systems is a crucial aspect of its performance, as it measures how many different speaker profiles TTS systems could possibly synthesize. However, this important aspect is often overlooked when building multi-speaker TTS systems and there is no established framework to evaluate this diversity. The reason behind is that most multi-speaker TTS systems are limited to generate speech signals with the same speaker profiles as its training data. They often use discrete speaker embedding vectors which have a one-to-one correspondence with individual speakers. This correspondence limits TTS systems and hinders their capability of generating unseen speaker profiles that did not appear during training. In this paper, we aim to build multi-speaker TTS systems that have a greater variety of speaker profiles and can generate new synthetic speaker profiles that are different from training data. To this end, we propose to use generative models with a triplet loss and a specific shuffle mechanism. In our experiments, the effectiveness and advantages of the proposed method have been demonstrated in terms of both the distinctiveness and intelligibility of synthesized speech signals.      
### 28.Control of cascading failures in dynamical models of power grids  [ :arrow_down: ](https://arxiv.org/pdf/2202.03056.pdf)
>  In this paper, we introduce a distributed control strategy to prevent dynamically-induced cascading failures in power grids. We model power grids using complex networks and nonlinear dynamics to provide a coarse-grained description of the electro-mechanical phenomena taking place on them (in particular, we use coupled swing equations) and restrict our analysis to cascades of line failures, i.e., failures due to power flows exceeding the maximum capacity of a line. We formulate a distributed control protocol relying on the same topology of the physical layer and apply it to several power grid models, including a small-size illustrative example with five nodes, the Italian high-voltage (380kV) power grid, and the IEEE 118-bus system. Our results indicate that the approach is capable of preventing cascading failures, either controlling each node of the network or a suitable subset of them.      
### 29.A comprehensive benchmark analysis for sand dust image reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2202.03031.pdf)
>  Numerous sand dust image enhancement algorithms have been proposed in recent years. To our best acknowledge, however, most methods evaluated their performance with no-reference way using few selected real-world images from internet. It is unclear how to quantitatively analysis the performance of the algorithms in a supervised way and how we could gauge the progress in the field. Moreover, due to the absence of large-scale benchmark datasets, there are no well-known reports of data-driven based method for sand dust image enhancement up till now. To advance the development of deep learning-based algorithms for sand dust image reconstruction, while enabling supervised objective evaluation of algorithm performance. In this paper, we presented a comprehensive perceptual study and analysis of real-world sand dust images, then constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training Convolutional Neural Networks (CNNs) and evaluating algorithms performance. In addition, we adopted the existing image transformation neural network trained on SIRB as baseline to illustrate the generalization of SIRB for training CNNs. Finally, we conducted the qualitative and quantitative evaluation to demonstrate the performance and limitations of the state-of-the-arts (SOTA), which shed light on future research in sand dust image reconstruction.      
### 30.EDCHO: High Order Exact Dynamic Consensus  [ :arrow_down: ](https://arxiv.org/pdf/2202.03012.pdf)
>  This article addresses the problem of average consensus in a multi-agent system when the desired consensus quantity is a time varying signal. Although this problem has been addressed in existing literature by linear schemes, only bounded steady-state errors have been achieved. Other approaches have used first order sliding modes to achieve zero steady-state error, but suffer from the chattering effect. In this work, we propose a new exact dynamic consensus algorithm which leverages high order sliding modes, in the form of a distributed differentiator to achieve zero steady-state error of the average of time varying reference signals in a group of agents. Moreover, our proposal is also able to achieve consensus to high order derivatives of the average signal, if desired. An in depth formal study on the stability and convergence for EDCHO is provided for undirected connected graphs. Finally, the effectiveness and advantages of our proposal are shown with concrete simulation scenarios.      
### 31.Deep Residual Shrinkage Networks for EMG-based Gesture Identification  [ :arrow_down: ](https://arxiv.org/pdf/2202.02984.pdf)
>  This work introduces a method for high-accuracy EMG based gesture identification. A newly developed deep learning method, namely, deep residual shrinkage network is applied to perform gesture identification. Based on the feature of EMG signal resulting from gestures, optimizations are made to improve the identification accuracy. Finally, three different algorithms are applied to compare the accuracy of EMG signal recognition with that of DRSN. The result shows that DRSN excel traditional neural networks in terms of EMG recognition accuracy. This paper provides a reliable way to classify EMG signals, as well as exploring possible applications of DRSN.      
### 32.SUD: Supervision by Denoising for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.02952.pdf)
>  Training a fully convolutional network for semantic segmentation typically requires a large, labeled dataset with little label noise if good generalization is to be guaranteed. For many segmentation problems, however, data with pixel- or voxel-level labeling accuracy are scarce due to the cost of manual labeling. This problem is exacerbated in domains where manual annotation is difficult, resulting in large amounts of variability in the labeling even across domain experts. Therefore, training segmentation networks to generalize better by learning from both labeled and unlabeled images (called semi-supervised learning) is problem of both practical and theoretical interest. However, traditional semi-supervised learning methods for segmentation often necessitate hand-crafting a differentiable regularizer specific to a given segmentation problem, which can be extremely time-consuming. In this work, we propose "supervision by denoising" (SUD), a framework that enables us to supervise segmentation models using their denoised output as targets. SUD unifies temporal ensembling and spatial denoising techniques under a spatio-temporal denoising framework and alternates denoising and network weight update in an optimization framework for semi-supervision. We validate SUD on three tasks-kidney and tumor (3D), and brain (3D) segmentation, and cortical parcellation (2D)-demonstrating a significant improvement in the Dice overlap and the Hausdorff distance of segmentations over supervised-only and temporal ensemble baselines.      
### 33.Deep Deterministic Independent Component Analysis for Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2202.02951.pdf)
>  We develop a new neural network based independent component analysis (ICA) method by directly minimizing the dependence amongst all extracted components. Using the matrix-based R{Ã©}nyi's $\alpha$-order entropy functional, our network can be directly optimized by stochastic gradient descent (SGD), without any variational approximation or adversarial training. As a solid application, we evaluate our ICA in the problem of hyperspectral unmixing (HU) and refute a statement that "\emph{ICA does not play a role in unmixing hyperspectral data}", which was initially suggested by~\cite{nascimento2005does}. Code and additional remarks of our DDICA is available at <a class="link-external link-https" href="https://github.com/hongmingli1995/DDICA" rel="external noopener nofollow">this https URL</a>.      
### 34.Inter-subject Contrastive Learning for Subject Adaptive EEG-based Visual Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.02901.pdf)
>  This paper tackles the problem of subject adaptive EEG-based visual recognition. Its goal is to accurately predict the categories of visual stimuli based on EEG signals with only a handful of samples for the target subject during training. The key challenge is how to appropriately transfer the knowledge obtained from abundant data of source subjects to the subject of interest. To this end, we introduce a novel method that allows for learning subject-independent representation by increasing the similarity of features sharing the same class but coming from different subjects. With the dedicated sampling principle, our model effectively captures the common knowledge shared across different subjects, thereby achieving promising performance for the target subject even under harsh problem settings with limited data. Specifically, on the EEG-ImageNet40 benchmark, our model records the top-1 / top-3 test accuracy of 72.6% / 91.6% when using only five EEG samples per class for the target subject. Our code is available at <a class="link-external link-https" href="https://github.com/DeepBCI/Deep-BCI/tree/master/1_Intelligent_BCI/Inter_Subject_Contrastive_Learning_for_EEG" rel="external noopener nofollow">this https URL</a>.      
### 35.On Using Transformers for Speech-Separation  [ :arrow_down: ](https://arxiv.org/pdf/2202.02884.pdf)
>  Transformers have enabled major improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we have proposed SepFormer, which uses self-attention and obtains state-of-the art results on WSJ0-2/3 Mix datasets for speech separation. In this paper, we extend our previous work by providing results on more datasets including LibriMix, and WHAM!, WHAMR! which include noisy and noisy-reverberant conditions. Moreover we provide denoising, and denoising+dereverberation results in the context of speech enhancement, respectively on WHAM! and WHAMR! datasets. We also investigate incorporating recently proposed efficient self-attention mechanisms inside the SepFormer model, and show that by using efficient self-attention mechanisms it is possible to reduce the memory requirements significantly while performing better than the popular convtasnet model on WSJ0-2Mix dataset.      
### 36.Deep Convolutional Learning-Aided Detector for Generalized Frequency Division Multiplexing with Index Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2202.02876.pdf)
>  In this paper, a deep convolutional neural network-based symbol detection and demodulation is proposed for generalized frequency division multiplexing with index modulation (GFDM-IM) scheme in order to improve the error performance of the system. The proposed method first pre-processes the received signal by using a zero-forcing (ZF) detector and then uses a neural network consisting of a convolutional neural network (CNN) followed by a fully-connected neural network (FCNN). The FCNN part uses only two fully-connected layers, which can be adapted to yield a trade-off between complexity and bit error rate (BER) performance. This two-stage approach prevents the getting stuck of neural network in a saddle point and enables IM blocks processing independently. It has been demonstrated that the proposed deep convolutional neural network-based detection and demodulation scheme provides better BER performance compared to ZF detector with a reasonable complexity increase. We conclude that non-orthogonal waveforms combined with IM schemes with the help of deep learning is a promising physical layer (PHY) scheme for future wireless networks      
### 37.Towards Modeling Human Motor Learning Dynamics in High-Dimensional Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2202.02863.pdf)
>  Designing effective rehabilitation strategies for upper extremities, particularly hands and fingers, warrants the need for a computational model of human motor learning. The presence of large degrees of freedom (DoFs) available in these systems makes it difficult to balance the trade-off between learning the full dexterity and accomplishing manipulation goals. The motor learning literature argues that humans use motor synergies to reduce the dimension of control space. Using the low-dimensional space spanned by these synergies, we develop a computational model based on the internal model theory of motor control. We analyze the proposed model in terms of its convergence properties and fit it to the data collected from human experiments. We compare the performance of the fitted model to the experimental data and show that it captures human motor learning behavior well.      
### 38.Deep Learning-Aided Spatial Multiplexing with Index Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2202.02856.pdf)
>  In this paper, deep learning (DL)-aided data detection of spatial multiplexing (SMX) multiple-input multiple-output (MIMO) transmission with index modulation (IM) (Deep-SMX-IM) has been proposed. Deep-SMX-IM has been constructed by combining a zero-forcing (ZF) detector and DL technique. The proposed method uses the significant advantages of DL techniques to learn transmission characteristics of the frequency and spatial domains. Furthermore, thanks to using subblockbased detection provided by IM, Deep-SMX-IM is a straightforward method, which eventually reveals reduced complexity. It has been shown that Deep-SMX-IM has significant error performance gains compared to ZF detector without increasing computational complexity for different system configurations.      
### 39.CheXstray: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging AI  [ :arrow_down: ](https://arxiv.org/pdf/2202.02833.pdf)
>  Rapidly expanding Clinical AI applications worldwide have the potential to impact to all areas of medical practice. Medical imaging applications constitute a vast majority of approved clinical AI applications. Though healthcare systems are eager to adopt AI solutions a fundamental question remains: \textit{what happens after the AI model goes into production?} We use the CheXpert and PadChest public datasets to build and test a medical imaging AI drift monitoring workflow that tracks data and model drift without contemporaneous ground truth. We simulate drift in multiple experiments to compare model performance with our novel multi-modal drift metric, which uses DICOM metadata, image appearance representation from a variational autoencoder (VAE), and model output probabilities as input. Through experimentation, we demonstrate a strong proxy for ground truth performance using unsupervised distributional shifts in relevant metadata, predicted probabilities, and VAE latent representation. Our key contributions include (1) proof-of-concept for medical imaging drift detection including use of VAE and domain specific statistical methods (2) a multi-modal methodology for measuring and unifying drift metrics (3) new insights into the challenges and solutions for observing deployed medical imaging AI (4) creation of open-source tools enabling others to easily run their own workflows or scenarios. This work has important implications for addressing the translation gap related to continuous medical imaging AI model monitoring in dynamic healthcare environments.      
### 40.Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.02832.pdf)
>  Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we utilise a modified variational autoencoder to uncover skin tone bias in datasets commonly used as benchmarks. We propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use two leading bias unlearning techniques to mitigate skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that unlearning skin tone improves generalisation and can reduce the performance disparity between melanoma detection in lighter and darker skin tones.      
### 41.Automated Vehicle Safety Guarantee, Verification and Certification: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2202.02818.pdf)
>  Challenges related to automated driving are no longer focused on just the construction of such automated vehicles (AVs), but in assuring the safety of their operation. Recent advances in Level 3 and Level 4 autonomous driving have motivated more extensive study in safety guarantees of complicated AV maneuvers, which aligns with the goal of ISO 21448 (Safety of the Intended Functions, or SOTIF), i.e. minimizing unsafe scenarios both known and unknown, as well as Vision Zero -- eliminating highway fatalities by 2050. A majority of approaches used in providing safety guarantees for AV motion control originate from formal methods, especially reachability analysis (RA), which relies on mathematical models for the dynamic evolution of the system to provide guarantees. However, to the best of the authors' knowledge, there have been no review papers dedicated to describing and interpreting state-of-the-art of formal methods in the context of AVs. In this work, we provide both an overview of the safety verification, validation and certification process, as well as review formal safety techniques that are best suited to AV applications. We also propose a unified scenario coverage framework that can provide either a formal or sample-based estimate of safety verification for full AVs.      
### 42.Learning Sparse Graphs via Majorization-Minimization for Smooth Node Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.02815.pdf)
>  In this letter, we propose an algorithm for learning a sparse weighted graph by estimating its adjacency matrix under the assumption that the observed signals vary smoothly over the nodes of the graph. The proposed algorithm is based on the principle of majorization-minimization (MM), wherein we first obtain a tight surrogate function for the graph learning objective and then solve the resultant surrogate problem which has a simple closed form solution. The proposed algorithm does not require tuning of any hyperparameter and it has the desirable feature of eliminating the inactive variables in the course of the iterations - which can help speeding up the algorithm. The numerical simulations conducted using both synthetic and real world (brain-network) data show that the proposed algorithm converges faster, in terms of the average number of iterations, than several existing methods in the literature.      
### 43.Wave-Encoded Model-based Deep Learning for Highly Accelerated Imaging with Joint Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2202.02814.pdf)
>  Purpose: To propose a wave-encoded model-based deep learning (wave-MoDL) strategy for highly accelerated 3D imaging and joint multi-contrast image reconstruction, and further extend this to enable rapid quantitative imaging using an interleaved look-locker acquisition sequence with T2 preparation pulse (3D-QALAS). <br>Method: Recently introduced MoDL technique successfully incorporates convolutional neural network (CNN)-based regularizers into physics-based parallel imaging reconstruction using a small number of network parameters. Wave-CAIPI is an emerging parallel imaging method that accelerates the imaging speed by employing sinusoidal gradients in the phase- and slice-encoding directions during the readout to take better advantage of 3D coil sensitivity profiles. In wave-MoDL, we propose to combine the wave-encoding strategy with unrolled network constraints to accelerate the acquisition speed while enforcing wave-encoded data consistency. We further extend wave-MoDL to reconstruct multi-contrast data with controlled aliasing in parallel imaging (CAIPI) sampling patterns to leverage similarity between multiple images to improve the reconstruction quality. <br>Result: Wave-MoDL enables a 47-second MPRAGE acquisition at 1 mm resolution at 16-fold acceleration. For quantitative imaging, wave-MoDL permits a 2-minute acquisition for T1, T2, and proton density mapping at 1 mm resolution at 12-fold acceleration, from which contrast weighted images can be synthesized as well. <br>Conclusion: Wave-MoDL allows rapid MR acquisition and high-fidelity image reconstruction and may facilitate clinical and neuroscientific applications by incorporating unrolled neural networks into wave-CAIPI reconstruction.      
### 44.Perceptual Coding for Compressed Video Understanding: A New Framework and Benchmark  [ :arrow_down: ](https://arxiv.org/pdf/2202.02813.pdf)
>  Most video understanding methods are learned on high-quality videos. However, in most real-world scenarios, the videos are first compressed before the transportation and then decompressed for understanding. The decompressed videos are degraded in terms of perceptual quality, which may degenerate the downstream tasks. To address this issue, we propose the first coding framework for compressed video understanding, where another learnable perceptual bitstream is introduced and simultaneously transported with the video bitstream. With the sophisticatedly designed optimization target and network architectures, this new stream largely boosts the perceptual quality of the decoded videos yet with a small bit cost. Our framework can enjoy the best of both two worlds, (1) highly efficient content-coding of industrial video codec and (2) flexible perceptual-coding of neural networks (NNs). Finally, we build a rigorous benchmark for compressed video understanding over four different compression levels, six large-scale datasets, and two popular tasks. The proposed Dual-bitstream Perceptual Video Coding framework Dual-PVC consistently demonstrates significantly stronger performances than the baseline codec under the same bitrate level.      
### 45.An evaluation of the data space dimension in phase retrieval: results in Fresnel zone  [ :arrow_down: ](https://arxiv.org/pdf/2202.02809.pdf)
>  In this paper, we address the problem of computing the dimension of data space in phase retrieval problem. Starting from the quadratic formulation of the phase retrieval,the analysis is performed in two steps. First, we exploit the lifting technique to obtain a linear representation of the data. Later, we evaluate the dimension of data space by computing analytically the number of relevant singular values of the linear operator that represents the data. The study is done with reference to a 2D scalar geometry consisting of an electric current strip whose square amplitude of the electric radiated field is observed on a twodimensional extended domain in Fresnel zone.      
### 46.Formulating Connectedness in Security-Constrained Optimal Transmission Switching Problems  [ :arrow_down: ](https://arxiv.org/pdf/2202.02805.pdf)
>  This paper focuses on the issue of network connectedness (NC) in security-constrained optimal transmission switching problems, which is complicated by branch contingencies and corrective line switching. Two criteria are firstly proposed with the principle of preserving NC as much as possible within reasonable limits. By extending the electrical flow based NC constraints, a proposition is derived to associate different cases of NC with the optimum of a linear program, yielding the mathematical formulation of the NC criteria. By Karush-Kuhn-Tucker conditions, this formulation is further transformed into a tractable version which can be incorporated with existing SCOTS models without affecting the applicability of original solution approaches. Finally, case studies on various networks and SCOTS models demonstrate the efficacy of the proposed approach.      
### 47.On Smart Gaze based Annotation of Histopathology Images for Training of Deep Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.02764.pdf)
>  Unavailability of large training datasets is a bottleneck that needs to be overcome to realize the true potential of deep learning in histopathology applications. Although slide digitization via whole slide imaging scanners has increased the speed of data acquisition, labeling of virtual slides requires a substantial time investment from pathologists. Eye gaze annotations have the potential to speed up the slide labeling process. This work explores the viability and timing comparisons of eye gaze labeling compared to conventional manual labeling for training object detectors. Challenges associated with gaze based labeling and methods to refine the coarse data annotations for subsequent object detection are also discussed. Results demonstrate that gaze tracking based labeling can save valuable pathologist time and delivers good performance when employed for training a deep object detector. Using the task of localization of Keratin Pearls in cases of oral squamous cell carcinoma as a test case, we compare the performance gap between deep object detectors trained using hand-labelled and gaze-labelled data. On average, compared to `Bounding-box' based hand-labeling, gaze-labeling required $57.6\%$ less time per label and compared to `Freehand' labeling, gaze-labeling required on average $85\%$ less time per label.      
### 48.Node-wise monotone barrier coupling law for central pattern generation  [ :arrow_down: ](https://arxiv.org/pdf/2202.02759.pdf)
>  With the aim of providing new insights into the mechanisms behind the emergence of collective behaviors via nonlinear coupling, we propose a node-wise monotone barrier coupling law that imitates the behavior and beneficial properties of neural central pattern generators (CPGs). In particular, the coupling law 1) allows us to assign multiple central patterns on the circle and 2) allows for rapid switching between different patterns via simple `kicks'. In the end, we achieve full control by partitioning the state space by utilizing a barrier effect and assigning unique steady-state behaviors to each element of the resulting partition. We analyze the global behavior and study the viability of the design.      
### 49.3D Map Reconstruction of an Orchard using an Angle-Aware Covering Control Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2202.02758.pdf)
>  In the last years, unmanned aerial vehicles are becoming a reality in the context of precision agriculture, mainly for monitoring, patrolling and remote sensing tasks, but also for 3D map reconstruction. In this paper, we present an innovative approach where a fleet of unmanned aerial vehicles is exploited to perform remote sensing tasks over an apple orchard for reconstructing a 3D map of the field, formulating the covering control problem to combine the position of a monitoring target and the viewing angle. Moreover, the objective function of the controller is defined by an importance index, which has been computed from a multi-spectral map of the field, obtained by a preliminary flight, using a semantic interpretation scheme based on a convolutional neural network. This objective function is then updated according to the history of the past coverage states, thus allowing the drones to take situation-adaptive actions. The effectiveness of the proposed covering control strategy has been validated through simulations on a Robot Operating System.      
### 50.Exponentially Stable Adaptive Control of MIMO Systems with Unknown Control Matrix  [ :arrow_down: ](https://arxiv.org/pdf/2202.02749.pdf)
>  The scope of this research is a problem of the direct model reference adaptive control of linear time-invariant multi-input multi-output (MIMO) plants without any a priori knowledge about system matrices. To handle it, a new method is proposed, which includes three main stages. Firstly, using the well-known DREM procedure, the plant parametrization is made to obtain the linear regressions, in which the plant matrices and state initial conditions are the unknown parameters. Secondly, such regressions are substituted into the known equations for the controller parameters calculation. Thirdly, the controller parameters are identified using the novel adjustment law with exponential rate of convergence. To the best of the authors knowledge, such a method is the first one to provide the following features simultaneously: 1) it is applicable for the generic completely unknown MIMO systems (e.g. without any information about state or control allocation matrices, the sign of the latter, etc.); 2) it guarantees the exponential convergence of both the parameter and tracking errors under the mild requirement of the regressor finite excitation; 3) it ensures monotonicity of the transient curves of the control law parameters matrices. The results of the conducted experiments with the model of a rubber and ailerons control of a small passenger aircraft corroborate all the theoretical results.      
### 51.Diffractive deep neural network based adaptive optics scheme for vortex beam in oceanic turbulence  [ :arrow_down: ](https://arxiv.org/pdf/2202.02732.pdf)
>  Vortex beam carrying orbital angular momentum (OAM) is disturbed by oceanic turbulence (OT) when propagating in underwater wireless optical communication (UWOC) system. Adaptive optics (AO) is used to compensate for distortion and improve the performance of the UWOC system. In this work, we propose a diffractive deep neural network (DDNN) based AO scheme to compensate for the distortion caused by OT, where the DDNN is trained to obtain the mapping between the distortion intensity distribution of the vortex beam and its corresponding phase screen representating OT. The intensity pattern of the distorted vortex beam obtained in the experiment is input to the DDNN model, and the predicted phase screen can be used to compensate the distortion in real time. The experiment results show that the proposed scheme can extract quickly the characteristics of the intensity pattern of the distorted vortex beam, and output accurately the predicted phase screen. The mode purity of the compensated vortex beam is significantly improved, even with a strong OT. Our scheme may provide a new avenue for AO techniques, and is expected to promote the communication quality of UWOC system.      
### 52.Hydrogen and Battery Storage Technologies for Low Cost Energy Decarbonization in Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.02711.pdf)
>  Deep energy decarbonization cannot be achieved without high penetration of renewables. At higher renewable energy penetrations, the variability and intermittent nature of solar photovoltaic (PV) electricity can cause ramping issues with existing fossil fuel generation, requiring longer term energy storage to increase the reliability of grid operation. A proton exchange membrane electrolyzer can produce H2and serves as a utility controllable load. The produced H2 can then be stored and converted back into electricity, or mixed with natural gas, or used as transportation fuel, or chemical feedstock. This paper considers the perspective of the distribution system operator that operates the distributed energy resources on a standard IEEE 33-node distribution network considering the technical and physical constraints with the goal of minimizing total investment and operation cost. Different case studies, at very high PV penetrations are considered to show the challenges and path to net-zero emission energy production using H2 energy. Sensitivity of utility PV costs and electrolyzer capital costs on producing H2 at $1/kg are presented showing that the distribution network could produce 100% renewable electricity and H2 could be produced with the same price by 2050 with conservative cost estimates and by 2030 with accelerated cost declines.      
### 53.Hyper-Convolutions via Implicit Kernels for Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2202.02701.pdf)
>  The convolutional neural network (CNN) is one of the most commonly used architectures for computer vision tasks. The key building block of a CNN is the convolutional kernel that aggregates information from the pixel neighborhood and shares weights across all pixels. A standard CNN's capacity, and thus its performance, is directly related to the number of learnable kernel weights, which is determined by the number of channels and the kernel size (support). In this paper, we present the \textit{hyper-convolution}, a novel building block that implicitly encodes the convolutional kernel using spatial coordinates. Hyper-convolutions decouple kernel size from the total number of learnable parameters, enabling a more flexible architecture design. We demonstrate in our experiments that replacing regular convolutions with hyper-convolutions can improve performance with less parameters, and increase robustness against noise. We provide our code here: \emph{<a class="link-external link-https" href="https://github.com/tym002/Hyper-Convolution" rel="external noopener nofollow">this https URL</a>}      
### 54.Cross-Channel Attention-Based Target Speaker Voice Activity Detection: Experimental Results for M2MeT Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.02687.pdf)
>  In this paper, we present the speaker diarization system for the Multi-channel Multi-party Meeting Transcription Challenge (M2MeT) from team DKU_DukeECE. As the highly overlapped speech exists in the dataset, we employ an x-vector-based target-speaker voice activity detection (TS-VAD) to find the overlap between speakers. For the single-channel scenario, we separately train a model for each of the 8 channels and fuse the results. We also employ the cross-channel self-attention to further improve the performance, where the non-linear spatial correlations between different channels are learned and fused. Experimental results on the evaluation set show that the single-channel TS-VAD reduces the DER by over 75% from 12.68\% to 3.14%. The multi-channel TS-VAD further reduces the DER by 28% and achieves a DER of 2.26%. Our final submitted system achieves a DER of 2.98% on the AliMeeting test set, which ranks 1st in the M2MET challenge.      
### 55.Blind source separation of baseband RF communication signals using mixed-signal matrix multiplication circuit  [ :arrow_down: ](https://arxiv.org/pdf/2202.02685.pdf)
>  An 8 x 8 mixed-signal matrix multiplier architecture based on 64 hybrid capacitor-resistor multiplying digital to analogue converters implemented in a 65 nm CMOS technology was developed for the application of blind source separation of baseband RF signals. The integrated circuit has 13-bit resolution for each matrix weight and achieves a measured dynamic range of &gt; 62 dB with a bandwidth of &gt; 15 MHz and typical power dissipation of &lt; 30 mW per matrix row. Separation of single-tone signal is measured to be better than 57 dBc.      
### 56.Mitigating Coriolis Effects in Centrifuge Simulators Through Allowing Small, Unperceived G-Vector Misalignments  [ :arrow_down: ](https://arxiv.org/pdf/2202.02677.pdf)
>  When coupled with additional degrees of freedom, centrifuge-based motion platforms can combine the agility of hexapod-based platforms with the ability to sustain higher G-levels and an extended motion space, required for simulating extreme maneuvers. However, the false and often nauseating sensations of rotation, by Coriolis effects induced by the centrifuge rotation in combination with rotations of the centrifuge cabin or the pilot's head, are a major disadvantage. This paper discusses the development of a motion filter, the Coherent Alignment Method (COHAM), which aims at reducing Coriolis effects by allowing small mismatches in the G-vector alignment, reducing cabin rotations. Simulations show that as long as these mismatches remain within a region where humans perceive the G-vector as 'coherent', the Coherent Alignment Zone (CAZ), the cabin angular accelerations can indeed be reduced. COHAM was tested in a high G-maneuver task with a fixed CAZ threshold obtained in a previous study. It was experimentally compared to an existing motion filter, using metrics such as sickness, comfort and false cues. Results show that sickness, dizziness and discomfort are reduced, making the centrifuge sessions more bearable. It is recommended to further improve the filter design and tuning, and test it with more fighter pilots.      
### 57.Spatio-Temporal Failure Propagation in Cyber-Physical Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.02627.pdf)
>  Cascading failure in power systems is triggered by a small perturbation that leads to a sequence of failures spread through the system. The interconnection between different components in a power system causes failures to easily propagate across the system. The situation gets worse by considering the interconnection between cyber and physical layers in power systems. A plethora of work has studied the cascading failure in power systems to calculate its impact on the system. Understanding how failures propagate into the system in time and space can help the system operator to take preventive actions and upgrade the system accordingly. Due to the nonlinearity of the power flow equation as well as the engineering constraints in the power system, it is essential to understand the spatio-temporal failure propagation in cyber-physical power systems (CPPS). This paper proposes an asynchronous algorithm for investigating failure propagation in CPPS. The physics of the power system is addressed by the full AC power flow equations. Various practical constraints including load shedding, load-generation balance, and island operation are considered to address practical constraints in power system operation. The propagation of various random initial attacks of different sizes is analyzed and visualized to elaborate on the applicability of the proposed approach. Our findings shed light on the cascading failure evolution in CPPS.      
### 58.ROMNet: Renovate the Old Memories  [ :arrow_down: ](https://arxiv.org/pdf/2202.02606.pdf)
>  Renovating the memories in old photos is an intriguing research topic in computer vision fields. These legacy images often suffer from severe and commingled degradations such as cracks, noise, and color-fading, while lack of large-scale paired old photo datasets makes this restoration task very challenging. In this work, we present a novel reference-based end-to-end learning framework that can jointly repair and colorize the degraded legacy pictures. Specifically, the proposed framework consists of three modules: a restoration sub-network for degradation restoration, a similarity sub-network for color histogram matching and transfer, and a colorization subnet that learns to predict the chroma elements of the images conditioned on chromatic reference signals. The whole system takes advantage of the color histogram priors in a given reference image, which vastly reduces the dependency on large-scale training data. Apart from the proposed method, we also create, to our knowledge, the first public and real-world old photo dataset with paired ground truth for evaluating old photo restoration models, wherein each old photo is paired with a manually restored pristine image by PhotoShop experts. Our extensive experiments conducted on both synthetic and real-world datasets demonstrate that our method significantly outperforms state-of-the-arts both quantitatively and qualitatively.      
### 59.Connected and Automated Vehicle Platoon Formation Control via Differential Games  [ :arrow_down: ](https://arxiv.org/pdf/2202.02602.pdf)
>  In this study, the connected and automated vehicles (CAVs) platooning problem is resolved under a differential game framework. Three information topologies are considered here. Firstly, Predecessor-following (PF) topology is utilized where the vehicles control the distance with respect to the merely nearest predecessor via a sensor link-based information flow. Secondly, Two-predecessor-following topology (TPF) is exploited where each vehicle controls the distance with respect to the two nearest predecessors. In this topology, the second predecessor is communicated via a Vehicle-to-vehicle (V2V) link. The individual trajectories of CAVs under the Nash equilibrium are derived in closed-form for these two information topologies. Finally, general information topology is examined and the differential game is formulated in this context. In all these options, Pontryagin's principle is employed to investigate the existence and uniqueness of the Nash equilibrium and obtain its corresponding trajectories. In the general topology, we suppose numerical computation of eigenvalues and eigenvectors. All these approaches represent promising and powerful analytical representations of the CAV platoons under the differential games. Simulation experiments have verified the efficiency of the proposed models and their solutions.      
### 60.Temporal Robustness of Stochastic Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.02583.pdf)
>  We study the temporal robustness of stochastic signals. This topic is of particular interest in interleaving processes such as multi-agent systems where communication and individual agents induce timing uncertainty. For a deterministic signal and a given specification, we first introduce the synchronous and the asynchronous temporal robustness to quantify the signal's robustness with respect to synchronous and asynchronous time shifts in its sub-signals. We then define the temporal robustness risk by investigating the temporal robustness of the realizations of a stochastic signal. This definition can be interpreted as the risk associated with a stochastic signal to not satisfy a specification robustly in time. In this definition, general forms of specifications such as signal temporal logic specifications are permitted. We show how the temporal robustness risk is estimated from data for the value-at-risk. The usefulness of the temporal robustness risk is underlined by both theoretical and empirical evidence. In particular, we provide various numerical case studies including a T-intersection scenario in autonomous driving.      
### 61.AssistMe: Policy iteration for the longitudinal control of a non-holonomic vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2202.02569.pdf)
>  In this article we design a physically-inspired model-based assist-as-needed semi-autonomous control (ASC) algorithm to address the problem of safely driving a vehicle (a power wheelchair) in an environment with static obstacles. Once implemented online, the proposed algorithm requires limited computing power and relies on pre-computed (offline) maps (look-up tables). These are readily available by implementing policy iteration that minimizes the expected time to termination (safely stopping near an obstacle), by taking into account: (i) the vehicle dynamics; (ii) the drivers' intention modeled as three separate stochastic processes. We call them the expert driver, the naughty child and the blind driver models. A study with healthy participants confirmed that ASC outperforms a baseline rule-based control (a statistically significant result).      
### 62.Flexible fast-convolution processing for cellular radio evolution  [ :arrow_down: ](https://arxiv.org/pdf/2202.02515.pdf)
>  Orthogonal frequency-division multiplexing (OFDM) has been selected as a baseline waveform for long-term evolution (LTE) and fifth-generation new radio (5G NR). Fast-convolution (FC)-based frequency-domain signal processing has been considered recently as an effective tool for spectrum enhancement of OFDM-based waveforms. FC-based filtering approximates linear convolution by effective fast Fourier transform (FFT)-based circular convolutions using partly overlapping processing blocks. In earlier work, we have shown that FC-based filtering is a very flexible and efficient tool for filtered-OFDM signal generation and receiver side subband filtering. In this paper, we present a symbol-synchronous FC-processing scheme flexibly allowing filter re-configuration time resolution equal to one OFDM symbol while supporting tight carrier-wise filtering for 5G NR in mixed-numerology scenarios with adjustable subcarrier spacings, center frequencies, and subband bandwidths as well as providing co-exitence with LTE. The proposed scheme is demonstrated to support envisioned use cases of 5G NR and provide flexible starting point for sixth generation development.      
### 63.An Olfactory EEG Signal Classification Network Based on Frequency Band Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2202.02487.pdf)
>  Classification of olfactory-induced electroencephalogram (EEG) signals has shown great potential in many fields. Since different frequency bands within the EEG signals contain different information, extracting specific frequency bands for classification performance is important. Moreover, due to the large inter-subject variability of the EEG signals, extracting frequency bands with subject-specific information rather than general information is crucial. Considering these, the focus of this letter is to classify the olfactory EEG signals by exploiting the spectral-domain information of specific frequency bands. In this letter, we present an olfactory EEG signal classification network based on frequency band feature extraction. A frequency band generator is first designed to extract frequency bands via the sliding window technique. Then, a frequency band attention mechanism is proposed to optimize frequency bands for a specific subject adaptively. Last, a convolutional neural network (CNN) is constructed to extract the spatio-spectral information and predict the EEG category. Comparison experiment results reveal that the proposed method outperforms a series of baseline methods in terms of both classification quality and inter-subject robustness. Ablation experiment results demonstrate the effectiveness of each component of the proposed method.      
### 64.Tensor-CSPNet: A Novel Geometric Deep Learning Framework for Motor Imagery Classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.02472.pdf)
>  Deep learning (DL) has been widely investigated in a vast majority of applications in electroencephalography (EEG)-based brain-computer interfaces (BCIs), especially for motor imagery (MI) classification in the past five years. The mainstream DL methodology for the MI-EEG classification exploits the temporospatial patterns of EEG signals using convolutional neural networks (CNNs), which have been particularly successful in visual images. However, since the statistical characteristics of visual images may not benefit EEG signals, a natural question that arises is whether there exists an alternative network architecture despite CNNs to extract features for the MI-EEG classification. To address this question, we propose a novel geometric deep learning (GDL) framework called Tensor-CSPNet to characterize EEG signals on symmetric positive definite (SPD) manifolds and exploit the temporo-spatio-frequential patterns using deep neural networks on SPD manifolds. Meanwhile, many experiences of successful MI-EEG classifiers have been integrated into the Tensor-CSPNet framework to make it more efficient. In the experiments, Tensor-CSPNet attains or slightly outperforms the current state-of-the-art performance on the cross-validation and holdout scenarios of two MI-EEG datasets. The visualization and interpretability analyses also exhibit its validity for the MI-EEG classification. To conclude, we provide a feasible answer to the question by generalizing the previous DL methodologies on SPD manifolds, which indicates the start of a specific class from the GDL methodology for the MI-EEG classification.      
### 65.Machine Learning Method for Functional Assessment of Retinal Models  [ :arrow_down: ](https://arxiv.org/pdf/2202.02443.pdf)
>  Challenges in the field of retinal prostheses motivate the development of retinal models to accurately simulate Retinal Ganglion Cells (RGCs) responses. The goal of retinal prostheses is to enable blind individuals to solve complex, reallife visual tasks. In this paper, we introduce the functional assessment (FA) of retinal models, which describes the concept of evaluating the performance of retinal models on visual understanding tasks. We present a machine learning method for FA: we feed traditional machine learning classifiers with RGC responses generated by retinal models, to solve object and digit recognition tasks (CIFAR-10, MNIST, Fashion MNIST, Imagenette). We examined critical FA aspects, including how the performance of FA depends on the task, how to optimally feed RGC responses to the classifiers and how the number of output neurons correlates with the model's accuracy. To increase the number of output neurons, we manipulated input images - by splitting and then feeding them to the retinal model and we found that image splitting does not significantly improve the model's accuracy. We also show that differences in the structure of datasets result in largely divergent performance of the retinal model (MNIST and Fashion MNIST exceeded 80% accuracy, while CIFAR-10 and Imagenette achieved ~40%). Furthermore, retinal models which perform better in standard evaluation, i.e. more accurately predict RGC response, perform better in FA as well. However, unlike standard evaluation, FA results can be straightforwardly interpreted in the context of comparing the quality of visual perception.      
### 66.Stratification of carotid atheromatous plaque using interpretable deep learning methods on B-mode ultrasound images  [ :arrow_down: ](https://arxiv.org/pdf/2202.02428.pdf)
>  Carotid atherosclerosis is the major cause of ischemic stroke resulting in significant rates of mortality and disability annually. Early diagnosis of such cases is of great importance, since it enables clinicians to apply a more effective treatment strategy. This paper introduces an interpretable classification approach of carotid ultrasound images for the risk assessment and stratification of patients with carotid atheromatous plaque. To address the highly imbalanced distribution of patients between the symptomatic and asymptomatic classes (16 vs 58, respectively), an ensemble learning scheme based on a sub-sampling approach was applied along with a two-phase, cost-sensitive strategy of learning, that uses the original and a resampled data set. Convolutional Neural Networks (CNNs) were utilized for building the primary models of the ensemble. A six-layer deep CNN was used to automatically extract features from the images, followed by a classification stage of two fully connected layers. The obtained results (Area Under the ROC Curve (AUC): 73%, sensitivity: 75%, specificity: 70%) indicate that the proposed approach achieved acceptable discrimination performance. Finally, interpretability methods were applied on the model's predictions in order to reveal insights on the model's decision process as well as to enable the identification of novel image biomarkers for the stratification of patients with carotid atheromatous plaque.Clinical Relevance-The integration of interpretability methods with deep learning strategies can facilitate the identification of novel ultrasound image biomarkers for the stratification of patients with carotid atheromatous plaque.      
### 67.Learning a Discrete Set of Optimal Allocation Rules in Queueing Systems with Unknown Service Rates  [ :arrow_down: ](https://arxiv.org/pdf/2202.02419.pdf)
>  We study learning-based admission control for a classical Erlang-B blocking system with unknown service rate, i.e., an $M/M/k/k$ queueing system. At every job arrival, a dispatcher decides to assign the job to an available server or to block it. Every served job yields a fixed reward for the dispatcher, but it also results in a cost per unit time of service. Our goal is to design a dispatching policy that maximizes the long-term average reward for the dispatcher based on observing the arrival times and the state of the system at each arrival; critically, the dispatcher observes neither the service times nor departure times. <br>We develop our learning-based dispatch scheme as a parametric learning problem a'la self-tuning adaptive control. In our problem, certainty equivalent control switches between an always admit policy (always explore) and a never admit policy (immediately terminate learning), which is distinct from the adaptive control literature. Our learning scheme then uses maximum likelihood estimation followed by certainty equivalent control but with judicious use of the always admit policy so that learning doesn't stall. We prove that for all service rates, the proposed policy asymptotically learns to take the optimal action. Further, we also present finite-time regret guarantees for our scheme. The extreme contrast in the certainty equivalent optimal control policies leads to difficulties in learning that show up in our regret bounds for different parameter regimes. We explore this aspect in our simulations and also follow-up sampling related questions for our continuous-time system.      
### 68.Bregman Plug-and-Play Priors  [ :arrow_down: ](https://arxiv.org/pdf/2202.02388.pdf)
>  The past few years have seen a surge of activity around integration of deep learning networks and optimization algorithms for solving inverse problems. Recent work on plug-and-play priors (PnP), regularization by denoising (RED), and deep unfolding has shown the state-of-the-art performance of such integration in a variety of applications. However, the current paradigm for designing such algorithms is inherently Euclidean, due to the usage of the quadratic norm within the projection and proximal operators. We propose to broaden this perspective by considering a non-Euclidean setting based on the more general Bregman distance. Our new Bregman Proximal Gradient Method variant of PnP (PnP-BPGM) and Bregman Steepest Descent variant of RED (RED-BSD) replace the traditional updates in PnP and RED from the quadratic norms to more general Bregman distance. We present a theoretical convergence result for PnP-BPGM and demonstrate the effectiveness of our algorithms on Poisson linear inverse problems.      
### 69.Fully Automated Tree Topology Estimation and Artery-Vein Classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.02382.pdf)
>  We present a fully automatic technique for extracting the retinal vascular topology, i.e., how the different vessels are connected to each other, given a single color fundus image. Determining this connectivity is very challenging because vessels cross each other in a 2D image, obscuring their true paths. We validated the usefulness of our extraction method by using it to achieve state-of-the-art results in retinal artery-vein classification. <br>Our proposed approach works as follows. We first segment the retinal vessels using our previously developed state-of-the-art segmentation method. Then, we estimate an initial graph from the extracted vessels and assign the most likely blood flow to each edge. We then use a handful of high-level operations (HLOs) to fix errors in the graph. These HLOs include detaching neighboring nodes, shifting the endpoints of an edge, and reversing the estimated blood flow direction for a branch. We use a novel cost function to find the optimal set of HLO operations for a given graph. Finally, we show that our extracted vascular structure is correct by propagating artery/vein labels along the branches. As our experiments show, our topology-based artery-vein labeling achieved state-of-the-art results on multiple datasets. We also performed several ablation studies to verify the importance of the different components of our proposed method.      
### 70.Boundary-aware Information Maximization for Self-supervised Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.02371.pdf)
>  Unsupervised pre-training has been proven as an effective approach to boost various downstream tasks given limited labeled data. Among various methods, contrastive learning learns a discriminative representation by constructing positive and negative pairs. However, it is not trivial to build reasonable pairs for a segmentation task in an unsupervised way. In this work, we propose a novel unsupervised pre-training framework that avoids the drawback of contrastive learning. Our framework consists of two principles: unsupervised over-segmentation as a pre-train task using mutual information maximization and boundary-aware preserving learning. Experimental results on two benchmark medical segmentation datasets reveal our method's effectiveness in improving segmentation performance when few annotated images are available.      
### 71.Deep Impulse Responses: Estimating and Parameterizing Filters with Deep Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.03416.pdf)
>  Impulse response estimation in high noise and in-the-wild settings, with minimal control of the underlying data distributions, is a challenging problem. We propose a novel framework for parameterizing and estimating impulse responses based on recent advances in neural representation learning. Our framework is driven by a carefully designed neural network that jointly estimates the impulse response and the (apriori unknown) spectral noise characteristics of an observed signal given the source signal. We demonstrate robustness in estimation, even under low signal-to-noise ratios, and show strong results when learning from spatio-temporal real-world speech data. Our framework provides a natural way to interpolate impulse responses on a spatial grid, while also allowing for efficiently compressing and storing them for real-time rendering applications in augmented and virtual reality.      
### 72.Private Read Update Write (PRUW) with Storage Constrained Databases  [ :arrow_down: ](https://arxiv.org/pdf/2202.03400.pdf)
>  We investigate the problem of private read update write (PRUW) in relation to federated submodel learning (FSL) with storage constrained databases. In PRUW, a user privately reads a submodel from a system of $N$ databases containing $M$ submodels, updates it locally, and writes the update back to the databases without revealing the submodel index or the value of the update. The databases considered in this problem are only allowed to store a given amount of information specified by an arbitrary storage constraint. We provide a storage mechanism that determines the contents of each database prior to the application of the PRUW scheme, such that the total communication cost is minimized. We show that the proposed storage scheme achieves a lower total cost compared to what is achieved by using \emph{coded storage} or \emph{divided storage} to meet the given storage constraint.      
### 73.Gradient-Based Learning of Discrete Structured Measurement Operators for Signal Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2202.03391.pdf)
>  Countless signal processing applications include the reconstruction of signals from few indirect linear measurements. The design of effective measurement operators is typically constrained by the underlying hardware and physics, posing a challenging and often even discrete optimization task. While the potential of gradient-based learning via the unrolling of iterative recovery algorithms has been demonstrated, it has remained unclear how to leverage this technique when the set of admissible measurement operators is structured and discrete. We tackle this problem by combining unrolled optimization with Gumbel reparametrizations, which enable the computation of low-variance gradient estimates of categorical random variables. Our approach is formalized by GLODISMO (Gradient-based Learning of DIscrete Structured Measurement Operators). This novel method is easy-to-implement, computationally efficient, and extendable due to its compatibility with automatic differentiation. We empirically demonstrate the performance and flexibility of GLODISMO in several prototypical signal recovery applications, verifying that the learned measurement matrices outperform conventional designs based on randomization as well as discrete optimization baselines.      
### 74.FrePGAN: Robust Deepfake Detection Using Frequency-level Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2202.03347.pdf)
>  Various deepfake detectors have been proposed, but challenges still exist to detect images of unknown categories or GAN models outside of the training settings. Such issues arise from the overfitting issue, which we discover from our own analysis and the previous studies to originate from the frequency-level artifacts in generated images. We find that ignoring the frequency-level artifacts can improve the detector's generalization across various GAN models, but it can reduce the model's performance for the trained GAN models. Thus, we design a framework to generalize the deepfake detector for both the known and unseen GAN models. Our framework generates the frequency-level perturbation maps to make the generated images indistinguishable from the real images. By updating the deepfake detector along with the training of the perturbation generator, our model is trained to detect the frequency-level artifacts at the initial iterations and consider the image-level irregularities at the last iterations. For experiments, we design new test scenarios varying from the training settings in GAN models, color manipulations, and object categories. Numerous experiments validate the state-of-the-art performance of our deepfake detector.      
### 75.Differential Privacy for Symbolic Systems with Application to Markov Chains  [ :arrow_down: ](https://arxiv.org/pdf/2202.03325.pdf)
>  Data-driven systems are gathering increasing amounts of data from users, and sensitive user data requires privacy protections. In some cases, the data gathered is non-numerical or symbolic, and conventional approaches to privacy, e.g., adding noise, do not apply, though such systems still require privacy protections. Accordingly, we present a novel differential privacy framework for protecting trajectories generated by symbolic systems. These trajectories can be represented as words or strings over a finite alphabet. We develop new differential privacy mechanisms that approximate a sensitive word using a random word that is likely to be near it. An offline mechanism is implemented efficiently using a Modified Hamming Distance Automaton to generate whole privatized output words over a finite time horizon. Then, an online mechanism is implemented by taking in a sensitive symbol and generating a randomized output symbol at each timestep. This work is extended to Markov chains to generate differentially private state sequences that a given Markov chain could have produced. Statistical accuracy bounds are developed to quantify the accuracy of these mechanisms, and numerical results validate the accuracy of these techniques for strings of English words.      
### 76.Online Deep Neural Network for Optimization in Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2202.03244.pdf)
>  Recently, deep neural network (DNN) has been widely adopted in the design of intelligent communication systems thanks to its strong learning ability and low testing complexity. However, most current offline DNN-based methods still suffer from unsatisfactory performance, limited generalization ability, and poor interpretability. In this article, we propose an online DNN-based approach to solve general optimization problems in wireless communications, where a dedicated DNN is trained for each data sample. By treating the optimization variables and the objective function as network parameters and loss function, respectively, the optimization problem can be solved equivalently through network training. Thanks to the online optimization nature and meaningful network parameters, the proposed approach owns strong generalization ability and interpretability, while its superior performance is demonstrated through a practical example of joint beamforming in intelligent reflecting surface (IRS)-aided multi-user multiple-input multiple-output (MIMO) systems. Simulation results show that the proposed online DNN outperforms conventional offline DNN and state-of-the-art iterative optimization algorithm, but with low complexity.      
### 77.Air-Releasable Soft Robots for Explosive Ordnance Disposal  [ :arrow_down: ](https://arxiv.org/pdf/2202.03243.pdf)
>  The demining of landmines using drones is challenging; air-releasable payloads are typically non-intelligent (e.g., water balloons or explosives) and deploying them at even low altitudes (~6 meter) is inherently inaccurate due to complex deployment trajectories and constrained visual awareness by the drone pilot. Soft robotics offers a unique approach for aerial demining, namely due to the robust, low-cost, and lightweight designs of soft robots. Instead of non-intelligent payloads, here, we propose the use of air-releasable soft robots for demining. We developed a full system consisting of an unmanned aerial vehicle retrofitted to a soft robot carrier including a custom-made deployment mechanism, and an air-releasable, lightweight (296 g), untethered soft hybrid robot with integrated electronics that incorporates a new type of a vacuum-based flasher roller actuator system. We demonstrate a deployment cycle in which the drone drops the soft robotic hybrid from an altitude of 4.5 m meters and after which the robot approaches a dummy landmine. By deploying soft robots at points of interest, we can transition soft robotic technologies from the laboratory to real-world environments.      
### 78.SODA: Self-organizing data augmentation in deep neural networks -- Application to biomedical image segmentation tasks  [ :arrow_down: ](https://arxiv.org/pdf/2202.03223.pdf)
>  In practice, data augmentation is assigned a predefined budget in terms of newly created samples per epoch. When using several types of data augmentation, the budget is usually uniformly distributed over the set of augmentations but one can wonder if this budget should not be allocated to each type in a more efficient way. This paper leverages online learning to allocate on the fly this budget as part of neural network training. This meta-algorithm can be run at almost no extra cost as it exploits gradient based signals to determine which type of data augmentation should be preferred. Experiments suggest that this strategy can save computation time and thus goes in the way of greener machine learning practices.      
### 79.Deep Learning based Channel Estimation for Massive MIMO with Hybrid Transceivers  [ :arrow_down: ](https://arxiv.org/pdf/2202.03220.pdf)
>  Accurate and efficient estimation of the high dimensional channels is one of the critical challenges for practical applications of massive multiple-input multiple-output (MIMO). In the context of hybrid analog-digital (HAD) transceivers, channel estimation becomes even more complicated due to information loss caused by limited radio-frequency chains. The conventional compressive sensing (CS) algorithms usually suffer from unsatisfactory performance and high computational complexity. In this paper, we propose a novel deep learning (DL) based framework for uplink channel estimation in HAD massive MIMO systems. To better exploit the sparsity structure of channels in the angular domain, a novel angular space segmentation method is proposed, where the entire angular space is segmented into many small regions and a dedicated neural network is trained offline for each region. During online testing, the most suitable network is selected based on the information from the global positioning system. Inside each neural network, the region-specific measurement matrix and channel estimator are jointly optimized, which not only improves the signal measurement efficiency, but also enhances the channel estimation capability. Simulation results show that the proposed approach significantly outperforms the state-of-the-art CS algorithms in terms of estimation performance and computational complexity.      
### 80.Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.03218.pdf)
>  Self-supervised learning (SSL) is a powerful tool that allows learning of underlying representations from unlabeled data. Transformer based models such as wav2vec 2.0 and HuBERT are leading the field in the speech domain. Generally these models are fine-tuned on a small amount of labeled data for a downstream task such as Automatic Speech Recognition (ASR). This involves re-training the majority of the model for each task. Adapters are small lightweight modules which are commonly used in Natural Language Processing (NLP) to adapt pre-trained models to new tasks. In this paper we propose applying adapters to wav2vec 2.0 to reduce the number of parameters required for downstream ASR tasks, and increase scalability of the model to multiple tasks or languages. Using adapters we can perform ASR while training fewer than 10% of parameters per task compared to full fine-tuning with little degradation of performance. Ablations show that applying adapters into just the top few layers of the pre-trained network gives similar performance to full transfer, supporting the theory that higher pre-trained layers encode more phonemic information, and further optimizing efficiency.      
### 81.Over-the-Air Ensemble Inference with Model Privacy  [ :arrow_down: ](https://arxiv.org/pdf/2202.03129.pdf)
>  We consider distributed inference at the wireless edge, where multiple clients with an ensemble of models, each trained independently on a local dataset, are queried in parallel to make an accurate decision on a new sample. In addition to maximizing inference accuracy, we also want to maximize the privacy of local models. We exploit the superposition property of the air to implement bandwidth-efficient ensemble inference methods. We introduce different over-the-air ensemble methods and show that these schemes perform significantly better than their orthogonal counterparts, while using less resources and providing privacy guarantees. We also provide experimental results verifying the benefits of the proposed over-the-air inference approach, whose source code is shared publicly on Github.      
### 82.Free-breathing motion compensated 4D (3D+respiration) T2-weighted turbo spin-echo MRI for body imaging  [ :arrow_down: ](https://arxiv.org/pdf/2202.03021.pdf)
>  Purpose: To develop and evaluate a free-breathing respiratory motion compensated 4D (3D+respiration) $T_2$-weighted turbo spin echo sequence with application to radiology and MR-guided radiotherapy. <br>Methods: k-space data are continuously acquired using a rewound Cartesian acquisition with spiral profile ordering (rCASPR) to provide matching contrast to the conventional linear phase encode ordering and to sort data into multiple respiratory phases. Low-resolution respiratory-correlated 4D images were reconstructed with compressed sensing and used to estimate non-rigid deformation vector fields, which were subsequently used for a motion compensated image reconstruction. rCASPR sampling was compared to linear and CASPR sampling in terms of point-spread-function (PSF) and image contrast with in silico, phantom and in vivo experiments. Reconstruction parameters for low-resolution 4D-MRI (spatial resolution and temporal regularization) were determined using a grid search. The proposed motion compensated rCASPR was evaluated in eight healthy volunteers and compared to free-breathing scans with linear sampling. Image quality was compared based on visual inspection and quantitatively by means of the gradient entropy. <br>Results: rCASPR provided a superior PSF (similar in ky and narrower in kz) and showed no considerable differences in images contrast compared to linear sampling. The optimal 4D-MRI reconstruction parameters were spatial resolution=$4.5 mm^3$ and $\lambda_t=10^{-4}$. The groupwise average gradient entropy was 22.31 for linear, 22.20 for rCASPR, 22.14 for soft-gated rCASPR and 22.02 for motion compensated rCASPR. <br>Conclusion: The proposed motion compensated rCASPR enables high quality free-breathing T2-TSE with minimal changes in image contrast and scan time. The proposed method therefore enables direct transfer of clinically used 3D TSE sequences to free-breathing.      
### 83.Learning Sound Localization Better From Semantically Similar Samples  [ :arrow_down: ](https://arxiv.org/pdf/2202.03007.pdf)
>  The objective of this work is to localize the sound sources in visual scenes. Existing audio-visual works employ contrastive learning by assigning corresponding audio-visual pairs from the same source as positives while randomly mismatched pairs as negatives. However, these negative pairs may contain semantically matched audio-visual information. Thus, these semantically correlated pairs, "hard positives", are mistakenly grouped as negatives. Our key contribution is showing that hard positives can give similar response maps to the corresponding pairs. Our approach incorporates these hard positives by adding their response maps into a contrastive learning objective directly. We demonstrate the effectiveness of our approach on VGG-SS and SoundNet-Flickr test sets, showing favorable performance to the state-of-the-art methods.      
### 84.Energy-efficient Dynamic-subarray with Fixed True-time-delay Design for Terahertz Wideband Hybrid Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2202.02965.pdf)
>  Hybrid beamforming for Terahertz (THz) ultra-massive multiple-input multiple-output (UM-MIMO) systems is a promising technology for 6G space-air-ground integrated networks, which can overcome huge propagation loss and offer unprecedented data rates. With ultra-wide bandwidth and ultra-large-scale antennas array in THz band, the beam squint becomes one of the critical problems which could reduce the array gain and degrade the data rate substantially. However, the traditional phase-shifters-based hybrid beamforming architectures cannot tackle this issue due to the frequency-flat property of the phase shifters. In this paper, to combat the beam squint while keeping high energy efficiency, a novel dynamic-subarray with fixed true-time-delay (DS-FTTD) architecture is proposed. Compared to the existing studies which use the complicated adjustable TTDs, the DS-FTTD architecture has lower power consumption and hardware complexity, thanks to the low-cost FTTDs. Furthermore, a low-complexity row-decomposition (RD) algorithm is proposed to design hybrid beamforming matrices for the DS-FTTD architecture. Extensive simulation results show that, by using the RD algorithm, the DS-FTTD architecture achieves near-optimal array gain and significantly higher energy efficiency than the existing architectures. Moreover, the spectral efficiency of DS-FTTD architecture with the RD algorithm is robust to the imperfect channel state information.      
### 85.Efficient ADMM Decoder for Non-binary LDPC Codes with Codeword-Independent Performance  [ :arrow_down: ](https://arxiv.org/pdf/2202.02961.pdf)
>  In this paper, we devote to devise a non-binary low-density parity-check (LDPC) decoder in Galois fields of characteristic two ($\mathbb{F}_{2^q}$) via the alternating direction method of multipliers (ADMM) technique. Through the proposed bit embedding technique and the decomposition technique of the three-variables parity-check equation, an efficient ADMM decoding algorithm for non-binary LDPC codes is proposed. The computation complexity in each ADMM iteration is roughly $\mathcal{O}(nq)$, which is significantly lower than the existing LDPC decoders. Moreover, we prove that the proposed decoder satisfies the favorable property of the codeword-independent. Simulation results demonstrate the outstanding performance of the proposed decoder in contrast with state-of-the-art LDPC decoders.      
### 86.Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.02947.pdf)
>  Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the access point. PSL considers the realistic scenario where global aggregations are conducted with idle times in-between them for resource efficiency improvements, and incorporates data dispersion and model dispersion with local model condensation into FedL. Our analysis sheds light on the notion of cold vs. warmed up models, and model inertia in distributed machine learning. We then propose network-aware dynamic model tracking to optimize the model learning vs. resource efficiency tradeoff, which we show is an NP-hard signomial programming problem. We finally solve this problem through proposing a general optimization solver. Our numerical results reveal new findings on the interdependencies between the idle times in-between the global aggregations, model/concept drift, and D2D cooperation configuration.      
### 87.On the Stability of Super-Resolution and a Beurling-Selberg Type Extremal Problem  [ :arrow_down: ](https://arxiv.org/pdf/2202.02932.pdf)
>  Super-resolution estimation is the problem of recovering a stream of spikes (point sources) from the noisy observation of a few number of its first trigonometric moments. The performance of super-resolution is recognized to be intimately related to the separation between the spikes to recover. A novel notion of stability of the Fisher information matrix (FIM) of the super-resolution problem is introduced, when the minimal eigenvalue of the FIM is not asymptotically vanishing. The regime where the minimal separation is inversely proportional to the number of acquired moments is considered. It is shown that there is a separation threshold above which the eigenvalues of the FIM can be bounded by a quantity that does not depend on the number of moments. The proof relies on characterizing the connection between the stability of the FIM and a generalization of the Beurling-Selberg box approximation problem.      
### 88.3TO: THz-Enabled Throughput and Trajectory Optimization of UAVs in 6G Networks by Proximal Policy Optimization Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.02924.pdf)
>  Next-generation networks need to meet ubiquitous and high data-rate demand. Therefore, this paper considers the throughput and trajectory optimization of terahertz (THz)-enabled unmanned aerial vehicles (UAVs) in the sixth-generation (6G) communication networks. In the considered scenario, multiple UAVs must provide on-demand terabits per second (TB/s) services to an urban area along with existing terrestrial networks. However, THz-empowered UAVs pose some new constraints, e.g., dynamic THz-channel conditions for ground users (GUs) association and UAV trajectory optimization to fulfill GU's throughput demands. Thus, a framework is proposed to address these challenges, where a joint UAVs-GUs association, transmit power, and the trajectory optimization problem is studied. The formulated problem is mixed-integer non-linear programming (MINLP), which is NP-hard to solve. Consequently, an iterative algorithm is proposed to solve three sub-problems iteratively, i.e., UAVs-GUs association, transmit power, and trajectory optimization. Simulation results demonstrate that the proposed algorithm increased the throughput by up to 10%, 68.9%, and 69.1% respectively compared to baseline algorithms.      
### 89.Global convergence and optimality of the heavy ball method for non-convex optimization  [ :arrow_down: ](https://arxiv.org/pdf/2202.02914.pdf)
>  In this letter we revisit the famous heavy ball method and study its global convergence for a class of non-convex problems. We characterize the parameters that render the method globally convergent and yield the best R- convergence factor. We show that for a family of functions, this convergence factor is superior to the factor obtained from the triple momentum method.      
### 90.A Passivity Based Framework for Safe Physical Human Robot Interaction  [ :arrow_down: ](https://arxiv.org/pdf/2202.02900.pdf)
>  In this paper, the problem of making a safe compliant contact between a human and an assistive robot is considered. Users with disabilities have a need to utilize their assistive robots for physical human-robot interaction (PHRI) during certain activities of daily living (ADLs). Specifically, we propose a hybrid force/velocity/attitude control for a PHRI system based on measurements from a 6-axis force/torque sensor mounted on the robot wrist. While automatically aligning the end-effector surface with the unknown environmental (human) surface, a desired commanded force is applied in the normal direction while following desired velocity commands in the tangential directions. A Lyapunov based stability analysis is provided to prove both convergence as well as passivity of the interaction to ensure both performance and safety. Simulation as well as experimental results verify the performance and robustness of the proposed hybrid controller in the presence of dynamic uncertainties as well as safe physical human-robot interactions for a kinematically redundant robotic manipulator.      
### 91.Learning under Storage and Privacy Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2202.02892.pdf)
>  Storage-efficient privacy-guaranteed learning is crucial due to enormous amounts of sensitive user data required for increasingly many learning tasks. We propose a framework for reducing the storage cost while at the same time providing privacy guarantees, without essential loss in the utility of the data for learning. Our method comprises noise injection followed by lossy compression. We show that, when appropriately matching the lossy compression to the distribution of the added noise, the compressed examples converge, in distribution, to that of the noise-free training data. In this sense, the utility of the data for learning is essentially maintained, while reducing storage and privacy leakage by quantifiable amounts. We present experimental results on the CelebA dataset for gender classification and find that our suggested pipeline delivers in practice on the promise of the theory: the individuals in the images are unrecognizable (or less recognizable, depending on the noise level), overall storage of the data is substantially reduced, with no essential loss of the classification accuracy. As an added bonus, our experiments suggest that our method yields a substantial boost to robustness in the face of adversarial test data.      
### 92.Continuous-Time Channel Gain Control for Minimum-Information Kalman-Bucy Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2202.02880.pdf)
>  We consider the problem of estimating a continuous-time Gauss-Markov source process observed through a vector Gaussian channel with an adjustable channel gain matrix. For a given (generally time-varying) channel gain matrix, we provide formulas to compute (i) the mean-square estimation error attainable by the classical Kalman-Bucy filter, and (ii) the mutual information between the source process and its Kalman-Bucy estimate. We then formulate a novel "optimal channel gain control problem" where the objective is to control the channel gain matrix strategically to minimize the weighted sum of these two performance metrics. To develop insights into the optimal solution, we first consider the problem of controlling a time-varying channel gain over a finite time interval. A necessary optimality condition is derived based on Pontryagin's minimum principle. For a scalar system, we show that the optimal channel gain is a piece-wise constant signal with at most two switches. We also consider the problem of designing the optimal time-invariant gain to minimize the average cost over an infinite time horizon. A novel semidefinite programming (SDP) heuristic is proposed and the exactness of the solution is discussed.      
### 93.MIMO Systems with One-bit ADCs: Capacity Gains using Nonlinear Analog Operations  [ :arrow_down: ](https://arxiv.org/pdf/2202.02860.pdf)
>  Analog to Digital Converters (ADCs) are a major contributor to the energy consumption on the receiver side of millimeter-wave multiple-input multiple-output (MIMO) systems with large antenna arrays. Consequently, there has been significant interest in using low-resolution ADCs along with hybrid beam-forming at MIMO receivers for energy efficiency. However, decreasing the ADC resolution results in performance loss -- in terms of achievable rates -- due to increased quantization error. In this work, we study the application of practically implementable nonlinear analog operations, prior to sampling and quantization at the ADCs, as a way to mitigate the aforementioned rate-loss. A receiver architecture consisting of linear analog combiners, implementable nonlinear analog operators, and one-bit threshold ADCs is designed. The fundamental information theoretic performance limits of the resulting communication system, in terms of achievable rates, are investigated under various assumptions on the set of implementable nonlinear analog functions. In order to justify the feasibility of the nonlinear operations in the proposed receiver architecture, an analog circuit is introduced, and circuit simulations exhibiting the generation of the desired nonlinear analog operations are provided.      
### 94.Covertly Controlling a Linear System  [ :arrow_down: ](https://arxiv.org/pdf/2202.02853.pdf)
>  Consider the problem of covertly controlling a linear system. In this problem, Alice desires to control (stabilize or change the parameters of) a linear system, while keeping an observer, Willie, unable to decide if the system is indeed being controlled or not. <br>We formally define the problem, under two different models: (i) When Willie can only observe the system's output (ii) When Willie can directly observe the control signal. Focusing on AR(1) systems, we show that when Willie observes the system's output through a clean channel, an inherently unstable linear system can not be covertly stabilized. However, an inherently stable linear system can be covertly controlled, in the sense of covertly changing its parameter. Moreover, we give direct and converse results for two important controllers: a minimal-information controller, where Alice is allowed to used only $1$ bit per sample, and a maximal-information controller, where Alice is allowed to view the real-valued output. Unlike covert communication, where the trade-off is between rate and covertness, the results reveal an interesting \emph{three--fold} trade--off in covert control: the amount of information used by the controller, control performance and covertness. To the best of our knowledge, this is the first study formally defining covert control.      
### 95.Pipe Overflow: Smashing Voice Authentication for Fun and Profit  [ :arrow_down: ](https://arxiv.org/pdf/2202.02751.pdf)
>  Recent years have seen a surge of popularity of acoustics-enabled personal devices powered by machine learning. Yet, machine learning has proven to be vulnerable to adversarial examples. Large number of modern systems protect themselves against such attacks by targeting the artificiality, i.e., they deploy mechanisms to detect the lack of human involvement in generating the adversarial examples. However, these defenses implicitly assume that humans are incapable of producing meaningful and targeted adversarial examples. In this paper, we show that this base assumption is wrong. In particular, we demonstrate that for tasks like speaker identification, a human is capable of producing analog adversarial examples directly with little cost and supervision: by simply speaking through a tube, an adversary reliably impersonates other speakers in eyes of ML models for speaker identification. Our findings extend to a range of other acoustic-biometric tasks such as liveness, bringing into question their use in security-critical settings in real life, such as phone banking.      
### 96.A method for virtual optical sectioning and tomography utilizing shallow depth of field  [ :arrow_down: ](https://arxiv.org/pdf/2202.02692.pdf)
>  A method is proposed for high-resolution, three-dimensional reconstruction of internal structure of objects from planar transmission images. The described approach can be used with any form of radiation or matter waves, in principle, provided that the depth of field is smaller than the thickness of the sample. The physical optics basis for the method is elucidated and the reconstruction algorithm is presented in detail. A simulated example demonstrates an application of the method to three-dimensional electron transmission imaging of a nanoparticle under realistic radiation dose and spatial resolution constraints. It is envisaged that the method can be applicable in high-resolution transmission electron microscopy, soft X-ray microscopy, ultrasound imaging and other areas.      
### 97.Autocorrelation, Wigner and Ambiguity Transforms on Polygons for Coherent Radiation Rendering  [ :arrow_down: ](https://arxiv.org/pdf/2202.02676.pdf)
>  Simulating the radar illumination of large scenes generally relies on a geometric model of light transport which largely ignores prominent wave effects. This can be remedied through coherence ray-tracing, but this requires the Wigner transform of the aperture. This diffraction function has been historically difficult to generate, and is relevant in the fields of optics, holography, synchrotron-radiation, quantum systems and radar. In this paper we provide the Wigner transform of arbitrary polygons through geometric transforms and the Stokes Fourier transform; and display its use in Monte-Carlo rendering.      
### 98.PhysFad: Physics-Based End-to-End Channel Modeling of RIS-Parametrized Environments with Adjustable Fading  [ :arrow_down: ](https://arxiv.org/pdf/2202.02673.pdf)
>  Programmable radio environments parametrized by reconfigurable intelligent surfaces (RISs) are emerging as a new wireless communications paradigm, but currently used channel models for the design and analysis of signal-processing algorithms cannot include fading in a manner that is faithful to the underlying wave physics. To overcome this roadblock, we introduce a physics-based end-to-end model of RIS-parametrized wireless channels with adjustable fading (coined PhysFad) which is based on a first-principles coupled-dipole formalism. PhysFad naturally incorporates the notions of space and causality, dispersion (i.e., frequency selectivity) and the intertwinement of each RIS element's phase and amplitude response, as well as any arising mutual coupling effects including long-range mesoscopic correlations. PhysFad offers the to-date missing tuning knob for adjustable fading. We thoroughly characterize PhysFad and demonstrate its capabilities for a prototypical problem of RIS-enabled over-the-air channel equalization in rich-scattering wireless communications. We also share a user-friendly version of our code to help the community transition towards physics-based models with adjustable fading.      
### 99.Privacy-preserving Speech Emotion Recognition through Semi-Supervised Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.02611.pdf)
>  Speech Emotion Recognition (SER) refers to the recognition of human emotions from natural speech. If done accurately, it can offer a number of benefits in building human-centered context-aware intelligent systems. Existing SER approaches are largely centralized, without considering users' privacy. Federated Learning (FL) is a distributed machine learning paradigm dealing with decentralization of privacy-sensitive personal data. In this paper, we present a privacy-preserving and data-efficient SER approach by utilizing the concept of FL. To the best of our knowledge, this is the first federated SER approach, which utilizes self-training learning in conjunction with federated learning to exploit both labeled and unlabeled on-device data. Our experimental evaluations on the IEMOCAP dataset shows that our federated approach can learn generalizable SER models even under low availability of data labels and highly non-i.i.d. distributions. We show that our approach with as few as 10% labeled data, on average, can improve the recognition rate by 8.67% compared to the fully-supervised federated counterparts.      
### 100.Intelligent Reflecting Surface-Aided Spectrum Sensing for Cognitive Radio  [ :arrow_down: ](https://arxiv.org/pdf/2202.02550.pdf)
>  Spectrum sensing is a key enabling technique for cognitive radio (CR), which provides essential information on the spectrum availability. However, due to severe wireless channel fading and path loss, the primary user (PU) signals received at the CR or secondary user (SU) can be practically too weak for reliable detection. To tackle this issue, we consider in this letter a new intelligent reflecting surface (IRS)-aided spectrum sensing scheme for CR, by exploiting the large aperture and passive beamforming gains of IRS to boost the PU signal strength received at the SU to facilitate its spectrum sensing. Specifically, by dynamically changing the IRS reflection over time according to a given codebook, its reflected signal power varies substantially at the SU, which is utilized for opportunistic signal detection. Furthermore, we propose a weighted energy detection method by combining the received signal power values over different IRS reflections, which significantly improves the detection performance. Simulation results validate the performance gain of the proposed IRS-aided spectrum sensing scheme, as compared to different benchmark schemes.      
### 101.Optimization of a Real-Time Wavelet-Based Algorithm for Improving Speech Intelligibility  [ :arrow_down: ](https://arxiv.org/pdf/2202.02545.pdf)
>  The optimization of a wavelet-based algorithm to improve speech intelligibility is reported. The discrete-time speech signal is split into frequency sub-bands via a multi-level discrete wavelet transform. Various gains are applied to the sub-band signals before they are recombined to form a modified version of the speech. The sub-band gains are adjusted while keeping the overall signal energy unchanged, and the speech intelligibility under various background interference and simulated hearing loss conditions is enhanced and evaluated objectively and quantitatively using Google Speech-to-Text transcription. For English and Chinese noise-free speech, overall intelligibility is improved, and the transcription accuracy can be increased by as much as 80 percentage points by reallocating the spectral energy toward the mid-frequency sub-bands, effectively increasing the consonant-vowel intensity ratio. This is reasonable since the consonants are relatively weak and of short duration, which are therefore the most likely to become indistinguishable in the presence of background noise or high-frequency hearing impairment. For speech already corrupted by noise, improving intelligibility is challenging but still realizable. The proposed algorithm is implementable for real-time signal processing and comparatively simpler than previous algorithms. Potential applications include speech enhancement, hearing aids, machine listening, and a better understanding of speech intelligibility.      
### 102.Blue Data Computation Maximization in 6G Space-Air-Sea Non-Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.02533.pdf)
>  Non-terrestrial networks (NTN), encompassing space and air platforms, are a key component of the upcoming sixth-generation (6G) cellular network. Meanwhile, maritime network traffic has grown significantly in recent years due to sea transportation used for national defense, research, recreational activities, domestic and international trade. In this paper, the seamless and reliable demand for communication and computation in maritime wireless networks is investigated. Two types of marine user equipment (UEs), i.e., low-antenna gain and high-antenna gain UEs, are considered. A joint task computation and time allocation problem for weighted sum-rate maximization is formulated as mixed-integer linear programming (MILP). The goal is to design an algorithm that enables the network to efficiently provide backhaul resources to an unmanned aerial vehicle (UAV) and offload HUEs tasks to LEO satellite for blue data (i.e., marine user's data). To solve this MILP, a solution based on the Bender and primal decomposition is proposed. The Bender decomposes MILP into the master problem for binary task decision and subproblem for continuous-time resource allocation. Moreover, primal decomposition deals with a coupling constraint in the subproblem. Finally, numerical results demonstrate that the proposed algorithm provides the maritime UEs coverage demand in polynomial time computational complexity and achieves a near-optimal solution.      
### 103.Complex-amplitude Fourier single-pixel imaging via coherent structured illumination  [ :arrow_down: ](https://arxiv.org/pdf/2202.02527.pdf)
>  We propose a method of complex-amplitude Fourier single-pixel imaging (CFSI) with coherent structured illumination to acquire both the amplitude and phase of an object. In the proposed method, an object is illustrated by a series of coherent structured light fields which are generated by a phase-only spatial light modulator, the complex Fourier spectrum of the object can be acquired sequentially by a single-pixel photodetector. Then the desired complex-amplitude image can be retrieved directly by applying an inverse Fourier transform. We experimentally implemented this CFSI with several different types of objects. The experimental results show that the proposed method provides a promising complex-amplitude imaging approach with high quality and a stable configuration. Thus, it might find broad applications in optical metrology and biomedical science.      
### 104.Less is More: Reversible Steganography with Uncertainty-Aware Predictive Analytics  [ :arrow_down: ](https://arxiv.org/pdf/2202.02518.pdf)
>  Artificial neural networks have advanced the frontiers of reversible steganography. The core strength of neural networks is the ability to render accurate predictions for a bewildering variety of data. Residual modulation is recognised as the most advanced reversible steganographic algorithm for digital images and the pivot of which is the predictive module. The function of this module is to predict pixel intensity given some pixel-wise contextual information. This task can be perceived as a low-level vision problem and hence neural networks for addressing a similar class of problems can be deployed. On top of the prior art, this paper analyses the predictive uncertainty and endows the predictive module with the option to abstain when encountering a high level of uncertainty. Uncertainty analysis can be formulated as a pixel-level binary classification problem and tackled by both supervised and unsupervised learning. In contrast to handcrafted statistical analytics, learning-based analytics can learn to follow some general statistical principles and simultaneously adapt to a specific predictor. Experimental results show that steganographic performance can be remarkably improved by adaptively filtering out the unpredictable regions with the learning-based uncertainty analysers.      
### 105.A Neural Beam Filter for Real-time Multi-channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.02500.pdf)
>  Most deep learning-based multi-channel speech enhancement methods focus on designing a set of beamforming coefficients to directly filter the low signal-to-noise ratio signals received by microphones, which hinders the performance of these approaches. To handle these problems, this paper designs a causal neural beam filter that fully exploits the spatial-spectral information in the beam domain. Specifically, multiple beams are designed to steer towards all directions using a parameterized super-directive beamformer in the first stage. After that, the neural spatial filter is learned by simultaneously modeling the spatial and spectral discriminability of the speech and the interference, so as to extract the desired speech coarsely in the second stage. Finally, to further suppress the interference components especially at low frequencies, a residual estimation module is adopted to refine the output of the second stage. Experimental results demonstrate that the proposed approach outperforms many state-of-the-art multi-channel methods on the generated multi-channel speech dataset based on the DNS-Challenge dataset.      
### 106.Predicting Future CSI Feedback For Highly-Mobile Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.02492.pdf)
>  Massive multiple-input multiple-output (MIMO) system is promising in providing unprecedentedly high data rate. To achieve its full potential, the transceiver needs complete channel state information (CSI) to perform transmit/receive precoding/combining. This requirement, however, is challenging in the practical systems due to the unavoidable processing and feedback delays, which oftentimes degrades the performance to a great extent, especially in the high mobility scenarios. In this paper, we develop a deep learning based channel prediction framework that proactively predicts the downlink channel state information based on the past observed channel sequence. In its core, the model adopts a 3-D convolutional neural network (CNN) based architecture to efficiently learn the temporal, spatial and frequency correlations of downlink channel samples, based on which accurate channel prediction can be performed. Simulation results highlight the potential of the developed learning model in extracting information and predicting future downlink channels directly from the observed past channel sequence, which significantly improves the performance compared to the sample-and-hold approach, and mitigates the impact of the dynamic communication environment.      
### 107.Sensing Method for Two-Target Detection in Time-Constrained Vector Gaussian Channel  [ :arrow_down: ](https://arxiv.org/pdf/2202.02478.pdf)
>  This paper considers a vector Gaussian channel of fixed identity covariance matrix and binary input signalling as the mean of it. A linear transformation is performed on the vector input signal. The objective is to find the optimal scaling matrix, under the total time constraint, that would: i) maximize the mutual information between the input and output random vectors, ii) maximize the MAP detection. It was found that the two metrics lead to different optimal solutions for our experimental design problem. We have used the Monte Carlo method for our computational work.      
### 108.Advanced service data provisioning in ROF-based mobile backhauls/fronthauls  [ :arrow_down: ](https://arxiv.org/pdf/2202.02458.pdf)
>  A new cost-efficient concept to realize a real-time monitoring of quality-of-service metrics and other service data in 5G and beyond access network using a separate return channel based on a vertical cavity surface emitting laser in the optical injection locked mode that simultaneously operates as an optical transmitter and as a resonant cavity enhanced photodetector, is proposed and discussed. The feasibility and efficiency of the proposed approach are confirmed by a proof-of-concept experiment when optically transceiving high-speed digital signal with multi-position quadrature amplitude modulation of a radio-frequency carrier.      
### 109.Path Planning for the Dynamic UAV-Aided Wireless Systems using Monte Carlo Tree Search  [ :arrow_down: ](https://arxiv.org/pdf/2202.02457.pdf)
>  For UAV-aided wireless systems, online path planning attracts much attention recently. To better adapt to the real-time dynamic environment, we, for the first time, propose a Monte Carlo Tree Search (MCTS)-based path planning scheme. In details, we consider a single UAV acts as a mobile server to provide computation tasks offloading services for a set of mobile users on the ground, where the movement of ground users follows a Random Way Point model. Our model aims at maximizing the average throughput under energy consumption and user fairness constraints, and the proposed timesaving MCTS algorithm can further improve the performance. Simulation results show that the proposed algorithm achieves a larger average throughput and a faster convergence performance compared with the baseline algorithms of Q-learning and Deep Q-Network.      
### 110.Application of Machine Learning-Based Pattern Recognition in IoT Devices: Review  [ :arrow_down: ](https://arxiv.org/pdf/2202.02456.pdf)
>  The Internet of things (IoT) is a rapidly advancing area of technology that has quickly become more widespread in recent years. With greater numbers of everyday objects being connected to the Internet, many different innovations have been presented to make our everyday lives more straightforward. Pattern recognition is extremely prevalent in IoT devices because of the many applications and benefits that can come from it. A multitude of studies has been conducted with the intention of improving speed and accuracy, decreasing complexity, and reducing the overall required processing power of pattern recognition algorithms in IoT devices. After reviewing the applications of different machine learning algorithms, results vary from case to case, but a general conclusion can be drawn that the optimal machine learning-based pattern recognition algorithms to be used with IoT devices are support vector machine, k-nearest neighbor, and random forest.      
### 111.SEED: Sound Event Early Detection via Evidential Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2202.02441.pdf)
>  Sound Event Early Detection (SEED) is an essential task in recognizing the acoustic environments and soundscapes. However, most of the existing methods focus on the offline sound event detection, which suffers from the over-confidence issue of early-stage event detection and usually yield unreliable results. To solve the problem, we propose a novel Polyphonic Evidential Neural Network (PENet) to model the evidential uncertainty of the class probability with Beta distribution. Specifically, we use a Beta distribution to model the distribution of class probabilities, and the evidential uncertainty enriches uncertainty representation with evidence information, which plays a central role in reliable prediction. To further improve the event detection performance, we design the backtrack inference method that utilizes both the forward and backward audio features of an ongoing event. Experiments on the DESED database show that the proposed method can simultaneously improve 13.0\% and 3.8\% in time delay and detection F1 score compared to the state-of-the-art methods.      
### 112.Multistability and Paradoxes in Lossy Oscillator Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.02439.pdf)
>  The analysis of dissipatively coupled oscillators is a challenging problem with high stakes in actual applications, such as large scale physical systems. Many standard mathematical methods are not applicable to such systems, due to the lack of symmetry of the network induced by dissipative couplings. Here we emphasize that the synchronization of coupled oscillators can be equivalently interpreted as the problem of flow distribution over a network. Based on these equivalent interpretations, we demonstrate a close correspondence between multiple stable synchronous states and \emph{winding cells} in systems of dissipatively coupled oscillators. The recently introduced notion of winding cells, associated to a graph, forms a natural winding partition of the $n$-torus and capture essential characteristics of synchronous states in lossless systems. Leveraging the winding partition of the $n$-torus, we provide algorithms to compute the synchronous solutions of general networks of coupled oscillators. Furthermore, we identify three paradoxical behaviors of lossy networked systems, to be contrasted with the behavior lossless systems. Namely, we show that loop flows and dissipation can increase the system's transfer capacity, and that dissipation can promote multistability.      
### 113.An explicit dual control approach for constrained reference tracking of uncertain linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.02351.pdf)
>  A finite horizon optimal tracking problem is considered for linear dynamical systems subject to parametric uncertainties in the state-space matrices and exogenous disturbances. A suboptimal solution is proposed using a model predictive control (MPC) based explicit dual control approach which enables active uncertainty learning. A novel algorithm for the design of robustly invariant online terminal sets and terminal controllers is presented. Set membership identification is used to update the parameter uncertainty online. A predicted worst-case cost is used in the MPC optimization problem to model the dual effect of the control input. The cost-to-go is estimated using contractivity of the proposed terminal set and the remaining time horizon, so that the optimizer can estimate future benefits of exploration. The proposed dual control algorithm ensures robust constraint satisfaction and recursive feasibility, and navigates the exploration-exploitation trade-off using a robust performance metric.      
