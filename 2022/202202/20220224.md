# ArXiv eess --Thu, 24 Feb 2022
### 1.Interval Observer Synthesis for Locally Lipschitz Nonlinear Dynamical Systems via Mixed-Monotone Decompositions  [ :arrow_down: ](https://arxiv.org/pdf/2202.11689.pdf)
>  This paper proposes a novel unified interval-valued observer synthesis approach for locally Lipschitz nonlinear continuous-time (CT) and discrete-time (DT) systems with nonlinear observations. A key feature of our proposed observer, which is derived using mixed-monotone decompositions, is that it is correct by construction (i.e., the true state trajectory of the system is framed by the states of the observer) without the need for imposing additional constraints and assumptions such as global Lipschitz continuity or contraction, as is done in existing approaches in the literature. Furthermore, we derive sufficient conditions for designing stabilizing observer gains in the form of Linear Matrix Inequalities (LMIs). Finally, we compare the performance of our observer design with some benchmark CT and DT observers in the literature.      
### 2.Fusion of Probability Density Functions  [ :arrow_down: ](https://arxiv.org/pdf/2202.11633.pdf)
>  Fusing probabilistic information is a fundamental task in signal and data processing with relevance to many fields of technology and science. In this work, we investigate the fusion of multiple probability density functions (pdfs) of a continuous random variable or vector. Although the case of continuous random variables and the problem of pdf fusion frequently arise in multisensor signal processing, statistical inference, and machine learning, a universally accepted method for pdf fusion does not exist. The diversity of approaches, perspectives, and solutions related to pdf fusion motivates a unified presentation of the theory and methodology of the field. We discuss three different approaches to fusing pdfs. In the axiomatic approach, the fusion rule is defined indirectly by a set of properties (axioms). In the optimization approach, it is the result of minimizing an objective function that involves an information-theoretic divergence or a distance measure. In the supra-Bayesian approach, the fusion center interprets the pdfs to be fused as random observations. Our work is partly a survey, reviewing in a structured and coherent fashion many of the concepts and methods that have been developed in the literature. In addition, we present new results for each of the three approaches. Our original contributions include new fusion rules, axioms, and axiomatic and optimization-based characterizations; a new formulation of supra-Bayesian fusion in terms of finite-dimensional parametrizations; and a study of supra-Bayesian fusion of posterior pdfs for linear Gaussian models.      
### 3.Identifying Oscillations Injected by Inverter-Based Solar Energy Sources  [ :arrow_down: ](https://arxiv.org/pdf/2202.11579.pdf)
>  Inverter-based solar energy sources are becoming widely integrated into modern power systems. However, their impacts on the system in the frequency domain are rarely investigated at a higher frequency range than conventional electromechanical oscillations. This paper presents evidence of the emergence of an oscillation mode injected by inverter-based solar energy sources in Dominion Energy's service territory. This new mode was recognized from the analysis of real-world ambient synchrophasor and point-of-wave data. The analysis was performed by developing customized synchrophasor analysis tools deployed on the PredictiveGrid^{TM} platform implemented at Dominion Energy. Herein, we describe and illustrate the preliminary analysis results acquired from spectrogram observations, power spectral density plots, and mode shape estimation. The emergence and propagation of this new mode in Dominion Energy's footprint is illustrated using a heatmap based on a proposed frequency component energy metric, which helps to assess this oscillation's spread and impact.      
### 4.Weakly-supervised learning for image-based classification of primary melanomas into genomic immune subgroups  [ :arrow_down: ](https://arxiv.org/pdf/2202.11524.pdf)
>  Determining early-stage prognostic markers and stratifying patients for effective treatment are two key challenges for improving outcomes for melanoma patients. Previous studies have used tumour transcriptome data to stratify patients into immune subgroups, which were associated with differential melanoma specific survival and potential treatment strategies. However, acquiring transcriptome data is a time-consuming and costly process. Moreover, it is not routinely used in the current clinical workflow. Here we attempt to overcome this by developing deep learning models to classify gigapixel H&amp;E stained pathology slides, which are well established in clinical workflows, into these immune subgroups. Previous subtyping approaches have employed supervised learning which requires fully annotated data, or have only examined single genetic mutations in melanoma patients. We leverage a multiple-instance learning approach, which only requires slide-level labels and uses an attention mechanism to highlight regions of high importance to the classification. Moreover, we show that pathology-specific self-supervised models generate better representations compared to pathology-agnostic models for improving our model performance, achieving a mean AUC of 0.76 for classifying histopathology images as high or low immune subgroups. We anticipate that this method may allow us to find new biomarkers of high importance and could act as a tool for clinicians to infer the immune landscape of tumours and stratify patients, without needing to carry out additional expensive genetic tests.      
### 5.MITI: SLAM Benchmark for Laparoscopic Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2202.11496.pdf)
>  We propose a new benchmark for evaluating stereoscopic visual-inertial computer vision algorithms (SLAM/ SfM/ 3D Reconstruction/ Visual-Inertial Odometry) for minimally invasive surgical (MIS) interventions in the abdomen. Our MITI Dataset available at [<a class="link-external link-https" href="https://mediatum.ub.tum.de/1621941" rel="external noopener nofollow">this https URL</a>] provides all the necessary data by a complete recording of a handheld surgical intervention at Research Hospital Rechts der Isar of TUM. It contains multimodal sensor information from IMU, stereoscopic video, and infrared (IR) tracking as ground truth for evaluation. Furthermore, calibration for the stereoscope, accelerometer, magnetometer, the rigid transformations in the sensor setup, and time-offsets are available. We wisely chose a suitable intervention that contains very few cutting and tissue deformation and shows a full scan of the abdomen with a handheld camera such that it is ideal for testing SLAM algorithms. Intending to promote the progress of visual-inertial algorithms designed for MIS application, we hope that our clinical training dataset helps and enables researchers to enhance algorithms.      
### 6.Networked Online Learning for Control of Safety-Critical Resource-Constrained Systems based on Gaussian Processes  [ :arrow_down: ](https://arxiv.org/pdf/2202.11491.pdf)
>  Safety-critical technical systems operating in unknown environments require the ability to quickly adapt their behavior, which can be achieved in control by inferring a model online from the data stream generated during operation. Gaussian process-based learning is particularly well suited for safety-critical applications as it ensures bounded prediction errors. While there exist computationally efficient approximations for online inference, these approaches lack guarantees for the prediction error and have high memory requirements, and are therefore not applicable to safety-critical systems with tight memory constraints. In this work, we propose a novel networked online learning approach based on Gaussian process regression, which addresses the issue of limited local resources by employing remote data management in the cloud. Our approach formally guarantees a bounded tracking error with high probability, which is exploited to identify the most relevant data to achieve a certain control performance. We further propose an effective data transmission scheme between the local system and the cloud taking bandwidth limitations and time delay of the transmission channel into account. The effectiveness of the proposed method is successfully demonstrated in a simulation.      
### 7.Augmentation based unsupervised domain adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2202.11486.pdf)
>  The insertion of deep learning in medical image analysis had lead to the development of state-of-the art strategies in several applications such a disease classification, as well as abnormality detection and segmentation. However, even the most advanced methods require a huge and diverse amount of data to generalize. Because in realistic clinical scenarios, data acquisition and annotation is expensive, deep learning models trained on small and unrepresentative data tend to outperform when deployed in data that differs from the one used for training (e.g data from different scanners). In this work, we proposed a domain adaptation methodology to alleviate this problem in segmentation models. Our approach takes advantage of the properties of adversarial domain adaptation and consistency training to achieve more robust adaptation. Using two datasets with white matter hyperintensities (WMH) annotations, we demonstrated that the proposed method improves model generalization even in corner cases where individual strategies tend to fail.      
### 8.Personalized PPG Normalization based on Subject Heartbeat in Resting State Condition  [ :arrow_down: ](https://arxiv.org/pdf/2202.11465.pdf)
>  Physiological responses are nowadays widely used to recognize the affective state of subjects in real-life scenarios. However, these data are intrinsically subject-dependent, making machine learning techniques for data classification not easily applicable due to inter-subject variability. In this work, the reduction of inter-subject heterogeneity is considered in the case of PhotoPlethysmoGraphy (PPG), which is successfully used to detect stress and evaluate experienced cognitive load. To face the inter-subject heterogeneity, a novel personalized PPG normalization is here proposed. A subject-normalized discrete domain where the PPG signals are properly re-scaled is introduced, considering the subject's heartbeat frequency in resting state conditions. The effectiveness of the proposed normalization is evaluated in comparison with other normalization procedures in a binary classification task, where cognitive load and relaxing state are considered. The results obtained on two different datasets available in the literature confirm that applying the proposed normalization strategy permits to increase classification performance.      
### 9.Mixed-Block Neural Architecture Search for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.11401.pdf)
>  Deep Neural Networks (DNNs) have the potential for making various clinical procedures more time-efficient by automating medical image segmentation. Due to their strong, in some cases human-level, performance, they have become the standard approach in this field. The design of the best possible medical image segmentation DNNs, however, is task-specific. Neural Architecture Search (NAS), i.e., the automation of neural network design, has been shown to have the capability to outperform manually designed networks for various tasks. However, the existing NAS methods for medical image segmentation have explored a quite limited range of types of DNN architectures that can be discovered. In this work, we propose a novel NAS search space for medical image segmentation networks. This search space combines the strength of a generalised encoder-decoder structure, well known from U-Net, with network blocks that have proven to have a strong performance in image classification tasks. The search is performed by looking for the best topology of multiple cells simultaneously with the configuration of each cell within, allowing for interactions between topology and cell-level attributes. From experiments on two publicly available datasets, we find that the networks discovered by our proposed NAS method have better performance than well-known handcrafted segmentation networks, and outperform networks found with other NAS approaches that perform only topology search, and topology-level search followed by cell-level search.      
### 10.Measurement of the Interactions and Stability of MTDC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11399.pdf)
>  The small-signal stability of multi-terminal HVDC systems, which is related to the dynamic interactions among different VSCs through the coupling of DC and AC networks, has become one of the important issues for the safety and stable operation of modern power systems. On the other hand, the robust stability theory with {\nu}-gap metric is an effective tool for the stability analysis and synthesis of uncertain feedback systems. In this paper, we combine it with the self-/en-stabilizing coefficients method to measure the relative stability and analyze the stability influence of different paths of interactions in an MTDC system. The stability index is defined to represent the stability margin with respect to different paths of interactions in an MTDC system. A method for calculating the range of uncertain parameters preserving the stability is presented based on the stability criterion. The influence of control parameters on robust stability through interactions among VSCs can be analyzed quantitatively. Extensive examples are given to demonstrate the application and the effectiveness of the proposed method.      
### 11.Reinforcement Learning from Demonstrations by Novel Interactive Expert and Application to Automatic Berthing Control Systems for Unmanned Surface Vessel  [ :arrow_down: ](https://arxiv.org/pdf/2202.11325.pdf)
>  In this paper, two novel practical methods of Reinforcement Learning from Demonstration (RLfD) are developed and applied to automatic berthing control systems for Unmanned Surface Vessel. A new expert data generation method, called Model Predictive Based Expert (MPBE) which combines Model Predictive Control and Deep Deterministic Policy Gradient, is developed to provide high quality supervision data for RLfD algorithms. A straightforward RLfD method, model predictive Deep Deterministic Policy Gradient (MP-DDPG), is firstly introduced by replacing the RL agent with MPBE to directly interact with the environment. Then distribution mismatch problem is analyzed for MP-DDPG, and two techniques that alleviate distribution mismatch are proposed. Furthermore, another novel RLfD algorithm based on the MP-DDPG, called Self-Guided Actor-Critic (SGAC) is present, which can effectively leverage MPBE by continuously querying it to generate high quality expert data online. The distribution mismatch problem leading to unstable learning process is addressed by SGAC in a DAgger manner. In addition, theoretical analysis is given to prove that SGAC algorithm can converge with guaranteed monotonic improvement. Simulation results verify the effectiveness of MP-DDPG and SGAC to accomplish the ship berthing control task, and show advantages of SGAC comparing with other typical reinforcement learning algorithms and MP-DDPG.      
### 12.Improving fairness in speaker verification via Group-adapted Fusion Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.11323.pdf)
>  Modern speaker verification models use deep neural networks to encode utterance audio into discriminative embedding vectors. During the training process, these networks are typically optimized to differentiate arbitrary speakers. This learning process biases the learning of fine voice characteristics towards dominant demographic groups, which can lead to an unfair performance disparity across different groups. This is observed especially with underrepresented demographic groups sharing similar voice characteristics. In this work, we investigate the fairness of speaker verification models on controlled datasets with imbalanced gender distributions, providing direct evidence that model performance suffers for underrepresented groups. To mitigate this disparity we propose the group-adapted fusion network (GFN) architecture, a modular architecture based on group embedding adaptation and score fusion. We show that our method alleviates model unfairness by improving speaker verification both overall and for individual groups. Given imbalanced group representation in training, our proposed method achieves overall equal error rate (EER) reduction of 9.6% to 29.0% relative, reduces minority group EER by 13.7% to 18.6%, and results in 20.0% to 25.4% less EER disparity, compared to baselines. The approach is applicable to other types of training data skew in speaker recognition systems.      
### 13.End-to-end LPCNet: A Neural Vocoder With Fully-Differentiable LPC Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2202.11301.pdf)
>  Neural vocoders have recently demonstrated high quality speech synthesis, but typically require a high computational complexity. LPCNet was proposed as a way to reduce the complexity of neural synthesis by using linear prediction~(LP) to assist an autoregressive model. At inference time, LPCNet relies on the LP coefficients being explicitly computed from the input acoustic features. That makes the design of LPCNet-based systems more complicated, while adding the constraint that the input features must represent a clean speech spectrum. We propose an end-to-end version of LPCNet that lifts these limitations by learning to infer the LP coefficients in the frame rate network from the input features. Results show that the proposed end-to-end approach can reach the same level of quality as the original LPCNet model, but without explicit LP analysis. Our open-source end-to-end model still benefits from LPCNet's low complexity, while allowing for any type of conditioning features.      
### 14.A 5.3 GHz Al0.76Sc0.24N Two-Dimensional Resonant Rods Resonator with a Record kt2 of 23.9%  [ :arrow_down: ](https://arxiv.org/pdf/2202.11284.pdf)
>  This work reports on the measured performance of an Aluminum Scandium Nitride (AlScN) Two-Dimensional Resonant Rods resonator (2DRR), fabricated by using a Sc-doping concentration of 24%, characterized by an ultra-low impedance (~25 Ohm) and exhibiting an all-time record electromechanical coupling coefficient (kt2) of 23.9%. In order to achieve such unprecedented performance, we identified and relied on optimized deposition and etching processes for highly-doped AlScN films, aiming at achieving high crystalline quality, low density of abnormal grains in the 2DRR's active region and sharp lateral sidewalls. Also, the 2DRR's unit cell has been acoustically engineered to maximize the piezo-generated mechanical energy within each rod and to ensure a low transduction of spurious modes around resonance. Due to its unprecedented kt2, the reported 2DRR opens exciting scenarios towards the development of next-generation monolithic integrated radio-frequency (RF) filtering components. In fact, we show that 5th-order 2DRR-based ladder filters with fractional bandwidths (BW) of ~11%, insertion-loss (I.L) values of ~2.5 dB and with larger than 30 dB out-of-band rejections can now be envisioned, paving an unprecedented path towards the development of ultra-wide band (UWB) filters for next-generation Super-High-Frequency (SHF) radio front-ends.      
### 15.Learning Neural Networks under Input-Output Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2202.11246.pdf)
>  In this paper, we examine an important problem of learning neural networks that certifiably meet certain specifications on input-output behaviors. Our strategy is to find an inner approximation of the set of admissible policy parameters, which is convex in a transformed space. To this end, we address the key technical challenge of convexifying the verification condition for neural networks, which is derived by abstracting the nonlinear specifications and activation functions with quadratic constraints. In particular, we propose a reparametrization scheme of the original neural network based on loop transformation, which leads to a convex condition that can be enforced during learning. This theoretical construction is validated in an experiment that specifies reachable sets for different regions of inputs.      
### 16.Hierarchical MPC for coupled subsystems using adjustable tubes  [ :arrow_down: ](https://arxiv.org/pdf/2202.11228.pdf)
>  A hierarchical Model Predictive Control (MPC) formulation is presented for coupled discrete-time linear systems with state and input constraints. Compared to a centralized approach, a two-level hierarchical controller, with one controller in the upper-level and one controller per subsystem in the lower-level, can significantly reduce the computational cost associated with MPC. Hierarchical coordination is achieved using adjustable tubes, which are optimized by the upper-level controller and bound permissible lower-level controller deviations from the system trajectories determined by the upper-level controller. The size of these adjustable tubes determines the degree of uncertainty between subsystems and directly affects the required constraint tightening under a tube-based robust MPC framework. Sets are represented as zonotopes to enable the ability to optimize the size of these adjustable tubes and perform the necessary constraint tightening online as part of the MPC optimization problems. State and input constraint satisfaction is proven for the two-level hierarchical controller with an arbitrary number of controllers at the lower-level and a numerical example demonstrates the key features and performance of the approach.      
### 17.Fixed-time Stability of Discrete-time Autonomous Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11225.pdf)
>  The fixed-time stability of autonomous nonlinear discrete-time (DT) systems is studied in this paper. Novel Lyapunov conditions are presented under which the fixed-time stability of autonomous DT system is certified. Extension to systems under perturbations is also considered. The sensitivity to perturbations for fixed-time stable DT systems is analyzed and it is shown that fixed-time attractiveness can be resulted from the presented Lyapunov conditions. The fixed upper bound of the settling-time functions is derived for fixed-time stable and fixed-time attractive systems. An illustrative example with simulations are also given to verify the introduced results.      
### 18.Functional Parcellation of fMRI data using multistage k-means clustering  [ :arrow_down: ](https://arxiv.org/pdf/2202.11206.pdf)
>  Purpose: Functional Magnetic Resonance Imaging (fMRI) data acquired through resting-state studies have been used to obtain information about the spontaneous activations inside the brain. One of the approaches for analysis and interpretation of resting-state fMRI data require spatially and functionally homogenous parcellation of the whole brain based on underlying temporal fluctuations. Clustering is often used to generate functional parcellation. However, major clustering algorithms, when used for fMRI data, have their limitations. Among commonly used parcellation schemes, a tradeoff exists between intra-cluster functional similarity and alignment with anatomical regions. Approach: In this work, we present a clustering algorithm for resting state and task fMRI data which is developed to obtain brain parcellations that show high structural and functional homogeneity. The clustering is performed by multistage binary k-means clustering algorithm designed specifically for the 4D fMRI data. The results from this multistage k-means algorithm show that by modifying and combining different algorithms, we can take advantage of the strengths of different techniques while overcoming their limitations. Results: The clustering output for resting state fMRI data using the multistage k-means approach is shown to be better than simple k-means or functional atlas in terms of spatial and functional homogeneity. The clusters also correspond to commonly identifiable brain networks. For task fMRI, the clustering output can identify primary and secondary activation regions and provide information about the varying hemodynamic response across different brain regions. Conclusion: The multistage k-means approach can provide functional parcellations of the brain using resting state fMRI data. The method is model-free and is data driven which can be applied to both resting state and task fMRI.      
### 19.r-G2P: Evaluating and Enhancing Robustness of Grapheme to Phoneme Conversion by Controlled noise introducing and Contextual information incorporation  [ :arrow_down: ](https://arxiv.org/pdf/2202.11194.pdf)
>  Grapheme-to-phoneme (G2P) conversion is the process of converting the written form of words to their pronunciations. It has an important role for text-to-speech (TTS) synthesis and automatic speech recognition (ASR) systems. In this paper, we aim to evaluate and enhance the robustness of G2P models. We show that neural G2P models are extremely sensitive to orthographical variations in graphemes like spelling mistakes. To solve this problem, we propose three controlled noise introducing methods to synthesize noisy training data. Moreover, we incorporate the contextual information with the baseline and propose a robust training strategy to stabilize the training process. The experimental results demonstrate that our proposed robust G2P model (r-G2P) outperforms the baseline significantly (-2.73\% WER on Dict-based benchmarks and -9.09\% WER on Real-world sources).      
### 20.Modal Estimation on a Warped Frequency Axis for Linear System Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2202.11192.pdf)
>  Linear systems such as room acoustics and string oscillations may be modeled as the sum of mode responses, each characterized by a frequency, damping and amplitude. Here, we consider finding the mode parameters from impulse response measurements, and estimate the mode frequencies and decay rates as the generalized eigenvalues of Hankel matrices of system response samples, similar to ESPRIT. For greater resolution at low frequencies, such as desired in room acoustics and musical instrument modeling, the estimation is done on a warped frequency axis. The approach has the benefit of selecting the number of modes to achieve a desired fidelity to the measured impulse response. An optimization to further refine the frequency and damping parameters is presented. The method is used to model coupled piano strings and room impulse responses, with its performance comparing favorably to FZ-ARMA.      
### 21.Real Time Adaptive Estimation of Li-ion Battery Bank Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2202.11191.pdf)
>  This paper proposes an accurate and efficient Universal Adaptive Stabilizer (UAS) based online parameters estimation technique for a 400 V Li-ion battery bank. The battery open circuit voltage, parameters modeling the transient response, and series resistance are all estimated in a single real-time test. In contrast to earlier UAS based work on individual battery packs, this work does not require prior offline experimentation or any post-processing. Real time fast convergence of parameters' estimates with minimal experimental effort enables self-update of battery parameters in run-time. The proposed strategy is mathematically validated and its performance is demonstrated on a 400 V, 6.6 Ah Li-ion battery bank powering the induction motor driven prototype electric vehicle (EV) traction system.      
### 22.Mathematical foundation of sparsity-based multi-illumination super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2202.11189.pdf)
>  It is well-known that the resolution of a traditional optical imaging system is limited by the so-called Rayleigh limit, which is of several hundreds of nanometers. By employing fluorescence techniques, modern microscopic methods can resolve point scatterers separated much lower than the Rayleigh limit. Localization-based fluorescence subwavelength imaging techniques such as PALM and STORM can achieve a spatial resolution of several tens of nanometers. However, these techniques have limited temporal resolution as they require tens of thousands of exposures. Employing sparsity-based models and recovery algorithms is a natural way to reduce the number of exposures and hence obtain high temporal resolution. Recently, a new multi-illumination imaging technique called Brownian Excitation Amplitude Modulation microscopy (BEAM) is introduced. BEAM achieves a threefold resolution improvement by applying a compressive sensing recovery algorithm over only few frames. Motivated by BEAM, our aim in this paper is to pioneer the mathematical foundation for sparsity-based multi-illumination super-resolution. We consider several diffraction-limited images from samples exposed to different illumination patterns and recover the source by considering the sparsest solution. We estimate the minimum separation distance between point scatterers so that they could be stably recovered. By this estimation, we reveal the dependence of the resolution on the cut-off frequency of the imaging system, the SNR, the sparsity of point scatterers, and the incoherence of illumination patterns. Our theory particularly highlights the importance of the high incoherence of illumination patterns in enhancing the resolution. It also demonstrates that super-resolution can be achieved using sparsity-based multi-illumination imaging with very few frames, whereby the spatio-temporal super-resolution becomes possible.      
### 23.A Barrier-Based Scenario Approach to Verify Safety-Critical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11177.pdf)
>  In this letter, we detail our randomized approach to safety-critical system verification. Our method requires limited system data to make a strong verification statement. Specifically, our method first randomly samples initial conditions and parameters for a controlled, continuous-time system and records the ensuing state trajectory at discrete intervals. Then, we evaluate these states under a candidate barrier function $h$ to determine the constraints for a randomized linear program. The solution to this program then provides either a probabilistic verification statement or a counterexample. To show the validity of our results, we verify the robotarium simulator and identify counterexamples for its hardware counterpart. We also provide numerical evidence to validate our verification statements in the same setting. Furthermore, we show that our method is system-independent by performing the same verification method on a quadrupedal system in a multi-agent setting as well.      
### 24.Neural Speech Synthesis on a Shoestring: Improving the Efficiency of LPCNet  [ :arrow_down: ](https://arxiv.org/pdf/2202.11169.pdf)
>  Neural speech synthesis models can synthesize high quality speech but typically require a high computational complexity to do so. In previous work, we introduced LPCNet, which uses linear prediction to significantly reduce the complexity of neural synthesis. In this work, we further improve the efficiency of LPCNet -- targeting both algorithmic and computational improvements -- to make it usable on a wide variety of devices. We demonstrate an improvement in synthesis quality while operating 2.5x faster. The resulting open-source LPCNet algorithm can perform real-time neural synthesis on most existing phones and is even usable in some embedded devices.      
### 25.RIS-Enabled Self-Localization: Leveraging Controllable Reflections With Zero Access Points  [ :arrow_down: ](https://arxiv.org/pdf/2202.11159.pdf)
>  Reconfigurable intelligent surfaces (RISs) are one of the most promising technological enablers of the next (6th) generation of wireless systems. In this paper, we introduce a novel use-case of the RIS technology in radio localization, which is enabling the user to estimate its own position via transmitting orthogonal frequency-division multiplexing (OFDM) pilots and processing the signal reflected from the RIS. We demonstrate that user localization in this scenario is possible by deriving Cramer-Rao lower bounds on the positioning error and devising a low-complexity position estimation algorithm. We consider random and directional RIS phase profiles and apply a specific temporal coding to them, such that the reflected signal from the RIS can be separated from the uncontrolled multipath. Finally, we assess the performance of our position estimator for an example system and show that the proposed algorithm can attain the derived bound at high signal-to-noise ratio values.      
### 26.Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal Texture Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2202.11703.pdf)
>  We present a novel U-Attention vision Transformer for universal texture synthesis. We exploit the natural long-range dependencies enabled by the attention mechanism to allow our approach to synthesize diverse textures while preserving their structures in a single inference. We propose a multi-stage hourglass backbone that attends to the global structure and performs patch mapping at varying scales in a coarse-to-fine-to-coarse stream. Further completed by skip connection and convolution designs that propagate and fuse information at different scales, our U-Attention architecture unifies attention to microstructures, mesostructures and macrostructures, and progressively refines synthesis results at successive stages. We show that our method achieves stronger 2$\times$ synthesis than previous work on both stochastic and structured textures while generalizing to unseen textures without fine-tuning. Ablation studies demonstrate the effectiveness of each component of our architecture.      
### 27.Deep Reinforcement Learning based Joint Active and Passive Beamforming Design for RIS-Assisted MISO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11702.pdf)
>  Owing to the unique advantages of low cost and controllability, reconfigurable intelligent surface (RIS) is a promising candidate to address the blockage issue in millimeter wave (mmWave) communication systems, consequently has captured widespread attention in recent years. However, the joint active beamforming and passive beamforming design is an arduous task due to the high computational complexity and the dynamic changes of wireless environment. In this paper, we consider a RIS-assisted multi-user multiple-input single-output (MU-MISO) mmWave system and aim to develop a deep reinforcement learning (DRL) based algorithm to jointly design active hybrid beamformer at the base station (BS) side and passive beamformer at the RIS side. By employing an advanced soft actor-critic (SAC) algorithm, we propose a maximum entropy based DRL algorithm, which can explore more stochastic policies than deterministic policy, to design active analog precoder and passive beamformer simultaneously. Then, the digital precoder is determined by minimum mean square error (MMSE) method. The experimental results demonstrate that our proposed SAC algorithm can achieve better performance compared with conventional optimization algorithm and DRL algorithm.      
### 28.Gaussian Process-Driven History Matching for Physical Layer Parameter Estimation in Optical Fiber Communication Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.11700.pdf)
>  We present a methodology for the estimation of optical network physical layer parameters from signal to noise ratio via history matching. An expensive network link simulator is emulated by a Gaussian process surrogate model, which is used to estimate a set of physical layer parameters from simulated ground truth data. The a priori knowledge assumed consists of broad parameter bounds obtained from the literature and specification sheets of typical network components, and the physics-based model of the simulator. Accurate estimation of the physical layer parameters is demonstrated with a signal to noise ratio penalty of 1~dB or greater, using only 3 simulated measurements. The proposed approach is highly flexible, allowing for the calibration of any unknown simulator input from broad a priori bounds. The role of this method in the improvement of optical network modeling is discussed.      
### 29.Exploiting Side Information for Improved Online Learning Algorithms in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.11699.pdf)
>  In wireless networks, the rate achieved depends on factors like level of interference, hardware impairments, and channel gain. Often, instantaneous values of some of these factors can be measured, and they provide useful information about the instantaneous rate achieved. For example, higher interference implies a lower rate. In this work, we treat any such measurable quality that has a non-zero correlation with the rate achieved as side-information and study how it can be exploited to quickly learn the channel that offers higher throughput (reward). When the mean value of the side-information is known, using control variate theory we develop algorithms that require fewer samples to learn the parameters and can improve the learning rate compared to cases where side-information is ignored. Specifically, we incorporate side-information in the classical Upper Confidence Bound (UCB) algorithm and quantify the gain achieved in the regret performance. We show that the gain is proportional to the amount of the correlation between the reward and associated side-information. We discuss in detail various side-information that can be exploited in cognitive radio and air-to-ground communication in $L-$band. We demonstrate that correlation between the reward and side-information is often strong in practice and exploiting it improves the throughput significantly.      
### 30.Double Threshold based Optimal Device Selection Scheme for D2D or Sidelink Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.11696.pdf)
>  Device-to-device (D2D) or Sidelink aided communication is regarded as one of the most promising technologies to improve the spectral efficiency of the 5G and beyond communication system. However, two main challenges exist: 1) the selection of the optimal number of devices for improving the spectral efficiency, and 2) improving the physical layer security of such a communication system. The optimal device improves the secrecy capacity, and the selection of optimal devices enhances the physical layer security. Therefore, in this paper, we propose a double threshold-based optimal device selection (ODS) scheme for a cooperative wireless network with amplify and forward (AF) protocol in the presence and absence of an eavesdropper to enhance the physical layer security for the D2D network. The proposed scheme is analyzed with different distance cases, device scenarios, and modulation schemes. The bit error rate (BER) performance analysis concludes that a performance gain of more than 4 dB is achieved at the BER of 0:004 for the proposed double threshold-based ODS scheme compared to the existing ODS scheme at a high signal to noise ratio (SNR). Furthermore, the proposed scheme enhances the physical layer security.      
### 31.Hybrid Mechanical and Electronic Beam Steering for Maximizing OAM Channel Capacity  [ :arrow_down: ](https://arxiv.org/pdf/2202.11693.pdf)
>  Radio frequency-orbital angular momentum (RF-OAM) is a novel approach of multiplexing a set of orthogonal modes on the same frequency channel to achieve high spectrum efficiencies. Since OAM requires precise alignment of the transmit and the receive antennas, the electronic beam steering approach has been proposed for the uniform circular array (UCA)-based OAM communication system to circumvent large performance degradation induced by small antenna misalignment in practical environment. However, in the case of large-angle misalignment, the OAM channel capacity can not be effectively compensated only by the electronic beam steering. To solve this problem, we propose a hybrid mechanical and electronic beam steering scheme, in which mechanical rotating devices controlled by pulse width modulation (PWM) signals as the execution unit are utilized to eliminate the large misalignment angle, while electronic beam steering is in charge of the remaining small misalignment angle caused by perturbations. Furthermore, due to the interferometry, the receive signal-to-noise ratios (SNRs) are not uniform at the elements of the receive UCA. Therefore, a rotatable UCA structure is proposed for the OAM receiver to maximize the channel capacity, in which the simulated annealing algorithm is adopted to obtain the optimal rotation angle at first, then the servo system performs mechanical rotation, at last the electronic beam steering is adjusted accordingly. Both mathematical analysis and simulation results validate that the proposed hybrid mechanical and electronic beam steering scheme can effectively eliminate the effect of diverse misalignment errors of any practical OAM channel and maximize the OAM channel capacity.      
### 32.Brain Structural Saliency Over The Ages  [ :arrow_down: ](https://arxiv.org/pdf/2202.11690.pdf)
>  Brain Age (BA) estimation via Deep Learning has become a strong and reliable bio-marker for brain health, but the black-box nature of Neural Networks does not easily allow insight into the causal features of brain ageing. We trained a ResNet model as a BA regressor on T1 structural MRI volumes from a small cross-sectional cohort of 524 individuals. Using Layer-wise Relevance Propagation (LRP) and DeepLIFT saliency mapping techniques, we analysed the trained model to determine the most revealing structures over the course of brain ageing for the network, and compare these between the saliency mapping techniques. We show the change in attribution of relevance to different brain regions through the course of ageing. A tripartite pattern of relevance attribution to brain regions emerges. Some regions increase in relevance with age (e.g. the right Transverse Temporal Gyrus, known to be affected by healthy ageing); some decrease in relevance with age (e.g. the right Fourth Ventricle, known to dilate with age); and others remained consistently relevant across ages. We also examine the effect of Brain Age Delta (DBA) on the distribution of relevance within the brain volume. It is hoped that these findings will provide clinically relevant region-wise trajectories for normal brain ageing, and a baseline against which to compare brain ageing trajectories.      
### 33.Cyclical Variational Bayes Monte Carlo for Efficient Multi-Modal Posterior Distributions Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2202.11645.pdf)
>  Statistical model updating is frequently used in engineering to calculate the uncertainty of some unknown latent parameters when a set of measurements on observable quantities is given. Variational inference is an alternative approach to sampling methods that has been developed by the machine learning community to estimate posterior approximations through an optimization approach. In this paper, the Variational Bayesian Monte Carlo (VBMC) method is investigated with the purpose of dealing with statistical model updating problems in engineering involving expensive-to-run models. This method combines the active-sampling Bayesian quadrature with a Gaussian-process based variational inference to yield a non-parametric estimation of the posterior distribution of the identified parameters involving few runs of the expensive-to-run model. VBMC can also be used for model selection as it produces an estimation of the model's evidence lower bound. In this paper, a variant of the VBMC algorithm is developed through the introduction of a cyclical annealing schedule into the algorithm. The proposed cyclical VBMC algorithm allows to deal effectively with multi-modal posteriors by having multiple cycles of exploration and exploitation phases. Four numerical examples are used to compare the standard VBMC algorithm, the monotonic VBMC, the cyclical VBMC and the Transitional Ensemble Markov Chain Monte Carlo (TEMCMC). Overall, it is found that the proposed cyclical VBMC approach yields accurate results with a very reduced number of model runs compared to the state of the art sampling technique TEMCMC. In the presence of potential multi-modal problems, the proposed cyclical VBMC algorithm outperforms all the other approaches in terms of accuracy of the resulting posterior.      
### 34.Joint IRS Location and Size Optimization in Multi-IRS Aided Two-Way Full-Duplex Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11602.pdf)
>  Intelligent reflecting surfaces (IRSs) have emerged as a promising wireless technology for the dynamic configuration and control of electromagnetic waves, thus creating a smart (programmable) radio environment. In this context, we study a multi-IRS assisted two-way communication system consisting of two users that employ full-duplex (FD) technology. More specifically, we deal with the joint IRS location and size (i.e., the number of reflecting elements) optimization in order to minimize an upper bound of system outage probability under various constraints, namely, minimum and maximum number of reflecting elements per IRS, maximum number of installed IRSs, maximum total number of reflecting elements (implicit bound on the signaling overhead) as well as maximum total IRS installation cost. First, the problem is formulated as a discrete optimization problem and, then, a theoretical proof of its NP-hardness is given. Moreover, we provide a lower bound on the optimum value by solving a linear-programming relaxation (LPR) problem. Subsequently, we design two polynomial-time algorithms, a deterministic greedy algorithm and a randomized approximation algorithm, based on the LPR solution. The former is a heuristic method that always computes a feasible solution for which (a posteriori) performance guarantee can be provided. The latter achieves an approximate solution, using randomized rounding, with provable (a priori) probabilistic guarantees on the performance. Furthermore, extensive numerical simulations demonstrate the superiority of the proposed algorithms compared to the baseline schemes. Finally, useful conclusions regarding the comparison between FD and conventional half-duplex (HD) systems are also drawn.      
### 35.Diffractive optical system design by cascaded propagation  [ :arrow_down: ](https://arxiv.org/pdf/2202.11535.pdf)
>  Modern design of complex optical systems relies heavily on computational tools. These typically utilize geometrical optics as well as Fourier optics, which enables the use of diffractive elements to manipulate light with features on the scale of a wavelength. Fourier optics is typically used for designing thin elements, placed in the system's aperture, generating a shift-invariant Point Spread Function (PSF). A major bottleneck in applying Fourier Optics in many cases of interest, e.g. when dealing with multiple, or out-of-aperture elements, comes from numerical complexity. In this work, we propose and implement an efficient and differentiable propagation model based on the Collins integral, which enables the optimization of diffraction optical systems with unprecedented design freedom using backpropagation. We demonstrate the applicability of our method, numerically and experimentally, by engineering shift-variant PSFs via thin plate elements placed in arbitrary planes inside complex imaging systems, performing cascaded optimization of multiple planes, and designing optimal machine-vision systems by deep learning.      
### 36.A Comparative Study of Deep Reinforcement Learning-based Transferable Energy Management Strategies for Hybrid Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2202.11514.pdf)
>  The deep reinforcement learning-based energy management strategies (EMS) has become a promising solution for hybrid electric vehicles (HEVs). When driving cycles are changed, the network will be retrained, which is a time-consuming and laborious task. A more efficient way of choosing EMS is to combine deep reinforcement learning (DRL) with transfer learning, which can transfer knowledge of one domain to the other new domain, making the network of the new domain reach convergence values quickly. Different exploration methods of RL, including adding action space noise and parameter space noise, are compared against each other in the transfer learning process in this work. Results indicate that the network added parameter space noise is more stable and faster convergent than the others. In conclusion, the best exploration method for transferable EMS is to add noise in the parameter space, while the combination of action space noise and parameter space noise generally performs poorly. Our code is available at <a class="link-external link-https" href="https://github.com/BIT-XJY/RL-based-Transferable-EMS.git" rel="external noopener nofollow">this https URL</a>.      
### 37.AI-empowered Joint Communication and Radar Systems with Adaptive Waveform for Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2202.11508.pdf)
>  In Joint Communication and Radar (JCR)-based Autonomous Vehicle (AV) systems, optimizing waveform structure is one of the most challenging tasks due to strong influences between radar and data communication functions. Specifically, the preamble of a data communication frame is typically leveraged for the radar function. As such, the higher number of preambles in a Coherent Processing Interval (CPI) is, the greater radar's performance is. In contrast, communication efficiency decreases as the number of preambles increases. Moreover, AVs' surrounding radio environments are usually dynamic with high uncertainties due to their high mobility, making the JCR's waveform optimization problem even more challenging. To that end, this paper develops a novel JCR framework based on the Markov decision process framework and recent advanced techniques in deep reinforcement learning. By doing so, without requiring complete knowledge of the surrounding environment in advance, the JCR-AV can adaptively optimize its waveform structure (i.e., number of frames in the CPI) to maximize radar and data communication performance under the surrounding environment's dynamic and uncertainty. Extensive simulations show that our proposed approach can improve the joint communication and radar performance up to 46.26% compared with those of conventional methods (e.g., greedy policy- and fixed waveform-based approaches).      
### 38.A Survey on Technological Trends to Enhance Spectrum Efficiency in 6G Communications  [ :arrow_down: ](https://arxiv.org/pdf/2202.11493.pdf)
>  The research community has already identified that, by 2030, 5G networks will reach the capacity limits, and hence, will be inadequate to support next generation bandwidth-hungry, ubiquitous, intelligent services, and applications. Therefore, in view of sustaining the competitive edge of wireless technology and stratifying the next decade's communication requirements both, industry and research community have already begun conceptualizing the 6G technology. This article presents a detailed survey on the recent technological trends which address the capacity issues and enhance the spectrum-efficiency in 6G Communications. We present these trends in detail and then identify the challenges that need solutions before the practical deployment to realize 6G communications. Our survey article attempts to significantly contribute to initiating future research directions in the area of spectrum-efficiency in 6G communications.      
### 39.Towards Tailored Models on Private AIoT Devices: Federated Direct Neural Architecture Search  [ :arrow_down: ](https://arxiv.org/pdf/2202.11490.pdf)
>  Neural networks often encounter various stringent resource constraints while deploying on edge devices. To tackle these problems with less human efforts, automated machine learning becomes popular in finding various neural architectures that fit diverse Artificial Intelligence of Things (AIoT) scenarios. Recently, to prevent the leakage of private information while enable automated machine intelligence, there is an emerging trend to integrate federated learning and neural architecture search (NAS). Although promising as it may seem, the coupling of difficulties from both tenets makes the algorithm development quite challenging. In particular, how to efficiently search the optimal neural architecture directly from massive non-independent and identically distributed (non-IID) data among AIoT devices in a federated manner is a hard nut to crack. In this paper, to tackle this challenge, by leveraging the advances in ProxylessNAS, we propose a Federated Direct Neural Architecture Search (FDNAS) framework that allows for hardware-friendly NAS from non- IID data across devices. To further adapt to both various data distributions and different types of devices with heterogeneous embedded hardware platforms, inspired by meta-learning, a Cluster Federated Direct Neural Architecture Search (CFDNAS) framework is proposed to achieve device-aware NAS, in the sense that each device can learn a tailored deep learning model for its particular data distribution and hardware constraint. Extensive experiments on non-IID datasets have shown the state-of-the-art accuracy-efficiency trade-offs achieved by the proposed solution in the presence of both data and device heterogeneity.      
### 40.Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF  [ :arrow_down: ](https://arxiv.org/pdf/2202.11479.pdf)
>  This paper tackles post-hoc interpretability for audio processing networks. Our goal is to interpret decisions of a network in terms of high-level audio objects that are also listenable for the end-user. To this end, we propose a novel interpreter design that incorporates non-negative matrix factorization (NMF). In particular, a carefully regularized interpreter module is trained to take hidden layer representations of the targeted network as input and produce time activations of pre-learnt NMF components as intermediate outputs. Our methodology allows us to generate intuitive audio-based interpretations that explicitly enhance parts of the input signal most relevant for a network's decision. We demonstrate our method's applicability on popular benchmarks, including a real-world multi-label classification task.      
### 41.Towards Speaker Age Estimation with Label Distribution Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.11424.pdf)
>  Existing methods for speaker age estimation usually treat it as a multi-class classification or a regression problem. However, precise age identification remains a challenge due to label ambiguity, \emph{i.e.}, utterances from adjacent age of the same person are often indistinguishable. To address this, we utilize the ambiguous information among the age labels, convert each age label into a discrete label distribution and leverage the label distribution learning (LDL) method to fit the data. For each audio data sample, our method produces a age distribution of its speaker, and on top of the distribution we also perform two other tasks: age prediction and age uncertainty minimization. Therefore, our method naturally combines the age classification and regression approaches, which enhances the robustness of our method. We conduct experiments on the public NIST SRE08-10 dataset and a real-world dataset, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding a 10\% reduction in terms of mean absolute error (MAE) on a real-world dataset.      
### 42.Multi-scale Sparse Representation-Based Shadow Inpainting for Retinal OCT Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.11377.pdf)
>  Inpainting shadowed regions cast by superficial blood vessels in retinal optical coherence tomography (OCT) images is critical for accurate and robust machine analysis and clinical diagnosis. Traditional sequence-based approaches such as propagating neighboring information to gradually fill in the missing regions are cost-effective. But they generate less satisfactory outcomes when dealing with larger missing regions and texture-rich structures. Emerging deep learning-based methods such as encoder-decoder networks have shown promising results in natural image inpainting tasks. However, they typically need a long computational time for network training in addition to the high demand on the size of datasets, which makes it difficult to be applied on often small medical datasets. To address these challenges, we propose a novel multi-scale shadow inpainting framework for OCT images by synergically applying sparse representation and deep learning: sparse representation is used to extract features from a small amount of training images for further inpainting and to regularize the image after the multi-scale image fusion, while convolutional neural network (CNN) is employed to enhance the image quality. During the image inpainting, we divide preprocessed input images into different branches based on the shadow width to harvest complementary information from different scales. Finally, a sparse representation-based regularizing module is designed to refine the generated contents after multi-scale feature aggregation. Experiments are conducted to compare our proposal versus both traditional and deep learning-based techniques on synthetic and real-world shadows. Results demonstrate that our proposed method achieves favorable image inpainting in terms of visual quality and quantitative metrics, especially when wide shadows are presented.      
### 43.Low-complexity Joint Beamforming for RIS-Aided Multi-User Downlink over Correlated Channels  [ :arrow_down: ](https://arxiv.org/pdf/2202.11354.pdf)
>  This paper considers the reconfigurable intelligent surface (RIS)-assisted communication scenario, where an RIS is used to assist the base station (BS) for serving multiple users. The RIS consisting of passive reflecting elements can manipulate the reflected direction of the incoming electromagnetic waves and thus it offers a new design dimension to the system designer. To maximize the sum rate, the active beamforming at the BS and the passive phase shifts at the RIS need to be jointly optimized, which is an NP-hard problem. In this work, we consider the joint active and passive (JAPB) beamforming problem over correlated fading channels. To facilitate practical implementation, we propose two low-complexity schemes along with user grouping to solve JAPB. Besides, we theoretically analyze the mean correlation coefficient between two cascade RIS channels and obtain a closed-form expression for arbitrary phase-shift values. Asymptotic analysis is also conducted to get insights into the channel correlation of cascade RIS channels when the numbers of BS antennas and RIS elements are large. Simulation results are presented to validate the analysis accuracy of the derived mean correlation coefficient. Also, the sum rate performance of the proposed methods under different system settings is evaluated and compared with the benchmark that optimizes the RIS phase shifts using element-wise successive refinement.      
### 44.Training Adaptive Reconstruction Networks for Inverse Problems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11342.pdf)
>  Neural networks are full of promises for the resolution of ill-posed inverse problems. In particular, physics informed learning approaches already seem to progressively gradually replace carefully hand-crafted reconstruction algorithms, for their superior quality. The aim of this paper is twofold. First we show a significant weakness of these networks: they do not adapt efficiently to variations of the forward model. Second, we show that training the network with a family of forward operators allows to solve the adaptivity problem without compromising the reconstruction quality significantly. All our experiments are carefully devised on partial Fourier sampling problems arising in magnetic resonance imaging (MRI).      
### 45.Is Global Asymptotic Stability Necessarily Uniform for Time-Delay Systems?  [ :arrow_down: ](https://arxiv.org/pdf/2202.11298.pdf)
>  For time-invariant finite-dimensional systems, it is notoriously known that global asymptotic stability (GAS) is equivalent to uniform global asymptotic stability (UGAS), in which the decay rate and transient overshoot of solutions is requested to be uniform on bounded sets of initial states. This paper investigates this relationship for time-invariant delay systems. We show that UGAS and GAS are equivalent for such class of systems under the assumption of robust forward completeness (RFC, meaning that the reachable set from any bounded set of initial states on any finite time horizon is bounded). We also show that, if the state space is a space in a particular family of Sobolev spaces, then GAS is equivalent to UGAS without any additional assumption and that usual forward completeness is equivalent to RFC, just like for finite-dimensional systems. Based on these equivalences, we provide a novel Lyapunov characterization of GAS (and UGAS) in the aforementioned family of Sobolev spaces.      
### 46.Continual learning-based probabilistic slow feature analysis for multimode dynamic process monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2202.11295.pdf)
>  In this paper, a novel multimode dynamic process monitoring approach is proposed by extending elastic weight consolidation (EWC) to probabilistic slow feature analysis (PSFA) in order to extract multimode slow features for online monitoring. EWC was originally introduced in the setting of machine learning of sequential multi-tasks with the aim of avoiding catastrophic forgetting issue, which equally poses as a major challenge in multimode dynamic process monitoring. When a new mode arrives, a set of data should be collected so that this mode can be identified by PSFA and prior knowledge. Then, a regularization term is introduced to prevent new data from significantly interfering with the learned knowledge, where the parameter importance measures are estimated. The proposed method is denoted as PSFA-EWC, which is updated continually and capable of achieving excellent performance for successive modes. Different from traditional multimode monitoring algorithms, PSFA-EWC furnishes backward and forward transfer ability. The significant features of previous modes are retained while consolidating new information, which may contribute to learning new relevant modes. Compared with several known methods, the effectiveness of the proposed method is demonstrated via a continuous stirred tank heater and a practical coal pulverizing system.      
### 47.An End-to-End Cascaded Image Deraining and Object Detection Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.11279.pdf)
>  While the deep learning-based image deraining methods have made great progress in recent years, there are two major shortcomings in their application in real-world situations. Firstly, the gap between the low-level vision task represented by rain removal and the high-level vision task represented by object detection is significant, and the low-level vision task can hardly contribute to the high-level vision task. Secondly, the quality of the deraining dataset needs to be improved. In fact, the rain lines in many baselines have a large gap with the real rain lines, and the resolution of the deraining dataset images is generally not ideally. Meanwhile, there are few common datasets for both the low-level vision task and the high-level vision task. In this paper, we explore the combination of the low-level vision task with the high-level vision task. Specifically, we propose an end-to-end object detection network for reducing the impact of rainfall, which consists of two cascaded networks, an improved image deraining network and an object detection network, respectively. We also design the components of the loss function to accommodate the characteristics of the different sub-networks. We then propose a dataset based on the KITTI dataset for rainfall removal and object detection, on which our network surpasses the state-of-the-art with a significant improvement in metrics. Besides, our proposed network is measured on driving videos collected by self-driving vehicles and shows positive results for rain removal and object detection.      
### 48.Minimax Optimal Quantization of Linear Models: Information-Theoretic Limits and Efficient Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2202.11277.pdf)
>  We consider the problem of quantizing a linear model learned from measurements $\mathbf{X} = \mathbf{W}\boldsymbol{\theta} + \mathbf{v}$. The model is constrained to be representable using only $dB$-bits, where $B \in (0, \infty)$ is a pre-specified budget and $d$ is the dimension of the model. We derive an information-theoretic lower bound for the minimax risk under this setting and show that it is tight with a matching upper bound. This upper bound is achieved using randomized embedding based algorithms. We propose randomized Hadamard embeddings that are computationally efficient while performing near-optimally. We also show that our method and upper-bounds can be extended for two-layer ReLU neural networks. Numerical simulations validate our theoretical claims.      
### 49.ViKiNG: Vision-Based Kilometer-Scale Navigation with Geographic Hints  [ :arrow_down: ](https://arxiv.org/pdf/2202.11271.pdf)
>  Robotic navigation has been approached as a problem of 3D reconstruction and planning, as well as an end-to-end learning problem. However, long-range navigation requires both planning and reasoning about local traversability, as well as being able to utilize information about global geography, in the form of a roadmap, GPS, or other side information, which provides important navigational hints but may be low-fidelity or unreliable. In this work, we propose a learning-based approach that integrates learning and planning, and can utilize side information such as schematic roadmaps, satellite maps and GPS coordinates as a planning heuristic, without relying on them being accurate. Our method, ViKiNG, incorporates a local traversability model, which looks at the robot's current camera observation and a potential subgoal to infer how easily that subgoal can be reached, as well as a heuristic model, which looks at overhead maps and attempts to estimate the distance to the destination for various subgoals. These models are used by a heuristic planner to decide the best next subgoal in order to reach the final destination. Our method performs no explicit geometric reconstruction, utilizing only a topological representation of the environment. Despite having never seen trajectories longer than 80 meters in its training dataset, ViKiNG can leverage its image-based learned controller and goal-directed heuristic to navigate to goals up to 3 kilometers away in previously unseen environments, and exhibit complex behaviors such as probing potential paths and doubling back when they are found to be non-viable. ViKiNG is also robust to unreliable maps and GPS, since the low-level controller ultimately makes decisions based on egocentric image observations, using maps only as planning heuristics. For videos of our experiments, please check out <a class="link-external link-https" href="https://sites.google.com/view/viking-release" rel="external noopener nofollow">this https URL</a>.      
### 50.NetRCA: An Effective Network Fault Cause Localization Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2202.11269.pdf)
>  Localizing the root cause of network faults is crucial to network operation and maintenance. However, due to the complicated network architectures and wireless environments, as well as limited labeled data, accurately localizing the true root cause is challenging. In this paper, we propose a novel algorithm named NetRCA to deal with this problem. Firstly, we extract effective derived features from the original raw data by considering temporal, directional, attribution, and interaction characteristics. Secondly, we adopt multivariate time series similarity and label propagation to generate new training data from both labeled and unlabeled data to overcome the lack of labeled samples. Thirdly, we design an ensemble model which combines XGBoost, rule set learning, attribution model, and graph algorithm, to fully utilize all data information and enhance performance. Finally, experiments and analysis are conducted on the real-world dataset from ICASSP 2022 AIOps Challenge to demonstrate the superiority and effectiveness of our approach.      
### 51.FUNQUE: Fusion of Unified Quality Evaluators  [ :arrow_down: ](https://arxiv.org/pdf/2202.11241.pdf)
>  Fusion-based quality assessment has emerged as a powerful method for developing high-performance quality models from quality models that individually achieve lower performances. A prominent example of such an algorithm is VMAF, which has been widely adopted as an industry standard for video quality prediction along with SSIM. In addition to advancing the state-of-the-art, it is imperative to alleviate the computational burden presented by the use of a heterogeneous set of quality models. In this paper, we unify "atom" quality models by computing them on a common transform domain that accounts for the Human Visual System, and we propose FUNQUE, a quality model that fuses unified quality evaluators. We demonstrate that in comparison to the state-of-the-art, FUNQUE offers significant improvements in both correlation against subjective scores and efficiency, due to computation sharing.      
### 52.Lattices from Linear Codes: Source and Channel Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.11238.pdf)
>  In this paper, we consider the information-theoretic characterization of the set of achievable rates and distortions in a broad class of multiterminal communication scenarios with general continuous-valued sources and channels. A framework is presented which involves fine discretization of the source and channel variables followed by communication over the resulting discretized network. In order to evaluate fundamental performance limits, convergence results for information measures are provided under the proposed discretization process. Using this framework, we consider point-to-point source coding and channel coding with side-information, distributed source coding with distortion constraints, the function reconstruction problems (two-help-one), computation over multiple access channel, the interference channel, and the multiple-descriptions source coding problem. We construct lattice-like codes for general sources and channels, and derive inner-bounds to set of achievable rates and distortions in these communication scenarios.      
### 53.Democratizing Aviation Emissions Estimation: Development of an Open-Source, Data-Driven Methodology  [ :arrow_down: ](https://arxiv.org/pdf/2202.11208.pdf)
>  Through an aviation emissions estimation tool that is both publicly-accessible and comprehensive, researchers, planners, and community advocates can help shape a more sustainable and equitable U.S. air transportation system. To this end, we develop an open-source, data-driven methodology to calculate the system-wide emissions of the U.S. domestic civil aviation industry. This process utilizes and integrates six different public datasets provided by the Bureau of Transportation Statistics (BTS), the Federal Aviation Agency (FAA), EUROCONTROL, and the International Civil Aviation Organization (ICAO). At the individual flight level, our approach examines the specific aircraft type, equipped engine, and time in stage of flight to produce a more granular estimate than competing approaches. Enabled by our methodology, we then calculate system-wide emissions, considering four different greenhouse gases (CO2, NOx, CO, HC) during the LTO and CCD flight cycles. Our results elucidate that emissions on a particular route can vary significantly due to aircraft and engine choice, and that emission rates differ significantly from airline to airline. We also find that CO2 alone is not a sufficient proxy for emissions, as NOx, when converted to its CO2-equivalency, exceeds CO2 during both LTO and CCD.      
### 54.Impact of Channel Correlation on Subspace-Based Activity Detection in Grant-Free NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2202.11161.pdf)
>  In this paper, we consider the problem of activity detection in grant-free code-domain non-orthogonal multiple access (NOMA). We focus on performing activity detection via subspace methods under a setup where the data and pilot spreading signatures are of different lengths, and consider a realistic frame-structure similar to existing mobile networks. We investigate the impact of channel correlation on the activity detection performance; first, we consider the case where the channel exhibits high correlation in time and frequency and show how it can heavily deteriorate the performance. To tackle that, we propose to apply user-specific masking sequences overlaid on top of the pilot signatures. Second, we consider the other extreme with the channel being highly selective, and show that it can also negatively impact the performance. We investigate possible pilots' reallocation strategies that can help reduce its impact.      
### 55.Robust Hierarchical Patterns for identifying MDD patients: A Multisite Study  [ :arrow_down: ](https://arxiv.org/pdf/2202.11144.pdf)
>  Many supervised machine learning frameworks have been proposed for disease classification using functional magnetic resonance imaging (fMRI) data, producing important biomarkers. More recently, data pooling has flourished, making the result generalizable across a large population. But, this success depends on the population diversity and variability introduced due to the pooling of the data that is not a primary research interest. Here, we look at hierarchical Sparse Connectivity Patterns (hSCPs) as biomarkers for major depressive disorder (MDD). We propose a novel model based on hSCPs to predict MDD patients from functional connectivity matrices extracted from resting-state fMRI data. Our model consists of three coupled terms. The first term decomposes connectivity matrices into hierarchical low-rank sparse components corresponding to synchronous patterns across the human brain. These components are then combined via patient-specific weights capturing heterogeneity in the data. The second term is a classification loss that uses the patient-specific weights to classify MDD patients from healthy ones. Both of these terms are combined with the third term, a robustness loss function to improve the reproducibility of hSCPs. This reduces the variability introduced due to site and population diversity (age and sex) on the predictive accuracy and pattern stability in a large dataset pooled from five different sites. Our results show the impact of diversity on prediction performance. Our model can reduce diversity and improve the predictive and generalizing capability of the components. Finally, our results show that our proposed model can robustly identify clinically relevant patterns characteristic of MDD with high reproducibility.      
### 56.FlowSense: Monitoring Airflow in Building Ventilation Systems Using Audio Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2202.11136.pdf)
>  Proper indoor ventilation through buildings' heating, ventilation, and air conditioning (HVAC) systems has become an increasing public health concern that significantly impacts individuals' health and safety at home, work, and school. While much work has progressed in providing energy-efficient and user comfort for HVAC systems through IoT devices and mobile-sensing approaches, ventilation is an aspect that has received lesser attention despite its importance. With a motivation to monitor airflow from building ventilation systems through commodity sensing devices, we present FlowSense, a machine learning-based algorithm to predict airflow rate from sensed audio data in indoor spaces. Our ML technique can predict the state of an air vent-whether it is on or off-as well as the rate of air flowing through active vents. By exploiting a low-pass filter to obtain low-frequency audio signals, we put together a privacy-preserving pipeline that leverages a silence detection algorithm to only sense for sounds of air from HVAC air vent when no human speech is detected. We also propose the Minimum Persistent Sensing (MPS) as a post-processing algorithm to reduce interference from ambient noise, including ongoing human conversation, office machines, and traffic noises. Together, these techniques ensure user privacy and improve the robustness of FlowSense. We validate our approach yielding over 90% accuracy in predicting vent status and 0.96 MSE in predicting airflow rate when the device is placed within 2.25 meters away from an air vent. Additionally, we demonstrate how our approach as a mobile audio-sensing platform is robust to smartphone models, distance, and orientation. Finally, we evaluate FlowSense privacy-preserving pipeline through a user study and a Google Speech Recognition service, confirming that the audio signals we used as input data are inaudible and inconstructible.      
### 57.ProtoSound: A Personalized and Scalable Sound Recognition System for Deaf and Hard-of-Hearing Users  [ :arrow_down: ](https://arxiv.org/pdf/2202.11134.pdf)
>  Recent advances have enabled automatic sound recognition systems for deaf and hard of hearing (DHH) users on mobile devices. However, these tools use pre-trained, generic sound recognition models, which do not meet the diverse needs of DHH users. We introduce ProtoSound, an interactive system for customizing sound recognition models by recording a few examples, thereby enabling personalized and fine-grained categories. ProtoSound is motivated by prior work examining sound awareness needs of DHH people and by a survey we conducted with 472 DHH participants. To evaluate ProtoSound, we characterized performance on two real-world sound datasets, showing significant improvement over state-of-the-art (e.g., +9.7% accuracy on the first dataset). We then deployed ProtoSound's end-user training and real-time recognition through a mobile application and recruited 19 hearing participants who listened to the real-world sounds and rated the accuracy across 56 locations (e.g., homes, restaurants, parks). Results show that ProtoSound personalized the model on-device in real-time and accurately learned sounds across diverse acoustic contexts. We close by discussing open challenges in personalizable sound recognition, including the need for better recording interfaces and algorithmic improvements.      
