# ArXiv eess --Tue, 15 Feb 2022
### 1.A Graphical Approach For Brain Haemorrhage Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.06876.pdf)
>  Haemorrhaging of the brain is the leading cause of death in people between the ages of 15 and 24 and the third leading cause of death in people older than that. Computed tomography (CT) is an imaging modality used to diagnose neurological emergencies, including stroke and traumatic brain injury. Recent advances in Deep Learning and Image Processing have utilised different modalities like CT scans to help automate the detection and segmentation of brain haemorrhage occurrences. In this paper, we propose a novel implementation of an architecture consisting of traditional Convolutional Neural Networks(CNN) along with Graph Neural Networks(GNN) to produce a holistic model for the task of brain haemorrhage segmentation.GNNs work on the principle of neighbourhood aggregation thus providing a reliable estimate of global structures present in images. GNNs work with few layers thus in turn requiring fewer parameters to work with. We were able to achieve a dice coefficient score of around 0.81 with limited data with our implementation.      
### 2.Elastic Optical Fibres for Wearable Devices  [ :arrow_down: ](https://arxiv.org/pdf/2202.06863.pdf)
>  Wearable devices are becoming increasingly common, addressing needs in both the fitness and the medical markets. This trend has accelerated with the growth in telemedicine, particularly after COVID-19. Optical fibre-based sensors are an ideal candidate technology to be included in such monitoring devices as they can measure a large number of parameters and they are easily integrated in wearables. However, their use has been limited due to their sensitivity being too low compared to the one needed to obtain information from the external perturbations of the human body. This requires highly deformable fibres. In this paper, we describe the use of a hollow polyurethane optical fibre sensor, operating through capillary guidance, to monitor breathing and cardiac activity. These fibres can be conformal to the skin and could potentially be incorporated into clothing. The extreme deformability of the material combined with the hollow nature of the fibre provide the necessary sensitivity to capture the pressure changes associated with breathing and arterial pulses. They allow us to recover and identify features of the cardiac pulse wave with high fidelity. In order to reduce sources of error and to increase the signal to noise ratio, we use black fibres. We discuss some unusual features of these fibre sensors, including the transfer function and how to configure for a monotonic response. Using these unusual fibres, we demonstrate wearable sensors for respiration and cardiac signals that produce physiologically rich data.      
### 3.A Machine Learning Framework for Event Identification via Modal Analysis of PMU Data  [ :arrow_down: ](https://arxiv.org/pdf/2202.06836.pdf)
>  Power systems are prone to a variety of events (e.g. line trips and generation loss) and real-time identification of such events is crucial in terms of situational awareness, reliability, and security. Using measurements from multiple synchrophasors, i.e., phasor measurement units (PMUs), we propose to identify events by extracting features based on modal dynamics. We combine such traditional physics-based feature extraction methods with machine learning to distinguish different event types. Including all measurement channels at each PMU allows exploiting diverse features but also requires learning classification models over a high-dimensional space. To address this issue, various feature selection methods are implemented to choose the best subset of features. Using the obtained subset of features, we investigate the performance of two well-known classification models, namely, logistic regression (LR) and support vector machines (SVM) to identify generation loss and line trip events in two datasets. The first dataset is obtained from simulated generation loss and line trip events in the Texas 2000-bus synthetic grid. The second is a proprietary dataset with labeled events obtained from a large utility in the USA involving measurements from nearly 500 PMUs. Our results indicate that the proposed framework is promising for identifying the two types of events.      
### 4.Sequential Doppler Shift based Optimal Localization and Synchronization with TOA  [ :arrow_down: ](https://arxiv.org/pdf/2202.06807.pdf)
>  Doppler shift is an important measurement for localization and synchronization (LAS), and is available in various practical systems. Existing studies on LAS techniques in a time division broadcast LAS system (TDBS) only use sequential time-of-arrival (TOA) measurements from the broadcast signals. In this paper, we develop a new optimal LAS method in the TDBS, namely LAS-SDT, by taking advantage of the sequential Doppler shift and TOA measurements. It achieves higher accuracy compared with the conventional TOA-only method for user devices (UDs) with motion and clock drift. Another two variant methods, LAS-SDT-v for the case with UD velocity aiding, and LAS-SDT-k for the case with UD clock drift aiding, are developed. We derive the Cramer-Rao lower bound (CRLB) for these different cases. We show analytically that the accuracies of the estimated UD position, clock offset, velocity and clock drift are all significantly higher than those of the conventional LAS method using TOAs only. Numerical results corroborate the theoretical analysis and show the optimal estimation performance of the LAS-SDT.      
### 5.Speech Analysis for Automatic Mania Assessment in Bipolar Disorder  [ :arrow_down: ](https://arxiv.org/pdf/2202.06766.pdf)
>  Bipolar disorder is a mental disorder that causes periods of manic and depressive episodes. In this work, we classify recordings from Bipolar Disorder corpus that contain 7 different tasks, into hypomania, mania, and remission classes using only speech features. We perform our experiments on splitted tasks from the interviews. Best results achieved on the model trained with 6th and 7th tasks together gives 0.53 UAR (unweighted average recall) result which is higher than the baseline results of the corpus.      
### 6.Low-latency Monaural Speech Enhancement with Deep Filter-bank Equalizer  [ :arrow_down: ](https://arxiv.org/pdf/2202.06764.pdf)
>  It is highly desirable that speech enhancement algorithms can achieve good performance while keeping low latency for many applications, such as digital hearing aids, acoustically transparent hearing devices, and public address systems. To improve the performance of traditional low-latency speech enhancement algorithms, a deep filter-bank equalizer (FBE) framework was proposed, which integrated a deep learning-based subband noise reduction network with a deep learning-based shortened digital filter mapping network. In the first network, a deep learning model was trained with a controllable small frame shift to satisfy the low-latency demand, i.e., $\le$ 4 ms, so as to obtain (complex) subband gains, which could be regarded as an adaptive digital filter in each frame. In the second network, to reduce the latency, this adaptive digital filter was implicitly shortened by a deep learning-based framework, and was then applied to noisy speech to reconstruct the enhanced speech without the overlap-add method. Experimental results on the WSJ0-SI84 corpus indicated that the proposed deep FBE with only 4-ms latency achieved much better performance than traditional low-latency speech enhancement algorithms in terms of the indices such as PESQ, STOI, and the amount of noise reduction.      
### 7.Attention-based Deep Neural Networks for Battery Discharge Capacity Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2202.06738.pdf)
>  Battery discharge capacity forecasting is critically essential for the applications of lithium-ion batteries. The capacity degeneration can be treated as the memory of the initial battery state of charge from the data point of view. The streaming sensor data collected by battery management systems (BMS) reflect the usable battery capacity degradation rates under various operational working conditions. The battery capacity in different cycles can be measured with the temporal patterns extracted from the streaming sensor data based on the attention mechanism. The attention-based similarity regarding the first cycle can describe the battery capacity degradation in the following cycles. The deep degradation network (DDN) is developed with the attention mechanism to measure similarity and predict battery capacity. The DDN model can extract the degeneration-related temporal patterns from the streaming sensor data and perform the battery capacity prediction efficiently online in real-time. Based on the MIT-Stanford open-access battery aging dataset, the root-mean-square error of the capacity estimation is 1.3 mAh. The mean absolute percentage error of the proposed DDN model is 0.06{\%}. The DDN model also performance well in the Oxford Battery Degradation Dataset with dynamic load profiles. Therefore, the high accuracy and strong robustness of the proposed algorithm are verified.      
### 8.Active and Passive Hybrid Detection Method for Power CPS False Data Injection Attacks with Improved AKF and GRU-CNN  [ :arrow_down: ](https://arxiv.org/pdf/2202.06722.pdf)
>  Influenced by deep penetration of the new generation of information technology, power systems have gradually evolved into highly coupled cyber-physical systems (CPS). Among many possible power CPS network attacks, a false data injection attacks (FDIAs) is the most serious. Taking account of the fact that the existing knowledge-driven detection process for FDIAs has been in a passive detection state for a long time and ignores the advantages of data-driven active capture of features, an active and passive hybrid detection method for power CPS FDIAs with improved adaptive Kalman filter (AKF) and convolutional neural networks (CNN) is proposed in this paper. First, we analyze the shortcomings of the traditional AKF algorithm in terms of filtering divergence and calculation speed. The state estimation algorithm based on non-negative positive-definite adaptive Kalman filter (NDAKF) is improved, and a passive detection method of FDIAs is constructed, with similarity Euclidean distance detection and residual detection at its core. Then, combined with the advantages of gate recurrent unit (GRU) and CNN in terms of temporal memory and feature-expression ability, an active detection method of FDIAs based on a GRU-CNN hybrid neural network is proposed. Finally, the results of joint knowledge-driven and data-driven parallel detection are used to define a mixed fixed-calculation formula, and an active and passive hybrid detection method of FDIAs is established, considering the characteristic constraints of the parallel mode. A simulation system example of power CPS FDIAs verifies the effectiveness and accuracy of the method proposed in this paper.      
### 9.Spiking Cochlea with System-level Local Automatic Gain Control  [ :arrow_down: ](https://arxiv.org/pdf/2202.06707.pdf)
>  Including local automatic gain control (AGC) circuitry into a silicon cochlea design has been challenging because of transistor mismatch and model complexity. To address this, we present an alternative system-level algorithm that implements channel-specific AGC in a silicon spiking cochlea by measuring the output spike activity of individual channels. The bandpass filter gain of a channel is adapted dynamically to the input amplitude so that the average output spike rate stays within a defined range. Because this AGC mechanism only needs counting and adding operations, it can be implemented at low hardware cost in a future design. We evaluate the impact of the local AGC algorithm on a classification task where the input signal varies over 32 dB input range. Two classifier types receiving cochlea spike features were tested on a speech versus noise classification task. The logistic regression classifier achieves an average of 6% improvement and 40.8% relative improvement in accuracy when the AGC is enabled. The deep neural network classifier shows a similar improvement for the AGC case and achieves a higher mean accuracy of 96% compared to the best accuracy of 91% from the logistic regression classifier.      
### 10.Partially Fake Audio Detection by Self-attention-based Fake Span Discovery  [ :arrow_down: ](https://arxiv.org/pdf/2202.06684.pdf)
>  The past few years have witnessed the significant advances of speech synthesis and voice conversion technologies. However, such technologies can undermine the robustness of broadly implemented biometric identification models and can be harnessed by in-the-wild attackers for illegal uses. The ASVspoof challenge mainly focuses on synthesized audios by advanced speech synthesis and voice conversion models, and replay attacks. Recently, the first Audio Deep Synthesis Detection challenge (ADD 2022) extends the attack scenarios into more aspects. Also ADD 2022 is the first challenge to propose the partially fake audio detection task. Such brand new attacks are dangerous and how to tackle such attacks remains an open question. Thus, we propose a novel framework by introducing the question-answering (fake span discovery) strategy with the self-attention mechanism to detect partially fake audios. The proposed fake span detection module tasks the anti-spoofing model to predict the start and end positions of the fake clip within the partially fake audio, address the model's attention into discovering the fake spans rather than other shortcuts with less generalization, and finally equips the model with the discrimination capacity between real and partially fake audios. Our submission ranked second in the partially fake audio detection track of ADD 2022.      
### 11.Resource allocation for reconfigurable intelligent surface aided broadcast channels  [ :arrow_down: ](https://arxiv.org/pdf/2202.06668.pdf)
>  A two-user downlink network aided by a reconfigurable intelligent surface is considered. The weighted sum signal to interference plus noise ratio maximization and the sum rate maximization models are presented, where the precoding vectors and the RIS matrix are jointly optimized. Since the optimization problem is non-convex and difficult, new approximation models are proposed. The upper bounds of the corresponding objective functions are derived and maximized. Two new algorithms based on the alternating direction method of multiplier are proposed. It is proved that the proposed algorithms converge to the KKT points of the approximation models as long as the iteration points converge. Simulation results show the good performances of the proposed models compared to state of the art algorithms.      
### 12.Jointly Learned Symbol Detection and Signal Reflection in RIS-Aided Multi-user MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06663.pdf)
>  Reconfigurable Intelligent Surfaces (RISs) are regarded as a key technology for future wireless communications, enabling programmable radio propagation environments. However, the passive reflecting feature of RISs induces notable challenges on channel estimation, making coherent symbol detection a challenging task. In this paper, we consider the uplink of RIS-aided multi-user Multiple-Input Multiple-Output (MIMO) systems and propose a Machine Learning (ML) approach to jointly design the multi-antenna receiver and configure the RIS reflection coefficients, which does not require explicit full knowledge of the channel input-output relationship. Our approach devises a ML-based receiver, while the configurations of the RIS reflection patterns affecting the underlying propagation channel are treated as hyperparameters. Based on this system design formulation, we propose a Bayesian ML framework for optimizing the RIS hyperparameters, according to which the transmitted pilots are directly used to jointly tune the RIS and the multi-antenna receiver. Our simulation results demonstrate the capability of the proposed approach to provide reliable communications in non-linear channel conditions corrupted by Gaussian noise.      
### 13.MuZero with Self-competition for Rate Control in VP9 Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2202.06626.pdf)
>  Video streaming usage has seen a significant rise as entertainment, education, and business increasingly rely on online video. Optimizing video compression has the potential to increase access and quality of content to users, and reduce energy use and costs overall. In this paper, we present an application of the MuZero algorithm to the challenge of video compression. Specifically, we target the problem of learning a rate control policy to select the quantization parameters (QP) in the encoding process of libvpx, an open source VP9 video compression library widely used by popular video-on-demand (VOD) services. We treat this as a sequential decision making problem to maximize the video quality with an episodic constraint imposed by the target bitrate. Notably, we introduce a novel self-competition based reward mechanism to solve constrained RL with variable constraint satisfaction difficulty, which is challenging for existing constrained RL methods. We demonstrate that the MuZero-based rate control achieves an average 6.28% reduction in size of the compressed videos for the same delivered video quality level (measured as PSNR BD-rate) compared to libvpx's two-pass VBR rate control policy, while having better constraint satisfaction behavior.      
### 14.Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2202.06599.pdf)
>  Segmentation and spatial alignment of ultrasound (US) imaging data acquired in the in first trimester are crucial for monitoring human embryonic growth and development throughout this crucial period of life. Current approaches are either manual or semi-automatic and are therefore very time-consuming and prone to errors. To automate these tasks, we propose a multi-atlas framework for automatic segmentation and spatial alignment of the embryo using deep learning with minimal supervision. Our framework learns to register the embryo to an atlas, which consists of the US images acquired at a range of gestational age (GA), segmented and spatially aligned to a predefined standard orientation. From this, we can derive the segmentation of the embryo and put the embryo in standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used and eight pregnancies were selected as atlas images. We evaluated different fusion strategies to incorporate multiple atlases: 1) training the framework using atlas images from a single pregnancy, 2) training the framework with data of all available atlases and 3) ensembling of the frameworks trained per pregnancy. To evaluate the performance, we calculated the Dice score over the test set. We found that training the framework using all available atlases outperformed ensembling and gave similar results compared to the best of all frameworks trained on a single subject. Furthermore, we found that selecting images from the four atlases closest in GA out of all available atlases, regardless of the individual quality, gave the best results with a median Dice score of 0.72. We conclude that our framework can accurately segment and spatially align the embryo in first trimester 3D US images and is robust for the variation in quality that existed in the available atlases. Our code is publicly available at: <a class="link-external link-https" href="https://github.com/wapbastiaansen/multi-atlas-seg-reg" rel="external noopener nofollow">this https URL</a>.      
### 15.A Pragmatic Machine Learning Approach to Quantify Tumor Infiltrating Lymphocytes in Whole Slide Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.06590.pdf)
>  Increased levels of tumor infiltrating lymphocytes (TILs) in cancer tissue indicate favourable outcomes in many types of cancer. Manual quantification of immune cells is inaccurate and time consuming for pathologists. Our aim is to leverage a computational solution to automatically quantify TILs in whole slide images (WSIs) of standard diagnostic haematoxylin and eosin stained sections (H&amp;E slides) from lung cancer patients. Our approach is to transfer an open source machine learning method for segmentation and classification of nuclei in H&amp;E slides trained on public data to TIL quantification without manual labeling of our data. Our results show that additional augmentation improves model transferability when training on few samples/limited tissue types. Models trained with sufficient samples/tissue types do not benefit from our additional augmentation policy. Further, the resulting TIL quantification correlates to patient prognosis and compares favorably to the current state-of-the-art method for immune cell detection in non-small lung cancer (current standard CD8 cells in DAB stained TMAs HR 0.34 95% CI 0.17-0.68 vs TILs in HE WSIs: HoVer-Net PanNuke Aug Model HR 0.30 95% CI 0.15-0.60, HoVer-Net MoNuSAC Aug model HR 0.27 95% CI 0.14-0.53). Moreover, we implemented a cloud based system to train, deploy and visually inspect machine learning based annotation for H&amp;E slides. Our pragmatic approach bridges the gap between machine learning research, translational clinical research and clinical implementation. However, validation in prospective studies is needed to assert that the method works in a clinical setting.      
### 16.Modelling Underwater Acoustic Propagation using One-way Wave Equations  [ :arrow_down: ](https://arxiv.org/pdf/2202.06559.pdf)
>  The primary contribution of this paper is to characterize the propagation of acoustic signal carrying information through any medium and the interaction of the travelling acoustic signal with the surrounding medium. We will use the concept of damped harmonic oscillator to model the medium and Milne's oscillator technique to map the interaction of the acoustic signal with the medium. The acoustic signal itself will be modelled using the one-way wave equation formulated in terms of acoustic pressure and velocity of acoustic waves through the medium. Using the above-mentioned concepts, we calculated the effective signal strength, phase shift and time period of the communicated signal. Numerical results are generated to present the evolution of signal strength and received signal envelope in underwater environment.      
### 17.A resource-efficient deep learning framework for low-dose brain PET image reconstruction and analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.06548.pdf)
>  18F-fluorodeoxyglucose (18F-FDG) Positron Emission Tomography (PET) imaging usually needs a full-dose radioactive tracer to obtain satisfactory diagnostic results, which raises concerns about the potential health risks of radiation exposure, especially for pediatric patients. Reconstructing the low-dose PET (L-PET) images to the high-quality full-dose PET (F-PET) ones is an effective way that both reduces the radiation exposure and remains diagnostic accuracy. In this paper, we propose a resource-efficient deep learning framework for L-PET reconstruction and analysis, referred to as transGAN-SDAM, to generate F-PET from corresponding L-PET, and quantify the standard uptake value ratios (SUVRs) of these generated F-PET at whole brain. The transGAN-SDAM consists of two modules: a transformer-encoded Generative Adversarial Network (transGAN) and a Spatial Deformable Aggregation Module (SDAM). The transGAN generates higher quality F-PET images, and then the SDAM integrates the spatial information of a sequence of generated F-PET slices to synthesize whole-brain F-PET images. Experimental results demonstrate the superiority and rationality of our approach.      
### 18.Tight integration of neural- and clustering-based diarization through deep unfolding of infinite Gaussian mixture model  [ :arrow_down: ](https://arxiv.org/pdf/2202.06524.pdf)
>  Speaker diarization has been investigated extensively as an important central task for meeting analysis. Recent trend shows that integration of end-to-end neural (EEND)-and clustering-based diarization is a promising approach to handle realistic conversational data containing overlapped speech with an arbitrarily large number of speakers, and achieved state-of-the-art results on various tasks. However, the approaches proposed so far have not realized {\it tight} integration yet, because the clustering employed therein was not optimal in any sense for clustering the speaker embeddings estimated by the EEND module. To address this problem, this paper introduces a {\it trainable} clustering algorithm into the integration framework, by deep-unfolding a non-parametric Bayesian model called the infinite Gaussian mixture model (iGMM). Specifically, the speaker embeddings are optimized during training such that it better fits iGMM clustering, based on a novel clustering loss based on Adjusted Rand Index (ARI). Experimental results based on CALLHOME data show that the proposed approach outperforms the conventional approach in terms of diarization error rate (DER), especially by substantially reducing speaker confusion errors, that indeed reflects the effectiveness of the proposed iGMM integration.      
### 19.EMGSE: Acoustic/EMG Fusion for Multimodal Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.06507.pdf)
>  Multimodal learning has been proven to be an effective method to improve speech enhancement (SE) performance, especially in challenging situations such as low signal-to-noise ratios, speech noise, or unseen noise types. In previous studies, several types of auxiliary data have been used to construct multimodal SE systems, such as lip images, electropalatography, or electromagnetic midsagittal articulography. In this paper, we propose a novel EMGSE framework for multimodal SE, which integrates audio and facial electromyography (EMG) signals. Facial EMG is a biological signal containing articulatory movement information, which can be measured in a non-invasive way. Experimental results show that the proposed EMGSE system can achieve better performance than the audio-only SE system. The benefits of fusing EMG signals with acoustic signals for SE are notable under challenging circumstances. Furthermore, this study reveals that cheek EMG is sufficient for SE.      
### 20.Opinions Vary? Diagnosis First!  [ :arrow_down: ](https://arxiv.org/pdf/2202.06505.pdf)
>  In medical image segmentation, images are usually annotated by several different clinical experts. This clinical routine helps to mitigate the personal bias. However, Computer Vision models often assume there has a unique ground-truth for each of the instance. This research gap between Computer Vision and medical routine is commonly existed but less explored by the current <a class="link-external link-http" href="http://research.In" rel="external noopener nofollow">this http URL</a> this paper, we try to answer the following two questions: 1. How to learn an optimal combination of the multiple segmentation labels? and 2. How to estimate this segmentation mask from the raw image? We note that in clinical practice, the image segmentation mask usually exists as an auxiliary information for disease diagnosis. Adhering to this mindset, we propose a framework taking the diagnosis result as the gold standard, to estimate the segmentation mask upon the multi-rater segmentation labels, named DiFF (Diagnosis First segmentation Framework).DiFF is implemented by two novelty techniques. First, DFSim (Diagnosis First Simulation of gold label) is learned as an optimal combination of multi-rater segmentation labels for the disease diagnosis. Then, toward estimating DFSim mask from the raw image, we further propose T\&amp;G Module (Take and Give Module) to instill the diagnosis knowledge into the segmentation network. The experiments show that compared with commonly used majority vote, the proposed DiFF is able to segment the masks with 6% improvement on diagnosis AUC score, which also outperforms various state-of-the-art multi-rater methods by a large margin.      
### 21.Model-Based Neural Network and Its Application to Line Spectral Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2202.06485.pdf)
>  This paper presents the concept of "model-based neural network"(MNN), which is inspired by the classic artificial neural network (ANN) but for different usages. Instead of being used as a data-driven classifier, a MNN serves as a modeling tool with artfully defined inputs, outputs, and activation functions which have explicit physical meanings. Owing to the same layered form as an ANN, a MNN can also be optimized using the back-propagation (BP) algorithm. As an interesting application, the classic problem of line spectral estimation can be modeled by a MNN. We propose to first initialize the MNN by the fast Fourier transform (FFT) based spectral estimation, and then optimize the MNN by the BP algorithm, which automatically yields the maximum likelihood (ML) parameter estimation of the frequency spectrum. We also design a method of merging and pruning the hidden-layer nodes of the MNN, which can be used for model-order selection, i.e., to estimate the number of sinusoids. Numerical simulations verify the effectiveness of the proposed method.      
### 22.A State-of-the-art Survey of U-Net in Microscopic Image Analysis: from Simple Usage to Structure Mortification  [ :arrow_down: ](https://arxiv.org/pdf/2202.06465.pdf)
>  Image analysis technology is used to solve the inadvertences of artificial traditional methods in disease, wastewater treatment, environmental change monitoring analysis and convolutional neural networks (CNN) play an important role in microscopic image analysis. An important step in detection, tracking, monitoring, feature extraction, modeling and analysis is image segmentation, in which U-Net has increasingly applied in microscopic image segmentation. This paper comprehensively reviews the development history of U-Net, and analyzes various research results of various segmentation methods since the emergence of U-Net and conducts a comprehensive review of related papers. First, This paper has summarizes the improved methods of U-Net and then listed the existing significances of image segmentation techniques and their improvements that has introduced over the years. Finally, focusing on the different improvement strategies of U-Net in different papers, the related work of each application target is reviewed according to detailed technical categories to facilitate future research. Researchers can clearly see the dynamics of transmission of technological development and keep up with future trends in this interdisciplinary field.      
### 23.Faster hyperspectral image classification based on selective kernel mechanism using deep convolutional networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.06458.pdf)
>  Hyperspectral imagery is rich in spatial and spectral information. Using 3D-CNN can simultaneously acquire features of spatial and spectral dimensions to facilitate classification of features, but hyperspectral image information spectral dimensional information redundancy. The use of continuous 3D-CNN will result in a high amount of parameters, and the computational power requirements of the device are high, and the training takes too long. This letter designed the Faster selective kernel mechanism network (FSKNet), FSKNet can balance this problem. It designs 3D-CNN and 2D-CNN conversion modules, using 3D-CNN to complete feature extraction while reducing the dimensionality of spatial and spectrum. However, such a model is not lightweight enough. In the converted 2D-CNN, a selective kernel mechanism is proposed, which allows each neuron to adjust the receptive field size based on the two-way input information scale. Under the Selective kernel mechanism, it mainly includes two components, se module and variable convolution. Se acquires channel dimensional attention and variable convolution to obtain spatial dimension deformation information of ground objects. The model is more accurate, faster, and less computationally intensive. FSKNet achieves high accuracy on the IN, UP, Salinas, and Botswana data sets with very small parameters.      
### 24.Ultrasonic Backscatter Communication for Implantable Medical Devices  [ :arrow_down: ](https://arxiv.org/pdf/2202.06440.pdf)
>  This paper proposes an ultrasonic backscatter communication (UsBC) system for passive implantable medical devices (IMDs) that can operate without batteries, enabling versatile revolutionary applications for future healthcare. The proposed UsBC system consists of a reader and a tag. The reader sends interrogation pulses to the tag. The tag backscatters the pulses based on the piezoelectric effect of a piezo transducer. We present several basic modulation schemes for UsBC by impedance matching of the piezo transducer. To mitigate the interference of other scatters in the human body, the tag transmits information bits by codeword mapping, and the reader performs codeword matching before energy detection in the reader. We further derive the theoretical bit-error rate (BER) expression. Monte Carlo simulations verify the theoretical analysis and show that passive UsBC can achieve low BER and low complexity, which is desirable for size- and energy-constrained IMDs.      
### 25.Robust Time-Varying Graph Signal Recovery Over Dynamic Topology  [ :arrow_down: ](https://arxiv.org/pdf/2202.06432.pdf)
>  We propose a time-varying graph signal recovery method that estimates the true graph signal from an observation corrupted by missing values, outliers of unknown positions, and some random noise. Furthermore, we assume the underlying graph to be time-varying like the signals, which we integrate into our formulation for better performance. Conventional studies on time-varying graph signal recovery have been focusing on online estimation and graph learning under the assumption that entire dynamic graphs are unavailable. However, there are many practical situations where the underlying graphs can be observed or easily generated by simple algorithms like the k-nearest neighbor, especially when targeting physical sensing data, where the graphs can be defined to represent spatial correlations. To address such cases, in this paper, we tackle a dynamic graph Laplacian-based recovery problem on given dynamic graphs. To solve this, we formulate the recovery problem as a constrained convex optimization problem to estimate both the time-varying graph signal and the sparsely modeled outliers simultaneously. We integrate the graph dynamics into the formulation by exploiting the given dynamic graph. In such a manner, we succeed in separating the different types of corruption and achieving state-of-the-art recovery performance in both synthetic and real-world problem settings. We also conduct an extensive study to compare the contribution of vertex and temporal domain regularization on the recovery performance.      
### 26.AI can evolve without labels: self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation  [ :arrow_down: ](https://arxiv.org/pdf/2202.06431.pdf)
>  Although deep learning-based computer-aided diagnosis systems have recently achieved expert-level performance, developing a robust deep learning model requires large, high-quality data with manual annotation, which is expensive to obtain. This situation poses the problem that the chest x-rays collected annually in hospitals cannot be used due to the lack of manual labeling by experts, especially in deprived areas. To address this, here we present a novel deep learning framework that uses knowledge distillation through self-supervised learning and self-training, which shows that the performance of the original model trained with a small number of labels can be gradually improved with more unlabeled data. Experimental results show that the proposed framework maintains impressive robustness against a real-world environment and has general applicability to several diagnostic tasks such as tuberculosis, pneumothorax, and COVID-19. Notably, we demonstrated that our model performs even better than those trained with the same amount of labeled data. The proposed framework has a great potential for medical imaging, where plenty of data is accumulated every year, but ground truth annotations are expensive to obtain.      
### 27.Distribution augmentation for low-resource expressive text-to-speech  [ :arrow_down: ](https://arxiv.org/pdf/2202.06409.pdf)
>  This paper presents a novel data augmentation technique for text-to-speech (TTS), that allows to generate new (text, audio) training examples without requiring any additional data. Our goal is to increase diversity of text conditionings available during training. This helps to reduce overfitting, especially in low-resource settings. Our method relies on substituting text and audio fragments in a way that preserves syntactical correctness. We take additional measures to ensure that synthesized speech does not contain artifacts caused by combining inconsistent audio samples. The perceptual evaluations show that our method improves speech quality over a number of datasets, speakers, and TTS architectures. We also demonstrate that it greatly improves robustness of attention-based TTS models.      
### 28.A Survey of Deep Learning Techniques for the Analysis of COVID-19 and their usability for Detecting Omicron  [ :arrow_down: ](https://arxiv.org/pdf/2202.06372.pdf)
>  The Coronavirus (COVID-19) outbreak in December 2019 has become an ongoing threat to humans worldwide, creating a health crisis that infected millions of lives, as well as devastating the global economy. Deep learning (DL) techniques have proved helpful in analysis and delineation of infectious regions in radiological images in a timely manner. This paper makes an in-depth survey of DL techniques and draws a taxonomy based on diagnostic strategies and learning approaches. DL techniques are systematically categorized into classification, segmentation, and multi-stage approaches for COVID-19 diagnosis at image and region level analysis. Each category includes pre-trained and custom-made Convolutional Neural Network architectures for detecting COVID-19 infection in radiographic imaging modalities; X-Ray, and Computer Tomography (CT). Furthermore, a discussion is made on challenges in developing diagnostic techniques in pandemic, cross-platform interoperability, and examining imaging modality, in addition to reviewing methodologies and performance measures used in these techniques. This survey provides an insight into promising areas of research in DL for analyzing radiographic images and thus, may further accelerate the research in designing of customized DL based diagnostic tools for effectively dealing with new variants of COVID-19 and emerging challenges.      
### 29.Learning Perspective Deformation in X-Ray Transmission Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2202.06366.pdf)
>  In cone-beam X-ray transmission imaging, due to the divergence of X-rays, imaged structures with different depths have different magnification factors on an X-ray detector, which results in perspective deformation. Perspective deformation causes difficulty in direct, accurate geometric assessments of anatomical structures. In this work, to reduce perspective deformation in X-ray images acquired from regular cone-beam computed tomography (CBCT) systems, we investigate on learning perspective deformation, i.e., converting perspective projections into orthogonal projections. Directly converting a single perspective projection image into an orthogonal projection image is extremely challenging due to the lack of depth information. Therefore, we propose to utilize one additional perspective projection, a complementary (180-degree) or orthogonal (90-degree) view, to provide a certain degree of depth information. Furthermore, learning perspective deformation in different spatial domains is investigated. Our proposed method is evaluated on numerical spherical bead phantoms as well as patients' chest and head X-ray data. The experiments on numerical bead phantom data demonstrate that learning perspective deformation in polar coordinates has significant advantages over learning in Cartesian coordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40, while learning in log-polar coordinates has no further considerable improvement (RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better than an orthogonal view (RMSE = 3.87). The experiments on patients' chest and head data demonstrate that learning perspective deformation using dual complementary views is also applicable in anatomical X-ray data, allowing accurate cardiothoracic ratio measurements in chest X-ray images and cephalometric analysis in synthetic cephalograms from cone-beam X-ray projections.      
### 30.A Data Augmentation Method for Fully Automatic Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.06344.pdf)
>  Automatic segmentation of glioma and its subregions is of great significance for diagnosis, treatment and monitoring of disease. In this paper, an augmentation method, called TensorMixup, was proposed and applied to the three dimensional U-Net architecture for brain tumor segmentation. The main ideas included that first, two image patches with size of 128 in three dimensions were selected according to glioma information of ground truth labels from the magnetic resonance imaging data of any two patients with the same modality. Next, a tensor in which all elements were independently sampled from Beta distribution was used to mix the image patches. Then the tensor was mapped to a matrix which was used to mix the one-hot encoded labels of the above image patches. Therefore, a new image and its one-hot encoded label were synthesized. Finally, the new data was used to train the model which could be used to segment glioma. The experimental results show that the mean accuracy of Dice scores are 91.32%, 85.67%, and 82.20% respectively on the whole tumor, tumor core, and enhancing tumor segmentation, which proves that the proposed TensorMixup is feasible and effective for brain tumor segmentation.      
### 31.DEEPCHORUS: A Hybrid Model of Multi-scale Convolution and Self-attention for Chorus Detection  [ :arrow_down: ](https://arxiv.org/pdf/2202.06338.pdf)
>  Chorus detection is a challenging problem in musical signal processing as the chorus often repeats more than once in popular songs, usually with rich instruments and complex rhythm forms. Most of the existing works focus on the receptiveness of chorus sections based on some explicit features such as loudness and occurrence frequency. These pre-assumptions for chorus limit the generalization capacity of these methods, causing misdetection on other repeated sections such as verse. To solve the problem, in this paper we propose an end-to-end chorus detection model DeepChorus, reducing the engineering effort and the need for prior knowledge. The proposed model includes two main structures: i) a Multi-Scale Network to derive preliminary representations of chorus segments, and ii) a Self-Attention Convolution Network to further process the features into probability curves representing chorus presence. To obtain the final results, we apply an adaptive threshold to binarize the original curve. The experimental results show that DeepChorus outperforms existing state-of-the-art methods in most cases.      
### 32.Adaptive Control with Guaranteed Transient Behavior and Zero Steady-State Error for Systems with Time-Varying Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2202.06320.pdf)
>  It is nontrivial to achieve global zero-error regulation for uncertain nonlinear systems. The underlying problem becomes even more challenging if mismatched uncertainties and unknown time-varying control gain are involved, yet certain performance specifications are also pursued. In this work, we present an adaptive control method, which, without the persistent excitation (PE) condition, is able to ensure global zero-error regulation with guaranteed output performance for parametric strict-feedback systems involving fast time-varying parameters in the feedback path and input path. The development of our control scheme benefits from generalized t-dependent and x-dependent functions, a novel coordinate transformation and "congelation of variables" method. Both theoretical analysis and numerical simulation verify the effectiveness and benefits of the proposed method.      
### 33.Significant Low-dimensional Spectral-temporal Features for Seizure Detection  [ :arrow_down: ](https://arxiv.org/pdf/2202.06284.pdf)
>  Seizure onset detection in electroencephalography (EEG) signals is a challenging task due to the non-stereotyped seizure activities as well as their stochastic and non-stationary characteristics in nature. Joint spectral-temporal features are believed to contain sufficient and powerful feature information for absence seizure detection. However, the resulting high-dimensional features involve redundant information and require heavy computational load. Here, we discover significant low-dimensional spectral-temporal features in terms of mean-standard deviation of wavelet transform coefficient (MS-WTC), based on which a novel absence seizure detection framework is developed. The EEG signals are transformed into the spectral-temporal domain, with their low-dimensional features fed into a convolutional neural network. Superior detection performance is achieved on the widely-used benchmark dataset as well as a clinical dataset from the Chinese 301 Hospital. For the former, seven classification tasks were evaluated with the accuracy from 99.8% to 100.0%, while for the latter, the method achieved a mean accuracy of 94.7%, overwhelming other methods with low-dimensional temporal and spectral features. Experimental results on two seizure datasets demonstrate reliability, efficiency and stability of our proposed MS-WTC method, validating the significance of the extracted low-dimensional spectral-temporal features.      
### 34.Distributed Periodic Event-triggered Control of Nonlinear Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06282.pdf)
>  We present a general emulation-based framework to address the distributed control of multi-agent systems over packet-based networks. We consider the setup where information is only transmitted at (non-uniform) sampling times and where packets are received with unknown delays. We design local dynamic event triggering mechanisms to generate the transmissions. The triggering mechanisms can run on non-synchronized digital platforms, i.e., we ensure that the conditions must only be verified at asynchronous sampling times, which differ for each platform. Different stability and performance characteristics can be considered as we follow a general dissipativity-based approach. Moreover, Zeno-free properties are guaranteed by design. The results are illustrated on a consensus problem.      
### 35.Robust Consensus of Higher-Order Multi-Agent Systems With Attrition and Inclusion of Agents and Switching Topologies  [ :arrow_down: ](https://arxiv.org/pdf/2202.06261.pdf)
>  Some of the issues associated with the practical applications of consensus of multi-agent systems (MAS) include switching topologies, attrition and inclusion of agents from an existing network, and model uncertainties of agents. In this paper, a single distributed dynamic state-feedback protocol referred to as the Robust Attrition-Inclusion Distributed Dynamic (RAIDD) consensus protocol, is synthesized for achieving the consensus of MAS with attrition and inclusion of linear time-invariant higher-order uncertain homogeneous agents and switching topologies. A state consensus problem termed as the Robust Attrition-Inclusion (RAI) consensus problem is formulated to find this RAIDD consensus protocol. To solve this RAI consensus problem, first, the sufficient condition for the existence of the RAIDD protocol is obtained using the $\nu$-gap metric-based simultaneous stabilization approach. Next, the RAIDD consensus protocol is attained using the Glover-McFarlane robust stabilization method if the sufficient condition is satisfied. The performance of this RAIDD protocol is validated by numerical simulations.      
### 36.LTSP: Long-Term Slice Propagation for Accurate Airway Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.06260.pdf)
>  Purpose: Bronchoscopic intervention is a widely-used clinical technique for pulmonary diseases, which requires an accurate and topological complete airway map for its localization and guidance. The airway map could be extracted from chest computed tomography (CT) scans automatically by airway segmentation methods. Due to the complex tree-like structure of the airway, preserving its topology completeness while maintaining the segmentation accuracy is a challenging task. <br>Methods: In this paper, a long-term slice propagation (LTSP) method is proposed for accurate airway segmentation from pathological CT scans. We also design a two-stage end-to-end segmentation framework utilizing the LTSP method in the decoding process. Stage 1 is used to generate a coarse feature map by an encoder-decoder architecture. Stage 2 is to adopt the proposed LTSP method for exploiting the continuity information and enhancing the weak airway features in the coarse feature map. The final segmentation result is predicted from the refined feature map. <br>Results: Extensive experiments were conducted to evaluate the performance of the proposed method on 70 clinical CT scans. The results demonstrate the considerable improvements of the proposed method compared to some state-of-the-art methods as most breakages are eliminated and more tiny bronchi are detected. The ablation studies further confirm the effectiveness of the constituents of the proposed method. <br>Conclusion: Slice continuity information is beneficial to accurate airway segmentation. Furthermore, by propagating the long-term slice feature, the airway topology connectivity is preserved with overall segmentation accuracy maintained.      
### 37.Multimodal Depression Classification Using Articulatory Coordination Features And Hierarchical Attention Based Text Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2202.06238.pdf)
>  Multimodal depression classification has gained immense popularity over the recent years. We develop a multimodal depression classification system using articulatory coordination features extracted from vocal tract variables and text transcriptions obtained from an automatic speech recognition tool that yields improvements of area under the receiver operating characteristics curve compared to uni-modal classifiers (7.5% and 13.7% for audio and text respectively). We show that in the case of limited training data, a segment-level classifier can first be trained to then obtain a session-wise prediction without hindering the performance, using a multi-stage convolutional recurrent neural network. A text model is trained using a Hierarchical Attention Network (HAN). The multimodal system is developed by combining embeddings from the session-level audio model and the HAN text model      
### 38.Performance Analysis of Multi-Reconfigurable Intelligent Surface-Empowered THz Wireless Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06231.pdf)
>  In this paper, we introduce a theoretical framework for analyzing the performance of multi-reconfigurable intelligence surface (RIS) empowered terahertz (THz) wireless systems subject to turbulence and stochastic beam misalignment. In more detail, we extract a closed-form expression for the outage probability that quantifies the joint impact of turbulence and misalignment as well as the effect of transceivers' hardware imperfections. Our results highlight the importance of accurately modeling both turbulence and misalignment when assessing the performance of multi-RIS-empowered THz wireless systems.      
### 39.Feature Construction and Selection for PV Solar Power Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2202.06226.pdf)
>  Using solar power in the process industry can reduce greenhouse gas emissions and make the production process more sustainable. However, the intermittent nature of solar power renders its usage challenging. Building a model to predict photovoltaic (PV) power generation allows decision-makers to hedge energy shortages and further design proper operations. The solar power output is time-series data dependent on many factors, such as irradiance and weather. A machine learning framework for 1-hour ahead solar power prediction is developed in this paper based on the historical data. Our method extends the input dataset into higher dimensional Chebyshev polynomial space. Then, a feature selection scheme is developed with constrained linear regression to construct the predictor for different weather types. Several tests show that the proposed approach yields lower mean squared error than classical machine learning methods, such as support vector machine (SVM), random forest (RF), and gradient boosting decision tree (GBDT).      
### 40.Do cities have a unique magnetic pulse?  [ :arrow_down: ](https://arxiv.org/pdf/2202.06166.pdf)
>  We present a comparative analysis of urban magnetic fields between two American cities: Berkeley (California) and Brooklyn Borough of New York City (New York). Our analysis uses data taken over a four-week period during which magnetic field data were continuously recorded using a fluxgate magnetometer of 70 pT/$\sqrt{\mathrm{Hz}}$ sensitivity. We identified significant differences in the magnetic signatures. In particular, we noticed that Berkeley reaches a near-zero magnetic field activity at night whereas magnetic activity in Brooklyn continues during nighttime. We also present auxiliary measurements acquired using magnetoresistive vector magnetometers (VMR), with sensitivity of 300 pT/$\sqrt{\mathrm{Hz}}$, and demonstrate how cross-correlation, and frequency-domain analysis, combined with data filtering can be used to extract urban magnetometry signals and study local anthropogenic activities. Finally, we discuss the potential of using magnetometer networks to characterize the global magnetic field of cities and give directions for future development.      
### 41.Multi-task Deep Learning for Cerebrovascular Disease Classification and MRI-to-PET Translation  [ :arrow_down: ](https://arxiv.org/pdf/2202.06142.pdf)
>  Accurate quantification of cerebral blood flow (CBF) is essential for the diagnosis and assessment of cerebrovascular diseases such as Moyamoya, carotid stenosis, aneurysms, and stroke. Positron emission tomography (PET) is currently regarded as the gold standard for the measurement of CBF in the human brain. PET imaging, however, is not widely available because of its prohibitive costs, use of ionizing radiation, and logistical challenges, which require a co-localized cyclotron to deliver the 2 min half-life Oxygen-15 radioisotope. Magnetic resonance imaging (MRI), in contrast, is more readily available and does not involve ionizing radiation. In this study, we propose a multi-task learning framework for brain MRI-to-PET translation and disease diagnosis. The proposed framework comprises two prime networks: (1) an attention-based 3D encoder-decoder convolutional neural network (CNN) that synthesizes high-quality PET CBF maps from multi-contrast MRI images, and (2) a multi-scale 3D CNN that identifies the brain disease corresponding to the input MRI images. Our multi-task framework yields promising results on the task of MRI-to-PET translation, achieving an average structural similarity index (SSIM) of 0.94 and peak signal-to-noise ratio (PSNR) of 38dB on a cohort of 120 subjects. In addition, we show that integrating multiple MRI modalities can improve the clinical diagnosis of brain diseases.      
### 42.Grasp-and-Lift Detection from EEG Signal Using Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.06128.pdf)
>  People undergoing neuromuscular dysfunctions and amputated limbs require automatic prosthetic appliances. In developing such prostheses, the precise detection of brain motor actions is imperative for the Grasp-and-Lift (GAL) tasks. Because of the low-cost and non-invasive essence of Electroencephalography (EEG), it is widely preferred for detecting motor actions during the controls of prosthetic tools. This article has automated the hand movement activity viz GAL detection method from the 32-channel EEG signals. The proposed pipeline essentially combines preprocessing and end-to-end detection steps, eliminating the requirement of hand-crafted feature engineering. Preprocessing action consists of raw signal denoising, using either Discrete Wavelet Transform (DWT) or highpass or bandpass filtering and data standardization. The detection step consists of Convolutional Neural Network (CNN)- or Long Short Term Memory (LSTM)-based model. All the investigations utilize the publicly available WAY-EEG-GAL dataset, having six different GAL events. The best experiment reveals that the proposed framework achieves an average area under the ROC curve of 0.944, employing the DWT-based denoising filter, data standardization, and CNN-based detection model. The obtained outcome designates an excellent achievement of the introduced method in detecting GAL events from the EEG signals, turning it applicable to prosthetic appliances, brain-computer interfaces, robotic arms, etc.      
### 43.Multicasting in NOMA-Based UAV Networks: Path Design and Throughput Maximization  [ :arrow_down: ](https://arxiv.org/pdf/2202.06127.pdf)
>  In this paper, we propose a new resource allocation framework for unmanned aerial vehicle (UAV) assisted multicast wireless networks in which the network users according to their request are divided into several multicast groups. We adopt power domain non-orthogonal multiple access (PD-NOMA) as the transmission technology using which the dedicated signals of multicast groups are superimposed and transmitted simultaneously as the UAV passes over the communication area for fixed and mobile users. We discuss the proposed scenarios from two perspectives, offline and online mode. In offline mode, we implement the problem for fixed and mobile users whose locations are predictable (the location of users, over the communication time, is known at the beginning of the communication time) and in online mode for mobile users whose locations are unpredictable (the location of users, over the communication time, is unknown at the beginning of the communication time). Also, we proposed a scenario in which the online model the number of mobile users can grow in each time slot. We formulate the problem of joint power allocation and UAV trajectory design as an optimization problem that is non-linear and nonconvex for two proposed scenarios. To solve the problem, we adopt an alternate search method (ASM), successive convex approximation (SCA), and geometric programming (GP). Using simulation results, the performance of the proposed scheme is evaluated for different values of the network parameters.      
### 44.An efficient implementation of graph-based invariant set algorithm for constrained nonlinear dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06111.pdf)
>  The graph-based invariant set (GIS) algorithm is a promising set-based technique for computing the largest (with respect to inclusion) control invariant set of general discrete-time nonlinear dynamical systems. However, like other invariant set algorithms for nonlinear systems, the GIS algorithm may require a lot of resources when computing the control invariant set. This limits its applicability to higher dimensional systems. In this work, we present an improved and efficient implementation of the GIS algorithm for general discrete-time controlled nonlinear systems. We first identify the bottlenecks through extensive analysis, and then provide remedial procedures to improve the implementation of the GIS algorithm. Specifically, we developed an adaptive subdivision scheme using a supervised machine learning-based algorithm to reduce the cell growth rate and parallelize the graph construction step. We extensively demonstrate the performance of the improved GIS algorithm using a numerical example and compare the result to that of the standard GIS algorithm. The results show that the adaptive subdivision and the parallelization improved the speed of the algorithm by about 8x and 3x respectively, that of the standard GIS algorithm.      
### 45.Breast Cancer Detection using Histopathological Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.06109.pdf)
>  Cancer is one of the most common and fatal diseases in the world. Breast cancer affects one in every eight women and one in every eight hundred men. Hence, our prime target should be early detection of cancer because the early detection of cancer can be helpful to cure cancer effectively. Therefore, we propose a saliency detection system with the help of advanced deep learning techniques, such that the machine will be taught to emulate actions of pathologists for localization of diagnostically pertinent regions. We study identification of five diagnostic categories of breast cancer by training a CNN (VGG16, ResNet architecture). We have used BreakHis dataset to train our model. We focus on both detection and classification of cancerous regions in histopathology images. The diagnostically relevant regions are salient. The detection system will be available as an open source web application which can be used by pathologists and medical institutions.      
### 46.A Fully Decentralized Tuning-free Inexact Projection Method for P2P Energy Trading  [ :arrow_down: ](https://arxiv.org/pdf/2202.06106.pdf)
>  Agent-based solutions lend themselves well to address privacy concerns and the computational scalability needs of future distributed electric grids and end-use energy exchanges. Decentralized decision-making methods are the key to enabling peer-to-peer energy trading between electricity prosumers. However, the performance of existing decentralized decision-making algorithms highly depends on the algorithmic design and hyperparameter tunings, limiting applicability. This paper aims to address this gap by proposing a decentralized inexact projection method that does not rely on parameter tuning or central coordination to achieve the optimal solution for Peer-to-Peer (P2P) energy trading problems. The proposed algorithm does not require parameter readjustments, and once tuned, it converges for a wide range of P2P setups. Moreover, each prosumer only needs to share limited information (i.e., updated coupled variable) with neighboring prosumers. The IEEE 13 bus test system is used to showcase our proposed method's robustness and privacy advantages.      
### 47.On Federated Learning with Energy Harvesting Clients  [ :arrow_down: ](https://arxiv.org/pdf/2202.06105.pdf)
>  Catering to the proliferation of Internet of Things devices and distributed machine learning at the edge, we propose an energy harvesting federated learning (EHFL) framework in this paper. The introduction of EH implies that a client's availability to participate in any FL round cannot be guaranteed, which complicates the theoretical analysis. We derive novel convergence bounds that capture the impact of time-varying device availabilities due to the random EH characteristics of the participating clients, for both parallel and local stochastic gradient descent (SGD) with non-convex loss functions. The results suggest that having a uniform client scheduling that maximizes the minimum number of clients throughout the FL process is desirable, which is further corroborated by the numerical experiments using a real-world FL task and a state-of-the-art EH scheduler.      
### 48.Semi-supervised Medical Image Segmentation via Geometry-aware Consistency Training  [ :arrow_down: ](https://arxiv.org/pdf/2202.06104.pdf)
>  The performance of supervised deep learning methods for medical image segmentation is often limited by the scarcity of labeled data. As a promising research direction, semi-supervised learning addresses this dilemma by leveraging unlabeled data information to assist the learning process. In this paper, a novel geometry-aware semi-supervised learning framework is proposed for medical image segmentation, which is a consistency-based method. Considering that the hard-to-segment regions are mainly located around the object boundary, we introduce an auxiliary prediction task to learn the global geometric information. Based on the geometric constraint, the ambiguous boundary regions are emphasized through an exponentially weighted strategy for the model training to better exploit both labeled and unlabeled data. In addition, a dual-view network is designed to perform segmentation from different perspectives and reduce the prediction uncertainty. The proposed method is evaluated on the public left atrium benchmark dataset and improves fully supervised method by 8.7% in Dice with 10% labeled images, while 4.3% with 20% labeled images. Meanwhile, our framework outperforms six state-of-the-art semi-supervised segmentation methods.      
### 49.Classification of Microscopy Images of Breast Tissue: Region Duplication based Self-Supervision vs. Off-the Shelf Deep Representations  [ :arrow_down: ](https://arxiv.org/pdf/2202.06073.pdf)
>  Breast cancer is one of the leading causes of female mortality in the world. This can be reduced when diagnoses are performed at the early stages of progression. Further, the efficiency of the process can be significantly improved with computer aided diagnosis. Deep learning based approaches have been successfully applied to achieve this. One of the limiting factors for training deep networks in a supervised manner is the dependency on large amounts of expert annotated data. In reality, large amounts of unlabelled data and only small amounts of expert annotated data are available. In such scenarios, transfer learning approaches and self-supervised learning (SSL) based approaches can be leveraged. In this study, we propose a novel self-supervision pretext task to train a convolutional neural network (CNN) and extract domain specific features. This method was compared with deep features extracted using pre-trained CNNs such as DenseNet-121 and ResNet-50 trained on ImageNet. Additionally, two types of patch-combination methods were introduced and compared with majority voting. The methods were validated on the BACH microscopy images dataset. Results indicated that the best performance of 99% sensitivity was achieved for the deep features extracted using ResNet50 with concatenation of patch-level embedding. Preliminary results of SSL to extract domain specific features indicated that with just 15% of unlabelled data a high sensitivity of 94% can be achieved for a four class classification of microscopy images.      
### 50.Application of Modular Vehicle Technology to Mitigate Bus Bunching  [ :arrow_down: ](https://arxiv.org/pdf/2202.06039.pdf)
>  The stochastic nature of public transport systems leads to headway variability and bus bunching, causing both operator and passenger cost to increase significantly. Traditional strategies to counter bus bunching, including bus-holding, stop-skipping, and bus substitution/insertion, suffer from trade-offs and shortcomings. Autonomous modular vehicle (AMV) technology provides an additional level of flexibility in bus dispatching and operations, which can offer significant benefits in mitigating bus bunching compared to strategies available with conventional buses. This paper introduces a novel alternative to stop-skipping by leveraging the new capabilities offered by AMVs (in particular, en-route coupling and decoupling of modular units). We develop a simple bus-splitting strategy that directs a modular bus to decouple into individual units when it experiences a headway longer than a given threshold. We then use a macroscopic simulation to present a proof-of-concept evaluation of the proposed modular strategy compared to a benchmark traditional stop-skipping strategy and the base (no control) case. We find that the proposed strategy outperforms the benchmark in decreasing each of the three travel time components: waiting time, in-vehicle time, and walking time (which it eliminates completely). It therefore reduces the overhead of bus bunching and thus the travel cost by more than twice as much as the benchmark for busy bus lines. Simultaneously, it also reduces headway variability to a comparable degree. Furthermore, we analyze different control thresholds for applying the proposed strategy, and show that it is most effective when applied proactively, i.e. with the control action being triggered even by small headway deviations.      
### 51.FrAUG: A Frame Rate Based Data Augmentation Method for Depression Detection from Speech Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.05912.pdf)
>  In this paper, a data augmentation method is proposed for depression detection from speech signals. Samples for data augmentation were created by changing the frame-width and the frame-shift parameters during the feature extraction process. Unlike other data augmentation methods (such as VTLP, pitch perturbation, or speed perturbation), the proposed method does not explicitly change acoustic parameters but rather the time-frequency resolution of frame-level features. The proposed method was evaluated using two different datasets, models, and input acoustic features. For the DAIC-WOZ (English) dataset when using the DepAudioNet model and mel-Spectrograms as input, the proposed method resulted in an improvement of 5.97% (validation) and 25.13% (test) when compared to the baseline. The improvements for the CONVERGE (Mandarin) dataset when using the x-vector embeddings with CNN as the backend and MFCCs as input features were 9.32% (validation) and 12.99% (test). Baseline systems do not incorporate any data augmentation. Further, the proposed method outperformed commonly used data-augmentation methods such as noise augmentation, VTLP, Speed, and Pitch Perturbation. All improvements were statistically significant.      
### 52.An Invariant Set Construction Method, Applied to Safe Coordination of Thermostatic Loads  [ :arrow_down: ](https://arxiv.org/pdf/2202.05887.pdf)
>  We consider the problem of coordinating a collection of switched subsystems under both local and global constraints for safe operation of the system. Although an invariant set can be leveraged to construct a safety-guaranteed controller for this kind of problem, computing an invariant set is not scalable to high-dimensional systems. In this paper, we introduce a strategy to obtain an implicit representation of a controlled invariant set for a collection of switched subsystems, and construct a safety-guaranteed controller to coordinate the subsystems using the representation. Specifically, we incorporate the invariant set into a model predictive controller to guarantee safety and recursive feasibility. Since the amount of computations is independent of the number of subsystems, this approach scales to large collections of switched subsystems. We use our approach to safely control a collection of thermostatically controlled loads to provide grid balancing services. The problem includes constraints on each load's temperature and duration it must remain in a mode after a switch, and also on aggregate power consumption to ensure network safety. Numerical simulations show that the proposed approach outperforms benchmark strategies in terms of safety and recursive feasibility.      
### 53.Convex Programs and Lyapunov Functions for Reinforcement Learning: A Unified Perspective on the Analysis of Value-Based Methods  [ :arrow_down: ](https://arxiv.org/pdf/2202.06922.pdf)
>  Value-based methods play a fundamental role in Markov decision processes (MDPs) and reinforcement learning (RL). In this paper, we present a unified control-theoretic framework for analyzing valued-based methods such as value computation (VC), value iteration (VI), and temporal difference (TD) learning (with linear function approximation). Built upon an intrinsic connection between value-based methods and dynamic systems, we can directly use existing convex testing conditions in control theory to derive various convergence results for the aforementioned value-based methods. These testing conditions are convex programs in form of either linear programming (LP) or semidefinite programming (SDP), and can be solved to construct Lyapunov functions in a straightforward manner. Our analysis reveals some intriguing connections between feedback control systems and RL algorithms. It is our hope that such connections can inspire more work at the intersection of system/control theory and RL.      
### 54.Visual Acoustic Matching  [ :arrow_down: ](https://arxiv.org/pdf/2202.06875.pdf)
>  We introduce the visual acoustic matching task, in which an audio clip is transformed to sound like it was recorded in a target environment. Given an image of the target environment and a waveform for the source audio, the goal is to re-synthesize the audio to match the target room acoustics as suggested by its visible geometry and materials. To address this novel task, we propose a cross-modal transformer model that uses audio-visual attention to inject visual properties into the audio and generate realistic audio output. In addition, we devise a self-supervised training objective that can learn acoustic matching from in-the-wild Web videos, despite their lack of acoustically mismatched audio. We demonstrate that our approach successfully translates human speech to a variety of real-world environments depicted in images, outperforming both traditional acoustic matching and more heavily supervised baselines.      
### 55.Multi-Task Deep Residual Echo Suppression with Echo-aware Loss  [ :arrow_down: ](https://arxiv.org/pdf/2202.06850.pdf)
>  This paper introduces the NWPU Team's entry to the ICASSP 2022 AEC Challenge. We take a hybrid approach that cascades a linear AEC with a neural post-filter. The former is used to deal with the linear echo components while the latter suppresses the residual non-linear echo components. We use gated convolutional F-T-LSTM neural network (GFTNN) as the backbone and shape the post-filter by a multi-task learning (MTL) framework, where a voice activity detection (VAD) module is adopted as an auxiliary task along with echo suppression, with the aim to avoid over suppression that may cause speech distortion. Moreover, we adopt an echo-aware loss function, where the mean square error (MSE) loss can be optimized particularly for every time-frequency bin (TF-bin) according to the signal-to-echo ratio (SER), leading to further suppression on the echo. Extensive ablation study shows that the time delay estimation (TDE) module in neural post-filter leads to better perceptual quality, and an adaptive filter with better convergence will bring consistent performance gain for the post-filter. Besides, we find that using the linear echo as the input of our neural post-filter is a better choice than using the reference signal directly. In the ICASSP 2022 AEC-Challenge, our approach has ranked the 1st place on word acceptance rate (WAcc) (0.817) and the 3rd place on both mean opinion score (MOS) (4.502) and the final score (0.864).      
### 56.Graph-GAN: A spatial-temporal neural network for short-term passenger flow prediction in urban rail transit systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06727.pdf)
>  Short-term passenger flow prediction plays an important role in better managing the urban rail transit (URT) systems. Emerging deep learning models provide good insights to improve short-term prediction accuracy. However, a large number of existing prediction models combine diverse neural network layers to improve accuracy, making their model structures extremely complex and difficult to be applied to the real world. Therefore, it is necessary to trade off between the model complexity and prediction performance from the perspective of real-world applications. To this end, we propose a deep learning-based Graph-GAN model with a simple structure and high prediction accuracy to predict short-term passenger flows of the URT network. The Graph-GAN consists of two major parts: (1) a simplified and static version of the graph convolution network (GCN) used to extract network topological information; (2) a generative adversarial network (GAN) used to predict passenger flows, with generators and discriminators in GAN just composed of simple fully connected neural networks. The Graph-GAN is tested on two large-scale real-world datasets from Beijing Subway. A comparison of the prediction performance of Graph-GAN with those of several state-of-the-art models illustrates its superiority and robustness. This study can provide critical experience in conducting short-term passenger flow predictions, especially from the perspective of real-world applications.      
### 57.Secure-by-Construction Synthesis of Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06677.pdf)
>  Correct-by-construction synthesis is a cornerstone of the confluence of formal methods and control theory towards designing safety-critical systems. Instead of following the time-tested, albeit laborious (re)design-verify-validate loop, correct-by-construction methodology advocates the use of continual refinements of formal requirements -- connected by chains of formal proofs -- to build a system that assures the correctness by design. A remarkable progress has been made in scaling the scope of applicability of correct-by-construction synthesis -- with a focus on cyber-physical systems that tie discrete-event control with continuous environment -- to enlarge control systems by combining symbolic approaches with principled state-space reduction techniques. <br>Unfortunately, in the security-critical control systems, the security properties are verified ex post facto the design process in a way that undermines the correct-by-construction paradigm. We posit that, to truly realize the dream of correct-by-construction synthesis for security-critical systems, security considerations must take center-stage with the safety considerations. Moreover, catalyzed by the recent progress on the opacity sub-classes of security properties and the notion of hyperproperties capable of combining security with safety properties, we believe that the time is ripe for the research community to holistically target the challenge of secure-by-construction synthesis. This paper details our vision by highlighting the recent progress and open challenges that may serve as bricks for providing a solid foundation for secure-by-construction synthesis of cyber-physical systems.      
### 58.Embedded quantitative MRI T1rho mapping using non-linear primal-dual proximal splitting  [ :arrow_down: ](https://arxiv.org/pdf/2202.06613.pdf)
>  Quantitative MRI (qMRI) methods allow reducing the subjectivity of clinical MRI by providing numerical values on which diagnostic assessment or predictions of tissue properties can be based. However, qMRI measurements typically take more time than anatomical imaging due to requiring multiple measurements with varying contrasts for, e.g., relaxation time mapping. To reduce the scanning time, undersampled data may be combined with compressed sensing reconstruction techniques. Typical CS reconstructions first reconstruct a complex-valued set of images corresponding to the varying contrasts, followed by a non-linear signal model fit to obtain the parameter maps. We propose a direct, embedded reconstruction method for T1rho mapping. The proposed method capitalizes on a known signal model to directly reconstruct the desired parameter map using a non-linear optimization model. The proposed reconstruction method also allows directly regularizing the parameter map of interest, and greatly reduces the number of unknowns in the reconstruction. We test the proposed model using a simulated radially sampled data from a 2D phantom and 2D cartesian ex vivo measurements of a mouse kidney specimen. We compare the embedded reconstruction model to two CS reconstruction models, and in the cartesian test case also iFFT. The proposed, embedded model outperformed the reference methods on both test cases, especially with higher acceleration factors.      
### 59.An Introduction to Neural Data Compression  [ :arrow_down: ](https://arxiv.org/pdf/2202.06533.pdf)
>  Neural compression is the application of neural networks and other machine learning methods to data compression. While machine learning deals with many concepts closely related to compression, entering the field of neural compression can be difficult due to its reliance on information theory, perceptual metrics, and other knowledge specific to the field. This introduction hopes to fill in the necessary background by reviewing basic coding topics such as entropy coding and rate-distortion theory, related machine learning ideas such as bits-back coding and perceptual metrics, and providing a guide through the representative works in the literature so far.      
### 60.Semantic Communication Meets Edge Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2202.06471.pdf)
>  The development of emerging applications, such as autonomous transportation systems, are expected to result in an explosive growth in mobile data traffic. As the available spectrum resource becomes more and more scarce, there is a growing need for a paradigm shift from Shannon's Classical Information Theory (CIT) to semantic communication (SemCom). Specifically, the former adopts a "transmit-before-understanding" approach while the latter leverages artificial intelligence (AI) techniques to "understand-before-transmit", thereby alleviating bandwidth pressure by reducing the amount of data to be exchanged without negating the semantic effectiveness of the transmitted symbols. However, the semantic extraction (SE) procedure incurs costly computation and storage overheads. In this article, we introduce an edge-driven training, maintenance, and execution of SE. We further investigate how edge intelligence can be enhanced with SemCom through improving the generalization capabilities of intelligent agents at lower computation overheads and reducing the communication overhead of information exchange. Finally, we present a case study involving semantic-aware resource optimization for the wireless powered Internet of Things (IoT).      
### 61.Multi-user Beam Alignment in Presence of Multi-path  [ :arrow_down: ](https://arxiv.org/pdf/2202.06452.pdf)
>  To overcome the high path-loss and the intense shadowing in millimeter-wave (mmWave) communications, effective beamforming schemes are required which incorporate narrow beams with high beamforming gains. The mmWave channel consists of a few spatial clusters each associated with an angle of departure (AoD). The narrow beams must be aligned with the channel AoDs to increase the beamforming gain. This is achieved through a procedure called beam alignment (BA). Most of the BA schemes in the literature consider channels with a single dominant path while in practice the channel has a few resolvable paths with different AoDs, hence, such BA schemes may not work correctly in the presence of multi-path or at the least do not exploit such multipath to achieve diversity or increase robustness. <br>In this paper, we propose an efficient BA scheme in presence of multi-path. The proposed BA scheme transmits probing packets using a set of scanning beams and receives feedback for all the scanning beams at the end of the probing phase from each user. We formulate the BA scheme as minimizing the expected value of the average transmission beamwidth under different policies. The policy is defined as a function from the set of received feedback to the set of transmission beams (TB). In order to maximize the number of possible feedback sequences, we prove that the set of scanning beams (SB) has a special form, namely, Tulip Design. Consequently, we rewrite the minimization problem with a set of linear constraints and a reduced number of variables which is solved by using an efficient greedy algorithm.      
### 62.Communication and Computation O-RAN Resource Slicing for URLLC Services Using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.06439.pdf)
>  The evolution of the future beyond-5G/6G networks towards a service-aware network is based on network slicing technology. With network slicing, communication service providers seek to meet all the requirements imposed by the verticals, including ultra-reliable low-latency communication (URLLC) services. In addition, the open radio access network (O-RAN) architecture paves the way for flexible sharing of network resources by introducing more programmability into the RAN. RAN slicing is an essential part of end-to-end network slicing since it ensures efficient sharing of communication and computation resources. However, due to the stringent requirements of URLLC services and the dynamics of the RAN environment, RAN slicing is challenging. In this article, we propose a two-level RAN slicing approach based on the O-RAN architecture to allocate the communication and computation RAN resources among URLLC end-devices. For each RAN slicing level, we model the resource slicing problem as a single-agent Markov decision process and design a deep reinforcement learning algorithm to solve it. Simulation results demonstrate the efficiency of the proposed approach in meeting the desired quality of service requirements.      
### 63.Visual Sound Localization in the Wild by Cross-Modal Interference Erasing  [ :arrow_down: ](https://arxiv.org/pdf/2202.06406.pdf)
>  The task of audio-visual sound source localization has been well studied under constrained scenes, where the audio recordings are clean. However, in real-world scenarios, audios are usually contaminated by off-screen sound and background noise. They will interfere with the procedure of identifying desired sources and building visual-sound connections, making previous studies non-applicable. In this work, we propose the Interference Eraser (IEr) framework, which tackles the problem of audio-visual sound source localization in the wild. The key idea is to eliminate the interference by redefining and carving discriminative audio representations. Specifically, we observe that the previous practice of learning only a single audio representation is insufficient due to the additive nature of audio signals. We thus extend the audio representation with our Audio-Instance-Identifier module, which clearly distinguishes sounding instances when audio signals of different volumes are unevenly mixed. Then we erase the influence of the audible but off-screen sounds and the silent but visible objects by a Cross-modal Referrer module with cross-modality distillation. Quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior results on sound localization tasks, especially under real-world scenarios. Code is available at <a class="link-external link-https" href="https://github.com/alvinliu0/Visual-Sound-Localization-in-the-Wild" rel="external noopener nofollow">this https URL</a>.      
### 64.Scheduling Techniques for Liver Segmentation: ReduceLRonPlateau Vs OneCycleLR  [ :arrow_down: ](https://arxiv.org/pdf/2202.06373.pdf)
>  Machine learning and computer vision techniques have influenced many fields including the biomedical one. The aim of this paper is to investigate the important concept of schedulers in manipulating the learning rate (LR), for the liver segmentation task, throughout the training process, focusing on the newly devised OneCycleLR against the ReduceLRonPlateau. A dataset, published in 2018 and produced by the Medical Segmentation Decathlon Challenge organizers, called Task 8 Hepatic Vessel (MSDC-T8) has been used for testing and validation. The reported results that have the same number of maximum epochs (75), and are the average of 5-fold cross-validation, indicate that ReduceLRonPlateau converges faster while maintaining a similar or even better loss score on the validation set when compared to OneCycleLR. The epoch at which the peak LR occurs perhaps should be made early for the OneCycleLR such that the super-convergence feature can be observed. Moreover, the overall results outperform the state-of-the-art results from the researchers who published the liver masks for this dataset. To conclude, both schedulers are suitable for medical segmentation challenges, especially the MSDC-T8 dataset, and can be used confidently in rapidly converging the validation loss with a minimal number of epochs.      
### 65.Omnifont Persian OCR System Using Primitives  [ :arrow_down: ](https://arxiv.org/pdf/2202.06371.pdf)
>  In this paper, we introduce a model-based omnifont Persian OCR system. The system uses a set of 8 primitive elements as structural features for recognition. First, the scanned document is preprocessed. After normalizing the preprocessed image, text rows and sub-words are separated and then thinned. After recognition of dots in sub-words, strokes are extracted and primitive elements of each sub-word are recognized using the strokes. Finally, the primitives are compared with a predefined set of character identification vectors in order to identify sub-word characters. The separation and recognition steps of the system are concurrent, eliminating unavoidable errors of independent separation of letters. The system has been tested on documents with 14 standard Persian fonts in 6 sizes. The achieved precision is 97.06%.      
### 66.Incremental user embedding modeling for personalized text classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.06369.pdf)
>  Individual user profiles and interaction histories play a significant role in providing customized experiences in real-world applications such as chatbots, social media, retail, and education. Adaptive user representation learning by utilizing user personalized information has become increasingly challenging due to ever-growing history data. In this work, we propose an incremental user embedding modeling approach, in which embeddings of user's recent interaction histories are dynamically integrated into the accumulated history vectors via a transformer encoder. This modeling paradigm allows us to create generalized user representations in a consecutive manner and also alleviate the challenges of data management. We demonstrate the effectiveness of this approach by applying it to a personalized multi-class classification task based on the Reddit dataset, and achieve 9% and 30% relative improvement on prediction accuracy over a baseline system for two experiment settings through appropriate comment history encoding and task modeling.      
### 67.Geometric deep learning reveals the spatiotemporal fingerprint of microscopic motion  [ :arrow_down: ](https://arxiv.org/pdf/2202.06355.pdf)
>  The characterization of dynamical processes in living systems provides important clues for their mechanistic interpretation and link to biological functions. Thanks to recent advances in microscopy techniques, it is now possible to routinely record the motion of cells, organelles, and individual molecules at multiple spatiotemporal scales in physiological conditions. However, the automated analysis of dynamics occurring in crowded and complex environments still lags behind the acquisition of microscopic image sequences. Here, we present a framework based on geometric deep learning that achieves the accurate estimation of dynamical properties in various biologically-relevant scenarios. This deep-learning approach relies on a graph neural network enhanced by attention-based components. By processing object features with geometric priors, the network is capable of performing multiple tasks, from linking coordinates into trajectories to inferring local and global dynamic properties. We demonstrate the flexibility and reliability of this approach by applying it to real and simulated data corresponding to a broad range of biological experiments.      
### 68.On the Exactness of an Energy-efficient Train Control model based on Convex Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2202.06303.pdf)
>  In this paper, we demonstrate the exactness proof for the energy-efficient train control (EETC) model based on convex optimization. The proof of exactness shows that the convex optimization model will share the same optimization results with the initial model on which the convex relaxations are conducted. We first show how the relaxation on the initial non-convex model is conducted and provide analysis to show that the relaxations are convex constraints and the relaxed model is thus a convex model. Subsequently, we prove that the relaxed convex model will always achieve its optimal solution on the initial equality constraints and the optimal solution achieved by convex optimization will be the same as the one obtained by the initial non-convex model and the relaxations applied are exact. A numerical verification has been conducted based on a typical urban rail system with a steep gradient. The results of this paper shed lights on further applications of convex optimization on energy-efficient train control and relevant areas related to operation and control of low-carbon transportation systems.      
### 69.Fundamental Performance of Integrated Sensing and Communications (ISAC) Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.06207.pdf)
>  This letter analyzes the fundamental performance of integrated sensing and communications (ISAC) systems. For downlink and uplink ISAC, the diversity orders are analyzed to evaluate the communication rate (CR) and the high signal-to-noise ratio (SNR) slopes are unveiled for the CR as well as the sensing rate (SR). Furthermore, the achievable downlink and uplink CR-SR regions are characterized. It is shown that ISAC can provide more degrees of freedom for both the CR and the SR than conventional frequency-division sensing and communications systems where isolated frequency bands are used for sensing and communications, respectively.      
### 70.Learning long-term music representations via hierarchical contextual constraints  [ :arrow_down: ](https://arxiv.org/pdf/2202.06180.pdf)
>  Learning symbolic music representations, especially disentangled representations with probabilistic interpretations, has been shown to benefit both music understanding and generation. However, most models are only applicable to short-term music, while learning long-term music representations remains a challenging task. We have seen several studies attempting to learn hierarchical representations directly in an end-to-end manner, but these models have not been able to achieve the desired results and the training process is not stable. In this paper, we propose a novel approach to learn long-term symbolic music representations through contextual constraints. First, we use contrastive learning to pre-train a long-term representation by constraining its difference from the short-term representation (extracted by an off-the-shelf model). Then, we fine-tune the long-term representation by a hierarchical prediction model such that a good long-term representation (e.g., an 8-bar representation) can reconstruct the corresponding short-term ones (e.g., the 2-bar representations within the 8-bar range). Experiments show that our method stabilizes the training and the fine-tuning steps. In addition, the designed contextual constraints benefit both reconstruction and disentanglement, significantly outperforming the baselines.      
### 71.Grasp Control of a Cable-Driven Robotic Hand Using a PVDF Slip Detection Sensor  [ :arrow_down: ](https://arxiv.org/pdf/2202.06140.pdf)
>  Detecting and preventing slip is a major challenge in robotic hand operation, underpinning the robot's ability to perform safe and reliable grasps. Using the robotic hand design from the authors' earlier work, a sensing and control strategy is proposed here to prevent object slippage. The robotic hand is cable-driven, single-actuated, has five fingers, and is capable of replicating most human hand motions. The slip sensing approach utilizes a piezoelectric vibration sensor, namely, polyvinylidene fluoride (PVDF), which is a flexible, thin, cheap, and highly sensitive material. The power of the filtered PVDF signal is shown to exhibit identifiable signatures during slip, thus providing a suitable slip detection mechanism. Using the PVDF feedback, an integral controller is implemented to prevent the grasped object from falling and ensure a safe, powerful, and reliable grasp. The extension movement of the robotic hand is controlled using a bend sensor, through a proportional-integral (PI) controller. The robotic hand weights 338 gr. The functionality and robustness of the proposed slip-detection sensory system and control logic implementation are evaluated through experiments.      
### 72.Learning by Doing: Controlling a Dynamical System using Causality, Control, and Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.06052.pdf)
>  Questions in causality, control, and reinforcement learning go beyond the classical machine learning task of prediction under i.i.d. observations. Instead, these fields consider the problem of learning how to actively perturb a system to achieve a certain effect on a response variable. Arguably, they have complementary views on the problem: In control, one usually aims to first identify the system by excitation strategies to then apply model-based design techniques to control the system. In (non-model-based) reinforcement learning, one directly optimizes a reward. In causality, one focus is on identifiability of causal structure. We believe that combining the different views might create synergies and this competition is meant as a first step toward such synergies. The participants had access to observational and (offline) interventional data generated by dynamical systems. Track CHEM considers an open-loop problem in which a single impulse at the beginning of the dynamics can be set, while Track ROBO considers a closed-loop problem in which control variables can be set at each time step. The goal in both tracks is to infer controls that drive the system to a desired state. Code is open-sourced ( <a class="link-external link-https" href="https://github.com/LearningByDoingCompetition/learningbydoing-comp" rel="external noopener nofollow">this https URL</a> ) to reproduce the winning solutions of the competition and to facilitate trying out new methods on the competition tasks.      
### 73.Hierarchical Aerial Computing for Internet of Things via Cooperation of HAPs and UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2202.06046.pdf)
>  With the explosive increment of computation requirements, the multi-access edge computing (MEC) paradigm appears as an effective mechanism. Besides, as for the Internet of Things (IoT) in disasters or remote areas requiring MEC services, unmanned aerial vehicles (UAVs) and high altitude platforms (HAPs) are available to provide aerial computing services for these IoT devices. In this paper, we develop the hierarchical aerial computing framework composed of HAPs and UAVs, to provide MEC services for various IoT applications. In particular, the problem is formulated to maximize the total IoT data computed by the aerial MEC platforms, restricted by the delay requirement of IoT and multiple resource constraints of UAVs and HAPs, which is an integer programming problem and intractable to solve. Due to the prohibitive complexity of exhaustive search, we handle the problem by presenting the matching game theory based algorithm to deal with the offloading decisions from IoT devices to UAVs, as well as a heuristic algorithm for the offloading decisions between UAVs and HAPs. The external effect affected by interplay of different IoT devices in the matching is tackled by the externality elimination mechanism. Besides, an adjustment algorithm is also proposed to make the best of aerial resources. The complexity of proposed algorithms is analyzed and extensive simulation results verify the efficiency of the proposed algorithms, and the system performances are also analyzed by the numerical results.      
### 74.USTED: Improving ASR with a Unified Speech and Text Encoder-Decoder  [ :arrow_down: ](https://arxiv.org/pdf/2202.06045.pdf)
>  Improving end-to-end speech recognition by incorporating external text data has been a longstanding research topic. There has been a recent focus on training E2E ASR models that get the performance benefits of external text data without incurring the extra cost of evaluating an external language model at inference time. In this work, we propose training ASR model jointly with a set of text-to-text auxiliary tasks with which it shares a decoder and parts of the encoder. When we jointly train ASR and masked language model with the 960-hour Librispeech and Opensubtitles data respectively, we observe WER reductions of 16% and 20% on test-other and test-clean respectively over an ASR-only baseline without any extra cost at inference time, and reductions of 6% and 8% compared to a stronger MUTE-L baseline which trains the decoder with the same text data as our model. We achieve further improvements when we train masked language model on Librispeech data or when we use machine translation as the auxiliary task, without significantly sacrificing performance on the task itself.      
### 75.Deep Performer: Score-to-Audio Music Performance Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2202.06034.pdf)
>  Music performance synthesis aims to synthesize a musical score into a natural performance. In this paper, we borrow recent advances in text-to-speech synthesis and present the Deep Performer -- a novel system for score-to-audio music performance synthesis. Unlike speech, music often contains polyphony and long notes. Hence, we propose two new techniques for handling polyphonic inputs and providing a fine-grained conditioning in a transformer encoder-decoder model. To train our proposed system, we present a new violin dataset consisting of paired recordings and scores along with estimated alignments between them. We show that our proposed model can synthesize music with clear polyphony and harmonic structures. In a listening test, we achieve competitive quality against the baseline model, a conditional generative audio model, in terms of pitch accuracy, timbre and noise level. Moreover, our proposed model significantly outperforms the baseline on an existing piano dataset in overall quality.      
### 76.Cloud-based computational model predictive control using a parallel multi-block ADMM approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.06012.pdf)
>  Heavy computational load for solving nonconvex problems for large-scale systems or systems with real-time demands at each sample step has been recognized as one of the reasons for preventing a wider application of nonlinear model predictive control (NMPC). To improve the real-time feasibility of NMPC with input nonlinearity, we devise an innovative scheme called cloud-based computational model predictive control (MPC) by using an elaborately designed parallel multi-block alternating direction method of multipliers (ADMM) algorithm. This novel parallel multi-block ADMM algorithm is tailored to tackle the computational issue of solving a nonconvex problem with nonlinear constraints.      
### 77.Wav2Vec2.0 on the Edge: Performance Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2202.05993.pdf)
>  Wav2Vec2.0 is a state-of-the-art model which learns speech representations through unlabeled speech data, aka, self supervised learning. The pretrained model is then fine tuned on small amounts of labeled data to use it for speech-to-text and machine translation tasks. Wav2Vec 2.0 is a transformative solution for low resource languages as it is mainly developed using unlabeled audio data. Getting large amounts of labeled data is resource intensive and especially challenging to do for low resource languages such as Swahilli, Tatar, etc. Furthermore, Wav2Vec2.0 word-error-rate(WER) matches or surpasses the very recent supervised learning algorithms while using 100x less labeled data. Given its importance and enormous potential in enabling speech based tasks on world's 7000 languages, it is key to evaluate the accuracy, latency and efficiency of this model on low resource and low power edge devices and investigate the feasibility of using it in such devices for private, secure and reliable speech based tasks. On-device speech tasks preclude sending audio data to the server hence inherently providing privacy, reduced latency and enhanced reliability. In this paper, Wav2Vec2.0 model's accuracy and latency has been evaluated on Raspberry Pi along with the KenLM language model for speech recognition tasks. How to tune certain parameters to achieve desired level of WER rate and latency while meeting the CPU, memory and energy budgets of the product has been discussed.      
### 78.RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework  [ :arrow_down: ](https://arxiv.org/pdf/2202.05988.pdf)
>  We tackle the problem of image inpainting in the remote sensing domain. Remote sensing images possess high resolution and geographical variations, that render the conventional inpainting methods less effective. This further entails the requirement of models with high complexity to sufficiently capture the spectral, spatial and textural nuances within an image, emerging from its high spatial variability. To this end, we propose a novel inpainting method that individually focuses on each aspect of an image such as edges, colour and texture using a task specific GAN. Moreover, each individual GAN also incorporates the attention mechanism that explicitly extracts the spectral and spatial features. To ensure consistent gradient flow, the model uses residual learning paradigm, thus simultaneously working with high and low level features. We evaluate our model, alongwith previous state of the art models, on the two well known remote sensing datasets, Open Cities AI and Earth on Canvas, and achieve competitive performance.      
### 79.On the Impacts of Phase Shifting Design and Eavesdropping Uncertainty on Secrecy Metrics of RIS-aided Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.05979.pdf)
>  This paper investigates the secrecy outage probability (SOP), the lower bound of SOP, and the probability of non-zero secrecy capacity (PNZ) of reconfigurable intelligent surface (RIS)-assisted systems from an information-theoretic perspective. In particular, we consider the impacts of eavesdroppers' location uncertainty and the phase adjustment uncertainty, namely imperfect coherent phase shifting and discrete phase shifting on RIS. More specifically, analytical and simulation results are presented to show that (i) the SOP gain due to the increase of the RIS reflecting elements number gradually decreases; and (ii) both phase shifting designs demonstrate the same PNZ secrecy performance, in other words, the random discrete phase shifting outperforms the imperfect coherent phase shifting design with reduced complexity.      
### 80.Low-light Image Enhancement by Retinex Based Algorithm Unrolling and Adjustment  [ :arrow_down: ](https://arxiv.org/pdf/2202.05972.pdf)
>  Motivated by their recent advances, deep learning techniques have been widely applied to low-light image enhancement (LIE) problem. Among which, Retinex theory based ones, mostly following a decomposition-adjustment pipeline, have taken an important place due to its physical interpretation and promising performance. However, current investigations on Retinex based deep learning are still not sufficient, ignoring many useful experiences from traditional methods. Besides, the adjustment step is either performed with simple image processing techniques, or by complicated networks, both of which are unsatisfactory in practice. To address these issues, we propose a new deep learning framework for the LIE problem. The proposed framework contains a decomposition network inspired by algorithm unrolling, and adjustment networks considering both global brightness and local brightness sensitivity. By virtue of algorithm unrolling, both implicit priors learned from data and explicit priors borrowed from traditional methods can be embedded in the network, facilitate to better decomposition. Meanwhile, the consideration of global and local brightness can guide designing simple yet effective network modules for adjustment. Besides, to avoid manually parameter tuning, we also propose a self-supervised fine-tuning strategy, which can always guarantee a promising performance. Experiments on a series of typical LIE datasets demonstrated the effectiveness of the proposed method, both quantitatively and visually, as compared with existing methods.      
### 81.Capacity Maximization Pattern Design for Reconfigurable MIMO Antenna Array  [ :arrow_down: ](https://arxiv.org/pdf/2202.05965.pdf)
>  Multi-functional and reconfigurable multiple-input multiple-output (MR-MIMO) can provide performance gains over traditional MIMO by introducing additional degrees of freedom. In this paper, we focus on the capacity maximization pattern design for MR-MIMO systems. Firstly, we introduce the matrix representation of MR-MIMO, based on which a pattern design problem is formulated. To further reveal the effect of the radiation pattern on the wireless channel, we consider pattern design for both the single-pattern case where the optimized radiation pattern is the same for all the antenna elements, and the multi-pattern case where different antenna elements can adopt different radiation patterns. For the single-pattern case, we show that the pattern design is equivalent to a redistribution of power among all scattering paths, and an eigenvalue optimization based solution is obtained. For the multi-pattern case, we propose a sequential optimization framework with manifold optimization and eigenvalue decomposition to obtain near-optimal solutions. Numerical results validate the superiority of MR-MIMO systems over traditional MIMO in terms of capacity, and also show the effectiveness of the proposed solutions.      
### 82.Audio-Visual Fusion Layers for Event Type Aware Video Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.05961.pdf)
>  Human brain is continuously inundated with the multisensory information and their complex interactions coming from the outside world at any given moment. Such information is automatically analyzed by binding or segregating in our brain. While this task might seem effortless for human brains, it is extremely challenging to build a machine that can perform similar tasks since complex interactions cannot be dealt with single type of integration but requires more sophisticated approaches. In this paper, we propose a new model to address the multisensory integration problem with individual event-specific layers in a multi-task learning scheme. Unlike previous works where single type of fusion is used, we design event-specific layers to deal with different audio-visual relationship tasks, enabling different ways of audio-visual formation. Experimental results show that our event-specific layers can discover unique properties of the audio-visual relationships in the videos. Moreover, although our network is formulated with single labels, it can output additional true multi-labels to represent the given videos. We demonstrate that our proposed framework also exposes the modality bias of the video data category-wise and dataset-wise manner in popular benchmark datasets.      
### 83.Channel Modelling and Error Performance Investigation for Reading Lights Based In-flight LiFi  [ :arrow_down: ](https://arxiv.org/pdf/2202.05911.pdf)
>  The new generation of communication technologies are constantly being pushed to meet a diverse range of user requirements such as high data rate, low power consumption, very low latency, very high reliability and broad availability. To address all these demands, 5G radio access technologies have been extended into a wide range of new services. However, there are still only a limited number of applications for RF based wireless communications inside aircraft cabins that comply with the 5G vision. Potential interference and safety issues in on-board wireless communications pose significant deployment challenges. By transforming each reading light into an optical wireless AP, LiFi, could provide seamless on-board connectivity in dense cabin environments without RF interference. Furthermore, the utilization of available reading lights allows for a relatively simple, cost-effective deployment with the high energy and spectral efficiency. To successfully implement the aeronautical cabin LiFi applications, comprehensive optical channel characterization is required. In this paper, we propose a novel MCRT channel modelling technique to capture the details of in-flight LiFi links. Accordingly, a realistic channel simulator, which takes the cabin models, interior elements and measurement based optical source, receiver, surface material characteristics into account is developed. The effect of the operation wavelength, cabin model accuracy and user terminal mobility on the optical channel conditions is also investigated. As a final step, the on-board DCO-OFDM performance is evaluated by using obtained in-flight LiFi channels. Numerical results show that the location of a mobile terminal and accurate aircraft cabin modelling yield as much as 12 and 2 dB performance difference, respectively.      
### 84.Towards the Maximum Traffic Demand and Throughput Supported by Relay-Assisted mmWave Backhaul Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.05908.pdf)
>  This paper investigates the throughput performance issue of the relay-assisted mmWave backhaul network. The maximum traffic demand of small-cell base stations (BSs) and the maximum throughput at the macro-cell BS have been found in a tree-style backhaul network through linear programming under different network settings, which concern both the number of radio chains available on BSs and the interference relationship between logical links in the backhaul network. A novel interference model for the relay-assisted mmWave backhaul network in the dense urban environment is proposed, which demonstrates the limited interference footprint of mmWave directional communications. Moreover, a scheduling algorithm is developed to find the optimal scheduling for tree-style mmWave backhaul networks. Extensive numerical analysis and simulations are conducted to show and validate the network throughput performance and the scheduling algorithm.      
### 85.Uncertainty Aware System Identification with Universal Policies  [ :arrow_down: ](https://arxiv.org/pdf/2202.05844.pdf)
>  Sim2real transfer is primarily concerned with transferring policies trained in simulation to potentially noisy real world environments. A common problem associated with sim2real transfer is estimating the real-world environmental parameters to ground the simulated environment to. Although existing methods such as Domain Randomisation (DR) can produce robust policies by sampling from a distribution of parameters during training, there is no established method for identifying the parameters of the corresponding distribution for a given real-world setting. In this work, we propose Uncertainty-aware policy search (UncAPS), where we use Universal Policy Network (UPN) to store simulation-trained task-specific policies across the full range of environmental parameters and then subsequently employ robust Bayesian optimisation to craft robust policies for the given environment by combining relevant UPN policies in a DR like fashion. Such policy-driven grounding is expected to be more efficient as it estimates only task-relevant sets of parameters. Further, we also account for the estimation uncertainties in the search process to produce policies that are robust against both aleatoric and epistemic uncertainties. We empirically evaluate our approach in a range of noisy, continuous control environments, and show its improved performance compared to competing baselines.      
### 86.Fast Model-based Policy Search for Universal Policy Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.05843.pdf)
>  Adapting an agent's behaviour to new environments has been one of the primary focus areas of physics based reinforcement learning. Although recent approaches such as universal policy networks partially address this issue by enabling the storage of multiple policies trained in simulation on a wide range of dynamic/latent factors, efficiently identifying the most appropriate policy for a given environment remains a challenge. In this work, we propose a Gaussian Process-based prior learned in simulation, that captures the likely performance of a policy when transferred to a previously unseen environment. We integrate this prior with a Bayesian Optimisation-based policy search process to improve the efficiency of identifying the most appropriate policy from the universal policy network. We empirically evaluate our approach in a range of continuous and discrete control environments, and show that it outperforms other competing baselines.      
