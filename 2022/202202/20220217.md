# ArXiv eess --Thu, 17 Feb 2022
### 1.A multi-reconstruction study of breast density estimation using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.08238.pdf)
>  Breast density estimation is one of the key tasks in recognizing individuals predisposed to breast cancer. It is often challenging because of low contrast and fluctuations in mammograms' fatty tissue background. Most of the time, the breast density is estimated manually where a radiologist assigns one of the four density categories decided by the Breast Imaging and Reporting Data Systems (BI-RADS). There have been efforts in the direction of automating a breast density classification pipeline. <br>Breast density estimation is one of the key tasks performed during a screening exam. Dense breasts are more susceptible to breast cancer. The density estimation is challenging because of low contrast and fluctuations in mammograms' fatty tissue background. Traditional mammograms are being replaced by tomosynthesis and its other low radiation dose variants (for example Hologic' Intelligent 2D and C-View). Because of the low-dose requirement, increasingly more screening centers are favoring the Intelligent 2D view and C-View. Deep-learning studies for breast density estimation use only a single modality for training a neural network. However, doing so restricts the number of images in the dataset. In this paper, we show that a neural network trained on all the modalities at once performs better than a neural network trained on any single modality. We discuss these results using the area under the receiver operator characteristics curves.      
### 2.Automatic Depression Detection: An Emotional Audio-Textual Corpus and a GRU/BiLSTM-based Model  [ :arrow_down: ](https://arxiv.org/pdf/2202.08210.pdf)
>  Depression is a global mental health problem, the worst case of which can lead to suicide. An automatic depression detection system provides great help in facilitating depression self-assessment and improving diagnostic accuracy. In this work, we propose a novel depression detection approach utilizing speech characteristics and linguistic contents from participants' interviews. In addition, we establish an Emotional Audio-Textual Depression Corpus (EATD-Corpus) which contains audios and extracted transcripts of responses from depressed and non-depressed volunteers. To the best of our knowledge, EATD-Corpus is the first and only public depression dataset that contains audio and text data in Chinese. Evaluated on two depression datasets, the proposed method achieves the state-of-the-art performances. The outperforming results demonstrate the effectiveness and generalization ability of the proposed method. The source code and EATD-Corpus are available at <a class="link-external link-https" href="https://github.com/speechandlanguageprocessing/ICASSP2022-Depression" rel="external noopener nofollow">this https URL</a>.      
### 3.Modelling multi-cell edge video analytics  [ :arrow_down: ](https://arxiv.org/pdf/2202.08200.pdf)
>  Edge intelligence is a scalable solution for analyzing distributed data, but it cannot provide reliable services in large-scale cellular networks unless the inherent aspects of fading and interference are also taken into consideration. In this paper, we present the first mathematical framework for modelling edge video analytics in multi-cell cellular systems. We derive the expressions for the coverage probability, the ergodic capacity, the probability of successfully completing the video analytics within a target delay requirement, and the effective frame rate. We also analyze the effect of the system parameters on the accuracy of the detection algorithm, the supported frame rate at the edge server, and the system fairness.      
### 4.Label Propagation for Annotation-Efficient Nuclei Segmentation from Pathology Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.08195.pdf)
>  Nuclei segmentation is a crucial task for whole slide image analysis in digital pathology. Generally, the segmentation performance of fully-supervised learning heavily depends on the amount and quality of the annotated data. However, it is time-consuming and expensive for professional pathologists to provide accurate pixel-level ground truth, while it is much easier to get coarse labels such as point annotations. In this paper, we propose a weakly-supervised learning method for nuclei segmentation that only requires point annotations for training. The proposed method achieves label propagation in a coarse-to-fine manner as follows. First, coarse pixel-level labels are derived from the point annotations based on the Voronoi diagram and the k-means clustering method to avoid overfitting. Second, a co-training strategy with an exponential moving average method is designed to refine the incomplete supervision of the coarse labels. Third, a self-supervised visual representation learning method is tailored for nuclei segmentation of pathology images that transforms the hematoxylin component images into the H\&amp;E stained images to gain better understanding of the relationship between the nuclei and cytoplasm. We comprehensively evaluate the proposed method using two public datasets. Both visual and quantitative results demonstrate the superiority of our method to the state-of-the-art methods, and its competitive performance compared to the fully-supervised methods. The source codes for implementing the experiments will be released after acceptance.      
### 5.Increasing loudness in audio signals: a perceptually motivated approach to preserve audio quality  [ :arrow_down: ](https://arxiv.org/pdf/2202.08183.pdf)
>  We present a method to maintain the subjective perception of volume of audio signals and, at the same time, reduce their absolute peak value. We focus on achieving this without compromising the perceived audio quality. This is specially useful, for example, to maximize the perceived reproduction level of loudspeakers where simply amplifying the signal amplitude, and hence their peak value, is limited due to already constrained physical designs. In particular, we minimize the absolute peak value subject to a constraint based on auditory masking. This limits the perceptual difference between the original and the modified signals. Moreover, this constraint can be tuned and allows to control the resulting audio quality. We show results comparing loudness and audio quality as a function of peak reduction. These results suggest that our method presents the best trade-off between loudness and audio quality when compared against classical methods based on compression and clipping.      
### 6.Voice Filter: Few-shot text-to-speech speaker adaptation using voice conversion as a post-processing module  [ :arrow_down: ](https://arxiv.org/pdf/2202.08164.pdf)
>  State-of-the-art text-to-speech (TTS) systems require several hours of recorded speech data to generate high-quality synthetic speech. When using reduced amounts of training data, standard TTS models suffer from speech quality and intelligibility degradations, making training low-resource TTS systems problematic. In this paper, we propose a novel extremely low-resource TTS method called Voice Filter that uses as little as one minute of speech from a target speaker. It uses voice conversion (VC) as a post-processing module appended to a pre-existing high-quality TTS system and marks a conceptual shift in the existing TTS paradigm, framing the few-shot TTS problem as a VC task. Furthermore, we propose to use a duration-controllable TTS system to create a parallel speech corpus to facilitate the VC task. Results show that the Voice Filter outperforms state-of-the-art few-shot speech synthesis techniques in terms of objective and subjective metrics on one minute of speech on a diverse set of voices, while being competitive against a TTS model built on 30 times more data.      
### 7.An alternative paradigm of fault diagnosis in dynamic systems: orthogonal projection-based methods  [ :arrow_down: ](https://arxiv.org/pdf/2202.08108.pdf)
>  In this paper, we propose a new paradigm of fault diagnosis in dynamic systems as an alternative to the well-established observer-based framework. The basic idea behind this work is to (i) formulate fault detection and isolation as projection of measurement signals onto (system) subspaces in Hilbert space, and (ii) solve the resulting problems by means of projection methods with orthogonal projection operators and gap metric as major tools. In the new framework, fault diagnosis issues are uniformly addressed both in the model-based and data-driven fashions. Moreover, the design and implementation of the projection-based fault diagnosis systems, from residual generation to threshold setting, can be unifiedly handled. Thanks to the well-defined distance metric for projections in Hilbert subspaces, the projection-based fault diagnosis systems deliver optimal fault detectability. In particular, a new type of residual-driven thresholds is proposed, which significantly increases the fault detectability. In this work, various design schemes are proposed, including a basic projection-based fault detection scheme, a fault detection scheme for feedback control systems, fault classification as well as two modified fault detection schemes. As a part of our study, relations to the existing observer-based fault detection systems are investigated, which showcases that, with comparable online computations, the proposed projection-based detection methods offer improved detection performance.      
### 8.Formulating Beurling LASSO for Source Separation via Proximal Gradient Iteration  [ :arrow_down: ](https://arxiv.org/pdf/2202.08082.pdf)
>  Beurling LASSO generalizes the LASSO problem to finite Radon measures regularized via their total variation. Despite its theoretical appeal, this space is hard to parametrize, which poses an algorithmic challenge. We propose a formulation of continuous convolutional source separation with Beurling LASSO that avoids the explicit computation of the measures and instead employs the duality transform of the proximal mapping.      
### 9.Graph Neural Network and Koopman Models for Learning Networked Dynamics: A Comparative Study on Power Grid Transients Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2202.08065.pdf)
>  Continuous monitoring of the spatio-temporal dynamic behavior of critical infrastructure networks, such as the power systems, is a challenging but important task. In particular, accurate and timely prediction of the (electro-mechanical) transient dynamic trajectories of the power grid is necessary for early detection of any instability and prevention of catastrophic failures. Existing approaches for the prediction of dynamic trajectories either rely on the availability of accurate physical models of the system, use computationally expensive time-domain simulations, or are applicable only at local prediction problems (e.g., a single generator). In this paper, we report the application of two broad classes of data-driven learning models -- along with their algorithmic implementation and performance evaluation -- in predicting transient trajectories in power networks using only streaming measurements and the network topology as input. One class of models is based on the Koopman operator theory which allows for capturing the nonlinear dynamic behavior via an infinite-dimensional linear operator. The other class of models is based on the graph convolutional neural networks which are adept at capturing the inherent spatio-temporal correlations within the power network. Transient dynamic datasets for training and testing the models are synthesized by simulating a wide variety of load change events in the IEEE 68-bus system, categorized by the load change magnitudes, as well as by the degree of connectivity and the distance to nearest generator nodes. The results confirm that the proposed predictive models can successfully predict the post-disturbance transient evolution of the system with a high level of accuracy.      
### 10.Image translation of Ultrasound to Pseudo Anatomical Display Using Artificial Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2202.08053.pdf)
>  Ultrasound is the second most used modality in medical imaging. It is cost effective, hazardless, portable and implemented routinely in numerous clinical procedures. Nonetheless, image quality is characterized by granulated appearance, poor SNR and speckle noise. Specific for malignant tumors, the margins are blurred and indistinct. Thus, there is a great need for improving ultrasound image quality. We hypothesize that this can be achieved by translation into a more realistic anatomic display, using neural networks. In order to achieve this goal, the preferable approach would be to use a set of paired images. However, this is practically impossible in our case. Therefore, CycleGAN was used, to learn each domain properties separately and enforce cross domain cycle consistency. The two datasets which were used for training the model were "Breast Ultrasound Images" (BUSI) and a set of optic images of poultry breast tissue samples acquired at our lab. The generated pseudo anatomical images provide improved visual discrimination of the lesions with clearer border definition and pronounced contrast. Furthermore, the algorithm manages to overcome the acoustic shadows artifacts commonly appearing in ultrasonic images. In order to evaluate the preservation of the anatomical features, the lesions in the ultrasonic images and the generated pseudo anatomical images were both automatically segmented and compared. This comparison yielded median dice score of 0.78 for the benign tumors and 0.43 for the malignancies. Median lesion center error of 2.38% and 8.42% for the benign and malignancies respectively and median area error index of 0.77% and 5.06% for the benign and malignancies respectively. In conclusion, these generated pseudo anatomical images, which are presented in a more intuitive way, preserve tissue anatomy and can potentially simplify the diagnosis and improve the clinical outcome.      
### 11.A BiLSTM-CNN based Multitask Learning Approach for Fiber Fault Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2202.08034.pdf)
>  A novel multitask learning approach based on stacked bidirectional long short-term memory (BiLSTM) networks and convolutional neural networks (CNN) for detecting, locating, characterizing, and identifying fiber faults is proposed. It outperforms conventionally employed techniques.      
### 12.APPLADE: Adjustable Plug-and-play Audio Declipper Combining DNN with Sparse Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2202.08028.pdf)
>  In this paper, we propose an audio declipping method that takes advantages of both sparse optimization and deep learning. Since sparsity-based audio declipping methods have been developed upon constrained optimization, they are adjustable and well-studied in theory. However, they always uniformly promote sparsity and ignore the individual properties of a signal. Deep neural network (DNN)-based methods can learn the properties of target signals and use them for audio declipping. Still, they cannot perform well if the training data have mismatches and/or constraints in the time domain are not imposed. In the proposed method, we use a DNN in an optimization algorithm. It is inspired by an idea called plug-and-play (PnP) and enables us to promote sparsity based on the learned information of data, considering constraints in the time domain. Our experiments confirmed that the proposed method is stable and robust to mismatches between training and test data.      
### 13.Frequency Response from Aggregated V2G Chargers With Uncertain EV Connections  [ :arrow_down: ](https://arxiv.org/pdf/2202.08026.pdf)
>  Fast frequency response (FR) is highly effective at securing frequency dynamics after a generator outage in low inertia systems. Electric vehicles (EVs) equipped with vehicle to grid (V2G) chargers could offer an abundant source of FR in future. However, the uncertainty associated with V2G aggregation, driven by the uncertain number of connected EVs at the time of an outage, has not been fully understood and prevents its participation in the existing service provision framework. To tackle this limitation, this paper, for the first time, incorporates such uncertainty into system frequency dynamics, from which probabilistic nadir and steady state frequency requirements are enforced via a derived momeent-based distributionally-robust chance constraint. Field data from over 25,000 chargers is analysed to provide realistic parameters and connection forecasts to examine the value of FR from V2G chargers in annual operation of the the GB 2030 system. The case study demonstrates that uncertainty of EV connections can be effectively managed through the proposed scheduling framework, which results in annual savings of Â£6,300 or 37.4 tCO2 per charger. The sensitivity of this value to renewable capacity and FR delays is explored, with V2G capacity shown to be a third as valuable as the same grid battery capacity.      
### 14.Data-Driven Control of Event- and Self-Triggered Discrete-Time Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.08019.pdf)
>  The present paper considers the model-based and data-driven control of unknown linear time-invariant discrete-time systems under event-triggering and self-triggering transmission schemes. To this end, we begin by presenting a dynamic event-triggering scheme (ETS) based on periodic sampling, and a discrete-time looped-functional (DLF) approach, through which a model-based stability condition is derived. Combining the model-based condition with a recent data-based system representation, a data-driven stability criterion in the form of linear matrix inequalities (LMIs) is established, which offers a way of co-designing the ETS matrix and the controller. To further alleviate the sampling burden of ETS due to its continuous/periodic detection, a self-triggering scheme (STS) is developed. Leveraging pre-collected input-state data, an algorithm for predicting the next transmission instant is given, while achieving system stability. A co-design method of the ETS matrix and the controller under the proposed data-driven STS is given. Finally, numerical simulations showcase the efficacy of ETS and STS in reducing data transmissions as well as of the proposed co-design methods.      
### 15.DeepTx: Deep Learning Beamforming with Channel Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2202.07998.pdf)
>  Machine learning algorithms have recently been considered for many tasks in the field of wireless communications. Previously, we have proposed the use of a deep fully convolutional neural network (CNN) for receiver processing and shown it to provide considerable performance gains. In this study, we focus on machine learning algorithms for the transmitter. In particular, we consider beamforming and propose a CNN which, for a given uplink channel estimate as input, outputs downlink channel information to be used for beamforming. The CNN is trained in a supervised manner considering both uplink and downlink transmissions with a loss function that is based on UE receiver performance. The main task of the neural network is to predict the channel evolution between uplink and downlink slots, but it can also learn to handle inefficiencies and errors in the whole chain, including the actual beamforming phase. The provided numerical experiments demonstrate the improved beamforming performance.      
### 16.ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.07983.pdf)
>  Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance as the vision loss caused by AMD is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. \textcolor{red}{Recently, some algorithms based on deep learning had been developed for fundus image analysis and automatic AMD detection. However, a comprehensive annotated dataset and a standard evaluation benchmark are still missing.} To deal with this issue, we set up the Automatic Detection challenge on Age-related Macular degeneration (ADAM) for the first time, held as a satellite event of the ISBI 2020 conference. The ADAM challenge consisted of four tasks which cover the main topics in detecting AMD from fundus images, including classification of AMD, detection and segmentation of optic disc, localization of fovea, and detection and segmentation of lesions. The ADAM challenge has released a comprehensive dataset of 1200 fundus images with the category labels of AMD, the pixel-wise segmentation masks of the full optic disc and lesions (drusen, exudate, hemorrhage, scar, and other), as well as the location coordinates of the macular fovea. A uniform evaluation framework has been built to make a fair comparison of different models. During the ADAM challenge, 610 results were submitted for online evaluation, and finally, 11 teams participated in the onsite challenge. This paper introduces the challenge, dataset, and evaluation methods, as well as summarizes the methods and analyzes the results of the participating teams of each task. In particular, we observed that ensembling strategy and clinical prior knowledge can better improve the performances of the deep learning models.      
### 17.Separation and Estimation of Periodic/Aperiodic State  [ :arrow_down: ](https://arxiv.org/pdf/2202.07937.pdf)
>  Periodicity and aperiodicity can exist in a state simultaneously and typically become quasi-periodicity and quasi-aperiodicity in a dynamically changing state. The quasi-periodic and quasi-aperiodic states existing in the periodic/aperiodic state mostly correspond to different phenomena and require different controls. For separation control of these states, this paper defines the periodic/aperiodic, quasi-periodic, and quasi-aperiodic states to construct a periodic/aperiodic separation filter that separates the periodic/aperiodic state into the quasi-periodic and quasi-aperiodic states. Based on these definitions, the linearity of periodic-pass and aperiodic-pass functions and the orthogonality of quasi-periodic and quasi-aperiodic state functions are proved. Subsequently, the periodic/aperiodic separation filter composed of periodic-pass and aperiodic-pass filters that realize the periodic-pass and aperiodic-pass functions is designed and integrated with a Kalman filter for estimation of the quasi-periodic and quasi-aperiodic states.      
### 18.Application of Long Short-Term Memory Recurrent Neural Networks Based on the BAT-MCS for Binary-State Network Approximated Time-Dependent Reliability Problems  [ :arrow_down: ](https://arxiv.org/pdf/2202.07837.pdf)
>  Reliability is an important tool for evaluating the performance of modern networks. Currently, it is NP-hard and #P-hard to calculate the exact reliability of a binary-state network when the reliability of each component is assumed to be fixed. However, this assumption is unrealistic because the reliability of each component always varies with time. To meet this practical requirement, we propose a new algorithm called the LSTM-BAT-MCS, based on long short-term memory (LSTM), the Monte Carlo simulation (MCS), and the binary-adaption-tree algorithm (BAT). The superiority of the proposed LSTM-BAT-MCS was demonstrated by experimental results of three benchmark networks with at most 10-4 mean square error.      
### 19.A Survey of Semen Quality Evaluation in Microscopic Videos Using Computer Assisted Sperm Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.07820.pdf)
>  The Computer Assisted Sperm Analysis (CASA) plays a crucial role in male reproductive health diagnosis and Infertility treatment. With the development of the computer industry in recent years, a great of accurate algorithms are proposed. With the assistance of those novel algorithms, it is possible for CASA to achieve a faster and higher quality result. Since image processing is the technical basis of CASA, including pre-processing,feature extraction, target detection and tracking, these methods are important technical steps in dealing with CASA. The various works related to Computer Assisted Sperm Analysis methods in the last 30 years (since 1988) are comprehensively introduced and analysed in this survey. To facilitate understanding, the methods involved are analysed in the sequence of general steps in sperm analysis. In other words, the methods related to sperm detection (localization) are first analysed, and then the methods of sperm tracking are analysed. Beside this, we analyse and prospect the present situation and future of CASA. According to our work, the feasible for applying in sperm microscopic video of methods mentioned in this review is explained. Moreover, existing challenges of object detection and tracking in microscope video are potential to be solved inspired by this survey.      
### 20.ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2202.07816.pdf)
>  Expressive text-to-speech (TTS) has become a hot research topic recently, mainly focusing on modeling prosody in speech. Prosody modeling has several challenges: 1) the extracted pitch used in previous prosody modeling works have inevitable errors, which hurts the prosody modeling; 2) different attributes of prosody (e.g., pitch, duration and energy) are dependent on each other and produce the natural prosody together; and 3) due to high variability of prosody and the limited amount of high-quality data for TTS training, the distribution of prosody cannot be fully shaped. To tackle these issues, we propose ProsoSpeech, which enhances the prosody using quantized latent vectors pre-trained on large-scale unpaired and low-quality text and speech data. Specifically, we first introduce a word-level prosody encoder, which quantizes the low-frequency band of the speech and compresses prosody attributes in the latent prosody vector (LPV). Then we introduce an LPV predictor, which predicts LPV given word sequence. We pre-train the LPV predictor on large-scale text and low-quality speech data and fine-tune it on the high-quality TTS dataset. Finally, our model can generate expressive speech conditioned on the predicted LPV. Experimental results show that ProsoSpeech can generate speech with richer prosody compared with baseline methods.      
### 21.Low Latency Real-Time Seizure Detection Using Transfer Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.07796.pdf)
>  Scalp electroencephalogram (EEG) signals inherently have a low signal-to-noise ratio due to the way the signal is electrically transduced. Temporal and spatial information must be exploited to achieve accurate detection of seizure events. Most popular approaches to seizure detection using deep learning do not jointly model this information or require multiple passes over the signal, which makes the systems inherently non-causal. In this paper, we exploit both simultaneously by converting the multichannel signal to a grayscale image and using transfer learning to achieve high performance. The proposed system is trained end-to-end with only very simple pre- and postprocessing operations which are computationally lightweight and have low latency, making them conducive to clinical applications that require real-time processing. We have achieved a performance of 42.05% sensitivity with 5.78 false alarms per 24 hours on the development dataset of v1.5.2 of the Temple University Hospital Seizure Detection Corpus. On a single-core CPU operating at 1.7 GHz, the system runs faster than real-time (0.58 xRT), uses 16 Gbytes of memory, and has a latency of 300 msec.      
### 22.Direction of Arrival Estimation and Phase-Correction for Non-Coherent Sub-Arrays: A Convex Optimization Approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.07781.pdf)
>  Estimating the direction of arrival (DOA) of sources is an important problem in aerospace and vehicular communication, localization and radar. In this paper, we consider a challenging multi-source DOA estimation task, where the receiving antenna array is composed of non-coherent sub-arrays, i.e., sub-arrays that observe different unknown phase shifts at every snapshot (e.g., due to waiving the demanding synchronization of local oscillators across the entire array). We formulate this problem as the reconstruction of joint sparse and low-rank matrices, and solve the problem's convex relaxation. To scale the optimization complexity with the number of snapshots better than general-purpose solvers, we design an optimization scheme, based on integrating the alternating direction method of multipliers and the accelerated proximal gradient techniques, that exploits the structure of the problem. While the DOAs can be estimated from the solution of the aforementioned convex problem, we further show how an improvement is obtained if, instead, one estimates from this solution only the sub-arrays' phase shifts. This is done using another, computationally-light, convex relaxation that is practically tight. Using the estimated phase shifts, "phase-corrected" observations are created and a final plain ("coherent") DOA estimation method can be applied. Numerical experiments show the performance advantages of the proposed strategies over existing methods.      
### 23.Deep Learning-Assisted Co-registration of Full-Spectral Autofluorescence Lifetime Microscopic Images with H&amp;E-Stained Histology Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.07755.pdf)
>  Autofluorescence lifetime images reveal unique characteristics of endogenous fluorescence in biological samples. Comprehensive understanding and clinical diagnosis rely on co-registration with the gold standard, histology images, which is extremely challenging due to the difference of both images. Here, we show an unsupervised image-to-image translation network that significantly improves the success of the co-registration using a conventional optimisation-based regression network, applicable to autofluorescence lifetime images at different emission wavelengths. A preliminary blind comparison by experienced researchers shows the superiority of our method on co-registration. The results also indicate that the approach is applicable to various image formats, like fluorescence intensity images. With the registration, stitching outcomes illustrate the distinct differences of the spectral lifetime across an unstained tissue, enabling macro-level rapid visual identification of lung cancer and cellular-level characterisation of cell variants and common types. The approach could be effortlessly extended to lifetime images beyond this range and other staining technologies.      
### 24.Nonverbal Sound Detection for Disordered Speech  [ :arrow_down: ](https://arxiv.org/pdf/2202.07750.pdf)
>  Voice assistants have become an essential tool for people with various disabilities because they enable complex phone- or tablet-based interactions without the need for fine-grained motor control, such as with touchscreens. However, these systems are not tuned for the unique characteristics of individuals with speech disorders, including many of those who have a motor-speech disorder, are deaf or hard of hearing, have a severe stutter, or are minimally verbal. We introduce an alternative voice-based input system which relies on sound event detection using fifteen nonverbal mouth sounds like "pop," "click," or "eh." This system was designed to work regardless of ones' speech abilities and allows full access to existing technology. In this paper, we describe the design of a dataset, model considerations for real-world deployment, and efforts towards model personalization. Our fully-supervised model achieves segment-level precision and recall of 88.6% and 88.4% on an internal dataset of 710 adults, while achieving 0.31 false positives per hour on aggressors such as speech. Five-shot personalization enables satisfactory performance in 84.5% of cases where the generic model fails.      
### 25.High-dimensional dynamic factor models: a selective survey and lines of future research  [ :arrow_down: ](https://arxiv.org/pdf/2202.07745.pdf)
>  High-Dimensional Dynamic Factor Models are presented in detail: The main assumptions and their motivation, main results, illustrations by means of elementary examples. In particular, the role of singular ARMA models in the theory and applications of High-Dimensional Dynamic Factor Models is discussed.The emphasis of the paper is on model classes and their structure theory, rather than on estimation in the narrow sense. Our aim is not a comprehensive survey. Rather we try to point out promising lines of research and applications that have not yet been sufficiently developed.      
### 26.A Subjective Quality Study for Video Frame Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2202.07727.pdf)
>  Video frame interpolation (VFI) is one of the fundamental research areas in video processing and there has been extensive research on novel and enhanced interpolation algorithms. The same is not true for quality assessment of the interpolated content. In this paper, we describe a subjective quality study for VFI based on a newly developed video database, BVI-VFI. BVI-VFI contains 36 reference sequences at three different frame rates and 180 distorted videos generated using five conventional and learning based VFI algorithms. Subjective opinion scores have been collected from 60 human participants, and then employed to evaluate eight popular quality metrics, including PSNR, SSIM and LPIPS which are all commonly used for assessing VFI methods. The results indicate that none of these metrics provide acceptable correlation with the perceived quality on interpolated content, with the best-performing metric, LPIPS, offering a SROCC value below 0.6. Our findings show that there is an urgent need to develop a bespoke perceptual quality metric for VFI. The BVI-VFI dataset is publicly available and can be accessed at <a class="link-external link-https" href="https://danielism97.github.io/BVI-VFI/" rel="external noopener nofollow">this https URL</a>.      
### 27.Control Co-design of Actively Controlled Lightweight Structures for High-acceleration Precision Motion Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.07722.pdf)
>  Precision motion stages are an essential part of a wide range of manufacturing equipment, and their motion performance are critical to the quality and throughput of the systems. The drastically increasing demand for higher manufacturing throughput in various processes necessities the development of next-generation motion systems with reduced moving weight and high control bandwidth. However, the reduction of moving stage's weight can lower the stage's structural resonance frequencies, making the hardware dynamics and controller design problem strongly coupled. Aiming at this challenge, this paper proposes a new formulation of nested hardware and control co-design framework for precision motion stages. The proposed framework explicitly optimizes the closed-loop control bandwidth with guaranteed robustness, and explicitly considers the limits in the physical system. Two case studies, including a motivating example using lumped-parameter mechanical system and a finite-element-simulated lightweight motion stage, are being used to evaluate the effectiveness of the proposed nested CCD framework. Simulation results show that the proposed nested CCD framework has 42\% of weight reduction and 28\% bandwidth improvement compared with a sequential design baseline, which demonstrates the efficacy of the proposed approach.      
### 28.Finite- and Fixed-Time Nonovershooting Stabilizers and Safety Filters by Homogeneous Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2202.07717.pdf)
>  Non-overshooting stabilization is a form of safe control where the setpoint chosen by the user is at the boundary of the safe set. Exponential non-overshooting stabilization, including suitable extensions to systems with deterministic and stochastic disturbances, has been solved by the second author and his coauthors. In this paper we develop homogeneous feedback laws for fixed-time nonovershooting stabilization for nonlinear systems that are input-output linearizable with a full relative degree, i.e., for systems that are diffeomorphically equivalent to the chain of integrators. These homogeneous feedback laws can also assume the secondary role of `fixed-time safety filters' (FxTSf filters) which keep the system within the closed safe set for all time but, in the case where the user's nominal control commands approach to the unsafe set, allow the system to reach the boundary of the safe set no later than a desired time that is independent of nominal control and independent of the value of the state at the time the nominal control begins to be overridden.      
### 29.Deep Contextual Bandits for Orchestrating Multi-User MISO Systems with Multiple RISs  [ :arrow_down: ](https://arxiv.org/pdf/2202.08194.pdf)
>  The emergent technology of Reconfigurable Intelligent Surfaces (RISs) has the potential to transform wireless environments into controllable systems, through programmable propagation of information-bearing signals. Techniques stemming from the field of Deep Reinforcement Learning (DRL) have recently gained popularity in maximizing the sum-rate performance in multi-user communication systems empowered by RISs. Such approaches are commonly based on Markov Decision Processes (MDPs). In this paper, we instead investigate the sum-rate design problem under the scope of the Multi-Armed Bandits (MAB) setting, which is a relaxation of the MDP framework. Nevertheless, in many cases, the MAB formulation is more appropriate to the channel and system models under the assumptions typically made in the RIS literature. To this end, we propose a simpler DRL approach for orchestrating multiple metasurfaces in RIS-empowered multi-user Multiple-Input Single-Output (MISO) systems, which we numerically show to perform equally well with a state-of-the-art MDP-based approach, while being less demanding computationally.      
### 30.Tomographic Muon Imaging of the Great Pyramid of Giza  [ :arrow_down: ](https://arxiv.org/pdf/2202.08184.pdf)
>  The pyramids of the Giza plateau have fascinated visitors since ancient times and are the last of the Seven Wonders of the ancient world still standing. It has been half a century since Luiz Alvarez and his team used cosmic-ray muon imaging to look for hidden chambers in Khafres Pyramid. Advances in instrumentation for High-Energy Physics (HEP) allowed a new survey, ScanPyramids, to make important new discoveries at the Great Pyramid (Khufu) utilizing the same basic technique that the Alvarez team used, but now with modern instrumentation. The Exploring the Great Pyramid Mission plans to field a very-large muon telescope system that will be transformational with respect to the field of cosmic-ray muon imaging. We plan to field a telescope system that has upwards of 100 times the sensitivity of the equipment that has recently been used at the Great Pyramid, will image muons from nearly all angles and will, for the first time, produce a true tomographic image of such a large structure.      
### 31.Towards Battery-Free Machine Learning and Inference in Underwater Environments  [ :arrow_down: ](https://arxiv.org/pdf/2202.08174.pdf)
>  This paper is motivated by a simple question: Can we design and build battery-free devices capable of machine learning and inference in underwater environments? An affirmative answer to this question would have significant implications for a new generation of underwater sensing and monitoring applications for environmental monitoring, scientific exploration, and climate/weather prediction. <br>To answer this question, we explore the feasibility of bridging advances from the past decade in two fields: battery-free networking and low-power machine learning. Our exploration demonstrates that it is indeed possible to enable battery-free inference in underwater environments. We designed a device that can harvest energy from underwater sound, power up an ultra-low-power microcontroller and on-board sensor, perform local inference on sensed measurements using a lightweight Deep Neural Network, and communicate the inference result via backscatter to a receiver. We tested our prototype in an emulated marine bioacoustics application, demonstrating the potential to recognize underwater animal sounds without batteries. Through this exploration, we highlight the challenges and opportunities for making underwater battery-free inference and machine learning ubiquitous.      
### 32.Cell-Free MIMO Systems Powered by Intelligent Reflecting Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2202.08152.pdf)
>  Cell-free massive multiple-input multiple-output (MIMO) and intelligent reflecting surface (IRS) are considered as the prospective multiple antenna technologies for beyond the fifth-generation (5G) networks. Cell-free MIMO systems powered by IRSs, combining both technologies, can further improve the performance of cell-free MIMO systems at low cost and energy consumption. Prior works focused on instantaneous performance metrics and relied on alternating optimization algorithms, which impose huge computational complexity and signaling overhead. To address these challenges, we propose a novel two-step algorithm that provides the long-term passive beamformers at the IRSs using statistical channel state information (S-CSI) and short-term active precoders and long-term power allocation at the access points (APs) to maximize the minimum achievable rate. Simulation results verify that the proposed scheme outperforms benchmark schemes and brings a significant performance gain to the cell-free MIMO systems powered by IRSs.      
### 33.A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2202.08146.pdf)
>  With the recent advances in multi-disciplinary human activity recognition techniques, it has become inevitable to find an efficient, economical &amp; privacy-friendly approach for human-to-human mutual interaction recognition in order to breakthrough the modern artificial intelligence centric indoor monitoring &amp; surveillance system. This study initially attempted to set its sights on the already proposed human activity recognition mechanisms and found a void in mutual interaction recognition from Wi-Fi channel information which is convenient &amp; affordable to be utilized. Then it elucidated on the corresponding components of wireless local area network gadgets along with the channel properties, and notable underlying causes of signal &amp; channel perturbation. Thenceforth the study conducted three experiments on human-to-human mutual interaction recognition using the proposed Self-Attention furnished Bidirectional Gated Recurrent Neural Network deep learning model which is perceived to become emergent nowadays for time-series data classification through automated temporal feature extraction. Single pair mutual interaction recognition experiment achieved a maximum of 94% test benchmark while the experiment involving ten subject-pairs secured 88% benchmark with improved classification around interaction-transition region. Demonstration of a graphical user interface executable software designed using PyQt5 python module subsequently portrayed the overall mutual human-interaction recognition procedure, and finally the study concluded with a brief discourse regarding the possible solutions to the handicaps that resulted in curtailments observed in the case of cross-test experiment.      
### 34.ADIMA: Abuse Detection In Multilingual Audio  [ :arrow_down: ](https://arxiv.org/pdf/2202.07991.pdf)
>  Abusive content detection in spoken text can be addressed by performing Automatic Speech Recognition (ASR) and leveraging advancements in natural language processing. However, ASR models introduce latency and often perform sub-optimally for profane words as they are underrepresented in training corpora and not spoken clearly or completely. Exploration of this problem entirely in the audio domain has largely been limited by the lack of audio datasets. Building on these challenges, we propose ADIMA, a novel, linguistically diverse, ethically sourced, expert annotated and well-balanced multilingual profanity detection audio dataset comprising of 11,775 audio samples in 10 Indic languages spanning 65 hours and spoken by 6,446 unique users. Through quantitative experiments across monolingual and cross-lingual zero-shot settings, we take the first step in democratizing audio based content moderation in Indic languages and set forth our dataset to pave future work.      
### 35.On loss functions and evaluation metrics for music source separation  [ :arrow_down: ](https://arxiv.org/pdf/2202.07968.pdf)
>  We investigate which loss functions provide better separations via benchmarking an extensive set of those for music source separation. To that end, we first survey the most representative audio source separation losses we identified, to later consistently benchmark them in a controlled experimental setup. We also explore using such losses as evaluation metrics, via cross-correlating them with the results of a subjective test. Based on the observation that the standard signal-to-distortion ratio metric can be misleading in some scenarios, we study alternative evaluation metrics based on the considered losses.      
### 36.NORM: An FPGA-based Non-volatile Memory Emulation Framework for Intermittent Computing  [ :arrow_down: ](https://arxiv.org/pdf/2202.07948.pdf)
>  Intermittent computing systems operate by relying only on harvested energy accumulated in their tiny energy reservoirs, typically capacitors. An intermittent device dies due to a power failure when there is no energy in its capacitor and boots again when the harvested energy is sufficient to power its hardware components. Power failures prevent the forward progress of computation due to the frequent loss of computational state. To remedy this problem, intermittent computing systems comprise built-in fast non-volatile memories with high write endurance to store information that persists despite frequent power failures. However, the lack of design tools makes fast-prototyping these systems difficult. Even though FPGAs are common platforms for fast prototyping and behavioral verification of continuously-powered architectures, they do not target prototyping intermittent computing systems. This article introduces a new FPGA-based framework, named NORM (Non-volatile memORy eMulator), to emulate and verify the behavior of any intermittent computing system that exploits fast non-volatile memories. Our evaluation showed that NORM can be used to emulate and validate FeRAM-based transiently-powered hardware architectures successfully.      
### 37.Clustering Enabled Few-Shot Load Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2202.07939.pdf)
>  While the advanced machine learning algorithms are effective in load forecasting, they often suffer from low data utilization, and hence their superior performance relies on massive datasets. This motivates us to design machine learning algorithms with improved data utilization. Specifically, we consider the load forecasting for a new user in the system by observing only few shots (data points) of its energy consumption. This task is challenging since the limited samples are insufficient to exploit the temporal characteristics, essential for load forecasting. Nonetheless, we notice that there are not too many temporal characteristics for residential loads due to the limited kinds of human lifestyle. Hence, we propose to utilize the historical load profile data from existing users to conduct effective clustering, which mitigates the challenges brought by the limited samples. Specifically, we first design a feature extraction clustering method for categorizing historical data. Then, inheriting the prior knowledge from the clustering results, we propose a two-phase Long Short Term Memory (LSTM) model to conduct load forecasting for new users. The proposed method outperforms the traditional LSTM model, especially when the training sample size fails to cover a whole period (i.e., 24 hours in our task). Extensive case studies on two real-world datasets and one synthetic dataset verify the effectiveness and efficiency of our method.      
### 38.DBT-Net: Dual-branch federative magnitude and phase estimation with attention-in-attention transformer for monaural speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.07931.pdf)
>  The decoupling-style concept begins to ignite in the speech enhancement area, which decouples the original complex spectrum estimation task into multiple easier sub-tasks (i.e., magnitude and phase), resulting in better performance and easier interpretability. In this paper, we propose a dual-branch federative magnitude and phase estimation framework, dubbed DBT-Net, for monaural speech enhancement, which aims at recovering the coarse- and fine-grained regions of the overall spectrum in parallel. From the complementary perspective, the magnitude estimation branch is designed to filter out dominant noise components in the magnitude domain, while the complex spectrum purification branch is elaborately designed to inpaint the missing spectral details and implicitly estimate the phase information in the complex domain. To facilitate the information flow between each branch, interaction modules are introduced to leverage features learned from one branch, so as to suppress the undesired parts and recover the missing components of the other branch. Instead of adopting the conventional RNNs and temporal convolutional networks for sequence modeling, we propose a novel attention-in-attention transformer-based network within each branch for better feature learning. More specially, it is composed of several adaptive spectro-temporal attention transformer-based modules and an adaptive hierarchical attention module, aiming to capture long-term time-frequency dependencies and further aggregate intermediate hierarchical contextual information. Comprehensive evaluations on the WSJ0-SI84 + DNS-Challenge and VoiceBank + DEMAND dataset demonstrate that the proposed approach consistently outperforms previous advanced systems and yields state-of-the-art performance in terms of speech quality and intelligibility.      
### 39.Willems' fundamental lemma for linear descriptor systems and its use for data-driven output-feedback MPC  [ :arrow_down: ](https://arxiv.org/pdf/2202.07930.pdf)
>  In this paper we investigate data-driven predictive control of discrete-time linear descriptor systems. Specifically, we give a tailored variant of Willems' fundamental lemma, which shows that for descriptor systems the non-parametric modelling via a Hankel matrix requires less data compared to linear time-invariant systems without algebraic constraints. Moreover, we use this description to propose a data-driven framework for optimal control and predictive control of discrete-time linear descriptor systems. For the latter, we provide a sufficient stability condition for receding-horizon control before we illustrate our findings with an example.      
### 40.Can Deep Learning be Applied to Model-Based Multi-Object Tracking?  [ :arrow_down: ](https://arxiv.org/pdf/2202.07909.pdf)
>  Multi-object tracking (MOT) is the problem of tracking the state of an unknown and time-varying number of objects using noisy measurements, with important applications such as autonomous driving, tracking animal behavior, defense systems, and others. In recent years, deep learning (DL) has been increasingly used in MOT for improving tracking performance, but mostly in settings where the measurements are high-dimensional and there are no available models of the measurement likelihood and the object dynamics. The model-based setting instead has not attracted as much attention, and it is still unclear if DL methods can outperform traditional model-based Bayesian methods, which are the state of the art (SOTA) in this context. In this paper, we propose a Transformer-based DL tracker and evaluate its performance in the model-based setting, comparing it to SOTA model-based Bayesian methods in a variety of different tasks. Our results show that the proposed DL method can match the performance of the model-based methods in simple tasks, while outperforming them when the task gets more complicated, either due to an increase in the data association complexity, or to stronger nonlinearities of the models of the environment.      
### 41.Singing-Tacotron: Global duration control attention and dynamic filter for End-to-end singing voice synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2202.07907.pdf)
>  End-to-end singing voice synthesis (SVS) is attractive due to the avoidance of pre-aligned data. However, the auto learned alignment of singing voice with lyrics is difficult to match the duration information in musical score, which will lead to the model instability or even failure to synthesize voice. To learn accurate alignment information automatically, this paper proposes an end-to-end SVS framework, named Singing-Tacotron. The main difference between the proposed framework and Tacotron is that the speech can be controlled significantly by the musical score's duration information. Firstly, we propose a global duration control attention mechanism for the SVS model. The attention mechanism can control each phoneme's duration. Secondly, a duration encoder is proposed to learn a set of global transition tokens from the musical score. These transition tokens can help the attention mechanism decide whether moving to the next phoneme or staying at each decoding step. Thirdly, to further improve the model's stability, a dynamic filter is designed to help the model overcome noise interference and pay more attention to local context information. Subjective and objective evaluation verify the effectiveness of the method. Furthermore, the role of global transition tokens and the effect of duration control are explored. Examples of experiments can be found at <a class="link-external link-https" href="https://hairuo55.github.io/SingingTacotron" rel="external noopener nofollow">this https URL</a>.      
### 42.Enhancing Causal Estimation through Unlabeled Offline Data  [ :arrow_down: ](https://arxiv.org/pdf/2202.07895.pdf)
>  Consider a situation where a new patient arrives in the Intensive Care Unit (ICU) and is monitored by multiple sensors. We wish to assess relevant unmeasured physiological variables (e.g., cardiac contractility and output and vascular resistance) that have a strong effect on the patients diagnosis and treatment. We do not have any information about this specific patient, but, extensive offline information is available about previous patients, that may only be partially related to the present patient (a case of dataset shift). This information constitutes our prior knowledge, and is both partial and approximate. The basic question is how to best use this prior knowledge, combined with online patient data, to assist in diagnosing the current patient most effectively. Our proposed approach consists of three stages: (i) Use the abundant offline data in order to create both a non-causal and a causal estimator for the relevant unmeasured physiological variables. (ii) Based on the non-causal estimator constructed, and a set of measurements from a new group of patients, we construct a causal filter that provides higher accuracy in the prediction of the hidden physiological variables for this new set of patients. (iii) For any new patient arriving in the ICU, we use the constructed filter in order to predict relevant internal variables. Overall, this strategy allows us to make use of the abundantly available offline data in order to enhance causal estimation for newly arriving patients. We demonstrate the effectiveness of this methodology on a (non-medical) real-world task, in situations where the offline data is only partially related to the new observations. We provide a mathematical analysis of the merits of the approach in a linear setting of Kalman filtering and smoothing, demonstrating its utility.      
### 43.Knowledge Transfer from Large-scale Pretrained Language Models to End-to-end Speech Recognizers  [ :arrow_down: ](https://arxiv.org/pdf/2202.07894.pdf)
>  End-to-end speech recognition is a promising technology for enabling compact automatic speech recognition (ASR) systems since it can unify the acoustic and language model into a single neural network. However, as a drawback, training of end-to-end speech recognizers always requires transcribed utterances. Since end-to-end models are also known to be severely data hungry, this constraint is crucial especially because obtaining transcribed utterances is costly and can possibly be impractical or impossible. This paper proposes a method for alleviating this issue by transferring knowledge from a language model neural network that can be pretrained with text-only data. Specifically, this paper attempts to transfer semantic knowledge acquired in embedding vectors of large-scale language models. Since embedding vectors can be assumed as implicit representations of linguistic information such as part-of-speech, intent, and so on, those are also expected to be useful modeling cues for ASR decoders. This paper extends two types of ASR decoders, attention-based decoders and neural transducers, by modifying training loss functions to include embedding prediction terms. The proposed systems were shown to be effective for error rate reduction without incurring extra computational costs in the decoding phase.      
### 44.Deep-Learning-Assisted Configuration of Reconfigurable Intelligent Surfaces in Dynamic rich-scattering Environments  [ :arrow_down: ](https://arxiv.org/pdf/2202.07884.pdf)
>  The integration of Reconfigurable Intelligent Surfaces (RISs) into wireless environments endows channels with programmability, and is expected to play a key role in future communication standards. To date, most RIS-related efforts focus on quasi-free-space, where wireless channels are typically modeled analytically. Many realistic communication scenarios occur, however, in rich-scattering environments which, moreover, evolve dynamically. These conditions present a tremendous challenge in identifying an RIS configuration that optimizes the achievable communication rate. In this paper, we make a first step toward tackling this challenge. Based on a simulator that is faithful to the underlying wave physics, we train a deep neural network as surrogate forward model to capture the stochastic dependence of wireless channels on the RIS configuration under dynamic rich-scattering conditions. Subsequently, we use this model in combination with a genetic algorithm to identify RIS configurations optimizing the communication rate. We numerically demonstrate the ability of the proposed approach to tune RISs to improve the achievable rate in rich-scattering setups.      
### 45.An efficient distributed scheduling algorithm for relay-assisted mmWave backhaul networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.07872.pdf)
>  In this paper, a novel distributed scheduling algorithm is proposed, which aims to efficiently schedule both the uplink and downlink backhaul traffic in the relay-assisted mmWave backhaul network with a tree topology. The handshaking of control messages, calculation of local schedules, and the determination of final valid schedule are all discussed. Simulation results show that the performance of the distributed algorithm can reach very close to the maximum traffic demand of the backhaul network, and it can also adapt to the dynamic traffic with sharp traffic demand change of small-cell BSs quickly and accurately.      
### 46.Fixed-time Synchronization of Networked Uncertain Euler-Lagrange Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.07866.pdf)
>  This paper considers the fixed-time control problem of a multi-agent system composed of a class of Euler-Lagrange dynamics with parametric uncertainty and a dynamic leader under a directed communication network. A distributed fixed-time observer is first proposed to estimate the desired trajectory and then a fixed-time controller is constructed by transforming uncertain Euler-Lagrange systems into second-order systems and utilizing the backstepping design procedure. The overall design guarantees that the synchronization errors converge to zero in a prescribed time independent of initial conditions. The control design conditions can also be relaxed for a weaker finite-time control requirement.      
### 47.SRP-DNN: Learning Direct-Path Phase Difference for Multiple Moving Sound Source Localization  [ :arrow_down: ](https://arxiv.org/pdf/2202.07859.pdf)
>  Multiple moving sound source localization in real-world scenarios remains a challenging issue due to interaction between sources, time-varying trajectories, distorted spatial cues, etc. In this work, we propose to use deep learning techniques to learn competing and time-varying direct-path phase differences for localizing multiple moving sound sources. A causal convolutional recurrent neural network is designed to extract the direct-path phase difference sequence from signals of each microphone pair. To avoid the assignment ambiguity and the problem of uncertain output-dimension encountered when simultaneously predicting multiple targets, the learning target is designed in a weighted sum format, which encodes source activity in the weight and direct-path phase differences in the summed value. The learned direct-path phase differences for all microphone pairs can be directly used to construct the spatial spectrum according to the formulation of steered response power (SRP). This deep neural network (DNN) based SRP method is referred to as SRP-DNN. The locations of sources are estimated by iteratively detecting and removing the dominant source from the spatial spectrum, in which way the interaction between sources is reduced. Experimental results on both simulated and real-world data show the superiority of the proposed method in the presence of noise and reverberation.      
### 48.Conversational Speech Recognition By Learning Conversation-level Characteristics  [ :arrow_down: ](https://arxiv.org/pdf/2202.07855.pdf)
>  Conversational automatic speech recognition (ASR) is a task to recognize conversational speech including multiple speakers. Unlike sentence-level ASR, conversational ASR can naturally take advantages from specific characteristics of conversation, such as role preference and topical coherence. This paper proposes a conversational ASR model which explicitly learns conversation-level characteristics under the prevalent end-to-end neural framework. The highlights of the proposed model are twofold. First, a latent variational module (LVM) is attached to a conformer-based encoder-decoder ASR backbone to learn role preference and topical coherence. Second, a topic model is specifically adopted to bias the outputs of the decoder to words in the predicted topics. Experiments on two Mandarin conversational ASR tasks show that the proposed model achieves a maximum 12% relative character error rate (CER) reduction.      
### 49.Learning Deep Direct-Path Relative Transfer Function for Binaural Sound Source Localization  [ :arrow_down: ](https://arxiv.org/pdf/2202.07841.pdf)
>  Direct-path relative transfer function (DP-RTF) refers to the ratio between the direct-path acoustic transfer functions of two microphone channels. Though DP-RTF fully encodes the sound spatial cues and serves as a reliable localization feature, it is often erroneously estimated in the presence of noise and reverberation. This paper proposes to learn DP-RTF with deep neural networks for robust binaural sound source localization. A DP-RTF learning network is designed to regress the binaural sensor signals to a real-valued representation of DP-RTF. It consists of a branched convolutional neural network module to separately extract the inter-channel magnitude and phase patterns, and a convolutional recurrent neural network module for joint feature learning. To better explore the speech spectra to aid the DP-RTF estimation, a monaural speech enhancement network is used to recover the direct-path spectrograms from the noisy ones. The enhanced spectrograms are stacked onto the noisy spectrograms to act as the input of the DP-RTF learning network. We train one unique DP-RTF learning network using many different binaural arrays to enable the generalization of DP-RTF learning across arrays. This way avoids time-consuming training data collection and network retraining for a new array, which is very useful in practical application. Experimental results on both simulated and real-world data show the effectiveness of the proposed method for direction of arrival (DOA) estimation in the noisy and reverberant environment, and a good generalization ability to unseen binaural arrays.      
### 50.Segmentation and Risk Score Prediction of Head and Neck Cancers in PET/CT Volumes with 3D U-Net and Cox Proportional Hazard Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.07823.pdf)
>  We utilized a 3D nnU-Net model with residual layers supplemented by squeeze and excitation (SE) normalization for tumor segmentation from PET/CT images provided by the Head and Neck Tumor segmentation chal-lenge (HECKTOR). Our proposed loss function incorporates the Unified Fo-cal and Mumford-Shah losses to take the advantage of distribution, region, and boundary-based loss functions. The results of leave-one-out-center-cross-validation performed on different centers showed a segmentation performance of 0.82 average Dice score (DSC) and 3.16 median Hausdorff Distance (HD), and our results on the test set achieved 0.77 DSC and 3.01 HD. Following lesion segmentation, we proposed training a case-control proportional hazard Cox model with an MLP neural net backbone to predict the hazard risk score for each discrete lesion. This hazard risk prediction model (CoxCC) was to be trained on a number of PET/CT radiomic features extracted from the segmented lesions, patient and lesion demographics, and encoder features provided from the penultimate layer of a multi-input 2D PET/CT convolutional neural network tasked with predicting time-to-event for each lesion. A 10-fold cross-validated CoxCC model resulted in a c-index validation score of 0.89, and a c-index score of 0.61 on the HECKTOR challenge test dataset.      
### 51.Applying adversarial networks to increase the data efficiency and reliability of Self-Driving Cars  [ :arrow_down: ](https://arxiv.org/pdf/2202.07815.pdf)
>  Convolutional Neural Networks (CNNs) are vulnerable to misclassifying images when small perturbations are present. With the increasing prevalence of CNNs in self-driving cars, it is vital to ensure these algorithms are robust to prevent collisions from occurring due to failure in recognizing a situation. In the Adversarial Self-Driving framework, a Generative Adversarial Network (GAN) is implemented to generate realistic perturbations in an image that cause a classifier CNN to misclassify data. This perturbed data is then used to train the classifier CNN further. The Adversarial Self-driving framework is applied to an image classification algorithm to improve the classification accuracy on perturbed images and is later applied to train a self-driving car to drive in a simulation. A small-scale self-driving car is also built to drive around a track and classify signs. The Adversarial Self-driving framework produces perturbed images through learning a dataset, as a result removing the need to train on significant amounts of data. Experiments demonstrate that the Adversarial Self-driving framework identifies situations where CNNs are vulnerable to perturbations and generates new examples of these situations for the CNN to train on. The additional data generated by the Adversarial Self-driving framework provides sufficient data for the CNN to generalize to the environment. Therefore, it is a viable tool to increase the resilience of CNNs to perturbations. Particularly, in the real-world self-driving car, the application of the Adversarial Self-Driving framework resulted in an 18 % increase in accuracy, and the simulated self-driving model had no collisions in 30 minutes of driving.      
### 52.Efficient Content Delivery in Cache-Enabled VEN with Deadline-Constrained Heterogeneous Demands: A User-Centric Approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.07792.pdf)
>  Modern connected vehicles (CVs) frequently require diverse types of content for mission-critical decision-making and onboard users' entertainment. These contents are required to be fully delivered to the requester CVs within stringent deadlines that the existing radio access technology (RAT) solutions may fail to ensure. Motivated by the above consideration, this paper exploits content caching with a software-defined user-centric virtual cell (VC) based RAT solution for delivering the requested contents from a proximity edge server. Moreover, to capture the heterogeneous demands of the CVs, we introduce a preference-popularity tradeoff in their content request model. To that end, we formulate a joint optimization problem for content placement, CV scheduling, VC configuration, VC-CV association and radio resource allocation to minimize long-term content delivery delay. However, the joint problem is highly complex and cannot be solved efficiently in polynomial time. As such, we decompose the original problem into a cache placement problem and a content delivery delay minimization problem given the cache placement policy. We use deep reinforcement learning (DRL) as a learning solution for the first sub-problem. Furthermore, we transform the delay minimization problem into a priority-based weighted sum rate (WSR) maximization problem, which is solved leveraging maximum bipartite matching (MWBM) and a simple linear search algorithm. Our extensive simulation results demonstrate the effectiveness of the proposed method.      
### 53.Speech Denoising in the Waveform Domain with Self-Attention  [ :arrow_down: ](https://arxiv.org/pdf/2202.07790.pdf)
>  In this work, we present CleanUNet, a causal speech denoising model on the raw waveform. The proposed model is based on an encoder-decoder architecture combined with several self-attention blocks to refine its bottleneck representations, which is crucial to obtain good results. The model is optimized through a set of losses defined over both waveform and multi-resolution spectrograms. The proposed method outperforms the state-of-the-art models in terms of denoised speech quality from various objective and subjective evaluation metrics.      
### 54.The Promising Marriage of Mobile Edge Computing and Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2202.07775.pdf)
>  This paper considers a mobile edge computing-enabled cell-free massive MIMO wireless network. An optimization problem for the joint allocation of uplink powers and remote computational resources is formulated, aimed at minimizing the total uplink power consumption under latency constraints, while simultaneously also maximizing the minimum SE throughout the network. Since the considered problem is non-convex, an iterative algorithm based on sequential convex programming is devised. A detailed performance comparison between the proposed distributed architecture and its co-located counterpart, based on a multi-cell massive MIMO deployment, is provided. Numerical results reveal the natural suitability of cell-free massive MIMO in supporting computation-offloading applications, with benefits over users' transmit power and energy consumption, the offloading latency experienced, and the total amount of allocated remote computational resources.      
### 55.General-purpose, long-context autoregressive modeling with Perceiver AR  [ :arrow_down: ](https://arxiv.org/pdf/2202.07765.pdf)
>  Real-world data is high-dimensional: a book, image, or musical performance can easily contain hundreds of thousands of elements even after compression. However, the most commonly used autoregressive models, Transformers, are prohibitively expensive to scale to the number of inputs and layers needed to capture this long-range structure. We develop Perceiver AR, an autoregressive, modality-agnostic architecture which uses cross-attention to map long-range inputs to a small number of latents while also maintaining end-to-end causal masking. Perceiver AR can directly attend to over a hundred thousand tokens, enabling practical long-context density estimation without the need for hand-crafted sparsity patterns or memory mechanisms. When trained on images or music, Perceiver AR generates outputs with clear long-term coherence and structure. Our architecture also obtains state-of-the-art likelihood on long-sequence benchmarks, including 64 x 64 ImageNet images and PG-19 books.      
### 56.Normalized K-Means for Noise-Insensitive Multi-Dimensional Feature Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.07754.pdf)
>  Many measurement modalities which perform imaging by probing an object pixel-by-pixel, such as via Photoacoustic Microscopy, produce a multi-dimensional feature (typically a time-domain signal) at each pixel. In principle, the many degrees of freedom in the time-domain signal would admit the possibility of significant multi-modal information being implicitly present, much more than a single scalar "brightness", regarding the underlying targets being observed. However, the measured signal is neither a weighted-sum of basis functions (such as principal components) nor one of a set of prototypes (K-means), which has motivated the novel clustering method proposed here, capable of learning centroids (signal shapes) that are related to the underlying, albeit unknown, target characteristics in a scalable and noise-robust manner.      
### 57.Enhancing Deformable Convolution based Video Frame Interpolation with Coarse-to-fine 3D CNN  [ :arrow_down: ](https://arxiv.org/pdf/2202.07731.pdf)
>  This paper presents a new deformable convolution-based video frame interpolation (VFI) method, using a coarse to fine 3D CNN to enhance the multi-flow prediction. This model first extracts spatio-temporal features at multiple scales using a 3D CNN, and estimates multi-flows using these features in a coarse-to-fine manner. The estimated multi-flows are then used to warp the original input frames as well as context maps, and the warped results are fused by a synthesis network to produce the final output. This VFI approach has been fully evaluated against 12 state-of-the-art VFI methods on three commonly used test databases. The results evidently show the effectiveness of the proposed method, which offers superior interpolation performance over other state of the art algorithms, with PSNR gains up to 0.19dB.      
### 58.Active Uncertainty Learning for Human-Robot Interaction: An Implicit Dual Control Approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.07720.pdf)
>  Predictive models are effective in reasoning about human motion, a crucial part that affects safety and efficiency in human-robot interaction. However, robots often lack access to certain key parameters of such models, for example, human's objectives, their level of distraction, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters as stochastic hidden states and identifying their values using information gathered during control of the robot. Despite its ability to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general human-in-the-loop motion planning, mainly due to nested trajectory optimization and human intent prediction. In this paper, we present a novel algorithmic approach to enable active uncertainty learning for human-in-the-loop motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by real-time gradient-based optimization methods. The resulting policy is shown to preserve the dual control effect for generic human predictive models with both continuous and categorical uncertainty. The efficacy of our approach is demonstrated with simulated driving examples.      
### 59.The Design and Analysis of a Mobility Game  [ :arrow_down: ](https://arxiv.org/pdf/2202.07691.pdf)
>  In this paper, we study a routing and travel-mode choice problem for mobility systems with a multimodal transportation network as a mobility game with coupled hybrid action sets. The mobility resources (modes of transportation) may experience delays that grow with the aggregate utilization of the resource. We develop a theoretical framework based on repeated non-cooperative game theory for the travelers' routing and travel-mode choice within a general mobility system. This framework aims to study the behavioral impact of the travelers' decision-making on efficiency. We consider the traffic congestion and the waiting times at different transport hubs and introduce mobility monetary incentives as part of a pricing scheme. We show that the travelers' selfish behavior results in a Nash equilibrium, and then we perform a Price of Anarchy analysis to establish that the mobility system's inefficiencies remain relatively low as the number of travelers increases. We deviate from the standard game-theoretic analysis of decision-making by extending our modeling framework to capture the subjective behavior of travelers using prospect theory. Finally, we provide a simple example to showcase the effectiveness of our mobility game and incentives.      
