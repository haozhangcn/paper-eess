# ArXiv eess --Fri, 25 Feb 2022
### 1.Towards Low-distortion Multi-channel Speech Enhancement: The ESPNet-SE Submission to The L3DAS22 Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.12298.pdf)
>  This paper describes our submission to the L3DAS22 Challenge Task 1, which consists of speech enhancement with 3D Ambisonic microphones. The core of our approach combines Deep Neural Network (DNN) driven complex spectral mapping with linear beamformers such as the multi-frame multi-channel Wiener filter. Our proposed system has two DNNs and a linear beamformer in between. Both DNNs are trained to perform complex spectral mapping, using a combination of waveform and magnitude spectrum losses. The estimated signal from the first DNN is used to drive a linear beamformer, and the beamforming result, together with this enhanced signal, are used as extra inputs for the second DNN which refines the estimation. Then, from this new estimated signal, the linear beamformer and second DNN are run iteratively. The proposed method was ranked first in the challenge, achieving, on the evaluation set, a ranking metric of 0.984, versus 0.833 of the challenge baseline.      
### 2.Factorizer: A Scalable Interpretable Approach to Context Modeling for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.12295.pdf)
>  Convolutional Neural Networks (CNNs) with U-shaped architectures have dominated medical image segmentation, which is crucial for various clinical purposes. However, the inherent locality of convolution makes CNNs fail to fully exploit global context, essential for better recognition of some structures, e.g., brain lesions. Transformers have recently proved promising performance on vision tasks, including semantic segmentation, mainly due to their capability of modeling long-range dependencies. Nevertheless, the quadratic complexity of attention makes existing Transformer-based models use self-attention layers only after somehow reducing the image resolution, which limits the ability to capture global contexts present at higher resolutions. Therefore, this work introduces a family of models, dubbed Factorizer, which leverages the power of low-rank matrix factorization for constructing an end-to-end segmentation model. Specifically, we propose a linearly scalable approach to context modeling, formulating Nonnegative Matrix Factorization (NMF) as a differentiable layer integrated into a U-shaped architecture. The shifted window technique is also utilized in combination with NMF to effectively aggregate local information. Factorizers compete favorably with CNNs and Transformers in terms of accuracy, scalability, and interpretability, achieving state-of-the-art results on the BraTS dataset for brain tumor segmentation, with Dice scores of 79.33%, 83.14%, and 90.16% for enhancing tumor, tumor core, and whole tumor, respectively. Highly meaningful NMF components give an additional interpretability advantage to Factorizers over CNNs and Transformers. Moreover, our ablation studies reveal a distinctive feature of Factorizers that enables a significant speed-up in inference for a trained Factorizer without any extra steps and without sacrificing much accuracy.      
### 3.Inflation of test accuracy due to data leakage in deep learning-based classification of OCT images  [ :arrow_down: ](https://arxiv.org/pdf/2202.12267.pdf)
>  In the application of deep learning on optical coherence tomography (OCT) data, it is common to train classification networks using 2D images originating from volumetric data. Given the micrometer resolution of OCT systems, consecutive images are often very similar in both visible structures and noise. Thus, an inappropriate data split can result in overlap between the training and testing sets, with a large portion of the literature overlooking this aspect. In this study, the effect of improper dataset splitting on model evaluation is demonstrated for two classification tasks using two OCT open-access datasets extensively used in the literature, Kermany's ophthalmology dataset and AIIMS breast tissue dataset. Our results show that the classification accuracy is inflated by 3.9 to 26 percentage units for models tested on a dataset with improper splitting, highlighting the considerable effect of dataset handling on model evaluation. This study intends to raise awareness on the importance of dataset splitting for research on deep learning using OCT data and volumetric data in general.      
### 4.Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.12233.pdf)
>  The performance of spoofing countermeasure systems depends fundamentally upon the use of sufficiently representative training data. With this usually being limited, current solutions typically lack generalisation to attacks encountered in the wild. Strategies to improve reliability in the face of uncontrolled, unpredictable attacks are hence needed. We report in this paper our efforts to use self-supervised learning in the form of a wav2vec 2.0 front-end with fine tuning. Despite initial base representations being learned using only bona fide data and no spoofed data, we obtain the lowest equal error rates reported in the literature for both the ASVspoof 2021 Logical Access and Deepfake databases. When combined with data augmentation, these results correspond to an improvement of almost 90 % relative to our baseline system.      
### 5.Detection by Sampling: Massive MIMO Detector based on Langevin Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2202.12199.pdf)
>  Optimal symbol detection in multiple-input multiple-output (MIMO) systems is known to be an NP-hard problem. Hence, the objective of any detector of practical relevance is to get reasonably close to the optimal solution while keeping the computational complexity in check. In this work, we propose a MIMO detector based on an annealed version of Langevin (stochastic) dynamics. More precisely, we define a stochastic dynamical process whose stationary distribution coincides with the posterior distribution of the symbols given our observations. In essence, this allows us to approximate the maximum a posteriori estimator of the transmitted symbols by sampling from the proposed Langevin dynamic. Furthermore, we carefully craft this stochastic dynamic by gradually adding a sequence of noise with decreasing variance to the trajectories, which ensures that the estimated symbols belong to a pre-specified discrete constellation. Through numerical experiments, we show that our proposed detector yields state-of-the-art symbol error rate performance.      
### 6.Closing the Gap between Single-User and Multi-User VoiceFilter-Lite  [ :arrow_down: ](https://arxiv.org/pdf/2202.12169.pdf)
>  VoiceFilter-Lite is a speaker-conditioned voice separation model that plays a crucial role in improving speech recognition and speaker verification by suppressing overlapping speech from non-target speakers. However, one limitation of VoiceFilter-Lite, and other speaker-conditioned speech models in general, is that these models are usually limited to a single target speaker. This is undesirable as most smart home devices now support multiple enrolled users. In order to extend the benefits of personalization to multiple users, we previously developed an attention-based speaker selection mechanism and applied it to VoiceFilter-Lite. However, the original multi-user VoiceFilter-Lite model suffers from significant performance degradation compared with single-user models. In this paper, we devised a series of experiments to improve the multi-user VoiceFilter-Lite model. By incorporating a dual learning rate schedule and by using feature-wise linear modulation (FiLM) to condition the model with the attended speaker embedding, we successfully closed the performance gap between multi-user and single-user VoiceFilter-Lite models on single-speaker evaluations. At the same time, the new model can also be easily extended to support any number of users, and significantly outperforms our previously published model on multi-speaker evaluations.      
### 7.Attentive Temporal Pooling for Conformer-based Streaming Language Identification in Long-form Speech  [ :arrow_down: ](https://arxiv.org/pdf/2202.12163.pdf)
>  In this paper, we introduce a novel language identification system based on conformer layers. We propose an attentive temporal pooling mechanism to allow the model to carry information in long-form audio via a recurrent form, such that the inference can be performed in a streaming fashion. Additionally, a simple domain adaptation mechanism is introduced to allow adapting an existing language identification model to a new domain where the prior language distribution is different. We perform a comparative study of different model topologies under different constraints of model size, and find that conformer-base models outperform LSTM and transformer based models. Our experiments also show that attentive temporal pooling and domain adaptation significantly improve the model accuracy.      
### 8.A novel unsupervised covid lung lesion segmentation based on the lung tissue identification  [ :arrow_down: ](https://arxiv.org/pdf/2202.12148.pdf)
>  This study aimed to evaluate the performance of a novel unsupervised deep learning-based framework for automated infections lesion segmentation from CT images of Covid patients. In the first step, two residual networks were independently trained to identify the lung tissue for normal and Covid patients in a supervised manner. These two models, referred to as DL-Covid and DL-Norm for Covid-19 and normal patients, respectively, generate the voxel-wise probability maps for lung tissue identification. To detect Covid lesions, the CT image of the Covid patient is processed by the DL-Covid and DL-Norm models to obtain two lung probability maps. Since the DL-Norm model is not familiar with Covid infections within the lung, this model would assign lower probabilities to the lesions than the DL-Covid. Hence, the probability maps of the Covid infections could be generated through the subtraction of the two lung probability maps obtained from the DL-Covid and DL-Norm models. Manual lesion segmentation of 50 Covid-19 CT images was used to assess the accuracy of the unsupervised lesion segmentation approach. The Dice coefficients of 0.985 and 0.978 were achieved for the lung segmentation of normal and Covid patients in the external validation dataset, respectively. Quantitative results of infection segmentation by the proposed unsupervised method showed the Dice coefficient and Jaccard index of 0.67 and 0.60, respectively. Quantitative evaluation of the proposed unsupervised approach for Covid-19 infectious lesion segmentation showed relatively satisfactory results. Since this framework does not require any annotated dataset, it could be used to generate very large training samples for the supervised machine learning algorithms dedicated to noisy and/or weakly annotated datasets.      
### 9.Projected gradient-tracking in multi-cluster games and its application to power management  [ :arrow_down: ](https://arxiv.org/pdf/2202.12124.pdf)
>  We are concerned with a distributed approach to solve multi-cluster games arising in multi-agent systems. In such games, agents are separated into distinct clusters. The agents belonging to the same cluster cooperate with each other to achieve a common cluster goal while a non-cooperative game is played between the clusters. To be able to deal with the sparsity of information, as each agent only knows a specific part of the problem, we combine gradient-tracking and consensus methods for information distribution into an algorithm that can solve both the cooperative and non-cooperative problem in a single run. The constraints of the problem are taken into account by the corresponding projection operators and linear convergence is proven given an appropriate constant step size. The algorithm is applied to a day-ahead power management problem, posed as a multi-cluster game, and its efficiency is demonstrated by simulations.      
### 10.A Transformer-based Network for Deformable Medical Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2202.12104.pdf)
>  Deformable medical image registration plays an important role in clinical diagnosis and treatment. Recently, the deep learning (DL) based image registration methods have been widely investigated and showed excellent performance in computational speed. However, these methods cannot provide enough registration accuracy because of insufficient ability in representing both the global and local features of the moving and fixed images. To address this issue, this paper has proposed the transformer based image registration method. This method uses the distinctive transformer to extract the global and local image features for generating the deformation fields, based on which the registered image is produced in an unsupervised way. Our method can improve the registration accuracy effectively by means of self-attention mechanism and bi-level information flow. Experimental results on such brain MR image datasets as LPBA40 and OASIS-1 demonstrate that compared with several traditional and DL based registration methods, our method provides higher registration accuracy in terms of dice values.      
### 11.Data variation-aware medical image segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.12099.pdf)
>  Deep learning algorithms have become the golden standard for segmentation of medical imaging data. In most works, the variability and heterogeneity of real clinical data is acknowledged to still be a problem. One way to automatically overcome this is to capture and exploit this variation explicitly. Here, we propose an approach that improves on our previous work in this area and explain how it potentially can improve clinical acceptance of (semi-)automatic segmentation methods. In contrast to a standard neural network that produces one segmentation, we propose to use a multi-pathUnet network that produces multiple segmentation variants, presumably corresponding to the variations that reside in the dataset. Different paths of the network are trained on disjoint data subsets. Because a priori it may be unclear what variations exist in the data, the subsets should be automatically determined. This is achieved by searching for the best data partitioning with an evolutionary optimization algorithm. Because each network path can become more specialized when trained on a more homogeneous data subset, better segmentation quality can be achieved. In practical usage, various automatically produced segmentations can be presented to a medical expert, from which the preferred segmentation can be selected. In experiments with a real clinical dataset of CT scans with prostate segmentations, our approach provides an improvement of several percentage points in terms of Dice and surface Dice coefficients compared to when all network paths are trained on all training data. Noticeably, the largest improvement occurs in the upper part of the prostate that is known to be most prone to inter-observer segmentation variation.      
### 12.Deep learning-based UAV detection in the low altitude clutter background  [ :arrow_down: ](https://arxiv.org/pdf/2202.12053.pdf)
>  Unmanned aerial vehicles (UAVs) are widely used due to their low cost and versatility, but they also pose security and privacy threats. Therefore, reliable detection for low-altitude UAVs is an important issue. The strong ground clutter makes the radar echoes from small UAVs submerged in noise, resulting in low radar detection reliability. A low-altitude UAV detection method based on deep contrastive learning is proposed to address the above problems: Concretely, a low-altitude UAV radar echo model under low-altitude clutter interference is first established. Based on the echo components and the UAV Doppler domain identifiable mechanism, a time-frequency transformation method combining ZAM transform and morphological operations is used to suppress the ambiguity problem under clutter. Then feature extraction and fusion method introducing contrast learning is utilized to suppress non-target ground clutter interference. Finally, a detector relying on semantic features is designed for the reliable identification of low-altitude UAVs. The experiments carried out on both real and simulated data confirm that the proposed method can effectively suppress ground clutter and reliably extract recognizable semantic features of UAVs. The proposed method achieves lower false and missing alarms compared with recent state-of-art solutions and improves the detection accuracy by more than 5% for the same signal-to-noise ratio, which effectively improves the detection reliability.      
### 13.Evolutionary Multi-Objective Reinforcement Learning Based Trajectory Control and Task Offloading in UAV-Assisted Mobile Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2202.12028.pdf)
>  This paper studies the trajectory control and task offloading (TCTO) problem in an unmanned aerial vehicle (UAV)-assisted mobile edge computing system, where a UAV flies along a planned trajectory to collect computation tasks from smart devices (SDs). We consider a scenario that SDs are not directly connected by the base station (BS) and the UAV has two roles to play: MEC server or wireless relay. The UAV makes task offloading decisions online, in which the collected tasks can be executed locally on the UAV or offloaded to the BS for remote processing. The TCTO problem involves multi-objective optimization as its objectives are to minimize the task delay and the UAV's energy consumption, and maximize the number of tasks collected by the UAV, simultaneously. This problem is challenging because the three objectives conflict with each other. The existing reinforcement learning (RL) algorithms, either single-objective RLs or single-policy multi-objective RLs, cannot well address the problem since they cannot output multiple policies for various preferences (i.e. weights) across objectives in a single run. This paper adapts the evolutionary multi-objective RL (EMORL), a multi-policy multi-objective RL, to the TCTO problem. This algorithm can output multiple optimal policies in just one run, each optimizing a certain preference. The simulation results demonstrate that the proposed algorithm can obtain more excellent nondominated policies by striking a balance between the three objectives regarding policy quality, compared with two evolutionary and two multi-policy RL algorithms.      
### 14.Supervised Learning based Sparse Channel Estimation for RIS aided Communications  [ :arrow_down: ](https://arxiv.org/pdf/2202.11997.pdf)
>  An reconfigurable intelligent surface (RIS) can be used to establish line-of-sight (LoS) communication when the direct path is compromised, which is a common occurrence in a millimeter wave (mmWave) network. In this paper, we focus on the uplink channel estimation of a such network. We formulate this as a sparse signal recovery problem, by discretizing the angle of arrivals (AoAs) at the base station (BS). On-grid and off-grid AoAs are considered separately. In the on-grid case, we propose an algorithm to estimate the direct and RIS channels. Neural networks trained based on supervised learning is used to estimate the residual angles in the off-grid case, and the AoAs in both cases. Numerical results show the performance gains of the proposed algorithms in both cases.      
### 15.Noncoherent Massive MIMO with Embedded One-Way Function Physical Layer Security  [ :arrow_down: ](https://arxiv.org/pdf/2202.11893.pdf)
>  We propose a novel physical layer security scheme that exploits an optimization method as a one-way function. The proposed scheme builds on nonsquare differential multiple-input multiple-output (MIMO), which is capable of noncoherent detection even in massive MIMO scenarios and thus resilient against risky pilot insertion and pilot contamination attacks. In contrast to conventional nonsquare differential MIMO schemes, which require space-time projection matrices designed via highly complex, discrete, and combinatorial optimization, the proposed scheme utilizes projection matrices constructed via low-complexity continuous optimization designed to maximize the coding gain of the system. Furthermore, using a secret key generated from the true randomness nature of the wireless channel as an initial value, the proposed continuous optimization-based projection matrix construction method becomes a one way-function (in a cryptographic sense), making the proposed scheme a physical layer secure differential MIMO system. An attack algorithm to challenge the proposed scheme is also devised, which demonstrate that the security level achieved improves as the number of transmit antennas increases, even in an environment where the eavesdropper can perfectly estimate channel coefficients and experiences asymptotically large signal-to-noise ratios.      
### 16.A spectral-spatial fusion anomaly detection method for hyperspectral imagery  [ :arrow_down: ](https://arxiv.org/pdf/2202.11889.pdf)
>  In hyperspectral, high-quality spectral signals convey subtle spectral differences to distinguish similar materials, thereby providing unique advantage for anomaly detection. Hence fine spectra of anomalous pixels can be effectively screened out from heterogeneous background pixels. Since the same materials have similar characteristics in spatial and spectral dimension, detection performance can be significantly enhanced by jointing spatial and spectral information. In this paper, a spectralspatial fusion anomaly detection (SSFAD) method is proposed for hyperspectral imagery. First, original spectral signals are mapped to a local linear background space composed of median and mean with high confidence, where saliency weight and feature enhancement strategies are implemented to obtain an initial detection map in spectral domain. Futhermore, to make full use of similarity information of local background around testing pixel, a new detector is designed to extract the local similarity spatial features of patch images in spatial domain. Finally, anomalies are detected by adaptively combining the spectral and spatial detection maps. The experimental results demonstrate that our proposed method has superior detection performance than traditional methods.      
### 17.A Note on Machine Learning Approach for Computational Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2202.11883.pdf)
>  Computational imaging has been playing a vital role in the development of natural sciences. Advances in sensory, information, and computer technologies have further extended the scope of influence of imaging, making digital images an essential component of our daily lives. For the past three decades, we have witnessed phenomenal developments of mathematical and machine learning methods in computational imaging. In this note, we will review some of the recent developments of the machine learning approach for computational imaging and discuss its differences and relations to the mathematical approach. We will demonstrate how we may combine the wisdom from both approaches, discuss the merits and potentials of such a combination and present some of the new computational and theoretical challenges it brings about.      
### 18.Positive Trigonometric Polynomials on the Stability of Spatially Interconnected Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.11879.pdf)
>  This paper is devoted to the stability analysis of spatially interconnected systems (SISs) via the sum-of-squares (SOS) decomposition of positive trigonometric polynomials. For each spatial direction of SISs, three types of interconnected structures are considered. Inspired by the idea of rational parameterization and robust stabilizability function, necessary and sufficient conditions are derived for establishing the stability of SISs with two different combined topologies respectively. For these results, the primary issue concerns the global or local positivity of trigonometric polynomials. SOS decomposition and generalized trace parameterization of positive trigonometric polynomials are utilized so that the addressed problems can be quantified by two semidefinite programs (SDPs). The proposed methods are applicable to all possible interconnected structures due to the assumption of spatial reversibility. Numerical examples are given to illustrate the efficiency of the derived theoretical results.      
### 19.Nuclei panoptic segmentation and composition regression with multi-task deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.11804.pdf)
>  Nuclear segmentation, classification and quantification within Haematoxylin &amp; Eosin stained histology images enables the extraction of interpretable cell-based features that can be used in downstream explainable models in computational pathology. The Colon Nuclei Identification and Counting (CoNIC) Challenge is held to help drive forward research and innovation for automatic nuclei recognition in computational pathology. This report describes our proposed method submitted to the CoNIC challenge. Our method employs a multi-task learning framework, which performs a panoptic segmentation task and a regression task. For the panoptic segmentation task, we use encoder-decoder type deep neural networks predicting a direction map in addition to a segmentation map in order to separate neighboring nuclei into different instances      
### 20.Blind Reverberation Time Estimation in Dynamic Acoustic Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2202.11790.pdf)
>  The estimation of reverberation time from real-world signals plays a central role in a wide range of applications. In many scenarios, acoustic conditions change over time which in turn requires the estimate to be updated continuously. Previously proposed methods involving deep neural networks were mostly designed and tested under the assumption of static acoustic conditions. In this work, we show that these approaches can perform poorly in dynamically evolving acoustic environments. Motivated by a recent trend towards data-centric approaches in machine learning, we propose a novel way of generating training data and demonstrate, using an existing deep neural network architecture, the considerable improvement in the ability to follow temporal changes in reverberation time.      
### 21.Single Image Super-Resolution Methods: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2202.11763.pdf)
>  Super-resolution (SR), the process of obtaining high-resolution images from one or more low-resolution observations of the same scene, has been a very popular topic of research in the last few decades in both signal processing and image processing areas. Due to the recent developments in Convolutional Neural Networks, the popularity of SR algorithms has skyrocketed as the barrier of entry has been lowered significantly. Recently, this popularity has spread into video processing areas to the lengths of developing SR models that work in real-time. In this paper, we compare different SR models that specialize in single image processing and will take a glance at how they evolved to take on many different objectives and shapes over the years.      
### 22.Degradation-Reducing Control for Dynamically Reconfigurable Batteries  [ :arrow_down: ](https://arxiv.org/pdf/2202.11757.pdf)
>  Cascaded circuits such as modular multilevel con-verters (MMC) offer attractive qualities in reconfigurable battery applications. In contrast to conventional hard-wired dc battery packs, the MMC topology loads modules with ac current, which may lead to additional ageing of batteries. As recent studies reveal, such ageing of batteries occurs at low-frequency load ripple, and almost vanishes at high frequencies. State of the art in MMC bat-tery control focuses on state of charge and temperature balancing of individual modules. Previous methods to suppress ripple rely on slow feedback loops and low dynamics, which tends to form low-frequency patterns in the module load that negatively contribute to their ageing. This paper presents a novel module-current-oriented high-bandwidth control technique which minimizes low-frequency components in the module load spectrum. The control method respects limitations related to module data acquisition and enhanc-es the feedback bandwidth using observation techniques. We verify the proposed method experimentally on a laboratory setup and estimate the influence on the battery cells.      
### 23.Co-occurring Diseases Heavily Influence the Performance of Weakly Supervised Learning Models for Classification of Chest CT  [ :arrow_down: ](https://arxiv.org/pdf/2202.11709.pdf)
>  Despite the potential of weakly supervised learning to automatically annotate massive amounts of data, little is known about its limitations for use in computer-aided diagnosis (CAD). For CT specifically, interpreting the performance of CAD algorithms can be challenging given the large number of co-occurring diseases. This paper examines the effect of co-occurring diseases when training classification models by weakly supervised learning, specifically by comparing multi-label and multiple binary classifiers using the same training data. Our results demonstrated that the binary model outperformed the multi-label classification in every disease category in terms of AUC. However, this performance was heavily influenced by co-occurring diseases in the binary model, suggesting it did not always learn the correct appearance of the specific disease. For example, binary classification of lung nodules resulted in an AUC of &lt; 0.65 when there were no other co-occurring diseases, but when lung nodules co-occurred with emphysema, the performance reached AUC &gt; 0.80. We hope this paper revealed the complexity of interpreting disease classification performance in weakly supervised models and will encourage researchers to examine the effect of co-occurring diseases on classification performance in the future.      
### 24.Noisy Group Testing with Side Information  [ :arrow_down: ](https://arxiv.org/pdf/2202.12284.pdf)
>  Group testing has recently attracted significant attention from the research community due to its applications in diagnostic virology. An instance of the group testing problem includes a ground set of individuals which includes a small subset of infected individuals. The group testing procedure consists of a number of tests, such that each test indicates whether or not a given subset of individuals includes one or more infected individuals. The goal of the group testing procedure is to identify the subset of infected individuals with the minimum number of tests. Motivated by practical scenarios, such as testing for viral diseases, this paper focuses on the following group testing settings: (i) the group testing procedure is noisy, i.e., the outcome of the group testing procedure can be flipped with a certain probability; (ii) there is a certain amount of side information on the distribution of the infected individuals available to the group testing algorithm. The paper makes the following contributions. First, we propose a probabilistic model, referred to as an interaction model, that captures the side information about the probability distribution of the infected individuals. Next, we present a decoding scheme, based on the belief propagation, that leverages the interaction model to improve the decoding accuracy. Our results indicate that the proposed algorithm achieves higher success probability and lower false-negative and false-positive rates when compared to the traditional belief propagation especially in the high noise regime.      
### 25.A Method for Waste Segregation using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.12258.pdf)
>  Segregation of garbage is a primary concern in many nations across the world. Even though we are in the modern era, many people still do not know how to distinguish between organic and recyclable waste. It is because of this that the world is facing a major crisis of waste disposal. In this paper, we try to use deep learning algorithms to help solve this problem of waste classification. The waste is classified into two categories like organic and recyclable. Our proposed model achieves an accuracy of 94.9%. Although the other two models also show promising results, the Proposed Model stands out with the greatest accuracy. With the help of deep learning, one of the greatest obstacles to efficient waste management can finally be removed.      
### 26.A Perceptual Measure for Evaluating the Resynthesis of Automatic Music Transcriptions  [ :arrow_down: ](https://arxiv.org/pdf/2202.12257.pdf)
>  This study focuses on the perception of music performances when contextual factors, such as room acoustics and instrument, change. We propose to distinguish the concept of "performance" from the one of "interpretation", which expresses the "artistic intention". Towards assessing this distinction, we carried out an experimental evaluation where 91 subjects were invited to listen to various audio recordings created by resynthesizing MIDI data obtained through Automatic Music Transcription (AMT) systems and a sensorized acoustic piano. During the resynthesis, we simulated different contexts and asked listeners to evaluate how much the interpretation changes when the context changes. Results show that: (1) MIDI format alone is not able to completely grasp the artistic intention of a music performance; (2) usual objective evaluation measures based on MIDI data present low correlations with the average subjective evaluation. To bridge this gap, we propose a novel measure which is meaningfully correlated with the outcome of the tests. In addition, we investigate multimodal machine learning by providing a new score-informed AMT method and propose an approximation algorithm for the $p$-dispersion problem.      
### 27.Flat latent manifolds for music improvisation between human and machine  [ :arrow_down: ](https://arxiv.org/pdf/2202.12243.pdf)
>  The use of machine learning in artistic music generation leads to controversial discussions of the quality of art, for which objective quantification is nonsensical. We therefore consider a music-generating algorithm as a counterpart to a human musician, in a setting where reciprocal improvisation is to lead to new experiences, both for the musician and the audience. To obtain this behaviour, we resort to the framework of recurrent Variational Auto-Encoders (VAE) and learn to generate music, seeded by a human musician. In the learned model, we generate novel musical sequences by interpolation in latent space. Standard VAEs however do not guarantee any form of smoothness in their latent representation. This translates into abrupt changes in the generated music sequences. To overcome these limitations, we regularise the decoder and endow the latent space with a flat Riemannian manifold, i.e., a manifold that is isometric to the Euclidean space. As a result, linearly interpolating in the latent space yields realistic and smooth musical changes that fit the type of machine--musician interactions we aim for. We provide empirical evidence for our method via a set of experiments on music datasets and we deploy our model for an interactive jam session with a professional drummer. The live performance provides qualitative evidence that the latent representation can be intuitively interpreted and exploited by the drummer to drive the interplay. Beyond the musical application, our approach showcases an instance of human-centred design of machine-learning models, driven by interpretability and the interaction with the end user.      
### 28.Energy-Efficient Transmission Range and Duration for Cognitive Radio Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.12201.pdf)
>  Cognitive Radio (CR) promises an efficient utilization of radio spectrum resources by enabling dynamic spectrum access to overcome the spectrum scarcity problem. Cognitive Radio Sensor Networks (CRSNs) are one type of Wireless Sensor Networks (WSNs) equipped with CR capabilities. CRSN nodes need to operate energy-efficiently to extend network lifetime due to their limited battery capacity. In this paper, for the first time in literature, we formulate the problem of finding a common energy-efficient transmission range and transmission duration for all CRSN nodes and network deployment that would minimize the energy consumed per goodput per meter toward the sink in a greedy forwarding scenario. Results reveal non-trivial relations for energy-efficient CRSN transmission range and duration as a function of nine critical network parameters such as primary user activity levels. These relations provide valuable insights for detailed CRSN designs prior to deployment.      
### 29.SonOpt: Sonifying Bi-objective Population-Based Optimization Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2202.12187.pdf)
>  We propose SonOpt, the first (open source) data sonification application for monitoring the progress of bi-objective population-based optimization algorithms during search, to facilitate algorithm understanding. SonOpt provides insights into convergence/stagnation of search, the evolution of the approximation set shape, location of recurring points in the approximation set, and population diversity. The benefits of data sonification have been shown for various non-optimization related monitoring tasks. However, very few attempts have been made in the context of optimization and their focus has been exclusively on single-objective problems. In comparison, SonOpt is designed for bi-objective optimization problems, relies on objective function values of non-dominated solutions only, and is designed with the user (listener) in mind; avoiding convolution of multiple sounds and prioritising ease of familiarizing with the system. This is achieved using two sonification paths relying on the concepts of wavetable and additive synthesis. This paper motivates and describes the architecture of SonOpt, and then validates SonOpt for two popular multi-objective optimization algorithms (NSGA-II and MOEA/D). Experience SonOpt yourself via <a class="link-external link-https" href="https://github.com/tasos-a/SonOpt-1.0" rel="external noopener nofollow">this https URL</a> .      
### 30.Tube Stochastic Optimal Control for Nonlinear Constrained Trajectory Optimization Problems  [ :arrow_down: ](https://arxiv.org/pdf/2202.12158.pdf)
>  Recent low-thrust space missions have highlighted the importance of designing trajectories that are robust against uncertainties. In its complete form, this process is formulated as a nonlinear constrained stochastic optimal control problem. This problem is among the most complex in control theory, and no practically applicable method to low-thrust trajectory optimization problems has been proposed to date. This paper presents a new algorithm to solve stochastic optimal control problems with nonlinear systems and constraints. The proposed algorithm uses the unscented transform to convert a stochastic optimal control problem into a deterministic problem, which is then solved by trajectory optimization methods such as differential dynamic programming. Two numerical examples, one of which applies the proposed method to low-thrust trajectory design, illustrate that it automatically introduces margins that improve robustness. Finally, Monte Carlo simulations are used to evaluate the robustness and optimality of the solution.      
### 31.Word Segmentation on Discovered Phone Units with Dynamic Programming and Self-Supervised Scoring  [ :arrow_down: ](https://arxiv.org/pdf/2202.11929.pdf)
>  Recent work on unsupervised speech segmentation has used self-supervised models with a phone segmentation module and a word segmentation module that are trained jointly. This paper compares this joint methodology with an older idea: bottom-up phone-like unit discovery is performed first, and symbolic word segmentation is then performed on top of the discovered units (without influencing the lower level). I specifically describe a duration-penalized dynamic programming (DPDP) procedure that can be used for either phone or word segmentation by changing the self-supervised scoring network that gives segment costs. For phone discovery, DPDP is applied with a contrastive predictive coding clustering model, while for word segmentation it is used with an autoencoding recurrent neural network. The two models are chained in order to segment speech. This approach gives comparable word segmentation results to state-of-the-art joint self-supervised models on an English benchmark. On French and Mandarin data, it outperforms previous systems on the ZeroSpeech benchmarks. Analysis shows that the chained DPDP system segments shorter filler words well, but longer words might require an external top-down signal.      
### 32.Phase Continuity: Learning Derivatives of Phase Spectrum for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2202.11918.pdf)
>  Modern neural speech enhancement models usually include various forms of phase information in their training loss terms, either explicitly or implicitly. However, these loss terms are typically designed to reduce the distortion of phase spectrum values at specific frequencies, which ensures they do not significantly affect the quality of the enhanced speech. In this paper, we propose an effective phase reconstruction strategy for neural speech enhancement that can operate in noisy environments. Specifically, we introduce a phase continuity loss that considers relative phase variations across the time and frequency axes. By including this phase continuity loss in a state-of-the-art neural speech enhancement system trained with reconstruction loss and a number of magnitude spectral losses, we show that our proposed method further improves the quality of enhanced speech signals over the baseline, especially when training is done jointly with a magnitude spectrum loss.      
### 33.On Nash-Stackelberg-Nash Games under Decision-Dependent Uncertainties: Model and Equilibrium  [ :arrow_down: ](https://arxiv.org/pdf/2202.11880.pdf)
>  In this paper, we discuss a class of two-stage hierarchical games with multiple leaders and followers, which is called Nash-Stackelberg-Nash (N-S-N) games. Particularly, we consider N-S-N games under decision-dependent uncertainties (DDUs). DDUs refer to the uncertainties that are affected by the strategies of decision-makers and have been rarely addressed in game equilibrium analysis. In this paper, we first formulate the N-S-N games with DDUs of complete ignorance, where the interactions between the players and DDUs are characterized by uncertainty sets that depend parametrically on the players' strategies. Then, a rigorous definition for the equilibrium of the game is established by consolidating generalized Nash equilibrium and Pareto-Nash equilibrium. Afterward, we prove the existence of the equilibrium of N-S-N games under DDUs by applying Kakutani's fixed-point theorem. Finally, an illustrative example is provided to show the impact of DDUs on the equilibrium of N-S-N games.      
### 34.Robust Transmission Design for RIS-assisted Secure Multiuser Communication Systems in the Presence of Hardware Impairments  [ :arrow_down: ](https://arxiv.org/pdf/2202.11860.pdf)
>  This paper investigates reconfigurable intelligent surface (RIS)-assisted secure multiuser communication systems subject to hardware impairments (HIs). We jointly optimize the beamforming vectors at the base station (BS) and the phase shifts of the reflecting elements at the RIS so as to maximize the weighted minimum secrecy rate (WMSR), subject to both transmission power constraints at the BS and unit-modulus constraints at the RIS. To address the formulated optimization problem, we first decouple it into two tractable subproblems and then use the block coordinate descent (BCD) method to alternately optimize the subproblems. Two different methods are proposed to solve the two obtained subproblems. The first method transforms each subproblem into a second order cone programming (SOCP) problem, which can be directly solved using CVX. The second method leverages the Minorization- Maximization (MM) algorithm. Specifically, we first derive a concave approximation function, which is a lower bound of the original objective function, and then the two subproblems are transformed into two simple surrogate problems with closedform solutions. Simulation results verify the performance gains of the proposed robust transmission method over existing nonrobust designs. In addition, the MM algorithm is shown to have much lower complexity than the SOCP-based algorithm.      
### 35.Computational 3D microscopy with optical coherence refraction tomography  [ :arrow_down: ](https://arxiv.org/pdf/2202.11837.pdf)
>  Optical coherence tomography (OCT) has seen widespread success as an in vivo clinical diagnostic 3D imaging modality, impacting areas including ophthalmology, cardiology, and gastroenterology. Despite its many advantages, such as high sensitivity, speed, and depth penetration, OCT suffers from several shortcomings that ultimately limit its utility as a 3D microscopy tool, such as its pervasive coherent speckle noise and poor lateral resolution required to maintain millimeter-scale imaging depths. Here, we present 3D optical coherence refraction tomography (OCRT), a computational extension of OCT which synthesizes an incoherent contrast mechanism by combining multiple OCT volumes, acquired across two rotation axes, to form a resolution-enhanced, speckle-reduced, refraction-corrected 3D reconstruction. Our label-free computational 3D microscope features a novel optical design incorporating a parabolic mirror to enable the capture of 5D plenoptic datasets, consisting of millimetric 3D fields of view over up to $\pm75^\circ$ without moving the sample. We demonstrate that 3D OCRT reveals 3D features unobserved by conventional OCT in fruit fly, zebrafish, and mouse samples.      
### 36.Differentially Private Speaker Anonymization  [ :arrow_down: ](https://arxiv.org/pdf/2202.11823.pdf)
>  Sharing real-world speech utterances is key to the training and deployment of voice-based services. However, it also raises privacy risks as speech contains a wealth of personal data. Speaker anonymization aims to remove speaker information from a speech utterance while leaving its linguistic and prosodic attributes intact. State-of-the-art techniques operate by disentangling the speaker information (represented via a speaker embedding) from these attributes and re-synthesizing speech based on the speaker embedding of another speaker. Prior research in the privacy community has shown that anonymization often provides brittle privacy protection, even less so any provable guarantee. In this work, we show that disentanglement is indeed not perfect: linguistic and prosodic attributes still contain speaker information. We remove speaker information from these attributes by introducing differentially private feature extractors based on an autoencoder and an automatic speech recognizer, respectively, trained using noise layers. We plug these extractors in the state-of-the-art anonymization pipeline and generate, for the first time, differentially private utterances with a provable upper bound on the speaker information they contain. We evaluate empirically the privacy and utility resulting from our differentially private speaker anonymization approach on the LibriSpeech data set. Experimental results show that the generated utterances retain very high utility for automatic speech recognition training and inference, while being much better protected against strong adversaries who leverage the full knowledge of the anonymization process to try to infer the speaker identity.      
### 37.Design and experimental investigation of a vibro-impact self-propelled capsule robot with orientation control  [ :arrow_down: ](https://arxiv.org/pdf/2202.11784.pdf)
>  This paper presents a novel design and experimental investigation for a self-propelled capsule robot that can be used for painless colonoscopy during a retrograde progression from the patient's rectum. The steerable robot is driven forward and backward via its internal vibration and impact with orientation control by using an electromagnetic actuator. The actuator contains four sets of coils and a shaft made by permanent magnet. The shaft can be excited linearly in a controllable and tilted angle, so guide the progression orientation of the robot. Two control strategies are studied in this work and compared via simulation and experiment. Extensive results are presented to demonstrate the progression efficiency of the robot and its potential for robotic colonoscopy.      
### 38.Safe Control with Learned Certificates: A Survey of Neural Lyapunov, Barrier, and Contraction methods  [ :arrow_down: ](https://arxiv.org/pdf/2202.11762.pdf)
>  Learning-enabled control systems have demonstrated impressive empirical performance on challenging control problems in robotics, but this performance comes at the cost of reduced transparency and lack of guarantees on the safety or stability of the learned controllers. In recent years, new techniques have emerged to provide these guarantees by learning certificates alongside control policies -- these certificates provide concise, data-driven proofs that guarantee the safety and stability of the learned control system. These methods not only allow the user to verify the safety of a learned controller but also provide supervision during training, allowing safety and stability requirements to influence the training process itself. In this paper, we provide a comprehensive survey of this rapidly developing field of certificate learning. We hope that this paper will serve as an accessible introduction to the theory and practice of certificate learning, both to those who wish to apply these tools to practical robotics problems and to those who wish to dive more deeply into the theory of learning for control.      
### 39.ML-based Anomaly Detection in Optical Fiber Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2202.11756.pdf)
>  Secure and reliable data communication in optical networks is critical for high-speed internet. We propose a data driven approach for the anomaly detection and faults identification in optical networks to diagnose physical attacks such as fiber breaks and optical tapping. The proposed methods include an autoencoder-based anomaly detection and an attention-based bidirectional gated recurrent unit algorithm for the fiber fault identification and localization. We verify the efficiency of our methods by experiments under various attack scenarios using real operational data.      
