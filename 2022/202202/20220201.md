# ArXiv eess --Tue, 1 Feb 2022
### 1.RC Filter Design for Wireless Power Transfer: A Fourier Series Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.13411.pdf)
>  In this letter, we study the impact of the low-pass resistor-capacitor (RC) filter on radio frequency (RF) wireless power transfer (WPT). The RC filter influences both the RF bandwidth by removing the harmonics as well as the ripple voltage at the output of the rectifier. In particular, a large (small) RC time constant, reduces (increases) the ripple but decreases (enhances) the direct-current (DC) component. By following a Fourier series approach, we obtain closed-form expressions for the rectifier's output voltage, the RC filter's output as well as the DC voltage. Our analytical framework provides a complete characterization of the RC filter's impact on the WPT performance. We show that this complete and tractable analytical framework is suitable for the proper design of the RC filter in WPT systems.      
### 2.Spectral image clustering on dual-energy CT scans using functional regression mixtures  [ :arrow_down: ](https://arxiv.org/pdf/2201.13398.pdf)
>  Dual-energy computed tomography (DECT) is an advanced CT scanning technique enabling material characterization not possible with conventional CT scans. It allows the reconstruction of energy decay curves at each 3D image voxel, representing varying image attenuation at different effective scanning energy levels. In this paper, we develop novel functional data analysis (FDA) techniques and adapt them to the analysis of DECT decay curves. More specifically, we construct functional mixture models that integrate spatial context in mixture weights, with mixture component densities being constructed upon the energy decay curves as functional observations. We design unsupervised clustering algorithms by developing dedicated expectation maximization (EM) algorithms for the maximum likelihood estimation of the model parameters. To our knowledge, this is the first article to adapt statistical FDA tools and model-based clustering to take advantage of the full spectral information provided by DECT. We evaluate our methods on 91 head and neck cancer DECT scans. We compare our unsupervised clustering results to tumor contours traced manually by radiologists, as well as to several baseline algorithms. Given the inter-rater variability even among experts at delineating head and neck tumors, and given the potential importance of tissue reactions surrounding the tumor itself, our proposed methodology has the potential to add value in downstream machine learning applications for clinical outcome prediction based on DECT data in head and neck cancer.      
### 3.Bi-Directional Semi-Supervised Training of Convolutional Neural Networks for Ultrasound Elastography Displacement Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2201.13340.pdf)
>  The performance of ultrasound elastography (USE) heavily depends on the accuracy of displacement estimation. Recently, Convolutional Neural Networks (CNN) have shown promising performance in optical flow estimation and have been adopted for USE displacement estimation. Networks trained on computer vision images are not optimized for USE displacement estimation since there is a large gap between the computer vision images and the high-frequency Radio Frequency (RF) ultrasound data. Many researchers tried to adopt the optical flow CNNs to USE by applying transfer learning to improve the performance of CNNs for USE. However, the ground truth displacement in real ultrasound data is unknown, and simulated data exhibits a domain shift compared to the real data and is also computationally expensive to generate. To resolve this issue, semi-supervised methods have been proposed wherein the networks pre-trained on computer vision images are fine-tuned using real ultrasound data. In this paper, we employ a semi-supervised method by exploiting the first and second-order derivatives of the displacement field for the regularization. We also modify the network structure to estimate both forward and backward displacements, and propose to use consistency between the forward and backward strains as an additional regularizer to further enhance the performance. We validate our method using several experimental phantom and in vivo data. We also show that the network fine-tuned by our proposed method using experimental phantom data performs well on in vivo data similar to the network fine-tuned on in vivo data. Our results also show that the proposed method outperforms current deep learning methods and is comparable to computationally expensive optimization-based algorithms.      
### 4.Steady-State Error Compensation in Reference Tracking and Disturbance Rejection Problems for Reinforcement Learning-Based Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.13331.pdf)
>  Reinforcement learning (RL) is a promising, upcoming topic in automatic control applications. Where classical control approaches require a priori system knowledge, data-driven control approaches like RL allow a model-free controller design procedure, rendering them emergent techniques for systems with changing plant structures and varying parameters. While it was already shown in various applications that the transient control behavior for complex systems can be sufficiently handled by RL, the challenge of non-vanishing steady-state control errors remains, which arises from the usage of control policy approximations and finite training times. To overcome this issue, an integral action state augmentation (IASA) for actor-critic-based RL controllers is introduced that mimics an integrating feedback, which is inspired by the delta-input formulation within model predictive control. This augmentation does not require any expert knowledge, leaving the approach model free. As a result, the RL controller learns how to suppress steady-state control deviations much more effectively. Two exemplary applications from the domain of electrical energy engineering validate the benefit of the developed method both for reference tracking and disturbance rejection. In comparison to a standard deep deterministic policy gradient (DDPG) setup, the suggested IASA extension allows to reduce the steady-state error by up to 52 $\%$ within the considered validation scenarios.      
### 5.End-to-End Quality-of-Service Assurance with Autonomous Systems: 5G/6G Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2201.13300.pdf)
>  Providing differentiated services to meet the unique requirements of different use cases is a major goal of the fifth generation (5G) telecommunication networks and will be even more critical for future 6G systems. Fulfilling this goal requires the ability to assure quality of service (QoS) end to end (E2E), which remains a challenge. A key factor that makes E2E QoS assurance difficult in a telecommunication system is that access networks (ANs) and core networks (CNs) manage their resources autonomously. So far, few results have been available that can ensure E2E QoS over autonomously managed ANs and CNs. Existing techniques rely predominately on each subsystem to meet static local QoS budgets with no recourse in case any subsystem fails to meet its local budgets and, hence will have difficulty delivering E2E assurance. Moreover, most existing distributed optimization techniques that can be applied to assure E2E QoS over autonomous subsystems require the subsystems to exchange sensitive information such as their local decision variables. This paper presents a novel framework and a distributed algorithm that can enable ANs and CNs to autonomously "cooperate" with each other to dynamically negotiate their local QoS budgets and to collectively meet E2E QoS goals by sharing only their estimates of the global constraint functions, without disclosing their local decision variables. We prove that this new distributed algorithm converges to an optimal solution almost surely, and also present numerical results to demonstrate that the convergence occurs quickly even with measurement noise.      
### 6.A Safe Control Architecture Based on a Model Predictive Control Supervisor for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2201.13298.pdf)
>  This paper presents a novel, safe control architecture (SCA) for controlling an important class of systems: safety-critical systems. Ensuring the safety of control decisions has always been a challenge in automatic control. The proposed SCA aims to address this challenge by using a Model Predictive Controller (MPC) that acts as a supervisor for the operating controller, in the sense that the MPC constantly checks the safety of the control inputs generated by the operating controller and intervenes if the control input is predicted to lead to a hazardous situation in the foreseeable future invariably. Then an appropriate backup scheme can be activated, e.g., a degraded control mechanism, the transfer of the system to a safe state, or a warning signal issued to a human supervisor. For a proof of concept, the proposed SCA is applied to an autonomous driving scenario, where it is illustrated and compared in different obstacle avoidance scenarios. A major challenge of the SCA lies in the mismatch between the MPC prediction model and the real system, for which possible remedies are explored.      
### 7.Impact of Naturalistic Field Acoustic Environments on Forensic Text-independent Speaker Verification System  [ :arrow_down: ](https://arxiv.org/pdf/2201.13246.pdf)
>  Audio analysis for forensic speaker verification offers unique challenges in system performance due in part to data collected in naturalistic field acoustic environments where location/scenario uncertainty is common in the forensic data collection process. Forensic speech data as potential evidence can be obtained in random naturalistic environments resulting in variable data quality. Speech samples may include variability due to vocal efforts such as yelling over 911 emergency calls, whereas others might be whisper or situational stressed voice in a field location or interview room. Such speech variability consists of intrinsic and extrinsic characteristics and makes forensic speaker verification a complicated and daunting task. Extrinsic properties include recording equipment such as microphone type and placement, ambient noise, room configuration including reverberation, and other environmental scenario-based issues. Some factors, such as noise and non-target speech, will impact the verification system performance by their mere presence. To investigate the impact of field acoustic environments, we performed a speaker verification study based on the CRSS-Forensic corpus with audio collected from 8 field locations including police interviews. This investigation includes an analysis of the impact of seven unseen acoustic environments on speaker verification system performance using an x-Vector system.      
### 8.A holistic approach for rapid development of IIoT systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.13243.pdf)
>  While lots of research has been conducted on the architecture of Industrial Internet of Things (IIoT) systems, concepts of structuring their development processes are missing. Therefore, we propose a holistic approach supporting organizations in rapid development of IIoT systems. It includes the structuring of the development process into multiple projects sharing project conventions. Utilizing a single configurable build script for all projects, our goal is to make the integration of code from various projects into broader IIoT systems easy and the systems decomposable with minimal effort.      
### 9.Risk-based Design of Regular Plane Frames Subject to Damage by Abnormal Events: a Conceptual Study  [ :arrow_down: ](https://arxiv.org/pdf/2201.13221.pdf)
>  Constructed facilities should be robust with respect to the loss of load-bearing elements due to abnormal events. Yet, strengthening structures to withstand such damage has a significant impact on construction costs. Strengthening costs should be justified by the threat and should result in smaller expected costs of progressive collapse. In regular frame structures, beams and columns compete for the strengthening budget. In this paper, we present a risk-based formulation to address the optimal design of regular plane frames under element loss conditions. We address the threat probabilities for which strengthening has better cost-benefit than usual design, for different frame configurations, and study the impacts of strengthening extent and cost. The risk-based optimization reveals optimum points of compromise between competing failure modes: local bending of beams, local crushing of columns, and global pancake collapse, for frames of different aspect ratios. The conceptual study is based on a simple analytical model for progressive collapse, but it provides relevant insight for the design and strengthening of real structures.      
### 10.Computational Scatter Correction for High-Resolution Flat-Panel CT Based on a Fast Monte Carlo Photon Transport Model  [ :arrow_down: ](https://arxiv.org/pdf/2201.13191.pdf)
>  In computed tomography (CT) reconstruction, scattering causes server quality degradation of the reconstructed CT images by introducing streaks and cupping artifacts which reduce the detectability of low contrast objects. Monte Carlo (MC) simulation is considered as the most accurate approach for scatter estimation. However, the existing MC estimators are computationally expensive especially for the considered high-resolution flat-panel CT. In this paper, we propose a fast and accurate photon transport model which describes the physics within the 1 keV to 1 MeV range using multiple controllable key parameters. Based on this model, scatter computation for a single projection can be completed within a range of few seconds under well-defined model parameters. Smoothing and interpolation are performed on the estimated scatter to accelerate the scatter calculation without compromising accuracy too much compared to measured near scatter-free projection images. Combining the scatter estimation with the filtered backprojection (FBP), scatter correction is performed effectively in an iterative manner. In order to evaluate the proposed MC model, we have conducted extensive experiments on the simulated data and real-world high-resolution flat-panel CT. Comparing to the state-of-the-art MC simulators, our photon transport model achieved a 202$\times$ speed-up on a four GPU system comparing to the multi-threaded state-of-the-art EGSnrc MC simulator. Besides, it is shown that for real-world high-resolution flat-panel CT, scatter correction with sufficient accuracy is accomplished within one to three iterations using a FBP and a forward projection computed with the proposed fast MC photon transport model.      
### 11.Sparse algorithms for EEG source localization  [ :arrow_down: ](https://arxiv.org/pdf/2201.13181.pdf)
>  Source localization using EEG is important in diagnosing various physiological and psychiatric diseases related to the brain. The high temporal resolution of EEG helps medical professionals assess the internal physiology of the brain in a more informative way. The internal sources are obtained from EEG by an inversion process. The number of sources in the brain outnumbers the number of measurements. In this article, a comprehensive review of the state of the art sparse source localization methods in this field is presented. A recently developed method, certainty based reduced sparse solution (CARSS), is implemented and is examined. A vast comparative study is performed using a sixty four channel setup involving two source spaces. The first source space has 5004 sources and the other has 2004 sources. Four test cases with one, three, five, and seven simulated active sources are considered. Two noise levels are also being added to the noiseless data. The CARSS is also evaluated. The results are examined. A real EEG study is also attempted.      
### 12.Probabilistically Robust Optimization of IRS-aided SWIPT Under Coordinated Spectrum Underlay  [ :arrow_down: ](https://arxiv.org/pdf/2201.13151.pdf)
>  This study considers the Joint Transmit/Reflect Beamforming and Power Splitting (JTRBPS) optimization problem in a spectrum underlay setting, such that the transmit sum-energy of the intelligent reflecting surface (IRS)-aided secondary transmitter (ST) is minimized subject to the quality-of-service requirements of the PS-simultaneous wireless information and power transfer (SWIPT) secondary receivers and the interference constraints of the primary receivers (PR). The interference at the PRs caused by the reception of IRS-reflected signals sent by the primary transmitter is taken into account. A coordinated channel state information (CSI) acquisition protocol is proposed. Next, assuming availability at the ST of perfect CSI for all direct and IRS-cascaded transmitter--receiver channels, two penalty-based iterative algorithms are developed: an alternating minimization algorithm that involves semi-definite relaxation in JTBPS design and successive convex approximation in RB optimization, and a block coordinate descent algorithm that employs the Riemannian conjugate gradient algorithm in RB updates. Finally, an outage-constrained robust design under imperfect CSI is devised. Numerical simulations highlight the performance gains of the proposed strategies over benchmarks, corroborate the benefits of using an IRS, and provide valuable insights.      
### 13.Threshold Independent Evaluation of Sound Event Detection Scores  [ :arrow_down: ](https://arxiv.org/pdf/2201.13148.pdf)
>  Performing an adequate evaluation of sound event detection (SED) systems is far from trivial and is still subject to ongoing research. The recently proposed polyphonic sound detection (PSD)-receiver operating characteristic (ROC) and PSD score (PSDS) make an important step into the direction of an evaluation of SED systems which is independent from a certain decision threshold. This allows to obtain a more complete picture of the overall system behavior which is less biased by threshold tuning. Yet, the PSD-ROC is currently only approximated using a finite set of thresholds. The choice of the thresholds used in approximation, however, can have a severe impact on the resulting PSDS. In this paper we propose a method which allows for computing system performance on an evaluation set for all possible thresholds jointly, enabling accurate computation not only of the PSD-ROC and PSDS but also of other collar-based and intersection-based performance curves. It further allows to select the threshold which best fulfills the requirements of a given application. Source code is publicly available in our SED evaluation package sed_scores_eval.      
### 14.PostGAN: A GAN-Based Post-Processor to Enhance the Quality of Coded Speech  [ :arrow_down: ](https://arxiv.org/pdf/2201.13093.pdf)
>  The quality of speech coded by transform coding is affected by various artefacts especially when bitrates to quantize the frequency components become too low. In order to mitigate these coding artefacts and enhance the quality of coded speech, a post-processor that relies on a-priori information transmitted from the encoder is traditionally employed at the decoder side. In recent years, several data-driven post-postprocessors have been proposed which were shown to outperform traditional approaches. In this paper, we propose PostGAN, a GAN-based neural post-processor that operates in the sub-band domain and relies on the U-Net architecture and a learned affine transform. It has been tested on the recently standardized low-complexity, low-delay bluetooth codec (LC3) for wideband speech at the lowest bitrate (16 kbit/s). Subjective evaluations and objective scores show that the newly introduced post-processor surpasses previously published methods and can improve the quality of coded speech by around 20 MUSHRA points.      
### 15.Implementation of an Elastic Reconfigurable Optical Add/Drop Multiplexer based on Subcarriers for Application in Optical Multichannel Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.12972.pdf)
>  We designed a Reconfigurable Optical Add/Drop Multiplexer (ROADM) based on a subcarrier add/drop node in an optical communication system that is suitable for all kinds of optical multiplexing signals. To achieve this goal, at first, we designed an optical comb generator based on a dual-drive Mach Zehnder. The new ROADM setup is validated by a 100 Gb/s 4-subcarrier. In the final step, we checked the performance of the system in terms of the bit error rate (BER) versus optical signal-to-noise ratio (OSNR) to verify the add/drop operation had been successfully performed at 10-9 and is suitable to apply in an all-optical multiplexing technique.      
### 16.Lyapunov Conditions for Input-to-State Stability of Hybrid Systems with Memory  [ :arrow_down: ](https://arxiv.org/pdf/2201.12862.pdf)
>  This paper studies input-to-state stability for hybrid systems with memory, which models hybrid dynamics affected by time delays. Using both Lyapunov-Razumikhin functions and Lyapunov-Krasovskii functionals, Lyapunov-based sufficient conditions are established for input-to-state stability. In addition, further extensions and relaxations are proposed for special cases, such as the stable flow/jump cases and the cases that Lyapunov functions do not decrease strictly during flow/jumps. Finally, two examples are used to illustrate the developed results.      
### 17.Adaptive Contraction-based Control of Uncertain Nonlinear Processes using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.12816.pdf)
>  Driven by the flexible manufacturing trend in the process control industry and the uncertain nature of chemical process models, this article aims to achieve offset-free tracking for a family of uncertain nonlinear systems (e.g., using process models with parametric uncertainties) with adaptable performance. The proposed adaptive control approach incorporates into the control loop an adaptive neural network embedded contraction-based controller (to ensure convergence to time-varying references) and an online parameter identification module coupled with reference generation (to ensure modelled parameters converge those of the physical system). The integrated learning and control approach involves training a state and parameter dependent neural network to learn a contraction metric parameterized by the uncertain parameter and a differential feedback gain. This neural network is then embedded in an adaptive contraction-based control law which is updated by parameter estimates online. As uncertain parameter estimates converge to the corresponding physical values, offset-free tracking, simultaneously with improved convergence rates, can be achieved, resulting in a flexible, efficient and less conservative approach to the reference tracking control of uncertain nonlinear processes. An illustrative example is included to demonstrate the overall approach. An illustrative example is included to demonstrate the overall approach.      
### 18.Electrolyte Flow Rate Control for Vanadium Redox Flow Batteries using the Linear Parameter Varying Framework  [ :arrow_down: ](https://arxiv.org/pdf/2201.12812.pdf)
>  In this article, an electrolyte flow rate control approach is developed for an all-vanadium redox flow battery (VRB) system based on the linear parameter varying (LPV) framework. The electrolyte flow rate is regulated to provide a trade-off between stack voltage efficiency and pumping energy losses, so as to achieve optimal battery energy efficiency. The nonlinear process model is embedded in a linear parameter varying state-space description and a set of state feedback controllers are designed to handle fluctuations in current during both charging and discharging. Simulation studies have been conducted under different operating conditions to demonstrate the performance of the proposed approach. This control approach was further implemented on a laboratory scale VRB system.      
### 19.TransBTSV2: Wider Instead of Deeper Transformer for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.12785.pdf)
>  Transformer, benefiting from global (long-range) information modeling using self-attention mechanism, has been successful in natural language processing and computer vision recently. Convolutional Neural Networks, capable of capturing local features, are unable to model explicit long-distance dependencies from global feature space. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we exploit Transformer in 3D CNN for 3D medical image volumetric segmentation and propose a novel network named TransBTSV2 based on the encoder-decoder structure. Different from our original TransBTS, the proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but focuses on general medical image segmentation, providing a strong and efficient 3D baseline for volumetric segmentation of medical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of medical images without any pre-training. With the proposed insight to redesign the internal structure of Transformer and the introduced Deformable Bottleneck Module, a highly efficient architecture is achieved with superior performance. Extensive experimental results on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results as compared to the state-of-the-art methods for the segmentation of brain tumor, liver tumor as well as kidney tumor. Code is available at <a class="link-external link-https" href="https://github.com/Wenxuan-1119/TransBTS" rel="external noopener nofollow">this https URL</a>.      
### 20.Practical Noise Simulation for RGB Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.12773.pdf)
>  This document describes a noise generator that simulates realistic noise found in smartphone cameras. The generator simulates Poissonian-Gaussian noise whose parameters have been estimated on the Smartphone Image Denoising Dataset (SIDD). The generator is available online, and is currently being used in compressed-domain denoising exploration experiments in JPEG AI.      
### 21.HGCN: harmonic gated compensation network for speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2201.12755.pdf)
>  Mask processing in the time-frequency (T-F) domain through the neural network has been one of the mainstreams for single-channel speech enhancement. However, it is hard for most models to handle the situation when harmonics are partially masked by noise. To tackle this challenge, we propose a harmonic gated compensation network (HGCN). We design a high-resolution harmonic integral spectrum to improve the accuracy of harmonic locations prediction. Then we add voice activity detection (VAD) and voiced region detection (VRD) to the convolutional recurrent network (CRN) to filter harmonic locations. Finally, the harmonic gating mechanism is used to guide the compensation model to adjust the coarse results from CRN to obtain the refinedly enhanced results. Our experiments show HGCN achieves substantial gain over a number of advanced approaches in the community.      
### 22.Joint Vehicular Localization and Reflective Mapping Based on Team Channel-SLAM  [ :arrow_down: ](https://arxiv.org/pdf/2201.12726.pdf)
>  This paper addresses high-resolution vehicle positioning and tracking. In recent work, it was shown that a fleet of independent but neighboring vehicles can cooperate for the task of localization by capitalizing on the existence of common surrounding reflectors, using the concept of Team Channel-SLAM. This approach exploits an initial (e.g. GPS-based) vehicle position information and allows subsequent tracking of vehicles by exploiting the shared nature of virtual transmitters associated to the reflecting surfaces. In this paper, we show that the localization can be greatly enhanced by joint sensing and mapping of reflecting surfaces. To this end, we propose a combined approach coined Team Channel-SLAM Evolution (TCSE) which exploits the intertwined relation between (i) the position of virtual transmitters, (ii) the shape of reflecting surfaces, and (iii) the paths described by the radio propagation rays, in order to achieve high-resolution vehicle localization. Overall, TCSE yields a complete picture of the trajectories followed by dominant paths together with a mapping of reflecting surfaces. While joint localization and mapping is a well researched topic within robotics using inputs such as radar and vision, this paper is first to demonstrate such an approach within mobile networking framework based on radio data.      
### 23.Few-Shot Transfer Learning for Device-Free Fingerprinting Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2201.12656.pdf)
>  Device-free wireless indoor localization is an essential technology for the Internet of Things (IoT), and fingerprint-based methods are widely used. A common challenge to fingerprint-based methods is data collection and labeling. This paper proposes a few-shot transfer learning system that uses only a small amount of labeled data from the current environment and reuses a large amount of existing labeled data previously collected in other environments, thereby significantly reducing the data collection and labeling cost for localization in each new environment. The core method lies in graph neural network (GNN) based few-shot transfer learning and its modifications. Experimental results conducted on real-world environments show that the proposed system achieves comparable performance to a convolutional neural network (CNN) model, with 40 times fewer labeled data.      
### 24.On Optimizing Shared-ride Mobility Services with Walking Legs  [ :arrow_down: ](https://arxiv.org/pdf/2201.12639.pdf)
>  Shared-ride mobility services that incorporate traveler walking legs aim to reduce vehicle-kilometers-travelled (VKT), vehicle-hours-travelled (VHT), request rejections, fleet size, or some combination of these factors, compared to door-to-door (D2D) shared-ride services. This paper provides a review of shared-ride services with walking legs (SRSWL), particularly the studies in the literature that model the operational problem(s) associated with SRSWL. The paper describes the operational and societal benefits of SRSWL as well as compares the SRSWL to circuitous D2D shared-ride services, ride-hailing services, and fixed-route transit services, in terms of VKT and traveler walking distance. The paper then delineates the operational subproblems associated with the SRSWL and discusses their computational complexity. Additionally, the review classifies configurations of SRSWL based on flexibility in assigning travelers to pickup and drop-off locations. The paper also discusses four modelling challenge: short-distance person trips, drop-off location choice for a vehicle's last remaining passenger, allowing vehicles to wait for travelers at pickup locations, and simultaneously reducing VHT/VKT and improving customer service quality relative to D2D shared-ride services. The review paper concludes by discussing the most critical areas of future research related to SRSWL.      
### 25.Deep Task-Based Analog-to-Digital Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2201.12634.pdf)
>  Analog-to-digital converters (ADCs) allow physical signals to be processed using digital hardware. Their conversion consists of two stages: Sampling, which maps a continuous-time signal into discrete-time, and quantization, i.e., representing the continuous-amplitude quantities using a finite number of bits. ADCs typically implement generic uniform conversion mappings that are ignorant of the task for which the signal is acquired, and can be costly when operating in high rates and fine resolutions. In this work we design task-oriented ADCs which learn from data how to map an analog signal into a digital representation such that the system task can be efficiently carried out. We propose a model for sampling and quantization that facilitates the learning of non-uniform mappings from data. Based on this learnable ADC mapping, we present a mechanism for optimizing a hybrid acquisition system comprised of analog combining, tunable ADCs with fixed rates, and digital processing, by jointly learning its components end-to-end. Then, we show how one can exploit the representation of hybrid acquisition systems as deep network to optimize the sampling rate and quantization rate given the task by utilizing Bayesian meta-learning techniques. We evaluate the proposed deep task-based ADC in two case studies: the first considers symbol detection in multi-antenna digital receivers, where multiple analog signals are simultaneously acquired in order to recover a set of discrete information symbols. The second application is the beamforming of analog channel data acquired in ultrasound imaging. Our numerical results demonstrate that the proposed approach achieves performance which is comparable to operating with high sampling rates and fine resolution quantization, while operating with reduced overall bit rate.      
### 26.Learning Stochastic Graph Neural Networks with Constrained Variance  [ :arrow_down: ](https://arxiv.org/pdf/2201.12611.pdf)
>  Stochastic graph neural networks (SGNNs) are information processing architectures that learn representations from data over random graphs. SGNNs are trained with respect to the expected performance, which comes with no guarantee about deviations of particular output realizations around the optimal expectation. To overcome this issue, we propose a variance-constrained optimization problem for SGNNs, balancing the expected performance and the stochastic deviation. An alternating primal-dual learning procedure is undertaken that solves the problem by updating the SGNN parameters with gradient descent and the dual variable with gradient ascent. To characterize the explicit effect of the variance-constrained learning, we conduct a theoretical analysis on the variance of the SGNN output and identify a trade-off between the stochastic robustness and the discrimination power. We further analyze the duality gap of the variance-constrained optimization problem and the converging behavior of the primal-dual learning procedure. The former indicates the optimality loss induced by the dual transformation and the latter characterizes the limiting error of the iterative algorithm, both of which guarantee the performance of the variance-constrained learning. Through numerical simulations, we corroborate our theoretical findings and observe a strong expected performance with a controllable standard deviation.      
### 27.FedMed-ATL: Misaligned Unpaired Brain Image Synthesis via Affine Transform Loss  [ :arrow_down: ](https://arxiv.org/pdf/2201.12589.pdf)
>  The existence of completely aligned and paired multi-modal neuroimaging data has proved its effectiveness in the diagnosis of brain diseases. However, collecting the full set of well-aligned and paired data is impractical or even luxurious, since the practical difficulties may include high cost, long time acquisition, image corruption, and privacy issues. Previously, the misaligned unpaired neuroimaging data (termed as MUD) are generally treated as noisy label. However, such a noisy label-based method could not work very well when misaligned data occurs distortions severely, for example, different angles of rotation. In this paper, we propose a novel federated self-supervised learning (FedMed) for brain image synthesis. An affine transform loss (ATL) was formulated to make use of severely distorted images without violating privacy legislation for the hospital. We then introduce a new data augmentation procedure for self-supervised training and fed it into three auxiliary heads, namely auxiliary rotation, auxiliary translation, and auxiliary scaling heads. The proposed method demonstrates advanced performance in both the quality of synthesized results under a severely misaligned and unpaired data setting, and better stability than other GAN-based algorithms. The proposed method also reduces the demand for deformable registration while encouraging to realize the usage of those misaligned and unpaired data. Experimental results verify the outstanding ability of our learning paradigm compared to other state-of-the-art approaches. Our code is available on the website: <a class="link-external link-https" href="https://github.com/FedMed-Meta/FedMed-ATL" rel="external noopener nofollow">this https URL</a>      
### 28.Convolutional Filtering in Simplicial Complexes  [ :arrow_down: ](https://arxiv.org/pdf/2201.12584.pdf)
>  This paper proposes convolutional filtering for data whose structure can be modeled by a simplicial complex (SC). SCs are mathematical tools that not only capture pairwise relationships as graphs but account also for higher-order network structures. These filters are built by following the shift-and-sum principle of the convolution operation and rely on the Hodge-Laplacians to shift the signal within the simplex. But since in SCs we have also inter-simplex coupling, we use the incidence matrices to transfer the signal in adjacent simplices and build a filter bank to jointly filter signals from different levels. We prove some interesting properties for the proposed filter bank, including permutation and orientation equivariance, a computational complexity that is linear in the SC dimension, and a spectral interpretation using the simplicial Fourier transform. We illustrate the proposed approach with numerical experiments.      
### 29.Probabilistic Power Flow Calculation of AC/DC Hybrid System Based on Cumulant Method  [ :arrow_down: ](https://arxiv.org/pdf/2201.12571.pdf)
>  The operating conditions of the power system have become more complex and changeable. This paper proposes a probabilistic power flow calculation method based on the cumulant method for the voltage sourced converter high voltage direct current (VSC-HVDC) hybrid system containing photovoltaic grid-connected systems. Firstly, the corresponding control mode is set for the converter, including droop control and master-slave control. The unified iterative method is used to calculate the conventional AC/DC power flow. Secondly, on the basis of the probability model of load and photovoltaic output, based on the aforementioned power flow results, use correlation coefficient matrix of this paper will change the relevant sample into independent sample, the cumulants of the load and photovoltaic output are obtained; Then, the probability density function (PDF) and cumulative distribution function (CDF) of state variables are obtained by using Gram-Charlie series expansion method. Finally, the mean value and standard deviation of node voltage and line power are calculated on the modified IEEE 34-bus and IEEE 57-bus transmission systems. The algorithm can reflect the inherent uncertainty of new energy sources, and replace the complex convolution operation, greatly improving the calculation speed and the convergence.      
### 30.Polyphonic audio event detection: multi-label or multi-class multi-task classification problem?  [ :arrow_down: ](https://arxiv.org/pdf/2201.12557.pdf)
>  Polyphonic events are the main error source of audio event detection (AED) systems. In deep-learning context, the most common approach to deal with event overlaps is to treat the AED task as a multi-label classification problem. By doing this, we inherently consider multiple one-vs.-rest classification problems, which are jointly solved by a single (i.e. shared) network. In this work, to better handle polyphonic mixtures, we propose to frame the task as a multi-class classification problem by considering each possible label combination as one class. To circumvent the large number of arising classes due to combinatorial explosion, we divide the event categories into multiple groups and construct a multi-task problem in a divide-and-conquer fashion, where each of the tasks is a multi-class classification problem. A network architecture is then devised for multi-class multi-task modelling. The network is composed of a backbone subnet and multiple task-specific subnets. The task-specific subnets are designed to learn time-frequency and channel attention masks to extract features for the task at hand from the common feature maps learned by the backbone. Experiments on the TUT-SED-Synthetic-2016 with high degree of event overlap show that the proposed approach results in more favorable performance than the common multi-label approach.      
### 31.Validation and Generalizability of Self-Supervised Image Reconstruction Methods for Undersampled MRI  [ :arrow_down: ](https://arxiv.org/pdf/2201.12535.pdf)
>  Purpose: To investigate aspects of the validation of self-supervised algorithms for reconstruction of undersampled MR images: quantitative evaluation of prospective reconstructions, potential differences between prospective and retrospective reconstructions, suitability of commonly used quantitative metrics, and generalizability. <br>Theory and Methods: Two self-supervised algorithms based on self-supervised denoising and neural network image priors were investigated. These methods are compared to a least squares fitting and a compressed sensing reconstruction using in-vivo and phantom data. Their generalizability was tested with prospectively under-sampled data from experimental conditions different to the training. <br>Results: Prospective reconstructions can exhibit significant distortion relative to retrospective reconstructions/ground truth. Pixel-wise quantitative metrics may not capture differences in perceptual quality accurately, in contrast to a perceptual metric. All methods showed potential for generalization; generalizability is more affected by changes in anatomy/contrast than other changes. No-reference image metrics correspond well with human rating of image quality for studying generalizability. Compressed Sensing and learned denoising perform similarly well on all data. <br>Conclusion: Self-supervised methods show promising results for accelerating image reconstruction in clinical routines. Nonetheless, more work is required to investigate standardized methods to validate reconstruction algorithms for future clinical use.      
### 32.DoubleU-Net++: Architecture with Exploit Multiscale Features for Vertebrae Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.12389.pdf)
>  Accurate segmentation of the vertebra is an important prerequisite in various medical applications (E.g. tele surgery) to assist surgeons. Following the successful development of deep neural networks, recent studies have focused on the essential rule of vertebral segmentation. Prior works contain a large number of parameters, and their segmentation is restricted to only one view. Inspired by DoubleU-Net, we propose a novel model named DoubleU-Net++ in which DensNet as feature extractor, special attention module from Convolutional Block Attention on Module (CBAM) and, Pyramid Squeeze Attention (PSA) module are employed to improve extracted features. We evaluate our proposed model on three different views (sagittal, coronal, and axial) of VerSe2020 and xVertSeg datasets. Compared with state-of-the-art studies, our architecture is trained faster and achieves higher precision, recall, and F1-score as evaluation (imporoved by 4-6%) and the result of above 94% for sagittal view and above 94% for both coronal view and above 93% axial view were gained for VerSe2020 dataset, respectively. Also, for xVertSeg dataset, we achieved precision, recall,and F1-score of above 97% for sagittal view, above 93% for coronal view ,and above 96% for axial view.      
### 33.Physics-informed neural networks to learn cardiac fiber orientation from multiple electroanatomical maps  [ :arrow_down: ](https://arxiv.org/pdf/2201.12362.pdf)
>  We propose FiberNet, a method to estimate in-vivo the cardiac fiber architecture of the human atria from multiple catheter recordings of the electrical activation. Cardiac fibers play a central rolein the electro-mechanical function of the heart, yet they aredifficult to determine in-vivo, and hence rarely truly patient-specificin existing cardiac models.FiberNet learns the fibers arrangement by solvingan inverse problem with physics-informed neural networks. The inverse problem amounts to identifyingthe conduction velocity tensor of a cardiac propagation modelfrom a set of sparse activation maps. The use of multiple mapsenables the simultaneous identification of all the componentsof the conduction velocity tensor, including the local fiber angle.We extensively test FiberNet on synthetic 2-D and 3-D examples, diffusion tensor fibers, and a patient-specific case. We show that 3 maps are sufficient to accurately capture the fibers, also in thepresence of noise. With fewer maps, the role of regularization becomesprominent. Moreover, we show that the fitted model can robustlyreproduce unseen activation maps. We envision that FiberNet will help the creation of patient-specific models for personalized medicine.The full code is available at <a class="link-external link-http" href="http://github.com/fsahli/FiberNet" rel="external noopener nofollow">this http URL</a>.      
### 34.End-to-End Optimization of Metasurfaces for Imaging with Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2201.12348.pdf)
>  We present a method for the end-to-end optimization of computational imaging systems that reconstruct targets using compressed sensing. Using an adjoint analysis of the Karush-Kuhn-Tucker conditions, we incorporate a fully iterative compressed sensing algorithm that solves an $\ell_1$-regularized minimization problem, nested within the end-to-end optimization pipeline. We apply this method to jointly optimize the optical and computational parameters of metasurface-based imaging systems for underdetermined recovery problems. This allows us to investigate the interplay of nanoscale optics with the design goals of compressed sensing imaging systems. Our optimized metasurface imaging systems are robust to noise, significantly improving over random scattering surfaces and approaching the ideal compressed sensing performance of a Gaussian matrix.      
### 35.Neural Network Training with Asymmetric Crosspoint Elements  [ :arrow_down: ](https://arxiv.org/pdf/2201.13377.pdf)
>  Analog crossbar arrays comprising programmable nonvolatile resistors are under intense investigation for acceleration of deep neural network training. However, the ubiquitous asymmetric conductance modulation of practical resistive devices critically degrades the classification performance of networks trained with conventional algorithms. Here, we describe and experimentally demonstrate an alternative fully-parallel training algorithm: Stochastic Hamiltonian Descent. Instead of conventionally tuning weights in the direction of the error function gradient, this method programs the network parameters to successfully minimize the total energy (Hamiltonian) of the system that incorporates the effects of device asymmetry. We provide critical intuition on why device asymmetry is fundamentally incompatible with conventional training algorithms and how the new approach exploits it as a useful feature instead. Our technique enables immediate realization of analog deep learning accelerators based on readily available device technologies.      
### 36.On the Innocuousness of Deterministic p-type Antithetic Integral Controllers Arising in Integral Rein Control  [ :arrow_down: ](https://arxiv.org/pdf/2201.13375.pdf)
>  The innocuousness property of a controller is that property that makes the closed-loop system stable regardless the values of the controller parameters. In other words, the closed-loop system exhibits some structural stability property with respect to the parameters of the controller. The innocuousness property was first emphasized in [Briat, Gupta, and Khammash, Cell Systems, 2016] where it was shown that for stochastic unimolecular networks, the Antithetic Integral Controller (AIC) is innocuous under very mild conditions on the controlled network; namely the ergodicity and the output-controllability of the open-loop network, and the admissibility of the set-point value. We show here that the class of p-type AIC controllers arising in the use of Antithetic Integral Rein Controllers (AIRC) also exhibit such a property. It is shown in the unimolecular reaction network case that the closed-loop network is structurally stable with respect to the controller parameters provided that the open-loop dynamics is stable and that the set-point is admissible. Those results are then extended to the case of so-called output unstable linear systems and to stable nonlinear networks. Some examples are given for illustration.      
### 37.Exact linear reduction for rational dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.13373.pdf)
>  Detailed dynamical systems models used in life sciences may include dozens or even hundreds of state variables. Models of large dimension are not only harder from the numerical perspective (e.g., for parameter estimation or simulation), but it is also becoming challenging to derive mechanistic insights from such models. Exact model reduction is a way to address this issue by finding a self-consistent lower-dimensional projection of the corresponding dynamical system. A recent algorithm CLUE allows one to construct an exact linear reduction of the smallest possible dimension such that the fixed variables of interest are preserved. However, CLUE is restricted to systems with polynomial dynamics. Since rational dynamics occurs frequently in the life sciences (e.g., Michaelis-Menten or Hill kinetics), it is desirable to extend CLUE to the models with rational dynamics. <br>In this paper, we present an extension of CLUE to the case of rational dynamics and demonstrate its applicability on examples from literature. Our implementation is available in version 1.5 of CLUE at <a class="link-external link-https" href="https://github.com/pogudingleb/CLUE" rel="external noopener nofollow">this https URL</a>.      
### 38.Beyond synchronization: Body gestures and gaze direction in duo performance  [ :arrow_down: ](https://arxiv.org/pdf/2201.13297.pdf)
>  In this chapter, we focus on two main categories of visual interaction: body gestures and gaze direction. Our focus on body gestures is motivated by research showing that gesture patterns often change during joint action tasks to become more predictable (van der Wel et al., 2016). Moreover, coordination sometimes emerges between musicians at the level of body sway (Chang et al., 2017). Our focus on gaze direction was motivated by the fact that gaze can serve simultaneously as a means of obtaining information about the world and as a means of communicating one's own attention and intent.      
### 39.Model-Based Engineering of CPPS Functions and Code Generation for Skills  [ :arrow_down: ](https://arxiv.org/pdf/2201.13290.pdf)
>  Today's production systems are complex networks of cyber-physical systems which combine mechanical and electronic parts with software and networking capabilities. To the inherent complexity of such systems additional complexity arises from the context in which these systems operate. Manufacturing companies need to be able to adapt their production to ever changing customer demands as well as decreasing lot sizes. Engineering such systems, which need to be combined and reconfigured into different networks under changing conditions, requires engineering methods to carefully design them for possible future uses. Such engineering methods need to preserve the flexibility of functions into runtime, so that reconfiguring machines can be done with as little effort as possible. In this paper we present a model-based approach that is focused on machine functions and allows to methodically develop system functionalities for changing system networks. These functions are implemented as so-called skills using automated code-generation.      
### 40.Output-Feedback Control of Viscous Liquid-Tank System and its Numerical Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2201.13272.pdf)
>  We solve the output-feedback stabilization problem for a tank with a liquid modeled by the viscous Saint-Venant PDE system. The control input is the acceleration of the tank and a Control Lyapunov Functional methodology is used. The measurements are the tank position and the liquid level at the tank walls. The control scheme is a combination of a state feedback law with functional observers for the tank velocity and the liquid momentum. Four different types of output feedback stabilizers are proposed. A full-order observer and a reduced-order observer are used in order to estimate the tank velocity while the unmeasured liquid momentum is either estimated by using an appropriate scalar filter or is ignored. The reduced order observer differs from the full order observer because it omits the estimation of the measured tank position. Exponential convergence of the closed-loop system to the desired equilibrium point is achieved in each case. An algorithm is provided that guarantees that a robotic arm can move a glass of water to a pre-specified position no matter how full the glass is, without spilling water out of the glass, without residual end point sloshing and without measuring the water momentum and the glass velocity. Finally, the efficiency of the proposed output feedback laws is validated by numerical examples, obtained by using a simple finite-difference numerical scheme. The properties of the proposed, explicit, finite-difference scheme are determined.      
### 41.partitura: A Python Package for Handling Symbolic Musical Data  [ :arrow_down: ](https://arxiv.org/pdf/2201.13144.pdf)
>  This demo paper introduces partitura, a Python package for handling symbolic musical information. The principal aim of this package is to handle richly structured musical information as conveyed by modern staff music notation. It provides a much wider range of possibilities to deal with music than the more reductive (but very common) piano roll-oriented approach inspired by the MIDI standard. The package is an open source project and is available on GitHub.      
### 42.CoTV: Cooperative Control for Traffic Light Signals and Connected Autonomous Vehicles using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.13143.pdf)
>  The target of reducing travel time only is insufficient to support the development of future smart transportation systems. To align with the United Nations Sustainable Development Goals (UN-SDG), a further reduction of fuel and emissions, improvements of traffic safety, and the ease of infrastructure deployment and maintenance should also be considered. Different from existing work focusing on the optimization of the control in either traffic light signal (to improve the intersection throughput), or vehicle speed (to stabilize the traffic), this paper presents a multi-agent deep reinforcement learning (DRL) system called CoTV, which Cooperatively controls both Traffic light signals and connected autonomous Vehicles (CAV). Therefore, our CoTV can well balance the achievement of the reduction of travel time, fuel, and emission. In the meantime, CoTV can also be easy to deploy by cooperating with only one CAV that is the nearest to the traffic light controller on each incoming road. This enables more efficient coordination between traffic light controllers and CAV, thus leading to the convergence of training CoTV under the large-scale multi-agent scenario that is traditionally difficult to converge. We give the detailed system design of CoTV, and demonstrate its effectiveness in a simulation study using SUMO under various grid maps and realistic urban scenarios with mixed-autonomy traffic.      
### 43.Race Driver Evaluation at a Driving Simulator using a physical Model and a Machine Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2201.12939.pdf)
>  Professional race drivers are still superior to automated systems at controlling a vehicle at its dynamic limit. Gaining insight into race drivers' vehicle handling process might lead to further development in the areas of automated driving systems. We present a method to study and evaluate race drivers on a driver-in-the-loop simulator by analysing tire grip potential exploitation. Given initial data from a simulator run, two optimiser based on physical models maximise the horizontal vehicle acceleration or the tire forces, respectively. An overall performance score, a vehicle-trajectory score and a handling score are introduced to evaluate drivers. Our method is thereby completely track independent and can be used from one single corner up to a large data set. We apply the proposed method to a motorsport data set containing over 1200 laps from seven professional race drivers and two amateur drivers whose lap times are 10-20% slower. The difference to the professional drivers comes mainly from their inferior handling skills and not their choice of driving line. A downside of the presented method for certain applications is an extensive computation time. Therefore, we propose a Long-short-term memory (LSTM) neural network to estimate the driver evaluation scores. We show that the neural network is accurate and robust with a root-mean-square error between 2-5% and can replace the optimisation based method. The time for processing the data set considered in this work is reduced from 68 hours to 12 seconds, making the neural network suitable for real-time application.      
### 44.COIN++: Data Agnostic Neural Compression  [ :arrow_down: ](https://arxiv.org/pdf/2201.12904.pdf)
>  Neural compression algorithms are typically based on autoencoders that require specialized encoder and decoder architectures for different data modalities. In this paper, we propose COIN++, a neural compression framework that seamlessly handles a wide range of data modalities. Our approach is based on converting data to implicit neural representations, i.e. neural functions that map coordinates (such as pixel locations) to features (such as RGB values). Then, instead of storing the weights of the implicit neural representation directly, we store modulations applied to a meta-learned base network as a compressed code for the data. We further quantize and entropy code these modulations, leading to large compression gains while reducing encoding time by two orders of magnitude compared to baselines. We empirically demonstrate the effectiveness of our method by compressing various data modalities, from images to medical and climate data.      
### 45.ClearingPayments in Dynamic Financial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.12898.pdf)
>  In this paper, we propose a novel dynamical model of clearing in a financial network, which stems from the classical Eisenberg- Noe model of financial contagion. The Eisenberg-Noe model assumes that at one point in time (say, at the end of a day), all liabilities are claimed and due simultaneously, and that the entire network of banks becomes aware of the claims and possible defaults and instantaneously agrees on the clearing payments. The motivation for the dynamic model we propose in this paper is that one may expect that if financial operations are allowed for a given number of time periods after the initial theoretical defaults, some nodes may actually recover and eventually manage to fulfill their obligations. We prove that the proposed model obeys the standard requirement known as the priority of debt claims, that is, each node either pays its liabilities in full, or it pays out all its balance. We also show that the requirements of ro-rata payments determines the solution uniquely.      
### 46.Comprehensive Saliency Fusion for Object Co-segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2201.12828.pdf)
>  Object co-segmentation has drawn significant attention in recent years, thanks to its clarity on the expected foreground, the shared object in a group of images. Saliency fusion has been one of the promising ways to carry it out. However, prior works either fuse saliency maps of the same image or saliency maps of different images to extract the expected foregrounds. Also, they rely on hand-crafted saliency extraction and correspondence processes in most cases. This paper revisits the problem and proposes fusing saliency maps of both the same image and different images. It also leverages advances in deep learning for the saliency extraction and correspondence processes. Hence, we call it comprehensive saliency fusion. Our experiments reveal that our approach achieves much-improved object co-segmentation results compared to prior works on important benchmark datasets such as iCoseg, MSRC, and Internet Images.      
### 47.A Safety-Critical Decision Making and Control Framework Combining Machine Learning and Rule-based Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2201.12819.pdf)
>  While artificial-intelligence-based methods suffer from lack of transparency, rule-based methods dominate in safety-critical systems. Yet, the latter cannot compete with the first ones in robustness to multiple requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, to benefit from both methods they must be joined in a single system. This paper proposes a decision making and control framework, which profits from advantages of both the rule- and machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. A rule-based switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized every time, when the Learned one does not meet the safety constraint, and also directly participates in the safe Learned controller training. Decision making and control in autonomous driving is chosen as the system case study, where an autonomous vehicle learns a multi-task policy to safely cross an unprotected intersection. Multiple requirements (i.e., safety, efficiency, and comfort) are set for vehicle operation. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environment is successfully demonstrated.      
### 48.Improving End-to-End Contextual Speech Recognition with Fine-grained Contextual Knowledge Selection  [ :arrow_down: ](https://arxiv.org/pdf/2201.12806.pdf)
>  Nowadays, most methods in end-to-end contextual speech recognition bias the recognition process towards contextual knowledge. Since all-neural contextual biasing methods rely on phrase-level contextual modeling and attention-based relevance modeling, they may encounter confusion between similar context-specific phrases, which hurts predictions at the token level. In this work, we focus on mitigating confusion problems with fine-grained contextual knowledge selection (FineCoS). In FineCoS, we introduce fine-grained knowledge to reduce the uncertainty of token predictions. Specifically, we first apply phrase selection to narrow the range of phrase candidates, and then conduct token attention on the tokens in the selected phrase candidates. Moreover, we re-normalize the attention weights of most relevant phrases in inference to obtain more focused phrase-level contextual representations, and inject position information to better discriminate phrases or tokens. On LibriSpeech and an in-house 160,000-hour dataset, we explore the proposed methods based on a controllable all-neural biasing method, collaborative decoding (ColDec). The proposed methods provide at most 6.1% relative word error rate reduction on LibriSpeech and 16.4% relative character error rate reduction on the in-house dataset over ColDec.      
### 49.Automatic Segmentation of Left Ventricle in Cardiac Magnetic Resonance Images  [ :arrow_down: ](https://arxiv.org/pdf/2201.12805.pdf)
>  Segmentation of the left ventricle in cardiac magnetic resonance imaging MRI scans enables cardiologists to calculate the volume of the left ventricle and subsequently its ejection fraction. The ejection fraction is a measurement that expresses the percentage of blood leaving the heart with each contraction. Cardiologists often use ejection fraction to determine one's cardiac function. We propose multiscale template matching technique for detection and an elliptical active disc for automated segmentation of the left ventricle in MR images. The elliptical active disc optimizes the local energy function with respect to its five free parameters which define the disc. Gradient descent is used to minimize the energy function along with Green's theorem to optimize the computation expenses. We report validations on 320 scans containing 5,273 annotated slices which are publicly available through the Multi-Centre, Multi-Vendor, and Multi-Disease Cardiac Segmentation (M&amp;Ms) Challenge. We achieved successful localization of the left ventricle in 89.63% of the cases and a Dice coefficient of 0.873 on diastole slices and 0.770 on systole slices. The proposed technique is based on traditional image processing techniques with a performance on par with the deep learning techniques.      
### 50.You Only Demonstrate Once: Category-Level Manipulation from Single Visual Demonstration  [ :arrow_down: ](https://arxiv.org/pdf/2201.12716.pdf)
>  Promising results have been achieved recently in category-level manipulation that generalizes across object instances. Nevertheless, it often requires expensive real-world data collection and manual specification of semantic keypoints for each object category and task. Additionally, coarse keypoint predictions and ignoring intermediate action sequences hinder adoption in complex manipulation tasks beyond pick-and-place. This work proposes a novel, category-level manipulation framework that leverages an object-centric, category-level representation and model-free 6 DoF motion tracking. The canonical object representation is learned solely in simulation and then used to parse a category-level, task trajectory from a single demonstration video. The demonstration is reprojected to a target trajectory tailored to a novel object via the canonical representation. During execution, the manipulation horizon is decomposed into long-range, collision-free motion and last-inch manipulation. For the latter part, a category-level behavior cloning (CatBC) method leverages motion tracking to perform closed-loop control. CatBC follows the target trajectory, projected from the demonstration and anchored to a dynamically selected category-level coordinate frame. The frame is automatically selected along the manipulation horizon by a local attention mechanism. This framework allows to teach different manipulation strategies by solely providing a single demonstration, without complicated manual programming. Extensive experiments demonstrate its efficacy in a range of challenging industrial tasks in high-precision assembly, which involve learning complex, long-horizon policies. The process exhibits robustness against uncertainty due to dynamics as well as generalization across object instances and scene configurations.      
### 51.Robotic Wireless Energy Transfer in Dynamic Environments: System Design and Experimental Validation  [ :arrow_down: ](https://arxiv.org/pdf/2201.12702.pdf)
>  Wireless energy transfer (WET) is a ground-breaking technology for cutting the last wire between mobile sensors and power grids in smart cities. Yet, WET only offers effective transmission of energy over a short distance. Robotic WET is an emerging paradigm that mounts the energy transmitter on a mobile robot and navigates the robot through different regions in a large area to charge remote energy harvesters. However, it is challenging to determine the robotic charging strategy in an unknown and dynamic environment due to the uncertainty of obstacles. This paper proposes a hardware-in-the-loop joint optimization framework that offers three distinctive features: 1) efficient model updates and re-optimization based on the last-round experimental data; 2) iterative refinement of the anchor list for adaptation to different environments; 3) verification of algorithms in a high-fidelity Gazebo simulator and a multi-robot testbed. Experimental results show that the proposed framework significantly saves the WET mission completion time while satisfying collision avoidance and energy harvesting constraints.      
### 52.A Deep Learning and Geospatial Data-Based Channel Estimation Technique for Hybrid Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.12676.pdf)
>  This paper presents a novel channel estimation technique for the multi-user massive multiple-input multiple-output (MU-mMIMO) systems using angular-based hybrid precoding (AB-HP). The proposed channel estimation technique generates group-wise channel state information (CSI) of user terminal (UT) zones in the service area by deep neural networks (DNN) and fuzzy c-Means (FCM) clustering. The slow time-varying CSI between the base station (BS) and feasible UT locations in the service area is calculated from the geospatial data by offline ray tracing and a DNN-based path estimation model associated with the 1-dimensional convolutional neural network (1D-CNN) and regression tree ensembles. Then, the UT-level CSI of all feasible locations is grouped into clusters by a proposed FCM clustering. Finally, the service area is divided into a number of non-overlapping UT zones. Each UT zone is characterized by a corresponding set of clusters named as UT-group CSI, which is utilized in the analog RF beamformer design of AB-HP to reduce the required large online CSI overhead in the MU-mMIMO systems. Then, the reduced-size online CSI is employed in the baseband (BB) precoder of AB-HP. Simulations are conducted in the indoor scenario at 28 GHz and tested in an AB-HP MU-mMIMO system with a uniform rectangular array (URA) having 16x16=256 antennas and 22 RF chains. Illustrative results indicate that 91.4% online CSI can be reduced by using the proposed offline channel estimation technique as compared to the conventional online channel sounding. The proposed DNN-based path estimation technique produces same amount of UT-level CSI with runtime reduced by 65.8% as compared to the computationally expensive ray tracing.      
### 53.Identification of MIMO Wiener-type Koopman Models for Data-Driven Model Reduction using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2201.12669.pdf)
>  We use Koopman theory to develop a data-driven nonlinear model reduction and identification strategy for multiple-input multiple-output (MIMO) input-affine dynamical systems. While the present literature has focused on linear and bilinear Koopman models, we derive and use a Wiener-type Koopman formulation. We discuss that the Wiener structure is particularly suitable for model reduction, and can be naturally derived from Koopman theory. Moreover, the Wiener block-structure unifies the mathematical simplicity of linear dynamical blocks and the accuracy of bilinear dynamics. We present a Koopman deep-learning strategy combining autoencoders and linear dynamics that generates low-order surrogate models of MIMO Wiener type. In three case studies, we apply our framework for identification and reduction of a system with input multiplicity, a chemical reactor and a high-purity distillation column. We compare the prediction performance of the identified Wiener models to linear and bilinear Koopman models. We observe the highest accuracy and strongest model reduction capabilities of low-order Wiener-type Koopman models, making them promising for control.      
### 54.Full-Duplex Non-Coherent Communications for Massive MIMO Systems with Analog Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2201.12660.pdf)
>  In this paper, a novel full-duplex non-coherent (FD-NC) transmission scheme is developed for massive multiple-input multiple-output (mMIMO) systems using analog beamforming (ABF). We propose to use a structured Grassmannian constellation for the non-coherent communications that does not require channel estimation. Then, we design the transmit and receive ABF via the slow time-varying angle-of-departure (AoD) and angle-of-arrival (AoA) information, respectively. The ABF design targets maximizing the intended signal power while suppressing the strong self-interference (SI) occurred in the FD transmission. Also, the proposed ABF technique only needs a single transmit and receive RF chain to support large antenna arrays, thus, it reduces hardware cost/complexity in the mMIMO systems. It is shown that the proposed FD-NC offers a great improvement in bit error rate (BER) in comparison to both half-duplex non-coherent (HD-NC) and HD coherent schemes. We also observe that the proposed FD-NC both reduces the error floor resulted from the residual SI in FD transmission, and provides lower BER compared to the FD coherent transmission.      
### 55.Deep Learning based Multi-User Power Allocation and Hybrid Precoding in Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.12659.pdf)
>  This paper proposes a deep learning based power allocation (DL-PA) and hybrid precoding technique for multiuser massive multiple-input multiple-output (MU-mMIMO) systems. We first utilize an angular-based hybrid precoding technique for reducing the number of RF chains and channel estimation overhead. Then, we develop the DL-PA algorithm via a fully-connected deep neural network (DNN). DL-PA has two phases: (i) offline supervised learning with the optimal allocated powers obtained by particle swarm optimization based PA (PSO-PA) algorithm, (ii) online power prediction by the trained DNN. In comparison to the computationally expensive PSO-PA, it is shown that DL-PA greatly reduces the runtime by 98.6%-99.9%, while closely achieving the optimal sum-rate capacity. It makes DL-PA a promising algorithm for the real-time online applications in MU-mMIMO systems.      
### 56.Transfer Learning for Estimation of Pendubot Angular Position Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2201.12649.pdf)
>  In this paper, a machine learning based approach is introduced to estimate Pendubot angular position from its captured images. Initially, a baseline algorithm is introduced to estimate the angle using conventional image processing technique. The baseline algorithm performs well for the cases that the Pendubot is not moving fast. However, when moving quickly due to a free fall, the Pendubot appears as a blurred object in the captured image in a way that the baseline algorithm fails to estimate the angle. Consequently, a Deep Neural Network (DNN) based algorithm is introduced to cope with this challenge. The approach relies on the concept of transfer learning to allow the training of the DNN on a very small fine-tuning dataset. The base algorithm is used to create the ground truth labels of the fine-tuning dataset. Experimental results on the held-out evaluation set show that the proposed approach achieves a median absolute error of 0.02 and 0.06 degrees for the sharp and blurry images respectively.      
### 57.Design of Outdoor Autonomous Moble Robot  [ :arrow_down: ](https://arxiv.org/pdf/2201.12605.pdf)
>  This study presents the design of a six-wheeled outdoor autonomous mobile robot. The main design goal of our robot is to increase its adaptability and flexibility when moving outdoors. This six-wheeled robot platform was equipped with some sensors, such as a global positioning system (GPS), high definition (HD) webcam, light detection and ranging (LiDAR), and rotary encoders. A personal mobile computer and 86Duino ONE microcontroller were used as the algorithm computing platform. In terms of control, the lateral offset and head angle offset of the robot were calculated using a differential GPS or a camera to detect structured and unstructured road boundaries. The lateral offset and head angle offset were fed to a fuzzy controller. The control input was designed by Q-learning of the differential speed between the left and right wheels. This made the robot track a reference route so that it could stay in its own lane. 2D LiDAR was also used to measure the relative distance from the front obstacle. The robot would immediately stop to avoid a collision when the distance between the robot and obstacle was less than a specific safety distance. A custom-designed rocker arm gave the robot the ability to climb a low step. Body balance could be maintained by controlling the angle of the rocker arm when the robot changed its pose. The autonomous mobile robot has been used for delivery service on our campus road by integrating the above system functionality.      
### 58.Semantic-assisted image compression  [ :arrow_down: ](https://arxiv.org/pdf/2201.12599.pdf)
>  Conventional image compression methods typically aim at pixel-level consistency while ignoring the performance of downstream AI <a class="link-external link-http" href="http://tasks.To" rel="external noopener nofollow">this http URL</a> solve this problem, this paper proposes a Semantic-Assisted Image Compression method (SAIC), which can maintain semantic-level consistency to enable high performance of downstream AI <a class="link-external link-http" href="http://tasks.To" rel="external noopener nofollow">this http URL</a> this end, we train the compression network using semantic-level loss function. In particular, semantic-level loss is measured using gradient-based semantic weights mechanism (GSW). GSW directly consider downstream AI tasks' perceptual results. Then, this paper proposes a semantic-level distortion evaluation metric to quantify the amount of semantic information retained during the compression process. Experimental results show that the proposed SAIC method can retain more semantic-level information and achieve better performance of downstream AI tasks compared to the traditional deep learning-based method and the advanced perceptual method at the same compression ratio.      
### 59.The HCCL-DKU system for fake audio generation task of the 2022 ICASSP ADD Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2201.12567.pdf)
>  The voice conversion task is to modify the speaker identity of continuous speech while preserving the linguistic content. Generally, the naturalness and similarity are two main metrics for evaluating the conversion quality, which has been improved significantly in recent years. This paper presents the HCCL-DKU entry for the fake audio generation task of the 2022 ICASSP ADD challenge. We propose a novel ppg-based voice conversion model that adopts a fully end-to-end structure. Experimental results show that the proposed method outperforms other conversion models, including Tacotron-based and Fastspeech-based models, on conversion quality and spoofing performance against anti-spoofing systems. In addition, we investigate several post-processing methods for better spoofing power. Finally, we achieve second place with a deception success rate of 0.916 in the ADD challenge.      
### 60.Recommender System Expedited Quantum Control Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2201.12550.pdf)
>  Quantum control optimization algorithms are routinely used to generate optimal quantum gates or efficient quantum state transfers. However, there are two main challenges in designing efficient optimization algorithms, namely overcoming the sensitivity to local optima and improving the computational speed. The former challenge can be dealt with by designing hybrid algorithms, such as a combination of gradient and simulated annealing methods. Here, we propose and demonstrate the use of a machine learning method, specifically the recommender system (RS), to deal with the latter challenge of enhancing computational efficiency. We first describe ways to set up a rating matrix involving gradients or gate fidelities. We then establish that RS can rapidly and accurately predict elements of a sparse rating matrix. Using this approach, we expedite a gradient ascent based quantum control optimization, namely GRAPE and demonstrate the faster performance for up to 8 qubits. Finally, we describe and implement the enhancement of the computational speed of a hybrid algorithm, namely SAGRAPE.      
### 61.Progressive Continual Learning for Spoken Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2201.12546.pdf)
>  Catastrophic forgetting is a thorny challenge when updating keyword spotting (KWS) models after deployment. To tackle such challenges, we propose a progressive continual learning strategy for small-footprint spoken keyword spotting (PCL-KWS). Specifically, the proposed PCL-KWS framework introduces a network instantiator to generate the task-specific sub-networks for remembering previously learned keywords. As a result, the PCL-KWS approach incrementally learns new keywords without forgetting prior knowledge. Besides, the keyword-aware network scaling mechanism of PCL-KWS constrains the growth of model parameters while achieving high performance. Experimental results show that after learning five new tasks sequentially, our proposed PCL-KWS approach archives the new state-of-the-art performance of 92.8% average accuracy for all the tasks on Google Speech Command dataset compared with other baselines.      
### 62.Data-Driven Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2201.12539.pdf)
>  Optimum parameter estimation methods require knowledge of a parametric probability density that statistically describes the available observations. In this work we examine Bayesian and non-Bayesian parameter estimation problems under a data-driven formulation where the necessary parametric probability density is replaced by available data. We present various data-driven versions that either result in neural network approximations of the optimum estimators or in well defined optimization problems that can be solved numerically. In particular, for the data-driven equivalent of non-Bayesian estimation we end up with optimization problems similar to the ones encountered for the design of generative networks.      
### 63.ItWave: It Stochastic Differential Equation Is All You Need For Wave Generation  [ :arrow_down: ](https://arxiv.org/pdf/2201.12519.pdf)
>  In this paper, we propose a vocoder based on a pair of forward and reverse-time linear stochastic differential equations (SDE). The solutions of this SDE pair are two stochastic processes, one of which turns the distribution of wave, that we want to generate, into a simple and tractable distribution. The other is the generation procedure that turns this tractable simple signal into the target wave. The model is called ItWave. ItWave use the Wiener process as a driver to gradually subtract the excess signal from the noise signal to generate realistic corresponding meaningful audio respectively, under the conditional inputs of original mel spectrogram. The results of the experiment show that the mean opinion scores (MOS) of ItWave can exceed the current state-of-the-art (SOTA) methods, and reached 4.35$\pm$0.115. The generated audio samples are available online\footnotemark[2].      
### 64.Zeroth-Order Actor-Critic  [ :arrow_down: ](https://arxiv.org/pdf/2201.12518.pdf)
>  Zeroth-order optimization methods and policy gradient based first-order methods are two promising alternatives to solve reinforcement learning (RL) problems with complementary advantages. The former work with arbitrary policies, drive state-dependent and temporally-extended exploration, possess robustness-seeking property, but suffer from high sample complexity, while the latter are more sample efficient but restricted to differentiable policies and the learned policies are less robust. We propose Zeroth-Order Actor-Critic algorithm (ZOAC) that unifies these two methods into an on-policy actor-critic architecture to preserve the advantages from both. ZOAC conducts rollouts collection with timestep-wise perturbation in parameter space, first-order policy evaluation (PEV) and zeroth-order policy improvement (PIM) alternately in each iteration. We evaluate our proposed method on a range of challenging continuous control benchmarks using different types of policies, where ZOAC outperforms zeroth-order and first-order baseline algorithms.      
### 65.Random Orthogonalization for Federated Learning in Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2201.12490.pdf)
>  We propose a novel uplink communication method, coined random orthogonalization, for federated learning (FL) in a massive multiple-input and multiple-output (MIMO) wireless system. The key novelty of random orthogonalization comes from the tight coupling of FL model aggregation and two unique characteristics of massive MIMO - channel hardening and favorable propagation. As a result, random orthogonalization can achieve natural over-the-air model aggregation without requiring transmitter side channel state information, while significantly reducing the channel estimation overhead at the receiver. Theoretical analyses with respect to both communication and machine learning performances are carried out. In particular, an explicit relationship among the convergence rate, the number of clients and the number of antennas is established. Experimental results validate the effectiveness and efficiency of random orthogonalization for FL in massive MIMO.      
### 66.A deep Q-learning method for optimizing visual search strategies in backgrounds of dynamic noise  [ :arrow_down: ](https://arxiv.org/pdf/2201.12385.pdf)
>  Humans process visual information with varying resolution (foveated visual system) and explore images by orienting through eye movements the high-resolution fovea to points of interest. The Bayesian ideal searcher (IS) that employs complete knowledge of task-relevant information optimizes eye movement strategy and achieves the optimal search performance. The IS can be employed as an important tool to evaluate the optimality of human eye movements, and potentially provide guidance to improve human observer visual search strategies. Najemnik and Geisler (2005) derived an IS for backgrounds of spatial 1/f noise. The corresponding template responses follow Gaussian distributions and the optimal search strategy can be analytically determined. However, the computation of the IS can be intractable when considering more realistic and complex backgrounds such as medical images. Modern reinforcement learning methods, successfully applied to obtain optimal policy for a variety of tasks, do not require complete knowledge of the background generating functions and can be potentially applied to anatomical backgrounds. An important first step is to validate the optimality of the reinforcement learning method. In this study, we investigate the ability of a reinforcement learning method that employs Q-network to approximate the IS. We demonstrate that the search strategy corresponding to the Q-network is consistent with the IS search strategy. The findings show the potential of the reinforcement learning with Q-network approach to estimate optimal eye movement planning with real anatomical backgrounds.      
### 67.Detecting Electric Vehicle Battery Failure via Dynamic-VAE  [ :arrow_down: ](https://arxiv.org/pdf/2201.12358.pdf)
>  In this note, we describe a battery failure detection pipeline backed up by deep learning models. We first introduce a large-scale Electric vehicle (EV) battery dataset including cleaned battery-charging data from hundreds of vehicles. We then formulate battery failure detection as an outlier detection problem, and propose a new algorithm named Dynamic-VAE based on dynamic system and variational autoencoders. We validate the performance of our proposed algorithm against several baselines on our released dataset and demonstrated the effectiveness of Dynamic-VAE.      
### 68.Automatic Audio Captioning using Attention weighted Event based Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2201.12352.pdf)
>  Automatic Audio Captioning (AAC) refers to the task of translating audio into a natural language that describes the audio events, source of the events and their relationships. The limited samples in AAC datasets at present, has set up a trend to incorporate transfer learning with Audio Event Detection (AED) as a parent task. Towards this direction, in this paper, we propose an encoder-decoder architecture with light-weight (i.e. with lesser learnable parameters) Bi-LSTM recurrent layers for AAC and compare the performance of two state-of-the-art pre-trained AED models as embedding extractors. Our results show that an efficient AED based embedding extractor combined with temporal attention and augmentation techniques is able to surpass existing literature with computationally intensive architectures. Further, we provide evidence of the ability of the non-uniform attention weighted encoding generated as a part of our model to facilitate the decoder glance over specific sections of the audio while generating each token.      
### 69.DiriNet: A network to estimate the spatial and spectral degradation functions  [ :arrow_down: ](https://arxiv.org/pdf/2201.12346.pdf)
>  The spatial and spectral degradation functions are critical to hyper- and multi-spectral image fusion. However, few work has been payed on the estimation of the degradation functions. To learn the spatial response function and the point spread function from the image pairs to be fused, we propose a Dirichlet network, where both functions are properly constrained. Specifically, the spatial response function is constrained with positivity, while the Dirichlet distribution along with a total variation is imposed on the point spread function. To the best of our knowledge, the neural netwrok and the Dirichlet regularization are exclusively investigated, for the first time, to estimate the degradation functions. Both image degradation and fusion experiments demonstrate the effectiveness and superiority of the proposed Dirichlet network.      
