# ArXiv eess --Wed, 23 Feb 2022
### 1.Wavebender GAN: An architecture for phonetically meaningful speech manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2202.10973.pdf)
>  Deep learning has revolutionised synthetic speech quality. However, it has thus far delivered little value to the speech science community. The new methods do not meet the controllability demands that practitioners in this area require e.g.: in listening tests with manipulated speech stimuli. Instead, control of different speech properties in such stimuli is achieved by using legacy signal-processing methods. This limits the range, accuracy, and speech quality of the manipulations. Also, audible artefacts have a negative impact on the methodological validity of results in speech perception studies. <br>This work introduces a system capable of manipulating speech properties through learning rather than design. The architecture learns to control arbitrary speech properties and leverages progress in neural vocoders to obtain realistic output. Experiments with copy synthesis and manipulation of a small set of core speech features (pitch, formants, and voice quality measures) illustrate the promise of the approach for producing speech stimuli that have accurate control and high perceptual quality.      
### 2.Improving Classification Model Performance on Chest X-Rays through Lung Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2202.10971.pdf)
>  Chest radiography is an effective screening tool for diagnosing pulmonary diseases. In computer-aided diagnosis, extracting the relevant region of interest, i.e., isolating the lung region of each radiography image, can be an essential step towards improved performance in diagnosing pulmonary disorders. Methods: In this work, we propose a deep learning approach to enhance abnormal chest x-ray (CXR) identification performance through segmentations. Our approach is designed in a cascaded manner and incorporates two modules: a deep neural network with criss-cross attention modules (XLSor) for localizing lung region in CXR images and a CXR classification model with a backbone of a self-supervised momentum contrast (MoCo) model pre-trained on large-scale CXR data sets. The proposed pipeline is evaluated on Shenzhen Hospital (SH) data set for the segmentation module, and COVIDx data set for both segmentation and classification modules. Novel statistical analysis is conducted in addition to regular evaluation metrics for the segmentation module. Furthermore, the results of the optimized approach are analyzed with gradient-weighted class activation mapping (Grad-CAM) to investigate the rationale behind the classification decisions and to interpret its choices. Results and Conclusion: Different data sets, methods, and scenarios for each module of the proposed pipeline are examined for designing an optimized approach, which has achieved an accuracy of 0.946 in distinguishing abnormal CXR images (i.e., Pneumonia and COVID-19) from normal ones. Numerical and visual validations suggest that applying automated segmentation as a pre-processing step for classification improves the generalization capability and the performance of the classification models.      
### 3.Data-Consistent Local Superresolution for Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2202.10875.pdf)
>  In this work we propose a new paradigm of iterative model-based reconstruction algorithms for providing real-time solution for zooming-in and refining a region of interest in medical and clinical tomographic (such as CT/MRI/PET, etc) images. This algorithmic framework is tailor for a clinical need in medical imaging practice, that after a reconstruction of the full tomographic image, the clinician may believe that some critical parts of the image are not clear enough, and may wish to see clearer these regions-of-interest. A naive approach (which is highly not recommended) would be performing the global reconstruction of a higher resolution image, which has two major limitations: firstly, it is computationally inefficient, and secondly, the image regularization is still applied globally which may over-smooth some local regions. Furthermore if one wish to fine-tune the regularization parameter for local parts, it would be computationally infeasible in practice for the case of using global reconstruction. Our new iterative approaches for such tasks are based on jointly utilizing the measurement information, efficient upsampling/downsampling across image spaces, and locally adjusted image prior for efficient and high-quality post-processing. The numerical results in low-dose X-ray CT image local zoom-in demonstrate the effectiveness of our approach.      
### 4.UncertaINR: Uncertainty Quantification of End-to-End Implicit Neural Representations for Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2202.10847.pdf)
>  Implicit neural representations (INRs) have achieved impressive results for scene reconstruction and computer graphics, where their performance has primarily been assessed on reconstruction accuracy. However, in medical imaging, where the reconstruction problem is underdetermined and model predictions inform high-stakes diagnoses, uncertainty quantification of INR inference is critical. To that end, we study UncertaINR: a Bayesian reformulation of INR-based image reconstruction, for computed tomography (CT). We test several Bayesian deep learning implementations of UncertaINR and find that they achieve well-calibrated uncertainty, while retaining accuracy competitive with other classical, INR-based, and CNN-based reconstruction techniques. In contrast to the best-performing prior approaches, UncertaINR does not require a large training dataset, but only a handful of validation images.      
### 5.Cyber-Physical Risk Assessment for False Data Injection Attacks Considering Moving Target Defences  [ :arrow_down: ](https://arxiv.org/pdf/2202.10841.pdf)
>  In this paper, we examine the factors that influence the success of false data injection (FDI) attacks in the context of both cyber and physical styles of reinforcement. Many works consider the FDI attack in the context of the ability to change a measurement in a static system only. However, successful attacks will require first intrusion into a system followed by construction of an attack vector that can bypass bad data detection (BDD). In this way, we develop a full service framework for FDI risk assessment. The framework considers both the costs of system intrusion via a weighted graph assessment in combination with a physical, line overload-based vulnerability assessment. We present our simulations on a IEEE 14-bus system with an overlain RTU network to model the true risk of intrusion. The cyber model considers multiple methods of entry for the FDI attack including meter intrusion, RTU intrusion and combined style attacks. Post-intrusion our physical reinforcement model analyses the required level of topology divergence to protect against a branch overload from an optimised attack vector.      
### 6.SADN: Learned Light Field Image Compression with Spatial-Angular Decorrelation  [ :arrow_down: ](https://arxiv.org/pdf/2202.10837.pdf)
>  Light field image becomes one of the most promising media types for immersive video applications. In this paper, we propose a novel end-to-end spatial-angular-decorrelated network (SADN) for high-efficiency light field image compression. Different from the existing methods that exploit either spatial or angular consistency in the light field image, SADN decouples the angular and spatial information by dilation convolution and stride convolution in spatial-angular interaction, and performs feature fusion to compress spatial and angular information jointly. To train a stable and robust algorithm, a large-scale dataset consisting of 7549 light field images is proposed and built. The proposed method provides 2.137 times and 2.849 times higher compression efficiency relative to H.266/VVC and H.265/HEVC inter coding, respectively. It also outperforms the end-to-end image compression networks by an average of 79.6% bitrate saving with much higher subjective quality and light field consistency.      
### 7.Behaviour-neutral Smart Charging of Plugin Electric Vehicles: Reinforcement learning approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.10823.pdf)
>  Smart charging aims to satisfy EV charging demand without overloading the grid. A common approach is to postpone the charging to off-peak hours or dynamically control the charging speed depending on available grid capacity using dynamic pricing. However, this requires either accurately predicting the plugin duration to ensure the EV is charged before it is plugged out or influencing the driver behaviour to ensure the that a vehicle remains connected until the EV is charged. In this paper, we introduce a novel approach that uses reinforcement learning for EV charging and requires neither coordination with the grid nor predicting the session duration. The algorithm trains on a driver's personal history of past charging sessions to identify the optimal charging parameters that minimise the effective charging current while satisfying the target energy demand. We evaluate the performance on a dataset of 3.1 million charging sessions from 22,731 domestic charge stations. Our experiments demonstrate that the approach results in 31\% of aggregate peak reduction and significant reduction in charging power demand. The results also reveal two interesting features: Firstly, our approach is able to spread the charging power in time without knowing the session duration. Secondly, the proposed approach does not require any coordination with the grid or other charge points and can be implemented on a standalone charge point reducing the complexity.      
### 8.Resilient Average Consensus: A Detection and Compensation Approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.10814.pdf)
>  We study the problem of resilient average consensus for multi-agent systems with misbehaving nodes. To protect consensus valuefrom being influenced by misbehaving nodes, we address this problem by detecting misbehaviors, mitigating the corresponding adverse impact and achieving the resilient average consensus. In this paper, general types of misbehaviors are considered,including deception attacks, accidental faults and link failures. We characterize the adverse impact of misbehaving nodes in a distributed manner via two-hop communication information and develop a deterministic detection-compensation-based consensus (D-DCC) algorithm with a decaying fault-tolerant error bound. Considering scenarios where information sets are intermittently available due to link failures, a stochastic extension named stochastic detection-compensation-based consensus(S-DCC) algorithm is proposed. We prove that D-DCC and S-DCC allow nodes to asymptotically achieve resilient averageconsensus exactly and in expectation, respectively. Then, the Wasserstein distance is introduced to analyze the accuracy ofS-DCC. Finally, extensive simulations are conducted to verify the effectiveness of the proposed algorithm      
### 9.Continuous Speech for Improved Learning Pathological Voice Disorders  [ :arrow_down: ](https://arxiv.org/pdf/2202.10777.pdf)
>  Goal: Numerous studies had successfully differentiated normal and abnormal voice samples. Nevertheless, further classification had rarely been attempted. This study proposes a novel approach, using continuous Mandarin speech instead of a single vowel, to classify four common voice disorders (i.e. functional dysphonia, neoplasm, phonotrauma, and vocal palsy). Methods: In the proposed framework, acoustic signals are transformed into mel-frequency cepstral coefficients, and a bi-directional long-short term memory network (BiLSTM) is adopted to model the sequential features. The experiments were conducted on a large-scale database, wherein 1,045 continuous speech were collected by the speech clinic of a hospital from 2012 to 2019. Results: Experimental results demonstrated that the proposed framework yields significant accuracy and unweighted average recall improvements of 78.12-89.27% and 50.92-80.68%, respectively, compared with systems that use a single vowel. Conclusions: The results are consistent with other machine learning algorithms, including gated recurrent units, random forest, deep neural networks, and LSTM. The sensitivities for each disorder were also analyzed, and the model capabilities were visualized via principal component analysis. An alternative experiment based on a balanced dataset again confirms the advantages of using continuous speech for learning voice disorders.      
### 10.Station-keeping of $L_2$ halo orbits under sampled-data model predictive control  [ :arrow_down: ](https://arxiv.org/pdf/2202.10755.pdf)
>  The paper deals with the design of an improved model predictive control scheme for achieving station-keeping in a quasi Halo orbit around the $L_2$ point in the Earth-Moon system. The improvement is obtained thanks to a multi-rate sampled-data trajectory planner that allows for simplifying the optimization problem of the model predictive controller while guaranteeing feasibility and convergence to the desired orbit. The multi-rate planner is designed based on a simplified model of the dynamics under a preliminary nonlinear regulation feedback. The proposed control scheme is shown to outperform recent station-keeping nonlinear model predictive control designs both in terms of tracking error and energy expenditures in different situations. Finally, a brief study of aspects pertaining to computational time are carried out so highlighting the possibility for real time implementation on modern hardware.      
### 11.Event-Triggered Tracking Control of Networked Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.10752.pdf)
>  This paper studies the tracking control problem of networked multi-agent systems under both multiple networks and event-triggered mechanisms. Multiple networks are to connect multiple agents and reference systems with decentralized controllers to guarantee their information transmission, whereas the event-triggered mechanisms are to reduce the information transmission via the networks. In this paper, each agent has a network to communicate with its controller and reference system, and all networks are independent and asynchronous and have local event-triggered mechanisms, which are based on local measurements and determine whether the local measurements need to be transmitted via the corresponding network. To address this scenario, we first implement the emulation-based approach to develop a novel hybrid model for the tracking control of networked multi-agent systems. Next, sufficient conditions are derived and decentralized event-triggered mechanisms are designed to guarantee the desired tracking performance. Furthermore, the proposed approach is applied to derive novel results for the event-triggered observer design problem of networked multi-agent systems. Finally, two numerical examples are presented to illustrate the validity of the developed results.      
### 12.Location-based Initial Access for Wireless Power Transfer with Physically Large Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2202.10749.pdf)
>  Radio frequency (RF) wireless power transfer (WPT) is a promising technology for 6G use cases. It enables a massive, yet sustainable deployment of batteryless energy neutral (EN) devices at unprecedented scale. Recent research on 6G is exploring high operating frequencies up to the THz spectrum, where antenna arrays with large apertures are capable of forming narrow, "laser-like" beams. At sub-10 GHz frequencies, physically large antenna arrays are considered that are operating in the array near field. Transmitting spherical wavefronts, power can be focused in a focal point rather than a beam, which allows for efficient and radiation-safe WPT. We formulate a multipath channel model comprising specular components and diffuse scattering to find the WPT power budget in a realistic indoor scenario. Specular components can be predicted by means of a geometric model. This is used to transmit power via multiple beams simultaneously, increasing the available power budget and expanding the initial access distance. We show that exploiting this "beam diversity" reduces the required fading margin for the initial access to EN devices.      
### 13.Feature reconstruction from incomplete tomographic data without detour  [ :arrow_down: ](https://arxiv.org/pdf/2202.10724.pdf)
>  In this paper, we consider the problem of feature reconstruction from incomplete x-ray CT data. Such problems occurs, e.g., as a result of dose reduction in the context medical imaging. Since image reconstruction from incomplete data is a severely ill-posed problem, the reconstructed images may suffer from characteristic artefacts or missing features, and significantly complicate subsequent image processing tasks (e.g., edge detection or segmentation). In this paper, we introduce a novel framework for the robust reconstruction of convolutional image features directly from CT data, without the need of computing a reconstruction firs. Within our framework we use non-linear (variational) regularization methods that can be adapted to a variety of feature reconstruction tasks and to several limited data situations . In our numerical experiments, we consider several instances of edge reconstructions from angularly undersampled data and show that our approach is able to reliably reconstruct feature maps in this case.      
### 14.An Object Aware Hybrid U-Net for Breast Tumour Annotation  [ :arrow_down: ](https://arxiv.org/pdf/2202.10691.pdf)
>  In the clinical settings, during digital examination of histopathological slides, the pathologist annotate the slides by marking the rough boundary around the suspected tumour region. The marking or annotation is generally represented as a polygonal boundary that covers the extent of the tumour in the slide. These polygonal markings are difficult to imitate through CAD techniques since the tumour regions are heterogeneous and hence segmenting them would require exhaustive pixel wise ground truth annotation. Therefore, for CAD analysis, the ground truths are generally annotated by pathologist explicitly for research purposes. However, this kind of annotation which is generally required for semantic or instance segmentation is time consuming and tedious. In this proposed work, therefore, we have tried to imitate pathologist like annotation by segmenting tumour extents by polygonal boundaries. For polygon like annotation or segmentation, we have used Active Contours whose vertices or snake points move towards the boundary of the object of interest to find the region of minimum energy. To penalize the Active Contour we used modified U-Net architecture for learning penalization values. The proposed hybrid deep learning model fuses the modern deep learning segmentation algorithm with traditional Active Contours segmentation technique. The model is tested against both state-of-the-art semantic segmentation and hybrid models for performance evaluation against contemporary work. The results obtained show that the pathologist like annotation could be achieved by developing such hybrid models that integrate the domain knowledge through classical segmentation methods like Active Contours and global knowledge through semantic segmentation deep learning models.      
### 15.An Energy-concentrated Wavelet Transform for Time Frequency Analysis of Transient Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.10690.pdf)
>  Transient signals are often composed of a series of modes that have multivalued time-dependent instantaneous frequency (IF), which brings challenges to the development of signal processing technology. Fortunately, the group delay (GD) of such signal can be well expressed as a single valued function of frequency. By considering the frequency-domain signal model, we present a postprocessing method called wavelet transform (WT)-based time-reassigned synchrosqueezing transform (WTSST). Our proposed method embeds a two-dimensional GD operator into a synchrosqueezing framework to generate a time-frequency representation (TFR) of transient signal with high energy concentration and allows to retrieve the whole or part of the signal. The theoretical analyses of the WTSST are provided, including the analysis of GD candidate accuracy and signal reconstruction accuracy. Moreover, based on WTSST, the WT-based time-reassigned multisynchrosqueezing transform (WTMSST) is proposed by introducing a stepwise refinement scheme, which further improves the drawback that the WTSST method is unable to deal with strong frequency-varying signal. Simulation and real signal analysis illustrate that the proposed methods have the capacity to appropriately describe the features of transient signals.      
### 16.Contrastive-mixup learning for improved speaker verification  [ :arrow_down: ](https://arxiv.org/pdf/2202.10672.pdf)
>  This paper proposes a novel formulation of prototypical loss with mixup for speaker verification. Mixup is a simple yet efficient data augmentation technique that fabricates a weighted combination of random data point and label pairs for deep neural network training. Mixup has attracted increasing attention due to its ability to improve robustness and generalization of deep neural networks. Although mixup has shown success in diverse domains, most applications have centered around closed-set classification tasks. In this work, we propose contrastive-mixup, a novel augmentation strategy that learns distinguishing representations based on a distance metric. During training, mixup operations generate convex interpolations of both inputs and virtual labels. Moreover, we have reformulated the prototypical loss function such that mixup is enabled on metric learning objectives. To demonstrate its generalization given limited training data, we conduct experiments by varying the number of available utterances from each speaker in the VoxCeleb database. Experimental results show that applying contrastive-mixup outperforms the existing baseline, reducing error rate by 16% relatively, especially when the number of training utterances per speaker is limited.      
### 17.On Local Distributions in Graph Signal Processing  [ :arrow_down: ](https://arxiv.org/pdf/2202.10649.pdf)
>  Graph filtering is the cornerstone operation in graph signal processing (GSP). Thus, understanding it is key in developing potent GSP methods. Graph filters are local and distributed linear operations, whose output depends only on the local neighborhood of each node. Moreover, a graph filter's output can be computed separately at each node by carrying out repeated exchanges with immediate neighbors. Graph filters can be compactly written as polynomials of a graph shift operator (typically, a sparse matrix description of the graph). This has led to relating the properties of the filters with the spectral properties of the corresponding matrix -- which encodes global structure of the graph. In this work, we propose a framework that relies solely on the local distribution of the neighborhoods of a graph. The crux of this approach is to describe graphs and graph signals in terms of a measurable space of rooted balls. Leveraging this, we are able to seamlessly compare graphs of different sizes and coming from different models, yielding results on the convergence of spectral densities, transferability of filters across arbitrary graphs, and continuity of graph signal properties with respect to the distribution of local substructures.      
### 18.Online Learning of Trellis Diagram Using Neural Network for Robust Detection and Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2202.10635.pdf)
>  This paper studies machine learning-assisted maximum likelihood (ML) and maximum a posteriori (MAP) receivers for a communication system with memory, which can be modelled by a trellis diagram. The prerequisite of the ML/MAP receiver is to obtain the likelihood of the received samples under different state transitions of the trellis diagram, which relies on the channel state information (CSI) and the distribution of the channel noise. We propose to learn the trellis diagram real-time using an artificial neural network (ANN) trained by a pilot sequence. This approach, termed as the online learning of trellis diagram (OLTD), requires neither the CSI nor statistics of the noise, and can be incorporated into the classic Viterbi and the BCJR algorithm. %Compared with the state-of-the-art ViterbiNet and BCJRNet algorithms in the literature, it It is shown to significantly outperform the model-based methods in non-Gaussian channels. It requires much less training overhead than the state-of-the-art methods, and hence is more feasible for real implementations. As an illustrative example, the OLTD-based BCJR is applied to a Bluetooth low energy (BLE) receiver trained only by a 256-sample pilot sequence. Moreover, the OLTD-based BCJR can accommodate for turbo equalization, while the state-of-the-art BCJRNet/ViterbiNet cannot. As an interesting by-product, we propose an enhancement to the BLE standard by introducing a bit interleaver to its physical layer; the resultant improvement of the receiver sensitivity can make it a better fit for some Internet of Things (IoT) communications.      
### 19.Disentangling Light Fields for Super-Resolution and Disparity Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2202.10603.pdf)
>  Light field (LF) cameras record both intensity and directions of light rays, and encode 3D scenes into 4D LF images. Recently, many convolutional neural networks (CNNs) have been proposed for various LF image processing tasks. However, it is challenging for CNNs to effectively process LF images since the spatial and angular information are highly inter-twined with varying disparities. In this paper, we propose a generic mechanism to disentangle these coupled information for LF image processing. Specifically, we first design a class of domain-specific convolutions to disentangle LFs from different dimensions, and then leverage these disentangled features by designing task-specific modules. Our disentangling mechanism can well incorporate the LF structure prior and effectively handle 4D LF data. Based on the proposed mechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp) for spatial super-resolution, angular super-resolution and disparity estimation. Experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efficiency, and generality of our disentangling mechanism. Project page: <a class="link-external link-https" href="https://yingqianwang.github.io/DistgLF/" rel="external noopener nofollow">this https URL</a>.      
### 20.VADOI:Voice-Activity-Detection Overlapping Inference For End-to-end Long-form Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.10593.pdf)
>  While end-to-end models have shown great success on the Automatic Speech Recognition task, performance degrades severely when target sentences are long-form. The previous proposed methods, (partial) overlapping inference are shown to be effective on long-form decoding. For both methods, word error rate (WER) decreases monotonically when overlapping percentage decreases. Setting aside computational cost, the setup with 50% overlapping during inference can achieve the best performance. However, a lower overlapping percentage has an advantage of fast inference speed. In this paper, we first conduct comprehensive experiments comparing overlapping inference and partial overlapping inference with various configurations. We then propose Voice-Activity-Detection Overlapping Inference to provide a trade-off between WER and computation cost. Results show that the proposed method can achieve a 20% relative computation cost reduction on Librispeech and Microsoft Speech Language Translation long-form corpus while maintaining the WER performance when comparing to the best performing overlapping inference algorithm. We also propose Soft-Match to compensate for similar words mis-aligned problem.      
### 21.Deep Iterative Phase Retrieval for Ptychography  [ :arrow_down: ](https://arxiv.org/pdf/2202.10573.pdf)
>  One of the most prominent challenges in the field of diffractive imaging is the phase retrieval (PR) problem: In order to reconstruct an object from its diffraction pattern, the inverse Fourier transform must be computed. This is only possible given the full complex-valued diffraction data, i.e. magnitude and phase. However, in diffractive imaging, generally only magnitudes can be directly measured while the phase needs to be estimated. In this work we specifically consider ptychography, a sub-field of diffractive imaging, where objects are reconstructed from multiple overlapping diffraction images. We propose an augmentation of existing iterative phase retrieval algorithms with a neural network designed for refining the result of each iteration. For this purpose we adapt and extend a recently proposed architecture from the speech processing field. Evaluation results show the proposed approach delivers improved convergence rates in terms of both iteration count and algorithm runtime.      
### 22.Ghost projection. II. Beam shaping using realistic spatially-random masks  [ :arrow_down: ](https://arxiv.org/pdf/2202.10572.pdf)
>  The spatial light modulator and optical data projector both rely on precisely configurable optical elements to shape a light beam. Here we explore an image-projection approach which does not require a configurable beam-shaping element. We term this approach {\em ghost projection} on account of its conceptual relation to computational ghost imaging. Instead of a configurable beam shaping element, the method transversely displaces a single illuminated mask, such as a spatially-random screen, to create specified distributions of radiant exposure. The method has potential applicability to image projection employing a variety of radiation and matter wave fields, such as hard x rays, neutrons, muons, atomic beams and molecular beams. Building on our previous theoretical and computational studies, we here seek to understand the effects, sensitivity, and tolerance of some key experimental limitations of the method. Focusing on the case of hard x rays, we employ experimentally acquired masks to numerically study the deleterious effects of photon shot noise, inaccuracies in random-mask exposure time, and inaccuracies in mask positioning, as well as adapting to spatially non-uniform illumination. Understanding the influence of these factors will assist in optimizing experimental design and work towards achieving ghost projection in practice.      
### 23.Energy-Efficient Respiratory Anomaly Detection in Premature Newborn Infants  [ :arrow_down: ](https://arxiv.org/pdf/2202.10570.pdf)
>  Precise monitoring of respiratory rate in premature infants is essential to initiate medical interventions as required. Wired technologies can be invasive and obtrusive to the patients. We propose a Deep Learning enabled wearable monitoring system for premature newborn infants, where respiratory cessation is predicted using signals that are collected wirelessly from a non-invasive wearable Bellypatch put on infant's body. We propose a five-stage design pipeline involving data collection and labeling, feature scaling, model selection with hyperparameter tuning, model training and validation, model testing and deployment. The model used is a 1-D Convolutional Neural Network (1DCNN) architecture with 1 convolutional layer, 1 pooling layer and 3 fully-connected layers, achieving 97.15% accuracy. To address energy limitations of wearable processing, several quantization techniques are explored and their performance and energy consumption are analyzed. We propose a novel Spiking-Neural-Network(SNN) based respiratory classification solution, which can be implemented on event-driven neuromorphic hardware. We propose an approach to convert the analog operations of our baseline 1DCNN to their spiking equivalent. We perform a design-space exploration using the parameters of the converted SNN to generate inference solutions having different accuracy and energy footprints. We select a solution that achieves 93.33% accuracy with 18 times lower energy compared with baseline 1DCNN model. Additionally the proposed SNN solution achieves similar accuracy but with 4 times less energy.      
### 24.CROMOSim: A Deep Learning-based Cross-modality Inertial Measurement Simulator  [ :arrow_down: ](https://arxiv.org/pdf/2202.10562.pdf)
>  With the prevalence of wearable devices, inertial measurement unit (IMU) data has been utilized in monitoring and assessment of human mobility such as human activity recognition (HAR). Training deep neural network (DNN) models for these tasks require a large amount of labeled data, which are hard to acquire in uncontrolled environments. To mitigate the data scarcity problem, we design CROMOSim, a cross-modality sensor simulator that simulates high fidelity virtual IMU sensor data from motion capture systems or monocular RGB cameras. It utilizes a skinned multi-person linear model (SMPL) for 3D body pose and shape representations, to enable simulation from arbitrary on-body positions. A DNN model is trained to learn the functional mapping from imperfect trajectory estimations in a 3D SMPL body tri-mesh due to measurement noise, calibration errors, occlusion and other modeling artifacts, to IMU data. We evaluate the fidelity of CROMOSim simulated data and its utility in data augmentation on various HAR datasets. Extensive experiment results show that the proposed model achieves a 6.7% improvement over baseline methods in a HAR task.      
### 25.Linking stability of nonlinear sampled-data systems and their continuous-time limits  [ :arrow_down: ](https://arxiv.org/pdf/2202.10549.pdf)
>  Discrete-time (DT) models of sampled-data systems can be regarded as predictions of the future state value given current values for the state, input and sampling period. A DT model is exact if its prediction coincides with the true value and is otherwise approximate. Conditions under which Semiglobal Exponential Stability under nonuniform sampling (SES-VSR) is preserved among different (exact or approximate) DT models when fed back with the same sampling-period dependent control law exist. The current paper proves that the exact DT model is SES-VSR if and only if its corresponding continuous-time (CT) limit (for infinitesimally small sampling period) is Globally Exponentially Stable (GES), which is restrictive. The contribution of this note consists in extending and relaxing the assumptions of previous results, as follows: (i) only mild conditions are given in order to link stability properties between DT models fed back with \emph{different} control laws, and (ii) the DT model properties making the CT limit Globally Asymptotically and Locally Exponentially Stable (GALES), instead of GES, are provided.      
### 26.Spanish and English Phoneme Recognition by Training on Simulated Classroom Audio Recordings of Collaborative Learning Environments  [ :arrow_down: ](https://arxiv.org/pdf/2202.10536.pdf)
>  Audio recordings of collaborative learning environments contain a constant presence of cross-talk and background noise. Dynamic speech recognition between Spanish and English is required in these environments. To eliminate the standard requirement of large-scale ground truth, the thesis develops a simulated dataset by transforming audio transcriptions into phonemes and using 3D speaker geometry and data augmentation to generate an acoustic simulation of Spanish and English speech. The thesis develops a low-complexity neural network for recognizing Spanish and English phonemes (available at <a class="link-external link-http" href="http://github.com/muelitas/keywordRec" rel="external noopener nofollow">this http URL</a>). When trained on 41 English phonemes, 0.099 PER is achieved on Speech Commands. When trained on 36 Spanish phonemes and tested on real recordings of collaborative learning environments, a 0.7208 LER is achieved. Slightly better than Google's Speech-to-text 0.7272 LER, which used anywhere from 15 to 1,635 times more parameters and trained on 300 to 27,500 hours of real data as opposed to 13 hours of simulated audios.      
### 27.Demand and Price Fluctuations Effect on Risk and Profit of Single and Clustered Microgrids during COVID-19 Pandemic  [ :arrow_down: ](https://arxiv.org/pdf/2202.10494.pdf)
>  COVID19s widespread distribution is wreaking havoc on peoples lives all over the world. This pandemic has also had a significant impact on energy consumption. Its influence can be seen in the power systems operation and the market as well. The power consumers habits and demand curves have been changed at a breakneck pace. In this work, a one year mixed integer programming (MIP) problem has been developed to compare the power consumption between 2019 and 2020 in the United States as an example regarding the COVID19 pandemic effect in order to better prepare for possible similar future events. 100 percent renewable single microgrids (SMGs) are studied using wind turbines and photovoltaics. Batteries are also employed since it is inevitable when the system uses renewables. Additionally, it is possible for the SMGs to trade power with the main grid as needed. The effect of the SMGs clustering to form the multi microgrids (MMGs) is also considered. In order to investigate the risk of the system during the COVID19 and formation of MMG, downside risk constraints are applied to the proposed model. Furthermore, a stylized short run consumers demand model is proposed, using elasticity and assessed responses regarding the average household consumption for households during on peak and off peak periods. The simulation results show that COVID19 generally reduces the demand, increases the profit of the system, and decreases the economic risk of the power systems operation. Moreover, SMGs clustering to organize MMG dramatically enhances the profit of the system as well as improves the risk level of the system.      
### 28.Semi-Blind Joint Channel and Symbol Estimation in IRS-Assisted Multi-User MIMO Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.11087.pdf)
>  Intelligent reflecting surface (IRS) is a promising technology for beyond 5G wireless communications. In fully passive IRS-assisted systems, channel estimation is challenging and should be carried out only at the base station or at the terminals since the elements of the IRS are incapable of processing signals. In this letter, we formulate a tensor-based semi-blind receiver that solves the joint channel and symbol estimation problem in an IRS-assisted multi-user multiple-input multiple-output system. The proposed approach relies on a generalized PARATUCK tensor model of the signals reflected by the IRS, based on a two-stage closed-form semi-blind receiver using Khatri-Rao and Kronecker factorizations. Simulation results demonstrate the superior performance of the proposed semi-blind receiver, in terms of the normalized mean squared error and symbol error rate, as well as a lower computational complexity, compared to recently proposed parallel factor analysis-based receivers.      
### 29.A Survey on Scalable LoRaWAN for Massive IoT: Recent Advances, Potentials, and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2202.11082.pdf)
>  Long Range (LoRa) is the most widely used technology for enabling Low Power Wide Area Networks (LPWANs) on unlicensed frequency bands. Despite its modest Data Rates (DRs), it provides extensive coverage for low-power devices, making it an ideal communication system for many Internet of Things (IoT) applications. In general, LoRa radio is considered as the physical layer, whereas Long Range Wide Area Networks (LoRaWAN) is the MAC layer of the LoRa stack that adopts star topology to enable communication between multiple End Devices (EDs) and the network Gateway (GW). The Chirp Spread Spectrum (CSS) modulation deals with LoRa signals interference and ensures long-range communication. At the same time, the Adaptive Data Rate (ADR) mechanism allows EDs to dynamically alter some LoRa features such as the Spreading Factor (SF), Code Rate (CR), and carrier frequency to address the time variance of communication conditions in dense networks. Despite the high LoRa connectivity demand, LoRa signals interference and concurrent transmission collisions are major limitations. Therefore, to enhance LoRaWAN capacity, the LoRa alliance released many LoRaWAN versions, and the research community provided numerous solutions to develop scalable LoRaWAN technology. Hence, we thoroughly examined LoRaWAN scalability challenges and the state-of-the-art solutions in both the PHY and MAC layers. Most of these solutions rely on SF, logical, and frequency channel assignment, while others propose new network topologies or implement signal processing schemes to cancel the interference and allow LoRaWAN to connect more EDs efficiently. A summary of the existing solutions in the literature is provided at the end of the paper by describing the advantages and drawbacks of each solution and suggesting possible enhancements as future research directions.      
### 30.Banding vs. Quality: Perceptual Impact and Objective Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2202.11038.pdf)
>  Staircase-like contours introduced to a video by quantization in flat areas, commonly known as banding, have been a long-standing problem in both video processing and quality assessment communities. The fact that even a relatively small change of the original pixel values can result in a strong impact on perceived quality makes banding especially difficult to be detected by objective quality metrics. In this paper, we study how banding annoyance compares to more commonly studied scaling and compression artifacts with respect to the overall perceptual quality. We further propose a simple combination of VMAF and the recently developed banding index, CAMBI, into a banding-aware video quality metric showing improved correlation with overall perceived quality.      
### 31.DRVC: A Framework of Any-to-Any Voice Conversion with Self-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.10976.pdf)
>  Any-to-any voice conversion problem aims to convert voices for source and target speakers, which are out of the training data. Previous works wildly utilize the disentangle-based models. The disentangle-based model assumes the speech consists of content and speaker style information and aims to untangle them to change the style information for conversion. Previous works focus on reducing the dimension of speech to get the content information. But the size is hard to determine to lead to the untangle overlapping problem. We propose the Disentangled Representation Voice Conversion (DRVC) model to address the issue. DRVC model is an end-to-end self-supervised model consisting of the content encoder, timbre encoder, and generator. Instead of the previous work for reducing speech size to get content, we propose a cycle for restricting the disentanglement by the Cycle Reconstruct Loss and Same Loss. The experiments show there is an improvement for converted speech on quality and voice similarity.      
### 32.A Deep Reinforcement Learning based Approach for NOMA-based Random Access Network with Truncated Channel Inversion Power Control  [ :arrow_down: ](https://arxiv.org/pdf/2202.10955.pdf)
>  As a main use case of 5G and Beyond wireless network, the ever-increasing machine type communications (MTC) devices pose critical challenges over MTC network in recent years. It is imperative to support massive MTC devices with limited resources. To this end, Non-orthogonal multiple access (NOMA) based random access network has been deemed as a prospective candidate for MTC network. In this paper, we propose a deep reinforcement learning (RL) based approach for NOMA-based random access network with truncated channel inversion power control. Specifically, each MTC device randomly selects a pre-defined power level with a certain probability for data transmission. Devices are using channel inversion power control yet subject to the upper bound of the transmission power. Due to the stochastic feature of the channel fading and the limited transmission power, devices with different achievable power levels have been categorized as different types of devices. In order to achieve high throughput with considering the fairness between all devices, two objective functions are formulated. One is to maximize the minimum long-term expected throughput of all MTC devices, the other is to maximize the geometric mean of the long-term expected throughput for all MTC devices. A Policy based deep reinforcement learning approach is further applied to tune the transmission probabilities of each device to solve the formulated optimization problems. Extensive simulations are conducted to show the merits of our proposed approach.      
### 33.UAV/HAP-Assisted Vehicular Edge Computing in 6G: Where and What to Offload?  [ :arrow_down: ](https://arxiv.org/pdf/2202.10953.pdf)
>  In the context of 6th generation (6G) networks, vehicular edge computing (VEC) is emerging as a promising solution to let battery-powered ground vehicles with limited computing and storage resources offload processing tasks to more powerful devices. Given the dynamic vehicular environment, VEC systems need to be as flexible, intelligent, and adaptive as possible. To this aim, in this paper we study the opportunity to realize VEC via non-terrestrial networks (NTNs), where ground vehicles offload resource-hungry tasks to Unmanned Aerial Vehicles (UAVs), High Altitude Platforms (HAPs), or a combination of the two. We define an optimization problem in which tasks are modeled as a Poisson arrival process, and apply queuing theory to find the optimal offloading factor in the system. Numerical results show that aerial-assisted VEC is feasible even in dense networks, provided that high-capacity HAP/UAV platforms are available.      
### 34.Optical Field Characterization using Off-axis Digital Holography  [ :arrow_down: ](https://arxiv.org/pdf/2202.10949.pdf)
>  Angular resolved digital holography is presented as a technique for real-time characterization of the full optical field (amplitude and phase) of space-division multiplexing components and fibers, here a 6-mode photonic-lantern is characterized.      
### 35.Subtyping brain diseases from imaging data  [ :arrow_down: ](https://arxiv.org/pdf/2202.10945.pdf)
>  The imaging community has increasingly adopted machine learning (ML) methods to provide individualized imaging signatures related to disease diagnosis, prognosis, and response to treatment. Clinical neuroscience and cancer imaging have been two areas in which ML has offered particular promise. However, many neurologic and neuropsychiatric diseases, as well as cancer, are often heterogeneous in terms of their clinical manifestations, neuroanatomical patterns or genetic underpinnings. Therefore, in such cases, seeking a single disease signature might be ineffectual in delivering individualized precision diagnostics. The current chapter focuses on ML methods, especially semi-supervised clustering, that seek disease subtypes using imaging data. Work from Alzheimer Disease and its prodromal stages, psychosis, depression, autism, and brain cancer are discussed. Our goal is to provide the readers with a broad overview in terms of methodology and clinical applications.      
### 36.Recognizing Concepts and Recognizing Musical Themes. A Quantum Semantic Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.10941.pdf)
>  How are abstract concepts and musical themes recognized on the basis of some previous experience? It is interesting to compare the different behaviors of human and of artificial intelligences with respect to this problem. Generally, a human mind that abstracts a concept (say, table) from a given set of known examples creates a table-Gestalt: a kind of vague and out of focus image that does not fully correspond to a particular table with well determined features. A similar situation arises in the case of musical themes. Can the construction of a gestaltic pattern, which is so natural for human minds, be taught to an intelligent machine? This problem can be successfully discussed in the framework of a quantum approach to pattern recognition and to machine learning. The basic idea is replacing classical data sets with quantum data sets, where either objects or musical themes can be formally represented as pieces of quantum information, involving the uncertainties and the ambiguities that characterize the quantum world. In this framework, the intuitive concept of Gestalt can be simulated by the mathematical concept of positive centroid of a given quantum data set. Accordingly, the crucial problem "how can we classify a new object or a new musical theme (we have listened to) on the basis of a previous experience?" can be dealt with in terms of some special quantum similarity-relations. Although recognition procedures are different for human and for artificial intelligences, there is a common method of "facing the problems" that seems to work in both cases.      
### 37.Remaining Useful Life Prediction Using Temporal Deep Degradation Network for Complex Machinery with Attention-based Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2202.10916.pdf)
>  The precise estimate of remaining useful life (RUL) is vital for the prognostic analysis and predictive maintenance that can significantly reduce failure rate and maintenance costs. The degradation-related features extracted from the sensor streaming data with neural networks can dramatically improve the accuracy of the RUL prediction. The Temporal deep degradation network (TDDN) model is proposed to make the RUL prediction with the degradation-related features given by the one-dimensional convolutional neural network (1D CNN) feature extraction and attention mechanism. 1D CNN is used to extract the temporal features from the streaming sensor data. Temporal features have monotonic degradation trends from the fluctuating raw sensor streaming data. Attention mechanism can improve the RUL prediction performance by capturing the fault characteristics and the degradation development with the attention weights. The performance of the TDDN model is evaluated on the public C-MAPSS dataset and compared with the existing methods. The results show that the TDDN model can achieve the best RUL prediction accuracy in complex conditions compared to current machine learning models. The degradation-related features extracted from the high-dimension sensor streaming data demonstrate the clear degradation trajectories and degradation stages that enable TDDN to predict the turbofan-engine RUL accurately and efficiently.      
### 38.Sound Adversarial Audio-Visual Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2202.10910.pdf)
>  Audio-visual navigation task requires an agent to find a sound source in a realistic, unmapped 3D environment by utilizing egocentric audio-visual observations. Existing audio-visual navigation works assume a clean environment that solely contains the target sound, which, however, would not be suitable in most real-world applications due to the unexpected sound noise or intentional interference. In this work, we design an acoustically complex environment in which, besides the target sound, there exists a sound attacker playing a zero-sum game with the agent. More specifically, the attacker can move and change the volume and category of the sound to make the agent suffer from finding the sounding object while the agent tries to dodge the attack and navigate to the goal under the intervention. Under certain constraints to the attacker, we can improve the robustness of the agent towards unexpected sound attacks in audio-visual navigation. For better convergence, we develop a joint training mechanism by employing the property of a centralized critic with decentralized actors. Experiments on two real-world 3D scan datasets, Replica, and Matterport3D, verify the effectiveness and the robustness of the agent trained under our designed environment when transferred to the clean environment or the one containing sound attackers with random policy. Project: \url{<a class="link-external link-https" href="https://yyf17.github.io/SAAVN" rel="external noopener nofollow">this https URL</a>}.      
### 39.Tracking moving objects through scattering media via speckle correlations  [ :arrow_down: ](https://arxiv.org/pdf/2202.10804.pdf)
>  Scattering can rapidly degrade our ability to form an optical image, to the point where only speckle-like patterns can be measured. Truly non-invasive imaging through a strongly scattering obstacle is difficult, and usually reliant on a computationally intensive numerical reconstruction. In this work we show that, by combining the cross-correlations of the measured speckle pattern at different times, it is possible to track a moving object with minimal computational effort and over a large field of view.      
### 40.Secure Joint Communication and Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2202.10790.pdf)
>  This work considers mitigation of information leakage between communication and sensing operations in joint communication and sensing systems. Specifically, a discrete memoryless state-dependent broadcast channel model is studied in which (i) the presence of feedback enables a transmitter to simultaneously achieve reliable communication and channel state estimation; (ii) one of the receivers is treated as an eavesdropper whose state should be estimated but which should remain oblivious to a part of the transmitted information. The model abstracts the challenges behind security for joint communication and sensing if one views the channel state as a characteristic of the receiver, e.g., its location. For independent identically distributed (i.i.d.) states, perfect output feedback, and when part of the transmitted message should be kept secret, a partial characterization of the secrecy-distortion region is developed. The characterization is exact when the broadcast channel is either physically-degraded or reversely-physically-degraded. The characterization is also extended to the situation in which the entire transmitted message should be kept secret. The benefits of a joint approach compared to separation-based secure communication and state-sensing methods are illustrated with a binary joint communication and sensing model.      
### 41.Convolutional Neural Network Modelling for MODIS Land Surface Temperature Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2202.10753.pdf)
>  Nowadays, thermal infrared satellite remote sensors enable to extract very interesting information at large scale, in particular Land Surface Temperature (LST). However such data are limited in spatial and/or temporal resolutions which prevents from an analysis at fine scales. For example, MODIS satellite provides daily acquisitions with 1Km spatial resolutions which is not sufficient to deal with highly heterogeneous environments as agricultural parcels. Therefore, image super-resolution is a crucial task to better exploit MODIS LSTs. This issue is tackled in this paper. We introduce a deep learning-based algorithm, named Multi-residual U-Net, for super-resolution of MODIS LST single-images. Our proposed network is a modified version of U-Net architecture, which aims at super-resolving the input LST image from 1Km to 250m per pixel. The results show that our Multi-residual U-Net outperforms other state-of-the-art methods.      
### 42.Improving Cross-lingual Speech Synthesis with Triplet Training Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2202.10729.pdf)
>  Recent advances in cross-lingual text-to-speech (TTS) made it possible to synthesize speech in a language foreign to a monolingual speaker. However, there is still a large gap between the pronunciation of generated cross-lingual speech and that of native speakers in terms of naturalness and intelligibility. In this paper, a triplet training scheme is proposed to enhance the cross-lingual pronunciation by allowing previously unseen content and speaker combinations to be seen during training. Proposed method introduces an extra fine-tune stage with triplet loss during training, which efficiently draws the pronunciation of the synthesized foreign speech closer to those from the native anchor speaker, while preserving the non-native speaker's timbre. Experiments are conducted based on a state-of-the-art baseline cross-lingual TTS system and its enhanced variants. All the objective and subjective evaluations show the proposed method brings significant improvement in both intelligibility and naturalness of the synthesized cross-lingual speech.      
### 43.nnSpeech: Speaker-Guided Conditional Variational Autoencoder for Zero-shot Multi-speaker Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2202.10712.pdf)
>  Multi-speaker text-to-speech (TTS) using a few adaption data is a challenge in practical applications. To address that, we propose a zero-shot multi-speaker TTS, named nnSpeech, that could synthesis a new speaker voice without fine-tuning and using only one adaption utterance. Compared with using a speaker representation module to extract the characteristics of new speakers, our method bases on a speaker-guided conditional variational autoencoder and can generate a variable Z, which contains both speaker characteristics and content information. The latent variable Z distribution is approximated by another variable conditioned on reference mel-spectrogram and phoneme. Experiments on the English corpus, Mandarin corpus, and cross-dataset proves that our model could generate natural and similar speech with only one adaption speech.      
### 44.Decentralized Safe Multi-agent Stochastic Optimal Control using Deep FBSDEs and ADMM  [ :arrow_down: ](https://arxiv.org/pdf/2202.10658.pdf)
>  In this work, we propose a novel safe and scalable decentralized solution for multi-agent control in the presence of stochastic disturbances. Safety is mathematically encoded using stochastic control barrier functions and safe controls are computed by solving quadratic programs. Decentralization is achieved by augmenting to each agent's optimization variables, copy variables, for its neighbors. This allows us to decouple the centralized multi-agent optimization problem. However, to ensure safety, neighboring agents must agree on "what is safe for both of us" and this creates a need for consensus. To enable safe consensus solutions, we incorporate an ADMM-based approach. Specifically, we propose a Merged CADMM-OSQP implicit neural network layer, that solves a mini-batch of both, local quadratic programs as well as the overall consensus problem, as a single optimization problem. This layer is embedded within a Deep FBSDEs network architecture at every time step, to facilitate end-to-end differentiable, safe and decentralized stochastic optimal control. The efficacy of the proposed approach is demonstrated on several challenging multi-robot tasks in simulation. By imposing requirements on safety specified by collision avoidance constraints, the safe operation of all agents is ensured during the entire training process. We also demonstrate superior scalability in terms of computational and memory savings as compared to a centralized approach.      
### 45.Multiview Scattering Scanning Imaging Confocal Microscopy through a Multimode Fiber  [ :arrow_down: ](https://arxiv.org/pdf/2202.10644.pdf)
>  Confocal and multiphoton microscopy are effective techniques to obtain high-contrast images of 2-D sections within bulk tissue. However, scattering limits their application to depths only up to ~1 millimeter. Multimode fibers make excellent ultrathin endoscopes that can penetrate deep inside the tissue with minimal damage. Here, we present Multiview Scattering Scanning Imaging Confocal (MUSSIC) Microscopy that enables high signal-to-noise ratio (SNR) imaging through a multimode fiber, hence combining the optical sectioning and resolution gain of confocal microscopy with the minimally invasive penetration capability of multimode fibers. The key advance presented here is the high SNR image reconstruction enabled by employing multiple coplanar virtual pinholes to capture multiple perspectives of the object, re-shifting them appropriately and combining them to obtain a high-contrast and high-resolution confocal image. We present the theory for the gain in contrast and resolution in MUSSIC microscopy and validate the concept through experimental results.      
### 46.Hidden bawls, whispers, and yelps: can text be made to sound more than just its words?  [ :arrow_down: ](https://arxiv.org/pdf/2202.10631.pdf)
>  Whether a word was bawled, whispered, or yelped, captions will typically represent it in the same way. If they are your only way to access what is being said, subjective nuances expressed in the voice will be lost. Since so much of communication is carried by these nuances, we posit that if captions are to be used as an accurate representation of speech, embedding visual representations of paralinguistic qualities into captions could help readers use them to better understand speech beyond its mere textual content. This paper presents a model for processing vocal prosody (its loudness, pitch, and duration) and mapping it into visual dimensions of typography (respectively, font-weight, baseline shift, and letter-spacing), creating a visual representation of these lost vocal subtleties that can be embedded directly into the typographical form of text. An evaluation was carried out where participants were exposed to this speech-modulated typography and asked to match it to its originating audio, presented between similar alternatives. Participants (n=117) were able to correctly identify the original audios with an average accuracy of 65%, with no significant difference when showing them modulations as animated or static text. Additionally, participants' comments showed their mental models of speech-modulated typography varied widely.      
### 47.Myriad: a real-world testbed to bridge trajectory optimization and deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.10600.pdf)
>  We present Myriad, a testbed written in JAX for learning and planning in real-world continuous environments. The primary contributions of Myriad are threefold. First, Myriad provides machine learning practitioners access to trajectory optimization techniques for application within a typical automatic differentiation workflow. Second, Myriad presents many real-world optimal control problems, ranging from biology to medicine to engineering, for use by the machine learning community. Formulated in continuous space and time, these environments retain some of the complexity of real-world systems often abstracted away by standard benchmarks. As such, Myriad strives to serve as a stepping stone towards application of modern machine learning techniques for impactful real-world tasks. Finally, we use the Myriad repository to showcase a novel approach for learning and control tasks. Trained in a fully end-to-end fashion, our model leverages an implicit planning module over neural ordinary differential equations, enabling simultaneous learning and planning with complex environment dynamics.      
### 48.Adversarial Attacks on Speech Recognition Systems for Mission-Critical Applications: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2202.10594.pdf)
>  A Machine-Critical Application is a system that is fundamentally necessary to the success of specific and sensitive operations such as search and recovery, rescue, military, and emergency management actions. Recent advances in Machine Learning, Natural Language Processing, voice recognition, and speech processing technologies have naturally allowed the development and deployment of speech-based conversational interfaces to interact with various machine-critical applications. While these conversational interfaces have allowed users to give voice commands to carry out strategic and critical activities, their robustness to adversarial attacks remains uncertain and unclear. Indeed, Adversarial Artificial Intelligence (AI) which refers to a set of techniques that attempt to fool machine learning models with deceptive data, is a growing threat in the AI and machine learning research community, in particular for machine-critical applications. The most common reason of adversarial attacks is to cause a malfunction in a machine learning model. An adversarial attack might entail presenting a model with inaccurate or fabricated samples as it's training data, or introducing maliciously designed data to deceive an already trained model. While focusing on speech recognition for machine-critical applications, in this paper, we first review existing speech recognition techniques, then, we investigate the effectiveness of adversarial attacks and defenses against these systems, before outlining research challenges, defense recommendations, and future work. This paper is expected to serve researchers and practitioners as a reference to help them in understanding the challenges, position themselves and, ultimately, help them to improve existing models of speech recognition for mission-critical applications. Keywords: Mission-Critical Applications, Adversarial AI, Speech Recognition Systems.      
### 49.Heuristic Sensing Schemes for Four-Target Detection in Time-Constrained Vector Poisson and Gaussian Channels  [ :arrow_down: ](https://arxiv.org/pdf/2202.10563.pdf)
>  In this work, we investigate the different sensing schemes for the detection of four targets as observed through a vector Poisson and Gaussian channels when the sensing time resource is limited and the source signals can be observed through a variety of sum combinations during that fixed time. For this purpose, we can maximize the mutual information or the detection probability with respect to the time allocated to different sum combinations, for a given total fixed time. It is observed that for both Poisson and Gaussian channels; mutual information and Bayes risk with $0-1$ cost are not necessarily consistent with each other. Concavity of mutual information between input and output, for certain sensing schemes, in Poisson channel and Gaussian channel is shown to be concave w.r.t given times as linear time constraint is imposed. No optimal sensing scheme for any of the two channels is investigated in this work.      
### 50.Guidelines and evaluation for clinical explainable AI on medical image analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.10553.pdf)
>  Explainable artificial intelligence (XAI) is essential for enabling clinical users to get informed decision support from AI and comply with evidence-based medical practice. Applying XAI in clinical settings requires proper evaluation criteria to ensure the explanation technique is both technically sound and clinically useful, but specific support is lacking to achieve this goal. To bridge the research gap, we propose the Clinical XAI Guidelines that consist of five criteria a clinical XAI needs to be optimized for. The guidelines recommend choosing an explanation form based on Guideline 1 (G1) Understandability and G2 Clinical relevance. For the chosen explanation form, its specific XAI technique should be optimized for G3 Truthfulness, G4 Informative plausibility, and G5 Computational efficiency. <br>Following the guidelines, we conducted a systematic evaluation on a novel problem of multi-modal medical image explanation with two clinical tasks, and proposed new evaluation metrics accordingly. The evaluated 16 commonly-used heatmap XAI techniques were not suitable for clinical use due to their failure in \textbf{G3} and \textbf{G4}. Our evaluation demonstrated the use of Clinical XAI Guidelines to support the design and evaluation for clinically viable XAI.      
### 51.Feasibility Study of Multi-Site Split Learning for Privacy-Preserving Medical Systems under Data Imbalance Constraints in COVID-19, X-Ray, and Cholesterol Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2202.10456.pdf)
>  It seems as though progressively more people are in the race to upload content, data, and information online; and hospitals haven't neglected this trend either. Hospitals are now at the forefront for multi-site medical data sharing to provide groundbreaking advancements in the way health records are shared and patients are diagnosed. Sharing of medical data is essential in modern medical research. Yet, as with all data sharing technology, the challenge is to balance improved treatment with protecting patient's personal information. This paper provides a novel split learning algorithm coined the term, "multi-site split learning", which enables a secure transfer of medical data between multiple hospitals without fear of exposing personal data contained in patient records. It also explores the effects of varying the number of end-systems and the ratio of data-imbalance on the deep learning performance. A guideline for the most optimal configuration of split learning that ensures privacy of patient data whilst achieving performance is empirically given. We argue the benefits of our multi-site split learning algorithm, especially regarding the privacy preserving factor, using CT scans of COVID-19 patients, X-ray bone scans, and cholesterol level medical data.      
### 52.Predicting emotion from music videos: exploring the relative contribution of visual and auditory information to affective responses  [ :arrow_down: ](https://arxiv.org/pdf/2202.10453.pdf)
>  Although media content is increasingly produced, distributed, and consumed in multiple combinations of modalities, how individual modalities contribute to the perceived emotion of a media item remains poorly understood. In this paper we present MusicVideos (MuVi), a novel dataset for affective multimedia content analysis to study how the auditory and visual modalities contribute to the perceived emotion of media. The data were collected by presenting music videos to participants in three conditions: music, visual, and audiovisual. Participants annotated the music videos for valence and arousal over time, as well as the overall emotion conveyed. We present detailed descriptive statistics for key measures in the dataset and the results of feature importance analyses for each condition. Finally, we propose a novel transfer learning architecture to train Predictive models Augmented with Isolated modality Ratings (PAIR) and demonstrate the potential of isolated modality ratings for enhancing multimodal emotion recognition. Our results suggest that perceptions of arousal are influenced primarily by auditory information, while perceptions of valence are more subjective and can be influenced by both visual and auditory information. The dataset is made publicly available.      
