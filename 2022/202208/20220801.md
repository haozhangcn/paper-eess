# ArXiv eess --Mon, 1 Aug 2022
### 1.Artifact Identification in X-ray Diffraction Data using Machine Learning Methods  [ :arrow_down: ](https://arxiv.org/pdf/2207.14804.pdf)
>  The in situ synchrotron high-energy X-ray powder diffraction (XRD) technique is highly utilized by researchers to analyze the crystallographic structures of materials in functional devices (e.g., battery materials) or in complex sample environments (e.g., diamond anvil cells or syntheses reactors). An atomic structure of a material can be identified by its diffraction pattern, along with detailed analysis such as Rietveld refinement which indicates how the measured structure deviates from the ideal structure (e.g., internal stresses or defects). For in situ experiments, a series of XRD images is usually collected on the same sample at different conditions (e.g., adiabatic conditions), yielding different states of matter, or simply collected continuously as a function of time to track the change of a sample over a chemical or physical process. In situ experiments are usually performed with area detectors, collecting 2D images composed of diffraction rings for ideal powders. Depending on the material's form, one may observe different characteristics other than the typical Debye Scherrer rings for a realistic sample and its environments, such as textures or preferred orientations and single crystal diffraction spots in the 2D XRD image. In this work, we present an investigation of machine learning methods for fast and reliable identification and separation of the single crystal diffraction spots in XRD images. The exclusion of artifacts during an XRD image integration process allows a precise analysis of the powder diffraction rings of interest. We observe that the gradient boosting method can consistently produce high accuracy results when it is trained with small subsets of highly diverse datasets. The method dramatically decreases the amount of time spent on identifying and separating single crystal spots in comparison to the conventional method.      
### 2.Robust Quantitative Susceptibility Mapping via Approximate Message Passing  [ :arrow_down: ](https://arxiv.org/pdf/2207.14709.pdf)
>  Purpose: It has been challenging to recover QSM in the presence of phase errors, which could be caused by the noise or strong local susceptibility shifts in cases of brain hemorrhage and calcification. We propose a Bayesian formulation for QSM where a two-component Gaussian-mixture distribution is used to model the long-tailed noise (error) distribution, and design an approximate message passing (AMP) algorithm with automatic and adaptive parameter estimation. <br>Theory: Wavelet coefficients of the susceptibility map follow the Laplace distribution. The measurement noise follows a two-component Gaussian-mixture distribution where the second Gaussian component models the noise outliers. The distribution parameters are treated as unknown variables and jointly recovered with the susceptibility using AMP. <br>Methods: The proposed AMP with parameter estimation (AMP-PE) is compared with the state-of-the-art nonlinear L1-QSM and MEDI approaches that adopt the L1-norm and L2-norm data-fidelity terms respectively. The three approaches are tested on the Sim2Snr1 data from QSM challenge 2.0, the in vivo data from both healthy and hemorrhage scans. <br>Results: On the simulated Sim2Snr1 dataset, AMP-PE achieved the lowest NRMSE and SSIM, MEDI achieved the lowest HFEN, and each approach also has its own strong suit when it comes to various local evaluation metrics. On the in vivo dataset, AMP-PE is better at preserving structural details and removing streaking artifacts than L1-QSM and MEDI. <br>Conclusion: By leveraging a customized Gaussian-mixture noise prior, AMP-PE achieves better performance on the challenging QSM cases involving hemorrhage and calcification. It is equipped with built-in parameter estimation, which avoids subjective bias from the usual visual fine-tuning step of in vivo reconstruction.      
### 3.Improving Small Lesion Segmentation in CT Scans using Intensity Distribution Supervision: Application to Small Bowel Carcinoid Tumor  [ :arrow_down: ](https://arxiv.org/pdf/2207.14700.pdf)
>  Finding small lesions is very challenging due to lack of noticeable features, severe class imbalance, as well as the size itself. One approach to improve small lesion segmentation is to reduce the region of interest and inspect it at a higher sensitivity rather than performing it for the entire region. It is usually implemented as sequential or joint segmentation of organ and lesion, which requires additional supervision on organ segmentation. Instead, we propose to utilize an intensity distribution of a target lesion at no additional labeling cost to effectively separate regions where the lesions are possibly located from the background. It is incorporated into network training as an auxiliary task. We applied the proposed method to segmentation of small bowel carcinoid tumors in CT scans. We observed improvements for all metrics (33.5% $\rightarrow$ 38.2%, 41.3% $\rightarrow$ 47.8%, 30.0% $\rightarrow$ 35.9% for the global, per case, and per tumor Dice scores, respectively.) compared to the baseline method, which proves the validity of our idea. Our method can be one option for explicitly incorporating intensity distribution information of a target in network training.      
### 4.Replacing the Framingham-based equation for prediction of cardiovascular disease risk and adverse outcome by using artificial intelligence and retinal imaging  [ :arrow_down: ](https://arxiv.org/pdf/2207.14685.pdf)
>  Purpose: To create and evaluate the accuracy of an artificial intelligence Deep learning platform (ORAiCLE) capable of using only retinal fundus images to predict both an individuals overall 5 year cardiovascular risk (CVD) and the relative contribution of the component risk factors that comprise this risk. Methods: We used 165,907 retinal images from a database of 47,236 patient visits. Initially, each image was paired with biometric data age, ethnicity, sex, presence and duration of diabetes a HDL/LDL ratios as well as any CVD event wtihin 5 years of the retinal image acquisition. A risk score based on Framingham equations was calculated. The real CVD event rate was also determined for the individuals and overall population. Finally, ORAiCLE was trained using only age, ethnicity, sex plus retinal images. Results: Compared to Framingham-based score, ORAiCLE was up to 12% more accurate in prediciting cardiovascular event in he next 5-years, especially for the highest risk group of people. The reliability and accuracy of each of the restrictive models was suboptimal to ORAiCLE performance ,indicating that it was using data from both sets of data to derive its final results. Conclusion: Retinal photography is inexpensive and only minimal training is required to acquire them as fully automated, inexpensive camera systems are now widely available. As such, AI-based CVD risk algorithms such as ORAiCLE promise to make CV health screening more accurate, more afforadable and more accessible for all. Furthermore, ORAiCLE unique ability to assess the relative contribution of the components that comprise an individuals overall risk would inform treatment decisions based on the specific needs of an individual, thereby increasing the likelihood of positive health outcomes.      
### 5.Going Off-Grid: Continuous Implicit Neural Representations for 3D Vascular Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2207.14663.pdf)
>  Personalised 3D vascular models are valuable for diagnosis, prognosis and treatment planning in patients with cardiovascular disease. Traditionally, such models have been constructed with explicit representations such as meshes and voxel masks, or implicit representations such as radial basis functions or atomic (tubular) shapes. Here, we propose to represent surfaces by the zero level set of their signed distance function (SDF) in a differentiable implicit neural representation (INR). This allows us to model complex vascular structures with a representation that is implicit, continuous, light-weight, and easy to integrate with deep learning algorithms. We here demonstrate the potential of this approach with three practical examples. First, we obtain an accurate and watertight surface for an abdominal aortic aneurysm (AAA) from CT images and show robust fitting from as little as 200 points on the surface. Second, we simultaneously fit nested vessel walls in a single INR without intersections. Third, we show how 3D models of individual arteries can be smoothly blended into a single watertight surface. Our results show that INRs are a flexible representation with potential for minimally interactive annotation and manipulation of complex vascular structures.      
### 6.SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data  [ :arrow_down: ](https://arxiv.org/pdf/2207.14650.pdf)
>  Artificial intelligence (AI), machine learning, and deep learning (DL) methods are becoming increasingly important in the field of biomedical image analysis. However, to exploit the full potential of such methods, a representative number of experimentally acquired images containing a significant number of manually annotated objects is needed as training data. Here we introduce SYNTA (synthetic data) as a novel approach for the generation of synthetic, photo-realistic, and highly complex biomedical images as training data for DL systems. We show the versatility of our approach in the context of muscle fiber and connective tissue analysis in histological sections. We demonstrate that it is possible to perform robust and expert-level segmentation tasks on previously unseen real-world data, without the need for manual annotations using synthetic training data alone. Being a fully parametric technique, our approach poses an interpretable and controllable alternative to Generative Adversarial Networks (GANs) and has the potential to significantly accelerate quantitative image analysis in a variety of biomedical applications in microscopy and beyond.      
### 7.Active Distribution System Coordinated Control Method via Artificial Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2207.14642.pdf)
>  The increasing deployment of end use power resources in distribution systems created active distribution systems. Uncontrolled active distribution systems exhibit wide variations of voltage and loading throughout the day as some of these resources operate under max power tracking control of highly variable wind and solar irradiation while others exhibit random variations and/or dependency on weather conditions. It is necessary to control the system to provide power reliably and securely under normal voltages and frequency. Classical optimization approaches to control the system towards this goal suffer from the dimensionality of the problem and the need for a global optimization approach to coordinate a huge number of small resources. Artificial Intelligence (AI) methods offer an alternative that can provide a practical approach to this problem. We suggest that neural networks with self-attention mechanisms have the potential to aid in the optimization of the system. In this paper, we present this approach and provide promising preliminary results.      
### 8.Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.14607.pdf)
>  The availability of data in expressive styles across languages is limited, and recording sessions are costly and time consuming. To overcome these issues, we demonstrate how to build low-resource, neural text-to-speech (TTS) voices with only 1 hour of conversational speech, when no other conversational data are available in the same language. Assuming the availability of non-expressive speech data in that language, we propose a 3-step technology: 1) we train an F0-conditioned voice conversion (VC) model as data augmentation technique; 2) we train an F0 predictor to control the conversational flavour of the voice-converted synthetic data; 3) we train a TTS system that consumes the augmented data. We prove that our technology enables F0 controllability, is scalable across speakers and languages and is competitive in terms of naturalness over a state-of-the-art baseline model, another augmented method which does not make use of F0 information.      
### 9.An Industrial Applicable Approach towards Design Optimization of a Mechanism: a Coronaventilator Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2207.14537.pdf)
>  Design optimization of mechanisms is a promising research area as it results in more energy-efficient machines without compromising performance. However, machine builders do not actually use the potential described in the literature as these methods require too much theoretical analysis. <br>This paper introduces a convenient optimization workflow allowing wide industrial adoption, by using CAD models. The 3D multi-body software is used to perform motion simulations, from which the objective value samples can be extracted. These motion simulations determine the required torque for a certain combination of design parameters to fulfill the movement. Dedicated software can execute multiple motion simulations sequentially and interchange data between the different simulations, which automates the process of retrieving objective value samples. Therefore, without in-depth analytical design analysis, a machine designer can evaluate multiple designs at low cost. Moreover, by implementing an optimization algorithm, an optimal design can be found that meets the objective. In a case study of a coronaventilator mechanism with three design parameters (DP's), 39 CAD motion simulations allowed to reduce the RMS torque of the mechanism by 57.2% in 42 minutes.      
### 10.Evaluating the Practicality of Learned Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2207.14524.pdf)
>  Learned image compression has achieved extraordinary rate-distortion performance in PSNR and MS-SSIM compared to traditional methods. However, it suffers from intensive computation, which is intolerable for real-world applications and leads to its limited industrial application for now. In this paper, we introduce neural architecture search (NAS) to designing more efficient networks with lower latency, and leverage quantization to accelerate the inference process. Meanwhile, efforts in engineering like multi-threading and SIMD have been made to improve efficiency. Optimized using a hybrid loss of PSNR and MS-SSIM for better visual quality, we obtain much higher MS-SSIM than JPEG, JPEG XL and AVIF over all bit rates, and PSNR between that of JPEG XL and AVIF. Our software implementation of LIC achieves comparable or even faster inference speed compared to jpeg-turbo while being multiple times faster than JPEG XL and AVIF. Besides, our implementation of LIC reaches stunning throughput of 145 fps for encoding and 208 fps for decoding on a Tesla T4 GPU for 1080p images. On CPU, the latency of our implementation is comparable with JPEG XL.      
### 11.FCSN: Global Context Aware Segmentation by Learning the Fourier Coefficients of Objects in Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2207.14477.pdf)
>  The encoder-decoder model is a commonly used Deep Neural Network (DNN) model for medical image segmentation. Conventional encoder-decoder models make pixel-wise predictions focusing heavily on local patterns around the pixel. This makes it challenging to give segmentation that preserves the object's shape and topology, which often requires an understanding of the global context of the object. In this work, we propose a Fourier Coefficient Segmentation Network~(FCSN) -- a novel DNN-based model that segments an object by learning the complex Fourier coefficients of the object's masks. The Fourier coefficients are calculated by integrating over the whole contour. Therefore, for our model to make a precise estimation of the coefficients, the model is motivated to incorporate the global context of the object, leading to a more accurate segmentation of the object's shape. This global context awareness also makes our model robust to unseen local perturbations during inference, such as additive noise or motion blur that are prevalent in medical images. When FCSN is compared with other state-of-the-art models (UNet+, DeepLabV3+, UNETR) on 3 medical image segmentation tasks (ISIC\_2018, RIM\_CUP, RIM\_DISC), FCSN attains significantly lower Hausdorff scores of 19.14 (6\%), 17.42 (6\%), and 9.16 (14\%) on the 3 tasks, respectively. Moreover, FCSN is lightweight by discarding the decoder module, which incurs significant computational overhead. FCSN only requires 22.2M parameters, 82M and 10M fewer parameters than UNETR and DeepLabV3+. FCSN attains inference and training speeds of 1.6ms/img and 6.3ms/img, that is 8$\times$ and 3$\times$ faster than UNet and UNETR.      
### 12.Beyond CNNs: Exploiting Further Inherent Symmetries in Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.14472.pdf)
>  Automatic tumor or lesion segmentation is a crucial step in medical image analysis for computer-aided diagnosis. Although the existing methods based on Convolutional Neural Networks (CNNs) have achieved the state-of-the-art performance, many challenges still remain in medical tumor segmentation. This is because, although the human visual system can detect symmetries in 2D images effectively, regular CNNs can only exploit translation invariance, overlooking further inherent symmetries existing in medical images such as rotations and reflections. To solve this problem, we propose a novel group equivariant segmentation framework by encoding those inherent symmetries for learning more precise representations. First, kernel-based equivariant operations are devised on each orientation, which allows it to effectively address the gaps of learning symmetries in existing approaches. Then, to keep segmentation networks globally equivariant, we design distinctive group layers with layer-wise symmetry constraints. Finally, based on our novel framework, extensive experiments conducted on real-world clinical data demonstrate that a Group Equivariant Res-UNet (named GER-UNet) outperforms its regular CNN-based counterpart and the state-of-the-art segmentation methods in the tasks of hepatic tumor segmentation, COVID-19 lung infection segmentation and retinal vessel detection. More importantly, the newly built GER-UNet also shows potential in reducing the sample complexity and the redundancy of filters, upgrading current segmentation CNNs and delineating organs on other medical imaging modalities.      
### 13.Low-Complexity Loeffler DCT Approximations for Image and Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.14463.pdf)
>  This paper introduced a matrix parametrization method based on the Loeffler discrete cosine transform (DCT) algorithm. As a result, a new class of eight-point DCT approximations was proposed, capable of unifying the mathematical formalism of several eight-point DCT approximations archived in the literature. Pareto-efficient DCT approximations are obtained through multicriteria optimization, where computational complexity, proximity, and coding performance are considered. Efficient approximations and their scaled 16- and 32-point versions are embedded into image and video encoders, including a JPEG-like codec and H.264/AVC and H.265/HEVC standards. Results are compared to the unmodified standard codecs. Efficient approximations are mapped and implemented on a Xilinx VLX240T FPGA and evaluated for area, speed, and power consumption.      
### 14.PC-GANs: Progressive Compensation Generative Adversarial Networks for Pan-sharpening  [ :arrow_down: ](https://arxiv.org/pdf/2207.14451.pdf)
>  The fusion of multispectral and panchromatic images is always dubbed pansharpening. Most of the available deep learning-based pan-sharpening methods sharpen the multispectral images through a one-step scheme, which strongly depends on the reconstruction ability of the network. However, remote sensing images always have large variations, as a result, these one-step methods are vulnerable to the error accumulation and thus incapable of preserving spatial details as well as the spectral information. In this paper, we propose a novel two-step model for pan-sharpening that sharpens the MS image through the progressive compensation of the spatial and spectral information. Firstly, a deep multiscale guided generative adversarial network is used to preliminarily enhance the spatial resolution of the MS image. Starting from the pre-sharpened MS image in the coarse domain, our approach then progressively refines the spatial and spectral residuals over a couple of generative adversarial networks (GANs) that have reverse architectures. The whole model is composed of triple GANs, and based on the specific architecture, a joint compensation loss function is designed to enable the triple GANs to be trained simultaneously. Moreover, the spatial-spectral residual compensation structure proposed in this paper can be extended to other pan-sharpening methods to further enhance their fusion results. Extensive experiments are performed on different datasets and the results demonstrate the effectiveness and efficiency of our proposed method.      
### 15.Deep learning for understanding multilabel imbalanced Chest X-ray datasets  [ :arrow_down: ](https://arxiv.org/pdf/2207.14408.pdf)
>  Over the last few years, convolutional neural networks (CNNs) have dominated the field of computer vision thanks to their ability to extract features and their outstanding performance in classification problems, for example in the automatic analysis of X-rays. Unfortunately, these neural networks are considered black-box algorithms, i.e. it is impossible to understand how the algorithm has achieved the final result. To apply these algorithms in different fields and test how the methodology works, we need to use eXplainable AI techniques. Most of the work in the medical field focuses on binary or multiclass classification problems. However, in many real-life situations, such as chest X-rays, radiological signs of different diseases can appear at the same time. This gives rise to what is known as "multilabel classification problems". A disadvantage of these tasks is class imbalance, i.e. different labels do not have the same number of samples. The main contribution of this paper is a Deep Learning methodology for imbalanced, multilabel chest X-ray datasets. It establishes a baseline for the currently underutilised PadChest dataset and a new eXplainable AI technique based on heatmaps. This technique also includes probabilities and inter-model matching. The results of our system are promising, especially considering the number of labels used. Furthermore, the heatmaps match the expected areas, i.e. they mark the areas that an expert would use to make the decision.      
### 16.A Deep Generative Approach to Oversampling in Ptychography  [ :arrow_down: ](https://arxiv.org/pdf/2207.14392.pdf)
>  Ptychography is a well-studied phase imaging method that makes non-invasive imaging possible at a nanometer scale. It has developed into a mainstream technique with various applications across a range of areas such as material science or the defense industry. One major drawback of ptychography is the long data acquisition time due to the high overlap requirement between adjacent illumination areas to achieve a reasonable reconstruction. Traditional approaches with reduced overlap between scanning areas result in reconstructions with artifacts. In this paper, we propose complementing sparsely acquired or undersampled data with data sampled from a deep generative network to satisfy the oversampling requirement in ptychography. Because the deep generative network is pre-trained and its output can be computed as we collect data, the experimental data and the time to acquire the data can be reduced. We validate the method by presenting the reconstruction quality compared to the previously proposed and traditional approaches and comment on the strengths and drawbacks of the proposed approach.      
### 17.Model Reduction for Nonlinear Systems by Balanced Truncation of State and Gradient Covariance  [ :arrow_down: ](https://arxiv.org/pdf/2207.14387.pdf)
>  Data-driven reduced-order models often fail to make accurate forecasts of high-dimensional nonlinear systems that are sensitive along coordinates with low-variance because such coordinates are often truncated, e.g., by proper orthogonal decomposition, kernel principal component analysis, and autoencoders. Such systems are encountered frequently in shear-dominated fluid flows where non-normality plays a significant role in the growth of disturbances. In order to address these issues, we employ ideas from active subspaces to find low-dimensional systems of coordinates for model reduction that balance adjoint-based information about the system's sensitivity with the variance of states along trajectories. The resulting method, which we refer to as covariance balancing reduction using adjoint snapshots (CoBRAS), is identical to balanced truncation with state and adjoint-based gradient covariance matrices replacing the system Gramians and obeying the same key transformation laws. Here, the extracted coordinates are associated with an oblique projection that can be used to construct Petrov-Galerkin reduced-order models. We provide an efficient snapshot-based computational method analogous to balanced proper orthogonal decomposition. This also leads to the observation that the reduced coordinates can be computed relying on inner products of state and gradient samples alone, allowing us to find rich nonlinear coordinates by replacing the inner product with a kernel function. In these coordinates, reduced-order models can be learned using regression. We demonstrate these techniques and compare to a variety of other methods on a simple, yet challenging three-dimensional system and an axisymmetric jet flow simulation with $10^5$ state variables.      
### 18.Predicting Global Head-Related Transfer Functions From Scanned Head Geometry Using Deep Learning and Compact Representations  [ :arrow_down: ](https://arxiv.org/pdf/2207.14352.pdf)
>  In the growing field of virtual auditory display, personalized head-related transfer functions (HRTFs) play a vital role in establishing an accurate sound image. In this work, we propose an HRTF personalization method employing convolutional neural networks (CNN) to predict a subject's HRTFs for all directions from their scanned head geometry. To ease the training of the CNN models, we propose novel pre-processing methods for both the head scans and HRTF data to achieve compact representations. For the head scan, we use truncated spherical cap harmonic (SCH) coefficients to represent the pinna area, which is important in the acoustic scattering process. For the HRTF data, we use truncated spherical harmonic (SH) coefficients to represent the HRTF magnitudes and onsets. One CNN model is trained to predict the SH coefficients of the HRTF magnitudes from the SCH coefficients of the scanned ear geometry and other anthropometric measurements of the head. The other CNN model is trained to predict SH coefficients of the HRTF onsets from only the anthropometric measurements of the ear, head, and torso. Combining the magnitude and onset predictions, our method is able to predict the complete and global HRTF data. A leave-one-out validation with the log-spectral distortion (LSD) metric is used for objective evaluation. The results show a decent LSD level at both spatial \&amp; temporal dimensions compared to the ground-truth HRTFs and a lower LSD than the boundary element method (BEM) simulation of HRTFs that the database provides. The localization simulation results with an auditory model are also consistent with the objective evaluation metrics, showing the localization responses with our predicted HRTFs are significantly better than with the BEM calculated ones.      
### 19.The Novel Approach to the Closed-Form Average Bit Error Rate Calculation for the Nakagami-m Fading Channel  [ :arrow_down: ](https://arxiv.org/pdf/2207.14791.pdf)
>  The research presents a procedure of the closed-form average bit error rate evaluation for wireless communication systems in the presence of multipath fading. A generalization of the classical moment generating function is applied, and a connection between the Hankel-type contour-integral representation of the Marcum Q-function and Gauss Q-function is obtained and applied to the analytic derivation of the quadrature amplitude modulated signal average bit error rate in the presence of Nakagami-m fading. The correctness of the obtained solution was verified, and its computational efficiency (in terms of accuracy and time gain), compared with the prevailing approximation, was demonstrated. The proposed methodology can be extended to a wide variety of composite channels.      
### 20.Encoder-Decoder Architecture for 3D Seismic Inversion  [ :arrow_down: ](https://arxiv.org/pdf/2207.14789.pdf)
>  Inverting seismic data to build 3D geological structures is a challenging task due to the overwhelming amount of acquired seismic data, and the very-high computational load due to iterative numerical solutions of the wave equation, as required by industry-standard tools such as Full Waveform Inversion (FWI). For example, in an area with surface dimensions of 4.5km $\times$ 4.5km, hundreds of seismic shot-gather cubes are required for 3D model reconstruction, leading to Terabytes of recorded data. This paper presents a deep learning solution for the reconstruction of realistic 3D models in the presence of field noise recorded in seismic surveys. We implement and analyze a convolutional encoder-decoder architecture that efficiently processes the entire collection of hundreds of seismic shot-gather cubes. The proposed solution demonstrates that realistic 3D models can be reconstructed with a structural similarity index measure (SSIM) of 0.8554 (out of 1.0) in the presence of field noise at 10dB signal-to-noise ratio.      
### 21.Multiple-hypothesis RNN-T Loss for Unsupervised Fine-tuning and Self-training of Neural Transducer  [ :arrow_down: ](https://arxiv.org/pdf/2207.14736.pdf)
>  This paper proposes a new approach to perform unsupervised fine-tuning and self-training using unlabeled speech data for recurrent neural network (RNN)-Transducer (RNN-T) end-to-end (E2E) automatic speech recognition (ASR) systems. Conventional systems perform fine-tuning/self-training using ASR hypothesis as the targets when using unlabeled audio data and are susceptible to the ASR performance of the base model. Here in order to alleviate the influence of ASR errors while using unlabeled data, we propose a multiple-hypothesis RNN-T loss that incorporates multiple ASR 1-best hypotheses into the loss function. For the fine-tuning task, ASR experiments on Librispeech show that the multiple-hypothesis approach achieves a relative reduction of 14.2% word error rate (WER) when compared to the single-hypothesis approach, on the test_other set. For the self-training task, ASR models are trained using supervised data from Wall Street Journal (WSJ), Aurora-4 along with CHiME-4 real noisy data as unlabeled data. The multiple-hypothesis approach yields a relative reduction of 3.3% WER on the CHiME-4's single-channel real noisy evaluation set when compared with the single-hypothesis approach.      
### 22.Towards Unconstrained Audio Splicing Detection and Localization with Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.14682.pdf)
>  Freely available and easy-to-use audio editing tools make it straightforward to perform audio splicing. Convincing forgeries can be created by combining various speech samples from the same person. Detection of such splices is important both in the public sector when considering misinformation, and in a legal context to verify the integrity of evidence. Unfortunately, most existing detection algorithms for audio splicing use handcrafted features and make specific assumptions. However, criminal investigators are often faced with audio samples from unconstrained sources with unknown characteristics, which raises the need for more generally applicable methods. <br>With this work, we aim to take a first step towards unconstrained audio splicing detection to address this need. We simulate various attack scenarios in the form of post-processing operations that may disguise splicing. We propose a Transformer sequence-to-sequence (seq2seq) network for splicing detection and localization. Our extensive evaluation shows that the proposed method outperforms existing dedicated approaches for splicing detection [3, 10] as well as the general-purpose networks EfficientNet [28] and RegNet [25].      
### 23.High Dynamic Range and Super-Resolution from Raw Image Bursts  [ :arrow_down: ](https://arxiv.org/pdf/2207.14671.pdf)
>  Photographs captured by smartphones and mid-range cameras have limited spatial resolution and dynamic range, with noisy response in underexposed regions and color artefacts in saturated areas. This paper introduces the first approach (to the best of our knowledge) to the reconstruction of high-resolution, high-dynamic range color images from raw photographic bursts captured by a handheld camera with exposure bracketing. This method uses a physically-accurate model of image formation to combine an iterative optimization algorithm for solving the corresponding inverse problem with a learned image representation for robust alignment and a learned natural image prior. The proposed algorithm is fast, with low memory requirements compared to state-of-the-art learning-based approaches to image restoration, and features that are learned end to end from synthetic yet realistic data. Extensive experiments demonstrate its excellent performance with super-resolution factors of up to $\times 4$ on real photographs taken in the wild with hand-held cameras, and high robustness to low-light conditions, noise, camera shake, and moderate object motion.      
### 24.Multi-stage warm started optimal motion planning for over-actuated mobile platforms  [ :arrow_down: ](https://arxiv.org/pdf/2207.14659.pdf)
>  This work presents a computationally lightweight motion planner for over-actuated platforms. For this purpose, a general state-space model for mobile platforms with several kinematic chains is defined, which considers non-linearities and constraints. The proposed motion planner is based on a sequential multi-stage approach that takes advantage of the warm start on each step. Firstly, a globally optimal and smooth 2D/3D trajectory is generated using the Fast Marching Method. This trajectory is fed as a warm start to a sequential linear quadratic regulator that is able to generate an optimal motion plan without constraints for all the platform actuators. Finally, a feasible motion plan is generated considering the constraints defined in the model. In this respect, the sequential linear quadratic regulator is employed again, taking the previously generated unconstrained motion plan as a warm start. This novel approach has been deployed into the Exomars Testing Rover of the European Space Agency. This rover is an Ackermann-capable planetary exploration testbed that is equipped with a robotic arm. Several experiments were carried out demonstrating that the proposed approach speeds up the computation time, increasing the success ratio for a martian sample retrieval mission, which can be considered as a representative use case of an over-actuated mobile platform.      
### 25.EmoSens: Emotion Recognition based on Sensor data analysis using LightGBM  [ :arrow_down: ](https://arxiv.org/pdf/2207.14640.pdf)
>  Smart wearables have played an integral part in our day to day life. From recording ECG signals to analysing body fat composition, the smart wearables can do it all. The smart devices encompass various sensors which can be employed to derive meaningful information regarding the user's physical and psychological conditions. Our approach focuses on employing such sensors to identify and obtain the variations in the mood of a user at a given instance through the use of supervised machine learning techniques. The study examines the performance of various supervised learning models such as Decision Trees, Random Forests, XGBoost, LightGBM on the dataset. With our proposed model, we obtained a high recognition rate of 92.5% using XGBoost and LightGBM for 9 different emotion classes. By utilizing this, we aim to improvise and suggest methods to aid emotion recognition for better mental health analysis and mood monitoring.      
### 26.Haptic Teleoperation of High-dimensional Robotic Systems Using a Feedback MPC Framework  [ :arrow_down: ](https://arxiv.org/pdf/2207.14635.pdf)
>  Model Predictive Control (MPC) schemes have proven their efficiency in controlling high degree-of-freedom (DoF) complex robotic systems. However, they come at a high computational cost and an update rate of about tens of hertz. This relatively slow update rate hinders the possibility of stable haptic teleoperation of such systems since the slow feedback loops can cause instabilities and loss of transparency to the operator. This work presents a novel framework for transparent teleoperation of MPC-controlled complex robotic systems. In particular, we employ a feedback MPC approach and exploit its structure to account for the operator input at a fast rate which is independent of the update rate of the MPC loop itself. We demonstrate our framework on a mobile manipulator platform and show that it significantly improves haptic teleoperation's transparency and stability. We also highlight that the proposed feedback structure is constraint satisfactory and does not violate any constraints defined in the optimal control problem. To the best of our knowledge, this work is the first realization of the bilateral teleoperation of a legged manipulator using a whole-body MPC framework.      
### 27.Joint Beam Placement and Load Balancing Optimization for Non-Geostationary Satellite Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.14633.pdf)
>  Non-geostationary (Non-GSO) satellite constellations have emerged as a promising solution to enable ubiquitous high-speed low-latency broadband services by generating multiple spot-beams placed on the ground according to the user locations. However, there is an inherent trade-off between the number of active beams and the complexity of generating a large number of beams. This paper formulates and solves a joint beam placement and load balancing problem to carefully optimize the satellite beam and enhance the link budgets with a minimal number of active beams. We propose a two-stage algorithm design to overcome the combinatorial structure of the considered optimization problem providing a solution in polynomial time. The first stage minimizes the number of active beams, while the second stage performs a load balancing to distribute users in the coverage area of the active beams. Numerical results confirm the benefits of the proposed methodology both in carrier-to-noise ratio and multiplexed users per beam over other benchmarks.      
### 28.Phase Code Discovery for Pulse Compression Radar: A Genetic Algorithm Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.14631.pdf)
>  Discovering sequences with desired properties has long been an interesting intellectual pursuit. In pulse compression radar (PCR), discovering phase codes with low aperiodic autocorrelations is essential for a good estimation performance. The design of phase code, however, is mathematically non-trivial as the aperiodic autocorrelation properties of a sequence are intractable to characterize. In this paper, we put forth a genetic algorithm (GA) approach to discover new phase codes for PCR with the mismatched filter (MMF) receiver. The developed GA, dubbed GASeq, discovers better phase codes than the state of the art. At a code length of 59, the sequence discovered by GASeq achieves a signal-to-clutter ratio (SCR) of 50.84, while the best-known sequence has an SCR of 45.16. In addition, the efficiency and scalability of GASeq enable us to search phase codes with a longer code length, which thwarts existing deep learning-based approaches. At a code length of 100, the best phase code discovered by GASeq exhibit an SCR of 63.23.      
### 29.Image Augmentation for Satellite Images  [ :arrow_down: ](https://arxiv.org/pdf/2207.14580.pdf)
>  This study proposes the use of generative models (GANs) for augmenting the EuroSAT dataset for the Land Use and Land Cover (LULC) Classification task. We used DCGAN and WGAN-GP to generate images for each class in the dataset. We then explored the effect of augmenting the original dataset by about 10% in each case on model performance. The choice of GAN architecture seems to have no apparent effect on the model performance. However, a combination of geometric augmentation and GAN-generated images improved baseline results. Our study shows that GANs augmentation can improve the generalizability of deep classification models on satellite images.      
### 30.The Yakubovich S-Lemma Revisited: Stability and Contractivity in Non-Euclidean Norms  [ :arrow_down: ](https://arxiv.org/pdf/2207.14579.pdf)
>  The celebrated S-Lemma was originally proposed to ensure the existence of a quadratic Lyapunov function in the Lur'e problem of absolute stability. A quadratic Lyapunov function is, however, nothing else than a squared Euclidean norm on the state space (that is, a norm induced by an inner product). A natural question arises as to whether squared non-Euclidean norms $V(x)=\|x\|^2$ may serve as Lyapunov functions in stability problems. This paper presents a novel non-polynomial S-Lemma that leads to constructive criteria for the existence of such functions defined by weighted $\ell_p$ norms. Our generalized S-Lemma leads to new absolute stability and absolute contractivity criteria for Lur'e-type systems, including, for example, a new simple proof of the Aizerman and Kalman conjectures for positive Lur'e systems.      
### 31.Pronunciation-aware unique character encoding for RNN Transducer-based Mandarin speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.14578.pdf)
>  For Mandarin end-to-end (E2E) automatic speech recognition (ASR) tasks, compared to character-based modeling units, pronunciation-based modeling units could improve the sharing of modeling units in model training but meet homophone problems. In this study, we propose to use a novel pronunciation-aware unique character encoding for building E2E RNN-T-based Mandarin ASR systems. The proposed encoding is a combination of pronunciation-base syllable and character index (CI). By introducing the CI, the RNN-T model can overcome the homophone problem while utilizing the pronunciation information for extracting modeling units. With the proposed encoding, the model outputs can be converted into the final recognition result through a one-to-one mapping. We conducted experiments on Aishell and MagicData datasets, and the experimental results showed the effectiveness of the proposed method.      
### 32.Learning Phone Recognition from Unpaired Audio and Phone Sequences Based on Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.14568.pdf)
>  ASR has been shown to achieve great performance recently. However, most of them rely on massive paired data, which is not feasible for low-resource languages worldwide. This paper investigates how to learn directly from unpaired phone sequences and speech utterances. We design a two-stage iterative framework. GAN training is adopted in the first stage to find the mapping relationship between unpaired speech and phone sequence. In the second stage, another HMM model is introduced to train from the generator's output, which boosts the performance and provides a better segmentation for the next iteration. In the experiment, we first investigate different choices of model designs. Then we compare the framework to different types of baselines: (i) supervised methods (ii) acoustic unit discovery based methods (iii) methods learning from unpaired data. Our framework performs consistently better than all acoustic unit discovery methods and previous methods learning from unpaired data based on the TIMIT dataset.      
### 33.PSM: A Predictive Safety Model for Body Motion Based On the Spring-Damper Pendulum  [ :arrow_down: ](https://arxiv.org/pdf/2207.14556.pdf)
>  Quantifying the safety of the human body orientation is an important issue in human-robot interaction. Knowing the changing physical constraints on human motion can improve inspection of safe human motions and bring essential information about stability and normality of human body orientations with real-time risk assessment. Also, this information can be used in cooperative robots and monitoring systems to evaluate and interact in the environment more freely. Furthermore, the workspace area can be more deterministic with the known physical characteristics of safety. Based on this motivation, we propose a novel predictive safety model (PSM) that relies on the information of an inertial measurement unit on the human chest. The PSM encompasses a 3-Dofs spring-damper pendulum model that predicts human motion based on a safe motion dataset. The estimated safe orientation of humans is obtained by integrating a safety dataset and an elastic spring-damper model in a way that the proposed approach can realize complex motions at different safety levels. We did experiments in a real-world scenario to verify our novel proposed model. This novel approach can be used in different guidance/assistive robots and health monitoring systems to support and evaluate the human condition, particularly elders.      
### 34.Linearization of a dual-parallel Mach-Zehnder modulator using optical carrier band processing  [ :arrow_down: ](https://arxiv.org/pdf/2207.14547.pdf)
>  The linearization of a microwave photonic link based on a dual-parallel Mach-Zehnder modulator is theoretically described and experimentally demonstrated. Up to four different radio frequency tones are considered in the study, which allow us to provide a complete mathematical description of all third-order distortion terms that arise at the photodetector. Simulations show that a complete linearization is obtained by properly tuning the DC bias voltages and processing the optical carrier and. As a result, a suppression of 17 dBm is experimentally obtained for the third-order distortion terms, as well as a SDFR improvement of 3 dB. The proposed linearization method enables the simultaneous modulation of four different signals without the need of additional radio frequency components, which is desirable to its implementation in integrated optics and makes it suitable for several applications in microwave photonics.      
### 35.GPU-accelerated SIFT-aided source identification of stabilized videos  [ :arrow_down: ](https://arxiv.org/pdf/2207.14507.pdf)
>  Video stabilization is an in-camera processing commonly applied by modern acquisition devices. While significantly improving the visual quality of the resulting videos, it has been shown that such operation typically hinders the forensic analysis of video signals. In fact, the correct identification of the acquisition source usually based on Photo Response non-Uniformity (PRNU) is subject to the estimation of the transformation applied to each frame in the stabilization phase. A number of techniques have been proposed for dealing with this problem, which however typically suffer from a high computational burden due to the grid search in the space of inversion parameters. Our work attempts to alleviate these shortcomings by exploiting the parallelization capabilities of Graphics Processing Units (GPUs), typically used for deep learning applications, in the framework of stabilised frames inversion. Moreover, we propose to exploit SIFT features {to estimate the camera momentum and} %to identify less stabilized temporal segments, thus enabling a more accurate identification analysis, and to efficiently initialize the frame-wise parameter search of consecutive frames. Experiments on a consolidated benchmark dataset confirm the effectiveness of the proposed approach in reducing the required computational time and improving the source identification accuracy. {The code is available at \url{<a class="link-external link-https" href="https://github.com/AMontiB/GPU-PRNU-SIFT" rel="external noopener nofollow">this https URL</a>}}.      
### 36.Deep Learning Based Successive Interference Cancellation for the Non-Orthogonal Downlink  [ :arrow_down: ](https://arxiv.org/pdf/2207.14468.pdf)
>  Non-orthogonal communications are expected to play a key role in future wireless systems. In downlink transmissions, the data symbols are broadcast from a base station to different users, which are superimposed with different power to facilitate high-integrity detection using successive interference cancellation (SIC). However, SIC requires accurate knowledge of both the channel model and channel state information (CSI), which may be difficult to acquire. We propose a deep learningaided SIC detector termed SICNet, which replaces the interference cancellation blocks of SIC by deep neural networks (DNNs). Explicitly, SICNet jointly trains its internal DNN-aided blocks for inferring the soft information representing the interfering symbols in a data-driven fashion, rather than using hard-decision decoders as in classical SIC. As a result, SICNet reliably detects the superimposed symbols in the downlink of non-orthogonal systems without requiring any prior knowledge of the channel model, while being less sensitive to CSI uncertainty than its model-based counterpart. SICNet is also robust to changes in the number of users and to their power allocation. Furthermore, SICNet learns to produce accurate soft outputs, which facilitates improved soft-input error correction decoding compared to model-based SIC. Finally, we propose an online training method for SICNet under block fading, which exploits the channel decoding for accurately recovering online data labels for retraining, hence, allowing it to smoothly track the fading envelope without requiring dedicated pilots. Our numerical results show that SICNet approaches the performance of classical SIC under perfect CSI, while outperforming it under realistic CSI uncertainty.      
### 37.Generalized BER of MCIK-OFDM with Imperfect CSI: Selection combining GD versus ML receivers  [ :arrow_down: ](https://arxiv.org/pdf/2207.14459.pdf)
>  This paper analyzes the bit error rate (BER) of multicarrier index keying - orthogonal frequency division multiplexing (MCIK-OFDM) with selection combining (SC) diversity reception. Particularly, we propose a generalized framework to derive the BER for both the low-complexity greedy detector (GD) and maximum likelihood (ML) detector. Based on this, closedform expressions for the BERs of MCIK-OFDM with the SC using either the ML or the GD are derived in presence of the channel state information (CSI) imperfection. The asymptotic analysis is presented to gain helpful insights into effects of different CSI conditions on the BERs of these two detectors. More importantly, we theoretically provide opportunities for using the GD instead of the ML under each specific CSI uncertainty, which depend on the number of receiver antennas and the M-ary modulation size. Finally, extensive simulation results are provided in order to validate our theoretical expressions and analysis.      
### 38.Enhancing Diversity of OFDM with Joint Spread Spectrum and Subcarrier Index Modulations  [ :arrow_down: ](https://arxiv.org/pdf/2207.14454.pdf)
>  This paper proposes a novel spread spectrum and sub-carrier index modulation (SS-SIM) scheme, which is integrated to orthogonal frequency division multiplexing (OFDM) framework to enhance the diversity over the conventional IM schemes. Particularly, the resulting scheme, called SS-SIMOFDM, jointly employs both spread spectrum and sub-carrier index modulations to form a precoding vector which is then used to spread an M-ary complex symbol across all active sub-carriers. As a result, the proposed scheme enables a novel transmission of three signal domains: SS and sub-carrier indices, and a single M-ary symbol. For practical implementations, two reduced-complexity near-optimal detectors are proposed, which have complexities less depending on the M-ary modulation size. Then, the bit error probability and its upper bound are analyzed to gain an insight into the diversity gain, which is shown to be strongly affected by the order of sub-carrier indices. Based on this observation, we propose two novel sub-carrier index mapping methods, which significantly increase the diversity gain of SSSIM-OFDM. Finally, simulation results show that our scheme achieves better error performance than the benchmarks at the cost of lower spectral efficiency compared to classical OFDM and OFDM-IM, which can carry multiple M-ary symbols.      
### 39.Graph-Based Small Bowel Path Tracking with Cylindrical Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2207.14436.pdf)
>  We present a new graph-based method for small bowel path tracking based on cylindrical constraints. A distinctive characteristic of the small bowel compared to other organs is the contact between parts of itself along its course, which makes the path tracking difficult together with the indistinct appearance of the wall. It causes the tracked path to easily cross over the walls when relying on low-level features like the wall detection. To circumvent this, a series of cylinders that are fitted along the course of the small bowel are used to guide the tracking to more reliable directions. It is implemented as soft constraints using a new cost function. The proposed method is evaluated against ground-truth paths that are all connected from start to end of the small bowel for 10 abdominal CT scans. The proposed method showed clear improvements compared to the baseline method in tracking the path without making an error. Improvements of 6.6% and 17.0%, in terms of the tracked length, were observed for two different settings related to the small bowel segmentation.      
### 40.Phase retrieval of programmable photonic integrated circuits based on an on-chip fractional-delay reference path  [ :arrow_down: ](https://arxiv.org/pdf/2207.14424.pdf)
>  Programmable photonic integrated circuits (PICs), offering diverse signal processing functions within a single chip, are promising solutions for applications ranging from optical communications to artificial intelligence. While the scale and complexity of programmable PICs is increasing, the characterization, and thus calibration, of them becomes increasingly challenging. Here we demonstrate a phase retrieval method for programmable PICs using an on-chip fractional-delay reference path. The impulse response of the chip can be uniquely and precisely identified from only the insertion loss using a standard complex Fourier transform. We demonstrate our approach experimentally with a 4-tap finite-impulse-response chip. The results match well with expectations and verifies our approach as effective for individually determining the taps' weights without the need for additional ports and photodiodes.      
### 41.Sample-efficient Safe Learning for Online Nonlinear Control with Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2207.14419.pdf)
>  Reinforcement Learning (RL) and continuous nonlinear control have been successfully deployed in multiple domains of complicated sequential decision-making tasks. However, given the exploration nature of the learning process and the presence of model uncertainty, it is challenging to apply them to safety-critical control tasks due to the lack of safety guarantee. On the other hand, while combining control-theoretical approaches with learning algorithms has shown promise in safe RL applications, the sample efficiency of safe data collection process for control is not well addressed. In this paper, we propose a \emph{provably} sample efficient episodic safe learning framework for online control tasks that leverages safe exploration and exploitation in an unknown, nonlinear dynamical system. In particular, the framework 1) extends control barrier functions (CBFs) in a stochastic setting to achieve provable high-probability safety under uncertainty during model learning and 2) integrates an optimism-based exploration strategy to efficiently guide the safe exploration process with learned dynamics for \emph{near optimal} control performance. We provide formal analysis on the episodic regret bound against the optimal controller and probabilistic safety with theoretical guarantees. Simulation results are provided to demonstrate the effectiveness and efficiency of the proposed algorithm.      
### 42.Domain Specific Wav2vec 2.0 Fine-tuning For The SE&amp;R 2022 Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2207.14418.pdf)
>  This paper presents our efforts to build a robust ASR model for the shared task Automatic Speech Recognition for spontaneous and prepared speech &amp; Speech Emotion Recognition in Portuguese (SE&amp;R 2022). The goal of the challenge is to advance the ASR research for the Portuguese language, considering prepared and spontaneous speech in different dialects. Our method consist on fine-tuning an ASR model in a domain-specific approach, applying gain normalization and selective noise insertion. The proposed method improved over the strong baseline provided on the test set in 3 of the 4 tracks available      
### 43.Low Cost Embedded Vision System For Location And Tracking Of A Color Object  [ :arrow_down: ](https://arxiv.org/pdf/2207.14396.pdf)
>  This paper describes the development of an embedded vision system for detection, location, and tracking of a color object; it makes use of a single 32-bit microprocessor to acquire image data, process, and perform actions according to the interpreted data. The system is intended for applications that need to make use of artificial vision for detection, location and tracking of a color object and its objective is to have achieve at reduced terms of size, power consumption, and cost.      
