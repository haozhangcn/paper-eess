# ArXiv eess --Tue, 30 Aug 2022
### 1.Vapor Cloud Delayed-DPPM Modulation Technique for nonlinear Optoacoustic Communication  [ :arrow_down: ](https://arxiv.org/pdf/2208.13763.pdf)
>  The optoacoustic process can solve the longstanding challenge of wireless information transmission from an airborne unit to an underwater node (UWN). The nonlinear optoacoustic signal generated by proper laser parameters can propagate long distances in water. However, forming such a signal requires a high-power laser, and the buildup of a vapor cloud precludes the subsequent acoustic signal generation. Therefore, pursuing the traditional on-off keying (OOK) modulation technique will limit the data rate and power efficiency. In this paper, we analyze different modulation techniques and propose a vapor cloud delayed-differential pulse position modulation (VCD-DPPM) technique to improve the data rate and achieve high power efficiency for a single stationary laser transmitter. The symbol rate of VCD-DPPM is approximately 6.9 times and 1.69 times higher than OOK in our text communication simulation using a laser repetition rate of 10 kHz and 40 Hz, respectively. Furthermore, VCD-DPPM is 137% more power efficient than the OOK technique for both cases. We have generated different acoustic signal levels in laboratory conditions and simulated the bit error rate (BER) for different depths and positions of the UWN, while considering ambient underwater noises. Our results indicate that VCD-DPPM enables efficient data transmission.      
### 2.Optimization of Vehicle Trajectories Considering Uncertainty in Actuated Traffic Signal Timings  [ :arrow_down: ](https://arxiv.org/pdf/2208.13709.pdf)
>  This paper introduces a robust optimal green light speed advisory system for fixed and actuated traffic signals when a probability distribution is provided. These distributions represent the domain of possible switching times from the Signal Phasing and Timing (SPaT) messages. The system finds the least-cost vehicle trajectory using a computationally efficient A-star algorithm incorporated in a dynamic programming procedure to minimize the vehicle's total fuel consumption. Constraints are introduced to ensure that vehicles do not, collide with other vehicles, run red lights, or exceed a maximum vehicular jerk for passenger comfort. Results of simulation scenarios are evaluated against comparable trajectories of uninformed drivers to compute fuel consumption savings. The proposed approach produced significant fuel savings compared to the uninformed driver amounting to 37 percent on average for deterministic SPAT and 28 percent for stochastic SPaT. A sensitivity analysis is performed to understand how the degree of uncertainty in SPaT predictions affects the optimal trajectory's fuel consumption. The results present the required levels of confidence in these predictions to achieve most of the possible savings in fuel consumption. Specifically, the proposed system can be within 85 percent of the maximum savings if the timing error is (within 3.3 seconds) at a 95 percent confidence level. They also emphasize the importance of more reliable SPaT predictions the closer the time to green is relative to the time the vehicle is expected to reach the intersection given its current speed.      
### 3.Terahertz Communications Can Work in Rain and Snow: Impact of Adverse Weather Conditions on Channels at 140 GHz  [ :arrow_down: ](https://arxiv.org/pdf/2208.13690.pdf)
>  Next-generation wireless networks will leverage the spectrum above 100 GHz to enable ultra-high data rate communications over multi-GHz-wide bandwidths. The propagation environment at such high frequencies, however, introduces challenges throughout the whole protocol stack design, from physical layer signal processing to application design. Therefore, it is fundamental to develop a holistic understanding of the channel propagation and fading characteristics over realistic deployment scenarios and ultra-wide bands. In this paper, we conduct an extensive measurement campaign to evaluate the impact of weather conditions on a wireless link in the 130-150 GHz band through a channel sounding campaign with clear weather, rain, and snow in a typical urban backhaul scenario. We present a novel channel sounder design that captures signals with -82 dBm sensitivity and 20 GHz of bandwidth. We analyze link budget, capacity, as well as channel parameters such as the delay spread and the K-factor. Our experimental results indicate that in the considered context the adverse weather does not interrupt the link, but introduces some additional constraints (e.g., high delay spread and increase in path loss in snow conditions) that need to be accounted for in the design of reliable Sixth Generation (6G) communication links above 100 GHz.      
### 4.Deformable Image Registration using Unsupervised Deep Learning for CBCT-guided Abdominal Radiotherapy  [ :arrow_down: ](https://arxiv.org/pdf/2208.13686.pdf)
>  CBCTs in image-guided radiotherapy provide crucial anatomy information for patient setup and plan evaluation. Longitudinal CBCT image registration could quantify the inter-fractional anatomic changes. The purpose of this study is to propose an unsupervised deep learning based CBCT-CBCT deformable image registration. The proposed deformable registration workflow consists of training and inference stages that share the same feed-forward path through a spatial transformation-based network (STN). The STN consists of a global generative adversarial network (GlobalGAN) and a local GAN (LocalGAN) to predict the coarse- and fine-scale motions, respectively. The network was trained by minimizing the image similarity loss and the deformable vector field (DVF) regularization loss without the supervision of ground truth DVFs. During the inference stage, patches of local DVF were predicted by the trained LocalGAN and fused to form a whole-image DVF. The local whole-image DVF was subsequently combined with the GlobalGAN generated DVF to obtain final DVF. The proposed method was evaluated using 100 fractional CBCTs from 20 abdominal cancer patients in the experiments and 105 fractional CBCTs from a cohort of 21 different abdominal cancer patients in a holdout test. Qualitatively, the registration results show great alignment between the deformed CBCT images and the target CBCT image. Quantitatively, the average target registration error (TRE) calculated on the fiducial markers and manually identified landmarks was 1.91+-1.11 mm. The average mean absolute error (MAE), normalized cross correlation (NCC) between the deformed CBCT and target CBCT were 33.42+-7.48 HU, 0.94+-0.04, respectively. This promising registration method could provide fast and accurate longitudinal CBCT alignment to facilitate inter-fractional anatomic changes analysis and prediction.      
### 5.Data-Driven Distributed Voltage Control for Microgrids: A Koopman-based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.13682.pdf)
>  This paper presents a distributed data-driven control to regulate the voltage in an alternate current microgrid (MG). Following the hierarchical control frame for MGs, a secondary control for voltage is designed with a data-driven strategy using the Koopman operator. The Koopman operator approach represents the nonlinear behavior of voltage as a linear problem in the space of observables or lifted space. The representation in the lifted space is used together with linear consensus to design a model predictive control (MPC). The complete algorithm is proved in an MG model including changes in load, transmission lines, and the communication graph. The data-driven model regulates voltage using a distributed approach based only on local measurements, and includes reactive power constraints and control cost minimization.      
### 6.Comprehensive study of good model training for prostate segmentation in volumetric MRI  [ :arrow_down: ](https://arxiv.org/pdf/2208.13671.pdf)
>  Prostate cancer was the third most common cancer in 2020 internationally, coming after breast cancer and lung cancer. Furthermore, in recent years prostate cancer has shown an increasing trend. According to clinical experience, if this problem is detected and treated early, there can be a high chance of survival for the patient. One task that helps diagnose prostate cancer is prostate segmentation from magnetic resonance imaging. Manual segmentation performed by clinical experts has its drawbacks such as: the high time and concentration required from observers; and inter- and intra-observer variability. This is why in recent years automatic approaches to segment a prostate based on convolutional neural networks have emerged. Many of them have novel proposed architectures. In this paper I make an exhaustive study of several deep learning models by adjusting them to the task of prostate prediction. I do not use novel architectures, but focus my work more on how to train the networks. My approach is based on a ResNext101 3D encoder and a Unet3D decoder. I provide a study of the importance of resolutions in resampling data, something that no one else has done before.      
### 7.Maximization of User Association Deploying IRS in 6G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.13609.pdf)
>  Prospective mobile networks will be heterogeneous multi-tier networks, with different classes of base stations (BS) installed dependent on user demand. Multi-tier networks enable operators to enhance system capacity and coverage through flexible implementations. Intelligent reflecting surfaces (IRS) or reconfigurable intelligent surfaces (RIS) is a novel and disruptive technology that comprises a vast number of cost-efficient passive components, each of which can intelligently reflect the incident wave or signal with a reconfigurable phase shift. The objective of this work is to maximize the probability of user association for millimeter-wave (mmWave) carriers in a two-tier 6G network consisting of micro and macro cell tiers. Therefore, the work analyzed and compared the probability of user association for conventional and IRS-assisted micro base stations. The research derived that, the deployment of IRS in a micro cell significantly enhances or maximizes the probability of user association of the entire coverage region including the cell edge. Moreover, the deployment of IRS reduces the transmit power consumption of the micro base station which assures a notable energy efficiency.      
### 8.Energy-Efficient Cell-Free Massive MIMO Through Sparse Large-Scale Fading Processing  [ :arrow_down: ](https://arxiv.org/pdf/2208.13552.pdf)
>  Cell-free massive multiple-input multiple-output (CF mMIMO) systems serve the user equipments (UEs) by joint transmission and reception by geographically distributed access points (APs). To limit the power consumption due to fronthaul signaling and processing, each UE should only be served by a subset of the APs, but it is hard to identify that subset. Previous works have tackled this combinatorial problem heuristically. In this paper, we propose a sparse distributed processing design for CF mMIMO, where the AP-UE association and long-term signal processing coefficients are jointly optimized. We formulate two sparsity-inducing mean-squared error (MSE) minimization problems and solve them by using efficient proximal approaches with block-coordinate descent. For the downlink, more specifically, we develop a virtually optimized large-scale fading precoding (V-LSFP) scheme using uplink-downlink duality. The numerical results show that the proposed sparse processing schemes work well in both uplink and downlink. In particular, they achieve almost the same spectral efficiency as if all APs would serve all UEs, while the energy efficiency is 2-4 higher thanks to the reduced processing and signaling.      
### 9.Dynamic RF Beam Codebook Reduction for Cost-Efficient mmWave Full-Duplex Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13547.pdf)
>  The recent attempts to realize full-duplex (FD) communications in millimeter wave (mmWave) systems have garnered a significant amount of interest for its potential. In this paper, we present a cost-efficient design of mmWave FD systems, where the system dynamically reduces the RF beam codebook in a computationally efficient manner, so that it is comprised of the RF beams that will prevent the Rx receive chain from saturating due to the self-interference (SI). The analog beamformer will suppress the SI to the level that the residual SI can be completely removed with digital SI cancellation, allowing the digital beamformer to concentrate on the desired channel, free of the SI. To reduce the computation required for the proposed method, we propose two sufficient conditions that prevent the Rx side saturations, which are practically tight. Through performance valuations conducted in realistically modeled mmWave FD scenarios, we demonstrate that the proposed design achieves comparable performance with the ideal FD and other benchmarks with significantly lower costs.      
### 10.Distributed Dynamic Platoons Control and Junction Crossing Optimization for Mixed Traffic Flow in Smart Cities- Part II. Stability, Optimization, and Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.13514.pdf)
>  In part II, we present a fully distributed nonlinear variable time headway space strategy to ensure the subsequent safe cruising and junction crossing, where the cooperative perception of multiple neighbors stimuli and the cooperative tracking of the follower connected automated vehicles(CAVs) to the leader CAV are developed, which will result in a heterogeneous traffic flow dynamic. Once the proper length of the platoon determined by the ADSCAS characterized in part I is formed, we propose a cooperative observer design to estimate the leader CAV's acceleration adjustment which is affected by the unknown traffic lights. We shall show that the distributed and resilient nonlinear platoons control and junction crossing problem will be solved by a robust cooperative trajectory tracking optimization algorithm to ensure the fast formation and split of the platoons and safe junction cruising within the finite time horizons, taking into account the social driving behaviors(SDBs) of the surrounding vehicles(SVs), the dynamics of the follower CAVs, and an upcoming traffic signal schedule while minimizing the overall platoons fuel consumption. Performance analysis and case studies are presented to illustrate the effectiveness of the proposed approaches for multiple platoon dynamic management, which also show that the cooperation between CAVs and human-driven vehicles(HDVs) can further smooth out the driving trajectory, reduce the fuel consumption, and enhance the safety of the mixed traffic flow      
### 11.Minimum Input Design for Direct Data-driven Property Identification of Unknown Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13454.pdf)
>  In a direct data-driven approach, this paper studies the {\em property identification(ID)} problem to analyze whether an unknown linear system has a property of interest, e.g., stabilizability and structural properties. In sharp contrast to the model-based analysis, we approach it by directly using the input and state feedback data of the unknown system. Via a new concept of sufficient richness of input sectional data, we first establish the necessary and sufficient condition for the minimum input design to excite the system for property ID. Specifically, the input sectional data is sufficiently rich for property ID {\em if and only if} it spans a linear subspace that contains a property dependent minimum linear subspace, any basis of which can also be easily used to form the minimum excitation input. Interestingly, we show that many structural properties can be identified with the minimum input that is however unable to identify the explicit system model. Overall, our results rigorously quantify the advantages of the direct data-driven analysis over the model-based analysis for linear systems in terms of data efficiency.      
### 12.Distributed Dynamic Platoons Control and Junction Crossing Optimization for Mixed Traffic Flows in Smart Cities- Part I. Fundamentals, Theoretical and Automatic Decision Framework  [ :arrow_down: ](https://arxiv.org/pdf/2208.13435.pdf)
>  This article studies the problems of distributed dynamic platoons control and smart junction crossing optimization for a mixed traffic flow with connected automated vehicles(CAVs) and social human-driven vehicles(HDVs) in a smart city. The goal of this two-part article is to provide an automatic decision framework to ensure the safe and efficient cruising and crossing junctions with multiple dynamic and resilient platoons CAVs, while against the social driving behaviors(SDBs) of the HDVs considered as surrounding vehicles(SVs) and unknown traffic lights connected by Cellular-Vehicle-to-X (C-V2X) infrastructure. We shall show that despite the nonlinearity, non-smoothness, and uncertainty of mixed traffic flows in smart cities, the solutions of safe, efficient, and fuel economic platoon CAVs control for the above problems can be solved by our proposed automatic decision and smart crossing assistant system(ADSCAS) that includes four decision stages and six cruising states characterized within this article. More precisely, in part I, we provide a dynamic platoon management strategy to determine the platoon size with respect to the SDBs of the preceding HDVs and upcoming traffic lights, where the dynamic platoon size is composed of minimizing safe distances and nonlinear functions of the platoon's cruising velocity. We also present the decision and switching scheme for a finite state machine of platoons in the ADSCAS, the design of the reference trajectory planning, and the solution of the fuel economic optimization problem.      
### 13.An Accurate and Hardware-Efficient Dual Spike Detector for Implantable Neural Interfaces  [ :arrow_down: ](https://arxiv.org/pdf/2208.13432.pdf)
>  Spike detection plays a central role in neural data processing and brain-machine interfaces (BMIs). A challenge for future-generation implantable BMIs is to build a spike detector that features both low hardware cost and high performance. In this work, we propose a novel hardware-efficient and high-performance spike detector for implantable BMIs. The proposed design is based on a dual-detector architecture with adaptive threshold estimation. The dual-detector comprises two separate TEO-based detectors that distinguish a spike occurrence based on its discriminating features in both high and low noise scenarios. We evaluated the proposed spike detection algorithm on the Wave Clus dataset. It achieved an average detection accuracy of 98.9%, and over 95% in high-noise scenarios, ensuring the reliability of our method. When realized in hardware with a sampling rate of 16kHz and 7-bits resolution, the detection accuracy is 97.4%. Designed in 65nm TSMC process, a 256-channel detector based on this architecture occupies only 682$\mu m^2$ /Channel and consumes 0.07$\mu$W/Channel, improving over the state-of-the-art spike detectors by 39.7% in power consumption and 78.8% in area, while maintaining a high accuracy.      
### 14.An AFDM-Based Integrated Sensing and Communications  [ :arrow_down: ](https://arxiv.org/pdf/2208.13430.pdf)
>  This paper considers an affine frequency division multiplexing (AFDM)-based integrated sensing and communications (ISAC) system, where the AFDM waveform is used to simultaneously carry communications information and sense targets. To realize AFDM-based sensing functionality, two parameter estimation methods are designed to process echoes in the time domain and the discrete affine Fourier transform (DAFT) domain, respectively. It allows us to decouple delay and Doppler shift in the fast time axis and can maintain good sensing performance even in large Doppler shift scenarios. Numerical results verify the effectiveness of our proposed AFDM-based system in high mobility scenarios.      
### 15.Distributed Observers-based Cooperative Platooning Tracking Control and Intermittent Optimization for Connected Automated Vehicles with Unknown Jerk Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2208.13425.pdf)
>  The unknown sharp changes of vehicle acceleration rates, also called the unknown jerk dynamics, may significantly affect the driving performance of the leader vehicle in a platoon, resulting in more drastic car-following movements in platooning tracking control, which could cause safety and traffic capacity concerns. To address these issues, in this paper, we investigate cooperative platooning tracking control and intermittent optimization problems for connected automated vehicles (CAVs) with a nonlinear car-following model. We assume that the external inputs of the leader CAV contain unknown but bounded jerk parameters, and the acceleration signals of the leader CAV are known only to a few neighboring follower CAVs in a free-design but directed communication network. To solve these problems, a distributed observer law is developed to provide a reference signal expressed as an estimated unknown jerk dynamic of the leader CAV and implemented by each follower CAV. Then, a novel distributed platooning tracking control protocol is proposed to construct the cooperative tracking controllers under identical inter-vehicle constraints, which can ensure a desired safety distance among the CAVs and allow each follower CAV to track their leader CAV by using only local information interaction. We also present a novel intermittent sampling condition and a robust intermittent optimization design that can ensure optimally scheduled feedback gains for the cooperative platooning tracking controllers to minimize the control cost under nonidentical inter-vehicle constraints and unknown jerk dynamics. Simulation case studies are carried out to illustrate the effectiveness of the proposed approaches      
### 16.An Energy Activity Dataset for Smart Homes  [ :arrow_down: ](https://arxiv.org/pdf/2208.13416.pdf)
>  This paper provides a public energy dataset that records miscel-laneous energy usage data collected from smart homes. The pro-posed energy activity dataset (EAD) has a high data diversity in contrast to existing load monitoring datasets. In EAD, a simple data point is labeled with the appliance, brand, and event infor-mation, whereas a complex data point has an extra application label. Several discoveries have been made on the energy con-sumption patterns of various appliances operated under different events and applications. A revised longest-common-subsequence (LCS) similarity measurement algorithm is proposed to quantify the energy dataset similarities so that data quality information is provided before training deep learning models. In addition, a subsample convolutional neural network (CNN) is put forward as a flexible optical character recognition (OCR) approach to obtain energy data directly from monitors of power meters. The energy activity dataset can be downloaded from: <a class="link-external link-https" href="https://drive.google.com/drive/folders/1zn0V6Q8eXXSKxKgcs8ZRValL5VEn3anD" rel="external noopener nofollow">this https URL</a>      
### 17.Distributed Cooperative Control and Optimization of Connected Automated Vehicles Platoon Against Cut-in Behaviors of Social Drivers  [ :arrow_down: ](https://arxiv.org/pdf/2208.13412.pdf)
>  Connected automated vehicles (CAVs) have brought new opportunities to improve traffic throughput and reduce energy consumption. However, the uncertain lane-change behaviors (LCBs) of surrounding vehicles (SVs) as an uncontrollable factor significantly threaten the driving safety and the consistent movement of a group of platoon CAVs. How to ensure safe, efficient, and fuel economic platoon control poses a key challenge faced by researchers in complex traffic environments. This study proposes a dynamic platoon management and cooperative driving framework for a mixed traffic flow consisting of multiple CAVs and possible human-driven vehicles (HDVs) as the SVs on unsignalized roads. In the proposed framework, the leader CAV of the platoon provides a high-level automatic driving decision to the follower CAVs by developing an optimal trajectory estimation of the HDVs while distributed observers and tracking controllers are properly implemented by the follower CAVs. Specifically, the proposed framework consists of three stages. At the observation stage, the cruising information of all the SVs will be collected by the leader CAV through the Cellular-Vehicle-to-X (C-V2X) infrastructure, while an automatic decision-making driving assistance system (ADMDSS) is constructed to determine the driving states of the platoon. When the HDVs approach the communication range of the platoon, in the prediction stage, the trajectories of the HDVs as the target vehicles will be estimated and the reference trajectory planning for the leader CAV and the cooperative controller design for the follower CAVs will be respectively activated by using C-V2X infrastructure. Simulation cases are presented to illustrate the effectiveness of the proposed approaches.      
### 18.Boundary-Aware Network for Kidney Parsing  [ :arrow_down: ](https://arxiv.org/pdf/2208.13338.pdf)
>  Kidney structures segmentation is a crucial yet challenging task in the computer-aided diagnosis of surgery-based renal cancer. Although numerous deep learning models have achieved remarkable success in many medical image segmentation tasks, accurate segmentation of kidney structures on computed tomography angiography (CTA) images remains challenging, due to the variable sizes of kidney tumors and the ambiguous boundaries between kidney structures and their surroundings. In this paper, we propose a boundary-aware network (BA-Net) to segment kidneys, kidney tumors, arteries, and veins on CTA scans. This model contains a shared encoder, a boundary decoder, and a segmentation decoder. The multi-scale deep supervision strategy is adopted on both decoders, which can alleviate the issues caused by variable tumor sizes. The boundary probability maps produced by the boundary decoder at each scale are used as attention to enhance the segmentation feature maps. We evaluated the BA-Net on the Kidney PArsing (KiPA) Challenge dataset and achieved an average Dice score of 89.65$\%$ for kidney structure segmentation on CTA scans using 4-fold cross-validation. The results demonstrate the effectiveness of the BA-Net.      
### 19.Label Propagation for 3D Carotid Vessel Wall Segmentation and Atherosclerosis Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2208.13337.pdf)
>  Carotid vessel wall segmentation is a crucial yet challenging task in the computer-aided diagnosis of atherosclerosis. Although numerous deep learning models have achieved remarkable success in many medical image segmentation tasks, accurate segmentation of carotid vessel wall on magnetic resonance (MR) images remains challenging, due to limited annotations and heterogeneous arteries. In this paper, we propose a semi-supervised label propagation framework to segment lumen, normal vessel walls, and atherosclerotic vessel wall on 3D MR images. By interpolating the provided annotations, we get 3D continuous labels for training 3D segmentation model. With the trained model, we generate pseudo labels for unlabeled slices to incorporate them for model training. Then we use the whole MR scans and the propagated labels to re-train the segmentation model and improve its robustness. We evaluated the label propagation framework on the CarOtid vessel wall SegMentation and atherosclerOsis diagnosiS (COSMOS) Challenge dataset and achieved a QuanM score of 83.41\% on the testing dataset, which got the 1-st place on the online evaluation leaderboard. The results demonstrate the effectiveness of the proposed framework.      
### 20.Slice estimation in diffusion MRI of neonatal and fetal brains in image and spherical harmonics domains using autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2208.13328.pdf)
>  Diffusion MRI (dMRI) of the developing brain can provide valuable insights into the white matter development. However, slice thickness in fetal dMRI is typically high (i.e., 3-5 mm) to freeze the in-plane motion, which reduces the sensitivity of the dMRI signal to the underlying anatomy. In this study, we aim at overcoming this problem by using autoencoders to learn unsupervised efficient representations of brain slices in a latent space, using raw dMRI signals and their spherical harmonics (SH) representation. We first learn and quantitatively validate the autoencoders on the developing Human Connectome Project pre-term newborn data, and further test the method on fetal data. Our results show that the autoencoder in the signal domain better synthesized the raw signal. Interestingly, the fractional anisotropy and, to a lesser extent, the mean diffusivity, are best recovered in missing slices by using the autoencoder trained with SH coefficients. A comparison was performed with the same maps reconstructed using an autoencoder trained with raw signals, as well as conventional interpolation methods of raw signals and SH coefficients. From these results, we conclude that the recovery of missing/corrupted slices should be performed in the signal domain if the raw signal is aimed to be recovered, and in the SH domain if diffusion tensor properties (i.e., fractional anisotropy) are targeted. Notably, the trained autoencoders were able to generalize to fetal dMRI data acquired using a much smaller number of diffusion gradients and a lower b-value, where we qualitatively show the consistency of the estimated diffusion tensor maps.      
### 21.An Adaptive Pilot Model with Reaction Time-Delay  [ :arrow_down: ](https://arxiv.org/pdf/2208.13303.pdf)
>  Practical adaptive control implementations where human pilots coexist in the loop are still uncommon, despite their success in handling uncertain dynamical systems. This is owing to their special nonlinear characteristics which lead to unfavorable interactions between pilots and adaptive controllers. To pave the way for the implementation of adaptive controllers in piloted applications, we propose an adaptive human pilot model that takes into account the time delay in the pilot's response while operating on an adaptive control system. The model can be utilized in the evaluation of adaptive controllers through the simulation environment and guide in their design.      
### 22.Unsupervised diffeomorphic cardiac image registration using parameterization of the deformation field  [ :arrow_down: ](https://arxiv.org/pdf/2208.13275.pdf)
>  This study proposes an end-to-end unsupervised diffeomorphic deformable registration framework based on moving mesh parameterization. Using this parameterization, a deformation field can be modeled with its transformation Jacobian determinant and curl of end velocity field. The new model of the deformation field has three important advantages; firstly, it relaxes the need for an explicit regularization term and the corresponding weight in the cost function. The smoothness is implicitly embedded in the solution which results in a physically plausible deformation field. Secondly, it guarantees diffeomorphism through explicit constraints applied to the transformation Jacobian determinant to keep it positive. Finally, it is suitable for cardiac data processing, since the nature of this parameterization is to define the deformation field in terms of the radial and rotational components. The effectiveness of the algorithm is investigated by evaluating the proposed method on three different data sets including 2D and 3D cardiac MRI scans. The results demonstrate that the proposed framework outperforms existing learning-based and non-learning-based methods while generating diffeomorphic transformations.      
### 23.Efficient liver segmentation with 3D CNN using computed tomography scans  [ :arrow_down: ](https://arxiv.org/pdf/2208.13271.pdf)
>  The liver is one of the most critical metabolic organs in vertebrates due to its vital functions in the human body, such as detoxification of the blood from waste products and medications. Liver diseases due to liver tumors are one of the most common mortality reasons around the globe. Hence, detecting liver tumors in the early stages of tumor development is highly required as a critical part of medical treatment. Many imaging modalities can be used as aiding tools to detect liver tumors. Computed tomography (CT) is the most used imaging modality for soft tissue organs such as the liver. This is because it is an invasive modality that can be captured relatively quickly. This paper proposed an efficient automatic liver segmentation framework to detect and segment the liver out of CT abdomen scans using the 3D CNN DeepMedic network model. Segmenting the liver region accurately and then using the segmented liver region as input to tumors segmentation method is adopted by many studies as it reduces the false rates resulted from segmenting abdomen organs as tumors. The proposed 3D CNN DeepMedic model has two pathways of input rather than one pathway, as in the original 3D CNN model. In this paper, the network was supplied with multiple abdomen CT versions, which helped improve the segmentation quality. The proposed model achieved 94.36%, 94.57%, 91.86%, and 93.14% for accuracy, sensitivity, specificity, and Dice similarity score, respectively. The experimental results indicate the applicability of the proposed method.      
### 24.Detection and Classification of Brain tumors Using Deep Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.13264.pdf)
>  Abnormal development of tissues in the body as a result of swelling and morbid enlargement is known as a tumor. They are mainly classified as Benign and Malignant. Tumour in the brain is fatal as it may be cancerous, so it can feed on healthy cells nearby and keep increasing in size. This may affect the soft tissues, nerve cells, and small blood vessels in the brain. Hence there is a need to detect and classify them during the early stages with utmost precision. There are different sizes and locations of brain tumors which makes it difficult to understand their nature. The process of detection and classification of brain tumors can prove to be an onerous task even with advanced MRI (Magnetic Resonance Imaging) techniques due to the similarities between the healthy cells nearby and the tumor. In this paper, we have used Keras and Tensorflow to implement state-of-the-art Convolutional Neural Network (CNN) architectures, like EfficientNetB0, ResNet50, Xception, MobileNetV2, and VGG16, using Transfer Learning to detect and classify three types of brain tumors namely - Glioma, Meningioma, and Pituitary. The dataset we used consisted of 3264 2-D magnetic resonance images and 4 classes. Due to the small size of the dataset, various data augmentation techniques were used to increase the size of the dataset. Our proposed methodology not only consists of data augmentation, but also various image denoising techniques, skull stripping, cropping, and bias correction. In our proposed work EfficientNetB0 architecture performed the best giving an accuracy of 97.61%. The aim of this paper is to differentiate between normal and abnormal pixels and also classify them with better accuracy.      
### 25.Deep Learning for automatic head and neck lymph node level delineation  [ :arrow_down: ](https://arxiv.org/pdf/2208.13224.pdf)
>  Background: Deep learning-based head and neck lymph node level (HN_LNL) autodelineation is of high relevance to radiotherapy research and clinical treatment planning but still understudied in academic literature. <br>Methods: An expert-delineated cohort of 35 planning CTs was used for training of an nnU-net 3D-fullres/2D-ensemble model for autosegmentation of 20 different HN_LNL. Validation was performed in an independent test set (n=20). In a completely blinded evaluation, 3 clinical experts rated the quality of deep learning autosegmentations in a head-to-head comparison with expert-created contours. For a subgroup of 10 cases, intraobserver variability was compared to deep learning autosegmentation performance. The effect of autocontour consistency with CT slice plane orientation on geometric accuracy and expert rating was investigated. <br>Results: Mean blinded expert rating per level was significantly better for deep learning segmentations with CT slice plane adjustment than for expert-created contours (81.0 vs. 79.6, p&lt;0.001), but deep learning segmentations without slice plane adjustment were rated significantly worse than expert-created contours (77.2 vs. 79.6, p&lt;0.001). Geometric accuracy of deep learning segmentations was non-different from intraobserver variability (mean Dice per level, 0.78 vs. 0.77, p=0.064) with variance in accuracy between levels being improved (p&lt;0.001). Clinical significance of contour consistency with CT slice plane orientation was not represented by geometric accuracy metrics (Dice, 0.78 vs. 0.78, p=0.572) <br>Conclusions: We show that a nnU-net 3D-fullres/2D-ensemble model can be used for highly accurate autodelineation of HN_LNL using only a limited training dataset that is ideally suited for large-scale standardized autodelineation of HN_LNL in the research setting. Geometric accuracy metrics are only an imperfect surrogate for blinded expert rating.      
### 26.Structural Adaptivity of Directed Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.13223.pdf)
>  Network structure plays a critical role in functionality and performance of network systems. This paper examines structural adaptivity of diffusively coupled, directed multi-agent networks that are subject to diffusion performance. Inspired by the observation that the link redundancy in a network may degrade its diffusion performance, a distributed data-driven neighbor selection framework is proposed to adaptively adjust the network structure for improving the diffusion performance of exogenous influence over the network. Specifically, each agent is allowed to interact with only a specific subset of neighbors while global reachability from exogenous influence to all agents of the network is maintained. Both continuous-time and discrete-time directed networks are examined. For each of the two cases, we first examine the reachability properties encoded in the eigenvectors of perturbed variants of graph Laplacian or SIA matrix associated with directed networks, respectively. Then, an eigenvector-based rule for neighbor selection is proposed to derive a reduced network, on which the diffusion performance is enhanced. Finally, motivated by the necessity of distributed and data-driven implementation of the neighbor selection rule, quantitative connections between eigenvectors of the perturbed graph Laplacian and SIA matrix and relative rate of change in agent state are established, respectively. These connections immediately enable a data-driven inference of the reduced neighbor set for each agent using only locally accessible data. As an immediate extension, we further discuss the distributed data-driven construction of directed spanning trees of directed networks using the proposed neighbor selection framework. Numerical simulations are provided to demonstrate the theoretical results.      
### 27.An Unsupervised Learning-based Framework for Effective Representation Extraction of Reactor Accidents  [ :arrow_down: ](https://arxiv.org/pdf/2208.13147.pdf)
>  With the increasing use of high-precision system analysis programs in nuclear engineering, the number of high-fidelity computational data for accident simulation is exploding. Therefore, an algorithm that can achieve both automatic extraction of low-dimensional features from the data and guarantee the validity of the features is needed to improve the performance and confidence of the accident diagnosis system. This study proposes an autoencoder-based autonomous learning framework, namely Padded Auto-Encoder (PAE), which is able to automatically encode accident monitoring data that has been noise-added and with partially missing data into low-dimensional feature vectors via a Vision Transformer-based encoder, and to decode the feature vectors into noise-free and complete reconstructed monitoring data. Thus, the encoder part of the framework is able to automatically infer valid representations from partially missing and noisy monitoring data that reflect the complete and noise-free original data, and the representation vectors can be used for downstream tasks for accident diagnosis or else. In this paper, LOCA of HPR1000 was used as the study object, and the PAE was trained by an unsupervised method using cases with different break locations and sizes as the dataset. The encoder part of the pre-trained PAE was subsequently used as the feature extractor for the monitoring data, and several basic statistical learning algorithms for predicting the break locations and sizes. The results of the study show that the pre-trained diagnostic model with two stages has a better performance in break location and size diagnostic capability with an improvement of 41.62% and 80.86% in the metrics respectively, compared to the diagnostic model with end-to-end model structure.      
### 28.Generative Modelling of the Ageing Heart with Cross-Sectional Imaging and Clinical Data  [ :arrow_down: ](https://arxiv.org/pdf/2208.13146.pdf)
>  Cardiovascular disease, the leading cause of death globally, is an age-related disease. Understanding the morphological and functional changes of the heart during ageing is a key scientific question, the answer to which will help us define important risk factors of cardiovascular disease and monitor disease progression. In this work, we propose a novel conditional generative model to describe the changes of 3D anatomy of the heart during ageing. The proposed model is flexible and allows integration of multiple clinical factors (e.g. age, gender) into the generating process. We train the model on a large-scale cross-sectional dataset of cardiac anatomies and evaluate on both cross-sectional and longitudinal datasets. The model demonstrates excellent performance in predicting the longitudinal evolution of the ageing heart and modelling its data distribution.      
### 29.Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2208.13113.pdf)
>  Automatically measuring lesion/tumor size with RECIST (Response Evaluation Criteria In Solid Tumors) diameters and segmentation is important for computer-aided diagnosis. Although it has been studied in recent years, there is still space to improve its accuracy and robustness, such as (1) enhancing features by incorporating rich contextual information while keeping a high spatial resolution and (2) involving new tasks and losses for joint optimization. To reach this goal, this paper proposes a transformer-based network (MeaFormer, Measurement transFormer) for lesion RECIST diameter prediction and segmentation (LRDPS). It is formulated as three correlative and complementary tasks: lesion segmentation, heatmap prediction, and keypoint regression. To the best of our knowledge, it is the first time to use keypoint regression for RECIST diameter prediction. MeaFormer can enhance high-resolution features by employing transformers to capture their long-range dependencies. Two consistency losses are introduced to explicitly build relationships among these tasks for better optimization. Experiments show that MeaFormer achieves the state-of-the-art performance of LRDPS on the large-scale DeepLesion dataset and produces promising results of two downstream clinic-relevant tasks, i.e., 3D lesion segmentation and RECIST assessment in longitudinal studies.      
### 30.Transmission Line Parameter Estimation Under Non-Gaussian Measurement Noise  [ :arrow_down: ](https://arxiv.org/pdf/2208.13105.pdf)
>  Accurate knowledge of transmission line parameters is essential for a variety of power system monitoring, protection, and control applications. The use of phasor measurement unit (PMU) data for transmission line parameter estimation (TLPE) is well-documented. However, existing literature on PMU-based TLPE implicitly assumes the measurement noise to be Gaussian. Recently, it has been shown that the noise in PMU measurements (especially in the current phasors) is better represented by Gaussian mixture models (GMMs), i.e., the noises are non-Gaussian. We present a novel approach for TLPE that can handle non-Gaussian noise in the PMU measurements. The measurement noise is expressed as a GMM, whose components are identified using the expectation-maximization (EM) algorithm. Subsequently, noise and parameter estimation is carried out by solving a maximum likelihood estimation problem iteratively until convergence. The superior performance of the proposed approach over traditional approaches such as least squares and total least squares as well as the more recently proposed minimum total error entropy approach is demonstrated by performing simulations using the IEEE 118-bus system as well as proprietary PMU data obtained from a U.S. power utility.      
### 31.Target Speaker Voice Activity Detection with Transformers and Its Integration with End-to-End Neural Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2208.13085.pdf)
>  This paper describes a speaker diarization model based on target speaker voice activity detection (TS-VAD) using transformers. To overcome the original TS-VAD model's drawback of being unable to handle an arbitrary number of speakers, we investigate model architectures that use input tensors with variable-length time and speaker dimensions. Transformer layers are applied to the speaker axis to make the model output insensitive to the order of the speaker profiles provided to the TS-VAD model. Time-wise sequential layers are interspersed between these speaker-wise transformer layers to allow the temporal and cross-speaker correlations of the input speech signal to be captured. We also extend a diarization model based on end-to-end neural diarization with encoder-decoder based attractors (EEND-EDA) by replacing its dot-product-based speaker detection layer with the transformer-based TS-VAD. Experimental results on VoxConverse show that using the transformers for the cross-speaker modeling reduces the diarization error rate (DER) of TS-VAD by 10.9%, achieving a new state-of-the-art (SOTA) DER of 4.74%. Also, our extended EEND-EDA reduces DER by 6.9% on the CALLHOME dataset relative to the original EEND-EDA with a similar model size, achieving a new SOTA DER of 11.18% under a widely used training data setting.      
### 32.Lossy Image Compression with Quantized Hierarchical VAEs  [ :arrow_down: ](https://arxiv.org/pdf/2208.13056.pdf)
>  Recent work has shown a strong theoretical connection between variational autoencoders (VAEs) and the rate distortion theory. Motivated by this, we consider the problem of lossy image compression from the perspective of generative modeling. Starting from ResNet VAEs, which are originally designed for data (image) distribution modeling, we redesign their latent variable model using a quantization-aware posterior and prior, enabling easy quantization and entropy coding for image compression. Along with improved neural network blocks, we present a powerful and efficient class of lossy image coders, outperforming previous methods on natural image (lossy) compression. Our model compresses images in a coarse-to-fine fashion and supports parallel encoding and decoding, leading to fast execution on GPUs.      
### 33.Impact of Loss Model Selection on Power Semiconductor Lifetime Prediction in Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2208.13019.pdf)
>  Power loss estimation is an indispensable procedure to conduct lifetime prediction for power semiconductor device. The previous studies successfully perform steady-state power loss estimation for different applications, but which may be limited for the electric vehicles (EVs) with high dynamics. Based on two EV standard driving cycle profiles, this paper gives a comparative study of power loss estimation models with two different time resolutions, i.e., the output period average and the switching period average. The correspondingly estimated power losses, thermal profiles, and lifetime clearly pointed out that the widely applied power loss model with the output period average is limited for EV applications, in particular for the highly dynamic driving cycle. The difference in the predicted lifetime can be up to 300 times due to the unreasonable choice the loss model, which calls for the industry attention on the differences of the EVs and the importance of loss model selection in lifetime prediction.      
### 34.Latent Signal Models: Learning Compact Representations of Signal Evolution for Improved Time-Resolved, Multi-contrast MRI  [ :arrow_down: ](https://arxiv.org/pdf/2208.13003.pdf)
>  Purpose: Training auto-encoders on simulated signal evolution and inserting the decoder into the forward model improves reconstructions through more compact, Bloch-equation-based representations of signal in comparison to linear subspaces. <br>Methods: Building on model-based nonlinear and linear subspace techniques that enable reconstruction of signal dynamics, we train auto-encoders on dictionaries of simulated signal evolution to learn more compact, non-linear, latent representations. The proposed Latent Signal Model framework inserts the decoder portion of the auto-encoder into the forward model and directly reconstructs the latent representation. Latent Signal Models essentially serve as a proxy for fast and feasible differentiation through the Bloch-equations used to simulate signal. This work performs experiments in the context of T2-shuffling, gradient echo EPTI, and MPRAGE-shuffling. We compare how efficiently auto-encoders represent signal evolution in comparison to linear subspaces. Simulation and in-vivo experiments then evaluate if reducing degrees of freedom by inserting the decoder into the forward model improves reconstructions in comparison to subspace constraints. <br>Results: An auto-encoder with one real latent variable represents FSE, EPTI, and MPRAGE signal evolution as well as linear subspaces characterized by four basis vectors. In simulated/in-vivo T2-shuffling and in-vivo EPTI experiments, the proposed framework achieves consistent quantitative NRMSE and qualitative improvement over linear approaches. From qualitative evaluation, the proposed approach yields images with reduced blurring and noise amplification in MPRAGE shuffling experiments. <br>Conclusion: Directly solving for non-linear latent representations of signal evolution improves time-resolved MRI reconstructions through reduced degrees of freedom.      
### 35.Marine Integrated Energy Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2208.12980.pdf)
>  Marine sector decarbonization is another important battlefield for meeting the goal of climate action and ensuring the fulfillment of ambitions for a zero-emission society. Driven immediately by the policy incentives such as Energy Efficiency Design Index (EEDI) from International Maritime Organization(IMO), carbon taxation and labeling, a series of innovations centered around marine transportation are emerging from both industry and academia. As an efficient energy system form, the microgrid is playing an increasingly important role as the system constitution form for various marine energy systems. With specific concern about multi-energy integrations, the conventional definition of the microgrid needs also to be extended for an integrated energy system. In this paper, we will first introduce the extended concept of the microgrid as an integrated energy system and its applications in the marine sector, and then present the state of the art for the control, operation, and system integration for it and its clusters. The challenges and opportunities for marine integrated energy microgrids will also be discussed to shed light on future research.      
### 36.Grid-Forming Loads: Can the loads be in charge of forming the grid in modern power systems?  [ :arrow_down: ](https://arxiv.org/pdf/2208.12966.pdf)
>  Modern power systems are facing the tremendous challenge of integrating vast amounts of variable (non-dispatchable) renewable generation capacity, such as solar photovoltaic or wind power. In this context, the required power system flexibility needs to be allocated in other units that can include energy storage, demand management or providing reserve from renewables curtailing the output power. The present paper proposes the new concept of grid-forming load, which can be considered a totally flexible concept of demand. The concept is not only ensuring the load is supporting the grid stability by adapting the load to the overall system balancing, but also ensures that the load is actually contributing to form the grid and to provide synchronization power to the overall system. In this sense, the new concept allows running a system powered only by renewables operating at maximum power (or operator defined set-point) in grid-following mode, while the overall system control is ubicated in the demand side. This is an important change of paradigm as it considers that all the flexibility, synchronism and stability provision is on the demand side. This concept can applied either to isolated systems or also to future power systems, where millions of loads steer the power system while the renewables are operating at full power. The paper proposes the concept, suggests possible control implementations of the grid-forming load and analyses the concept in four simulation case studies.      
### 37.Uniformly Sampled Polar and Cylindrical Grid Approach for 2D, 3D Image Reconstruction using Algebraic Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2208.12964.pdf)
>  Image reconstruction by Algebraic Methods (AM) outperforms the transform methods in situations where the data collection procedure is constrained by time, space, and radiation dose. AM algorithms can also be applied for the cases where these constraints are not present but their high computational and storage requirement prohibit their actual breakthrough in such cases. In the present work, we propose a novel Uniformly Sampled Polar/Cylindrical Grid (USPG/USCG) discretization scheme to reduce the computational and storage burden of algebraic methods. The symmetries of USPG/USCG are utilized to speed up the calculations of the projection coefficients. In addition, we also offer an efficient approach for USPG to Cartesian Grid (CG) transformation for the visualization. The Multiplicative Algebraic Reconstruction Technique (MART) has been used to determine the field function of the suggested grids. Experimental projections data of a frog and Cu-Lump have been exercised to validate the proposed approach. A variety of image quality measures have been evaluated to check the accuracy of the reconstruction. Results indicate that the current strategies speed up (when compared to CG-based algorithms) the reconstruction process by a factor of 2.5 and reduce the memory requirement by the factor p, the number of projections used in the reconstruction.      
### 38.Global RTK Positioning in Graphical State Space  [ :arrow_down: ](https://arxiv.org/pdf/2208.12923.pdf)
>  This paper proposes a new method for RTK post-processing. Different from the traditional forward-backward Kalman filter, in our method, the whole system equation is built on a graphical state space model and solved by factor graph optimization. The position solution provided by the forward Kalman filter is used as the linearization points of the graphical state space model. Constant variables, such as double-difference ambiguity, will exist as constants in the graphical state space model, not as time-series variables. It is shown by experiment results that factor graph optimization with a graphical state space model is more effective than Kalman filter with a traditional discrete-time state space model for RTK post-processing problem.      
### 39.Research on Multi-Objective Planning of Electric Vehicle Charging Stations Considering the Condition of Urban Traffic Network  [ :arrow_down: ](https://arxiv.org/pdf/2208.12921.pdf)
>  As an important supporting facility for electric vehicles, the reasonable planning and layout of charging stations are of great significance to the development of electric vehicles. However, the planning and layout of charging stations is affected by various complex factors such as policy economy, charging demand, user charging comfort, and road traffic conditions. How to weigh various factors to construct a reasonable model of charging station location and capacity has become a major difficulty in the field of electric vehicle charging facility planning. Firstly, this paper constructs the location and capacity optimization model of the charging station with the goal of maximizing the revenue of operators and minimizing the user's charging additional cost. At the same time, the road time-consuming index is introduced to quantify the impact of road congestion on the user's charging additional cost, so as to effectively improve the user's satisfaction during charging. Then, aiming at the charging station planning model, a non-dominated sorting genetic algorithm with an elite strategy (NSGA-II) based on chaos initialization and arithmetic crossover operator is proposed. Finally, taking the Haidian District of Beijing as the simulation object, the results show that compared with the situation of urban traffic networks not considered, the model proposed in this paper significantly reduces the cost of lost time of users by 11.4% and the total additional cost of users' charging by 7.6%. It not only ensures the economy of the system, but also effectively improves the charging satisfaction of users, which further verifies the feasibility and effectiveness of the model, and can provide a reference for the planning and layout of charging stations in the future.      
### 40.Stackelberg game-based optimal scheduling of integrated energy systems considering differences in heat demand across multi-functional areas  [ :arrow_down: ](https://arxiv.org/pdf/2208.12916.pdf)
>  Demand-side management is very critical in China's energy systems because of its high fossil energy consumption and low system energy efficiency. A building shape factor is introduced in describing the architectural characteristics of different functional areas, which are combined with the characteristics of the energy consumed by users to investigate the features of heating load in different functional areas. A Stackelberg game-based optimal scheduling model is proposed for electro-thermal integrated energy systems, which seeks to maximize the revenue of integrated energy operator (IEO) and minimize the cost of users. Here, IEO and users are the Stackelberg game leader and followers, respectively. The leader uses real-time energy prices to guide loads to participate in demand response, while the followers make energy plans based on price feedback. Using the Karush-Kuhn-Tucker (KKT) condition and the big-M method, the model is transformed into a mixed-integer quadratic programming (MIQP) problem, which is solved by using MATLAB and CPLEX software. The results demonstrate that the proposal manages to balance the interests of IEO and users. Furthermore, the heating loads of public and residential areas can be managed separately based on the differences in energy consumption and building shape characteristics, thereby improving the system operational flexibility and promoting renewable energy consumption.      
### 41.Multi-Modality Cardiac Image Computing: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2208.12881.pdf)
>  Multi-modality cardiac imaging plays a key role in the management of patients with cardiovascular diseases. It allows a combination of complementary anatomical, morphological and functional information, increases diagnosis accuracy, and improves the efficacy of cardiovascular interventions and clinical outcomes. Fully-automated processing and quantitative analysis of multi-modality cardiac images could have a direct impact on clinical research and evidence-based patient management. However, these require overcoming significant challenges including inter-modality misalignment and finding optimal methods to integrate information from different modalities. <br>This paper aims to provide a comprehensive review of multi-modality imaging in cardiology, the computing methods, the validation strategies, the related clinical workflows and future perspectives. For the computing methodologies, we have a favored focus on the three tasks, i.e., registration, fusion and segmentation, which generally involve multi-modality imaging data, \textit{either combining information from different modalities or transferring information across modalities}. The review highlights that multi-modality cardiac imaging data has the potential of wide applicability in the clinic, such as trans-aortic valve implantation guidance, myocardial viability assessment, and catheter ablation therapy and its patient selection. Nevertheless, many challenges remain unsolved, such as missing modality, combination of imaging and non-imaging data, and uniform analysis and representation of different modalities. There is also work to do in defining how the well-developed techniques fit in clinical workflows and how much additional and relevant information they introduce. These problems are likely to continue to be an active field of research and the questions to be answered in the future.      
### 42.Digital Image Processing Applied To Object Segmentation By Intensity And Motion  [ :arrow_down: ](https://arxiv.org/pdf/2208.12870.pdf)
>  The current technological development allows us to carry out tasks that some time ago were unthinkable if not impossible, digital image processing has been one of the major constants of development today, taking into account that its implementation dates from a short time ago, OpenCV [1] is a tool focused on machine vision, In this case implemented in an object-oriented programming platform based on Java language offered by the NetBeans development software, based on the above, a physical platform was proposed and implemented as a closed environment which through the development of an algorithm allowed detection and segmentation of objects by means of the RGB color model; In future works this algorithm will provide the information base for the autonomous robotic platform; this advance opens a wide spectrum for the development of applications and tools in the field of artificial vision.      
### 43.Reducing Computational Complexity of Neural Networks in Optical Channel Equalization: From Concepts to Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2208.12866.pdf)
>  In this paper, a new methodology is proposed that allows for the low-complexity development of neural network (NN) based equalizers for the mitigation of impairments in high-speed coherent optical transmission systems. In this work, we provide a comprehensive description and comparison of various deep model compression approaches that have been applied to feed-forward and recurrent NN designs. Additionally, we evaluate the influence these strategies have on the performance of each NN equalizer. Quantization, weight clustering, pruning, and other cutting-edge strategies for model compression are taken into consideration. In this work, we propose and evaluate a Bayesian optimization-assisted compression, in which the hyperparameters of the compression are chosen to simultaneously reduce complexity and improve performance. In conclusion, the trade-off between the complexity of each compression approach and its performance is evaluated by utilizing both simulated and experimental data in order to complete the analysis. By utilizing optimal compression approaches, we show that it is possible to design an NN-based equalizer that is simpler to implement and has better performance than the conventional digital back-propagation (DBP) equalizer with only one step per span. This is accomplished by reducing the number of multipliers used in the NN equalizer after applying the weighted clustering and pruning algorithms. Furthermore, we demonstrate that an equalizer based on NN can also achieve superior performance while still maintaining the same degree of complexity as the full electronic chromatic dispersion compensation block. We conclude our analysis by highlighting open questions and existing challenges, as well as possible future research directions.      
### 44.Region-guided CycleGANs for Stain Transfer in Whole Slide Images  [ :arrow_down: ](https://arxiv.org/pdf/2208.12847.pdf)
>  In whole slide imaging, commonly used staining techniques based on hematoxylin and eosin (H&amp;E) and immunohistochemistry (IHC) stains accentuate different aspects of the tissue landscape. In the case of detecting metastases, IHC provides a distinct readout that is readily interpretable by pathologists. IHC, however, is a more expensive approach and not available at all medical centers. Virtually generating IHC images from H&amp;E using deep neural networks thus becomes an attractive alternative. Deep generative models such as CycleGANs learn a semantically-consistent mapping between two image domains, while emulating the textural properties of each domain. They are therefore a suitable choice for stain transfer applications. However, they remain fully unsupervised, and possess no mechanism for enforcing biological consistency in stain transfer. In this paper, we propose an extension to CycleGANs in the form of a region of interest discriminator. This allows the CycleGAN to learn from unpaired datasets where, in addition, there is a partial annotation of objects for which one wishes to enforce consistency. We present a use case on whole slide images, where an IHC stain provides an experimentally generated signal for metastatic cells. We demonstrate the superiority of our approach over prior art in stain transfer on histopathology tiles over two datasets. Our code and model are available at <a class="link-external link-https" href="https://github.com/jcboyd/miccai2022-roigan" rel="external noopener nofollow">this https URL</a>.      
### 45.A Path Towards Clinical Adaptation of Accelerated MRI  [ :arrow_down: ](https://arxiv.org/pdf/2208.12835.pdf)
>  Accelerated MRI reconstructs images of clinical anatomies from sparsely sampled signal data to reduce patient scan times. While recent works have leveraged deep learning to accomplish this task, such approaches have often only been explored in simulated environments where there is no signal corruption or resource limitations. In this work, we explore augmentations to neural network MRI image reconstructors to enhance their clinical relevancy. Namely, we propose a ConvNet model for detecting sources of image artifacts that achieves a classifer $F_2$ score of $79.1\%$. We also demonstrate that training reconstructors on MR signal data with variable acceleration factors can improve their average performance during a clinical patient scan by up to $2\%$. We offer a loss function to overcome catastrophic forgetting when models learn to reconstruct MR images of multiple anatomies and orientations. Finally, we propose a method for using simulated phantom data to pre-train reconstructors in situations with limited clinically acquired datasets and compute capabilities. Our results provide a potential path forward for clinical adaptation of accelerated MRI.      
### 46.Speech Emotion Recognition using Supervised Deep Recurrent System for Mental Health Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2208.12812.pdf)
>  Understanding human behavior and monitoring mental health are essential to maintaining the community and society's safety. As there has been an increase in mental health problems during the COVID-19 pandemic due to uncontrolled mental health, early detection of mental issues is crucial. Nowadays, the usage of Intelligent Virtual Personal Assistants (IVA) has increased worldwide. Individuals use their voices to control these devices to fulfill requests and acquire different services. This paper proposes a novel deep learning model based on the gated recurrent neural network and convolution neural network to understand human emotion from speech to improve their IVA services and monitor their mental health.      
### 47.Riesz-Quincunx-UNet Variational Auto-Encoder for Satellite Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2208.12810.pdf)
>  Multiresolution deep learning approaches, such as the U-Net architecture, have achieved high performance in classifying and segmenting images. However, these approaches do not provide a latent image representation and cannot be used to decompose, denoise, and reconstruct image data. The U-Net and other convolutional neural network (CNNs) architectures commonly use pooling to enlarge the receptive field, which usually results in irreversible information loss. This study proposes to include a Riesz-Quincunx (RQ) wavelet transform, which combines 1) higher-order Riesz wavelet transform and 2) orthogonal Quincunx wavelets (which have both been used to reduce blur in medical images) inside the U-net architecture, to reduce noise in satellite images and their time-series. In the transformed feature space, we propose a variational approach to understand how random perturbations of the features affect the image to further reduce noise. Combining both approaches, we introduce a hybrid RQUNet-VAE scheme for image and time series decomposition used to reduce noise in satellite imagery. We present qualitative and quantitative experimental results that demonstrate that our proposed RQUNet-VAE was more effective at reducing noise in satellite imagery compared to other state-of-the-art methods. We also apply our scheme to several applications for multi-band satellite images, including: image denoising, image and time-series decomposition by diffusion and image segmentation.      
### 48.Optimization of Spectral Efficiency in Cell-Free massive MIMO Systems Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.13727.pdf)
>  Cellular communication is a widely used technology in the world where the coverage area is divided into multiple cells. Interference is one of the most important challenges in cellular networks which causes problems by reducing the quality of the service. Cell-Free (CF) massive multiple-input multiple-output (MIMO) is a novel technolgy in which a large number of distributed access points (APs) are concurrently serving a small number of user equipment (UE). CF network can be an alternative technology to cellular networks for reducing interference. A challenging task in a CF network is scalability, where although the number of UEs tends to infinity, the computational complexity must remain finite in each AP or UE. In this paper, we provide two architectures of Dense fully connected neural network (Dense_Net) and 1D convolution neural network (Conv_Net) to be implemented in different cases in terms of the number of antennas in each AP/UE and the method for the combining vector. The Dense_Net outperforms the Conv_Net in all the cases. For instance, in the first case it has a %62.87 improvement in terms of loss. The results show that our proposed method performs better in terms of obtaining optimal values for spectral efficiency (SE).      
### 49.Categorical semantics of compositional reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.13687.pdf)
>  Reinforcement learning (RL) often requires decomposing a problem into subtasks and composing learned behaviors on these tasks. Compositionality in RL has the potential to create modular subtask units that interface with other system capabilities. However, generating compositional models requires the characterization of minimal assumptions for the robustness of the compositional feature. We develop a framework for a \emph{compositional theory} of RL using a categorical point of view. Given the categorical representation of compositionality, we investigate sufficient conditions under which learning-by-parts results in the same optimal policy as learning on the whole. In particular, our approach introduces a category $\mathsf{MDP}$, whose objects are Markov decision processes (MDPs) acting as models of tasks. We show that $\mathsf{MDP}$ admits natural compositional operations, such as certain fiber products and pushouts. These operations make explicit compositional phenomena in RL and unify existing constructions, such as puncturing hazardous states in composite MDPs and incorporating state-action symmetry. We also model sequential task completion by introducing the language of zig-zag diagrams that is an immediate application of the pushout operation in $\mathsf{MDP}$.      
### 50.Jacobian Methods for Dynamic Polarization Control in Optical Applications  [ :arrow_down: ](https://arxiv.org/pdf/2208.13660.pdf)
>  Dynamic polarization control (DPC) is beneficial for many optical applications. It uses adjustable waveplates to perform automatic polarization tracking and manipulation. Efficient algorithms are essential to realizing an endless polarization control process at high speed. However, the standard gradientbased algorithm is not well analyzed. Here we model the DPC with a Jacobian-based control theory framework that finds a lot in common with robot kinematics. We then give a detailed analysis of the condition of the Stokes vector gradient as a Jacobian matrix. We identify the multi-stage DPC as a redundant system enabling control algorithms with null-space operations. An efficient, reset-free algorithm can be found. We anticipate more customized DPC algorithms to follow the same framework in various optical systems.      
### 51.A Variance-Reduced Stochastic Gradient Tracking Algorithm for Decentralized Optimization with Orthogonality Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2208.13643.pdf)
>  Decentralized optimization with orthogonality constraints is found widely in scientific computing and data science. Since the orthogonality constraints are nonconvex, it is quite challenging to design efficient algorithms. Existing approaches leverage the geometric tools from Riemannian optimization to solve this problem at the cost of high sample and communication complexities. To relieve this difficulty, based on two novel techniques that can waive the orthogonality constraints, we propose a variance-reduced stochastic gradient tracking (VRSGT) algorithm with the convergence rate of $O(1 / k)$ to a stationary point. To the best of our knowledge, VRSGT is the first algorithm for decentralized optimization with orthogonality constraints that reduces both sampling and communication complexities simultaneously. In the numerical experiments, VRSGT has a promising performance in a real-world autonomous driving application.      
### 52.Convergent Economic Model Predictive Control through parameter-varying storage functions for dissipativity  [ :arrow_down: ](https://arxiv.org/pdf/2208.13342.pdf)
>  This paper presents a new concept of controlled dissipativity as an extension of the standard dissipativity property to systems with parameter-varying storage functions under the framework of economic model predictive control (EMPC). Based on this concept, two EMPC controllers, integrated with the dissipation inequality constraints rendering the storage function parameters as decision variables, are formulated and the associated recursive feasibility is ensured. Then, the asymptotic convergence to an optimal equilibrium in closed-loop, without requiring the standard dissipativity assumption, is enforced by trading it off with asymptotic performance. The upper bound of asymptotic average closed-loop performance is also evaluated. Finally, an illustrative example by using the EMPC controllers with terminal equilibrium or terminal region conditions is provided to show the effectiveness of our methods.      
### 53.Streaming Intended Query Detection using E2E Modeling for Continued Conversation  [ :arrow_down: ](https://arxiv.org/pdf/2208.13322.pdf)
>  In voice-enabled applications, a predetermined hotword isusually used to activate a device in order to attend to the query.However, speaking queries followed by a hotword each timeintroduces a cognitive burden in continued conversations. Toavoid repeating a hotword, we propose a streaming end-to-end(E2E) intended query detector that identifies the utterancesdirected towards the device and filters out other utterancesnot directed towards device. The proposed approach incor-porates the intended query detector into the E2E model thatalready folds different components of the speech recognitionpipeline into one neural network.The E2E modeling onspeech decoding and intended query detection also allows us todeclare a quick intended query detection based on early partialrecognition result, which is important to decrease latencyand make the system responsive. We demonstrate that theproposed E2E approach yields a 22% relative improvement onequal error rate (EER) for the detection accuracy and 600 mslatency improvement compared with an independent intendedquery detector. In our experiment, the proposed model detectswhether the user is talking to the device with a 8.7% EERwithin 1.4 seconds of median latency after user starts speaking.      
### 54.Turn-Taking Prediction for Natural Conversational Speech  [ :arrow_down: ](https://arxiv.org/pdf/2208.13321.pdf)
>  While a streaming voice assistant system has been used in many applications, this system typically focuses on unnatural, one-shot interactions assuming input from a single voice query without hesitation or disfluency. However, a common conversational utterance often involves multiple queries with turn-taking, in addition to disfluencies. These disfluencies include pausing to think, hesitations, word lengthening, filled pauses and repeated phrases. This makes doing speech recognition with conversational speech, including one with multiple queries, a challenging task. To better model the conversational interaction, it is critical to discriminate disfluencies and end of query in order to allow the user to hold the floor for disfluencies while having the system respond as quickly as possible when the user has finished speaking. In this paper, we present a turntaking predictor built on top of the end-to-end (E2E) speech recognizer. Our best system is obtained by jointly optimizing for ASR task and detecting when the user is paused to think or finished speaking. The proposed approach demonstrates over 97% recall rate and 85% precision rate on predicting true turn-taking with only 100 ms latency on a test set designed with 4 types of disfluencies inserted in conversational utterances.      
### 55.Minute ventilation measurement using Plethysmographic Imaging and lighting parameters  [ :arrow_down: ](https://arxiv.org/pdf/2208.13319.pdf)
>  Breathing disorders such as sleep apnea is a critical disorder that affects a large number of individuals due to the insufficient capacity of the lungs to contain/exchange oxygen and carbon dioxide to ensure that the body is in the stable state of homeostasis. Respiratory Measurements such as minute ventilation can be used in correlation with other physiological measurements such as heart rate and heart rate variability for remote monitoring of health and detecting symptoms of such breathing related disorders. In this work, we formulate a deep learning based approach to measure remote ventilation on a private dataset. The dataset will be made public upon acceptance of this work. We use two versions of a deep neural network to estimate the minute ventilation from data streams obtained through wearable heart rate and respiratory devices. We demonstrate that the simple design of our pipeline - which includes lightweight deep neural networks - can be easily incorporate into real time health monitoring systems.      
### 56.Computing with Hypervectors for Efficient Speaker Identification  [ :arrow_down: ](https://arxiv.org/pdf/2208.13285.pdf)
>  We introduce a method to identify speakers by computing with high-dimensional random vectors. Its strengths are simplicity and speed. With only 1.02k active parameters and a 128-minute pass through the training data we achieve Top-1 and Top-5 scores of 31% and 52% on the VoxCeleb1 dataset of 1,251 speakers. This is in contrast to CNN models requiring several million parameters and orders of magnitude higher computational complexity for only a 2$\times$ gain in discriminative power as measured in mutual information. An additional 92 seconds of training with Generalized Learning Vector Quantization (GLVQ) raises the scores to 48% and 67%. A trained classifier classifies 1 second of speech in 5.7 ms. All processing was done on standard CPU-based machines.      
### 57.Bipolar Almost Equiangular Tight Frames for NOMA Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13260.pdf)
>  Non-Orthogonal Multiple Access (NOMA) is a concept which is gaining a big popularity in multiuser networks. It's due to its advantages in sense of total network throughput. It becomes especially significant in large networks such as Internet of Things (IoT) networks or 5G networks. One of the known NOMA techniques is DS-CDMA NOMA, which make use of non-orthogonal coding schemes to optimize capacity at multiuser networks. Equiangular Tight Frames (ETF) are known to be an optimal sequences' sets (in sense of capacity) for this technique. Unfortunately, ETFs are limited to very specific pairs of users' number and sequence lengths which put undesirable constraints on practical systems. In this paper our goal is to break those constraints by proposing alternative family of non-orthogonal sequences which on the one hand, possess similar properties to those of ETFs (for that reason we'll denote them as Almost ETFs) and on the other hand, doesn't have those limitation on users' number and sequence length. We're basing our approach by starting with known technique of building standard ETFs, and extending it by slight modifications to technique of building AETFs. In this paper we'll concentrate on bipolar (+/-1 valued) and relatively short (up to length of 100) sequences, since we're interested in sequences which will be of practical value in real systems.      
### 58.Towards Disentangled Speech Representations  [ :arrow_down: ](https://arxiv.org/pdf/2208.13191.pdf)
>  The careful construction of audio representations has become a dominant feature in the design of approaches to many speech tasks. Increasingly, such approaches have emphasized "disentanglement", where a representation contains only parts of the speech signal relevant to transcription while discarding irrelevant information. In this paper, we construct a representation learning task based on joint modeling of ASR and TTS, and seek to learn a representation of audio that disentangles that part of the speech signal that is relevant to transcription from that part which is not. We present empirical evidence that successfully finding such a representation is tied to the randomness inherent in training. We then make the observation that these desired, disentangled solutions to the optimization problem possess unique statistical properties. Finally, we show that enforcing these properties during training improves WER by 24.5% relative on average for our joint modeling task. These observations motivate a novel approach to learning effective audio representations.      
### 59.Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2208.13183.pdf)
>  Transfer tasks in text-to-speech (TTS) synthesis - where one or more aspects of the speech of one set of speakers is transferred to another set of speakers that do not feature these aspects originally - remains a challenging task. One of the challenges is that models that have high-quality transfer capabilities can have issues in stability, making them impractical for user-facing critical tasks. This paper demonstrates that transfer can be obtained by training a robust TTS system on data generated by a less robust TTS system designed for a high-quality transfer task; in particular, a CHiVE-BERT monolingual TTS system is trained on the output of a Tacotron model designed for accent transfer. While some quality loss is inevitable with this approach, experimental results show that the models trained on synthetic data this way can produce high quality audio displaying accent transfer, while preserving speaker characteristics such as speaking style.      
### 60.Scalable Nanophotonic-Electronic Spiking Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.13144.pdf)
>  Spiking neural networks (SNN) provide a new computational paradigm capable of highly parallelized, real-time processing. Photonic devices are ideal for the design of high-bandwidth, parallel architectures matching the SNN computational paradigm. Co-integration of CMOS and photonic elements allow low-loss photonic devices to be combined with analog electronics for greater flexibility of nonlinear computational elements. As such, we designed and simulated an optoelectronic spiking neuron circuit on a monolithic silicon photonics (SiPh) process that replicates useful spiking behaviors beyond the leaky integrate-and-fire (LIF). Additionally, we explored two learning algorithms with the potential for on-chip learning using Mach-Zehnder Interferometric (MZI) meshes as synaptic interconnects. A variation of Random Backpropagation (RPB) was experimentally demonstrated on-chip and matched the performance of a standard linear regression on a simple classification task. Meanwhile, the Contrastive Hebbian Learning (CHL) rule was applied to a simulated neural network composed of MZI meshes for a random input-output mapping task. The CHL-trained MZI network performed better than random guessing but does not match the performance of the ideal neural network (without the constraints imposed by the MZI meshes). Through these efforts, we demonstrate that co-integrated CMOS and SiPh technologies are well-suited to the design of scalable SNN computing architectures.      
### 61.On the design of Massive MIMO-QAM detector via $\ell_2$-Box ADMM approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.13122.pdf)
>  In this letter, we develop an $\ell_2$-box maximum likelihood (ML) formulation for massive multiple-input multiple-output (MIMO) quadrature amplitude modulation (QAM) signal detection and customize an alternating direction method of multipliers (ADMM) algorithm to solve the nonconvex optimization model. In the $\ell_2$-box ADMM implementation, all variables are solved analytically. Moreover, several theoretical results related to convergence, iteration complexity, and computational complexity are presented. Simulation results demonstrate the effectiveness of the proposed $\ell_2$-box ADMM detector in comparison with state-of-the-arts approaches.      
### 62.Statistical Mechanics of Thermostatically Controlled Multi-Zone Buildings  [ :arrow_down: ](https://arxiv.org/pdf/2208.13099.pdf)
>  We study the collective phenomena and constraints associated with the aggregation of individual cooling units from a statistical mechanics perspective. These units are modelled as Thermostatically Controlled Loads (TCLs) and represent zones in a large commercial or residential building. Their energy input is centralized and controlled by a collective unit -- the Air Handling Unit (AHU) -- delivering cool air to all TCLs, thereby coupling them together. Aiming to identify representative qualitative features of the AHU-to-TCL coupling, we build a realistic but also sufficiently simple model and analyze it in two distinct regimes: the Constant Supply Temperature (CST) and the Constant Power Input (CPI) regimes. In both cases, we center our analysis on the relaxation dynamics of individual TCL temperatures to a statistically steady state. We observe that while the dynamics are relatively fast in the CST regime, resulting in all TCLs evolving around the control setpoint, the CPI regime reveals emergence of a \emph{bi-modal probability distribution and two, possibly strongly separated, time scales}. We observe that the two modes in the CPI regime are associated with all TCLs being in the same low and high-temperature states, respectively, with occasional (and therefore possibly rare) collective transition between the modes akin in the Kramer's phenomenon of statistical physics. To the best of our knowledge, this phenomenon was overlooked in the context of the multi-zone energy building engineering, even thought it has direct implications on the operations of centralized cooling systems in buildings. It teaches us that a balance needs to be struck between occupational comfort -- related to variations in the individual temperatures -- and power output predictability -- the main focus of the DR schemes.      
### 63.SA: Sliding attack for synthetic speech detection with resistance to clipping and self-splicing  [ :arrow_down: ](https://arxiv.org/pdf/2208.13066.pdf)
>  Deep neural networks are vulnerable to adversarial examples that mislead models with imperceptible perturbations. In audio, although adversarial examples have achieved incredible attack success rates on white-box settings and black-box settings, most existing adversarial attacks are constrained by the input length. A More practical scenario is that the adversarial examples must be clipped or self-spliced and input into the black-box model. Therefore, it is necessary to explore how to improve transferability in different input length settings. In this paper, we take the synthetic speech detection task as an example and consider two representative SOTA models. We observe that the gradients of fragments with the same sample value are similar in different models via analyzing the gradients obtained by feeding samples into the model after cropping or self-splicing. Inspired by the above observation, we propose a new adversarial attack method termed sliding attack. Specifically, we make each sampling point aware of gradients at different locations, which can simulate the situation where adversarial examples are input to black-box models with varying input lengths. Therefore, instead of using the current gradient directly in each iteration of the gradient calculation, we go through the following three steps. First, we extract subsegments of different lengths using sliding windows. We then augment the subsegments with data from the adjacent domains. Finally, we feed the sub-segments into different models to obtain aggregate gradients to update adversarial examples. Empirical results demonstrate that our method could significantly improve the transferability of adversarial examples after clipping or self-splicing. Besides, our method could also enhance the transferability between models based on different features.      
### 64.Improving Electricity Market Economy via Closed-Loop Predict-and-Optimize  [ :arrow_down: ](https://arxiv.org/pdf/2208.13065.pdf)
>  The electricity market clearing is usually implemented via an open-loop predict-then-optimize (O-PO) process: it first predicts the available power of renewable energy sources (RES) and the system reserve requirements; then, given the predictions, the markets are cleared via optimization models, i.e., unit commitment (UC) and economic dispatch (ED), to pursue the optimal electricity market economy. However, the market economy could suffer from the open-loop process because its predictions may be overly myopic to the optimizations, i.e., the predictions seek to improve the immediate statistical forecasting errors instead of the ultimate market economy. To this end, this paper proposes a closed-loop predict-and-optimize (C-PO) framework based on the tri-level mixed-integer programming, which trains economy-oriented predictors tailored for the market-clearing optimization to improve the ultimate market economy. Specifically, the upper level trains the economy-oriented RES and reserve predictors according to their induced market economy; the middle and lower levels, with given predictions, mimic the market-clearing process and feed the induced market economy results back to the upper level. The trained economy-oriented predictors are then embedded into the UC model, forming a prescriptive UC model that can simultaneously provide RES-reserve predictions and UC decisions with enhanced market economy. Numerical case studies on an IEEE 118-bus system illustrate potential economic and practical advantages of C-PO over O-PO, robust UC, and stochastic UC.      
### 65.Sub-mW Neuromorphic SNN audio processing applications with Rockpool and Xylo  [ :arrow_down: ](https://arxiv.org/pdf/2208.12991.pdf)
>  Spiking Neural Networks (SNNs) provide an efficient computational mechanism for temporal signal processing, especially when coupled with low-power SNN inference ASICs. SNNs have been historically difficult to configure, lacking a general method for finding solutions for arbitrary tasks. In recent years, gradient-descent optimization methods have been applied to SNNs with increasing ease. SNNs and SNN inference processors therefore offer a good platform for commercial low-power signal processing in energy constrained environments without cloud dependencies. However, to date these methods have not been accessible to ML engineers in industry, requiring graduate-level training to successfully configure a single SNN application. Here we demonstrate a convenient high-level pipeline to design, train and deploy arbitrary temporal signal processing applications to sub-mW SNN inference hardware. We apply a new straightforward SNN architecture designed for temporal signal processing, using a pyramid of synaptic time constants to extract signal features at a range of temporal scales. We demonstrate this architecture on an ambient audio classification task, deployed to the Xylo SNN inference processor in streaming mode. Our application achieves high accuracy (98%) and low latency (100ms) at low power (&lt;4muW inference power). Our approach makes training and deploying SNN applications available to ML engineers with general NN backgrounds, without requiring specific prior experience with spiking NNs. We intend for our approach to make Neuromorphic hardware and SNNs an attractive choice for commercial low-power and edge signal processing applications.      
### 66.SR-DCSK Cooperative Communication System with Code Index Modulation: A New Design for 6G New Radios  [ :arrow_down: ](https://arxiv.org/pdf/2208.12970.pdf)
>  This paper proposes a high-throughput short reference differential chaos shift keying cooperative communication system with the aid of code index modulation, referred to as CIM-SR-DCSK-CC system. In the proposed CIM-SR-DCSK-CC system, the source transmits information bits to both the relay and destination in the first time slot, while the relay not only forwards the source information bits but also sends new information bits to the destination in the second time slot. To be specific, the relay employs an $N$-order Walsh code to carry additional ${{\log }_{2}}N$ information bits, which are superimposed onto the SR-DCSK signal carrying the decoded source information bits. Subsequently, the superimposed signal carrying both the source and relay information bits is transmitted to the destination. Moreover, the theoretical bit error rate (BER) expressions of the proposed CIM-SR-DCSK-CC system are derived over additive white Gaussian noise (AWGN) and multipath Rayleigh fading channels. Compared with the conventional DCSK-CC system and SR-DCSK-CC system, the proposed CIM-SR-DCSK-CC system can significantly improve the throughput without deteriorating any BER performance. As a consequence, the proposed system is very promising for the applications of the 6G-enabled low-power and high-rate communication.      
### 67.Robust 3D Vision for Autonomous Robots  [ :arrow_down: ](https://arxiv.org/pdf/2208.12925.pdf)
>  This paper presents a fault-tolerant 3D vision system for autonomous robotic operation. In particular, pose estimation of space objects is achieved using 3D vision data in an integrated Kalman filter (KF) and an Iterative Closest Point (ICP) algorithm in a closed-loop configuration. The initial guess for the internal ICP iteration is provided by the state estimate propagation of the Kalman filer. The Kalman filter is capable of not only estimating the target's states but also its inertial parameters. This allows the motion of the target to be predictable as soon as the filter converges. Consequently, the ICP can maintain pose tracking over a wider range of velocity due to the increased precision of ICP initialization. Furthermore, incorporation of the target's dynamics model in the estimation process allows the estimator continuously provide pose estimation even when the sensor temporally loses its signal namely due to obstruction. The capabilities of the pose estimation methodology is demonstrated by a ground testbed for Automated Rendezvous &amp; Docking. In this experiment, Neptec's Laser Camera System (LCS) is used for real-time scanning of a satellite model attached to a manipulator arm, which is driven by a simulator according to orbital and attitude dynamics. The results showed that robust tracking of the free-floating tumbling satellite can be achieved only if the Kalman filter and ICP are in a closed-loop configuration.      
### 68.Pipeline-Invariant Representation Learning for Neuroimaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.12909.pdf)
>  Deep learning has been widely applied in neuroimaging, including to predicting brain-phenotype relationships from magnetic resonance imaging (MRI) volumes. MRI data usually requires extensive preprocessing before it is ready for modeling, even via deep learning, in part due to its high dimensionality and heterogeneity. A growing array of MRI preprocessing pipelines have been developed each with its own strengths and limitations. Recent studies have shown that pipeline-related variation may lead to different scientific findings, even when using the identical data. Meanwhile, the machine learning community has emphasized the importance of shifting from model-centric to data-centric approaches given that data quality plays an essential role in deep learning applications. Motivated by this idea, we first evaluate how preprocessing pipeline selection can impact the downstream performance of a supervised learning model. We next propose two pipeline-invariant representation learning methodologies, MPSL and PXL, to improve consistency in classification performance and to capture similar neural network representations between pipeline pairs. Using 2000 human subjects from the UK Biobank dataset, we demonstrate that both models present unique advantages, in particular that MPSL can be used to improve out-of-sample generalization to new pipelines, while PXL can be used to improve predictive performance consistency and representational similarity within a closed pipeline set. These results suggest that our proposed models can be applied to overcome pipeline-related biases and to improve reproducibility in neuroimaging prediction tasks.      
### 69.Investigating data partitioning strategies for crosslinguistic low-resource ASR evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2208.12888.pdf)
>  Many automatic speech recognition (ASR) data sets include a single pre-defined test set consisting of one or more speakers whose speech never appears in the training set. This "hold-speaker(s)-out" data partitioning strategy, however, may not be ideal for data sets in which the number of speakers is very small. This study investigates ten different data split methods for five languages with minimal ASR training resources. We find that (1) model performance varies greatly depending on which speaker is selected for testing; (2) the average word error rate (WER) across all held-out speakers is comparable not only to the average WER over multiple random splits but also to any given individual random split; (3) WER is also generally comparable when the data is split heuristically or adversarially; (4) utterance duration and intensity are comparatively more predictive factors of variability regardless of the data split. These results suggest that the widely used hold-speakers-out approach to ASR data partitioning can yield results that do not reflect model performance on unseen data or speakers. Random splits can yield more reliable and generalizable estimates when facing data sparsity.      
### 70.Constraining Pseudo-label in Self-training Unsupervised Domain Adaptation with Energy-based Model  [ :arrow_down: ](https://arxiv.org/pdf/2208.12885.pdf)
>  Deep learning is usually data starved, and the unsupervised domain adaptation (UDA) is developed to introduce the knowledge in the labeled source domain to the unlabeled target domain. Recently, deep self-training presents a powerful means for UDA, involving an iterative process of predicting the target domain and then taking the confident predictions as hard pseudo-labels for retraining. However, the pseudo-labels are usually unreliable, thus easily leading to deviated solutions with propagated errors. In this paper, we resort to the energy-based model and constrain the training of the unlabeled target sample with an energy function minimization objective. It can be achieved via a simple additional regularization or an energy-based loss. This framework allows us to gain the benefits of the energy-based model, while retaining strong discriminative performance following a plug-and-play fashion. The convergence property and its connection with classification expectation minimization are investigated. We deliver extensive experiments on the most popular and large-scale UDA benchmarks of image classification as well as semantic segmentation to demonstrate its generality and effectiveness.      
### 71.Neuromorphic Visual Scene Understanding with Resonator Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.12880.pdf)
>  Inferring the position of objects and their rigid transformations is still an open problem in visual scene understanding. Here we propose a neuromorphic solution that utilizes an efficient factorization network which is based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued vector binding on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN enables the definition of a partitioned architecture in which vector binding is equivariant for horizontal and vertical translation within one partition, and for rotation and scaling within the other partition. The spiking neuron model allows to map the resonator network onto efficient and low-power neuromorphic hardware. In this work, we demonstrate our approach using synthetic scenes composed of simple 2D shapes undergoing rigid geometric transformations and color changes. A companion paper demonstrates this approach in real-world application scenarios for machine vision and robotics.      
### 72.Solving large-scale MEG/EEG source localization and functional connectivity problems simultaneously using state-space models  [ :arrow_down: ](https://arxiv.org/pdf/2208.12854.pdf)
>  State-space models are used in many fields when dynamics are unobserved. Popular methods such as the Kalman filter and expectation maximization enable estimation of these models but pay a high computational cost in large-scale analysis. In these approaches, sparse inverse covariance estimators can reduce the cost; however, a trade-off between enforced sparsity and increased estimation bias occurs, which demands careful consideration in low signal-to-noise ratio scenarios. We overcome these limitations by 1) Introducing multiple penalized state-space models based on data-driven regularization; 2) Implementing novel algorithms such as backpropagation, state-space gradient descent, and alternating least squares; 3) Proposing an extension of K-fold cross-validation to evaluate the regularization parameters. Finally, we solve the simultaneous brain source localization and functional connectivity problems for simulated and real MEG/EEG signals for thousands of sources on the cortical surface, demonstrating a substantial improvement over state-of-the-art methods.      
