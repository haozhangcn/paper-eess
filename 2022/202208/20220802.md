# ArXiv eess --Tue, 2 Aug 2022
### 1.Information-Aware Guidance for Magnetic Anomaly based Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2208.00988.pdf)
>  In the absence of an absolute positioning system, such as GPS, autonomous vehicles are subject to accumulation of positional error which can interfere with reliable performance. Improved navigational accuracy without GPS enables vehicles to achieve a higher degree of autonomy and reliability, both in terms of decision making and safety. This paper details the use of two navigation systems for autonomous agents using magnetic field anomalies to localize themselves within a map; both techniques use the information content in the environment in distinct ways and are aimed at reducing the localization uncertainty. The first method is based on a nonlinear observability metric of the vehicle model, while the second is an information theory based technique which minimizes the expected entropy of the system. These conditions are used to design guidance laws that minimize the localization uncertainty and are verified both in simulation and hardware experiments are presented for the observability approach.      
### 2.DENT-DDSP: Data-efficient noisy speech generator using differentiable digital signal processors for explicit distortion modelling and noise-robust speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2208.00987.pdf)
>  The performances of automatic speech recognition (ASR) systems degrade drastically under noisy conditions. Explicit distortion modelling (EDM), as a feature compensation step, is able to enhance ASR systems under such conditions by simulating the in-domain noisy speeches from the clean counterparts. Yet, existing distortion models are either non-trainable or unexplainable and often lack controllability and generalization ability. In this paper, we propose a fully explainable and controllable model: DENT-DDSP to achieve EDM. DENT-DDSP utilizes novel differentiable digital signal processing (DDSP) components and requires only 10 seconds of training data to achieve high fidelity. The experiment shows that the simulated noisy data from DENT-DDSP achieves the highest simulation fidelity compared to other baseline models in terms of multi-scale spectral loss (MSSL). Moreover, to validate whether the data simulated by DENT-DDSP are able to replace the scarce in-domain noisy data in the noise-robust ASR tasks, several downstream ASR models with the same architecture are trained using the simulated data and the real data. The experiment shows that the model trained with the simulated noisy data from DENT-DDSP achieves similar performances to the benchmark with a 2.7\% difference in terms of word error rate (WER). The code of the model is released online.      
### 3.Fast Two-step Blind Optical Aberration Correction  [ :arrow_down: ](https://arxiv.org/pdf/2208.00950.pdf)
>  The optics of any camera degrades the sharpness of photographs, which is a key visual quality criterion. This degradation is characterized by the point-spread function (PSF), which depends on the wavelengths of light and is variable across the imaging field. In this paper, we propose a two-step scheme to correct optical aberrations in a single raw or JPEG image, i.e., without any prior information on the camera or lens. First, we estimate local Gaussian blur kernels for overlapping patches and sharpen them with a non-blind deblurring technique. Based on the measurements of the PSFs of dozens of lenses, these blur kernels are modeled as RGB Gaussians defined by seven parameters. Second, we remove the remaining lateral chromatic aberrations (not contemplated in the first step) with a convolutional neural network, trained to minimize the red/green and blue/green residual images. Experiments on both synthetic and real images show that the combination of these two stages yields a fast state-of-the-art blind optical aberration compensation technique that competes with commercial non-blind algorithms.      
### 4.Networked Drones for Industrial Emergency Events  [ :arrow_down: ](https://arxiv.org/pdf/2208.00931.pdf)
>  Uncontrolled emissions of gases from industrial accidents and disasters result in huge loss of life and property. Such extreme events require a quick and reliable survey of the site for effective rescue strategy planning. To achieve these goals, a network of unmanned aerial vehicles can be deployed that survey the affected region and identify safe and danger zones. Although single UAV-based systems for gas sensing applications are well-studied in literature, research on the deployment of a UAV network for such applications, which is more robust and fault tolerant, is still in infancy. The objective of this project is to design a system that can be deployed in emergency situations to provide a quick survey and identification of safe and dangerous zones in a given region that contains a toxic plume without making any assumptions about plume location. We focus on an end-to-end solution and formulate a two-phase strategy that can not only guarantee detection/acquisition of plume but also its characterization with high spatial resolution. To guarantee coverage of the region with a certain spatial resolution, we set up a vehicle routing problem. To overcome the limitations imposed by limited range of sensors and drone resources, we estimate the concentration map by using Gaussian kernel extrapolation. Finally, we evaluate the suggested framework in simulations. Our results suggest that this two-phase strategy not only gives better error performance but is also more efficient in terms of mission time. Moreover, the comparison between 2-phase random search and 2-phase uniform coverage suggest that the latter is better for single drone systems whereas for multiple drones the former gives reasonable performance at low computational cost.      
### 5.Many-to-One Knowledge Distillation of Real-Time Epileptic Seizure Detection for Low-Power Wearable Internet of Things Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.00885.pdf)
>  Integrating low-power wearable Internet of Things (IoT) systems into routine health monitoring is an ongoing challenge. Recent advances in the computation capabilities of wearables make it possible to target complex scenarios by exploiting multiple biosignals and using high-performance algorithms, such as Deep Neural Networks (DNNs). There is, however, a trade-off between performance of the algorithms and the low-power requirements of IoT platforms with limited resources. Besides, physically larger and multi-biosignal-based wearables bring significant discomfort to the patients. Consequently, reducing power consumption and discomfort is necessary for patients to use IoT devices continuously during everyday life. To overcome these challenges, in the context of epileptic seizure detection, we propose a many-to-one signals knowledge distillation approach targeting single-biosignal processing in IoT wearable systems. The starting point is to get a highly-accurate multi-biosignal DNN, then apply our approach to develop a single-biosignal DNN solution for IoT systems that achieves an accuracy comparable to the original multi-biosignal DNN. To assess the practicality of our approach to real-life scenarios, we perform a comprehensive simulation experiment analysis on several state-of-the-art edge computing platforms, such as Kendryte K210 and Raspberry Pi Zero.      
### 6.Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation  [ :arrow_down: ](https://arxiv.org/pdf/2208.00884.pdf)
>  In recent years, numerous automated approaches complementing the human Prechtl's general movements assessment (GMA) were developed. Most approaches utilised RGB or RGB-D cameras to obtain motion data, while a few employed accelerometers or inertial measurement units. In this paper, within a prospective longitudinal infant cohort study applying a multimodal approach for movement tracking and analyses, we examined for the first time the performance of pressure sensors for classifying an infant general movements pattern, the fidgety movements. We developed an algorithm to encode movements with pressure data from a 32x32 grid mat with 1024 sensors. Multiple neural network architectures were investigated to distinguish presence vs. absence of the fidgety movements, including the feed-forward networks (FFNs) with manually defined statistical features and the convolutional neural networks (CNNs) with learned features. The CNN with multiple convolutional layers and learned features outperformed the FFN with manually defined statistical features, with classification accuracy of $81.4\%$ and $75.6\%$, respectively. We compared the pros and cons of the pressure sensing approach to the video-based and inertial motion senor-based approaches for analysing infant movements. The non-intrusive, extremely easy-to-use pressure sensing approach has great potential for efficient large-scaled movement data acquisition across cites and for application in busy daily clinical routines for evaluating infant neuromotor functions. The pressure sensors can be combined with other sensor modalities to enhance infant movement analyses in research and practice, as proposed in our multimodal sensor fusion model.      
### 7.A Two-Stage Efficient 3-D CNN Framework for EEG Based Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2208.00883.pdf)
>  This paper proposes a novel two-stage framework for emotion recognition using EEG data that outperforms state-of-the-art models while keeping the model size small and computationally efficient. The framework consists of two stages; the first stage involves constructing efficient models named EEGNet, which is inspired by the state-of-the-art efficient architecture and employs inverted-residual blocks that contain depthwise separable convolutional layers. The EEGNet models on both valence and arousal labels achieve the average classification accuracy of 90%, 96.6%, and 99.5% with only 6.4k, 14k, and 25k parameters, respectively. In terms of accuracy and storage cost, these models outperform the previous state-of-the-art result by up to 9%. In the second stage, we binarize these models to further compress them and deploy them easily on edge devices. Binary Neural Networks (BNNs) typically degrade model accuracy. We improve the EEGNet binarized models in this paper by introducing three novel methods and achieving a 20\% improvement over the baseline binary models. The proposed binarized EEGNet models achieve accuracies of 81%, 95%, and 99% with storage costs of 0.11Mbits, 0.28Mbits, and 0.46Mbits, respectively. Those models help deploy a precise human emotion recognition system on the edge environment.      
### 8.Self-supervised Group Meiosis Contrastive Learning for EEG-Based Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2208.00877.pdf)
>  The progress in EEG-based emotion recognition has received widespread attention from the fields of human-machine interactions and cognitive science in recent years. However,the scarcity of high granularity and dense artificial labels contributes to the bottleneck of emotion recognition with high real-time and granularity. Neuroscience studies have discovered exploitable stimuli labels. It reveals that EEG samples recorded when subjects were triggered by the same stimuli share the same stimuli label. This paper proposed a Self-supervised Group Meiosis Contrastive Learning (SGMC) framework to exploit such stimuli labels for emotion recognition. The SGMC adopts a genetic inspired data augmentation method Meiosis. It achieves augmenting a group of samples sharing the same stimuli label to generate two augmented groups by pairing, cross exchanging, and separating. A group-projector-based model is adopted. The model achieves extracting a group-level representation by extracting individual representations from a group and aggregating them into a group-level representation. Contrastive learning is employed to maximize the similarity of group-level representations of augmented groups sharing the same stimuli label. The SGMC achieved the state-of-the-art results on the publicly available DEAP dataset with an accuracy of 94.72% and 95.68% in valence and arousal dimensions. Especially the SGMC shows more excellent performance on limited labeled sample learning. In addition, we verified the rationality of the framework design by control experiment and ablation study, and investigated the cause of the formation of good performance by feature visualization, and hyper parametric analysis. The code is provided publicly online      
### 9.Network Coexistence Analysis of RIS-Assisted Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2208.00875.pdf)
>  Reconfigurable intelligent surfaces (RISs) have attracted the attention of academia and industry circles because of their ability to control the electromagnetic characteristics of channel environments. However, it has been found that the introduction of an RIS may bring new and more serious network coexistence problems. It may even further deteriorate the network performance if these new network coexistence problems cannot be effectively solved. In this paper, an RIS network coexistence model is proposed and discussed in detail, and these problems are deeply analysed. Two novel RIS design mechanisms, including a novel multilayer RIS structure with an out-of-band filter and an RIS blocking mechanism, are further explored. Finally, numerical results and a discussion are given.      
### 10.Event-triggered Consensus Control of Heterogeneous Multi-agent Systems: Model- and Data-based Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.00867.pdf)
>  This article deals with model- and data-based consensus control of heterogenous leader-following multi-agent systems (MASs) under an event-triggering transmission scheme. A dynamic periodic transmission protocol is developed to significantly alleviate the transmission frequency and computational burden, where the followers can interact locally with each other approaching the dynamics of the leader. Capitalizing on a discrete-time looped-functional, a model-based consensus condition for the closed-loop MASs is derived in form of linear matrix inequalities (LMIs), as well as a design method for obtaining the distributed controllers and event-triggering parameters. Upon collecting noise-corrupted state-input measurements during open-loop operation, a data-driven leader-following MAS representation is presented, and employed to solve the data-driven consensus control problem without requiring any knowledge of the agents' models. This result is then extended to the case of guaranteeing an $\mathcal{H}_{\infty}$ performance. A simulation example is finally given to corroborate the efficacy of the proposed distributed event-triggering scheme in cutting off data transmissions and the data-driven design method.      
### 11.Spline-Shaped Microstrip Edge-Fed Antenna for 77 GHz Automotive Radar Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.00841.pdf)
>  An innovative millimeter-wave (mm-wave) microstrip edge-fed antenna (EFA) for 77 GHz automotive radars is proposed. The radiator contour is modeled with a sinusoidal spline-shaped (SS) profile characterized by a reduced number of geometrical descriptors, but still able to guarantee a high flexibility in the modeling for fulfilling challenging user-defined requirements. The SS-EFA descriptors are effectively and efficiently optimized with a customized implementation of the System-by-Design (SbD) paradigm. The synthesized EFA layout, integrated within a linear arrangement of identical replicas to account for the integration into the real radar system, exhibits suitable impedance matching, isolation, polarization purity, and stability of the beam shaping/pointing within the target band [76:78][GHz]. The experimental assessment, carried out with a Compact Antenna Test Range (CATR) system on a printed circuit board (PCB)-manufactured prototype, assess the reliability of the outcomes from the full-wave (FW) simulations as well as the suitability of the synthesized SS-EFA for automotive radars.      
### 12.Simulation and Measurement of Human Respiration and Heartbeat with Millimeter- Wave Radar  [ :arrow_down: ](https://arxiv.org/pdf/2208.00837.pdf)
>  This paper establishes a multi-scattering point chest wall motion model by combining the human respiration signal (RS) and HS (HS) measured by radar. An algorithmic process is designed based on the model to accurately separate the human respiration and heartbeat motion. Firstly, a human maximum motion velocity constraint method is proposed to correct human chest wall tracking, determine the radial position of the chest wall relative to the radar, and extract the phase signal corresponding to the chest wall motion. Then an improved time-difference method is proposed to suppress the interference of RS harmonics on HS and the interference of low-frequency noise on RS. Finally, an adaptive Gaussian weighting filter is designed to extract the RS with less distortion from the phase signal. A low-order finite-length unit impulse response (FIR) filter is used to extract the HS with less distortion from the phase signal. To verify the effectiveness of the proposed algorithm, simulating the process of measuring the RS and HS of the chest wall motion model by radar. The simulation results show that, ideally, the radar measurement results of the RS and HS are less distorted relative to the actual values. In addition, we used a millimeter-wave experimental radar system in the 60 GHz band to measure the respiration rate (RR) and HR (HR) of two subjects. The experimental results showed that the measured RR and HR correlated well with the actual values. The quantitative analysis of simulation results and experimental results show that the proposed method can achieve accurate and robust measurement of RS and HS.      
### 13.Enhanced Atmospheric Turbulence Resiliency with Successive Interference Cancellation DSP in Mode Division Multiplexing Free-Space Optical Links  [ :arrow_down: ](https://arxiv.org/pdf/2208.00836.pdf)
>  We experimentally demonstrate the enhanced atmospheric turbulence resiliency in a 137.8 Gbit/s/mode mode-division multiplexing free-space optical communication link through the application of a successive interference cancellation digital signal processing algorithm. The turbulence resiliency is further enhanced through redundant receive channels in the mode-division multiplexing link. The proof of concept demonstration is performed using commercially available mode-selective photonic lanterns, a commercial transponder, and a spatial light modulator based turbulence emulator. In this link, 5 spatial modes with each mode carrying 34.46 GBaud dual-polarization quadrature phase shift keying signals are successfully transmitted with an average bit error rate lower than the hard-decision forward error correction limit. As a result, we achieved a record-high mode- and polarization-division multiplexing channel number of 10, a record-high line rate of 689.23 Gbit/s, and a record-high net spectral efficiency of 13.9 bit/s/Hz in emulated turbulent links in a mode-division multiplexing free-space optical system.      
### 14.Characterization of noise regimes in Mid-IR free-space optical communication based on quantum cascade lasers  [ :arrow_down: ](https://arxiv.org/pdf/2208.00835.pdf)
>  The recent development of Quantum Cascade Lasers (QCLs) represents one of the biggest opportunities for the deployment of a new class of Free Space Optical (FSO) communication systems working in the mid-infrared (Mid-IR) wavelength range. As compared to more common FSO systems exploiting the telecom range, the larger wavelength employed in Mid-IR systems delivers exceptional benefits in case of adverse atmospheric conditions, as the reduced scattering rate strongly suppresses detrimental effects on the FSO link length given by the presence of rain, dust, fog and haze. In this work, we use a novel FSO testbed operating at \SI{4.7}{\micro m}, to provide a detailed experimental analysis of noise regimes that could occur in realistic FSO Mid-IR systems based on QCLs. Our analysis reveals the existence of two distinct noise regions, corresponding to different realistic channel attenuation conditions, which are precisely controlled in our setup. To relate our results with real outdoor configurations, we combine experimental data with predictions of an atmospheric channel loss model, finding that error-free communication could be attained for effective distances up to 8~km in low visibility conditions of 1 km. Our analysis of noise regimes may have a key relevance for the development of novel, long-range FSO communication systems based on Mid-IR QCL sources      
### 15.Synthetic Aperture Radar Doppler Tomography Reveals Details of Undiscovered High-Resolution Internal Structure of the Great Pyramid of Giza  [ :arrow_down: ](https://arxiv.org/pdf/2208.00811.pdf)
>  A problem with synthetic aperture radar (SAR) is that, due to the poor penetrating action of electromagnetic waves inside solid bodies, the capability to observe inside distributed targets is precluded. Under these conditions, imaging action is provided only on the surface of distributed targets. The present work describes an imaging method based on the analysis of micro-movements on the Khnum-Khufu Pyramid, which are usually generated by background seismic waves. The results obtained prove to be very promising, as high-resolution full 3D tomographic imaging of the pyramid's interior and subsurface was achieved. Khnum-Khufu becomes transparent like a crystal when observed in the micro-movement domain. Based on this novelty, we have completely reconstructed internal objects, observing and measuring structures that have never been discovered before. The experimental results are estimated by processing series of SAR images from the second-generation Italian COSMO-SkyMed satellite system, demonstrating the effectiveness of the proposed method.      
### 16.Deep COVID-19 Recognition using Chest X-ray Images: A Comparative Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.00784.pdf)
>  The novel coronavirus variant, which is also widely known as COVID-19, is currently a common threat to all humans across the world. Effective recognition of COVID-19 using advanced machine learning methods is a timely need. Although many sophisticated approaches have been proposed in the recent past, they still struggle to achieve expected performances in recognizing COVID-19 using chest X-ray images. In addition, the majority of them are involved with the complex pre-processing task, which is often challenging and time-consuming. Meanwhile, deep networks are end-to-end and have shown promising results in image-based recognition tasks during the last decade. Hence, in this work, some widely used state-of-the-art deep networks are evaluated for COVID-19 recognition with chest X-ray images. All the deep networks are evaluated on a publicly available chest X-ray image dataset. The evaluation results show that the deep networks can effectively recognize COVID-19 from chest X-ray images. Further, the comparison results reveal that the EfficientNetB7 network outperformed other existing state-of-the-art techniques.      
### 17.Brain Tumor Diagnosis and Classification via Pre-Trained Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00768.pdf)
>  The brain tumor is the most aggressive kind of tumor and can cause low life expectancy if diagnosed at the later stages. Manual identification of brain tumors is tedious and prone to errors. Misdiagnosis can lead to false treatment and thus reduce the chances of survival for the patient. Medical resonance imaging (MRI) is the conventional method used to diagnose brain tumors and their types. This paper attempts to eliminate the manual process from the diagnosis process and use machine learning instead. We proposed the use of pretrained convolutional neural networks (CNN) for the diagnosis and classification of brain tumors. Three types of tumors were classified with one class of non-tumor MRI images. Networks that has been used are ResNet50, EfficientNetB1, EfficientNetB7, EfficientNetV2B1. EfficientNet has shown promising results due to its scalable nature. EfficientNetB1 showed the best results with training and validation accuracy of 87.67% and 89.55%, respectively.      
### 18.Tutorial on the development of AI models for medical image analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.00766.pdf)
>  The idea of using computers to read medical scans was introduced as early as 1966. However, limits to machine learning technology meant progress was slow initially. The Alexnet breakthrough in 2012 sparked new interest in the topic, which resulted in the release of 100s of medical AI solutions on the market. In spite of success for some diseases and modalities, many challenges remain. Research typically focuses on the development of specific applications or techniques, clinical evaluation, or meta analysis of clinical studies or techniques through surveys or challenges. However, limited attention has been given to the development process of improving real world performance. In this tutorial, we address the latter and discuss some techniques to conduct the development process in order to make this as efficient as possible.      
### 19.Performance Comparison of Deep RL Algorithms for Energy Systems Optimal Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2208.00728.pdf)
>  Taking advantage of their data-driven and model-free features, Deep Reinforcement Learning (DRL) algorithms have the potential to deal with the increasing level of uncertainty due to the introduction of renewable-based generation. To deal simultaneously with the energy systems' operational cost and technical constraints (e.g, generation-demand power balance) DRL algorithms must consider a trade-off when designing the reward function. This trade-off introduces extra hyperparameters that impact the DRL algorithms' performance and capability of providing feasible solutions. In this paper, a performance comparison of different DRL algorithms, including DDPG, TD3, SAC, and PPO, are presented. We aim to provide a fair comparison of these DRL algorithms for energy systems optimal scheduling problems. Results show DRL algorithms' capability of providing in real-time good-quality solutions, even in unseen operational scenarios, when compared with a mathematical programming model of the energy system optimal scheduling problem. Nevertheless, in the case of large peak consumption, these algorithms failed to provide feasible solutions, which can impede their practical implementation.      
### 20.TransDeepLab: Convolution-Free Transformer-based DeepLab v3+ for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2208.00713.pdf)
>  Convolutional neural networks (CNNs) have been the de facto standard in a diverse set of computer vision tasks for many years. Especially, deep neural networks based on seminal architectures such as U-shaped models with skip-connections or atrous convolution with pyramid pooling have been tailored to a wide range of medical image analysis tasks. The main advantage of such architectures is that they are prone to detaining versatile local features. However, as a general consensus, CNNs fail to capture long-range dependencies and spatial correlations due to the intrinsic property of confined receptive field size of convolution operations. Alternatively, Transformer, profiting from global information modelling that stems from the self-attention mechanism, has recently attained remarkable performance in natural language processing and computer vision. Nevertheless, previous studies prove that both local and global features are critical for a deep model in dense prediction, such as segmenting complicated structures with disparate shapes and configurations. To this end, this paper proposes TransDeepLab, a novel DeepLab-like pure Transformer for medical image segmentation. Specifically, we exploit hierarchical Swin-Transformer with shifted windows to extend the DeepLabv3 and model the Atrous Spatial Pyramid Pooling (ASPP) module. A thorough search of the relevant literature yielded that we are the first to model the seminal DeepLab model with a pure Transformer-based model. Extensive experiments on various medical image segmentation tasks verify that our approach performs superior or on par with most contemporary works on an amalgamation of Vision Transformer and CNN-based methods, along with a significant reduction of model complexity. The codes and trained models are publicly available at <a class="link-external link-https" href="https://github.com/rezazad68/transdeeplab" rel="external noopener nofollow">this https URL</a>      
### 21.Iterative shaping of optical potentials for one-dimensional Bose-Einstein condensates  [ :arrow_down: ](https://arxiv.org/pdf/2208.00706.pdf)
>  The ability to manipulate clouds of ultra-cold atoms is crucial for modern experiments on quantum manybody systems and quantum thermodynamics as well as future metrological applications of Bose-Einstein condensate. While optical manipulation offers almost arbitrary flexibility, the precise control of the resulting dipole potentials and the mitigation of unwanted disturbances is quite involved and only heuristic algorithms with rather slow convergence rates are available up to now. This paper thus suggests the application of iterative learning control (ILC) methods to generate fine-tuned effective potentials in the presence of uncertainties and external disturbances. Therefore, the given problem is reformulated to obtain a one-dimensional tracking problem by using a quasicontinuous input mapping which can be treated by established ILC methods. Finally, the performance of the proposed concept is illustrated in a simulation scenario.      
### 22.UniToBrain dataset: a Brain Perfusion Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2208.00650.pdf)
>  The CT perfusion (CTP) is a medical exam for measuring the passage of a bolus of contrast solution through the brain on a pixel-by-pixel basis. The objective is to draw "perfusion maps" (namely cerebral blood volume, cerebral blood flow and time to peak) very rapidly for ischemic lesions, and to be able to distinguish between core and penumubra regions. A precise and quick diagnosis, in a context of ischemic stroke, can determine the fate of the brain tissues and guide the intervention and treatment in emergency conditions. In this work we present UniToBrain dataset, the very first open-source dataset for CTP. It comprises a cohort of more than a hundred of patients, and it is accompanied by patients metadata and ground truth maps obtained with state-of-the-art algorithms. We also propose a novel neural networks-based algorithm, using the European library ECVL and EDDL for the image processing and developing deep learning models respectively. The results obtained by the neural network models match the ground truth and open the road towards potential sub-sampling of the required number of CT maps, which impose heavy radiation doses to the patients.      
### 23.Lung nodules segmentation from CT with DeepHealth toolkit  [ :arrow_down: ](https://arxiv.org/pdf/2208.00641.pdf)
>  The accurate and consistent border segmentation plays an important role in the tumor volume estimation and its treatment in the field of Medical Image Segmentation. Globally, Lung cancer is one of the leading causes of death and the early detection of lung nodules is essential for the early cancer diagnosis and survival rate of patients. The goal of this study was to demonstrate the feasibility of Deephealth toolkit including PyECVL and PyEDDL libraries to precisely segment lung nodules. Experiments for lung nodules segmentation has been carried out on UniToChest using PyECVL and PyEDDL, for data pre-processing as well as neural network training. The results depict accurate segmentation of lung nodules across a wide diameter range and better accuracy over a traditional detection approach. The datasets and the code used in this paper are publicly available as a baseline reference.      
### 24.Software Package for Automated Analysis of Lung Ultrasound Videos  [ :arrow_down: ](https://arxiv.org/pdf/2208.00620.pdf)
>  In the recent past with the rapid surge of COVID-19 infections, lung ultrasound has emerged as a fast and powerful diagnostic tool particularly for continuous and periodic monitoring of the lung. There have been many attempts towards severity classification, segmentation and detection of key landmarks in the lung. Leveraging the progress, an automated lung ultrasound video analysis package is presented in this work, which can provide summary of key frames in the video, flagging of the key frames with lung infection and options to automatically detect and segment the lung landmarks. The integrated package is implemented as an open-source web application and available in the link <a class="link-external link-https" href="https://github.com/anitoanto/alus-package" rel="external noopener nofollow">this https URL</a>.      
### 25.Breast Cancer Classification Based on Histopathological Images Using a Deep Learning Capsule Network  [ :arrow_down: ](https://arxiv.org/pdf/2208.00594.pdf)
>  Breast cancer is one of the most serious types of cancer that can occur in women. The automatic diagnosis of breast cancer by analyzing histological images (HIs) is important for patients and their prognosis. The classification of HIs provides clinicians with an accurate understanding of diseases and allows them to treat patients more efficiently. Deep learning (DL) approaches have been successfully employed in a variety of fields, particularly medical imaging, due to their capacity to extract features automatically. This study aims to classify different types of breast cancer using HIs. In this research, we present an enhanced capsule network that extracts multi-scale features using the Res2Net block and four additional convolutional layers. Furthermore, the proposed method has fewer parameters due to using small convolutional kernels and the Res2Net block. As a result, the new method outperforms the old ones since it automatically learns the best possible features. The testing results show that the model outperformed the previous DL methods.      
### 26.A sensitivity-based approach to optimal sensor selection for process networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00584.pdf)
>  Sensor selection is critical for state estimation, control and monitoring of nonlinear processes. However, evaluating the performance of each possible combination of $m$ out of $n$ sensors is impractical unless $m$ and $n$ are small. In this paper, we propose a sensitivity-based approach to determine the minimum number of sensors and their optimal locations for state estimation. The local sensitivity matrix of the measured outputs to initial states is used as a measure of the observability. The minimum number of sensors is determined in a way such that the local sensitivity matrix is full column rank. The subset of sensors that satisfies the full-rank condition and provides the maximum degree of observability is considered as the optimal sensor placement. Successive orthogonalization of the sensitivity matrix is conducted in the proposed approach to significantly reduce the computational complexity in selecting the sensors. To validate the effectiveness of the proposed method, it is applied to two processes including a chemical process consisting of four continuous stirred-tank reactors and a wastewater treatment plant. In both cases, the proposed approach can obtain the optimal sensor subsets.      
### 27.An Enhanced Deep Learning Technique for Prostate Cancer Identification Based on MRI Scans  [ :arrow_down: ](https://arxiv.org/pdf/2208.00583.pdf)
>  Prostate cancer is the most dangerous cancer diagnosed in men worldwide. Prostate diagnosis has been affected by many factors, such as lesion complexity, observer visibility, and variability. Many techniques based on Magnetic Resonance Imaging (MRI) have been used for prostate cancer identification and classification in the last few decades. Developing these techniques is crucial and has a great medical effect because they improve the treatment benefits and the chance of patients' survival. A new technique that depends on MRI has been proposed to improve the diagnosis. This technique consists of two stages. First, the MRI images have been preprocessed to make the medical image more suitable for the detection step. Second, prostate cancer identification has been performed based on a pre-trained deep learning model, InceptionResNetV2, that has many advantages and achieves effective results. In this paper, the InceptionResNetV2 deep learning model used for this purpose has average accuracy equals to 89.20%, and the area under the curve (AUC) equals to 93.6%. The experimental results of this proposed new deep learning technique represent promising and effective results compared to other previous techniques.      
### 28.Distributed Intelligence in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00545.pdf)
>  The cloud-based solutions are becoming inefficient due to considerably large time delays, high power consumption, security and privacy concerns caused by billions of connected wireless devices and typically zillions bytes of data they produce at the network edge. A blend of edge computing and Artificial Intelligence (AI) techniques could optimally shift the resourceful computation servers closer to the network edge, which provides the support for advanced AI applications (e.g., video/audio surveillance and personal recommendation system) by enabling intelligent decision making on computing at the point of data generation as and when it is needed, and distributed Machine Learning (ML) with its potential to avoid the transmission of large dataset and possible compromise of privacy that may exist in cloud-based centralized learning. Therefore, AI is envisioned to become native and ubiquitous in future communication and networking systems. In this paper, we conduct a comprehensive overview of recent advances in distributed intelligence in wireless networks under the umbrella of native-AI wireless networks, with a focus on the basic concepts of native-AI wireless networks, on the AI-enabled edge computing, on the design of distributed learning architectures for heterogeneous networks, on the communication-efficient technologies to support distributed learning, and on the AI-empowered end-to-end communications. We highlight the advantages of hybrid distributed learning architectures compared to the state-of-art distributed learning techniques. We summarize the challenges of existing research contributions in distributed intelligence in wireless networks and identify the potential future opportunities.      
### 29.DeScoD-ECG: Deep Score-Based Diffusion Model for ECG Baseline Wander and Noise Removal  [ :arrow_down: ](https://arxiv.org/pdf/2208.00542.pdf)
>  Objective: Electrocardiogram (ECG) signals commonly suffer noise interference, such as baseline wander. High-quality and high-fidelity reconstruction of the ECG signals is of great significance to diagnosing cardiovascular diseases. Therefore, this paper proposes a novel ECG baseline wander and noise removal technology. Methods: We extended the diffusion model in a conditional manner that was specific to the ECG signals, namely the Deep Score-Based Diffusion model for Electrocardiogram baseline wander and noise removal (DeScoD-ECG). Moreover, we deployed a multi-shots averaging strategy that improved signal reconstructions. We conducted the experiments on the QT Database and the MIT-BIH Noise Stress Test Database to verify the feasibility of the proposed method. Baseline methods are adopted for comparison, including traditional digital filter-based and deep learning-based methods. Results: The quantities evaluation results show that the proposed method obtained outstanding performance on four distance-based similarity metrics (the sum of squared distance, maximum absolute square, percentage of root distance, and cosine similarity) with 3.771 $\pm$ 5.713 au, 0.329 $\pm$ 0.258 au, 40.527 $\pm$ 26.258 \%, and 0.926 $\pm$ 0.087. This led to at least 20\% overall improvement compared with the best baseline method. Conclusion: This paper demonstrates the state-of-the-art performance of the DeScoD-ECG for ECG noise removal, which has better approximations of the true data distribution and higher stability under extreme noise corruptions. Significance: This study is one of the first to extend the conditional diffusion-based generative model for ECG noise removal, and the DeScoD-ECG has the potential to be widely used in biomedical applications.      
### 30.Feather-Light Fourier Domain Adaptation in Magnetic Resonance Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.00474.pdf)
>  Generalizability of deep learning models may be severely affected by the difference in the distributions of the train (source domain) and the test (target domain) sets, e.g., when the sets are produced by different hardware. As a consequence of this domain shift, a certain model might perform well on data from one clinic, and then fail when deployed in another. We propose a very light and transparent approach to perform test-time domain adaptation. The idea is to substitute the target low-frequency Fourier space components that are deemed to reflect the style of an image. To maximize the performance, we implement the "optimal style donor" selection technique, and use a number of source data points for altering a single target scan appearance (Multi-Source Transferring). We study the effect of severity of domain shift on the performance of the method, and show that our training-free approach reaches the state-of-the-art level of complicated deep domain adaptation models. The code for our experiments is released.      
### 31.Learning while Acquisition: Towards Active Learning Framework for Beamforming in Ultrasound Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.00464.pdf)
>  In the recent past, there have been many efforts to accelerate adaptive beamforming for ultrasound (US) imaging using neural networks (NNs). However, most of these efforts are based on static models, i.e., they are trained to learn a single adaptive beamforming approach (e.g., minimum variance distortionless response (MVDR)) assuming that they result in the best image quality. Moreover, the training of such NNs is initiated only after acquiring a large set of data that consumes several gigabytes (GBs) of storage space. In this study, an active learning framework for beamforming is described for the first time in the context of NNs. The best quality image chosen by the user serves as the ground truth for the proposed technique, which trains the NN concurrently with data acqusition. On average, the active learning approach takes 0.5 seconds to complete a single iteration of training.      
### 32.Photon-Limited Blind Deconvolution using Unsupervised Iterative Kernel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2208.00451.pdf)
>  Blind deconvolution in low-light is one of the more challenging problems in image restoration because of the photon shot noise. However, existing algorithms -- both classical and deep-learning based -- are not designed for this condition. When the shot noise is strong, conventional deconvolution methods fail because (1) the presence of noise makes the estimation of the blur kernel difficult; (2) generic deep-restoration models rarely model the forward process explicitly; (3) there are currently no iterative strategies to incorporate a non-blind solver in a kernel estimation stage. This paper addresses these challenges by presenting an unsupervised blind deconvolution method. At the core of this method is a reformulation of the general blind deconvolution framework from the conventional image-kernel alternating minimization to a purely kernel-based minimization. This kernel-based minimization leads to a new iterative scheme that backpropagates an unsupervised loss through a pre-trained non-blind solver to update the blur kernel. Experimental results show that the proposed framework achieves superior results than state-of-the-art blind deconvolution algorithms in low-light conditions.      
### 33.Unitary Approximate Message Passing for Matrix Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2208.00422.pdf)
>  We consider matrix factorization (MF) with certain constraints, which finds wide applications in various areas. Leveraging variational inference (VI) and unitary approximate message passing (UAMP), we develop a Bayesian approach to MF with an efficient message passing implementation, called UAMPMF. With proper priors imposed on the factor matrices, UAMPMF can be used to solve many problems that can be formulated as MF, such as non negative matrix factorization, dictionary learning, compressive sensing with matrix uncertainty, robust principal component analysis, and sparse matrix factorization. Extensive numerical examples are provided to show that UAMPMF significantly outperforms state-of-the-art algorithms in terms of recovery accuracy, robustness and computational complexity.      
### 34.Speckle2Speckle: Unsupervised Learning of Ultrasound Speckle Filtering Without Clean Data  [ :arrow_down: ](https://arxiv.org/pdf/2208.00402.pdf)
>  In ultrasound imaging the appearance of homogeneous regions of tissue is subject to speckle, which for certain applications can make the detection of tissue irregularities difficult. To cope with this, it is common practice to apply speckle reduction filters to the images. Most conventional filtering techniques are fairly hand-crafted and often need to be finely tuned to the present hardware, imaging scheme and application. Learning based techniques on the other hand suffer from the need for a target image for training (in case of fully supervised techniques) or require narrow, complex physics-based models of the speckle appearance that might not apply in all cases. With this work we propose a deep-learning based method for speckle removal without these limitations. To enable this, we make use of realistic ultrasound simulation techniques that allow for instantiation of several independent speckle realizations that represent the exact same tissue, thus allowing for the application of image reconstruction techniques that work with pairs of differently corrupted data. Compared to two other state-of-the-art approaches (non-local means and the Optimized Bayesian non-local means filter) our method performs favorably in qualitative comparisons and quantitative evaluation, despite being trained on simulations alone, and is several orders of magnitude faster.      
### 35.NFT Hydroponic Control Using Mamdani Fuzzy Inference System  [ :arrow_down: ](https://arxiv.org/pdf/2208.00364.pdf)
>  The Nutrient Film Technique (NFT) method is one of the most popular hydroponic cultivation methods. This method has advantages such as easier maintenance, faster and optimal plant growth, better use of fertilizers, and less deposition. The disadvantages of NFT include the consumption of electrical power and the faster spread of disease. Therefore, NFT requires a good nutrient control and monitoring system to save electricity and achieve optimal growth and resistance to pests and diseases. In this study, a nutrient control was designed with indicators of pH and TDS levels and equipped with an Internet of Things (IoT) based monitoring system. The control system used is the Mamdani Fuzzy Inference System. The output of the system is the active time of the pH Up, pH Down, and AB Mix nutrient pumps, which aim to normalize the pH and TDS of nutrient liquids. The experimental results show that one to three control steps are needed to normalize pH. One control step has a response time of 60 seconds, and it can prevent pH Up and pH Down oscillations. As for TDS control, the prediction of AB mix pump active time works accurately, and TDS levels can be normalized in one control step. Overall, based on surface control, simulations, and real experimental data, it is indicated that the control system operates very well and can normalize pH and TDS to the desired normal standard.      
### 36.Decentralized Intermittent Feedback Adaptive Control of Non-triangular Nonlinear Time-varying Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.00356.pdf)
>  This paper investigates the decentralized stabilization problem for a class of interconnected systems in the presence of non-triangular structural uncertainties and time-varying parameters, where each subsystem exchanges information only with its neighbors and only intermittent (rather than continuous) states and input are to be utilized. Thus far to our best knowledge, no solution exists priori to this work, despite its high prevalence in practice. Two globally decentralized adaptive control schemes are presented based on the backstepping technique, the first one is developed in a continuous fashion by combining the philosophy of the modified congelation of variables based approach with the special treatment of non-triangular structural uncertainties, which avoids the derivative of time-varying parameters and eliminates the limitation of the triangular condition, thus largely broadens the scope of application. By making use of the important property that the partial derivatives of the constructed virtual controllers in each subsystem are all constant, the second scheme is developed through directly replacing the states in the preceding scheme with the triggered ones. Consequently, the non-differentiability of the virtual control stemming from intermittent state feedback is completely obviated. The internal signals under both schemes are rigorously shown to be globally uniformly bounded with the aid of several novel lemmas, while the stabilization performance can be enhanced by appropriately adjusting design parameters. Moreover, the inter-event intervals are ensured to be lower-bounded by a positive constant. Finally, numerical simulation verifies the benefits and efficiency of the proposed method.      
### 37.A Multi-View Learning Approach to Enhance Automatic 12-Lead ECG Diagnosis Performance  [ :arrow_down: ](https://arxiv.org/pdf/2208.00323.pdf)
>  The performances of commonly used electrocardiogram (ECG) diagnosis models have recently improved with the introduction of deep learning (DL). However, the impact of various combinations of multiple DL components and/or the role of data augmentation techniques on the diagnosis have not been sufficiently investigated. This study proposes an ensemble-based multi-view learning approach with an ECG augmentation technique to achieve a higher performance than traditional automatic 12-lead ECG diagnosis methods. The data analysis results show that the proposed model reports an F1 score of 0.840, which outperforms existing state-ofthe-art methods in the literature.      
### 38.On-CMOS High-Throughput Multi-Modal Amperometric DNA Analysis with Distributed Thermal Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2208.00248.pdf)
>  Accurate temperature regulation is critical for amperometric DNA analysis to achieve high fidelity, reliability, and throughput. In this work, a 9x6 cell array of mixed-signal CMOS distributed temperature regulators for on-CMOS multi-modal amperometric DNA analysis is presented. Three DNA analysis methods are supported, including constant potential amperometry (CPA), cyclic voltammetry (CV), and impedance spectroscopy (IS). In-cell heating and temperature sensing elements are implemented in standard CMOS technology without post-processing. Using proportional-integral-derivative (PID) control, the local temperature can be regulated to within +/-0.5C of any desired value between 20C and 90C. The two computationally intensive operations in the PID algorithm, multiplication, and subtraction, are performed by an in-cell dual-slope multiplying ADC in the mixed-signal domain, resulting in a small area and low power consumption. Over 95% of the circuit blocks are synergistically shared among the four operating modes, including CPA, CV, IS, and the proposed temperature regulation mode. A 3mmx3mm CMOS prototype fabricated in a 0.13um CMOS technology has been fully experimentally characterized. Each channel occupies an area of 0.06mm2 and consumes 42uW from a 1.2V supply. The proposed distributed temperature regulation design and the mixed-signal PID implementation can be applied to a wide range of sensory and other applications.      
### 39.Novel Maximum Likelihood Estimation of Clock Skew in One-Way Broadcast Time Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2208.00222.pdf)
>  Clock skew compensation is essential for accurate time synchronization in wireless networks. However, contemporary clock skew estimation is based on inaccurate transmission time measurement, which makes credible estimation challenging. Based on one-way broadcast synchronization, this study presents a novel maximum likelihood estimation (MLE) with an innovative implementation to minimize the clock skew estimation error caused by delay. A multiple one-way broadcast model is developed for observation and set collection, while the distribution of actual delay is discussed. A clock skew MLE based on Gaussian delay is then proposed and its Cramer-Rao lower bound provided. The developed MLE is independent of the clock offset estimation, and requires only two synchronization periods to generate an accurate estimation. Extensive experimental results indicate that the performance of the proposed MLE is better than the methods popularly used in existing time synchronization protocols.      
### 40.Fast Convergence Time Synchronization in Wireless Sensor Networks Based on Average Consensus  [ :arrow_down: ](https://arxiv.org/pdf/2208.00216.pdf)
>  Average consensus theory is intensely popular for building time synchronization in wireless sensor network (WSN). However, the average consensus-based time synchronization algorithm is based on iteration that pose challenges for efficiency, as they entail high communication cost and long convergence time in large-scale WSN. Based on the suggestion that the greater the algebraic connectivity leads to the faster the convergence, a novel multi-hop average consensus time synchronization (MACTS) is developed with innovative implementation in this paper. By employing multi-hop communication model, it shows that virtual communication links among multi-hop nodes are generated and algebraic connectivity of network increases. Meanwhile, a multihop controller is developed to balance the convergence time, accuracy and communication complexity. Moreover, the accurate relative clock offset estimation is yielded by delay compensation. Implementing the MACTS based on the popular one-way broadcast model and taking multi-hop over short distances, we achieve hundreds of times the MACTS convergence rate compared to ATS.      
### 41.Rapid-Flooding Time Synchronization for Large-Scale Wireless Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00213.pdf)
>  Accurate and fast-convergent time synchronization is very important for wireless sensor networks. The flooding time synchronization converges fast, but its transmission delay and by-hop error accumulation seriously reduce the synchronization accuracy. In this article, a rapidflooding multiple one-way broadcast time-synchronization (RMTS) protocol for large-scale wireless sensor networks is proposed. To minimize the by-hop error accumulation, the RMTS uses maximum likelihood estimations for clock skew estimation and clock offset estimation, and quickly shares the estimations among the networks. As a result, the synchronization error resulting from delays is greatly reduced, while faster convergence and higher-accuracy synchronization is achieved. Extensive experimental results demonstrate that, even over 24-hops networks, the RMTS is able to build accurate synchronization at the third synchronization period, and moreover, the by-hop error accumulation is slower when the network diameter increases.      
### 42.LRIP-Net: Low-Resolution Image Prior based Network for Limited-Angle CT Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2208.00207.pdf)
>  In the practical applications of computed tomography imaging, the projection data may be acquired within a limited-angle range and corrupted by noises due to the limitation of scanning conditions. The noisy incomplete projection data results in the ill-posedness of the inverse problems. In this work, we theoretically verify that the low-resolution reconstruction problem has better numerical stability than the high-resolution problem. In what follows, a novel low-resolution image prior based CT reconstruction model is proposed to make use of the low-resolution image to improve the reconstruction quality. More specifically, we build up a low-resolution reconstruction problem on the down-sampled projection data, and use the reconstructed low-resolution image as prior knowledge for the original limited-angle CT problem. We solve the constrained minimization problem by the alternating direction method with all subproblems approximated by the convolutional neural networks. Numerical experiments demonstrate that our double-resolution network outperforms both the variational method and popular learning-based reconstruction methods on noisy limited-angle reconstruction problems.      
### 43.Intelligent Reflecting Surface-Aided Maneuvering Target Sensing: True Velocity Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2208.00198.pdf)
>  Maneuvering target sensing will be an important service of future vehicular networks, where precise velocity estimation is one of the core tasks. To this end, the recently proposed integrated sensing and communications (ISAC) provides a promising platform for achieving accurate velocity estimation. However, with one mono-static ISAC base station (BS), only the radial projection of the true velocity can be estimated, which causes serious estimation error. In this paper, we investigate the estimation of the true velocity of a maneuvering target with the assistance of intelligent reflecting surfaces (IRSs). In particular, we propose an efficient velocity estimation algorithm exploiting the two perspectives from the BS and IRS to the target. In particular, we propose a two-stage scheme where the true velocity can be recovered based on the Doppler frequency of the BS-target link and BS-IRS-target link. Experimental results validate that the true velocity can be precisely recovered with the help of IRSs and demonstrate the advantage of adding the IRS.      
### 44.Temporal extrapolation of heart wall segmentation in cardiac magnetic resonance images via pixel tracking  [ :arrow_down: ](https://arxiv.org/pdf/2208.00165.pdf)
>  In this study, we have tailored a pixel tracking method for temporal extrapolation of the ventricular segmentation masks in cardiac magnetic resonance images. The pixel tracking process starts from the end-diastolic frame of the heart cycle using the available manually segmented images to predict the end-systolic segmentation mask. The superpixels approach is used to divide the raw images into smaller cells and in each time frame, new labels are assigned to the image cells which leads to tracking the movement of the heart wall elements through different frames. The tracked masks at the end of systole are compared with the already available manually segmented masks and dice scores are found to be between 0.81 to 0.84. Considering the fact that the proposed method does not necessarily require a training dataset, it could be an attractive alternative approach to deep learning segmentation methods in scenarios where training data are limited.      
### 45.Resolution enhancement of placenta histological images using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.00163.pdf)
>  In this study, a method has been developed to improve the resolution of histological human placenta images. For this purpose, a paired series of high- and low-resolution images have been collected to train a deep neural network model that can predict image residuals required to improve the resolution of the input images. A modified version of the U-net neural network model has been tailored to find the relationship between the low resolution and residual images. After training for 900 epochs on an augmented dataset of 1000 images, the relative mean squared error of 0.003 is achieved for the prediction of 320 test images. The proposed method has not only improved the contrast of the low-resolution images at the edges of cells but added critical details and textures that mimic high-resolution images of placenta villous space.      
### 46.Reducing Attack Opportunities Through Decentralized Event-Triggered Control  [ :arrow_down: ](https://arxiv.org/pdf/2208.00146.pdf)
>  Decentralized control systems are widely used in a number of situations and applications. In order for these systems to function properly and achieve their desired goals, information must be propagated between agents, which requires connecting to a network. To reduce opportunities for attacks that may be carried out through the network, we design an event-triggered mechanism for network connection and communication that minimizes the amount of time agents must be connected to the network, in turn decreasing communication costs. This mechanism is a function of only local information and ensures stability for the overall system in attack-free scenarios. Our approach distinguishes itself from current decentralized event-triggered control strategies by considering scenarios where agents are not always connected to the network to receive critical information from other agents and by considering scenarios where the communication graph is undirected and connected. An algorithm describing this network connection and communication protocol is provided, and our approach is illustrated via simulation.      
### 47.Distributed Scheduling in Non-Signalized Intersections with Mixed Cooperative and Non-Cooperative Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2208.00141.pdf)
>  Intersection management with mixed cooperative and non-cooperative vehicles is crucial in next-generation transportation systems. This letter addresses the intersection scheduling problem in mixed systems by exploiting the benefit of cooperation based on the minimax scheduling framework, which was originally proposed for fully non-cooperative systems. Specifically, a long-horizon self-organization policy is first developed to optimize the passing order of cooperative vehicles in a distributed manner, which is proved convergent in a long enough system. Then a short-horizon trajectory planning policy is proposed to improve the efficiency when both cooperative and non-cooperative vehicles exist, and its safety and efficiency are theoretically validated. Furthermore, numerical simulations verify that the proposed policies can effectively reduce the scheduling cost and improve the throughput for cooperative vehicles.      
### 48.Coordinated control in multi-terminal VSC-HVDC systems to improve transient stability: Impact on electromechanical-oscillation damping  [ :arrow_down: ](https://arxiv.org/pdf/2208.00083.pdf)
>  Multi-terminal high-voltage Direct Current technology based on Voltage-Source Converter stations (VSC-MTDC) is expected to be one of the most important contributors to the future of electric power systems. In fact, among other features, it has already been shown how this technology can contribute to improve transient stability in power systems by the use of supplementary controllers. Along this line, this paper will investigate in detail how these supplementary controllers may affect electromechanical oscillations, by means of small-signal stability analysis. The paper analyses two control strategies based on the modulation of active-power injections (P-WAF) and reactive-power injections (Q-WAF) in the VSC stations. Both control strategies use global signals of the frequencies of the VSC-MTDC system and they presented significant improvements on transient stability. The paper will provide guidelines for the design of these type of controllers to improve both, large- and small-disturbance angle stability. Small-signal stability techniques (in Matlab) will be used to assess electromechanical-oscillation damping, while non-linear time domain simulation (in PSS/E) will be used to confirm the results. Results will be illustrated in Nordic32A test system with an embedded VSC-MTDC system. The paper analyses the impact of the controller gains and communication latency on electromechanical-oscillation damping. The main conclusion of the paper is that transient-stability-tailored supplementary controllers in VSC-MTDC systems can be tuned to damp inter-area oscillations too, maintaining their effectiveness for transient-stability improvement.      
### 49.Coordinating Flexible Ramping Products with Dynamics of the Natural Gas Network  [ :arrow_down: ](https://arxiv.org/pdf/2208.00036.pdf)
>  In electricity networks with high penetration levels of renewable resources, Flexible Ramping Products (FRPs) are among the utilized measures for dealing with the potential fluctuations in the net demand. This paper investigates the impacts of FRPs on the operation of interdependent electricity and natural gas networks. To accurately model and reflect the effects of variations in the natural gas fuel demand on the natural gas network, a dynamic Optimal Gas Flow (OGF) formulation is utilized. The non-convex dynamic model of the natural gas system is represented in a convex form via a tight relaxation scheme. An improved distributed optimization method is proposed to solve the coordinated operation problem in a privacy-preserving manner, where the two infrastructures only share limited information. We introduce the Inexact Varying Alternating Direction Method of Multipliers (IV-ADMM) and show that compared with the classic ADMM, it converges considerably faster and in fewer iterations. Through a comparison of day-ahead and real-time operation planning results, it is concluded that without accounting for natural gas network dynamics, the FRP model is not a trustworthy tool in day-ahead planning against uncertainties.      
### 50.MulViMotion: Shape-aware 3D Myocardial Motion Tracking from Multi-View Cardiac MRI  [ :arrow_down: ](https://arxiv.org/pdf/2208.00034.pdf)
>  Recovering the 3D motion of the heart from cine cardiac magnetic resonance (CMR) imaging enables the assessment of regional myocardial function and is important for understanding and analyzing cardiovascular disease. However, 3D cardiac motion estimation is challenging because the acquired cine CMR images are usually 2D slices which limit the accurate estimation of through-plane motion. To address this problem, we propose a novel multi-view motion estimation network (MulViMotion), which integrates 2D cine CMR images acquired in short-axis and long-axis planes to learn a consistent 3D motion field of the heart. In the proposed method, a hybrid 2D/3D network is built to generate dense 3D motion fields by learning fused representations from multi-view images. To ensure that the motion estimation is consistent in 3D, a shape regularization module is introduced during training, where shape information from multi-view images is exploited to provide weak supervision to 3D motion estimation. We extensively evaluate the proposed method on 2D cine CMR images from 580 subjects of the UK Biobank study for 3D motion tracking of the left ventricular myocardium. Experimental results show that the proposed method quantitatively and qualitatively outperforms competing methods.      
### 51.A review of Deep learning Techniques for COVID-19 identification on Chest CT images  [ :arrow_down: ](https://arxiv.org/pdf/2208.00032.pdf)
>  The current COVID-19 pandemic is a serious threat to humanity that directly affects the lungs. Automatic identification of COVID-19 is a challenge for health care officials. The standard gold method for diagnosing COVID-19 is Reverse Transcription Polymerase Chain Reaction (RT-PCR) to collect swabs from affected people. Some limitations encountered while collecting swabs are related to accuracy and longtime duration. Chest CT (Computed Tomography) is another test method that helps healthcare providers quickly identify the infected lung areas. It was used as a supporting tool for identifying COVID-19 in an earlier stage. With the help of deep learning, the CT imaging characteristics of COVID-19. Researchers have proven it to be highly effective for COVID-19 CT image classification. In this study, we review the recent deep learning techniques that can use to detect the COVID-19 disease. Relevant studies were collected by various databases such as Web of Science, Google Scholar, and PubMed. Finally, we compare the results of different deep learning models, and CT image analysis is discussed.      
### 52.Multi-center Assessment of CNN-Transformer with Belief Matching Loss for Patient-independent Seizure Detection in Scalp and Intracranial EEG  [ :arrow_down: ](https://arxiv.org/pdf/2208.00025.pdf)
>  Neurologists typically identify epileptic seizures from electroencephalograms (EEGs) by visual inspection. This process is often time-consuming. To expedite the process, a reliable, automated, and patient-independent seizure detector is essential. However, developing such seizure detector is challenging as seizures exhibit diverse morphologies across different patients. In this study, we propose a patient-independent seizure detector to automatically detect seizures in both scalp EEG (sEEG) and intracranial EEG (iEEG). First, we deploy a convolutional neural network (CNN) with transformers (TRF) and belief matching (BM) loss to detect seizures in single-channel EEG segments. Next, we extract regional features from the channel-level outputs to detect seizures in multi-channel EEG segments. At last, we apply postprocessing filters to the segment-level outputs to determine the start and end points of seizures in multi-channel EEGs. We introduce the minimum overlap evaluation scoring (MOES) as an evaluation metric that accounts for minimum overlap between the detection and seizure, improving upon existing metrics. We trained the seizure detector on the Temple University Hospital Seizure sEEG dataset and evaluated it on five other independent sEEG and iEEG datasets. On the TUH-SZ dataset, the proposed patient-independent seizure detector achieves a sensitivity (SEN), precision (PRE), average and median false positive rate per hour (aFPR/h and mFPR/h), and median offset of 0.772, 0.429, 4.425, 0, and -2.125s, respectively. Across all six datasets, we obtained SEN of 0.227-1.00, PRE of 0.377-1.00, aFPR/h of 0.253-2.037 and mFPR/h of 0-0.559. The proposed seizure detector can reliably detect seizures in EEGs and takes less than 15s for a 30 minutes EEG. Hence, this system could aid the clinicians in reliably identifying seizures expeditiously, allocating more time for devising proper treatment.      
### 53.A differentiable forward model for the concurrent, multi-peak Bragg coherent x-ray diffraction imaging problem  [ :arrow_down: ](https://arxiv.org/pdf/2208.00970.pdf)
>  We present a general analytic approach to spatially resolve the nano-scale lattice distortion field of strained and defected compact crystals with Bragg coherent x-ray diffraction imaging (BCDI). Our approach relies on fitting a differentiable forward model simultaneously to multiple BCDI datasets corresponding to independent Bragg reflections from the same single crystal. It is designed to be faithful to heterogeneities that potentially manifest as phase discontinuities in the coherently diffracted wave, such as lattice dislocations in an imperfect crystal. We retain fidelity to such small features in the reconstruction process through a Fourier transform -based resampling algorithm designed to largely avoid the point spread tendencies of commonly employed interpolation methods. The reconstruction model defined in this manner brings BCDI reconstruction into the scope of explicit optimization driven by automatic differentiation. With results from simulations and experimental diffraction data, we demonstrate significant improvement in the final image quality compared to conventional phase retrieval, enabled by explicitly coupling multiple BCDI datasets into the reconstruction loss function.      
### 54.AdaWCT: Adaptive Whitening and Coloring Style Injection  [ :arrow_down: ](https://arxiv.org/pdf/2208.00921.pdf)
>  Adaptive instance normalization (AdaIN) has become the standard method for style injection: by re-normalizing features through scale-and-shift operations, it has found widespread use in style transfer, image generation, and image-to-image translation. In this work, we present a generalization of AdaIN which relies on the whitening and coloring transformation (WCT) which we dub AdaWCT, that we apply for style injection in large GANs. We show, through experiments on the StarGANv2 architecture, that this generalization, albeit conceptually simple, results in significant improvements in the quality of the generated images.      
### 55.A quantitative and constructive proof of Willems' Fundamental Lemma and its implications  [ :arrow_down: ](https://arxiv.org/pdf/2208.00905.pdf)
>  Willems' Fundamental Lemma provides a powerful data-driven parametrization of all trajectories of a controllable linear time-invariant system based on one trajectory with persistently exciting (PE) input. In this paper, we present a novel proof of this result which is inspired by the classical adaptive control literature and, in contrast to the existing proofs, is constructive. Our proof also provides a quantitative and directional PE notion, as opposed to binary rank-based PE conditions. Moreover, the proof shows that the PE requirements of the Fundamental Lemma are not only sufficient but also necessary when considering arbitrary controllable systems. As a contribution of independent interest, we generalize existing PE results from the adaptive control literature and reveal a crucial role of the system's zeros.      
### 56.Continuous locomotion mode recognition and gait phase estimation based on a shank-mounted IMU with artificial neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00861.pdf)
>  To improve the control of wearable robotics for gait assistance, we present an approach for continuous locomotion mode recognition as well as gait phase and stair slope estimation based on artificial neural networks that include time history information. The input features consist exclusively of processed variables that can be measured with a single shank-mounted inertial measurement unit. We introduce a wearable device to acquire real-world environment test data to demonstrate the performance and the robustness of the approach. Mean absolute error (gait phase, stair slope) and accuracy (locomotion mode) were determined for steady level walking and steady stair ambulation. Robustness was assessed using test data from different sensor hardware, sensor fixations, ambulation environments and subjects. The mean absolute error from the steady gait test data for the gait phase was 2.0-3.5 % for gait phase estimation and 3.3-3.8 for stair slope estimation. The accuracy of classifying the correct locomotion mode on the test data with the utilization of time history information was in between 98.51 % and 99.67 %. Results show high performance and robustness for continuously predicting gait phase, stair slope and locomotion mode during steady gait. As hypothesized, time history information improves the locomotion mode recognition. However, while the gait phase estimation performed well for untrained transitions between locomotion modes, our qualitative analysis revealed that it may be beneficial to include transition data into the training of the neural network to improve the prediction of the slope and the locomotion mode. Our results suggest that artificial neural networks could be used for high level control of wearable lower limb robotics.      
### 57.Guidance on the Safety Assurance of Autonomous Systems in Complex Environments (SACE)  [ :arrow_down: ](https://arxiv.org/pdf/2208.00853.pdf)
>  Autonomous systems (AS) are systems that have the capability to take decisions free from direct human control. AS are increasingly being considered for adoption for applications where their behaviour may cause harm, such as when used for autonomous driving, medical applications or in domestic environments. For such applications, being able to ensure and demonstrate (assure) the safety of the operation of the AS is crucial for their adoption. This can be particularly challenging where AS operate in complex and changing real-world environments. Establishing justified confidence in the safety of AS requires the creation of a compelling safety case. This document introduces a methodology for the Safety Assurance of Autonomous Systems in Complex Environments (SACE). SACE comprises a set of safety case patterns and a process for (1) systematically integrating safety assurance into the development of the AS and (2) for generating the evidence base for explicitly justifying the acceptable safety of the AS.      
### 58.A Transformer-based Neural Language Model that Synthesizes Brain Activation Maps from Free-Form Text Queries  [ :arrow_down: ](https://arxiv.org/pdf/2208.00840.pdf)
>  Neuroimaging studies are often limited by the number of subjects and cognitive processes that can be feasibly interrogated. However, a rapidly growing number of neuroscientific studies have collectively accumulated an extensive wealth of results. Digesting this growing literature and obtaining novel insights remains to be a major challenge, since existing meta-analytic tools are constrained to keyword queries. In this paper, we present Text2Brain, an easy to use tool for synthesizing brain activation maps from open-ended text queries. Text2Brain was built on a transformer-based neural network language model and a coordinate-based meta-analysis of neuroimaging studies. Text2Brain combines a transformer-based text encoder and a 3D image generator, and was trained on variable-length text snippets and their corresponding activation maps sampled from 13,000 published studies. In our experiments, we demonstrate that Text2Brain can synthesize meaningful neural activation patterns from various free-form textual descriptions. Text2Brain is available at <a class="link-external link-https" href="https://braininterpreter.com" rel="external noopener nofollow">this https URL</a> as a web-based tool for efficiently searching through the vast neuroimaging literature and generating new hypotheses.      
### 59.A Whole-Body Controller Based on a Simplified Template for Rendering Impedances in Quadruped Manipulators  [ :arrow_down: ](https://arxiv.org/pdf/2208.00810.pdf)
>  Quadrupedal manipulators require to be compliant when dealing with external forces during autonomous manipulation, tele-operation or physical human-robot interaction. This paper presents a whole-body controller that allows for the implementation of a Cartesian impedance control to coordinate tracking performance and desired compliance for the robot base and manipulator arm. The controller is formulated through an optimization problem using Quadratic Programming (QP) to impose a desired behavior for the system while satisfying friction cone constraints, unilateral force constraints, joint and torque limits. The presented strategy decouples the arm and the base of the platform, enforcing the behavior of a linear double-mass spring damper system, and allows to independently tune their inertia, stiffness and damping properties. The control architecture is validated through an extensive simulation study using the 90kg HyQ robot equipped with a 7-DoF manipulator arm. Simulation results show the impedance rendering performance when external forces are applied at the arm's end-effector. The paper presents results for full stance condition (all legs on the ground) and, for the first time, also shows how the impedance rendering is affected by the contact conditions during a dynamic gait.      
### 60.Jazz Contrafact Detection  [ :arrow_down: ](https://arxiv.org/pdf/2208.00792.pdf)
>  In jazz, a contrafact is a new melody composed over an existing, but often reharmonized chord progression. Because reharmonization can introduce a wide range of variations, detecting contrafacts is a challenging task. This paper develops a novel vector-space model to represent chord progressions, and uses it for contrafact detection. The process applies principles from music theory to reduce the dimensionality of chord space, determine a common key signature representation, and compute a chordal co-occurrence matrix. The rows of the matrix form a basis for the vector space in which chord progressions are represented as piecewise linear functions, and harmonic similarity is evaluated by computing the membrane area, a novel distance metric. To illustrate our method's effectiveness, we apply it to the Impro-Visor corpus of 2,612 chord progressions, and present examples demonstrating its ability to account for reharmonizations and find contrafacts.      
### 61.Real Time Object Detection System with YOLO and CNN Models: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2208.00773.pdf)
>  The field of artificial intelligence is built on object detection techniques. YOU ONLY LOOK ONCE (YOLO) algorithm and it's more evolved versions are briefly described in this research survey. This survey is all about YOLO and convolution neural networks (CNN)in the direction of real time object detection.YOLO does generalized object representation more effectively without precision losses than other object detection models.CNN architecture models have the ability to eliminate highlights and identify objects in any given image. When implemented appropriately, CNN models can address issues like deformity diagnosis, creating educational or instructive application, etc. This article reached atnumber of observations and perspective findings through the analysis.Also it provides support for the focused visual information and feature extraction in the financial and other industries, highlights the method of target detection and feature selection, and briefly describe the development process of YOLO algorithm.      
### 62.Learning to Navigate using Visual Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00759.pdf)
>  We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. While prior work in sensor network based navigation uses explicit mapping and planning techniques, and are often aided by external positioning systems, we propose a vision-only based learning approach that leverages a Graph Neural Network (GNN) to encode and communicate relevant viewpoint information to the mobile robot. During navigation, the robot is guided by a model that we train through imitation learning to approximate optimal motion primitives, thereby predicting the effective cost-to-go (to the target). In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Simulation results show that by utilizing communication among the sensors and robot, we can achieve a $18.1\%$ improvement in success rate while decreasing path detour mean by $29.3\%$ and variability by $48.4\%$. This is done without requiring a global map, positioning data, nor pre-calibration of the sensor network. Second, we perform a zero-shot transfer of our model from simulation to the real world. To this end, we train a `translator' model that translates between {latent encodings of} real and simulated images so that the navigation policy (which is trained entirely in simulation) can be used directly on the real robot, without additional fine-tuning. Physical experiments demonstrate our effectiveness in various cluttered environments.      
### 63.Joint Active and Passive Beamforming for RIS-aided MIMO Communications with Low-Resolution Phase Shifts  [ :arrow_down: ](https://arxiv.org/pdf/2208.00717.pdf)
>  In recent years, reconfigurable intelligent surface (RIS) has emerged as an appealing technology due to its potential capability to enhance the performance of wireless networks with a low-cost and low energy-consumption. Most works often assume continuous phase-shits at the RIS elements for the transmit and passive beamforming optimization. However, practical hardware limitations often impose a reduced number of available phase shifts at the RIS elements which can lead to substantial performance loss. Therefore, to harvest the gains of RIS-assisted multi-stream multiple-input multiple-output (MIMO) communications under the constraint of discrete-valued phases shifts, this correspondence proposes an iterative algorithm that can efficiently tackle the mixed integer non-linear optimization problem associated to the maximization of the achievable rate over the transmit precoder and RIS elements. Simulation results demonstrate that the proposed design can be very effective, especially when using low-resolution phase-shifts.      
### 64.Hybrid Precoding for Mixture Use of Phase Shifters and Switches in mmWave Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2208.00714.pdf)
>  A variable-phase-shifter (VPS) architecture with hybrid precoding for mixture use of phase shifters and switches, is proposed for millimeter wave massive multiple-input multiple-output communications. For the VPS architecture, a hybrid precoding design (HPD) scheme, called VPS-HPD, is proposed to optimize the phases according to the channel state information by alternately optimizing the analog precoder and digital precoder. To reduce the computational complexity of the VPS-HPD scheme, a low-complexity HPD scheme for the VPS architecture (VPS-LC-HPD) including alternating optimization in three stages is then proposed, where each stage has a closed-form solution and can be efficiently implemented. To reduce the hardware complexity introduced by the large number of switches, we consider a group-connected VPS architecture and propose a HPD scheme, where the HPD problem is divided into multiple independent subproblems with each subproblem flexibly solved by the VPS-HPD or VPS-LC-HPD scheme. Simulation results verify the effectiveness of the propose schemes and show that the proposed schemes can achieve satisfactory spectral efficiency performance with reduced computational complexity or hardware complexity.      
### 65.A 23 $$W Keyword Spotting IC with Ring-Oscillator-Based Time-Domain Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2208.00693.pdf)
>  This article presents the first keyword spotting (KWS) IC which uses a ring-oscillator-based time-domain processing technique for its analog feature extractor (FEx). Its extensive usage of time-encoding schemes allows the analog audio signal to be processed in a fully time-domain manner except for the voltage-to-time conversion stage of the analog front-end. Benefiting from fundamental building blocks based on digital logic gates, it offers a better technology scalability compared to conventional voltage-domain designs. Fabricated in a 65 nm CMOS process, the prototyped KWS IC occupies 2.03mm$^{2}$ and dissipates 23 $\mu$W power consumption including analog FEx and digital neural network classifier. The 16-channel time-domain FEx achieves 54.89 dB dynamic range for 16 ms frame shift size while consuming 9.3 $\mu$W. The measurement result verifies that the proposed IC performs a 12-class KWS task on the Google Speech Command Dataset (GSCD) with &gt;86% accuracy and 12.4 ms latency.      
### 66.Rate-Splitting Multiple Access for Quantized Multiuser MIMO Communications  [ :arrow_down: ](https://arxiv.org/pdf/2208.00643.pdf)
>  This paper investigates the sum spectral efficiency maximization problem in downlink multiuser multiple-input multiple-output (MIMO) systems with low-resolution quantizers at an access point (AP) and users. In particular, we consider rate-splitting multiple access (RSMA) to enhance spectral efficiency by offering opportunities to boost achievable degrees of freedom. Optimizing RSMA precoders, however, is highly challenging due to the minimum rate constraint when determining the rate of the common stream. The quantization errors coupled with the precoders further make the problem more complicated and difficult to solve. In this paper, we develop a novel RSMA precoding algorithm incorporating quantization errors for maximizing the sum spectral efficiency. To this end, we first obtain an approximate spectral efficiency in a smooth function. Subsequently, we derive the first-order optimality condition in the form of the nonlinear eigenvalue problem (NEP). We propose a computationally efficient algorithm to find the principal eigenvector of the NEP as a sub-optimal solution. Simulation results validate the superior spectral efficiency of the proposed method. The key benefit of using RSMA over spatial division multiple access (SDMA) comes from the ability of the common stream to balance between the channel gain and quantization error in multiuser MIMO systems with different quantization resolutions.      
### 67.Quality Evaluation of Arbitrary Style Transfer: Subjective Study and Objective Metric  [ :arrow_down: ](https://arxiv.org/pdf/2208.00623.pdf)
>  Arbitrary neural style transfer is a vital topic with research value and industrial application prospect, which strives to render the structure of one image using the style of another. Recent researches have devoted great efforts on the task of arbitrary style transfer (AST) for improving the stylization quality. However, there are very few explorations about the quality evaluation of AST images, even it can potentially guide the design of different algorithms. In this paper, we first construct a new AST images quality assessment database (AST-IQAD) that consists 150 content-style image pairs and the corresponding 1200 stylized images produced by eight typical AST algorithms. Then, a subjective study is conducted on our AST-IQAD database, which obtains the subjective rating scores of all stylized images on the three subjective evaluations, i.e., content preservation (CP), style resemblance (SR), and overall visual (OV). To quantitatively measure the quality of AST image, we proposed a new sparse representation-based image quality evaluation metric (SRQE), which computes the quality using the sparse feature similarity. Experimental results on the AST-IQAD have demonstrated the superiority of the proposed method. The dataset and source code will be released at <a class="link-external link-https" href="https://github.com/Hangwei-Chen/AST-IQAD-SRQE" rel="external noopener nofollow">this https URL</a>      
### 68.Vector-Based Data Improves Left-Right Eye-Tracking Classifier Performance After a Covariate Distributional Shift  [ :arrow_down: ](https://arxiv.org/pdf/2208.00465.pdf)
>  The main challenges of using electroencephalogram (EEG) signals to make eye-tracking (ET) predictions are the differences in distributional patterns between benchmark data and real-world data and the noise resulting from the unintended interference of brain signals from multiple sources. Increasing the robustness of machine learning models in predicting eye-tracking position from EEG data is therefore integral for both research and consumer use. In medical research, the usage of more complicated data collection methods to test for simpler tasks has been explored to address this very issue. In this study, we propose a fine-grain data approach for EEG-ET data collection in order to create more robust benchmarking. We train machine learning models utilizing both coarse-grain and fine-grain data and compare their accuracies when tested on data of similar/different distributional patterns in order to determine how susceptible EEG-ET benchmarks are to differences in distributional data. We apply a covariate distributional shift to test for this susceptibility. Results showed that models trained on fine-grain, vector-based data were less susceptible to distributional shifts than models trained on coarse-grain, binary-classified data.      
### 69.Doppler Shift and Channel Estimation for Intelligent Transparent Surface Assisted Communication Systems on High-Speed Railways  [ :arrow_down: ](https://arxiv.org/pdf/2208.00455.pdf)
>  The critical distinction between the emerging intelligent transparent surface (ITS) and intelligent reflection surface (IRS) is that the incident signals can penetrate the ITS instead of being reflected, which enables the ITS to combat the severe signal penetration loss for high-speed railway (HSR) wireless communications. This paper thus investigates the channel estimation problem for an ITS-assisted HSR network where the ITS is embedded into the carriage window. We first formulate the channels as functions of physical parameters, and thus transform the channel estimation into a parameter recovery problem. Next, we design the first two pilot blocks within each frame and develop a serial low-complexity channel estimation algorithm. Specifically, the channel estimates are initially obtained, and each estimate is further expressed as the sum of its perfectly known value and the estimation error. By leveraging the relationship between channels for the two pilot blocks, we recover the Doppler shifts from the channel estimates, based on which we can further acquire other channel parameters. Moreover, the Cramer-Rao lower bound (CRLB) for each parameter is derived as a performance benchmark. Finally, we provide numerical results to establish the effectiveness of our proposed estimators.      
### 70.Robust Real-World Image Super-Resolution against Adversarial Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00428.pdf)
>  Recently deep neural networks (DNNs) have achieved significant success in real-world image super-resolution (SR). However, adversarial image samples with quasi-imperceptible noises could threaten deep learning SR models. In this paper, we propose a robust deep learning framework for real-world SR that randomly erases potential adversarial noises in the frequency domain of input images or features. The rationale is that on the SR task clean images or features have a different pattern from the attacked ones in the frequency domain. Observing that existing adversarial attacks usually add high-frequency noises to input images, we introduce a novel random frequency mask module that blocks out high-frequency components possibly containing the harmful perturbations in a stochastic manner. Since the frequency masking may not only destroys the adversarial perturbations but also affects the sharp details in a clean image, we further develop an adversarial sample classifier based on the frequency domain of images to determine if applying the proposed mask module. Based on the above ideas, we devise a novel real-world image SR framework that combines the proposed frequency mask modules and the proposed adversarial classifier with an existing super-resolution backbone network. Experiments show that our proposed method is more insensitive to adversarial attacks and presents more stable SR results than existing models and defenses.      
### 71.On the Uplink Performance of Finite-Capacity Radio Stripes  [ :arrow_down: ](https://arxiv.org/pdf/2208.00407.pdf)
>  Cell-Free (CF) Massive MIMO (mMIMO) is a technology which can potentially augment not only the deployment of 5G, but also the deployment of beyond 5G (B5G) wireless networks. However, the cost for rolling out such systems may be significant. Radio stripes form a promising solution which offers the potential of scalability at a reduced price. This paper investigates the uplink scenario of a CF mMIMO system, implemented with a limited-capacity radio stripe which integrates a novel arrangement of access points (APs), fully exploiting macro-diversity benefits. We also analyze a heuristic Compare-and-Forward (CnF) strategy, which, by comparing normalized linear minimum mean square error (N-LMMSE) soft estimates, enables optimal dynamic cooperation clustering, thus leading to a user-centric radio stripe network approach. Aiming at maximizing the per-user uplink spectral efficiency (SE), we ensure that, under finite capacity constraints, our solution can guarantee better performance than existing radio stripe architectures, especially when system size increases.      
### 72.Array Antenna Power Pattern Analysis Through Quantum Computing  [ :arrow_down: ](https://arxiv.org/pdf/2208.00401.pdf)
>  A method for the analysis of the power pattern of phased array antennas (PAs) based on the quantum Fourier transform (QFT) is proposed. The computation of the power pattern given the set of complex excitations of the PA elements is addressed within the quantum computing (QC) framework by means of a customized procedure that exploits the quantum mechanics principles and theory. A representative set of numerical results, yielded with a quantum computer emulator, is reported and discussed to assess the reliability of the proposed method by pointing out its features in comparison with the classical approach based on the discrete Fourier transform (DFT), as well.      
### 73.An Experimental Study on Learning Correlated Equilibrium in Routing Games  [ :arrow_down: ](https://arxiv.org/pdf/2208.00391.pdf)
>  We study route choice in a repeated routing game where an uncertain state of nature determines link latency functions, and agents receive private route recommendation. The state is sampled in an i.i.d. manner in every round from a publicly known distribution, and the recommendations are generated by a randomization policy whose mapping from the state is known publicly. In a one-shot setting, the agents are said to obey recommendation if it gives the smallest travel time in a posteriori expectation. A plausible extension to repeated setting is that the likelihood of following recommendation in a round is related to regret from previous rounds. If the regret is of satisficing type with respect to a default choice and is averaged over past rounds and over all agents, then the asymptotic outcome under an obedient recommendation policy coincides with the one-shot outcome. We report findings from an experiment with one participant at a time engaged in repeated route choice decision on computer. In every round, the participant is shown travel time distribution for each route, a route recommendation generated by an obedient policy, and a rating suggestive of average experience of previous participants with the quality of recommendation. Upon entering route choice, the actual travel times are revealed. The participant evaluates the quality of recommendation by submitting a review. This is combined with historical reviews to update rating for the next round. Data analysis from 33 participants each with 100 rounds suggests moderate negative correlation between the display rating and the average regret, and a strong positive correlation between the rating and the likelihood of following recommendation. Overall, under obedient recommendation policy, the rating converges close to its maximum value by the end of the experiments in conjunction with very high frequency of following recommendations.      
### 74.Neural Correlates of Face Familiarity Perception  [ :arrow_down: ](https://arxiv.org/pdf/2208.00352.pdf)
>  In the domain of face recognition, there exists a puzzling timing discrepancy between results from macaque neurophysiology on the one hand and human electrophysiology on the other. Single unit recordings in macaques have demonstrated face identity specific responses in extra-striate visual cortex within 100 milliseconds of stimulus onset. In EEG and MEG experiments with humans, however, a consistent distinction between neural activity corresponding to unfamiliar and familiar faces has been reported to emerge around 250 ms. This points to the possibility that there may be a hitherto undiscovered early correlate of face familiarity perception in human electrophysiological traces. We report here a successful search for such a correlate in dense MEG recordings using pattern classification techniques. Our analyses reveal markers of face familiarity as early as 85 ms after stimulus onset. Low-level attributes of the images, such as luminance and color distributions, are unable to account for this early emerging response difference. These results help reconcile human and macaque data, and provide clues regarding neural mechanisms underlying familiar face perception.      
### 75.Electromagnetic Signal Injection Attacks on Differential Signaling  [ :arrow_down: ](https://arxiv.org/pdf/2208.00343.pdf)
>  Differential signaling is a method of data transmission that uses two complementary electrical signals to encode information. This allows a receiver to reject any noise by looking at the difference between the two signals, assuming the noise affects both signals in the same way. Many protocols such as USB, Ethernet, and HDMI use differential signaling to achieve a robust communication channel in a noisy environment. This generally works well and has led many to believe that it is infeasible to remotely inject attacking signals into such a differential pair. In this paper we challenge this assumption and show that an adversary can in fact inject malicious signals from a distance, purely using common-mode injection, i.e., injecting into both wires at the same time. We show how this allows an attacker to inject bits or even arbitrary messages into a communication line. Such an attack is a significant threat to many applications, from home security and privacy to automotive systems, critical infrastructure, or implantable medical devices; in which incorrect data or unauthorized control could cause significant damage, or even fatal accidents. <br>We show in detail the principles of how an electromagnetic signal can bypass the noise rejection of differential signaling, and eventually result in incorrect bits in the receiver. We show how an attacker can exploit this to achieve a successful injection of an arbitrary bit, and we analyze the success rate of injecting longer arbitrary messages. We demonstrate the attack on a real system and show that the success rate can reach as high as $90\%$. Finally, we present a case study where we wirelessly inject a message into a Controller Area Network (CAN) bus, which is a differential signaling bus protocol used in many critical applications, including the automotive and aviation sector.      
### 76.Untargeted Region of Interest Selection for GC-MS Data using a Pseudo F-Ratio Moving Window ($$FRMV)  [ :arrow_down: ](https://arxiv.org/pdf/2208.00313.pdf)
>  There are many challenges associated with analysing gas chromatography - mass spectrometry (GC-MS) data. Many of these challenges stem from the fact that electron ionisation can make it difficult to recover molecular information due to the high degree of fragmentation with concomitant loss of molecular ion signal. With GC-MS data there are often many common fragment ions shared among closely-eluting peaks, necessitating sophisticated methods for analysis. Some of these methods are fully automated, but make some assumptions about the data which can introduce artifacts during the analysis. Chemometric methods such as Multivariate Curve Resolution, or Parallel Factor Analysis are particularly attractive, since they are flexible and make relatively few assumptions about the data - ideally resulting in fewer artifacts. These methods do require expert user intervention to determine the most relevant regions of interest and an appropriate number of components, $k$, for each region. Automated region of interest selection is needed to permit automated batch processing of chromatographic data with advanced signal deconvolution. Here, we propose a new method for automated, untargeted region of interest selection that accounts for the multivariate information present in GC-MS data to select regions of interest based on the ratio of the squared first, and second singular values from the Singular Value Decomposition of a window that moves across the chromatogram. Assuming that the first singular value accounts largely for signal, and that the second singular value accounts largely for noise, it is possible to interpret the relationship between these two values as a probabilistic distribution of Fisher Ratios. The sensitivity of the algorithm was tested by investigating the concentration at which the algorithm can no longer pick out chromatographic regions known to contain signal.      
### 77.Energy-Aware, Collision-Free Information Gathering for Heterogeneous Robot Teams  [ :arrow_down: ](https://arxiv.org/pdf/2208.00262.pdf)
>  This paper considers the problem of safely coordinating a team of sensor-equipped robots to reduce uncertainty about a dynamical process, where the objective trades off information gain and energy cost. Optimizing this trade-off is desirable, but leads to a non-monotone objective function in the set of robot trajectories. Therefore, common multi-robot planners based on coordinate descent lose their performance guarantees. Furthermore, methods that handle non-monotonicity lose their performance guarantees when subject to inter-robot collision avoidance constraints. As it is desirable to retain both the performance guarantee and safety guarantee, this work proposes a hierarchical approach with a distributed planner that uses local search with a worst-case performance guarantees and a decentralized controller based on control barrier functions that ensures safety and encourages timely arrival at sensing locations. Via extensive simulations, hardware-in-the-loop tests and hardware experiments, we demonstrate that the proposed approach achieves a better trade-off between sensing and energy cost than coordinate descent based algorithms.      
### 78.Max-Min Data Rate Optimization for RIS-aided Uplink Communications with Green Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2208.00182.pdf)
>  Smart radio environments aided by reconfigurable intelligent reflecting surfaces (RIS) have attracted much research attention recently. We propose a joint optimization strategy for beamforming, RIS phases, and power allocation to maximize the minimum SINR of an uplink RIS-aided communication system. The users are subject to constraints on their transmit power. We derive a closed-form expression for the beam forming vectors and a geometric programming-based solution for power allocation. We also propose two solutions for optimizing the phase shifts at the RIS, one based on the matrix lifting method and one using an approximation for the minimum function. We also propose a heuristic algorithm for optimizing quantized phase shift values. The proposed algorithms are of practical interest for systems with constraints on the maximum allowable electromagnetic field exposure. For instance, considering $24$-element RIS, $12$-antenna BS, and $6$ users, numerical results show that the proposed algorithm achieves close to $300 \%$ gain in terms of minimum SINR compared to a scheme with random RIS phases.      
### 79.Global, Unified Representation of Heterogenous Robot Dynamics Using Composition Operators  [ :arrow_down: ](https://arxiv.org/pdf/2208.00175.pdf)
>  The complexity of robot dynamics often pertains to the hybrid nature of dynamics, where governing dynamic equations consist of heterogenous equations that are switched depending on the state of the system. Legged robots and manipulator robots experience contact-noncontact discrete transitions, causing switching of governing equations. Analysis of these robot systems have been a challenge due to the lack of a global, unified model that is amenable to analysis of the global behaviors. Composition operator theory has the potential to provide a global, unified representation of those heterogenous dynamical systems. It is expected that, if the theory is applicable, those fundamentally challenging robotics systems can be treated as linear dynamical systems in a lifted space. The current work addresses under which conditions a unified linear representation exists in a global sense for a class of heterogenous dynamical systems and how the theory can be applied to those robotics problems. First, a kernel representation of composition operators is obtained, and conditions required for converting the kernel representation to a linear state transition equation are established. This analysis results in an algorithm to convert a class of heterogenous systems including hybrid and switched systems directly to a global, unified linear model. Unlike prevalent data-driven methods and Dynamic Mode Decomposition, where results can vary depending on numerical data, the proposed method does not require numerical simulation of the original dynamics. The implication of the new method and its impact upon robotics are discussed. A few examples validate and demonstrate the method.      
### 80.Learning inverse robot dynamics using sparse online Gaussian process with forgetting mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2208.00135.pdf)
>  Online Gaussian processes (GPs), typically used for learning models from time-series data, are more flexible and robust than offline GPs. Both local and sparse approximations of GPs can efficiently learn complex models online. Yet, these approaches assume that all signals are relatively accurate and that all data are available for learning without misleading data. Besides, the online learning capacity of GPs is limited for high-dimension problems and long-term tasks in practice. This paper proposes a sparse online GP (SOGP) with a forgetting mechanism to forget distant model information at a specific rate. The proposed approach combines two general data deletion schemes for the basis vector set of SOGP: The position information-based scheme and the oldest points-based scheme. We apply our approach to learn the inverse dynamics of a collaborative robot with 7 degrees of freedom under a two-segment trajectory tracking problem with task switching. Both simulations and experiments have shown that the proposed approach achieves better tracking accuracy and predictive smoothness compared with the two general data deletion schemes.      
### 81.Robust Rayleigh Regression Method for SAR Image Processing in Presence of Outliers  [ :arrow_down: ](https://arxiv.org/pdf/2208.00097.pdf)
>  The presence of outliers (anomalous values) in synthetic aperture radar (SAR) data and the misspecification in statistical image models may result in inaccurate inferences. To avoid such issues, the Rayleigh regression model based on a robust estimation process is proposed as a more realistic approach to model this type of data. This paper aims at obtaining Rayleigh regression model parameter estimators robust to the presence of outliers. The proposed approach considered the weighted maximum likelihood method and was submitted to numerical experiments using simulated and measured SAR images. Monte Carlo simulations were employed for the numerical assessment of the proposed robust estimator performance in finite signal lengths, their sensitivity to outliers, and the breakdown point. For instance, the non-robust estimators show a relative bias value $65$-fold larger than the results provided by the robust approach in corrupted signals. In terms of sensitivity analysis and break down point, the robust scheme resulted in a reduction of about $96\%$ and $10\%$, respectively, in the mean absolute value of both measures, in compassion to the non-robust estimators. Moreover, two SAR data sets were used to compare the ground type and anomaly detection results of the proposed robust scheme with competing methods in the literature.      
### 82.Signal Detection and Inference Based on the Beta Binomial Autoregressive Moving Average Model  [ :arrow_down: ](https://arxiv.org/pdf/2208.00095.pdf)
>  This paper proposes the beta binomial autoregressive moving average model (BBARMA) for modeling quantized amplitude data and bounded count data. The BBARMA model estimates the conditional mean of a beta binomial distributed variable observed over the time by a dynamic structure including: (i) autoregressive and moving average terms; (ii) a set of regressors; and (iii) a link function. Besides introducing the new model, we develop parameter estimation, detection tools, an out-of-signal forecasting scheme, and diagnostic measures. In particular, we provide closed-form expressions for the conditional score vector and the conditional information matrix. The proposed model was submitted to extensive Monte Carlo simulations in order to evaluate the performance of the conditional maximum likelihood estimators and of the proposed detector. The derived detector outperforms the usual ARMA- and Gaussian-based detectors for sinusoidal signal detection. We also presented an experiment for modeling and forecasting the monthly number of rainy days in Recife, Brazil.      
### 83.Low-complexity Approximate Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.00087.pdf)
>  In this paper, we present an approach for minimizing the computational complexity of trained Convolutional Neural Networks (ConvNet). The idea is to approximate all elements of a given ConvNet and replace the original convolutional filters and parameters (pooling and bias coefficients; and activation function) with efficient approximations capable of extreme reductions in computational complexity. Low-complexity convolution filters are obtained through a binary (zero-one) linear programming scheme based on the Frobenius norm over sets of dyadic rationals. The resulting matrices allow for multiplication-free computations requiring only addition and bit-shifting operations. Such low-complexity structures pave the way for low-power, efficient hardware designs. We applied our approach on three use cases of different complexity: (i) a "light" but efficient ConvNet for face detection (with around 1000 parameters); (ii) another one for hand-written digit classification (with more than 180000 parameters); and (iii) a significantly larger ConvNet: AlexNet with $\approx$1.2 million matrices. We evaluated the overall performance on the respective tasks for different levels of approximations. In all considered applications, very low-complexity approximations have been derived maintaining an almost equal classification performance.      
### 84.UAVM: A Unified Model for Audio-Visual Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.00061.pdf)
>  Conventional audio-visual models have independent audio and video branches. We design a unified model for audio and video processing called Unified Audio-Visual Model (UAVM). In this paper, we describe UAVM, report its new state-of-the-art audio-visual event classification accuracy of 65.8% on VGGSound, and describe the intriguing properties of the model.      
### 85.RangL: A Reinforcement Learning Competition Platform  [ :arrow_down: ](https://arxiv.org/pdf/2208.00003.pdf)
>  The RangL project hosted by The Alan Turing Institute aims to encourage the wider uptake of reinforcement learning by supporting competitions relating to real-world dynamic decision problems. This article describes the reusable code repository developed by the RangL team and deployed for the 2022 Pathways to Net Zero Challenge, supported by the UK Net Zero Technology Centre. The winning solutions to this particular Challenge seek to optimize the UK's energy transition policy to net zero carbon emissions by 2050. The RangL repository includes an OpenAI Gym reinforcement learning environment and code that supports both submission to, and evaluation in, a remote instance of the open source EvalAI platform as well as all winning learning agent strategies. The repository is an illustrative example of RangL's capability to provide a reusable structure for future challenges.      
### 86.HOB-CNN: Hallucination of Occluded Branches with a Convolutional Neural Network for 2D Fruit Trees  [ :arrow_down: ](https://arxiv.org/pdf/2208.00002.pdf)
>  Orchard automation has attracted the attention of researchers recently due to the shortage of global labor force. To automate tasks in orchards such as pruning, thinning, and harvesting, a detailed understanding of the tree structure is required. However, occlusions from foliage and fruits can make it challenging to predict the position of occluded trunks and branches. This work proposes a regression-based deep learning model, Hallucination of Occluded Branch Convolutional Neural Network (HOB-CNN), for tree branch position prediction in varying occluded conditions. We formulate tree branch position prediction as a regression problem towards the horizontal locations of the branch along the vertical direction or vice versa. We present comparative experiments on Y-shaped trees with two state-of-the-art baselines, representing common approaches to the problem. Experiments show that HOB-CNN outperform the baselines at predicting branch position and shows robustness against varying levels of occlusion. We further validated HOB-CNN against two different types of 2D trees, and HOB-CNN shows generalization across different trees and robustness under different occluded conditions.      
### 87.FastGeodis: Fast Generalised Geodesic Distance Transform  [ :arrow_down: ](https://arxiv.org/pdf/2208.00001.pdf)
>  The FastGeodis package provides an efficient implementation for computing Geodesic and Euclidean distance transforms (or a mixture of both) targeting efficient utilisation of CPU and GPU hardwares. In particular, it implements paralellisable raster scan method from Criminisi et al, where elements in row (2D) or plane (3D) can be computed with parallel threads. This package is able to handle 2D as well as 3D data where it achieves up to 15x speed-up on CPU and up to 60x speed-up on GPU as compared to existing open-source libraries, which uses non-parallelisable single-thread CPU implementation. The performance speed-ups reported here were evaluated using 3D volume data on Nvidia GeForce Titan X (12 GB) with 6-Core Intel Xeon E5-1650 CPU. This package is available at: <a class="link-external link-https" href="https://github.com/masadcv/FastGeodis" rel="external noopener nofollow">this https URL</a>      
