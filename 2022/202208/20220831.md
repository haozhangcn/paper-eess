# ArXiv eess --Wed, 31 Aug 2022
### 1.Comparing Results of Thermographic Images Based Diagnosis for Breast Diseases  [ :arrow_down: ](https://arxiv.org/pdf/2208.14410.pdf)
>  This paper examines the potential contribution of infrared (IR) imaging in breast diseases detection. It compares obtained results using some algorithms for detection of malignant breast conditions such as Support Vector Machine (SVM) regarding the consistency of different approaches when applied to public data. Moreover, in order to avail the actual IR imaging's capability as a complement on clinical trials and to promote researches using high-resolution IR imaging we deemed the use of a public database revised by confidently trained breast physicians as essential. Only the static acquisition protocol is regarded in our work. We used lO2 IR single breast images from the Pro Engenharia (PROENG) public database (54 normal and 48 with some finding). These images were collected from Universidade Federal de Pernambuco (UFPE) University's Hospital. We employed the same features proposed by the authors of the work that presented the best results and achieved an accuracy of 61.7 % and Youden index of 0.24 using the Sequential Minimal Optimization (SMO) classifier.      
### 2.Evolutionary Deep Reinforcement Learning for Dynamic Slice Management in O-RAN  [ :arrow_down: ](https://arxiv.org/pdf/2208.14394.pdf)
>  The next-generation wireless networks are required to satisfy a variety of services and criteria concurrently. To address upcoming strict criteria, a new open radio access network (O-RAN) with distinguishing features such as flexible design, disaggregated virtual and programmable components, and intelligent closed-loop control was developed. O-RAN slicing is being investigated as a critical strategy for ensuring network quality of service (QoS) in the face of changing circumstances. However, distinct network slices must be dynamically controlled to avoid service level agreement (SLA) variation caused by rapid changes in the environment. Therefore, this paper introduces a novel framework able to manage the network slices through provisioned resources intelligently. Due to diverse heterogeneous environments, intelligent machine learning approaches require sufficient exploration to handle the harshest situations in a wireless network and accelerate convergence. To solve this problem, a new solution is proposed based on evolutionary-based deep reinforcement learning (EDRL) to accelerate and optimize the slice management learning process in the radio access network's (RAN) intelligent controller (RIC) modules. To this end, the O-RAN slicing is represented as a Markov decision process (MDP) which is then solved optimally for resource allocation to meet service demand using the EDRL approach. In terms of reaching service demands, simulation results show that the proposed approach outperforms the DRL baseline by 62.2%.      
### 3.Digital Twin Assisted Risk-Aware Sleep Mode Management Using Deep Q-Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.14380.pdf)
>  Base stations (BSs) are the most energy-consuming segment of mobile networks. To reduce BS energy consumption, different components of BSs can sleep when BS is not active. According to the activation/deactivation time of the BS components, multiple sleep modes (SMs) are defined in the literature. In this study, we model the problem of BS energy saving utilizing multiple sleep modes as a sequential MDP and propose an online traffic-aware deep reinforcement learning approach to maximize the long-term energy saving. However, there is a risk that BS is not sleeping at the right time and incurs large delays to the users. To tackle this issue, we propose to use a digital twin model to encapsulate the dynamics underlying the investigated system and estimate the risk of decision-making (RDM) in advance. We define a novel metric to quantify RDM and predict the performance degradation. The RDM calculated by DT is compared with a tolerable threshold set by the mobile operator. Based on this comparison, BS can decide to deactivate the SMs, re-train when needed to avoid taking high risks, or activate the SMs to benefit from energy savings. For deep reinforcement learning, we use long-short term memory (LSTM), to take into account the long and short-term dependencies in input traffic, and approximate the Q-function. We train the LSTM network using the experience replay method over a real traffic data set obtained from an operator BS in Stockholm. The data set contains data rate information with very coarse-grained time granularity. Thus, we propose a scheme to generate a new data set using the real network data set which 1) has finer-grained time granularity and 2) considers the bursty behavior of traffic data. Simulation results show that using proposed methods, considerable energy saving is obtained, compared to the baselines at cost of negligible number of delayed users.      
### 4.Automated recognition of the pericardium contour on processed CT images using genetic algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2208.14375.pdf)
>  This work proposes the use of Genetic Algorithms (GA) in tracing and recognizing the pericardium contour of the human heart using Computed Tomography (CT) images. We assume that each slice of the pericardium can be modelled by an ellipse, the parameters of which need to be optimally determined. An optimal ellipse would be one that closely follows the pericardium contour and, consequently, separates appropriately the epicardial and mediastinal fats of the human heart. Tracing and automatically identifying the pericardium contour aids in medical diagnosis. Usually, this process is done manually or not done at all due to the effort required. Besides, detecting the pericardium may improve previously proposed automated methodologies that separate the two types of fat associated to the human heart. Quantification of these fats provides important health risk marker information, as they are associated with the development of certain cardiovascular pathologies. Finally, we conclude that GA offers satisfiable solutions in a feasible amount of processing time.      
### 5.Deep Spatial and Tonal Data Optimisation for Homogeneous Diffusion Inpainting  [ :arrow_down: ](https://arxiv.org/pdf/2208.14371.pdf)
>  Diffusion-based inpainting can reconstruct missing image areas with high quality from sparse data, provided that their location and their values are well optimised. This is particularly useful for applications such as image compression, where the original image is known. Selecting the known data constitutes a challenging optimisation problem, that has so far been only investigated with model-based approaches. So far, these methods require a choice between either high quality or high speed since qualitatively convincing algorithms rely on many time-consuming inpaintings. We propose the first neural network architecture that allows fast optimisation of pixel positions and pixel values for homogeneous diffusion inpainting. During training, we combine two optimisation networks with a neural network-based surrogate solver for diffusion inpainting. This novel concept allows us to perform backpropagation based on inpainting results that approximate the solution of the inpainting equation. Without the need for a single inpainting during test time, our deep optimisation combines the high quality of model-based approaches with real-time performance.      
### 6.FAST-AID Brain: Fast and Accurate Segmentation Tool using Artificial Intelligence Developed for Brain  [ :arrow_down: ](https://arxiv.org/pdf/2208.14360.pdf)
>  Medical images used in clinical practice are heterogeneous and not the same quality as scans studied in academic research. Preprocessing breaks down in extreme cases when anatomy, artifacts, or imaging parameters are unusual or protocols are different. Methods robust to these variations are most needed. A novel deep learning method is proposed for fast and accurate segmentation of the human brain into 132 regions. The proposed model uses an efficient U-Net-like network and benefits from the intersection points of different views and hierarchical relations for the fusion of the orthogonal 2D planes and brain labels during the end-to-end training. Weakly supervised learning is deployed to take the advantage of partially labeled data for the whole brain segmentation and estimation of the intracranial volume (ICV). Moreover, data augmentation is used to expand the magnetic resonance imaging (MRI) data by generating realistic brain scans with high variability for robust training of the model while preserving data privacy. The proposed method can be applied to brain MRI data including skull or any other artifacts without preprocessing the images or a drop in performance. Several experiments using different atlases are conducted to evaluate the segmentation performance of the trained model compared to the state-of-the-art, and the results show higher segmentation accuracy and robustness of the proposed model compared to the existing methods across different intra- and inter-domain datasets.      
### 7.On the Automated Segmentation of Epicardial and Mediastinal Cardiac Adipose Tissues Using Classification Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2208.14352.pdf)
>  The quantification of fat depots on the surroundings of the heart is an accurate procedure for evaluating health risk factors correlated with several diseases. However, this type of evaluation is not widely employed in clinical practice due to the required human workload. This work proposes a novel technique for the automatic segmentation of cardiac fat pads. The technique is based on applying classification algorithms to the segmentation of cardiac CT images. Furthermore, we extensively evaluate the performance of several algorithms on this task and discuss which provided better predictive models. Experimental results have shown that the mean accuracy for the classification of epicardial and mediastinal fats has been 98.4% with a mean true positive rate of 96.2%. On average, the Dice similarity index, regarding the segmented patients and the ground truth, was equal to 96.8%. Therfore, our technique has achieved the most accurate results for the automatic segmentation of cardiac fats, to date.      
### 8.3D Near-Field Virtual MIMO-SAR Imaging using FMCW Radar Systems at 77 GHz  [ :arrow_down: ](https://arxiv.org/pdf/2208.14328.pdf)
>  In this paper, we present 3D high resolution radar imaging at millimeter-Wave (mmWave) frequencies via a combination of virtual Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous Wave (FMCW) Radars with Synthetic Aperture Radar (SAR) which results in a low cost and high speed 3D mmWave imagery system with low complexity.      
### 9.Representation Learning based and Interpretable Reactor System Diagnosis Using Denoising Padded Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2208.14319.pdf)
>  With the mass construction of Gen III nuclear reactors, it is a popular trend to use deep learning (DL) techniques for fast and effective diagnosis of possible accidents. To overcome the common problems of previous work in diagnosing reactor accidents using deep learning theory, this paper proposes a diagnostic process that ensures robustness to noisy and crippled data and is interpretable. First, a novel Denoising Padded Autoencoder (DPAE) is proposed for representation extraction of monitoring data, with representation extractor still effective on disturbed data with signal-to-noise ratios up to 25.0 and monitoring data missing up to 40.0%. Secondly, a diagnostic framework using DPAE encoder for extraction of representations followed by shallow statistical learning algorithms is proposed, and such stepwise diagnostic approach is tested on disturbed datasets with 41.8% and 80.8% higher classification and regression task evaluation metrics, in comparison with the end-to-end diagnostic approaches. Finally, a hierarchical interpretation algorithm using SHAP and feature ablation is presented to analyze the importance of the input monitoring parameters and validate the effectiveness of the high importance parameters. The outcomes of this study provide a referential method for building robust and interpretable intelligent reactor anomaly diagnosis systems in scenarios with high safety requirements.      
### 10.A Timing-Based Framework for Designing Resilient Cyber-Physical Systems under Safety Constraint  [ :arrow_down: ](https://arxiv.org/pdf/2208.14282.pdf)
>  Cyber-physical systems (CPS) are required to satisfy safety constraints in various application domains such as robotics, industrial manufacturing systems, and power systems. Faults and cyber attacks have been shown to cause safety violations, which can damage the system and endanger human lives. Resilient architectures have been proposed to ensure safety of CPS under such faults and attacks via methodologies including redundancy and restarting from safe operating conditions. The existing resilient architectures for CPS utilize different mechanisms to guarantee safety, and currently there is no approach to compare them. Moreover, the analysis and design undertaken for CPS employing one architecture is not readily extendable to another. In this paper, we propose a timing-based framework for CPS employing various resilient architectures and develop a common methodology for safety analysis and computation of control policies and design parameters. Using the insight that the cyber subsystem operates in one out of a finite number of statuses, we first develop a hybrid system model that captures CPS adopting any of these architectures. Based on the hybrid system, we formulate the problem of joint computation of control policies and associated timing parameters for CPS to satisfy a given safety constraint and derive sufficient conditions for the solution. Utilizing the derived conditions, we provide an algorithm to compute control policies and timing parameters relevant to the employed architecture. We also note that our solution can be applied to a wide class of CPS with polynomial dynamics and also allows incorporation of new architectures. We verify our proposed framework by performing a case study on adaptive cruise control of vehicles.      
### 11.FUSION: Fully Unsupervised Test-Time Stain Adaptation via Fused Normalization Statistics  [ :arrow_down: ](https://arxiv.org/pdf/2208.14206.pdf)
>  Staining reveals the micro structure of the aspirate while creating histopathology slides. Stain variation, defined as a chromatic difference between the source and the target, is caused by varying characteristics during staining, resulting in a distribution shift and poor performance on the target. The goal of stain normalization is to match the target's chromatic distribution to that of the source. However, stain normalisation causes the underlying morphology to distort, resulting in an incorrect diagnosis. We propose FUSION, a new method for promoting stain-adaption by adjusting the model to the target in an unsupervised test-time scenario, eliminating the necessity for significant labelling at the target end. FUSION works by altering the target's batch normalization statistics and fusing them with source statistics using a weighting factor. The algorithm reduces to one of two extremes based on the weighting factor. Despite the lack of training or supervision, FUSION surpasses existing equivalent algorithms for classification and dense predictions (segmentation), as demonstrated by comprehensive experiments on two public datasets.      
### 12.ORACLE: Occlusion-Resilient And self-Calibrating mmWave Radar Network for People Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2208.14199.pdf)
>  Millimeter wave (mmWave) radars are emerging as valid alternatives to cameras for the pervasive contactless monitoring of industrial environments, enabling very tight human-machine interactions. However, commercial mmWave radars feature a limited range (up to 6-8 m) and are subject to occlusion, which may constitute a significant drawback in large industrial settings containing machinery and obstacles. Thus, covering large indoor spaces requires multiple radars with known relative position and orientation. As a result, we necessitate algorithms to combine their outputs. In this work, we present ORACLE, an autonomous system that (i) integrates automatic relative position and orientation estimation from multiple radar devices by exploiting the trajectories of people moving freely in the radars' common fields of view and (ii) fuses the tracking information from multiple radars to obtain a unified tracking among all sensors. Our implementation and experimental evaluation of ORACLE results in median errors of 0.18 m and 2.86° for radars location and orientation estimates, respectively. The fused tracking improves upon single sensor tracking by 6%, in terms of mean tracking accuracy, while reaching a mean tracking error as low as 0.14 m. Finally, ORACLE is robust to fusion relative time synchronization mismatches between -20% and +50%.      
### 13.A Study on the relationship between the geometrical shapes and the biometrical acoustic characteristics of human ear canal  [ :arrow_down: ](https://arxiv.org/pdf/2208.14182.pdf)
>  Ear acoustic authentication is a new biometrics method and it utilizes the differences in acoustic characteristics of the ear canal between users. However, there have been few reports on the factors that cause differences in the acoustic characteristics. We investigate the relationship between ear canal shapes and acoustic characteristics in terms of user-to-user similarity. We used magnetic resonance imaging (MRI) to measure ear canal geometry. As a result, the correlation coefficient between shape similarity and acoustic characteristic similarity is higher than 0.7 and the coefficient of determination is higher than 0.5. This suggests that the difference in the shape of the ear canal is one of the important factors.      
### 14.Ærø: A Platform Architecture for Mixed-Criticality Airborne Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.14158.pdf)
>  Real-time embedded platforms with resource constraints can take the benefits of mixed-criticality system where applications with different criticality-level share computational resources, with isolation in the temporal and spatial domain. A conventional software-based isolation mechanism adds additional overhead and requires certification with the highest level of criticality present in the system, which is often an expensive process. In this article, we present a different approach where the required isolation is established at the hardware-level by featuring partitions within the processor. A four-stage pipelined soft-processor with replicated resources in the data-path is introduced to establish isolation and avert interference between the partitions. A cycle-accurate scheduling mechanism is implemented in the hardware for hard-real-time partition scheduling that can accommodate different periodicity and execution time for each partition as per user needs, while preserving time-predictability at the individual application level. Applications running within a partition has no sense of the virtualization and can execute either on a host-software or directly on the hardware. The proposed architecture is implemented on FPGA thread and demonstrated with an avionics use case.      
### 15.Airway measurement by refinement of synthetic images improves mortality prediction in idiopathic pulmonary fibrosis  [ :arrow_down: ](https://arxiv.org/pdf/2208.14141.pdf)
>  Several chronic lung diseases, like idiopathic pulmonary fibrosis (IPF) are characterised by abnormal dilatation of the airways. Quantification of airway features on computed tomography (CT) can help characterise disease progression. Physics based airway measurement algorithms have been developed, but have met with limited success in part due to the sheer diversity of airway morphology seen in clinical practice. Supervised learning methods are also not feasible due to the high cost of obtaining precise airway annotations. We propose synthesising airways by style transfer using perceptual losses to train our model, Airway Transfer Network (ATN). We compare our ATN model with a state-of-the-art GAN-based network (simGAN) using a) qualitative assessment; b) assessment of the ability of ATN and simGAN based CT airway metrics to predict mortality in a population of 113 patients with IPF. ATN was shown to be quicker and easier to train than simGAN. ATN-based airway measurements were also found to be consistently stronger predictors of mortality than simGAN-derived airway metrics on IPF CTs. Airway synthesis by a transformation network that refines synthetic data using perceptual losses is a realistic alternative to GAN-based methods for clinical CT analyses of idiopathic pulmonary fibrosis. Our source code can be found at <a class="link-external link-https" href="https://github.com/ashkanpakzad/ATN" rel="external noopener nofollow">this https URL</a> that is compatible with the existing open-source airway analysis framework, AirQuant.      
### 16.A General Model for Pointing Error of High Frequency Directional Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2208.14140.pdf)
>  This paper focuses on providing an analytical framework for the quantification and evaluation of the pointing error for a general case at high-frequency millimeter wave (mmWave) and terahertz (THz) communication links. For this aim, we first derive the the probability density function (PDF) and cumulative distribution functions (CDF) of the pointing error between an unstable transmitter (Tx) and receiver (Rx), that have different antenna patterns and for which the vibrations are not similar in the Yaw and Pitch directions. The special case where the Tx and Rx are both equipped with uniform linear array antenna is also investigated. In addition, using $\alpha-\mu$ distribution, which is a valid model for small-scale fading of mmWave/THz links, the end-to-end PDF and CDF of the considered channel is derived for all the considered cases. Finally, by employing Monte-Carlo simulations, the accuracy of the analytical expressions is verified and the performance of the system is studied.      
### 17.Distributed Constraint-Coupled Optimization over Lossy Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.14116.pdf)
>  This paper considers distributed resource allocation and sum-preserving constrained optimization over lossy networks, where the links are unreliable and subject to packet drops. We define the conditions to ensure convergence under packet drops and link removal by focusing on two main properties of our allocation algorithm: (i) The weight-stochastic condition in typical consensus schemes is reduced to balanced weights, with no need for readjusting the weights to satisfy stochasticity. (ii) The algorithm does not require all-time connectivity but instead uniform connectivity over some non-overlapping finite time intervals. First, we prove that our algorithm provides primal-feasible allocation at every iteration step and converges under the conditions (i)-(ii) and some other mild conditions on the nonlinear iterative dynamics. These nonlinearities address possible practical constraints in real applications due to, for example, saturation or quantization among others. Then, using (i)-(ii) and the notion of bond-percolation theory, we relate the packet drop rate and the network percolation threshold to the (finite) number of iterations ensuring uniform connectivity and, thus, convergence towards the optimum value.      
### 18.G-SemTMO: Tone Mapping with a Trainable Semantic Graph  [ :arrow_down: ](https://arxiv.org/pdf/2208.14113.pdf)
>  A Tone Mapping Operator (TMO) is required to render images with a High Dynamic Range (HDR) on media with limited dynamic capabilities. TMOs compress the dynamic range with the aim of preserving the visually perceptual cues of the scene. Previous literature has established the benefits of TMOs being semantic aware, understanding the content in the scene to preserve the cues better. Expert photographers analyze the semantic and the contextual information of a scene and decide tonal transformations or local luminance adjustments. This process can be considered a manual analogy to tone mapping. In this work, we draw inspiration from an expert photographer's approach and present a Graph-based Semantic-aware Tone Mapping Operator, G-SemTMO. We leverage semantic information as well as the contextual information of the scene in the form of a graph capturing the spatial arrangements of its semantic segments. Using Graph Convolutional Network (GCN), we predict intermediate parameters called Semantic Hints and use these parameters to apply tonal adjustments locally to different semantic segments in the image. In addition, we also introduce LocHDR, a dataset of 781 HDR images tone mapped manually by an expert photo-retoucher with local tonal enhancements. We conduct ablation studies to show that our approach, G-SemTMO\footnote{Code and dataset to be published with the final version of the manuscript}, can learn both global and local tonal transformations from a pair of input linear and manually retouched images by leveraging the semantic graphs and produce better results than both classical and learning based TMOs. We also conduct ablation experiments to validate the advantage of using GCN.      
### 19.DTAC-ADMM: Delay-Tolerant Augmented Consensus ADMM-based Algorithm for Distributed Resource Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2208.14077.pdf)
>  Latency is inherent in almost all real-world networked applications. In this paper, we propose a distributed allocation strategy over multi-agent networks with delayed communications. The state of each agent (or node) represents its share of assigned resources out of a fixed amount (equal to overall demand). Every node locally updates its state toward optimizing a global allocation cost function via received information of its neighbouring nodes even when the data exchange over the network is heterogeneously delayed at different links. The update is based on the alternating direction method of multipliers (ADMM) formulation subject to both sum-preserving coupling-constraint and local box-constraints. The solution is derivative-free and holds for general (not necessarily differentiable) convex cost models. We use the notion of augmented consensus over undirected networks to model delayed information exchange and for convergence analysis. We simulate our \textit{delay-tolerant} algorithm for      
### 20.Distributed CPU Scheduling Subject to Nonlinear Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2208.14059.pdf)
>  This paper considers a network of collaborating agents for local resource allocation subject to nonlinear model constraints. In many applications, it is required (or desirable) that the solution be anytime feasible in terms of satisfying the sum-preserving global constraint. Motivated by this, sufficient conditions on the nonlinear mapping for anytime feasibility (or non-asymptotic feasibility) are addressed in this paper. For the two proposed distributed solutions, one converges over directed weight-balanced networks and the other one over undirected networks. In particular, we elaborate on uniform quantization and discuss the notion of {\epsilon}-accurate solution, which gives an estimate of how close we can get to the exact optimizer subject to different quantization levels. This work, further, handles general (possibly non-quadratic) strictly convex objective functions with application to CPU allocation among a cloud of data centers via distributed solutions. The results can be used as a coordination mechanism to optimally balance the tasks and CPU resources among a group of networked servers while addressing quantization or limited server capacity. <br>Index Terms: multi-agent systems, sum-preserving resource allocation, distributed optimization, anytime feasibility      
### 21.Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2208.14022.pdf)
>  Fluoroscopy is an imaging technique that uses X-ray to obtain a real-time 2D video of the interior of a 3D object, helping surgeons to observe pathological structures and tissue functions especially during intervention. However, it suffers from heavy noise that mainly arises from the clinical use of a low dose X-ray, thereby necessitating the technology of fluoroscopy denoising. Such denoising is challenged by the relative motion between the object being imaged and the X-ray imaging system. We tackle this challenge by proposing a self-supervised, three-stage framework that exploits the domain knowledge of fluoroscopy imaging. (i) Stabilize: we first construct a dynamic panorama based on optical flow calculation to stabilize the non-stationary background induced by the motion of the X-ray detector. (ii) Decompose: we then propose a novel mask-based Robust Principle Component Analysis (RPCA) decomposition method to separate a video with detector motion into a low-rank background and a sparse foreground. Such a decomposition accommodates the reading habit of experts. (iii) Denoise: we finally denoise the background and foreground separately by a self-supervised learning strategy and fuse the denoised parts into the final output via a bilateral, spatiotemporal filter. To assess the effectiveness of our work, we curate a dedicated fluoroscopy dataset of 27 videos (1,568 frames) and corresponding ground truth. Our experiments demonstrate that it achieves significant improvements in terms of denoising and enhancement effects when compared with standard approaches. Finally, expert rating confirms this efficacy.      
### 22.EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.14003.pdf)
>  Ejection fraction (EF) is a key indicator of cardiac function, allowing identification of patients prone to heart dysfunctions such as heart failure. EF is estimated from cardiac ultrasound videos known as echocardiograms (echo) by manually tracing the left ventricle and estimating its volume on certain frames. These estimations exhibit high inter-observer variability due to the manual process and varying video quality. Such sources of inaccuracy and the need for rapid assessment necessitate reliable and explainable machine learning techniques. In this work, we introduce EchoGNN, a model based on graph neural networks (GNNs) to estimate EF from echo videos. Our model first infers a latent echo-graph from the frames of one or multiple echo cine series. It then estimates weights over nodes and edges of this graph, indicating the importance of individual frames that aid EF estimation. A GNN regressor uses this weighted graph to predict EF. We show, qualitatively and quantitatively, that the learned graph weights provide explainability through identification of critical frames for EF estimation, which can be used to determine when human intervention is required. On EchoNet-Dynamic public EF dataset, EchoGNN achieves EF prediction performance that is on par with state of the art and provides explainability, which is crucial given the high inter-observer variability inherent in this task.      
### 23.Comments on "Time-Varying Lyapunov Functions for Tracking Control of Mechanical Systems With and Without Frictions"  [ :arrow_down: ](https://arxiv.org/pdf/2208.13981.pdf)
>  In the article$^a$, the authors introduced a time-varying Lyapunov function for the stability analysis of nonlinear systems whose motion is governed by standard Newton-Euler equations. The authors established asymptotic stability with the choice of two symmetric positive definite matrices restricted by certain eigenvalue bounds in the control law. Exponential stability in the sense of Lyapunov using integrator backstepping and Lyapunov redesign is established in this note using just one matrix in the derived controller. We do not impose minimum eigenvalue bound requirements on the symmetric positive definite matrix introduced in our analysis to guarantee stability. Reducing the parameters needed in the control law, our analysis improves the stability and convergence rates of tracking errors reported in the article$^a$. <br>$^a$Ren, W., Zhang, B, Li, H, and Yan L. IEEE Access. vol. 8. pp. 51510-51517. 2020.      
### 24.Learned Lossless Image Compression With Combined Autoregressive Models And Attention Modules  [ :arrow_down: ](https://arxiv.org/pdf/2208.13974.pdf)
>  Lossless image compression is an essential research field in image compression. Recently, learning-based image compression methods achieved impressive performance compared with traditional lossless methods, such as WebP, JPEG2000, and FLIF. However, there are still many impressive lossy compression methods that can be applied to lossless compression. Therefore, in this paper, we explore the methods widely used in lossy compression and apply them to lossless compression. Inspired by the impressive performance of the Gaussian mixture model (GMM) shown in lossy compression, we generate a lossless network architecture with GMM. Besides noticing the successful achievements of attention modules and autoregressive models, we propose to utilize attention modules and add an extra autoregressive model for raw images in our network architecture to boost the performance. Experimental results show that our approach outperforms most classical lossless compression methods and existing learning-based methods.      
### 25.Airway Tree Modeling Using Dual-channel 3D UNet 3+ with Vesselness Prior  [ :arrow_down: ](https://arxiv.org/pdf/2208.13969.pdf)
>  The lung airway tree modeling is essential to work for the diagnosis of pulmonary diseases, especially for X-Ray computed tomography (CT). The airway tree modeling on CT images can provide the experts with 3-dimension measurements like wall thickness, etc. This information can tremendously aid the diagnosis of pulmonary diseases like chronic obstructive pulmonary disease [1-4]. Many scholars have attempted various ways to model the lung airway tree, which can be split into two major categories based on its nature. Namely, the model-based approach and the deep learning approach. The performance of a typical model-based approach usually depends on the manual tuning of the model parameter, which can be its advantages and disadvantages. The advantage is its don't require a large amount of training data which can be beneficial for a small dataset like medical imaging. On the other hand, the performance of model-based may be a misconcep-tion [5,6]. <br>In recent years, deep learning has achieved good results in the field of medical image processing, and many scholars have used UNet-based methods in medical image segmentation [7-11]. Among all the variation of UNet, the UNet 3+ [11] have relatively good result compare to the rest of the variation of UNet. Therefor to further improve the accuracy of lung airway tree modeling, this study combines the Frangi filter [5] with UNet 3+ [11] to develop a dual-channel 3D UNet 3+. The Frangi filter is used to extracting vessel-like feature. The vessel-like feature then used as input to guide the dual-channel UNet 3+ training and testing procedures.      
### 26.A Comprehensive Survey on Aerial Mobile Edge Computing: Challenges, State-of-the-Art, and Future Directions  [ :arrow_down: ](https://arxiv.org/pdf/2208.13965.pdf)
>  Driven by the visions of Internet of Things (IoT), there is an ever-increasing demand for computation resources of IoT users to support diverse applications. Mobile edge computing (MEC) has been deemed a promising solution to settle the conflict between the resource-hungry mobile applications and the resource-constrained IoT users. On the other hand, in order to provide ubiquitous and reliable connectivity in wireless networks, unmanned aerial vehicles (UAVs) can be leveraged as efficient aerial platforms by exploiting their inherent attributes, such as the on-demand deployment, high cruising altitude, and controllable maneuverability in three-dimensional (3D) space. Thus, the UAV-enabled aerial MEC is believed as a win-win solution to facilitate cost-effective and energy-saving communication and computation services in various environments. In this paper, we provide a comprehensive survey on the UAV-enabled aerial MEC. Firstly, the related advantages and research challenges for aerial MEC are discussed. Then, we provide a comprehensive review of the recent research advances, which is categorized by different domains, including the joint optimization of UAV trajectory, computation offloading and resource allocation, UAV deployment, task scheduling and load balancing, interplay between aerial MEC and other technologies, as well as the machine-learning (ML)-driven optimization. Finally, some important research directions deserved more efforts in future work are summarized.      
### 27.Mathematical certification of motion planning on uncertain terrain with limited perception: a case study  [ :arrow_down: ](https://arxiv.org/pdf/2208.13961.pdf)
>  We design a controller for an agent whose mission is to reach a stationary target while avoiding a family of obstacles which are not known a-priori. The agent moves in the two dimensional plane with non-trivial double integrator dynamics and receives only local information from its surroundings. Under mild assumptions on the family of obstacles (smoothness, sufficient distance from each other, bounded curvature, etc), we prove that our control algorithm yields guaranteed obstacle avoidance and convergence to the target.      
### 28.Joint Optimization of Resource Allocation, Phase Shift and UAV Trajectory for Energy-Efficient RIS-Assisted UAV-Enabled MEC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13958.pdf)
>  The unmanned aerial vehicle (UAV) enabled mobile edge computing (MEC) has been deemed a promising paradigm to provide ubiquitous communication and computing services for the Internet of Things (IoT). Besides, by intelligently reflecting the received signals, the reconfigurable intelligent surface (RIS) can significantly improve the propagation environment and further enhance the service quality of the UAV-enabled MEC. Motivated by this vision, in this paper, we consider both the amount of completed task bits and the energy consumption to maximize the energy efficiency of the RIS-assisted UAV-enabled MEC systems, where the bit allocation, transmit power, phase shift, and UAV trajectory are jointly optimized by an iterative algorithm with a double-loop structure based on the Dinkelbach's method and block coordinate decent (BCD) technique. Simulation results demonstrate that: 1) with the deployment of RIS, our proposed algorithm can achieve higher energy efficiency than baseline schemes while satisfying the task tolerance latency; 2) the energy efficiency first increases and then decreases with the increase of the mission period and the total amount of task-input bits of IoT devices; 3) when the CPU cycles required for computing 1-bit of task-input data becomes larger, more task bits will be offloaded to the UAV while the energy efficiency will be decreased.      
### 29.Micro-Vibration Modes Reconstruction Based on Micro-Doppler Coincidence Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.13952.pdf)
>  Micro-vibration, a ubiquitous nature phenomenon, can be seen as a characteristic feature on the objects, these vibrations always have tiny amplitudes which are much less than the wavelengths of the sensing systems, thus these motions information can only be reflected in the phase item of echo. Normally the conventional radar system can detect these micro vibrations through the time frequency analyzing, but these vibration characteristics can only be reflected by time-frequency spectrum, the spatial distribution of these micro vibrations can not be reconstructed precisely. Ghost imaging (GI), a novel imaging method also known as Coincidence Imaging that originated in the quantum and optical fields, can reconstruct unknown images using computational methods. To reconstruct the spatial distribution of micro vibrations, this paper proposes a new method based on a coincidence imaging system. A detailed model of target micro-vibration is created first, taking into account two categories: discrete and continuous targets. We use the first-order field correlation feature to obtain objective different micro vibration distribution based on the complex target models and time-frequency analysis in this work.      
### 30.CD and PMD Effect on Cyclostationarity-Based Timing Recovery for Optical Coherent Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2208.13951.pdf)
>  Timing recovery is critical for synchronizing the clocks at the transmitting and receiving ends of a digital coherent communication system. The core of timing recovery is to determine reliably the current sampling error of the local digitizer so that the timing circuit may lock to a stable operation point. Conventional timing phase detectors need to adapt to the optical fiber channel so that the common effects of this channel, such as chromatic dispersion (CD) and polarization mode dispersion (PMD), on the timing phase extraction must be understood. Here we exploit the cyclostationarity of the optical signal and derive a model for studying the CD and PMD effect. We prove that the CD-adjusted cyclic correlation matrix contains full information about timing and PMD, and the determinant of the matrix is a timing phase detector immune to both CD and PMD. We also obtain other results such as a completely PMD-independent CD estimator, etc. Our analysis is supported by both simulations and experiments over a field implemented optical cable.      
### 31.Classify Respiratory Abnormality in Lung Sounds Using STFT and a Fine-Tuned ResNet18 Network  [ :arrow_down: ](https://arxiv.org/pdf/2208.13943.pdf)
>  Recognizing patterns in lung sounds is crucial to detecting and monitoring respiratory diseases. Current techniques for analyzing respiratory sounds demand domain experts and are subject to interpretation. Hence an accurate and automatic respiratory sound classification system is desired. In this work, we took a data-driven approach to classify abnormal lung sounds. We compared the performance using three different feature extraction techniques, which are short-time Fourier transformation (STFT), Mel spectrograms, and Wav2vec, as well as three different classifiers, including pre-trained ResNet18, LightCNN, and Audio Spectrogram Transformer. Our key contributions include the bench-marking of different audio feature extractors and neural network based classifiers, and the implementation of a complete pipeline using STFT and a fine-tuned ResNet18 network. The proposed method achieved Harmonic Scores of 0.89, 0.80, 0.71, 0.36 for tasks 1-1, 1-2, 2-1 and 2-2, respectively on the testing sets in the IEEE BioCAS 2022 Grand Challenge on Respiratory Sound Classification.      
### 32.SB-SSL: Slice-Based Self-Supervised Transformers for Knee Abnormality Classification from MRI  [ :arrow_down: ](https://arxiv.org/pdf/2208.13923.pdf)
>  The availability of large scale data with high quality ground truth labels is a challenge when developing supervised machine learning solutions for healthcare domain. Although, the amount of digital data in clinical workflows is increasing, most of this data is distributed on clinical sites and protected to ensure patient privacy. Radiological readings and dealing with large-scale clinical data puts a significant burden on the available resources, and this is where machine learning and artificial intelligence play a pivotal role. Magnetic Resonance Imaging (MRI) for musculoskeletal (MSK) diagnosis is one example where the scans have a wealth of information, but require a significant amount of time for reading and labeling. Self-supervised learning (SSL) can be a solution for handling the lack of availability of ground truth labels, but generally requires a large amount of training data during the pretraining stage. Herein, we propose a slice-based self-supervised deep learning framework (SB-SSL), a novel slice-based paradigm for classifying abnormality using knee MRI scans. We show that for a limited number of cases (&lt;1000), our proposed framework is capable to identify anterior cruciate ligament tear with an accuracy of 89.17% and an AUC of 0.954, outperforming state-of-the-art without usage of external data during pretraining. This demonstrates that our proposed framework is suited for SSL in the limited data regime.      
### 33.A Language Agnostic Multilingual Streaming On-Device ASR System  [ :arrow_down: ](https://arxiv.org/pdf/2208.13916.pdf)
>  On-device end-to-end (E2E) models have shown improvements over a conventional model on English Voice Search tasks in both quality and latency. E2E models have also shown promising results for multilingual automatic speech recognition (ASR). In this paper, we extend our previous capacity solution to streaming applications and present a streaming multilingual E2E ASR system that runs fully on device with comparable quality and latency to individual monolingual models. To achieve that, we propose an Encoder Endpointer model and an End-of-Utterance (EOU) Joint Layer for a better quality and latency trade-off. Our system is built in a language agnostic manner allowing it to natively support intersentential code switching in real time. To address the feasibility concerns on large models, we conducted on-device profiling and replaced the time consuming LSTM decoder with the recently developed Embedding decoder. With these changes, we managed to run such a system on a mobile device in less than real time.      
### 34.A review of transcranial magnetic stimulation and Alzheimer's disease  [ :arrow_down: ](https://arxiv.org/pdf/2208.13905.pdf)
>  Since four decades ago, that the transcranial magnetic stimulation (TMS) technique was introduced, the increasing attention of neuroscience researchers and medical engineers has been focused on the development of this technique and its use to manage the treatment of a wide range of neurological conditions, including Alzheimer's disease. The ability of TMS, specifically a substantial type known as repetitive transcranial magnetic stimulation (rTMS) in changing the plasticity of the cortex, has been the most important feature that has created the hopes of controlling Alzheimer's disease with this technique more than ever.      
### 35.Investigation of A Biomass Gasification System Based on Energy and Exergy Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.13887.pdf)
>  Biomass gasification procedure is a very complex process and it is influenced by many physical and chemical factors such as biomass gasification temperature and gasifier type. Thermodynamic assessment methodology based on the energy and exergy analysis can be used to evaluate the system performance and environmental impacts. In this paper, thermodynamic analysis of the biomass gasification system is given for the whole system and its components. The parametric studies reveal the effects of design and operating indicators on the exergy efficiency and exergy destruction rate. The result shows that the gasification temperatures for the biomass gasification system change significantly with the type of the gasifying medium.      
### 36.Complex-Frequency Synchronization of Converter-Based Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13860.pdf)
>  In this paper, we study the phase-amplitude coupled dynamics in converter-based power systems from the complex-frequency perspective. A complex-frequency quantity represents the rate of change of the voltage amplitude and the phase angle by its real and imaginary parts, respectively. This emerging notion is of significance as it accommodates the multivariable characteristics of general power systems where active and reactive power are inherently coupled with both the voltage amplitude and phase. We propose the notion of complex-frequency synchronization to study the phase-amplitude coupled stability of a power system with grid-forming virtual oscillator-controlled converters. To do so, we formulate the system into a linear fast system and another linear slow system. This linearity property makes it tractable to analyze fast complex-frequency synchronization and slower voltage stabilization. From the perspective of complex-frequency synchronization, we provide novel insights into the equivalence of virtual oscillator control to complex-power-frequency droop control, the stability analysis methodologies, and the stability criteria. Our study provides a practical solution to address challenging stability issues in converter-dominated power systems.      
### 37.An Effective Technique for Increasing Capacity and Improving Bandwidth in 5G NB-IoT  [ :arrow_down: ](https://arxiv.org/pdf/2208.13849.pdf)
>  With hundreds of billions of the IoT connected devices, it is important for researchers to create effective resource management approach to satisfy the quality of service (QoS) requirements of 5th generation (5G) and beyond. Furthermore, wireless spectrum is increasingly scarce as demand for wireless services develops, demanding imaginative approaches to increase capacity within a limited spectral resource in order to meet service demands. In this article, the modified symbol time compression (M-STC) technique is suggested to paves the way for 5G networks and beyond to enhance the capacity and throughput. The M-STC method is a compressed signal waveform technique that increases the capacity by compressing the occupied bandwidth without increasing the complexity, losing data throughput or bit error rate (BER) performance. A comparative analysis is provided between the traditional orthogonal frequency division multiplexing (OFDM) system, OFDM using conventional symbol time compression (C-STC-OFDM) and OFDM using the proposed technique (M-STC-OFDM). The simulation results using Matlab-2021a show that the suggested method, M-STC-OFDM, drastically lowers the time needed for each OFDM signal by 75%. As a consequence, the M-STC-OFDM system decreases bandwidth (BW) by 75% when compared to a standard OFDM system (BW_OFDM = 180 kHz and BW_M-STC-OFDM = 45 kHz), while the C-STC-OFDM system reduces BW by 50% (BW_C-STC-OFDM = 90 kHz). Furthermore, using the M-STC-OFDM system reduces peak to average-power-ratio (PAPR) by 2.09 dB when compared to the standard OFDM system and 1.18 dB when compared to C-STC-OFDM with no BER deterioration. Moreover, as compared to the 16QAM-OFDM system, the proposed M-STC-OFDM system reduces the signal-to-noise-ratio (SNR) by 3.8 dB to transmit the same amount of data.      
### 38.Provably Stabilizing Model-Free Q-Learning for Unknown Bilinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13843.pdf)
>  In this paper, we present a provably convergent Model-Free ${Q}$-Learning algorithm that learns a stabilizing control policy for an unknown Bilinear System from a single online run. Given an unknown bilinear system, we study the interplay between its equivalent control-affine linear time-varying and linear time-invariant representations to derive i) from Pontryagin's Minimum Principle, a pair of point-to-point model-free policy improvement and evaluation laws that iteratively solves for an optimal state-dependent control policy; and ii) the properties under which the state-input data is sufficient to characterize system behavior in a model-free manner. We demonstrate the performance of the proposed algorithm via illustrative numerical examples and compare it to the model-based case.      
### 39.Dynamic Calibration of Nonlinear Sensors with Time-Drifts and Delays by Bayesian Inference  [ :arrow_down: ](https://arxiv.org/pdf/2208.13819.pdf)
>  Most sensor calibrations rely on the linearity and steadiness of their response characteristics, but practical sensors are nonlinear, and their response drifts with time, restricting their choices for adoption. To broaden the realm of sensors to allow nonlinearity and time-drift in the underlying dynamics, a Bayesian inference-based nonlinear, non-causal dynamic calibration method is introduced, where the sensed value is estimated as a posterior conditional mean given a finite-length sequence of the sensor measurements and the elapsed time. Additionally, an algorithm is proposed to adjust an already learned calibration map online whenever new data arrives. The effectiveness of the proposed method is validated on continuous-glucose-monitoring (CGM) data from an alive rat equipped with an in-house optical glucose sensor. To allow flexibility in choice, the validation is also performed on a synthetic blood glucose level (BGL) dataset generated using FDA-approved virtual diabetic patient models together with an illustrative CGM sensor model.      
### 40.Verifiable Obstacle Detection  [ :arrow_down: ](https://arxiv.org/pdf/2208.14403.pdf)
>  Perception of obstacles remains a critical safety concern for autonomous vehicles. Real-world collisions have shown that the autonomy faults leading to fatal collisions originate from obstacle existence detection. Open source autonomous driving implementations show a perception pipeline with complex interdependent Deep Neural Networks. These networks are not fully verifiable, making them unsuitable for safety-critical tasks. <br>In this work, we present a safety verification of an existing LiDAR based classical obstacle detection algorithm. We establish strict bounds on the capabilities of this obstacle detection algorithm. Given safety standards, such bounds allow for determining LiDAR sensor properties that would reliably satisfy the standards. Such analysis has as yet been unattainable for neural network based perception systems. We provide a rigorous analysis of the obstacle detection system with empirical results based on real-world sensor data.      
### 41.Expert Opinion Elicitation for Assisting Deep Learning based Lyme Disease Classifier with Patient Data  [ :arrow_down: ](https://arxiv.org/pdf/2208.14384.pdf)
>  Diagnosing erythema migrans (EM) skin lesion, the most common early symptom of Lyme disease using deep learning techniques can be effective to prevent long-term complications. Existing works on deep learning based EM recognition only utilizes lesion image due to the lack of a dataset of Lyme disease related images with associated patient data. Physicians rely on patient information about the background of the skin lesion to confirm their diagnosis. In order to assist the deep learning model with a probability score calculated from patient data, this study elicited opinion from fifteen doctors. For the elicitation process, a questionnaire with questions and possible answers related to EM was prepared. Doctors provided relative weights to different answers to the questions. We converted doctors evaluations to probability scores using Gaussian mixture based density estimation. For elicited probability model validation, we exploited formal concept analysis and decision tree. The elicited probability scores can be utilized to make image based deep learning Lyme disease pre-scanners robust.      
### 42.Dead-beat model predictive control for discrete-time linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.14372.pdf)
>  In this paper, model predictive control (MPC) strategies are proposed for dead-beat control of linear systems with and without state and control constraints. In unconstrained MPC, deadbeat performance can be guaranteed by setting the control horizon to the system dimension, and adding an terminal equality constraint. It is proved that the unconstrained deadbeat MPC is equivalent to linear deadbeat control. The proposed constrained deadbeat MPC is designed by setting the control horizon equal to the system dimension and penalizing only the terminal cost. The recursive feasibility and deadbeat performance are proved theoretically.      
### 43.Towards robust music source separation on loud commercial music  [ :arrow_down: ](https://arxiv.org/pdf/2208.14355.pdf)
>  Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process. Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data.      
### 44.Ergodic Secrecy Rate of Optimal Source-Destination Pair Selection in Frequency-Selective Fading  [ :arrow_down: ](https://arxiv.org/pdf/2208.14348.pdf)
>  Node selection is a simple technique to achieve diversity and thereby enhance the physical layer security in future wireless communication systems which require low complexity. High-speed data transmission often encounters frequency selective fading. In this context, we evaluate the exact closed-form expression for the ergodic secrecy rate (ESR) of the optimal source-destination pair selection scheme with single-carrier cyclic-prefix modulation, where the destination and eavesdropper channels both exhibit independent frequency selective fading with an arbitrary number of multipath components. A simplified analysis in the high-SNR scenario along with an asymptotic analysis is also provided. We also derive and compare the corresponding results for the sub-optimal source-destination pair selection scheme. We show that our analysis produces the corresponding ESR results under narrowband independent Nakagami-$m$ fading channel with any arbitrary integer parameter $m$. The effect of transmitters, destination and eavesdropping paths correlation on the ESR is also demonstrated. Our solution approach is general and can be used to find the ESR of a wider variety of transmitter selection schemes.      
### 45.MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.14345.pdf)
>  Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc.      
### 46.HPPNet: Modeling the Harmonic Structure and Pitch Invariance in Piano Transcription  [ :arrow_down: ](https://arxiv.org/pdf/2208.14339.pdf)
>  While neural network models are making significant progress in piano transcription, they are becoming more resource-consuming due to requiring larger model size and more computing power. In this paper, we attempt to apply more prior about piano to reduce model size and improve the transcription performance. The sound of a piano note contains various overtones, and the pitch of a key does not change over time. To make full use of such latent information, we propose HPPNet that using the Harmonic Dilated Convolution to capture the harmonic structures and the Frequency Grouped Recurrent Neural Network to model the pitch-invariance over time. Experimental results on the MAESTRO dataset show that our piano transcription system achieves state-of-the-art performance both in frame and note scores (frame F1 93.15%, note F1 97.18%). Moreover, the model size is much smaller than the previous state-of-the-art deep learning models.      
### 47.Distributed Ensembles of Reinforcement Learning Agents for Electricity Control  [ :arrow_down: ](https://arxiv.org/pdf/2208.14338.pdf)
>  Deep Reinforcement Learning (or just "RL") is gaining popularity for industrial and research applications. However, it still suffers from some key limits slowing down its widespread adoption. Its performance is sensitive to initial conditions and non-determinism. To unlock those challenges, we propose a procedure for building ensembles of RL agents to efficiently build better local decisions toward long-term cumulated rewards. For the first time, hundreds of experiments have been done to compare different ensemble constructions procedures in 2 electricity control environments. We discovered an ensemble of 4 agents improves accumulated rewards by 46%, improves reproducibility by a factor of 3.6, and can naturally and efficiently train and predict in parallel on GPUs and CPUs.      
### 48.Optimal Power Allocation for Integrated Visible Light Positioning and Communication System with a Single LED-Lamp  [ :arrow_down: ](https://arxiv.org/pdf/2208.14268.pdf)
>  In this paper, we investigate an integrated visible light positioning and communication (VLPC) system with a single LED-lamp. First, by leveraging the fact that the VLC channel model is a function of the receiver's location, we propose a system model that estimates the channel state information (CSI) based on the positioning information without transmitting pilot sequences. Second, we derive the Cramer-Rao lower bound (CRLB) on the positioning error variance and a lower bound on the achievable rate with on-off keying modulation. Third, based on the derived performance metrics, we optimize the power allocation to minimize the CRLB, while satisfying the rate outage probability constraint. To tackle this non-convex optimization problem, we apply the worst-case distribution of the Conditional Value-at-Risk (CVaR) and the block coordinate descent (BCD) methods to obtain the feasible solutions. Finally, the effects of critical system parameters, such as outage probability, rate threshold, total power threshold, are revealed by numerical results.      
### 49.RIS-Aided Multiuser MIMO-OFDM with Linear Precoding and Iterative Detection: Analysis and Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2208.14259.pdf)
>  In this paper, we consider a reconfigurable intelligence surface (RIS) aided uplink multiuser multi-input multi-output (MIMO) orthogonal frequency division multiplexing (OFDM) system, where the receiver is assumed to conduct low-complexity iterative detection. We aim to minimize the total transmit power by jointly designing the precoder of the transmitter and the passive beamforming of the RIS. This problem can be tackled from the perspective of information theory. But this information-theoretic approach may involve prohibitively high complexity since the number of rate constraints that specify the capacity region of the uplink multiuser channel is exponential in the number of users. To avoid this difficulty, we formulate the design problem of the iterative receiver under the constraints of a maximal iteration number and target bit error rates of users. To tackle this challenging problem, we propose a groupwise successive interference cancellation (SIC) optimization approach, where the signals of users are decoded and cancelled in a group-by-group manner. We present a heuristic user grouping strategy, and resort to the alternating optimization technique to iteratively solve the precoding and passive beamforming sub-problems. Specifically, for the precoding sub-problem, we employ fractional programming to convert it to a convex problem; for the passive beamforming sub-problem, we adopt successive convex approximation to deal with the unit-modulus constraints of the RIS. We show that the proposed groupwise SIC approach has significant advantages in both performance and computational complexity, as compared with the counterpart approaches.      
### 50.Robust Quantum Control: Analysis &amp; Synthesis via Averaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.14193.pdf)
>  An approach is presented for robustness analysis and quantum (unitary) control synthesis based on the classic method of averaging. The result is a multicriterion optimization competing the nominal (uncertainty-free) fidelity with a well known robustness measure: the size of an interaction (error) Hamiltonian, essentially the first term in the Magnus expansion of an interaction unitary. Combining this with the fact that the topology of the control landscape at high fidelity is determined by the null space of the nominal fidelity Hessian, we arrive at a new two-stage algorithm. Once the nominal fidelity is sufficiently high, we approximate both the nominal fidelity and robustness measure as quadratics in the control increments. An optimal solution is obtained by solving a convex optimization for the control increments at each iteration to keep the nominal fidelity high and reduce the robustness measure. Additionally, by separating fidelity from the robustness measure, more flexibility is available for uncertainty modeling.      
### 51.Optimal Management of Renewable Generation and Uncertain Demand with Reverse Fuel Cells by Stochastic Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2208.14163.pdf)
>  This paper proposes a control strategy for a Reverse Fuel Cell used to manage a Renewable Energy Community. A two-stage scenario-based Model Predictive Control algorithm is designed to define the best economic strategy to be followed during operation. Renewable energy generation and users' demand are forecasted by a suitably defined Discrete Markov Chain based method. The control algorithm is able to take into account the uncertainties of forecasts and the nonlinear behaviour of the Reversible Fuel Cell. The performance of proposed approach is tested on a Renewable Energy Community composed by an aggregation of industrial buildings equipped with PV.      
### 52.Treating Point Cloud as Moving Camera Videos: A No-Reference Quality Assessment Metric  [ :arrow_down: ](https://arxiv.org/pdf/2208.14085.pdf)
>  Point cloud is one of the most widely used digital representation formats for 3D contents, the visual quality of which may suffer from noise and geometric shift during the production procedure as well as compression and downsampling during the transmission process. To tackle the challenge of point cloud quality assessment (PCQA), many PCQA methods have been proposed to evaluate the visual quality levels of point clouds by assessing the rendered static 2D projections. Although such projection-based PCQA methods achieve competitive performance with the assistance of mature image quality assessment (IQA) methods, they neglect the dynamic quality-aware information, which does not fully match the fact that observers tend to perceive the point clouds through both static and dynamic views. Therefore, in this paper, we treat the point clouds as moving camera videos and explore the way of dealing with PCQA tasks via using video quality assessment (VQA) methods in a no-reference (NR) manner. First, we generate the captured videos by rotating the camera around the point clouds through four circular pathways. Then we extract both spatial and temporal quality-aware features from the selected key frames and the video clips by using trainable 2D-CNN and pre-trained 3D-CNN models respectively. Finally, the visual quality of point clouds is represented by the regressed video quality values. The experimental results reveal that the proposed method is effective for predicting the visual quality levels of the point clouds and even competitive with full-reference (FR) PCQA methods. The ablation studies further verify the rationality of the proposed framework and confirm the contributions made by the quality-aware features extracted from dynamic views.      
### 53.Gridless 3D Recovery of Image Sources from Room Impulse Responses  [ :arrow_down: ](https://arxiv.org/pdf/2208.14017.pdf)
>  Given a sound field generated by a sparse distribution of impulse image sources, can the continuous 3D positions and amplitudes of these sources be recovered from discrete, bandlimited measurements of the field at a finite set of locations, e.g., a multichannel room impulse response? Borrowing from recent advances in super-resolution imaging, it is shown that this nonlinear, non-convex inverse problem can be efficiently relaxed into a convex linear inverse problem over the space of Radon measures in R3. The linear operator introduced here stems from the fundamental solution of the free-field inhomogenous wave equation combined with the receivers' responses. An adaptation of the Sliding Frank-Wolfe algorithm is proposed to numerically solve the problem off-the-grid, i.e., in continuous 3D space. Simulated experiments show that the approach achieves near-exact recovery of hundreds of image sources using an arbitrarily placed compact 32-channel spherical microphone array in random rectangular rooms. The impact of noise, sampling rate and array diameter on these results is also examined.      
### 54.Finding neural signatures for obesity using source-localized EEG features  [ :arrow_down: ](https://arxiv.org/pdf/2208.14007.pdf)
>  Obesity is a serious issue in the modern society since it associates to a significantly reduced quality of life. Current research conducted to explore the obesity-related neurological evidences using electroencephalography (EEG) data are limited to traditional approaches. In this study, we developed a novel machine learning model to identify brain networks of obese females using alpha band functional connectivity features derived from EEG data. An overall classification accuracy of 90% is achieved. Our finding suggests that the obese brain is characterized by a dysfunctional network in which the areas that are responsible for processing self-referential information such as energy requirement are impaired.      
### 55.Virtual impactor-based label-free bio-aerosol detection using holography and deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.13979.pdf)
>  Exposure to bio-aerosols such as mold spores and pollen can lead to adverse health effects. There is a need for a portable and cost-effective device for long-term monitoring and quantification of various bio-aerosols. To address this need, we present a mobile and cost-effective label-free bio-aerosol sensor that takes holographic images of flowing particulate matter concentrated by a virtual impactor, which selectively slows down and guides particles larger than ~6 microns to fly through an imaging window. The flowing particles are illuminated by a pulsed laser diode, casting their inline holograms on a CMOS image sensor in a lens-free mobile imaging device. The illumination contains three short pulses with a negligible shift of the flowing particle within one pulse, and triplicate holograms of the same particle are recorded at a single frame before it exits the imaging field-of-view, revealing different perspectives of each particle. The particles within the virtual impactor are localized through a differential detection scheme, and a deep neural network classifies the aerosol type in a label-free manner, based on the acquired holographic images. We demonstrated the success of this mobile bio-aerosol detector with a virtual impactor using different types of pollen (i.e., bermuda, elm, oak, pine, sycamore, and wheat) and achieved a blind classification accuracy of 92.91%. This mobile and cost-effective device weighs ~700 g and can be used for label-free sensing and quantification of various bio-aerosols over extended periods since it is based on a cartridge-free virtual impactor that does not capture or immobilize particulate matter.      
### 56.Joint Resource Allocation and Configuration Design for STAR-RIS-Enhanced Wireless-Powered MEC  [ :arrow_down: ](https://arxiv.org/pdf/2208.13970.pdf)
>  In this paper, a novel concept called simultaneously transmitting and reflecting RIS (STAR-RIS) is introduced into the wireless-powered mobile edge computing (MEC) systems to improve the efficiency of energy transfer and task offloading. Compared with traditional reflecting-only RIS, STAR-RIS extends the half-space coverage to full-space coverage by simultaneously transmitting and reflecting incident signals, and also provides new degrees-of-freedom (DoFs) for manipulating signal propagation. We aim to maximize the total computation rate of all users, where the energy transfer time, transmit power and CPU frequencies of users, and the configuration design of STAR-RIS are jointly optimized. Considering the characteristics of STAR-RIS, three operating protocols, namely energy splitting (ES), mode switching (MS), and time splitting (TS) are studied, respectively. For the ES protocol, based on the penalty method, successive convex approximation (SCA), and the linear search method, an iterative algorithm is proposed to solve the formulated non-convex problem. Then, the proposed algorithm for ES protocol is extended to solve the MS and TS problems. Simulation results illustrate that the STAR-RIS outperforms traditional reflecting/transmitting-only RIS. More importantly, the TS protocol can achieve the largest computation rate among the three operating protocols of STAR-RIS.      
### 57.Video-based Cross-modal Auxiliary Network for Multimodal Sentiment Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.13954.pdf)
>  Multimodal sentiment analysis has a wide range of applications due to its information complementarity in multimodal interactions. Previous works focus more on investigating efficient joint representations, but they rarely consider the insufficient unimodal features extraction and data redundancy of multimodal fusion. In this paper, a Video-based Cross-modal Auxiliary Network (VCAN) is proposed, which is comprised of an audio features map module and a cross-modal selection module. The first module is designed to substantially increase feature diversity in audio feature extraction, aiming to improve classification accuracy by providing more comprehensive acoustic representations. To empower the model to handle redundant visual features, the second module is addressed to efficiently filter the redundant visual frames during integrating audiovisual data. Moreover, a classifier group consisting of several image classification networks is introduced to predict sentiment polarities and emotion categories. Extensive experimental results on RAVDESS, CMU-MOSI, and CMU-MOSEI benchmarks indicate that VCAN is significantly superior to the state-of-the-art methods for improving the classification accuracy of multimodal sentiment analysis.      
### 58.Finite Sample Identification of Bilinear Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.13915.pdf)
>  Bilinear dynamical systems are ubiquitous in many different domains and they can also be used to approximate more general control-affine systems. This motivates the problem of learning bilinear systems from a single trajectory of the system's states and inputs. Under a mild marginal mean-square stability assumption, we identify how much data is needed to estimate the unknown bilinear system up to a desired accuracy with high probability. Our sample complexity and statistical error rates are optimal in terms of the trajectory length, the dimensionality of the system and the input size. Our proof technique relies on an application of martingale small-ball condition. This enables us to correctly capture the properties of the problem, specifically our error rates do not deteriorate with increasing instability. Finally, we show that numerical experiments are well-aligned with our theoretical results.      
