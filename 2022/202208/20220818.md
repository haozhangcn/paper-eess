# ArXiv eess --Thu, 18 Aug 2022
### 1.Computationally Efficient Robust Model Predictive Control for Uncertain System Using Causal State-Feedback Parameterization  [ :arrow_down: ](https://arxiv.org/pdf/2208.08431.pdf)
>  This paper investigates the problem of robust model predictive control (RMPC) of linear-time-invariant (LTI) discrete-time systems subject to structured uncertainty and bounded disturbances. Typically, the constrained RMPC problem with state-feedback parameterizations is nonlinear (and nonconvex) with a prohibitively high computational burden for online implementation. To remedy this, a novel approach is proposed to linearize the state-feedback RMPC problem, with minimal conservatism, through the use of semidefinite relaxation techniques. The proposed algorithm computes the state-feedback gain and perturbation online by solving a linear matrix inequality (LMI) optimization that, in comparison to other schemes in the literature is shown to have a substantially reduced computational burden without adversely affecting the tracking performance of the controller. Additionally, an offline strategy that provides initial feasibility on the RMPC problem is presented. The effectiveness of the proposed scheme is demonstrated through numerical examples from the literature.      
### 2.Temporal Relaxation of Signal Temporal Logic Specifications for Resilient Control Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2208.08384.pdf)
>  We introduce a metric that can quantify the temporal relaxation of Signal Temporal Logic (STL) specifications and facilitate resilient control synthesis in the face of infeasibilities. The proposed metric quantifies a cumulative notion of relaxation among the subtasks, and minimizing it yields to structural changes in the original STL specification by i) modifying time-intervals, ii) removing subtasks entirely if needed. To this end, we formulate an optimal control problem that extracts state and input sequences by minimally violating the temporal requirements while achieving the desired predicates. We encode this problem in the form of a computationally efficient mixed-integer program. We show some theoretical results on the properties of the new metric. Finally, we present a case study of a robot that minimally violates the time constraints of desired tasks in the face of an infeasibility.      
### 3.Energy-grade double pricing mechanism for a combined heat and power system using the asynchronous dispatch method  [ :arrow_down: ](https://arxiv.org/pdf/2208.08370.pdf)
>  The problem of heat and electricity pricing in combined heat and power systems regarding the time scales of electricity and heat, as well as thermal energy quality, is studied. Based on the asynchronous coordinated dispatch of the combined heat and power system, an energy-grade double pricing mechanism is proposed. Under the pricing mechanism, the resulting merchandise surplus of the heat system operator at each heat dispatch interval can be decomposed into interpretable parts and its revenue adequacy can be guaranteed for all heat dispatch intervals. And the electric power system operator's resulting merchandise surplus is composed of non-negative components at each electricity dispatch interval, also ensuring its revenue adequacy. In addition, the effects of different time scales and cogeneration are analyzed in different kinds of combined heat and power units' pricing.      
### 4.FCN-Transformer Feature Fusion for Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2208.08352.pdf)
>  Colonoscopy is widely recognised as the gold standard procedure for the early detection of colorectal cancer (CRC). Segmentation is valuable for two significant clinical applications, namely lesion detection and classification, providing means to improve accuracy and robustness. The manual segmentation of polyps in colonoscopy images is time-consuming. As a result, the use of deep learning (DL) for automation of polyp segmentation has become important. However, DL-based solutions can be vulnerable to overfitting and the resulting inability to generalise to images captured by different colonoscopes. Recent transformer-based architectures for semantic segmentation both achieve higher performance and generalise better than alternatives, however typically predict a segmentation map of $\frac{h}{4}\times\frac{w}{4}$ spatial dimensions for a $h\times w$ input image. To this end, we propose a new architecture for full-size segmentation which leverages the strengths of a transformer in extracting the most important features for segmentation in a primary branch, while compensating for its limitations in full-size prediction with a secondary fully convolutional branch. The resulting features from both branches are then fused for final prediction of a $h\times w$ segmentation map. We demonstrate our method's state-of-the-art performance with respect to the mDice, mIoU, mPrecision, and mRecall metrics, on both the Kvasir-SEG and CVC-ClinicDB dataset benchmarks. Additionally, we train the model on each of these datasets and evaluate on the other to demonstrate its superior generalisation performance.      
### 5.Transferability limitations for Covid 3D Localization Using SARS-CoV-2 segmentation models in 4D CT images  [ :arrow_down: ](https://arxiv.org/pdf/2208.08343.pdf)
>  In this paper, we investigate the transferability limitations when using deep learning models, for semantic segmentation of pneumonia-infected areas in CT images. The proposed approach adopts a 4 channel input; 3 channels based on Hounsfield scale, plus one channel (binary) denoting the lung area. We used 3 different, publicly available, CT datasets. If the lung area mask was not available, a deep learning model generates a proxy image. Experimental results suggesting that transferability should be used carefully, when creating Covid segmentation models; retraining the model more than one times in large sets of data results in a decrease in segmentation accuracy.      
### 6.Leukocyte Classification using Multimodal Architecture Enhanced by Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2208.08331.pdf)
>  Recently, a lot of automated white blood cells (WBC) or leukocyte classification techniques have been developed. However, all of these methods only utilize a single modality microscopic image i.e. either blood smear or fluorescence based, thus missing the potential of a better learning from multimodal images. In this work, we develop an efficient multimodal architecture based on a first of its kind multimodal WBC dataset for the task of WBC classification. Specifically, our proposed idea is developed in two steps - 1) First, we learn modality specific independent subnetworks inside a single network only; 2) We further enhance the learning capability of the independent subnetworks by distilling knowledge from high complexity independent teacher networks. With this, our proposed framework can achieve a high performance while maintaining low complexity for a multimodal dataset. Our unique contribution is two-fold - 1) We present a first of its kind multimodal WBC dataset for WBC classification; 2) We develop a high performing multimodal architecture which is also efficient and low in complexity at the same time.      
### 7.Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2208.08315.pdf)
>  We propose Video-TransUNet, a deep architecture for instance segmentation in medical CT videos constructed by integrating temporal feature blending into the TransUNet deep learning framework. In particular, our approach amalgamates strong frame representation via a ResNet CNN backbone, multi-frame feature blending via a Temporal Context Module (TCM), non-local attention via a Vision Transformer, and reconstructive capabilities for multiple targets via a UNet-based convolutional-deconvolutional architecture with multiple heads. We show that this new network design can significantly outperform other state-of-the-art systems when tested on the segmentation of bolus and pharynx/larynx in Videofluoroscopic Swallowing Study (VFSS) CT sequences. On our VFSS2022 dataset it achieves a dice coefficient of $0.8796\%$ and an average surface distance of $1.0379$ pixels. Note that tracking the pharyngeal bolus accurately is a particularly important application in clinical practice since it constitutes the primary method for diagnostics of swallowing impairment. Our findings suggest that the proposed model can indeed enhance the TransUNet architecture via exploiting temporal information and improving segmentation performance by a significant margin. We publish key source code, network weights, and ground truth annotations for simplified performance reproduction.      
### 8.Metal artifact correction in cone beam computed tomography using synthetic X-ray data  [ :arrow_down: ](https://arxiv.org/pdf/2208.08288.pdf)
>  Metal artifact correction is a challenging problem in cone beam computed tomography (CBCT) scanning. Metal implants inserted into the anatomy cause severe artifacts in reconstructed images. Widely used inpainting-based metal artifact reduction (MAR) methods require segmentation of metal traces in the projections as a first step which is a challenging task. One approach is to use a deep learning method to segment metals in the projections. However, the success of deep learning methods is limited by the availability of realistic training data. It is challenging and time consuming to get reliable ground truth annotations due to unclear implant boundary and large number of projections. We propose to use X-ray simulations to generate synthetic metal segmentation training dataset from clinical CBCT scans. We compare the effect of simulations with different number of photons and also compare several training strategies to augment the available data. We compare our model's performance on real clinical scans with conventional threshold-based MAR and a recent deep learning method. We show that simulations with relatively small number of photons are suitable for the metal segmentation task and that training the deep learning model with full size and cropped projections together improves the robustness of the model. We show substantial improvement in the image quality affected by severe motion, voxel size under-sampling, and out-of-FOV metals. Our method can be easily implemented into the existing projection-based MAR pipeline to get improved image quality. This method can provide a novel paradigm to accurately segment metals in CBCT projections.      
### 9.Novel Deep Learning Approach to Derive Cytokeratin Expression and Epithelium Segmentation from DAPI  [ :arrow_down: ](https://arxiv.org/pdf/2208.08284.pdf)
>  Generative Adversarial Networks (GANs) are state of the art for image synthesis. Here, we present dapi2ck, a novel GAN-based approach to synthesize cytokeratin (CK) staining from immunofluorescent (IF) DAPI staining of nuclei in non-small cell lung cancer (NSCLC) images. We use the synthetic CK to segment epithelial regions, which, compared to expert annotations, yield equally good results as segmentation on stained CK. Considering the limited number of markers in a multiplexed IF (mIF) panel, our approach allows to replace CK by another marker addressing the complexity of the tumor micro-environment (TME) to facilitate patient selection for immunotherapies. In contrast to stained CK, dapi2ck does not suffer from issues like unspecific CK staining or loss of tumoral CK expression.      
### 10.Safety Assessment for Autonomous System Perception Capabilities  [ :arrow_down: ](https://arxiv.org/pdf/2208.08237.pdf)
>  Autonomous Systems (AS) are being increasingly proposed, or used, in Safety Critical (SC) applications, e.g., road vehicles. Many such systems make use of sophisticated sensor suites and processing to provide scene understanding which informs the AS' decision-making, e.g., path planning. The sensor processing typically makes use of Machine Learning (ML) and has to work in challenging environments, further the ML algorithms have known limitations, e.g., the possibility of false negatives or false positives in object classification. The well-established safety analysis methods developed for conventional SC systems are not well-matched to AS, ML, or the sensing systems used by AS. This paper proposes an adaptation of well-established safety analysis methods to address the specifics of sensing systems for AS, including addressing environmental effects and the potential failure modes of ML, and provides a rationale for choosing particular sets of guide words, or prompts, for safety analysis. It goes on to show how the results of the analysis can be used to inform the design and verification of the AS system and illustrates the new method by presenting a partial analysis of a mobile robot. The illustrations in the paper are primarily based on optical sensing, however the paper discusses the applicability of the method to other sensing modalities and its role in a wider safety process addressing the overall capabilities of AS      
### 11.Auto-segmentation of Hip Joints using MultiPlanar UNet with Transfer learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.08226.pdf)
>  Accurate geometry representation is essential in developing finite element models. Although generally good, deep-learning segmentation approaches with only few data have difficulties in accurately segmenting fine features, e.g., gaps and thin structures. Subsequently, segmented geometries need labor-intensive manual modifications to reach a quality where they can be used for simulation purposes. We propose a strategy that uses transfer learning to reuse datasets with poor segmentation combined with an interactive learning step where fine-tuning of the data results in anatomically accurate segmentations suitable for simulations. We use a modified MultiPlanar UNet that is pre-trained using inferior hip joint segmentation combined with a dedicated loss function to learn the gap regions and post-processing to correct tiny inaccuracies on symmetric classes due to rotational invariance. We demonstrate this robust yet conceptually simple approach applied with clinically validated results on publicly available computed tomography scans of hip joints. Code and resulting 3D models are available at: \url{<a class="link-external link-https" href="https://github.com/MICCAI2022-155/AuToSeg" rel="external noopener nofollow">this https URL</a>}      
### 12.Estimating Sparse Sources from Data Mixtures using Maxima in Phase Space Plots  [ :arrow_down: ](https://arxiv.org/pdf/2208.08210.pdf)
>  In Blind Source Separation (BSS), one estimates sources from data mixtures where the mixing coefficients are unknown. In the particular case of Sparse Component Analysis (SCA), each underlying source exists for only a finite amount of time when other sources are negligible. In this paper, one approach to SCA is presented where the data are represented using phase space analysis and one estimates the main source from the maximum in the phase plot. Deflation is used to estimate the other sources. The proposed method is tested on simulated data and experimental ECG data taken from an expectant mother. It is shown that, in most cases, the performance of the proposed method is comparable to that of Principal Component Analysis (PCA) and FastICA for clean data. In the case of noisy data, PCA is found to be more robust for higher noise levels. For situations where the sources have coincident peaks, the method breaks down as expected, as the maximum in the phase plot does not correspond to an individual source.      
### 13.Evaluation of 3D GANs for Lung Tissue Modelling in Pulmonary CT  [ :arrow_down: ](https://arxiv.org/pdf/2208.08184.pdf)
>  GANs are able to model accurately the distribution of complex, high-dimensional datasets, e.g. images. This makes high-quality GANs useful for unsupervised anomaly detection in medical imaging. However, differences in training datasets such as output image dimensionality and appearance of semantically meaningful features mean that GAN models from the natural image domain may not work `out-of-the-box' for medical imaging, necessitating re-implementation and re-evaluation. In this work we adapt and evaluate three GAN models to the task of modelling 3D healthy image patches for pulmonary CT. To the best of our knowledge, this is the first time that such an evaluation has been performed. The DCGAN, styleGAN and the bigGAN architectures were investigated due to their ubiquity and high performance in natural image processing. We train different variants of these methods and assess their performance using the FID score. In addition, the quality of the generated images was evaluated by a human observer study, the ability of the networks to model 3D domain-specific features was investigated, and the structure of the GAN latent spaces was analysed. Results show that the 3D styleGAN produces realistic-looking images with meaningful 3D structure, but suffer from mode collapse which must be addressed during training to obtain samples diversity. Conversely, the 3D DCGAN models show a greater capacity for image variability, but at the cost of poor-quality images. The 3D bigGAN models provide an intermediate level of image quality, but most accurately model the distribution of selected semantically meaningful features. The results suggest that future development is required to realise a 3D GAN with sufficient capacity for patch-based lung CT anomaly detection and we offer recommendations for future areas of research, such as experimenting with other architectures and incorporation of position-encoding.      
### 14.Expressivity of Hidden Markov Chains vs. Recurrent Neural Networks from a system theoretic viewpoint  [ :arrow_down: ](https://arxiv.org/pdf/2208.08175.pdf)
>  Hidden Markov Chains (HMC) and Recurrent Neural Networks (RNN) are two well known tools for predicting time series. Even though these solutions were developed independently in distinct communities, they share some similarities when considered as probabilistic structures. So in this paper we first consider HMC and RNN as generative models, and we embed both structures in a common generative unified model (GUM). We next address a comparative study of the expressivity of these models. To that end we assume that the models are furthermore linear and Gaussian. The probability distributions produced by these models are characterized by structured covariance series, and as a consequence expressivity reduces to comparing sets of structured covariance series, which enables us to call for stochastic realization theory (SRT). We finally provide conditions under which a given covariance series can be realized by a GUM, an HMC or an RNN.      
### 15.A Monotonicity Constrained Attention Module for Emotion Classification with Limited EEG Data  [ :arrow_down: ](https://arxiv.org/pdf/2208.08155.pdf)
>  In this work, a parameter-efficient attention module is presented for emotion classification using a limited, or relatively small, number of electroencephalogram (EEG) signals. This module is called the Monotonicity Constrained Attention Module (MCAM) due to its capability of incorporating priors on the monotonicity when converting features' Gram matrices into attention matrices for better feature refinement. Our experiments have shown that MCAM's effectiveness is comparable to state-of-the-art attention modules in boosting the backbone network's performance in prediction while requiring less parameters. Several accompanying sensitivity analyses on trained models' prediction concerning different attacks are also performed. These attacks include various frequency domain filtering levels and gradually morphing between samples associated with multiple labels. Our results can help better understand different modules' behaviour in prediction and can provide guidance in applications where data is limited and are with noises.      
### 16.Implementation of Multi-channel Active Noise Control based on Back-propagation Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2208.08086.pdf)
>  Active noise control (ANC) systems can efficiently attenuate low-frequency noises by introducing anti-noises to combine with the unwanted noises. In ANC systems, the filtered-x least mean square (FxLMS) and filtered-X normalized least-mean-square (FxNLMS) algorithm are well-known algorithms for adaptively adjusting control filters. Multi-channel ANC systems are typically required to attenuate unwanted noises in a large space. However, open-source implementations of the multi-channel FxLMS (McFxLMS) and multi-channel FxNLMS (McFxNLMS) algorithm continue to be scarce. Therefore, this paper proposes a simple and effective implementation approach of the McFxLMS and McFxNLMS algorithm. Motivated by the back-propagation process during neural network training, the McFxLMS and McFxNLMS algorithm can be implemented via automatic derivation mechanism. We implemented the two algorithms using the automatic derivation mechanism in PyTorch and made the source code available on GitHub. This implementation method can improve the practicality of multi-channel ANC systems, which is expected to be widely used in ANC applications.      
### 17.A Hybrid SFANC-FxNLMS Algorithm for Active Noise Control based on Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.08082.pdf)
>  The selective fixed-filter active noise control (SFANC) method selecting the best pre-trained control filters for various types of noise can achieve a fast response time. However, it may lead to large steady-state errors due to inaccurate filter selection and the lack of adaptability. In comparison, the filtered-X normalized least-mean-square (FxNLMS) algorithm can obtain lower steady-state errors through adaptive optimization. Nonetheless, its slow convergence has a detrimental effect on dynamic noise attenuation. Therefore, this paper proposes a hybrid SFANC-FxNLMS approach to overcome the adaptive algorithm's slow convergence and provide a better noise reduction level than the SFANC method. A lightweight one-dimensional convolutional neural network (1D CNN) is designed to automatically select the most suitable pre-trained control filter for each frame of the primary noise. Meanwhile, the FxNLMS algorithm continues to update the coefficients of the chosen pre-trained control filter at the sampling rate. Owing to the effective combination of the two algorithms, experimental results show that the hybrid SFANC-FxNLMS algorithm can achieve a rapid response time, a low noise reduction error, and a high degree of robustness.      
### 18.REGAS: REspiratory-GAted Synthesis of Views for Multi-Phase CBCT Reconstruction from a single 3D CBCT Acquisition  [ :arrow_down: ](https://arxiv.org/pdf/2208.08048.pdf)
>  It is a long-standing challenge to reconstruct Cone Beam Computed Tomography (CBCT) of the lung under respiratory motion. This work takes a step further to address a challenging setting in reconstructing a multi-phase}4D lung image from just a single}3D CBCT acquisition. To this end, we introduce REpiratory-GAted Synthesis of views, or REGAS. REGAS proposes a self-supervised method to synthesize the undersampled tomographic views and mitigate aliasing artifacts in reconstructed images. This method allows a much better estimation of between-phase Deformation Vector Fields (DVFs), which are used to enhance reconstruction quality from direct observations without synthesis. To address the large memory cost of deep neural networks on high resolution 4D data, REGAS introduces a novel Ray Path Transformation (RPT) that allows for distributed, differentiable forward projections. REGAS require no additional measurements like prior scans, air-flow volume, or breathing velocity. Our extensive experiments show that REGAS significantly outperforms comparable methods in quantitative metrics and visual quality.      
### 19.Soft MIMO Detection Using Marginal Posterior Probability Statistics  [ :arrow_down: ](https://arxiv.org/pdf/2208.08045.pdf)
>  Soft demodulation of received symbols into bit log-likelihood ratios (LLRs) is at the very heart of multiple-input-multiple-output (MIMO) detection. However, the optimal maximum a posteriori (MAP) detector is complicated and infeasible to be used in a practical system. In this paper, we propose a soft MIMO detection algorithm based on marginal posterior probability statistics (MPPS). With the help of optimal transport theory and order statistics theory, we transform the posteriori probability distribution of each layer into a Gaussian distribution. Then the full sampling paths can be implicitly restored from the first- and second-order moment statistics of the transformed distribution. A lightweight network is designed to learn to recovery the log-MAP LLRs from the moment statistics with low complexity. Simulation results show that the proposed algorithm can improve the performance significantly with reduced samples under fading and correlated channels.      
### 20.Artificial Intelligence Empowered Multiple Access for Ultra Reliable and Low Latency THz Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.08039.pdf)
>  Terahertz (THz) wireless networks are expected to catalyze the beyond fifth generation (B5G) era. However, due to the directional nature and the line-of-sight demand of THz links, as well as the ultra-dense deployment of THz networks, a number of challenges that the medium access control (MAC) layer needs to face are created. In more detail, the need of rethinking user association and resource allocation strategies by incorporating artificial intelligence (AI) capable of providing "real-time" solutions in complex and frequently changing environments becomes evident. Moreover, to satisfy the ultra-reliability and low-latency demands of several B5G applications, novel mobility management approaches are required. Motivated by this, this article presents a holistic MAC layer approach that enables intelligent user association and resource allocation, as well as flexible and adaptive mobility management, while maximizing systems' reliability through blockage minimization. In more detail, a fast and centralized joint user association, radio resource allocation, and blockage avoidance by means of a novel metaheuristic-machine learning framework is documented, that maximizes the THz networks performance, while minimizing the association latency by approximately three orders of magnitude. To support, within the access point (AP) coverage area, mobility management and blockage avoidance, a deep reinforcement learning (DRL) approach for beam-selection is discussed. Finally, to support user mobility between coverage areas of neighbor APs, a proactive hand-over mechanism based on AI-assisted fast channel prediction is~reported.      
### 21.Deep Learning based Security-Constrained Unit Commitment Considering Locational Frequency Stability in Low-Inertia Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.08028.pdf)
>  With the goal of electricity system decarbonization, conventional synchronous generators are gradually replaced by converter-interfaced renewable generations. Such transition is causing concerns over system frequency and rate-of-change-of-frequency (RoCoF) security due to significant reduction in system inertia. Existing efforts are mostly derived from uniform system frequency response model which may fail to capture all characteristics of the systems. To ensure the locational frequency security, this paper presents a deep neural network (DNN) based RoCoF-constrained unit commitment (DNN-RCUC) model. RoCoF predictor is trained to predict the highest locational RoCoF based on a high-fidelity simulation dataset. Training samples are generated from models over various scenarios, which can avoid simulation divergence and system instability. The trained network is then reformulated into a set of mixed-integer linear constraints representing the locational RoCoF-limiting constraints in unit commitment. The proposed DNN-RCUC model is studied on the IEEE 24-bus system. Time domain simulation results on PSS/E demonstrate the effectiveness of the proposed algorithm.      
### 22.Disentangled Speaker Representation Learning via Mutual Information Minimization  [ :arrow_down: ](https://arxiv.org/pdf/2208.08012.pdf)
>  Domain mismatch problem caused by speaker-unrelated feature has been a major topic in speaker recognition. In this paper, we propose an explicit disentanglement framework to unravel speaker-relevant features from speaker-unrelated features via mutual information (MI) minimization. To achieve our goal of minimizing MI between speaker-related and speaker-unrelated features, we adopt a contrastive log-ratio upper bound (CLUB), which exploits the upper bound of MI. Our framework is constructed in a 3-stage structure. First, in the front-end encoder, input speech is encoded into shared initial embedding. Next, in the decoupling block, shared initial embedding is split into separate speaker-related and speaker-unrelated embeddings. Finally, disentanglement is conducted by MI minimization in the last stage. Experiments on Far-Field Speaker Verification Challenge 2022 (FFSVC2022) demonstrate that our proposed framework is effective for disentanglement. Also, to utilize domain-unknown datasets containing numerous speakers, we pre-trained the front-end encoder with VoxCeleb datasets. We then fine-tuned the speaker embedding model in the disentanglement framework with FFSVC 2022 dataset. The experimental results show that fine-tuning with a disentanglement framework on a existing pre-trained model is valid and can further improve performance.      
### 23.Vehicle Electrification Solutions: review and open challengers  [ :arrow_down: ](https://arxiv.org/pdf/2208.07986.pdf)
>  An Electric Vehicle (EV) usually refers to any vehicle that is partially or fully powered by a battery that can be directly plugged into the mains. Therefore, the new vehicles provide various benefits, including convenience, efficiency, sustainability, and economy. The present study concerns a comprehensive review of the vehicle electrification solutions. Indeed, the major electric vehicle technologies are presented. Moreover, based on several research works from the literature, the main electrification solutions are illustrated, including: degree of electrification, battery system management, on-board chargers, and power converters. In addition, these solutions, as well as the open challenges, are discussed and evaluated.      
### 24.Digital autofocusing of a coded-aperture Laue diffraction microscope  [ :arrow_down: ](https://arxiv.org/pdf/2208.07873.pdf)
>  To provide optimal depth resolution with a coded-aperture Laue diffraction microscope, an accurate position of the coded-aperture and its scanning geometry need to be known. However, finding the geometry by trial and error is a time-consuming and often challenging process because of the large number of parameters involved. In this paper, we propose an optimization approach to automate the focusing process after data is collected. We demonstrate the robustness and efficiency of the proposed approach with experimental data taken at a synchrotron facility.      
### 25.Label Flipping Data Poisoning Attack Against Wearable Human Activity Recognition System  [ :arrow_down: ](https://arxiv.org/pdf/2208.08433.pdf)
>  Human Activity Recognition (HAR) is a problem of interpreting sensor data to human movement using an efficient machine learning (ML) approach. The HAR systems rely on data from untrusted users, making them susceptible to data poisoning attacks. In a poisoning attack, attackers manipulate the sensor readings to contaminate the training set, misleading the HAR to produce erroneous outcomes. This paper presents the design of a label flipping data poisoning attack for a HAR system, where the label of a sensor reading is maliciously changed in the data collection phase. Due to high noise and uncertainty in the sensing environment, such an attack poses a severe threat to the recognition system. Besides, vulnerability to label flipping attacks is dangerous when activity recognition models are deployed in safety-critical applications. This paper shades light on how to carry out the attack in practice through smartphone-based sensor data collection applications. This is an earlier research work, to our knowledge, that explores attacking the HAR models via label flipping poisoning. We implement the proposed attack and test it on activity recognition models based on the following machine learning algorithms: multi-layer perceptron, decision tree, random forest, and XGBoost. Finally, we evaluate the effectiveness of K-nearest neighbors (KNN)-based defense mechanism against the proposed attack.      
### 26.Global Speed-of-Sound Prediction Using Transmission Geometry  [ :arrow_down: ](https://arxiv.org/pdf/2208.08377.pdf)
>  Most ultrasound (US) imaging techniques use spatially-constant speed-of-sound (SoS) values for beamforming. Having a discrepancy between the actual and used SoS value leads to aberration artifacts, e.g., reducing the image resolution, which may affect diagnostic usability. Accuracy and quality of different US imaging modalities, such as tomographic reconstruction of local SoS maps, also depend on a good initial beamforming SoS. In this work, we develop an analytical method for estimating mean SoS in an imaged medium. We show that the relative shifts between beamformed frames depend on the SoS offset and the geometric disparities in transmission paths. Using this relation, we estimate a correction factor and hence a corrected mean SoS in the medium. We evaluated our proposed method on a set of numerical simulations, demonstrating its utility both for global SoS prediction and for local SoS tomographic reconstruction. For our evaluation dataset, for an initial SoS under- and over-assumption of 5% the medium SoS, our method is able to predict the actual mean SoS within 0.3% accuracy. For the tomographic reconstruction of local SoS maps, the reconstruction accuracy is improved on average by 78.5% and 87%, respectively, compared to an initial SoS under- and over-assumption of 5%.      
### 27.Extract fundamental frequency based on CNN combined with PYIN  [ :arrow_down: ](https://arxiv.org/pdf/2208.08354.pdf)
>  This paper refers to the extraction of multiple fundamental frequencies (multiple F0) based on PYIN, an algorithm for extracting the fundamental frequency (F0) of monophonic music, and a trained convolutional neural networks (CNN) model, where a pitch salience function of the input signal is produced to estimate the multiple F0. The implementation of these two algorithms and their corresponding advantages and disadvantages are discussed in this article. Analysing the different performance of these two methods, PYIN is applied to supplement the F0 extracted from the trained CNN model to combine the advantages of these two algorithms. For evaluation, four pieces played by two violins are used, and the performance of the models are evaluated accoring to the flatness of the F0 curve extracted. The result shows the combined model outperforms the original algorithms when extracting F0 from monophonic music and polyphonic music.      
### 28.Semantic Communications with Discrete-time Analog Transmission: A PAPR Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2208.08342.pdf)
>  Recent progress in deep learning (DL)-based joint source-channel coding (DeepJSCC) has led to a new paradigm of semantic communications. Two salient features of DeepJSCC-based semantic communications are the exploitation of semantic-aware features directly from the source signal, and the discrete-time analog transmission (DTAT) of these features. Compared with traditional digital communications, semantic communications with DeepJSCC provide superior reconstruction performance at the receiver and graceful degradation with diminishing channel quality, but also exhibit a large peak-to-average power ratio (PAPR) in the transmitted signal. An open question has been whether the gains of DeepJSCC come from the additional freedom brought by the high-PAPR continuous-amplitude signal. In this paper, we address this question by exploring three PAPR reduction techniques in the application of image transmission. We confirm that the superior image reconstruction performance of DeepJSCC-based semantic communications can be retained while the transmitted PAPR is suppressed to an acceptable level. This observation is an important step towards the implementation of DeepJSCC in practical semantic communication systems.      
### 29.Low-Gain Stabilizers for Linear-Convex Optimal Steady-State Control  [ :arrow_down: ](https://arxiv.org/pdf/2208.08304.pdf)
>  We consider the problem of designing a feedback controller which robustly regulates an LTI system to an optimal operating point in the presence of unmeasured disturbances. A general design framework based on so-called optimality models was previously put forward for this class of problems, effectively reducing the problem to that of stabilization of an associated nonlinear plant. This paper presents several simple and fully constructive stabilizer designs to accompany the optimality model designs from [1]. The designs are based on a low-gain integral control approach, which enforces time-scale separation between the exponentially stable plant and the controller. We provide explicit formulas for controllers and gains, along with LMI-based methods for the computation of robust/optimal gains. The results are illustrated via an academic example and an application to power system frequency control.      
### 30.On the Elements of Datasets for Cyber Physical Systems Security  [ :arrow_down: ](https://arxiv.org/pdf/2208.08255.pdf)
>  Datasets are essential to apply AI algorithms to Cyber Physical System (CPS) Security. Due to scarcity of real CPS datasets, researchers elected to generate their own datasets using either real or virtualized testbeds. However, unlike other AI domains, a CPS is a complex system with many interfaces that determine its behavior. A dataset that comprises merely a collection of sensor measurements and network traffic may not be sufficient to develop resilient AI defensive or offensive agents. In this paper, we study the \emph{elements} of CPS security datasets required to capture the system behavior and interactions, and propose a dataset architecture that has the potential to enhance the performance of AI algorithms in securing cyber physical systems. The framework includes dataset elements, attack representation, and required dataset features. We compare existing datasets to the proposed architecture to identify the current limitations and discuss the future of CPS dataset generation using testbeds.      
### 31.Two-Stage Robust and Sparse Distributed Statistical Inference for Large-Scale Data  [ :arrow_down: ](https://arxiv.org/pdf/2208.08230.pdf)
>  In this paper, we address the problem of conducting statistical inference in settings involving large-scale data that may be high-dimensional and contaminated by outliers. The high volume and dimensionality of the data require distributed processing and storage solutions. We propose a two-stage distributed and robust statistical inference procedures coping with high-dimensional models by promoting sparsity. In the first stage, known as model selection, relevant predictors are locally selected by applying robust Lasso estimators to the distinct subsets of data. The variable selections from each computation node are then fused by a voting scheme to find the sparse basis for the complete data set. It identifies the relevant variables in a robust manner. In the second stage, the developed statistically robust and computationally efficient bootstrap methods are employed. The actual inference constructs confidence intervals, finds parameter estimates and quantifies standard deviation. Similar to stage 1, the results of local inference are communicated to the fusion center and combined there. By using analytical methods, we establish the favorable statistical properties of the robust and computationally efficient bootstrap methods including consistency for a fixed number of predictors, and robustness. The proposed two-stage robust and distributed inference procedures demonstrate reliable performance and robustness in variable selection, finding confidence intervals and bootstrap approximations of standard deviations even when data is high-dimensional and contaminated by outliers.      
### 32.Blind-Spot Collision Detection System for Commercial Vehicles Using Multi Deep CNN Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2208.08224.pdf)
>  Buses and heavy vehicles have more blind spots compared to cars and other road vehicles due to their large sizes. Therefore, accidents caused by these heavy vehicles are more fatal and result in severe injuries to other road users. These possible blind-spot collisions can be identified early using vision-based object detection approaches. Yet, the existing state-of-the-art vision-based object detection models rely heavily on a single feature descriptor for making decisions. In this research, the design of two convolutional neural networks (CNNs) based on high-level feature descriptors and their integration with faster R-CNN is proposed to detect blind-spot collisions for heavy vehicles. Moreover, a fusion approach is proposed to integrate two pre-trained networks (i.e., Resnet 50 and Resnet 101) for extracting high level features for blind-spot vehicle detection. The fusion of features significantly improves the performance of faster R-CNN and outperformed the existing state-of-the-art methods. Both approaches are validated on a self-recorded blind-spot vehicle detection dataset for buses and an online LISA dataset for vehicle detection. For both proposed approaches, a false detection rate (FDR) of 3.05% and 3.49% are obtained for the self recorded dataset, making these approaches suitable for real time applications.      
### 33.DeepSportradar-v1: Computer Vision Dataset for Sports Understanding with High Quality Annotations  [ :arrow_down: ](https://arxiv.org/pdf/2208.08190.pdf)
>  With the recent development of Deep Learning applied to Computer Vision, sport video understanding has gained a lot of attention, providing much richer information for both sport consumers and leagues. This paper introduces DeepSportradar-v1, a suite of computer vision tasks, datasets and benchmarks for automated sport understanding. The main purpose of this framework is to close the gap between academic research and real world settings. To this end, the datasets provide high-resolution raw images, camera parameters and high quality annotations. DeepSportradar currently supports four challenging tasks related to basketball: ball 3D localization, camera calibration, player instance segmentation and player re-identification. For each of the four tasks, a detailed description of the dataset, objective, performance metrics, and the proposed baseline method are provided. To encourage further research on advanced methods for sport understanding, a competition is organized as part of the MMSports workshop from the ACM Multimedia 2022 conference, where participants have to develop state-of-the-art methods to solve the above tasks. The four datasets, development kits and baselines are publicly available.      
### 34.On Specifications and Proofs of Timed Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2208.08147.pdf)
>  Given a discrete-state continuous-time reactive system, like a digital circuit, the classical approach is to first model it as a state transition system and then prove its properties. Our contribution advocates a different approach: to directly operate on the input-output behavior of such systems, without identifying states and their transitions in the first place. We discuss the benefits of this approach at hand of some examples, which demonstrate that it nicely integrates with concepts of self-stabilization and fault-tolerance. We also elaborate on some unexpected artefacts of module composition in our framework, and conclude with some open research questions.      
### 35.Domestic sound event detection by shift consistency mean-teacher training and adversarial domain adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2208.08131.pdf)
>  Semi-supervised learning and domain adaptation techniques have drawn increasing attention in the field of domestic sound event detection thanks to the availability of large amounts of unlabeled data and the relative ease to generate synthetic strongly-labeled data. In a previous work, several semi-supervised learning strategies were designed to boost the performance of a mean-teacher model. Namely, these strategies include shift consistency training (SCT), interpolation consistency training (ICT), and pseudo-labeling. However, adversarial domain adaptation (ADA) did not seem to improve the event detection accuracy further when we attempt to compensate for the domain gap between synthetic and real data. In this research, we empirically found that ICT tends to pull apart the distributions of synthetic and real data in t-SNE plots. Therefore, ICT is abandoned while SCT, in contrast, is applied to train both the student and the teacher models. With these modifications, the system successfully integrates with an ADA network, and we achieve 47.2% in the F1 score on the DCASE 2020 task 4 dataset, which is 2.1% higher than what was reported in the previous work.      
### 36.On the Performance of Deep Learning-based Data-aided Active-user Detection for GF-SCMA System  [ :arrow_down: ](https://arxiv.org/pdf/2208.08128.pdf)
>  The recent works on a deep learning (DL)-based joint design of preamble set for the transmitters and data-aided active user detection (AUD) in the receiver has demonstrated a significant performance improvement for grant-free sparse code multiple access (GF-SCMA) system. The autoencoder for the joint design can be trained only in a given environment, but in an actual situation where the operating environment is constantly changing, it is difficult to optimize the preamble set for every possible environment. Therefore, a conventional, yet general approach may implement the data-aided AUD while relying on the preamble set that is designed independently rather than the joint design. In this paper, the activity detection error rate (ADER) performance of the data-aided AUD subject to the two preamble designs, i.e., independently designed preamble and jointly designed preamble, were directly compared. Fortunately, it was found that the performance loss in the data-aided AUD induced by the independent preamble design is limited to only 1dB. Furthermore, such performance characteristics of jointly designed preamble set is interpreted through average cross-correlation among the preambles associated with the same codebook (CB) (average intra-CB cross-correlation) and average cross-correlation among preambles associated with the different CBs (average inter-CB cross-correlation).      
### 37.Reach-avoid Verification Based on Convex Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2208.08105.pdf)
>  In this paper we propose novel optimization-based methods for verifying reach-avoid (or, eventuality) properties of continuous-time systems modelled by ordinary differential equations. Given a system, an initial set, a safe set and a target set of states, we say that the reach-avoid property holds if for all initial conditions in the initial set, any trajectory of the system starting at them will eventually, i.e.\ in unbounded yet finite time, enter the target set while remaining inside the safe set until that first target hit. Based on a discount value function, two sets of quantified constraints are derived for verifying the reach-avoid property via the computation of exponential/asymptotic guidance-barrier functions (they form a barrier escorting the system to the target set safely at an exponential or asymptotic rate). It is interesting to find that one set of constraints whose solution is termed exponential guidance-barrier functions is just a simplified version of the existing one derived from the moment based method, while the other one whose solution is termed asymptotic guidance-barrier functions is completely new. Furthermore, built upon this new set of constraints, we derive a set of more expressive constraints, which includes the aforementioned two sets of constraints as special instances, providing more chances for verifying the reach-avoid properties successfully. When the involved datum are polynomials, i.e., the initial set, safe set and target set are semi-algebraic, and the system has polynomial dynamics, the problem of solving these sets of constraints can be framed as a semi-definite optimization problem using sum-of-squares decomposition techniques and thus can be efficiently solved in polynomial time via interior point methods. Finally, several examples demonstrate the theoretical developments and performance of proposed methods.      
### 38.Efficient dynamic point cloud coding using Slice-Wise Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2208.08061.pdf)
>  With the fast growth of immersive video sequences, achieving seamless and high-quality compressed 3D content is even more critical. MPEG recently developed a video-based point cloud compression (V-PCC) standard for dynamic point cloud coding. However, reconstructed point clouds using V-PCC suffer from different artifacts, including losing data during pre-processing before applying existing video coding techniques, e.g., High-Efficiency Video Coding (HEVC). Patch generations and self-occluded points in the 3D to the 2D projection are the main reasons for missing data using V-PCC. This paper proposes a new method that introduces overlapping slicing as an alternative to patch generation to decrease the number of patches generated and the amount of data lost. In the proposed method, the entire point cloud has been cross-sectioned into variable-sized slices based on the number of self-occluded points so that data loss can be minimized in the patch generation process and projection. For this, a variable number of layers are considered, partially overlapped to retain the self-occluded points. The proposed method's added advantage is to reduce the bits requirement and to encode geometric data using the slicing base position. The experimental results show that the proposed method is much more flexible than the standard V-PCC method, improves the rate-distortion performance, and decreases the data loss significantly compared to the standard V-PCC method.      
### 39.Performance Analysis and Optimization for RIS-Assisted Multi-User Massive MIMO Systems with Imperfect Hardware  [ :arrow_down: ](https://arxiv.org/pdf/2208.08055.pdf)
>  The paper studies a reconfigurable intelligent surface (RIS)-assisted multi-user uplink massive multiple-input multiple-output (MIMO) system with imperfect hardware. At the RIS, the paper considers phase noise, while at the base station, the paper takes into consideration the radio frequency impairments and low-resolution analog-to-digital converters. The paper derives approximate expressions for the ergodic achievable rate in closed forms under Rician fading channels. For the cases of infinite numbers of antennas and infinite numbers of reflecting elements, asymptotic data rates are derived to provide new design insights. The derived power scaling laws indicate that while guaranteeing a required system performance, the transmit power of the users can be scaled down at most by the factor 1/M when M goes infinite, or by the factor 1/(MN) when M and N go infinite, where M is the number of antennas and N is the number of the reflecting units. Furthermore, an optimization algorithm is proposed based on the genetic algorithm to solve the phase shift optimization problem with the aim of maximizing the sum rate of the system. Additionally, the optimization problem with discrete phase shifts is considered. Finally, numerical results are provided to validate the correctness of the analytical results.      
### 40.The Conversational Short-phrase Speaker Diarization (CSSD) Task: Dataset, Evaluation Metric and Baselines  [ :arrow_down: ](https://arxiv.org/pdf/2208.08042.pdf)
>  The conversation scenario is one of the most important and most challenging scenarios for speech processing technologies because people in conversation respond to each other in a casual style. Detecting the speech activities of each person in a conversation is vital to downstream tasks, like natural language processing, machine translation, etc. People refer to the detection technology of "who speak when" as speaker diarization (SD). Traditionally, diarization error rate (DER) has been used as the standard evaluation metric of SD systems for a long time. However, DER fails to give enough importance to short conversational phrases, which are short but important on the semantic level. Also, a carefully and accurately manually-annotated testing dataset suitable for evaluating the conversational SD technologies is still unavailable in the speech community. In this paper, we design and describe the Conversational Short-phrases Speaker Diarization (CSSD) task, which consists of training and testing datasets, evaluation metric and baselines. In the dataset aspect, despite the previously open-sourced 180-hour conversational MagicData-RAMC dataset, we prepare an individual 20-hour conversational speech test dataset with carefully and artificially verified speakers timestamps annotations for the CSSD task. In the metric aspect, we design the new conversational DER (CDER) evaluation metric, which calculates the SD accuracy at the utterance level. In the baseline aspect, we adopt a commonly used method: Variational Bayes HMM x-vector system, as the baseline of the CSSD task. Our evaluation metric is publicly available at <a class="link-external link-https" href="https://github.com/SpeechClub/CDER_Metric" rel="external noopener nofollow">this https URL</a>.      
### 41.Enhancing Audio Perception of Music By AI Picked Room Acoustics  [ :arrow_down: ](https://arxiv.org/pdf/2208.07994.pdf)
>  Every sound that we hear is the result of successive convolutional operations (e.g. room acoustics, microphone characteristics, resonant properties of the instrument itself, not to mention characteristics and limitations of the sound reproduction system). In this work we seek to determine the best room in which to perform a particular piece using AI. Additionally, we use room acoustics as a way to enhance the perceptual qualities of a given sound. Historically, rooms (particularly Churches and concert halls) were designed to host and serve specific musical functions. In some cases the architectural acoustical qualities enhanced the music performed there. We try to mimic this, as a first step, by designating room impulse responses that would correlate to producing enhanced sound quality for particular music. A convolutional architecture is first trained to take in an audio sample and mimic the ratings of experts with about 78 % accuracy for various instrument families and notes for perceptual qualities. This gives us a scoring function for any audio sample which can rate the perceptual pleasantness of a note automatically. Now, via a library of about 60,000 synthetic impulse responses mimicking all kinds of room, materials, etc, we use a simple convolution operation, to transform the sound as if it was played in a particular room. The perceptual evaluator is used to rank the musical sounds, and yield the "best room or the concert hall" to play a sound. As a byproduct it can also use room acoustics to turn a poor quality sound into a "good" sound.      
### 42.Tiny-HR: Towards an interpretable machine learning pipeline for heart rate estimation on edge devices  [ :arrow_down: ](https://arxiv.org/pdf/2208.07981.pdf)
>  The focus of this paper is a proof of concept, machine learning (ML) pipeline that extracts heart rate from pressure sensor data acquired on low-power edge devices. The ML pipeline consists an upsampler neural network, a signal quality classifier, and a 1D-convolutional neural network optimized for efficient and accurate heart rate estimation. The models were designed so the pipeline was less than 40 kB. Further, a hybrid pipeline consisting of the upsampler and classifier, followed by a peak detection algorithm was developed. The pipelines were deployed on ESP32 edge device and benchmarked against signal processing to determine the energy usage, and inference times. The results indicate that the proposed ML and hybrid pipeline reduces energy and time per inference by 82% and 28% compared to traditional algorithms. The main trade-off for ML pipeline was accuracy, with a mean absolute error (MAE) of 3.28, compared to 2.39 and 1.17 for the hybrid and signal processing pipelines. The ML models thus show promise for deployment in energy and computationally constrained devices. Further, the lower sampling rate and computational requirements for the ML pipeline could enable custom hardware solutions to reduce the cost and energy needs of wearable devices.      
