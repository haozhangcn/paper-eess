# ArXiv eess --Mon, 22 Aug 2022
### 1.PrepNet: A Convolutional Auto-Encoder to Homogenize CT Scans for Cross-Dataset Medical Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.09408.pdf)
>  With the spread of COVID-19 over the world, the need arose for fast and precise automatic triage mechanisms to decelerate the spread of the disease by reducing human efforts e.g. for image-based diagnosis. Although the literature has shown promising efforts in this direction, reported results do not consider the variability of CT scans acquired under varying circumstances, thus rendering resulting models unfit for use on data acquired using e.g. different scanner technologies. While COVID-19 diagnosis can now be done efficiently using PCR tests, this use case exemplifies the need for a methodology to overcome data variability issues in order to make medical image analysis models more widely applicable. In this paper, we explicitly address the variability issue using the example of COVID-19 diagnosis and propose a novel generative approach that aims at erasing the differences induced by e.g. the imaging technology while simultaneously introducing minimal changes to the CT scans through leveraging the idea of deep auto-encoders. The proposed prepossessing architecture (PrepNet) (i) is jointly trained on multiple CT scan datasets and (ii) is capable of extracting improved discriminative features for improved diagnosis. Experimental results on three public datasets (SARS-COVID-2, UCSD COVID-CT, MosMed) show that our model improves cross-dataset generalization by up to $11.84$ percentage points despite a minor drop in within dataset performance.      
### 2.Approximate Dynamic Programming for Platoon Coordination under Hours-of-Service Regulations  [ :arrow_down: ](https://arxiv.org/pdf/2208.09366.pdf)
>  Truck drivers are required to stop and rest with a certain regularity according to the driving and rest time regulations, also called Hours-of-Service (HoS) regulations. This paper studies the problem of optimally forming platoons when considering realistic HoS regulations. In our problem, trucks have fixed routes in a transportation network and can wait at hubs along their routes to form platoons with others while fulfilling the driving and rest time constraints. We propose a distributed decision-making scheme where each truck controls its waiting times at hubs based on the predicted schedules of others. The decoupling of trucks' decision-makings contributes to an approximate dynamic programming approach for platoon coordination under HoS regulations. Finally, we perform a simulation over the Swedish road network with one thousand trucks to evaluate the achieved platooning benefits under the HoS regulations in the European Union (EU). The simulation results show that, on average, trucks drive in platoons for 37% of their routes if each truck is allowed to be delayed for 5% of its total travel time. If trucks are not allowed to be delayed, they drive in platoons for 12% of their routes.      
### 3.PyMIC: A deep learning toolkit for annotation-efficient medical image segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2208.09350.pdf)
>  Background and Objective: Existing deep learning platforms for medical image segmentation mainly focus on fully supervised segmentation that assumes full and accurate pixel-level annotations are available. We aim to develop a new deep learning toolkit to support annotation-efficient learning for medical image segmentation, which can accelerate and simply the development of deep learning models with limited annotation budget, e.g., learning from partial, sparse or noisy annotations. <br>Methods: Our proposed toolkit named PyMIC is a modular deep learning platform for medical image segmentation tasks. In addition to basic components that support development of high-performance models for fully supervised segmentation, it contains several advanced components that are tailored for learning from imperfect annotations, such as loading annotated and unannounced images, loss functions for unannotated, partially or inaccurately annotated images, and training procedures for co-learning between multiple networks, etc. PyMIC is built on the PyTorch framework and supports development of semi-supervised, weakly supervised and noise-robust learning methods for medical image segmentation. <br>Results: We present four illustrative medical image segmentation tasks based on PyMIC: (1) Achieving competitive performance on fully supervised learning; (2) Semi-supervised cardiac structure segmentation with only 10% training images annotated; (3) Weakly supervised segmentation using scribble annotations; and (4) Learning from noisy labels for chest radiograph segmentation. <br>Conclusions: The PyMIC toolkit is easy to use and facilitates efficient development of medical image segmentation models with imperfect annotations. It is modular and flexible, which enables researchers to develop high-performance models with low annotation cost. The source code is available at: <a class="link-external link-https" href="https://github.com/HiLab-git/PyMIC" rel="external noopener nofollow">this https URL</a>.      
### 4.DCNNV-19: Uma rede neural convolucional profunda para detecção de COVID-19 em tomografias computadorizadas torácicas  [ :arrow_down: ](https://arxiv.org/pdf/2208.09349.pdf)
>  This technical report proposes the use of a deep convolutional neural network as a preliminary diagnostic method in the analysis of chest computed tomography images from patients with symptoms of Severe Acute Respiratory Syndrome (SARS) and suspected COVID-19 disease, especially on occasions when the delay of the RT-PCR result and the absence of urgent care could result in serious temporary, long-term, or permanent health damage. The model was trained on 83,391 images, validated on 15,297, and tested on 22,185 figures, achieving an F1-Score of 98%, 97.59% in Cohen's Kappa, 98.4% in Accuracy, and 5.09% in Loss. Attesting a highly accurate automated classification and providing results in less time than the current gold-standard exam, Real-Time reverse-transcriptase Polymerase Chain Reaction (RT-PCR). <br>-- <br>O presente relatório técnico propõe a utilização de uma rede neural convolucional profunda como método diagnóstico preliminar na análise de imagens de tomografia computadorizada torácica em pacientes com sintomas de Síndrome Respiratória Aguda Grave (SRAG) e suspeita de COVID-19, principalmente em ocasiões em que a demora do resultado do RT-PCR e a ausência de cuidados urgentes poderia acarretar graves danos temporários, à longo prazo, ou permanentes à saúde. O modelo foi treinado em 83.391 imagens, validado em 15.297, e testado em 22.185 figuras, atingindo pontuação no F1-Score de 98%, 97,59% em Cohen's Kappa, 98,4% de Acurácia e 5,09% de Loss. Atestando uma classificação automatizada rápida e de alta precisão, e fornecendo resultado em tempo menor ao do exame padrão-ouro atual, o Real-Time reverse-transcriptase Polymerase Chain Reaction (RT-PCR).      
### 5.Machine Learning-Based Distributed Authentication of UWAN Nodes with Limited Shared Information  [ :arrow_down: ](https://arxiv.org/pdf/2208.09340.pdf)
>  We propose a technique to authenticate received packets in underwater acoustic networks based on the physical layer features of the underwater acoustic channel (UWAC). Several sensors a) locally estimate features (e.g., the number of taps or the delay spread) of the UWAC over which the packet is received, b) obtain a compressed feature representation through a neural network (NN), and c) transmit their representations to a central sink node that, using a NN, decides whether the packet has been transmitted by the legitimate node or by an impersonating attacker. Although the purpose of the system is to make a binary decision as to whether a packet is authentic or not, we show the importance of having a rich set of compressed features, while still taking into account transmission rate limits among the nodes. We consider both global training, where all NNs are trained together, and local training, where each NN is trained individually. For the latter scenario, several alternatives for the NN structure and loss function were used for training.      
### 6.Consensus optimization approach for distributed Kalman filtering: performance recovery of centralized filtering with proofs  [ :arrow_down: ](https://arxiv.org/pdf/2208.09328.pdf)
>  This paper investigates the distributed Kalman filtering (DKF) from distributed optimization viewpoint. Motivated by the fact that Kalman filtering is a maximum a posteriori estimation (MAP) problem, which is a quadratic optimization problem, we reformulate DKF problem as a consensus optimization problem, resulting in that it can be solved by many existing distributed optimization algorithms. A new DKF algorithm employing the dual ascent method is proposed, and its stability is proved under mild assumptions. The performance of the proposed algorithm is evaluated through numerical experiments.      
### 7.Learn to Detect and Detect to Learn: Structure Learning and Decision Feedback for MIMO-OFDM Receive Processing  [ :arrow_down: ](https://arxiv.org/pdf/2208.09287.pdf)
>  One of the major open challenges in MIMO-OFDM receive processing is how to efficiently and effectively utilize the extremely limited over-the-air pilot symbols to detect the transmitted data symbols. Recent advances have been devoted to investigating effective ways to utilize the limited pilots. However, we notice that besides exploiting the pilots, one can take advantage of the data symbols to improve the detection performance. Thus, this paper introduces an online subframe-based approach, namely RC-StructNet, that can efficiently learn from the precious pilot symbols and be dynamically updated with the detected payload data using the decision feedback (DF) approach. The network consists of a reservoir computing (RC) module in the time domain and a neural network StructNet in the frequency domain. The unique design of the network allows it to be dynamically updated with the changes of the channel by learning from the detected data symbols. Experiments demonstrate the effectiveness of RC-StructNet in detection under dynamic transmission modes and in reducing the training overhead requirement when taking the DF approach.      
### 8.Feature Selection Enhancement and Feature Space Visualization for Speech-Based Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2208.09269.pdf)
>  Robust speech emotion recognition relies on the quality of the speech features. We present speech features enhancement strategy that improves speech emotion recognition. We used the INTERSPEECH 2010 challenge feature-set. We identified subsets from the features set and applied Principle Component Analysis to the subsets. Finally, the features are fused horizontally. The resulting feature set is analyzed using t-distributed neighbour embeddings (t-SNE) before the application of features for emotion recognition. The method is compared with the state-of-the-art methods used in the literature. The empirical evidence is drawn using two well-known datasets: Emotional Speech Dataset (EMO-DB) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) for two languages, German and English, respectively. Our method achieved an average recognition gain of 11.5\% for six out of seven emotions for the EMO-DB dataset, and 13.8\% for seven out of eight emotions for the RAVDESS dataset as compared to the baseline study.      
### 9.Ensemble uncertainty as a criterion for dataset expansion in distinct bone segmentation from upper-body CT images  [ :arrow_down: ](https://arxiv.org/pdf/2208.09216.pdf)
>  Purpose: The localisation and segmentation of individual bones is an important preprocessing step in many planning and navigation applications. It is, however, a time-consuming and repetitive task if done manually. This is true not only for clinical practice but also for the acquisition of training data. We therefore not only present an end-to-end learnt algorithm that is capable of segmenting 125 distinct bones in an upper-body CT, but also provide an ensemble-based uncertainty measure that helps to single out scans to enlarge the training dataset with. Methods We create fully automated end-to-end learnt segmentations using a neural network architecture inspired by the 3D-Unet and fully supervised training. The results are improved using ensembles and inference-time augmentation. We examine the relationship of ensemble-uncertainty to an unlabelled scan's prospective usefulness as part of the training dataset. Results: Our methods are evaluated on an in-house dataset of 16 upper-body CT scans with a resolution of \SI{2}{\milli\meter} per dimension. Taking into account all 125 bones in our label set, our most successful ensemble achieves a median dice score coefficient of 0.83. We find a lack of correlation between a scan's ensemble uncertainty and its prospective influence on the accuracies achieved within an enlarged training set. At the same time, we show that the ensemble uncertainty correlates to the number of voxels that need manual correction after an initial automated segmentation, thus minimising the time required to finalise a new ground truth segmentation. Conclusion: In combination, scans with low ensemble uncertainty need less annotator time while yielding similar future DSC improvements. They are thus ideal candidates to enlarge a training set for upper-body distinct bone segmentation from CT scans. }      
### 10.Learning Local Volt/Var Controllers Towards Efficient Network Operation with Stability Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2208.09117.pdf)
>  This paper considers the problem of voltage regulation in distribution networks. The primary motivation is to keep voltages within preassigned operating limits by commanding the reactive power output of distributed energy resources (DERs) deployed in the grid. We develop a framework for developing local Volt/Var control that comprises two main steps. In the first, by exploiting historical data and for each DER, we learn a function representing the desirable equilibrium points for the power network. These points approximate solutions of an Optimal Power Flow (OPF) problem. In the second, we propose a control scheme for steering the network towards these favorable configurations. Theoretical conditions are derived to formally guarantee the stability of the developed control scheme, and numerical simulations illustrate the effectiveness of the proposed approach.      
### 11.Resilience assessment and improvement for electric power transmission systems against typhoon disasters: A data-model hybrid driven approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.09115.pdf)
>  In response to the damage to electric power transmission systems caused by typhoon disasters in coastal areas, a planning-targeted resilience assessment framework that considers the impact of multiple factors is established to accurately find the weak links of the transmission system and improve the system resilience. Firstly, this paper constructs the attenuation model of the wind field and the comprehensive failure model of the system, in which the model drive to establish the cumulative failure model of the transmission system is adopted, and multiple data-driven schemes to correct the cumulative failure rate by relying on the feature factor information of the transmission system is adopted. At the same time, an analytic hierarchy process-weighted arithmetic averaging (AHP-WAA) method is introduced to select the optimal data-driven evaluation scheme. Secondly, this paper adopts the impact-increment-based state enumeration (IISE) method to establish resilience indicators for systems and corridors separately. On this basis, the optimal promotion strategy is selected according to the construction difficulty, resilience improvement ability, and cost analysis. Finally, simulations on the IEEE RTS-79 system have been carried out considering the influence of the real typhoon scenarios, micro-topographic and transmission corridor information factors. The results demonstrate that the hybrid-driven system resilience assessment and improvement method can assist planners in accurately judging the resilience level of the system against typhoon disasters and selecting the best resilience improvement strategy based on the cost-effectiveness ratio.      
### 12.A Unified Algorithmic Framework for Distributed Adaptive Signal and Feature Fusion Problems -- Part II: Convergence Properties  [ :arrow_down: ](https://arxiv.org/pdf/2208.09088.pdf)
>  This paper studies the convergence conditions and properties of the distributed adaptive signal fusion (DASF) algorithm, the framework itself having been introduced in a `Part I' companion paper. The DASF algorithm can be used to solve linear signal and feature fusion optimization problems in a distributed fashion, and is in particular well-suited for solving spatial filtering optimization problems encountered in wireless sensor networks. The convergence results are provided along with rigorous proofs and analyses, as well as various example problems to which they apply. Additionally, we describe procedures that can be added to the DASF algorithm to ensure convergence in specific cases where some of the technical convergence conditions are not satisfied.      
### 13.On the Accuracy of the One-step UKF and the Two-step UKF  [ :arrow_down: ](https://arxiv.org/pdf/2208.09055.pdf)
>  The most accurate version of the unscented Kalman filter (UKF) involves the construction of two ensembles. To reduce computational cost, however, UKF is often implemented without the second ensemble. This simplification comes at a price, however, since, for linear systems, the one-step variation of the two-step UKF does not specialize to the classical Kalman filter, with an associated loss of accuracy. This paper remedies this drawback by developing a modified one-step UKF that recovers the classical Kalman filter for linear systems. Numerical examples show that the modified one-step UKF also recovers the accuracy of the two-step UKF in nonlinear systems with linear outputs.      
### 14.Automated Detection of Acute Lymphoblastic Leukemia Subtypes from Microscopic Blood Smear Images using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.08992.pdf)
>  An estimated 300,000 new cases of leukemia are diagnosed each year which is 2.8 percent of all new cancer cases and the prevalence is rising day by day. The most dangerous and deadly type of leukemia is acute lymphoblastic leukemia (ALL), which affects people of all age groups, including children and adults. In this study, we propose an automated system to detect various-shaped ALL blast cells from microscopic blood smears images using Deep Neural Networks (DNN). The system can detect multiple subtypes of ALL cells with an accuracy of 98 percent. Moreover, we have developed a telediagnosis software to provide real-time support to diagnose ALL subtypes from microscopic blood smears images.      
### 15.Low-light Enhancement Method Based on Attention Map Net  [ :arrow_down: ](https://arxiv.org/pdf/2208.09330.pdf)
>  Low-light image enhancement is a crucial preprocessing task for some complex vision tasks. Target detection, image segmentation, and image recognition outcomes are all directly impacted by the impact of image enhancement. However, the majority of the currently used image enhancement techniques do not produce satisfactory outcomes, and these enhanced networks have relatively weak robustness. We suggest an improved network called BrightenNet that uses U-Net as its primary structure and incorporates a number of different attention mechanisms as a solution to this issue. In a specific application, we employ the network as the generator and LSGAN as the training framework to achieve better enhancement results. We demonstrate the validity of the proposed network BrightenNet in the experiments that follow in this paper. The results it produced can both preserve image details and conform to human vision standards.      
### 16.Accelerating sampling-based optimal path planning via adaptive informed sampling  [ :arrow_down: ](https://arxiv.org/pdf/2208.09318.pdf)
>  This paper improves the performance of RRT*-like sampling-based path planners by combining admissible informed sampling and local sampling (i.e., sampling the neighborhood of the current solution). An adaptive strategy that accounts for the cost progression regulates the trade-off between exploration (admissible informed sampling) and exploitation (local sampling). The paper proves that the resulting algorithm is asymptotically optimal. Furthermore, its convergence rate is superior to that of state-of-the-art path planners, such as Informed-RRT*, both in simulations and manufacturing case studies. An open-source ROS-compatible implementation is also released.      
### 17.Sparse Structure Design for Stochastic Linear Systems via a Linear Matrix Inequality Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.09268.pdf)
>  In this paper, we propose a sparsity-promoting feedback control design for stochastic linear systems with multiplicative noise. The objective is to identify a sparse control architecture that optimizes the closed-loop performance while stabilizing the system in the mean-square sense. The proposed approach approximates the nonconvex combinatorial optimization problem by minimizing various matrix norms subject to the Linear Matrix Inequality (LMI) stability condition. We present two design problems to reduce the number of actuators via the static state-feedback and a low-dimensional output. A regularized linear quadratic regulator with multiplicative noise (LQRm) optimal control problem and its convex relaxation are presented to demonstrate the tradeoff between the suboptimal closed-loop performance and the sparsity degree of control structure. Case studies on power grids for wide-area frequency control show that the proposed sparsity-promoting control can considerably reduce the number of actuators without significant loss in system performance. The sparse control architecture is robust to substantial system-level disturbances while achieving mean-square stability.      
### 18.A Two-phase Metamorphic Approach for Testing Industrial Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.09261.pdf)
>  We elaborate on a metamorphic approach for testing industrial control systems. The proposed approach consists of two phases: an exploration phase in which we learn about fault patterns of the system under test and an exploitation phase where the observed fault patterns are used for targeted testing. Our method extracts metamorphic relations and input space of the system from its requirements. The seed input used for testing is extracted from the execution logs of the system and used to generate source tests and follow-up tests automatically. The morphed input is constructed based on the seed input and refined using a set of constraints. The approach is exemplified on a position control system and the results show that it is effective in discovering faults with an increased level of automation.      
### 19.Deep Joint Source-Channel and Encryption Coding: Secure Semantic Communications  [ :arrow_down: ](https://arxiv.org/pdf/2208.09245.pdf)
>  Deep learning driven joint source-channel coding (JSCC) for wireless image or video transmission, also called DeepJSCC, has been a topic of interest recently with very promising results. The idea is to map similar source samples to nearby points in the channel input space such that, despite the noise introduced by the channel, the input can be recovered with minimal distortion. In DeepJSCC, this is achieved by an autoencoder architecture with a non-trainable channel layer between the encoder and decoder. DeepJSCC has many favorable properties, such as better end-to-end distortion performance than its separate source and channel coding counterpart as well as graceful degradation with respect to channel quality. However, due to the inherent correlation between the source sample and channel input, DeepJSCC is vulnerable to eavesdropping attacks. In this paper, we propose the first DeepJSCC scheme for wireless image transmission that is secure against eavesdroppers, called DeepJSCEC. DeepJSCEC not only preserves the favorable properties of DeepJSCC, it also provides security against chosen-plaintext attacks from the eavesdropper, without the need to make assumptions about the eavesdropper's channel condition, or its intended use of the intercepted signal. Numerical results show that DeepJSCEC achieves similar or better image quality than separate source coding using BPG compression, AES encryption, and LDPC codes for channel coding, while preserving the graceful degradation of image quality with respect to channel quality. We also show that the proposed encryption method is problem agnostic, meaning it can be applied to other end-to-end JSCC problems, such as remote classification, without modification. Given the importance of security in modern wireless communication systems, we believe this work brings DeepJSCC schemes much closer to adoption in practice.      
### 20.Improving Post-Processing of Audio Event Detectors Using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.09201.pdf)
>  We apply post-processing to the class probability distribution outputs of audio event classification models and employ reinforcement learning to jointly discover the optimal parameters for various stages of a post-processing stack, such as the classification thresholds and the kernel sizes of median filtering algorithms used to smooth out model predictions. To achieve this we define a reinforcement learning environment where: 1) a state is the class probability distribution provided by the model for a given audio sample, 2) an action is the choice of a candidate optimal value for each parameter of the post-processing stack, 3) the reward is based on the classification accuracy metric we aim to optimize, which is the audio event-based macro F1-score in our case. We apply our post-processing to the class probability distribution outputs of two audio event classification models submitted to the DCASE Task4 2020 challenge. We find that by using reinforcement learning to discover the optimal per-class parameters for the post-processing stack that is applied to the outputs of audio event classification models, we can improve the audio event-based macro F1-score (the main metric used in the DCASE challenge to compare audio event classification accuracy) by 4-5% compared to using the same post-processing stack with manually tuned parameters.      
### 21.A Physics-informed Deep Learning Approach for Minimum Effort Stochastic Control of Colloidal Self-Assembly  [ :arrow_down: ](https://arxiv.org/pdf/2208.09182.pdf)
>  We propose formulating the finite-horizon stochastic optimal control problem for colloidal self-assembly in the space of probability density functions (PDFs) of the underlying state variables (namely, order parameters). The control objective is formulated in terms of steering the state PDFs from a prescribed initial probability measure towards a prescribed terminal probability measure with minimum control effort. For specificity, we use a univariate stochastic state model from the literature. Both the analysis and the computational steps for control synthesis as developed in this paper generalize for multivariate stochastic state dynamics given by generic nonlinear in state and non-affine in control models. We derive the conditions of optimality for the associated optimal control problem. This derivation yields a system of three coupled partial differential equations together with the boundary conditions at the initial and terminal times. The resulting system is a generalized instance of the so-called Schrödinger bridge problem. We then determine the optimal control policy by training a physics-informed deep neural network, where the "physics" are the derived conditions of optimality. The performance of the proposed solution is demonstrated via numerical simulations on a benchmark colloidal self-assembly problem.      
### 22.Requirements and impacts of energy storage characteristics in a highly renewable European energy system  [ :arrow_down: ](https://arxiv.org/pdf/2208.09169.pdf)
>  The European energy system is increasing its renewable share, primarily that of wind and solar photovoltaic energy. Going forward, the system will need better interconnections and storage integration to balance the mismatch between electricity demand and renewable generation. Europe has already a large capacity of pumped-hydro storage installed. Moreover, Li-Ion batteries are emerging as a promising cost-effective storage option. Taking that into consideration, we analyze to which extent a highly renewable system benefits from an additional storage technology, named storage-X, in addition to pumped-hydro storage and batteries. In our analysis, we identify which characteristics of storage-X are required for it to play a prominent role in the system. For the study, we use an overnight capacity and dispatch-optimization model of a future decarbonized sector-coupled European energy system. In total, the system is optimized for 2,016 configurations of storage-X characteristics, including efficiency and cost properties, to define the space of feasible configurations. By comparing this space with the characteristics of storage technologies currently under development (e.g., pumped thermal energy storage, compressed air energy storage, etc.), we learn that significant improvements are needed for them to become part of the optimal solutions. Reducing energy capacity cost, increasing discharge efficiency, and reducing the cost of charging power capacity must be prioritized. As other sectors are electrified, the design space for storage-X shows similar characteristics. We find a potential system cost reduction of 10% with the best storage configuration. But for this to be realized, the storage is required to provide a high load coverage. Even in that case, renewable curtailment of 1% is still present in the optimal system.      
### 23.Photonics-enabled wavelet-like transform via nonlinear optical frequency sweeping and stimulated Brillouin scattering-based frequency-to-time mapping  [ :arrow_down: ](https://arxiv.org/pdf/2208.09143.pdf)
>  A photonics-enabled wavelet-like transform system, characterized by multi-resolution time-frequency analysis, is proposed based on a typical stimulated Brillouin scattering (SBS) pump-probe setup using an optical nonlinear frequency-sweep signal. In the pump path, a continuous-wave optical signal is injected into an SBS medium to generate an SBS gain. In the probe path, a periodic nonlinear frequency-sweep optical signal with a time-varying chirp rate is generated, which is then modulated at a Mach-Zehnder modulator (MZM) by the electrical signal under test (SUT). The optical signal from the MZM is selectively amplified by the SBS gain and converted back to the electrical domain using a low-speed photodetector, implementing the periodic SBS-based frequency-to-time mapping (FTTM). The frequency-domain information corresponding to different periods is mapped to the time domain via the FTTM in the form of low-speed electrical pulses, which is then spliced to analyze the time-frequency relationship of the SUT in real-time. The time-varying chirp rate in each sweep period makes the signals with different frequencies have different frequency resolutions in the FTTM process, which is very similar to the characteristics of the wavelet transform, so we call it wavelet-like transform. An experiment is carried out. Multi-resolution time-frequency analysis of a variety of RF signals is carried out in a 4-GHz bandwidth limited only by the equipment.      
### 24.An Optimal Energy Efficient Design of Artificial Noise for Preventing Power Leakage based Side-Channel Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2208.09140.pdf)
>  Side-channel attacks (SCAs), which infer secret information (for example secret keys) by exploiting information that leaks from the implementation (such as power consumption), have been shown to be a non-negligible threat to modern cryptographic implementations and devices in recent years. Hence, how to prevent side-channel attacks on cryptographic devices has become an important problem. One of the widely used countermeasures to against power SCAs is the injection of random noise sequences into the raw leakage traces. However, the indiscriminate injection of random noise can lead to significant increases in energy consumption in device, and ways must be found to reduce the amount of energy in noise generation while keeping the side-channel invisible. In this paper, we propose an optimal energy-efficient design for artificial noise generation to prevent side-channel attacks. This approach exploits the sparsity among the leakage traces. We model the side-channel as a communication channel, which allows us to use channel capacity to measure the mutual information between the secret and the leakage traces. For a given energy budget in the noise generation, we obtain the optimal design of the artificial noise injection by solving the side-channel's channel capacity minimization problem. The experimental results also validate the effectiveness of our proposed scheme.      
### 25.3M: An Effective Multi-view, Multi-granularity, and Multi-aspect Modeling Approach to English Pronunciation Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2208.09110.pdf)
>  As an indispensable ingredient of computer-assisted pronunciation training (CAPT), automatic pronunciation assessment (APA) plays a pivotal role in aiding self-directed language learners by providing multi-aspect and timely feedback. However, there are at least two potential obstacles that might hinder its performance for practical use. On one hand, most of the studies focus exclusively on leveraging segmental (phonetic)-level features such as goodness of pronunciation (GOP); this, however, may cause a discrepancy of feature granularity when performing suprasegmental (prosodic)-level pronunciation assessment. On the other hand, automatic pronunciation assessments still suffer from the lack of large-scale labeled speech data of non-native speakers, which inevitably limits the performance of pronunciation assessment. In this paper, we tackle these problems by integrating multiple prosodic and phonological features to provide a multi-view, multi-granularity, and multi-aspect (3M) pronunciation modeling. Specifically, we augment GOP with prosodic and self-supervised learning (SSL) features, and meanwhile develop a vowel/consonant positional embedding for a more phonology-aware automatic pronunciation assessment. A series of experiments conducted on the publicly-available speechocean762 dataset show that our approach can obtain significant improvements on several assessment granularities in comparison with previous work, especially on the assessment of speaking fluency and speech prosody.      
### 26.Representation Learning for the Automatic Indexing of Sound Effects Libraries  [ :arrow_down: ](https://arxiv.org/pdf/2208.09096.pdf)
>  Labeling and maintaining a commercial sound effects library is a time-consuming task exacerbated by databases that continually grow in size and undergo taxonomy updates. Moreover, sound search and taxonomy creation are complicated by non-uniform metadata, an unrelenting problem even with the introduction of a new industry standard, the Universal Category System. To address these problems and overcome dataset-dependent limitations that inhibit the successful training of deep learning models, we pursue representation learning to train generalized embeddings that can be used for a wide variety of sound effects libraries and are a taxonomy-agnostic representation of sound. We show that a task-specific but dataset-independent representation can successfully address data issues such as class imbalance, inconsistent class labels, and insufficient dataset size, outperforming established representations such as OpenL3. Detailed experimental results show the impact of metric learning approaches and different cross-dataset training methods on representational effectiveness.      
### 27.k-Dimensional Agreement in Multiagent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.08999.pdf)
>  We study the problem of k-dimensional linear agreement, whereby a group of agents is interested in computing k independent weighted means of a global vector whose entries are known only by individual agents. This problem is motivated by applications in distributed computing and sensing, where agents seek to evaluate multiple independent functions at a common vector point by running a single distributed algorithm. We propose the use of linear network protocols for this task, and we show that linear dynamics can agree on quantities that are oblique projections of the global vector onto certain subspaces. Moreover, we provide algebraic necessary and sufficient conditions that characterize all agreement protocols that are consistent with a certain graph, we propose a design procedure for constructing such protocols, and we study what classes of graphs can achieve agreement on arbitrary weights. Overall, our results suggest that k-dimensional agreement requires the use of communication graphs with higher connectivity compared to standard consensus algorithms; more precisely, we relate the existence of Hamiltonian decompositions in a graph with the capability of that graph to sustain an agreement protocol. The applicability of the framework is illustrated via simulations for two problems in robotic formation and in distributed regression.      
