# ArXiv eess --Tue, 9 Aug 2022
### 1.Boosting neural video codecs by exploiting hierarchical redundancy  [ :arrow_down: ](https://arxiv.org/pdf/2208.04303.pdf)
>  In video compression, coding efficiency is improved by reusing pixels from previously decoded frames via motion and residual compensation. We define two levels of hierarchical redundancy in video frames: 1) first-order: redundancy in pixel space, i.e., similarities in pixel values across neighboring frames, which is effectively captured using motion and residual compensation, 2) second-order: redundancy in motion and residual maps due to smooth motion in natural videos. While most of the existing neural video coding literature addresses first-order redundancy, we tackle the problem of capturing second-order redundancy in neural video codecs via predictors. We introduce generic motion and residual predictors that learn to extrapolate from previously decoded data. These predictors are lightweight, and can be employed with most neural video codecs in order to improve their rate-distortion performance. Moreover, while RGB is the dominant colorspace in neural video coding literature, we introduce general modifications for neural video codecs to embrace the YUV420 colorspace and report YUV420 results. Our experiments show that using our predictors with a well-known neural video codec leads to 38% and 34% bitrate savings in RGB and YUV420 colorspaces measured on the UVG dataset.      
### 2.High-gain observer for the nitrification process including sensor dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2208.04300.pdf)
>  Fertilization is commonly used to increase harvests. The lack of knowledge of soil properties and the excessive use of fertilizers can result in overfertilization. Current sensor technology is able to measure the concentrations of some of the involved substances only at selected locations and depths. Point measurements of adjacent sensors in coarse sensor networks can be used to infer upon the state of nitrate concentrations in the sensor surroundings. For this purpose, a high gain observer is proposed. Models of the nitrification process as well as the measurement dynamics for the observer design are derived and discretized on a grid to obtain a system of ordinary differential equations. It is shown that the nonlinearities of the model can be bounded and how the observer gain can be computed via linear matrix inequalities. Furthermore, a model reduction is proposed, which allows the consideration of more grid points. A simulation study demonstrates the proposed approach.      
### 3.MIMO Super-Twisting Controller using a passivity-based design  [ :arrow_down: ](https://arxiv.org/pdf/2208.04276.pdf)
>  A novel MIMO homogeneous Super-Twisting Algorithm is proposed in this paper for nonlinear systems with relative degree one, having a time and state-varying uncertain control matrix. The uncertainty is represented by a constant but unknown left matrix factor. Sufficient conditions for stability with full-matrix control gains are established, in contrast to the usual scalar gains. For this a smooth Lyapunov function, based on a passivity interpretation, is used. Moreover, continuous and homogeneous approximations of the classical discontinuous Super-Twisting algorithm are obtained, using a unified analysis method.      
### 4.AI-based Optimal scheduling of Renewable AC Microgrids with bidirectional LSTM-Based Wind Power Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2208.04156.pdf)
>  In terms of the operation of microgrids, optimal scheduling is a vital issue that must be taken into account. In this regard, this paper proposes an effective framework for optimal scheduling of renewable microgrids considering energy storage devices, wind turbines, micro turbines. Due to the nonlinearity and complexity of operation problems in microgrids, it is vital to use an accurate and robust optimization technique to efficiently solve this problem. To this end, in the proposed framework, the teacher learning-based optimization is utilized to efficiently solve the scheduling problem in the system. Moreover, a deep learning model based on bidirectional long short-term memory is proposed to address the short-term wind power forecasting problem. The feasibility and performance of the proposed framework as well as the effect of wind power forecasting on the operation efficiency are examined using IEEE 33-bus test system. Also, the Australian Wool north wind site data is utilized as a real-world dataset to evaluate the performance of the forecasting model. Results show the effective and efficient performance of the proposed framework in the optimal scheduling of microgrids.      
### 5.Reliability Analysis of Complex Multi-State System Based on Universal Generating Function and Bayesian Network  [ :arrow_down: ](https://arxiv.org/pdf/2208.04130.pdf)
>  In the complex multi-state system (MSS), reliability analysis is a significant research content, both for equipment design, manufacturing, usage and maintenance. Universal Generating Function (UGF) is an important method in the reliability analysis, which efficiently obtains the system reliability by a fast algebraic procedure. However, when structural relationships between subsystems or components are not clear or without explicit expressions, the UGF method is difficult to use or not applicable at all. Bayesian Network (BN) has a natural advantage in terms of uncertainty inference for the relationship without explicit expressions. For the number of components is extremely large, though, it has the defects of low efficiency. To overcome the respective defects of UGF and BN, a novel reliability analysis method called UGF-BN is proposed for the complex MSS. In the UGF-BN framework, the UGF method is firstly used to analyze the bottom components with a large number. Then probability distributions obtained are taken as the input of BN. Finally, the reliability of the complex MSS is modeled by the BN method. This proposed method improves the computational efficiency, especially for the MSS with the large number of bottom components. Besides, the aircraft reliability-based design optimization based on the UGF-BN method is further studied with budget constraints on mass, power, and cost. Finally, two cases are used to demonstrate and verify the proposed method.      
### 6.FRA-RIR: Fast Random Approximation of the Image-source Method  [ :arrow_down: ](https://arxiv.org/pdf/2208.04101.pdf)
>  The training of modern speech processing systems often requires a large amount of simulated room impulse response (RIR) data in order to allow the systems to generalize well in real-world, reverberant environments. However, simulating realistic RIR data typically requires accurate physical modeling, and the acceleration of such simulation process typically requires certain computational platforms such as a graphics processing unit (GPU). In this paper, we propose FRA-RIR, a fast random approximation method of the widely-used image-source method (ISM), to efficiently generate realistic RIR data without specific computational devices. FRA-RIR replaces the physical simulation in the standard ISM by a series of random approximations, which significantly speeds up the simulation process and enables its application in on-the-fly data generation pipelines. Experiments show that FRA-RIR can not only be significantly faster than other existing ISM-based RIR simulation tools on standard computational platforms, but also improves the performance of speech denoising systems evaluated on real-world RIR when trained with simulated RIR. A Python implementation of FRA-RIR is available online\footnote{\url{<a class="link-external link-https" href="https://github.com/yluo42/FRA-RIR" rel="external noopener nofollow">this https URL</a>}}.      
### 7.Image Quality Assessment with Gradient Siamese Network  [ :arrow_down: ](https://arxiv.org/pdf/2208.04081.pdf)
>  In this work, we introduce Gradient Siamese Network (GSN) for image quality assessment. The proposed method is skilled in capturing the gradient features between distorted images and reference images in full-reference image quality assessment(IQA) task. We utilize Central Differential Convolution to obtain both semantic features and detail difference hidden in image pair. Furthermore, spatial attention guides the network to concentrate on regions related to image detail. For the low-level, mid-level and high-level features extracted by the network, we innovatively design a multi-level fusion method to improve the efficiency of feature utilization. In addition to the common mean square error supervision, we further consider the relative distance among batch samples and successfully apply KL divergence loss to the image quality assessment task. We experimented the proposed algorithm GSN on several publicly available datasets and proved its superior performance. Our network won the second place in NTIRE 2022 Perceptual Image Quality Assessment Challenge track 1 Full-Reference.      
### 8.Channel Estimation under Hardware Impairments: Bayesian Methods versus Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.04033.pdf)
>  This paper considers the impact of general hardware impairments in a multiple-antenna base station and user equipments on the uplink performance. First, the effective channels are analytically derived for distortion-aware receivers when using finite-sized signal constellations. Next, a deep feedforward neural network is designed and trained to estimate the effective channels. Its performance is compared with state-of-the-art distortion-aware and unaware Bayesian linear minimum mean-squared error (LMMSE) estimators. The proposed deep learning approach improves the estimation quality by exploiting impairment characteristics, while LMMSE methods treat distortion as noise.      
### 9.Stain-Adaptive Self-Supervised Learning for Histopathology Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.04017.pdf)
>  It is commonly recognized that color variations caused by differences in stains is a critical issue for histopathology image analysis. Existing methods adopt color matching, stain separation, stain transfer or the combination of them to alleviate the stain variation problem. In this paper, we propose a novel Stain-Adaptive Self-Supervised Learning(SASSL) method for histopathology image analysis. Our SASSL integrates a domain-adversarial training module into the SSL framework to learn distinctive features that are robust to both various transformations and stain variations. The proposed SASSL is regarded as a general method for domain-invariant feature extraction which can be flexibly combined with arbitrary downstream histopathology image analysis modules (e.g. nuclei/tissue segmentation) by fine-tuning the features for specific downstream tasks. We conducted experiments on publicly available pathological image analysis datasets including the PANDA, BreastPathQ, and CAMELYON16 datasets, achieving the state-of-the-art performance. Experimental results demonstrate that the proposed method can robustly improve the feature extraction ability of the model, and achieve stable performance improvement in downstream tasks.      
### 10.Ensembled Autoencoder Regularization for Multi-Structure Segmentation for Kidney Cancer Treatment  [ :arrow_down: ](https://arxiv.org/pdf/2208.04007.pdf)
>  The kidney cancer is one of the most common cancer types. The treatment frequently include surgical intervention. However, surgery is in this case particularly challenging due to regional anatomical relations. Organ delineation can significantly improve surgical planning and execution. In this contribution, we propose ensemble of two fully convolutional networks for segmentation of kidney, tumor, veins and arteries. While SegResNet architecture achieved better performance on tumor, the nnU-Net provided more precise segmentation for kidneys, arteries and veins. So in our proposed approach we combine these two networks, and further boost the performance by mixup augmentation.      
### 11.Towards lifelong learning of Recurrent Neural Networks for control design  [ :arrow_down: ](https://arxiv.org/pdf/2208.03980.pdf)
>  This paper proposes a method for lifelong learning of Recurrent Neural Networks, such as NNARX, ESN, LSTM, and GRU, to be used as plant models in control system synthesis. The problem is significant because in many practical applications it is required to adapt the model when new information is available and/or the system undergoes changes, without the need to store an increasing amount of data as time proceeds. Indeed, in this context, many problems arise, such as the well known Catastrophic Forgetting and Capacity Saturation ones. We propose an adaptation algorithm inspired by Moving Horizon Estimators, deriving conditions for its convergence. The described method is applied to a simulated chemical plant, already adopted as a challenging benchmark in the existing literature. The main results achieved are discussed.      
### 12.Exponentially Stable MRAC of MIMO Switched Systems with Matched Uncertainty and Completely Unknown Control Matrix  [ :arrow_down: ](https://arxiv.org/pdf/2208.03972.pdf)
>  In this paper an attempt is made to extend the concept of the exponentially stable adaptive control to one class of multi-input-multi-output (MIMO) plants with matched nonlinearity and unknown piecewise constant parameters. Within the intervals between two consecutive parameter switches, the proposed adaptive control system ensures: 1) exponential convergence to zero of the parameter and reference model tracking errors, 2) the monotonicity of the control law adjustable parameters. Both properties are guaranteed in case the regressor is finitely exciting somewhere inside each of such intervals. Compared to the existing methods, the proposed one is applicable to systems with unknown switching signal function and completely unknown control matrix. The theoretical results are supported by the numerical experiments.      
### 13.NPB-REC: Non-parametric Assessment of Uncertainty in Deep-learning-based MRI Reconstruction from Undersampled Data  [ :arrow_down: ](https://arxiv.org/pdf/2208.03966.pdf)
>  Uncertainty quantification in deep-learning (DL) based image reconstruction models is critical for reliable clinical decision making based on the reconstructed images. We introduce "NPB-REC", a non-parametric fully Bayesian framework for uncertainty assessment in MRI reconstruction from undersampled "k-space" data. We use Stochastic gradient Langevin dynamics (SGLD) during the training phase to characterize the posterior distribution of the network weights. We demonstrated the added-value of our approach on the multi-coil brain MRI dataset, from the fastmri challenge, in comparison to the baseline E2E-VarNet with and without inference-time dropout. Our experiments show that NPB-REC outperforms the baseline by means of reconstruction accuracy (PSNR and SSIM of $34.55$, $0.908$ vs. $33.08$, $0.897$, $p&lt;0.01$) in high acceleration rates ($R=8$). This is also measured in regions of clinical annotations. More significantly, it provides a more accurate estimate of the uncertainty that correlates with the reconstruction error, compared to the Monte-Carlo inference time Dropout method (Pearson correlation coefficient of $R=0.94$ vs. $R=0.91$). The proposed approach has the potential to facilitate safe utilization of DL based methods for MRI reconstruction from undersampled data. Code and trained models are available in \url{<a class="link-external link-https" href="https://github.com/samahkh/NPB-REC" rel="external noopener nofollow">this https URL</a>}.      
### 14.Intelligent MIMO Detection Using Meta Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.03953.pdf)
>  In a K-best detector for multiple-input-multiple-output(MIMO) systems, the value of K needs to be sufficiently large to achieve near-maximum-likelihood (ML) performance. By treating K as a variable that can be adjusted according to a fitting function of some learnable coefficients, an intelligent MIMO detection network based on deep neural networks (DNN) is proposed to reduce complexity of the detection algorithm with little performance degradation. In particular, the proposed intelligent detection algorithm uses meta learning to learn the coefficients of the fitting function for K to circumvent the problem of learning K directly. The idea of network fusion is used to combine the learning results of the meta learning component networks. Simulation results show that the proposed scheme achieves near-ML detection performance while its complexity is close to that of linear detectors. Besides, it also exhibits strong ability of fast training.      
### 15.A Multiple Market Trading Mechanism for Electricity, Renewable Energy Certificate and Carbon Emission Right of Virtual Power Plants  [ :arrow_down: ](https://arxiv.org/pdf/2208.03952.pdf)
>  A multiple market trading mechanism for the VPP to participate in electricity, renewable energy certificate (REC) and carbon emission right (CER) markets is proposed. With the introduction of the inventory mechanism of REC and CER, the profit of the VPP increases and better trading decisions with multiple markets are made under the requirements of renewable portfolio standard (RPS) and carbon emission (CE) quota requirements. According to the Karush-Kuhn-Tucker (KKT) conditions of the proposed model, properties of the multiple market trading mechanism are discussed. Results from case studies verify the effectiveness of the proposed model.      
### 16.Inflating 2D Convolution Weights for Efficient Generation of 3D Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2208.03934.pdf)
>  The generation of three-dimensional (3D) medical images can have great application potential since it takes into account the 3D anatomical structure. There are two problems, however, that prevent effective training of a 3D medical generative model: (1) 3D medical images are very expensive to acquire and annotate, resulting in an insufficient number of training images, (2) a large number of parameters are involved in 3D convolution. To address both problems, we propose a novel GAN model called 3D Split&amp;Shuffle-GAN. In order to address the 3D data scarcity issue, we first pre-train a two-dimensional (2D) GAN model using abundant image slices and inflate the 2D convolution weights to improve initialization of the 3D GAN. Novel 3D network architectures are proposed for both the generator and discriminator of the GAN model to significantly reduce the number of parameters while maintaining the quality of image generation. A number of weight inflation strategies and parameter-efficient 3D architectures are investigated. Experiments on both heart (Stanford AIMI Coronary Calcium) and brain (Alzheimer's Disease Neuroimaging Initiative) datasets demonstrate that the proposed approach leads to improved 3D images generation quality with significantly fewer parameters.      
### 17.Self-supervised Enhanced Radar Imaging Based on Deep-Learning-Assisted Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2208.03911.pdf)
>  Traditional radar imaging methods suffer from the problems of low resolution and poor noise suppression. We propose a new radar imaging method based on Self-supervised deep-learning-assisted compressed sensing (SS-DL-CS-Net). The original radar image as the input of net. The net is trained to learn the mapping function between the original radar image and the high quality radar image. However, the high quality radar image cant be obtained. We solve this problem by used the sparsity of radar image. The original radar image and image with the zeros value as the reference of net. Ours net dont need a lot of data to train. Real radar data are used to evaluate the performance of the proposed method. The experimental results demonstrate the superiority of the proposed method      
### 18.Towards Analytical Electromagnetic Models for Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2208.03905.pdf)
>  Physically accurate and mathematically tractable models are presented to characterize scattering and reflection properties of reconfigurable intelligent surfaces (RISs). We take continuous and discrete strategies to model a single patch and patch array and their interactions with multiple incident electromagnetic (EM) waves. The proposed models consider the effect of the incident and scattered angles, polarization features, and the topology and geometry of RISs. Particularly, a simple system of linear equations can describe the multiple-input multiple-output (MIMO) behaviors of RISs under reasonable assumptions. It can serve as a fundamental model for analyzing and optimizing the performance of RIS-aided systems in the far-field regime. The proposed models are employed to identify the advantages and limitations of three typical configurations. One important finding is that complicated beam reshaping functionality can not be endowed by popular phase compensation configurations. A possible solution is the simultaneous configurations of collecting area and phase shifting. We finally discuss the inherent inverse sensing capabilities of RISs. The reconfigurability is crucial for wireless environmental sensing. Numerical simulations verify the effectiveness of the proposed inverse sensing scheme.      
### 19.SelfCoLearn: Self-supervised collaborative learning for accelerating dynamic MR imaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.03904.pdf)
>  Lately, deep learning has been extensively investigated for accelerating dynamic magnetic resonance (MR) imaging, with encouraging progresses achieved. However, without fully sampled reference data for training, current approaches may have limited abilities in recovering fine details or structures. To address this challenge, this paper proposes a self-supervised collaborative learning framework (SelfCoLearn) for accurate dynamic MR image reconstruction from undersampled k-space data. The proposed framework is equipped with three important components, namely, dual-network collaborative learning, reunderampling data augmentation and a specially designed co-training loss. The framework is flexible to be integrated with both data-driven networks and model-based iterative un-rolled networks. Our method has been evaluated on in-vivo dataset and compared it to four state-of-the-art methods. Results show that our method possesses strong capabilities in capturing essential and inherent representations for direct reconstructions from the undersampled k-space data and thus enables high-quality and fast dynamic MR imaging.      
### 20.Data-centric AI approach to improve optic nerve head segmentation and localization in OCT en face images  [ :arrow_down: ](https://arxiv.org/pdf/2208.03868.pdf)
>  The automatic detection and localization of anatomical features in retinal imaging data are relevant for many aspects. In this work, we follow a data-centric approach to optimize classifier training for optic nerve head detection and localization in optical coherence tomography en face images of the retina. We examine the effect of domain knowledge driven spatial complexity reduction on the resulting optic nerve head segmentation and localization performance. We present a machine learning approach for segmenting optic nerve head in 2D en face projections of 3D widefield swept source optical coherence tomography scans that enables the automated assessment of large amounts of data. Evaluation on manually annotated 2D en face images of the retina demonstrates that training of a standard U-Net can yield improved optic nerve head segmentation and localization performance when the underlying pixel-level binary classification task is spatially relaxed through domain knowledge.      
### 21.Distributed Contrastive Learning for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2208.03808.pdf)
>  Supervised deep learning needs a large amount of labeled data to achieve high performance. However, in medical imaging analysis, each site may only have a limited amount of data and labels, which makes learning ineffective. Federated learning (FL) can learn a shared model from decentralized data. But traditional FL requires fully-labeled data for training, which is very expensive to obtain. Self-supervised contrastive learning (CL) can learn from unlabeled data for pre-training, followed by fine-tuning with limited annotations. However, when adopting CL in FL, the limited data diversity on each site makes federated contrastive learning (FCL) ineffective. In this work, we propose two federated self-supervised learning frameworks for volumetric medical image segmentation with limited annotations. The first one features high accuracy and fits high-performance servers with high-speed connections. The second one features lower communication costs, suitable for mobile devices. In the first framework, features are exchanged during FCL to provide diverse contrastive data to each site for effective local CL while keeping raw data private. Global structural matching aligns local and remote features for a unified feature space among different sites. In the second framework, to reduce the communication cost for feature exchanging, we propose an optimized method FCLOpt that does not rely on negative samples. To reduce the communications of model download, we propose the predictive target network update (PTNU) that predicts the parameters of the target network. Based on PTNU, we propose the distance prediction (DP) to remove most of the uploads of the target network. Experiments on a cardiac MRI dataset show the proposed two frameworks substantially improve the segmentation and generalization performance compared with state-of-the-art techniques.      
### 22.Exploring Long &amp; Short Range Temporal Information for Learned Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2208.03754.pdf)
>  Learned video compression methods have gained a variety of interest in the video coding community since they have matched or even exceeded the rate-distortion (RD) performance of traditional video codecs. However, many current learning-based methods are dedicated to utilizing short-range temporal information, thus limiting their performance. In this paper, we focus on exploiting the unique characteristics of video content and further exploring temporal information to enhance compression performance. Specifically, for long-range temporal information exploitation, we propose temporal prior that can update continuously within the group of pictures (GOP) during inference. In that case temporal prior contains valuable temporal information of all decoded images within the current GOP. As for short-range temporal information, we propose a progressive guided motion compensation to achieve robust and effective compensation. In detail, we design a hierarchical structure to achieve multi-scale compensation. More importantly, we use optical flow guidance to generate pixel offsets between feature maps at each scale, and the compensation results at each scale will be used to guide the following scale's compensation. Sufficient experimental results demonstrate that our method can obtain better RD performance than state-of-the-art video compression approaches. The code is publicly available on: <a class="link-external link-https" href="https://github.com/Huairui/LSTVC" rel="external noopener nofollow">this https URL</a>.      
### 23.Automatic reorientation by deep learning to generate short axis SPECT myocardial perfusion images  [ :arrow_down: ](https://arxiv.org/pdf/2208.03752.pdf)
>  Single photon emission computed tomography (SPECT) myocardial perfusion images (MPI) can be displayed both in traditional short-axis (SA) cardiac planes and polar maps for interpretation and quantification. It is essential to reorient the reconstructed transaxial SPECT MPI into standard SA slices. This study is aimed to develop a deep-learning-based approach for automatic reorientation of MPI. Methods: A total of 254 patients were enrolled, including 228 stress SPECT MPIs and 248 rest SPECT MPIs. Five-fold cross-validation with 180 stress and 201 rest MPIs was used for training and internal validation; the remaining images were used for testing. The rigid transformation parameters (translation and rotation) from manual reorientation were annotated by an experienced operator and used as the ground truth. A convolutional neural network (CNN) was designed to predict the transformation parameters. Then, the derived transform was applied to the grid generator and sampler in spatial transformer network (STN) to generate the reoriented image. A loss function containing mean absolute errors for translation and mean square errors for rotation was employed. A three-stage optimization strategy was adopted for model optimization: 1) optimize the translation parameters while fixing the rotation parameters; 2) optimize rotation parameters while fixing the translation parameters; 3) optimize both translation and rotation parameters together.      
### 24.Omni-directional Pathloss Measurement Based on Virtual Antenna Array with Directional Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2208.03750.pdf)
>  Omni-directional pathloss, which refers to the pathloss when omni-directional antennas are used at the link ends, are essential for system design and evaluation. In the millimeter-wave (mm-Wave) and beyond bands, high gain directional antennas are widely used for channel measurements due to the significant signal attenuation. Conventional methods for omni-directional pathloss estimation are based on directional scanning sounding (DSS) system, i.e., a single directional antenna placed at the center of a rotator capturing signals from different rotation angles. The omni-directional pathloss is obtained by either summing up all the powers above the noise level or just summing up the powers of detected propagation paths. However, both methods are problematic with relatively wide main beams and high side-lobes provided by the directional antennas. In this letter, directional antenna based virtual antenna array (VAA) system is implemented for omni-directional pathloss estimation. The VAA scheme uses the same measurement system as the DSS, yet it offers high angular resolution (i.e. narrow main beam) and low side-lobes, which is essential for achieving accurate multipath detection in the power angular delay profiles (PADPs) and thereby obtaining accurate omni-directional pathloss. A measurement campaign was designed and conducted in an indoor corridor at 28-30 GHz to verify the effectiveness of the proposed method.      
### 25.Optimal Parking Planning for Shared Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2208.03718.pdf)
>  Parking is a crucial element of the driving experience in urban transportation systems. Especially in the coming era of Shared Autonomous Vehicles (SAVs), parking operations in urban transportation networks will inevitably change. Parking stations will serve as storage places for unused vehicles and depots that control the level-of-service of SAVs. This study presents an Analytical Parking Planning Model (APPM) for the SAV environment to provide broader insights into parking planning decisions. Two specific planning scenarios are considered for the APPM: (i) Single-zone APPM (S-APPM), which considers the target area as a single homogeneous zone, and (ii) Two-zone APPM (T-APPM), which considers the target area as two different zones, such as city center and suburban area. S-APPM offers a closed-form solution to find the optimal density of parking stations and parking spaces and the optimal number of SAV fleets, which is beneficial for understanding the explicit relationship between planning decisions and the given environments, including demand density and cost factors. In addition, to incorporate different macroscopic characteristics across two zones, T-APPM accounts for inter- and intra-zonal passenger trips and the relocation of vehicles. We conduct a case study to demonstrate the proposed method with the actual data collected in Seoul Metropolitan Area, South Korea. Sensitivity analyses with respect to cost factors are performed to provide decision-makers with further insights. Also, we find that the optimal densities of parking stations and spaces in the target area are much lower than the current situations.      
### 26.Image denoising in acoustic field microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2208.03688.pdf)
>  Scanning acoustic microscopy (SAM) has been employed since microscopic images are widely used for biomedical or materials research. Acoustic imaging is an important and well-established method used in nondestructive testing (NDT), bio-medical imaging, and structural health monitoring.The imaging is frequently carried out with signals of low amplitude, which might result in leading that are noisy and lacking in details of image information. In this work, we attempted to analyze SAM images acquired from low amplitude signals and employed a block matching filter over time domain signals to obtain a denoised image. We have compared the images with conventional filters applied over time domain signals, such as the gaussian filter, median filter, wiener filter, and total variation filter. The noted outcomes are shown in this article.      
### 27.Terahertz-Band Channel and Beam Split Estimation via Array Perturbation Model  [ :arrow_down: ](https://arxiv.org/pdf/2208.03683.pdf)
>  For the demonstration of ultra-wideband bandwidth and pencil-beamforming, the terahertz (THz)-band has been envisioned as one of the key enabling technologies for the sixth generation networks. However, the acquisition of the THz channel entails several unique challenges such as severe path loss and beam-split. Prior works usually employ ultra-massive arrays and additional hardware components comprised of time-delayers to compensate for these loses. In order to provide a cost-effective solution, this paper introduces a sparse-Bayesian-learning (SBL) technique for joint channel and beam-split estimation. Specifically, we first model the beam-split as an array perturbation inspired from array signal processing. Next, a low-complexity approach is developed by exploiting the line-of-sight-dominant feature of THz channel to reduce the computational complexity involved in the proposed SBL technique for channel estimation (SBCE). Additionally, based on federated-learning, we implement a model-free technique to the proposed model-based SBCE solution. Further to that, we examine the near-field considerations of THz channel, and introduce the range-dependent near-field beam-split. The theoretical performance bounds, i.e., Cramér-Rao lower bounds, are derived for near- and far-field parameters, e.g., user directions, ranges and beam-split, and several numerical experiments are conducted. Numerical simulations demonstrate that SBCE outperforms the existing approaches and exhibits lower hardware cost.      
### 28.Energy Efficiency Analysis of Charging Pads-powered UAV-enabled Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.03649.pdf)
>  This paper analyzes the energy efficiency of a novel system model where unmanned aerial vehicles (UAVs) are used to provide coverage for user hotspots (user clusters) and are deployed on charging pads to enhance the flight time. We introduce a new notion of "cluster pairs" to capture the dynamic nature of the users' spatial distribution in order to exploit one of the top advantages of UAVs, which is the mobility and relocation flexibility. Using tools from stochastic geometry, we first derive a new distance distribution that is vital for energy efficiency analysis. Next, we compute the coverage probability under two deployment strategies: (i) one UAV per cluster pair, and (ii) one UAV per cluster. Finally, we compute the energy efficiency for both strategies. Our numerical results reveal which of the two strategies is better for different system parameters. Our work investigates some new aspects of the UAV-enabled communication system such as the dynamic density of users and the advantages or disadvantages of one- or two-UAV deployment strategies per cluster pair. By considering the relationships between the densities of user cluster pairs and the charging pads, it is shown that an optimal cluster pair density exists to maximize energy efficiency.      
### 29.Continual Learning for Tumor Classification in Histopathology Images  [ :arrow_down: ](https://arxiv.org/pdf/2208.03609.pdf)
>  Recent years have seen great advancements in the development of deep learning models for histopathology image analysis in digital pathology applications, evidenced by the increasingly common deployment of these models in both research and clinical settings. Although such models have shown unprecedented performance in solving fundamental computational tasks in DP applications, they suffer from catastrophic forgetting when adapted to unseen data with transfer learning. With an increasing need for deep learning models to handle ever changing data distributions, including evolving patient population and new diagnosis assays, continual learning models that alleviate model forgetting need to be introduced in DP based analysis. However, to our best knowledge, there is no systematic study of such models for DP-specific applications. Here, we propose CL scenarios in DP settings, where histopathology image data from different sources/distributions arrive sequentially, the knowledge of which is integrated into a single model without training all the data from scratch. We then established an augmented dataset for colorectal cancer H&amp;E classification to simulate shifts of image appearance and evaluated CL model performance in the proposed CL scenarios. We leveraged a breast tumor H&amp;E dataset along with the colorectal cancer to evaluate CL from different tumor types. In addition, we evaluated CL methods in an online few-shot setting under the constraints of annotation and computational resources. We revealed promising results of CL in DP applications, potentially paving the way for application of these methods in clinical practice.      
### 30.Optimal Operation of HVDC Interconnector: Irish Case  [ :arrow_down: ](https://arxiv.org/pdf/2208.03593.pdf)
>  In September 2018 EirGrid launched the new electricity market. These new market arrangements integrate the all island electricity market with European electricity markets, making optimal use of cross border transmission assets. Ireland operates three operational HVDC interconnectors: Moyle, East-West, and Greenlink and one in development Celtic interconnector which connect Ireland to Scotland, Wales, and France respectively. Irish market operator, EirGrid, can maximize their operational profit by using the price difference in these electricity markets. We propose a profit maximization modelling which considers the line losses and price difference in these different electricity markets and identifies the optimal import/export of power using HVDC interconnectors. These models in future should incorporate the distribution losses, renewable energy curtailment, and Irish power network congestion levels. The proposed modeling is the first step towards implementing a multi-objective HVDC interconnector operating strategy.      
### 31.Optimal transport through a toll  [ :arrow_down: ](https://arxiv.org/pdf/2208.03587.pdf)
>  We address the problem of optimal transport with a quadratic cost functional and a constraint on the flux through a constriction along the path. The constriction, conceptually represented by a toll station, limits the flow rate across. We provide a precise formulation which, in addition, is amenable to generalization in higher dimensions. We work out in detail the case of transport in one dimension by proving existence and uniqueness of solution. Under suitable regularity assumptions we give an explicit construction of the transport plan. Generalization of flux constraints to higher dimensions and possible extensions of the theory are discussed.      
### 32.Constrained self-supervised method with temporal ensembling for fiber bundle detection on anatomic tracing data  [ :arrow_down: ](https://arxiv.org/pdf/2208.03569.pdf)
>  Anatomic tracing data provides detailed information on brain circuitry essential for addressing some of the common errors in diffusion MRI tractography. However, automated detection of fiber bundles on tracing data is challenging due to sectioning distortions, presence of noise and artifacts and intensity/contrast variations. In this work, we propose a deep learning method with a self-supervised loss function that takes anatomy-based constraints into account for accurate segmentation of fiber bundles on the tracer sections from macaque brains. Also, given the limited availability of manual labels, we use a semi-supervised training technique for efficiently using unlabeled data to improve the performance, and location constraints for further reduction of false positives. Evaluation of our method on unseen sections from a different macaque yields promising results with a true positive rate of ~0.90. The code for our method is available at <a class="link-external link-https" href="https://github.com/v-sundaresan/fiberbundle_seg_tracing" rel="external noopener nofollow">this https URL</a>.      
### 33.An Adaptive and Altruistic PSO-based Deep Feature Selection Method for Pneumonia Detection from Chest X-Rays  [ :arrow_down: ](https://arxiv.org/pdf/2208.03558.pdf)
>  Pneumonia is one of the major reasons for child mortality especially in income-deprived regions of the world. Although it can be detected and treated with very less sophisticated instruments and medication, Pneumonia detection still remains a major concern in developing countries. Computer-aided based diagnosis (CAD) systems can be used in such countries due to their lower operating costs than professional medical experts. In this paper, we propose a CAD system for Pneumonia detection from Chest X-rays, using the concepts of deep learning and a meta-heuristic algorithm. We first extract deep features from the pre-trained ResNet50, fine-tuned on a target Pneumonia dataset. Then, we propose a feature selection technique based on particle swarm optimization (PSO), which is modified using a memory-based adaptation parameter, and enriched by incorporating an altruistic behavior into the agents. We name our feature selection method as adaptive and altruistic PSO (AAPSO). The proposed method successfully eliminates non-informative features obtained from the ResNet50 model, thereby improving the Pneumonia detection ability of the overall framework. Extensive experimentation and thorough analysis on a publicly available Pneumonia dataset establish the superiority of the proposed method over several other frameworks used for Pneumonia detection. Apart from Pneumonia detection, AAPSO is further evaluated on some standard UCI datasets, gene expression datasets for cancer prediction and a COVID-19 prediction dataset. The overall results are satisfactory, thereby confirming the usefulness of AAPSO in dealing with varied real-life problems. The supporting source codes of this work can be found at <a class="link-external link-https" href="https://github.com/rishavpramanik/AAPSO" rel="external noopener nofollow">this https URL</a>      
### 34.An Accurate and Explainable Deep Learning System Improves Interobserver Agreement in the Interpretation of Chest Radiograph  [ :arrow_down: ](https://arxiv.org/pdf/2208.03545.pdf)
>  Recent artificial intelligence (AI) algorithms have achieved radiologist-level performance on various medical classification tasks. However, only a few studies addressed the localization of abnormal findings from CXR scans, which is essential in explaining the image-level classification to radiologists. We introduce in this paper an explainable deep learning system called VinDr-CXR that can classify a CXR scan into multiple thoracic diseases and, at the same time, localize most types of critical findings on the image. VinDr-CXR was trained on 51,485 CXR scans with radiologist-provided bounding box annotations. It demonstrated a comparable performance to experienced radiologists in classifying 6 common thoracic diseases on a retrospective validation set of 3,000 CXR scans, with a mean area under the receiver operating characteristic curve (AUROC) of 0.967 (95% confidence interval [CI]: 0.958-0.975). The VinDr-CXR was also externally validated in independent patient cohorts and showed its robustness. For the localization task with 14 types of lesions, our free-response receiver operating characteristic (FROC) analysis showed that the VinDr-CXR achieved a sensitivity of 80.2% at the rate of 1.0 false-positive lesion identified per scan. A prospective study was also conducted to measure the clinical impact of the VinDr-CXR in assisting six experienced radiologists. The results indicated that the proposed system, when used as a diagnosis supporting tool, significantly improved the agreement between radiologists themselves with an increase of 1.5% in mean Fleiss' Kappa. We also observed that, after the radiologists consulted VinDr-CXR's suggestions, the agreement between each of them and the system was remarkably increased by 3.3% in mean Cohen's Kappa.      
### 35.Stochastic MPC with Dual Control for Autonomous Driving with Multi-Modal Interaction-Aware Predictions  [ :arrow_down: ](https://arxiv.org/pdf/2208.03525.pdf)
>  We propose a Stochastic MPC (SMPC) approach for autonomous driving which incorporates multi-modal, interaction-aware predictions of surrounding vehicles. For each mode, vehicle motion predictions are obtained by a control model described using a basis of fixed features with unknown weights. The proposed SMPC formulation finds optimal controls which serves two purposes: 1) reducing conservatism of the SMPC by optimizing over parameterized control laws and 2) prediction and estimation of feature weights used in interaction-aware modeling using Kalman filtering. The proposed approach is demonstrated on a longitudinal control example, with uncertainties in predictions of the autonomous and surrounding vehicles.      
### 36.Deep Learning-enabled Spatial Phase Unwrapping for 3D Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2208.03524.pdf)
>  In terms of 3D imaging speed and system cost, the single-camera system projecting single-frequency patterns is the ideal option among all proposed Fringe Projection Profilometry (FPP) systems. This system necessitates a robust spatial phase unwrapping (SPU) algorithm. However, robust SPU remains a challenge in complex scenes. Quality-guided SPU algorithms need more efficient ways to identify the unreliable points in phase maps before unwrapping. End-to-end deep learning SPU methods face generality and interpretability problems. This paper proposes a hybrid method combining deep learning and traditional path-following for robust SPU in FPP. This hybrid SPU scheme demonstrates better robustness than traditional quality-guided SPU methods, better interpretability than end-to-end deep learning scheme, and generality on unseen data. Experiments on the real dataset of multiple illumination conditions and multiple FPP systems differing in image resolution, the number of fringes, fringe direction, and optics wavelength verify the effectiveness of the proposed method.      
### 37.Compositional Reinforcement Learning for Discrete-Time Stochastic Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.03485.pdf)
>  We propose a compositional approach to synthesize policies for networks of continuous-space stochastic control systems with unknown dynamics using model-free reinforcement learning (RL). The approach is based on implicitly abstracting each subsystem in the network with a finite Markov decision process with unknown transition probabilities, synthesizing a strategy for each abstract model in an assume-guarantee fashion using RL, and then mapping the results back over the original network with approximate optimality guarantees. We provide lower bounds on the satisfaction probability of the overall network based on those over individual subsystems. A key contribution is to leverage the convergence results for adversarial RL (minimax Q-learning) on finite stochastic arenas to provide control strategies maximizing the probability of satisfaction over the network of continuous-space systems. We consider finite-horizon properties expressed in the syntactically co-safe fragment of linear temporal logic. These properties can readily be converted into automata-based reward functions, providing scalar reward signals suitable for RL. Since such reward functions are often sparse, we supply a potential-based reward shaping technique to accelerate learning by producing dense rewards. The effectiveness of the proposed approaches is demonstrated via two physical benchmarks including regulation of a room temperature network and control of a road traffic network.      
### 38.Safety Barrier Certificates for Stochastic Hybrid Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.03478.pdf)
>  This work is concerned with the safety controller synthesis of stochastic hybrid systems, in which continuous evolutions are described by stochastic differential equations with both Brownian motions and Poisson processes, and instantaneous jumps are governed by stochastic difference equations with additive noises. Our proposed framework leverages the notion of control barrier certificates (CBC), as a discretization-free approach, to synthesize safety controllers for stochastic hybrid systems while providing safety guarantees in finite time horizons. In our proposed scheme, we first provide an augmented framework to characterize each stochastic hybrid system containing continuous evolutions and instantaneous jumps with a unified system covering both scenarios. We then introduce an augmented control barrier certificate (ACBC) for augmented systems and propose sufficient conditions to construct an ACBC based on CBC of original hybrid systems. By utilizing the constructed ACBC, we quantify upper bounds on the probability that the stochastic hybrid system reaches certain unsafe regions in a finite time horizon. The proposed approach is verified over a nonlinear case study.      
### 39.Compositional Controller Synthesis for Interconnected Stochastic Systems with Markovian Switching  [ :arrow_down: ](https://arxiv.org/pdf/2208.03476.pdf)
>  In this work, we propose a compositional scheme for the safety controller synthesis of interconnected discrete-time stochastic systems with Markovian switching signals. Our proposed approach is based on a notion of so-called control storage certificates computed for individual subsystems, by leveraging which, one can synthesize state-feedback controllers for interconnected systems to enforce safety specifications over finite time horizons. To do so, we employ a sum-of-squares (SOS) optimization approach to search for multiple storage certificates of each switching subsystem while synthesizing its corresponding safety controller. We then utilize dissipativity theory to compositionally construct barrier certificates for interconnected systems based on storage certificates of individual subsystems. The proposed dissipativity-type compositional conditions can leverage the structure of the interconnection topology and be fulfilled independently of the number or gains of subsystems. We eventually employ the constructed barrier certificate and quantify upper bounds on the probability that the interconnected system reaches certain unsafe regions in a finite time horizon. We apply our results to a room temperature network of 200 rooms with Markovian switching signals while accepting multiple storage certificates. We compositionally synthesize safety controllers to maintain the temperature of each room in a comfort zone for a bounded time horizon.      
### 40.Prediction-based Hybrid Slicing Framework for Service Level Agreement Guarantee in Mobility Scenarios: A Deep Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.03460.pdf)
>  Network slicing is a critical driver for guaranteeing the diverse service level agreements (SLA) in 5G and future networks. Inter-slice radio resource allocation (IS-RRA) in the radio access network (RAN) is very important. However, user mobility brings new challenges for optimal IS-RRA. This paper first proposes a soft and hard hybrid slicing framework where a common slice is introduced to realize a trade-off between isolation and spectrum efficiency (SE). To address the challenges posed by user mobility, we propose a two-step deep learning-based algorithm: joint long short-term memory (LSTM)-based network state prediction and deep Q network (DQN)-based slicing strategy. In the proposal, LSTM networks are employed to predict traffic demand and the location of each user in a slicing window level. Moreover, channel gain is mapped by location and a radio map. Then, the predicted channel gain and traffic demand are input to the DQN to output the precise slicing adjustment. Finally, experiment results confirm the effectiveness of our proposed slicing framework: the slices' SLA can be guaranteed well, and the proposed algorithm can achieve near-optimal performance in terms of the SLA satisfaction ratio, isolation degree and SE.      
### 41.Towards a real-time continuous ultrafast ultrasound beamformer with programmable logic  [ :arrow_down: ](https://arxiv.org/pdf/2208.03429.pdf)
>  Ultrafast ultrasound imaging is essential for advanced ultrasound imaging techniques such as ultrasound localization microscopy (ULM) and functional ultrasound (fUS). Current ultrafast ultrasound imaging is challenged by the ultrahigh data bandwidth associated with the radio frequency (RF) signal, and by the latency of the computationally expensive beamforming process. As such, continuous ultrafast data acquisition and beamforming remain elusive with existing software beamformers based on CPUs or GPUs. To address these challenges, the proposed work introduces a hybrid solution composed of an improved delay and sum (DAS) algorithm with high hardware efficiency and an ultrafast beamformer based on the field programmable gate array (FPGA). Our proposed method presents two unique advantages over conventional FPGA-based beamformers: 1) high scalability that allows fast adaptation to different FPGA platforms; 2) high adaptability to different imaging probes and applications thanks to the absence of hard-coded imaging parameters. With the proposed method, we measured an ultrafast beamforming frame rate of over 3.38 GPixels/second. The performance of the proposed beamformer was compared with the software beamformer on the Verasonics Vantage system for both phantom imaging and in vivo imaging of a mouse brain. Multiple imaging schemes including B-mode, power Doppler and ULM were evaluated with the proposed solution.      
### 42.SSDPT: Self-Supervised Dual-Path Transformer for Anomalous Sound Detection in Machine Condition Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2208.03421.pdf)
>  Anomalous sound detection for machine condition monitoring has great potential in the development of Industry 4.0. However, these anomalous sounds of machines are usually unavailable in normal conditions. Therefore, the models employed have to learn acoustic representations with normal sounds for training, and detect anomalous sounds while testing. In this article, we propose a self-supervised dual-path Transformer (SSDPT) network to detect anomalous sounds in machine monitoring. The SSDPT network splits the acoustic features into segments and employs several DPT blocks for time and frequency modeling. DPT blocks use attention modules to alternately model the interactive information about the frequency and temporal components of the segmented acoustic features. To address the problem of lack of anomalous sound, we adopt a self-supervised learning approach to train the network with normal sound. Specifically, this approach randomly masks and reconstructs the acoustic features, and jointly classifies machine identity information to improve the performance of anomalous sound detection. We evaluated our method on the DCASE2021 task2 dataset. The experimental results show that the SSDPT network achieves a significant increase in the harmonic mean AUC score, in comparison to present state-of-the-art methods of anomalous sound detection.      
### 43.Noise-Type Radars: Probability of Detection vs. Correlation Coefficient and Integration Time  [ :arrow_down: ](https://arxiv.org/pdf/2208.03417.pdf)
>  Noise radars have the same mathematical description as a type of quantum radar known as quantum two-mode squeezing radar. Although their physical implementations are very different, this mathematical similarity allows us to analyze them collectively. We may consider the two types of radars as forming a single class of radars, called noise-type radars. The target detection performance of noise-type radars depends on two parameters: the number of integrated samples and a correlation coefficient. In this paper, we show that when the number of integrated samples is large and the correlation coefficient is low, the detection performance becomes a function of a single parameter: the number of integrated samples multiplied by the square of the correlation coefficient. We then explore the detection performance of noise-type radars in terms of this emergent parameter; in particular, we determine the probability of detection as a function of this parameter.      
### 44.Trust-Aware Control of Automated Vehicles in Car-Following Interactions with Human Drivers  [ :arrow_down: ](https://arxiv.org/pdf/2208.03385.pdf)
>  Trust is essential for automated vehicles (AVs) to promote and sustain technology acceptance in human-dominated traffic scenarios. However, computational trust dynamic models describing the interactive relationship between the AVs and surrounding human drivers in traffic rarely exist. This paper aims to fill this gap by developing a quantitative trust dynamic model of the human driver in the car-following interaction with the AV and incorporating the proposed trust dynamic model into the AV's control design. The human driver's trust level is modeled as a plan evaluation metric that measures the explicability of the AV's plan from the human driver's perspective, and the explicability score of the AV's plan is integrated into the AV's decision-making process. With the proposed approach, trust-aware AVs generate explicable plans by optimizing both predefined plans and explicability of the plans in the car-following interactions with the following human driver. The results collectively demonstrate that the trust-aware AV can generate more explicable plans and achieve a higher trust level for the human driver compared to trust-unaware AV in human-AV interactions.      
### 45.A Spatially Separable Attention Mechanism for massive MIMO CSI Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2208.03369.pdf)
>  Channel State Information (CSI) Feedback plays a crucial role in achieving higher gains through beamforming. However, for a massive MIMO system, this feedback overhead is huge and grows linearly with the number of antennas. To reduce the feedback overhead several compressive sensing (CS) techniques were implemented in recent years but these techniques are often iterative and are computationally complex to realize in power-constrained user equipment (UE). Hence, a data-based deep learning approach took over in these recent years introducing a variety of neural networks for CSI compression. Specifically, transformer-based networks have been shown to achieve state-of-the-art performance. However, the multi-head attention operation, which is at the core of transformers, is computationally complex making transformers difficult to implement on a UE. In this work, we present a lightweight transformer named STNet which uses a spatially separable attention mechanism that is significantly less complex than the traditional full-attention. Equipped with this, STNet outperformed state-of-the-art models in some scenarios with approximately $1/10^{th}$ of the resources.      
### 46.Seamless Iterative Semi-Supervised Correction of Imperfect Labels in Microscopy Images  [ :arrow_down: ](https://arxiv.org/pdf/2208.03327.pdf)
>  In-vitro tests are an alternative to animal testing for the toxicity of medical devices. Detecting cells as a first step, a cell expert evaluates the growth of cells according to cytotoxicity grade under the microscope. Thus, human fatigue plays a role in error making, making the use of deep learning appealing. Due to the high cost of training data annotation, an approach without manual annotation is needed. We propose Seamless Iterative Semi-Supervised correction of Imperfect labels (SISSI), a new method for training object detection models with noisy and missing annotations in a semi-supervised fashion. Our network learns from noisy labels generated with simple image processing algorithms, which are iteratively corrected during self-training. Due to the nature of missing bounding boxes in the pseudo labels, which would negatively affect the training, we propose to train on dynamically generated synthetic-like images using seamless cloning. Our method successfully provides an adaptive early learning correction technique for object detection. The combination of early learning correction that has been applied in classification and semantic segmentation before and synthetic-like image generation proves to be more effective than the usual semi-supervised approach by &gt; 15% AP and &gt; 20% AR across three different readers. Our code is available at <a class="link-external link-https" href="https://github.com/marwankefah/SISSI" rel="external noopener nofollow">this https URL</a>.      
### 47.Perception-Distortion Balanced ADMM Optimization for Single-Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2208.03324.pdf)
>  In image super-resolution, both pixel-wise accuracy and perceptual fidelity are desirable. However, most deep learning methods only achieve high performance in one aspect due to the perception-distortion trade-off, and works that successfully balance the trade-off rely on fusing results from separately trained models with ad-hoc post-processing. In this paper, we propose a novel super-resolution model with a low-frequency constraint (LFc-SR), which balances the objective and perceptual quality through a single model and yields super-resolved images with high PSNR and perceptual scores. We further introduce an ADMM-based alternating optimization method for the non-trivial learning of the constrained model. Experiments showed that our method, without cumbersome post-processing procedures, achieved the state-of-the-art performance. The code is available at <a class="link-external link-https" href="https://github.com/Yuehan717/PDASR" rel="external noopener nofollow">this https URL</a>.      
### 48.DeepWSD: Projecting Degradations in Perceptual Space to Wasserstein Distance in Deep Feature Space  [ :arrow_down: ](https://arxiv.org/pdf/2208.03323.pdf)
>  Existing deep learning-based full-reference IQA (FR-IQA) models usually predict the image quality in a deterministic way by explicitly comparing the features, gauging how severely distorted an image is by how far the corresponding feature lies from the space of the reference images. Herein, we look at this problem from a different viewpoint and propose to model the quality degradation in perceptual space from a statistical distribution perspective. As such, the quality is measured based upon the Wasserstein distance in the deep feature domain. More specifically, the 1DWasserstein distance at each stage of the pre-trained VGG network is measured, based on which the final quality score is performed. The deep Wasserstein distance (DeepWSD) performed on features from neural networks enjoys better interpretability of the quality contamination caused by various types of distortions and presents an advanced quality prediction capability. Extensive experiments and theoretical analysis show the superiority of the proposed DeepWSD in terms of both quality prediction and optimization.      
### 49.Underwater enhancement based on a self-learning strategy and attention mechanism for high-intensity regions  [ :arrow_down: ](https://arxiv.org/pdf/2208.03319.pdf)
>  Images acquired during underwater activities suffer from environmental properties of the water, such as turbidity and light attenuation. These phenomena cause color distortion, blurring, and contrast reduction. In addition, irregular ambient light distribution causes color channel unbalance and regions with high-intensity pixels. Recent works related to underwater image enhancement, and based on deep learning approaches, tackle the lack of paired datasets generating synthetic ground-truth. In this paper, we present a self-supervised learning methodology for underwater image enhancement based on deep learning that requires no paired datasets. The proposed method estimates the degradation present in underwater images. Besides, an autoencoder reconstructs this image, and its output image is degraded using the estimated degradation information. Therefore, the strategy replaces the output image with the degraded version in the loss function during the training phase. This procedure \textit{misleads} the neural network that learns to compensate the additional degradation. As a result, the reconstructed image is an enhanced version of the input image. Also, the algorithm presents an attention module to reduce high-intensity areas generated in enhanced images by color channel unbalances and outlier regions. Furthermore, the proposed methodology requires no ground-truth. Besides, only real underwater images were used to train the neural network, and the results indicate the effectiveness of the method in terms of color preservation, color cast reduction, and contrast improvement.      
### 50.Image Quality Assessment: Learning to Rank Image Distortion Level  [ :arrow_down: ](https://arxiv.org/pdf/2208.03317.pdf)
>  Over the years, various algorithms were developed, attempting to imitate the Human Visual System (HVS), and evaluate the perceptual image quality. However, for certain image distortions, the functionality of the HVS continues to be an enigma, and echoing its behavior remains a challenge (especially for ill-defined distortions). In this paper, we learn to compare the image quality of two registered images, with respect to a chosen distortion. Our method takes advantage of the fact that at times, simulating image distortion and later evaluating its relative image quality, is easier than assessing its absolute value. Thus, given a pair of images, we look for an optimal dimensional reduction function that will map each image to a numerical score, so that the scores will reflect the image quality relation (i.e., a less distorted image will receive a lower score). We look for an optimal dimensional reduction mapping in the form of a Deep Neural Network which minimizes the violation of image quality order. Subsequently, we extend the method to order a set of images by utilizing the predicted level of the chosen distortion. We demonstrate the validity of our method on Latent Chromatic Aberration and Moire distortions, on synthetic and real datasets.      
### 51.Integrated Sensing and Communications: A Mutual Information-Based Framework  [ :arrow_down: ](https://arxiv.org/pdf/2208.04260.pdf)
>  Integrated sensing and communications (ISAC) is potentially capable of circumventing the limitations of existing frequency-division sensing and communications (FDSAC) techniques. Hence, it has recently attracted significant attention. This article aims to propose a unified analytical framework for ISAC from a mutual information (MI) perspective. Based on the proposed framework, the sensing performance and the communication performance are evaluated by the sensing MI and the communication MI, respectively. The unity of this framework is originated from the fact that the sensing and communication (S\&amp;C) performance metrics, i.e., the S\&amp;C MI, have the similar physical and mathematical properties as well as the same unit of measurement. Based on this framework, the S\&amp;C performance of downlink and uplink ISAC systems is investigated and compared with that of FDSAC systems. Along each considered system settings, numerical results are provided to demonstrate the superiority of ISAC over conventional FDSAC designs. Finally, promising open research directions are provided in the context of MI-based ISAC.      
### 52.Coherent Time-Domain Canceling of Interference for Radio Astronomy  [ :arrow_down: ](https://arxiv.org/pdf/2208.04256.pdf)
>  Radio astronomy is vulnerable to interference from a variety of anthropogenic sources. Among the many strategies for mitigation of this interference is coherent time-domain canceling (CTC), which ideally allows one to "look through" interference, as opposed to avoiding the interference or deleting the afflicted data. However, CTC is difficult to implement, not well understood, and at present this strategy is not in regular use at any major radio telescope. This paper presents a review of CTC including a new comprehensive study of the capabilities and limitations of CTC using metrics relevant to radio astronomy, including fraction of interference power removed and increase in noise. This work is motivated by the emergence of a new generation of communications systems which pose a significantly increased threat to radio astronomy and which may overwhelm mitigation methods now in place.      
### 53.Capacity Scaling Law in Massive MIMO with Antenna Selection  [ :arrow_down: ](https://arxiv.org/pdf/2208.04252.pdf)
>  Antenna selection is capable of handling the cost and complexity issues in massive multiple-input multiple-output (MIMO) channels. The sum-rate capacity of a multiuser massive MIMO uplink channel is characterized under the Nakagami fading. A mathematically tractable sum-rate capacity upper bound is derived for the considered system. Moreover, for a sufficiently large base station (BS) antenna number, a deterministic equivalent (DE) of the sum-rate bound is derived. Based on this DE, the sum-rate capacity is shown to grow double logarithmically with the number of BS antennas. The validity of the analytical result is confirmed by numerical experiments.      
### 54.Challenges and Opportunities for Simultaneous Multi-functional Networks in the UHF Bands  [ :arrow_down: ](https://arxiv.org/pdf/2208.04247.pdf)
>  Multi-functional wireless networks are rapidly evolving and aspire to become a promising attribute of the upcoming 6G networks. Enabling multiple simultaneous networking functions with a single radio fosters the development of more integrated and simpler equipment, overcoming design and technology barriers inherited from radio systems of the past. We are seeing numerous trends exploiting these features in newly designed radios, such as those operating on the mmWave band. In this article, however, we carefully analyze the challenges and opportunities for multi-functional wireless networks in UHF bands, advocating the reuse of existing infrastructures and technologies, and exploring the possibilities of expanding their functionality without requiring architectural changes. We believe that both modern and legacy technologies can be turned into multi-functional systems if the right scientific and technological challenges are properly addressed. This transformation can foster the development of new applications and extend the useful life of these systems, contributing to a more sustainable digitization by delaying equipment obsolescence.      
### 55.Joint Channel Measurement and Simulation Analysis in an L-shaped Indoor Hallway in the Terahertz Band  [ :arrow_down: ](https://arxiv.org/pdf/2208.04191.pdf)
>  The Terahertz (THz) band (0.1-10~THz), which supports Terabit-per-second (Tbps) data rates, has been envisioned as one of the promising spectrum bands for sixth-generation (6G) and beyond communications. In this paper, an angular-resolvable wideband channel measurement campaign in an indoor L-shaped hallway at 306-321~GHz is presented, by using a frequency-domain vector network analyzer (VNA)-based channel sounder. Four line-of-sight (LoS), six quasi-line-of-sight (QLoS) and eight non-line-of-sight (NLoS) receiver points are measured. However, measured data spreads due to the rich scattering environment and the antenna pattern, which puzzles traditional clustering algorithms. To solve this problem, a simulation-assisted Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering algorithm is proposed, where the deterministic simulation result is extracted to adapt the conventional DBSCAN algorithm. The proposed algorithm outperforms conventional clustering algorithms like DBSCAN, K-means, and K-power-means in terms of Silhouette, Calinski-Harabasz and Davies-Bouldin indices. Furthermore, the THz multi-path propagation in the L-shaped hallway is elaborated, and channel characteristics of multipath and clusters are analyzed in depth.      
### 56.fMRI-S4: learning short- and long-range dynamic fMRI dependencies using 1D Convolutions and State Space Models  [ :arrow_down: ](https://arxiv.org/pdf/2208.04166.pdf)
>  Single-subject mapping of resting-state brain functional activity to non-imaging phenotypes is a major goal of neuroimaging. The large majority of learning approaches applied today rely either on static representations or on short-term temporal correlations. This is at odds with the nature of brain activity which is dynamic and exhibit both short- and long-range dependencies. Further, new sophisticated deep learning approaches have been developed and validated on single tasks/datasets. The application of these models for the study of a different targets typically require exhaustive hyperparameter search, model engineering and trial and error to obtain competitive results with simpler linear models. This in turn limit their adoption and hinder fair benchmarking in a rapidly developing area of research. To this end, we propose fMRI-S4; a versatile deep learning model for the classification of phenotypes and psychiatric disorders from the timecourses of resting-state functional magnetic resonance imaging scans. fMRI-S4 capture short- and long- range temporal dependencies in the signal using 1D convolutions and the recently introduced state-space models S4. The proposed architecture is lightweight, sample-efficient and robust across tasks/datasets. We validate fMRI-S4 on the tasks of diagnosing major depressive disorder (MDD), autism spectrum disorder (ASD) and sex classifcation on three multi-site rs-fMRI datasets. We show that fMRI-S4 can outperform existing methods on all three tasks and can be trained as a plug&amp;play model without special hyperpararameter tuning for each setting      
### 57.Efficient Neural Net Approaches in Metal Casting Defect Detection  [ :arrow_down: ](https://arxiv.org/pdf/2208.04150.pdf)
>  One of the most pressing challenges prevalent in the steel manufacturing industry is the identification of surface defects. Early identification of casting defects can help boost performance, including streamlining production processes. Though, deep learning models have helped bridge this gap and automate most of these processes, there is a dire need to come up with lightweight models that can be deployed easily with faster inference times. This research proposes a lightweight architecture that is efficient in terms of accuracy and inference time compared with sophisticated pre-trained CNN architectures like MobileNet, Inception, and ResNet, including vision transformers. Methodologies to minimize computational requirements such as depth-wise separable convolution and global average pooling (GAP) layer, including techniques that improve architectural efficiencies and augmentations, have been experimented. Our results indicate that a custom model of 590K parameters with depth-wise separable convolutions outperformed pretrained architectures such as Resnet and Vision transformers in terms of accuracy (81.87%) and comfortably outdid architectures such as Resnet, Inception, and Vision transformers in terms of faster inference times (12 ms). Blurpool fared outperformed other techniques, with an accuracy of 83.98%. Augmentations had a paradoxical effect on the model performance. No direct correlation between depth-wise and 3x3 convolutions on inference time, they, however, they played a direct role in improving model efficiency by enabling the networks to go deeper and by decreasing the number of trainable parameters. Our work sheds light on the fact that custom networks with efficient architectures and faster inference times can be built without the need of relying on pre-trained architectures.      
### 58.Virtual grating approach for Monte Carlo simulations of edge illumination-based x-ray phase contrast imaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.04137.pdf)
>  The design of new x-ray phase contrast imaging setups often relies on Monte Carlo simulations for prospective parameter studies. Monte Carlo simulations are known to be accurate but time consuming, leading to long simulation times, especially when many parameter variations are required. This is certainly the case for imaging methods relying on absorbing masks or gratings, with various tunable properties, such as pitch, aperture size, and thickness. In this work, we present the virtual grating approach to overcome this limitation. By replacing the gratings in the simulation with virtual gratings, the parameters of the gratings can be changed after the simulation, thereby significantly reducing the overall simulation time. The method is validated by comparison to explicit grating simulations, followed by representative demonstration cases.      
### 59.TGAVC: Improving Autoencoder Voice Conversion with Text-Guided and Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2208.04035.pdf)
>  Non-parallel many-to-many voice conversion remains an interesting but challenging speech processing task. Recently, AutoVC, a conditional autoencoder based method, achieved excellent conversion results by disentangling the speaker identity and the speech content using information-constraining bottlenecks. However, due to the pure autoencoder training method, it is difficult to evaluate the separation effect of content and speaker identity. In this paper, a novel voice conversion framework, named $\boldsymbol T$ext $\boldsymbol G$uided $\boldsymbol A$utoVC(TGAVC), is proposed to more effectively separate content and timbre from speech, where an expected content embedding produced based on the text transcriptions is designed to guide the extraction of voice content. In addition, the adversarial training is applied to eliminate the speaker identity information in the estimated content embedding extracted from speech. Under the guidance of the expected content embedding and the adversarial training, the content encoder is trained to extract speaker-independent content embedding from speech. Experiments on AIShell-3 dataset show that the proposed model outperforms AutoVC in terms of naturalness and similarity of converted speech.      
### 60.CaST: A Toolchain for Creating and Characterizing Realistic Wireless Network Emulation Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2208.03993.pdf)
>  Large-scale wireless testbeds have been extensively used by researchers in the past years. Among others, high-fidelity FPGA-based emulation platforms have unique capabilities in faithfully mimicking the conditions of real-world wireless environments in real-time, at scale, and with full repeatability. However, the reliability of the solutions developed in emulated platforms is heavily dependent on the emulation precision. CaST brings to the wireless network emulator landscape what it has been missing so far: an open, virtualized and software-based channel generator and sounder toolchain with unmatched precision in creating and characterizing quasi-real-world wireless network scenarios. CaST consists of (i) a framework to create mobile wireless scenarios from ray-tracing models for FPGA-based emulation platforms, and (ii) a containerized Software Defined Radio-based channel sounder to precisely characterize the emulated channels. We design, deploy and validate multi-path mobile scenarios on the world's largest wireless network emulator, Colosseum, and further demonstrate that CaST achieves up to 20 ns accuracy in sounding the Channel Impulse Response tap delays, and 0.5 dB accuracy in measuring the tap gains.      
### 61.Optimized Design for IRS-Assisted Integrated Sensing and Communication Systems in Clutter Environments  [ :arrow_down: ](https://arxiv.org/pdf/2208.03970.pdf)
>  In this paper, we investigate an intelligent reflecting surface (IRS)-assisted integrated sensing and communication (ISAC) system design in a clutter environment. Assisted by an IRS equipped with a uniform linear array (ULA), a multi-antenna base station (BS) is targeted for communicating with multiple communication users (CUs) and sensing multiple targets simultaneously. We consider the IRS-assisted ISAC design in the case with Type-I or Type-II CUs, where each Type-I and Type-II CU can and cannot cancel the interference from sensing signals, respectively. In particular, we aim to maximize the minimum sensing beampattern gain among multiple targets, by jointly optimizing the BS transmit beamforming vectors and the IRS phase shifting matrix, subject to the signal-to-interference-plus-noise ratio (SINR) constraint for each Type-I/Type-II CU, the interference power constraint per clutter, the transmission power constraint at the BS, and the cross-correlation pattern constraint. Due to the coupling of the BS's transmit design variables and the IRS's phase shifting matrix, the formulated max-min IRS-assisted ISAC design problem in the case with Type-I/Type-II CUs is highly non-convex. As such, we propose an efficient algorithm based on the alternating-optimization and semi-definite relaxation (SDR) techniques. In the case with Type-I CUs, we show that the dedicated sensing signal at the BS is always beneficial to improve the sensing performance. By contrast, the dedicated sensing signal at the BS is not required in the case with Type-II CUs. Numerical results are provided to show that the proposed IRS-assisted ISAC design schemes achieve a significant gain over the existing benchmark schemes.      
### 62.Robust Training and Verification of Implicit Neural Networks: A Non-Euclidean Contractive Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.03889.pdf)
>  This paper proposes a theoretical and computational framework for training and robustness verification of implicit neural networks based upon non-Euclidean contraction theory. The basic idea is to cast the robustness analysis of a neural network as a reachability problem and use (i) the $\ell_{\infty}$-norm input-output Lipschitz constant and (ii) the tight inclusion function of the network to over-approximate its reachable sets. First, for a given implicit neural network, we use $\ell_{\infty}$-matrix measures to propose sufficient conditions for its well-posedness, design an iterative algorithm to compute its fixed points, and provide upper bounds for its $\ell_\infty$-norm input-output Lipschitz constant. Second, we introduce a related embedded network and show that the embedded network can be used to provide an $\ell_\infty$-norm box over-approximation of the reachable sets of the original network. Moreover, we use the embedded network to design an iterative algorithm for computing the upper bounds of the original system's tight inclusion function. Third, we use the upper bounds of the Lipschitz constants and the upper bounds of the tight inclusion functions to design two algorithms for the training and robustness verification of implicit neural networks. Finally, we apply our algorithms to train implicit neural networks on the MNIST dataset and compare the robustness of our models with the models trained via existing approaches in the literature.      
### 63.Network Critical Slowing Down: Data-Driven Detection of Critical Transitions in Nonlinear Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.03881.pdf)
>  In a Nature article, Scheffer et al. presented a novel data-driven framework to predict critical transitions in complex systems. These transitions, which may stem from failures, degradation, or adversarial actions, have been attributed to bifurcations in the nonlinear dynamics. Their approach was built upon the phenomenon of critical slowing down, i.e., slow recovery in response to small perturbations near bifurcations. We extend their approach to detect and localize critical transitions in nonlinear networks. By introducing the notion of network critical slowing down, the objective of this paper is to detect that the network is undergoing a bifurcation only by analyzing its signatures from measurement data. We focus on two classes of widely-used nonlinear networks: (1) Kuramoto model for the synchronization of coupled oscillators and (2) attraction-repulsion dynamics in swarms, each of which presents a specific type of bifurcation. Based on the phenomenon of critical slowing down, we study the asymptotic behavior of the perturbed system away and close to the bifurcation and leverage this fact to develop a deterministic method to detect and identify critical transitions in nonlinear networks. Furthermore, we study the state covariance matrix subject to a stochastic noise process away and close to the bifurcation and use it to develop a stochastic framework for detecting critical transitions. Our simulation results show the strengths and limitations of the methods.      
### 64.When can I Speak? Predicting initiation points for spoken dialogue agents  [ :arrow_down: ](https://arxiv.org/pdf/2208.03812.pdf)
>  Current spoken dialogue systems initiate their turns after a long period of silence (700-1000ms), which leads to little real-time feedback, sluggish responses, and an overall stilted conversational flow. Humans typically respond within 200ms and successfully predicting initiation points in advance would allow spoken dialogue agents to do the same. In this work, we predict the lead-time to initiation using prosodic features from a pre-trained speech representation model (wav2vec 1.0) operating on user audio and word features from a pre-trained language model (GPT-2) operating on incremental transcriptions. To evaluate errors, we propose two metrics w.r.t. predicted and true lead times. We train and evaluate the models on the Switchboard Corpus and find that our method outperforms features from prior work on both metrics and vastly outperforms the common approach of waiting for 700ms of silence.      
### 65.Codebook Based Two-Time Scale Resource Allocation Design for IRS-Assisted eMBB-URLLC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.03798.pdf)
>  This paper investigates the resource allocation algorithm design for wireless systems assisted by large intelligent reflecting surfaces (IRSs) with coexisting enhanced mobile broadband (eMBB) and ultra reliable low-latency communication (URLLC) users. We consider a two-time scale resource allocation scheme, whereby the base station's precoders are optimized in each mini-slot to adapt to newly arriving URLLC traffic, whereas the IRS phase shifts are reconfigured only in each time slot to avoid excessive base station-IRS signaling. To facilitate efficient resource allocation design for large IRSs, we employ a codebook-based optimization framework, where the IRS is divided into several tiles and the phase-shift elements of each tile are selected from a pre-defined codebook. The resource allocation algorithm design is formulated as an optimization problem for the maximization of the average sum data rate of the eMBB users over a time slot while guaranteeing the quality-of-service (QoS) of each URLLC user in each mini-slot. An iterative algorithm based on alternating optimization (AO) is proposed to find a high-quality suboptimal solution. As a case study, the proposed algorithm is applied in an industrial indoor environment modelled via the Quadriga channel simulator. Our simulation results show that the proposed algorithm design enables the coexistence of eMBB and URLLC users and yields large performance gains compared to three baseline schemes. Furthermore, our simulation results reveal that the proposed two-time scale resource allocation design incurs only a small performance loss compared to the case when the IRSs are optimized in each mini-slot.      
### 66.Flocks, Games, and Cognition: A Geometric Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.03786.pdf)
>  Avian flocks display a wide variety of flight behaviors, including steady directed translation of center of mass, rapid change of overall morphology, re-shuffling of positions of individuals within a persistent form, etc. These behaviors may be viewed as flock-scale strategies, emerging from interactions between individuals, accomplishing some collective adaptive purpose such as finding a roost, or mitigating the danger from predator attacks. While we do not conceive the flock as a single cognitive agent, the moment-to-moment decisions of individuals, influenced by their neighbors, appear as if to realize collective strategies that are cognizant of purpose. In this paper, we identify the actions of the flock as allocation of energetic resources, and thereby associate a cognitive cost to behavior. Our notion of cognitive cost reflects the burden arising from rapid re-allocation of resource. Using a recently developed natural geometric approach to kinetic energy allocation, we map the flock behavior to a temporal signature on the standard (probability) simplex. Given the signature of a flocking event, we calculate the cognitive cost as a solution to an optimal control problem based on a game-theoretic model. Alternatively, one can associate to a signature an entropic cost. These two cost measures, when applied to data on starling flocks, show a consistent spread in value across events, and we suggest the possibility that higher cost may arise from predator attacks.      
### 67.Design and Analysis of Cold Gas Thruster to De-Orbit the PSLV Debris  [ :arrow_down: ](https://arxiv.org/pdf/2208.03773.pdf)
>  Todayś world of spaceś primary concern is the uncontrolled growth of space debris and its probability of collision with spacecraft, particularly in the low earth orbit (LEO) regions. This paper is aimed to design an optimized micro-propulsion system, Cold Gas Thruster, to de-orbit the PSLV debris from 668km to 250 km height after capturing process. The propulsion system mainly consists of a storage tank, pipes, control valves, and a convergent-divergent nozzle. The paper gives an idea of the design of each component based on a continuous iterative process until the design thrust requirements are met. All the components are designed in the CATIA V5, and the structural analysis is done in the ANSYS tool for each component where our cylinder tank can withstand the high hoop stress generated on its wall of it. And flow analysis is done by using the K-$\epsilon$ turbulence model for the CD nozzle, which provides the required thrust to de-orbit PSLV from a higher orbit to a lower orbit, after which the air drag will be enough to bring back to earthś atmosphere and burn it. Hohmannś orbit transfer method has been used to de-orbit the PSLV space debris, and it has been simulated by STK tools. And the result shows that our optimized designed thruster generates enough thrust to de-orbit the PSLV debris to a very low orbit.      
### 68.Rate Splitting Multiple Access for Next Generation Cognitive Radio Enabled LEO Satellite Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.03705.pdf)
>  This paper proposes a cognitive radio enabled LEO SatCom using RSMA radio access technique with the coexistence of GEO SatCom network. In particular, this work aims to maximize the sum rate of LEO SatCom by simultaneously optimizing the power budget over different beams, RSMA power allocation for users over each beam, and subcarrier user assignment while restricting the interference temperature to GEO SatCom. The problem of sum rate maximization is formulated as non-convex, where the global optimal solution is challenging to obtain. Thus, an efficient solution can be obtained in three steps: first we employ a successive convex approximation technique to reduce the complexity and make the problem more tractable. Second, for any given resource block user assignment, we adopt KKT conditions to calculate the transmit power over different beams and RSMA power allocation of users over each beam. Third, using the allocated power, we design an efficient algorithm based on the greedy approach for resource block user assignment. Numerical results demonstrate the benefits of the proposed optimization scheme compared to the benchmark schemes.      
### 69.Weakly Supervised Online Action Detection for Infant General Movements  [ :arrow_down: ](https://arxiv.org/pdf/2208.03648.pdf)
>  To make the earlier medical intervention of infants' cerebral palsy (CP), early diagnosis of brain damage is critical. Although general movements assessment(GMA) has shown promising results in early CP detection, it is laborious. Most existing works take videos as input to make fidgety movements(FMs) classification for the GMA automation. Those methods require a complete observation of videos and can not localize video frames containing normal FMs. Therefore we propose a novel approach named WO-GMA to perform FMs localization in the weakly supervised online setting. Infant body keypoints are first extracted as the inputs to WO-GMA. Then WO-GMA performs local spatio-temporal extraction followed by two network branches to generate pseudo clip labels and model online actions. With the clip-level pseudo labels, the action modeling branch learns to detect FMs in an online fashion. Experimental results on a dataset with 757 videos of different infants show that WO-GMA can get state-of-the-art video-level classification and cliplevel detection results. Moreover, only the first 20% duration of the video is needed to get classification results as good as fully observed, implying a significantly shortened FMs diagnosis time. Code is available at: <a class="link-external link-https" href="https://github.com/scofiedluo/WO-GMA" rel="external noopener nofollow">this https URL</a>.      
### 70.Debiased Cross-modal Matching for Content-based Micro-video Background Music Recommendation  [ :arrow_down: ](https://arxiv.org/pdf/2208.03633.pdf)
>  Micro-video background music recommendation is a complicated task where the matching degree between videos and uploader-selected background music is a major issue. However, the selection of the user-generated content (UGC) is biased caused by knowledge limitations and historical preferences among music of each uploader. In this paper, we propose a Debiased Cross-Modal (DebCM) matching model to alleviate the influence of such selection bias. Specifically, we design a teacher-student network to utilize the matching of segments of music videos, which is professional-generated content (PGC) with specialized music-matching techniques, to better alleviate the bias caused by insufficient knowledge of users. The PGC data is captured by a teacher network to guide the matching of uploader-selected UGC data of the student network by KL-based knowledge transfer. In addition, uploaders' personal preferences of music genres are identified as confounders that spuriously correlate music embeddings and background music selections, resulting in the learned recommender system to over-recommend music from the majority groups. To resolve such confounders in the UGC data of the student network, backdoor adjustment is utilized to deconfound the spurious correlation between music embeddings and prediction scores. We further utilize Monte Carlo (MC) estimator with batch-level average as the approximations to avoid integrating the entire confounder space calculated by the adjustment. Extensive experiments on the TT-150k-genre dataset demonstrate the effectiveness of the proposed method towards the selection bias. The code is publicly available on: \url{<a class="link-external link-https" href="https://github.com/jing-1/DebCM" rel="external noopener nofollow">this https URL</a>}.      
### 71.Transmission Neural Networks: From Virus Spread Models to Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.03616.pdf)
>  This work connects models for virus spread on networks with their equivalent neural network representations. Based on this connection, we propose a new neural network architecture, called Transmission Neural Networks (TransNNs) where activation functions are primarily associated with links and are allowed to have different activation levels. Furthermore, this connection leads to the discovery and the derivation of three new activation functions with tunable or trainable parameters. Moreover, we prove that TransNNs with a single hidden layer and a fixed non-zero bias term are universal function approximators. Finally, we present new fundamental derivations of continuous time epidemic network models based on TransNNs.      
### 72.2-D Rayleigh Autoregressive Moving Average Model for SAR Image Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2208.03615.pdf)
>  Two-dimensional (2-D) autoregressive moving average (ARMA) models are commonly applied to describe real-world image data, usually assuming Gaussian or symmetric noise. However, real-world data often present non-Gaussian signals, with asymmetrical distributions and strictly positive values. In particular, SAR images are known to be well characterized by the Rayleigh distribution. In this context, the ARMA model tailored for 2-D Rayleigh-distributed data is introduced -- the 2-D RARMA model. The 2-D RARMA model is derived and conditional likelihood inferences are discussed. The proposed model was submitted to extensive Monte Carlo simulations to evaluate the performance of the conditional maximum likelihood estimators. Moreover, in the context of SAR image processing, two comprehensive numerical experiments were performed comparing anomaly detection and image modeling results of the proposed model with traditional 2-D ARMA models and competing methods in the literature.      
### 73.Improved Point Estimation for the Rayleigh Regression Model  [ :arrow_down: ](https://arxiv.org/pdf/2208.03611.pdf)
>  The Rayleigh regression model was recently proposed for modeling amplitude values of synthetic aperture radar (SAR) image pixels. However, inferences from such model are based on the maximum likelihood estimators, which can be biased for small signal lengths. The Rayleigh regression model for SAR images often takes into account small pixel windows, which may lead to inaccurate results. In this letter, we introduce bias-adjusted estimators tailored for the Rayleigh regression model based on: (i) the Cox and Snell's method; (ii) the Firth's scheme; and (iii) the parametric bootstrap method. We present numerical experiments considering synthetic and actual SAR data sets. The bias-adjusted estimators yield nearly unbiased estimates and accurate modeling results.      
### 74.An FPGA framework for Interferometric Vision-Based Navigation (iVisNav)  [ :arrow_down: ](https://arxiv.org/pdf/2208.03605.pdf)
>  Interferometric Vision-Based Navigation (iVisNav) is a novel optoelectronic sensor for autonomous proximity operations. iVisNav employs laser emitting structured beacons and precisely characterizes six degrees of freedom relative motion rates by measuring changes in the phase of the transmitted laser pulses. iVisNav's embedded package must efficiently process high frequency dynamics for robust sensing and estimation. A new embedded system for least squares-based rate estimation is developed in this paper. The resulting system is capable of interfacing with the photonics and implement the estimation algorithm in a field-programmable gate array. The embedded package is shown to be a hardware/software co-design handling estimation procedure using finite precision arithmetic for high-speed computation. The accuracy of the finite precision FPGA hardware design is compared with the floating-point software evaluation of the algorithm on MATLAB to benchmark its performance and statistical consistency with the error measures. Implementation results demonstrate the utility of FPGA computing capabilities for high-speed proximity navigation using iVisNav.      
### 75.Development of a mobile robot assistant for wind turbines manufacturing  [ :arrow_down: ](https://arxiv.org/pdf/2208.03584.pdf)
>  The thrust for increased rating capacity of wind turbines has resulted into larger generators, longer blades, and taller towers. Presently, up to 16 MW wind turbines are being offered by wind turbines manufacturers which is nearly a 60 percent increase in the design capacity over the last five years. Manufacturing of these turbines involves assembling of gigantic sized components. Due to the frequent design changes and the variety of tasks involved, conventional automation is not possible making it a labor-intensive activity. However the handling and assembling of large components are challenging the human capabilities. The article proposes the use of mobile robotic assistants for partial automation of wind turbines manufacturing. The robotic assistant can result into reducing production costs, and better work conditions. The article presents development of a robot assistant for human operators to effectively perform assembly of wind turbines. The case is from a leading wind turbines manufacturer. The developed system is also applicable to other cases of large component manufacturing involving intensive manual effort.      
### 76.Reconfigurable Intelligent Surface Enabled Over-the-Air Uplink Non-orthogonal Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2208.03582.pdf)
>  Innovative reconfigurable intelligent surface (RIS) technologies are rising and recognized as promising candidates to enhance 6G and beyond wireless communication systems. RISs acquire the ability to manipulate electromagnetic signals, thus, offering a degree of control over the wireless channel and the potential for many more benefits. Furthermore, active RIS designs have recently been introduced to combat the critical double fading problem and other impairments passive RIS designs may possess. In this paper, the potential and flexibility of active RIS technology are exploited for uplink systems to achieve virtual non-orthogonal multiple access (NOMA) through power disparity over-the-air rather than controlling transmit powers at the user side. Specifically, users with identical transmit power, path loss, and distance can communicate with a base station sharing time and frequency resources in a NOMA fashion with the aid of the proposed hybrid RIS system. Here, the RIS is partitioned into active and passive parts and the distinctive partitions serve different users aligning their phases accordingly while introducing a power difference to the users' signals to enable NOMA. First, the end-to-end system model is presented considering two users. Furthermore, outage probability calculations and theoretical error probability analysis are discussed and reinforced with computer simulation results.      
### 77.On Rate-Splitting With Non-unique Decoding In Multi-cell Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.03532.pdf)
>  We consider the downlink of a multi-cell massive MIMO system suffering from asymptotic rate saturation due to pilot contamination. As opposed to treating pilot contamination interference as noise (TIN), we study the performance of decoding the pilot contamination interference. We model pilot-sharing users as an interference channel (IC) and study the performance of schemes that decode this interference partially based on rate-splitting (RS), and compare the performance to schemes that decode the interference in its entirety based on simultaneous unique decoding (SD) or non-unique decoding (SND). For RS, we non-uniquely decode each layer of the pilot contamination interference and use one common power splitting coefficient per IC. Additionally, we establish an achievable region for this RS scheme. Solving a maximum symmetric rate allocation problem based on linear programming (LP), we show that for zero-forcing (ZF) with spatially correlated/uncorrelated channels and with a practical number of BS antennas, RS achieves significantly higher spectral efficiencies than TIN, SD and SND. Furthermore, we numerically examine the impact of increasing the correlation of the channel across antennas, the number of users as well as the degree of shadow fading. In all cases, we show that RS maintains significant gain over TIN, SD and SND.      
### 78.Probabilistic Amplitude Shaping and Nonlinearity Tolerance: Analysis and Sequence Selection Method  [ :arrow_down: ](https://arxiv.org/pdf/2208.03449.pdf)
>  Probabilistic amplitude shaping (PAS) is a practical means to achieve a shaping gain in optical fiber communication. However, PAS and shaping in general also affect the signal-dependent generation of nonlinear interference. This provides an opportunity for nonlinearity mitigation through PAS, which is also referred to as a nonlinear shaping gain. In this paper, we introduce a linear lowpass filter model that relates transmitted symbol-energy sequences and nonlinear distortion experienced in an optical fiber channel. Based on this model, we conduct a nonlinearity analysis of PAS with respect to shaping blocklength and mapping strategy. Our model explains results and relationships found in literature and can be used as a design tool for PAS with improved nonlinearity tolerance. We use the model to introduce a new metric for PAS with sequence selection. We perform simulations of selection-based PAS with various amplitude shapers and mapping strategies to demonstrate the effectiveness of the new metric in different optical fiber system scenarios.      
### 79.Full-Waveform Modeling for Time-of-Flight Measurements based on Arrival Time of Photons  [ :arrow_down: ](https://arxiv.org/pdf/2208.03426.pdf)
>  Modern LiDAR sensors find increasing use in safety-critical applications. Therefore, highly accurate modeling of the system's behavior under demanding environmental conditions is necessary. In this paper, we present a modular structure to accurately simulate the amplified raw detector signal of a direct time-of-flight LiDAR system for coaxial transmitter-receiver optics. Our model describes, a measurement system based on standard optical components and a detector able of converting single photons to an electrical signal. To verify the model's predictions, single-point measurements for targets of different reflectivity at defined distances were performed. Statistical analysis shows an R-squared value greater than 0.990 for simulated and measured signal amplitude levels. Noise modeling shows good accordance with the performed measurements for different target irradiance levels. The presented results have a guiding significance in the modeling of the complex signal processing chain of LiDAR systems, as it enables the prediction of key parameters of the system early in the development process. Hence, unnecessary costs by design flaws can be mitigated. The modular structure allows easy adaption for arbitrary LiDAR systems.      
### 80.Chronological Self-Training for Real-Time Speaker Diarization  [ :arrow_down: ](https://arxiv.org/pdf/2208.03393.pdf)
>  Diarization partitions an audio stream into segments based on the voices of the speakers. Real-time diarization systems that include an enrollment step should limit enrollment training samples to reduce user interaction time. Although training on a small number of samples yields poor performance, we show that the accuracy can be improved dramatically using a chronological self-training approach. We studied the tradeoff between training time and classification performance and found that 1 second is sufficient to reach over 95% accuracy. We evaluated on 700 audio conversation files of about 10 minutes each from 6 different languages and demonstrated average diarization error rates as low as 10%.      
### 81.Autonomous Rapid Exploration in Close-Proximity of an Asteroid  [ :arrow_down: ](https://arxiv.org/pdf/2208.03378.pdf)
>  We use nonlinear robust guidance and control to assess the possibility of an autonomous spacecraft fast approaching and orbiting an asteroid without knowledge of its properties. The spacecraft uses onboard batch-sequential filtering to navigate while making a rapid approach with the aim of orbital insertion. We show through conservative assumptions that the proposed autonomous GN\&amp;C architecture is viable within current technology. Greater importance is in showing that an autonomous spacecraft can have a much bolder operational profile around an asteroid, with no need to inherit the conservative and cautious approach of current missions, which rely on ground intervention for firstly constraining uncertainties to a very low level before close-proximity. The results suggest that such paradigm shift can significantly impact costs, and exploration time, which can be very useful for exploring highly populated regions.      
### 82.Variational Autoencoders for Anomaly Detection in Respiratory Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2208.03326.pdf)
>  This paper proposes a weakly-supervised machine learning-based approach aiming at a tool to alert patients about possible respiratory diseases. Various types of pathologies may affect the respiratory system, potentially leading to severe diseases and, in certain cases, death. In general, effective prevention practices are considered as major actors towards the improvement of the patient's health condition. The proposed method strives to realize an easily accessible tool for the automatic diagnosis of respiratory diseases. Specifically, the method leverages Variational Autoencoder architectures permitting the usage of training pipelines of limited complexity and relatively small-sized datasets. Importantly, it offers an accuracy of 57 %, which is in line with the existing strongly-supervised approaches.      
### 83.Homomorphisms Between Transfer, Multi-Task, and Meta-Learning Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.03316.pdf)
>  Transfer learning, multi-task learning, and meta-learning are well-studied topics concerned with the generalization of knowledge across learning tasks and are closely related to general intelligence. But, the formal, general systems differences between them are underexplored in the literature. This lack of systems-level formalism leads to difficulties in coordinating related, inter-disciplinary engineering efforts. This manuscript formalizes transfer learning, multi-task learning, and meta-learning as abstract learning systems, consistent with the formal-minimalist abstract systems theory of Mesarovic and Takahara. Moreover, it uses the presented formalism to relate the three concepts of learning in terms of composition, hierarchy, and structural homomorphism. Findings are readily depicted in terms of input-output systems, highlighting the ease of delineating formal, general systems differences between transfer, multi-task, and meta-learning.      
### 84.A Case for Dataset Specific Profiling  [ :arrow_down: ](https://arxiv.org/pdf/2208.03315.pdf)
>  Data-driven science is an emerging paradigm where scientific discoveries depend on the execution of computational AI models against rich, discipline-specific datasets. With modern machine learning frameworks, anyone can develop and execute computational models that reveal concepts hidden in the data that could enable scientific applications. For important and widely used datasets, computing the performance of every computational model that can run against a dataset is cost prohibitive in terms of cloud resources. Benchmarking approaches used in practice use representative datasets to infer performance without actually executing models. While practicable, these approaches limit extensive dataset profiling to a few datasets and introduce bias that favors models suited for representative datasets. As a result, each dataset's unique characteristics are left unexplored and subpar models are selected based on inference from generalized datasets. This necessitates a new paradigm that introduces dataset profiling into the model selection process. To demonstrate the need for dataset-specific profiling, we answer two questions:(1) Can scientific datasets significantly permute the rank order of computational models compared to widely used representative datasets? (2) If so, could lightweight model execution improve benchmarking accuracy? Taken together, the answers to these questions lay the foundation for a new dataset-aware benchmarking paradigm.      
