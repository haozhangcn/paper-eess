# ArXiv eess --Wed, 24 Aug 2022
### 1.Locally temporal-spatial pattern learning with graph attention mechanism for EEG-based emotion recognition  [ :arrow_down: ](https://arxiv.org/pdf/2208.11087.pdf)
>  Technique of emotion recognition enables computers to classify human affective states into discrete categories. However, the emotion may fluctuate instead of maintaining a stable state even within a short time interval. There is also a difficulty to take the full use of the EEG spatial distribution due to its 3-D topology structure. To tackle the above issues, we proposed a locally temporal-spatial pattern learning graph attention network (LTS-GAT) in the present study. In the LTS-GAT, a divide-and-conquer scheme was used to examine local information on temporal and spatial dimensions of EEG patterns based on the graph attention mechanism. A dynamical domain discriminator was added to improve the robustness against inter-individual variations of the EEG statistics to learn robust EEG feature representations across different participants. We evaluated the LTS-GAT on two public datasets for affective computing studies under individual-dependent and independent paradigms. The effectiveness of LTS-GAT model was demonstrated when compared to other existing mainstream methods. Moreover, visualization methods were used to illustrate the relations of different brain regions and emotion recognition. Meanwhile, the weights of different time segments were also visualized to investigate emotion sparsity problems.      
### 2.A Compact Quasi-Yagi Antenna for FMCW Radar-on-Chip based Through-Wall Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2208.11034.pdf)
>  A compact quasi-Yagi antenna with a modified ground plane is designed for a through-wall radar (TWR) on-chip. A slot-based ground plane modification in the proposed antenna results in significant miniaturization with an increase in the impedance bandwidth by 44.62%. The antenna has a high directivity of 9.02 dBi and a front-to-back ratio of 25.76 dB at 2.4 GHz. Based on experiments in real-world deployment scenarios, the performance of the proposed quasi-Yagi antenna is found to be comparable to that of a Vivaldi antenna and a commercial-off-the-shelf (COTS) horn antenna. Spectrogram-based signatures of a moving person behind a wooden partition and a 40 cm thick masonry wall are successfully obtained using the designed antenna, demonstrating the suitability of the quasi-Yagi antenna for portable applications using a radar-on-chip.      
### 3.Learning linear modules in a dynamic network with missing node observations  [ :arrow_down: ](https://arxiv.org/pdf/2208.10995.pdf)
>  In order to identify a system (module) embedded in a dynamic network, one has to formulate a multiple-input estimation problem that necessitates certain nodes to be measured and included as predictor inputs. However, some of these nodes may not be measurable in many practical cases due to sensor selection and placement issues. This may result in biased estimates of the target module. Furthermore, the identification problem associated with the multiple-input structure may require determining a large number of parameters that are not of particular interest to the experimenter, with increased computational complexity in large-sized networks. In this paper, we tackle these problems by using a data augmentation strategy that allows us to reconstruct the missing node measurements and increase the accuracy of the estimated target module. To this end, we develop a system identification method using regularized kernel-based methods coupled with approximate inference methods. Keeping a parametric model for the module of interest, we model the other modules as Gaussian Processes (GP) with a kernel given by the so-called stable spline kernel. An Empirical Bayes (EB) approach is used to estimate the parameters of the target module. The related optimization problem is solved using an Expectation-Maximization (EM) method, where we employ a Markov-chain Monte Carlo (MCMC) technique to reconstruct the unknown missing node information and the network dynamics. Numerical simulations on dynamic network examples illustrate the potentials of the developed method.      
### 4.Unsupervised Anomaly Localization with Structural Feature-Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2208.10992.pdf)
>  Unsupervised Anomaly Detection has become a popular method to detect pathologies in medical images as it does not require supervision or labels for training. Most commonly, the anomaly detection model generates a "normal" version of an input image, and the pixel-wise $l^p$-difference of the two is used to localize anomalies. However, large residuals often occur due to imperfect reconstruction of the complex anatomical structures present in most medical images. This method also fails to detect anomalies that are not characterized by large intensity differences to the surrounding tissue. We propose to tackle this problem using a feature-mapping function that transforms the input intensity images into a space with multiple channels where anomalies can be detected along different discriminative feature maps extracted from the original image. We then train an Autoencoder model in this space using structural similarity loss that does not only consider differences in intensity but also in contrast and structure. Our method significantly increases performance on two medical data sets for brain MRI. Code and experiments are available at <a class="link-external link-https" href="https://github.com/FeliMe/feature-autoencoder" rel="external noopener nofollow">this https URL</a>      
### 5.Multi-Resolution Subspace-Based Optimization Method for the Retrieval of 2D Perfect Electric Conductors  [ :arrow_down: ](https://arxiv.org/pdf/2208.10944.pdf)
>  Perfect Electric Conductors (PECs) are imaged integrating the subspace-based optimizationmethod (SOM) within the iterative multi-scaling scheme (IMSA). Without a-priori information on the number or/and the locations of the scatterers and modelling their EM scattering interactions with a (known) probing source in terms of surface electric field integral equations, a segment-based representation of PECs is retrieved from the scattered field samples. The proposed IMSA-SOM inversion method is validated against both synthetic and experimental data by assessing the reconstruction accuracy, the robustness to the noise, and the computational efficiency with some comparisons, as well.      
### 6.A Simulation Method for MMW Radar Sensing in Traffic Intersection Based on BART Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2208.10939.pdf)
>  Millimeter-wave (mmw) radar is indispensable for Intelligent Transportation Systems (ITS), which can monitor traffic conditions in all weathers. An end-to-end simulation method for mmw radar monitoring and identification at traffic intersections is proposed in this paper. In this method, a virtual intersection scenario model is constructed, and the scattering coefficient of the target is calculated using the Bidirectional Analytical Ray Tracing (BART) algorithm. Combined with the generation of time-domain waveforms, the operation of frequency-domain convolution is simplified by inverse Fourier transform, and the echo signals received by the sparse array are simulated. After raw signal processing, point cloud images containing target position information and Range-Doppler Map (RDM) containing target state feature are obtained. The performance of mmw radar in detecting the specific location information of the target is evaluated by analyzing point cloud images. In addition, a self-defined convolutional neural network is introduced in this paper to evaluate the object recognition performance of the RDM. After the training of the neural network, the classification accuracy of this method for four types of vehicle targets can reach 92%.      
### 7.Improving Computed Tomography (CT) Reconstruction via 3D Shape Induction  [ :arrow_down: ](https://arxiv.org/pdf/2208.10937.pdf)
>  Chest computed tomography (CT) imaging adds valuable insight in the diagnosis and management of pulmonary infectious diseases, like tuberculosis (TB). However, due to the cost and resource limitations, only X-ray images may be available for initial diagnosis or follow up comparison imaging during treatment. Due to their projective nature, X-rays images may be more difficult to interpret by clinicians. The lack of publicly available paired X-ray and CT image datasets makes it challenging to train a 3D reconstruction model. In addition, Chest X-ray radiology may rely on different device modalities with varying image quality and there may be variation in underlying population disease spectrum that creates diversity in inputs. We propose shape induction, that is, learning the shape of 3D CT from X-ray without CT supervision, as a novel technique to incorporate realistic X-ray distributions during training of a reconstruction model. Our experiments demonstrate that this process improves both the perceptual quality of generated CT and the accuracy of down-stream classification of pulmonary infectious diseases.      
### 8.Data-driven Signal Decomposition Approaches: A Comparative Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.10874.pdf)
>  Signal decomposition (SD) approaches aim to decompose non-stationary signals into their constituent amplitude- and frequency-modulated components. This represents an important preprocessing step in many practical signal processing pipelines, providing useful knowledge and insight into the data and relevant underlying system(s) while also facilitating tasks such as noise or artefact removal and feature extraction. The popular SD methods are mostly data-driven, striving to obtain inherent well-behaved signal components without making many prior assumptions on input data. Among those methods include empirical mode decomposition (EMD) and variants, variational mode decomposition (VMD) and variants, synchrosqueezed transform (SST) and variants and sliding singular spectrum analysis (SSA). With the increasing popularity and utility of these methods in wide-ranging application, it is imperative to gain a better understanding and insight into the operation of these algorithms, evaluate their accuracy with and without noise in input data and gauge their sensitivity against algorithmic parameter changes. In this work, we achieve those tasks through extensive experiments involving carefully designed synthetic and real-life signals. Based on our experimental observations, we comment on the pros and cons of the considered SD algorithms as well as highlighting the best practices, in terms of parameter selection, for the their successful operation. The SD algorithms for both single- and multi-channel (multivariate) data fall within the scope of our work. For multivariate signals, we evaluate the performance of the popular algorithms in terms of fulfilling the mode-alignment property, especially in the presence of noise.      
### 9.Contact-Free Multi-Target Tracking Using Distributed Massive MIMO-OFDM Communication System: Prototype and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2208.10863.pdf)
>  Wireless-based human activity recognition has become an essential technology that enables contact-free human-machine and human-environment interactions. In this paper, we consider contact-free multi-target tracking (MTT) based on available communication systems. A radar-like prototype is built upon a sub-6 GHz distributed massive multiple-input and multiple-output (MIMO) orthogonal frequency-division multiplexing communication system. Specifically, the raw channel state information (CSI) is calibrated in the frequency and antenna domain before being used for tracking. Then the targeted CSIs reflected or scattered from the moving pedestrians are extracted. To evade the complex association problem of distributed massive MIMO-based MTT, we propose to use a complex Bayesian compressive sensing (CBCS) algorithm to estimate the targets' locations based on the extracted target-of-interest CSI signal directly. The estimated locations from CBCS are fed to a Gaussian mixture probability hypothesis density filter for tracking. A multi-pedestrian tracking experiment is conducted in a room with size of 6.5 m$\times$10 m to evaluate the performance of the proposed algorithm. According to experimental results, we achieve 75th and 95th percentile accuracy of 12.7 cm and 18.2 cm for single-person tracking and 28.9 cm and 45.7 cm for multi-person tracking, respectively. Furthermore, the proposed algorithm achieves the tracking purposes in real-time, which is promising for practical MTT use cases.      
### 10.Latent Variable Models in the Era of Industrial Big Data: Extension and Beyond  [ :arrow_down: ](https://arxiv.org/pdf/2208.10847.pdf)
>  A rich supply of data and innovative algorithms have made data-driven modeling a popular technique in modern industry. Among various data-driven methods, latent variable models (LVMs) and their counterparts account for a major share and play a vital role in many industrial modeling areas. LVM can be generally divided into statistical learning-based classic LVM and neural networks-based deep LVM (DLVM). We first discuss the definitions, theories and applications of classic LVMs in detail, which serves as both a comprehensive tutorial and a brief application survey on classic LVMs. Then we present a thorough introduction to current mainstream DLVMs with emphasis on their theories and model architectures, soon afterwards provide a detailed survey on industrial applications of DLVMs. The aforementioned two types of LVM have obvious advantages and disadvantages. Specifically, classic LVMs have concise principles and good interpretability, but their model capacity cannot address complicated tasks. Neural networks-based DLVMs have sufficient model capacity to achieve satisfactory performance in complex scenarios, but it comes at sacrifices in model interpretability and efficiency. Aiming at combining the virtues and mitigating the drawbacks of these two types of LVMs, as well as exploring non-neural-network manners to build deep models, we propose a novel concept called lightweight deep LVM (LDLVM). After proposing this new idea, the article first elaborates the motivation and connotation of LDLVM, then provides two novel LDLVMs, along with thorough descriptions on their principles, architectures and merits. Finally, outlooks and opportunities are discussed, including important open questions and possible research directions.      
### 11.Handling Disjunctions in Signal Temporal Logic Based Control Through Nonsmooth Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2208.10803.pdf)
>  For a class of spatio-temporal tasks defined by a fragment of Signal Temporal Logic (STL), we construct a nonsmooth time-varying control barrier function (CBF) and develop a controller based on a set of simple optimization problems. Each of the optimization problems invokes constraints that allow to exploit the piece-wise smoothness of the CBF for optimization additionally to the common gradient constraint in the context of CBFs. In this way, the conservativeness of the control approach is reduced in those points where the CBF is nonsmooth. Thereby, nonsmooth CBFs become applicable to time-varying control tasks. Moreover, we overcome the problem of vanishing gradients for the considered class of constraints which allows us to consider more complex tasks including disjunctions compared to approaches based on smooth CBFs. As a well-established and systematic method to encode spatiotemporal constraints, we define the class of tasks under consideration as an STL-fragment. The results are demonstrated in a relevant simulation example.      
### 12.Aging prediction using deep generative model toward the development of preventive medicine  [ :arrow_down: ](https://arxiv.org/pdf/2208.10797.pdf)
>  From birth to death, we all experience surprisingly ubiquitous changes over time due to aging. If we can predict aging in the digital domain, that is, the digital twin of the human body, we would be able to detect lesions in their very early stages, thereby enhancing the quality of life and extending the life span. We observed that none of the previously developed digital twins of the adult human body explicitly trained longitudinal conversion rules between volumetric medical images with deep generative models, potentially resulting in poor prediction performance of, for example, ventricular volumes. Here, we establish a new digital twin of an adult human body that adopts longitudinally acquired head computed tomography (CT) images for training, enabling prediction of future volumetric head CT images from a single present volumetric head CT image. We, for the first time, adopt one of the three-dimensional flow-based deep generative models to realize this sequential three-dimensional digital twin. We show that our digital twin outperforms the latest methods of prediction of ventricular volumes in relatively short terms.      
### 13.Extending nnU-Net is all you need  [ :arrow_down: ](https://arxiv.org/pdf/2208.10791.pdf)
>  Semantic segmentation is one of the most popular research areas in medical image computing. Perhaps surprisingly, despite its conceptualization dating back to 2018, nnU-Net continues to provide competitive out-of-the-box solutions for a broad variety of segmentation problems and is regularly used as a development framework for challenge-winning algorithms. Here we use nnU-Net to participate in the AMOS2022 challenge, which comes with a unique set of tasks: not only is the dataset one of the largest ever created and boasts 15 target structures, but the competition also requires submitted solutions to handle both MRI and CT scans. Through careful modification of nnU-net's hyperparameters, the addition of residual connections in the encoder and the design of a custom postprocessing strategy, we were able to substantially improve upon the nnU-Net baseline. Our final ensemble achieves Dice scores of 90.13 for Task 1 (CT) and 89.06 for Task 2 (CT+MRI) in a 5-fold cross-validation on the provided training cases.      
### 14.Unsupervised Detection of Sub-Territories of the Subthalamic Nucleus During DBS Surgery with Manifold Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.10788.pdf)
>  During Deep Brain Stimulation(DBS) surgery for treating Parkinson's disease, one vital task is to detect a specific brain area called the Subthalamic Nucleus(STN) and a sub-territory within the STN called the Dorsolateral Oscillatory Region(DLOR). Accurate detection of the STN borders is crucial for adequate clinical outcomes. Currently, the detection is based on human experts, guided by supervised machine learning detection algorithms. Consequently, this procedure depends on the knowledge and experience of particular experts and on the amount and quality of the labeled data used for training the machine learning algorithms. In this paper, to circumvent the dependence and bias caused by the training data, we present a data-driven unsupervised method for detecting the STN and the DLOR during DBS surgery. Our method is based on an agnostic modeling approach for general target detection tasks. Given a set of measurements, we extract features and propose a variant of the Mahalanobis distance between these features. We show theoretically that this distance enhances the differences between measurements with different intrinsic characteristics. Then, we incorporate the new features and distances into a manifold learning method, called Diffusion Maps. We show that this method gives rise to a representation that is consistent with the underlying factors that govern the measurements. Since the construction of this representation is carried out without rigid modeling assumptions, it can facilitate a wide range of detection tasks; here, we propose a specification for the STN and DLOR detection tasks. We present detection results on 25 sets of measurements recorded from 16 patients during surgery. Compared to a competing supervised algorithm based on a Hidden Markov Model, our unsupervised method demonstrates similar results in the STN detection task and superior results in the DLOR detection task.      
### 15.Features and Potentialities of Static Passive EM Skins for NLOS Specular Wireless Links  [ :arrow_down: ](https://arxiv.org/pdf/2208.10778.pdf)
>  The ability of passive flat patterned electromagnetic skins (EMSs) to overcome the asymptotic limit of the total path attenuation (TPA) of flat metallic reflectors of arbitrary size in non-line-of-sight (NLOS) specular wireless links is assessed. Closed-form expressions for the achievable TPA in EMS-powered NLOS links as well as the condition on the panel size of EMS-screens to improve the performance of flat passive conductive screens (PCSs) with the same aperture are derived and numerically validated by considering different incidence angles, screen apertures, transmitter/receiver distances, antenna gains, meta-atom geometries, and carrier frequencies.      
### 16.Retinal Structure Detection in OCTA Image via Voting-based Multi-task Learning  [ :arrow_down: ](https://arxiv.org/pdf/2208.10745.pdf)
>  Automated detection of retinal structures, such as retinal vessels (RV), the foveal avascular zone (FAZ), and retinal vascular junctions (RVJ), are of great importance for understanding diseases of the eye and clinical decision-making. In this paper, we propose a novel Voting-based Adaptive Feature Fusion multi-task network (VAFF-Net) for joint segmentation, detection, and classification of RV, FAZ, and RVJ in optical coherence tomography angiography (OCTA). A task-specific voting gate module is proposed to adaptively extract and fuse different features for specific tasks at two levels: features at different spatial positions from a single encoder, and features from multiple encoders. In particular, since the complexity of the microvasculature in OCTA images makes simultaneous precise localization and classification of retinal vascular junctions into bifurcation/crossing a challenging task, we specifically design a task head by combining the heatmap regression and grid classification. We take advantage of three different \textit{en face} angiograms from various retinal layers, rather than following existing methods that use only a single \textit{en face}. To facilitate further research, part of these datasets with the source code and evaluation benchmark have been released for public access:<a class="link-external link-https" href="https://github.com/iMED-Lab/VAFF-Net" rel="external noopener nofollow">this https URL</a>.      
### 17.Parameter Sensitivity of Deep-Feature based Evaluation Metrics for Audio Textures  [ :arrow_down: ](https://arxiv.org/pdf/2208.10743.pdf)
>  Standard evaluation metrics such as the Inception score and Fréchet Audio Distance provide a general audio quality distance metric between the synthesized audio and reference clean audio. However, the sensitivity of these metrics to variations in the statistical parameters that define an audio texture is not well studied. In this work, we provide a systematic study of the sensitivity of some of the existing audio quality evaluation metrics to parameter variations in audio textures. Furthermore, we also study three more potentially parameter-sensitive metrics for audio texture synthesis, (a) a Gram matrix based distance, (b) an Accumulated Gram metric using a summarized version of the Gram matrices, and (c) a cochlear-model based statistical features metric. These metrics use deep features that summarize the statistics of any given audio texture, thus being inherently sensitive to variations in the statistical parameters that define an audio texture. We study and evaluate the sensitivity of existing standard metrics as well as Gram matrix and cochlear-model based metrics to control-parameter variations in audio textures across a wide range of texture and parameter types, and validate with subjective evaluation. We find that each of the metrics is sensitive to different sets of texture-parameter types. This is the first step towards investigating objective metrics for assessing parameter sensitivity in audio textures.      
### 18.Semi-Automatic Labeling and Semantic Segmentation of Gram-Stained Microscopic Images from DIBaS Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2208.10737.pdf)
>  In this paper, a semi-automatic annotation of bacteria genera and species from DIBaS dataset is implemented using clustering and thresholding algorithms. A Deep learning model is trained to achieve the semantic segmentation and classification of the bacteria species. Classification accuracy of 95% is achieved. Deep learning models find tremendous applications in biomedical image processing. Automatic segmentation of bacteria from gram-stained microscopic images is essential to diagnose respiratory and urinary tract infections, detect cancers, etc. Deep learning will aid the biologists to get reliable results in less time. Additionally, a lot of human intervention can be reduced. This work can be helpful to detect bacteria from urinary smear images, sputum smear images, etc to diagnose urinary tract infections, tuberculosis, pneumonia, etc.      
### 19.Probabilistic Safe Online Learning with Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2208.10733.pdf)
>  Learning-based control schemes have recently shown great efficacy performing complex tasks. However, in order to deploy them in real systems, it is of vital importance to guarantee that the system will remain safe during online training and execution. We therefore need safe online learning frameworks able to autonomously reason about whether the current information at their disposal is enough to ensure safety or, in contrast, new measurements are required. In this paper, we present a framework consisting of two parts: first, an out-of-distribution detection mechanism actively collecting measurements when needed to guarantee that at least one safety backup direction is always available for use; and second, a Gaussian Process-based probabilistic safety-critical controller that ensures the system stays safe at all times with high probability. Our method exploits model knowledge through the use of Control Barrier Functions, and collects measurements from the stream of online data in an event-triggered fashion to guarantee recursive feasibility of the learned safety-critical controller. This, in turn, allows us to provide formal results of forward invariance of a safe set with high probability, even in a priori unexplored regions. Finally, we validate the proposed framework in numerical simulations of an adaptive cruise control system.      
### 20.Error Propagation and Overhead Reduced Channel Estimation for RIS-Aided Multi-User mmWave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2208.10732.pdf)
>  In this paper, we propose a novel two-stage based uplink channel estimation strategy with reduced pilot overhead and error propagation for a reconfigurable intelligent surface (RIS)-aided multi-user (MU) millimeter wave (mmWave) system. Specifically, in Stage I, with the carefully designed RIS phase shift matrix and introduced matching matrices, all users jointly estimate the correlation factors between different paths of the common RIS-base station (BS) channel, which achieves significant multi-user diversity gain. Then, the inherent scaling ambiguity and angle ambiguity of the mmWave cascaded channel are utilized to construct an ambiguous common RIS-BS channel composed of the estimated correlation factors. In Stage II, with the constructed ambiguous common RIS-BS channel, each user uses reduced pilots to estimate their specific user-RIS channel independently so as to obtain the entire cascaded channel. The theoretical number of pilots required for the proposed method is analyzed and the simulation results are presented to validate the effectiveness of this strategy.      
### 21.Ultra-high-resolution unpaired stain transformation via Kernelized Instance Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2208.10730.pdf)
>  While hematoxylin and eosin (H&amp;E) is a standard staining procedure, immunohistochemistry (IHC) staining further serves as a diagnostic and prognostic method. However, acquiring special staining results requires substantial costs. <br>Hence, we proposed a strategy for ultra-high-resolution unpaired image-to-image translation: Kernelized Instance Normalization (KIN), which preserves local information and successfully achieves seamless stain transformation with constant GPU memory usage. Given a patch, corresponding position, and a kernel, KIN computes local statistics using convolution operation. In addition, KIN can be easily plugged into most currently developed frameworks without re-training. <br>We demonstrate that KIN achieves state-of-the-art stain transformation by replacing instance normalization (IN) layers with KIN layers in three popular frameworks and testing on two histopathological datasets. Furthermore, we manifest the generalizability of KIN with high-resolution natural images. Finally, human evaluation and several objective metrics are used to compare the performance of different approaches. <br>Overall, this is the first successful study for the ultra-high-resolution unpaired image-to-image translation with constant space complexity. Code is available at: <a class="link-external link-https" href="https://github.com/Kaminyou/URUST" rel="external noopener nofollow">this https URL</a>      
### 22.Feeder Microgrid Management on an Active Distribution System during a Severe Outage  [ :arrow_down: ](https://arxiv.org/pdf/2208.10712.pdf)
>  Forming a microgrid on a distribution system with large scale outage after a severe weather event is emerging as a viable solution to improve resiliency at the distribution level. This option becomes more attractive when the distribution system has high levels of distributed PV. The management of such feeder-level microgrid has however many challenges, such as limited resources that can be deployed on the feeder quickly, and the limited real-time monitoring and control on the distribution system. Effective use of the distributed PV is also challenging as they are not monitored and controlled. To handle these challenges, the paper proposes a 2-stage hierarchical energy management scheme to securely operate these feeder level micorgrids. The first stage of the scheme solves a sequential rolling optimization problem to optimally schedule the main resources (such as a mobile diesel generator and battery storage unit). The second stage adopts a dispatching scheme for the main resources to adjust the stage-1 set-points closer to real- time. The proposed scheme has unique features to assure that the scheme is robust under highly varying operating conditions with limited system observability: (i) an innovative PV forecast error adjustment and a dynamic reserve adjustment scheme to handle the extreme uncertainty on PV power output, and (ii) an intelligent fuel management scheme to assure that the resources are utilized optimally over the multiple days of the restoration period. The proposed algorithm is tested on sample system with real-time data. The results show that the proposed scheme performs well in maximizing service to loads by effective use of all the resources and by properly taking into account the challenging operating conditions.      
### 23.Convolutional Neural Networks with A Topographic Representation Module for EEG-Based Brain-Computer Interfaces  [ :arrow_down: ](https://arxiv.org/pdf/2208.10708.pdf)
>  Objective: Convolutional Neural Networks (CNNs) have shown great potential in the field of Brain-Computer Interface (BCI) due to their ability to directly process the raw Electroencephalogram (EEG) without artificial feature extraction. The raw EEG signal is usually represented as 2-Dimensional (2-D) matrix composed of channels and time points, which ignores the spatial topological information of EEG. Our goal is to make the CNN with the raw EEG signal as input have the ability to learn the EEG spatial topological features, and improve its classification performance while essentially maintaining its original structure. Methods: We propose an EEG Topographic Representation Module (TRM). This module consists of (1) a mapping block from the raw EEG signal to a 3-D topographic map and (2) a convolution block from the topographic map to an output of the same size as the input. We embed the TRM to 3 widely used CNNs, and tested them on 2 different types of publicly available datasets. Results: The results show that the classification accuracies of the 3 CNNs are improved on both datasets after using TRM. The average classification accuracies of DeepConvNet, EEGNet and ShallowConvNet with TRM are improved by 4.70\%, 1.29\% and 0.91\% on Emergency Braking During Simulated Driving Dataset (EBDSDD), and 2.83\%, 2.17\% and 2.00\% on High Gamma Dataset (HGD), respectively. Significance: By using TRM to mine the spatial topological features of EEG, we improve the classification performance of 3 CNNs on 2 datasets. In addition,since the output of TRM has the same size as the input, any CNN with the raw EEG signal as input can use this module without changing the original structure.      
### 24.CM-MLP: Cascade Multi-scale MLP with Axial Context Relation Encoder for Edge Segmentation of Medical Image  [ :arrow_down: ](https://arxiv.org/pdf/2208.10701.pdf)
>  The convolutional-based methods provide good segmentation performance in the medical image segmentation task. However, those methods have the following challenges when dealing with the edges of the medical images: (1) Previous convolutional-based methods do not focus on the boundary relationship between foreground and background around the segmentation edge, which leads to the degradation of segmentation performance when the edge changes complexly. (2) The inductive bias of the convolutional layer cannot be adapted to complex edge changes and the aggregation of multiple-segmented areas, resulting in its performance improvement mostly limited to segmenting the body of segmented areas instead of the edge. To address these challenges, we propose the CM-MLP framework on MFI (Multi-scale Feature Interaction) block and ACRE (Axial Context Relation Encoder) block for accurate segmentation of the edge of medical image. In the MFI block, we propose the cascade multi-scale MLP (Cascade MLP) to process all local information from the deeper layers of the network simultaneously and utilize a cascade multi-scale mechanism to fuse discrete local information gradually. Then, the ACRE block is used to make the deep supervision focus on exploring the boundary relationship between foreground and background to modify the edge of the medical image. The segmentation accuracy (Dice) of our proposed CM-MLP framework reaches 96.96%, 96.76%, and 82.54% on three benchmark datasets: CVC-ClinicDB dataset, sub-Kvasir dataset, and our in-house dataset, respectively, which significantly outperform the state-of-the-art method. The source code and trained models will be available at <a class="link-external link-https" href="https://github.com/ProgrammerHyy/CM-MLP" rel="external noopener nofollow">this https URL</a>.      
### 25.Performance Analysis of Semi-Persistent Scheduling Throughput in 5G NR-V2X: A MAC Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2208.10653.pdf)
>  The packet throughput in 5th Generation (5G) New Radio (NR) Vehicle-to-Everything (V2X) is highly dependent on the Medium Access Control (MAC) based scheduling algorithm with no base station participation. In particular, the Semi-Persistent Scheduling (SPS) algorithm has been standardized by the 3rd Generation Partnership Project (3GPP) for V2X resource scheduling in the out-of-coverage scenario. This paper analyzes the NR-V2X SPS throughput from the MAC perspective, where the packet reception ratio (PRR) and half-duplex (HD) effect dominate. We first investigate the average throughput in the fully connected vehicular network, in which all the vehicles share the same throughput. Subsequently, the average throughput as a function of distance in the partially connected vehicular network is analyzed. The Monte Carlo simulation results show that increasing the resource keeping probability can improve the average throughput. Meanwhile, in the partially connected network, the lower resource keeping probability is prone to obtaining the higher throughput gain by increasing the number of subchannels.      
### 26.Fault Current-Constrained Optimal Power Flow on Unbalanced Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.10630.pdf)
>  With the proliferation of distributed generation into distribution networks, the need to consider fault currents in the dispatch problem becomes increasingly relevant. This paper introduces a method for adding fault current constraints into optimal power flow in order to reduce fault currents while minimizing generation cost. The optimal power flow problem is formulated as a single optimization problem with sub-networks representing the faults of interest. Having a single optimization problem allows the decision variables to be coupled across the optimal power flow and the fault current studies without having to iterate over possible solutions. The proposed method is applicable to unbalanced distribution networks, including those with transformers that introduce phase-shifts.      
### 27.Learning to Solve Optimization Problems with Hard Linear Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2208.10611.pdf)
>  Constrained optimization problems appear in a wide variety of challenging real-world problems, where constraints often capture the physics of the underlying system. Classic methods for solving these problems rely on iterative algorithms that explore the feasible domain in the search for the best solution. These iterative methods are often the computational bottleneck in decision-making and adversely impact time-sensitive applications. Recently, neural approximators have shown promise as a replacement for the iterative solvers that can output the optimal solution in a single feed-forward providing rapid solutions to optimization problems. However, enforcing constraints through neural networks remains an open challenge. This paper develops a neural approximator that maps the inputs to an optimization problem with hard linear constraints to a feasible solution that is nearly optimal. Our proposed approach consists of four main steps: 1) reducing the original problem to optimization on a set of independent variables, 2) finding a gauge function that maps the infty-norm unit ball to the feasible set of the reduced problem, 3)learning a neural approximator that maps the optimization's inputs to an optimal point in the infty-norm unit ball, and 4) find the values of the dependent variables from the independent variable and recover the solution to the original problem. We can guarantee hard feasibility through this sequence of steps. Unlike the current learning-assisted solutions, our method is free of parameter-tuning and removes iterations altogether. We demonstrate the performance of our proposed method in quadratic programming in the context of the optimal power dispatch (critical to the resiliency of our electric grid) and a constrained non-convex optimization in the context of image registration problems.      
### 28.Deriving time-averaged active inference from control principles  [ :arrow_down: ](https://arxiv.org/pdf/2208.10601.pdf)
>  Active inference offers a principled account of behavior as minimizing average sensory surprise over time. Applications of active inference to control problems have heretofore tended to focus on finite-horizon or discounted-surprise problems, despite deriving from the infinite-horizon, average-surprise imperative of the free-energy principle. Here we derive an infinite-horizon, average-surprise formulation of active inference from optimal control principles. Our formulation returns to the roots of active inference in neuroanatomy and neurophysiology, formally reconnecting active inference to optimal feedback control. Our formulation provides a unified objective functional for sensorimotor control and allows for reference states to vary over time.      
### 29.Widely-Linear MMSE Estimation of Complex-Valued Graph Signals  [ :arrow_down: ](https://arxiv.org/pdf/2208.10588.pdf)
>  In this paper, we consider the problem of recovering random graph signals with complex values. For general Bayesian estimation of complex-valued vectors, it is known that the widely-linear minimum mean-squared-error (WLMMSE) estimator can achieve a lower mean-squared-error (MSE) than that of the linear minimum MSE (LMMSE) estimator. Inspired by the WLMMSE estimator, in this paper we develop the graph signal processing (GSP)-WLMMSE estimator, which minimizes the MSE among estimators that are represented as a two-channel output of a graph filter, i.e. widely-linear GSP estimators. We discuss the properties of the proposed GSP-WLMMSE estimator. In particular, we show that the MSE of the GSP-WLMMSE estimator is always equal to or lower than the MSE of the GSP-LMMSE estimator. The GSP-WLMMSE estimator is based on diagonal covariance matrices in the graph frequency domain, and thus has reduced complexity compared with the WLMMSE estimator. This property is especially important when using the sample-mean versions of these estimators that are based on a training dataset. We then state conditions under which the low-complexity GSP-WLMMSE estimator coincides with the WLMMSE estimator. In the simulations, we investigate two synthetic estimation problems (with linear and nonlinear models) and the problem of state estimation in power systems. For these problems, it is shown that the GSP-WLMMSE estimator outperforms the GSP-LMMSE estimator and achieves similar performance to that of the WLMMSE estimator.      
### 30.Towards a Constructive Framework for Stabilization and Control of Nonlinear Systems: Passivity and Immersion (P\&amp;I) Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.10539.pdf)
>  The varied and complex dynamics of real-world systems challenge the formulation of a systematic strategy for designing a stabilizing feedback law. Rather than taking a universal approach, the control strategies developed thus far to handle this problem are specific to the inherent structure of the system under consideration. Therefore, this paper attempts to develop a generalized theory for the design of the stabilizing feedback law wherever possible for a general class of systems, including the systems in standard structured and unstructured forms discussed in the existing literature. The theory behind this general controller design theory utilizes the idea of an invariant target manifold giving rise to a non-degenerate two form, through which the definition of certain passive outputs and storage functions leads to a generation of control law for stabilizing the system. Because the above concepts are linked with the Immersion and Invariance (I&amp;I) design policy and the passivity theory of controller design, the proposed methodology is labeled as the "Passivity and Immersion (P&amp;I) approach". Furthermore, being a constructive methodology, the various worked examples in this paper exemplify and demonstrate the various design paradigms such as Backstepping, Incremental Backstepping, Forwarding, I&amp;I, and the techniques based on the generation of Control Contraction Metrics (CCM) can be unified in the P&amp;I methodology.      
### 31.Large-Scale Integrated Flexible Tactile Sensor Array for Sensitive Smart Robotic Touch  [ :arrow_down: ](https://arxiv.org/pdf/2208.10933.pdf)
>  In the long pursuit of smart robotics, it has been envisioned to empower robots with human-like senses, especially vision and touch. While tremendous progress has been made in image sensors and computer vision over the past decades, the tactile sense abilities are lagging behind due to the lack of large-scale flexible tactile sensor array with high sensitivity, high spatial resolution, and fast response. In this work, we have demonstrated a 64x64 flexible tactile sensor array with a record-high spatial resolution of 0.9 mm (equivalently 28.2 pixels per inch), by integrating a high-performance piezoresistive film (PRF) with a large-area active matrix of carbon nanotube thin-film transistors. PRF with self-formed microstructures exhibited high pressure-sensitivity of ~385 kPa-1 for MWCNTs concentration of 6%, while the 14% one exhibited fast response time of ~3 ms, good linearity, broad detection range beyond 1400 kPa, and excellent cyclability over 3000 cycles. Using this fully integrated tactile sensor array, the footprint maps of an artificial honeybee were clearly identified. Furthermore, we hardware-implemented a smart tactile system by integrating the PRF-based sensor array with a memristor-based computing-in-memory chip to record and recognize handwritten digits and Chinese calligraphy, achieving high classification accuracies of 98.8% and 97.3% in hardware, respectively. The integration of sensor networks with deep learning hardware may enable edge or near-sensor computing with significantly reduced power consumption and latency. Our work could pave the road to building large-scale intelligent sensor networks for next-generation smart robotics.      
### 32.Reconfiguring Wireless Environment via Intelligent Surfaces for 6G: Reflection, Modulation, and Security  [ :arrow_down: ](https://arxiv.org/pdf/2208.10931.pdf)
>  Reconfigurable intelligent surface (RIS) has been recognized as an essential enabling technique for the sixth-generation (6G) mobile communication network. Specifically, an RIS is comprised of a large number of small and low-cost reflecting elements whose parameters are dynamically adjustable with a programmable controller. Each of these elements can effectively reflect a phase-shifted version of the incident electromagnetic wave. By adjusting the wave phases in real time, the propagation environment of the reflected signals can be dynamically reconfigured to enhance communication reliability, boost transmission rate, expand cellular coverage, and strengthen communication security. In this paper, we provide an overview on RIS-assisted wireless communications. Specifically, we elaborate on the state-of-the-art enabling techniques of RISs as well as their corresponding substantial benefits from the perspectives of RIS reflection and RIS modulation. With these benefits, we envision the integration of RIS into emerging applications for 6G. In addition, communication security is of unprecedented importance in the 6G network with ubiquitous wireless services in multifarious verticals and areas. We highlight potential contributions of RIS to physical-layer security in terms of secrecy rate and secrecy outage probability, exemplified by a typical case study from both theoretical and numerical aspects. Finally, we discuss challenges and opportunities on the deployment of RISs in practice to motivate future research.      
### 33.StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation  [ :arrow_down: ](https://arxiv.org/pdf/2208.10922.pdf)
>  We propose StyleTalker, a novel audio-driven talking head generation model that can synthesize a video of a talking person from a single reference image with accurately audio-synced lip shapes, realistic head poses, and eye blinks. Specifically, by leveraging a pretrained image generator and an image encoder, we estimate the latent codes of the talking head video that faithfully reflects the given audio. This is made possible with several newly devised components: 1) A contrastive lip-sync discriminator for accurate lip synchronization, 2) A conditional sequential variational autoencoder that learns the latent motion space disentangled from the lip movements, such that we can independently manipulate the motions and lip movements while preserving the identity. 3) An auto-regressive prior augmented with normalizing flow to learn a complex audio-to-motion multi-modal latent space. Equipped with these components, StyleTalker can generate talking head videos not only in a motion-controllable way when another motion source video is given but also in a completely audio-driven manner by inferring realistic motions from the input audio. Through extensive experiments and user studies, we show that our model is able to synthesize talking head videos with impressive perceptual quality which are accurately lip-synced with the input audios, largely outperforming state-of-the-art baselines.      
### 34.Network Slicing for eMBB, URLLC, and mMTC: An Uplink Rate-Splitting Multiple Access Approach  [ :arrow_down: ](https://arxiv.org/pdf/2208.10841.pdf)
>  There are three generic services in 5G: enhanced mobile broadband (eMBB), ultra-reliable low-latency communications (URLLC), and massive machine-type communications (mMTC). To guarantee the performance of heterogeneous services, network slicing is proposed to allocate resources to different services. Network slicing is typically done in an orthogonal multiple access (OMA) fashion, which means different services are allocated non-interfering resources. However, as the number of users grows, OMA-based slicing is not always optimal, and a non-orthogonal scheme may achieve a better performance. This work aims to analyse the performances of different slicing schemes in uplink, and a promising scheme based on rate-splitting multiple access (RSMA) is studied. RSMA can provide a more flexible decoding order and theoretically has the largest achievable rate region than OMA and non-orthogonal multiple access (NOMA) without time-sharing. Hence, RSMA has the potential to increase the rate of users requiring different services. In addition, it is not necessary to decode the two split streams of one user successively, so RSMA lets suitable users split messages and designs an appropriate decoding order depending on the service requirements. This work shows that for network slicing RSMA can outperform NOMA counterpart, and obtain significant gains over OMA in some region.      
### 35.In-Air Imaging Sonar Sensor Network with Real-Time Processing Using GPUs  [ :arrow_down: ](https://arxiv.org/pdf/2208.10839.pdf)
>  For autonomous navigation and robotic applications, sensing the environment correctly is crucial. Many sensing modalities for this purpose exist. In recent years, one such modality that is being used is in-air imaging sonar. It is ideal in complex environments with rough conditions such as dust or fog. However, like with most sensing modalities, to sense the full environment around the mobile platform, multiple such sensors are needed to capture the full 360-degree range. Currently the processing algorithms used to create this data are insufficient to do so for multiple sensors at a reasonably fast update rate. Furthermore, a flexible and robust framework is needed to easily implement multiple imaging sonar sensors into any setup and serve multiple application types for the data. In this paper we present a sensor network framework designed for this novel sensing modality. Furthermore, an implementation of the processing algorithm on a Graphics Processing Unit is proposed to potentially decrease the computing time to allow for real-time processing of one or more imaging sonar sensors at a sufficiently high update rate.      
### 36.Automatic Calibration of a Six-Degrees-of-Freedom Pose Estimation System  [ :arrow_down: ](https://arxiv.org/pdf/2208.10837.pdf)
>  Systems for estimating the six-degrees-of-freedom human body pose have been improving for over two decades. Technologies such as motion capture cameras, advanced gaming peripherals and more recently both deep learning techniques and virtual reality systems have shown impressive results. However, most systems that provide high accuracy and high precision are expensive and not easy to operate. Recently, research has been carried out to estimate the human body pose using the HTC Vive virtual reality system. This system shows accurate results while keeping the cost under a 1000 USD. This system uses an optical approach. Two transmitter devices emit infrared pulses and laser planes are tracked by use of photo diodes on receiver hardware. A system using these transmitter devices combined with low-cost custom-made receiver hardware was developed previously but requires manual measurement of the position and orientation of the transmitter devices. These manual measurements can be time consuming, prone to error and not possible in particular setups. We propose an algorithm to automatically calibrate the poses of the transmitter devices in any chosen environment with custom receiver/calibration hardware. Results show that the calibration works in a variety of setups while being more accurate than what manual measurements would allow. Furthermore, the calibration movement and speed has no noticeable influence on the precision of the results.      
### 37.Reach-Avoid Analysis for Stochastic Differential Equations  [ :arrow_down: ](https://arxiv.org/pdf/2208.10752.pdf)
>  In this paper we propose a novel semi-definite programming approach that solves reach-avoid problems over open (i.e., not bounded a priori) time horizons for dynamical systems modeled by polynomial stochastic differential equations. The reach-avoid problem in this paper is a probabilistic guarantee: we approximate from the inner a p-reach-avoid set, i.e., the set of initial states guaranteeing with probability larger than p that the system eventually enters a given target set while remaining inside a specified safe set till the target hit. Our approach begins with the construction of a bounded value function, whose strict p super-level set is equal to the p-reach-avoid set. This value function is then reduced to a twice continuously differentiable solution to a system of equations. The system of equations facilitates the construction of a semi-definite program using sum-of-squares decomposition for multivariate polynomials and thus the transformation of nonconvex reach-avoid problems into a convex optimization problem. The semi-definite program can be solved efficiently in polynomial time via interior point methods. There are many existing powerful algorithms and off-the-shelf software packages to solve it. It is worth noting here that our approach can straightforwardly be specialized to address classical safety verification by, a.o., stochastic barrier certificate methods and reach-avoid analysis for ordinary differential equations. Several examples demonstrate theoretical and algorithmic developments of the proposed method.      
### 38.Quality-Constant Per-Shot Encoding by Two-Pass Learning-based Rate Factor Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2208.10739.pdf)
>  Providing quality-constant streams can simultaneously guarantee user experience and prevent wasting bit-rate. In this paper, we propose a novel deep learning based two-pass encoder parameter prediction framework to decide rate factor (RF), with which encoder can output streams with constant quality. For each one-shot segment in a video, the proposed method firstly extracts spatial, temporal and pre-coding features by an ultra fast pre-process. Based on these features, a RF parameter is predicted by a deep neural network. Video encoder uses the RF to compress segment as the first encoding pass. Then VMAF quality of the first pass encoding is measured. If the quality doesn't meet target, a second pass RF prediction and encoding will be performed. With the help of first pass predicted RF and corresponding actual quality as feedback, the second pass prediction will be highly accurate. Experiments show the proposed method requires only 1.55 times encoding complexity on average, meanwhile the accuracy, that the compressed video's actual VMAF is within $\pm1$ around the target VMAF, reaches 98.88%.      
### 39.Optimization of Mobile Robotic Relay Operation for Minimal Average Wait Time  [ :arrow_down: ](https://arxiv.org/pdf/2208.10736.pdf)
>  This paper considers trajectory planning for a mobile robot which persistently relays data between pairs of far-away communication nodes. Data accumulates stochastically at each source, and the robot must move to appropriate positions to enable data offload to the corresponding destination. The robot needs to minimize the average time that data waits at a source before being serviced. We are interested in finding optimal robotic routing policies consisting of 1) locations where the robot stops to relay (relay positions) and 2) conditional transition probabilities that determine the sequence in which the pairs are serviced. We first pose this problem as a non-convex problem that optimizes over both relay positions and transition probabilities. To find approximate solutions, we propose a novel algorithm which alternately optimizes relay positions and transition probabilities. For the former, we find efficient convex partitions of the non-convex relay regions, then formulate a mixed-integer second-order cone problem. For the latter, we find optimal transition probabilities via sequential least squares programming. We extensively analyze the proposed approach and mathematically characterize important system properties related to the robot's long-term energy consumption and service rate. Finally, through extensive simulation with real channel parameters, we verify the efficacy of our approach.      
### 40.Security and Reliability Analysis of Satellite-Terrestrial Multi-Relay Networks with Imperfect CSI  [ :arrow_down: ](https://arxiv.org/pdf/2208.10723.pdf)
>  This work investigates the security and reliability analysis for a novel satellite-terrestrial (SatTer) network. Specifically, a satellite attempts to transmit confidential information to a ground user (GU) via the support of multiple relay nodes in the presence of an eavesdropper that tries to overhear the information. A friendly jammer is deployed to improve the secure transmission between the satellite and the relays. Furthermore, satellite-to-relay generalized Rician fading channels and imperfect channel state information (CSI) are deployed to examine a general system model. In this context, the closed-formed expressions for the outage probability (OP) and intercept probability (IP) are derived corresponding to an amplify-and-forward (AF)-based relaying scheme, which is challenging and has not been studied before. Finally, the exactness of the mathematical analyses is validated through Monte Carlo simulations. Furthermore, the effects of various key parameters (e.g., channel estimation errors, satellite's transmit power, relay's transmit power, number of relays, and fading severity parameter) are examined.      
### 41.A Review of Machine Learning-based Failure Management in Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2208.10677.pdf)
>  Failure management plays a significant role in optical networks. It ensures secure operation, mitigates potential risks, and executes proactive protection. Machine learning (ML) is considered to be an extremely powerful technique for performing comprehensive data analysis and complex network management and is widely utilized for failure management in optical networks to revolutionize the conventional manual methods. In this study, the background of failure management is introduced, where typical failure tasks, physical objects, ML algorithms, data source, and extracted information are illustrated in detail. An overview of the applications of ML in failure management is provided in terms of alarm analysis, failure prediction, failure detection, failure localization, and failure identification. Finally, the future directions on ML for failure management are discussed from the perspective of data, model, task, and emerging techniques.      
### 42.Fall Detection from Audios with Audio Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2208.10659.pdf)
>  Fall detection for the elderly is a well-researched problem with several proposed solutions, including wearable and non-wearable techniques. While the existing techniques have excellent detection rates, their adoption by the target population is lacking due to the need for wearing devices and user privacy concerns. Our paper provides a novel, non-wearable, non-intrusive, and scalable solution for fall detection, deployed on an autonomous mobile robot equipped with a microphone. The proposed method uses ambient sound input recorded in people's homes. We specifically target the bathroom environment as it is highly prone to falls and where existing techniques cannot be deployed without jeopardizing user privacy. The present work develops a solution based on a Transformer architecture that takes noisy sound input from bathrooms and classifies it into fall/no-fall class with an accuracy of 0.8673. Further, the proposed approach is extendable to other indoor environments, besides bathrooms and is suitable for deploying in elderly homes, hospitals, and rehabilitation facilities without requiring the user to wear any device or be constantly "watched" by the sensors.      
### 43.Machine Learning-Enabled Cyber Attack Prediction and Mitigation for EV Charging Stations  [ :arrow_down: ](https://arxiv.org/pdf/2208.10644.pdf)
>  Safe and reliable electric vehicle charging stations (EVCSs) have become imperative in an intelligent transportation infrastructure. Over the years, there has been a rapid increase in the deployment of EVCSs to address the upsurging charging demands. However, advances in information and communication technologies (ICT) have rendered this cyber-physical system (CPS) vulnerable to suffering cyber threats, thereby destabilizing the charging ecosystem and even the entire electric grid infrastructure. This paper develops an advanced cybersecurity framework, where STRIDE threat modeling is used to identify potential vulnerabilities in an EVCS. Further, the weighted attack defense tree approach is employed to create multiple attack scenarios, followed by developing Hidden Markov Model (HMM) and Partially Observable Monte-Carlo Planning (POMCP) algorithms for modeling the security attacks. Also, potential mitigation strategies are suggested for the identified threats.      
### 44.Anatomy-Aware Contrastive Representation Learning for Fetal Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2208.10642.pdf)
>  Self-supervised contrastive representation learning offers the advantage of learning meaningful visual representations from unlabeled medical datasets for transfer learning. However, applying current contrastive learning approaches to medical data without considering its domain-specific anatomical characteristics may lead to visual representations that are inconsistent in appearance and semantics. In this paper, we propose to improve visual representations of medical images via anatomy-aware contrastive learning (AWCL), which incorporates anatomy information to augment the positive/negative pair sampling in a contrastive learning manner. The proposed approach is demonstrated for automated fetal ultrasound imaging tasks, enabling the positive pairs from the same or different ultrasound scans that are anatomically similar to be pulled together and thus improving the representation learning. We empirically investigate the effect of inclusion of anatomy information with coarse- and fine-grained granularity, for contrastive learning and find that learning with fine-grained anatomy information which preserves intra-class difference is more effective than its counterpart. We also analyze the impact of anatomy ratio on our AWCL framework and find that using more distinct but anatomically similar samples to compose positive pairs results in better quality representations. Experiments on a large-scale fetal ultrasound dataset demonstrate that our approach is effective for learning representations that transfer well to three clinical downstream tasks, and achieves superior performance compared to ImageNet supervised and the current state-of-the-art contrastive learning methods. In particular, AWCL outperforms ImageNet supervised method by 13.8% and state-of-the-art contrastive-based method by 7.1% on a cross-domain segmentation task.      
### 45.Scalable Hybrid Classification-Regression Solution for High-Frequency Nonintrusive Load Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2208.10638.pdf)
>  Residential buildings with the ability to monitor and control their net-load (sum of load and generation) can provide valuable flexibility to power grid operators. We present a novel multiclass nonintrusive load monitoring (NILM) approach that enables effective net-load monitoring capabilities at high-frequency with minimal additional equipment and cost. The proposed machine learning based solution provides accurate multiclass state predictions while operating at a faster timescale (able to provide a prediction for each 60-Hz ac cycle used in US power grid) without relying on event-detection techniques. We also introduce an innovative hybrid classification-regression method that allows for the prediction of not only load on/off states via classification but also individual load operating power levels via regression. A test bed with eight residential appliances is used for validating the NILM approach. Results show that the overall method has high accuracy and, good scaling and generalization properties. Furthermore, the method is shown to have sufficient response time (within 160ms, corresponding to 10 ac cycles) to support building grid-interactive control at fast timescales relevant to the provision of grid frequency support services.      
### 46.Low Complexity Classification Approach for Faster-than-Nyquist (FTN) Signalling Detection  [ :arrow_down: ](https://arxiv.org/pdf/2208.10637.pdf)
>  Faster-than-Nyquist (FTN) signaling can improve the spectral efficiency (SE); however, at the expense of high computational complexity to remove the introduced intersymbol interference (ISI). Motivated by the recent success of ML in physical layer (PHY) problems, in this paper we investigate the use of ML in reducing the detection complexity of FTN signaling. In particular, we view the FTN signaling detection problem as a classification task, where the received signal is considered as an unlabeled class sample that belongs to a set of all possible classes samples. If we use an off-shelf classifier, then the set of all possible classes samples belongs to an $N$-dimensional space, where $N$ is the transmission block length, which has a huge computational complexity. We propose a low-complexity classifier (LCC) that exploits the ISI structure of FTN signaling to perform the classification task in $N_p \ll N$-dimension space. The proposed LCC consists of two stages: 1) offline pre-classification that constructs the labeled classes samples in the $N_p$-dimensional space and 2) online classification where the detection of the received samples occurs. The proposed LCC is extended to produce soft-outputs as well. Simulation results show the effectiveness of the proposed LCC in balancing performance and complexity.      
### 47.Concurrent Validity of Automatic Speech and Pause Measures During Passage Reading in ALS  [ :arrow_down: ](https://arxiv.org/pdf/2208.10597.pdf)
>  The analysis of speech measures in individuals with amyotrophic lateral sclerosis (ALS) can provide essential information for early diagnosis and tracking disease progression. However, current methods for extracting speech and pause features are manual or semi-automatic, which makes them time-consuming and labour-intensive. The advent of speech-text alignment algorithms provides an opportunity for inexpensive, automated, and accurate analysis of speech measures in individuals with ALS. There is a need to validate speech and pause features calculated by these algorithms against current gold standard methods. In this study, we extracted 8 speech/pause features from 646 audio files of individuals with ALS and healthy controls performing passage reading. Two pretrained forced alignment models - one using transformers and another using a Gaussian mixture / hidden Markov architecture - were used for automatic feature extraction. The results were then validated against semi-automatic speech/pause analysis software, with further subgroup analyses based on audio quality and disease severity. Features extracted using transformer-based forced alignment had the highest agreement with gold standards, including in terms of audio quality and disease severity. This study lays the groundwork for future intelligent diagnostic support systems for clinicians, and for novel methods of tracking disease progression remotely from home.      
### 48.Automated Temporal Segmentation of Orofacial Assessment Videos  [ :arrow_down: ](https://arxiv.org/pdf/2208.10591.pdf)
>  Computer vision techniques can help automate or partially automate clinical examination of orofacial impairments to provide accurate and objective assessments. Towards the development of such automated systems, we evaluated two approaches to detect and temporally segment (parse) repetitions in orofacial assessment videos. Recorded videos of participants with amyotrophic lateral sclerosis (ALS) and healthy control (HC) individuals were obtained from the Toronto NeuroFace Dataset. Two approaches for repetition detection and parsing were examined: one based on engineered features from tracked facial landmarks and peak detection in the distance between the vermilion-cutaneous junction of the upper and lower lips (baseline analysis), and another using a pre-trained transformer-based deep learning model called RepNet (Dwibedi et al, 2020), which automatically detects periodicity, and parses periodic and semi-periodic repetitions in video data. In experimental evaluation of two orofacial assessments tasks, - repeating maximum mouth opening (OPEN) and repeating the sentence "Buy Bobby a Puppy" (BBP) - RepNet provided better parsing than the landmark-based approach, quantified by higher mean intersection-over-union (IoU) with respect to ground truth manual parsing. Automated parsing using RepNet also clearly separated HC and ALS participants based on the duration of BBP repetitions, whereas the landmark-based method could not.      
### 49.EBSnoR: Event-Based Snow Removal by Optimal Dwell Time Thresholding  [ :arrow_down: ](https://arxiv.org/pdf/2208.10581.pdf)
>  We propose an Event-Based Snow Removal algorithm called EBSnoR. We developed a technique to measure the dwell time of snowflakes on a pixel using event-based camera data, which is used to carry out a Neyman-Pearson hypothesis test to partition event stream into snowflake and background events. The effectiveness of the proposed EBSnoR was verified on a new dataset called UDayton22EBSnow, comprised of front-facing event-based camera in a car driving through snow with manually annotated bounding boxes around surrounding vehicles. Qualitatively, EBSnoR correctly identifies events corresponding to snowflakes; and quantitatively, EBSnoR-preprocessed event data improved the performance of event-based car detection algorithms.      
### 50.Underwater Messaging Using Mobile Devices  [ :arrow_down: ](https://arxiv.org/pdf/2208.10569.pdf)
>  Since its inception, underwater digital acoustic communication has required custom hardware that neither has the economies of scale nor is pervasive. We present the first acoustic system that brings underwater messaging capabilities to existing mobile devices like smartphones and smart watches. Our software-only solution leverages audio sensors, i.e., microphones and speakers, ubiquitous in today's devices to enable acoustic underwater communication between mobile devices. To achieve this, we design a communication system that in real-time adapts to differences in frequency responses across mobile devices, changes in multipath and noise levels at different locations and dynamic channel changes due to mobility. We evaluate our system in six different real-world underwater environments with depths of 2-15 m in the presence of boats, ships and people fishing and kayaking. Our results show that our system can in real-time adapt its frequency band and achieve bit rates of 100 bps to 1.8 kbps and a range of 30 m. By using a lower bit rate of 10-20 bps, we can further increase the range to 100 m. As smartphones and watches are increasingly being used in underwater scenarios, our software-based approach has the potential to make underwater messaging capabilities widely available to anyone with a mobile device. Project page with open-source code and data can be found here: <a class="link-external link-https" href="https://underwatermessaging.cs.washington.edu/" rel="external noopener nofollow">this https URL</a>      
### 51.Atrial Fibrillation Recurrence Risk Prediction from 12-lead ECG Recorded Pre- and Post-Ablation Procedure  [ :arrow_down: ](https://arxiv.org/pdf/2208.10550.pdf)
>  Introduction: 12-lead electrocardiogram (ECG) is recorded during atrial fibrillation (AF) catheter ablation procedure (CAP). It is not easy to determine if CAP was successful without a long follow-up assessing for AF recurrence (AFR). Therefore, an AFR risk prediction algorithm could enable a better management of CAP patients. In this research, we extracted features from 12-lead ECG recorded before and after CAP and train an AFR risk prediction machine learning model. Methods: Pre- and post-CAP segments were extracted from 112 patients. The analysis included a signal quality criterion, heart rate variability and morphological biomarkers engineered from the 12-lead ECG (804 features overall). 43 out of the 112 patients (n) had AFR clinical endpoint available. These were utilized to assess the feasibility of AFR risk prediction, using either pre or post CAP features. A random forest classifier was trained within a nested cross validation framework. Results: 36 features were found statistically significant for distinguishing between the pre and post surgery states (n=112). For the classification, an area under the receiver operating characteristic (AUROC) curve was reported with AUROC_pre=0.64 and AUROC_post=0.74 (n=43). Discussion and conclusions: This preliminary analysis showed the feasibility of AFR risk prediction. Such a model could be used to improve CAP management.      
### 52.Fast Updating the STBC Decoder Matrices in the Uplink of a Massive MIMO System  [ :arrow_down: ](https://arxiv.org/pdf/2208.10540.pdf)
>  Reducing computational complexity of the modern wireless communication systems such as massive Multiple-Input Multiple-Output (MIMO) configurations is of utmost interest. In this paper, we propose new algorithm that can be used to accelerate matrix inversion in the decoding of space-time block codes (STBC) in the uplink of dynamic massive MIMO systems. A multi-user system in which the base station is equipped with a large number of antennas and each user has two antennas is considered. In addition, users can enter or exit the system dynamically. For a given space-time block coding/decoding scheme the computational complexity of the receiver will be significantly reduced when a user is added to or removed from the system by employing the proposed method. In the proposed scheme, the matrix inversion for zero-forcing (ZF) as well as minimum mean square error (MMSE) decoding is derived from the inverse of a partitioned matrix and the Woodbury matrix identity. Furthermore, the suggested technique can be utilized when the number of users is fixed but the channel estimate changes for a particular user. The mathematical equations for updating the inverse of the decoding matrices are derived and its complexity is compared to the direct way of computing the inverse. Evaluations confirm the effectiveness of the proposed approach.      
### 53.DualVoice: Speech Interaction that Discriminates between Normal and Whispered Voice Input  [ :arrow_down: ](https://arxiv.org/pdf/2208.10499.pdf)
>  Interactions based on automatic speech recognition (ASR) have become widely used, with speech input being increasingly utilized to create documents. However, as there is no easy way to distinguish between commands being issued and text required to be input in speech, misrecognitions are difficult to identify and correct, meaning that documents need to be manually edited and corrected. The input of symbols and commands is also challenging because these may be misrecognized as text letters. To address these problems, this study proposes a speech interaction method called DualVoice, by which commands can be input in a whispered voice and letters in a normal voice. The proposed method does not require any specialized hardware other than a regular microphone, enabling a complete hands-free interaction. The method can be used in a wide range of situations where speech recognition is already available, ranging from text input to mobile/wearable computing. Two neural networks were designed in this study, one for discriminating normal speech from whispered speech, and the second for recognizing whisper speech. A prototype of a text input system was then developed to show how normal and whispered voice can be used in speech text input. Other potential applications using DualVoice are also discussed.      
### 54.Are disentangled representations all you need to build speaker anonymization systems?  [ :arrow_down: ](https://arxiv.org/pdf/2208.10497.pdf)
>  Speech signals contain a lot of sensitive information, such as the speaker's identity, which raises privacy concerns when speech data get collected. Speaker anonymization aims to transform a speech signal to remove the source speaker's identity while leaving the spoken content unchanged. Current methods perform the transformation by relying on content/speaker disentanglement and voice conversion. Usually, an acoustic model from an automatic speech recognition system extracts the content representation while an x-vector system extracts the speaker representation. Prior work has shown that the extracted features are not perfectly disentangled. This paper tackles how to improve features disentanglement, and thus the converted anonymized speech. We propose enhancing the disentanglement by removing speaker information from the acoustic model using vector quantization. Evaluation done using the VoicePrivacy 2022 toolkit showed that vector quantization helps conceal the original speaker identity while maintaining utility for speech recognition.      
### 55.Predicting microsatellite instability and key biomarkers in colorectal cancer from H&amp;E-stained images: Achieving SOTA with Less Data using Swin Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2208.10495.pdf)
>  Artificial intelligence (AI) models have been developed for predicting clinically relevant biomarkers, including microsatellite instability (MSI), for colorectal cancers (CRC). However, the current deep-learning networks are data-hungry and require large training datasets, which are often lacking in the medical domain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin-T), we developed an efficient workflow for biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, BRAF, and TP53 mutation) that only required relatively small datasets, but achieved the state-of-the-art (SOTA) predictive performance. Our Swin-T workflow not only substantially outperformed published models in an intra-study cross-validation experiment using TCGA-CRC-DX dataset (N = 462), but also showed excellent generalizability in cross-study external validation and delivered a SOTA AUROC of 0.90 for MSI using the MCO dataset for training (N = 1065) and the same TCGA-CRC-DX for testing. Similar performance (AUROC=0.91) was achieved by Echle and colleagues using 8000 training samples (ResNet18) on the same testing dataset. Swin-T was extremely efficient using small training datasets and exhibits robust predictive performance with only 200-500 training samples. These data indicate that Swin-T may be 5-10 times more efficient than the current state-of-the-art algorithms for MSI based on ResNet18 and ShuffleNet. Furthermore, the Swin-T models showed promise as pre-screening tests for MSI status and BRAF mutation status, which could exclude and reduce the samples before the subsequent standard testing in a cascading diagnostic workflow to allow turnaround time reduction and cost saving.      
### 56.Improving Speech Emotion Recognition Through Focus and Calibration Attention Mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2208.10491.pdf)
>  Attention has become one of the most commonly used mechanisms in deep learning approaches. The attention mechanism can help the system focus more on the feature space's critical regions. For example, high amplitude regions can play an important role for Speech Emotion Recognition (SER). In this paper, we identify misalignments between the attention and the signal amplitude in the existing multi-head self-attention. To improve the attention area, we propose to use a Focus-Attention (FA) mechanism and a novel Calibration-Attention (CA) mechanism in combination with the multi-head self-attention. Through the FA mechanism, the network can detect the largest amplitude part in the segment. By employing the CA mechanism, the network can modulate the information flow by assigning different weights to each attention head and improve the utilization of surrounding contexts. To evaluate the proposed method, experiments are performed with the IEMOCAP and RAVDESS datasets. Experimental results show that the proposed framework significantly outperforms the state-of-the-art approaches on both datasets.      
### 57.System Fingerprints Detection for DeepFake Audio: An Initial Dataset and Investigation  [ :arrow_down: ](https://arxiv.org/pdf/2208.10489.pdf)
>  Many effective attempts have been made for deepfake audio detection. However, they can only distinguish between real and fake. For many practical application scenarios, what tool or algorithm generated the deepfake audio also is needed. This raises a question: Can we detect the system fingerprints of deepfake audio? Therefore, this paper conducts a preliminary investigation to detect system fingerprints of deepfake audio. Experiments are conducted on deepfake audio datasets from five latest deep-learning speech synthesis systems. The results show that LFCC features are relatively more suitable for system fingerprints detection. Moreover, the ResNet achieves the best detection results among LCNN and x-vector based models. The t-SNE visualization shows that different speech synthesis systems generate distinct system fingerprints.      
