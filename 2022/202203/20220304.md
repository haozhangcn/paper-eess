# ArXiv eess --Fri, 4 Mar 2022
### 1.NUQ: A Noise Metric for Diffusion MRI via Uncertainty Discrepancy Quantification  [ :arrow_down: ](https://arxiv.org/pdf/2203.01921.pdf)
>  Diffusion MRI (dMRI) is the only non-invasive technique sensitive to tissue micro-architecture, which can, in turn, be used to reconstruct tissue microstructure and white matter pathways. The accuracy of such tasks is hampered by the low signal-to-noise ratio in dMRI. Today, the noise is characterized mainly by visual inspection of residual maps and estimated standard deviation. However, it is hard to estimate the impact of noise on downstream tasks based only on such qualitative assessments. To address this issue, we introduce a novel metric, Noise Uncertainty Quantification (NUQ), for quantitative image quality analysis in the absence of a ground truth reference image. NUQ uses a recent Bayesian formulation of dMRI models to estimate the uncertainty of microstructural measures. Specifically, NUQ uses the maximum mean discrepancy metric to compute a pooled quality score by comparing samples drawn from the posterior distribution of the microstructure measures. We show that NUQ allows a fine-grained analysis of noise, capturing details that are visually imperceptible. We perform qualitative and quantitative comparisons on real datasets, showing that NUQ generates consistent scores across different denoisers and acquisitions. Lastly, by using NUQ on a cohort of schizophrenics and controls, we quantify the substantial impact of denoising on group differences.      
### 2.Computer Vision Aided Blockage Prediction in Real-World Millimeter Wave Deployments  [ :arrow_down: ](https://arxiv.org/pdf/2203.01907.pdf)
>  This paper provides the first real-world evaluation of using visual (RGB camera) data and machine learning for proactively predicting millimeter wave (mmWave) dynamic link blockages before they happen. Proactively predicting line-of-sight (LOS) link blockages enables mmWave/sub-THz networks to make proactive network management decisions, such as proactive beam switching and hand-off) before a link failure happens. This can significantly enhance the network reliability and latency while efficiently utilizing the wireless resources. To evaluate this gain in reality, this paper (i) develops a computer vision based solution that processes the visual data captured by a camera installed at the infrastructure node and (ii) studies the feasibility of the proposed solution based on the large-scale real-world dataset, DeepSense 6G, that comprises multi-modal sensing and communication data. Based on the adopted real-world dataset, the developed solution achieves $\approx 90\%$ accuracy in predicting blockages happening within the future $0.1$s and $\approx 80\%$ for blockages happening within $1$s, which highlights a promising solution for mmWave/sub-THz communication networks.      
### 3.ROCT-Net: A new ensemble deep convolutional model with improved spatial resolution learning for detecting common diseases from retinal OCT images  [ :arrow_down: ](https://arxiv.org/pdf/2203.01883.pdf)
>  Optical coherence tomography (OCT) imaging is a well-known technology for visualizing retinal layers and helps ophthalmologists to detect possible diseases. Accurate and early diagnosis of common retinal diseases can prevent the patients from suffering critical damages to their vision. Computer-aided diagnosis (CAD) systems can significantly assist ophthalmologists in improving their examinations. This paper presents a new enhanced deep ensemble convolutional neural network for detecting retinal diseases from OCT images. Our model generates rich and multi-resolution features by employing the learning architectures of two robust convolutional models. Spatial resolution is a critical factor in medical images, especially the OCT images that contain tiny essential points. To empower our model, we apply a new post-architecture model to our ensemble model for enhancing spatial resolution learning without increasing computational costs. The introduced post-architecture model can be deployed to any feature extraction model to improve the utilization of the feature map's spatial values. We have collected two open-source datasets for our experiments to make our models capable of detecting six crucial retinal diseases: Age-related Macular Degeneration (AMD), Central Serous Retinopathy (CSR), Diabetic Retinopathy (DR), Choroidal Neovascularization (CNV), Diabetic Macular Edema (DME), and Drusen alongside the normal cases. Our experiments on two datasets and comparing our model with some other well-known deep convolutional neural networks have proven that our architecture can increase the classification accuracy up to 5%. We hope that our proposed methods create the next step of CAD systems development and help future researches. The code of this paper is shared at <a class="link-external link-https" href="https://github.com/mr7495/OCT-classification" rel="external noopener nofollow">this https URL</a>.      
### 4.DenseUNets with feedback non-local attention for the segmentation of specular microscopy images of the corneal endothelium with Fuchs dystrophy  [ :arrow_down: ](https://arxiv.org/pdf/2203.01882.pdf)
>  To estimate the corneal endothelial parameters from specular microscopy images depicting cornea guttata (Fuchs endothelial dystrophy), we propose a new deep learning methodology that includes a novel attention mechanism named feedback non-local attention (fNLA). Our approach first infers the cell edges, then selects the cells that are well detected, and finally applies a postprocessing method to correct mistakes and provide the binary segmentation from which the corneal parameters are estimated (cell density [ECD], coefficient of variation [CV], and hexagonality [HEX]). In this study, we analyzed 1203 images acquired with a Topcon SP-1P microscope, 500 of which contained guttae. Manual segmentation was performed in all images. We compared the results of different networks (UNet, ResUNeXt, DenseUNets, UNet++) and found that DenseUNets with fNLA provided the best performance, with a mean absolute error of 23.16 [cells/mm$^{2}$] in ECD, 1.28 [%] in CV, and 3.13 [%] in HEX, which was 3-6 times smaller than the error obtained by Topcon's built-in software. Our approach handled the cells affected by guttae remarkably well, detecting cell edges occluded by small guttae while discarding areas covered by large guttae. fNLA made use of the local information, providing sharper edges in guttae areas and better results in the selection of well-detected cells. Overall, the proposed method obtained reliable and accurate estimations in extremely challenging specular images with guttae, being the first method in the literature to solve this problem adequately. Code is available in our GitHub.      
### 5.Gaussian Process-based Spatial Reconstruction of Electromagnetic fields  [ :arrow_down: ](https://arxiv.org/pdf/2203.01869.pdf)
>  These days we live in a world with a permanent electromagnetic field. This raises many questions about our health and the deployment of new equipment. The problem is that these fields remain difficult to visualize easily, which only some experts can understand. To tackle this problem, we propose to spatially estimate the level of the field based on a few observations at all positions of the considered space. This work presents an algorithm for spatial reconstruction of electromagnetic fields using the Gaussian Process. We consider a spatial, physical phenomenon observed by a sensor network. A Gaussian Process regression model with selected mean and covariance function is implemented to develop a 9 sensors-based estimation algorithm. A Bayesian inference approach is used to perform the model selection of the covariance function and to learn the hyperparameters from our data set. We present the prediction performance of the proposed model and compare it with the case where the mean is zero. The results show that the proposed Gaussian Process-based prediction model reconstructs the EM fields in all positions only using 9 sensors.      
### 6.Stochastic Model Predictive Control using Initial State Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2203.01844.pdf)
>  We propose a stochastic MPC scheme using an optimization over the initial state for the predicted trajectory. Considering linear discrete-time systems under unbounded additive stochastic disturbances subject to chance constraints, we use constraint tightening based on probabilistic reachable sets to design the MPC. The scheme avoids the infeasibility issues arising from unbounded disturbances by including the initial state as a decision variable. We show that the stabilizing control scheme can guarantee constraint satisfaction in closed loop, assuming unimodal disturbances. In addition to illustrating these guarantees, the numerical example indicates further advantages of optimizing over the initial state for the transient behavior.      
### 7.A Brief Overview of Unsupervised Neural Speech Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.01829.pdf)
>  Unsupervised representation learning for speech processing has matured greatly in the last few years. Work in computer vision and natural language processing has paved the way, but speech data offers unique challenges. As a result, methods from other domains rarely translate directly. We review the development of unsupervised representation learning for speech over the last decade. We identify two primary model categories: self-supervised methods and probabilistic latent variable models. We describe the models and develop a comprehensive taxonomy. Finally, we discuss and compare models from the two categories.      
### 8.Improving Non-native Word-level Pronunciation Scoring with Phone-level Mixup Data Augmentation and Multi-source Information  [ :arrow_down: ](https://arxiv.org/pdf/2203.01826.pdf)
>  Deep learning-based pronunciation scoring models highly rely on the availability of the annotated non-native data, which is costly and has scalability issues. To deal with the data scarcity problem, data augmentation is commonly used for model pretraining. In this paper, we propose a phone-level mixup, a simple yet effective data augmentation method, to improve the performance of word-level pronunciation scoring. Specifically, given a phoneme sequence from lexicon, the artificial augmented word sample can be generated by randomly sampling from the corresponding phone-level features in training data, while the word score is the average of their GOP scores. Benefit from the arbitrary phone-level combination, the mixup is able to generate any word with various pronunciation scores. Moreover, we utilize multi-source information (e.g., MFCC and deep features) to further improve the scoring system performance. The experiments conducted on the Speechocean762 show that the proposed system outperforms the baseline by adding the mixup data for pretraining, with Pearson correlation coefficients (PCC) increasing from 0.567 to 0.61. The results also indicate that proposed method achieves similar performance by using 1/10 unlabeled data of baseline. In addition, the experimental results also demonstrate the efficiency of our proposed multi-source approach.      
### 9.Speech segmentation using multilevel hybrid filters  [ :arrow_down: ](https://arxiv.org/pdf/2203.01819.pdf)
>  A novel approach for speech segmentation is proposed, based on Multilevel Hybrid (mean/min) Filters (MHF) with the following features: An accurate transition location. Good performance in noisy environments (gaussian and impulsive noise). The proposed method is based on spectral changes, with the goal of segmenting the voice into homogeneous acoustic segments. This algorithm is being used for phoneticallysegmented speech coder, with successful results.      
### 10.ADPCM with nonlinear prediction  [ :arrow_down: ](https://arxiv.org/pdf/2203.01818.pdf)
>  Many speech coders are based on linear prediction coding (LPC), nevertheless with LPC is not possible to model the nonlinearities present in the speech signal. Because of this there is a growing interest for nonlinear techniques. In this paper we discuss ADPCM schemes with a nonlinear predictor based on neural nets, which yields an increase of 1-2.5dB in the SEGSNR over classical methods. This paper will discuss the block-adaptive and sample-adaptive predictions.      
### 11.Deep Learning-Based Joint Control of Acoustic Echo Cancellation, Beamforming and Postfiltering  [ :arrow_down: ](https://arxiv.org/pdf/2203.01793.pdf)
>  We introduce a novel method for controlling the functionality of a hands-free speech communication device which comprises a model-based acoustic echo canceller (AEC), minimum variance distortionless response (MVDR) beamformer (BF) and spectral postfilter (PF). While the AEC removes the early echo component, the MVDR BF and PF suppress the residual echo and background noise. As key innovation, we suggest to use a single deep neural network (DNN) to jointly control the adaptation of the various algorithmic components. This allows for rapid convergence and high steady-state performance in the presence of high-level interfering double-talk. End-to-end training of the DNN using a time-domain speech extraction loss function avoids the design of individual control strategies.      
### 12.Multi-Objective Design Space Exploration for the Optimization of the HEVC Mode Decision Process  [ :arrow_down: ](https://arxiv.org/pdf/2203.01782.pdf)
>  Finding the best possible encoding decisions for compressing a video sequence is a highly complex problem. In this work, we propose a multi-objective Design Space Exploration (DSE) method to automatically find HEVC encoder implementations that are optimized for several different criteria. The DSE shall optimize the coding mode evaluation order of the mode decision process and jointly explore early skip conditions to minimize the four objectives a) bitrate, b) distortion, c) encoding time, and d) decoding energy. In this context, we use a SystemC-based actor model of the HM test model encoder for the evaluation of each explored solution. The evaluation that is based on real measurements shows that our framework can automatically generate encoder solutions that save more than 60% of encoding time or 3% of decoding energy when accepting bitrate increases of around 3%.      
### 13.Estimation of Non-Functional Properties for Embedded Hardware with Application to Image Processing  [ :arrow_down: ](https://arxiv.org/pdf/2203.01771.pdf)
>  In recent years, due to a higher demand for portable devices, which provide restricted amounts of processing capacity and battery power, the need for energy and time efficient hard- and software solutions has increased. Preliminary estimations of time and energy consumption can thus be valuable to improve implementations and design decisions. To this end, this paper presents a method to estimate the time and energy consumption of a given software solution, without having to rely on the use of a traditional Cycle Accurate Simulator (CAS). Instead, we propose to utilize a combination of high-level functional simulation with a mechanistic extension to include non-functional properties: Instruction counts from virtual execution are multiplied with corresponding specific energies and times. By evaluating two common image processing algorithms on an FPGA-based CPU, where a mean relative estimation error of 3% is achieved for cacheless systems, we show that this estimation tool can be a valuable aid in the development of embedded processor architectures. The tool allows the developer to reach well-suited design decisions regarding the optimal processor hardware configuration for a given algorithm at an early stage in the design process.      
### 14.Estimating the HEVC Decoding Energy Using the Decoder Processing Time  [ :arrow_down: ](https://arxiv.org/pdf/2203.01767.pdf)
>  This paper presents a method to accurately estimate the required decoding energy for a given HEVC software decoding solution. We show that the decoder's processing time as returned by common C++ and UNIX functions is a highly suitable parameter to obtain valid estimations for the actual decoding energy. We verify this hypothesis by performing an exhaustive measurement series using different decoder setups and video bit streams. Our findings can be used by developers and researchers in the search for new energy saving video compression algorithms.      
### 15.Joint Optimization of Rate, Distortion, and Decoding Energy for HEVC Intraframe Coding  [ :arrow_down: ](https://arxiv.org/pdf/2203.01765.pdf)
>  This paper presents a novel algorithm that aims at minimizing the required decoding energy by exploiting a general energy model for HEVC-decoder solutions. We incorporate the energy model into the HEVC encoder such that it is capable of constructing a bit stream whose decoding process consumes less energy than the decoding process of a conventional bit stream. To achieve this, we propose to extend the traditional Rate-Distortion-Optimization scheme to a Decoding-Energy-Rate-Distortion approach. To obtain fast encoding decisions in the optimization process, we derive a fixed relation between the quantization parameter and the Lagrange multiplier for energy optimization. Our experiments show that this concept is applicable for intraframe-coded videos and that for local playback as well as online streaming scenarios, up to 15% of the decoding energy can be saved at the expense of a bitrate increase of approximately the same magnitude.      
### 16.Modeling the Energy Consumption of HEVC Intra Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2203.01755.pdf)
>  Battery life is one of the major limitations to mobile device use, which makes research on energy efficient soft- and hardware an important task. This paper investigates the energy required by a CPU when decoding compressed bitstream videos on mobile platforms. A model is derived that describes the energy consumption of the new HEVC decoder for intra coded videos. We show that the relative estimation error of the model is smaller than 3.2% and that the model can be used to build encoders aiming at minimizing decoding energy.      
### 17.Constrained unsupervised anomaly segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.01671.pdf)
>  Current unsupervised anomaly localization approaches rely on generative models to learn the distribution of normal images, which is later used to identify potential anomalous regions derived from errors on the reconstructed images. However, a main limitation of nearly all prior literature is the need of employing anomalous images to set a class-specific threshold to locate the anomalies. This limits their usability in realistic scenarios, where only normal data is typically accessible. Despite this major drawback, only a handful of works have addressed this limitation, by integrating supervision on attention maps during training. In this work, we propose a novel formulation that does not require accessing images with abnormalities to define the threshold. Furthermore, and in contrast to very recent work, the proposed constraint is formulated in a more principled manner, leveraging well-known knowledge in constrained optimization. In particular, the equality constraint on the attention maps in prior work is replaced by an inequality constraint, which allows more flexibility. In addition, to address the limitations of penalty-based functions we employ an extension of the popular log-barrier methods to handle the constraint. Last, we propose an alternative regularization term that maximizes the Shannon entropy of the attention maps, reducing the amount of hyperparameters of the proposed model. Comprehensive experiments on two publicly available datasets on brain lesion segmentation demonstrate that the proposed approach substantially outperforms relevant literature, establishing new state-of-the-art results for unsupervised lesion segmentation, and without the need to access anomalous images.      
### 18.Translational Lung Imaging Analysis Through Disentangled Representations  [ :arrow_down: ](https://arxiv.org/pdf/2203.01668.pdf)
>  The development of new treatments often requires clinical trials with translational animal models using (pre)-clinical imaging to characterize inter-species pathological processes. Deep Learning (DL) models are commonly used to automate retrieving relevant information from the images. Nevertheless, they typically suffer from low generability and explainability as a product of their entangled design, resulting in a specific DL model per animal model. Consequently, it is not possible to take advantage of the high capacity of DL to discover statistical relationships from inter-species images. <br>To alleviate this problem, in this work, we present a model capable of extracting disentangled information from images of different animal models and the mechanisms that generate the images. Our method is located at the intersection between deep generative models, disentanglement and causal representation learning. It is optimized from images of pathological lung infected by Tuberculosis and is able: a) from an input slice, infer its position in a volume, the animal model to which it belongs, the damage present and even more, generate a mask covering the whole lung (similar overlap measures to the nnU-Net), b) generate realistic lung images by setting the above variables and c) generate counterfactual images, namely, healthy versions of a damaged input slice.      
### 19.Selective Residual M-Net for Real Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2203.01645.pdf)
>  Image restoration is a low-level vision task which is to restore degraded images to noise-free images. With the success of deep neural networks, the convolutional neural networks surpass the traditional restoration methods and become the mainstream in the computer vision area. To advance the performanceof denoising algorithms, we propose a blind real image denoising network (SRMNet) by employing a hierarchical architecture improved from U-Net. Specifically, we use a selective kernel with residual block on the hierarchical structure called M-Net to enrich the multi-scale semantic information. Furthermore, our SRMNet has competitive performance results on two synthetic and two real-world noisy datasets in terms of quantitative metrics and visual quality. The source code and pretrained model are available at <a class="link-external link-https" href="https://github.com/TentativeGitHub/SRMNet" rel="external noopener nofollow">this https URL</a>.      
### 20.Risk-aware Stochastic Shortest Path  [ :arrow_down: ](https://arxiv.org/pdf/2203.01640.pdf)
>  We treat the problem of risk-aware control for stochastic shortest path (SSP) on Markov decision processes (MDP). Typically, expectation is considered for SSP, which however is oblivious to the incurred risk. We present an alternative view, instead optimizing conditional value-at-risk (CVaR), an established risk measure. We treat both Markov chains as well as MDP and introduce, through novel insights, two algorithms, based on linear programming and value iteration, respectively. Both algorithms offer precise and provably correct solutions. Evaluation of our prototype implementation shows that risk-aware control is feasible on several moderately sized models.      
### 21.ETCetera: beyond Event-Triggered Control  [ :arrow_down: ](https://arxiv.org/pdf/2203.01623.pdf)
>  We present ETCetera, a Python library developed for the analysis and synthesis of the sampling behaviour of event triggered control (ETC) systems. In particular, the tool constructs abstractions of the sampling behaviour of given ETC systems, in the form of timed automata (TA) or finite-state transition systems (FSTSs). When the abstraction is an FSTS, ETCetera provides diverse manipulation tools for analysis of ETC's sampling performance, synthesis of communication traffic schedulers (when networks shared by multiple ETC loops are considered), and optimization of sampling strategies. Additionally, the TA models may be exported to UPPAAL for analysis and synthesis of schedulers. Several examples of the tool's application for analysis and synthesis problems with different types of dynamics and event-triggered implementations are provided.      
### 22.The Vicomtech Audio Deepfake Detection System based on Wav2Vec2 for the 2022 ADD Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2203.01573.pdf)
>  This paper describes our submitted systems to the 2022 ADD challenge withing the tracks 1 and 2. Our approach is based on the combination of a pre-trained wav2vec2 feature extractor and a downstream classifier to detect spoofed audio. This method exploits the contextualized speech representations at the different transformer layers to fully capture discriminative information. Furthermore, the classification model is adapted to the application scenario using different data augmentation techniques. We evaluate our system for audio synthesis detection in both the ASVspoof 2021 and the 2022 ADD challenges, showing its robustness and good performance in realistic challenging environments such as telephonic and audio codec systems, noisy audio, and partial deepfakes.      
### 23.Phased array beamforming methods for powering biomedical ultrasonic implants  [ :arrow_down: ](https://arxiv.org/pdf/2203.01493.pdf)
>  Millimeter-scale implants using ultrasound for power and communication have been proposed for a range of deep-tissue applications, including neural recording and stimulation. However, published implementations have shown high sensitivity to misalignment with the external ultrasound transducer. Ultrasonic beamforming using a phased array to these implants can improve tolerance to misalignment, reduce implant volume, and allow multiple implants to be operated simultaneously in different locations. This paper details the design of a custom planar phased array ultrasound system, which is capable of steering and focusing ultrasound power within a 3D volume. Analysis and simulation is performed to determine the choice of array element pitch, with special attention given to maximizing the power available at the implant while meeting FDA limits for diagnostic ultrasound. Time reversal is proposed as a computationally simple approach to beamforming that is robust despite scattering and inhomogeneity of the acoustic medium. This technique is demonstrated both in active drive and pulse-echo modes, and it is experimentally compared with other beamforming techniques by measuring energy transfer efficiency at varying depths and angles. Simultaneous power delivery to multiple implants is also demonstrated.      
### 24.CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2203.01475.pdf)
>  Curating a large set of fully annotated training data can be costly, especially for the tasks of medical image segmentation. Scribble, a weaker form of annotation, is more obtainable in practice, but training segmentation models from limited supervision of scribbles is still challenging. To address the difficulties, we propose a new framework for scribble learning-based medical image segmentation, which is composed of mix augmentation and cycle consistency and thus is referred to as CycleMix. For augmentation of supervision, CycleMix adopts the mixup strategy with a dedicated design of random occlusion, to perform increments and decrements of scribbles. For regularization of supervision, CycleMix intensifies the training objective with consistency losses to penalize inconsistent segmentation, which results in significant improvement of segmentation performance. Results on two open datasets, i.e., ACDC and MSCMRseg, showed that the proposed method achieved exhilarating performance, demonstrating comparable or even better accuracy than the fully-supervised methods. The code and expert-made scribble annotations for MSCMRseg will be released once this article is accepted for publication.      
### 25.Hierarchical and Modular Supervisory Control under Partial Observation: Normality  [ :arrow_down: ](https://arxiv.org/pdf/2203.01444.pdf)
>  Conditions preserving observability of specifications between the plant and its abstraction are fundamental for hierarchical supervisory control of discrete-event systems under partial observation. Observation consistency and local observation consistency were identified as such conditions. To preserve normality, only observation consistency is required. Although observation consistency preserves normality between the levels for normal specifications, we show that for specifications that are not normal, observation consistency does not guarantee that the supremal normal sublanguage computed on the low level and on the high level coincide. Therefore, we define modified observation consistency, under which the supremal normal sublanguages of different levels coincide. Since we show that the verification of (modified) observation consistency is PSPACE-hard for finite automata models and undecidable for systems slightly more expressive than finite automata, we further discuss two stronger conditions that are easy to verify, and hence of practical interest. We apply modified observation consistency in modular systems to guarantee that the global supervisor can be computed locally. In particular, we use the coordination control framework, where the global and local computations of supervisors coincide under the assumption that the coordinated events are all observable.      
### 26.Multi-system intervention optimization for interdependent infrastructure  [ :arrow_down: ](https://arxiv.org/pdf/2203.01411.pdf)
>  The wellbeing of modern societies is dependent upon the functioning of their infrastructure networks. This paper introduces the 3C concept, an integrative multi-system and multi-stakeholder optimization approach for managing infrastructure interventions (e.g., maintenance, renovation, etc.). The proposed approach takes advantage of the benefits achieved by grouping (i.e., optimizing) intervention activities. Intervention optimization leads to substantial savings on both direct intervention costs (operator) and indirect unavailability costs (society) by reducing the number of system interruptions. The proposed optimization approach is formalized into a structured mathematical model that can account for the interactions between multiple infrastructure networks and the impact on multiple stakeholders (e.g., society and infrastructure operators), and it can accommodate different types of intervention, such as maintenance, removal, and upgrading. The different types of interdependencies, within and across infrastructures, are modeled using a proposed interaction matrix (IM). The IM allows integrating the interventions of different infrastructure networks whose interventions are normally planned independently. Moreover, the introduced 3C concept accounts for central interventions, which are those that must occur at a pre-established time moment, where neither delay nor advance is permitted. To demonstrate the applicability of the proposed approach, an illustrative example of a multi-system and multi-actor intervention planning is introduced. Results show a substantial reduction in the operator and societal costs. In addition, the optimal intervention program obtained in the analysis shows no predictable patterns, which indicates it is a useful managerial decision support tool.      
### 27.Design and comparison of two linear controllers with precompensation gain for the Quadruple inverted pendulum  [ :arrow_down: ](https://arxiv.org/pdf/2203.01409.pdf)
>  In this work we present a workflow for designing two linear control techniques applied to the dynamic system quadruple inverted pendulum mounted on a cart (QIP) where the steady state error on cart position is eliminated through a precompensation gain. The first control law designed was based on LQR, technique that stabilizes the system states based on a minimization of the total energy of the system states and the second control technique was pole placement by 2nd order system approximation, this method allowed to define a settling time and overshoot of the desired response of the plant, for this control the system was considered SISO, being the only output of the plant the horizontal position of the cart. To eliminate the steady-state error generated by both linear controllers, an analysis of the stationary response of the system under control in the Laplace Domain was performed. The simulation environment used to validate the results presented in this paper was Simscape of Matlab, it is a simulation environment that allowed to simulate the dynamic of nonlinear mechanical systems. The results obtained indicate that both controllers are able to stabilize the plant and move the cart position to the desired setpoint. Finally, we discuss about the performances of both linear control techniques.      
### 28.Simplified Stability Assessment of Power Systems with Variable-Delay Wide-Area Damping Control  [ :arrow_down: ](https://arxiv.org/pdf/2203.01362.pdf)
>  Power electronic devices such as HVDC and FACTS can be used to improve the damping of poorly damped inter-area modes in large power systems. This involves the use of wide-area feedback signals, which are transmitted via communication networks. The performance of the closed-loop system is strongly influenced by the delay associated with wide-area signals. The random nature of this delay introduces a switched linear system model. The stability assessment of such a system requires linear matrix inequality based approaches. This makes the stability analysis more complicated as the system size increases. To address this challenge, this paper proposes a delay-processing strategy that simplifies the modelling and analysis in discrete-domain. In contrast to the existing stability assessment techniques, the proposed approach is advantageous because the stability, as well as damping performance, can be accurately predicted by a simplified analysis. The proposed methodology is verified with a case study on the 2-area 4-machine power system with a series compensated tie-line. The results are found to be in accordance with the predictions of the proposed simplified analysis.      
### 29.Hyperspectral Pixel Unmixing with Latent Dirichlet Variational Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2203.01327.pdf)
>  Hyperspectral pixel intensities result from a mixing of reflectances from several materials. This paper develops a method of hyperspectral pixel {\it unmixing} that aims to recover the "pure" spectral signal of each material (hereafter referred to as {\it endmembers}) together with the mixing ratios ({\it abundances}) given the spectrum of a single pixel. The unmixing problem is particularly relevant in the case of low-resolution hyperspectral images captured in a remote sensing setting, where individual pixels can cover large regions of the scene. Under the assumptions that (1) a multivariate Normal distribution can represent the spectra of an endmember and (2) a Dirichlet distribution can encode abundances of different endmembers, we develop a Latent Dirichlet Variational Autoencoder for hyperspectral pixel unmixing. Our approach achieves state-of-the-art results on standard benchmarks and on synthetic data generated using United States Geological Survey spectral library.      
### 30.Self-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Observations  [ :arrow_down: ](https://arxiv.org/pdf/2203.01325.pdf)
>  In this paper, we consider two challenging issues in reference-based super-resolution (RefSR), (i) how to choose a proper reference image, and (ii) how to learn real-world RefSR in a self-supervised manner. Particularly, we present a novel self-supervised learning approach for real-world image SR from observations at dual camera zooms (SelfDZSR). For the first issue, the more zoomed (telephoto) image can be naturally leveraged as the reference to guide the SR of the lesser zoomed (short-focus) image. For the second issue, SelfDZSR learns a deep network to obtain the SR result of short-focal image and with the same resolution as the telephoto image. For this purpose, we take the telephoto image instead of an additional high-resolution image as the supervision information and select a patch from it as the reference to super-resolve the corresponding short-focus image patch. To mitigate the effect of various misalignment between the short-focus low-resolution (LR) image and telephoto ground-truth (GT) image, we design a degradation model and map the GT to a pseudo-LR image aligned with GT. Then the pseudo-LR and LR image can be fed into the proposed adaptive spatial transformer networks (AdaSTN) to deform the LR features. During testing, SelfDZSR can be directly deployed to super-solve the whole short-focus image with the reference of telephoto image. Experiments show that our method achieves better quantitative and qualitative performance against state-of-the-arts. The code and pre-trained models will be publicly available.      
### 31.Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.01324.pdf)
>  Semi-supervised segmentation remains challenging in medical imaging since the amount of annotated medical data is often limited and there are many blurred pixels near the adhesive edges or low-contrast regions. To address the issues, we advocate to firstly constrain the consistency of samples with and without strong perturbations to apply sufficient smoothness regularization and further encourage the class-level separation to exploit the unlabeled ambiguous pixels for the model training. Particularly, in this paper, we propose the SS-Net for semi-supervised medical image segmentation tasks, via exploring the pixel-level Smoothness and inter-class Separation at the same time. The pixel-level smoothness forces the model to generate invariant results under adversarial perturbations. Meanwhile, the inter-class separation constrains individual class features should approach their corresponding high-quality prototypes, in order to make each class distribution compact and separate different classes. We evaluated our SS-Net against five recent methods on the public LA and ACDC datasets. The experimental results under two semi-supervised settings demonstrate the superiority of our proposed SS-Net, achieving new state-of-the-art (SOTA) performance on both datasets. The codes will be released.      
### 32.Colon Nuclei Instance Segmentation using a Probabilistic Two-Stage Detector  [ :arrow_down: ](https://arxiv.org/pdf/2203.01321.pdf)
>  Cancer is one of the leading causes of death in the developed world. Cancer diagnosis is performed through the microscopic analysis of a sample of suspicious tissue. This process is time consuming and error prone, but Deep Learning models could be helpful for pathologists during cancer diagnosis. We propose to change the CenterNet2 object detection model to also perform instance segmentation, which we call SegCenterNet2. We train SegCenterNet2 in the CoNIC challenge dataset and show that it performs better than Mask R-CNN in the competition metrics.      
### 33.Investigating the limited performance of a deep-learning-based SPECT denoising approach: An observer-study-based characterization  [ :arrow_down: ](https://arxiv.org/pdf/2203.01918.pdf)
>  Multiple objective assessment of image-quality-based studies have reported that several deep-learning-based denoising methods show limited performance on signal-detection tasks. Our goal was to investigate the reasons for this limited performance. To achieve this goal, we conducted a task-based characterization of a DL-based denoising approach for individual signal properties. We conducted this study in the context of evaluating a DL-based approach for denoising SPECT images. The training data consisted of signals of different sizes and shapes within a clustered-lumpy background, imaged with a 2D parallel-hole-collimator SPECT system. The projections were generated at normal and 20% low count level, both of which were reconstructed using an OSEM algorithm. A CNN-based denoiser was trained to process the low-count images. The performance of this CNN was characterized for five different signal sizes and four different SBR by designing each evaluation as an SKE/BKS signal-detection task. Performance on this task was evaluated using an anthropomorphic CHO. As in previous studies, we observed that the DL-based denoising method did not improve performance on signal-detection tasks. Evaluation using the idea of observer-study-based characterization demonstrated that the DL-based denoising approach did not improve performance on the signal-detection task for any of the signal types. Overall, these results provide new insights on the performance of the DL-based denoising approach as a function of signal size and contrast. More generally, the observer study-based characterization provides a mechanism to evaluate the sensitivity of the method to specific object properties and may be explored as analogous to characterizations such as modulation transfer function for linear systems. Finally, this work underscores the need for objective task-based evaluation of DL-based denoising approaches.      
### 34.An Adaptive Human Driver Model for Realistic Race Car Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2203.01909.pdf)
>  Engineering a high-performance race car requires a direct consideration of the human driver using real-world tests or Human-Driver-in-the-Loop simulations. Apart from that, offline simulations with human-like race driver models could make this vehicle development process more effective and efficient but are hard to obtain due to various challenges. With this work, we intend to provide a better understanding of race driver behavior and introduce an adaptive human race driver model based on imitation learning. Using existing findings and an interview with a professional race engineer, we identify fundamental adaptation mechanisms and how drivers learn to optimize lap time on a new track. Subsequently, we use these insights to develop generalization and adaptation techniques for a recently presented probabilistic driver modeling approach and evaluate it using data from professional race drivers and a state-of-the-art race car simulator. We show that our framework can create realistic driving line distributions on unseen race tracks with almost human-like performance. Moreover, our driver model optimizes its driving lap by lap, correcting driving errors from previous laps while achieving faster lap times. This work contributes to a better understanding and modeling of the human driver, aiming to expedite simulation methods in the modern vehicle development process and potentially supporting automated driving and racing technologies.      
### 35.An observer cascade for velocity and multiple line estimation  [ :arrow_down: ](https://arxiv.org/pdf/2203.01879.pdf)
>  Previous incremental estimation methods consider estimating a single line, requiring as many observers as the number of lines to be mapped. This leads to the need for having at least $4N$ state variables, with $N$ being the number of lines. This paper presents the first approach for multi-line incremental estimation. Since lines are common in structured environments, we aim to exploit that structure to reduce the state space. The modeling of structured environments proposed in this paper reduces the state space to $3N + 3$ and is also less susceptible to singular configurations. An assumption the previous methods make is that the camera velocity is available at all times. However, the velocity is usually retrieved from odometry, which is noisy. With this in mind, we propose coupling the camera with an Inertial Measurement Unit (IMU) and an observer cascade. A first observer retrieves the scale of the linear velocity and a second observer for the lines mapping. The stability of the entire system is analyzed. The cascade is shown to be asymptotically stable and shown to converge in experiments with simulated data.      
### 36.Low complexity equalization for AFDM in doubly dispersive channels  [ :arrow_down: ](https://arxiv.org/pdf/2203.01875.pdf)
>  Affine Frequency Division Multiplexing (AFDM), which is based on discrete affine Fourier transform (DAFT), has recently been proposed for reliable communication in high-mobility scenarios. Two low complexity detectors for AFDM are introduced here. Approximating the channel matrix as a band matrix via placing null symbols in the AFDM frame in the DAFT domain, a low complexity MMSE detection is proposed by means of the $\rm{LDL}$ factorization. Furthermore, exploiting the sparsity of the channel matrix, we propose a low complexity iterative decision feedback equalizer (DFE) based on weighted maximal ratio combining (MRC), which extracts and combines the received multipath components of the transmitted symbols in the DAFT domain. Simulation results show that the proposed detectors have similar performance, while weighted MRC-based DFE has lower complexity than band-matrix-approximation LMMSE when the channel impulse response has gaps.      
### 37.Quantum Image Processing  [ :arrow_down: ](https://arxiv.org/pdf/2203.01831.pdf)
>  Image processing is popular in our daily life because of the need to extract essential information from our 3D world, including a variety of applications in widely separated fields like bio-medicine, economics, entertainment, and industry. The nature of visual information, algorithm complexity, and the representation of 3D scenes in 2D spaces are all popular research topics. In particular, the rapidly increasing volume of image data as well as increasingly challenging computational tasks have become important driving forces for further improving the efficiency of image processing and analysis. Since the concept of quantum computing was proposed by Feynman in 1982, many achievements have shown that quantum computing has dramatically improved computational efficiency [1]. Quantum information processing exploit quantum mechanical properties, such as quantum superposition, entanglement and parallelism, and effectively accelerate many classical problems like factoring large numbers, searching an unsorted database, Boson sampling, quantum simulation, solving linear systems of equations, and machine learning. These unique quantum properties may also be used to speed up signal and data processing. In quantum image processing, quantum image representation plays a key role, which substantively determines the kinds of processing tasks and how well they can be performed.      
### 38.What Makes Transfer Learning Work For Medical Images: Feature Reuse &amp; Other Factors  [ :arrow_down: ](https://arxiv.org/pdf/2203.01825.pdf)
>  Transfer learning is a standard technique to transfer knowledge from one domain to another. For applications in medical imaging, transfer from ImageNet has become the de-facto approach, despite differences in the tasks and image characteristics between the domains. However, it is unclear what factors determine whether - and to what extent - transfer learning to the medical domain is useful. The long-standing assumption that features from the source domain get reused has recently been called into question. Through a series of experiments on several medical image benchmark datasets, we explore the relationship between transfer learning, data size, the capacity and inductive bias of the model, as well as the distance between the source and target domain. Our findings suggest that transfer learning is beneficial in most cases, and we characterize the important role feature reuse plays in its success.      
### 39.Intensity Image-based LiDAR Fiducial Marker System  [ :arrow_down: ](https://arxiv.org/pdf/2203.01816.pdf)
>  The fiducial marker system for LiDAR is crucial for the robotic application but it is still rare to date. In this paper, an Intensity Image-based LiDAR Fiducial Marker (IILFM) system is developed. This system only requires an unstructured point cloud with intensity as the input and it has no restriction on marker placement and shape. A marker detection method that locates the predefined 3D fiducials in the point cloud through the intensity image is introduced. Then, an approach that utilizes the detected 3D fiducials to estimate the LiDAR 6-DOF pose that describes the transmission from the world coordinate system to the LiDAR coordinate system is developed. Moreover, all these processes run in real-time (approx 40 Hz on Livox Mid-40 and approx 143 Hz on VLP-16). Qualitative and quantitative experiments are conducted to demonstrate that the proposed system has similar convenience and accuracy as the conventional visual fiducial marker system. The codes and results are available at: <a class="link-external link-https" href="https://github.com/York-SDCNLab/IILFM" rel="external noopener nofollow">this https URL</a>.      
### 40.Generative Modeling for Low Dimensional Speech Attributes with Neural Spline Flows  [ :arrow_down: ](https://arxiv.org/pdf/2203.01786.pdf)
>  Despite recent advances in generative modeling for text-to-speech synthesis, these models do not yet have the same fine-grained adjustability of pitch-conditioned deterministic models such as FastPitch and FastSpeech2. Pitch information is not only low-dimensional, but also discontinuous, making it particularly difficult to model in a generative setting. Our work explores several techniques for handling the aforementioned issues in the context of Normalizing Flow models. We also find this problem to be very well suited for Neural Spline flows, which is a highly expressive alternative to the more common affine-coupling mechanism in Normalizing Flows.      
### 41.Random Quantum Neural Networks (RQNN) for Noisy Image Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.01764.pdf)
>  Classical Random Neural Networks (RNNs) have demonstrated effective applications in decision making, signal processing, and image recognition tasks. However, their implementation has been limited to deterministic digital systems that output probability distributions in lieu of stochastic behaviors of random spiking signals. We introduce the novel class of supervised Random Quantum Neural Networks (RQNNs) with a robust training strategy to better exploit the random nature of the spiking RNN. The proposed RQNN employs hybrid classical-quantum algorithms with superposition state and amplitude encoding features, inspired by quantum information theory and the brain's spatial-temporal stochastic spiking property of neuron information encoding. We have extensively validated our proposed RQNN model, relying on hybrid classical-quantum algorithms via the PennyLane Quantum simulator with a limited number of \emph{qubits}. Experiments on the MNIST, FashionMNIST, and KMNIST datasets demonstrate that the proposed RQNN model achieves an average classification accuracy of $94.9\%$. Additionally, the experimental findings illustrate the proposed RQNN's effectiveness and resilience in noisy settings, with enhanced image classification accuracy when compared to the classical counterparts (RNNs), classical Spiking Neural Networks (SNNs), and the classical convolutional neural network (AlexNet). Furthermore, the RQNN can deal with noise, which is useful for various applications, including computer vision in NISQ devices. The PyTorch code (<a class="link-external link-https" href="https://github.com/darthsimpus/RQN" rel="external noopener nofollow">this https URL</a>) is made available on GitHub to reproduce the results reported in this manuscript.      
### 42.A Remark on Evolution Equation of Stochastic Logical Dynamic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.01722.pdf)
>  Modelling is an essential procedure in analyzing and controlling a given logical dynamic system (LDS). It has been proved that deterministic LDS can be modeled as a linear-like system using algebraic state space representation. However, due to the inherently non-linear, it is difficult to obtain the algebraic expression of a stochastic LDS. This paper provides a unified framework for transition analysis of LDSs with deterministic and stochastic dynamics. First, modelling of LDS with deterministic dynamics is reviewed. Then modeling of LDS with stochastic dynamics is considered, and non-equivalence between subsystems and global system is proposed. Next, the reason for the non-equivalence is provided. Finally, consistency condition is presented for independent model and conditional independent model.      
### 43.Detecting High-Quality GAN-Generated Face Images using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.01716.pdf)
>  In the past decades, the excessive use of the last-generation GAN (Generative Adversarial Networks) models in computer vision has enabled the creation of artificial face images that are visually indistinguishable from genuine ones. These images are particularly used in adversarial settings to create fake social media accounts and other fake online profiles. Such malicious activities can negatively impact the trustworthiness of users identities. On the other hand, the recent development of GAN models may create high-quality face images without evidence of spatial artifacts. Therefore, reassembling uniform color channel correlations is a challenging research problem. To face these challenges, we need to develop efficient tools able to differentiate between fake and authentic face images. In this chapter, we propose a new strategy to differentiate GAN-generated images from authentic images by leveraging spectral band discrepancies, focusing on artificial face image synthesis. In particular, we enable the digital preservation of face images using the Cross-band co-occurrence matrix and spatial co-occurrence matrix. Then, we implement these techniques and feed them to a Convolutional Neural Networks (CNN) architecture to identify the real from artificial faces. Additionally, we show that the performance boost is particularly significant and achieves more than 92% in different post-processing environments. Finally, we provide several research observations demonstrating that this strategy improves a comparable detection method based only on intra-band spatial co-occurrences.      
### 44.Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2203.01703.pdf)
>  Reconstructing 3D objects from 2D images is both challenging for our brains and machine learning algorithms. To support this spatial reasoning task, contextual information about the overall shape of an object is critical. However, such information is not captured by established loss terms (e.g. Dice loss). We propose to complement geometrical shape information by including multi-scale topological features, such as connected components, cycles, and voids, in the reconstruction loss. Our method calculates topological features from 3D volumetric data based on cubical complexes and uses an optimal transport distance to guide the reconstruction process. This topology-aware loss is fully differentiable, computationally efficient, and can be added to any neural network. We demonstrate the utility of our loss by incorporating it into SHAPR, a model for predicting the 3D cell shape of individual cells based on 2D microscopy images. Using a hybrid loss that leverages both geometrical and topological information of single objects to assess their shape, we find that topological information substantially improves the quality of reconstructions, thus highlighting its ability to extract more relevant features from image datasets.      
### 45.Unfolding-Aided Bootstrapped Phase Retrieval in Optical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2203.01695.pdf)
>  Phase retrieval in optical imaging refers to the recovery of a complex signal from phaseless data acquired in the form of its diffraction patterns. These patterns are acquired through a system with a coherent light source that employs a diffractive optical element (DOE) to modulate the scene resulting in coded diffraction patterns at the sensor. Recently, the hybrid approach of model-driven network or deep unfolding has emerged as an effective alternative because it allows for bounding the complexity of phase retrieval algorithms while also retaining their efficacy. Additionally, such hybrid approaches have shown promise in improving the design of DOEs that follow theoretical uniqueness conditions. There are opportunities to exploit novel experimental setups and resolve even more complex DOE phase retrieval applications. This paper presents an overview of algorithms and applications of deep unfolding for bootstrapped - regardless of near, middle, and far zones - phase retrieval.      
### 46.A Simpler Alternative: Minimizing Transition Systems Modulo Alternating Simulation Equivalence  [ :arrow_down: ](https://arxiv.org/pdf/2203.01672.pdf)
>  This paper studies the reduction (abstraction) of finite-state transition systems for control synthesis problems. We revisit the notion of alternating simulation equivalence (ASE), a more relaxed condition than alternating bisimulations, to relate systems and their abstractions. As with alternating bisimulations, ASE preserves the property that the existence of a controller for the abstraction is necessary and sufficient for a controller to exist for the original system. Moreover, being a less stringent condition, ASE can reduce systems further to produce smaller abstractions. We provide an algorithm that produces minimal AS equivalent abstractions. The theoretical results are then applied to obtain (un)schedulability certificates of periodic event-triggered control systems sharing a communication channel. A numerical example illustrates the results.      
### 47.On partitioning of an SHM problem and parallels with transfer learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.01655.pdf)
>  In the current work, a problem-splitting approach and a scheme motivated by transfer learning is applied to a structural health monitoring problem. The specific problem in this case is that of localising damage on an aircraft wing. The original experiment is described, together with the initial approach, in which a neural network was trained to localise damage. The results were not ideal, partly because of a scarcity of training data, and partly because of the difficulty in resolving two of the damage cases. In the current paper, the problem is split into two sub-problems and an increase in classification accuracy is obtained. The sub-problems are obtained by separating out the most difficult-to-classify damage cases. A second approach to the problem is considered by adopting ideas from transfer learning (usually applied in much deeper) networks to see if a network trained on the simpler damage cases can help with feature extraction in the more difficult cases. The transfer of a fixed trained batch of layers between the networks is found to improve classification by making the classes more separable in the feature space and to speed up convergence.      
### 48.On an application of graph neural networks in population based SHM  [ :arrow_down: ](https://arxiv.org/pdf/2203.01646.pdf)
>  Attempts have been made recently in the field of population-based structural health monitoring (PBSHM), to transfer knowledge between SHM models of different structures. The attempts have been focussed on homogeneous and heterogeneous populations. A more general approach to transferring knowledge between structures, is by considering all plausible structures as points on a multidimensional base manifold and building a fibre bundle. The idea is quite powerful, since, a mapping between points in the base manifold and their fibres, the potential states of any arbitrary structure, can be learnt. A smaller scale problem, but still useful, is that of learning a specific point of every fibre, i.e. that corresponding to the undamaged state of structures within a population. Under the framework of PBSHM, a data-driven approach to the aforementioned problem is developed. Structures are converted into graphs and inference is attempted within a population, using a graph neural network (GNN) algorithm. The algorithm solves a major problem existing in such applications. Structures comprise different sizes and are defined as abstract objects, thus attempting to perform inference within a heterogeneous population is not trivial. The proposed approach is tested in a simulated population of trusses. The goal of the application is to predict the first natural frequency of trusses of different sizes, across different environmental temperatures and having different bar member types. After training the GNN using part of the total population, it was tested on trusses that were not included in the training dataset. Results show that the accuracy of the regression is satisfactory even in structures with higher number of nodes and members than those used to train it.      
### 49.Optimization-based Phase-shift Codebook Design for Large IRSs  [ :arrow_down: ](https://arxiv.org/pdf/2203.01630.pdf)
>  In this paper, we focus on large intelligent reflecting surfaces (IRSs) and propose a new codebook construction method to obtain a set of pre-designed phase-shift configurations for the IRS unit cells. Since the complexity of online optimization and the overhead for channel estimation for IRS-assisted communications scale with the size of the phase-shift codebook, the design of small codebooks is of high importance. We consider both continuous and discrete phase shift designs and formulate the codebook construction as optimization problems. To solve the optimization problems, we propose an optimal algorithm for the discrete phaseshift design and a low-complexity sub-optimal solution for the continuous design. Simulation results show that the proposed algorithms facilitate the construction of codebooks of different sizes and with different beamwidths. Moreover, the performance of the discrete phase-shift design with 2-bit quantization is shown to approach that of the continuous phase-shift design. Finally, our simulation results show that the proposed designs enable large transmit power savings compared to the existing linear and quadratic codebook designs.      
### 50.Endogenous Security of Computation Offloading in Blockchain-Empowered Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2203.01621.pdf)
>  This paper investigates an endogenous security architecture for computation offloading in the Internet of Things (IoT), where the blockchain technology enables the traceability of malicious behaviors, and the task data uploading link from sensors to small base station (SBS) is protected by intelligent reflecting surface (IRS)-assisted physical layer security (PLS). After receiving task data, the SBS allocates computational resources to help sensors perform the task. The existing computation offloading schemes usually focus on network performance improvement, such as energy consumption minimization, and neglect the Gas fee paid by sensors, resulting in the discontent of high Gas payers. Here, we design a Gas-oriented computation offloading scheme that guarantees the degree of satisfaction of sensors, while aiming to reduce energy consumption. Also, we deduce the ergodic secrecy rate of IRS-assisted PLS transmission that can represent the global secrecy performance to allocate computational resources. The simulations show that the proposed scheme ensures that the node paying higher Gas gets stronger computational resources, and just raises $4\%$ energy consumption in comparison with energy consumption minimization schemes.      
### 51.Reconfigurable Intelligent Surface Assisted OFDM Relaying: Subcarrier Matching with Balanced SNR  [ :arrow_down: ](https://arxiv.org/pdf/2203.01589.pdf)
>  Reconfigurable intelligent surface (RIS) is a promising solution to enhance the performance of wireless communications via reconfiguring the wireless propagation environment. In this paper, we investigate the joint design of RIS passive beamforming and subcarrier matching in RIS-assisted orthogonal frequency division multiplexing (OFDM) dual-hop relaying systems under two cases, depending on the presence of the RIS reflected link from the source to the destination in the first hop. Accordingly, we formulate a mixed-integer nonlinear programming (MINIP) problem to maximize the sum achievable rate over all subcarriers by jointly optimizing the RIS passive beamforming and subcarrier matching. To solve this challenging problem, we first develop a branch-and-bound (BnB)-based alternating optimization algorithm to obtain a near-optimal solution by alternatively optimizing the subcarrier matching by the BnB method and the RIS passive beamforming by using semidefinite relaxation techniques. Then, a low-complexity difference-of-convex penalty-based algorithm is proposed to reduce the computation complexity in the BnB method. To further reduce the computational complexity, we utilize the learning-to-optimize approach to learn the joint design obtained from optimization techniques, which is more amenable to practical implementations. Lastly, computer simulations are presented to evaluate the performance of the proposed algorithms in the two cases. Simulation results demonstrate that the RIS-assisted OFDM relaying system achieves sustainable achievable rate gain as compared to that without RIS, and that with random passive beamforming, since RIS passive beamforming can be leveraged to recast the subcarrier matching among different subcarriers and balance the signal-to-noise ratio within each subcarrier pair.      
### 52.DareFightingICE Competition: A Fighting Game Sound Design and AI Competition  [ :arrow_down: ](https://arxiv.org/pdf/2203.01556.pdf)
>  This paper presents a new competition -- at the 2022 IEEE Conference on Games (CoG) -- called DareFightingICE Competition. The competition has two tracks: a sound design track and an AI track. The game platform for this competition is also called DareFightingICE, a fighting game platform. DareFightingICE is a sound-design-enhanced version of FightingICE, used earlier in a competition at CoG until 2021 to promote artificial intelligence (AI) research in fighting games. In the sound design track, participants compete for the best sound design, given the default sound design of DareFightingICE as a sample. Participants of the AI track are asked to develop their AI algorithm that controls a character given only sound as the input (blind AI) to fight against their opponent; a sample deep-learning blind AI will be provided by us. Our means to maximize the synergy between the two tracks are also described. This competition serves to come up with effective sound designs for visually impaired players, a group in the gaming community which has been mostly ignored. To the best of our knowledge, DareFightingICE Competition is the first of its kind within and outside of CoG.      
### 53.Curriculum-style Local-to-global Adaptation for Cross-domain Remote Sensing Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.01539.pdf)
>  Although domain adaptation has been extensively studied in natural image-based segmentation task, the research on cross-domain segmentation for very high resolution (VHR) remote sensing images (RSIs) still remains underexplored. The VHR RSIs-based cross-domain segmentation mainly faces two critical challenges: 1) Large area land covers with many diverse object categories bring severe local patch-level data distribution deviations, thus yielding different adaptation difficulties for different local patches; 2) Different VHR sensor types or dynamically changing modes cause the VHR images to go through intensive data distribution differences even for the same geographical location, resulting in different global feature-level domain gap. To address these challenges, we propose a curriculum-style local-to-global cross-domain adaptation framework for the segmentation of VHR RSIs. The proposed curriculum-style adaptation performs the adaptation process in an easy-to-hard way according to the adaptation difficulties that can be obtained using an entropy-based score for each patch of the target domain, and thus well aligns the local patches in a domain image. The proposed local-to-global adaptation performs the feature alignment process from the locally semantic to globally structural feature discrepancies, and consists of a semantic-level domain classifier and an entropy-level domain classifier that can reduce the above cross-domain feature discrepancies. Extensive experiments have been conducted in various cross-domain scenarios, including geographic location variations and imaging mode variations, and the experimental results demonstrate that the proposed method can significantly boost the domain adaptability of segmentation networks for VHR RSIs. Our code is available at: <a class="link-external link-https" href="https://github.com/BOBrown/CCDA_LGFA" rel="external noopener nofollow">this https URL</a>.      
### 54.Calculation of Sub-bands {1,2,5,6} for 64-Point Complex FFT and Its extension to N (=2^N) Point FFT  [ :arrow_down: ](https://arxiv.org/pdf/2203.01529.pdf)
>  FFT algorithm is one of the most applied algorithmsin digital signal processing. Digital signal processing hasgradually become important in biomedical application. Herehardware implementation of FFTs have found useful appli-cations for bio-wearable devices. However, for these devices, low-power and low-area are of utmost <a class="link-external link-http" href="http://importance.In" rel="external noopener nofollow">this http URL</a> this report, we investigate a sub-structure of decimation-in-frequency (DIF) FFT where a number of sub-bands areof interest to us. Specifically, we divide the range of frequencies into 8 sub-bands (0-7) and calculate 4 of them( 1,2,5,6). We show that using concepts likepushingandradix22, the number of complex multiplications can be dras-tically reduced for 16-point, 32-point and 64-point FFTswhile computing those specific bands. Later, we also extendit toN= 2n-point FFT based on optimized 64-point FFTstructure. The number of complex multiplications is furtherreduced usingmerge-FFT. Our results show that the numberof multiplications (and hence power) can be reduced greatlyusing our optimized structure compared to an unoptimizedstructure. This can find application in biomedical signal processing specifically while computingp ower spectral density of a physiological time series where reducing computational power is of utmost importance      
### 55.Semi-supervised Learning using Robust Loss  [ :arrow_down: ](https://arxiv.org/pdf/2203.01524.pdf)
>  The amount of manually labeled data is limited in medical applications, so semi-supervised learning and automatic labeling strategies can be an asset for training deep neural networks. However, the quality of the automatically generated labels can be uneven and inferior to manual labels. In this paper, we suggest a semi-supervised training strategy for leveraging both manually labeled data and extra unlabeled data. In contrast to the existing approaches, we apply robust loss for the automated labeled data to automatically compensate for the uneven data quality using a teacher-student framework. First, we generate pseudo-labels for unlabeled data using a teacher model pre-trained on labeled data. These pseudo-labels are noisy, and using them along with labeled data for training a deep neural network can severely degrade learned feature representations and the generalization of the network. Here we mitigate the effect of these pseudo-labels by using robust loss functions. Specifically, we use three robust loss functions, namely beta cross-entropy, symmetric cross-entropy, and generalized cross-entropy. We show that our proposed strategy improves the model performance by compensating for the uneven quality of labels in image classification as well as segmentation applications.      
### 56.SMA-NBO: A Sequential Multi-Agent Planning with Nominal Belief-State Optimization in Target Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2203.01507.pdf)
>  In target tracking with mobile multi-sensor systems, sensor deployment impacts the observation capabilities and the resulting state estimation quality. Based on a partially observable Markov decision process (POMDP) formulation comprised of the observable sensor dynamics, unobservable target states, and accompanying observation laws, we present a distributed information-driven solution approach to the multi-agent target tracking problem, namely, sequential multi-agent nominal belief-state optimization (SMA-NBO). SMA-NBO seeks to minimize the expected tracking error via receding horizon control including a heuristic expected cost-to-go (HECTG). SMA-NBO incorporates a computationally efficient approximation of the target belief-state over the horizon. The agent-by-agent decision-making is capable of leveraging on-board (edge) compute for selecting (sub-optimal) target-tracking maneuvers exhibiting non-myopic cooperative fleet behavior. The optimization problem explicitly incorporates semantic information defining target occlusions from a world model. To illustrate the efficacy of our approach, a random occlusion forest environment is simulated. SMA-NBO is compared to other baseline approaches. The simulation results show SMA-NBO 1) maintains tracking performance and reduces the computational cost by replacing the calculation of the expected target trajectory with a single sample trajectory based on maximum a posteriori estimation; 2) generates cooperative fleet decision by sequentially optimizing single-agent policy with efficient usage of other agents' policy of intent; 3) aptly incorporates the multiple weighted trace penalty (MWTP) HECTG, which improves tracking performance with a computationally efficient heuristic.      
### 57.Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.01452.pdf)
>  Panoramic images with their 360-degree directional view encompass exhaustive information about the surrounding space, providing a rich foundation for scene understanding. To unfold this potential in the form of robust panoramic segmentation models, large quantities of expensive, pixel-wise annotations are crucial for success. Such annotations are available, but predominantly for narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal resources for training panoramic models. Distortions and the distinct image-feature distribution in 360-degree panoramas impede the transfer from the annotation-rich pinhole domain and therefore come with a big dent in performance. To get around this domain difference and bring together semantic annotations from pinhole- and 360-degree surround-visuals, we propose to learn object deformations and panoramic image distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components which blend into our Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we tie together shared semantics in pinhole- and panoramic feature embeddings by generating multi-scale prototype features and aligning them in our Mutual Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by 14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available at <a class="link-external link-https" href="https://github.com/jamycheung/Trans4PASS" rel="external noopener nofollow">this https URL</a>.      
### 58.Learning Stochastic Parametric Differentiable Predictive Control Policies  [ :arrow_down: ](https://arxiv.org/pdf/2203.01447.pdf)
>  The problem of synthesizing stochastic explicit model predictive control policies is known to be quickly intractable even for systems of modest complexity when using classical control-theoretic methods. To address this challenge, we present a scalable alternative called stochastic parametric differentiable predictive control (SP-DPC) for unsupervised learning of neural control policies governing stochastic linear systems subject to nonlinear chance constraints. SP-DPC is formulated as a deterministic approximation to the stochastic parametric constrained optimal control problem. This formulation allows us to directly compute the policy gradients via automatic differentiation of the problem's value function, evaluated over sampled parameters and uncertainties. In particular, the computed expectation of the SP-DPC problem's value function is backpropagated through the closed-loop system rollouts parametrized by a known nominal system dynamics model and neural control policy which allows for direct model-based policy optimization. We provide theoretical probabilistic guarantees for policies learned via the SP-DPC method on closed-loop stability and chance constraints satisfaction. Furthermore, we demonstrate the computational efficiency and scalability of the proposed policy optimization algorithm in three numerical examples, including systems with a large number of states or subject to nonlinear constraints.      
### 59.LILE: Look In-Depth before Looking Elsewhere -- A Dual Attention Network using Transformers for Cross-Modal Information Retrieval in Histopathology Archives  [ :arrow_down: ](https://arxiv.org/pdf/2203.01445.pdf)
>  The volume of available data has grown dramatically in recent years in many applications. Furthermore, the age of networks that used multiple modalities separately has practically ended. Therefore, enabling bidirectional cross-modality data retrieval capable of processing has become a requirement for many domains and disciplines of research. This is especially true in the medical field, as data comes in a multitude of types, including various types of images and reports as well as molecular data. Most contemporary works apply cross attention to highlight the essential elements of an image or text in relation to the other modalities and try to match them together. However, regardless of their importance in their own modality, these approaches usually consider features of each modality equally. In this study, self-attention as an additional loss term will be proposed to enrich the internal representation provided into the cross attention module. This work suggests a novel architecture with a new loss term to help represent images and texts in the joint latent space. Experiment results on two benchmark datasets, i.e. MS-COCO and ARCH, show the effectiveness of the proposed method.      
### 60.Deformable Radar Polygon: A Lightweight and Predictable Occupancy Representation for Short-range Collision Avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2203.01442.pdf)
>  Inferring the drivable area in a scene is a key capability for ensuring vehicle avoids obstacles and enabling safe autonomous driving. However, traditional occupancy grid map suffers from high memory consumption when forming a fine-resolution grid for a large map. In this paper, we propose a lightweight, accurate, and predictable occupancy representation for automotive radars working for short-range applications that take interest in instantaneous free space surrounding the sensor. This new occupancy format is a polygon composed of a bunch of vertexes selected from radar measurements, which covers free space inside and gives a Doppler moving velocity for each vertex. It not only takes a very small memory for storage and update at every timeslot, but also has the predictable shape-change property based on vertex Doppler velocity. We name this kind of occupancy representation `deformable radar polygon'. Two formation algorithms for radar polygon are introduced for both single timeslot and continuous ISM update. To fit this new polygon representation, a matrix-form collision detection method have been modeled as well. The radar polygon algorithms and collision detection model have been validated via extensive experiments with real collected data and simulations, showing that the deformable radar polygon is very competitive in terms of its completeness, smoothness, accuracy, lightweight as well as shape-predictable property. Our codes will be made publicly available for ease of future works.      
### 61.SMTNet: Hierarchical cavitation intensity recognition based on sub-main transfer network  [ :arrow_down: ](https://arxiv.org/pdf/2203.01429.pdf)
>  With the rapid development of smart manufacturing, data-driven machinery health management has been of growing attention. In situations where some classes are more difficult to be distinguished compared to others and where classes might be organised in a hierarchy of categories, current DL methods can not work well. In this study, a novel hierarchical cavitation intensity recognition framework using Sub-Main Transfer Network, termed SMTNet, is proposed to classify acoustic signals of valve cavitation. SMTNet model outputs multiple predictions ordered from coarse to fine along a network corresponding to a hierarchy of target cavitation states. Firstly, a data augmentation method based on Sliding Window with Fast Fourier Transform (Swin-FFT) is developed to solve few-shot problem. Secondly, a 1-D double hierarchical residual block (1-D DHRB) is presented to capture sensitive features of the frequency domain valve acoustic signals. Thirdly, hierarchical multi-label tree is proposed to assist the embedding of the semantic structure of target cavitation states into SMTNet. Fourthly, experience filtering mechanism is proposed to fully learn a prior knowledge of cavitation detection model. Finally, SMTNet has been evaluated on two cavitation datasets without noise (Dataset 1 and Dataset 2), and one cavitation dataset with real noise (Dataset 3) provided by SAMSON AG (Frankfurt). The prediction accurcies of SMTNet for cavitation intensity recognition are as high as 95.32%, 97.16% and 100%, respectively. At the same time, the testing accuracies of SMTNet for cavitation detection are as high as 97.02%, 97.64% and 100%. In addition, SMTNet has also been tested for different frequencies of samples and has achieved excellent results of the highest frequency of samples of mobile phones.      
### 62.A 915-1220 TOPS/W Hybrid In-Memory Computing based Image Restoration and Region Proposal Integrated Circuit for Neuromorphic Vision Sensors in 65nm CMOS  [ :arrow_down: ](https://arxiv.org/pdf/2203.01413.pdf)
>  In this work, we present a hybrid memory bit cell - collocated SRAM and DRAM (CRAM) consisting of 11 transistors for in-memory computing (IMC) based image restoration (IR) and region proposal (RP). A robust RP updated algorithm is proposed to improve the performance. This work demonstrates IMC based global parallel diffusion and column/row-wise projection to achieve a maximal energy efficiency of 1220 TOPS/W for image restoration and 915 TOPS/W when combined with region proposal.      
### 63.Graph-based Multi-sensor Fusion for Consistent Localization of Autonomous Construction Robots  [ :arrow_down: ](https://arxiv.org/pdf/2203.01389.pdf)
>  Enabling autonomous operation of large-scale construction machines, such as excavators, can bring key benefits for human safety and operational opportunities for applications in dangerous and hazardous environments. To facilitate robot autonomy, robust and accurate state-estimation remains a core component to enable these machines for operation in a diverse set of complex environments. In this work, a method for multi-modal sensor fusion for robot state-estimation and localization is presented, enabling operation of construction robots in real-world scenarios. The proposed approach presents a graph-based prediction-update loop that combines the benefits of filtering and smoothing in order to provide consistent state estimates at high update rate, while maintaining accurate global localization for large-scale earth-moving excavators. Furthermore, the proposed approach enables a flexible integration of asynchronous sensor measurements and provides consistent pose estimates even during phases of sensor dropout. For this purpose, a dual-graph design for switching between two distinct optimization problems is proposed, directly addressing temporary failure and the subsequent return of global position estimates. The proposed approach is implemented on-board two Menzi Muck walking excavators and validated during real-world tests conducted in representative operational environments.      
### 64.Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in Virtual Reality  [ :arrow_down: ](https://arxiv.org/pdf/2005.06291.pdf)
>  We present the Levitation Simulator, a system that enables researchers and designers to iteratively develop and prototype levitation interface ideas in Virtual Reality. This includes user tests and formal experiments. We derive a model of the movement of a levitating particle in such an interface. Based on this, we develop an interactive simulation of the levitation interface in VR, which exhibits the dynamical properties of the real interface. The results of a Fitts' Law pointing study show that the Levitation Simulator enables performance, comparable to the real prototype. We developed the first two interactive games, dedicated for levitation interfaces: LeviShooter and BeadBounce, in the Levitation Simulator, and then implemented them on the real interface. Our results indicate that participants experienced similar levels of user engagement when playing the games, in the two environments. We share our Levitation Simulator as Open Source, thereby democratizing levitation research, without the need for a levitation apparatus.      
