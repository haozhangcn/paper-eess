# ArXiv eess --Wed, 9 Mar 2022
### 1.Fast and selective super-resolution ultrasound in vivo with sono-switchable nanodroplets  [ :arrow_down: ](https://arxiv.org/pdf/2203.04263.pdf)
>  Perfusion by the microcirculation is key to the development, maintenance and pathology of tissue. Its measurement with high spatiotemporal resolution is consequently valuable but remains a challenge in deep tissue. Ultrasound Localization Microscopy (ULM) provides very high spatiotemporal resolution but the use of microbubbles requires low contrast agent concentrations, a long acquisition time, and gives little control over the spatial and temporal distribution of the bubbles. The present study is the first to demonstrate Acoustic Wave Sparsely-Activated Localization Microscopy (AWSALM) and fast-AWSALM for in vivo super-resolution ultrasound imaging, offering contrast on demand and vascular selectivity. Three different formulations of sono-switchable contrast agents were tested. We demonstrate their use with ultrasound mechanical indices well within recommended safety limits to enable fast on-demand sparse switching at very high agent concentrations. We produce super-localization maps of the rabbit renal vasculature with acquisition times between 5.5 s and 0.25 s, and an 4-fold improvement in spatial resolution. We present the unique selectivity of AWSALM in visualizing specific vascular branches and downstream microvasculature, and we show super-localized kidney structures in systole and diastole with fast-AWSALM. In conclusion we demonstrate the feasibility of fast and selective measurement of microvascular dynamics in vivo with subwavelength resolution using ultrasound and sono-switchable nanodroplets.      
### 2.Second-life Lithium-ion batteries: A chemistry-agnostic and scalable health estimation algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2203.04249.pdf)
>  Battery state of health is an essential metric for diagnosing battery degradation during testing and operation. While many unique measurements are possible in the design phase, for practical applications often only temperature, voltage and current sensing are accessible. This paper presents a novel combination of machine learning techniques to produce accurate predictions significantly faster than standard Gaussian processes. The data-driven approach uses feature generation with simple mathematics, feature filtering, and bagging, which is validated with publicly available aging datasets of more than 200 cells with slow and fast charging, across different cathode chemistries, and for various operating conditions. Based on multiple training-test partitions, average and median state of health prediction root mean square error (RMSE) is found to be less than 1.48% and 1.27%, respectively, with a limited amount of input data, showing the capability of the approach even when input data and time are limiting factors. The process developed in this paper has direct applicability to today's incumbent open challenge of assessing retired batteries on the basis of their residual health, and therefore nominal remaining useful life, to allow fast classification for second-life reutilization.      
### 3.Extending Life of Lithium-ion Battery Packs by Taming Heterogeneities via an Optimal Control-based Active Balancing Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2203.04226.pdf)
>  This paper develops a multi-objective fast charging-minimum degradation optimal control problem (OCP) for lithium-ion battery modules, made of series-connected cells, subject to heterogeneity induced by manufacturing defects and non uniform operating conditions, realized via an active balancing circuitry. Each cell is expressed via a coupled nonlinear electrochemical, thermal, and aging model and the direct collocation approach is employed to transcribe the OCP into a nonlinear programming problem (NLP). The proposed OCP is formulated under two different schemes of charging operation: (i) same-charging-time (OCP-SCT) and (ii) different-charging-time (OCP-DCT). The former assumes simultaneous charging of all cells irrespective of their initial conditions, whereas the latter allows for different charging times of the cells to account for heterogeneous initial conditions. Simulations on an illustrative case study -- a battery module with two series-connected cells -- are carried out in the presence of intrinsic heterogeneity among the cells in terms of state of charge and state of health. Results show that the OCP-DCT scheme provides more flexibility to deal with heterogeneity, boasting of lower temperature increase, charging current amplitudes, and degradation. Finally, comparison with the common practice of constant current (CC) charging over a long-term cycling operation shows that promising savings, in terms of retained capacity, are attainable under the new control.      
### 4.Multi-agent consensus over time-invariant and time-varying signed digraphs via eventual positivity  [ :arrow_down: ](https://arxiv.org/pdf/2203.04215.pdf)
>  Laplacian dynamics on signed digraphs have a richer behavior than those on nonnegative digraphs. In particular, for the so-called "repelling" signed Laplacians, the marginal stability property (needed to achieve consensus) is not guaranteed a priori and, even when it holds, it does not automatically lead to consensus, as these signed Laplacians may loose rank even in strongly connected digraphs. Furthermore, in the time-varying case, instability can occur even when switching in a family of systems each of which corresponds to a marginally stable signed Laplacian with the correct corank. In this paper we present conditions guaranteeing consensus of these signed Laplacians based on the property of eventual positivity, a Perron-Frobenius type of property for signed matrices. The conditions cover both time-invariant and time-varying cases. A particularly simple sufficient condition valid in both cases is that the Laplacians are normal matrices. Such condition can be relaxed in several ways. For instance in the time-invariant case it is enough that the Laplacian has this Perron-Frobenius property on the right but not on the left side (i.e., on the transpose). For the time-varying case, convergence to consensus can be guaranteed by the existence of a common Lyapunov function for all the signed Laplacians. All conditions can be easily extended to bipartite consensus.      
### 5.Locate This, Not That: Class-Conditioned Sound Event DOA Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2203.04197.pdf)
>  Existing systems for sound event localization and detection (SELD) typically operate by estimating a source location for all classes at every time instant. In this paper, we propose an alternative class-conditioned SELD model for situations where we may not be interested in localizing all classes all of the time. This class-conditioned SELD model takes as input the spatial and spectral features from the sound file, and also a one-hot vector indicating the class we are currently interested in localizing. We inject the conditioning information at several points in our model using feature-wise linear modulation (FiLM) layers. Through experiments on the DCASE 2020 Task 3 dataset, we show that the proposed class-conditioned SELD model performs better in terms of common SELD metrics than the baseline model that locates all classes simultaneously, and also outperforms specialist models that are trained to locate only a single class of interest. We also evaluate performance on the DCASE 2021 Task 3 dataset, which includes directional interference (sound events from classes we are not interested in localizing) and notice especially strong improvement from the class-conditioned model.      
### 6.Curriculum-based Reinforcement Learning for Distribution System Critical Load Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2203.04166.pdf)
>  This paper focuses on the critical load restoration problem in distribution systems following major outages. To provide fast online response and optimal sequential decision-making support, a reinforcement learning (RL) based approach is proposed to optimize the restoration. Due to the complexities stemming from the large policy search space, renewable uncertainty, and nonlinearity in a complex grid control problem, directly applying RL algorithms to train a satisfactory policy requires extensive tuning to be successful. To address this challenge, this paper leverages the curriculum learning (CL) technique to design a training curriculum involving a simpler steppingstone problem that guides the RL agent to learn to solve the original hard problem in a progressive and more efficient manner. We demonstrate that compared with direct learning, CL facilitates controller training to achieve better performance. In the experiments, to study realistic scenarios where renewable forecasts used for decision-making are in general imperfect, the trained RL controllers are compared with two model predictive controllers (MPCs) using renewable forecasts with different error levels and observe how these controllers can hedge against the uncertainty. Results show that RL controllers are less susceptible to forecast errors than the baseline MPCs and can provide a more reliable restoration process.      
### 7.Feedforward PID Control of Full-Car with Parallel Active Link Suspension for Improved Chassis Attitude Stabilization  [ :arrow_down: ](https://arxiv.org/pdf/2203.04162.pdf)
>  PID control is commonly utilized in an active suspension system to achieve desirable chassis attitude, where, due to delays, feedback information has much difficulty regulating the roll and pitch behavior, and stabilizing the chassis attitude, which may result in roll over when the vehicle steers at a large longitudinal velocity. To address the problem of the feedback delays in chassis attitude stabilization, in this paper, a feedforward control strategy is proposed to combine with a previously developed PID control scheme in the recently introduced Parallel Active Link Suspension (PALS). Numerical simulations with a nonlinear multi-body vehicle model are performed, where a set of ISO driving maneuvers are tested. Results demonstrate the feedforward-based control scheme has improved suspension performance as compared to the conventional PID control, with faster speed of convergence in brake in a turn and step steer maneuvers, and surviving the fishhook maneuver (although displaying two-wheel lift-off) with 50 mph maneuver entrance speed at which conventional PID control rolls over.      
### 8.Mu-synthesis PID Control of Full-Car with Parallel Active Link Suspension Under Variable Payload  [ :arrow_down: ](https://arxiv.org/pdf/2203.04147.pdf)
>  This paper presents a combined mu-synthesis PID control scheme, employing a frequency separation paradigm, for a recently proposed novel active suspension, the Parallel Active Link Suspension (PALS). The developed mu-synthesis control scheme is superior to the conventional H-infinity control, previously designed for the PALS, in terms of ride comfort and road holding (higher frequency dynamics), with important realistic uncertainties, such as in vehicle payload, taken into account. The developed PID control method is applied to guarantee good chassis attitude control capabilities and minimization of pitch and roll motions (low frequency dynamics). A multi-objective control method, which merges the aforementioned PID and mu-synthesis-based controls is further introduced to achieve simultaneously the low frequency mitigation of attitude motions and the high frequency vibration suppression of the vehicle. A seven-degree-of-freedom Sport Utility Vehicle (SUV) full car model with PALS, is employed in this work to test the synthesized controller by nonlinear simulations with different ISO-defined road events and variable vehicle payload. The results demonstrate the control scheme's significant robustness and performance, as compared to the conventional passive suspension as well as the actively controlled PALS by conventional H-infinity control, achieved for a wide range of vehicle payload considered in the investigation.      
### 9.An Efficient Polyp Segmentation Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.04118.pdf)
>  Cancer is a disease that occurs as a result of uncontrolled division and proliferation of cells. The number of cancer cases has been on the rise over the recent years.. Colon cancer is one of the most common types of cancer in the world. Polyps that can be seen in the large intestine can cause cancer if not removed with early intervention. Deep learning and image segmentation techniques are used to minimize the number of polyps that goes unnoticed by the experts during the diagnosis. Although these techniques give good results, they require too many parameters. We propose a new model to solve this problem. Our proposed model includes less parameters as well as outperforming the success of the state of the art models. In the proposed model, a partial decoder is used to reduce the number of parameters while maintaning success. EfficientNetB0, which gives successfull results as well as requiring few parameters, is used in the encoder part. Since polyps have variable aspect and aspect ratios, an asymetric convolution block was used instead of using classic convolution block. Kvasir and CVC-ClinicDB datasets were seperated as training, validation and testing, and CVC-ColonDB, ETIS and Endoscene datasets were used for testing. According to the dice metric, our model had the best results with %71.8 in the ColonDB test dataset, %89.3 in the EndoScene test dataset and %74.8 in the ETIS test dataset. Our model requires a total of 2.626.337 parameters. When we compare it in the literature, according to similar studies, the model that requires the least parameters is U-Net++ with 9.042.177 parameters.      
### 10.Comparing representations of biological data learned with different AI paradigms, augmenting and cropping strategies  [ :arrow_down: ](https://arxiv.org/pdf/2203.04107.pdf)
>  Recent advances in computer vision and robotics enabled automated large-scale biological image analysis. Various machine learning approaches have been successfully applied to phenotypic profiling. However, it remains unclear how they compare in terms of biological feature extraction. In this study, we propose a simple CNN architecture and implement 4 different representation learning approaches. We train 16 deep learning setups on the 770k cancer cell images dataset under identical conditions, using different augmenting and cropping strategies. We compare the learned representations by evaluating multiple metrics for each of three downstream tasks: i) distance-based similarity analysis of known drugs, ii) classification of drugs versus controls, iii) clustering within cell lines. We also compare training times and memory usage. Among all tested setups, multi-crops and random augmentations generally improved performance across tasks, as expected. Strikingly, self-supervised (implicit contrastive learning) models showed competitive performance being up to 11 times faster to train. Self-supervised regularized learning required the most of memory and computation to deliver arguably the most informative features. We observe that no single combination of augmenting and cropping strategies consistently results in top performance across tasks and recommend prospective research directions.      
### 11.Exploration of Various Deep Learning Models for Increased Accuracy in Automatic Polyp Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.04093.pdf)
>  This paper is created to explore deep learning models and algorithms that results in highest accuracy in detecting polyp on colonoscopy images. Previous studies implemented deep learning using convolution neural network (CNN) algorithm in detecting polyp and non-polyp. Other studies used dropout, and data augmentation algorithm but mostly not checking the overfitting, thus, include more than four-layer modelss. Rulei Yu <a class="link-external link-http" href="http://et.al" rel="external noopener nofollow">this http URL</a> from the Institute of Software, Chinese Academy of Sciences said that transfer learning is better talking about performance or improving the previous used algorithm. Most especially in applying the transfer learning in feature extraction. Series of experiments were conducted with only a minimum of 4 CNN layers applying previous used models and identified the model that produce the highest percentage accuracy of 98% among the other models that apply transfer learning. Further studies could use different optimizer to a different CNN modelsto increase accuracy.      
### 12.Learning to Erase the Bayer-Filter to See in the Dark  [ :arrow_down: ](https://arxiv.org/pdf/2203.04042.pdf)
>  Low-light image enhancement - a pervasive but challenging problem, plays a central role in enhancing the visibility of an image captured in a poor illumination environment. Due to the fact that not all photons can pass the Bayer-Filter on the sensor of the color camera, in this work, we first present a De-Bayer-Filter simulator based on deep neural networks to generate a monochrome raw image from the colored raw image. Next, a fully convolutional network is proposed to achieve the low-light image enhancement by fusing colored raw data with synthesized monochrome raw data. Channel-wise attention is also introduced to the fusion process to establish a complementary interaction between features from colored and monochrome raw images. To train the convolutional networks, we propose a dataset with monochrome and color raw pairs named Mono-Colored Raw paired dataset (MCR) collected by using a monochrome camera without Bayer-Filter and a color camera with Bayer-Filter. The proposed pipeline take advantages of the fusion of the virtual monochrome and the color raw images and our extensive experiments indicate that significant improvement can be achieved by leveraging raw sensor data and data-driven learning.      
### 13.Mutual Contrastive Learning to Disentangle Whole Slide Image Representations for Glioma Grading  [ :arrow_down: ](https://arxiv.org/pdf/2203.04013.pdf)
>  Whole slide images (WSI) provide valuable phenotypic information for histological assessment and malignancy grading of tumors. The WSI-based computational pathology promises to provide rapid diagnostic support and facilitate digital health. The most commonly used WSI are derived from formalin-fixed paraffin-embedded (FFPE) and frozen sections. Currently, the majority of automatic tumor grading models are developed based on FFPE sections, which could be affected by the artifacts introduced by tissue processing. Here we propose a mutual contrastive learning scheme to integrate FFPE and frozen sections and disentangle cross-modality representations for glioma grading. We first design a mutual learning scheme to jointly optimize the model training based on FFPE and frozen sections. Further, we develop a multi-modality domain alignment mechanism to ensure semantic consistency in the backbone model training. We finally design a sphere normalized temperature-scaled cross-entropy loss (NT-Xent), which could promote cross-modality representation disentangling of FFPE and frozen sections. Our experiments show that the proposed scheme achieves better performance than the model trained based on each single modality or mixed modalities. The sphere NT-Xent loss outperforms other typical metrics loss functions.      
### 14.Human Biometric Signals Monitoring based on WiFi Channel State Information using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.03980.pdf)
>  In this paper, we first present a single-input, multiple-output convolutional neural network that can estimate both heart rate and respiration rate simultaneously by exploiting the underlying link between heart rate and respiration rate. The inputs to the neural network are the amplitude and phase of channel state information collected by a pair of WiFi devices. Our WiFi-based technique addresses privacy concerns and is adapt- able to a variety of settings. This system overall accuracy for the heart and respiration rate estimation can reach 99.109% and 98.581%, respectively. Furthermore, we developed and analyzed two deep learning-based neural network classification algorithms for categorizing four types of sleep stages: wake, rapid eye movement (REM) sleep, non-rapid eye movement (NREM) light sleep, and NREM deep sleep. This system overall classification accuracy can reach 95.925%      
### 15.Online Dynamic Parameter Estimation of an Alkaline Electrolysis System Based on Bayesian Inference  [ :arrow_down: ](https://arxiv.org/pdf/2203.03883.pdf)
>  When directly coupled with fluctuating energy sources such as wind and photovoltage power, the alkaline electrolysis (AEL) in a power-to-hydrogen (P2H) system is required to operate flexibly by dynamically adjusting its hydrogen production rate. The flex-ibility characteristics, e.g., loading range and ramping rate, of an AEL system are significantly influenced by some parameters re-lated to the dynamic processes of the AEL system. These parame-ters are usually difficult to measure directly and may even change with time. To accurately evaluate the flexibility of an AEL system in online operation, this paper presents a Bayesian Inference-based Markov Chain Monte Carlo (MCMC) method to estimate these parameters. Meanwhile, posterior joint probability distribu-tions of the estimated parameters are obtained as a byproduct, which provides valuable physical insight into the AEL systems. Experiments on a 25 kW electrolyzer validate the proposed pa-rameter estimation method.      
### 16.A Cauchy-type Hamilton-Jacobi Successive Approximation Scheme For Reachable Sets Computation  [ :arrow_down: ](https://arxiv.org/pdf/2203.03865.pdf)
>  Motivated by the scalability limitations of Eulerian methods for variational Hamilton-Jacobi-Isaacs (HJI) formulations that provide a least restrictive controller in problems that involve state or input constraints under a worst-possible disturbance, we introduce a second-order, successive sweep algorithm for computing the zero sublevel sets of a popular reachability value functional. Under sufficient HJI partial differential equation regularity and continuity assumption throughout the state space, we show that with state feedback control under the worst-possible disturbance, we can compute the state set that are reachable within a prescribed verification time bound.      
### 17.Amplitude-Constrained Constellation and Reflection Pattern Designs for Directional Backscatter Communications Using Programmable Metasurface  [ :arrow_down: ](https://arxiv.org/pdf/2203.03863.pdf)
>  The large-scale reflector array of programmable metasurfaces is capable of increasing the power efficiency of backscatter communications via passive beamforming and thus has the potential to revolutionize the low-data-rate nature of backscatter communications. In this paper, we propose to design the power-efficient higher-order constellation and reflection pattern under the amplitude constraint brought by backscatter communications. For constellation design, we adopt the amplitude and phase-shift keying (APSK) constellation and optimize the parameters of APSK such as ring number, ring radius, and inter-ring phase difference. Specifically, we derive closed-form solutions to the optimal ring radius and inter-ring phase difference for an arbitrary modulation order. For reflection pattern design, we propose to optimize the passive beamforming vector by solving a multi-objective optimization problem that maximizes reflection power and guarantees beam homogenization within the interested angle range. To solve the problem, we propose a constant-modulus power iteration method, which is proven to be monotonically increasing, to maximize the objective function in each iteration. Numerical results show that the proposed APSK constellation design and reflection pattern design outperform the existing modulation and beam pattern design in programmable metasurface-enabled backscatter communications.      
### 18.Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.03844.pdf)
>  Light-weight super-resolution (SR) models have received considerable attention for their serviceability in mobile devices. Many efforts employ network quantization to compress SR models. However, these methods suffer from severe performance degradation when quantizing the SR models to ultra-low precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In this paper, we identify that the performance drop comes from the contradiction between the layer-wise symmetric quantizer and the highly asymmetric activation distribution in SR models. This discrepancy leads to either a waste on the quantization levels or detail loss in reconstructed images. Therefore, we propose a novel activation quantizer, referred to as Dynamic Dual Trainable Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically, DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower bounds to tackle the highly asymmetric activations. 2) A dynamic gate controller to adaptively adjust the upper and lower bounds at runtime to overcome the drastically varying activation ranges over different <a class="link-external link-http" href="http://samples.To" rel="external noopener nofollow">this http URL</a> reduce the extra overhead, the dynamic gate controller is quantized to 2-bit and applied to only part of the SR networks according to the introduced dynamic intensity. Extensive experiments demonstrate that our DDTB exhibits significant performance improvements in ultra-low precision. For example, our DDTB achieves a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and scaling up output images to x4. Code is at \url{<a class="link-external link-https" href="https://github.com/zysxmu/DDTB" rel="external noopener nofollow">this https URL</a>}.      
### 19.Slice-Connection Clustering Algorithm for Tree Roots Recognition in Noisy 3D GPR Data  [ :arrow_down: ](https://arxiv.org/pdf/2203.03830.pdf)
>  3D mapping of tree roots is a popular ground-penetrating radar (GPR) application. In real field tests, the recognition of tree roots suffers due to noisey reflection patterns from subsurface targets that are not of interest, such as rocks, cavities, soil unevenness, etc. A Slice-Connection Clustering Algorithm (SCC) is applied to separate the regions of interest from each other in a reconstructed 3D image. The proposed method can successfully recognize the radar signatures of the roots and distinguish roots from other objects. Meanwhile, most noise radar features are ignored through our method. The final 3D mapping of the radargram obtained by the method can be used to estimate the location and extension trend of the tree roots. The effectiveness of the proposed system is tested on real GPR data.      
### 20.Generating 3D Bio-Printable Patches Using Wound Segmentation and Reconstruction to Treat Diabetic Foot Ulcers  [ :arrow_down: ](https://arxiv.org/pdf/2203.03814.pdf)
>  We introduce AiD Regen, a novel system that generates 3D wound models combining 2D semantic segmentation with 3D reconstruction so that they can be printed via 3D bio-printers during the surgery to treat diabetic foot ulcers (DFUs). AiD Regen seamlessly binds the full pipeline, which includes RGB-D image capturing, semantic segmentation, boundary-guided point-cloud processing, 3D model reconstruction, and 3D printable G-code generation, into a single system that can be used out of the box. We developed a multi-stage data preprocessing method to handle small and unbalanced DFU image datasets. AiD Regen's human-in-the-loop machine learning interface enables clinicians to not only create 3D regenerative patches with just a few touch interactions but also customize and confirm wound boundaries. As evidenced by our experiments, our model outperforms prior wound segmentation models and our reconstruction algorithm is capable of generating 3D wound models with compelling accuracy. We further conducted a case study on a real DFU patient and demonstrated the effectiveness of AiD Regen in treating DFU wounds.      
### 21.Dense Urban Outdoor-Indoor Coverage from 3.5 to 28 GHz  [ :arrow_down: ](https://arxiv.org/pdf/2203.03813.pdf)
>  In the US, people spend 87% of their time indoors and have an average of four connected devices per person (in 2020). As such, providing indoor coverage has always been a challenge but becomes even more difficult as carrier frequencies increase to mmWave and beyond. This paper investigates the outdoor and outdoor-indoor coverage of an urban network comparing globally standardized building penetration models and implementing models to corresponding scenarios. The glass used in windows of buildings in the grid plays a pivotal role in determining the outdoor-to-indoor propagation loss. For 28 GHz with 1 W/polarization transmit power in the urban street grid, the downlink data rates for 90% of outdoor users are estimated at over 250 Mbps. In contrast, 15% of indoor users are estimated to be in outage, with SNR $&lt;-$3 dB when base stations are 400 m apart with one-fifth of the buildings imposing high penetration loss ($\sim$ 35 dB). At 3.5 GHz, base stations may achieve over 250 Mbps for 90% indoor users if 400 MHz bandwidth with 100 W/polarization transmit power is available. The methods and models presented can be used to facilitate decisions regarding the density and transmit power required to provide high data rates to majority users in urban centers.      
### 22.Discrete Robust Control of Robot Manipulators using an Uncertainty and Disturbance Estimator  [ :arrow_down: ](https://arxiv.org/pdf/2203.03805.pdf)
>  This article presents the design of a robust observer based on the discrete-time formulation of Uncertainty and Disturbance Estimator (UDE), a well-known robust control technique, for the purpose of controlling robot manipulators. The design results in a complete closed-loop, robust, controller--observer structure. The observer incorporates the estimate of the overall uncertainty associated with the plant, in order to mimic its dynamics, and the control law is generated using an auxiliary error instead of state tracking error. A detailed qualitative and quantitative stability analysis is provided, and simulations are performed on the two-link robot manipulator system. Further, a comparative study with well-known control strategies for robot manipulators is presented. The results demonstrate the efficacy of the proposed technique, with better tracking performance and lower control energy compared to other strategies.      
### 23.Sub-Terahertz Channel Measurements and Characterization in a Factory Building  [ :arrow_down: ](https://arxiv.org/pdf/2203.03799.pdf)
>  Sub-Terahertz (THz) frequencies between 100 GHz and 300 GHz are being considered as a key enabler for the sixth-generation (6G) wireless communications due to the vast amounts of unused spectrum. The 3rd Generation Partnership Project (3GPP) included the indoor industrial environments as a scenario of interest since Release 15. This paper presents recent sub-THz channel measurements using directional horn antennas of 27 dBi gain at 142 GHz in a factory building, which hosts equipment manufacturing startups. Directional measurements with co-polarized and cross-polarized antenna configurations were conducted over distances from 6 to 40 meters. Omnidirectional and directional path loss with two antenna polarization configurations produce the gross cross-polarization discrimination (XPD) with a mean of 27.7 dB, which suggests that dual-polarized antenna arrays can provide good multiplexing gain for sub-THz wireless systems. The measured power delay profile and power angular spectrum show the maximum root mean square (RMS) delay spread of 66.0 nanoseconds and the maximum RMS angular spread of 103.7 degrees using a 30 dB threshold, indicating the factory scenario is a rich-scattering environment due to a massive number of metal structures and objects. This work will facilitate emerging sub-THz applications such as super-resolution sensing and positioning for future smart factories.      
### 24.Effect of Channel Geometry and Flow Rates in Hydrodynamic Focusing on Impedance Detection of Circulating Tumor Cells  [ :arrow_down: ](https://arxiv.org/pdf/2203.03787.pdf)
>  Cells, other than their biological properties, have different electric and physical properties. In an impedance cytometer, cells should pass one by one in the detection region where pairs of electrodes are located. When cells are located between electrodes, the impedance changes, and this can be indicative of the presence of a cell. This is basically because the electric properties of cells are different from the medium between the electrodes which is important in determining the impedance. One of the most important aspects which influence the performance of an impedance cytometer performance is the microchannel design. In this work, in the first step, the microchannel was designed in a way to have the best detection in the impedance cytometer. In this regard, hydrodynamic focusing was selected to focus the population of cells entering from the inlet of the main channel. To find the optimal parameters of the microchannel, different geometry for the channel itself, along with flow rates and other parameters related to sheath flow were simulated. In the next step, impedance was measured in COMSOL for White blood cells, MCF7, and MDA-MB-231 breast cancer cells. The results show that by measuring the impedance of cells using the optimized channel design, CTCs can be successfully differentiated from WBCs.      
### 25.Exploring Physical-Based Constraints in Short-Term Load Forecasting: A Defense Mechanism Against Cyberattack  [ :arrow_down: ](https://arxiv.org/pdf/2203.03774.pdf)
>  Short-term load forecasting is an essential task that supports utilities to schedule generating sufficient power for balancing supply and demand, and can become an attractive target for cyber attacks. It has been shown that the power system state estimation is vulnerable to false data injection attacks. Similarly, false data injection on input variables can result in large forecast errors. The load forecasting system should have a protective mechanism to mitigate such attacks. One approach is to model physical system constraints that would identify anomalies. This study investigates possible constraints associated with a load forecasting application. Looking at regional forecasted loads, we analyze the relation between each zone through similarity measures used in time series in order to identify constraints. Comprehensive results for historical ERCOT load data indicate variation in the measures recognizing the existence of malicious action. Still, these static measures can not be considered an efficient index across different scenarios.      
### 26.Battery Cloud with Advanced Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2203.03737.pdf)
>  A Battery Cloud or cloud battery management system leverages the cloud computational power and data storage to improve battery safety, performance, and economy. This work will present the Battery Cloud that collects measured battery data from electric vehicles and energy storage systems. Advanced algorithms are applied to improve battery performance. Using remote vehicle data, we train and validate an artificial neural network to estimate pack SOC during vehicle charging. The strategy is then tested on vehicles. Furthermore, high accuracy and onboard battery state of health estimation methods for electric vehicles are developed based on the differential voltage (DVA) and incremental capacity analysis (ICA). Using cycling data from battery cells at various temperatures, we extract the charging cycles and calculate the DVA and ICA curves, from which multiple features are extracted, analyzed, and eventually used to estimate the state of health. For battery safety, a data-driven thermal anomaly detection method is developed. The method can detect unforeseen anomalies such as thermal runaways at the very early stage. With the further development of the internet of things, more and more battery data will be available. Potential applications of battery cloud also include areas such as battery manufacture, recycling, and electric vehicle battery swap.      
### 27.The Braess Paradox in Dynamic Traffic  [ :arrow_down: ](https://arxiv.org/pdf/2203.03726.pdf)
>  The Braess's Paradox (BP) is the observation that adding one or more roads to the existing road network will counter-intuitively increase traffic congestion and slow down the overall traffic flow. Previously, the existence of the BP is modeled using the static traffic assignment model, which solves for the user equilibrium subject to network flow conservation to find the equilibrium state and distributes all vehicles instantaneously. Such approach neglects the dynamic nature of real-world traffic, including vehicle behaviors and the interaction between vehicles and the infrastructure. As such, this article proposes a dynamic traffic network model and empirically validates the existence of the BP under dynamic traffic. In particular, we use microsimulation environment to study the impacts of an added path on a grid network. We explore how the network flow, vehicle travel time, and network capacity respond, as well as when the BP will occur.      
### 28.On a Continuous-Time Version of Willems' Lemma  [ :arrow_down: ](https://arxiv.org/pdf/2203.03702.pdf)
>  In this paper, a method to represent every input-output trajectory of a continuous-time linear system in terms of previously collected data is presented. This corresponds to a continuous-time version of the well-known Willems' lemma. The result is obtained by sampling the continuous signals at regular intervals, and constructing Hankel-like structures that closely resemble their discrete-time counterparts. Then, it is shown how to use measured persistently excited data to design a time-varying vector of parameters that allows the generation of arbitrary piecewise differentiable trajectories. A class of input signals that satisfies the conditions for persistence of excitation is also provided.      
### 29.Robust Design of Rate-Splitting Multiple Access With Imperfect CSI for Cell-Free MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.03690.pdf)
>  Rate-Splitting Multiple Access (RSMA) for multi-user downlink operates by splitting the message for each user equipment (UE) into a private message and a set of common messages, which are simultaneously transmitted by means of superposition coding. The RSMA scheme can enhance throughput and connectivity as compared to conventional multiple access techniques by optimizing the rate-splitting ratios along with the corresponding downlink beamforming vectors. This work examines the impact of erroneous channel state information (CSI) on the performance of RSMA in cell-free multiple-input multiple-output (MIMO) systems. An efficient robust optimization algorithm is proposed by using closed-form lower bound expressions on the expected data rates. Extensive numerical results show the importance of robust design in the presence of CSI errors and how the performance gain of RSMA over conventional schemes is affected by CSI imperfection.      
### 30.Distributed Consensus of Stochastic Multi-agent Systems with Prescribed Performance Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2203.03665.pdf)
>  This paper focuses on the problem of distributed consensus control of multi-agent systems while considering two main practical concerns (i) stochastic noise in the agent dynamics and (ii) predefined performance constraints over evolutions of multi-agent systems. In particular, we consider that each agent is driven by a stochastic differential equation with state-dependent noise which makes the considered problem more challenging compared to non-stochastic agents. The work provides sufficient conditions under which the proposed timevarying distributed control laws ensure consensus in expectation and almost sure consensus of stochastic multi-agent systems while satisfying prescribed performance constraints over evolutions of the systems in the sense of the qth moment. Finally, we demonstrate the effectiveness of the proposed results with a numerical example.      
### 31.Conquering Data Variations in Resolution: A Slice-Aware Multi-Branch Decoder Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.03640.pdf)
>  Fully convolutional neural networks have made promising progress in joint liver and liver tumor segmentation. Instead of following the debates over 2D versus 3D networks (for example, pursuing the balance between large-scale 2D pretraining and 3D context), in this paper, we novelly identify the wide variation in the ratio between intra- and inter-slice resolutions as a crucial obstacle to the performance. To tackle the mismatch between the intra- and inter-slice information, we propose a slice-aware 2.5D network that emphasizes extracting discriminative features utilizing not only in-plane semantics but also out-of-plane coherence for each separate slice. Specifically, we present a slice-wise multi-input multi-output architecture to instantiate such a design paradigm, which contains a Multi-Branch Decoder (MD) with a Slice-centric Attention Block (SAB) for learning slice-specific features and a Densely Connected Dice (DCD) loss to regularize the inter-slice predictions to be coherent and continuous. Based on the aforementioned innovations, we achieve state-of-the-art results on the MICCAI 2017 Liver Tumor Segmentation (LiTS) dataset. Besides, we also test our model on the ISBI 2019 Segmentation of THoracic Organs at Risk (SegTHOR) dataset, and the result proves the robustness and generalizability of the proposed method in other segmentation tasks.      
### 32.Unsupervised Image Registration Towards Enhancing Performance and Explainability in Cardiac And Brain Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2203.03638.pdf)
>  Magnetic Resonance Imaging (MRI) typically recruits multiple sequences (defined here as "modalities"). As each modality is designed to offer different anatomical and functional clinical information, there are evident disparities in the imaging content across modalities. Inter- and intra-modality affine and non-rigid image registration is an essential medical image analysis process in clinical imaging, as for example before imaging biomarkers need to be derived and clinically evaluated across different MRI modalities, time phases and slices. Although commonly needed in real clinical scenarios, affine and non-rigid image registration is not extensively investigated using a single unsupervised model architecture. In our work, we present an un-supervised deep learning registration methodology which can accurately model affine and non-rigid trans-formations, simultaneously. Moreover, inverse-consistency is a fundamental inter-modality registration property that is not considered in deep learning registration algorithms. To address inverse-consistency, our methodology performs bi-directional cross-modality image synthesis to learn modality-invariant latent rep-resentations, while involves two factorised transformation networks and an inverse-consistency loss to learn topology-preserving anatomical transformations. Overall, our model (named "FIRE") shows improved performances against the reference standard baseline method on multi-modality brain 2D and 3D MRI and intra-modality cardiac 4D MRI data experiments.      
### 33.Clustering and classification of low-dimensional data in explicit feature map domain: intraoperative pixel-wise diagnosis of adenocarcinoma of a colon in a liver  [ :arrow_down: ](https://arxiv.org/pdf/2203.03636.pdf)
>  Application of artificial intelligence in medicine brings in highly accurate predictions achieved by complex models, the reasoning of which is hard to interpret. Their generalization ability can be reduced because of the lack of pixel wise annotated images that occurs in frozen section tissue analysis. To partially overcome this gap, this paper explores the approximate explicit feature map (aEFM) transform of low-dimensional data into a low-dimensional subspace in Hilbert space. There, with a modest increase in computational complexity, linear algorithms yield improved performance and keep interpretability. They remain amenable to incremental learning that is not a trivial issue for some nonlinear algorithms. We demonstrate proposed methodology on a very large-scale problem related to intraoperative pixel-wise semantic segmentation and clustering of adenocarcinoma of a colon in a liver. Compared to the results in the input space, logistic classifier achieved statistically significant performance improvements in micro balanced accuracy and F1 score in the amounts of 12.04% and 12.58%, respectively. Support vector machine classifier yielded the increase of 8.04% and 9.41%. For clustering, increases of 0.79% and 0.85% are obtained with ultra large-scale spectral clustering algorithm. Results are supported by a discussion of interpretability using Shapely additive explanation values for predictions of linear classifier in input space and aEFM induced space.      
### 34.Stepwise Feature Fusion: Local Guides Global  [ :arrow_down: ](https://arxiv.org/pdf/2203.03635.pdf)
>  Colonoscopy, currently the most efficient and recognized colon polyp detection technology, is necessary for early screening and prevention of colorectal cancer. However, due to the varying size and complex morphological features of colonic polyps as well as the indistinct boundary between polyps and mucosa, accurate segmentation of polyps is still challenging. Deep learning has become popular for accurate polyp segmentation tasks with excellent results. However, due to the structure of polyps image and the varying shapes of polyps, it easy for existing deep learning models to overfitting the current dataset. As a result, the model may not process unseen colonoscopy data. To address this, we propose a new State-Of-The-Art model for medical image segmentation, the SSFormer, which uses a pyramid Transformer encoder to improve the generalization ability of models. Specifically, our proposed Progressive Locality Decoder can be adapted to the pyramid Transformer backbone to emphasize local features and restrict attention dispersion. The SSFormer achieves statet-of-the-art performance in both learning and generalization assessment.      
### 35.InsightNet: non-contact blood pressure measuring network based on face video  [ :arrow_down: ](https://arxiv.org/pdf/2203.03634.pdf)
>  Blood pressure indicates cardiac function and peripheral vascular resistance and is critical for disease diagnosis. Traditionally, blood pressure data are mainly acquired through contact sensors, which require high maintenance and may be inconvenient and unfriendly to some people (e.g., burn patients). In this paper, an efficient non-contact blood pressure measurement network based on face videos is proposed for the first time. An innovative oversampling training strategy is proposed to handle the unbalanced data distribution. The input video sequences are first normalized and converted to our proposed YUVT color space. Then, the Spatio-temporal slicer encodes it into a multi-domain Spatio-temporal mapping. Finally, the neural network computation module, used for high-dimensional feature extraction of the multi-domain spatial feature mapping, after which the extracted high-dimensional features are used to enhance the time-domain feature association using LSTM, is computed by the blood pressure classifier to obtain the blood pressure measurement intervals. Combining the output of feature extraction and the result after classification, the blood pressure calculator, calculates the blood pressure measurement values. The solution uses a blood pressure classifier to calculate blood pressure intervals, which can help the neural network distinguish between the high-dimensional features of different blood pressure intervals and alleviate the overfitting phenomenon. It can also locate the blood pressure intervals, correct the final blood pressure values and improve the network performance. Experimental results on two datasets show that the network outperforms existing state-of-the-art methods.      
### 36.Student Become Decathlon Master in Retinal Vessel Segmentation via Dual-teacher Multi-target Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2203.03631.pdf)
>  Unsupervised domain adaptation has been proposed recently to tackle the so-called domain shift between training data and test data with different distributions. However, most of them only focus on single-target domain adaptation and cannot be applied to the scenario with multiple target domains. In this paper, we propose RVms, a novel unsupervised multi-target domain adaptation approach to segment retinal vessels (RVs) from multimodal and multicenter retinal images. RVms mainly consists of a style augmentation and transfer (SAT) module and a dual-teacher knowledge distillation (DTKD) module. SAT augments and clusters images into source-similar domains and source-dissimilar domains via Bézier and Fourier transformations. DTKD utilizes the augmented and transformed data to train two teachers, one for source-similar domains and the other for source-dissimilar domains. Afterwards, knowledge distillation is performed to iteratively distill different domain knowledge from teachers to a generic student. The local relative intensity transformation is employed to characterize RVs in a domain invariant manner and promote the generalizability of teachers and student models. Moreover, we construct a new multimodal and multicenter vascular segmentation dataset from existing publicly-available datasets, which can be used to benchmark various domain adaptation and domain generalization methods. Through extensive experiments, RVms is found to be very close to the target-trained Oracle in terms of segmenting the RVs, largely outperforming other state-of-the-art methods.      
### 37.Multi-channel deep convolutional neural networks for multi-classifying thyroid disease  [ :arrow_down: ](https://arxiv.org/pdf/2203.03627.pdf)
>  Thyroid disease instances have been continuously increasing since the 1990s, and thyroid cancer has become the most rapidly rising disease among all the malignancies in recent years. Most existing studies focused on applying deep convolutional neural networks for detecting thyroid cancer. Despite their satisfactory performance on binary classification tasks, limited studies have explored multi-class classification of thyroid disease types; much less is known of the diagnosis of co-existence situation for different types of thyroid diseases. Therefore, this study proposed a novel multi-channel convolutional neural network (CNN) architecture to address the multi-class classification task of thyroid disease. The multi-channel CNN merits from computed tomography to drive a comprehensive diagnostic decision for the overall thyroid gland, emphasizing the disease co-existence circumstance. Moreover, this study also examined alternative strategies to enhance the diagnostic accuracy of CNN models through concatenation of different scales of feature maps. Benchmarking experiments demonstrate the improved performance of the proposed multi-channel CNN architecture compared with the standard single-channel CNN architecture. More specifically, the multi-channel CNN achieved an accuracy of 0.909, precision of 0.944, recall of 0.896, specificity of 0.994, and F1 of 0.917, in contrast to the single-channel CNN, which obtained 0.902, 0.892, 0.909, 0.993, 0.898, respectively. In addition, the proposed model was evaluated in different gender groups; it reached a diagnostic accuracy of 0.908 for the female group and 0.901 for the male group. Collectively, the results highlight that the proposed multi-channel CNN has excellent generalization and has the potential to be deployed to provide computational decision support in clinical settings.      
### 38.Coordinate Translator for Learning Deformable Medical Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2203.03626.pdf)
>  The majority of deep learning (DL) based deformable image registration methods use convolutional neural networks (CNNs) to estimate displacement fields from pairs of moving and fixed images. This, however, requires the convolutional kernels in the CNN to not only extract intensity features from the inputs but also understand image coordinate systems. We argue that the latter task is challenging for traditional CNNs, limiting their performance in registration tasks. To tackle this problem, we first introduce Coordinate Translator (CoTr), a differentiable module that identifies matched features between the fixed and moving image and outputs their coordinate correspondences without the need for training. It unloads the burden of understanding image coordinate systems for CNNs, allowing them to focus on feature extraction. We then propose a novel deformable registration network, im2grid, that uses multiple CoTr's with the hierarchical features extracted from a CNN encoder and outputs a deformation field in a coarse-to-fine fashion. We compared im2grid with the state-of-the-art DL and non-DL methods for unsupervised 3D magnetic resonance image registration. Our experiments show that im2grid outperforms these methods both qualitatively and quantitatively.      
### 39.Fusion-Correction Network for Single-Exposure Correction and Multi-Exposure Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2203.03624.pdf)
>  The photographs captured by digital cameras usually suffer from over-exposure or under-exposure problems. The Single-Exposure Correction (SEC) and Multi-Exposure Fusion (MEF) are two widely studied image processing tasks for image exposure enhancement. However, current SEC and MEF methods ignore the internal correlation between SEC and MEF, and are proposed under distinct frameworks. What's more, most MEF methods usually fail at processing a sequence containing only under-exposed or over-exposed images. To alleviate these problems, in this paper, we develop an integrated framework to simultaneously tackle the SEC and MEF tasks. Built upon the Laplacian Pyramid (LP) decomposition, we propose a novel Fusion-Correction Network (FCNet) to fuse and correct an image sequence sequentially in a multi-level scheme. In each LP level, the image sequence is feed into a Fusion block and a Correction block for consecutive image fusion and exposure correction. The corrected image is upsampled and re-composed with the high-frequency detail components in next-level, producing the base sequence for the next-level blocks. Experiments on the benchmark dataset demonstrate that our FCNet is effective on both the SEC and MEF tasks.      
### 40.Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2203.03623.pdf)
>  We propose a novel and unified method, measurement-conditioned denoising diffusion probabilistic model (MC-DDPM), for under-sampled medical image reconstruction based on DDPM. Different from previous works, MC-DDPM is defined in measurement domain (e.g. k-space in MRI reconstruction) and conditioned on under-sampling mask. We apply this method to accelerate MRI reconstruction and the experimental results show excellent performance, outperforming full supervision baseline and the state-of-the-art score-based reconstruction method. Due to its generative nature, MC-DDPM can also quantify the uncertainty of reconstruction. Our code is available on github.      
### 41.Deep-ASPECTS: A Segmentation-Assisted Model for Stroke Severity Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2203.03622.pdf)
>  A stroke occurs when an artery in the brain ruptures and bleeds or when the blood supply to the brain is cut off. Blood and oxygen cannot reach the brain's tissues due to the rupture or obstruction resulting in tissue death. The Middle cerebral artery (MCA) is the largest cerebral artery and the most commonly damaged vessel in stroke. The quick onset of a focused neurological deficit caused by interruption of blood flow in the territory supplied by the MCA is known as an MCA stroke. Alberta stroke programme early CT score (ASPECTS) is used to estimate the extent of early ischemic changes in patients with MCA stroke. This study proposes a deep learning-based method to score the CT scan for ASPECTS. Our work has three highlights. First, we propose a novel method for medical image segmentation for stroke detection. Second, we show the effectiveness of AI solution for fully-automated ASPECT scoring with reduced diagnosis time for a given non-contrast CT (NCCT) Scan. Our algorithms show a dice similarity coefficient of 0.64 for the MCA anatomy segmentation and 0.72 for the infarcts segmentation. Lastly, we show that our model's performance is inline with inter-reader variability between radiologists.      
### 42.Triple Motion Estimation and Frame Interpolation based on Adaptive Threshold for Frame Rate Up-Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2203.03621.pdf)
>  In this paper, we propose a novel motion-compensated frame rate up-conversion (MC-FRUC) algorithm. The proposed algorithm creates interpolated frames by first estimating motion vectors using unilateral (jointing forward and backward) and bilateral motion estimation. Then motion vectors are combined based on adaptive threshold, in order to creates high-quality interpolated frames and reduce block artifacts. Since motion-compensated frame interpolation along unilateral motion trajectories yields holes, a new algorithm is introduced to resolve this problem. The experimental results show that the quality of the interpolated frames using the proposed algorithm is much higher than the existing algorithms.      
### 43.Adaptive Cross-Layer Attention for Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2203.03619.pdf)
>  Non-local attention module has been proven to be crucial for image restoration. Conventional non-local attention processes features of each layer separately, so it risks missing correlation between features among different layers. To address this problem, we propose Cross-Layer Attention (CLA) module in this paper. Instead of finding correlated key pixels within the same layer, each query pixel can attend to key pixels at previous layers of the network. In order to further enhance the learning capability and reduce the inference cost of CLA, we further propose Adaptive CLA, or ACLA, as an improved CLA. Two adaptive designs are proposed for ACLA: 1) adaptively selecting the keys for non-local attention at each layer; 2) automatically searching for the insertion locations for ACLA modules. By these two adaptive designs, ACLA dynamically selects the number of keys to be aggregated for non-local attention at layer. In addition, ACLA searches for the optimal insert positions of ACLA modules by a neural architecture search method to render a compact neural network with compelling performance. Extensive experiments on image restoration tasks, including single image super-resolution, image denoising, image demosaicing, and image compression artifacts reduction, validate the effectiveness and efficiency of ACLA.      
### 44.Mammograms Classification: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2203.03618.pdf)
>  An advanced reliable low-cost form of screening method, Digital mammography has been used as an effective imaging method for breast cancer detection. With an increased focus on technologies to aid healthcare, Mammogram images have been utilized in developing computer-aided diagnosis systems that will potentially help in clinical diagnosis. Researchers have proved that artificial intelligence with its emerging technologies can be used in the early detection of the disease and improve radiologists' performance in assessing breast cancer. In this paper, we review the methods developed for mammogram mass classification in two categories. The first one is classifying manually provided cropped region of interests (ROI) as either malignant or benign, and the second one is the classification of automatically segmented ROIs as either malignant or benign. We also provide an overview of datasets and evaluation metrics used in the classification task. Finally, we compare and discuss the deep learning approach to classical image processing and learning approach in this domain.      
### 45.Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP)  [ :arrow_down: ](https://arxiv.org/pdf/2203.04180.pdf)
>  Purpose: To develop a tuning-free method for multi-coil compressed sensing MRI that performs competitively with algorithms with an optimally tuned sparse parameter. <br>Theory: The Parallel Variable Density Approximate Message Passing (P-VDAMP) algorithm is proposed. For Bernoulli random variable density sampling, P-VDAMP obeys a "state evolution", where the intermediate per-iteration image estimate is distributed according to the ground truth corrupted by a Gaussian vector with approximately known covariance. State evolution is leveraged to automatically tune sparse parameters on-the-fly with Stein's Unbiased Risk Estimate (SURE). <br>Methods: P-VDAMP is evaluated on brain, knee and angiogram datasets at acceleration factors 5 and 10 and compared with four variants of the Fast Iterative Shrinkage-Thresholding algorithm (FISTA), including two tuning-free variants from the literature. <br>Results: The proposed method is found to have a similar reconstruction quality and time to convergence as FISTA with an optimally tuned sparse weighting. <br>Conclusions: P-VDAMP is an efficient, robust and principled method for on-the-fly parameter tuning that is competitive with optimally tuned FISTA and offers substantial robustness and reconstruction quality improvements over competing tuning-free methods.      
### 46.Distributed Control using Reinforcement Learning with Temporal-Logic-Based Reward Shaping  [ :arrow_down: ](https://arxiv.org/pdf/2203.04172.pdf)
>  We present a computational framework for synthesis of distributed control strategies for a heterogeneous team of robots in a partially observable environment. The goal is to cooperatively satisfy specifications given as Truncated Linear Temporal Logic (TLTL) formulas. Our approach formulates the synthesis problem as a stochastic game and employs a policy graph method to find a control strategy with memory for each agent. We construct the stochastic game on the product between the team transition system and a finite state automaton (FSA) that tracks the satisfaction of the TLTL formula. We use the quantitative semantics of TLTL as the reward of the game, and further reshape it using the FSA to guide and accelerate the learning process. Simulation results demonstrate the efficacy of the proposed solution under demanding task specifications and the effectiveness of reward shaping in significantly accelerating the speed of learning.      
### 47.A study on joint modeling and data augmentation of multi-modalities for audio-visual scene classification  [ :arrow_down: ](https://arxiv.org/pdf/2203.04114.pdf)
>  In this paper, we propose two techniques, namely joint modeling and data augmentation, to improve system performances for audio-visual scene classification (AVSC). We employ pre-trained networks trained only on image data sets to extract video embedding; whereas for audio embedding models, we decide to train them from scratch. We explore different neural network architectures for joint modeling to effectively combine the video and audio modalities. Moreover, data augmentation strategies are investigated to increase audio-visual training set size. For the video modality the effectiveness of several operations in RandAugment is verified. An audio-video joint mixup scheme is proposed to further improve AVSC performances. Evaluated on the development set of TAU Urban Audio Visual Scenes 2021, our final system can achieve the best accuracy of 94.2% among all single AVSC systems submitted to DCASE 2021 Task 1b.      
### 48.VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2203.04099.pdf)
>  This paper presents an audio-visual approach for voice separation which outperforms state-of-the-art methods at a low latency in two scenarios: speech and singing voice. The model is based on a two-stage network. Motion cues are obtained with a lightweight graph convolutional network that processes face landmarks. Then, both audio and motion features are fed to an audio-visual transformer which produces a fairly good estimation of the isolated target source. In a second stage, the predominant voice is enhanced with an audio-only network. We present different ablation studies and comparison to state-of-the-art methods. Finally, we explore the transferability of models trained for speech separation in the task of singing voice separation. The demos, code, and weights will be made publicly available at <a class="link-external link-https" href="https://ipcv.github.io/VoViT/" rel="external noopener nofollow">this https URL</a>      
### 49.Bayesian Optimisation-Assisted Neural Network Training Technique for Radio Localisation  [ :arrow_down: ](https://arxiv.org/pdf/2203.04032.pdf)
>  Radio signal-based (indoor) localisation technique is important for IoT applications such as smart factory and warehouse. Through machine learning, especially neural networks methods, more accurate mapping from signal features to target positions can be achieved. However, different radio protocols, such as WiFi, Bluetooth, etc., have different features in the transmitted signals that can be exploited for localisation purposes. Also, neural networks methods often rely on carefully configured models and extensive training processes to obtain satisfactory performance in individual localisation scenarios. The above poses a major challenge in the process of determining neural network model structure, or hyperparameters, as well as the selection of training features from the available data. This paper proposes a neural network model hyperparameter tuning and training method based on Bayesian optimisation. Adaptive selection of model hyperparameters and training features can be realised with minimal need for manual model training design. With the proposed technique, the training process is optimised in a more automatic and efficient way, enhancing the applicability of neural networks in localisation.      
### 50.Adaptations to a geomagnetic field interpolation method in Southern Africa  [ :arrow_down: ](https://arxiv.org/pdf/2203.03976.pdf)
>  Space weather and its impact on infrastructure presents a clear risk in the modern era, as evidenced by the adverse effects of geomagnetically induced currents (GICs) in power networks. To model GICs, ground-based geomagnetic field (B-field) measurements are critical and need to be available in the region of interest. A challenge globally lies in the sparse distribution of magnetometer arrays, which are seldom located near critical power network nodes. Interpolation of the geomagnetic field (B-field) is often needed, with the spherical elementary current system (SECS) approach developed for high-latitude regions favoured. We adapt this interpolation scheme to include low-cost variometers to interpolate dB/dt directly and increase interpolation accuracy. A further adaptation to the scheme is to physically represent the mid-latitude context where most power networks and pipelines lie. The driving current systems in these regions differ from their high-latitude counterparts. Using a physics-consistent mid-latitude version of SECS, we show why previous implementations in Southern Africa are incorrect but still result in useful interpolation. The scope of these adaptations not only has direct application to research in general, but also to utilities, where effective low-cost instrumentation can be used to improve GIC modelling accuracy.      
### 51.A Dynamic Hierarchical Framework for IoT-assisted Metaverse Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2203.03969.pdf)
>  Metaverse has recently attracted much attention from both academia and industry. Virtual services, ranging from virtual driver training to online route optimization for smart good delivery, are emerging in the Metaverse. To make the human experience of virtual life real, digital twins (DTs), namely digital replications of physical objects in life, are the key enablers. However, the status of DTs is not always reliable because their physical counterparties can be moving objects or subject to changes as time passes. As such, it is necessary to synchronize DTs with their physical objects to make DTs status reliable for virtual businesses in the Metaverse. In this paper, we propose a dynamic hierarchical framework in, which a group of IoTs devices assists virtual service providers (VSPs) in synchronizing DTs: the devices sense and collect physical objects' status information collectively in return for incentives. Based on the collected sync data and the value decay rate of the DTs, the VSPs can determine a sync intensity to maximize their payoffs. We adopt a dynamic hierarchical framework in which the lower-level evolutionary game captures the VSPs selection by the population of IoT devices, and the upper-level (Stackelberg) differential game captures the VSPs payoffs affected by the sync strategy, UAVs selection shares, and the DTs value status. We theoretically and experimentally prove the equilibrium to the lower-level game exists and is evolutionarily robust, and provide the sensitivity analysis w.r.t various system parameters. Experiment shows that the dynamic Stackelberg differential game gives higher accumulated payoffs compared to the static Stackelberg game and the simultaneous differential game.      
### 52.Digital Speech Algorithms for Speaker De-Identification  [ :arrow_down: ](https://arxiv.org/pdf/2203.03932.pdf)
>  The present work is based on the COST Action IC1206 for De-identification in multimedia content. It was performed to test four algorithms of voice modifications on a speech gender recognizer to find the degree of modification of pitch when the speech recognizer have the probability of success equal to the probability of failure. The purpose of this analysis is to assess the intensity of the speech tone modification, the quality, the reversibility and not-reversibility of the changes made.      
### 53.Quadruped Guidance Robot for the Visually Impaired: A Comfort-Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.03927.pdf)
>  A quadrupedal guidance robot that can guide people and avoid various obstacles, could potentially be owned by more visually impaired people at a fairly low cost. In this paper, we propose a novel guidance robot system with a comfort-based concept. We design a leash containing an elastic rope and a thin string, and use a motor to adjust the length of the string to ensure comfort. We use the force-based human motion model to plan the forces experienced by the human. Afterward, the direction and magnitude of the force are controlled by the motion of the robot, and the rotation of the motor, respectively. This allows humans to be guided safely and more comfortably to the target position in complex environments. The system has been deployed on Unitree Laikago quadrupedal platform and validated in real-world scenarios.      
### 54.Graph Reinforcement Learning for Predictive Power Allocation to Mobile Users  [ :arrow_down: ](https://arxiv.org/pdf/2203.03906.pdf)
>  Allocating resources with future channels can save resource to ensure quality-of-service of video streaming. In this paper, we optimize predictive power allocation to minimize the energy consumed at distributed units (DUs) by using deep deterministic policy gradient (DDPG) to find optimal policy and predict average channel gains. To improve training efficiency, we resort to graph DDPG for exploiting two kinds of relational priors: (a) permutation equivariant (PE) and permutation invariant (PI) properties of policy function and action-value function, (b) topology relation among users and DUs. To design graph DDPG framework more systematically in harnessing the priors, we first demonstrate how to transform matrix-based DDPG into graph-based DDPG. Then, we respectively design the actor and critic networks to satisfy the permutation properties when graph neural networks are used in embedding and end to-end manners. To avoid destroying the PE/PI properties of the actor and critic networks, we conceive a batch normalization method. Finally, we show the impact of leveraging each prior. Simulation results show that the learned predictive policy performs close to the optimal solution with perfect future information, and the graph DDPG algorithms converge much faster than existing DDPG algorithms.      
### 55.An Efficient Two-Stage SPARC Decoder for Massive MIMO Unsourced Random Access  [ :arrow_down: ](https://arxiv.org/pdf/2203.03836.pdf)
>  In this paper, we study a concatenate coding scheme based on sparse regression code (SPARC) and tree code for unsourced random access in massive multiple-input and multiple-output (MIMO) systems. Our focus is concentrated on efficient decoding for the inner SPARC with practical concerns. A two-stage method is proposed to achieve near-optimal performance while maintaining low computational complexity. Specifically, an one-step thresholding-based algorithm is first used for reducing large dimensions of the SPARC decoding, after which an relaxed maximum-likelihood estimator is employed for refinement. Adequate simulation results are provided to validate the near-optimal performance and the low computational complexity. Besides, for covariance-based sparse recovery method, theoretical analyses are given to characterize the upper bound of the number of active users supported when convex relaxation is considered, and the probability of successful dimension reduction by the one-step thresholding-based algorithm.      
### 56.SpeechFormer: A Hierarchical Efficient Framework Incorporating the Characteristics of Speech  [ :arrow_down: ](https://arxiv.org/pdf/2203.03812.pdf)
>  Transformer has obtained promising results on cognitive speech signal processing field, which is of interest in various applications ranging from emotion to neurocognitive disorder analysis. However, most works treat speech signal as a whole, leading to the neglect of the pronunciation structure that is unique to speech and reflects the cognitive process. Meanwhile, Transformer has heavy computational burden due to its full attention operation. In this paper, a hierarchical efficient framework, called SpeechFormer, which considers the structural characteristics of speech, is proposed and can be served as a general-purpose backbone for cognitive speech signal processing. The proposed SpeechFormer consists of frame, phoneme, word and utterance stages in succession, each performing a neighboring attention according to the structural pattern of speech with high computational efficiency. SpeechFormer is evaluated on speech emotion recognition (IEMOCAP &amp; MELD) and neurocognitive disorder detection (Pitt &amp; DAIC-WOZ) tasks, and the results show that SpeechFormer outperforms the standard Transformer-based framework while greatly reducing the computational cost. Furthermore, our SpeechFormer achieves comparable results to the state-of-the-art approaches.      
### 57.Double-Sided Beamforming in OWC Systems Using Omni-Digital Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2203.03781.pdf)
>  In this paper, we introduce a variant of reconfigurable intelligent surfaces (RISs) called omni-digital-RISs (DRISs), which allow multiple physical processes, with application to optical wireless communications systems. The proposed omni-DRIS contains both reflectors and refractive elements, as well as elements that perform both simultaneously. We describe and explain the concept of omni-DRIS, suggest and analyze an omni-DRIS coding structure, discuss metamaterials to be used, and provide a design example. Furthermore, we demonstrate that the achievable rate of an omni-DRIS system depends on the number of omni-DRIS elements, bits per phase shift, and the number of unused elements. In addition, we show that the achievable rate upper bound is related to the number of omni-DRIS elements, and conclude by discussing future research directions.      
### 58.Zero-delay Consistent and Smooth Trainable Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2203.03776.pdf)
>  The question of how to produce a smooth interpolating curve from a stream of data points is addressed in this paper. To this end, we formalize the concept of real-time interpolator (RTI): a trainable unit that recovers smooth signals that are consistent with the received input samples in an online manner. Specifically, an RTI works under the requirement of producing a function section immediately after a sample is received (zero delay), without changing the reconstructed signal in past time sections. This work formulates the design of spline-based RTIs as a bi-level optimization problem. Their training consists in minimizing the average curvature of the interpolated signals over a set of example sequences. The latter are representative of the nature of the data sequence to be interpolated, allowing to tailor the RTI to a specific signal source. Our overall design allows for different possible schemes. In this work, we present two approaches, namely, the parametrized RTI and the recurrent neural network (RNN)-based RTI, including their architecture and properties. Experimental results show that the two proposed RTIs can be trained in a data-driven fashion to achieve improved performance (in terms of the curvature loss metric) with respect to a myopic-type RTI that only exploits the local information at each time sample, while maintaining smooth, zero-delay, and consistency requirements.      
### 59.Detection of AI Synthesized Hindi Speech  [ :arrow_down: ](https://arxiv.org/pdf/2203.03706.pdf)
>  The recent advancements in generative artificial speech models have made possible the generation of highly realistic speech signals. At first, it seems exciting to obtain these artificially synthesized signals such as speech clones or deep fakes but if left unchecked, it may lead us to digital dystopia. One of the primary focus in audio forensics is validating the authenticity of a speech. Though some solutions are proposed for English speeches but the detection of synthetic Hindi speeches have not gained much attention. Here, we propose an approach for discrimination of AI synthesized Hindi speech from an actual human speech. We have exploited the Bicoherence Phase, Bicoherence Magnitude, Mel Frequency Cepstral Coefficient (MFCC), Delta Cepstral, and Delta Square Cepstral as the discriminating features for machine learning models. Also, we extend the study to using deep neural networks for extensive experiments, specifically VGG16 and homemade CNN as the architecture models. We obtained an accuracy of 99.83% with VGG16 and 99.99% with homemade CNN models.      
### 60.Learning to Bound: A Generative Cramér-Rao Bound  [ :arrow_down: ](https://arxiv.org/pdf/2203.03695.pdf)
>  The Cramér-Rao bound (CRB), a well-known lower bound on the performance of any unbiased parameter estimator, has been used to study a wide variety of problems. However, to obtain the CRB, requires an analytical expression for the likelihood of the measurements given the parameters, or equivalently a precise and explicit statistical model for the data. In many applications, such a model is not available. Instead, this work introduces a novel approach to approximate the CRB using data-driven methods, which removes the requirement for an analytical statistical model. This approach is based on the recent success of deep generative models in modeling complex, high-dimensional distributions. Using a learned normalizing flow model, we model the distribution of the measurements and obtain an approximation of the CRB, which we call Generative Cramér-Rao Bound (GCRB). Numerical experiments on simple problems validate this approach, and experiments on two image processing tasks of image denoising and edge detection with a learned camera noise model demonstrate its power and benefits.      
### 61.Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.03664.pdf)
>  Accurate segmentation of retinal fluids in 3D Optical Coherence Tomography images is key for diagnosis and personalized treatment of eye diseases. While deep learning has been successful at this task, trained supervised models often fail for images that do not resemble labeled examples, e.g. for images acquired using different devices. We hereby propose a novel semi-supervised learning framework for segmentation of volumetric images from new unlabeled domains. We jointly use supervised and contrastive learning, also introducing a contrastive pairing scheme that leverages similarity between nearby slices in 3D. In addition, we propose channel-wise aggregation as an alternative to conventional spatial-pooling aggregation for contrastive feature map projection. We evaluate our methods for domain adaptation from a (labeled) source domain to an (unlabeled) target domain, each containing images acquired with different acquisition devices. In the target domain, our method achieves a Dice coefficient 13.8% higher than SimCLR (a state-of-the-art contrastive framework), and leads to results comparable to an upper bound with supervised training in that domain. In the source domain, our model also improves the results by 5.4% Dice, by successfully leveraging information from many unlabeled images.      
### 62.Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language  [ :arrow_down: ](https://arxiv.org/pdf/2203.03598.pdf)
>  Learning to classify video data from classes not included in the training data, i.e. video-based zero-shot learning, is challenging. We conjecture that the natural alignment between the audio and visual modalities in video data provides a rich training signal for learning discriminative multi-modal representations. Focusing on the relatively underexplored task of audio-visual zero-shot learning, we propose to learn multi-modal representations from audio-visual data using cross-modal attention and exploit textual label embeddings for transferring knowledge from seen classes to unseen classes. Taking this one step further, in our generalised audio-visual zero-shot learning setting, we include all the training classes in the test-time search space which act as distractors and increase the difficulty while making the setting more realistic. Due to the lack of a unified benchmark in this domain, we introduce a (generalised) zero-shot learning benchmark on three audio-visual datasets of varying sizes and difficulty, VGGSound, UCF, and ActivityNet, ensuring that the unseen test classes do not appear in the dataset used for supervised training of the backbone deep models. Comparing multiple relevant and recent methods, we demonstrate that our proposed AVCA model achieves state-of-the-art performance on all three datasets. Code and data will be available at \url{<a class="link-external link-https" href="https://github.com/ExplainableML/AVCA-GZSL" rel="external noopener nofollow">this https URL</a>}.      
### 63.Learning Parameters for a Generalized Vidale-Wolfe Response Model with Flexible Ad Elasticity and Word-of-Mouth  [ :arrow_down: ](https://arxiv.org/pdf/2202.13566.pdf)
>  In this research, we investigate a generalized form of Vidale-Wolfe (GVW) model. One key element of our modeling work is that the GVW model contains two useful indexes representing advertiser's elasticity and the word-of-mouth (WoM) effect, respectively. Moreover, we discuss some desirable properties of the GVW model, and present a deep neural network (DNN)-based estimation method to learn its parameters. Furthermore, based on three realworld datasets, we conduct computational experiments to validate the GVW model and identified properties. In addition, we also discuss potential advantages of the GVW model over econometric models. The research outcome shows that both the ad elasticity index and the WoM index have significant influences on advertising responses, and the GVW model has potential advantages over econometric models of advertising, in terms of several interesting phenomena drawn from practical advertising situations. The GVW model and its deep learning-based estimation method provide a basis to support big data-driven advertising analytics and decision makings; in the meanwhile, identified properties and experimental findings of this research illuminate critical managerial insights for advertisers in various advertising forms.      
