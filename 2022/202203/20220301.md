# ArXiv eess --Tue, 1 Mar 2022
### 1.Distributed-MPC with Data-Driven Estimation of Bus Admittance Matrix in Voltage Stabilization  [ :arrow_down: ](https://arxiv.org/pdf/2202.14014.pdf)
>  This paper presents a distributed model-predictive control (MPC) design for real-time voltage stabilization in power systems, allowing the bus admittance matrix $\mathbf{Y} = \mathbf{G} + j\mathbf{B}$ to be not known a priori (it may be time-varying), and so is estimated online. The prevalent control designs are either centralized and optimal but lack scalability and are subject to network attacks, or decentralized that are scalable and less vulnerable to attacks but are suboptimal. The proposed distributed solution offers the attractive features of both types of schemes, namely, optimality, scalability, as well as enhanced security to network attacks. In addition, since acquiring the exact knowledge of the line conductance and susceptance, which are required to form $\mathbf{Y}$, is in general challenging, the presented framework integrates data-driven estimation of $\mathbf{Y}$ to circumvent this challenge. We first introduce the centralized version of the formulation, and next transfer it to a distributed version for efficiency, scalability, and attack-resilience, leveraging the graph structure of the power system. Only local computation and communication are used for (i) computing local control via distributed optimization (which is solved by the alternating direction method of multipliers ADMM), as well as (ii) data-driven estimation of $\mathbf{Y}$. The performance of the proposed methodology is validated using numerical examples of the IEEE-30 Bus and IEEE-57 Bus systems.      
### 2.SUNet: Swin Transformer UNet for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2202.14009.pdf)
>  Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at <a class="link-external link-https" href="https://github.com/FanChiMao/SUNet" rel="external noopener nofollow">this https URL</a>.      
### 3.Systematic Stabilization of Constrained Piecewise Affine Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.14002.pdf)
>  This paper presents an efficient, offline method to simultaneously synthesize controllers and seek closed-loop Lyapunov functions for constrained piecewise affine systems on triangulated subsets of the admissible states. Triangulation refinements explore a rich class of controllers and Lyapunov functions. Since an explicit Lipschitz Lyapunov function is found, an invariant subset of the closed-loop region of attraction is obtained. Moreover, it is a control Lyapunov function, so minimum-norm controllers can be realized through online quadratic programming. It is formulated as a sequence of semi-definite programs. The method avoids computationally burdensome non-convex optimizations and a-priori design choices that are typical of similar existing methods.      
### 4.Precision-medicine-toolbox: An open-source python package for facilitation of quantitative medical imaging and radiomics analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.13965.pdf)
>  Medical image analysis plays a key role in precision medicine as it allows the clinicians to identify anatomical abnormalities and it is routinely used in clinical assessment. Data curation and pre-processing of medical images are critical steps in the quantitative medical image analysis that can have a significant impact on the resulting model performance. In this paper, we introduce a precision-medicine-toolbox that allows researchers to perform data curation, image pre-processing and handcrafted radiomics extraction (via Pyradiomics) and feature exploration tasks with Python. With this open-source solution, we aim to address the data preparation and exploration problem, bridge the gap between the currently existing packages, and improve the reproducibility of quantitative medical imaging research.      
### 5.ciscNet -- A Single-Branch Cell Instance Segmentation and Classification Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.13960.pdf)
>  Automated cell nucleus segmentation and classification are required to assist pathologists in their decision making. The Colon Nuclei Identification and Counting Challenge 2022 (CoNIC Challenge 2022) supports the development and comparability of segmentation and classification methods for histopathological images. In this contribution, we describe our CoNIC Challenge 2022 method ciscNet to segment, classify and count cell nuclei, and report preliminary evaluation results. Our code is available at <a class="link-external link-https" href="https://git.scc.kit.edu/ciscnet/ciscnet-conic-2022" rel="external noopener nofollow">this https URL</a>.      
### 6.Real-Time Frequency Selective Reconstruction through Register-Based Argmax Calculation  [ :arrow_down: ](https://arxiv.org/pdf/2202.13926.pdf)
>  Frequency Selective Reconstruction (FSR) is a state-of-the-art algorithm for solving diverse image reconstruction tasks, where a subset of pixel values in the image is missing. However, it entails a high computational complexity due to its iterative, blockwise procedure to reconstruct the missing pixel values. Although the complexity of FSR can be considerably decreased by performing its computations in the frequency domain, the reconstruction procedure still takes multiple seconds up to multiple minutes depending on the parameterization. However, FSR has the potential for a massive parallelization greatly improving its reconstruction time. In this paper, we introduce a novel highly parallelized formulation of FSR adapted to the capabilities of modern GPUs and propose a considerably accelerated calculation of the inherent argmax calculation. Altogether, we achieve a 100-fold speed-up, which enables the usage of FSR for real-time applications.      
### 7.Motion dynamics of inertial pair coupled via frictional interface  [ :arrow_down: ](https://arxiv.org/pdf/2202.13913.pdf)
>  Understanding how the motion dynamics of two moving bodies with an unbounded friction interface arise, is essential for multiple system and control applications. Coupling terms in the dynamics of an inertial pair, which is linked to each other through a passive frictional contact, is nontrivial and, for a long time, remained less studied. This problem is especially demanding from a viewpoint of the interaction forces and motion states. This paper introduces a generalized problem of relative motion in systems with an unbounded (i.e. free of motion constraints) frictional interface, while assuming a classical Coulomb friction with discontinuity at the velocity zero crossing. We formulate the motion dynamics in a closed form of ordinary differential equations, which include the sign operator for mapping both the Coulomb friction and switching conditions, and discuss their validity in the generalized force and motion coordinates. Here the system with one active degree of freedom (meaning a driving body) and one passive degree of freedom (meaning a driven body) is studied. We analyze and demonstrate the global convergence of trajectories for a free system case, i.e. without an external control. An illustrative case study of solutions is presented for a harmonic oscillator, which has a friction-coupled second mass not connected (or joint-linked) to the ground. This example elucidates the addressed problem statement and the proposed modeling framework. Relevant future developments and related challenging questions are discussed at the end of the paper.      
### 8.Bounded-error constrained state estimation in presence of sporadic measurements  [ :arrow_down: ](https://arxiv.org/pdf/2202.13900.pdf)
>  This contribution proposes a recursive set-membership method for the ellipsoidal state characterization for linear discrete-time models with additive unknown disturbances vectors, bounded by known possibly degenerate zonotopes and polytopes, corrupting respectively, the state difference equation and the sporadic measurement vectors, which are expressed as linear inequality and equality constraints on the state vector. New algorithms are designed considering the unprecedented fact that, due to equality constraints, the shape matrix of the ellipsoid characterizing all possible values of the state vector is non invertible. The two main size minimizing criterions (volume and sum of squared axes lengths) are examined in the time update step and also in the observation updating, in addition to a third one, minimizing some error norm and ensuring the input-to-state stability of the estimation error.      
### 9.Resource Allocation for Single Carrier Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2202.13881.pdf)
>  Resource allocation in orthogonal frequency division multiplexing (OFDM) systems is performed through allocating blocks of subcarriers to each user. Even though OFDM is the primary waveform for 5G NR systems, research reports have noted that single carrier modulation (SCM) offers several advantages over OFDM in massive multiple input multiple output (MIMO) systems, making it a preferred candidate for some future applications such as massive machine type communications (mMTC). This paper presents a method for SCM resource allocation and the relevant information recovery algorithms at the receiver. Our emphasis is on cyclic prefixed SCM, where highly flexible and efficient frequency domain detection algorithms enable the operation of many simultaneous users in a massive MIMO uplink scenario. The proposed resource allocation method allows the number of users to exceed the number of antennas at the base station (BS). Each single carrier transmission is partitioned into $L$ interleaved streams, and each user is allocated a number of such streams. One major benefit of SCM is that each data symbol is spread over the entire bandwidth. As such, the receiver performance is dictated by the average channel gain across the transmission band rather than the channel gain at a given frequency bin or a small group of frequencies. In the proposed setup, each stream may be thought of as a resource block in SCM, analogous to resource blocks in OFDM. Hence, in the context of this paper, the terms resource blocks and streams may be used interchangeably.      
### 10.Severity classification in cases of Collagen VI-related myopathy with Convolutional Neural Networks and handcrafted texture features  [ :arrow_down: ](https://arxiv.org/pdf/2202.13853.pdf)
>  Magnetic Resonance Imaging (MRI) is a non-invasive tool for the clinical assessment of low-prevalence neuromuscular disorders. Automated diagnosis methods might reduce the need for biopsies and provide valuable information on disease follow-up. In this paper, three methods are proposed to classify target muscles in Collagen VI-related myopathy cases, based on their degree of involvement, notably a Convolutional Neural Network, a Fully Connected Network to classify texture features, and a hybrid method combining the two feature sets. The proposed methods was evaluated on axial T1-weighted Turbo Spin-Echo MRI from 26 subjects, including Ullrich Congenital Muscular Dystrophy or Bethlem Myopathy patients at different evolution stages. The best results were obtained with the hybrid model, resulting in a global accuracy of 93.8\%, and F-scores of 0.99, 0.82, and 0.95, for healthy, mild and moderate/severe cases, respectively.      
### 11.Magnitude-aware Probabilistic Speaker Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2202.13826.pdf)
>  Recently, hyperspherical embeddings have established themselves as a dominant technique for face and voice recognition. Specifically, Euclidean space vector embeddings are learned to encode person-specific information in their direction while ignoring the magnitude. However, recent studies have shown that the magnitudes of the embeddings extracted by deep neural networks may indicate the quality of the corresponding inputs. This paper explores the properties of the magnitudes of the embeddings related to quality assessment and out-of-distribution detection. We propose a new probabilistic speaker embedding extractor using the information encoded in the embedding magnitude and leverage it in the speaker verification pipeline. We also propose several quality-aware diarization methods and incorporate the magnitudes in those. Our results indicate significant improvements over magnitude-agnostic baselines both in speaker verification and diarization tasks.      
### 12.Parameter estimation and model reduction for retinal laser treatment  [ :arrow_down: ](https://arxiv.org/pdf/2202.13806.pdf)
>  Laser photocoagulation is one of the most frequently used treatment approaches for retinal diseases such as diabetic retinopathy and macular edema. The use of model-based control, such as Model Predictive Control (MPC), enhances a safe and effective treatment by guaranteeing temperature bounds. In general, real-time requirements for model-based control designs are not met since the temperature distribution in the eye fundus is governed by a heat equation with a nonlinear parameter dependency. This issue is circumvented by representing the model by a lower-dimensional system which well-approximates the original model, including the parametric dependency. We combine a global-basis approach with the discrete empirical interpolation method, tailor its hyperparameters to laser photocoagulation, and show its superiority in comparison to a recently proposed method based on Taylor-series approximation. Its effectiveness is measured in computation time for MPC. We further present a case study to estimate the range of absorption parameters in porcine eyes, and by means of a theoretical and numerical sensitivity analysis we show that the sensitivity of the temperature increase is higher with respect to the absorption coefficient of the retinal pigment epithelium (RPE) than of the choroid's.      
### 13.RestainNet: a self-supervised digital re-stainer for stain normalization  [ :arrow_down: ](https://arxiv.org/pdf/2202.13804.pdf)
>  Color inconsistency is an inevitable challenge in computational pathology, which generally happens because of stain intensity variations or sections scanned by different scanners. It harms the pathological image analysis methods, especially the learning-based models. A series of approaches have been proposed for stain normalization. However, most of them are lack flexibility in practice. In this paper, we formulated stain normalization as a digital re-staining process and proposed a self-supervised learning model, which is called RestainNet. Our network is regarded as a digital restainer which learns how to re-stain an unstained (grayscale) image. Two digital stains, Hematoxylin (H) and Eosin (E) were extracted from the original image by Beer-Lambert's Law. We proposed a staining loss to maintain the correctness of stain intensity during the restaining process. Thanks to the self-supervised nature, paired training samples are no longer necessary, which demonstrates great flexibility in practical usage. Our RestainNet outperforms existing approaches and achieves state-of-the-art performance with regard to color correctness and structure preservation. We further conducted experiments on the segmentation and classification tasks and the proposed RestainNet achieved outstanding performance compared with SOTA methods. The self-supervised design allows the network to learn any staining style with no extra effort.      
### 14.Compilation of Hazardous and Benign Material Complex Dielectric Constants at Millimeter Wave Frequencies for Security Applications  [ :arrow_down: ](https://arxiv.org/pdf/2202.13779.pdf)
>  This paper compiles measured complex dielectric constants of benign and hazardous materials used for developing algorithms for personnel scanners at airports at 30 GHz. The materials are grouped into broad classifications of potential threat by mapping the materials to the complex dielectric constant plane.      
### 15.A Note on "Optimum Sets of Interference-Free Sequences With Zero Autocorrelation Zone"  [ :arrow_down: ](https://arxiv.org/pdf/2202.13764.pdf)
>  In this paper, a simple construction of interference-free zero correlation zone (IF-ZCZ) sequence sets is proposed by well designed finite Zak transform lattice tessellation. Each set is characterized by the period of sequences $KM$, the set size $K$ and the length of zero correlation zone $M-1$, which is optimal with respect to the Tang-Fan-Matsufuji bound. Secondly, the transformations that keep the properties of the optimal IF-ZCZ sequence set unchanged are given, and the equivalent relation of the optimal IF-ZCZ sequence set is defined based on these transformations. Then, it is proved that the general construction of the optimal IF-ZCZ sequence set proposed by Popovic is equivalent to the simple construction of the optimal IF-ZCZ sequence set, which indicates that the generation of the optimal IF-ZCZ sequence set can be simplified. Moreover, it is pointed out that the alphabet size for the special case of the simple construction of the optimal IF-ZCZ sequence set can be a factor of the period. Finally, both the simple construction of the optimal IF-ZCZ sequence set and its special case have sparse and highly structured Zak spectra, which can greatly reduce the computational complexity of implementing matched filter banks.      
### 16.A System Level Approach to Regret Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2202.13763.pdf)
>  We present an optimisation-based method for synthesising a dynamic regret optimal controller for linear systems with potentially adversarial disturbances and known or adversarial initial conditions. The dynamic regret is defined as the difference between the true incurred cost of the system and the cost which could have optimally been achieved under any input sequence having full knowledge of all future disturbances for a given disturbance energy. This problem formulation can be seen as an alternative to classical $\mathcal{H}_2$- or $\mathcal{H}_\infty$-control. The proposed controller synthesis is based on the system level parametrisation, which allows reformulating the dynamic regret problem as a semi-definite problem. This yields a new framework that allows to consider structured dynamic regret problems, which have not yet been considered in the literature. For known pointwise ellipsoidal bounds on the disturbance, we show that the dynamic regret bound can be improved compared to using only a bounded energy assumption and that the optimal dynamic regret bound differs by at most a factor of $\frac{2}{\pi}$ from the computed solution. Furthermore, the proposed framework allows guaranteeing state and input constraint satisfaction. Finally, we present a numerical mass-spring-damper example.      
### 17.Fast off-the-grid sparse recovery with over-parametrized projected gradient descent  [ :arrow_down: ](https://arxiv.org/pdf/2202.13757.pdf)
>  We consider the problem of recovering off-the-grid spikes from Fourier measurements. Successful methods such as sliding Frank-Wolfe and continuous orthogonal matching pursuit (OMP) iteratively add spikes to the solution then perform a costly (when the number of spikes is large) descent on all parameters at each iteration. In 2D, it was shown that performing a projected gradient descent (PGD) from a gridded over-parametrized initialization was faster than continuous orthogonal matching pursuit. In this paper, we propose an off-the-grid over-parametrized initialization of the PGD based on OMP that permits to fully avoid grids and gives faster results in 3D.      
### 18.Explainable deepfake and spoofing detection: an attack analysis using SHapley Additive exPlanations  [ :arrow_down: ](https://arxiv.org/pdf/2202.13693.pdf)
>  Despite several years of research in deepfake and spoofing detection for automatic speaker verification, little is known about the artefacts that classifiers use to distinguish between bona fide and spoofed utterances. An understanding of these is crucial to the design of trustworthy, explainable solutions. In this paper we report an extension of our previous work to better understand classifier behaviour to the use of SHapley Additive exPlanations (SHAP) to attack analysis. Our goal is to identify the artefacts that characterise utterances generated by different attacks algorithms. Using a pair of classifiers which operate either upon raw waveforms or magnitude spectrograms, we show that visualisations of SHAP results can be used to identify attack-specific artefacts and the differences and consistencies between synthetic speech and converted voice spoofing attacks.      
### 19.Automated generation of large-scale distribution grid models based on open data and open source software using an optimization approach  [ :arrow_down: ](https://arxiv.org/pdf/2202.13692.pdf)
>  The increasing share of renewable energy sources on distribution grid level as well as the emerging active role of prosumers lead to both higher distribution grid utilization, and at the same time greater unpredictability of energy generation and consumption. This poses major problems for grid operators in view of, e.g., voltage stability and line (over)loading. Thus, detailed and comprehensive simulation models are essential for planning future distribution grid expansion in view of the expected strong electrification of society. In this context, the contribution of the present paper is a new, more refined method for automated creation of large-scale detailed distribution grid models based solely on publicly available GIS and statistical data. Utilizing the street layouts in Open Street Maps as potential cable routes, a graph representation is created and complemented by residential units that are extracted from the same data source. This graph structure is adjusted to match electrical low-voltage grid topology by solving a variation of the minimum cost flow linear optimization problem with provided data on secondary substations. In a final step, the generated grid representation is transferred to a DIgSILENT PowerFactory model with photovoltaic systems. The presented workflow uses open source software and is fully automated and scalable that allows the generation of ready-to-use distribution grid simulation models for given 20kV substation locations and additional data on residential unit properties for improved results. The performance of the developed method with respect to grid utilization is presented for a selected suburban residential area with power flow simulations for eight scenarios including current residential PV installation and a future scenario with full PV expansion. Furthermore, the suitability of the generated models for quasi-dynamic simulations is shown.      
### 20.Hierarchical Multi-Agent DRL-Based Framework for Joint Multi-RAT Assignment and Dynamic Resource Allocation in Next-Generation HetNets  [ :arrow_down: ](https://arxiv.org/pdf/2202.13652.pdf)
>  This paper considers the problem of cost-aware downlink sum-rate maximization via joint optimal radio access technologies (RATs) assignment and power allocation in next-generation heterogeneous wireless networks (HetNets). We consider a future HetNet comprised of multi-RATs and serving multi-connectivity edge devices (EDs), and we formulate the problem as mixed-integer non-linear programming (MINP) problem. Due to the high complexity and combinatorial nature of this problem and the difficulty to solve it using conventional methods, we propose a hierarchical multi-agent deep reinforcement learning (DRL)-based framework, called DeepRAT, to solve it efficiently and learn system dynamics. In particular, the DeepRAT framework decomposes the problem into two main stages; the RATs-EDs assignment stage, which implements a single-agent Deep Q Network (DQN) algorithm, and the power allocation stage, which utilizes a multi-agent Deep Deterministic Policy Gradient (DDPG) algorithm. Using simulations, we demonstrate how the various DRL agents efficiently interact to learn system dynamics and derive the global optimal policy. Furthermore, our simulation results show that the proposed DeepRAT algorithm outperforms existing state-of-the-art heuristic approaches in terms of network utility. Finally, we quantitatively show the ability of the DeepRAT model to quickly and dynamically adapt to abrupt changes in network dynamics, such as EDs mobility.      
### 21.Signal-to-noise ratio is more important than sampling rate in beat-to-beat interval estimation from optical sensors  [ :arrow_down: ](https://arxiv.org/pdf/2202.13651.pdf)
>  Photoplethysmographic Imaging (PPGI) allows the determination of pulse rate variability from sequential beat-to-beat intervals (BBI) and pulse wave velocity from spatially resolved recorded pulse waves. In either case, sufficient temporal accuracy is essential. <br>The presented work investigates the temporal accuracy of BBI estimation from photoplethysmographic signals. Within comprehensive numerical simulation, we systematically assess the impact of sampling rate, signal-to-noise ratio (SNR), and beat-to-beat shape variations on the root mean square error (RMSE) between real and estimated BBI. <br>Our results show that at sampling rates beyond 14 Hz only small errors exist when interpolation is used. For example, the average RMSE is 3 ms for a sampling rate of 14 Hz and an SNR of 18 dB. Further increasing the sampling rate only results in marginal improvements, e.g. more than tripling the sampling rate to 50 Hz reduces the error by approx. 14%. The most important finding relates to the SNR, which is shown to have a much stronger influence on the error than the sampling rate. For example, increasing the SNR from 18 dB to 24 dB at 14 Hz sampling rate reduced the error by almost 50% to 1.5 ms. Subtle beat-to-beat shape variations, moreover, increase the error decisively by up to 800%. <br>Our results are highly relevant in three regards: first, they partially explain different results in the literature on minimum sampling rates. Second, they emphasize the importance to consider SNR and possibly shape variation in investigations on the minimal sampling rate. Third, they underline the importance of appropriate processing techniques to increase SNR. Importantly, though our motivation is PPGI, the presented work immediately applies to contact PPG and PPG in other settings such as wearables. To enable further investigations, we make the scripts used in modelling and simulation freely available.      
### 22.Improved Sensing and Positioning via 5G and mmWave radar for Airport Surveillance  [ :arrow_down: ](https://arxiv.org/pdf/2202.13650.pdf)
>  This paper explores an integrated approach for improved sensing and positioning with applications in air traffic management (ATM) and in the Advanced Surface Movement Guidance and Control System (A-SMGCS). The integrated approach includes the synergy of 3D Vector Antenna with the novel time-of-arrival and angle-of-arrival estimate methods for accurate positioning, combining the sensing on the sub-6GHz and mmWave spectrum for the enhanced non-cooperative surveillance. For the positioning scope, both uplink and downlink 5G reference signals are investigated and their performance is evaluated. For the non-cooperative sensing scope, a novel 5G-signal-based imaging function is proposed and verified with realistic airport radio-propagation modelling and the AI-based targets tracking-and-motion recognition are investigated. The 5G-based imaging and mmWave radar based detection can be potentially fused to enhance surveillance in the airport. The work is being done within the European-funded project NewSense and it delves into the 5G, Vector Antennas, and mmWave capabilities for future ATM solutions.      
### 23.Red Light, Green Light Game of Multi-Robot Systems with Safety Barrier Certificates  [ :arrow_down: ](https://arxiv.org/pdf/2202.13598.pdf)
>  In this paper, we propose the safety barrier certificates for uncertain multi-robot systems playing red light, green light game. According to the rule of the game, the robots are allowed to move forward after a doll shouts `green light' and must stop when it shouts `red light'. Following this rule, a two-mode nominal controller is designed where one mode is for moving forward and the other one is for slowing down and being motionless. Then, multiple exponential control barrier functions(ECBFs) are developed to handle safety constraints for limited playground, collision avoidance, and saturation of the velocity. While designing the nominal controller and ECBFs, an estimated braking time and robust inequality constraints are derived to deal with the system uncertainty. Consequently, a controller guaranteeing safety barrier certificates of each robot has been formulated by a quadratic programming with the nominal controller and the robust inequality constraints. Finally, red light, green light game is simulated to validate the proposed safety-critical control system.      
### 24.Using Multi-scale SwinTransformer-HTC with Data augmentation in CoNIC Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.13588.pdf)
>  Colorectal cancer is one of the most common cancers worldwide, so early pathological examination is very important. However, it is time-consuming and labor-intensive to identify the number and type of cells on H&amp;E images in clinical. Therefore, automatic segmentation and classification task and counting the cellular composition of H&amp;E images from pathological sections is proposed by CoNIC Challenge 2022. We proposed a multi-scale Swin transformer with HTC for this challenge, and also applied the known normalization methods to generate more augmentation data. Finally, our strategy showed that the multi-scale played a crucial role to identify different scale features and the augmentation arose the recognition of model.      
### 25.A Holistic Review on Advanced Bi-directional EV Charging Control Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2202.13565.pdf)
>  The rapid growth of electric vehicles (EVs) has promised a next-generation transportation system with reduced carbon emission. The fast development of EVs and charging facilities is driving the evolution of Internet of Vehicles (IoV) to Internet of Electric Vehicles (IoEV). IoEV benefits from both smart grid and Internet of Things (IoT) technologies which provide advanced bi-directional charging services and real-time data processing capability, respectively. The major design challenges of the IoEV charging control lie in the randomness of charging events and the mobility of EVs. In this article, we present a holistic review on advanced bi-directional EV charging control algorithms. For Grid-to-Vehicle (G2V), we introduce the charging control problem in two scenarios: 1) Operation of a single charging station and 2) Operation of multiple charging stations in coupled transportation and power networks. For Vehicle-to-Grid (V2G), we discuss how EVs can perform energy trading in the electricity market and provide ancillary services to the power grid. Besides, a case study is provided to illustrate the economic benefit of the joint optimization of routing and charging scheduling of multiple EVs in the IoEV. Last but not the least, we will highlight some open problems and future research directions of charging scheduling problems for IoEVs.      
### 26.ConvNeXt-backbone HoVerNet for nuclei segmentation and classification  [ :arrow_down: ](https://arxiv.org/pdf/2202.13560.pdf)
>  This manuscript gives a brief description of the algorithm used to participate in CoNIC Challenge 2022. We first try out Deeplab-v3+ and Swin-Transformer for semantic segmentation. After the baseline was made available, we follow the method in it and replace the ResNet baseline with ConvNeXtone. Results on validation set shows that even with channel ofeach stage significant smaller in number, it still improves the mPQ+ by 0.04 and multi r2 by 0.0144.      
### 27.Research Needs for Realization of Zero-Carbon Power Grids with Selected Case Studies  [ :arrow_down: ](https://arxiv.org/pdf/2202.13557.pdf)
>  The attainment of carbon neutrality requires a research agenda that addresses the technical and economic challenges that will be encountered as we progress toward 100% renewable electricity generation. Increasing proportions of variable renewable energy (VRE) sources (such as wind turbines and photovoltaic systems) render the supply-and-demand balance of VRE-dominated power grids difficult. The operational characteristics and effects of VRE inverters also require attention. Here, we examine the implications of the paradigm shift to carbon neutrality and summarize the associated research challenges in terms of system planning, operation, and sta-bility, and the need for energy storage integration, demand-side participation, distributed con-trol and estimation, and energy sector coupling. We also highlight the existing literature gaps, and our recent studies that can fill in the gaps, thereby facilitating the improvement of grid op-eration and estimation. The numerical results of comparative case studies are also provided on the operational stability and economics of power grids with a high level of VRE sources, assist-ing stakeholders in establishing specific roadmaps and making relevant decisions.      
### 28.Towards A Device-Independent Deep Learning Approach for the Automated Segmentation of Sonographic Fetal Brain Structures: A Multi-Center and Multi-Device Validation  [ :arrow_down: ](https://arxiv.org/pdf/2202.13553.pdf)
>  Quality assessment of prenatal ultrasonography is essential for the screening of fetal central nervous system (CNS) anomalies. The interpretation of fetal brain structures is highly subjective, expertise-driven, and requires years of training experience, limiting quality prenatal care for all pregnant mothers. With recent advancement in Artificial Intelligence (AI), specifically deep learning (DL), assistance in precise anatomy identification through semantic segmentation essential for the reliable assessment of growth and neurodevelopment, and detection of structural abnormalities have been proposed. However, existing works only identify certain structures (e.g., cavum septum pellucidum, lateral ventricles, cerebellum) from either of the axial views (transventricular, transcerebellar), limiting the scope for a thorough anatomical assessment as per practice guidelines necessary for the screening of CNS anomalies. Further, existing works do not analyze the generalizability of these DL algorithms across images from multiple ultrasound devices and centers, thus, limiting their real-world clinical impact. In this study, we propose a DL based segmentation framework for the automated segmentation of 10 key fetal brain structures from 2 axial planes from fetal brain USG images (2D). We developed a custom U-Net variant that uses inceptionv4 block as a feature extractor and leverages custom domain-specific data augmentation. Quantitatively, the mean (10 structures; test sets 1/2/3/4) Dice-coefficients were: 0.827, 0.802, 0.731, 0.783. Irrespective of the USG device/center, the DL segmentations were qualitatively comparable to their manual segmentations. The proposed DL system offered a promising and generalizable performance (multi-centers, multi-device) and also presents evidence in support of device-induced variation in image quality (a challenge to generalizibility) by using UMAP analysis.      
### 29.Single-shot self-supervised particle tracking  [ :arrow_down: ](https://arxiv.org/pdf/2202.13546.pdf)
>  Particle tracking is a fundamental task in digital microscopy. Recently, machine-learning approaches have made great strides in overcoming the limitations of more classical approaches. The training of state-of-the-art machine-learning methods almost universally relies on either vast amounts of labeled experimental data or the ability to numerically simulate realistic datasets. However, the data produced by experiments are often challenging to label and cannot be easily reproduced numerically. Here, we propose a novel deep-learning method, named LodeSTAR (Low-shot deep Symmetric Tracking And Regression), that learns to tracks objects with sub-pixel accuracy from a single unlabeled experimental image. This is made possible by exploiting the inherent roto-translational symmetries of the data. We demonstrate that LodeSTAR outperforms traditional methods in terms of accuracy. Furthermore, we analyze challenging experimental data containing densely packed cells or noisy backgrounds. We also exploit additional symmetries to extend the measurable particle properties to the particle's vertical position by propagating the signal in Fourier space and its polarizability by scaling the signal strength. Thanks to the ability to train deep-learning models with a single unlabeled image, LodeSTAR can accelerate the development of high-quality microscopic analysis pipelines for engineering, biology, and medicine.      
### 30.Pursuit-evasion differential games of players with different speeds in spaces of different dimensions  [ :arrow_down: ](https://arxiv.org/pdf/2202.13522.pdf)
>  We study pursuit-evasion differential games between a faster pursuer moving in 3D space and an evader moving in a plane. We first extend the well-known Apollonius circle to 3D space, by which we construct the isochron for the considered two players. Then both cases with and without a static target are considered and the corresponding optimal strategies are derived using the concept of isochron. In order to guarantee the optimality of the proposed strategies, the value functions are given and are further proved to be the solution of Hamilton-Jacobi-Isaacs equation. Simulations with comparison between the proposed strategies and other classical strategies are carried out and the results show the optimality of the proposed strategies.      
### 31.CTformer: Convolution-free Token2Token Dilated Vision Transformer for Low-dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2202.13517.pdf)
>  Low-dose computed tomography (LDCT) denoising is an important problem in CT research. Compared to the normal dose CT (NDCT), LDCT images are subjected to severe noise and artifacts. Recently in many studies, vision transformers have shown superior feature representation ability over convolutional neural networks (CNNs). However, unlike CNNs, the potential of vision transformers in LDCT denoising was little explored so far. To fill this gap, we propose a Convolution-free Token2Token Dilated Vision Transformer for low-dose CT denoising. The CTformer uses a more powerful token rearrangement to encompass local contextual information and thus avoids convolution. It also dilates and shifts feature maps to capture longer-range interaction. We interpret the CTformer by statically inspecting patterns of its internal attention maps and dynamically tracing the hierarchical attention flow with an explanatory graph. Furthermore, an overlapped inference mechanism is introduced to effectively eliminate the boundary artifacts that are common for encoder-decoder-based denoising models. Experimental results on Mayo LDCT dataset suggest that the CTformer outperforms the state-of-the-art denoising methods with a low computation overhead.      
### 32.Robust Control of Partially Specified Boolean Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.13440.pdf)
>  Regulatory networks (RNs) are a well-accepted modelling formalism in computational systems biology. The control of RNs is currently receiving a lot of attention because it provides a computational basis for cell reprogramming -- an attractive technology developed in regenerative medicine. By solving the control problem, we learn which parts of a biological system should be perturbed to stabilise the system in the desired phenotype. <br>We allow the specification of the Boolean model representing a given RN to be incomplete. To that end, we utilise the formalism of partially specified Boolean networks which covers every possible behaviour of unspecified parts of the system. Such an approach causes a significant state explosion. This problem is addressed by using symbolic methods to represent both the unspecified model parts and all possible perturbations of the system. <br>Additionally, to make the control design efficient and practically applicable, the optimal control should be minimal in terms of size. Moreover, in a partially specified model, a control may achieve the desired stabilisation only for a subset of the possible fully specified model instantiations. To address these aspects, we utilise several quantitative measures. Apart from the size of perturbation, we also examine its robustness -- a portion of instantiations for which the control is applicable. <br>We show that proposed symbolic methods solving the control problem for partially specified BNs are efficient and scale well. We also evaluate the robustness metrics in cases of all three studied control types. The robustness metric tells us how big a proportion of fully defined systems the given perturbation works. Our experiments support the hypothesis that one-step perturbations may be less robust than temporary or permanent perturbations. <br>This is a full version of a paper that is submitted to a journal.      
### 33.Thermal Modelling and Controller Design of an Alkaline Electrolysis System under Dynamic Operating Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2202.13422.pdf)
>  Thermal management is vital for the efficient and safe operation of alkaline electrolysis systems. Traditional alkaline electrolysis systems use simple proportional-integral-differentiation (PID) controllers to maintain the stack temperature near the rated value. However, in renewable-to-hydrogen scenarios, the stack temperature is disturbed by load fluctuations, and the temperature overshoot phenomenon occurs which can exceed the upper limit and harm the stack. This paper focuses on the thermal modelling and controller design of an alkaline electrolysis system under dynamic operating conditions. A control-oriented thermal model is established in the form of a third-order time-delay process, which is used for simulation and controller design. Based on this model, we propose two novel controllers to reduce temperature overshoot: one is a current feed-forward PID controller (PID-I), the other is a model predictive controller (MPC). Their performances are tested on a lab-scale system and the experimental results are satisfying: the temperature overshoot is reduced by 2.2 degree with the PID-I controller, and no obvious overshoot is observed with the MPC controller. Furthermore, the thermal dynamic performance of an MW-scale alkaline electrolysis system is analyzed by simulation, which shows that the temperature overshoot phenomenon is more general in large systems. The proposed method allows for higher temperature set points which can improve system efficiency by 1%.      
### 34.Weakly Supervised Learning for cell recognition in immunohistochemical cytoplasm staining images  [ :arrow_down: ](https://arxiv.org/pdf/2202.13372.pdf)
>  Cell classification and counting in immunohistochemical cytoplasm staining images play a pivotal role in cancer diagnosis. Weakly supervised learning is a potential method to deal with labor-intensive labeling. However, the inconstant cell morphology and subtle differences between classes also bring challenges. To this end, we present a novel cell recognition framework based on multi-task learning, which utilizes two additional auxiliary tasks to guide robust representation learning of the main task. To deal with misclassification, the tissue prior learning branch is introduced to capture the spatial representation of tumor cells without additional tissue annotation. Moreover, dynamic masks and consistency learning are adopted to learn the invariance of cell scale and shape. We have evaluated our framework on immunohistochemical cytoplasm staining images, and the results demonstrate that our method outperforms recent cell recognition approaches. Besides, we have also done some ablation studies to show significant improvements after adding the auxiliary branches.      
### 35.Topology-Preserving Segmentation Network: A Deep Learning Segmentation Framework for Connected Component  [ :arrow_down: ](https://arxiv.org/pdf/2202.13331.pdf)
>  Medical image segmentation, which aims to automatically extract anatomical or pathological structures, plays a key role in computer-aided diagnosis and disease analysis. Despite the problem has been widely studied, existing methods are prone to topological errors. In medical imaging, the topology of the structure, such as the kidney or lung, is usually known. Preserving the topology of the structure in the segmentation process is of utmost importance for accurate image analysis. In this work, a novel learning-based segmentation model is proposed. A {\it topology-preserving segmentation network (TPSN)} is trained to give an accurate segmentation result of an input image that preserves the prescribed topology. TPSN is a deformation-based model that yields a deformation map through a UNet, which takes the medical image and a template mask as inputs. The main idea is to deform a template mask describing the prescribed topology by a diffeomorphism to segment the object in the image. The topology of the shape in the template mask is well preserved under the diffeomorphic map. The diffeomorphic property of the map is controlled by introducing a regularization term related to the Jacobian in the loss function. As such, a topology-preserving segmentation result can be guaranteed. Furthermore, a multi-scale TPSN is developed in this paper that incorporates multi-level information of images to produce more precise segmentation results. To evaluate our method, we applied the 2D TPSN on Ham10000 and 3D TPSN on KiTS21. Experimental results illustrate our method outperforms the baseline UNet segmentation model with/without connected-component analysis (CCA) by both the dice score and IoU score. Besides, results show that our method can produce reliable results even in challenging cases, where pixel-wise segmentation models by UNet and CCA fail to obtain accurate results.      
### 36.Application of Event-Triggered Sliding Mode Control of 2-DOF Humanoid's Lower-Limb Powered by Series Elastic Actuator  [ :arrow_down: ](https://arxiv.org/pdf/2202.13318.pdf)
>  This paper proposes an event-triggered sliding mode control (SMC) scheme combined with a backstepping algorithm for control of 2-DOF humanoid's lower-limb powered by Series elastic actuator (SEA). First, the modelling process for the lower-limb system is implemented by using the Euler-Lagrange theory. With the obtained dynamical equations of lower-limb, the model of the SEA is achieved in both mechanical and electrical perspectives. Then, the event-triggered SMC approach is utilized to ensure the system's stability and eliminate the effect of bounded external disturbance. Next, some assumptions and designed thresholds for the tracking error are given, together with the proof for the convergence of the inter-event time. The backstepping algorithm is applied in the end to determine the needed input control voltage signal. Finally, the results of this research are demonstrated through some simulations in order to prove the effi-ciency and appropriation of this method.      
### 37.DXM-TransFuse U-net: Dual Cross-Modal Transformer Fusion U-net for Automated Nerve Identification  [ :arrow_down: ](https://arxiv.org/pdf/2202.13304.pdf)
>  Accurate nerve identification is critical during surgical procedures for preventing any damages to nerve tissues. Nerve injuries can lead to long-term detrimental effects for patients as well as financial overburdens. In this study, we develop a deep-learning network framework using the U-Net architecture with a Transformer block based fusion module at the bottleneck to identify nerve tissues from a multi-modal optical imaging system. By leveraging and extracting the feature maps of each modality independently and using each modalities information for cross-modal interactions, we aim to provide a solution that would further increase the effectiveness of the imaging systems for enabling the noninvasive intraoperative nerve identification.      
### 38.A Simple Discretization Scheme for Gain Matrix Conditioning  [ :arrow_down: ](https://arxiv.org/pdf/2202.13291.pdf)
>  In industrial model predictive controllers (MPCs), models generated from regression-based system identification methods typically contain small or even physically non-existent degrees of freedom. Control issues can arise when the steady-state optimizer uses these small degrees of freedom to calculate targets for plant operation due to matrix ill-conditioning. Mathematical techniques like Relative Gain Array (RGA) and Singular Value Decomposition (SVD) are helpful for analyzing controller gain interactions and identifying conditioning issues, which can be corrected relatively easily in small models. However, these techniques are difficult and tedious to apply for larger, more complex models. This paper describes a novel, non-iterative, RGA-based, binning technique for discretizing the gain matrix and quickly solving 2$\times$2 conditioning issues for any model size, while guaranteeing gain adjustments below a certain threshold. Higher order interactions are also discussed.      
### 39.ICASSP 2022 Acoustic Echo Cancellation Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.13290.pdf)
>  The ICASSP 2022 Acoustic Echo Cancellation Challenge is intended to stimulate research in acoustic echo cancellation (AEC), which is an important area of speech enhancement and still a top issue in audio communication. This is the third AEC challenge and it is enhanced by including mobile scenarios, adding speech recognition rate in the challenge goal metrics, and making the default sample rate 48 kHz. In this challenge, we open source two large datasets to train AEC models under both single talk and double talk scenarios. These datasets consist of recordings from more than 10,000 real audio devices and human speakers in real environments, as well as a synthetic dataset. We also open source an online subjective test framework and provide an online objective metric service for researchers to quickly test their results. The winners of this challenge are selected based on the average Mean Opinion Score achieved across all different single talk and double talk scenarios, and the speech recognition word acceptance rate.      
### 40.ICASSP 2022 Deep Noise Suppression Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2202.13288.pdf)
>  The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality. This is the 4th DNS challenge, with the previous editions held at INTERSPEECH 2020, ICASSP 2021, and INTERSPEECH 2021. We open-source datasets and test sets for researchers to train their deep noise suppression models, as well as a subjective evaluation framework based on ITU-T P.835 to rate and rank-order the challenge entries. We provide access to DNSMOS P.835 and word accuracy (WAcc) APIs to challenge participants to help with iterative model improvements. In this challenge, we introduced the following changes: (i) Included mobile device scenarios in the blind test set; (ii) Included a personalized noise suppression track with baseline; (iii) Added WAcc as an objective metric; (iv) Included DNSMOS P.835; (v) Made the training datasets and test sets fullband (48 kHz). We use an average of WAcc and subjective scores P.835 SIG, BAK, and OVRL to get the final score for ranking the DNS models. We believe that as a research community, we still have a long way to go in achieving excellent speech quality in challenging noisy real-world scenarios.      
### 41.Learning the Beauty in Songs: Neural Singing Voice Beautifier  [ :arrow_down: ](https://arxiv.org/pdf/2202.13277.pdf)
>  We are interested in a novel task, singing voice beautifying (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality. Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone. In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve. Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one. To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions. Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics. Audio samples are available at~\url{<a class="link-external link-https" href="https://neuralsvb.github.io" rel="external noopener nofollow">this https URL</a>}.      
### 42.Observers for Differential Algebraic Equation Models of Power Networks: Jointly Estimating Dynamic and Algebraic States  [ :arrow_down: ](https://arxiv.org/pdf/2202.13254.pdf)
>  Phasor measurement units ({PMUs}) have become instrumental in modern power systems for enabling real-time, wide-area monitoring and control. Accordingly, many studies have investigated efficient and robust dynamic state estimation (DSE) methods in order to accurately compute the dynamic states of generation units. Nonetheless, most of them forego the dynamic-algebraic nature of power networks and only consider their nonlinear dynamic representations. Motivated by the lack of DSE methods based on power network's differential-algebraic equations (DAEs), this paper develops a novel observer-based DSE framework in order to perform simultaneous estimation of the dynamic and algebraic states of multi-machine power networks. Specifically, we leverage the DAE dynamics of a power network around an operating point and combine them with a PMU-based measurement model capable of capturing bus voltages and line currents. The proposed $\mathcal{H}_{\infty}$ observer, which only requires detectability and impulse observability conditions which are satisfied for various power networks, is designed to handle various noise, unknown inputs, and input sensor failures. The results obtained from performing extensive numerical simulations on the IEEE $9$-bus and $39$-bus systems showcase the effectiveness of the proposed approach for DSE purposes.      
### 43.Weight Selection for Pattern Control of Paraboloidal Reflector Antennas with Reconfigurable Rim Scattering  [ :arrow_down: ](https://arxiv.org/pdf/2202.13219.pdf)
>  It has been recently demonstrated that modifying the rim scattering of a paraboloidal reflector antenna through the use of reconfigurable elements along the rim facilitates sidelobe modification including cancelling sidelobes. In this work we investigate techniques for determining the unit-magnitude weights (i.e., weights which modify the phase of the scattered signals) to accomplish sidelobe cancellation at arbitrary angles from the reflector axis. Specifically, it is shown that despite the large search space and the non-convexity of the cost function, weights can be found with reasonable complexity which provide significant cancellation capability. First, the optimal weights without any magnitude constraints are found. Afterwards, algorithms are developed for determining the unit-modulus weights with both quantized and unquantized phases. Further, it is shown that weights can be obtained that both cancel sidelobes while providing a constant main lobe gain. A primary finding is that sufficiently deep nulls are possible with essentially no change in the main lobe with practical (binary or quaternary) phase-only weights.      
### 44.Opening the Black Box of Learned Image Coders  [ :arrow_down: ](https://arxiv.org/pdf/2202.13209.pdf)
>  End-to-end learned lossy image coders, as opposed to hand-crafted image codecs, have shown increasing superiority in terms of the rate-distortion performance. However, they are mainly treated as a black-box system and their interpretability is not well studied. In this paper, we investigate learned image coders from the perspective of linear transform coding by measuring their channel response and linearity. For different learned image coder designs, we show that their end-to-end learned non-linear transforms share similar properties with linear orthogonal transformations. Our analysis provides insights into understanding how learned image coders work and could benefit future design and development.      
### 45.A Feasibility Study on Real-Time High Resolution Imaging of the Brain Using Electrical Impedance Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2202.13159.pdf)
>  Objective: The strengths of Electrical Impedance Tomography (EIT) are its capability of imaging the internal body by using a noninvasive, radiation safe technique, and the absence of known hazards. In this paper we introduce a novel idea of using EIT in microelectrodes during Deep Brain Stimulation (DBS) surgery in order to obtain an image of the electrical conductivities of the brain tissues surrounding the microelectrodes. DBS is a surgical treatment involving the implantation of a medical probe inside the brain. For such application, the EIT reconstruction method has to offer both high quality and robustness against noise. Methods: A post-processing method for open-domain EIT is introduced in this paper, which combines linear and nonlinear methods in order to use the advantages of both with limited drawbacks. The reconstruction method is a two-steps method, the first solves the inverse problem with a linear algorithm and the second brings the nonlinear aspects back into the image. Results: The proposed method is tested on both simulation and phantom data, and compared to three widely used method for EIT imaging. Resulting images and errors gives a strong advantage to the proposed solution. Conclusion: High quality reconstruction from phantom data validates the efficiency of the novel reconstruction method. Significance: This feasibility study presents an efficient method for open domain EIT and opens the way to clinical trials.      
### 46.Multi-image Super-resolution via Quality Map Associated Temporal Attention Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.13124.pdf)
>  With the rising interest in deep learning-based methods in remote sensing, neural networks have made remarkable advancements in multi-image fusion and super-resolution. To fully exploit the advantages of multi-image super-resolution, temporal attention is crucial as it allows a model to focus on reliable features rather than noises. Despite the presence of quality maps (QMs) that indicate noises in images, most of the methods tested in the PROBA-V dataset have not been used QMs for temporal attention. We present a quality map associated temporal attention network (QA-Net), a novel method that incorporates QMs into both feature representation and fusion processes for the first time. Low-resolution features are temporally attended by QM features in repeated multi-head attention modules. The proposed method achieved state-of-the-art results in the PROBA-V dataset.      
### 47.What ODE-Approximation Schemes of Time-Delay Systems Reveal about Lyapunov-Krasovskii Functionals  [ :arrow_down: ](https://arxiv.org/pdf/2202.13122.pdf)
>  Lyapunov-Krasovskii functionals are found to be related to Lyapunov functions that prove partial stability in finite dimensional system approximations. These approximations are ordinary differential equations, which, in the present paper, originate from the Chebyshev (pseudospectral) collocation or the Legendre tau method. Lyapunov functions that prove partial stability are simply obtained by solving a Lyapunov equation. They approximate the Lyapunov-Krasovskii functional. A formula for the partial positive definiteness bound on the Lyapunov function is derived. The formula is also applied to a numerical integration of the known Lyapunov-Krasovskii functional. An example shows that both approaches converge to identical results, representing the largest quadratic lower bound on complete-type or related functionals.      
### 48.Revisiting Over-Smoothness in Text to Speech  [ :arrow_down: ](https://arxiv.org/pdf/2202.13066.pdf)
>  Non-autoregressive text to speech (NAR-TTS) models have attracted much attention from both academia and industry due to their fast generation speed. One limitation of NAR-TTS models is that they ignore the correlation in time and frequency domains while generating speech mel-spectrograms, and thus cause blurry and over-smoothed results. In this work, we revisit this over-smoothing problem from a novel perspective: the degree of over-smoothness is determined by the gap between the complexity of data distributions and the capability of modeling methods. Both simplifying data distributions and improving modeling methods can alleviate the problem. Accordingly, we first study methods reducing the complexity of data distributions. Then we conduct a comprehensive study on NAR-TTS models that use some advanced modeling methods. Based on these studies, we find that 1) methods that provide additional condition inputs reduce the complexity of data distributions to model, thus alleviating the over-smoothing problem and achieving better voice quality. 2) Among advanced modeling methods, Laplacian mixture loss performs well at modeling multimodal distributions and enjoys its simplicity, while GAN and Glow achieve the best voice quality while suffering from increased training or model complexity. 3) The two categories of methods can be combined to further alleviate the over-smoothness and improve the voice quality. 4) Our experiments on the multi-speaker dataset lead to similar conclusions as above and providing more variance information can reduce the difficulty of modeling the target data distribution and alleviate the requirements for model capacity.      
### 49.Random Access with Massive MIMO-OTFS in LEO Satellite Communications  [ :arrow_down: ](https://arxiv.org/pdf/2202.13058.pdf)
>  This paper considers the joint channel estimation and device activity detection in the grant-free random access systems, where a large number of Internet-of-Things devices intend to communicate with a low-earth orbit satellite in a sporadic way. In addition, the massive multiple-input multiple-output (MIMO) with orthogonal time-frequency space (OTFS) modulation is adopted to combat the dynamics of the terrestrial-satellite link. We first analyze the input-output relationship of the single-input single-output OTFS when the large delay and Doppler shift both exist, and then extend it to the grant-free random access with massive MIMO-OTFS. Next, by exploring the sparsity of channel in the delay-Doppler-angle domain, a two-dimensional pattern coupled hierarchical prior with the sparse Bayesian learning and covariance-free method (TDSBL-FM) is developed for the channel estimation. Then, the active devices are detected by computing the energy of the estimated channel. Finally, the generalized approximate message passing algorithm combined with the sparse Bayesian learning and two-dimensional convolution (ConvSBL-GAMP) is proposed to decrease the computations of the TDSBL-FM algorithm. Simulation results demonstrate that the proposed algorithms outperform conventional methods.      
### 50.Bayesian Ridge Regression Based Model to Predict Fault Location in HVdc Network  [ :arrow_down: ](https://arxiv.org/pdf/2202.13048.pdf)
>  This paper discusses a method for accurately estimating the fault location in multi-terminal High Voltage direct current (HVdc) transmission network using single ended current and voltage measurements. The post-fault voltage and current signatures are a function of multiple factors and thus accurately locating faults on a multi-terminal network is challenging. We discuss a novel data-driven Bayes Regression based method for accurately predicting fault locations. The sensitivity of the proposed algorithm to measurement noise, fault location, resistance and current limiting inductance are performed on a radial three-terminal MTdc network. The test system is designed in Power System Computer Aided Design (PSCAD)/Electromagnetic Transients including dc (EMTdc).      
### 51.Low SNR Multiframe Registration for Cubesats  [ :arrow_down: ](https://arxiv.org/pdf/2202.13042.pdf)
>  We present a registration algorithm which jointly estimates motion and the ground truth image from a set of noisy frames under rigid, constant translation. The algorithm is non-iterative and needs no hyperparameter tuning. It requires a fixed number of FFT, multiplication, and downsampling operations for a given input size, enabling fast implementation on embedded platforms like cubesats where on-board image fusion can greatly save on limited downlink bandwidth. The algorithm is optimal in the maximum likelihood sense for additive white Gaussian noise and non-stationary Gaussian approximations of Poisson noise. Accurate registration is achieved for very low SNR, even when visible features are below the noise floor.      
### 52.Quickest Change Detection in Anonymous Heterogeneous Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.13023.pdf)
>  The problem of quickest change detection (QCD) in anonymous heterogeneous sensor networks is studied. There are $n$ heterogeneous sensors and a fusion center. The sensors are clustered into $K$ groups, and different groups follow different data-generating distributions. At some unknown time, an event occurs in the network and changes the data-generating distribution of the sensors. The goal is to detect the change as quickly as possible, subject to false alarm constraints. The anonymous setting is studied, where at each time step, the fusion center receives $n$ unordered samples, and the fusion center does not know which sensor each sample comes from, and thus does not know its exact distribution. A simple optimality proof is first derived for the mixture likelihood ratio test, which was constructed and proved to be optimal for the non-sequential anonymous setting in (Chen and Wang, 2019). For the QCD problem, a mixture CuSum algorithm is further constructed, and is further shown to be optimal under Lorden's criterion. For large networks, a computationally efficient test is proposed and a novel theoretical characterization of its false alarm rate is developed. Numerical results are provided to validate the theoretical results.      
### 53.Image reconstruction algorithms in radio interferometry: from handcrafted to learned denoisers  [ :arrow_down: ](https://arxiv.org/pdf/2202.12959.pdf)
>  We introduce a new class of iterative image reconstruction algorithms for radio interferometry, at the interface of convex optimization and deep learning, inspired by plug-and-play methods. The approach consists in learning a prior image model by training a deep neural network (DNN) as a denoiser, and substituting it for the handcrafted proximal regularization operator of an optimization algorithm. The proposed AIRI ("AI for Regularization in Radio-Interferometric Imaging") framework, for imaging complex intensity structure with diffuse and faint emission, inherits the robustness and interpretability of optimization, and the learning power and speed of networks. Our approach relies on three steps. Firstly, we design a low dynamic range database for supervised training from optical intensity images. Secondly, we train a DNN denoiser with basic architecture ensuring positivity of the output image, at a noise level inferred from the signal-to-noise ratio of the data. We use either $\ell_2$ or $\ell_1$ training losses, enhanced with a nonexpansiveness term ensuring algorithm convergence, and including on-the-fly database dynamic range enhancement via exponentiation. Thirdly, we plug the learned denoiser into the forward-backward optimization algorithm, resulting in a simple iterative structure alternating a denoising step with a gradient-descent data-fidelity step. The resulting AIRI-$\ell_2$ and AIRI-$\ell_1$ were validated against CLEAN and optimization algorithms of the SARA family, propelled by the "average sparsity" proximal regularization operator. Simulation results show that these first AIRI incarnations are competitive in imaging quality with SARA and its unconstrained forward-backward-based version uSARA, while providing significant acceleration. CLEAN remains faster but offers lower reconstruction quality.      
### 54.Deep Neural Network for Automatic Assessment of Dysphonia  [ :arrow_down: ](https://arxiv.org/pdf/2202.12957.pdf)
>  The purpose of this work is to contribute to the understanding and improvement of deep neural networks in the field of vocal quality. A neural network that predicts the perceptual assessment of overall severity of dysphonia in GRBAS scale is obtained. The design focuses on amplitude perturbations, frequency perturbations, and noise. Results are compared with performance of human raters on the same data. Both the precision and the mean absolute error of the neural network are close to human intra-rater performance, exceeding inter-rater performance.      
### 55.2021 BEETL Competition: Advancing Transfer Learning for Subject Independence &amp; Heterogenous EEG Data Sets  [ :arrow_down: ](https://arxiv.org/pdf/2202.12950.pdf)
>  Transfer learning and meta-learning offer some of the most promising avenues to unlock the scalability of healthcare and consumer technologies driven by biosignal data. This is because current methods cannot generalise well across human subjects' data and handle learning from different heterogeneously collected data sets, thus limiting the scale of training data. On the other side, developments in transfer learning would benefit significantly from a real-world benchmark with immediate practical application. Therefore, we pick electroencephalography (EEG) as an exemplar for what makes biosignal machine learning hard. We design two transfer learning challenges around diagnostics and Brain-Computer-Interfacing (BCI), that have to be solved in the face of low signal-to-noise ratios, major variability among subjects, differences in the data recording sessions and techniques, and even between the specific BCI tasks recorded in the dataset. Task 1 is centred on the field of medical diagnostics, addressing automatic sleep stage annotation across subjects. Task 2 is centred on Brain-Computer Interfacing (BCI), addressing motor imagery decoding across both subjects and data sets. The BEETL competition with its over 30 competing teams and its 3 winning entries brought attention to the potential of deep transfer learning and combinations of set theory and conventional machine learning techniques to overcome the challenges. The results set a new state-of-the-art for the real-world BEETL benchmark.      
### 56.Multi-View Fusion Transformer for Sensor-Based Human Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.12949.pdf)
>  As a fundamental problem in ubiquitous computing and machine learning, sensor-based human activity recognition (HAR) has drawn extensive attention and made great progress in recent years. HAR aims to recognize human activities based on the availability of rich time-series data collected from multi-modal sensors such as accelerometers and gyroscopes. However, recent deep learning methods are focusing on one view of the data, i.e., the temporal view, while shallow methods tend to utilize the hand-craft features for recognition, e.g., the statistics view. In this paper, to extract a better feature for advancing the performance, we propose a novel method, namely multi-view fusion transformer (MVFT) along with a novel attention mechanism. First, MVFT encodes three views of information, i.e., the temporal, frequent, and statistical views to generate multi-view features. Second, the novel attention mechanism uncovers inner- and cross-view clues to catalyze mutual interactions between three views for detailed relation modeling. Moreover, extensive experiments on two datasets illustrate the superiority of our methods over several state-of-the-art methods.      
### 57.DAGAM: A Domain Adversarial Graph Attention Model for Subject Independent EEG-Based Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2202.12948.pdf)
>  One of the most significant challenges of EEG-based emotion recognition is the cross-subject EEG variations, leading to poor performance and generalizability. This paper proposes a novel EEG-based emotion recognition model called the domain adversarial graph attention model (DAGAM). The basic idea is to generate a graph to model multichannel EEG signals using biological topology. Graph theory can topologically describe and analyze relationships and mutual dependency between channels of EEG. Then, unlike other graph convolutional networks, self-attention pooling is applied to benefit salient EEG feature extraction from the graph, which effectively improves the performance. Finally, after graph pooling, the domain adversarial based on the graph is employed to identify and handle EEG variation across subjects, efficiently reaching good generalizability. We conduct extensive evaluations on two benchmark datasets (SEED and SEED IV) and obtain state-of-the-art results in subject-independent emotion recognition. Our model boosts the SEED accuracy to 92.59% (4.69% improvement) with the lowest standard deviation of 3.21% (2.92% decrements) and SEED IV accuracy to 80.74% (6.90% improvement) with the lowest standard deviation of 4.14% (3.88% decrements) respectively.      
### 58.Arrhythmia Classifier Using Convolutional Neural Network with Adaptive Loss-aware Multi-bit Networks Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2202.12943.pdf)
>  Cardiovascular disease (CVDs) is one of the universal deadly diseases, and the detection of it in the early stage is a challenging task to tackle. Recently, deep learning and convolutional neural networks have been employed widely for the classification of objects. Moreover, it is promising that lots of networks can be deployed on wearable devices. An increasing number of methods can be used to realize ECG signal classification for the sake of arrhythmia detection. However, the existing neural networks proposed for arrhythmia detection are not hardware-friendly enough due to a remarkable quantity of parameters resulting in memory and power consumption. <br>In this paper, we present a 1-D adaptive loss-aware quantization, achieving a high compression rate that reduces memory consumption by 23.36 times. In order to adapt to our compression method, we need a smaller and simpler network. We propose a 17 layer end-to-end neural network classifier to classify 17 different rhythm classes trained on the MIT-BIH dataset, realizing a classification accuracy of 93.5%, which is higher than most existing methods. Due to the adaptive bitwidth method making important layers get more attention and offered a chance to prune useless parameters, the proposed quantization method avoids accuracy degradation. It even improves the accuracy rate, which is 95.84%, 2.34% higher than before. Our study achieves a 1-D convolutional neural network with high performance and low resources consumption, which is hardware-friendly and illustrates the possibility of deployment on wearable devices to realize a real-time arrhythmia diagnosis.      
### 59.Quadratic phase wave packet transform  [ :arrow_down: ](https://arxiv.org/pdf/2202.12942.pdf)
>  The quadratic phase Fourier transform has gained much popularity in recent years because of its applications in image and signal processing. However, the QPFT is inadequate for localizing the quadratic phase spectrum which is required in some applications. In this paper, the quadratic phase wave packet transform QP WPT is proposed to address this problem, based on the wave packet transform WPT and QPFT. Firstly, we propose the definition of the QP WPT and gave its relation with windowed Fourier transform WFT. Secondly, several notable inequalities and important properties of newly defined QP WPT, such as boundedness, reconstruction formula, Moyals formula, Reproducing kernel are derived. Finally, we formulate several classes of uncertainty inequalities such as Leibs uncertainty principle, logarithmic uncertainty inequality and the Heisenberg uncertainty inequality.      
### 60.Digital Signal Analysis based on Convolutional Neural Networks for Active Target Time Projection Chambers  [ :arrow_down: ](https://arxiv.org/pdf/2202.12941.pdf)
>  An algorithm for digital signal analysis using convolutional neural networks (CNN) was developed in this work. The main objective of this algorithm is to make the analysis of experiments with active target time projection chambers more efficient. The code is divided in three steps: baseline correction, signal deconvolution and peak detection and integration. The CNNs were able to learn the signal processing models with relative errors of less than 6\%. The analysis based on CNNs provides the same results as the traditional deconvolution algorithms, but considerably more efficient in terms of computing time (about 65 times faster). This opens up new possibilities to improve existing codes and to simplify the analysis of the large amount of data produced in active target experiments.      
### 61.Fully-integrated multipurpose microwave frequency identification system on a single chip  [ :arrow_down: ](https://arxiv.org/pdf/2202.12940.pdf)
>  We demonstrate a fully-integrated multipurpose microwave frequency identification system on silicon-on-insulator platform. Thanks to its multipurpose features, the chip is able to identify different types of microwave signals, including single-frequency, multiple-frequency, chirped and frequency-hopping microwave signals, as well as discriminate instantaneous frequency variation among the frequency-modulated signals. This demonstration exhibits fully integrated solution and fully functional microwave frequency identification, which can meet the requirements in reduction of size, weight and power for future advanced microwave photonic processor.      
### 62.Automated Extraction of Energy Systems Information from Remotely Sensed Data: A Review and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2202.12939.pdf)
>  High quality energy systems information is a crucial input to energy systems research, modeling, and decision-making. Unfortunately, precise information about energy systems is often of limited availability, incomplete, or only accessible for a substantial fee or through a non-disclosure agreement. Recently, remotely sensed data (e.g., satellite imagery, aerial photography) have emerged as a potentially rich source of energy systems information. However, the use of these data is frequently challenged by its sheer volume and complexity, precluding manual analysis. Recent breakthroughs in machine learning have enabled automated and rapid extraction of useful information from remotely sensed data, facilitating large-scale acquisition of critical energy system variables. Here we present a systematic review of the literature on this emerging topic, providing an in-depth survey and review of papers published within the past two decades. We first taxonomize the existing literature into ten major areas, spanning the energy value chain. Within each research area, we distill and critically discuss major features that are relevant to energy researchers, including, for example, key challenges regarding the accessibility and reliability of the methods. We then synthesize our findings to identify limitations and trends in the literature as a whole, and discuss opportunities for innovation.      
### 63.Assessing the State of Self-Supervised Human Activity Recognition using Wearables  [ :arrow_down: ](https://arxiv.org/pdf/2202.12938.pdf)
>  The emergence of self-supervised learning in the field of wearables-based human activity recognition (HAR) has opened up opportunities to tackle the most pressing challenges in the field, namely to exploit unlabeled data to derive reliable recognition systems from only small amounts of labeled training samples. Furthermore, self-supervised methods enable a host of new application domains such as, for example, domain adaptation and transfer across sensor positions, activities etc. As such, self-supervision, i.e., the paradigm of 'pretrain-then-finetune' has the potential to become a strong alternative to the predominant end-to-end training approaches, let alone the classic activity recognition chain with hand-crafted features of sensor data. Recently a number of contributions have been made that introduced self-supervised learning into the field of HAR, including, Multi-task self-supervision, Masked Reconstruction, CPC to name but a few. With the initial success of these methods, the time has come for a systematic inventory and analysis of the potential self-supervised learning has for the field. This paper provides exactly that. We assess the progress of self-supervised HAR research by introducing a framework that performs a multi-faceted exploration of model performance. We organize the framework into three dimensions, each containing three constituent criteria, and utilize it to assess state-of-the-art self-supervised learning methods in a large empirical study on a curated set of nine diverse benchmarks. This exploration leads us to the formulation of insights into the properties of these techniques and to establish their value towards learning representations for diverse scenarios. Based on our findings we call upon the community to join our efforts and to contribute towards shaping the evaluation of the ongoing paradigm change in modeling human activities from body-worn sensor data.      
### 64.An Evaluation of the EEG alpha-to-theta and theta-to-alpha band Ratios as Indexes of Mental Workload  [ :arrow_down: ](https://arxiv.org/pdf/2202.12937.pdf)
>  Many research works indicate that EEG bands, specifically the alpha and theta bands, have been potentially helpful cognitive load indicators. However, minimal research exists to validate this claim. This study aims to assess and analyze the impact of the alpha-to-theta and the theta-to-alpha band ratios on supporting the creation of models capable of discriminating self-reported perceptions of mental workload. A dataset of raw EEG data was utilized in which 48 subjects performed a resting activity and an induced task demanding exercise in the form of a multitasking SIMKAP test. Band ratios were devised from frontal and parietal electrode clusters. Building and model testing was done with high-level independent features from the frequency and temporal domains extracted from the computed ratios over time. Target features for model training were extracted from the subjective ratings collected after resting and task demand activities. Models were built by employing Logistic Regression, Support Vector Machines and Decision Trees and were evaluated with performance measures including accuracy, recall, precision and f1-score. The results indicate high classification accuracy of those models trained with the high-level features extracted from the alpha-to-theta ratios and theta-to-alpha ratios. Preliminary results also show that models trained with logistic regression and support vector machines can accurately classify self-reported perceptions of mental workload. This research contributes to the body of knowledge by demonstrating the richness of the information in the temporal, spectral and statistical domains extracted from the alpha-to-theta and theta-to-alpha EEG band ratios for the discrimination of self-reported perceptions of mental workload.      
### 65.Automated Parkinson's Disease Detection and Affective Analysis from Emotional EEG Signals  [ :arrow_down: ](https://arxiv.org/pdf/2202.12936.pdf)
>  While Parkinson's disease (PD) is typically characterized by motor disorder, there is evidence of diminished emotion perception in PD patients. This study examines the utility of affective Electroencephalography (EEG) signals to understand emotional differences between PD vs Healthy Controls (HC), and for automated PD detection. Employing traditional machine learning and deep learning methods, we explore (a) dimensional and categorical emotion recognition, and (b) PD vs HC classification from emotional EEG signals. Our results reveal that PD patients comprehend arousal better than valence, and amongst emotion categories, \textit{fear}, \textit{disgust} and \textit{surprise} less accurately, and \textit{sadness} most accurately. Mislabeling analyses confirm confounds among opposite-valence emotions with PD data. Emotional EEG responses also achieve near-perfect PD vs HC recognition. {Cumulatively, our study demonstrates that (a) examining \textit{implicit} responses alone enables (i) discovery of valence-related impairments in PD patients, and (ii) differentiation of PD from HC, and (b) emotional EEG analysis is an ecologically-valid, effective, facile and sustainable tool for PD diagnosis vis-á-vis self reports, expert assessments and resting-state analysis.}      
### 66.Semi-Supervised Learning and Data Augmentation in Wearable-based Momentary Stress Detection in the Wild  [ :arrow_down: ](https://arxiv.org/pdf/2202.12935.pdf)
>  Physiological and behavioral data collected from wearable or mobile sensors have been used to estimate self-reported stress levels. Since the stress annotation usually relies on self-reports during the study, a limited amount of labeled data can be an obstacle in developing accurate and generalized stress predicting models. On the other hand, the sensors can continuously capture signals without annotations. This work investigates leveraging unlabeled wearable sensor data for stress detection in the wild. We first applied data augmentation techniques on the physiological and behavioral data to improve the robustness of supervised stress detection models. Using an auto-encoder with actively selected unlabeled sequences, we pre-trained the supervised model structure to leverage the information learned from unlabeled samples. Then, we developed a semi-supervised learning framework to leverage the unlabeled data sequences. We combined data augmentation techniques with consistency regularization, which enforces the consistency of prediction output based on augmented and original unlabeled data. We validated these methods using three wearable/mobile sensor datasets collected in the wild. Our results showed that combining the proposed methods improved stress classification performance by 7.7% to 13.8% on the evaluated datasets, compared to the baseline supervised learning models.      
### 67.Modulation and signal class labelling using active learning and classification using machine learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.12930.pdf)
>  Supervised learning in machine learning (ML) requires labelled data set. Further real-time data classification requires an easily available methodology for labelling. Wireless modulation and signal classification find their application in plenty of areas such as military, commercial and electronic reconaissance and cognitive radio. This paper mainly aims to solve the problem of real-time wireless modulation and signal class labelling with an active learning framework. Further modulation and signal classification is performed with machine learning algorithms such as KNN, SVM, Naive bayes. Active learning helps in labelling the data points belonging to different classes with the least amount of data samples trained. An accuracy of 86 percent is obtained by the active learning algorithm for the signal with SNR 18 dB. Further, KNN based model for modulation and signal classification performs well over range of SNR, and an accuracy of 99.8 percent is obtained for 18 dB signal. The novelty of this work exists in applying active learning for wireless modulation and signal class labelling. Both modulation and signal classes are labelled at a given time with help of couplet formation from the data samples.      
### 68.Deep, Deep Learning with BART  [ :arrow_down: ](https://arxiv.org/pdf/2202.14005.pdf)
>  Purpose: To develop a deep-learning-based image reconstruction framework for reproducible research in MRI. <br>Methods: The BART toolbox offers a rich set of implementations of calibration and reconstruction algorithms for parallel imaging and compressed sensing. In this work, BART was extended by a non-linear operator framework that provides automatic differentiation to allow computation of gradients. Existing MRI-specific operators of BART, such as the non-uniform fast Fourier transform, are directly integrated into this framework and are complemented by common building blocks used in neural networks. To evaluate the use of the framework for advanced deep-learning-based reconstruction, two state-of-the-art unrolled reconstruction networks, namely the Variational Network [1] and MoDL [2], were implemented. <br>Results: State-of-the-art deep image-reconstruction networks can be constructed and trained using BART's gradient based optimization algorithms. The BART implementation achieves a similar performance in terms of training time and reconstruction quality compared to the original implementations based on TensorFlow. <br>Conclusion: By integrating non-linear operators and neural networks into BART, we provide a general framework for deep-learning-based reconstruction in MRI.      
### 69.Implementation Technologies of an Advanced Cloud-based System for Distribution Operations  [ :arrow_down: ](https://arxiv.org/pdf/2202.13954.pdf)
>  Today's era is characterized as the "digital transformation era". Digital processes and information systems are used in every aspect of social and business activity. The use of information technology over the internet is so extensive that we interact with it daily without even recognizing it. The technological advances can offer a plethora of improvements for the supply chain processes, especially in the field of distribution planning and execution. The scope of this paper is to present the technological content of an advanced routing and scheduling system for transportation and delivery of goods. The system focuses on the routing and scheduling problem in urban areas, as city logistics have become a complex environment for companies to deliver their goods. The presented system deals with both static and dynamic routing and scheduling problems. More specifically, the system can create initial routing plans based on orders, available vehicles, time windows, and traffic forecasting data. Afterwards, during the execution of the plans, the system can monitor the fleet, detect deviations from the original plans, and finally, perform rerouting operations when needed. After a brief presentation of the system's modules and functionality, the paper describes thoroughly the technologies used to develop the system. The technological elements of the system are integrated into a cloud environment offering a system that is easy to maintain and can effectively support logistics companies' distribution activities. The system is provided as a Software as a Service with data being maintained on a central host and processed on the cloud. Therefore, logistics companies that decide to implement it can achieve faster, more accurate and more cost-efficient distribution activities while ensuring better customer service.      
### 70.Localization via Multiple Reconfigurable Intelligent Surfaces Equipped with Single Receive RF Chains  [ :arrow_down: ](https://arxiv.org/pdf/2202.13939.pdf)
>  The extra degrees of freedom resulting from the consideration of Reconfigurable Intelligent Surfaces (RISs) for smart signal propagation can be exploited for high accuracy localization and tracking. In this paper, capitalizing on a recent RIS hardware architecture incorporating a single receive Radio Frequency (RF) chain for measurement collection, we present a user localization method with multiple RISs. The proposed method includes an initial step for direction estimation at each RIS, followed by maximum likelihood position estimation, which is initialized with a least squares line intersection technique. Our numerical results showcase the accuracy of the proposed localization, verifying our theoretical estimation analysis.      
### 71.Nonlinear Model Predictive Control and System Identification for a Dual-hormone Artificial Pancreas  [ :arrow_down: ](https://arxiv.org/pdf/2202.13938.pdf)
>  In this work, we present a switching nonlinear model predictive control (NMPC) algorithm for a dual-hormone artificial pancreas (AP), and we use maximum likelihood estimation (MLE) to identify model parameters. A dual-hormone AP consists of a continuous glucose monitor (CGM), a control algorithm, an insulin pump, and a glucagon pump. The AP is designed with a heuristic to switch between insulin and glucagon as well as state-dependent constraints. We extend an existing glucoregulatory model with glucagon and exercise for simulation, and we use a simpler model for control. We test the AP (NMPC and MLE) using in silico numerical simulations on 50 virtual people with type 1 diabetes. The system is identified for each virtual person based on data generated with the simulation model. The simulations show a mean of 89.3% time in range (3.9-10 mmol/L) and no hypoglycemic events.      
### 72.Leveraging Channel Noise for Sampling and Privacy via Quantized Federated Langevin Monte Carlo  [ :arrow_down: ](https://arxiv.org/pdf/2202.13932.pdf)
>  For engineering applications of artificial intelligence, Bayesian learning holds significant advantages over standard frequentist learning, including the capacity to quantify uncertainty. Langevin Monte Carlo (LMC) is an efficient gradient-based approximate Bayesian learning strategy that aims at producing samples drawn from the posterior distribution of the model parameters. Prior work focused on a distributed implementation of LMC over a multi-access wireless channel via analog modulation. In contrast, this paper proposes quantized federated LMC (FLMC), which integrates one-bit stochastic quantization of the local gradients with channel-driven sampling. Channel-driven sampling leverages channel noise for the purpose of contributing to Monte Carlo sampling, while also serving the role of privacy mechanism. Analog and digital implementations of wireless LMC are compared as a function of differential privacy (DP) requirements, revealing the advantages of the latter at sufficiently high signal-to-noise ratio.      
### 73.High-performance Uncertainty Quantification in Large-scale Virtual Clinical Trials of Closed-loop Diabetes Treatment  [ :arrow_down: ](https://arxiv.org/pdf/2202.13927.pdf)
>  In this paper, we propose a virtual clinical trial for assessing the performance and identifying risks in closed-loop diabetes treatments. Virtual clinical trials enable fast and risk-free tests of many treatment variations for large populations of fictive patients (represented by mathematical models). We use closed-loop Monte Carlo simulation, implemented in high-performance software and hardware, to quantify the uncertainty in treatment performance as well as to compare the performance in different scenarios or of different closed-loop treatments. Our software can be used for testing a wide variety of control strategies ranging from heuristical approaches to nonlinear model predictive control. We present an example of a virtual clinical trial with one million patients over 52 weeks, and we use high-performance software and hardware to conduct the virtual trial in 1 h and 22 min.      
### 74.A Novel Viewport-Adaptive Motion Compensation Technique for Fisheye Video  [ :arrow_down: ](https://arxiv.org/pdf/2202.13892.pdf)
>  Although fisheye cameras are in high demand in many application areas due to their large field of view, many image and video signal processing tasks such as motion compensation suffer from the introduced strong radial distortions. A recently proposed projection-based approach takes the fisheye projection into account to improve fisheye motion compensation. However, the approach does not consider the large field of view of fisheye lenses that requires the consideration of different motion planes in 3D space. We propose a novel viewport-adaptive motion compensation technique that applies the motion vectors in different perspective viewports in order to realize these motion planes. Thereby, some pixels are mapped to so-called virtual image planes and require special treatment to obtain reliable mappings between the perspective viewports and the original fisheye image. While the state-of-the-art ultra wide-angle compensation is sufficiently accurate, we propose a virtual image plane compensation that leads to perfect mappings. All in all, we achieve average gains of +2.40 dB in terms of PSNR compared to the state of the art in fisheye motion compensation.      
### 75.Simulating Network Paths with Recurrent Buffering Units  [ :arrow_down: ](https://arxiv.org/pdf/2202.13870.pdf)
>  Simulating physical network paths (e.g., Internet) is a cornerstone research problem in the emerging sub-field of AI-for-networking. We seek a model that generates end-to-end packet delay values in response to the time-varying load offered by a sender, which is typically a function of the previously output delays. We formulate an ML problem at the intersection of dynamical systems, sequential decision making, and time-series generative modeling. We propose a novel grey-box approach to network simulation that embeds the semantics of physical network path in a new RNN-style architecture called Recurrent Buffering Unit, providing the interpretability of standard network simulator tools, the power of neural models, the efficiency of SGD-based techniques for learning, and yielding promising results on synthetic and real-world network traces.      
### 76.On the relevance of bandwidth extension for speaker identification  [ :arrow_down: ](https://arxiv.org/pdf/2202.13865.pdf)
>  In this paper we discuss the relevance of bandwidth extension for speaker identification tasks. Mainly we want to study if it is possible to recognize voices that have been bandwith extended. For this purpose, we created two different databases (microphonic and ISDN) of speech signals that were bandwidth extended from telephone bandwidth ([300, 3400] Hz) to full bandwidth ([100, 8000] Hz). We have evaluated different parameterizations, and we have found that the MELCEPST parameterization can take advantage of the bandwidth extension algorithms in several situations.      
### 77.Variable Rate Compression for Raw 3D Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2202.13862.pdf)
>  In this paper, we propose a novel variable rate deep compression architecture that operates on raw 3D point cloud data. The majority of learning-based point cloud compression methods work on a downsampled representation of the data. Moreover, many existing techniques require training multiple networks for different compression rates to generate consolidated point clouds of varying quality. In contrast, our network is capable of explicitly processing point clouds and generating a compressed description at a comprehensive range of bitrates. Furthermore, our approach ensures that there is no loss of information as a result of the voxelization process and the density of the point cloud does not affect the encoder/decoder performance. An extensive experimental evaluation shows that our model obtains state-of-the-art results, it is computationally efficient, and it can work directly with point cloud data thus avoiding an expensive voxelized representation.      
### 78.OUR-GAN: One-shot Ultra-high-Resolution Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.13799.pdf)
>  We propose OUR-GAN, the first one-shot ultra-high-resolution (UHR) image synthesis framework that generates non-repetitive images with 4K or higher resolution from a single training image. OUR-GAN generates a visually coherent image at low resolution and then gradually increases the resolution by super-resolution. Since OUR-GAN learns from a real UHR image, it can synthesize large-scale shapes with fine details while maintaining long-range coherence, which is difficult with conventional generative models that generate large images based on the patch distribution learned from relatively small images. OUR-GAN applies seamless subregion-wise super-resolution that synthesizes 4k or higher UHR images with limited memory, preventing discontinuity at the boundary. Additionally, OUR-GAN improves visual coherence maintaining diversity by adding vertical positional embeddings to the feature maps. In experiments on the ST4K and RAISE datasets, OUR-GAN exhibited improved fidelity, visual coherency, and diversity compared with existing methods. The synthesized images are presented at <a class="link-external link-https" href="https://anonymous-62348.github.io" rel="external noopener nofollow">this https URL</a>.      
### 79.A GNSS Aided Initial Alignment Method for MEMS-IMU Based on Backtracking Algorithm and Backward Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2202.13700.pdf)
>  To obtain a high-accuracy position with SINS(Strapdown Inertial Navigation System), initial alignment needs to determine initial attitude rapidly and accurately. High-accuracy grade IMU(Inertial Measurement Uint) can obtain the initial attitude indenpendently, however, the low-accuracy grade gyroscope doesn't adapt to determine the heading angle, hence the initial attitude matrix will not be obtained. If using large misalignment angle model to estiamting heading angle, the convergence time will become much longer. For solving these two problems, a novel alignment algorithm combined backtracking algorithm and reverse navigation updating method with GNSS(Global Navigation Satellite System) aiding is proposed herein. The simulation and land vehicle test were finished to evaluate the alignment accuracy of the proposed algorithm. The horizontal misalignment is less than 2.3 arcmin and the heading misalignment is less than 10.1 arcmin in test. The proposed algorithm is a feasible and practical alignment method for low-cost IMU to obtain initial attitude in short term and large misalignment condition aided by GNSS.      
### 80.Recent Advances and Challenges in Deep Audio-Visual Correlation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.13673.pdf)
>  Audio-visual correlation learning aims to capture essential correspondences and understand natural phenomena between audio and video. With the rapid growth of deep learning, an increasing amount of attention has been paid to this emerging research issue. Through the past few years, various methods and datasets have been proposed for audio-visual correlation learning, which motivate us to conclude a comprehensive survey. This survey paper focuses on state-of-the-art (SOTA) models used to learn correlations between audio and video, but also discusses some tasks of definition and paradigm applied in AI multimedia. In addition, we investigate some objective functions frequently used for optimizing audio-visual correlation learning models and discuss how audio-visual data is exploited in the optimization process. Most importantly, we provide an extensive comparison and summarization of the recent progress of SOTA audio-visual correlation learning and discuss future research directions.      
### 81.Restless Multi-Armed Bandits under Exogenous Global Markov Process  [ :arrow_down: ](https://arxiv.org/pdf/2202.13665.pdf)
>  We consider an extension to the restless multi-armed bandit (RMAB) problem with unknown arm dynamics, where an unknown exogenous global Markov process governs the rewards distribution of each arm. Under each global state, the rewards process of each arm evolves according to an unknown Markovian rule, which is non-identical among different arms. At each time, a player chooses an arm out of N arms to play, and receives a random reward from a finite set of reward states. The arms are restless, that is, their local state evolves regardless of the player's actions. The objective is an arm-selection policy that minimizes the regret, defined as the reward loss with respect to a player that knows the dynamics of the problem, and plays at each time t the arm that maximizes the expected immediate value. We develop the Learning under Exogenous Markov Process (LEMP) algorithm, that achieves a logarithmic regret order with time, and a finite-sample bound on the regret is established. Simulation results support the theoretical study and demonstrate strong performances of LEMP.      
### 82.GPU-Accelerated Policy Optimization via Batch Automatic Differentiation of Gaussian Processes for Real-World Control  [ :arrow_down: ](https://arxiv.org/pdf/2202.13638.pdf)
>  The ability of Gaussian processes (GPs) to predict the behavior of dynamical systems as a more sample-efficient alternative to parametric models seems promising for real-world robotics research. However, the computational complexity of GPs has made policy search a highly time and memory consuming process that has not been able to scale to larger problems. In this work, we develop a policy optimization method by leveraging fast predictive sampling methods to process batches of trajectories in every forward pass, and compute gradient updates over policy parameters by automatic differentiation of Monte Carlo evaluations, all on GPU. We demonstrate the effectiveness of our approach in training policies on a set of reference-tracking control experiments with a heavy-duty machine. Benchmark results show a significant speedup over exact methods and showcase the scalability of our method to larger policy networks, longer horizons, and up to thousands of trajectories with a sublinear drop in speed.      
### 83.Changeable Rate and Novel Quantization for CSI Feedback Based on Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2202.13627.pdf)
>  Deep learning (DL)-based channel state information (CSI) feedback improves the capacity and energy efficiency of massive multiple-input multiple-output (MIMO) systems in frequency division duplexing mode. However, multiple neural networks with different lengths of feedback overhead are required by time-varying bandwidth resources. The storage space required at the user equipment (UE) and the base station (BS) for these models increases linearly with the number of models. In this paper, we propose a DL-based changeable-rate framework with novel quantization scheme to improve the efficiency and feasibility of CSI feedback systems. This framework can reutilize all the network layers to achieve overhead-changeable CSI feedback to optimize the storage efficiency at the UE and the BS sides. Designed quantizer in this framework can avoid the normalization and gradient problems faced by traditional quantization schemes. Specifically, we propose two DL-based changeable-rate CSI feedback networks CH-CsiNetPro and CH-DualNetSph by introducing a feedback overhead control unit. Then, a pluggable quantization block (PQB) is developed to further improve the encoding efficiency of CSI feedback in an end-to-end way. Compared with existing CSI feedback methods, the proposed framework saves the storage space by about 50% with changeable-rate scheme and improves the encoding efficiency with the quantization module.      
### 84.Optimizing Information Freshness in RIS-assisted NOMA-based IoT Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.13572.pdf)
>  This paper investigates the benefits of integrating reconfigurable intelligent surface (RIS) on minimizing the average sum age of information (AoI) in uplink non-orthogonal multiple access-based Internet-of-Things (IoT) networks. In this setup, an optimization problem is formulated to optimize the RIS configuration, the transmit power per IoT device and the clustering policy of IoT devices. The formulated problem is a mixed-integer non-convex one, and in order to solve it we obtain first the RIS configuration by adopting a semi-definite relaxation (SDR) approach. Afterwards, the joint power allocation and user-clustering problem is solved using the concept of bi-level optimization and is decomposed into an outer user clustering problem and an inner power allocation problem. Optimal closed-form expressions are derived for the inner problem and the Hungarian method is employed to solve the outer one. Numerical results demonstrate the performance superiority of our approach.      
### 85.Name Your Style: An Arbitrary Artist-aware Image Style Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2202.13562.pdf)
>  Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a text-driven image style transfer (TxST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient attention module that explores cross-attentions to fuse style and content features. Finally, we achieve an arbitrary artist-aware image style transfer to learn and transfer specific artistic characters such as Picasso, oil painting, or a rough sketch. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods on both image and textual styles. Moreover, it can mimic the styles of one or many artists to achieve attractive results, thus highlighting a promising direction in image style transfer.      
### 86.Pattern Based Multivariate Regression using Deep Learning (PBMR-DP)  [ :arrow_down: ](https://arxiv.org/pdf/2202.13541.pdf)
>  We propose a deep learning methodology for multivariate regression that is based on pattern recognition that triggers fast learning over sensor data. We used a conversion of sensors-to-image which enables us to take advantage of Computer Vision architectures and training processes. In addition to this data preparation methodology, we explore the use of state-of-the-art architectures to generate regression outputs to predict agricultural crop continuous yield information. Finally, we compare with some of the top models reported in MLCAS2021. We found that using a straightforward training process, we were able to accomplish a MAE of 4.394, RMSE of 5.945, and R^2 of 0.861.      
### 87.Sparse Graph Learning with Eigen-gap for Spectral Filter Training in Graph Convolutional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2202.13526.pdf)
>  It is now known that the expressive power of graph convolutional neural nets (GCN) does not grow infinitely with the number of layers. Instead, the GCN output approaches a subspace spanned by the first eigenvector of the normalized graph Laplacian matrix with the convergence rate characterized by the "eigen-gap": the difference between the Laplacian's first two distinct eigenvalues. To promote a deeper GCN architecture with sufficient expressiveness, in this paper, given an empirical covariance matrix $\bar{C}$ computed from observable data, we learn a sparse graph Laplacian matrix $L$ closest to $\bar{C}^{-1}$ while maintaining a desirable eigen-gap that slows down convergence. Specifically, we first define a sparse graph learning problem with constraints on the first eigenvector (the most common signal) and the eigen-gap. We solve the corresponding dual problem greedily, where a locally optimal eigen-pair is computed one at a time via a fast approximation of a semi-definite programming (SDP) formulation. The computed $L$ with the desired eigen-gap is normalized spectrally and used for supervised training of GCN for a targeted task. Experiments show that our proposal produced deeper GCNs and smaller errors compared to a competing scheme without explicit eigen-gap optimization.      
### 88.Scalable Simulation and Demonstration of Jumping Piezoelectric 2-D Soft Robots  [ :arrow_down: ](https://arxiv.org/pdf/2202.13521.pdf)
>  Soft robots have drawn great interest due to their ability to take on a rich range of shapes and motions, compared to traditional rigid robots. However, the motions, and underlying statics and dynamics, pose significant challenges to forming well-generalized and robust models necessary for robot design and control. In this work, we demonstrate a five-actuator soft robot capable of complex motions and develop a scalable simulation framework that reliably predicts robot motions. The simulation framework is validated by comparing its predictions to experimental results, based on a robot constructed from piezoelectric layers bonded to a steel-foil substrate. The simulation framework exploits the physics engine PyBullet, and employs discrete rigid-link elements connected by motors to model the actuators. We perform static and AC analyses to validate a single-unit actuator cantilever setup and observe close agreement between simulation and experiments for both the cases. The analyses are extended to the five-actuator robot, where simulations accurately predict the static and AC robot motions, including shapes for applied DC voltage inputs, nearly-static "inchworm" motion, and jumping (in vertical as well as vertical and horizontal directions). These motions exhibit complex non-linear behavior, with forward robot motion reaching ~1 cm/s. Our open-source code can be found at: <a class="link-external link-https" href="https://github.com/zhiwuz/sfers" rel="external noopener nofollow">this https URL</a>.      
### 89.Limitations of Deep Learning for Inverse Problems on Digital Hardware  [ :arrow_down: ](https://arxiv.org/pdf/2202.13490.pdf)
>  Deep neural networks have seen tremendous success over the last years. Since the training is performed on digital hardware, in this paper, we analyze what actually can be computed on current hardware platforms modeled as Turing machines, which would lead to inherent restrictions of deep learning. For this, we focus on the class of inverse problems, which, in particular, encompasses any task to reconstruct data from measurements. We prove that finite-dimensional inverse problems are not Banach-Mazur computable for small relaxation parameters. In fact, our result even holds for Borel-Turing computability., i.e., there does not exist an algorithm which performs the training of a neural network on digital hardware for any given accuracy. This establishes a conceptual barrier on the capabilities of neural networks for finite-dimensional inverse problems given that the computations are performed on digital hardware.      
### 90.Fourier--Hermite Dynamic Programming for Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2202.13453.pdf)
>  In this paper, we propose a novel computational method for solving non-linear optimal control problems. The method is based on the use of Fourier--Hermite series for approximating the action-value function arising in dynamic programming instead of the conventional Taylor series expansion used in differential dynamic programming (DDP). The coefficients of the Fourier--Hermite series can be numerically computed by using sigma-point methods, which leads to a novel class of sigma-point based dynamic programming methods. We also prove the quadratic convergence of the method and experimentally test its performance against other methods.      
### 91.Transformer-based Knowledge Distillation for Efficient Semantic Segmentation of Road-driving Scenes  [ :arrow_down: ](https://arxiv.org/pdf/2202.13393.pdf)
>  For scene understanding in robotics and automated driving, there is a growing interest in solving semantic segmentation tasks with transformer-based methods. However, effective transformers are always too cumbersome and computationally expensive to solve semantic segmentation in real time, which is desired for robotic systems. Moreover, due to the lack of inductive biases compared to Convolutional Neural Networks (CNNs), pre-training on a large dataset is essential but it takes a long time. Knowledge Distillation (KD) speeds up inference and maintains accuracy while transferring knowledge from a pre-trained cumbersome teacher model to a compact student model. Most traditional KD methods for CNNs focus on response-based knowledge and feature-based knowledge. In contrast, we present a novel KD framework according to the nature of transformers, i.e., training compact transformers by transferring the knowledge from feature maps and patch embeddings of large transformers. To this purpose, two modules are proposed: (1) the Selective Kernel Fusion (SKF) module, which helps to construct an efficient relation-based KD framework, Selective Kernel Review (SKR); (2) the Patch Embedding Alignment (PEA) module, which performs the dimensional transformation of patch embeddings. The combined KD framework is called SKR+PEA. Through comprehensive experiments on Cityscapes and ACDC datasets, it indicates that our proposed approach outperforms recent state-of-the-art KD frameworks and rivals the time-consuming pre-training method. Code will be made publicly available at <a class="link-external link-https" href="https://github.com/RuipingL/SKR_PEA.git" rel="external noopener nofollow">this https URL</a>      
### 92.PanoFlow: Learning Optical Flow for Panoramic Images  [ :arrow_down: ](https://arxiv.org/pdf/2202.13388.pdf)
>  Optical flow estimation is a basic task in self-driving and robotics systems, which enables to temporally interpret the traffic scene. Autonomous vehicles clearly benefit from the ultra-wide Field of View (FoV) offered by 360-degree panoramic sensors. However, due to the unique imaging process of panoramic images, models designed for pinhole images do not directly generalize satisfactorily to 360-degree panoramic images. In this paper, we put forward a novel network framework--PanoFlow, to learn optical flow for panoramic images. To overcome the distortions introduced by equirectangular projection in panoramic transformation, we design a Flow Distortion Augmentation (FDA) method. We further propose a Cyclic Flow Estimation (CFE) method by leveraging the cyclicity of spherical images to infer 360-degree optical flow and converting large displacement to relatively small displacement. PanoFlow is applicable to any existing flow estimation method and benefit from the progress of narrow-FoV flow estimation. In addition, we create and release a synthetic panoramic dataset Flow360 based on CARLA to facilitate training and quantitative analysis. PanoFlow achieves state-of-the-art performance. Our proposed approach reduces the End-Point-Error (EPE) on the established Flow360 dataset by 26%. On the public OmniFlowNet dataset, PanoFlow achieves an EPE of 3.34 pixels, a 53.1% error reduction from the best published result (7.12 pixels). We also validate our method via an outdoor collection vehicle, indicating strong potential and robustness for real-world navigation applications. Code and dataset are publicly available at <a class="link-external link-https" href="https://github.com/MasterHow/PanoFlow" rel="external noopener nofollow">this https URL</a>.      
### 93.A one-size-fits-all artificial pancreas for people with type 1 diabetes based on physiological insight and feedback control  [ :arrow_down: ](https://arxiv.org/pdf/2202.13338.pdf)
>  We propose a model-free artificial pancreas (AP) for people with type 1 diabetes. The algorithmic parameters are tuned to a virtual population of 1,000,000 individuals, and the AP repeatedly estimates the basal and bolus insulin requirements necessary for maintaining normal blood glucose levels. Therefore, the AP can be used without healthcare personnel or engineers customizing the algorithm to each user. The estimates are based on bodyweight, measurements from a continuous glucose monitor (CGM), and estimates of the meal carbohydrate contents. In a virtual clinical trial with all 1,000,000 individuals (i.e., a Monte Carlo closed-loop simulation), the AP achieves a mean time in range of more than 87% and almost 89% of the participants satisfy several glycemic targets.      
### 94.Deep Learning-Based Inverse Design for Engineering Systems: Multidisciplinary Design Optimization of Automotive Brakes  [ :arrow_down: ](https://arxiv.org/pdf/2202.13309.pdf)
>  The braking performance of the brake system is a target performance that must be considered for vehicle development. Apparent piston travel (APT) and drag torque are the most representative factors for evaluating braking performance. In particular, as the two performance factors have a conflicting relationship with each other, a multidisciplinary design optimization (MDO) approach is required for brake design. However, the computational cost of MDO increases as the number of disciplines increases. Recent studies on inverse design that use deep learning (DL) have established the possibility of instantly generating an optimal design that can satisfy the target performance without implementing an iterative optimization process. This study proposes a DL-based multidisciplinary inverse design (MID) that simultaneously satisfies multiple targets, such as the APT and drag torque of the brake system. Results show that the proposed inverse design can find the optimal design more efficiently compared with the conventional optimization methods, such as backpropagation and sequential quadratic programming. The MID achieved a similar performance to the single-disciplinary inverse design in terms of accuracy and computational cost. A novel design was derived on the basis of results, and the same performance was satisfied as that of the existing design.      
### 95.Private and Decentralized Location Sharing for Congestion-Aware Routing  [ :arrow_down: ](https://arxiv.org/pdf/2202.13305.pdf)
>  Data-driven methodologies offer promising ways to improve the efficiency, safety, and adaptability of modern and future mobility systems. While these methodologies offer many exciting upsides, they also introduce new challenges, particularly in the realm of user privacy. Specifically, data-driven algorithms often require user data, and the way this data is collected can pose privacy risks to end users. Centralized data sharing systems where a single entity (mobility service provider or transit network operator) collects and manages user data have a central point of failure. Users have to trust that this entity will not sell or use their data to infer sensitive private information. Unfortunately, in practice many advertising companies offer to buy such data for the sake of targeted advertisements. <br>With this as motivation, we study the problem of using location data for congestion-aware routing in a privacy-preserving way. Rather than having users report their location to a central operator, we present a protocol in which users participate in a decentralized and privacy-preserving computation to estimate travel times for the roads in the network in a way that no individuals' location is ever observed by any other party. The protocol uses the Laplace mechanism in conjunction with secure multi-party computation to ensure that it is cryptogrpahically secure and that its output is differentially private. <br>A natural question is if privacy necessitates degradation in accuracy or system performance. We show that if a road has sufficiently high capacity, then the travel time estimated by our protocol is provably close to the travel time estimated by the ground truth. We also evaluate the protocol through numerical experiments which show that the protocol provides privacy guarantees with minimal overhead to system performance.      
### 96.Texture Characterization of Histopathologic Images Using Ecological Diversity Measures and Discrete Wavelet Transform  [ :arrow_down: ](https://arxiv.org/pdf/2202.13270.pdf)
>  Breast cancer is a health problem that affects mainly the female population. An early detection increases the chances of effective treatment, improving the prognosis of the disease. In this regard, computational tools have been proposed to assist the specialist in interpreting the breast digital image exam, providing features for detecting and diagnosing tumors and cancerous cells. Nonetheless, detecting tumors with a high sensitivity rate and reducing the false positives rate is still challenging. Texture descriptors have been quite popular in medical image analysis, particularly in histopathologic images (HI), due to the variability of both the texture found in such images and the tissue appearance due to irregularity in the staining process. Such variability may exist depending on differences in staining protocol such as fixation, inconsistency in the staining condition, and reagents, either between laboratories or in the same laboratory. Textural feature extraction for quantifying HI information in a discriminant way is challenging given the distribution of intrinsic properties of such images forms a non-deterministic complex system. This paper proposes a method for characterizing texture across HIs with a considerable success rate. By employing ecological diversity measures and discrete wavelet transform, it is possible to quantify the intrinsic properties of such images with promising accuracy on two HI datasets compared with state-of-the-art methods.      
### 97.Hierarchical Linear Dynamical System for Representing Notes from Recorded Audio  [ :arrow_down: ](https://arxiv.org/pdf/2202.13255.pdf)
>  We seek to develop simultaneous segmentation and classification of notes from audio recordings in presence of outliers. The selected architecture for modeling time series is hierarchical linear dynamical system (HLDS). We propose a novel method for its parameter setting. HLDS can potentially be employed in two ways: 1) simultaneous segmentation and clustering for exploring data, i.e. finding unknown notes, 2) simultaneous segmentation and classification of audio recording for finding the notes of interest in the presence of outliers. We adapted HLDS for the second purpose since it is an easier task and still a challenging problem, e.g. in the field of bioacoustics. Each test clip has the same notes (but different instances) as of the training clip and also contain outlier notes. At test, it is automatically decided to which class of interest a note belongs to if any. Two applications of this work are to the fields of bioacoustics for detection of animal sounds in audio field recordings and also to musicology. Experiments have been conducted for segmentation and classification of both avian and musical notes from recorded audio.      
### 98.Regional-Local Adversarially Learned One-Class Classifier Anomalous Sound Detection in Global Long-Term Space  [ :arrow_down: ](https://arxiv.org/pdf/2202.13245.pdf)
>  Anomalous sound detection (ASD) is one of the most significant tasks of mechanical equipment monitoring and maintaining in complex industrial systems. In practice, it is vital to precisely identify abnormal status of the working mechanical system, which can further facilitate the failure troubleshooting. In this paper, we propose a multi-pattern adversarial learning one-class classification framework, which allows us to use both the generator and the discriminator of an adversarial model for efficient ASD. The core idea is learning to reconstruct the normal patterns of acoustic data through two different patterns of auto-encoding generators, which succeeds in extending the fundamental role of a discriminator from identifying real and fake data to distinguishing between regional and local pattern reconstructions. Furthermore, we present a global filter layer for long-term interactions in the frequency domain space, which directly learns from the original data without introducing any human priors. Extensive experiments performed on four real-world datasets from different industrial domains (three cavitation datasets provided by SAMSON AG, and one existing publicly) for anomaly detection show superior results, and outperform recent state-of-the-art ASD methods.      
### 99.Fixed Point Iterations for SURE-based PSF Estimation for Image Deconvolution  [ :arrow_down: ](https://arxiv.org/pdf/2202.13242.pdf)
>  Stein's unbiased risk estimator (SURE) has been shown to be an effective metric for determining optimal parameters for many applications. The topic of this article is focused on the use of SURE for determining parameters for blind deconvolution. The parameters include those that define the shape of the point spread function (PSF), as well as regularization parameters in the deconvolution formulas. Within this context, the optimal parameters are typically determined via a brute for search over the feasible parameter space. When multiple parameters are involved, this parameter search is prohibitively costly due to the curse of dimensionality. In this work, novel fixed point iterations are proposed for optimizing these parameters, which allows for rapid estimation of a relatively large number of parameters. We demonstrate that with some mild tuning of the optimization parameters, these fixed point methods typically converge to the ideal PSF parameters in relatively few iterations, e.g. 50-100, with each iteration requiring very low computational cost.      
### 100.An acoustic signal cavitation detection framework based on XGBoost with adaptive selection feature engineering  [ :arrow_down: ](https://arxiv.org/pdf/2202.13226.pdf)
>  Valves are widely used in industrial and domestic pipeline systems. However, during their operation, they may suffer from the occurrence of the cavitation, which can cause loud noise, vibration and damage to the internal components of the valve. Therefore, monitoring the flow status inside valves is significantly beneficial to prevent the additional cost induced by cavitation. In this paper, a novel acoustic signal cavitation detection framework--based on XGBoost with adaptive selection feature engineering--is proposed. Firstly, a data augmentation method with non-overlapping sliding window (NOSW) is developed to solve small-sample problem involved in this study. Then, the each segmented piece of time-domain acoustic signal is transformed by fast Fourier transform (FFT) and its statistical features are extracted to be the input to the adaptive selection feature engineering (ASFE) procedure, where the adaptive feature aggregation and feature crosses are performed. Finally, with the selected features the XGBoost algorithm is trained for cavitation detection and tested on valve acoustic signal data provided by Samson AG (Frankfurt). Our method has achieved state-of-the-art results. The prediction performance on the binary classification (cavitation and no-cavitation) and the four-class classification (cavitation choked flow, constant cavitation, incipient cavitation and no-cavitation) are satisfactory and outperform the traditional XGBoost by 4.67% and 11.11% increase of the accuracy.      
### 101.How much depth information can radar infer and contribute  [ :arrow_down: ](https://arxiv.org/pdf/2202.13220.pdf)
>  Since the release of radar data in large scale autonomous driving dataset, many works have been proposed fusing radar data as an additional guidance signal into monocular depth estimation models. Although positive performances are reported, it is still hard to tell how much depth information radar can infer and contribute in depth estimation models. In this paper, we conduct two experiments to investigate the intrinsic depth capability of radar data using state-of-the-art depth estimation models. Our experiments demonstrate that the estimated depth from only sparse radar input can detect the shape of surroundings to a certain extent. Furthermore, the monocular depth estimation model supervised by preprocessed radar only during training can achieve 70% performance in delta_1 score compared to the baseline model trained with sparse lidar.      
### 102.Kinematic Control of Redundant Robots with Online Handling of Variable Generalized Hard Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2202.13184.pdf)
>  We present a generalized version of the Saturation in the Null Space (SNS) algorithm for the task control of redundant robots when hard inequality constraints are simultaneously present both in the joint and in the Cartesian space. These hard bounds should never be violated, are treated equally and in a unified way by the algorithm, and may also be varied, inserted or deleted online. When a joint/Cartesian bound saturates, the robot redundancy is exploited to continue fulfilling the primary task. If no feasible solution exists, an optimal scaling procedure is applied to enforce directional consistency with the original task. Simulation and experimental results on different robotic systems demonstrate the efficiency of the approach. The proposed algorithm can be viewed as a generic platform that is easily applicable to any robotic application in which robots operate in an unstructured environment and online handling of joint and Cartesian constraints is critical.      
### 103.High Dimensional Statistical Estimation under One-bit Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2202.13157.pdf)
>  Compared with data with high precision, one-bit (binary) data are preferable in many applications because of the efficiency in signal storage, processing, transmission, and enhancement of privacy. In this paper, we study three fundamental statistical estimation problems, i.e., sparse covariance matrix estimation, sparse linear regression, and low-rank matrix completion via binary data arising from an easy-to-implement one-bit quantization process that contains truncation, dithering and quantization as typical steps. Under both sub-Gaussian and heavy-tailed regimes, new estimators that handle high-dimensional scaling are proposed. In sub-Gaussian case, we show that our estimators achieve minimax rates up to logarithmic factors, hence the quantization nearly costs nothing from the perspective of statistical learning rate. In heavy-tailed case, we truncate the data before dithering to achieve a bias-variance trade-off, which results in estimators embracing convergence rates that are the square root of the corresponding minimax rates. Experimental results on synthetic data are reported to support and demonstrate the statistical properties of our estimators under one-bit quantization.      
### 104.Impact of Interference Subtraction on Grant-Free Multiple Access with Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2202.13156.pdf)
>  The design of highly scalable multiple access schemes is a main challenge in the evolution towards future massive machine-type communications, where reliability and latency constraints must be ensured to a large number of uncoordinated devices. In this scenario, coded random access (CRA) schemes, where successive interference cancellation algorithms allow large improvements with respect to classical random access protocols, have recently attracted an increasing interest. Impressive performance can be potentially obtained by combining CRA with massive multiple input multiple output (MIMO). In this paper we provide an analysis of such schemes focusing on the effects of imperfect channel estimation on successive interference cancellation. Based on the analysis we then propose an innovative signal processing algorithm for CRA in massive MIMO systems.      
### 105.Integrating Text Inputs For Training and Adapting RNN Transducer ASR Models  [ :arrow_down: ](https://arxiv.org/pdf/2202.13155.pdf)
>  Compared to hybrid automatic speech recognition (ASR) systems that use a modular architecture in which each component can be independently adapted to a new domain, recent end-to-end (E2E) ASR system are harder to customize due to their all-neural monolithic construction. In this paper, we propose a novel text representation and training framework for E2E ASR models. With this approach, we show that a trained RNN Transducer (RNN-T) model's internal LM component can be effectively adapted with text-only data. An RNN-T model trained using both speech and text inputs improves over a baseline model trained on just speech with close to 13% word error rate (WER) reduction on the Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation. The usefulness of the proposed approach is further demonstrated by customizing this general purpose RNN-T model to three separate datasets. We observe 20-45% relative word error rate (WER) reduction in these settings with this novel LM style customization technique using only unpaired text data from the new domains.      
### 106.Content-Variant Reference Image Quality Assessment via Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2202.13123.pdf)
>  Generally, humans are more skilled at perceiving differences between high-quality (HQ) and low-quality (LQ) images than directly judging the quality of a single LQ image. This situation also applies to image quality assessment (IQA). Although recent no-reference (NR-IQA) methods have made great progress to predict image quality free from the reference image, they still have the potential to achieve better performance since HQ image information is not fully exploited. In contrast, full-reference (FR-IQA) methods tend to provide more reliable quality evaluation, but its practicability is affected by the requirement for pixel-level aligned reference images. To address this, we firstly propose the content-variant reference method via knowledge distillation (CVRKD-IQA). Specifically, we use non-aligned reference (NAR) images to introduce various prior distributions of high-quality images. The comparisons of distribution differences between HQ and LQ images can help our model better assess the image quality. Further, the knowledge distillation transfers more HQ-LQ distribution difference information from the FR-teacher to the NAR-student and stabilizing CVRKD-IQA performance. Moreover, to fully mine the local-global combined information, while achieving faster inference speed, our model directly processes multiple image patches from the input with the MLP-mixer. Cross-dataset experiments verify that our model can outperform all NAR/NR-IQA SOTAs, even reach comparable performance with FR-IQA methods on some occasions. Since the content-variant and non-aligned reference HQ images are easy to obtain, our model can support more IQA applications with its relative robustness to content variations. Our code and more detailed elaborations of supplements are available: <a class="link-external link-https" href="https://github.com/guanghaoyin/CVRKD-IQA" rel="external noopener nofollow">this https URL</a>.      
### 107.Mobile Device Association and Resource Allocation in Small-Cell IoT Networks with Mobile Edge Computing and Caching  [ :arrow_down: ](https://arxiv.org/pdf/2202.13116.pdf)
>  To meet the need of computation-sensitive (CS) and high-rate (HR) communications, the framework of mobile edge computing and caching has been widely regarded as a promising solution. When such a framework is implemented in small-cell IoT (Internet of Tings) networks, it is a key and open topic how to assign mobile edge computing and caching servers to mobile devices (MDs) with CS and HR communications. Since these servers are integrated into small base stations (BSs), the assignment of them refers to not only the BS selection (i.e., MD association), but also the selection of computing and caching modes. To mitigate the network interference and thus enhance the system performance, some highly-effective resource partitioning mechanisms are introduced for access and backhaul links firstly. After that a problem with minimizing the sum of MDs' weighted delays is formulated to attain a goal of joint MD association and resource allocation under limited resources. Considering that the MD association and resource allocation parameters are coupling in such a formulated problem, we develop an alternating optimization algorithm according to the coalitional game and convex optimization theorems. To ensure that the designed algorithm begins from a feasible initial solution, we develop an initiation algorithm according to the conventional best channel association, which is used for comparison and the input of coalition game in the simulation. Simulation results show that the algorithm designed for minimizing the sum of MDs' weighted delays may achieve a better performance than the initiation (best channel association) algorithm in general.      
### 108.Language-Independent Speaker Anonymization Approach using Self-Supervised Pre-Trained Models  [ :arrow_down: ](https://arxiv.org/pdf/2202.13097.pdf)
>  Speaker anonymization aims to protect the privacy of speakers while preserving spoken linguistic information from speech. Current mainstream neural network speaker anonymization systems are complicated, containing an F0 extractor, speaker encoder, automatic speech recognition acoustic model (ASR AM), speech synthesis acoustic model and speech waveform generation model. Moreover, as an ASR AM is language-dependent, trained on English data, it is hard to adapt it into another language. In this paper, we propose a simpler self-supervised learning (SSL)-based method for language-independent speaker anonymization without any explicit language-dependent model, which can be easily used for other languages. Extensive experiments were conducted on the VoicePrivacy Challenge 2020 datasets in English and AISHELL-3 datasets in Mandarin to demonstrate the effectiveness of our proposed SSL-based language-independent speaker anonymization method.      
### 109.Visual Speech Recognition for Multiple Languages in the Wild  [ :arrow_down: ](https://arxiv.org/pdf/2202.13084.pdf)
>  Visual speech recognition (VSR) aims to recognise the content of speech based on the lip movements without relying on the audio stream. Advances in deep learning and the availability of large audio-visual datasets have led to the development of much more accurate and robust VSR models than ever before. However, these advances are usually due to larger training sets rather than the model design. In this work, we demonstrate that designing better models is equally important to using larger training sets. We propose the addition of prediction-based auxiliary tasks to a VSR model and highlight the importance of hyper-parameter optimisation and appropriate data augmentations. We show that such model works for different languages and outperforms all previous methods trained on publicly available datasets by a large margin. It even outperforms models that were trained on non-publicly available datasets containing up to to 21 times more data. We show furthermore that using additional training data, even in other languages or with automatically generated transcriptions, results in further improvement.      
### 110.SWIS: Self-Supervised Representation Learning For Writer Independent Offline Signature Verification  [ :arrow_down: ](https://arxiv.org/pdf/2202.13078.pdf)
>  Writer independent offline signature verification is one of the most challenging tasks in pattern recognition as there is often a scarcity of training data. To handle such data scarcity problem, in this paper, we propose a novel self-supervised learning (SSL) framework for writer independent offline signature verification. To our knowledge, this is the first attempt to utilize self-supervised setting for the signature verification task. The objective of self-supervised representation learning from the signature images is achieved by minimizing the cross-covariance between two random variables belonging to different feature directions and ensuring a positive cross-covariance between the random variables denoting the same feature direction. This ensures that the features are decorrelated linearly and the redundant information is discarded. Through experimental results on different data sets, we obtained encouraging results.      
### 111.Near Optimal Reconstruction of Spherical Harmonic Expansions  [ :arrow_down: ](https://arxiv.org/pdf/2202.12995.pdf)
>  We propose an algorithm for robust recovery of the spherical harmonic expansion of functions defined on the d-dimensional unit sphere $\mathbb{S}^{d-1}$ using a near-optimal number of function evaluations. We show that for any $f \in L^2(\mathbb{S}^{d-1})$, the number of evaluations of $f$ needed to recover its degree-$q$ spherical harmonic expansion equals the dimension of the space of spherical harmonics of degree at most $q$ up to a logarithmic factor. Moreover, we develop a simple yet efficient algorithm to recover degree-$q$ expansion of $f$ by only evaluating the function on uniformly sampled points on $\mathbb{S}^{d-1}$. Our algorithm is based on the connections between spherical harmonics and Gegenbauer polynomials and leverage score sampling methods. Unlike the prior results on fast spherical harmonic transform, our proposed algorithm works efficiently using a nearly optimal number of samples in any dimension d. We further illustrate the empirical performance of our algorithm on numerical examples.      
### 112.Refining Self-Supervised Learning in Imaging: Beyond Linear Metric  [ :arrow_down: ](https://arxiv.org/pdf/2202.12921.pdf)
>  We introduce in this paper a new statistical perspective, exploiting the Jaccard similarity metric, as a measure-based metric to effectively invoke non-linear features in the loss of self-supervised contrastive learning. Specifically, our proposed metric may be interpreted as a dependence measure between two adapted projections learned from the so-called latent representations. This is in contrast to the cosine similarity measure in the conventional contrastive learning model, which accounts for correlation information. To the best of our knowledge, this effectively non-linearly fused information embedded in the Jaccard similarity, is novel to self-supervision learning with promising results. The proposed approach is compared to two state-of-the-art self-supervised contrastive learning methods on three image datasets. We not only demonstrate its amenable applicability in current ML problems, but also its improved performance and training efficiency.      
### 113.Learning English with Peppa Pig  [ :arrow_down: ](https://arxiv.org/pdf/2202.12917.pdf)
>  Attempts to computationally simulate the acquisition of spoken language via grounding in perception have a long tradition but have gained momentum in the past few years. Current neural approaches exploit associations between the spoken and visual modality and learn to represent speech and visual data in a joint vector space. A major unresolved issue from the point of ecological validity is the training data, typically consisting of images or videos paired with spoken descriptions of what is depicted. Such a setup guarantees an unrealistically strong correlation between speech and the visual world. In the real world the coupling between the linguistic and the visual is loose, and often contains confounds in the form of correlations with non-semantic aspects of the speech signal. The current study is a first step towards simulating a naturalistic grounding scenario by using a dataset based on the children's cartoon Peppa Pig. We train a simple bi-modal architecture on the portion of the data consisting of naturalistic dialog between characters, and evaluate on segments containing descriptive narrations. Despite the weak and confounded signal in this training data our model succeeds at learning aspects of the visual semantics of spoken language.      
