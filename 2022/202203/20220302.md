# ArXiv eess --Wed, 2 Mar 2022
### 1.Multi-Task Multi-Scale Learning For Outcome Prediction in 3D PET Images  [ :arrow_down: ](https://arxiv.org/pdf/2203.00641.pdf)
>  Background and Objectives: Predicting patient response to treatment and survival in oncology is a prominent way towards precision medicine. To that end, radiomics was proposed as a field of study where images are used instead of invasive methods. The first step in radiomic analysis is the segmentation of the lesion. However, this task is time consuming and can be physician subjective. Automated tools based on supervised deep learning have made great progress to assist physicians. However, they are data hungry, and annotated data remains a major issue in the medical field where only a small subset of annotated images is available. Methods: In this work, we propose a multi-task learning framework to predict patient's survival and response. We show that the encoder can leverage multiple tasks to extract meaningful and powerful features that improve radiomics performance. We show also that subsidiary tasks serve as an inductive bias so that the model can better generalize. Results: Our model was tested and validated for treatment response and survival in lung and esophageal cancers, with an area under the ROC curve of 77% and 71% respectively, outperforming single task learning methods. Conclusions: We show that, by using a multi-task learning approach, we can boost the performance of radiomic analysis by extracting rich information of intratumoral and peritumoral regions.      
### 2.Distributional Reinforcement Learning for Scheduling of (Bio)chemical Production Processes  [ :arrow_down: ](https://arxiv.org/pdf/2203.00636.pdf)
>  Reinforcement Learning (RL) has recently received significant attention from the process systems engineering and control communities. Recent works have investigated the application of RL to identify optimal scheduling decision in the presence of uncertainty. In this work, we present a RL methodology to address precedence and disjunctive constraints as commonly imposed on production scheduling problems. This work naturally enables the optimization of risk-sensitive formulations such as the conditional value-at-risk (CVaR), which are essential in realistic scheduling processes. The proposed strategy is investigated thoroughly in a single-stage, parallel batch production environment, and benchmarked against mixed integer linear programming (MILP) strategies. We show that the policy identified by our approach is able to account for plant uncertainties in online decision-making, with expected performance comparable to existing MILP methods. Additionally, the framework gains the benefits of optimizing for risk-sensitive measures, and identifies decisions orders of magnitude faster than the most efficient optimization approaches. This promises to mitigate practical issues and ease in handling realizations of process uncertainty in the paradigm of online production scheduling.      
### 3.Full RGB Just Noticeable Difference (JND) Modelling  [ :arrow_down: ](https://arxiv.org/pdf/2203.00629.pdf)
>  Just Noticeable Difference (JND) has many applications in multimedia signal processing, especially for visual data processing up to date. It's generally defined as the minimum visual content changes that the human can perspective, which has been studied for decades. However, most of the existing methods only focus on the luminance component of JND modelling and simply regard chrominance components as scaled versions of luminance. In this paper, we propose a JND model to generate the JND by taking the characteristics of full RGB channels into account, termed as the RGB-JND. To this end, an RGB-JND-NET is proposed, where the visual content in full RGB channels is used to extract features for JND generation. To supervise the JND generation, an adaptive image quality assessment combination (AIC) is developed. Besides, the RDB-JND-NET also takes the visual attention into account by automatically mining the underlying relationship between visual attention and the JND, which is further used to constrain the JND spatial distribution. To the best of our knowledge, this is the first work on careful investigation of JND modelling for full-color space. Experimental results demonstrate that the RGB-JND-NET model outperforms the relevant state-of-the-art JND models. Besides, the JND of the red and blue channels are larger than that of the green one according to the experimental results of the proposed model, which demonstrates that more changes can be tolerated in the red and blue channels, in line with the well-known fact that the human visual system is more sensitive to the green channel in comparison with the red and blue ones.      
### 4.Towards a unified view of unsupervised non-local methods for image denoising: the NL-Ridge approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.00570.pdf)
>  We propose a unified view of unsupervised non-local methods for image denoising that linearily combine noisy image patches. The best methods, established in different modeling and estimation frameworks, are two-step algorithms. Leveraging Stein's unbiased risk estimate (SURE) for the first step and the "internal adaptation", a concept borrowed from deep learning theory, for the second one, we show that our NL-Ridge approach enables to reconcile several patch aggregation methods for image denoising. In the second step, our closed-form aggregation weights are computed through multivariate Ridge regressions. Experiments on artificially noisy images demonstrate that NL-Ridge may outperform well established state-of-the-art unsupervised denoisers such as BM3D and NL-Bayes, as well as recent unsupervised deep learning methods, while being simpler conceptually.      
### 5.Towards deep learning-powered IVF: A large public benchmark for morphokinetic parameter prediction  [ :arrow_down: ](https://arxiv.org/pdf/2203.00531.pdf)
>  An important limitation to the development of Artificial Intelligence (AI)-based solutions for In Vitro Fertilization (IVF) is the absence of a public reference benchmark to train and evaluate deep learning (DL) models. In this work, we describe a fully annotated dataset of 756 videos of developing embryos, for a total of 337k images. We applied ResNet, LSTM, and ResNet-3D architectures to our dataset and demonstrate that they overperform algorithmic approaches to automatically annotate stage development phases. Altogether, we propose the first public benchmark that will allow the community to evaluate morphokinetic models. This is the first step towards deep learning-powered IVF. Of note, we propose highly detailed annotations with 16 different development phases, including early cell division phases, but also late cell divisions, phases after morulation, and very early phases, which have never been used before. We postulate that this original approach will help improve the overall performance of deep learning approaches on time-lapse videos of embryo development, ultimately benefiting infertile patients with improved clinical success rates (Code and data are available at <a class="link-external link-https" href="https://gitlab.univ-nantes.fr/E144069X/bench_mk_pred.git" rel="external noopener nofollow">this https URL</a>).      
### 6.Multi-task Learning Approach for Modulation and Wireless Signal Classification for 5G and Beyond: Edge Deployment via Model Compression  [ :arrow_down: ](https://arxiv.org/pdf/2203.00517.pdf)
>  Future communication networks must address the scarce spectrum to accommodate extensive growth of heterogeneous wireless devices. Wireless signal recognition is becoming increasingly more significant for spectrum monitoring, spectrum management, secure communications, among others. Consequently, comprehensive spectrum awareness on the edge has the potential to serve as a key enabler for the emerging beyond 5G networks. State-of-the-art studies in this domain have (i) only focused on a single task - modulation or signal (protocol) classification - which in many cases is insufficient information for a system to act on, (ii) consider either radar or communication waveforms (homogeneous waveform category), and (iii) does not address edge deployment during neural network design phase. In this work, for the first time in the wireless communication domain, we exploit the potential of deep neural networks based multi-task learning (MTL) framework to simultaneously learn modulation and signal classification tasks while considering heterogeneous wireless signals such as radar and communication waveforms in the electromagnetic spectrum. The proposed MTL architecture benefits from the mutual relation between the two tasks in improving the classification accuracy as well as the learning efficiency with a lightweight neural network model. We additionally include experimental evaluations of the model with over-the-air collected samples and demonstrate first-hand insight on model compression along with deep learning pipeline for deployment on resource-constrained edge devices. We demonstrate significant computational, memory, and accuracy improvement of the proposed model over two reference architectures. In addition to modeling a lightweight MTL model suitable for resource-constrained embedded radio platforms, we provide a comprehensive heterogeneous wireless signals dataset for public use.      
### 7.Mental State Classification Using Multi-graph Features  [ :arrow_down: ](https://arxiv.org/pdf/2203.00516.pdf)
>  We consider the problem of extracting features from passive, multi-channel electroencephalogram (EEG) devices for downstream inference tasks related to high-level mental states such as stress and cognitive load. Our proposed method leverages recently developed multi-graph tools and applies them to the time series of graphs implied by the statistical dependence structure (e.g., correlation) amongst the multiple sensors. We compare the effectiveness of the proposed features to traditional band power-based features in the context of three classification experiments and find that the two feature sets offer complementary predictive information. We conclude by showing that the importance of particular channels and pairs of channels for classification when using the proposed features is neuroscientifically valid.      
### 8.A Deep Bayesian Neural Network for Cardiac Arrhythmia Classification with Rejection from ECG Recordings  [ :arrow_down: ](https://arxiv.org/pdf/2203.00512.pdf)
>  With the development of deep learning-based methods, automated classification of electrocardiograms (ECGs) has recently gained much attention. Although the effectiveness of deep neural networks has been encouraging, the lack of information given by the outputs restricts clinicians' reexamination. If the uncertainty estimation comes along with the classification results, cardiologists can pay more attention to "uncertain" cases. Our study aims to classify ECGs with rejection based on data uncertainty and model uncertainty. We perform experiments on a real-world 12-lead ECG dataset. First, we estimate uncertainties using the Monte Carlo dropout for each classification prediction, based on our Bayesian neural network. Then, we accept predictions with uncertainty under a given threshold and provide "uncertain" cases for clinicians. Furthermore, we perform a simulation experiment using varying thresholds. Finally, with the help of a clinician, we conduct case studies to explain the results of large uncertainties and incorrect predictions with small uncertainties. The results show that correct predictions are more likely to have smaller uncertainties, and the performance on accepted predictions improves as the accepting ratio decreases (i.e. more rejections). Case studies also help explain why rejection can improve the performance. Our study helps neural networks produce more accurate results and provide information on uncertainties to better assist clinicians in the diagnosis process. It can also enable deep-learning-based ECG interpretation in clinical implementation.      
### 9.Wavelet-Based Multi-Class Seizure Type Classification System  [ :arrow_down: ](https://arxiv.org/pdf/2203.00511.pdf)
>  Epilepsy is one of the most common brain diseases that affect more than 1\% of the world's population. It is characterized by recurrent seizures, which come in different types and are treated differently. Electroencephalography (EEG) is commonly used in medical services to diagnose seizures and their types. The accurate identification of seizures helps to provide optimal treatment and accurate information to the patient. However, the manual diagnostic procedures of epileptic seizures are laborious and highly-specialized. Moreover, EEG manual evaluation is a process known to have a low inter-rater agreement among experts. This paper presents a novel automatic technique that involves extraction of specific features from EEG signals using Dual-tree Complex Wavelet Transform (DTCWT) and classifying them. We evaluated the proposed technique on TUH EEG Seizure Corpus (TUSZ) ver.1.5.2 dataset and compared the performance with existing state-of-the-art techniques using overall F1-score due to class imbalance seizure types. Our proposed technique achieved the best results of weighted F1-score of 99.1\% and 74.7\% for seizure-wise and patient-wise classification respectively, thereby setting new benchmark results for this dataset.      
### 10.Multi-Modal Recurrent Fusion for Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2203.00510.pdf)
>  This paper considers indoor localization using multi-modal wireless signals including Wi-Fi, inertial measurement unit (IMU), and ultra-wideband (UWB). By formulating the localization as a multi-modal sequence regression problem, a multi-stream recurrent fusion method is proposed to combine the current hidden state of each modality in the context of recurrent neural networks while accounting for the modality uncertainty which is directly learned from its own immediate past states. The proposed method was evaluated on the large-scale SPAWC2021 multi-modal localization dataset and compared with a wide range of baseline methods including the trilateration method, traditional fingerprinting methods, and convolution network-based methods.      
### 11.Analysis of Digitalized ECG Signals Based on Artificial Intelligence and Spectral Analysis Methods Specialized in ARVC  [ :arrow_down: ](https://arxiv.org/pdf/2203.00504.pdf)
>  Arrhythmogenic right ventricular cardiomyopathy (ARVC) is an inherited heart muscle disease that appears between the second and forth decade of a patient's life, being responsible for 20% of sudden cardiac deaths before the age of 35. The effective and punctual diagnosis of this disease based on Electrocardiograms (ECGs) could have a vital role in reducing premature cardiovascular mortality. In our analysis, we firstly outline the digitalization process of paper-based ECG signals enhanced by a spatial filter aiming to eliminate dark regions in the dataset's images that do not correspond to ECG waveform, producing undesirable noise. Next, we propose the utilization of a low-complexity convolutional neural network for the detection of an arrhythmogenic heart disease, that has not been studied through the usage of deep learning methodology to date, achieving high classification accuracy on a disease the major identification criterion of which are infinitesimal millivolt variations in the ECG's morphology, in contrast with other arrhythmogenic abnormalities. Finally, by performing spectral analysis we investigate significant differentiations in the field of frequencies between normal ECGs and ECGs corresponding to patients suffering from ARVC. The overall research carried out in this article highlights the importance of integrating mathematical methods into the examination and effective diagnosis of various diseases, aiming to a substantial contribution to their successful treatment.      
### 12.Gait Events Prediction using Hybrid CNN-RNN-based Deep Learning models through a Single Waist-worn Wearable Sensor  [ :arrow_down: ](https://arxiv.org/pdf/2203.00503.pdf)
>  Elderly gait is a source of rich information about their physical and mental health condition. As an alternative to the multiple sensors on the lower body parts, a single sensor on the pelvis has a positional advantage and an abundance of information acquirable. This study aimed to explore a way of improving the accuracy of gait event detection in the elderly using a single sensor on the waist and deep learning models. Data was gathered from elderly subjects equipped with three IMU sensors while they walked. The input was taken only from the waist sensor was used to train 16 deep-learning models including CNN, RNN, and CNN-RNN hybrid with or without the Bidirectional and Attention mechanism. The groundtruth was extracted from foot IMU sensors. Fairly high accuracy of 99.73% and 93.89% was achieved by the CNN-BiGRU-Att model at the tolerance window of $\pm$6TS ($\pm$6ms) and $\pm$1TS ($\pm$1ms) respectively. Advancing from the previous studies exploring gait event detection, the model showed a great improvement in terms of its prediction error having an MAE of 6.239ms and 5.24ms for HS and TO events respectively at the tolerance window of $\pm$1TS. The results showed that the use of CNN-RNN hybrid models with Attention and Bidirectional mechanisms is promising for accurate gait event detection using a single waist sensor. The study can contribute to reducing the burden of gait detection and increase its applicability in future wearable devices that can be used for remote health monitoring (RHM) or diagnosis based thereon.      
### 13.Sensor technologies in cancer research for new directions in diagnosis and treatment: and exploratory analysis  [ :arrow_down: ](https://arxiv.org/pdf/2203.00502.pdf)
>  The goal of this study is an exploratory analysis concerning main sensor technologies applied in cancer research to detect new directions in diagnosis and treatments. The study focused on types of cancer having a high incidence and mortality worldwide: breast, lung, colorectal and prostate. Data of the Web of Science (WOS) core collection database are used to retrieve articles related to sensor technologies and cancer research over 1991-2021 period. We utilized Gephi software version 0.9.2 to visualize the co-word networks of the interaction between sensor technologies and cancers under study. Results show main clusters of interaction per typology of cancer. Biosensor is the only type of sensor that plays an essential role in all types of cancer: breast cancer, lung cancer, prostate cancer, and colorectal cancer. Electrochemical sensor is applied in all types of cancer under study except lung cancer. Electrochemical biosensor is used in breast cancer, lung cancer, and prostate cancer research but not colorectal cancer. Optical sensor can also be considered one of the sensor technologies that significantly is used in breast cancer, prostate cancer, and colorectal cancer. This study shows that this type of sensor is applied in more diversified approaches. Moreover, the oxygen sensor is mostly studied in lung cancer and breast cancer due to the usage in breath analysis for the treatment process. Finally, Cmos sensor is a technology used mainly in lung cancer and colorectal cancer. Results here suggest new directions for the evolution of science and technology of sensors in cancer research to support innovation and research policy directed to new technological trajectories having a potential of accelerated growth and positive social impact for diagnosis and treatments of cancer.      
### 14.A Probabilistic Deep Image Prior for Computational Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2203.00479.pdf)
>  Existing deep-learning based tomographic image reconstruction methods do not provide accurate estimates of reconstruction uncertainty, hindering their real-world deployment. To address this limitation, we construct a Bayesian prior for tomographic reconstruction, which combines the classical total variation (TV) regulariser with the modern deep image prior (DIP). Specifically, we use a change of variables to connect our prior beliefs on the image TV semi-norm with the hyper-parameters of the DIP network. For the inference, we develop an approach based on the linearised Laplace method, which is scalable to high-dimensional settings. The resulting framework provides pixel-wise uncertainty estimates and a marginal likelihood objective for hyperparameter optimisation. We demonstrate the method on synthetic and real-measured high-resolution $\mu$CT data, and show that it provides superior calibration of uncertainty estimates relative to previous probabilistic formulations of the DIP.      
### 15.Modeling the Energy Consumption of the HEVC Decoding Process  [ :arrow_down: ](https://arxiv.org/pdf/2203.00466.pdf)
>  In this paper, we present a bit stream feature based energy model that accurately estimates the energy required to decode a given HEVC-coded bit stream. Therefore, we take a model from literature and extend it by explicitly modeling the inloop filters, which was not done before. Furthermore, to prove its superior estimation performance, it is compared to seven different energy models from literature. By using a unified evaluation framework we show how accurately the required decoding energy for different decoding systems can be approximated. We give thorough explanations on the model parameters and explain how the model variables are derived. To show the modeling capabilities in general, we test the estimation performance for different decoding software and hardware solutions, where we find that the proposed model outperforms the models from literature by reaching frame-wise mean estimation errors of less than 7% for software and less than 15% for hardware based systems.      
### 16.JOINED : Prior Guided Multi-task Learning for Joint Optic Disc/Cup Segmentation and Fovea Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.00461.pdf)
>  Fundus photography has been routinely used to document the presence and severity of various retinal degenerative diseases such as age-related macula degeneration, glaucoma, and diabetic retinopathy, for which the fovea, optic disc (OD), and optic cup (OC) are important anatomical landmarks. Identification of those anatomical landmarks is of great clinical importance. However, the presence of lesions, drusen, and other abnormalities during retinal degeneration severely complicates automatic landmark detection and segmentation. Most existing works treat the identification of each landmark as a single task and typically do not make use of any clinical prior information. In this paper, we present a novel method, named JOINED, for prior guided multi-task learning for joint OD/OC segmentation and fovea detection. An auxiliary branch for distance prediction, in addition to a segmentation branch and a detection branch, is constructed to effectively utilize the distance information from each image pixel to landmarks of interest. Our proposed JOINED pipeline consists of a coarse stage and a fine stage. At the coarse stage, we obtain the OD/OC coarse segmentation and the heatmap localization of fovea through a joint segmentation and detection module. Afterwards, we crop the regions of interest for subsequent fine processing and use predictions obtained at the coarse stage as additional information for better performance and faster convergence. Experimental results reveal that our proposed JOINED outperforms existing state-of-the-art approaches on the publicly-available GAMMA, PALM, and REFUGE datasets of fundus images. Furthermore, JOINED ranked the 5th on the OD/OC segmentation and fovea detection tasks in the GAMMA challenge hosted by the MICCAI2021 workshop OMIA8.      
### 17.Interblock redundancy reduction using QUADTREES  [ :arrow_down: ](https://arxiv.org/pdf/2203.00445.pdf)
>  This paper applies the quadtree structure for image coding. The goal is to adapt the block size and thus to increase the compression ratio (without reducing SNR). Also, the computational time is not significatively increased. It has been applied to Block Truncation Coding of still images, and motion vector coding (interframe). An inter/intraframe application is also discussed.      
### 18.Recovery of Missing Sensor Data by Reconstructing Time-varying Graph Signals  [ :arrow_down: ](https://arxiv.org/pdf/2203.00418.pdf)
>  Wireless sensor networks are among the most promising technologies of the current era because of their small size, lower cost, and ease of deployment. With the increasing number of wireless sensors, the probability of generating missing data also rises. This incomplete data could lead to disastrous consequences if used for decision-making. There is rich literature dealing with this problem. However, most approaches show performance degradation when a sizable amount of data is lost. Inspired by the emerging field of graph signal processing, this paper performs a new study of a Sobolev reconstruction algorithm in wireless sensor networks. Experimental comparisons on several publicly available datasets demonstrate that the algorithm surpasses multiple state-of-the-art techniques by a maximum margin of 54%. We further show that this algorithm consistently retrieves the missing data even during massive data loss situations.      
### 19.Beam-Shape Effects and Noise Removal from THz Time-Domain Images in Reflection Geometry in the 0.25-6 THz Range  [ :arrow_down: ](https://arxiv.org/pdf/2203.00417.pdf)
>  The increasing need of restoring high-resolution Hyper-Spectral (HS) images is determining a growing reliance on Computer Vision-based processing to enhance the clarity of the image content. HS images can, in fact, suffer from degradation effects or artefacts caused by instrument limitations. This paper focuses on a procedure aimed at reducing the degradation effects, frequency-dependent blur and noise, in Terahertz Time-Domain Spectroscopy (THz-TDS) images in reflection geometry. It describes the application of a joint deblurring and denoising approach that had been previously proved to be effective for the restoration of THz-TDS images in transmission geometry, but that had never been tested in reflection modality. This mode is often the only one that can be effectively used in most cases, for example when analyzing objects that are either opaque in the THz range, or that cannot be displaced from their location (e.g., museums), such as those of cultural interest. Compared to transmission mode, reflection geometry introduces, however, further distortion to THz data, neglected in existing literature. In this work, we successfully implement image deblurring and denoising of both uniform-shape samples (a contemporary 1 Euro cent coin and an inlaid pendant) and samples with the uneven reliefs and corrosion products on the surface which make the analysis of the object particularly complex (an ancient Roman silver coin). The study demonstrates the ability of image processing to restore data in the 0.25 - 6 THz range, spanning over more than four octaves, and providing the foundation for future analytical approaches of cultural heritage using the far-infrared spectrum still not sufficiently investigated in literature.      
### 20.A Multidisciplinary Approach to Optimal Communication and Flight Operation of High Altitude Long Endurance Platform  [ :arrow_down: ](https://arxiv.org/pdf/2203.00363.pdf)
>  Aerial communication platforms especially stratospheric high altitude pseudo-satellite (HAPS) has the potential to provide/catalyze advanced mobile wireless communication services with its ubiquitous connectivity and ultra-wide coverage radius. Recently, HAPS has gained immense popularity - achieved primarily through self-sufficient energy systems - to render long endurance characteristics. The photo voltaic cells mounted on the aircraft harvest solar energy during the day, which is partially used for communication and station keeping, whereas, the excess is stored in the rechargeable batteries for the night time operation. We carried out an adroit power budgeting to ascertain if the available solar power can simultaneously and efficiently self-sustain the requisite propulsion and communication power expense. We propose an energy optimum trajectory for station-keeping flight and non-orthogonal multiple access (NOMA) for users in multicells served by the directional beams from HAPS communication system. We design optimal power allocation for downlink (DL) NOMA users along with the ideal position and speed of flight with the aim to maximize sum data rate during the day and minimize power expenditure during the night while ensuring quality of service. Our findings reveal the significance of joint design of communication and aerodynamics parameters for optimum energy utilization and resource allocation.      
### 21.Scan-specific Self-supervised Bayesian Deep Non-linear Inversion for Undersampled MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2203.00361.pdf)
>  Magnetic resonance imaging is subject to slow acquisition times due to the inherent limitations in data sampling. Recently, supervised deep learning has emerged as a promising technique for reconstructing sub-sampled MRI. However, supervised deep learning requires a large dataset of fully-sampled data. Although unsupervised or self-supervised deep learning methods have emerged to address the limitations of supervised deep learning approaches, they still require a database of images. In contrast, scan-specific deep learning methods learn and reconstruct using only the sub-sampled data from a single scan. Current scan-specific approaches require a fully-sampled auto calibration scan region in k-space that cost additional scan time. Here, we introduce Scan-Specific Self-Supervised Bayesian Deep Non-Linear Inversion (DNLINV) that does not require an auto calibration scan region. DNLINV utilizes a deep image prior-type generative modeling approach and relies on approximate Bayesian inference to regularize the deep convolutional neural network. We demonstrate our approach on several anatomies, contrasts, and sampling patterns and show improved performance over existing approaches in scan-specific calibrationless parallel imaging and compressed sensing.      
### 22.Safe Control with Minimal Regret  [ :arrow_down: ](https://arxiv.org/pdf/2203.00358.pdf)
>  As we move towards safety-critical cyber-physical systems that operate in non-stationary and uncertain environments, it becomes crucial to close the gap between classical optimal control algorithms and adaptive learning-based methods. In this paper, we present an efficient optimization-based approach for computing a finite-horizon robustly safe control policy that minimizes dynamic regret, in the sense of the loss relative to the optimal sequence of control actions selected in hindsight by a clairvoyant controller. By leveraging the system level synthesis framework (SLS), our method extends recent results on regret minimization for the linear quadratic regulator to optimal control subject to hard safety constraints, and allows competing against a safety-aware clairvoyant policy with minor modifications. Numerical experiments confirm superior performance with respect to finite-horizon constrained $\mathcal{H}_2$ and $\mathcal{H}_\infty$ control laws when the disturbance realizations poorly fit classical assumptions.      
### 23.Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.00355.pdf)
>  Assessing the structure and function of the right ventricle (RV) is important in the diagnosis of several cardiac pathologies. However, it remains more challenging to segment the RV than the left ventricle (LV). In this paper, we focus on segmenting the RV in both short (SA) and long-axis (LA) cardiac MR images simultaneously. For this task, we propose a new multi-input/output architecture, hybrid 2D/3D geometric spatial TransformEr Multi-Pass fEature pyRAmid (Tempera). Our feature pyramid extends current designs by allowing not only a multi-scale feature output but multi-scale SA and LA input images as well. Tempera transfers learned features between SA and LA images via layer weight sharing and incorporates a geometric target transformer to map the predicted SA segmentation to LA space. Our model achieves an average Dice score of 0.836 and 0.798 for the SA and LA, respectively, and 26.31 mm and 31.19 mm Hausdorff distances. This opens up the potential for the incorporation of RV segmentation models into clinical workflows.      
### 24.Enhanced Image Reconstruction From Quarter Sampling Measurements Using An Adapted Very Deep Super Resolution Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.00336.pdf)
>  Quarter sampling is a novel sensor concept that enables the acquisition of higher resolution images without increasing the number of pixels. This is achieved by covering three quarters of each pixel of a low-resolution sensor such that only one quadrant of the sensor area of each pixel is sensitive to light. By randomly masking different parts, effectively a non-regular sampling of a higher resolution image is performed. Combining a properly designed mask and a high-quality reconstruction algorithm, a higher image quality can be achieved than using a low-resolution sensor and subsequent upsampling. For the latter case, the image quality can be enhanced using super resolution algorithms. Recently, algorithms based on machine learning such as the Very Deep Super Resolution network (VDSR) proofed to be successful for this task. In this work, we transfer the concepts of VDSR to the special case of quarter sampling. Besides adapting the network layout to take advantage of the case of quarter sampling, we introduce a novel data augmentation technique enabled by quarter sampling. Altogether, using the quarter sampling sensor, the image quality in terms of PSNR can be increased by +0.67 dB for the Urban 100 dataset compared to using a low-resolution sensor with VDSR.      
### 25.Design Techniques for Incremental Non-Regular Image Sampling Patterns  [ :arrow_down: ](https://arxiv.org/pdf/2203.00327.pdf)
>  Even though image signals are typically acquired on a regular two dimensional grid, there exist many scenarios where non-regular sampling is possible. Non-regular sampling can remove aliasing. In terms of the non-regular sampling patterns, there is a high degree of freedom in how to actually arrange the sampling positions. In literature, random patterns show higher reconstruction quality compared to regular patterns due to reduced aliasing effects. On the downside, random patterns feature large void areas which is also disadvantageous. In the scope of this work, we present two techniques to design optimized non-regular image sampling patterns for arbitrary sampling densities. Both techniques create incremental sampling patterns, i.e., one pixel position is added in each step until the desired sampling density is reached. Our proposed patterns increase the reconstruction quality by more than +0.5 dB in PSNR for a broad density range. Visual comparisons are provided.      
### 26.Graph Normalized-LMP Algorithm for Signal Estimation Under Impulsive Noise  [ :arrow_down: ](https://arxiv.org/pdf/2203.00320.pdf)
>  In this paper, we introduce an adaptive graph normalized least mean pth power (GNLMP) algorithm for graph signal processing (GSP) that utilizes GSP techniques, including bandlimited filtering and node sampling, to estimate sampled graph signals under impulsive noise. Different from least-squares-based algorithms, such as the adaptive GSP Least Mean Squares (GLMS) algorithm and the normalized GLMS (GNLMS) algorithm, the GNLMP algorithm has the ability to reconstruct a graph signal that is corrupted by non-Gaussian noise with heavy-tailed characteristics. Compared to the recently introduced adaptive GSP least mean pth power (GLMP) algorithm, the GNLMP algorithm reduces the number of iterations to converge to a steady graph signal. The convergence condition of the GNLMP algorithm is derived, and the ability of the GNLMP algorithm to process multidimensional time-varying graph signals with multiple features is demonstrated as well. Simulations show the performance of the GNLMP algorithm in estimating steady-state and time-varying graph signals is faster than GLMP and more robust in comparison to GLMS and GNLMS.      
### 27.Iterative Optimization of Quarter Sampling Masks for Non-Regular Sampling Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2203.00305.pdf)
>  Non-regular sampling can reduce aliasing at the expense of noise. Recently, it has been shown that non-regular sampling can be carried out using a conventional regular imaging sensor when the surface of its individual pixels is partially covered. This technique is called quarter sampling (also 1/4 sampling), since only one quarter of each pixel is sensitive to light. For this purpose, the choice of a proper sampling mask is crucial to achieve a high reconstruction quality. In the scope of this work, we present an iterative algorithm to improve an arbitrary quarter sampling mask which results in a continuous increase of the reconstruction quality. In terms of the reconstruction algorithms, we test two simple algorithms, namely, linear interpolation and nearest neighbor interpolation, as well as two more sophisticated algorithms, namely, steering kernel regression and frequency selective extrapolation. Besides PSNR gains of +0.31 dB to +0.68 dB relative to a random quarter sampling mask resulting from our optimized mask, visually noticeable enhancements are perceptible.      
### 28.PhyCOM: A Multi-Layer Parametric Network for Joint Linear Impairments Compensation and Symbol Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.00266.pdf)
>  In this paper, we focus on the joint impairments compensation and symbol detection problem in communication systems. First, we introduce a new multi-layer channel model that represents the underlying physics of multiple impairments in communication systems. This model is composed of widely linear parametric layers that describe the input-output relationship of the front-end impairments and channel effects. Using this particular model, we show that the joint compensation and zero-forcing detection problem can be solved by a particular feedforward network called PhyCOM. Because of the small number of network parameters, a PhyCOM network can be trained efficiently using sophisticated optimization algorithms and a limited number of pilot symbols. <br>Numerical examples are provided to demonstrate the effectiveness of PhyCOM networks with communication systems corrupted by transmitter and receiver IQ imbalances, carrier frequency offset, finite impulse response channels, and transmitter and receiver phase noise distortions. Compared to conventional digital signal processing approaches, simulation results show that the proposed technique is much more flexible and offers better statistical performance both in terms of MSE and SER with a moderate increase of the computation complexity.      
### 29.Joint Beamforming and Reflection Design for RIS-assisted ISAC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.00265.pdf)
>  In this paper, we investigate the potential of employing reconfigurable intelligent surface (RIS) in integrated sensing and communication (ISAC) systems. In particular, we consider an RIS-assisted ISAC system in which a multi-antenna base station (BS) simultaneously performs multi-user multi-input singleoutput (MU-MISO) communication and target detection. We aim to jointly design the transmit beamforming and receive filter of the BS, and the reflection coefficients of the RIS to maximize the sum-rate of the communication users, while satisfying a worst-case radar output signal-to-noise ratio (SNR), the transmit power constraint, and the unit modulus property of the reflecting coefficients. An efficient iterative algorithm based on fractional programming (FP), majorization-minimization (MM), and alternative direction method of multipliers (ADMM) is developed to solve the complicated non-convex problem. Simulation results verify the advantage of the proposed RIS-assisted ISAC scheme and the effectiveness of the developed algorithm.      
### 30.Separable-HoverNet and Instance-YOLO for Colon Nuclei Identification and Counting  [ :arrow_down: ](https://arxiv.org/pdf/2203.00262.pdf)
>  Nuclear segmentation, classification and quantification within Haematoxylin &amp; Eosin stained histology images enables the extraction of interpretable cell-based features that can be used in downstream explainable models in computational pathology (CPath). However, automatic recognition of different nuclei is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intraclass variability. In this work, we propose an approach that combine Separable-HoverNet and Instance-YOLOv5 to indentify colon nuclei small and unbalanced. Our approach can achieve mPQ+ 0.389 on the Segmentation and Classification-Preliminary Test Dataset and r2 0.599 on the Cellular Composition-Preliminary Test Dataset on ISBI 2022 CoNIC Challenge.      
### 31.TRILLsson: Distilled Universal Paralinguistic Speech Representations  [ :arrow_down: ](https://arxiv.org/pdf/2203.00236.pdf)
>  Recent advances in self-supervision have dramatically improved the quality of speech representations. However, deployment of state-of-the-art embedding models on devices has been restricted due to their limited public availability and large resource footprint. Our work addresses these issues by publicly releasing a collection of paralinguistic speech models that are small and near state-of-the-art performance. Our approach is based on knowledge distillation, and our models are distilled on public data only. We explore different architectures and thoroughly evaluate our models on the Non-Semantic Speech (NOSS) benchmark. Our largest distilled model is less than 15% the size of the original model (314MB vs 2.2GB), achieves over 96% the accuracy on 6 of 7 tasks, and is trained on 6.5% the data. The smallest model is 1% in size (22MB) and achieves over 90% the accuracy on 6 of 7 tasks. Our models outperform the open source Wav2Vec 2.0 model on 6 of 7 tasks, and our smallest model outperforms the open source Wav2Vec 2.0 on both emotion recognition tasks despite being 7% the size.      
### 32.Online Gradient Descent for Flexible Power Point Tracking Under a Highly Fluctuating Weather and Load  [ :arrow_down: ](https://arxiv.org/pdf/2203.00197.pdf)
>  The increasing electricity demand and the need for clean and renewable energy resources to satisfy this demand in a cost-effective manner, imposes new challenges on researchers and developers to maximize the output of these renewable resources at all times. However, the increasing penetration of renewable energy into the grid imposes new challenges on the grid operators. All of these challenges and issues gave rise to the need of Maximum Power Point Tracker (MPPT) and Flexible Power Point Trackers (FPPT) in order to maximize the power extracted from Photovoltaic (PV) systems and meet the grid operation constraints. Existing solutions for these algorithms do not take into consideration the very high dynamical nature of weather conditions that affects the output power that can be extracted from the PV modules, whereas in practice, the weather changes dynamically faster than what the algorithms time needed to converge. The work in this document is an attempt to address this shortcoming address shortcoming by utilizing online optimization algorithms for this purpose. Numerical analysis and verification are presented in the document. Code for the algorithms can be found at this link      
### 33.Nuclear Segmentation and Classification Model with Imbalanced Classes for CoNiC Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2203.00171.pdf)
>  Nuclear segmentation and classification is an essential step for computational pathology. TIA lab from Warwick University organized a nuclear segmentation and classification challenge (CoNiC) for H&amp;E stained histopathology images in colorectal cancer based on the Lizard dataset. In this challenge, computer algorithms should be able to segment and recognize six types of nuclei, including Epithelial, Lymphocyte, Plasma, Eosinophil, Neutrophil, Connective tissue. This challenge introduces two highly correlated tasks, nuclei segmentation and classification task and prediction of cellular composition task. There are a few obstacles we have to address in this challenge, 1) imbalanced annotations with few training samples on minority classes, 2) color variation of the images from multiple centers or scanners, 3) limited training samples, 4) similar morphological appearance among classes. To deal with these challenges, we proposed a systematic pipeline for nuclear segmentation and classification. First, we built a GAN-based model to automatically generate pseudo images for data augmentation. Then we trained a self-supervised stain normalization model to solve the color variation problem. Next we constructed a baseline model HoVer-Net with cost-sensitive loss to encourage the model pay more attention on the minority classes. According to the results of the leaderboard, our proposed pipeline achieves 0.40665 mPQ+ (Rank 33rd) and 0.62199 r2 (Rank 4th) in the preliminary test phase.      
### 34.When to Crossover from Earth to Space for Lower Latency Data Communications?  [ :arrow_down: ](https://arxiv.org/pdf/2203.00154.pdf)
>  For data communications over long distances, optical wireless satellite networks (OWSNs) can offer lower latency than optical fiber terrestrial networks (OFTNs). However, when is it beneficial to switch or crossover from an OFTN to an OWSN for lower latency data communications? In this work, we introduce a crossover function that enables to find the crossover distance, i.e., a distance between two points on the surface of the Earth beyond which switching or crossing over from an OFTN to an OWSN for data communications between these points is useful in terms of latency. Numerical results reveal that a higher refractive index of optical fiber (or $i$) in an OFTN and a lower altitude of satellites (or $h$) in an OWSN result in a shorter crossover distance. To account for the variation in the end-to-end propagation distance that occurs over the OWSN, we examine the crossover function in four different scenarios. Numerical results indicate that the crossover distance varies with the end-to-end propagation distance over an OWSN and is different for different scenarios. We calculate the average crossover distance over all scenarios for different $h$ and $i$ and use it to evaluate the simulation results. Furthermore, for a comparative analysis of OFTNs and OWSNs in terms of latency, we study three different OFTNs having different refractive indices and three different OWSNs having different satellite altitudes in three different scenarios for long-distance inter-continental data communications, including connections between New York and Dublin, Sao Paulo and London, and Toronto and Sydney.      
### 35.Carrier-phase and IMU based GNSS Spoofing Detection for Ground Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2203.00140.pdf)
>  This paper develops, implements, and validates a powerful single-antenna carrier-phase-based test to detect Global Navigation Satellite Systems (GNSS) spoofing attacks on ground vehicles equipped with a low-cost inertial measurement unit (IMU). Increasingly-automated ground vehicles require precise positioning that is resilient to unusual natural or accidental events and secure against deliberate attack. This paper's spoofing detection technique capitalizes on the carrier-phase fixed-ambiguity residual cost produced by a well-calibrated carrier-phase-differential GNSS (CDGNSS) estimator that is tightly coupled with a low-cost IMU. The carrier-phase fixed-ambiguity residual cost is sensitive at the sub-centimeter-level to discrepancies between measured carrier phase values and the values predicted by prior measurements and by the dynamics model, which is based on IMU measurements and on vehicle constraints. Such discrepancies will arise in a spoofing attack due to the attacker's practical inability to predict the centimeter-amplitude vehicle movement caused by roadway irregularities. The effectiveness of the developed spoofing detection method is evaluated with data captured by a vehicle-mounted sensor suite in Austin, Texas. The dataset includes both consumer- and industrial-grade IMU data and a diverse set of multipath environments (open sky, shallow urban, and deep urban). Artificial worst-case spoofing attacks injected into the dataset are detected within two seconds.      
### 36.Learning Cross-Video Neural Representations for High-Quality Frame Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2203.00137.pdf)
>  This paper considers the problem of temporal video interpolation, where the goal is to synthesize a new video frame given its two neighbors. We propose Cross-Video Neural Representation (CURE) as the first video interpolation method based on neural fields (NF). NF refers to the recent class of methods for the neural representation of complex 3D scenes that has seen widespread success and application across computer vision. CURE represents the video as a continuous function parameterized by a coordinate-based neural network, whose inputs are the spatiotemporal coordinates and outputs are the corresponding RGB values. CURE introduces a new architecture that conditions the neural network on the input frames for imposing space-time consistency in the synthesized video. This not only improves the final interpolation quality, but also enables CURE to learn a prior across multiple videos. Experimental evaluations show that CURE achieves the state-of-the-art performance on video interpolation on several benchmark datasets.      
### 37.Investigating the Spatiotemporal Charging Demand and Travel Behavior of Electric Vehicles Using GPS Data: A Machine Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.00135.pdf)
>  The increasing market penetration of electric vehicles (EVs) may change the travel behavior of drivers and pose a significant electricity demand on the power system. Since the electricity demand depends on the travel behavior of EVs, which are inherently uncertain, the forecasting of daily charging demand (CD) will be a challenging task. In this paper, we use the recorded GPS data of EVs and conventional gasoline-powered vehicles from the same city to investigate the potential shift in the travel behavior of drivers from conventional vehicles to EVs and forecast the spatiotemporal patterns of daily CD. Our analysis reveals that the travel behavior of EVs and conventional vehicles are similar. Also, the forecasting results indicate that the developed models can generate accurate spatiotemporal patterns of the daily CD.      
### 38.BlazeNeo: Blazing fast polyp segmentation and neoplasm detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.00129.pdf)
>  In recent years, computer-aided automatic polyp segmentation and neoplasm detection have been an emerging topic in medical image analysis, providing valuable support to colonoscopy procedures. Attentions have been paid to improving the accuracy of polyp detection and segmentation. However, not much focus has been given to latency and throughput for performing these tasks on dedicated devices, which can be crucial for practical applications. This paper introduces a novel deep neural network architecture called BlazeNeo, for the task of polyp segmentation and neoplasm detection with an emphasis on compactness and speed while maintaining high accuracy. The model leverages the highly efficient HarDNet backbone alongside lightweight Receptive Field Blocks for computational efficiency, and an auxiliary training mechanism to take full advantage of the training data for the segmentation quality. Our experiments on a challenging dataset show that BlazeNeo achieves improvements in latency and model size while maintaining comparable accuracy against state-of-the-art methods. When deploying on the Jetson AGX Xavier edge device in INT8 precision, our BlazeNeo achieves over 155 fps while yielding the best accuracy among all compared methods.      
### 39.Dynamic Control of Service Systems with Returns: Application to Design of Post-Discharge Hospital Readmission Prevention Programs  [ :arrow_down: ](https://arxiv.org/pdf/2203.00093.pdf)
>  We study a control problem for queueing systems where customers may return for additional episodes of service after their initial service completion. At each service completion epoch, the decision maker can choose to reduce the probability of return for the departing customer but at a cost that is convex increasing in the amount of reduction in the return probability. Other costs are incurred as customers wait in the queue and every time they return for service. Our primary motivation comes from post-discharge Quality Improvement (QI) interventions (e.g., follow up phone-calls, appointments) frequently used in a variety of healthcare settings to reduce unplanned hospital readmissions. Our objective is to understand how the cost of interventions should be balanced with the reductions in congestion and service costs. To this end, we consider a fluid approximation of the queueing system and characterize the structure of optimal long-run average and bias-optimal transient control policies for the fluid model. Our structural results motivate the design of intuitive surge protocols whereby different intensities of interventions (corresponding to different levels of reduction in the return probability) are provided based on the congestion in the system. Through extensive simulation experiments, we study the performance of the fluid policy for the stochastic system and identify parameter regimes where it leads to significant cost savings compared to a fixed long-run average optimal policy that ignores holding costs and a simple policy that uses the highest level of intervention whenever the queue is non-empty. In particular, we find that in a parameter regime relevant to our motivating application, dynamically adjusting the intensity of interventions could result in up to 25.4% reduction in long-run average cost and 33.7% in finite-horizon costs compared to the simple aggressive policy.      
### 40.Virtual Reference Feedback Tuning for linear discrete-time systems with robust stability guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2203.00088.pdf)
>  This paper proposes a data-driven method based on Virtual Reference Feedback Tuning with robust closed-loop stability guarantees in a linear setting. An uncertainty set for the system is obtained through a Set Membership identification. Based on this set, robust stability conditions are enforced as Linear Matrix Inequality constraints within an optimization problem whose cost function relies on the Virtual Reference Feedback Tuning framework. The effectiveness of the developed algorithm is demonstrated with reference to two simulation examples.      
### 41.Elliptical Slice Sampling for Probabilistic Verification of Stochastic Systems with Signal Temporal Logic Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2203.00078.pdf)
>  Autonomous robots typically incorporate complex sensors in their decision-making and control loops. These sensors, such as cameras and Lidars, have imperfections in their sensing and are influenced by environmental conditions. In this paper, we present a method for probabilistic verification of linearizable systems with Gaussian and Gaussian mixture noise models (e.g. from perception modules, machine learning components). We compute the probabilities of task satisfaction under Signal Temporal Logic (STL) specifications, using its robustness semantics, with a Markov Chain Monte-Carlo slice sampler. As opposed to other techniques, our method avoids over-approximations and double-counting of failure events. Central to our approach is a method for efficient and rejection-free sampling of signals from a Gaussian distribution such that satisfy or violate a given STL formula. We show illustrative examples from applications in robot motion planning.      
### 42.Control-aware Probabilistic Load Flow for Transmission Systems: An Analytical Method  [ :arrow_down: ](https://arxiv.org/pdf/2203.00045.pdf)
>  Probabilistic load flow (PLF) calculation, as a fundamental tool to analyze transmission system behavior, has been studied for decades. Despite a variety of available methods, existing PLF approaches rarely take system control into account. However, system control, as an automatic buffer between the fluctuations in random variables and the variations in system states, has a significant impact on the final PLF result. To consider control actions' influence, this paper proposes the first analytical PLF method for the transmission grid that takes into account primary and secondary frequency controls. This method is based on a high-precision linear power flow model, whose precision is even further improved in this paper by an original correction approach. This paper also proves that if the joint probability distribution (JPD) of random variables is expressed by a Gaussian mixture model (GMM), then the JPD of system states (e.g., nodal voltages) is an infinite GMM. By leveraging this proposition, the proposed method can generate the joint PLF of the whole system, is applicable to random variables obeying any distributions, and is capable of capturing their correlation. The high accuracy and satisfactory efficiency of this method are verified on test cases scaling from 14 to 1354 buses.      
### 43.Chance-constrained OPF: A Distributed Method with Confidentiality Preservation  [ :arrow_down: ](https://arxiv.org/pdf/2203.00043.pdf)
>  Given the increased percentage of wind power in power systems, chance-constrained optimal power flow (CC-OPF) calculation, as a means to take wind power uncertainty into account with a guaranteed security level, is being promoted. Compared to the local CC-OPF within a regional grid, the global CC-OPF of a multi-regional interconnected grid is able to coordinate across different regions and therefore improve the economic efficiency when integrating high percentage of wind power generation. In this global problem, however, multiple regional independent system operators (ISOs) participate in the decision-making process, raising the need for distributed but coordinated approaches. Most notably, due to regulation restrictions, commercial interest, and data security, regional ISOs may refuse to share confidential information with others, including generation cost, load data, system topologies, and line parameters. But this information is needed to build and solve the global CC-OPF spanning multiple areas. To tackle these issues, this paper proposes a distributed CC-OPF method with confidentiality preservation, which enables regional ISOs to determine the optimal dispatchable generations within their regions without disclosing confidential data. This method does not require parameter tunings and will not suffer from convergence challenges. Results from IEEE test cases show that this method is highly accurate.      
### 44.Spatio-temporal Vision Transformer for Super-resolution Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2203.00030.pdf)
>  Structured illumination microscopy (SIM) is an optical super-resolution technique that enables live-cell imaging beyond the diffraction limit. Reconstruction of SIM data is prone to artefacts, which becomes problematic when imaging highly dynamic samples because previous methods rely on the assumption that samples are static. We propose a new transformer-based reconstruction method, VSR-SIM, that uses shifted 3-dimensional window multi-head attention in addition to channel attention mechanism to tackle the problem of video super-resolution (VSR) in SIM. The attention mechanisms are found to capture motion in sequences without the need for common motion estimation techniques such as optical flow. We take an approach to training the network that relies solely on simulated data using videos of natural scenery with a model for SIM image formation. We demonstrate a use case enabled by VSR-SIM referred to as rolling SIM imaging, which increases temporal resolution in SIM by a factor of 9. Our method can be applied to any SIM setup enabling precise recordings of dynamic processes in biomedical research with high temporal resolution.      
### 45.Lodestar: An Integrated Embedded Real-Time Control Engine  [ :arrow_down: ](https://arxiv.org/pdf/2203.00649.pdf)
>  In this work we present Lodestar, an integrated engine for rapid real-time control system development. Using a functional block diagram paradigm, Lodestar allows for complex multi-disciplinary control software design, while automatically resolving execution order, circular data-dependencies, and networking. In particular, Lodestar presents a unified set of control, signal processing, and computer vision routines to users, which may be interfaced with external hardware and software packages using interoperable user-defined wrappers. Lodestar allows for user-defined block diagrams to be directly executed, or for them to be translated to overhead-free source code for integration in other programs. We demonstrate how our framework departs from approaches used in state-of-the-art simulation frameworks to enable real-time performance, and compare its capabilities to existing solutions in the realm of control software. To demonstrate the utility of Lodestar in real-time control systems design, we have applied Lodestar to implement two real-time torque-based controller for a robotic arm. In addition, we have developed a novel autofocus algorithm for use in thermography-based localization and parameter estimation in electrosurgery and other areas of robot-assisted surgery. We compare our algorithm design approach in Lodestar to a classical ground-up approach, showing that Lodestar considerably eases the design process. We also show how Lodestar can seamlessly interface with existing simulation and networking framework in a number of simulation examples.      
### 46.Measuring the Impact of Individual Domain Factors in Self-Supervised Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2203.00648.pdf)
>  Human speech data comprises a rich set of domain factors such as accent, syntactic and semantic variety, or acoustic environment. Previous work explores the effect of domain mismatch in automatic speech recognition between pre-training and fine-tuning as a whole but does not dissect the contribution of individual factors. In this paper, we present a controlled study to better understand the effect of such factors on the performance of pre-trained representations. To do so, we pre-train models either on modified natural speech or synthesized audio, with a single domain factor modified, and then measure performance on automatic speech recognition after fine tuning. Results show that phonetic domain factors play an important role during pre-training while grammatical and syntactic factors are far less important. To our knowledge, this is the first study to better understand the domain characteristics in self-supervised pre-training for speech.      
### 47.A Neural Ordinary Differential Equation Model for Visualizing Deep Neural Network Behaviors in Multi-Parametric MRI based Glioma Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.00628.pdf)
>  Purpose: To develop a neural ordinary differential equation (ODE) model for visualizing deep neural network (DNN) behavior during multi-parametric MRI (mp-MRI) based glioma segmentation as a method to enhance deep learning explainability. Methods: By hypothesizing that deep feature extraction can be modeled as a spatiotemporally continuous process, we designed a novel deep learning model, neural ODE, in which deep feature extraction was governed by an ODE without explicit expression. The dynamics of 1) MR images after interactions with DNN and 2) segmentation formation can be visualized after solving ODE. An accumulative contribution curve (ACC) was designed to quantitatively evaluate the utilization of each MRI by DNN towards the final segmentation results. The proposed neural ODE model was demonstrated using 369 glioma patients with a 4-modality mp-MRI protocol: T1, contrast-enhanced T1 (T1-Ce), T2, and FLAIR. Three neural ODE models were trained to segment enhancing tumor (ET), tumor core (TC), and whole tumor (WT). The key MR modalities with significant utilization by DNN were identified based on ACC analysis. Segmentation results by DNN using only the key MR modalities were compared to the ones using all 4 MR modalities. Results: All neural ODE models successfully illustrated image dynamics as expected. ACC analysis identified T1-Ce as the only key modality in ET and TC segmentations, while both FLAIR and T2 were key modalities in WT segmentation. Compared to the U-Net results using all 4 MR modalities, Dice coefficient of ET (0.784-&gt;0.775), TC (0.760-&gt;0.758), and WT (0.841-&gt;0.837) using the key modalities only had minimal differences without significance. Conclusion: The neural ODE model offers a new tool for optimizing the deep learning model inputs with enhanced explainability. The presented methodology can be generalized to other medical image-related deep learning applications.      
### 48.Towards a Common Speech Analysis Engine  [ :arrow_down: ](https://arxiv.org/pdf/2203.00613.pdf)
>  Recent innovations in self-supervised representation learning have led to remarkable advances in natural language processing. That said, in the speech processing domain, self-supervised representation learning-based systems are not yet considered state-of-the-art. We propose leveraging recent advances in self-supervised-based speech processing to create a common speech analysis engine. Such an engine should be able to handle multiple speech processing tasks, using a single architecture, to obtain state-of-the-art accuracy. The engine must also enable support for new tasks with small training datasets. Beyond that, a common engine should be capable of supporting distributed training with client in-house private data. We present the architecture for a common speech analysis engine based on the HuBERT self-supervised speech representation. Based on experiments, we report our results for language identification and emotion recognition on the standard evaluations NIST-LRE 07 and IEMOCAP. Our results surpass the state-of-the-art performance reported so far on these tasks. We also analyzed our engine on the emotion recognition task using reduced amounts of training data and show how to achieve improved results.      
### 49.Algorithm Design and Integration for a Robotic Apple Harvesting System  [ :arrow_down: ](https://arxiv.org/pdf/2203.00582.pdf)
>  Due to labor shortage and rising labor cost for the apple industry, there is an urgent need for the development of robotic systems to efficiently and autonomously harvest apples. In this paper, we present a system overview and algorithm design of our recently developed robotic apple harvester prototype. Our robotic system is enabled by the close integration of several core modules, including calibration, visual perception, planning, and control. This paper covers the main methods and advancements in robust extrinsic parameter calibration, deep learning-based multi-view fruit detection and localization, unified picking and dropping planning, and dexterous manipulation control. Indoor and field experiments were conducted to evaluate the performance of the developed system, which achieved an average picking rate of 3.6 seconds per apple. This is a significant improvement over other reported apple harvesting robots with a picking rate in the range of 7-10 seconds per apple. The current prototype shows promising performance towards further development of efficient and automated apple harvesting technology. Finally, limitations of the current system and future work are discussed.      
### 50.A comparative study of several parameterizations for speaker recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.00513.pdf)
>  This paper presents an exhaustive study about the robustness of several parameterizations, in speaker verification and identification tasks. We have studied several mismatch conditions: different recording sessions, microphones, and different languages (it has been obtained from a bilingual set of speakers). This study reveals that the combination of several parameterizations can improve the robustness in all the scenarios for both tasks, identification and verification. In addition, two different methods have been evaluated: vector quantization, and covariance matrices with an arithmetic-harmonic sphericity measure.      
### 51.Reconfigurable Intelligent Surface-Aided Spectrum Sharing Coexisting with Multiple Primary Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.00508.pdf)
>  In the spectrum sharing system (SSS) coexisting with multiple primary networks, a well-designed reconfigurable intelligent surface (RIS) is employed to control the radio environments of wireless channels and relieve the scarcity of the spectrum resource in this work, which can realize high spectral efficiency (SE) and energy efficiency (EE). Specifically, the SE enhancement in the considered SSS is decomposed into two subproblems which are a second-order programming (SOP) and a fractional programming of the convex quadratic form (CQFP), respectively, to optimize alternatively the beamforming vector at the secondary access point and the reflecting coefficients at the RIS. The CQFP subproblem about optimizing the reflecting coefficients can be solved by the domain and envelope shrinking algorithm (DES), providing the best SE performance. Besides, a low-complexity method of gradient-based linearization with domain (GLD) is proposed for obtaining a sub-optimal reflecting coefficients for fast deployment. Considering the power consumption in the practical application of RIS-aided SSS, the EE performance of our proposed GLD method has significant gain over that of the SSS without RIS. The simulation results indicate the effectiveness of the DES algorithm and show that the GLD method improves the SE and EE performance in RIS-aided SSS with multiple primary networks.      
### 52.DreamingV2: Reinforcement Learning with Discrete World Models without Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2203.00494.pdf)
>  The present paper proposes a novel reinforcement learning method with world models, DreamingV2, a collaborative extension of DreamerV2 and Dreaming. DreamerV2 is a cutting-edge model-based reinforcement learning from pixels that uses discrete world models to represent latent states with categorical variables. Dreaming is also a form of reinforcement learning from pixels that attempts to avoid the autoencoding process in general world model training by involving a reconstruction-free contrastive learning objective. The proposed DreamingV2 is a novel approach of adopting both the discrete representation of DreamingV2 and the reconstruction-free objective of Dreaming. Compared to DreamerV2 and other recent model-based methods without reconstruction, DreamingV2 achieves the best scores on five simulated challenging 3D robot arm tasks. We believe that DreamingV2 will be a reliable solution for robot learning since its discrete representation is suitable to describe discontinuous environments, and the reconstruction-free fashion well manages complex vision observations.      
### 53.Strategies for modelling open-loop saccade control of a cable-driven biomimetic robot eye  [ :arrow_down: ](https://arxiv.org/pdf/2203.00488.pdf)
>  In human-robot interactions, eye movements play an important role in non-verbal communication. However, controlling the motions of a robotic eye that display similar performance as the human oculomotor system is still a major challenge. In this paper, we study how to control a realistic model of the human eye, with a cable-driven actuation system that mimicks the 6 extra-ocular muscles. We have built a robotic prototype and developed a non-linear simulation model, for which we compared different techniques to control its gaze behavior to match the main characteristics of saccade eye movements. In the first approach, we linearized the six degrees of freedom nonlinear model, using a local derivative technique, and designed linear-quadratic optimal controllers to optimize a cost function that accounts for accuracy, energy and duration. The second method learns a dynamic neural-network that matches the system dynamics, trained from sample trajectories of the system, and a non-linear trajectory optimization solver optimized a similar cost function. We focused on the generation of rapid saccadic eye movements with fully unconstrained kinematics, and the generation of control signals for the six cables that simultaneously satisfied several dynamic optimization criteria. The model faithfully mimicked the three-dimensional rotational kinematics and dynamics observed for human saccades. Our experimental results indicate that while the linear model provides a more accurate eye movement, the nonlinear model simulate eye dynamic properties in a better way faithful approximation to the properties of the human saccadic system than the linearized model, at the cost of larger training and optimization time.      
### 54.DMF-Net: A decoupling-style multi-band fusion model for real-time full-band speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2203.00472.pdf)
>  Full-band speech enhancement based on deep neural networks is still challenging for the difficulty of modeling more frequency bands and real-time implementation. Previous studies usually adopt compressed full-band speech features in Bark and ERB scale with relatively low frequency resolution, leading to degraded performance, especially in the high-frequency region. In this paper, we propose a decoupling-style multi-band fusion model to perform full-band speech denoising and dereverberation. Instead of optimizing the full-band speech by a single network structure, we decompose the full-band target into multi sub bands and then employ a multi-stage chain optimization strategy to estimate clean spectrum stage by stage. Specifically, the low- (0-8 kHz), middle- (8-16 kHz), and high-frequency (16-24 kHz) regions are mapped by three separate sub-networks and are then fused to obtain the full-band clean target STFT spectrum. Comprehensive experiments on two public datasets demonstrate that the proposed method outperforms previous advanced systems and yields promising performance in terms of speech quality and intelligibility in real complex scenarios.      
### 55.WEMAC: Women and Emotion Multi-modal Affective Computing dataset  [ :arrow_down: ](https://arxiv.org/pdf/2203.00456.pdf)
>  Among the seventeen Sustainable Development Goals (SDGs) proposed within the 2030 Agenda and adopted by all the United Nations member states, the Fifth SDG is a call for action to turn Gender Equality into a fundamental human right and an essential foundation for a better world. It includes the eradication of all types of violence against women. Within this context, the UC3M4Safety research team aims to develop Bindi. This is a cyber-physical system which includes embedded Artificial Intelligence algorithms, for user real-time monitoring towards the detection of affective states, with the ultimate goal of achieving the early detection of risk situations for women. On this basis, we make use of wearable affective computing including smart sensors, data encryption for secure and accurate collection of presumed crime evidence, as well as the remote connection to protecting agents. Towards the development of such system, the recordings of different laboratory and into-the-wild datasets are in process. These are contained within the UC3M4Safety Database. Thus, this paper presents and details the first release of WEMAC, a novel multi-modal dataset, which comprises a laboratory-based experiment for 47 women volunteers that were exposed to validated audio-visual stimuli to induce real emotions by using a virtual reality headset while physiological, speech signals and self-reports were acquired and collected. We believe this dataset will serve and assist research on multi-modal affective computing using physiological and speech information.      
### 56.Deep Learning based Prediction of MSI in Colorectal Cancer via Prediction of the Status of MMR Markers  [ :arrow_down: ](https://arxiv.org/pdf/2203.00449.pdf)
>  An accurate diagnosis and profiling of tumour are critical to the best treatment choices for cancer patients. In addition to the cancer type and its aggressiveness, molecular heterogeneity also plays a vital role in treatment selection. MSI or MMR deficiency is one of the well-studied aberrations in terms of molecular changes. Colorectal cancer patients with MMR deficiency respond well to immunotherapy, hence assessment of the relevant molecular markers can assist clinicians in making optimal treatment selections for patients. Immunohistochemistry is one of the ways for identifying these molecular changes which requires additional sections of tumour tissue. Introduction of automated methods that can predict MSI or MMR status from a target image without the need for additional sections can substantially reduce the cost associated with it. In this work, we present our work on predicting MSI status in a two-stage process using a single target slide either stained with CK818 or H\&amp;E. First, we train a multi-headed convolutional neural network model where each head is responsible for predicting one of the MMR protein expressions. To this end, we perform registration of MMR slides to the target slide as a pre-processing step. In the second stage, statistical features computed from the MMR prediction maps are used for the final MSI prediction. Our results demonstrate that MSI classification can be improved on incorporating fine-grained MMR labels in comparison to the previous approaches in which coarse labels (MSI/MSS) are utilised.      
### 57.Realtime strategy for image data labelling using binary models and active sampling  [ :arrow_down: ](https://arxiv.org/pdf/2203.00439.pdf)
>  Machine learning (ML) and Deep Learning (DL) tasks primarily depend on data. Most of the ML and DL applications involve supervised learning which requires labelled data. In the initial phases of ML realm lack of data used to be a problem, now we are in a new era of big data. The supervised ML algorithms require data to be labelled and of good quality. Labelling task requires a large amount of money and time investment. Data labelling require a skilled person who will charge high for this task, consider the case of the medical field or the data is in bulk that requires a lot of people assigned to label it. The amount of data that is well enough for training needs to be known, money and time can not be wasted to label the whole data. This paper mainly aims to propose a strategy that helps in labelling the data along with oracle in real-time. With balancing on model contribution for labelling is 89 and 81.1 for furniture type and intel scene image data sets respectively. Further with balancing being kept off model contribution is found to be 83.47 and 78.71 for furniture type and flower data sets respectively.      
### 58.RIS-Assisted Quasi-Static Broad Coverage for Wideband mmWave Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.00400.pdf)
>  Reconfigurable intelligent surfaces (RISs) can establish favorable wireless environments to combat the severe attenuation and blockages in millimeter-wave (mmWave) bands. However, to achieve the optimal enhancement of performance, the instantaneous channel state information (CSI) needs to be estimated at the cost of a large overhead that scales with the number of RIS elements and the number of users. In this paper, we design a quasi-static broad coverage at the RIS with the reduced overhead based on the statistical CSI. We propose a design framework to synthesize the power pattern reflected by the RIS that meets the customized requirements of broad coverage. For the communication of broadcast channels, we generalize the broad coverage of the single transmit stream to the scenario of multiple streams. Moreover, we employ the quasi-static broad coverage for a multiuser orthogonal frequency division multiplexing access (OFDMA) system, and derive the analytical expression of the downlink rate, which is proved to increase logarithmically with the power gain reflected by the RIS. By taking into account the overhead of channel estimation, the proposed quasi-static broad coverage even outperforms the design method that optimizes the RIS phases using the instantaneous CSI. Numerical simulations are conducted to verify these observations.      
### 59.A Semi-Markov Decision Process (SMDP)-based Channel Allocation Model for Unreliable Terahertz (THz) Reconfigurable Intelligent Surfaces (RIS)  [ :arrow_down: ](https://arxiv.org/pdf/2203.00344.pdf)
>  Terahertz (THz) communications and reconfigurable intelligent surfaces (RISs) have been recently proposed to enable various powerful indoor applications, such as wireless virtual reality (VR). For an efficient servicing of VR users, an efficient THz channel allocation solution becomes a necessity. Assuming that RIS component is the most critical one in enabling the service, we investigate the impact of RIS hardware failure on channel allocation performance. To this end, we study a THz network that employs THz operated RISs acting as base stations, servicing VR users. We propose a Semi-Markov decision Process (SMDP)-based channel allocation model to ensure the reliability of THz connection, while maximizing the total long-term expected system reward, considering the system gains, costs of channel utilization, and the penalty of RIS failure. The SMDP-based model of the RIS system is formulated by defining the state space, action space, reward model, and transition probability distribution. We propose an optimal iterative algorithm for channel allocation that decides the next action at each system state. The results show the average reward and VR service blocking probability under different scenarios and with various VR service arrivals and RIS failure rates, as first step towards feasible VR services over unreliable THz RIS.      
### 60.BERT-LID: Leveraging BERT to Improve Spoken Language Identification  [ :arrow_down: ](https://arxiv.org/pdf/2203.00328.pdf)
>  Language identification is a task of automatically determining the identity of a language conveyed by a spoken segment. It has a profound impact on the multilingual interoperability of an intelligent speech system. Despite language identification attaining high accuracy on medium or long utterances (&gt;3s), the performance on short utterances (&lt;=1s) is still far from satisfactory. We propose an effective BERT-based language identification system (BERT-LID) to improve language identification performance, especially on short-duration speech segments. To adapt BERT into the LID pipeline, we drop in a conjunction network prior to BERT to accommodate the frame-level Phonetic Posteriorgrams(PPG) derived from the frontend phone recognizer and then fine-tune the conjunction network and BERT pre-trained model together. We evaluate several variations within this piped framework, including combining BERT with CNN, LSTM, DPCNN, and RCNN. The experimental results demonstrate that the best-performing model is RCNN-BERT. Compared with the prior works, our RCNN-BERT model can improve the accuracy by about 5% on long-segment identification and 18% on short-segment identification. The outperformance of our model, especially on the short-segment task, demonstrates the applicability of our proposed BERT-based approach on language identification.      
### 61.Earthquake Control: An Emerging Application for Robust Control. Theory and Experimental Tests  [ :arrow_down: ](https://arxiv.org/pdf/2203.00296.pdf)
>  This paper addresses the possibility of using robust control theory for preventing earthquakes through fluid injections in the earth's crust. The designed robust controllers drive aseismically a fault system to a new equilibrium point of lower energy by tracking a slow reference signal. The control design is based on a reduced-order nonlinear model able to reproduce earthquake-like instabilities. Uncertainties related to the frictional and mechanical properties of the underlying physical process and external perturbations are considered. Two kinds of controllers are derived. The first one is based on sliding-mode theory and leads to local finite-time convergence of the tracking error and rejection of Lipschitz w.r.t. time perturbations. The second controller is based on LQR control and presents global exponential stability of the tracking error and rejection of Lipschitz w.r.t. states perturbations. Both controllers generate a continuous control signal, attenuating the chattering effect in the case of the sliding-mode algorithms. The developed controllers are tested extensively and compared on the basis of numerical simulations and experiments in the laboratory. The present work opens new perspectives for the application of robust nonlinear control theory to complex geosystems, earthquakes and sustainable energy production.      
### 62.Bidirectional Pricing and Demand Response for Nanogrids with HVAC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.00270.pdf)
>  Owing to the fluctuant renewable generation and power demand, the energy surplus or deficit in each nanogrid is embodied differently across time. To stimulate local renewable energy consumption and minimize the long-term energy cost, some issues still remain to be explored: when and how the energy demand and bidirectional trading prices are scheduled considering personal comfort preferences and environmental factors. For this purpose, the demand response and two-way pricing problems concurrently for nanogrids and a public monitoring entity (PME) are studied with exploiting the large potential thermal elastic ability of heating, ventilation and air-conditioning (HVAC) units. Different from nanogrids, in terms of minimizing time-average costs, PME aims to set reasonable prices and optimize profits by trading with nanogrids and the main grid bi-directionally. In particular, such bilevel energy management problem is formulated as a stochastic form in a long-term horizon. Since there are uncertain system parameters, time-coupled queue constraints and the interplay of bilevel decision-making, it is challenging to solve the formulated problems. To this end, we derive a form of relaxation based on Lyapunov optimization technique to make the energy management problem tractable without forecasting the related system parameters. The transaction between nanogrids and PME is captured by a one-leader and multi-follower Stackelberg game framework. Then, theoretical analysis of the existence and uniqueness of Stackelberg equilibrium (SE) is developed based on the proposed game property. Following that, we devise an optimization algorithm to reach the SE with less information exchange. Numerical experiments validate the effectiveness of the proposed approach.      
### 63.Multi-Area Distribution System State Estimation via Distributed Tensor Completion  [ :arrow_down: ](https://arxiv.org/pdf/2203.00260.pdf)
>  This paper proposes a model-free distribution system state estimation method based on tensor completion using canonical polyadic decomposition. In particular, we consider a setting where the network is divided into multiple areas. The measured physical quantities at buses located in the same area are processed by an area controller. A three-way tensor is constructed to collect these measured quantities. The measurements are analyzed locally to recover the full state information of the network. A distributed closed-form iterative algorithm based on the alternating direction method of multipliers is developed to obtain the low-rank factors of the whole network state tensor where information exchange happens only between neighboring areas. The convergence properties of the distributed algorithm and the sufficient conditions on the number of samples for each smaller network that guarantee the identifiability of the factors of the state tensor are presented. To demonstrate the efficacy of the proposed algorithm and to check the identifiability conditions, numerical simulations are carried out using the IEEE 123-bus system.      
### 64.Beam Squint-Aware Integrated Sensing and Communications for Hybrid Massive MIMO LEO Satellite Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.00235.pdf)
>  The space-air-ground-sea integrated network (SAGSIN) plays an important role in offering global coverage. To improve the efficient utilization of spectral and hardware resources in the SAGSIN, integrated sensing and communications (ISAC) has drawn extensive attention. Most existing ISAC works focus on terrestrial networks and can not be straightforwardly applied in satellite systems due to the significantly different electromagnetic wave propagation properties. In this work, we investigate the application of ISAC in massive multiple-input multiple-output (MIMO) low earth orbit (LEO) satellite systems. We first characterize the statistical wave propagation properties by considering beam squint effects. Based on this analysis, we propose a beam squint-aware ISAC technique for hybrid analog/digital massive MIMO LEO satellite systems exploiting statistical channel state information. Simulation results demonstrate that the proposed scheme can operate both the wireless communications and the target sensing simultaneously with satisfactory performance, and the beam-squint effects can be efficiently mitigated with the proposed method in typical LEO satellite systems.      
### 65.Extended Graph Temporal Classification for Multi-Speaker End-to-End ASR  [ :arrow_down: ](https://arxiv.org/pdf/2203.00232.pdf)
>  Graph-based temporal classification (GTC), a generalized form of the connectionist temporal classification loss, was recently proposed to improve automatic speech recognition (ASR) systems using graph-based supervision. For example, GTC was first used to encode an N-best list of pseudo-label sequences into a graph for semi-supervised learning. In this paper, we propose an extension of GTC to model the posteriors of both labels and label transitions by a neural network, which can be applied to a wider range of tasks. As an example application, we use the extended GTC (GTC-e) for the multi-speaker speech recognition task. The transcriptions and speaker information of multi-speaker speech are represented by a graph, where the speaker information is associated with the transitions and ASR outputs with the nodes. Using GTC-e, multi-speaker ASR modelling becomes very similar to single-speaker ASR modeling, in that tokens by multiple speakers are recognized as a single merged sequence in chronological order. For evaluation, we perform experiments on a simulated multi-speaker speech dataset derived from LibriSpeech, obtaining promising results with performance close to classical benchmarks for the task.      
### 66.On Orthogonal Approximate Message Passing  [ :arrow_down: ](https://arxiv.org/pdf/2203.00224.pdf)
>  Approximate Message Passing (AMP) is an efficient iterative parameter-estimation technique for certain high-dimensional linear systems with non-Gaussian distributions, such as sparse systems. In AMP, a so-called Onsager term is added to keep estimation errors approximately Gaussian. Orthogonal AMP (OAMP) does not require this Onsager term, relying instead on an orthogonalization procedure to keep the current errors uncorrelated with (i.e., orthogonal to) past errors. In this paper, we show that the orthogonality in OAMP ensures that errors are "asymptotically independently and identically distributed Gaussian" (AIIDG). This AIIDG property, which is essential for the attractive performance of OAMP, holds for separable functions. We present a procedure to realize the required orthogonality for OAMP through Gram-Schmidt orthogonalization (GSO). We show that expectation propagation (EP), AMP, OAMP and some other algorithms can be unified under this orthogonality framework. The simplicity and generality of OAMP provide efficient solutions for estimation problems beyond the classical linear models; related applications will be discussed in a companion paper where new algorithms are developed for problems with multiple constraints and multiple measurement variables.      
### 67.Optimal Routing for Multi-user Multi-hop Relay Networks via Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2203.00213.pdf)
>  In this paper, we study the relay selection problem in multi-user, multi-hop relay networks with the objective of minimizing the maximum outage probability across all users. When only one user is present, it is well known that the optimal relay selection problem can be solved efficiently via dynamic programming. This solution breaks down in the multi-user scenario due to dependence between users. We resolve this challenge using a novel relay aggregation approach. On the expanded trellis, dynamic programming can be used to solve the optimal relay selection problem with computational complexity linear in the number of hops. Numerical examples illustrate the efficient use of this algorithm for relay networks.      
### 68.Neural Ordinary Differential Equations for Nonlinear System Identification  [ :arrow_down: ](https://arxiv.org/pdf/2203.00120.pdf)
>  Neural ordinary differential equations (NODE) have been recently proposed as a promising approach for nonlinear system identification tasks. In this work, we systematically compare their predictive performance with current state-of-the-art nonlinear and classical linear methods. In particular, we present a quantitative study comparing NODE's performance against neural state-space models and classical linear system identification methods. We evaluate the inference speed and prediction performance of each method on open-loop errors across eight different dynamical systems. The experiments show that NODEs can consistently improve the prediction accuracy by an order of magnitude compared to benchmark methods. Besides improved accuracy, we also observed that NODEs are less sensitive to hyperparameters compared to neural state-space models. On the other hand, these performance gains come with a slight increase of computation at the inference time.      
### 69.Effectiveness of Delivered Information Trade Study  [ :arrow_down: ](https://arxiv.org/pdf/2203.00116.pdf)
>  The sensor to shooter timeline is affected by two main variables: satellite positioning and asset positioning. Speeding up satellite positioning by adding more sensors or by decreasing processing time is important only if there is a prepared shooter, otherwise the main source of time is getting the shooter into position. However, the intelligence community should work towards the exploitation of sensors to the highest speed and effectiveness possible. Achieving a high effectiveness while keeping speed high is a tradeoff that must be considered in the sensor to shooter timeline. In this paper we investigate two main ideas, increasing the effectiveness of satellite imagery through image manipulation and how on-board image manipulation would affect the sensor to shooter timeline. We cover these ideas in four scenarios: Discrete Event Simulation of onboard processing versus ground station processing, quality of information with cloud cover removal, information improvement with super resolution, and data reduction with image to caption. This paper will show how image manipulation techniques such as Super Resolution, Cloud Removal, and Image to Caption will improve the quality of delivered information in addition to showing how those processes effect the sensor to shooter timeline.      
### 70.MRI-GAN: A Generalized Approach to Detect DeepFakes using Perceptual Image Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2203.00108.pdf)
>  DeepFakes are synthetic videos generated by swapping a face of an original image with the face of somebody else. In this paper, we describe our work to develop general, deep learning-based models to classify DeepFake content. We propose a novel framework for using Generative Adversarial Network (GAN)-based models, we call MRI-GAN, that utilizes perceptual differences in images to detect synthesized videos. We test our MRI-GAN approach and a plain-frames-based model using the DeepFake Detection Challenge Dataset. Our plain frames-based-model achieves 91% test accuracy and a model which uses our MRI-GAN framework with Structural Similarity Index Measurement (SSIM) for the perceptual differences achieves 74% test accuracy. The results of MRI-GAN are preliminary and may be improved further by modifying the choice of loss function, tuning hyper-parameters, or by using a more advanced perceptual similarity metric.      
### 71.Optimal Transport-based Graph Matching for 3D retinal OCT image registration  [ :arrow_down: ](https://arxiv.org/pdf/2203.00069.pdf)
>  Registration of longitudinal optical coherence tomography (OCT) images assists disease monitoring and is essential in image fusion applications. Mouse retinal OCT images are often collected for longitudinal study of eye disease models such as uveitis, but their quality is often poor compared with human imaging. This paper presents a novel but efficient framework involving an optimal transport based graph matching (OT-GM) method for 3D mouse OCT image registration. We first perform registration of fundus-like images obtained by projecting all b-scans of a volume on a plane orthogonal to them, hereafter referred to as the x-y plane. We introduce Adaptive Weighted Vessel Graph Descriptors (AWVGD) and 3D Cube Descriptors (CD) to identify the correspondence between nodes of graphs extracted from segmented vessels within the OCT projection images. The AWVGD comprises scaling, translation and rotation, which are computationally efficient, whereas CD exploits 3D spatial and frequency domain information. The OT-GM method subsequently performs the correct alignment in the x-y plane. Finally, registration along the direction orthogonal to the x-y plane (the z-direction) is guided by the segmentation of two important anatomical features peculiar to mouse b-scans, the Internal Limiting Membrane (ILM) and the hyaloid remnant (HR). Both subjective and objective evaluation results demonstrate that our framework outperforms other well-established methods on mouse OCT images within a reasonable execution time.      
### 72.Risk-averse controller design against data injection attacks on actuators for uncertain control systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.00055.pdf)
>  In this paper, we consider the optimal controller design problem against data injection attacks on actuators for an uncertain control system. We consider attacks that aim at maximizing the attack impact while remaining stealthy in the finite horizon. To this end, we use the Conditional Value-at-Risk to characterize the risk associated with the impact of attacks. The worst-case attack impact is characterized using the recently proposed output-to-output $\ell_2$-gain (OOG). We formulate the design problem and observe that it is non-convex and hard to solve. Using the framework of scenario-based optimization and a convex proxy for the OOG, we propose a convex optimization problem that approximately solves the design problem with probabilistic certificates. Finally, we illustrate the results through a numerical example.      
### 73.Learned end-to-end high-resolution lensless fiber imaging toward intraoperative real-time cancer diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2203.00008.pdf)
>  Endomicroscopy is indispensable for minimally invasive diagnostics in clinical practice. For optical keyhole monitoring of surgical interventions, high-resolution fiber endoscopic imaging is considered to be very promising, especially in combination with label-free imaging techniques to realize in vivo diagnosis. However, the inherent honeycomb-artifacts of coherent fiber bundles (CFB) reduce the resolution and limit the clinical applications. We propose an end-to-end lensless fiber imaging scheme toward intraoperative real-time cancer diagnosis. The framework includes resolution enhancement and classification networks that use single-shot fiber bundle images to provide both high-resolution images and tumor diagnosis result. The well-trained resolution enhancement network not only recovers high-resolution features beyond the physical limitations of CFB, but also helps improving tumor recognition rate. Especially for glioblastoma, the resolution enhancement network helps increasing the classification accuracy from 90.8% to 95.6%. The novel technique can enable histological real-time imaging through lensless fiber endoscopy and is promising for rapid and minimal-invasive intraoperative diagnosis in clinics.      
### 74.Towards Reducing the Need for Speech Training Data To Build Spoken Language Understanding Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.00006.pdf)
>  The lack of speech data annotated with labels required for spoken language understanding (SLU) is often a major hurdle in building end-to-end (E2E) systems that can directly process speech inputs. In contrast, large amounts of text data with suitable labels are usually available. In this paper, we propose a novel text representation and training methodology that allows E2E SLU systems to be effectively constructed using these text resources. With very limited amounts of additional speech, we show that these models can be further improved to perform at levels close to similar systems built on the full speech datasets. The efficacy of our proposed approach is demonstrated on both intent and entity tasks using three different SLU datasets. With text-only training, the proposed system achieves up to 90% of the performance possible with full speech training. With just an additional 10% of speech data, these models significantly improve further to 97% of full performance.      
