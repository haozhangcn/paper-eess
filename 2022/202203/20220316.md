# ArXiv eess --Wed, 16 Mar 2022
### 1.Indoor Propagation Measurements with Sekisui Transparent Reflectors at 28/39/120/144 GHz  [ :arrow_down: ](https://arxiv.org/pdf/2203.08116.pdf)
>  One of the critical challenges of operating with the terahertz or millimeter-wave wireless networks is the necessity of at least a strong non-line-of-sight (NLoS) reflected path to form a stable link. Recent studies have shown that an economical way of enhancing/improving these NLoS links is by using passive metallic reflectors that provide strong reflections. However, despite its inherent radio advantage, metals can dramatically influence the landscape's appearance - especially the indoor environment. A conceptual view of escaping this is by using transparent reflectors. In this work, for the very first time, we evaluate the wireless propagation characteristics of passive transparent reflectors in an indoor environment at 28 GHz, 39 GHz, 120 GHz, and 144 GHz bands. In particular, we investigate the penetration loss and the reflection characteristics at different frequencies and compare them against the other common indoor materials such as ceiling tile, clear glass, drywall, plywood, and metal. The measurement results suggest that the transparent reflector, apart from an obvious advantage of transparency, has a higher penetration loss than the common indoor materials (excluding metal) and performs similarly to metal in terms of reflection. Our experimental results directly translate to better reflection performance and preserving the radio waves within the environment than common indoor materials, with potential applications in controlled wireless communication.      
### 2.Key Point Agnostic Frequency-Selective Mesh-to-Grid Image Resampling using Spectral Weighting  [ :arrow_down: ](https://arxiv.org/pdf/2203.08086.pdf)
>  Many applications in image processing require resampling of arbitrarily located samples onto regular grid positions. This is important in frame-rate up-conversion, super-resolution, and image warping among others. A state-of-the-art high quality model-based resampling technique is frequency-selective mesh-to-grid resampling which requires pre-estimation of key points. In this paper, we propose a new key point agnostic frequency-selective mesh-to-grid resampling that does not depend on pre-estimated key points. Hence, the number of data points that are included is reduced drastically and the run time decreases significantly. To compensate for the key points, a spectral weighting function is introduced that models the optical transfer function in order to favor low frequencies more than high ones. Thereby, resampling artefacts like ringing are supressed reliably and the resampling quality increases. On average, the new AFSMR is conceptually simpler and gains up to 1.2 dB in terms of PSNR compared to the original mesh-to-grid resampling while being approximately 14.5 times faster.      
### 3.Combining AI/ML and PHY Layer Rule Based Inference -- Some First Results  [ :arrow_down: ](https://arxiv.org/pdf/2203.08074.pdf)
>  In 3GPP New Radio (NR) Release 18 we see the first study item starting in May 2022, which will evaluate the potential of AI/ML methods for Radio Access Network (RAN) 1, i.e., for mobile radio PHY and MAC layer applications. We use the profiling method for accurate iterative estimation of multipath component parameters for PHY layer reference, as it promises a large channel prediction horizon. We investigate options to partly or fully replace some functionalities of this rule based PHY layer method by AI/ML inferences, with the goal to achieve either a higher performance, lower latency, or, reduced processing complexity. We provide first results for noise reduction, then a combined scheme for model order selection, compare options to infer multipath component start parameters, and, provide an outlook on a possible channel prediction framework.      
### 4.Graph filtering over expanding graphs  [ :arrow_down: ](https://arxiv.org/pdf/2203.08058.pdf)
>  Our capacity to learn representations from data is related to our ability to design filters that can leverage their coupling with the underlying domain. Graph filters are one such tool for network data and have been used in a myriad of applications. But graph filters work only with a fixed number of nodes despite the expanding nature of practical networks. Learning filters in this setting is challenging not only because of the increased dimensions but also because the connectivity is known only up to an attachment model. We propose a filter learning scheme for data over expanding graphs by relying only on such a model. By characterizing the filter stochastically, we develop an empirical risk minimization framework inspired by multi-kernel learning to balance the information inflow and outflow at the incoming nodes. We particularize the approach for denoising and semi-supervised learning (SSL) over expanding graphs and show near-optimal performance compared with baselines relying on the exact topology. For SSL, the proposed scheme uses the incoming node information to improve the task on the existing ones. These findings lay the foundation for learning representations over expanding graphs by relying only on the stochastic connectivity model.      
### 5.A Noise-level-aware Framework for PET Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2203.08034.pdf)
>  In PET, the amount of relative (signal-dependent) noise present in different body regions can be significantly different and is inherently related to the number of counts present in that region. The number of counts in a region depends, in principle and among other factors, on the total administered activity, scanner sensitivity, image acquisition duration, radiopharmaceutical tracer uptake in the region, and patient local body morphometry surrounding the region. In theory, less amount of denoising operations is needed to denoise a high-count (low relative noise) image than images a low-count (high relative noise) image, and vice versa. The current deep-learning-based methods for PET image denoising are predominantly trained on image appearance only and have no special treatment for images of different noise levels. Our hypothesis is that by explicitly providing the local relative noise level of the input image to a deep convolutional neural network (DCNN), the DCNN can outperform itself trained on image appearance only. To this end, we propose a noise-level-aware framework denoising framework that allows embedding of local noise level into a DCNN. The proposed is trained and tested on 30 and 15 patient PET images acquired on a GE Discovery MI PET/CT system. Our experiments showed that the increases in both PSNR and SSIM from our backbone network with relative noise level embedding (NLE) versus the same network without NLE were statistically significant with p&lt;0.001, and the proposed method significantly outperformed a strong baseline method by a large margin.      
### 6.Text-free non-parallel many-to-many voice conversion using normalising flows  [ :arrow_down: ](https://arxiv.org/pdf/2203.08009.pdf)
>  Non-parallel voice conversion (VC) is typically achieved using lossy representations of the source speech. However, ensuring only speaker identity information is dropped whilst all other information from the source speech is retained is a large challenge. This is particularly challenging in the scenario where at inference-time we have no knowledge of the text being read, i.e., text-free VC. To mitigate this, we investigate information-preserving VC approaches. <br>Normalising flows have gained attention for text-to-speech synthesis, however have been under-explored for VC. Flows utilize invertible functions to learn the likelihood of the data, thus provide a lossless encoding of speech. We investigate normalising flows for VC in both text-conditioned and text-free scenarios. Furthermore, for text-free VC we compare pre-trained and jointly-learnt priors. Flow-based VC evaluations show no degradation between text-free and text-conditioned VC, resulting in improvements over the state-of-the-art. Also, joint-training of the prior is found to negatively impact text-free VC quality.      
### 7.Distributed Pinning Set Stabilization of Large-Scale Boolean Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.07986.pdf)
>  In this article, we design the distributed pinning controllers to globally stabilize a Boolean network (BN), specially a sparsely connected large-scale one, towards a preassigned subset of state space through the node-to-node message exchange. Given an appointed state set, system nodes are partitioned into two disjoint parts, which respectively gather the nodes whose states are fixed or arbitrary with respect to the given state set. With such node division, three parts of pinned nodes are selected and the state feedback controllers are accordingly designed such that the resulting BN satisfies three conditions: the states of the other nodes cannot affect the nodal dynamics of fixed-state nodes, the subgraph of network structure induced by the fixed-state nodes is acyclic, and the steady state of the subnetwork induced by the fixed-state nodes lies in the state set given beforehand. If the BN after control is acyclic, the stabilizing time is revealed to be no more than the length of the longest path in the current network structure plus one. This enables us to further design the pinning controllers with the constraint of stabilizing time. Noting that the overall procedure runs in an exponentially increasing time with respect to the largest number of functional variables in the dynamics of pinned nodes, the sparsely-connected large-scale BNs can be well addressed in a reasonable amount of time. Finally, we demonstrate the applications of our theoretical results in a T-LGL survival signal network with $29$ nodes and T-cell receptor signaling network with $90$ nodes.      
### 8.Control Barrier Functions for Systems with Multiple Control Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2203.07978.pdf)
>  Control Barrier Functions (CBFs) are becoming popular tools in guaranteeing safety for nonlinear systems and constraints, and they can reduce a constrained optimal control problem into a sequence of Quadratic Programs (QPs) for affine control systems. The recently proposed High Order Control Barrier Functions (HOCBFs) work for arbitrary relative degree constraints. One of the challenges in a HOCBF is to address the relative degree problem when a system has multiple control inputs, i.e., the relative degree could be defined with respect to different components of the control vector. This paper proposes two methods for HOCBFs to deal with systems with multiple control inputs: a general integral control method and a method which is simpler but limited to specific classes of physical systems. When control bounds are involved, the feasibility of the above mentioned QPs can also be significantly improved with the proposed methods. We illustrate our approaches on a unicyle model with two control inputs, and compare the two proposed methods to demonstrate their effectiveness and performance.      
### 9.Learning Expanding Graphs for Signal Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2203.07966.pdf)
>  Performing signal processing over graphs requires knowledge of the underlying fixed topology. However, graphs often grow in size with new nodes appearing over time, whose connectivity is typically unknown; hence, making more challenging the downstream tasks in applications like cold start recommendation. We address such a challenge for signal interpolation at the incoming nodes blind to the topological connectivity of the specific node. Specifically, we propose a stochastic attachment model for incoming nodes parameterized by the attachment probabilities and edge weights. We estimate these parameters in a data-driven fashion by relying only on the attachment behaviour of earlier incoming nodes with the goal of interpolating the signal value. We study the non-convexity of the problem at hand, derive conditions when it can be marginally convexified, and propose an alternating projected descent approach between estimating the attachment probabilities and the edge weights. Numerical experiments with synthetic and real data dealing in cold start collaborative filtering corroborate our findings.      
### 10.Investigating self-supervised learning for speech enhancement and separation  [ :arrow_down: ](https://arxiv.org/pdf/2203.07960.pdf)
>  Speech enhancement and separation are two fundamental tasks for robust speech processing. Speech enhancement suppresses background noise while speech separation extracts target speech from interfering speakers. Despite a great number of supervised learning-based enhancement and separation methods having been proposed and achieving good performance, studies on applying self-supervised learning (SSL) to enhancement and separation are limited. In this paper, we evaluate 13 SSL upstream methods on speech enhancement and separation downstream tasks. Our experimental results on Voicebank-DEMAND and Libri2Mix show that some SSL representations consistently outperform baseline features including the short-time Fourier transform (STFT) magnitude and log Mel filterbank (FBANK). Furthermore, we analyze the factors that make existing SSL frameworks difficult to apply to speech enhancement and separation and discuss the representation properties desired for both tasks. Our study is included as the official speech enhancement and separation downstreams for SUPERB.      
### 11.Efficient Training of the Memristive Deep Belief Net Immune to Non-Idealities of the Synaptic Devices  [ :arrow_down: ](https://arxiv.org/pdf/2203.07884.pdf)
>  The tunability of conductance states of various emerging non-volatile memristive devices emulates the plasticity of biological synapses, making it promising in the hardware realization of large-scale neuromorphic systems. The inference of the neural network can be greatly accelerated by the vector-matrix multiplication (VMM) performed within a crossbar array of memristive devices in one step. Nevertheless, the implementation of the VMM needs complex peripheral circuits and the complexity further increases since non-idealities of memristive devices prevent precise conductance tuning (especially for the online training) and largely degrade the performance of the deep neural networks (DNNs). Here, we present an efficient online training method of the memristive deep belief net (DBN). The proposed memristive DBN uses stochastically binarized activations, reducing the complexity of peripheral circuits, and uses the contrastive divergence (CD) based gradient descent learning algorithm. The analog VMM and digital CD are performed separately in a mixed-signal hardware arrangement, making the memristive DBN high immune to non-idealities of synaptic devices. The number of write operations on memristive devices is reduced by two orders of magnitude. The recognition accuracy of 95%~97% can be achieved for the MNIST dataset using pulsed synaptic behaviors of various memristive synaptic devices.      
### 12.Recursive 3D Segmentation of Shoulder Joint with Coarse-scanned MR Image  [ :arrow_down: ](https://arxiv.org/pdf/2203.07846.pdf)
>  For diagnosis of shoulder illness, it is essential to look at the morphology deviation of scapula and humerus from the medical images that are acquired from Magnetic Resonance (MR) imaging. However, taking high-resolution MR images is time-consuming and costly because the reduction of the physical distance between image slices causes prolonged scanning time. Moreover, due to the lack of training images, images from various sources must be utilized, which creates the issue of high variance across the dataset. Also, there are human errors among the images due to the fact that it is hard to take the spatial relationship into consideration when labeling the 3D image in low resolution. In order to combat all obstacles stated above, we develop a fully automated algorithm for segmenting the humerus and scapula bone from coarsely scanned and low-resolution MR images and a recursive learning framework that iterative utilize the generated labels for reducing the errors among segmentations and increase our dataset set for training the next round network. In this study, 50 MR images are collected from several institutions and divided into five mutually exclusive sets for carrying five-fold cross-validation. Contours that are generated by the proposed method demonstrated a high level of accuracy when compared with ground truth and the traditional method. The proposed neural network and the recursive learning scheme improve the overall quality of the segmentation on humerus and scapula on the low-resolution dataset and reduced incorrect segmentation in the ground truth, which could have a positive impact on finding the cause of shoulder pain and patient's early relief.      
### 13.Image Quality Assessment for Magnetic Resonance Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2203.07809.pdf)
>  Image quality assessment (IQA) algorithms aim to reproduce the human's perception of the image quality. The growing popularity of image enhancement, generation, and recovery models instigated the development of many methods to assess their performance. However, most IQA solutions are designed to predict image quality in the general domain, with the applicability to specific areas, such as medical imaging, remaining questionable. Moreover, the selection of these IQA metrics for a specific task typically involves intentionally induced distortions, such as manually added noise or artificial blurring; yet, the chosen metrics are then used to judge the output of real-life computer vision models. In this work, we aspire to fill these gaps by carrying out the most extensive IQA evaluation study for Magnetic Resonance Imaging (MRI) to date (14,700 subjective scores). We use outputs of neural network models trained to solve problems relevant to MRI, including image reconstruction in the scan acceleration, motion correction, and denoising. Seven trained radiologists assess these distorted images, with their verdicts then correlated with 35 different image quality metrics (full-reference, no-reference, and distribution-based metrics considered). Our emphasis is on reflecting the radiologist's perception of the reconstructed images, gauging the most diagnostically influential criteria for the quality of MRI scans: signal-to-noise ratio, contrast-to-noise ratio, and the presence of artifacts.      
### 14.Training Generative Adversarial Networks for Optical Property Mapping using Synthetic Image Data  [ :arrow_down: ](https://arxiv.org/pdf/2203.07793.pdf)
>  We demonstrate training of a Generative Adversarial Network (GAN) for prediction of optical property maps (scattering and absorption) using spatial frequency domain imaging (SFDI) image data sets generated synthetically with free open-source 3D modelling and rendering software, Blender. The flexibility of Blender is exploited to simulate 3 models with real-life relevance to clinical SFDI of diseased tissue: flat samples, flat samples with spheroidal tumours and cylindrical samples with spheroidal tumours representing imaging inside a tubular organ e.g. the gastro-intestinal tract. In all 3 scenarios we show the GAN provides accurate reconstruction of optical properties from single SFDI images with mean normalised error ranging from 1-1.2% for absorption and 0.7-1.2% for scattering, resulting in visually improved contrast for tumour spheroid structures. This compares favourably with 25% absorption error and 10% scattering error achieved using GANs on experimental SFDI data. However, some of this improvement is due to lower noise and availability of perfect ground truths so we therefore cross-validate our synthetically-trained GAN with a GAN trained on experimental data and observe visually accurate results with error of &lt;40% for absorption and &lt;25% for scattering, due largely to the presence of spatial frequency mismatch artefacts. Our synthetically trained GAN is therefore highly relevant to real experimental samples, but provides significant added benefits of large training datasets, perfect ground-truths and the ability to test realistic imaging geometries, e.g. inside cylinders, for which no conventional single-shot demodulation algorithms exist. In future we expect that application of techniques such as domain adaptation or training on hybrid real-synthetic datasets will create a powerful tool for fast, accurate production of optical property maps from real clinical imaging systems.      
### 15.Fast Autofocusing using Tiny Networks for Digital Holographic Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2203.07772.pdf)
>  The numerical wavefront backpropagation principle of digital holography confers unique extended focus capabilities, without mechanical displacements along z-axis. However, the determination of the correct focusing distance is a non-trivial and time consuming issue. A deep learning (DL) solution is proposed to cast the autofocusing as a regression problem and tested over both experimental and simulated holograms. Single wavelength digital holograms were recorded by a Digital Holographic Microscope (DHM) with a 10$\mathrm{x}$ microscope objective from a patterned target moving in 3D over an axial range of 92 $\mu$m. Tiny DL models are proposed and compared such as a tiny Vision Transformer (TViT), tiny VGG16 (TVGG) and a tiny Swin-Transfomer (TSwinT). The experiments show that the predicted focusing distance $Z_R^{\mathrm{Pred}}$ is accurately inferred with an accuracy of 1.2 $\mu$m in average in comparison with the DHM depth of field of 15 $\mu$m. Numerical simulations show that all tiny models give the $Z_R^{\mathrm{Pred}}$ with an error below 0.3 $\mu$m. Such a prospect would significantly improve the current capabilities of computer vision position sensing in applications such as 3D microscopy for life sciences or micro-robotics. Moreover, all models reach state of the art inference time on CPU, less than 25 ms per inference.      
### 16.Optimal selection and tracking of generalized Nash equilibria in monotone games  [ :arrow_down: ](https://arxiv.org/pdf/2203.07765.pdf)
>  A fundamental open problem in monotone game theory is the computation of a specific generalized Nash equilibrium (GNE) among all the available ones, e.g. the optimal equilibrium with respect to a system-level objective. The existing GNE seeking algorithms have in fact convergence guarantees toward an arbitrary, possibly inefficient, equilibrium. In this paper, we solve this open problem by leveraging results from fixed-point selection theory and in turn derive distributed algorithms for the computation of an optimal GNE in monotone games. We then extend the technical results to the time-varying setting and propose an algorithm that tracks the sequence of optimal equilibria up to an asymptotic error, whose bound depends on the local computational capabilities of the agents.      
### 17.Development of a multi-timescale method for classifying hybrid energy storage systems in grid applications  [ :arrow_down: ](https://arxiv.org/pdf/2203.07750.pdf)
>  An extended use of renewable energies and a trend towards increasing energy consumption lead to challenges such as temporal and spatial decoupling of energy generation and consumption. This work evaluates the possible applications and advantages of hybrid energy storage systems compared to conventional, single energy storage applications. In a mathematical approach, evaluation criteria such as frequency, probability of power transients, as well as absolute power peaks are combined to identify suitable thresholds for energy management systems on a multi-timescale basis. With experimental load profiles from a municipal application, an airport, and an industrial application, four categories, clustering similar roles of the VRFB and the SC, are developed.      
### 18.Securing the Classification of COVID-19 in Chest X-ray Images: A Privacy-Preserving Deep Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.07728.pdf)
>  Deep learning (DL) is being increasingly utilized in healthcare-related fields due to its outstanding efficiency. However, we have to keep the individual health data used by DL models private and secure. Protecting data and preserving the privacy of individuals has become an increasingly prevalent issue. The gap between the DL and privacy communities must be bridged. In this paper, we propose privacy-preserving deep learning (PPDL)-based approach to secure the classification of Chest X-ray images. This study aims to use Chest X-ray images to their fullest potential without compromising the privacy of the data that it contains. The proposed approach is based on two steps: encrypting the dataset using partially homomorphic encryption and training/testing the DL algorithm over the encrypted images. Experimental results on the COVID-19 Radiography database show that the MobileNetV2 model achieves an accuracy of 94.2% over the plain data and 93.3% over the encrypted data.      
### 19.Magnification Prior: A Self-Supervised Method for Learning Representations on Breast Cancer Histopathological Images  [ :arrow_down: ](https://arxiv.org/pdf/2203.07707.pdf)
>  This work presents a novel self-supervised pre-training method to learn efficient representations without labels on histopathology medical images utilizing magnification factors. Other state-of-theart works mainly focus on fully supervised learning approaches that rely heavily on human annotations. However, the scarcity of labeled and unlabeled data is a long-standing challenge in histopathology. Currently, representation learning without labels remains unexplored for the histopathology domain. The proposed method, Magnification Prior Contrastive Similarity (MPCS), enables self-supervised learning of representations without labels on small-scale breast cancer dataset BreakHis by exploiting magnification factor, inductive transfer, and reducing human prior. The proposed method matches fully supervised learning state-of-the-art performance in malignancy classification when only 20% of labels are used in fine-tuning and outperform previous works in fully supervised learning settings. It formulates a hypothesis and provides empirical evidence to support that reducing human-prior leads to efficient representation learning in self-supervision. The implementation of this work is available online on GitHub - <a class="link-external link-https" href="https://github.com/prakashchhipa/Magnification-Prior-Self-Supervised-Method" rel="external noopener nofollow">this https URL</a>      
### 20.FB-MSTCN: A Full-Band Single-Channel Speech Enhancement Method Based on Multi-Scale Temporal Convolutional Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.07684.pdf)
>  In recent years, deep learning-based approaches have significantly improved the performance of single-channel speech enhancement. However, due to the limitation of training data and computational complexity, real-time enhancement of full-band (48 kHz) speech signals is still very challenging. Because of the low energy of spectral information in the high-frequency part, it is more difficult to directly model and enhance the full-band spectrum using neural networks. To solve this problem, this paper proposes a two-stage real-time speech enhancement model with extraction-interpolation mechanism for a full-band signal. The 48 kHz full-band time-domain signal is divided into three sub-channels by extracting, and a two-stage processing scheme of `masking + compensation' is proposed to enhance the signal in the complex domain. After the two-stage enhancement, the enhanced full-band speech signal is restored by interval interpolation. In the subjective listening and word accuracy test, our proposed model achieves superior performance and outperforms the baseline model overall by 0.59 MOS and 4.0% WAcc for the non-personalized speech denoising task.      
### 21.Unpaired Deep Image Dehazing Using Contrastive Disentanglement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.07677.pdf)
>  We present an effective unpaired learning based image dehazing network from an unpaired set of clear and hazy images. This paper provides a new perspective to treat image dehazing as a two-class separated factor disentanglement task, i.e, the task-relevant factor of clear image reconstruction and the task-irrelevant factor of haze-relevant distribution. To achieve the disentanglement of these two-class factors in deep feature space, contrastive learning is introduced into a CycleGAN framework to learn disentangled representations by guiding the generated images to be associated with latent factors. With such formulation, the proposed contrastive disentangled dehazing method (CDD-GAN) first develops negative generators to cooperate with the encoder network to update alternately, so as to produce a queue of challenging negative adversaries. Then these negative adversaries are trained end-to-end together with the backbone representation network to enhance the discriminative information and promote factor disentanglement performance by maximizing the adversarial contrastive loss. During the training, we further show that hard negative examples can suppress the task-irrelevant factors and unpaired clear exemples can enhance the task-relevant factors, in order to better facilitate haze removal and help image restoration. Extensive experiments on both synthetic and real-world datasets demonstrate that our method performs favorably against existing state-of-the-art unpaired dehazing approaches.      
### 22.Breast Cancer Molecular Subtypes Prediction on Pathological Images with Discriminative Patch Selecting and Multi-Instance Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.07659.pdf)
>  Molecular subtypes of breast cancer are important references to personalized clinical treatment. For cost and labor savings, only one of the patient's paraffin blocks is usually selected for subsequent immunohistochemistry (IHC) to obtain molecular subtypes. Inevitable sampling error is risky due to tumor heterogeneity and could result in a delay in treatment. Molecular subtype prediction from conventional H&amp;E pathological whole slide images (WSI) using AI method is useful and critical to assist pathologists pre-screen proper paraffin block for IHC. It's a challenging task since only WSI level labels of molecular subtypes can be obtained from IHC. Gigapixel WSIs are divided into a huge number of patches to be computationally feasible for deep learning. While with coarse slide-level labels, patch-based methods may suffer from abundant noise patches, such as folds, overstained regions, or non-tumor tissues. A weakly supervised learning framework based on discriminative patch selecting and multi-instance learning was proposed for breast cancer molecular subtype prediction from H&amp;E WSIs. Firstly, co-teaching strategy was adopted to learn molecular subtype representations and filter out noise patches. Then, a balanced sampling strategy was used to handle the imbalance in subtypes in the dataset. In addition, a noise patch filtering algorithm that used local outlier factor based on cluster centers was proposed to further select discriminative patches. Finally, a loss function integrating patch with slide constraint information was used to finetune MIL framework on obtained discriminative patches and further improve the performance of molecular subtyping. The experimental results confirmed the effectiveness of the proposed method and our models outperformed even senior pathologists, with potential to assist pathologists to pre-screen paraffin blocks for IHC in clinic.      
### 23.Joint Time-Vertex Fractional Fourier Transform  [ :arrow_down: ](https://arxiv.org/pdf/2203.07655.pdf)
>  Graphs signal processing successfully captures high-dimensional data on non-Euclidean domains by using graph signals defined on graph vertices. However, data sources on each vertex can also continually provide time-series signals such that graph signals on each vertex are now time-series signals. Joint time-vertex Fourier transform (JFT) and the associated framework of time-vertex signal processing enable us to study such signals defined on joint time-vertex domains by providing spectral analysis. Just as the fractional Fourier transform (FRT) generalizes the ordinary Fourier transform (FT), we propose the joint time-vertex fractional Fourier transform (JFRT) as a generalization to the JFT. JFRT provides an additional fractional analysis tool for joint time-vertex processing by extending both temporal and vertex domain Fourier analysis to fractional orders. We theoretically show that the proposed JFRT generalizes the JFT and satisfies the properties of index additivity, reversibility, reduction to identity, and unitarity (for certain graph topologies). We provide theoretical derivations for JFRT-based denoising as well as computational cost analysis. Results of numerical experiments are also presented to demonstrate the benefits of JFRT.      
### 24.Fast and Accurate Linear Fitting for Incompletely Sampled Gaussian Function With a Long Tail  [ :arrow_down: ](https://arxiv.org/pdf/2203.07639.pdf)
>  Fitting experiment data onto a curve is a common signal processing technique to extract data features and establish the relationship between variables. Often, we expect the curve to comply with some analytical function and then turn data fitting into estimating the unknown parameters of a function. Among analytical functions for data fitting, Gaussian function is the most widely used one due to its extensive applications in numerous science and engineering fields. To name just a few, Gaussian function is highly popular in statistical signal processing and analysis, thanks to the central limit theorem [1]; Gaussian function frequently appears in the quantum harmonic oscillator, quantum field theory, optics, lasers, and many other theories and models in Physics [2]; moreover, Gaussian function is widely applied in chemistry for depicting molecular orbitals, in computer science for imaging processing and in artificial intelligence for defining neural networks.      
### 25.Time-series image denoising of pressure-sensitive paint data by projected multivariate singular spectrum analysis  [ :arrow_down: ](https://arxiv.org/pdf/2203.07574.pdf)
>  Time-series data, such as unsteady pressure-sensitive paint (PSP) measurement data, may contain a significant amount of random noise. Thus, in this study, we investigated a noise-reduction method that combines multivariate singular spectrum analysis (MSSA) with low-dimensional data representation. MSSA is a state-space reconstruction technique that utilizes time-delay embedding, and the low-dimensional representation is achieved by projecting data onto the singular value decomposition (SVD) basis. The noise-reduction performance of the proposed method for unsteady PSP data, i.e., the projected MSSA, is compared with that of the truncated SVD method, one of the most employed noise-reduction methods. The result shows that the projected MSSA exhibits better performance in reducing random noise than the truncated SVD method. Additionally, in contrast to that of the truncated SVD method, the performance of the projected MSSA is less sensitive to the truncation rank. Furthermore, the projected MSSA achieves denoising effectively by extracting smooth trajectories in a state space from noisy input data. Expectedly, the projected MSSA will be effective for reducing random noise in not only PSP measurement data, but also various high-dimensional time-series data.      
### 26.Smoothed Phase-Coded FMCW: Waveform Properties and Transceiver Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2203.07508.pdf)
>  Smoothed phase-coded frequency modulated continuous waveform (SPC-FMCW), which is aimed to improve the coexistence of multiple radars operating within the same frequency bandwidth, is studied and the receiving strategy with a low ADC sampling requirement is investigated. The Gaussian filter is applied to obtain smooth phase transition, and the phase lag compensation is performed on transmitted phase code to enhance decoding. The proposed waveform is examined in different domains, and its waveform properties are analysed theoretically and demonstrated experimentally. Both the simulation and experimental results show that the introduced waveform with the investigated processing steps helps to combine all advantages of the FMCW waveform including hardware simplicity and small operational bandwidth of the receiver with advantages of the phase coding.      
### 27.Closing the Loop: A Framework for Trustworthy Machine Learning in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.07505.pdf)
>  Deep decarbonization of the energy sector will require massive penetration of stochastic renewable energy resources and an enormous amount of grid asset coordination; this represents a challenging paradigm for the power system operators who are tasked with maintaining grid stability and security in the face of such changes. With its ability to learn from complex datasets and provide predictive solutions on fast timescales, machine learning (ML) is well-posed to help overcome these challenges as power systems transform in the coming decades. In this work, we outline five key challenges (dataset generation, data pre-processing, model training, model assessment, and model embedding) associated with building trustworthy ML models which learn from physics-based simulation data. We then demonstrate how linking together individual modules, each of which overcomes a respective challenge, at sequential stages in the machine learning pipeline can help enhance the overall performance of the training process. In particular, we implement methods that connect different elements of the learning pipeline through feedback, thus "closing the loop" between model training, performance assessments, and re-training. We demonstrate the effectiveness of this framework, its constituent modules, and its feedback connections by learning the N-1 small-signal stability margin associated with a detailed model of a proposed North Sea Wind Power Hub system.      
### 28.Reinforcement Learning for Optimal Control of a District Cooling Energy Plant  [ :arrow_down: ](https://arxiv.org/pdf/2203.07500.pdf)
>  District cooling energy plants (DCEPs) consisting of chillers, cooling towers, and thermal energy storage (TES) systems consume a considerable amount of electricity. Optimizing the scheduling of the TES and chillers to take advantage of time-varying electricity price is a challenging optimal control problem. The classical method, model predictive control (MPC), requires solving a high dimensional mixed-integer nonlinear program (MINLP) because of the on/off actuation of the chillers and charging/discharging of TES, which are computationally challenging. RL is an attractive alternative to MPC: the real time control computation is a low-dimensional optimization problem that can be easily solved. However, the performance of an RL controller depends on many design choices. In this paper, we propose a Q-learning based reinforcement learning (RL) controller for this problem. Numerical simulation results show that the proposed RL controller is able to reduce energy cost over a rule-based baseline controller by approximately 8%, comparable to savings reported in the literature with MPC for similar DCEPs. We describe the design choices in the RL controller, including basis functions, reward function shaping, and learning algorithm parameters. Compared to existing work on RL for DCEPs, the proposed controller is designed for continuous state and actions spaces.      
### 29.Explainability and Graph Learning from Social Interactions  [ :arrow_down: ](https://arxiv.org/pdf/2203.07494.pdf)
>  Social learning algorithms provide models for the formation of opinions over social networks resulting from local reasoning and peer-to-peer exchanges. Interactions occur over an underlying graph topology, which describes the flow of information among the agents. In this work, we propose a technique that addresses questions of explainability and interpretability when the graph is hidden. Given observations of the evolution of the belief over time, we aim to infer the underlying graph topology, discover pairwise influences between the agents, and identify significant trajectories in the network. The proposed framework is online in nature and can adapt dynamically to changes in the graph topology or the true hypothesis.      
### 30.Bilinear Systems Induced by Proper Lie Group Actions  [ :arrow_down: ](https://arxiv.org/pdf/2203.07483.pdf)
>  In the study of induced bilinear systems, the classical Lie algebra rank condition (LARC) is known to be impractical since it requires computing the rank everywhere. On the other hand, the transitive Lie algebra condition, while more commonly used, relies on the classification of transitive Lie algebras, which is elusive except for few simple geometric objects such as spheres. We prove in this note that for bilinear systems induced by proper Lie group actions, the underlying Lie algebra is closely related to the orbits of the group action. Knowing the pattern of the Lie algebra rank over the manifold, we show that the LARC can be relaxed so that it suffices to check the rank at an arbitrary single point. Moreover, it removes the necessity for classifying transitive Lie algebras. Finally, this relaxed rank condition also leads to a characterization of controllable submanifolds by orbits.      
### 31.A deep learning pipeline for breast cancer ki-67 proliferation index scoring  [ :arrow_down: ](https://arxiv.org/pdf/2203.07452.pdf)
>  The Ki-67 proliferation index is an essential biomarker that helps pathologists to diagnose and select appropriate treatments. However, automatic evaluation of Ki-67 is difficult due to nuclei overlapping and complex variations in their properties. This paper proposes an integrated pipeline for accurate automatic counting of Ki-67, where the impact of nuclei separation techniques is highlighted. First, semantic segmentation is performed by combining the Squeez and Excitation Resnet and Unet algorithms to extract nuclei from the background. The extracted nuclei are then divided into overlapped and non-overlapped regions based on eight geometric and statistical features. A marker-based Watershed algorithm is subsequently proposed and applied only to the overlapped regions to separate nuclei. Finally, deep features are extracted from each nucleus patch using Resnet18 and classified into positive or negative by a random forest classifier. The proposed pipeline's performance is validated on a dataset from the Department of Pathology at Hôpital Nord Franche-Comté hospital.      
### 32.$\mathcal{H}_{\infty}$-optimal Interval Observer Synthesis for Uncertain Nonlinear Dynamical Systems via Mixed-Monotone Decompositions  [ :arrow_down: ](https://arxiv.org/pdf/2203.07430.pdf)
>  This paper introduces a novel $\mathcal{H}_{\infty}$-optimal interval observer synthesis for bounded-error/uncertain locally Lipschitz nonlinear continuous-time (CT) and discrete-time (DT) systems with noisy nonlinear observations. Specifically, using mixed-monotone decompositions, the proposed observer is correct by construction, i.e., the interval estimates readily frame the true states without additional constraints or procedures. In addition, we provide sufficient conditions for input-to-state (ISS) stability of the proposed observer and for minimizing the $\mathcal{H}_{\infty}$ gain of the framer error system in the form of semi-definite programs (SDPs) with Linear Matrix Inequalities (LMIs) constraints. Finally, we compare the performance of the proposed $\mathcal{H}_{\infty}$-optimal interval observers with some benchmark CT and DT interval observers.      
### 33.Dawn of the transformer era in speech emotion recognition: closing the valence gap  [ :arrow_down: ](https://arxiv.org/pdf/2203.07378.pdf)
>  Recent advances in transformer-based architectures which are pre-trained in self-supervised manner have shown great promise in several machine learning tasks. In the audio domain, such architectures have also been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of .638 on MSP-Podcast. Furthermore, our investigations reveal that transformer-based architectures are more robust to small perturbations compared to a CNN-based baseline and fair with respect to biological sex groups, but not towards individual speakers. Finally, we are the first to show that their extraordinary success on valence is based on implicit linguistic information learnt during fine-tuning of the transformer layers, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. Our findings collectively paint the following picture: transformer-based architectures constitute the new state-of-the-art in SER, but further advances are needed to mitigate remaining robustness and individual speaker issues. To make our findings reproducible, we release the best performing model to the community.      
### 34.SATr: Slice Attention with Transformer for Universal Lesion Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.07373.pdf)
>  Universal Lesion Detection (ULD) in computed tomography plays an essential role in computer-aided diagnosis. Promising ULD results have been reported by multi-slice-input detection approaches which model 3D context from multiple adjacent CT slices, but such methods still experience difficulty in obtaining a global representation among different slices and within each individual slice since they only use convolution-based fusion operations. In this paper, we propose a novel Slice Attention Transformer (SATr) block which can be easily plugged into convolution-based ULD backbones to form hybrid network structures. Such newly formed hybrid backbones can better model long-distance feature dependency via the cascaded self-attention modules in the Transformer block while still holding a strong power of modeling local features with the convolutional operations in the original backbone. Experiments with five state-of-the-art methods show that the proposed SATr block can provide an almost free boost to lesion detection accuracy without extra hyperparameters or special network designs.      
### 35.Intelligent Reconfigurable Surfaces vs. Decode-and-Forward: What is the Impact of Electromagnetic Interference?  [ :arrow_down: ](https://arxiv.org/pdf/2203.08046.pdf)
>  This paper considers the use of an intelligent reconfigurable surface (IRS) to aid wireless communication systems. The main goal is to compare this emerging technology with conventional decode-and-forward (DF) relaying. Unlike prior comparisons, we assume that electromagnetic interference (EMI), consisting of incoming waves from external sources, is present at the location where the IRS or DF relay are placed. The analysis, in terms of minimizing the total transmit power, shows that EMI has a strong impact on DF relay-assisted communications, even when the relaying protocol is optimized against EMI. It turns out that IRS-aided communications is more resilient to EMI. To beat an IRS, we show that the DF relay must use multiple antennas and actively suppress the EMI by beamforming.      
### 36.Leveraging Uni-Modal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.07996.pdf)
>  Training Transformer-based models demands a large amount of data, while obtaining parallel aligned and labelled data in multimodality is rather cost-demanding, especially for audio-visual speech recognition (AVSR). Thus it makes a lot of sense to make use of unlabelled uni-modal data. On the other side, although the effectiveness of large-scale self-supervised learning is well established in both audio and visual modalities, how to integrate those pre-trained models into a multimodal scenario remains underexplored. In this work, we successfully leverage uni-modal self-supervised learning to promote the multimodal AVSR. In particular, we first train audio and visual encoders on a large-scale uni-modal dataset, then we integrate components of both encoders into a larger multimodal framework which learns to recognize paired audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from uni-modal self-supervised learning cooperate well, resulting in that the multimodal framework yields competitive results through fine-tuning. Our model is experimentally validated on both word-level and sentence-level AVSR tasks. Especially, even without an external language model, our proposed model raises the state-of-the-art performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%.      
### 37.An Ultra-Compact Single FeFET Binary and Multi-Bit Associative Search Engine  [ :arrow_down: ](https://arxiv.org/pdf/2203.07948.pdf)
>  Content addressable memory (CAM) is widely used in associative search tasks for its highly parallel pattern matching capability. To accommodate the increasingly complex and data-intensive pattern matching tasks, it is critical to keep improving the CAM density to enhance the performance and area efficiency. In this work, we demonstrate: i) a novel ultra-compact 1FeFET CAM design that enables parallel associative search and in-memory hamming distance calculation; ii) a multi-bit CAM for exact search using the same CAM cell; iii) compact device designs that integrate the series resistor current limiter into the intrinsic FeFET structure to turn the 1FeFET1R into an effective 1FeFET cell; iv) a successful 2-step search operation and a sufficient sensing margin of the proposed binary and multi-bit 1FeFET1R CAM array with sizes of practical interests in both experiments and simulations, given the existing unoptimized FeFET device variation; v) 89.9x speedup and 66.5x energy efficiency improvement over the state-of-the art alignment tools on GPU in accelerating genome pattern matching applications through the hyperdimensional computing paradigm.      
### 38.Generalized Rectifier Wavelet Covariance Models For Texture Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2203.07902.pdf)
>  State-of-the-art maximum entropy models for texture synthesis are built from statistics relying on image representations defined by convolutional neural networks (CNN). Such representations capture rich structures in texture images, outperforming wavelet-based representations in this regard. However, conversely to neural networks, wavelets offer meaningful representations, as they are known to detect structures at multiple scales (e.g. edges) in images. In this work, we propose a family of statistics built upon non-linear wavelet based representations, that can be viewed as a particular instance of a one-layer CNN, using a generalized rectifier non-linearity. These statistics significantly improve the visual quality of previous classical wavelet-based models, and allow one to produce syntheses of similar quality to state-of-the-art models, on both gray-scale and color textures.      
### 39.Superconducting qubits as musical synthesizers for live performance  [ :arrow_down: ](https://arxiv.org/pdf/2203.07879.pdf)
>  In the frame of a year-long artistic residency at the Yale Quantum Institute in 2019, artist and technologist Spencer Topel and quantum physicists Kyle Serniak and Luke Burkhart collaborated to create Quantum Sound, the first-ever music created and performed directly from measurements of superconducting quantum devices. Using analog- and digital-signal-processing sonification techniques, the team transformed GHz-frequency signals from experiments inside dilution refrigerators into audible sounds. The project was performed live at the International Festival of Arts and Ideas in New Haven, Connecticut on June 14, 2019 as a structured improvisation using the synthesis methods described in this chapter. At the interface between research and art, Quantum Sound represents an earnest attempt to produce a sonic reflection of the quantum realm.      
### 40.End-to-end P300 BCI using Bayesian accumulation of Riemannian probabilities  [ :arrow_down: ](https://arxiv.org/pdf/2203.07807.pdf)
>  In brain-computer interfaces (BCI), most of the approaches based on event-related potential (ERP) focus on the detection of P300, aiming for single trial classification for a speller task. While this is an important objective, existing P300 BCI still require several repetitions to achieve a correct classification accuracy. Signal processing and machine learning advances in P300 BCI mostly revolve around the P300 detection part, leaving the character classification out of the scope. To reduce the number of repetitions while maintaining a good character classification, it is critical to embrace the full classification problem. We introduce an end-to-end pipeline, starting from feature extraction, and is composed of an ERP-level classification using probabilistic Riemannian MDM which feeds a character-level classification using Bayesian accumulation of confidence across trials. Whereas existing approaches only increase the confidence of a character when it is flashed, our new pipeline, called Bayesian accumulation of Riemannian probabilities (ASAP), update the confidence of each character after each flash. We provide the proper derivation and theoretical reformulation of this Bayesian approach for a seamless processing of information from signal to BCI characters. We demonstrate that our approach performs significantly better than standard methods on public P300 datasets.      
### 41.Comparison of propagation models and forward calculation methods on cellular, tissue and organ scale atrial electrophysiology  [ :arrow_down: ](https://arxiv.org/pdf/2203.07776.pdf)
>  Objective: The bidomain model and the finite element method are an established standard to mathematically describe cardiac electrophysiology, but are both suboptimal choices for fast and large-scale simulations due to high computational costs. We investigate to what extent simplified approaches for propagation models (monodomain, reaction-eikonal and eikonal) and forward calculation (boundary element and infinite volume conductor) deliver markedly accelerated, yet physiologically accurate simulation results in atrial electrophysiology. Methods: We compared action potential durations, local activation times (LATs), and electrocardiograms (ECGs) for sinus rhythm simulations on healthy and fibrotically infiltrated atrial models. Results: All simplified model solutions yielded LATs and P waves in accurate accordance with the bidomain results. Only for the eikonal model with pre-computed action potential templates shifted in time to derive transmembrane voltages, repolarization behavior notably deviated from the bidomain results. ECGs calculated with the boundary element method were characterized by correlation coefficients &gt;0.9 compared to the finite element method. The infinite volume conductor method led to lower correlation coefficients caused predominantly by systematic overestimations of P wave amplitudes in the precordial leads. Conclusion: Our results demonstrate that the eikonal model yields accurate LATs and combined with the boundary element method precise ECGs compared to markedly more expensive full bidomain simulations. However, for an accurate representation of atrial repolarization dynamics, diffusion terms must be accounted for in simplified models. Significance: Simulations of atrial LATs and ECGs can be notably accelerated to clinically feasible time frames at high accuracy by resorting to the eikonal and boundary element methods.      
### 42.Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2203.07747.pdf)
>  Model Predictive Control (MPC) has become a popular framework in embedded control for high-performance autonomous systems. However, to achieve good control performance using MPC, an accurate dynamics model is key. To maintain real-time operation, the dynamics models used on embedded systems have been limited to simple first-principle models, which substantially limits their representative power. In contrast, neural networks can model complex effects purely from data. In contrast to such simple models, machine learning approaches such as neural networks have been shown to accurately model even complex dynamic effects, but their large computational complexity hindered combination with fast real-time iteration loops. With this work, we present Neural-MPC, a framework to efficiently integrate large, complex neural network architectures as dynamics models within a model-predictive control pipeline. Our experiments, performed in simulation and the real world on a highly agile quadrotor platform, demonstrate up to 83% reduction in positional tracking error when compared to state-of-the-art MPC approaches without neural network dynamics.      
### 43.Bio-inspired Multi-robot Autonomy  [ :arrow_down: ](https://arxiv.org/pdf/2203.07718.pdf)
>  Increasingly, high value industrial markets are driving trends for improved functionality and resilience from resident autonomous systems. This led to an increase in multi-robot fleets that aim to leverage the complementary attributes of the diverse platforms. In this paper we introduce a novel bio-inspired Symbiotic System of Systems Approach (SSOSA) for designing the operational governance of a multi-robot fleet consisting of ground-based quadruped and wheeled platforms. SSOSA couples the MR-fleet to the resident infrastructure monitoring systems into one collaborative digital commons. The hyper visibility of the integrated distributed systems, achieved through a latency bidirectional communication network, supports collaboration, coordination and corroboration (3C) across the integrated systems. In our experiment, we demonstrate how an operator can activate a pre-determined autonomous mission and utilize SSOSA to overcome intrinsic and external risks to the autonomous missions. We demonstrate how resilience can be enhanced by local collaboration between SPOT and Husky wherein we detect a replacement battery, and utilize the manipulator arm of SPOT to support a Clearpath Husky A200 wheeled robotic platform. This allows for increased resilience of an autonomous mission as robots can collaborate to ensure the battery state of the Husky robot. Overall, these initial results demonstrate the value of a SSOSA approach in addressing a key operational barrier to scalable autonomy, the resilience.      
### 44.Benchmarking and Interpreting End-to-end Learning of MIMO and Multi-User Communication  [ :arrow_down: ](https://arxiv.org/pdf/2203.07703.pdf)
>  End-to-end autoencoder (AE) learning has the potential of exceeding the performance of human-engineered transceivers and encoding schemes, without a priori knowledge of communication-theoretic principles. In this work, we aim to understand to what extent and for which scenarios this claim holds true when comparing with fair benchmarks. Our particular focus is on memoryless multiple-input multiple-output (MIMO) and multi-user (MU) systems. Four case studies are considered: two point-to-point (closed-loop and open-loop MIMO) and two MU scenarios (MIMO broadcast and interference channels). For the point-to-point scenarios, we explain some of the performance gains observed in prior work through the selection of improved baseline schemes that include geometric shaping as well as bit and power allocation. For the MIMO broadcast channel, we demonstrate the feasibility of a novel AE method with centralized learning and decentralized execution. Interestingly, the learned scheme performs close to nonlinear vector-perturbation precoding and significantly outperforms conventional zero-forcing. Lastly, we highlight potential pitfalls when interpreting learned communication schemes. In particular, we show that the AE for the considered interference channel learns to avoid interference, albeit in a rotated reference frame. After de-rotating the learned signal constellation of each user, the resulting scheme corresponds to conventional time sharing with geometric shaping.      
### 45.Towards Adversarial Control Loops in Sensor Attacks: A Case Study to Control the Kinematics and Actuation of Embedded Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.07670.pdf)
>  Recent works investigated attacks on sensors by influencing analog sensor components with acoustic, light, and electromagnetic signals. Such attacks can have extensive security, reliability, and safety implications since many types of the targeted sensors are also widely used in critical process control, robotics, automation, and industrial control systems. While existing works advanced our understanding of the physical-level risks that are hidden from a digital-domain perspective, gaps exist in how the attack can be guided to achieve system-level control in real-time, continuous processes. This paper proposes an adversarial control loop-based approach for real-time attacks on control systems relying on sensors. We study how to utilize the system feedback extracted from physical-domain signals to guide the attacks. In the attack process, injection signals are adjusted in real time based on the extracted feedback to exert targeted influence on a victim control system that is continuously affected by the injected perturbations and applying changes to the physical environment. In our case study, we investigate how an external adversarial control system can be constructed over sensor-actuator systems and demonstrate the attacks with program-controlled processes to manipulate the victim system without accessing its internal statuses.      
### 46.Innovations in trigger and data acquisition systems for next-generation physics facilities  [ :arrow_down: ](https://arxiv.org/pdf/2203.07620.pdf)
>  Data-intensive physics facilities are increasingly reliant on heterogeneous and large-scale data processing and computational systems in order to collect, distribute, process, filter, and analyze the ever increasing huge volumes of data being collected. Moreover, these tasks are often performed in hard real-time or quasi real-time processing pipelines that place extreme constraints on various parameters and design choices for those systems. Consequently, a large number and variety of challenges are faced to design, construct, and operate such facilities. This is especially true at the energy and intensity frontiers of particle physics where bandwidths of raw data can exceed 100 Tb/s of heterogeneous, high-dimensional data sourced from &gt;300M individual sensors. Data filtering and compression algorithms deployed at these facilities often operate at the level of 1 part in $10^5$, and once executed, these algorithms drive the data curation process, further highlighting the critical roles that these systems have in the physics impact of those endeavors. This White Paper aims to highlight the challenges that these facilities face in the design of the trigger and data acquisition instrumentation and systems, as well as in their installation, commissioning, integration and operation, and in building the domain knowledge and technical expertise required to do so.      
### 47.Distributed Coded Modulation Schemes for Multiple Access Relay Channels  [ :arrow_down: ](https://arxiv.org/pdf/2203.07583.pdf)
>  In this paper, we investigate network nest coded modulation schemes for multiple access relay channels. The performance of the distributed systems which are based on distributed convolutional codes with network coded modulation is presented. An analytical upper bound on bit error probability performance for the studied distributed systems with Maximum Likelihood Sequence Detection (MLSD) is derived. The constructured bounds for the investigated systems are shown to be asymptotically tight for increasing channel Signal-to-Noise Ratio (SNR) values      
### 48.Thermodynamic engine powered by anisotropic fluctuations  [ :arrow_down: ](https://arxiv.org/pdf/2203.07573.pdf)
>  The purpose of this work is to present the concept of an autonomous Stirling-like engine powered by anisotropy of thermodynamic fluctuations. Specifically, simultaneous contact of a thermodynamic system with two heat baths along coupled degrees of freedom generates torque and circulatory currents -- an arrangement referred to as a Brownian gyrator. The embodiment that constitutes the engine includes an inertial wheel to sustain rotary motion and average out the generated fluctuating torque, ultimately delivering power to an external load. We detail an electrical model for such an engine that consists of two resistors in different temperatures and three reactive elements in the form of variable capacitors. The resistors generate Johnson-Nyquist current fluctuations that power the engine, while the capacitors generate driving forces via a coupling of their dielectric material with the inertial wheel. A proof-of-concept is established via stability analysis to ensure the existence of a stable periodic orbit generating sustained power output. We conclude by drawing a connection to the dynamics of a damped pendulum with constant torque and to those of a macroscopic Stirling engine. The sought insights aim at nano-engines and biological processes that are similarly powered by anisotropy in temperature and chemical potentials.      
### 49.Resilience of Input Metering in Dynamic Flow Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.07555.pdf)
>  In this paper, we study robustness of input metering policies in dynamic flow networks in the presence of transient disturbances and attacks. We consider a compartmental model for dynamic flow networks with a First-In-First-Out (FIFO) routing rule as found in, e.g., transportation networks. We model the effect of the transient disturbance as an abrupt change to the state of the network and use the notion of the region of attraction to measure the resilience of the network to these changes. For constant and periodic input metering, we introduce the notion of monotone-invariant points to establish inner-estimates for the regions of attraction of free-flow equilibrium points and free-flow periodic orbits using monotone systems theory. These results are applicable to, e.g., networks with cycles, which have not been considered in prior literature on dynamic flow networks with FIFO routing. Finally, we propose two approaches for finding suitable monotone-invariant points in the flow networks with FIFO rules.      
### 50.Agile Maneuvers in Legged Robots: a Predictive Control Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.07554.pdf)
>  Achieving agile maneuvers through multiple contact phases has been a longstanding challenge in legged robotics. It requires to derive motion plans and local control feedback policies in real-time to handle the nonholonomy of the kinetic momenta. While a few recent predictive control approaches based on centroidal momentum have been able to generate dynamic motions, they assume unlimited actuation capabilities. This assumption is quite restrictive and does not hold for agile maneuvers on most robots. In this work, we present a contact-phase predictive and state-feedback controllers that enables legged robots to plan and perform agile locomotion skills. Our predictive controller models the contact phases using a hybrid paradigm that considers the robot's actuation limits and full dynamics. We demonstrate the benefits of our approach on agile maneuvers on ANYmal robots in realistic scenarios. To the best of our knowledge, our work is the first to show that predictive control can handle actuation limits, generate agile locomotion maneuvers and execute locally optimal feedback policies on hardware without the use of a separate whole-body controller.      
### 51.Approaching Massive MIMO Performance with Reconfigurable Intelligent Surfaces: We Do Not Need Many Antennas  [ :arrow_down: ](https://arxiv.org/pdf/2203.07493.pdf)
>  This paper considers an antenna structure where a (non-large) array of radiating elements is placed at short distance in front of a reconfigurable intelligent surface (RIS). This structure is analyzed as a possible emulator of a traditional MIMO antenna with a large number of active antenna elements and RF chains. Focusing on both the cases of active and passive RIS, we tackle the issues of channel estimation, downlink signal processing, power control, and RIS configuration optimization. With regard to the last point, an optimization problem is formulated and solved, both for the cases of active and passive RIS, aimed at minimizing the channel signatures cross-correlations and thereby reducing the interference. Downlink spectral efficiency (SE) formulas are also derived by using the popular hardening lower-bound. Numerical results, represented with reference to max-fairness power control, show that the proposed structure is capable of outperforming conventional non-RIS aided MIMO systems even when the MIMO system has a considerably larger number of antennas and RF chains. The proposed antenna structure is thus shown to be able to approach massive MIMO performance levels in a cost-effective way with reduced hardware resources.      
### 52.Robust Dynamic Walking for a 3D Dual-SLIP Model under One-Step Unilateral Stiffness Perturbations: Towards Bipedal Locomotion over Compliant Terrain  [ :arrow_down: ](https://arxiv.org/pdf/2203.07471.pdf)
>  Bipedal walking is one of the most important hallmarks of human that robots have been trying to mimic for many decades. Although previous control methodologies have achieved robot walking on some terrains, there is a need for a framework allowing stable and robust locomotion over a wide range of compliant surfaces. This work proposes a novel biomechanics-inspired controller that adjusts the stiffness of the legs in support for robust and dynamic bipedal locomotion over compliant terrains. First, the 3D Dual-SLIP model is extended to support for the first time locomotion over compliant surfaces with variable stiffness and damping parameters. Then, the proposed controller is compared to a Linear-Quadratic Regulator (LQR) controller, in terms of robustness on stepping on soft terrain. The LQR controller is shown to be robust only up to a moderate ground stiffness level of 174 kN/m, while it fails in lower stiffness levels. On the contrary, the proposed controller can produce stable gait in stiffness levels as low as 30 kN/m, which results in a vertical ground penetration of the leg that is deeper than 10% of its rest length. The proposed framework could advance the field of bipedal walking, by generating stable walking trajectories for a wide range of compliant terrains useful for the control of bipeds and humanoids, as well as by improving controllers for prosthetic devices with tunable stiffness.      
### 53.ShapeNet: Shape Constraint for Galaxy Image Deconvolution  [ :arrow_down: ](https://arxiv.org/pdf/2203.07412.pdf)
>  Deep Learning (DL) has shown remarkable results in solving inverse problems in various domains. In particular, the Tikhonet approach is very powerful to deconvolve optical astronomical images (Sureau et al. 2020). Yet, this approach only uses the $\ell_2$ loss, which does not guarantee the preservation of physical information (e.g. flux and shape) of the object reconstructed in the image. In Nammour et al. (2021), a new loss function was proposed in the framework of sparse deconvolution, which better preserves the shape of galaxies and reduces the pixel error. In this paper, we extend Tikhonet to take into account this shape constraint, and apply our new DL method, called ShapeNet, to optical and radio-interferometry simulated data set. The originality of the paper relies on i) the shape constraint we use in the neural network framework, ii) the application of deep learning to radio-interferometry image deconvolution for the first time, and iii) the generation of a simulated radio data set that we make available for the community. A range of examples illustrates the results.      
