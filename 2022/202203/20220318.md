# ArXiv eess --Fri, 18 Mar 2022
### 1.Defending Against Adversarial Attack in ECG Classification with Adversarial Distillation Training  [ :arrow_down: ](https://arxiv.org/pdf/2203.09487.pdf)
>  In clinics, doctors rely on electrocardiograms (ECGs) to assess severe cardiac disorders. Owing to the development of technology and the increase in health awareness, ECG signals are currently obtained by using medical and commercial devices. Deep neural networks (DNNs) can be used to analyze these signals because of their high accuracy rate. However, researchers have found that adversarial attacks can significantly reduce the accuracy of DNNs. Studies have been conducted to defend ECG-based DNNs against traditional adversarial attacks, such as projected gradient descent (PGD), and smooth adversarial perturbation (SAP) which targets ECG classification; however, to the best of our knowledge, no study has completely explored the defense against adversarial attacks targeting ECG classification. Thus, we did different experiments to explore the effects of defense methods against white-box adversarial attack and black-box adversarial attack targeting ECG classification, and we found that some common defense methods performed well against these attacks. Besides, we proposed a new defense method called Adversarial Distillation Training (ADT) which comes from defensive distillation and can effectively improve the generalization performance of DNNs. The results show that our method performed more effectively against adversarial attacks targeting on ECG classification than the other baseline methods, namely, adversarial training, defensive distillation, Jacob regularization, and noise-to-signal ratio regularization. Furthermore, we found that our method performed better against PGD attacks with low noise levels, which means that our method has stronger robustness.      
### 2.Distributed formation control of networked mechanical systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.09484.pdf)
>  This paper investigates a distributed formation tracking control law for large-scale networks of mechanical systems. In particular, the formation network is represented by a directed communication graph with leaders and followers, where each agent is described as a port-Hamiltonian system with a constant mass matrix. Moreover, we adopt a distributed parameter approach to prove the scalable asymptotic stability of the network formation, i.e., the scalability with respect to the network size and the specific formation preservation. A simulation case illustrates the effectiveness of the proposed control approach.      
### 3.Causality issue in the heat balance method for calculating the design heating and cooling load  [ :arrow_down: ](https://arxiv.org/pdf/2203.09480.pdf)
>  Heating and cooling load calculation based on dynamic models is widely used in simulation software and it is the method recommended by ASHRAE and CEN. The principle is to make the heat balance for the air volume of a room space considered at uniform temperature and to calculate from this equation the load, i.e. the power needed to obtain the required indoor temperature. The problem is that, by doing so, the physical causality is not respected. If the model is approximated by a piecewise linear dynamical system, this procedure results in an improper transfer function. In order to point out this problem, a method to obtain state space and transfer function models from thermal networks is introduced. Then, the transfer function representation is employed to show that changing the physical causality results in an improper transfer function. The practical consequence is that when the space temperature has a step variation, the calculated load tends to infinity if the simulation time step tends to zero. The issue of causality may be a problem in equation-based simulation software, such as MODELICA, in which the equations do not represent causal relations: a wrong choice of the causality in a balance equation may result in improper transfer functions.      
### 4.A Decomposition-Based Hybrid Ensemble CNN Framework for Improving Cross-Subject EEG Decoding Performance  [ :arrow_down: ](https://arxiv.org/pdf/2203.09477.pdf)
>  Electroencephalogram (EEG) signals are complex, non-linear, and non-stationary in nature. However, previous studies that applied decomposition to minimize the complexity mainly exploited the hand-engineering features, limiting the information learned in EEG decoding. Therefore, extracting additional primary features from different disassembled components to improve the EEG-based recognition performance remains challenging. On the other hand, attempts have been made to use a single model to learn the hand-engineering features. Less work has been done to improve the generalization ability through ensemble learning. In this work, we propose a novel decomposition-based hybrid ensemble convolutional neural network (CNN) framework to enhance the capability of decoding EEG signals. CNNs, in particular, automatically learn the primary features from raw disassembled components but not handcraft features. The first option is to fuse the obtained score before the Softmax layer and execute back-propagation on the entire ensemble network, whereas the other is to fuse the probability output of the Softmax layer. Moreover, a component-specific batch normalization (CSBN) layer is employed to reduce subject variability. Against the challenging cross-subject driver fatigue-related situation awareness (SA) recognition task, eight models are proposed under the framework, which all showed superior performance than the strong baselines. The performance of different decomposition methods and ensemble modes were further compared. Results indicated that discrete wavelet transform (DWT)-based ensemble CNN achieves the best 82.11% among the proposed models. Our framework can be simply extended to any CNN architecture and applied in any EEG-related sectors, opening the possibility of extracting more preliminary information from complex EEG data.      
### 5.Beyond the Limitation of Pulse Width in Optical Time-domain Reflectometry  [ :arrow_down: ](https://arxiv.org/pdf/2203.09461.pdf)
>  Optical time-domain reflectometry (OTDR) is the basis for distributed time-domain optical fiber sensing techniques. By injecting pulse light into an optical fiber, the distance information of an event can be obtained based on the time of light flight. The minimum distinguishable event separation along the fiber length is called the spatial resolution, which is determined by the optical pulse width. By reducing the pulse width, the spatial resolution can be improved. However, at the same time, the signal-to-noise ratio of the system is degraded, and higher speed equipment is required. To solve this problem, data processing methods such as iterative subdivision, deconvolution, and neural networks have been proposed. However, they all have some shortcomings and thus have not been widely applied. Here, we propose and experimentally demonstrate an OTDR deconvolution neural network based on deep convolutional neural networks. A simplified OTDR model is built to generate a large amount of training data. By optimizing the network structure and training data, an effective OTDR deconvolution is achieved. The simulation and experimental results show that the proposed neural network can achieve more accurate deconvolution than the conventional deconvolution algorithm with a higher signal-to-noise ratio.      
### 6.Covariance Recovery for One-Bit Sampled Data With Time-Varying Sampling Thresholds-Part I: Stationary Signals  [ :arrow_down: ](https://arxiv.org/pdf/2203.09460.pdf)
>  One-bit quantization, which relies on comparing the signals of interest with given threshold levels, has attracted considerable attention in signal processing for communications and sensing. A useful tool for covariance recovery in such settings is the arcsine law, that estimates the normalized covariance matrix of zero-mean stationary input signals. This relation, however, only considers a zero sampling threshold, which can cause a remarkable information loss. In this paper, the idea of the arcsine law is extended to the case where one-bit analog-to-digital converters (ADCs) apply time-varying thresholds. Specifically, three distinct approaches are proposed, investigated, and compared, to recover the autocorrelation sequence of the stationary signals of interest. Additionally, we will study a modification of the Bussgang law, a famous relation facilitating the recovery of the cross-correlation between the one-bit sampled data and the zero-mean stationary input signal. Similar to the case of the arcsine law, the Bussgang law only considers a zero sampling threshold. This relation is also extended to accommodate the more general case of time-varying thresholds for the stationary input signals.      
### 7.Using the Order of Tomographic Slices as a Prior for Neural Networks Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2203.09372.pdf)
>  The technical advances in Computed Tomography (CT) allow to obtain immense amounts of 3D data. For such datasets it is very costly and time-consuming to obtain the accurate 3D segmentation markup to train neural networks. The annotation is typically done for a limited number of 2D slices, followed by an interpolation. In this work, we propose a pre-training method SortingLoss. It performs pre-training on slices instead of volumes, so that a model could be fine-tuned on a sparse set of slices, without the interpolation step. Unlike general methods (e.g. SimCLR or Barlow Twins), the task specific methods (e.g. Transferable Visual Words) trade broad applicability for quality benefits by imposing stronger assumptions on the input data. We propose a relatively mild assumption -- if we take several slices along some axis of a volume, structure of the sample presented on those slices, should give a strong clue to reconstruct the correct order of those slices along the axis. Many biomedical datasets fulfill this requirement due to the specific anatomy of a sample and pre-defined alignment of the imaging setup. We examine the proposed method on two datasets: medical CT of lungs affected by COVID-19 disease, and high-resolution synchrotron-based full-body CT of model organisms (Medaka fish). We show that the proposed method performs on par with SimCLR, while working 2x faster and requiring 1.5x less memory. In addition, we present the benefits in terms of practical scenarios, especially the applicability to the pre-training of large models and the ability to localize samples within volumes in an unsupervised setup.      
### 8.POSTER: Diagnosis of COVID-19 through Transfer Learning Techniques on CT Scans: A Comparison of Deep Learning Models  [ :arrow_down: ](https://arxiv.org/pdf/2203.09348.pdf)
>  The novel coronavirus disease (COVID-19) constitutes a public health emergency globally. It is a deadly disease which has infected more than 230 million people worldwide. Therefore, early and unswerving detection of COVID-19 is necessary. Evidence of this virus is most commonly being tested by RT-PCR test. This test is not 100% reliable as it is known to give false positives and false negatives. Other methods like X-Ray images or CT scans show the detailed imaging of lungs and have been proven more reliable. This paper compares different deep learning models used to detect COVID-19 through transfer learning technique on CT scan dataset. VGG-16 outperforms all the other models achieving an accuracy of 85.33% on the dataset.      
### 9.A Pricing Mechanism for Balancing the Charging of Ride-Hailing Electric Vehicle Fleets  [ :arrow_down: ](https://arxiv.org/pdf/2203.09327.pdf)
>  Both ride-hailing services and electric vehicles are becoming increasingly popular and it is likely that charging management of the ride-hailing vehicles will be a significant part of the ride-hailing company's operation in the near future. Motivated by this, we propose a game theoretic model for charging management, where we assume that it is the fleet-operator that wants to minimize its operational cost, which among others include the price of charging. To avoid overcrowded charging stations, a central authority will design pricing policies to incentivize the vehicles to spread out among the charging stations, in a setting where several ride-hailing companies compete about the resources. We show that it is possible to construct pricing policies that make the Nash-equilibrium between the companies follow the central authority's target value when the desired load is feasible. Moreover, we provide a decentralized algorithm for computation of the equilibrium and conclude the paper with a numerical example illustrating the results.      
### 10.Progressive Subsampling for Oversampled Data -- Application to Quantitative MRI  [ :arrow_down: ](https://arxiv.org/pdf/2203.09268.pdf)
>  We present PROSUB: PROgressive SUBsampling, a deep learning based, automated methodology that subsamples an oversampled data set (e.g. multi-channeled 3D images) with minimal loss of information. We build upon a recent dual-network approach that won the MICCAI MUlti-DIffusion (MUDI) quantitative MRI measurement sampling-reconstruction challenge, but suffers from deep learning training instability, by subsampling with a hard decision boundary. PROSUB uses the paradigm of recursive feature elimination (RFE) and progressively subsamples measurements during deep learning training, improving optimization stability. PROSUB also integrates a neural architecture search (NAS) paradigm, allowing the network architecture hyperparameters to respond to the subsampling process. We show PROSUB outperforms the winner of the MUDI MICCAI challenge, producing large improvements &gt;18% MSE on the MUDI challenge sub-tasks and qualitative improvements on downstream processes useful for clinical applications. We also show the benefits of incorporating NAS and analyze the effect of PROSUB's components. As our method generalizes to other problems beyond MRI measurement selection-reconstruction, our code is <a class="link-external link-https" href="https://github.com/sbb-gh/PROSUB" rel="external noopener nofollow">this https URL</a>      
### 11.Frequency-Selective Mesh-to-Mesh Resampling for Color Upsampling of Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2203.09224.pdf)
>  With the increased use of virtual and augmented reality applications, the importance of point cloud data rises. High-quality capturing of point clouds is still expensive and thus, the need for point cloud super-resolution or point cloud upsampling techniques emerges. In this paper, we propose an interpolation scheme for color upsampling of three-dimensional color point clouds. As a point cloud represents an object's surface in three-dimensional space, we first conduct a local transform of the surface into a two-dimensional plane. Secondly, we propose to apply a novel Frequency-Selective Mesh-to-Mesh Resampling (FSMMR) technique for the interpolation of the points in 2D. FSMMR generates a model of weighted superpositions of basis functions on scattered points. This model is then evaluated for the final points in order to increase the resolution of the original point cloud. Evaluation shows that our approach outperforms common interpolation schemes. Visual comparisons of the jaguar point cloud underlines the quality of our upsampling results. The high performance of FSMMR holds for various sampling densities of the input point cloud.      
### 12.Simulation-Driven Training of Vision Transformers Enabling Metal Segmentation in X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2203.09207.pdf)
>  In several image acquisition and processing steps of X-ray radiography, knowledge of the existence of metal implants and their exact position is highly beneficial (e.g. dose regulation, image contrast adjustment). Another application which would benefit from an accurate metal segmentation is cone beam computed tomography (CBCT) which is based on 2D X-ray projections. Due to the high attenuation of metals, severe artifacts occur in the 3D X-ray acquisitions. The metal segmentation in CBCT projections usually serves as a prerequisite for metal artifact avoidance and reduction algorithms. Since the generation of high quality clinical training is a constant challenge, this study proposes to generate simulated X-ray images based on CT data sets combined with self-designed computer aided design (CAD) implants and make use of convolutional neural network (CNN) and vision transformer (ViT) for metal segmentation. Model test is performed on accurately labeled X-ray test datasets obtained from specimen scans. The CNN encoder-based network like U-Net has limited performance on cadaver test data with an average dice score below 0.30, while the metal segmentation transformer with dual decoder (MST-DD) shows high robustness and generalization on the segmentation task, with an average dice score of 0.90. Our study indicates that the CAD model-based data generation has high flexibility and could be a way to overcome the problem of shortage in clinical data sampling and labelling. Furthermore, the MST-DD approach generates a more reliable neural network in case of training on simulated data.      
### 13.Novel Consistency Check For Fast Recursive Reconstruction Of Non-Regularly Sampled Video Data  [ :arrow_down: ](https://arxiv.org/pdf/2203.09200.pdf)
>  Quarter sampling is a novel sensor design that allows for an acquisition of higher resolution images without increasing the number of pixels. When being used for video data, one out of four pixels is measured in each frame. Effectively, this leads to a non-regular spatio-temporal sub-sampling. Compared to purely spatial or temporal sub-sampling, this allows for an increased reconstruction quality, as aliasing artifacts can be reduced. For the fast reconstruction of such sensor data with a fixed mask, recursive variant of frequency selective reconstruction (FSR) was proposed. Here, pixels measured in previous frames are projected into the current frame to support its reconstruction. In doing so, the motion between the frames is computed using template matching. Since some of the motion vectors may be erroneous, it is important to perform a proper consistency checking. In this paper, we propose faster consistency checking methods as well as a novel recursive FSR that uses the projected pixels different than in literature and can handle dynamic masks. Altogether, we are able to significantly increase the reconstruction quality by + 1.01 dB compared to the state-of-the-art recursive reconstruction method using a fixed mask. Compared to a single frame reconstruction, an average gain of about + 1.52 dB is achieved for dynamic masks. At the same time, the computational complexity of the consistency checks is reduced by a factor of 13 compared to the literature algorithm.      
### 14.A Novel End-To-End Network for Reconstruction of Non-Regularly Sampled Image Data Using Locally Fully Connected Layers  [ :arrow_down: ](https://arxiv.org/pdf/2203.09180.pdf)
>  Quarter sampling and three-quarter sampling are novel sensor concepts that enable the acquisition of higher resolution images without increasing the number of pixels. This is achieved by non-regularly covering parts of each pixel of a low-resolution sensor such that only one quadrant or three quadrants of the sensor area of each pixel is sensitive to light. Combining a properly designed mask and a high-quality reconstruction algorithm, a higher image quality can be achieved than using a low-resolution sensor and subsequent upsampling. For the latter case, the image quality can be further enhanced using super resolution algorithms such as the very deep super resolution network (VDSR). In this paper, we propose a novel end-to-end neural network to reconstruct high resolution images from non-regularly sampled sensor data. The network is a concatenation of a locally fully connected reconstruction network (LFCR) and a standard VDSR network. Altogether, using a three-quarter sampling sensor with our novel neural network layout, the image quality in terms of PSNR for the Urban100 dataset can be increased by 2.96 dB compared to the state-of-the-art approach. Compared to a low-resolution sensor with VDSR, a gain of 1.11 dB is achieved.      
### 15.Harmonic Pole Placement  [ :arrow_down: ](https://arxiv.org/pdf/2203.09140.pdf)
>  In this paper, we propose a method to design state feedback harmonic control laws that assign the closed loop poles of a linear harmonic model to some desired locations. The procedure is based on the solution of an infinite-dimensional harmonic Sylvester equation under an invertibility constraint. We provide a sufficient condition to ensure this invertibility and show how this infinite-dimensional Sylvester equation can be solved up to an arbitrary small error. The results are illustrated on an unstable linear periodic system. We also provide a counterexample to illustrate the fact that, unlike the classical finite dimensional case, the solution of the Sylvester equation may not be invertible in the infinite dimensional case even if an observability condition is satisfied.      
### 16.Feature-informed Latent Space Regularization for Music Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2203.09132.pdf)
>  The integration of additional side information to improve music source separation has been investigated numerous times, e.g., by adding features to the input or by adding learning targets in a multi-task learning scenario. These approaches, however, require additional annotations such as musical scores, instrument labels, etc. in training and possibly during inference. The available datasets for source separation do not usually provide these additional annotations. In this work, we explore transfer learning strategies to incorporate VGGish features with a state-of-the-art source separation model; VGGish features are known to be a very condensed representation of audio content and have been successfully used in many MIR tasks. We introduce three approaches to incorporate the features, including two latent space regularization methods and one naive concatenation method. Experimental results show that our proposed approaches improve several evaluation metrics for music source separation.      
### 17.To train or not to train adversarially: A study of bias mitigation strategies for speaker recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.09122.pdf)
>  Speaker recognition is increasingly used in several everyday applications including smart speakers, customer care centers and other speech-driven analytics. It is crucial to accurately evaluate and mitigate biases present in machine learning (ML) based speech technologies, such as speaker recognition, to ensure their inclusive adoption. ML fairness studies with respect to various demographic factors in modern speaker recognition systems are lagging compared to other human-centered applications such as face recognition. Existing studies on fairness in speaker recognition systems are largely limited to evaluating biases at specific operating points of the systems, which can lead to false expectations of fairness. Moreover, there are only a handful of bias mitigation strategies developed for speaker recognition systems. In this paper, we systematically evaluate the biases present in speaker recognition systems with respect to gender across a range of system operating points. We also propose adversarial and multi-task learning techniques to improve the fairness of these systems. We show through quantitative and qualitative evaluations that the proposed methods improve the fairness of ASV systems over baseline methods trained using data balancing techniques. We also present a fairness-utility trade-off analysis to jointly examine fairness and the overall system performance. We show that although systems trained using adversarial techniques improve fairness, they are prone to reduced utility. On the other hand, multi-task methods can improve the fairness while retaining the utility. These findings can inform the choice of bias mitigation strategies in the field of speaker recognition.      
### 18.Proactive Posturing of Large Power Grid for Mitigating Hurricane Impacts  [ :arrow_down: ](https://arxiv.org/pdf/2203.09070.pdf)
>  In the past decade, natural disasters such as hurricanes have challenged the operation and control of U.S. power grid. It is crucial to develop proactive strategies to assist grid operators for better emergency response and minimized electricity service interruptions; the better the grid may be preserved, the faster the grid can be restored. In this paper, we propose a proactive posturing of power system elements, and formulate a Security-Constrained Optimal Power Flow (SCOPF) informed by cross-domain hurricane modeling as well as its potential impacts on grid elements. Simulation results based on real-world power grid and historical hurricane event have verified the applicability of the proposed optimization formulation, which shows potential to enable grid operators and planners with interactive cross-domain data analytics for mitigating hurricane impacts.      
### 19.Optimal Control of Partially Observable Markov Decision Processes with Finite Linear Temporal Logic Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2203.09038.pdf)
>  Autonomous agents often operate in scenarios where the state is partially observed. In addition to maximizing their cumulative reward, agents must execute complex tasks with rich temporal and logical structures. These tasks can be expressed using temporal logic languages like finite linear temporal logic (LTL_f). This paper, for the first time, provides a structured framework for designing agent policies that maximize the reward while ensuring that the probability of satisfying the temporal logic specification is sufficiently high. We reformulate the problem as a constrained partially observable Markov decision process (POMDP) and provide a novel approach that can leverage off-the-shelf unconstrained POMDP solvers for solving it. Our approach guarantees approximate optimality and constraint satisfaction with high probability. We demonstrate its effectiveness by implementing it on several models of interest.      
### 20.Robust Control Approaches for Minimizing the Bandwidth Ratio in Multi-Loop Control  [ :arrow_down: ](https://arxiv.org/pdf/2203.09022.pdf)
>  Conventional dc-dc, dc-ac converter control entail an inner current and outer voltage (ICOV) control loop. Stability of multi-loop control is achieved by ensuring faster dynamics of inner loops, often separating the bandwidths by large factors. Heuristically a factor-of-ten has often been used to separate the bandwidths of two adjacent loops in the nested architecture. In this paper, we present a numerical method to optimally select the ratio of bandwidths for a nested controller. We first manipulate the robust framework to show that the optimal $H_\infty$ subsumes the classical inner current outer voltage control. We then use optimality of the proposed $H_\infty$ controller to inspect feasibility of different ratios of bandwidths of the inner and outer controllers. We finally select the numerically closest bandwidths of the two nested loops which the guarantee stability. Simulation models are used to verify the optimal bandwidth separation for a candidate grid-forming dc-ac converter with inner current-outer voltage control.      
### 21.Structure-Preserving Model Reduction for Nonlinear Power Grid Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.09021.pdf)
>  We develop a structure-preserving system-theoretic model reduction framework for nonlinear power grid networks. First, via a lifting transformation, we convert the original nonlinear system with trigonometric nonlinearities to an equivalent quadratic nonlinear model. This equivalent representation allows us to employ the $\mathcal{H}_2$-based model reduction approach, Quadratic Iterative Rational Krylov Algorithm (Q-IRKA), as an intermediate model reduction step. Exploiting the structure of the underlying power network model, we show that the model reduction bases resulting from Q-IRKA have a special subspace structure, which allows us to effectively construct the final model reduction basis. This final basis is applied on the original nonlinear structure to yield a reduced model that preserves the physically meaningful (second-order) structure of the original model. The effectiveness of our proposed framework is illustrated via two numerical examples.      
### 22.National Radio Dynamic Zone Concept with Autonomous Aerial and Ground Spectrum Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2203.09014.pdf)
>  National radio dynamic zone (NRDZs) are intended to be geographically bounded areas within which controlled experiments can be carried out while protecting the nearby licensed users of the spectrum. An NRDZ will facilitate research and development of new spectrum technologies, waveforms, and protocols, in typical outdoor operational environments of such technologies. In this paper, we introduce and describe an NRDZ concept that relies on a combination of autonomous aerial and ground sensor nodes for spectrum sensing and radio environment monitoring (REM). We elaborate on key characteristics and features of an NRDZ to enable advanced wireless experimentation while also coexisting with licensed users. Some preliminary results based on simulation and experimental evaluations are also provided on out-of-zone leakage monitoring and real-time REMs.      
### 23.Koopman-based Differentiable Predictive Control for the Dynamics-Aware Economic Dispatch Problem  [ :arrow_down: ](https://arxiv.org/pdf/2203.08984.pdf)
>  The dynamics-aware economic dispatch (DED) problem embeds low-level generator dynamics and operational constraints to enable near real-time scheduling of generation units in a power network. DED produces a more dynamic supervisory control policy than traditional economic dispatch (T-ED) that leads to reduced overall generation costs. However, the incorporation of differential equations that govern the system dynamics makes DED an optimization problem that is computationally prohibitive to solve. In this work, we present a new data-driven approach based on differentiable programming to efficiently obtain parametric solutions to the underlying DED problem. In particular, we employ the recently proposed differentiable predictive control (DPC) for offline learning of explicit neural control policies using an identified Koopman operator (KO) model of the power system dynamics. We demonstrate the high solution quality and five orders of magnitude computational-time savings of the DPC method over the original online optimization-based DED approach on a 9-bus test power grid network.      
### 24.One-Bit Phase Retrieval: More Samples Means Less Complexity?  [ :arrow_down: ](https://arxiv.org/pdf/2203.08982.pdf)
>  The classical problem of phase retrieval has found a wide array of applications in optics, imaging and signal processing. In this paper, we consider the phase retrieval problem in a one-bit setting, where the signals are sampled using one-bit analog-to-digital converters (ADCs). A significant advantage of deploying one-bit ADCs in signal processing systems is their superior sampling rates as compared to their high-resolution counterparts. This leads to an enormous amount of one-bit samples gathered at the output of the ADC in a short period of time. We demonstrate that this advantage pays extraordinary dividends when it comes to convex phase retrieval formulations, namely that the often encountered matrix semi-definiteness constraints as well as rank constraints (that are computationally prohibitive to enforce), become redundant for phase retrieval in the face of a growing sample size. Several numerical results are presented to illustrate the effectiveness of the proposed methodologies.      
### 25.CapsNet for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.08948.pdf)
>  Convolutional Neural Networks (CNNs) have been successful in solving tasks in computer vision including medical image segmentation due to their ability to automatically extract features from unstructured data. However, CNNs are sensitive to rotation and affine transformation and their success relies on huge-scale labeled datasets capturing various input variations. This network paradigm has posed challenges at scale because acquiring annotated data for medical segmentation is expensive, and strict privacy regulations. Furthermore, visual representation learning with CNNs has its own flaws, e.g., it is arguable that the pooling layer in traditional CNNs tends to discard positional information and CNNs tend to fail on input images that differ in orientations and sizes. Capsule network (CapsNet) is a recent new architecture that has achieved better robustness in representation learning by replacing pooling layers with dynamic routing and convolutional strides, which has shown potential results on popular tasks such as classification, recognition, segmentation, and natural language processing. Different from CNNs, which result in scalar outputs, CapsNet returns vector outputs, which aim to preserve the part-whole relationships. In this work, we first introduce the limitations of CNNs and fundamentals of CapsNet. We then provide recent developments of CapsNet for the task of medical image segmentation. We finally discuss various effective network architectures to implement a CapsNet for both 2D images and 3D volumetric medical image segmentation.      
### 26.Autonomous Wheel Loader Trajectory Tracking Control Using LPV-MPC  [ :arrow_down: ](https://arxiv.org/pdf/2203.08944.pdf)
>  In this paper, we present a systematic approach for high-performance and efficient trajectory tracking control of autonomous wheel loaders. With the nonlinear dynamic model of a wheel loader, nonlinear model predictive control (MPC) is used in offline trajectory planning to obtain a high-performance state-control trajectory while satisfying the state and control constraints. In tracking control, the nonlinear model is embedded into a Linear Parameter Varying (LPV) model and the LPV-MPC strategy is used to achieve fast online computation and good tracking performance. To demonstrate the effectiveness and the advantages of the LPV-MPC, we test and compare three model predictive control strategies in the high-fidelity simulation environment. With the planned trajectory, three tracking control strategies LPV-MPC, nonlinear MPC, and LTI-MPC are simulated and compared in the perspectives of computational burden and tracking performance. The LPV-MPC can achieve better performance than conventional LTI-MPC because more accurate nominal system dynamics are captured in the LPV model. In addition, LPV-MPC achieves slightly worse tracking performance but tremendously improved computational efficiency than nonlinear MPC. A video with loading cycles completed by our autonomous wheel loader in the simulation environment can be found here: <a class="link-external link-https" href="https://youtu.be/QbNfS_wZKKA" rel="external noopener nofollow">this https URL</a>.      
### 27.Improved Analysis of Current-Steering DACs Using Equivalent Timing Errors  [ :arrow_down: ](https://arxiv.org/pdf/2203.08939.pdf)
>  Current-steering (CS) digital-to-analog converters (DACs) generate analog signals by combining weighted current sources. Ideally, the current sources are combined at each switching instant simultaneously. However, this is not true in practice due to timing mismatch, resulting in nonlinear distortion. This work uses the equivalent timing error model, introduced by previous work, to analyze the signal-to-distortion ratio (SDR) resulting from these timing errors. Using a behavioral simulation model we demonstrate that our analysis is significantly more accurate than the previous methods. We also use our simulation model to investigate the effect of timing mismatch in partially-segmented CS-DACs, i.e., those comprised of both equally-weighted and binary-weighted current sources.      
### 28.The Digital Divide in Canada and the Role of LEO Satellites in Bridging the Gap  [ :arrow_down: ](https://arxiv.org/pdf/2203.08933.pdf)
>  Overcoming the digital divide in rural and remote areas has always been a big challenge for Canada with its huge geographical area. In 2016, the Canadian Radio-television and Telecommunications Commission announced broadband Internet as a basic service available for all Canadians. However, approximately one million Canadians still do not have access to broadband services as of 2020. The COVID-19 pandemic has made the situation more challenging, as social, economic, and educational activities have increasingly been transferred online. The condition is more unfavorable for Indigenous communities. A key challenge in deploying rural and remote broadband Internet is to plan and implement high-capacity backbones, which are now available only in denser urban areas. For any Internet provider, it is almost impossible to make a viable business proposal in these areas. For example, the vast land of the Northwest Territories, Yukon, and Nunavuts diverse geographical features present obstacles for broadband infrastructure. In this paper, we investigate the digital divide in Canada with a focus on rural and remote areas. In so doing, we highlight two potential solutions using low Earth orbit (LEO) constellations to deliver broadband Internet in rural and remote areas to address the access inequality and the digital divide. The first solution involves integrating LEO constellations as a backbone for the existing 4G/5G telecommunications network. This solution uses satellites in a LEO constellation to provide a backhaul network connecting the 4G/5G access network to its core network. The 3rd Generation Partnership Project already specifies how to integrate LEO satellite networks into the 4G/5G network, and the Canadian satellite operator Telesat has already showcased this solution with one terrestrial operator, TIM Brasil, in their 4G network.      
### 29.Neural network processing of holographic images  [ :arrow_down: ](https://arxiv.org/pdf/2203.08898.pdf)
>  HOLODEC, an airborne cloud particle imager, captures holographic images of a fixed volume of cloud to characterize the types and sizes of cloud particles, such as water droplets and ice crystals. Cloud particle properties include position, diameter, and shape. We present a hologram processing algorithm, HolodecML, that utilizes a neural segmentation model, GPUs, and computational parallelization. HolodecML is trained using synthetically generated holograms based on a model of the instrument, and predicts masks around particles found within reconstructed images. From these masks, the position and size of the detected particles can be characterized in three dimensions. In order to successfully process real holograms, we find we must apply a series of image corrupting transformations and noise to the synthetic images used in training. <br>In this evaluation, HolodecML had comparable position and size estimation performance to the standard processing method, but improved particle detection by nearly 20\% on several thousand manually labeled HOLODEC images. However, the improvement only occurred when image corruption was performed on the simulated images during training, thereby mimicking non-ideal conditions in the actual probe. The trained model also learned to differentiate artifacts and other impurities in the HOLODEC images from the particles, even though no such objects were present in the training data set, while the standard processing method struggled to separate particles from artifacts. The novelty of the training approach, which leveraged noise as a means for parameterizing non-ideal aspects of the HOLODEC detector, could be applied in other domains where the theoretical model is incapable of fully describing the real-world operation of the instrument and accurate truth data required for supervised learning cannot be obtained from real-world observations.      
### 30.A Real-Time Region Tracking Algorithm Tailored to Endoscopic Video with Open-Source Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2203.08858.pdf)
>  With a video data source, such as multispectral video acquired during administration of fluorescent tracers, extraction of time-resolved data typically requires the compensation of motion. While this can be done manually, which is arduous, or using off-the-shelf object tracking software, which often yields unsatisfactory performance, we present an algorithm which is simple and performant. Most importantly, we provide an open-source implementation, with an easy-to-use interface for researchers not inclined to write their own code, as well as Python modules that can be used programmatically.      
### 31.Self-Supervised Deep Learning to Enhance Breast Cancer Detection on Screening Mammography  [ :arrow_down: ](https://arxiv.org/pdf/2203.08812.pdf)
>  A major limitation in applying deep learning to artificial intelligence (AI) systems is the scarcity of high-quality curated datasets. We investigate strong augmentation based self-supervised learning (SSL) techniques to address this problem. Using breast cancer detection as an example, we first identify a mammogram-specific transformation paradigm and then systematically compare four recent SSL methods representing a diversity of approaches. We develop a method to convert a pretrained model from making predictions on uniformly tiled patches to whole images, and an attention-based pooling method that improves the classification performance. We found that the best SSL model substantially outperformed the baseline supervised model. The best SSL model also improved the data efficiency of sample labeling by nearly 4-fold and was highly transferrable from one dataset to another. SSL represents a major breakthrough in computer vision and may help the AI for medical imaging field to shift away from supervised learning and dependency on scarce labels.      
### 32.Semi-FedSER: Semi-supervised Learning for Speech Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling  [ :arrow_down: ](https://arxiv.org/pdf/2203.08810.pdf)
>  Speech Emotion Recognition (SER) application is frequently associated with privacy concerns as it often acquires and transmits speech data at the client-side to remote cloud platforms for further processing. These speech data can reveal not only speech content and affective information but the speaker's identity, demographic traits, and health status. Federated learning (FL) is a distributed machine learning algorithm that coordinates clients to train a model collaboratively without sharing local data. This algorithm shows enormous potential for SER applications as sharing raw speech or speech features from a user's device is vulnerable to privacy attacks. However, a major challenge in FL is limited availability of high-quality labeled data samples. In this work, we propose a semi-supervised federated learning framework, Semi-FedSER, that utilizes both labeled and unlabeled data samples to address the challenge of limited labeled data samples in FL. We show that our Semi-FedSER can generate desired SER performance even when the local label rate l=20 using two SER benchmark datasets: IEMOCAP and MSP-Improv.      
### 33.Disparities in Dermatology AI Performance on a Diverse, Curated Clinical Image Set  [ :arrow_down: ](https://arxiv.org/pdf/2203.08807.pdf)
>  Access to dermatological care is a major issue, with an estimated 3 billion people lacking access to care globally. Artificial intelligence (AI) may aid in triaging skin diseases. However, most AI models have not been rigorously assessed on images of diverse skin tones or uncommon diseases. To ascertain potential biases in algorithm performance in this context, we curated the Diverse Dermatology Images (DDI) dataset-the first publicly available, expertly curated, and pathologically confirmed image dataset with diverse skin tones. Using this dataset of 656 images, we show that state-of-the-art dermatology AI models perform substantially worse on DDI, with receiver operator curve area under the curve (ROC-AUC) dropping by 27-36 percent compared to the models' original test results. All the models performed worse on dark skin tones and uncommon diseases, which are represented in the DDI dataset. Additionally, we find that dermatologists, who typically provide visual labels for AI training and test datasets, also perform worse on images of dark skin tones and uncommon diseases compared to ground truth biopsy annotations. Finally, fine-tuning AI models on the well-characterized and diverse DDI images closed the performance gap between light and dark skin tones. Moreover, algorithms fine-tuned on diverse skin tones outperformed dermatologists on identifying malignancy on images of dark skin tones. Our findings identify important weaknesses and biases in dermatology AI that need to be addressed to ensure reliable application to diverse patients and diseases.      
### 34.Image Super-Resolution With Deep Variational Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2203.09445.pdf)
>  Image super-resolution (SR) techniques are used to generate a high-resolution image from a low-resolution image. Until now, deep generative models such as autoregressive models and Generative Adversarial Networks (GANs) have proven to be effective at modelling high-resolution images. Models based on Variational Autoencoders (VAEs) have often been criticized for their feeble generative performance, but with new advancements such as VDVAE (very deep VAE), there is now strong evidence that deep VAEs have the potential to outperform current state-of-the-art models for high-resolution image generation. In this paper, we introduce VDVAE-SR, a new model that aims to exploit the most recent deep VAE methodologies to improve upon image super-resolution using transfer learning on pretrained VDVAEs. Through qualitative and quantitative evaluations, we show that the proposed model is competitive with other state-of-the-art methods.      
### 35.Stability and Risk Bounds of Iterative Hard Thresholding  [ :arrow_down: ](https://arxiv.org/pdf/2203.09413.pdf)
>  In this paper, we analyze the generalization performance of the Iterative Hard Thresholding (IHT) algorithm widely used for sparse recovery problems. The parameter estimation and sparsity recovery consistency of IHT has long been known in compressed sensing. From the perspective of statistical learning, another fundamental question is how well the IHT estimation would predict on unseen data. This paper makes progress towards answering this open question by introducing a novel sparse generalization theory for IHT under the notion of algorithmic stability. Our theory reveals that: 1) under natural conditions on the empirical risk function over $n$ samples of dimension $p$, IHT with sparsity level $k$ enjoys an $\mathcal{\tilde O}(n^{-1/2}\sqrt{k\log(n)\log(p)})$ rate of convergence in sparse excess risk; 2) a tighter $\mathcal{\tilde O}(n^{-1/2}\sqrt{\log(n)})$ bound can be established by imposing an additional iteration stability condition on a hypothetical IHT procedure invoked to the population risk; and 3) a fast rate of order $\mathcal{\tilde O}\left(n^{-1}k(\log^3(n)+\log(p))\right)$ can be derived for strongly convex risk function under proper strong-signal conditions. The results have been substantialized to sparse linear regression and sparse logistic regression models to demonstrate the applicability of our theory. Preliminary numerical evidence is provided to confirm our theoretical predictions.      
### 36.Robust and Complex Approach of Pathological Speech Signal Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2203.09402.pdf)
>  This paper presents a study of the approaches in the state-of-the-art in the field of pathological speech signal analysis with a special focus on parametrization techniques. It provides a description of 92 speech features where some of them are already widely used in this field of science and some of them have not been tried yet (they come from different areas of speech signal processing like speech recognition or coding). As an original contribution, this work introduces 36 completely new pathological voice measures based on modulation spectra, inferior colliculus coefficients, bicepstrum, sample and approximate entropy and empirical mode decomposition. The significance of these features was tested on 3 (English, Spanish and Czech) pathological voice databases with respect to classification accuracy, sensitivity and specificity.      
### 37.Localizing Visual Sounds the Easy Way  [ :arrow_down: ](https://arxiv.org/pdf/2203.09324.pdf)
>  Unsupervised audio-visual source localization aims at localizing visible sound sources in a video without relying on ground-truth localization for training. Previous works often seek high audio-visual similarities for likely positive (sounding) regions and low similarities for likely negative regions. However, accurately distinguishing between sounding and non-sounding regions is challenging without manual annotations. In this work, we propose a simple yet effective approach for Easy Visual Sound Localization, namely EZ-VSL, without relying on the construction of positive and/or negative regions during training. Instead, we align audio and visual spaces by seeking audio-visual representations that are aligned in, at least, one location of the associated image, while not matching other images, at any location. We also introduce a novel object guided localization scheme at inference time for improved precision. Our simple and effective framework achieves state-of-the-art performance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In particular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to 83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is available at <a class="link-external link-https" href="https://github.com/stoneMo/EZ-VSL" rel="external noopener nofollow">this https URL</a>.      
### 38.A New Analytical Approximation of the Fluid Antenna System Channel  [ :arrow_down: ](https://arxiv.org/pdf/2203.09318.pdf)
>  Fluid antenna systems (FAS) are an emerging technology that promises a significant diversity gain even in the smallest spaces. Motivated by the groundbreaking potentials of liquid antennas, researchers in the wireless communication community are investigating a novel antenna system where a single antenna can freely switch positions along a small linear space to pick the strongest received signal. However, the FAS positions do not necessarily follow the ever-existing rule separating them by at least half the radiation wavelength. Previous work in the literature parameterized the channels of the FAS ports simply enough to provide a single-integral expression of the probability of outage and various insights on the achievable performance. Nevertheless, this channel model may not accurately capture the correlation between the ports, given by Jake's model. This work builds on the state-of-the-art and accurately approximates the FAS channel while maintaining analytical tractability. The approximation is performed in two stages. The first stage approximation considerably reduces the number of multi-fold integrals in the probability of outage expression, while the second stage approximation provides a single integral representation of the FAS probability of outage. Further, the performance of such innovative technology is investigated under a less-idealized correlation model. Numerical results validate our approximations of the FAS channel model and demonstrate a limited performance gain under realistic assumptions. Further, our work opens the door for future research to investigate scenarios in which the FAS provides a performance gain compared to the current multiple antennas solutions.      
### 39.Assessing Progress of Parkinson s Disease Using Acoustic Analysis of Phonation  [ :arrow_down: ](https://arxiv.org/pdf/2203.09295.pdf)
>  This paper deals with a complex acoustic analysis of phonation in patients with Parkinson's disease (PD) with a special focus on estimation of disease progress that is described by 7 different clinical scales ,e. g. Unified Parkinson's disease rating scale or Beck depression inventory. The analysis is based on parametrization of 5 Czech vowels pronounced by 84 PD patients. Using classification and regression trees we estimated all clinical scores with maximal error lower or equal to 13 %. Best estimation was observed in the case of Mini-mental state examination (MAE = 0.77, estimation error 5.50 %. Finally, we proposed a binary classification based on random forests that is able to identify Parkinson's disease with sensitivity SEN = 92.86 % (SPE = 85.71 %). The parametrization process was based on extraction of 107 speech features quantifying different clinical signs of hypokinetic dysarthria present in PD.      
### 40.Speaker recognition using residual signal of linear and nonlinear prediction models  [ :arrow_down: ](https://arxiv.org/pdf/2203.09231.pdf)
>  This Paper discusses the usefulness of the residual signal for speaker recognition. It is shown that the combination of both a measure defined over LPCC coefficients and a measure defined over the energy of the residual signal gives rise to an improvement over the classical method which considers only the LPCC coefficients. If the residual signal is obtained from a linear prediction analysis, the improvement is 2.63% (error rate drops from 6.31% to 3.68%) and if it is computed through a nonlinear predictive neural nets based model, the improvement is 3.68%.      
### 41.Prediction of speech intelligibility with DNN-based performance measures  [ :arrow_down: ](https://arxiv.org/pdf/2203.09148.pdf)
>  This paper presents a speech intelligibility model based on automatic speech recognition (ASR), combining phoneme probabilities from deep neural networks (DNN) and a performance measure that estimates the word error rate from these probabilities. This model does not require the clean speech reference nor the word labels during testing as the ASR decoding step, which finds the most likely sequence of words given phoneme posterior probabilities, is omitted. The model is evaluated via the root-mean-squared error between the predicted and observed speech reception thresholds from eight normal-hearing listeners. The recognition task consists of identifying noisy words from a German matrix sentence test. The speech material was mixed with eight noise maskers covering different modulation types, from speech-shaped stationary noise to a single-talker masker. The prediction performance is compared to five established models and an ASR-model using word labels. Two combinations of features and networks were tested. Both include temporal information either at the feature level (amplitude modulation filterbanks and a feed-forward network) or captured by the architecture (mel-spectrograms and a time-delay deep neural network, TDNN). The TDNN model is on par with the DNN while reducing the number of parameters by a factor of 37; this optimization allows parallel streams on dedicated hearing aid hardware as a forward-pass can be computed within the 10ms of each frame. The proposed model performs almost as well as the label-based model and produces more accurate predictions than the baseline models.      
### 42.Covid19 Reproduction Number: Credibility Intervals by Blockwise Proximal Monte Carlo Samplers  [ :arrow_down: ](https://arxiv.org/pdf/2203.09142.pdf)
>  Monitoring the Covid19 pandemic constitutes a critical societal stake that received considerable research efforts. The intensity of the pandemic on a given territory is efficiently measured by the reproduction number, quantifying the rate of growth of daily new infections. Recently, estimates for the time evolution of the reproduction number were produced using an inverse problem formulation with a nonsmooth functional minimization. While it was designed to be robust to the limited quality of the Covid19 data (outliers, missing counts), the procedure lacks the ability to output credibility interval based estimates. This remains a severe limitation for practical use in actual pandemic monitoring by epidemiologists that the present work aims to overcome by use of Monte Carlo sampling. After interpretation of the functional into a Bayesian framework, several sampling schemes are tailored to adjust the nonsmooth nature of the resulting posterior distribution. The originality of the devised algorithms stems from combining a Langevin Monte Carlo sampling scheme with Proximal operators. Performance of the new algorithms in producing relevant credibility intervals for the reproduction number estimates and denoised counts are compared. Assessment is conducted on real daily new infection counts made available by the Johns Hopkins University. The interest of the devised monitoring tools are illustrated on Covid19 data from several different countries.      
### 43.Contrastive Learning with Positive-Negative Frame Mask for Music Representation  [ :arrow_down: ](https://arxiv.org/pdf/2203.09129.pdf)
>  Self-supervised learning, especially contrastive learning, has made an outstanding contribution to the development of many deep learning research fields. Recently, researchers in the acoustic signal processing field noticed its success and leveraged contrastive learning for better music representation. Typically, existing approaches maximize the similarity between two distorted audio segments sampled from the same music. In other words, they ensure a semantic agreement at the music level. However, those coarse-grained methods neglect some inessential or noisy elements at the frame level, which may be detrimental to the model to learn the effective representation of music. Towards this end, this paper proposes a novel Positive-nEgative frame mask for Music Representation based on the contrastive learning framework, abbreviated as PEMR. Concretely, PEMR incorporates a Positive-Negative Mask Generation module, which leverages transformer blocks to generate frame masks on the Log-Mel spectrogram. We can generate self-augmented negative and positive samples by masking important components or inessential components, respectively. We devise a novel contrastive learning objective to accommodate both self-augmented positives/negatives sampled from the same music. We conduct experiments on four public datasets. The experimental results of two music-related downstream tasks, music classification, and cover song identification, demonstrate the generalization ability and transferability of music representation learned by PEMR.      
### 44.TMS: A Temporal Multi-scale Backbone Design for Speaker Embedding  [ :arrow_down: ](https://arxiv.org/pdf/2203.09098.pdf)
>  Speaker embedding is an important front-end module to explore discriminative speaker features for many speech applications where speaker information is needed. Current SOTA backbone networks for speaker embedding are designed to aggregate multi-scale features from an utterance with multi-branch network architectures for speaker representation. However, naively adding many branches of multi-scale features with the simple fully convolutional operation could not efficiently improve the performance due to the rapid increase of model parameters and computational complexity. Therefore, in the most current state-of-the-art network architectures, only a few branches corresponding to a limited number of temporal scales could be designed for speaker embeddings. To address this problem, in this paper, we propose an effective temporal multi-scale (TMS) model where multi-scale branches could be efficiently designed in a speaker embedding network almost without increasing computational costs. The new model is based on the conventional TDNN, where the network architecture is smartly separated into two modeling operators: a channel-modeling operator and a temporal multi-branch modeling operator. Adding temporal multi-scale in the temporal multi-branch operator needs only a little bit increase of the number of parameters, and thus save more computational budget for adding more branches with large temporal scales. Moreover, in the inference stage, we further developed a systemic re-parameterization method to convert the TMS-based model into a single-path-based topology in order to increase inference speed. We investigated the performance of the new TMS method for automatic speaker verification (ASV) on in-domain and out-of-domain conditions. Results show that the TMS-based model obtained a significant increase in the performance over the SOTA ASV models, meanwhile, had a faster inference speed.      
### 45.DeepAD: A Robust Deep Learning Model of Alzheimer's Disease Progression for Real-World Clinical Applications  [ :arrow_down: ](https://arxiv.org/pdf/2203.09096.pdf)
>  The ability to predict the future trajectory of a patient is a key step toward the development of therapeutics for complex diseases such as Alzheimer's disease (AD). However, most machine learning approaches developed for prediction of disease progression are either single-task or single-modality models, which can not be directly adopted to our setting involving multi-task learning with high dimensional images. Moreover, most of those approaches are trained on a single dataset (i.e. cohort), which can not be generalized to other cohorts. We propose a novel multimodal multi-task deep learning model to predict AD progression by analyzing longitudinal clinical and neuroimaging data from multiple cohorts. Our proposed model integrates high dimensional MRI features from a 3D convolutional neural network with other data modalities, including clinical and demographic information, to predict the future trajectory of patients. Our model employs an adversarial loss to alleviate the study-specific imaging bias, in particular the inter-study domain shifts. In addition, a Sharpness-Aware Minimization (SAM) optimization technique is applied to further improve model generalization. The proposed model is trained and tested on various datasets in order to evaluate and validate the results. Our results showed that 1) our model yields significant improvement over the baseline models, and 2) models using extracted neuroimaging features from 3D convolutional neural network outperform the same models when applied to MRI-derived volumetric features.      
### 46.On the convergence of decentralized gradient descent with diminishing stepsize, revisited  [ :arrow_down: ](https://arxiv.org/pdf/2203.09079.pdf)
>  Distributed optimization has received a lot of interest in recent years due to its wide applications in various fields. In this work, we revisit the convergence property of the decentralized gradient descent [A. Nedi{ć}-A.Ozdaglar (2009)] on the whole space given by $$ x_i(t+1) = \sum^m_{j=1}w_{ij}x_j(t) - \alpha(t) \nabla f_i(x_i(t)), $$ where the stepsize $\alpha (t) = \frac{a}{(t+w)^p}$ with $0&lt; p\leq 1$. Under the strongly convexity assumption on the total cost function $f$ with local cost functions $f_i$ not necessarily being convex, we show that the sequence converges to the optimizer with rate $O(t^{-p})$ when the values of $a&gt;0$ and $w&gt;0$ are suitably chosen.      
### 47.GATE: Graph CCA for Temporal SElf-supervised Learning for Label-efficient fMRI Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2203.09034.pdf)
>  In this work, we focus on the challenging task, neuro-disease classification, using functional magnetic resonance imaging (fMRI). In population graph-based disease analysis, graph convolutional neural networks (GCNs) have achieved remarkable success. However, these achievements are inseparable from abundant labeled data and sensitive to spurious signals. To improve fMRI representation learning and classification under a label-efficient setting, we propose a novel and theory-driven self-supervised learning (SSL) framework on GCNs, namely Graph CCA for Temporal self-supervised learning on fMRI analysis GATE. Concretely, it is demanding to design a suitable and effective SSL strategy to extract formation and robust features for fMRI. To this end, we investigate several new graph augmentation strategies from fMRI dynamic functional connectives (FC) for SSL training. Further, we leverage canonical-correlation analysis (CCA) on different temporal embeddings and present the theoretical implications. Consequently, this yields a novel two-step GCN learning procedure comprised of (i) SSL on an unlabeled fMRI population graph and (ii) fine-tuning on a small labeled fMRI dataset for a classification task. Our method is tested on two independent fMRI datasets, demonstrating superior performance on autism and dementia diagnosis.      
### 48.Coordinated Power Control for Network Integrated Sensing and Communication  [ :arrow_down: ](https://arxiv.org/pdf/2203.09032.pdf)
>  This correspondence paper studies a network integrated sensing and communication (ISAC) system that unifies the interference channel for communication and distributed radar sensing. In this system, a set of distributed ISAC transmitters send individual messages to their respective communication users (CUs), and at the same time cooperate with multiple sensing receivers to estimate the location of one target. We exploit the coordinated power control among ISAC transmitters to minimize their total transmit power while ensuring the minimum signal-to-interference-plus-noise ratio (SINR) constraints at individual CUs and the maximum Cramér-Rao lower bound (CRLB) requirement for target location estimation. Although the formulated coordinated power control problem is non-convex and difficult to solve in general, we propose two efficient algorithms to obtain high-quality solutions based on the semi-definite relaxation (SDR) and CRLB approximation, respectively. Numerical results show that the proposed designs achieve substantial performance gains in terms of power reduction, as compared to the benchmark with a heuristic separate communication-sensing design.      
### 49.Stochastic Simulation Uncertainty Analysis to Accelerate Flexible Biomanufacturing Process Development  [ :arrow_down: ](https://arxiv.org/pdf/2203.08980.pdf)
>  Motivated by critical challenges and needs from biopharmaceuticals manufacturing, we propose a general metamodel-assisted stochastic simulation uncertainty analysis framework to accelerate the development of a simulation model or digital twin with modular design for flexible production processes. Since we often face very limited observations and complex biomanufacturing processes with high inherent stochasticity, there exist both simulation and model uncertainties in the system performance estimates. In biopharmaceutical manufacturing, model uncertainty often dominates. The proposed framework can produce a confidence interval that accounts for simulation and model uncertainties by using a metamodel-assisted bootstrapping approach. Furthermore, a variance decomposition is utilized to estimate the relative contributions from each source of model uncertainty, as well as simulation uncertainty. This information can efficiently guide digital twin development. Asymptotic analysis provides theoretical support for our approach, while the empirical study demonstrates that it has good finite-sample performance.      
### 50.Meta-Learning of NAS for Few-shot Learning in Medical Image Applications  [ :arrow_down: ](https://arxiv.org/pdf/2203.08951.pdf)
>  Deep learning methods have been successful in solving tasks in machine learning and have made breakthroughs in many sectors owing to their ability to automatically extract features from unstructured data. However, their performance relies on manual trial-and-error processes for selecting an appropriate network architecture, hyperparameters for training, and pre-/post-procedures. Even though it has been shown that network architecture plays a critical role in learning feature representation feature from data and the final performance, searching for the best network architecture is computationally intensive and heavily relies on researchers' experience. Automated machine learning (AutoML) and its advanced techniques i.e. Neural Architecture Search (NAS) have been promoted to address those limitations. Not only in general computer vision tasks, but NAS has also motivated various applications in multiple areas including medical imaging. In medical imaging, NAS has significant progress in improving the accuracy of image classification, segmentation, reconstruction, and more. However, NAS requires the availability of large annotated data, considerable computation resources, and pre-defined tasks. To address such limitations, meta-learning has been adopted in the scenarios of few-shot learning and multiple tasks. In this book chapter, we first present a brief review of NAS by discussing well-known approaches in search space, search strategy, and evaluation strategy. We then introduce various NAS approaches in medical imaging with different applications such as classification, segmentation, detection, reconstruction, etc. Meta-learning in NAS for few-shot learning and multiple tasks is then explained. Finally, we describe several open problems in NAS.      
### 51.Sound Development of Safety Supervisors  [ :arrow_down: ](https://arxiv.org/pdf/2203.08917.pdf)
>  Safety supervisors are controllers enforcing safety properties by keeping a system in (or returning it to) a safe state. The development of such high-integrity components can benefit from a rigorous workflow integrating formal design and verification. In this paper, we present a workflow for the sound development of safety supervisors combining the best of two worlds, verified synthesis and complete testing. Synthesis allows one to focus on problem specification and model validation. Testing compensates for the crossing of abstraction, formalism, and tool boundaries and is a key element to obtain certification credit before entry into service. We establish soundness of our workflow through a rigorous argument. Our approach is tool-supported, aims at modern autonomous systems, and is illustrated with a collaborative robotics example.      
### 52.SMARTmBOT: A ROS2-based Low-cost and Open-source Mobile Robot Platform  [ :arrow_down: ](https://arxiv.org/pdf/2203.08903.pdf)
>  This paper introduces SMARTmBOT, an open-source mobile robot platform based on Robot Operating System 2 (ROS2). The characteristics of the SMARTmBOT, including low-cost, modular-typed, customizable and expandable design, make it an easily achievable and effective robot platform to support broad robotics research and education involving either single-robot or multi-robot systems. The total cost per robot is approximately $210, and most hardware components can be fabricated by a generic 3D printer, hence allowing users to build the robots or replace any broken parts conveniently. The SMARTmBot is also equipped with a rich range of sensors, making it competent for general task scenarios, such as point-to-point navigation and obstacle avoidance. We validated the mobility and function of SMARTmBOT through various robot navigation experiments and applications with tasks including go-to-goal, pure-pursuit, line following, and swarming. All source code necessary for reading sensors, streaming from an embedded camera, and controlling the robot including robot navigation controllers is available through an online repository that can be found at <a class="link-external link-https" href="https://github.com/SMARTlab-Purdue/SMARTmBOT" rel="external noopener nofollow">this https URL</a>.      
### 53.SC2: Supervised Compression for Split Computing  [ :arrow_down: ](https://arxiv.org/pdf/2203.08875.pdf)
>  Split computing distributes the execution of a neural network (e.g., for a classification task) between a mobile device and a more powerful edge server. A simple alternative to splitting the network is to carry out the supervised task purely on the edge server while compressing and transmitting the full data, and most approaches have barely outperformed this baseline. This paper proposes a new approach for discretizing and entropy-coding intermediate feature activations to efficiently transmit them from the mobile device to the edge server. We show that a efficient splittable network architecture results from a three-way tradeoff between (a) minimizing the computation on the mobile device, (b) minimizing the size of the data to be transmitted, and (c) maximizing the model's prediction performance. We propose an architecture based on this tradeoff and train the splittable network and entropy model in a knowledge distillation framework. In an extensive set of experiments involving three vision tasks, three datasets, nine baselines, and more than 180 trained models, we show that our approach improves supervised rate-distortion tradeoffs while maintaining a considerably smaller encoder size. We also release sc2bench, an installable Python package, to encourage and facilitate future studies on supervised compression for split computing (SC2).      
### 54.Understanding robustness and generalization of artificial neural networks through Fourier masks  [ :arrow_down: ](https://arxiv.org/pdf/2203.08822.pdf)
>  Despite the enormous success of artificial neural networks (ANNs) in many disciplines, the characterization of their computations and the origin of key properties such as generalization and robustness remain open questions. Recent literature suggests that robust networks with good generalization properties tend to be biased towards processing low frequencies in images. To explore the frequency bias hypothesis further, we develop an algorithm that allows us to learn modulatory masks highlighting the essential input frequencies needed for preserving a trained network's performance. We achieve this by imposing invariance in the loss with respect to such modulations in the input frequencies. We first use our method to test the low-frequency preference hypothesis of adversarially trained or data-augmented networks. Our results suggest that adversarially robust networks indeed exhibit a low-frequency bias but we find this bias is also dependent on directions in frequency space. However, this is not necessarily true for other types of data augmentation. Our results also indicate that the essential frequencies in question are effectively the ones used to achieve generalization in the first place. Surprisingly, images seen through these modulatory masks are not recognizable and resemble texture-like patterns.      
