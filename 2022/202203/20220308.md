# ArXiv eess --Tue, 8 Mar 2022
### 1.Compression of user generated content using denoised references  [ :arrow_down: ](https://arxiv.org/pdf/2203.03553.pdf)
>  Video shared over the internet is commonly referred to as user generated content (UGC). UGC video may have low quality due to various factors including previous compression. UGC video is uploaded by users, and then it is re encoded to be made available at various levels of quality and resolution. In a traditional video coding pipeline the encoder parameters are optimized to minimize a rate-distortion criteria, but when the input signal has low quality, this results in sub-optimal coding parameters optimized to preserve undesirable artifacts. In this paper we formulate the UGC compression problem as that of compression of a noisy/corrupted source. The noisy source coding theorem reveals that an optimal UGC compression system is comprised of optimal denoising of the UGC signal, followed by compression of the denoised signal. Since optimal denoising is unattainable and users may be against modification of their content, we propose using denoised references to compute distortion, so the encoding process can be guided towards perceptually better solutions. We demonstrate the effectiveness of the proposed strategy for JPEG compression of UGC images and videos.      
### 2.On observability and optimal gain design for distributed linear filtering and prediction  [ :arrow_down: ](https://arxiv.org/pdf/2203.03521.pdf)
>  This paper presents a new approach to distributed linear filtering and prediction. The problem under consideration consists of a random dynamical system observed by a multi-agent network of sensors where the network is sparse. Inspired by the consensus+innovations type of distributed estimation approaches, this paper proposes a novel algorithm that fuses the concepts of consensus and innovations. The paper introduces a definition of distributed observability, required by the proposed algorithm, which is a weaker assumption than that of global observability and connected network assumptions combined together. Following first principles, the optimal gain matrices are designed such that the mean-squared error of estimation is minimized at each agent and the distributed version of the algebraic Riccati equation is derived for computing the gains.      
### 3.Optimal Lockdown Management using Short Term COVID-19 Prediction Model  [ :arrow_down: ](https://arxiv.org/pdf/2203.03488.pdf)
>  This paper proposes optimal lockdown management policies based on short-term prediction of active COVID-19 confirmed cases to ensure the availability of critical medical resources. The optimal time to start the lockdown from the current time is obtained after maximizing a cost function considering economic value subject to constraints of availability of medical resources, and maximum allowable value of daily growth rate and Test Positive Ratio. The estimated value of required medical resources is calculated as a function of total active cases. The predicted value of active cases is calculated using an adaptive short-term prediction model. The proposed approach can be easily implementable by a local authority. An optimal lockdown case study for Delhi during the second wave in the month of April 2021 is presented using the proposed formulation.      
### 4.Safety Verification of Autonomous Systems: A Multi-Fidelity Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.03451.pdf)
>  As autonomous and semi-autonomous agents become more integrated with society, validation of their safety is increasingly important. The scenarios under which they are used, however, can be quite complicated; as such, formal verification may be impossible. To this end, simulation-based safety verification is being used more frequently to understand failure scenarios for the most complex problems. Recent approaches, such as adaptive stress testing (AST), use reinforcement learning, making them prone to excessive exploitation of known failures, limiting coverage of the space of failures. To overcome this, the work below defines a class of Markov decision processes, the knowledge MDP, which captures information about the learned model to reason over. More specifically, by leveraging, the "knows what it knows" (KWIK) framework, the learner estimates its knowledge (model estimates and confidence, as well as assumptions) about the underlying system. This formulation is vetted through MF-KWIK-AST which extends bidirectional learning in multiple fidelities (MF) of simulators to the safety verification problem. The knowledge MDP formulation is applied to detect convergence of the model, penalizing this behavior to encourage further exploration. Results are evaluated in a grid world, training an adversary to intercept a system under test. Monte Carlo trials compare the relative sample efficiency of MF-KWIK-AST to learning with a single-fidelity simulator, as well as demonstrate the utility of incorporating knowledge about learned models into the decision making process.      
### 5.Deep Neural Decision Forest for Acoustic Scene Classification  [ :arrow_down: ](https://arxiv.org/pdf/2203.03436.pdf)
>  Acoustic scene classification (ASC) aims to classify an audio clip based on the characteristic of the recording environment. In this regard, deep learning based approaches have emerged as a useful tool for ASC problems. Conventional approaches to improving the classification accuracy include integrating auxiliary methods such as attention mechanism, pre-trained models and ensemble multiple sub-networks. However, due to the complexity of audio clips captured from different environments, it is difficult to distinguish their categories without using any auxiliary methods for existing deep learning models using only a single classifier. In this paper, we propose a novel approach for ASC using deep neural decision forest (DNDF). DNDF combines a fixed number of convolutional layers and a decision forest as the final classifier. The decision forest consists of a fixed number of decision tree classifiers, which have been shown to offer better classification performance than a single classifier in some datasets. In particular, the decision forest differs substantially from traditional random forests as it is stochastic, differentiable, and capable of using the back-propagation to update and learn feature representations in neural network. Experimental results on the DCASE2019 and ESC-50 datasets demonstrate that our proposed DNDF method improves the ASC performance in terms of classification accuracy and shows competitive performance as compared with state-of-the-art baselines.      
### 6.A Deep Learning Framework for Nuclear Segmentation and Classification in Histopathological Images  [ :arrow_down: ](https://arxiv.org/pdf/2203.03420.pdf)
>  Nucleus segmentation and classification are the prerequisites in the workflow of digital pathology processing. However, it is very challenging due to its high-level heterogeneity and wide variations. This work proposes a deep neural network to simultaneously achieve nuclear classification and segmentation, which is designed using a unified framework with three different branches, including segmentation, HoVer mapping, and classification. The segmentation branch aims to generate the boundaries of each nucleus. The HoVer branch calculates the horizontal and vertical distances of nuclear pixels to their centres of mass. The nuclear classification branch is used to distinguish the class of pixels inside the nucleus obtained from segmentation.      
### 7.Scalable multi-agent reinforcement learning for distributed control of residential energy flexibility  [ :arrow_down: ](https://arxiv.org/pdf/2203.03417.pdf)
>  This paper proposes a novel scalable type of multi-agent reinforcement learning-based coordination for distributed residential energy. Cooperating agents learn to control the flexibility offered by electric vehicles, space heating and flexible loads in a partially observable stochastic environment. In the standard independent Q-learning approach, the coordination performance of agents under partial observability drops at scale in stochastic environments. Here, the novel combination of learning from off-line convex optimisations on historical data and isolating marginal contributions to total rewards in reward signals increases stability and performance at scale. Using fixed-size Q-tables, prosumers are able to assess their marginal impact on total system objectives without sharing personal data either with each other or with a central coordinator. Case studies are used to assess the fitness of different combinations of exploration sources, reward definitions, and multi-agent learning frameworks. It is demonstrated that the proposed strategies create value at individual and system levels thanks to reductions in the costs of energy imports, losses, distribution network congestion, battery depreciation and greenhouse gas emissions.      
### 8.CoNIC Solution  [ :arrow_down: ](https://arxiv.org/pdf/2203.03415.pdf)
>  Nuclei segmentation and classification has been a challenge due to the high inter-class similarity and intra-class variability. Thus, a large-scale annotation and a specially-designed algorithm are needed to solve this problem. Lizard is therefore a great promotion in this area, containing around half a million nuclei annotated. In this paper, we propose a two-stage pipeline used in the CoNIC competition, which achieves much better results than the baseline method. We adopt a similar model as the original baseline method: HoVerNet, as the segmentaion model and then develop a new classification model to fine-tune the classification results. Code for this method will be made public soon. This is a conic solution in testing.      
### 9.Estimation and Model Misspecification: Fake and Missing Features  [ :arrow_down: ](https://arxiv.org/pdf/2203.03398.pdf)
>  We consider estimation under model misspecification where there is a model mismatch between the underlying system, which generates the data, and the model used during estimation. We propose a model misspecification framework which enables a joint treatment of the model misspecification types of having fake and missing features, as well as incorrect covariance assumptions on the unknowns and the noise. Here, features which are included in the model but are not present in the underlying system, and features which are not included in the model but are present in the underlying system, are referred to as fake and missing features, respectively. Under this framework, we characterize the estimation performance and reveal trade-offs between the missing and fake features and the possibly incorrect noise level assumption. In contrast to existing work focusing on incorrect covariance assumptions or missing features, fake features is a central component of our framework. Our results show that fake features can significantly improve the estimation performance, even though they are not correlated with the features in the underlying system. In particular, we show that the estimation error can be decreased by including more fake features in the model, even to the point where the model is overparametrized, i.e., the model contains more unknowns than observations.      
### 10.Visually Supervised Speaker Detection and Localization via Microphone Array  [ :arrow_down: ](https://arxiv.org/pdf/2203.03291.pdf)
>  Active speaker detection (ASD) is a multi-modal task that aims to identify who, if anyone, is speaking from a set of candidates. Current audio-visual approaches for ASD typically rely on visually pre-extracted face tracks (sequences of consecutive face crops) and the respective monaural audio. However, their recall rate is often low as only the visible faces are included in the set of candidates. Monaural audio may successfully detect the presence of speech activity but fails in localizing the speaker due to the lack of spatial cues. Our solution extends the audio front-end using a microphone array. We train an audio convolutional neural network (CNN) in combination with beamforming techniques to regress the speaker's horizontal position directly in the video frames. We propose to generate weak labels using a pre-trained active speaker detector on pre-extracted face tracks. Our pipeline embraces the "student-teacher" paradigm, where a trained "teacher" network is used to produce pseudo-labels visually. The "student" network is an audio network trained to generate the same results. At inference, the student network can independently localize the speaker in the visual frames directly from the audio input. Experimental results on newly collected data prove that our approach significantly outperforms a variety of other baselines as well as the teacher network itself. It results in an excellent speech activity detector too.      
### 11.Enhance Language Identification using Dual-mode Model with Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2203.03218.pdf)
>  In this paper, we propose to employ a dual-mode framework on the x-vector self-attention (XSA-LID) model with knowledge distillation (KD) to enhance its language identification (LID) performance for both long and short utterances. The dual-mode XSA-LID model is trained by jointly optimizing both the full and short modes with their respective inputs being the full-length speech and its short clip extracted by a specific Boolean mask, and KD is applied to further boost the performance on short utterances. In addition, we investigate the impact of clip-wise linguistic variability and lexical integrity for LID by analyzing the variation of LID performance in terms of the lengths and positions of the mimicked speech clips. We evaluated our approach on the MLS14 data from the NIST 2017 LRE. With the 3~s random-location Boolean mask, our proposed method achieved 19.23%, 21.52% and 8.37% relative improvement in average cost compared with the XSA-LID model on 3s, 10s, and 30s speech, respectively.      
### 12.Stochastic Aperiodic Control of Networked Systems with i.i.d. Time-Varying Communication Delays  [ :arrow_down: ](https://arxiv.org/pdf/2203.03207.pdf)
>  This paper studies stochastic aperiodic stabilization of a networked control system (NCS) consisting of a continuous-time plant and a discrete-time controller. The plant and the controller are assumed to be connected by communication channels with i.i.d. time-varying delays. The delays are theoretically not required to be bounded even when the plant is unstable in the deterministic sense. In our NCS, the sampling interval is supposed to be determined directly by such communication delays. A necessary and sufficient inequality condition is presented for designing a state-feedback controller stabilizing the NCS at sampling points in a stochastic sense. The results are also illustrated numerically.      
### 13.Undersampled MRI Reconstruction with Side Information-Guided Normalisation  [ :arrow_down: ](https://arxiv.org/pdf/2203.03196.pdf)
>  Magnetic resonance (MR) images exhibit various contrasts and appearances based on factors such as different acquisition protocols, views, manufacturers, scanning parameters, etc. This generally accessible appearance-related side information affects deep learning-based undersampled magnetic resonance imaging (MRI) reconstruction frameworks, but has been overlooked in the majority of current works. In this paper, we investigate the use of such side information as normalisation parameters in a convolutional neural network (CNN) to improve undersampled MRI reconstruction. Specifically, a Side Information-Guided Normalisation (SIGN) module, containing only few layers, is proposed to efficiently encode the side information and output the normalisation parameters. We examine the effectiveness of such a module on two popular reconstruction architectures, D5C5 and OUCR. The experimental results on both brain and knee images under various acceleration rates demonstrate that the proposed method improves on its corresponding baseline architectures with a significant margin.      
### 14.HRTF measurement for accurate identification of binaural sound localization cues  [ :arrow_down: ](https://arxiv.org/pdf/2203.03166.pdf)
>  Although various research institutes have measured head-related transfer functions (HRTFs), the standards for system design, measurement, and post-processing have often been unclear or inaccurate. This paper presents accurate and practical methods for the major issues of HRTF database construction, especially on loudspeaker design, measurement of the origin transfer function at the head center, selection of time window interval, and compensation for non-causality of ipsilateral HRTF. Then, the effect of each method on binaural sound localization cues was investigated. Especially, interaural time difference (ITD), interaural level difference (ILD), spectral cue (SC), and horizontal plane directivity (HPD) were extracted from the HRTFs to examine the effect of each method on binaural sound localization cues. MATLAB codes and resulting HRTF database are available at GitHub (<a class="link-external link-https" href="https://github.com/hansaram80/HRTF-construction" rel="external noopener nofollow">this https URL</a>).      
### 15.An Improved Automatic Modulation Classification Scheme Based on Adaptive Fusion Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.03140.pdf)
>  Due to the over-fitting problem caused by imbalance samples, there is still room to improve the performance of data-driven automatic modulation classification (AMC) in noisy scenarios. By fully considering the signal characteristics, an AMC scheme based on adaptive fusion network (AFNet) is proposed in this work. The AFNet can extract and aggregate multi-scale spatial features of in-phase and quadrature (I/Q) signals intelligently, thus improving the feature representation capability. Moreover, a novel confidence weighted loss function is proposed to address the imbalance issue and it is implemented by a two-stage learning scheme.Through the two-stage learning, AFNet can focus on high-confidence samples with more valid information and extract effective representations, so as to improve the overall classification performance. In the simulations, the proposed scheme reaches an average accuracy of 62.66% on a wide range of SNRs, which outperforms other AMC models. The effects of the loss function on classification accuracy are further studied.      
### 16.Co-Optimization of On-Ramp Merging and Plug-In Hybrid Electric Vehicle Power Split Using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.03113.pdf)
>  Current research on Deep Reinforcement Learning (DRL) for automated on-ramp merging neglects vehicle powertrain and dynamics. This work considers automated on-ramp merging for a power-split Plug-In Hybrid Electric Vehicle (PHEV), the 2015 Toyota Prius Plug-In, using DRL. The on-ramp merging control and the PHEV energy management are co-optimized such that the DRL policy directly outputs the power split between the engine and the electric motor. The testing results show that DRL can be successfully used for co-optimization, leading to collision-free on-ramp merging. When compared with sequential approaches wherein the upper-level on-ramp merging control and the lower-level PHEV energy management are performed independently and in sequence, we found that co-optimization results in economic but jerky on-ramp merging while sequential approaches may result in collisions due to neglecting powertrain power limit constraints in designing the upper-level on-ramp merging controller.      
### 17.Load-Flow Solvability under Security Constraints in DC Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.03108.pdf)
>  We present sufficient conditions for the load-flow solvability under security constraints in DC distribution networks. In addition, we show that a load-flow solution that fulfills security constraints can be obtained via a convex optimization.      
### 18.On First Integrals of Hamiltonian Systems with Holonomic Hamiltonian  [ :arrow_down: ](https://arxiv.org/pdf/2203.03095.pdf)
>  This paper investigates the solution of the Hamilton-Jacobi equation (HJE) with holonomic Hamiltonian in terms of the first integrals of the corresponding Hamiltonian system. Holonomic functions are related to a specific type of partial differential equations called Pfaffian systems, whose solution space can be regarded as a finite dimensional real vector space. In the finite dimensional solution space, the existence of the first integrals that define a solution of the HJE is characterized by a finite number of algebraic equations for finite dimensional vectors, which is easy to solve and verify. A numerical example is provided to illustrate the derived characterization.      
### 19.Virtual vs. Reality: External Validation of COVID-19 Classifiers using XCAT Phantoms for Chest Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2203.03074.pdf)
>  Research studies of artificial intelligence models in medical imaging have been hampered by poor generalization. This problem has been especially concerning over the last year with numerous applications of deep learning for COVID-19 diagnosis. Virtual imaging trials (VITs) could provide a solution for objective evaluation of these models. In this work utilizing the VITs, we created the CVIT-COVID dataset including 180 virtually imaged computed tomography (CT) images from simulated COVID-19 and normal phantom models under different COVID-19 morphology and imaging properties. We evaluated the performance of an open-source, deep-learning model from the University of Waterloo trained with multi-institutional data and an in-house model trained with the open clinical dataset called MosMed. We further validated the model's performance against open clinical data of 305 CT images to understand virtual vs. real clinical data performance. The open-source model was published with nearly perfect performance on the original Waterloo dataset but showed a consistent performance drop in external testing on another clinical dataset (AUC=0.77) and our simulated CVIT-COVID dataset (AUC=0.55). The in-house model achieved an AUC of 0.87 while testing on the internal test set (MosMed test set). However, performance dropped to an AUC of 0.65 and 0.69 when evaluated on clinical and our simulated CVIT-COVID dataset. The VIT framework offered control over imaging conditions, allowing us to show there was no change in performance as CT exposure was changed from 28.5 to 57 mAs. The VIT framework also provided voxel-level ground truth, revealing that performance of in-house model was much higher at AUC=0.87 for diffuse COVID-19 infection size &gt;2.65% lung volume versus AUC=0.52 for focal disease with &lt;2.65% volume. The virtual imaging framework enabled these uniquely rigorous analyses of model performance.      
### 20.High Speed Emulation in a Vehicle-in-the-Loop Driving Simulator  [ :arrow_down: ](https://arxiv.org/pdf/2203.03043.pdf)
>  Rendering accurate multisensory feedback is critical to ensure natural user behavior in driving simulators. In this work, we present a virtual reality (VR)-based Vehicle-in-the-Loop (ViL) simulator that provides visual, vestibular, and haptic feedback to drivers in high speed driving conditions. Designing our simulator around a four-wheel steer-by-wire vehicle enables us to emulate the dynamics of a vehicle traveling significantly faster than the test vehicle and to transmit corresponding haptic steering feedback to the driver. By scaling the speed of the test vehicle through a combination of VR visuals, vehicle dynamics emulation, and steering wheel force feedback, we can safely and immersively run experiments up to highway speeds within a limited driving space. In double lane change and highway weaving experiments, our high speed emulation method tracks yaw motion within human perception limits and provides sensory feedback comparable to the same maneuvers driven manually.      
### 21.Frames for Graph Signals on the Symmetric Group: A Representation Theoretic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.03036.pdf)
>  An important problem in the field of graph signal processing is developing appropriate overcomplete dictionaries for signals defined on different families of graphs. The Cayley graph of the symmetric group has natural applications in ranked data analysis, as its vertices represent permutations, while the generating set formalizes a notion of distance between rankings. Taking advantage of the rich theory of representations of the symmetric group, we study a particular class of frames, called Frobenius-Schur frames, where every atom belongs to the coefficient space of only one irreducible representation of the symmetric group. We provide a characterization for all Frobenius-Schur frames on the group algebra of the symmetric group which are "compatible" with respect to the generating set. Such frames have been previously studied for the permutahedron, the Cayley graph of the symmetric group with the generating set of adjacent transpositions, and have proved to be capable of producing meaningful interpretation of the ranked data set via the analysis coefficients. Our results generalize frame constructions for the permutahedron to any inverse-closed generating set.      
### 22.Deep Reinforcement Learning based Model-free On-line Dynamic Multi-Microgrid Formation to Enhance Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2203.03030.pdf)
>  Multi-microgrid formation (MMGF) is a promising solution to enhance power system resilience. This paper proposes a new deep reinforcement learning (RL) based model-free on-line dynamic multi-MG formation (MMGF) scheme. The dynamic MMGF problem is formulated as a Markov decision process, and a complete deep RL framework is specially designed for the topology-transformable micro-grids. In order to reduce the large action space caused by flexible switch operations, a topology transformation method is proposed and an action-decoupling Q-value is applied. Then, a CNN based multi-buffer double deep Q-network (CM-DDQN) is developed to further improve the learning ability of original DQN method. The proposed deep RL method provides real-time computing to support on-line dynamic MMGF scheme, and the scheme handles a long-term resilience enhancement problem using adaptive on-line MMGF to defend changeable conditions. The effectiveness of the proposed method is validated using a 7-bus system and the IEEE 123-bus system. The results show strong learning ability, timely response for varying system conditions and convincing resilience enhancement.      
### 23.Extended Load Flexibility of Industrial P2H Plants: A Process Constraint-Aware Scheduling Approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.02991.pdf)
>  The operational flexibility of industrial power-to-hydrogen (P2H) plants enables admittance of volatile renewable power and provides auxiliary regulatory services for the power grid. Aiming to extend the flexibility of the P2H plant further, this work presents a scheduling method by considering detailed process constraints of the alkaline electrolyzers. Unlike existing works that assume constant load range, the presented scheduling framework fully exploits the dynamic processes of the electrolyzer, including temperature and hydrogen-to-oxygen (HTO) crossover, to improve operational flexibility. Varying energy conversion efficiency under different load levels and temperature is also considered. The scheduling model is solved by proper mathematical transformation as a mixed-integer linear programming (MILP), which determines the on-off-standby states and power levels of different electrolyzers in the P2H plant for daily operation. With experiment-verified constraints, a case study show that compared to the existing scheduling approach, the improved flexibility leads to a 1.627% profit increase when the P2H plant is directly coupled to the photovoltaic power.      
### 24.Two Channel Filter Banks on Arbitrary Graphs with Positive Semi Definite Variation Operators  [ :arrow_down: ](https://arxiv.org/pdf/2203.02858.pdf)
>  We study the design of filter banks for signals defined on the nodes of graphs. We propose novel two channel filter banks, that can be applied to arbitrary graphs, given a positive semi definite variation operator, while using downsampling operators on arbitrary vertex partitions. The proposed filter banks also satisfy several desirable properties, including perfect reconstruction, and critical sampling, while having efficient implementations. Our results generalize previous approaches only valid for the normalized Laplacian of bipartite graphs. We consider graph Fourier transforms (GFTs) given by the generalized eigenvectors of the variation operator. This GFT basis is orthogonal in an alternative inner product space, which depends on the choices of downsampling sets and variation operators. We show that the spectral folding property of the normalized Laplacian of bipartite graphs, at the core of bipartite filter bank theory, can be generalized for the proposed GFT if the inner product matrix is chosen properly. We give a probabilistic interpretation to the proposed filter banks using Gaussian graphical models. We also study orthogonality properties of tree structured filter banks, and propose a vertex partition algorithm for downsampling. We show that the proposed filter banks can be implemented efficiently on 3D point clouds, with hundreds of thousands of points (nodes), while also improving the color signal representation quality over competing state of the art approaches.      
### 25.Leveraging Pre-trained BERT for Audio Captioning  [ :arrow_down: ](https://arxiv.org/pdf/2203.02838.pdf)
>  Audio captioning aims at using natural language to describe the content of an audio clip. Existing audio captioning systems are generally based on an encoder-decoder architecture, in which acoustic information is extracted by an audio encoder and then a language decoder is used to generate the captions. Training an audio captioning system often encounters the problem of data scarcity. Transferring knowledge from pre-trained audio models such as Pre-trained Audio Neural Networks (PANNs) have recently emerged as a useful method to mitigate this issue. However, there is less attention on exploiting pre-trained language models for the decoder, compared with the encoder. BERT is a pre-trained language model that has been extensively used in Natural Language Processing (NLP) tasks. Nevertheless, the potential of BERT as the language decoder for audio captioning has not been investigated. In this study, we demonstrate the efficacy of the pre-trained BERT model for audio captioning. Specifically, we apply PANNs as the encoder and initialize the decoder from the public pre-trained BERT models. We conduct an empirical study on the use of these BERT models for the decoder in the audio captioning model. Our models achieve competitive results with the existing audio captioning methods on the AudioCaps dataset.      
### 26.Systematic, Lyapunov-Based, Safe and Stabilizing Controller Synthesis for Constrained Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.02835.pdf)
>  A controller synthesis method for state- and input-constrained nonlinear systems is presented that seeks continuous piecewise affine (CPA) Lyapunov-like functions and controllers simultaneously. Non-convex optimization problems are formulated on triangulated subsets of the admissible states that can be refined to meet primary control objectives, such as stability and safety, alongside secondary performance objectives. A multi-stage design is also given that enlarges the region of attraction (ROA) sequentially while allowing exclusive performance for each stage. A clear boundary for an invariant subset of closed-loop system's ROA is obtained from the resulting Lipschitz Lyapunov function. For control-affine nonlinear systems, the non-convex problem is formulated as a series of conservative, but well-posed, semi-definite programs. These decrease the cost function iteratively until the design objectives are met. Since the resulting CPA Lyapunov-like functions are also Lipschitz control (or barrier) Lyapunov functions, they can be used in online quadratic programming to find minimum-norm control inputs. Numerical examples are provided to demonstrate the effectiveness of the method.      
### 27.Data-driven input reconstruction and experimental validation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02827.pdf)
>  This paper addresses a data-driven input reconstruction problem based on Willems' Fundamental Lemma in which unknown input estimators (UIEs) are constructed directly from historical I/O data. Given only output measurements, the inputs are estimated by the UIE, which is shown to asymptotically converge to the true input without knowing the initial conditions. Both open-loop and closed-loop UIEs are developed based on Lyapunov conditions and the Luenberger-observer-type feedback, whose convergence properties are studied. An experimental study is presented demonstrating the efficacy of the closed-loop UIE for estimating the occupancy of a building on the EPFL campus via measured carbon dioxide levels.      
### 28.Safely: Safe Stochastic Motion Planning Under Constrained Sensing via Duality  [ :arrow_down: ](https://arxiv.org/pdf/2203.02816.pdf)
>  Consider a robot operating in an uncertain environment with stochastic, dynamic obstacles. Despite the clear benefits for trajectory optimization, it is often hard to keep track of each obstacle at every time step due to sensing and hardware limitations. We introduce the Safely motion planner, a receding-horizon control framework, that simultaneously synthesizes both a trajectory for the robot to follow as well as a sensor selection strategy that prescribes trajectory-relevant obstacles to measure at each time step while respecting the sensing constraints of the robot. We perform the motion planning using sequential quadratic programming, and prescribe obstacles to sense based on the duality information associated with the convex subproblems. We guarantee safety by ensuring that the probability of the robot colliding with any of the obstacles is below a prescribed threshold at every time step of the planned robot trajectory. We demonstrate the efficacy of the Safely motion planner through software and hardware experiments.      
### 29.28 GHz Phased Array-Based Self-Interference Measurements for Millimeter Wave Full-Duplex  [ :arrow_down: ](https://arxiv.org/pdf/2203.02809.pdf)
>  We present measurements of the 28 GHz self-interference channel for full-duplex sectorized multi-panel millimeter wave (mmWave) systems, such as integrated access and backhaul. We measure the isolation between the input of a transmitting phased array panel and the output of a co-located receiving phased array panel, each of which is electronically steered across a number of directions in azimuth and elevation. In total, nearly 6.5 million measurements were taken in an anechoic chamber to densely inspect the directional nature of the coupling between 256-element phased arrays. We observe that highly directional mmWave beams do not necessarily offer widespread high isolation between transmitting and receiving arrays. Rather, our measurements indicate that steering the transmitter or receiver away from the other tends to offer higher isolation but even slight steering changes can lead to drastic variations in isolation. These measurements can be useful references when developing mmWave full-duplex solutions and can motivate a variety of future topics including beam/user selection and beamforming codebook design.      
### 30.Algebraic theory of phase retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2203.02774.pdf)
>  The purpose of this article is to discuss recent advances in the growing field of phase retrieval, and to publicize open problems that we believe will be of interest to mathematicians in general, and algebraists in particular.      
### 31.Rib Suppression in Digital Chest Tomosynthesis  [ :arrow_down: ](https://arxiv.org/pdf/2203.02772.pdf)
>  Digital chest tomosynthesis (DCT) is a technique to produce sectional 3D images of a human chest for pulmonary disease screening, with 2D X-ray projections taken within an extremely limited range of angles. However, under the limited angle scenario, DCT contains strong artifacts caused by the presence of ribs, jamming the imaging quality of the lung area. Recently, great progress has been achieved for rib suppression in a single X-ray image, to reveal a clearer lung texture. We firstly extend the rib suppression problem to the 3D case at the software level. We propose a $\textbf{T}$omosynthesis $\textbf{RI}$b Su$\textbf{P}$pression and $\textbf{L}$ung $\textbf{E}$nhancement $\textbf{Net}$work (TRIPLE-Net) to model the 3D rib component and provide a rib-free DCT. TRIPLE-Net takes the advantages from both 2D and 3D domains, which model the ribs in DCT with the exact FBP procedure and 3D depth information, respectively. The experiments on simulated datasets and clinical data have shown the effectiveness of TRIPLE-Net to preserve lung details as well as improve the imaging quality of pulmonary diseases. Finally, an expert user study confirms our findings.      
### 32.Bounds on Power and Common Message Fractions for RSMA with Imperfect SIC  [ :arrow_down: ](https://arxiv.org/pdf/2203.02748.pdf)
>  Rate-Splitting multiple access (RSMA) has emerged as a key enabler to improve the performance of the beyond fifth-generation (5G) cellular networks. The existing RSMA literature has typically considered the sum-rate of the users to evaluate the performance of RSMA. However, it has been shown in the existing works that maximizing sum-rate can result in asymmetric user performance. It can result in a significant enhancement of one user rate at the cost of the other RSMA user. Further, imperfections can reduce the performance of successive interference cancellation (SIC)-based RSMA. Therefore, in this paper, we consider the imperfection in successive interference cancellation (SIC) and derive suitable bounds on fractions of the power of common and private messages and the fraction of common message intended for each user in an RSMA pair such that their individual RSMA rates are greater than their respective OMA rates. Through simulations, we validate the derived bounds and show that the individual RSMA rates of users can be greater than their respective OMA rates by selecting the parameters within the derived bounds.      
### 33.Convergence of the Distributed SG Algorithm Under Cooperative Excitation Condition  [ :arrow_down: ](https://arxiv.org/pdf/2203.02743.pdf)
>  In this paper, a distributed stochastic gradient (SG) algorithm is proposed where the estimators are aimed to collectively estimate an unknown time-invariant parameter from a set of noisy measurements obtained by distributed sensors. The proposed distributed SG algorithm combines the consensus strategy of the estimation of neighbors with the diffusion of regression vectors. A cooperative excitation condition is introduced, under which the convergence of the distributed SG algorithm can be obtained without relying on the independency and stationarity assumptions of regression vectors which are commonly used in existing literature. Furthermore, the convergence rate of the algorithm can be established. Finally, we show that all sensors can cooperate to fulfill the estimation task even though any individual sensor can not by a simulation example.      
### 34.Weighted Mean and Median graph Filters with Attenuation Factor for Sensor Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.02741.pdf)
>  This paper proposes a weighted attenuation k-hop graph, which depicts the spatial neighbor nodes with their hops from the central node. Based on this k-kop graph, we further propose a node selecting graph, which selects temporal neighbor nodes of multiple instances of the central node. With this node selecting graph, we propose a graph mean filter. In addition, we also apply the proposed node selecting graph to the median filter. Finally, the experimental results show that the proposed mean filter performs better than the original median filter in the signal denoising polluted by white noise and the median filter using node selecting graph also has better performance than the original median filter.      
### 35.Distributed Sparse Identification for Stochastic Dynamic Systems under Cooperative Non-Persistent Excitation Condition  [ :arrow_down: ](https://arxiv.org/pdf/2203.02737.pdf)
>  This paper considers the distributed sparse identification problem over wireless sensor networks such that all sensors cooperatively estimate the unknown sparse parameter vector of stochastic dynamic systems by using the local information from neighbors. A distributed sparse least squares algorithm is proposed by minimizing a local information criterion formulated as a linear combination of accumulative local estimation error and L_1-regularization term. The upper bounds of the estimation error and the regret of the adaptive predictor of the proposed algorithm are presented. Furthermore, by designing a suitable adaptive weighting coefficient based on the local observation data, the set convergence of zero elements with a finite number of observations is obtained under a cooperative non-persistent excitation condition. It is shown that the proposed distributed algorithm can work well in a cooperative way even though none of the individual sensors can fulfill the estimation task. Our theoretical results are obtained without relying on the independency assumptions of regression signals that have been commonly used in the existing literature. Thus, our results are expected to be applied to stochastic feedback systems. Finally, the numerical simulations are provided to demonstrate the effectiveness of our theoretical results.      
### 36.A Novel Dual Dense Connection Network for Video Super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2203.02723.pdf)
>  Video super-resolution (VSR) refers to the reconstruction of high-resolution (HR) video from the corresponding low-resolution (LR) video. Recently, VSR has received increasing attention. In this paper, we propose a novel dual dense connection network that can generate high-quality super-resolution (SR) results. The input frames are creatively divided into reference frame, pre-temporal group and post-temporal group, representing information in different time periods. This grouping method provides accurate information of different time periods without causing time information disorder. Meanwhile, we produce a new loss function, which is beneficial to enhance the convergence ability of the model. Experiments show that our model is superior to other advanced models in Vid4 datasets and SPMCS-11 datasets.      
### 37.High-resolution Coastline Extraction in SAR Images via MISP-GGD Superpixel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02708.pdf)
>  High accuracy coastline/shoreline extraction from SAR imagery is a crucial step in a number of maritime and coastal monitoring applications. We present a method based on image segmentation using the Generalised Gamma Mixture Model superpixel algorithm (MISP-GGD). MISP-GGD produces superpixels adhering with great accuracy to object edges in the image, such as the coastline. Unsupervised clustering of the generated superpixels according to textural and radiometric features allows for generation of a land/water mask from which a highly accurate coastline can be extracted. We present results of our proposed method on a number of SAR images of varying characteristics.      
### 38.Reconfigurable Intelligent Surface-Aided Joint Radar and Covert Communications: Fundamentals, Optimization, and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2203.02704.pdf)
>  Future wireless communication systems will evolve toward multi-functional integrated systems to improve spectrum utilization and reduce equipment sizes. A joint radar and communication (JRC) system, which can support simultaneous information transmission and target detection, has been regarded as a promising solution for emerging applications such as autonomous vehicles. In JRC, data security and privacy protection are critical issues. Thus, we first apply covert communication into JRC and propose a joint radar and covert communication (JRCC) system to achieve high spectrum utilization and secure data transmission simultaneously. In the JRCC system, an existence of sensitive data transmission is hidden from a maliciously observant warden. However, the performance of JRCC is restricted by severe signal propagation environment and hardware devices. Fortunately, reconfigurable intelligent surfaces (RISs) can change the signal propagation smartly to improve the networks performance with low cost. We first overview fundamental concepts of JRCC and RIS and then propose the RIS-aided JRCC system design. Furthermore, both covert communication and radar performance metrics are investigated and a game theory-based covert rate optimization scheme is designed to achieve secure communication. Finally, we present several promising applications and future directions of RIS-aided JRCC systems.      
### 39.Reconfigurable Intelligent Surface (RIS)-aided Vehicular Networks: Their Protocols, Resource Allocation, and Performance  [ :arrow_down: ](https://arxiv.org/pdf/2203.02691.pdf)
>  Reconfigurable intelligent surfaces (RISs) assist in paving the way for the evolution of conventional vehicular networks to autonomous driving. Having said that, the 3rd Generation Partnership Project (3GPP) faces numerous open challenges concerning the RIS-aided vehicle-to-everything (V2X) solutions of the near future. To tackle these challenges and to stimulate future research, this article focuses on the prospective transmission design of RIS-aided V2X communications. In particular, two V2X sidelink modes are enhanced by exploiting RISs and their variants, followed by a customized transmission frame structure that partitions the transmission efforts into different phases. Next, effective channel tracking and resource allocation techniques are developed for attaining a high beamforming gain at low overhead and complexity. Finally, promising research topics are highlighted and future 3GPP standardization items are proposed for RISaided V2X systems.      
### 40.IDmUNet: A new image decomposition induced network for sparse feature segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02690.pdf)
>  UNet and its variants are among the most popular methods for medical image segmentation. Despite their successes in task generality, most of them consider little mathematical modeling behind specific applications. In this paper, we focus on the sparse feature segmentation task and make a task-oriented network design, in which the target objects are sparsely distributed and the background is hard to be mathematically modeled. We start from an image decomposition model with sparsity regularization, and propose a deep unfolding network, namely IDNet, based on an iterative solver, scaled alternating direction method of multipliers (scaled-ADMM). The IDNet splits raw inputs into double feature layers. Then a new task-oriented segmentation network is constructed, dubbed as IDmUNet, based on the proposed IDNets and a mini-UNet. Because of the sparsity prior and deep unfolding method in the structure design, this IDmUNet combines the advantages of mathematical modeling and data-driven approaches. Firstly, our approach has mathematical interpretability and can achieve favorable performance with far fewer learnable parameters. Secondly, our IDmUNet is robust in a simple end-to-end training with explainable behaviors. In the experiments of retinal vessel segmentation (RVS), IDmUNet produces the state-of-the-art results with only 0.07m parameters, whereas SA-UNet, one of the latest variants of UNet, contains 0.54m and the original UNet 31.04m. Moreover, the training procedure of our network converges faster without overfitting phenomenon. This decomposition-based network construction strategy can be generalized to other problems with mathematically clear targets and complicated unclear backgrounds.      
### 41.Language vs Speaker Change: A Comparative Study  [ :arrow_down: ](https://arxiv.org/pdf/2203.02680.pdf)
>  Spoken language change detection (LCD) refers to detecting language switching points in a multilingual speech signal. Speaker change detection (SCD) refers to locating the speaker change points in a multispeaker speech signal. The objective of this work is to understand the challenges in LCD task by comparing it with SCD task. Human subjective study for change detection is performed for LCD and SCD. This study demonstrates that LCD requires larger duration spectro-temporal information around the change point compared to SCD. Based on this, the work explores automatic distance based and model based LCD approaches. The model based ones include Gaussian mixture model and universal background model (GMM-UBM), attention, and Generative adversarial network (GAN) based approaches. Both the human and automatic LCD tasks infer that the performance of the LCD task improves by incorporating more and more spectro-temporal duration.      
### 42.Security of Underwater and Air-Water Wireless Communication  [ :arrow_down: ](https://arxiv.org/pdf/2203.02667.pdf)
>  We present a first detailed survey that focuses on the security challenges faced by the underwater and air-water (A-W) wireless communication networks (WCNs), as well as the countermeasures proposed to date. Specifically, we provide a detailed literature review of the various kinds of active and passive attacks which hamper the confidentiality, integrity, authentication and availability of both underwater and A-W WCNs. For clarity of exposition, this survey paper is mainly divided into two parts. The first part of the paper is essentially a primer on underwater and A-W WCNs whereby we outline the benefits and drawbacks of the three promising underwater and A-W candidate technologies: radio frequency (RF), acoustic, and optical, along with channel modelling. To this end, we also describe the indirect (relay-aided) and direct mechanisms for the A-W WCNs along with channel modelling. This sets the stage for the second part (and main contribution) of the paper whereby we provide a thorough comparative discussion of a vast set of works that have reported the security breaches (as well as viable countermeasures) for many diverse configurations of the underwater and A-W WCNs. Finally, we highlight some research gaps in the open literature and identify some open problems for the future work.      
### 43.Online Stabilization of Unknown Networked Systems with Communication Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2203.02630.pdf)
>  We investigate the problem of stabilizing an unknown networked linear system under communication constraints and adversarial disturbances. We propose the first provably stabilizing algorithm for the problem. The algorithm uses a distributed version of nested convex body chasing to maintain a consistent estimate of the network dynamics and applies system level synthesis to determine a distributed controller based on this estimated model. Our approach avoids the need for system identification and accommodates a broad class of communication delay while being fully distributed and scaling favorably with the number of subsystems in the network.      
### 44.Geodesic Gramian Denoising Applied to the Images Contaminated With Noise Sampled From Diverse Probability Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2203.02600.pdf)
>  As quotidian use of sophisticated cameras surges, people in modern society are more interested in capturing fine-quality images. However, the quality of the images might be inferior to people's expectations due to the noise contamination in the images. Thus, filtering out the noise while preserving vital image features is an essential requirement. Current existing denoising methods have their own assumptions on the probability distribution in which the contaminated noise is sampled for the method to attain its expected denoising performance. In this paper, we utilize our recent Gramian-based filtering scheme to remove noise sampled from five prominent probability distributions from selected images. This method preserves image smoothness by adopting patches partitioned from the image, rather than pixels, and retains vital image features by performing denoising on the manifold underlying the patch space rather than in the image domain. We validate its denoising performance, using three benchmark computer vision test images applied to two state-of-the-art denoising methods, namely BM3D and K-SVD.      
### 45.A Scenario Approach to Risk-Aware Safety-Critical System Verification  [ :arrow_down: ](https://arxiv.org/pdf/2203.02595.pdf)
>  With the growing interest in deploying robots in unstructured and uncertain environments, there has been increasing interest in factoring risk into safety-critical control development. Similarly, the authors believe risk should also be accounted in the verification of these controllers. In pursuit of sample-efficient methods for uncertain black-box verification then, we first detail a method to estimate the Value-at-Risk of arbitrary scalar random variables without requiring \textit{apriori} knowledge of its distribution. Then, we reformulate the uncertain verification problem as a Value-at-Risk estimation problem making use of our prior results. In doing so, we provide fundamental sampling requirements to bound with high confidence the volume of states and parameters for a black-box system that could potentially yield unsafe phenomena. We also show that this procedure works independent of system complexity through simulated examples of the Robotarium.      
### 46.Virtual Histological Staining of Label-Free Total Absorption Photoacoustic Remote Sensing (TA-PARS)  [ :arrow_down: ](https://arxiv.org/pdf/2203.02584.pdf)
>  Histopathological visualizations are a pillar of modern medicine and biological research. For example, surgical oncology relies heavily on post-operative histology to determine surgical success and guide adjunct treatments. However, current histology, featuring bright-field microscopic assessment of histochemical stained tissues exhibits several major challenges. Developing stained specimens for brightfield assessment requires laborious sample preparation, potentially delaying adjunct treatments for days or weeks. Hence, there is motivation for an alternative histopathology method mitigating the challenges of current techniques. Here, a deep-learning-based method is presented for label-free virtual histochemical staining of total-absorption photoacoustic remote sensing (TA-PARS) images of unprocessed tissue sections. The TA-PARS provides an array of contrasts (scattering, and radiative and non-radiative absorption) ideal for developing AI-based H&amp;E colorizations. In this work, a Pix2Pix generative adversarial network (GAN) model, is applied to develop virtual H&amp;E staining from label-free TA-PARS images. Thin sections of human skin tissue are first virtually stained with TA-PARS, then are chemically stained with H&amp;E producing a direct one-to-one comparison. One-to-one matched virtually- and chemically- stained images exhibit high concordance validating the digital colorization of TA-PARS images against the gold-standard H&amp;E. Results demonstrate that virtual staining of TA-PARS images can provide label-free data of similar diagnostic quality to traditional H&amp;E. This paves the way for TA-PARS rapid slide-free histological imaging, to avoid the typical labor-intensive chemical staining procedures. In the future this may substantially reduce histological imaging times, while simultaneously reducing staining variability by removing operator dependency during histochemical staining.      
### 47.Improving the Energy Efficiency and Robustness of tinyML Computer Vision using Log-Gradient Input Images  [ :arrow_down: ](https://arxiv.org/pdf/2203.02571.pdf)
>  This paper studies the merits of applying log-gradient input images to convolutional neural networks (CNNs) for tinyML computer vision (CV). We show that log gradients enable: (i) aggressive 1.5-bit quantization of first-layer inputs, (ii) potential CNN resource reductions, and (iii) inherent robustness to illumination changes (1.7% accuracy loss across 1/32...8 brightness variation vs. up to 10% for JPEG). We establish these results using the PASCAL RAW image data set and through a combination of experiments using neural architecture search and a fixed three-layer network. The latter reveal that training on log-gradient images leads to higher filter similarity, making the CNN more prunable. The combined benefits of aggressive first-layer quantization, CNN resource reductions, and operation without tight exposure control and image signal processing (ISP) are helpful for pushing tinyML CV toward its ultimate efficiency limits.      
### 48.Improved Wavelets for Image Compression from Unitary Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2203.02556.pdf)
>  We benchmark the efficacy of several novel orthogonal, symmetric, dilation-3 wavelets, derived from a unitary circuit based construction, towards image compression. The performance of these wavelets is compared across several photo databases against the CDF-9/7 wavelets in terms of the minimum number of non-zero wavelet coefficients needed to obtain a specified image quality, as measured by the multi-scale structural similarity index (MS-SSIM). The new wavelets are found to consistently offer better compression efficiency than the CDF-9/7 wavelets across a broad range of image resolutions and quality requirements, averaging 7-8% improved compression efficiency on high-resolution photo images when high-quality (MS-SSIM = 0.99) is required.      
### 49.BoostMIS: Boosting Medical Image Semi-supervised Learning with Adaptive Pseudo Labeling and Informative Active Annotation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02533.pdf)
>  In this paper, we propose a novel semi-supervised learning (SSL) framework named BoostMIS that combines adaptive pseudo labeling and informative active annotation to unleash the potential of medical image SSL models: (1) BoostMIS can adaptively leverage the cluster assumption and consistency regularization of the unlabeled data according to the current learning status. This strategy can adaptively generate one-hot ``hard'' labels converted from task model predictions for better task model training. (2) For the unselected unlabeled images with low confidence, we introduce an Active learning (AL) algorithm to find the informative samples as the annotation candidates by exploiting virtual adversarial perturbation and model's density-aware entropy. These informative candidates are subsequently fed into the next training cycle for better SSL label propagation. Notably, the adaptive pseudo-labeling and informative active annotation form a learning closed-loop that are mutually collaborative to boost medical image SSL. To verify the effectiveness of the proposed method, we collected a metastatic epidural spinal cord compression (MESCC) dataset that aims to optimize MESCC diagnosis and classification for improved specialist referral and treatment. We conducted an extensive experimental study of BoostMIS on MESCC and another public dataset COVIDx. The experimental results verify our framework's effectiveness and generalisability for different medical image datasets with a significant improvement over various state-of-the-art methods.      
### 50.Optimization of Traffic Control in MMAP[c]/PH[c]/S Catastrophic Queueing Model with PH Retrial Times and Controllable Preemptive Repeat Priority Policy  [ :arrow_down: ](https://arxiv.org/pdf/2203.02508.pdf)
>  The presented study elaborates a multi-server catastrophic retrial queueing model considering preemptive repeat priority policy with phase-type (PH) distributed retrial times. For the sake of comprehension, the scenario of model operation prior and later to the occurrence of the disaster is referred to as the normal scenario and as the catastrophic scenario, respectively. In the normal scenario, the incoming heterogeneous calls are categorized as handoff calls and new calls. Handoff calls are provided controllable preemptive priority over new calls. In the catastrophic scenario, when a disaster causes the shut down of the entire system and failure of all functioning channels, a set of backup channels is quickly deployed to restore services. Due to the emergency situation in the concerned area, the incoming heterogeneous calls are divided into three categories: handoff, new call, and emergency calls. Emergency calls are provided controllable preemptive priority over new/handoff calls due to the pressing need to save lives in such situations. The Markov chain's ergodicity criteria are established by demonstrating that it belongs to the class of asymptotically quasi-Toeplitz Markov chains (AQTMC). Further, a multi-objective optimization problem to obtain optimal number of backup channels has been formulated and dealt by employing non-dominated sorting genetic algorithm-II (NSGA-II) approach.      
### 51.Parallel Fourier Ptychography reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2203.02507.pdf)
>  Fourier ptychography has attracted a wide range of focus for its ability of large space-bandwidth-produce, and quantative phase measurement. It is a typical computational imaging technique which refers to optimizing both the imaging hardware and reconstruction algorithms simultaneously. The data redundancy and inverse problem algorithms are the sources of FPM's excellent performance. But at the same time, this large amount of data processing and complex algorithms also greatly reduce the imaging speed. In this article, we propose a parallel Fourier ptychography reconstruction framework consisting of three levels of parallel computing parts and implemented it with both central processing unit (CPU) and compute unified device architecture (CUDA) platform. In the conventional FPM reconstruction framework, the sample image is divided into multiple sub-regions for separately processing because the illumination angles for different subregions are varied for the same LED and different subregions contain different defocus distances due to the non-planar distribution or non-ideal posture of biological sample. We first build a parallel computing sub-framework in spatial domain based on the above-mentioned characteristics. And then, by utilizing the sequential characteristics of different spectrum regions to update, a parallel computing sub-framework in the spectrum domain is carried out in our scheme. The feasibility of the proposed parallel FPM reconstruction framework is verified with different experimental results acquired with the system we built.      
### 52.Using Timeliness in Tracking Infections  [ :arrow_down: ](https://arxiv.org/pdf/2203.03602.pdf)
>  We consider real-time timely tracking of infection status (e.g., covid-19) of individuals in a population. In this work, a health care provider wants to detect infected people as well as people who have recovered from the disease as quickly as possible. In order to measure the timeliness of the tracking process, we use the long-term average difference between the actual infection status of the people and their real-time estimate by the health care provider based on the most recent test results. We first find an analytical expression for this average difference for given test rates, infection rates and recovery rates of people. Next, we propose an alternating minimization based algorithm to find the test rates that minimize the average difference. We observe that if the total test rate is limited, instead of testing all members of the population equally, only a portion of the population may be tested in unequal rates calculated based on their infection and recovery rates. Next, we characterize the average difference when the test measurements are erroneous (i.e., noisy). Further, we consider the case where the infection status of individuals may be dependent, which happens when an infected person spreads the disease to another person if they are not detected and isolated by the health care provider. Then, we consider an age of incorrect information based error metric where the staleness metric increases linearly over time as long as the health care provider does not detect the changes in the infection status of the people. In numerical results, we observe that an increased population size increases diversity of people with different infection and recovery rates which may be exploited to spend testing capacity more efficiently. Depending on the health care provider's preferences, test rate allocation can be adjusted to detect either the infected people or the recovered people more quickly.      
### 53.Manoeuvre detection in Low Earth Orbit with Radar Data  [ :arrow_down: ](https://arxiv.org/pdf/2203.03590.pdf)
>  This work outlines and assesses several methods for the detection of manoeuvres in Low Earth Orbit (LEO) from surveillance radar data. To be able to detect manoeuvres, the main starting assumption is that the object under analysis has an orbit known with a sufficient degree of precision. Based on the precise (a posteriori) orbit and radar data, several manoeuvre detection methods are presented; one is based on unscented Kalman filtering, whereas two others algorithms are based on reachability analysis of the state, which correlates its prediction set with the next track from the radar. The filtering algorithm can be extended for several radar tracks, whereas the reachability-based methods are more precise in detecting manoeuvres. Then, to inherit the best properties of both classes of algorithms, a manoeuvre detection filter that combines both concepts is finally presented. Manoeuvre detection results are presented first for simulated scenarios -- for validation and calibration purposes -- and later for real data. Radar information comes from the Spanish Space Surveillance Radar (S3TSR), with real manoeuvre information and high-quality ephemerides. The results show promise, taking into account that a single surveillance radar is the only source of data, obtaining manoeuvre detection rates of more than 50% and false positive rates of less than 10%.      
### 54.Korean Tokenization for Beam Search Rescoring in Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.03583.pdf)
>  The performance of automatic speech recognition (ASR) models can be greatly improved by proper beam-search decoding with external language model (LM). There has been an increasing interest in Korean speech recognition, but not many studies have been focused on the decoding procedure. In this paper, we propose a Korean tokenization method for neural network-based LM used for Korean ASR. Although the common approach is to use the same tokenization method for external LM as the ASR model, we show that it may not be the best choice for Korean. We propose a new tokenization method that inserts a special token, SkipTC, when there is no trailing consonant in a Korean syllable. By utilizing the proposed SkipTC token, the input sequence for LM becomes very regularly patterned so that the LM can better learn the linguistic characteristics. Our experiments show that the proposed approach achieves a lower word error rate compared to the same LM model without SkipTC. In addition, we are the first to report the ASR performance for the recently introduced large-scale 7,600h Korean speech dataset.      
### 55.Improving CTC-based speech recognition via knowledge transferring from pre-trained language models  [ :arrow_down: ](https://arxiv.org/pdf/2203.03582.pdf)
>  Recently, end-to-end automatic speech recognition models based on connectionist temporal classification (CTC) have achieved impressive results, especially when fine-tuned from wav2vec2.0 models. Due to the conditional independence assumption, CTC-based models are always weaker than attention-based encoder-decoder models and require the assistance of external language models (LMs). To solve this issue, we propose two knowledge transferring methods that leverage pre-trained LMs, such as BERT and GPT2, to improve CTC-based models. The first method is based on representation learning, in which the CTC-based models use the representation produced by BERT as an auxiliary learning target. The second method is based on joint classification learning, which combines GPT2 for text modeling with a hybrid CTC/attention architecture. Experiment on AISHELL-1 corpus yields a character error rate (CER) of 4.2% on the test set. When compared to the vanilla CTC-based models fine-tuned from the wav2vec2.0 models, our knowledge transferring method reduces CER by 16.1% relatively without external LMs.      
### 56.When BERT Meets Quantum Temporal Convolution Learning for Text Classification in Heterogeneous Computing  [ :arrow_down: ](https://arxiv.org/pdf/2203.03550.pdf)
>  The rapid development of quantum computing has demonstrated many unique characteristics of quantum advantages, such as richer feature representation and more secured protection on model parameters. This work proposes a vertical federated learning architecture based on variational quantum circuits to demonstrate the competitive performance of a quantum-enhanced pre-trained BERT model for text classification. In particular, our proposed hybrid classical-quantum model consists of a novel random quantum temporal convolution (QTC) learning framework replacing some layers in the BERT-based decoder. Our experiments on intent classification show that our proposed BERT-QTC model attains competitive experimental results in the Snips and ATIS spoken language datasets. Particularly, the BERT-QTC boosts the performance of the existing quantum circuit-based language model in two text classification datasets by 1.57% and 1.52% relative improvements. Furthermore, BERT-QTC can be feasibly deployed on both existing commercial-accessible quantum computation hardware and CPU-based interface for ensuring data isolation.      
### 57.State space partitioning based on constrained spectral clustering for block particle filtering  [ :arrow_down: ](https://arxiv.org/pdf/2203.03475.pdf)
>  The particle filter (PF) is a powerful inference tool widely used to estimate the filtering distribution in non-linear and/or non-Gaussian problems. To overcome the curse of dimensionality of PF, the block PF (BPF) inserts a blocking step to partition the state space into several subspaces or blocks of smaller dimension so that the correction and resampling steps can be performed independently on each subspace. Using blocks of small size reduces the variance of the filtering distribution estimate, but in turn the correlation between blocks is broken and a bias is introduced. <br>When the dependence relationships between state variables are unknown, it is not obvious to decide how to split the state space into blocks and a significant error overhead may arise from a poor choice of partitioning. In this paper, we formulate the partitioning problem in the BPF as a clustering problem and we propose a state space partitioning method based on spectral clustering (SC). We design a generalized BPF algorithm that contains two new steps: (i) estimation of the state vector correlation matrix from predicted particles, (ii) SC using this estimate as the similarity matrix to determine an appropriate partition. In addition, a constraint is imposed on the maximal cluster size to prevent SC from providing too large blocks. We show that the proposed method can bring together in the same blocks the most correlated state variables while successfully escaping the curse of dimensionality.      
### 58.Multilevel Monte Carlo with Surrogate Models for Resource Adequacy Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2203.03437.pdf)
>  Monte Carlo simulation is often used for the reliability assessment of power systems, but it converges slowly when the system is complex. Multilevel Monte Carlo (MLMC) can be applied to speed up computation without compromises on model complexity and accuracy that are limiting real-world effectiveness. In MLMC, models with different complexity and speed are combined, and having access to fast approximate models is essential for achieving high speedups. This paper demonstrates how machine-learned surrogate models are able to fulfil this role without excessive manual tuning of models. Different strategies for constructing and training surrogate models are discussed. A resource adequacy case study based on the Great Britain system with storage units is used to demonstrate the effectiveness of the proposed approach, and the sensitivity to surrogate model accuracy. The high accuracy and inference speed of machine-learned surrogates result in very large speedups, compared to using MLMC with hand-built models.      
### 59.Attention-based Region of Interest (ROI) Detection for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.03428.pdf)
>  Automatic emotion recognition for real-life appli-cations is a challenging task. Human emotion expressions aresubtle, and can be conveyed by a combination of several emo-tions. In most existing emotion recognition studies, each audioutterance/video clip is labelled/classified in its entirety. However,utterance/clip-level labelling and classification can be too coarseto capture the subtle intra-utterance/clip temporal dynamics. Forexample, an utterance/video clip usually contains only a fewemotion-salient regions and many emotionless regions. In thisstudy, we propose to use attention mechanism in deep recurrentneural networks to detection the Regions-of-Interest (ROI) thatare more emotionally salient in human emotional speech/video,and further estimate the temporal emotion dynamics by aggre-gating those emotionally salient regions-of-interest. We comparethe ROI from audio and video and analyse them. We comparethe performance of the proposed attention networks with thestate-of-the-art LSTM models on multi-class classification task ofrecognizing six basic human emotions, and the proposed attentionmodels exhibit significantly better performance. Furthermore, theattention weight distribution can be used to interpret how anutterance can be expressed as a mixture of possible emotions.      
### 60.Depth-resolved Laue microdiffraction with coded-apertures  [ :arrow_down: ](https://arxiv.org/pdf/2203.03386.pdf)
>  We introduce a rapid data acquisition and reconstruction method to image the crystalline structure of materials and associated strain and orientations at micrometer resolution using Laue diffraction. Our method relies on scanning a coded-aperture across the diffracted x-ray beams from a broadband illumination, and a reconstruction algorithm to resolve Laue microdiffraction patterns as a function of depth along the incident illumination path. This method provides a rapid access to full diffraction information at sub-micrometer volume elements in bulk materials. Here we present the theory as well as the experimental validation of this imaging approach.      
### 61.A Random Access Protocol for RIS-Aided Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2203.03377.pdf)
>  Reconfigurable intelligent surfaces (RISs) are arrays of passive elements that can control the reflection of the incident electromagnetic waves. While RIS are particularly useful to avoid blockages, the protocol aspects for their implementation have been largely overlooked. In this paper, we devise a random access protocol for a RIS-assisted wireless communication setting. Rather than tailoring RIS reflections to meet the positions of users equipment (UEs), our protocol relies on a finite set of RIS configurations designed to cover the area of interest. The protocol is comprised of a downlink training phase followed by an uplink access phase. During these phases, a base station (BS) controls the RIS to sweep over its configurations. The UEs then receive training signals to measure the channel quality with the different RIS configurations and refine their access policies. Numerical results show that our protocol increases the average number of successful access attempts; however, at the expense of increased access delay due to the realization of a training period. Promising results are further observed in scenarios with a high access load.      
### 62.A Unified Formulation of Geometry-aware Dynamic Movement Primitives  [ :arrow_down: ](https://arxiv.org/pdf/2203.03374.pdf)
>  Learning from demonstration (LfD) is considered as an efficient way to transfer skills from humans to robots. Traditionally, LfD has been used to transfer Cartesian and joint positions and forces from human demonstrations. The traditional approach works well for some robotic tasks, but for many tasks of interest it is necessary to learn skills such as orientation, impedance, and/or manipulability that have specific geometric characteristics. An effective encoding of such skills can be only achieved if the underlying geometric structure of the skill manifold is considered and the constrains arising from this structure are fulfilled during both learning and execution. However, typical learned skill models such as dynamic movement primitives (DMPs) are limited to Euclidean data and fail in correctly embedding quantities with geometric constraints. In this paper, we propose a novel and mathematically principled framework that uses concepts from Riemannian geometry to allow DMPs to properly embed geometric constrains. The resulting DMP formulation can deal with data sampled from any Riemannian manifold including, but not limited to, unit quaternions and symmetric and positive definite matrices. The proposed approach has been extensively evaluated both on simulated data and real robot experiments. The performed evaluation demonstrates that beneficial properties of DMPs, such as convergence to a given goal and the possibility to change the goal during operation, apply also to the proposed formulation.      
### 63.High-Resolution Peak Demand Estimation Using Generalized Additive Models and Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.03342.pdf)
>  This paper presents a method for estimating high-resolution electricity peak demand given lower resolution data. The technique won a data competition organized by the British distribution network operator Western Power Distribution. The exercise was to estimate the minimum and maximum load values in a single substation in a one-minute resolution as precisely as possible. In contrast, the data was given in half-hourly and hourly resolutions. The winning method combines generalized additive models (GAM) and deep artificial neural networks (DNN) which are popular in load forecasting. We provide an extensive analysis of the prediction models, including the importance of input parameters with a focus on load, weather, and seasonal effects. In addition, we provide a rigorous evaluation study that goes beyond the competition frame to analyze the robustness. The results show that the proposed methods are superior, not only in the single competition month but also in the meaningful evaluation study.      
### 64.Joint brain tumor segmentation from multi MR sequences through a deep convolutional neural network  [ :arrow_down: ](https://arxiv.org/pdf/2203.03338.pdf)
>  Brain tumor segmentation is highly contributive in diagnosing and treatment planning. The manual brain tumor delineation is a time-consuming and tedious task and varies depending on the radiologists skill. Automated brain tumor segmentation is of high importance, and does not depend on either inter or intra-observation. The objective of this study is to automate the delineation of brain tumors from the FLAIR, T1 weighted, T2 weighted, and T1 weighted contrast-enhanced MR sequences through a deep learning approach, with a focus on determining which MR sequence alone or which combination thereof would lead to the highest accuracy therein.      
### 65.A novel shape-based loss function for machine learning-based seminal organ segmentation in medical imaging  [ :arrow_down: ](https://arxiv.org/pdf/2203.03336.pdf)
>  Automated medical image segmentation is an essential task to aid/speed up diagnosis and treatment procedures in clinical practices. Deep convolutional neural networks have exhibited promising performance in accurate and automatic seminal segmentation. For segmentation tasks, these methods normally rely on minimizing a cost/loss function that is designed to maximize the overlap between the estimated target and the ground-truth mask delineated by the experts. A simple loss function based on the degrees of overlap (i.e., Dice metric) would not take into account the underlying shape and morphology of the target subject, as well as its realistic/natural variations; therefore, suboptimal segmentation results would be observed in the form of islands of voxels, holes, and unrealistic shapes or deformations. In this light, many studies have been conducted to refine/post-process the segmentation outcome and consider an initial guess as prior knowledge to avoid outliers and/or unrealistic estimations. In this study, a novel shape-based cost function is proposed which encourages/constrains the network to learn/capture the underlying shape features in order to generate a valid/realistic estimation of the target structure. To this end, the Principal Component Analysis (PCA) was performed on a vectorized training dataset to extract eigenvalues and eigenvectors of the target subjects. The key idea was to use the reconstruction weights to discriminate valid outcomes from outliers/erroneous estimations.      
### 66.Neural Enhancement of Factor Graph-based Symbol Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.03333.pdf)
>  We study the application of the factor graph framework for symbol detection on linear inter-symbol interference channels. Cyclic factor graphs have the potential to yield low-complexity symbol detectors, but are suboptimal if the ubiquitous sum-product algorithm is applied. In this paper, we present and evaluate strategies to improve the performance of cyclic factor graph-based symbol detection algorithms by means of neural enhancement. In particular, we apply neural belief propagation as an effective way to counteract the effect of cycles within the factor graph. We further propose the application and optimization of a linear preprocessor of the channel output. By modifying the observation model, the preprocessing can effectively change the underlying factor graph, thereby significantly improving the detection performance as well as reducing the complexity.      
### 67.Trajectory convergence from coordinate-wise decrease of general convex energy functions  [ :arrow_down: ](https://arxiv.org/pdf/2203.03316.pdf)
>  We consider arbitrary trajectories subject to a coordinate-wise energy decrease: the sign of the derivative of each entry is never the same as that of the corresponding entry of the gradient of some convex energy function. We show that this simple condition guarantees convergence to a point, to the minimum of the energy functions, or to a set where its Hessian has very specific properties. This extends and strengthens recent results that were restricted to quadratic energy functions.      
### 68.Language-Agnostic Meta-Learning for Low-Resource Text-to-Speech with Articulatory Features  [ :arrow_down: ](https://arxiv.org/pdf/2203.03191.pdf)
>  While neural text-to-speech systems perform remarkably well in high-resource scenarios, they cannot be applied to the majority of the over 6,000 spoken languages in the world due to a lack of appropriate training data. In this work, we use embeddings derived from articulatory vectors rather than embeddings derived from phoneme identities to learn phoneme representations that hold across languages. In conjunction with language agnostic meta learning, this enables us to fine-tune a high-quality text-to-speech model on just 30 minutes of data in a previously unseen language spoken by a previously unseen speaker.      
### 69.Speaker recognition by means of a combination of linear and nonlinear predictive models  [ :arrow_down: ](https://arxiv.org/pdf/2203.03190.pdf)
>  This paper deals the combination of nonlinear predictive models with classical LPCC parameterization for speaker recognition. It is shown that the combination of both a measure defined over LPCC coefficients and a measure defined over predictive analysis residual signal gives rise to an improvement over the classical method that considers only the LPCC coefficients. If the residual signal is obtained from a linear prediction analysis, the improvement is 2.63% (error rate drops from 6.31% to 3.68%) and if it is computed through a nonlinear predictive neural nets based model, the improvement is 3.68%. An efficient algorithm for reducing the computational burden is also proposed.      
### 70.Reconfigurable Intelligent Surfaces for Wireless Communications: Overview of Hardware Designs, Channel Models, and Estimation Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2203.03176.pdf)
>  The demanding objectives for the future sixth generation (6G) of wireless communication networks have spurred recent research efforts on novel materials and radio-frequency front-end architectures for wireless connectivity, as well as revolutionary communication and computing paradigms. Among the pioneering candidate technologies for 6G belong the reconfigurable intelligent surfaces (RISs), which are artificial planar structures with integrated electronic circuits that can be programmed to manipulate the incoming electromagnetic field in a wide variety of functionalities. Incorporating RISs in wireless networks has been recently advocated as a revolutionary means to transform any wireless signal propagation environment to a dynamically programmable one, intended for various networking objectives, such as coverage extension and capacity boosting, spatiotemporal focusing with benefits in energy efficiency and secrecy, and low electromagnetic field exposure. Motivated by the recent increasing interests in the field of RISs and the consequent pioneering concept of the RIS-enabled smart wireless environments, in this paper, we overview and taxonomize the latest advances in RIS hardware architectures as well as the most recent developments in the modeling of RIS unit elements and RIS-empowered wireless signal propagation. We also present a thorough overview of the channel estimation approaches for RIS-empowered communications systems, which constitute a prerequisite step for the optimized incorporation of RISs in future wireless networks. Finally, we discuss the relevance of the RIS technology in the latest wireless communication standards, and highlight the current and future standardization activities for the RIS technology and the consequent RIS-empowered wireless networking approaches.      
### 71.Viewpoint Leakage in Proactive VR Streaming: Modeling and Tradeoff  [ :arrow_down: ](https://arxiv.org/pdf/2203.03107.pdf)
>  Proactive tile-based virtual reality (VR) video streaming employs the trace of viewpoint of a user to predict future requested tiles, then renders and delivers the predicted tiles before playback. <br>Recently, it has been found that the identity and preference of the user can be inferred from the viewpoint data uploaded in proactive streaming procedure. When considering viewpoint leakage, several fundamental questions arise. When is the viewpoint leaked? Can privacy-preserving approaches, e.g., federated or individual training, or using local predicting and predictors with no need for training avoid viewpoint leakage? In this paper, we investigate viewpoint leakage during proactive streaming. We find that if the prediction error or the quality of experience (QoE) metric is uploaded for adaptive streaming, the real viewpoint can be inferred even with the above privacy-preserving approaches. Then, we define \textit{viewpoint leakage probability} to characterize the inference accuracy of the real viewpoint, and respectively derive the probability when uploading prediction error and the QoE. We find there is a conditional tradeoff between viewpoint leakage probability and prediction performance, QoE, or the resources. <br>Simulation with the state-of-the-art predictor over a real dataset shows that the tradeoff between viewpoint privacy and prediction performance, resources, and QoE exist for most cases.      
### 72.Non-Gaussian Risk Bounded Trajectory Optimization for Stochastic Nonlinear Systems in Uncertain Environments  [ :arrow_down: ](https://arxiv.org/pdf/2203.03038.pdf)
>  We address the risk bounded trajectory optimization problem of stochastic nonlinear robotic systems. More precisely, we consider the motion planning problem in which the robot has stochastic nonlinear dynamics and uncertain initial locations, and the environment contains multiple dynamic uncertain obstacles with arbitrary probabilistic distributions. The goal is to plan a sequence of control inputs for the robot to navigate to the target while bounding the probability of colliding with obstacles. Existing approaches to address risk bounded trajectory optimization problems are limited to particular classes of models and uncertainties such as Gaussian linear problems. In this paper, we deal with stochastic nonlinear models, nonlinear safety constraints, and arbitrary probabilistic uncertainties, the most general setting ever considered. To address the risk bounded trajectory optimization problem, we first formulate the problem as an optimization problem with stochastic dynamics equations and chance constraints. We then convert probabilistic constraints and stochastic dynamics constraints on random variables into a set of deterministic constraints on the moments of state probability distributions. Finally, we solve the resulting deterministic optimization problem using nonlinear optimization solvers and get a sequence of control inputs. To our best knowledge, it is the first time that the motion planning problem to such a general extent is considered and solved. To illustrate the performance of the proposed method, we provide several robotics examples.      
### 73.HEAR 2021: Holistic Evaluation of Audio Representations  [ :arrow_down: ](https://arxiv.org/pdf/2203.03022.pdf)
>  What audio embedding approach generalizes best to a wide range of downstream tasks across a variety of everyday domains without fine-tuning? The aim of the HEAR 2021 NeurIPS challenge is to develop a general-purpose audio representation that provides a strong basis for learning in a wide variety of tasks and scenarios. HEAR 2021 evaluates audio representations using a benchmark suite across a variety of domains, including speech, environmental sound, and music. In the spirit of shared exchange, each participant submitted an audio embedding model following a common API that is general-purpose, open-source, and freely available to use. Twenty-nine models by thirteen external teams were evaluated on nineteen diverse downstream tasks derived from sixteen datasets. Open evaluation code, submitted models and datasets are key contributions, enabling comprehensive and reproducible evaluation, as well as previously impossible longitudinal studies. It still remains an open question whether one single general-purpose audio representation can perform as holistically as the human ear.      
### 74.RAPTOR: Rapid Aerial Pickup and Transport of Objects by Robots  [ :arrow_down: ](https://arxiv.org/pdf/2203.03018.pdf)
>  Rapid aerial grasping promises vast applications that utilize the dynamic picking up and placing of objects by robots. Rigid grippers traditionally used in aerial manipulators require very high precision and specific object geometries for successful grasping. We propose RAPTOR, a quadcopter platform combined with a custom Fin Ray gripper to enable a more flexible grasping of objects with different geometries, leveraging the properties of soft materials to increase the contact surface between the gripper and the objects. To reduce the communication latency, we present a novel FastDDS-based middleware solution as an alternative to ROS (Robot Operating System). We show that RAPTOR achieves an average of 83% grasping efficacy in a real-world setting for four different object geometries while moving at an average velocity of 1 m/s during grasping, which is approximately five times faster than the state-of-the-art while supporting up to four times the payload. Our results further solidify the potential of quadcopters in warehouses and other automated pick-and-place applications over longer distances where speed and robustness become essential.      
### 75.Low-Complexity Beamforming Design for IRS-Aided NOMA Communication System with Imperfect CSI  [ :arrow_down: ](https://arxiv.org/pdf/2203.03004.pdf)
>  Intelligent reflecting surface (IRS) as a promising technology rendering high throughput in future communication systems is compatible with various communication techniques such as non-orthogonal multiple-access (NOMA). In this paper, the downlink transmission of IRS-assisted NOMA communication is considered while undergoing imperfect channel state information (CSI). Consequently, a robust IRS-aided NOMA design is proposed by solving the sum-rate maximization problem to jointly find the optimal beamforming vectors for the access point and the passive reflection matrix for the IRS, using the penalty dual decomposition (PDD) scheme. This problem can be solved through an iterative algorithm, with closed-form solutions in each step, and it is shown to have very close performance to its upper bound obtained from perfect CSI scenario. We also present a trellis-based method for optimal discrete phase shift selection of IRS which is shown to outperform the conventional quantization method. Our results show that the proposed algorithms, for both continuous and discrete IRS, have very low computational complexity compared to other schemes in the literature. Furthermore, we conduct a performance comparison from achievable sum-rate standpoint between IRS-aided NOMA and IRS-aided orthogonal multiple access (OMA), which demonstrates superiority of NOMA compared to OMA in case of a tolerated channel uncertainty.      
### 76.Smoothing with the Best Rectangle Window is Optimal for All Tapered Rectangle Windows  [ :arrow_down: ](https://arxiv.org/pdf/2203.02997.pdf)
>  We investigate the optimal selection of weight windows for the problem of weighted least squares. We show that weight windows should be symmetric around its center, which is also its peak. We consider the class of tapered rectangle window weights, which are nonincreasing away from the center. We show that the best rectangle window is optimal for such window definitions. We also extend our results to the least absolutes and more general case of arbitrary loss functions to find similar results.      
### 77.Variational Auto-Encoder based Mandarin Speech Cloning  [ :arrow_down: ](https://arxiv.org/pdf/2203.02967.pdf)
>  Speech cloning technology is becoming more sophisticated thanks to the advances in machine learning. Researchers have successfully implemented natural-sounding English speech synthesis and good English speech cloning by some effective models. However, because of prosodic phrasing and large character set of Mandarin, Chinese utilization of these models is not yet complete. By creating a new dataset and replacing Tacotron synthesizer with VAENAR-TTS, we improved the existing speech cloning technique CV2TTS to almost real-time speech cloning while guaranteeing synthesis quality. In the process, we customized the subjective tests of synthesis quality assessment by attaching various scenarios, so that subjects focus on the differences between voice and our improvements maybe were more advantageous to practical applications. The results of the A/B test, real-time factor (RTF) and 2.74 mean opinion score (MOS) in terms of naturalness and similarity, reflect the real-time high-quality Mandarin speech cloning we achieved.      
### 78.Precise Point Spread Function Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02953.pdf)
>  Point spread function (PSF) plays a crucial role in many fields, such as shape from focus/defocus, depth estimation, and imaging process in fluorescence microscopy. However, the mathematical model of the defocus process is still unclear because several variables in the point spread function are hard to measure accurately, such as the f-number of cameras, the physical size of a pixel, the focus depth, etc. In this work, we develop a precise mathematical model of the camera's point spread function to describe the defocus process. We first derive the mathematical algorithm for the PSF and extract two parameters A and e. A is the composite of camera's f-number, pixel-size, output scale, and scaling factor of the circle of confusion; e is the deviation of the focus depth. We design a novel metric based on the defocus histogram to evaluate the difference between the simulated focused image and the actual focused image to obtain optimal A and e. We also construct a hardware system consisting of a focusing system and a structured light system to acquire the all-in-focus image, the focused image with corresponding focus depth, and the depth map in the same view. The three types of images, as a dataset, are used to obtain the precise PSF. Our experiments on standard planes and actual objects show that the proposed algorithm can accurately describe the defocus process. The accuracy of our algorithm is further proved by evaluating the difference among the actual focused images, the focused image generated by our algorithm, the focused image generated by others. The results show that the loss of our algorithm is 40% less than others on average. The dataset, code, and model are available on GitHub: <a class="link-external link-https" href="https://github.com/cubhe/" rel="external noopener nofollow">this https URL</a> precise-point-spread-function-estimation.      
### 79.CNN self-attention voice activity detector  [ :arrow_down: ](https://arxiv.org/pdf/2203.02944.pdf)
>  In this work we present a novel single-channel Voice Activity Detector (VAD) approach. We utilize a Convolutional Neural Network (CNN) which exploits the spatial information of the noisy input spectrum to extract frame-wise embedding sequence, followed by a Self Attention (SA) Encoder with a goal of finding contextual information from the embedding sequence. Different from previous works which were employed on each frame (with context frames) separately, our method is capable of processing the entire signal at once, and thus enabling long receptive field. We show that the fusion of CNN and SA architectures outperforms methods based solely on CNN and SA. Extensive experimental-study shows that our model outperforms previous models on real-life benchmarks, and provides State Of The Art (SOTA) results with relatively small and lightweight model.      
### 80.C-P Map: A Novel Evaluation Toolkit for Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2203.02942.pdf)
>  Evaluation trials are used to probe performance of automatic speaker verification (ASV) systems. In spite of the clear importance and impact, evaluation trials have not been seriously treated in research and engineering practice. This paper firstly presents a theoretical analysis on evaluation trials and highlights potential bias with the most popular cross-pairing approach used in trials design. To interpret and settle this problem, we define the concept of trial config and C-P map derived from it. The C-P map measures the performance of an ASV system on various trial configs in a 2-dimensional map. On the map, each location represents a particular trial config and its corresponding color represents the system performance. Experiments conducted on representative ASV systems show that the proposed C-P map offers a powerful evaluation toolkit for ASV performance analysis and comparison. The source code for C-P map has been release at <a class="link-external link-https" href="https://gitlab.com/csltstu/sunine" rel="external noopener nofollow">this https URL</a>.      
### 81.Single microphone speaker extraction using unified time-frequency Siamese-Unet  [ :arrow_down: ](https://arxiv.org/pdf/2203.02941.pdf)
>  In this paper we present a unified time-frequency method for speaker extraction in clean and noisy conditions. Given a mixed signal, along with a reference signal, the common approaches for extracting the desired speaker are either applied in the time-domain or in the frequency-domain. In our approach, we propose a Siamese-Unet architecture that uses both representations. The Siamese encoders are applied in the frequency-domain to infer the embedding of the noisy and reference spectra, respectively. The concatenated representations are then fed into the decoder to estimate the real and imaginary components of the desired speaker, which are then inverse-transformed to the time-domain. The model is trained with the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) loss to exploit the time-domain information. The time-domain loss is also regularized with frequency-domain loss to preserve the speech patterns. Experimental results demonstrate that the unified approach is not only very easy to train, but also provides superior results as compared with state-of-the-art (SOTA) Blind Source Separation (BSS) methods, as well as commonly used speaker extraction approach.      
### 82.Detection of Parasitic Eggs from Microscopy Images and the emergence of a new dataset  [ :arrow_down: ](https://arxiv.org/pdf/2203.02940.pdf)
>  Automatic detection of parasitic eggs in microscopy images has the potential to increase the efficiency of human experts whilst also providing an objective assessment. The time saved by such a process would both help ensure a prompt treatment to patients, and off-load excessive work from experts' shoulders. Advances in deep learning inspired us to exploit successful architectures for detection, adapting them to tackle a different domain. We propose a framework that exploits two such state-of-the-art models. Specifically, we demonstrate results produced by both a Generative Adversarial Network (GAN) and Faster-RCNN, for image enhancement and object detection respectively, on microscopy images of varying quality. The use of these techniques yields encouraging results, though further improvements are still needed for certain egg types whose detection still proves challenging. As a result, a new dataset has been created and made publicly available, providing an even wider range of classes and variability.      
### 83.Optimally scheduling public safety power shutoffs  [ :arrow_down: ](https://arxiv.org/pdf/2203.02861.pdf)
>  In an effort to reduce power system-caused wildfires, utilities carry out public safety power shutoffs (PSPS) in which portions of the grid are de-energized to mitigate the risk of ignition. The decision to call a PSPS must balance reducing ignition risks and the negative impact of service interruptions. In this work, we consider three PSPS scheduling scenarios, which we model as dynamic programs. In the first two scenarios, we assume that N PSPSs are budgeted as part of the investment strategy. In the first scenario, a penalty is incurred for each PSPS declared past the Nth event. In the second, we assume that some costs can be recovered if the number of PSPSs is below $N$ while still being subject to a penalty if above N. In the third, the system operator wants to minimize the number of PSPS such that the total expected cost is below a threshold. We provide optimal or asymptotically optimal closed-form policies for the first two scenarios. Lastly, we apply the first PSPS model to critical-peak pricing, and obtain an optimal scheduling policy to reduce the peak demand based on weather observations. We evaluate the policies in numerical simulations on real data for California, USA and Québec, Canada.      
### 84.Leveraging Reward Gradients For Reinforcement Learning in Differentiable Physics Simulations  [ :arrow_down: ](https://arxiv.org/pdf/2203.02857.pdf)
>  In recent years, fully differentiable rigid body physics simulators have been developed, which can be used to simulate a wide range of robotic systems. In the context of reinforcement learning for control, these simulators theoretically allow algorithms to be applied directly to analytic gradients of the reward function. However, to date, these gradients have proved extremely challenging to use, and are outclassed by algorithms using no gradient information at all. In this work we present a novel algorithm, cross entropy analytic policy gradients, that is able to leverage these gradients to outperform state of art deep reinforcement learning on a set of challenging nonlinear control problems.      
### 85.Variable Selection with the Knockoffs: Composite Null Hypotheses  [ :arrow_down: ](https://arxiv.org/pdf/2203.02849.pdf)
>  The Fixed-X knockoff filter is a flexible framework for variable selection with false discovery rate (FDR) control in linear models with arbitrary (non-singular) design matrices and it allows for finite-sample selective inference via the LASSO estimates. In this paper, we extend the theory of the knockoff procedure to tests with composite null hypotheses, which are usually more relevant to real-world problems. The main technical challenge lies in handling composite nulls in tandem with dependent features from arbitrary designs. We develop two methods for composite inference with the knockoffs, namely, shifted ordinary least-squares (S-OLS) and feature-response product perturbation (FRPP), building on new structural properties of test statistics under composite nulls. We also propose two heuristic variants of the S-OLS method that outperform the celebrated Benjamini-Hochberg (BH) procedure for composite nulls, which serves as a heuristic baseline under dependent test statistics. Finally, we analyze the loss in FDR when the original knockoff procedure is naively applied on composite tests.      
### 86.Machine Learning Applications in Diagnosis, Treatment and Prognosis of Lung Cancer  [ :arrow_down: ](https://arxiv.org/pdf/2203.02794.pdf)
>  The recent development of imaging and sequencing technologies enables systematic advances in the clinical study of lung cancer. Meanwhile, the human mind is limited in effectively handling and fully utilizing the accumulation of such enormous amounts of data. Machine learning-based approaches play a critical role in integrating and analyzing these large and complex datasets, which have extensively characterized lung cancer through the use of different perspectives from these accrued data. In this article, we provide an overview of machine learning-based approaches that strengthen the varying aspects of lung cancer diagnosis and therapy, including early detection, auxiliary diagnosis, prognosis prediction and immunotherapy practice. Moreover, we highlight the challenges and opportunities for future applications of machine learning in lung cancer.      
### 87.Constructing Artificial Traffic Fluids by Designing Cruise Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2203.02788.pdf)
>  In this paper, we apply a Control Lyapunov Function methodology to design two families of cruise controllers for the two-dimensional movement of autonomous vehicles on lane-free roads using the bicycle kinematic model. The control Lyapunov functions are based on measures of the energy of the system with the kinetic energy expressed in ways similar to Newtonian or relativistic mechanics. The derived feedback laws (cruise controllers) are decentralized, as each vehicle determines its control input based on its own speed and on the relative speeds and distances from adjacent vehicles and from the boundary of the road. Moreover, the corresponding macroscopic models are derived, obtaining fluid-like models that consist of a conservation equation and a momentum equation with pressure and viscous terms. Finally, we show that, by selecting appropriately the parameters of the feedback laws, we can determine the physical properties of the "traffic fluid", i.e. we get free hand to create an artificial fluid that approximates the emerging traffic flow.      
### 88.aaeCAPTCHA: The Design and Implementation of Audio Adversarial CAPTCHA  [ :arrow_down: ](https://arxiv.org/pdf/2203.02735.pdf)
>  CAPTCHAs are designed to prevent malicious bot programs from abusing websites. Most online service providers deploy audio CAPTCHAs as an alternative to text and image CAPTCHAs for visually impaired users. However, prior research investigating the security of audio CAPTCHAs found them highly vulnerable to automated attacks using Automatic Speech Recognition (ASR) systems. To improve the robustness of audio CAPTCHAs against automated abuses, we present the design and implementation of an audio adversarial CAPTCHA (aaeCAPTCHA) system in this paper. The aaeCAPTCHA system exploits audio adversarial examples as CAPTCHAs to prevent the ASR systems from automatically solving them. Furthermore, we conducted a rigorous security evaluation of our new audio CAPTCHA design against five state-of-the-art DNN-based ASR systems and three commercial Speech-to-Text (STT) services. Our experimental evaluations demonstrate that aaeCAPTCHA is highly secure against these speech recognition technologies, even when the attacker has complete knowledge of the current attacks against audio adversarial examples. We also conducted a usability evaluation of the proof-of-concept implementation of the aaeCAPTCHA scheme. Our results show that it achieves high robustness at a moderate usability cost compared to normal audio CAPTCHAs. Finally, our extensive analysis highlights that aaeCAPTCHA can significantly enhance the security and robustness of traditional audio CAPTCHA systems while maintaining similar usability.      
### 89.Bayesian Learning Approach to Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2203.02720.pdf)
>  This study presents a Bayesian learning perspective towards model predictive control algorithms. High-level frameworks have been developed separately in the earlier studies on Bayesian learning and sampling-based model predictive control. On one hand, the Bayesian learning rule provides a general framework capable of generating various machine learning algorithms as special instances. On the other hand, the dynamic mirror descent model predictive control framework is capable of diversifying sample-rollout-based control algorithms. However, connections between the two frameworks have still not been fully appreciated in the context of stochastic optimal control. This study combines the Bayesian learning rule point of view into the model predictive control setting by taking inspirations from the view of understanding model predictive controller as an online learner. The selection of posterior class and natural gradient approximation for the variational formulation governs diversification of model predictive control algorithms in the Bayesian learning approach to model predictive control. This alternative viewpoint complements the dynamic mirror descent framework through streamlining the explanation of design choices.      
### 90.NeuralDPS: Neural Deterministic Plus Stochastic Model with Multiband Excitation for Noise-Controllable Waveform Generation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02678.pdf)
>  The traditional vocoders have the advantages of high synthesis efficiency, strong interpretability, and speech editability, while the neural vocoders have the advantage of high synthesis quality. To combine the advantages of two vocoders, inspired by the traditional deterministic plus stochastic model, this paper proposes a novel neural vocoder named NeuralDPS which can retain high speech quality and acquire high synthesis efficiency and noise controllability. Firstly, this framework contains four modules: a deterministic source module, a stochastic source module, a neural V/UV decision module and a neural filter module. The input required by the vocoder is just the spectral parameter, which avoids the error caused by estimating additional parameters, such as F0. Secondly, to solve the problem that different frequency bands may have different proportions of deterministic components and stochastic components, a multiband excitation strategy is used to generate a more accurate excitation signal and reduce the neural filter's burden. Thirdly, a method to control noise components of speech is proposed. In this way, the signal-to-noise ratio (SNR) of speech can be adjusted easily. Objective and subjective experimental results show that our proposed NeuralDPS vocoder can obtain similar performance with the WaveNet and it generates waveforms at least 280 times faster than the WaveNet vocoder. It is also 28% faster than WaveGAN's synthesis efficiency on a single CPU core. We have also verified through experiments that this method can effectively control the noise components in the predicted speech and adjust the SNR of speech. Examples of generated speech can be found at <a class="link-external link-https" href="https://hairuo55.github.io/NeuralDPS" rel="external noopener nofollow">this https URL</a>.      
### 91.Audio-visual speech separation based on joint feature representation with cross-modal attention  [ :arrow_down: ](https://arxiv.org/pdf/2203.02655.pdf)
>  Multi-modal based speech separation has exhibited a specific advantage on isolating the target character in multi-talker noisy environments. Unfortunately, most of current separation strategies prefer a straightforward fusion based on feature learning of each single modality, which is far from sufficient consideration of inter-relationships between modalites. Inspired by learning joint feature representations from audio and visual streams with attention mechanism, in this study, a novel cross-modal fusion strategy is proposed to benefit the whole framework with semantic correlations between different modalities. To further improve audio-visual speech separation, the dense optical flow of lip motion is incorporated to strengthen the robustness of visual representation. The evaluation of the proposed work is performed on two public audio-visual speech separation benchmark datasets. The overall improvement of the performance has demonstrated that the additional motion network effectively enhances the visual representation of the combined lip images and audio signal, as well as outperforming the baseline in terms of all metrics with the proposed cross-modal fusion.      
### 92.SwarmUS: An open hardware and software on-board platform for swarm robotics development  [ :arrow_down: ](https://arxiv.org/pdf/2203.02643.pdf)
>  Real life implementations of distributed swarm robotics are rare. The standardization of a general purpose swarm robotics platform could greatly accelerate swarm robotics towards real life implementations. The SwarmUS platform is an open-source hardware and software on-board embedded system designed to be added onto existing robots while providing them with swarm features, thus proposing a new take on the platform standardization problem. These features include a distributed relative localization system based on Ultra-Wideband, a local communication system based on Wi-Fi and a distributed coordination system based on the Buzz programming language between robots connected within a SwarmUS platform. Additionally, a human-swarm interaction mobile application and an emulation of the platform in the Robot Operating System (ROS) is presented. Finally, an implementation of the system was realized and tested on two types of robots : a TurtleBot3 Burger and two Pioneer 2DX.      
### 93.DEC-LOS-RRT: Decentralized Path Planning for Multi-robot Systems with Line-of-sight Constrained Communication  [ :arrow_down: ](https://arxiv.org/pdf/2203.02609.pdf)
>  Decentralized planning for multi-agent systems, such as fleets of robots in a search-and-rescue operation, is often constrained by limitations on how agents can communicate with each other. One such limitation is the case when agents can communicate with each other only when they are in line-of-sight (LOS). Developing decentralized planning methods that guarantee safety is difficult in this case, as agents that are occluded from each other might not be able to communicate until it's too late to avoid a safety violation. In this paper, we develop a decentralized planning method that explicitly avoids situations where lack of visibility of other agents would lead to an unsafe situation. Building on top of an existing Rapidly-exploring Random Tree (RRT)-based approach, our method guarantees safety at each iteration. Simulation studies show the effectiveness of our method and compare the degradation in performance with respect to a clairvoyant decentralized planning algorithm where agents can communicate despite not being in LOS of each other.      
### 94.A Small Gain Analysis of Single Timescale Actor Critic  [ :arrow_down: ](https://arxiv.org/pdf/2203.02591.pdf)
>  We consider a version of actor-critic which uses proportional step-sizes and only one critic update with a single sample from the stationary distribution per actor step. We provide an analysis of this method using the small gain theorem. Specifically, we prove that this method can be used to find a stationary point, and that the resulting sample complexity improves the state of the art for actor-critic methods from $O \left(\mu^{-4} \epsilon^{-2} \right)$ to $O \left(\mu^{-2} \epsilon^{-2} \right)$ to find an $\epsilon$-approximate stationary point where $\mu$ is the condition number associated with the critic.      
### 95.A Quality Index Metric and Method for Online Self-Assessment of Autonomous Vehicles Sensory Perception  [ :arrow_down: ](https://arxiv.org/pdf/2203.02588.pdf)
>  Perception is critical to autonomous driving safety. Camera-based object detection is one of the most important methods for autonomous vehicle perception. Current camera-based object detection solutions for autonomous driving cannot provide feedback on the detection performance for each frame. We propose an evaluation metric, namely the perception quality index (PQI), to assess the camera-based object detection algorithm performance and provide the perception quality feedback frame by frame. The method of the PQI generation is by combining the fine-grained saliency map intensity with the object detection algorithm's output results. Furthermore, we developed a superpixel-based attention network (SPA-NET) to predict the proposed PQI evaluation metric by using raw image pixels and superpixels as input. The proposed evaluation metric and prediction network are tested on three open-source datasets. The proposed evaluation metric can correctly assess the camera-based perception quality under the autonomous driving environment according to the experiment results. The network regression R-square values determine the comparison among models. It is shown that a Perception Quality Index is useful in self-evaluating a cameras visual scene perception.      
### 96.Bayesian Optimization Meets Hybrid Zero Dynamics: Safe Parameter Learning for Bipedal Locomotion Control  [ :arrow_down: ](https://arxiv.org/pdf/2203.02570.pdf)
>  In this paper, we propose a multi-domain control parameter learning framework that combines Bayesian Optimization (BO) and Hybrid Zero Dynamics (HZD) for locomotion control of bipedal robots. We leverage BO to learn the control parameters used in the HZD-based controller. The learning process is firstly deployed in simulation to optimize different control parameters for a large repertoire of gaits. Next, to tackle the discrepancy between the simulation and the real world, the learning process is applied on the physical robot to learn for corrections to the control parameters learned in simulation while also respecting a safety constraint for gait stability. This method empowers an efficient sim-to-real transition with a small number of samples in the real world, and does not require a valid controller to initialize the training in simulation. Our proposed learning framework is experimentally deployed and validated on a bipedal robot Cassie to perform versatile locomotion skills with improved performance on smoothness of walking gaits and reduction of steady-state tracking errors.      
### 97.UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation  [ :arrow_down: ](https://arxiv.org/pdf/2203.02557.pdf)
>  Image-to-image translation has broad applications in art, design, and scientific simulations. The original CycleGAN model emphasizes one-to-one mapping via a cycle-consistent loss, while more recent works promote one-to-many mapping to boost the diversity of the translated images. With scientific simulation and one-to-one needs in mind, this work examines if equipping CycleGAN with a vision transformer (ViT) and employing advanced generative adversarial network (GAN) training techniques can achieve better performance. The resulting UNet ViT Cycle-consistent GAN (UVCGAN) model is compared with previous best-performing models on open benchmark image-to-image translation datasets, Selfie2Anime and CelebA. UVCGAN performs better and retains a strong correlation between the original and translated images. An accompanying ablation study shows that the gradient penalty and BERT-like pre-training also contribute to the improvement.~To promote reproducibility and open science, the source code, hyperparameter configurations, and pre-trained model will be made available at: <a class="link-external link-https" href="https://github.com/LS4GAN/uvcga" rel="external noopener nofollow">this https URL</a>.      
### 98.Cellular Segmentation and Composition in Routine Histology Images using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.02510.pdf)
>  Identification and quantification of nuclei in colorectal cancer haematoxylin \&amp; eosin (H\&amp;E) stained histology images is crucial to prognosis and patient management. In computational pathology these tasks are referred to as nuclear segmentation, classification and composition and are used to extract meaningful interpretable cytological and architectural features for downstream analysis. The CoNIC challenge poses the task of automated nuclei segmentation, classification and composition into six different types of nuclei from the largest publicly known nuclei dataset - Lizard. In this regard, we have developed pipelines for the prediction of nuclei segmentation using HoVer-Net and ALBRT for cellular composition. On testing on the preliminary test set, HoVer-Net achieved a PQ of 0.58, a PQ+ of 0.58 and finally a mPQ+ of 0.35. For the prediction of cellular composition with ALBRT on the preliminary test set, we achieved an overall $R^2$ score of 0.53, consisting of 0.84 for lymphocytes, 0.70 for epithelial cells, 0.70 for plasma and .060 for eosinophils.      
### 99.Finite-time Correlations Boost Large Voltage-Angle Fluctuations in Electric Power Grids  [ :arrow_down: ](https://arxiv.org/pdf/2203.00590.pdf)
>  Decarbonization in the energy sector has been accompanied by an increased penetration of new renewable energy sources in electric power systems. Such sources differ from traditional productions in that, first, they induce larger, undispatchable fluctuations in power generation and second, they lack inertia. Therefore, substituting new renewables for traditional generation induces stronger and more frequent disturbances and modifies the way disturbances propagate across AC electric power grids. Recent measurements have indeed reported long, non-Gaussian tails in the distribution of local grid-frequency data. Large frequency deviations may induce grid instabilities, leading in worst-case scenarios to cascading failures and large-scale blackouts. In this manuscript, we investigate how correlated noise disturbances, characterized by the cumulants of their distribution, propagate through meshed, high-voltage power grids. We show that for a single source of fluctuations, non-Gaussianities in the form of finite skewness and positive kurtosis of the noise distribution propagate over the entire network when the noise correlation time is larger than the network's intrinsic time scales, but that they vanish over short distances if the noise fluctuates rapidly. We furthermore show that a Berry-Esseen theorem leads to the vanishing of non-Gaussianities as the number of uncorrelated noise sources increases. Our results show that the persistence of non-Gaussian fluctuations of feed-in power have a global impact on power-grid dynamics when they fluctuate over time scales larger than the intrinsic time scales of the system, which, we argue, is the relevant regime in real power grids. Our predictions are corroborated by numerical simulations on realistic models of power grids.      
