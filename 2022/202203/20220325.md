# ArXiv eess --Fri, 25 Mar 2022
### 1.Bidding and Scheduling in Energy Markets: Which Probabilistic Forecast Do We Need?  [ :arrow_down: ](https://arxiv.org/pdf/2203.13159.pdf)
>  Probabilistic forecasting in combination with stochastic programming is a key tool for handling the growing uncertainties in future energy systems. Derived from a general stochastic programming formulation for the optimal scheduling and bidding in energy markets we examine several common special instances containing uncertain loads, energy prices, and variable renewable energies. We analyze for each setup whether only an expected value forecast, marginal or bivariate predictive distributions, or the full joint predictive distribution is required. For market schedule optimization, we find that expected price forecasts are sufficient in almost all cases, while the marginal distributions of renewable energy production and demand are often required. For bidding curve optimization, pairwise or full joint distributions are necessary except for specific cases. This work helps practitioners choose the simplest type of forecast that can still achieve the best theoretically possible result for their problem and researchers to focus on the most relevant instances.      
### 2.Tuning rules for passivity-based integral control for a class of mechanical systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.13157.pdf)
>  This manuscript introduces a passivity-based integral control approach for fully-actuated mechanical systems. The novelty of our methodology is that we exploit the gyroscopic forces of the mechanical systems to exponentially stabilize the mechanical system at the desired equilibrium even in the presence of matched disturbances; additionally, we show that our approach is robust against unmatched disturbances. Furthermore, we provide tuning rules to prescribe the performance of the closed-loop system. We conclude this manuscript with experimental results obtained from a robotic arm.      
### 3.Self-Triggered Coordination Control of Connected Automated Vehicles in Traffic Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.13147.pdf)
>  In this paper, a self-triggered scheme is proposed to optimally control the traffic flow of Connected and Automated Vehicles (CAVs) at conflict areas of a traffic network with the main aim of reducing the data exchange among CAVs in the control zone and at the same to minimize the travel time and energy consumption. The safety constraints and the vehicle limitations are considered using the Control Barrier Function (CBF) framework and a self-triggered scheme is proposed using the CBF constraints. Moreover, modified CBF constraints are developed to ensure a minimum inter-event interval for the proposed self-triggered schemes. Finally, it is shown through a simulation study that the number of data exchanges among CAVs is significantly reduced using the proposed self-triggered schemes in comparison with the standard time-triggered framework.      
### 4.Mobile Wireless Rechargeable UAV Networks: Challenges and Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2203.13139.pdf)
>  Unmanned aerial vehicles (UAVs) can help facilitate cost-effective and flexible service provisioning in future smart cities. Nevertheless, UAV applications generally suffer severe flight time limitations due to constrained onboard battery capacity, causing a necessity of frequent battery recharging or replacement when performing persistent missions. Utilizing wireless mobile chargers, such as vehicles with wireless charging equipment for on-demand self-recharging has been envisioned as a promising solution to address this issue. In this article, we present a comprehensive study of \underline{v}ehicle-assisted \underline{w}ireless rechargeable \underline{U}AV \underline{n}etworks (VWUNs) to promote on-demand, secure, and efficient UAV recharging services. Specifically, we first discuss the opportunities and challenges of deploying VWUNs and review state-of-the-art solutions in this field. We then propose a secure and privacy-preserving VWUN framework for UAVs and ground vehicles based on differential privacy (DP). Within this framework, an online double auction mechanism is developed for optimal charging scheduling, and a two-phase DP algorithm is devised to preserve the sensitive bidding and energy trading information of participants. Experimental results demonstrate that the proposed framework can effectively enhance charging efficiency and security. Finally, we outline promising directions for future research in this emerging field.      
### 5.A New Virtual Oscillator based Grid-forming Controller with Decoupled Control Over Individual Phases and Improved Performance of Unbalanced Fault Ride-through  [ :arrow_down: ](https://arxiv.org/pdf/2203.13136.pdf)
>  Virtual Oscillator (VO) control is the latest and promising control technique for grid-forming and grid-supporting inverters. VO Controllers (VOCs) provide time-domain synchronization with a connected electrical network. At the same time, a VOC can incorporate additional nested control loops to meet the system-level requirements such as fault ride-through capability. However, existing VOCs have two limitations. Firstly, the existing VOCs do not have decoupled control over individual phases. As a result, the performance of the existing VOCs is not satisfactory in presence of unbalanced grid voltages. Secondly, the fault ride-through performance of the existing VOCs under unbalanced faults is not satisfactory. The power operation at a healthy phase is badly affected by a faulty phase. This paper has introduced a modified VO based system-level grid-forming controller to overcome the limitations mentioned above. Systematic development of the proposed control architecture with analytical reasoning is presented. Simulation studies and hardware experiments are conducted for validation.      
### 6.Characterizing Therapist's Speaking Style in Relation to Empathy in Psychotherapy  [ :arrow_down: ](https://arxiv.org/pdf/2203.13127.pdf)
>  In conversation-based psychotherapy, therapists use verbal techniques to help clients express thoughts and feelings and change behaviors. In particular, how well therapists convey empathy is an essential quality index of psychotherapy sessions and is associated with psychotherapy outcome. In this paper, we analyze the prosody of therapist speech and attempt to associate the therapist's speaking style with subjectively perceived empathy. An automatic speech and text processing system is developed to segment long recordings of psychotherapy sessions into pause-delimited utterances with text transcriptions. Data-driven clustering is applied to the utterances from different therapists in multiple sessions. For each cluster, a typological representation of utterance genre is derived based on quantized prosodic feature parameters. Prominent speaking styles of the therapist can be observed and interpreted from salient utterance genres that are correlated with empathy. Using the salient utterance genres, an accuracy of 71% is achieved in classifying psychotherapy sessions into "high" and "low" empathy level. Analysis of results suggests that empathy level tends to be (1) low if therapists speak long utterances slowly or speak short utterances quickly; and (2) high if therapists talk to clients with a steady tone and volume.      
### 7.X-ray Dissectography Improves Lung Nodule Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.13118.pdf)
>  Although radiographs are the most frequently used worldwide due to their cost-effectiveness and widespread accessibility, the structural superposition along the x-ray paths often renders suspicious or concerning lung nodules difficult to detect. In this study, we apply "X-ray dissectography" to dissect lungs digitally from a few radiographic projections, suppress the interference of irrelevant structures, and improve lung nodule detectability. For this purpose, a collaborative detection network is designed to localize lung nodules in 2D dissected projections and 3D physical space. Our experimental results show that our approach can significantly improve the average precision by 20+% in comparison with the common baseline that detects lung nodules from original projections using a popular detection network. Potentially, this approach could help re-design the current X-ray imaging protocols and workflows and improve the diagnostic performance of chest radiographs in lung diseases.      
### 8.Position Tracking using Likelihood Modeling of Channel Features with Gaussian Processes  [ :arrow_down: ](https://arxiv.org/pdf/2203.13110.pdf)
>  Recent localization frameworks exploit spatial information of complex channel measurements (CMs) to estimate accurate positions even in multipath propagation scenarios. State-of-the art CM fingerprinting(FP)-based methods employ convolutional neural networks (CNN) to extract the spatial information. However, they need spatially dense data sets (associated with high acquisition and maintenance efforts) to work well -- which is rarely the case in practical applications. If such data is not available (or its quality is low), we cannot compensate the performance degradation of CNN-based FP as they do not provide statistical position estimates, which prevents a fusion with other sources of information on the observation level. <br>We propose a novel localization framework that adapts well to sparse datasets that only contain CMs of specific areas within the environment with strong multipath propagation. Our framework compresses CMs into informative features to unravel spatial information. It then regresses Gaussian processes (GPs) for each of them, which imply statistical observation models based on distance-dependent covariance kernels. Our framework combines the trained GPs with line-of-sight ranges and a dynamics model in a particle filter. Our measurements show that our approach outperforms state-of-the-art CNN fingerprinting (0.52 m vs. 1.3 m MAE) on spatially sparse data collected in a realistic industrial indoor environment.      
### 9.Interpretable Prediction of Pulmonary Hypertension in Newborns using Echocardiograms  [ :arrow_down: ](https://arxiv.org/pdf/2203.13038.pdf)
>  Pulmonary hypertension (PH) in newborns and infants is a complex condition associated with several pulmonary, cardiac, and systemic diseases contributing to morbidity and mortality. Therefore, accurate and early detection of PH is crucial for successful management. Using echocardiography, the primary diagnostic tool in pediatrics, human assessment is both time-consuming and expertise-demanding, raising the need for an automated approach. In this work, we present an interpretable multi-view video-based deep learning approach to predict PH for a cohort of 194 newborns using echocardiograms. We use spatio-temporal convolutional architectures for the prediction of PH from each view, and aggregate the predictions of the different views using majority voting. To the best of our knowledge, this is the first work for an automated assessment of PH in newborns using echocardiograms. Our results show a mean F1-score of 0.84 for severity prediction and 0.92 for binary detection using 10-fold cross-validation. We complement our predictions with saliency maps and show that the learned model focuses on clinically relevant cardiac structures, motivating its usage in clinical practice.      
### 10.6G Wireless Communications: From Far-field Beam Steering to Near-field Beam Focusing  [ :arrow_down: ](https://arxiv.org/pdf/2203.13035.pdf)
>  6G networks will be required to support higher data rates, improved energy efficiency, lower latency, and more diverse users compared with 5G systems. To meet these requirements, extremely large antenna arrays and high-frequency signaling are envisioned to be key physical-layer technologies. The deployment of extremely large antenna arrays, especially in high-frequency bands, indicates that future 6G wireless networks are likely to operate in the radiating near-field (Fresnel) region, as opposed to the traditional far-field operation of current wireless technologies. In this article, we discuss the opportunities and challenges that arise in radiating near-field communications. We begin by discussing the key physical characteristics of near-field communications, where the standard plane-wave propagation assumption no longer holds, and clarify its implication on the modelling of wireless channels. Then, we elaborate on the ability to leverage spherical wavefronts via beam focusing, highlighting its advantages for 6G systems. We point out several appealing application scenarios which, with proper design, can benefit from near-field operation, including interference mitigation in multi-user communications, accurate localization and focused sensing, as well as wireless power transfer with minimal energy pollution. We conclude with discussing some of the design challenges and research directions that are yet to be explored to fully harness the potential of this emerging paradigm.      
### 11.Bioformers: Embedding Transformers for Ultra-Low Power sEMG-based Gesture Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2203.12932.pdf)
>  Human-machine interaction is gaining traction in rehabilitation tasks, such as controlling prosthetic hands or robotic arms. Gesture recognition exploiting surface electromyographic (sEMG) signals is one of the most promising approaches, given that sEMG signal acquisition is non-invasive and is directly related to muscle contraction. However, the analysis of these signals still presents many challenges since similar gestures result in similar muscle contractions. Thus the resulting signal shapes are almost identical, leading to low classification accuracy. To tackle this challenge, complex neural networks are employed, which require large memory footprints, consume relatively high energy and limit the maximum battery life of devices used for classification. This work addresses this problem with the introduction of the Bioformers. This new family of ultra-small attention-based architectures approaches state-of-the-art performance while reducing the number of parameters and operations of 4.9X. Additionally, by introducing a new inter-subjects pre-training, we improve the accuracy of our best Bioformer by 3.39%, matching state-of-the-art accuracy without any additional inference cost. Deploying our best performing Bioformer on a Parallel, Ultra-Low Power (PULP) microcontroller unit (MCU), the GreenWaves GAP8, we achieve an inference latency and energy of 2.72 ms and 0.14 mJ, respectively, 8.0X lower than the previous state-of-the-art neural network, while occupying just 94.2 kB of memory.      
### 12.SSGCNet: A Sparse Spectra Graph Convolutional Network for Epileptic EEG Signal Classification  [ :arrow_down: ](https://arxiv.org/pdf/2203.12910.pdf)
>  In this article, we propose a sparse spectra graph convolutional network (SSGCNet) for solving Epileptic EEG signal classification problems. The aim is to achieve a lightweight deep learning model without losing model classification accuracy. We propose a weighted neighborhood field graph (WNFG) to represent EEG signals, which reduces the redundant edges between graph nodes. WNFG has lower time complexity and memory usage than the conventional solutions. Using the graph representation, the sequential graph convolutional network is based on a combination of sparse weight pruning technique and the alternating direction method of multipliers (ADMM). Our approach can reduce computation complexity without effect on classification accuracy. We also present convergence results for the proposed approach. The performance of the approach is illustrated in public and clinical-real datasets. Compared with the existing literature, our WNFG of EEG signals achieves up to 10 times of redundant edge reduction, and our approach achieves up to 97 times of model pruning without loss of classification accuracy.      
### 13.Two-timescale Resource Allocation for Automated Networks in IIoT  [ :arrow_down: ](https://arxiv.org/pdf/2203.12900.pdf)
>  The rapid technological advances of cellular technologies will revolutionize network automation in industrial internet of things (IIoT). In this paper, we investigate the two-timescale resource allocation problem in IIoT networks with hybrid energy supply, where temporal variations of energy harvesting (EH), electricity price, channel state, and data arrival exhibit different granularity. The formulated problem consists of energy management at a large timescale, as well as rate control, channel selection, and power allocation at a small timescale. To address this challenge, we develop an online solution to guarantee bounded performance deviation with only causal information. Specifically, Lyapunov optimization is leveraged to transform the long-term stochastic optimization problem into a series of short-term deterministic optimization problems. Then, a low-complexity rate control algorithm is developed based on alternating direction method of multipliers (ADMM), which accelerates the convergence speed via the decomposition-coordination approach. Next, the joint channel selection and power allocation problem is transformed into a one-to-many matching problem, and solved by the proposed price-based matching with quota restriction. Finally, the proposed algorithm is verified through simulations under various system configurations.      
### 14.RSSI-CSI Measurement and Variation Mitigation with Commodity WiFi Device  [ :arrow_down: ](https://arxiv.org/pdf/2203.12888.pdf)
>  Owing to the plentiful information released by the commodity devices, WiFi signals have been widely studied for various wireless sensing applications. In many works, both received signal strength indicator (RSSI) and the channel state information (CSI) are utilized as the key factors for precise sensing. However, the calculation and relationship between RSSI and CSI is not explained in detail. Furthermore, there are few works focusing on the measurement variation of the WiFi signal which impacts the sensing results. In this paper, the relationship between RSSI and CSI is studied in detail and the measurement variation of amplitude and phase information is investigated by extensive experiments. In the experiments, transmitter and receiver are directly connected by power divider and RF cables and the signal transmission is quantitatively controlled by RF attenuators. By changing the intensity of attenuation, the measurement of RSSI and CSI is carried out under different conditions. From the results, it is found that in order to get a reliable measurement of the signal amplitude and phase by commodity WiFi, the attenuation of the channels should not exceed 60 dB. Meanwhile, the difference between two channels should be lower than 10 dB. An active control mechanism is suggested to ensure the measurement stability. The findings and criteria of this work is promising to facilitate more precise sensing technologies with WiFi signal.      
### 15.Kullback-Leibler control for discrete-time nonlinear systems on continuous spaces  [ :arrow_down: ](https://arxiv.org/pdf/2203.12864.pdf)
>  Kullback-Leibler (KL) control enables efficient numerical methods for nonlinear optimal control problems. The crucial assumption of KL control is the full controllability of the transition distribution. However, this assumption is often violated when the dynamics evolves in a continuous space. Consequently, applying KL control to problems with continuous spaces requires some approximation, which leads to the lost of the optimality. To avoid such approximation, in this paper, we reformulate the KL control problem for continuous spaces so that it does not require unrealistic assumptions. The key difference between the original and reformulated KL control is that the former measures the control effort by KL divergence between controlled and uncontrolled transition distributions while the latter replaces the uncontrolled transition by a noise-driven transition. We show that the reformulated KL control admits efficient numerical algorithms like the original one without unreasonable assumptions. Specifically, the associated value function can be computed by using a Monte Carlo method based on its path integral representation.      
### 16.Local Measurement Based Robust Voltage Stability Index &amp; Identification of Voltage Collapse Onset  [ :arrow_down: ](https://arxiv.org/pdf/2203.12857.pdf)
>  This paper addresses the problem of real-time monitoring of long-term voltage instability (LTVI) by using local field measurements. Existing local measurement-based methods use Thevenin equivalent parameter estimation that is sensitive to the noise in measurements. For solving this issue, we avoid the Thevenin approach by projecting the power flow equations as circles to develop local static-voltage stability indicator (LS-VSI) that is robust to measurement noises. The proposed method is an attractive option for practical implementation because of its decentralized nature, robustness to noise, and realistic modeling. Next, we utilize LS-VSI to propose a new local dynamic - VSI (LD-VSI) to identify the LTVI triggered by large disturbances and load tap changer dynamics. LD-VSI can identify not only the onset of instability with an alarm but also the voltage collapse point (VCP). Extensive numerical validation comparing LS-VSI with existing methods is presented on IEEE 30-bus and larger test systems like 2000-bus Texas synthetic grid to validate the robustness, accuracy, and situational awareness feature. We also verify the LD-VSI behavior on the Nordic power grid using PSSE dynamic simulation. When compared to existing methods, we observe that LD-VSI is not only more robust to measurement noise but also can identify VCP.      
### 17.Disturbance Observer-based Robust Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2203.12855.pdf)
>  This paper develops a novel safe control design framework that integrates the disturbance observer (DOB) and the control barrier function (CBF). Different from previous robust CBF results that are based on the worst-case of disturbances, this work utilizes a DOB to estimate the disturbances and construct a CBF-based safe controller to compensate for the disturbances such that the control performance can be significantly improved. The CBF-based safe controllers are obtained by solving convex quadratic programs (QPs) online. Application of the proposed control design method to Euler-Lagrange systems is also presented. The effectiveness of the proposed method is illustrated via numerical simulations.      
### 18.On the Achievable SINR in MU-MIMO Systems Operating in Time-Varying Rayleigh Fading  [ :arrow_down: ](https://arxiv.org/pdf/2203.12846.pdf)
>  Minimizing the symbol error in the uplink of multi-user multiple input multiple output systems is important, because the symbol error affects the achieved signal-to-interference-plus-noise ratio (SINR) and thereby the spectral efficiency of the system. Despite the vast literature available on minimum mean squared error (MMSE) receivers, previously proposed receivers for block fading channels do not minimize the symbol error in time-varying Rayleigh fading channels. Specifically, we show that the true MMSE receiver structure does not only depend on the statistics of the CSI error, but also on the autocorrelation coefficient of the time-variant channel. It turns out that calculating the average SINR when using the proposed receiver is highly non-trivial. In this paper, we employ a random matrix theoretical approach, which allows us to derive a quasi-closed form for the average SINR, which allows to obtain analytical exact results that give valuable insights into how the SINR depends on the number of antennas, employed pilot and data power and the covariance of the time-varying channel. We benchmark the performance of the proposed receiver against recently proposed receivers and find that the proposed MMSE receiver achieves higher SINR than the previously proposed ones, and this benefit increases with increasing autoregressive coefficient.      
### 19.When Accuracy Meets Privacy: Two-Stage Federated Transfer Learning Framework in Classification of Medical Images on Limited Data: A COVID-19 Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2203.12803.pdf)
>  COVID-19 pandemic has spread rapidly and caused a shortage of global medical resources. The efficiency of COVID-19 diagnosis has become highly significant. As deep learning and convolutional neural network (CNN) has been widely utilized and been verified in analyzing medical images, it has become a powerful tool for computer-assisted diagnosis. However, there are two most significant challenges in medical image classification with the help of deep learning and neural networks, one of them is the difficulty of acquiring enough samples, which may lead to model overfitting. Privacy concerns mainly bring the other challenge since medical-related records are often deemed patients' private information and protected by laws such as GDPR and HIPPA. Federated learning can ensure the model training is decentralized on different devices and no data is shared among them, which guarantees privacy. However, with data located on different devices, the accessible data of each device could be limited. Since transfer learning has been verified in dealing with limited data with good performance, therefore, in this paper, We made a trial to implement federated learning and transfer learning techniques using CNNs to classify COVID-19 using lung CT scans. We also explored the impact of dataset distribution at the client-side in federated learning and the number of training epochs a model is trained. Finally, we obtained very high performance with federated learning, demonstrating our success in leveraging accuracy and privacy.      
### 20.Energy-Efficient UAV-Mounted RIS Assisted Mobile Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2203.12799.pdf)
>  Unmanned aerial vehicle (UAV) and reconfigurable intelligent surface (RIS) have been recently applied in the field of mobile edge computing (MEC) to improve the data exchange environment by proactively changing the wireless channels through maneuverable location deployment and intelligent signals reflection, respectively. Nevertheless, they may suffer from inherent limitations in practical scenarios. UAV-mounted RIS (U-RIS), as a promising integrated approach, can combine the advantages of UAV and RIS to break the limit. Inspired by this, we consider a novel U-RIS assisted MEC system, where a U-RIS is deployed to assist the communication between the ground users and an MEC server. The joint UAV trajectory, RIS passive beamforming and MEC resource allocation design is developed to maximize the energy efficiency (EE) of the system. To tackle the intractable non-convex problem, we divide it into two subproblems and solve them iteratively based on successive convex approximation (SCA) and the Dinkelbach method. Finally we obtain a high-performance suboptimal solution. Simulation results show that the proposed algorithm significantly improves the energy efficiency of the MEC system.      
### 21.Learning the Dynamics of Autonomous Linear Systems From Multiple Trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2203.12794.pdf)
>  We consider the problem of learning the dynamics of autonomous linear systems (i.e., systems that are not affected by external control inputs) from observations of multiple trajectories of those systems, with finite sample guarantees. Existing results on learning rate and consistency of autonomous linear system identification rely on observations of steady state behaviors from a single long trajectory, and are not applicable to unstable systems. In contrast, we consider the scenario of learning system dynamics based on multiple short trajectories, where there are no easily observed steady state behaviors. We provide a finite sample analysis, which shows that the dynamics can be learned at a rate $\mathcal{O}(\frac{1}{\sqrt{N}})$ for both stable and unstable systems, where $N$ is the number of trajectories, when the initial state of the system has zero mean (which is a common assumption in the existing literature). We further generalize our result to the case where the initial state has non-zero mean. We show that one can adjust the length of the trajectories to achieve a learning rate of $\mathcal{O}(\sqrt{\frac{\log{N}}{N})}$ for strictly stable systems and a learning rate of $\mathcal{O}(\frac{(\log{N})^d}{\sqrt{N}})$ for marginally stable systems, where $d$ is some constant.      
### 22.Implementation of an Internet of Things System for Smart Hospitals  [ :arrow_down: ](https://arxiv.org/pdf/2203.12787.pdf)
>  With the rapid development of smart devices and the Internet of Things (IoT) technology, some traditional scenarios are exploring new possibilities. Especially in the field of healthcare, the mixed and large amount of people, the complex and professional data, and the strict environmental requirements on some medical scenes and equipment, all put forward extremely high requirements on the hospital management. Therefore, an efficient and secure Internet of things system is greatly necessary. This paper develops an IoT system that could be deployed in hospitals for various applications. This system supports LoRa, Wi-Fi and other data collection methods, uploads the data to the cloud platform for further processing through secure connection, and finally feeds back to users in real-time through the user interface. This system supports accurate indoor positioning based on UWB, ECG signal detection, environmental monitoring, people flow statistics and other functions.      
### 23.Kernel Robust Hypothesis Testing  [ :arrow_down: ](https://arxiv.org/pdf/2203.12777.pdf)
>  The problem of robust hypothesis testing is studied, where under the null and the alternative hypotheses, the data-generating distributions are assumed to be in some uncertainty sets, and the goal is to design a test that performs well under the worst-case distributions over the uncertainty sets. In this paper, uncertainty sets are constructed in a data-driven manner using kernel method, i.e., they are centered around empirical distributions of training samples from the null and alternative hypotheses, respectively; and are constrained via the distance between kernel mean embeddings of distributions in the reproducing kernel Hilbert space, i.e., maximum mean discrepancy (MMD). The Bayesian setting and the Neyman-Pearson setting are investigated. For the Bayesian setting where the goal is to minimize the worst-case error probability, an optimal test is firstly obtained when the alphabet is finite. When the alphabet is infinite, a tractable approximation is proposed to quantify the worst-case average error probability, and a kernel smoothing method is further applied to design test that generalizes to unseen samples. A direct robust kernel test is also proposed and proved to be exponentially consistent. For the Neyman-Pearson setting, where the goal is to minimize the worst-case probability of miss detection subject to a constraint on the worst-case probability of false alarm, an efficient robust kernel test is proposed and is shown to be asymptotically optimal. Numerical results are provided to demonstrate the performance of the proposed robust tests.      
### 24.Predicting Multi-Antenna Frequency-Selective Channels via Meta-Learned Linear Filters based on Long-Short Term Channel Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2203.12715.pdf)
>  An efficient data-driven prediction strategy for multi-antenna frequency-selective channels must operate based on a small number of pilot symbols. This paper proposes novel channel prediction algorithms that address this goal by integrating transfer and meta-learning with a reduced-rank parametrization of the channel. The proposed methods optimize linear predictors by utilizing data from previous frames, which are generally characterized by distinct propagation characteristics, in order to enable fast training on the time slots of the current frame. The proposed predictors rely on a novel long-short-term decomposition (LSTD) of the linear prediction model that leverages the disaggregation of the channel into long-term space-time signatures and fading amplitudes. We first develop predictors for single-antenna frequency-flat channels based on transfer/meta-learned quadratic regularization. Then, we introduce transfer and meta-learning algorithms for LSTD-based prediction models that build on equilibrium propagation (EP) and alternating least squares (ALS). Numerical results under the 3GPP 5G standard channel model demonstrate the impact of transfer and meta-learning on reducing the number of pilots for channel prediction, as well as the merits of the proposed LSTD parametrization.      
### 25.High Gain Array Antenna With FSS for Vital Sign Monitoring Through the Wall  [ :arrow_down: ](https://arxiv.org/pdf/2203.12682.pdf)
>  In this research, a 2.4 GHz planar array antenna 2X2 combined with frequency selective surface (FSS) is proposed to obtain a high gain and high efficiency for improving the performance of vital sign monitoring through the wall using Self-injection-locked radar systems. The FSS is designed on a double-layer with unit cells of 9X7 for each layer. Modified Jerusalem cross-shaped of FSS is proposed to enhance the gain. The antenna array and additional FSS can obtain a gain of 15.2 dB and 82% of efficiency. the vital sign monitoring of the subject behind the wall has been successfully carried out.      
### 26.Computed Tomography Reconstruction using Generative Energy-Based Priors  [ :arrow_down: ](https://arxiv.org/pdf/2203.12658.pdf)
>  In the past decades, Computed Tomography (CT) has established itself as one of the most important imaging techniques in medicine. Today, the applicability of CT is only limited by the deposited radiation dose, reduction of which manifests in noisy or incomplete measurements. Thus, the need for robust reconstruction algorithms arises. In this work, we learn a parametric regularizer with a global receptive field by maximizing it's likelihood on reference CT data. Due to this unsupervised learning strategy, our trained regularizer truly represents higher-level domain statistics, which we empirically demonstrate by synthesizing CT images. Moreover, this regularizer can easily be applied to different CT reconstruction problems by embedding it in a variational framework, which increases flexibility and interpretability compared to feed-forward learning-based approaches. In addition, the accompanying probabilistic perspective enables experts to explore the full posterior distribution and may quantify uncertainty of the reconstruction approach. We apply the regularizer to limited-angle and few-view CT reconstruction problems, where it outperforms traditional reconstruction algorithms by a large margin.      
### 27.MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion  [ :arrow_down: ](https://arxiv.org/pdf/2203.12621.pdf)
>  Patient scans from MRI often suffer from noise, which hampers the diagnostic capability of such images. As a method to mitigate such artifact, denoising is largely studied both within the medical imaging community and beyond the community as a general subject. However, recent deep neural network-based approaches mostly rely on the minimum mean squared error (MMSE) estimates, which tend to produce a blurred output. Moreover, such models suffer when deployed in real-world sitautions: out-of-distribution data, and complex noise distributions that deviate from the usual parametric noise models. In this work, we propose a new denoising method based on score-based reverse diffusion sampling, which overcomes all the aforementioned drawbacks. Our network, trained only with coronal knee scans, excels even on out-of-distribution in vivo liver MRI data, contaminated with complex mixture of noise. Even more, we propose a method to enhance the resolution of the denoised image with the same network. With extensive experiments, we show that our method establishes state-of-the-art performance, while having desirable properties which prior MMSE denoisers did not have: flexibly choosing the extent of denoising, and quantifying uncertainty.      
### 28.Evaluation of Non-Invasive Thermal Imaging for detection of Viability of Onchocerciasis worms  [ :arrow_down: ](https://arxiv.org/pdf/2203.12620.pdf)
>  Onchocerciasis is causing blindness in over half a million people in the world today. Drug development for the disease is crippled as there is no way of measuring effectiveness of the drug without an invasive procedure. Drug efficacy measurement through assessment of viability of onchocerca worms requires the patients to undergo nodulectomy which is invasive, expensive, time-consuming, skill-dependent, infrastructure dependent and lengthy process. In this paper, we discuss the first-ever study that proposes use of machine learning over thermal imaging to non-invasively and accurately predict the viability of worms. The key contributions of the paper are (i) a unique thermal imaging protocol along with pre-processing steps such as alignment, registration and segmentation to extract interpretable features (ii) extraction of relevant semantic features (iii) development of accurate classifiers for detecting the existence of viable worms in a nodule. When tested on a prospective test data of 30 participants with 48 palpable nodules, we achieved an Area Under the Curve (AUC) of 0.85.      
### 29.Complex Frequency Domain Linear Prediction: A Tool to Compute Modulation Spectrum of Speech  [ :arrow_down: ](https://arxiv.org/pdf/2203.13216.pdf)
>  Conventional Frequency Domain Linear Prediction (FDLP) technique models the squared Hilbert envelope of speech with varied degrees of approximation which can be sampled at the required frame rate and used as features for automatic speech recognition (ASR). Although previously the complex cepstrum of the conventional FDLP model has been used as compact frame-wise speech features, it has lacked interpretability in the context of the Hilbert envelope. In this paper, we propose a modification of the conventional FDLP model that allows easy interpretability of the complex cepstrum as temporal modulations in an all-pole model approximation to the power of the speech signal. Additionally, our "complex" FDLP yields significant speed-ups in comparison to conventional FDLP for the same degree of approximation.      
### 30.Physics-based Learning of Parameterized Thermodynamics from Real-time Thermography  [ :arrow_down: ](https://arxiv.org/pdf/2203.13148.pdf)
>  Progress in automatic control of thermal processes has long been limited by the difficulty of obtaining high-fidelity thermodynamic models. Traditionally, in complex thermodynamic systems, it is often infeasible to estimate the thermophysical parameters of spatiotemporally varying processes, forcing the adoption of model-free control architectures. This comes at the cost of losing any robustness guarantees, and implies a need for extensive real-life testing. In recent years, however, infrared cameras and other thermographic equipment have become readily applicable to these processes, allowing for a real-time, non-invasive means of sensing the thermal state of a process. In this work, we present a novel physics-based approach to learning a thermal process's dynamics directly from such real-time thermographic data, while focusing attention on regions with high thermal activity. We call this process, which applies to any higher-dimensional scalar field, attention-based noise robust averaging (ANRA). Given a partial-differential equation model structure, we show that our approach is robust against noise, and can be used to initialize optimization routines to further refine parameter estimates. We demonstrate our method on several simulation examples, as well as by applying it to electrosurgical thermal response data on in vivo porcine skin tissue.      
### 31.On Optimizing the Power Allocation and the Decoding Order in Uplink Cooperative NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2203.13100.pdf)
>  In this paper, we investigate for the first time the dynamic power allocation and decoding order at the base station (BS) of two-user uplink (UL) cooperative non-orthogonal multiple access (C-NOMA)-based cellular networks. In doing so, we formulate a joint optimization problem aiming at maximizing the minimum user achievable rate, which is a non-convex optimization problem and hard to be directly solved. To tackle this issue, an iterative algorithm based on successive convex approximation (SCA) is proposed. The numerical results reveal that the proposed scheme provides superior performance in comparison with the traditional UL NOMA. In addition, we demonstrated that in UL C-NOMA, decoding the far NOMA user first at the BS provides the best performance.      
### 32.HiFi++: a Unified Framework for Neural Vocoding, Bandwidth Extension and Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2203.13086.pdf)
>  Generative adversarial networks have recently demonstrated outstanding performance in neural vocoding outperforming best autoregressive and flow-based models. In this paper, we show that this success can be extended to other tasks of conditional audio generation. In particular, building upon HiFi vocoders, we propose a novel HiFi++ general framework for neural vocoding, bandwidth extension, and speech enhancement. We show that with the improved generator architecture and simplified multi-discriminator training, HiFi++ performs on par with the state-of-the-art in these tasks while spending significantly less memory and computational resources. The effectiveness of our approach is validated through a series of extensive experiments.      
### 33.Multitask Emotion Recognition Model with Knowledge Distillation and Task Discriminator  [ :arrow_down: ](https://arxiv.org/pdf/2203.13072.pdf)
>  Due to the collection of big data and the development of deep learning, research to predict human emotions in the wild is being actively conducted. We designed a multi-task model using ABAW dataset to predict valence-arousal, expression, and action unit through audio data and face images at in real world. We trained model from the incomplete label by applying the knowledge distillation technique. The teacher model was trained as a supervised learning method, and the student model was trained by using the output of the teacher model as a soft label. As a result we achieved 2.40 in Multi Task Learning task validation dataset.      
### 34.Inner and Outer Approximations of Star-Convex Semialgebraic Sets  [ :arrow_down: ](https://arxiv.org/pdf/2203.13071.pdf)
>  We consider the problem of approximating a semialgebraic set with a sublevel-set of a polynomial function. In this setting, it is standard to seek a minimum volume outer approximation and/or maximum volume inner approximation. As there is no known relationship between the coefficients of an arbitrary polynomial and the volume of its sublevel sets, previous works have proposed heuristics based on the determinant and trace objectives commonly used in ellipsoidal fitting. For the case of star-convex semialgebraic sets, we propose a novel objective which yields both an outer and an inner approximation while minimizing the ratio of their respective volumes. This objective is scale-invariant and easily interpreted. Numerical examples are given which show that the approximations obtained are often tighter than those returned by existing heuristics. We also provide methods for establishing the star-convexity of a semialgebraic set by finding inner and outer approximations of its kernel.      
### 35.Information Preferences of Individual Agents in Linear-Quadratic-Gaussian Network Games  [ :arrow_down: ](https://arxiv.org/pdf/2203.13056.pdf)
>  We consider linear-quadratic-Gaussian (LQG) network games in which agents have quadratic payoffs that depend on their individual and neighbors' actions, and an unknown payoff-relevant state. An information designer determines the fidelity of information revealed to the agents about the payoff state to maximize the social welfare. Prior results show that full information disclosure is optimal under certain assumptions on the payoffs, i.e., it is beneficial for the average individual. In this paper, we provide conditions based on the strength of the dependence of payoffs on neighbors' actions, i.e., competition, under which a rational agent is expected to benefit, i.e., receive higher payoffs, from full information disclosure. We find that all agents benefit from information disclosure for the star network structure when the game is symmetric and submodular or supermodular. We also identify that the central agent benefits more than a peripheral agent from full information disclosure unless the competition is strong and the number of peripheral agents is small enough. Despite the fact that all agents expect to benefit from information disclosure ex-ante, a central agent can be worse-off from information disclosure in many realizations of the payoff state under strong competition, indicating that a risk-averse central agent can prefer uninformative signals ex-ante.      
### 36.Bailando: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory  [ :arrow_down: ](https://arxiv.org/pdf/2203.13055.pdf)
>  Driving 3D characters to dance following a piece of music is highly challenging due to the spatial constraints applied to poses by choreography norms. In addition, the generated dance sequence also needs to maintain temporal coherency with different music genres. To tackle these challenges, we propose a novel music-to-dance framework, Bailando, with two powerful components: 1) a choreographic memory that learns to summarize meaningful dancing units from 3D pose sequence to a quantized codebook, 2) an actor-critic Generative Pre-trained Transformer (GPT) that composes these units to a fluent dance coherent to the music. With the learned choreographic memory, dance generation is realized on the quantized units that meet high choreography standards, such that the generated dancing sequences are confined within the spatial constraints. To achieve synchronized alignment between diverse motion tempos and music beats, we introduce an actor-critic-based reinforcement learning scheme to the GPT with a newly-designed beat-align reward function. Extensive experiments on the standard benchmark demonstrate that our proposed framework achieves state-of-the-art performance both qualitatively and quantitatively. Notably, the learned choreographic memory is shown to discover human-interpretable dancing-style poses in an unsupervised manner.      
### 37.Multi-modal Emotion Estimation for in-the-wild Videos  [ :arrow_down: ](https://arxiv.org/pdf/2203.13032.pdf)
>  In this paper, we briefly introduce our submission to the Valence-Arousal Estimation Challenge of the 3rd Affective Behavior Analysis in-the-wild (ABAW) competition. Our method utilizes the multi-modal information, i.e., the visual and audio information, and employs a temporal encoder to model the temporal context in the videos. Besides, a smooth processor is applied to get more reasonable predictions, and a model ensemble strategy is used to improve the performance of our proposed method. The experiment results show that our method achieves 65.55% ccc for valence and 70.88% ccc for arousal on the validation set of the Aff-Wild2 dataset, which prove the effectiveness of our proposed method.      
### 38.Score difficulty analysis for piano performance education based on fingering  [ :arrow_down: ](https://arxiv.org/pdf/2203.13010.pdf)
>  In this paper, we introduce score difficulty classification as a sub-task of music information retrieval (MIR), which may be used in music education technologies, for personalised curriculum generation, and score retrieval. We introduce a novel dataset for our task, Mikrokosmos-difficulty, containing 147 piano pieces in symbolic representation and the corresponding difficulty labels derived by its composer Béla Bartók and the publishers. As part of our methodology, we propose piano technique feature representations based on different piano fingering algorithms. We use these features as input for two classifiers: a Gated Recurrent Unit neural network (GRU) with attention mechanism and gradient-boosted trees trained on score segments. We show that for our dataset fingering based features perform better than a simple baseline considering solely the notes in the score. Furthermore, the GRU with attention mechanism classifier surpasses the gradient-boosted trees. Our proposed models are interpretable and are capable of generating difficulty feedback both locally, on short term segments, and globally, for whole pieces. Code, datasets, models, and an online demo are made available for reproducibility      
### 39.Millimeter-wave Foresight Sensing for Safety and Resilience in Autonomous Operations  [ :arrow_down: ](https://arxiv.org/pdf/2203.12987.pdf)
>  Robotic platforms are highly programmable, scalable and versatile to complete several tasks including Inspection, Maintenance and Repair (IMR). Mobile robotics offer reduced restrictions in operating environments, resulting in greater flexibility; operation at height, dangerous areas and repetitive tasks. Cyber physical infrastructures have been identified by the UK Robotics Growth Partnership as a key enabler in how we utilize and interact with sensors and machines via the virtual and physical worlds. Cyber Physical Systems (CPS) allow for robotics and artificial intelligence to adapt and repurpose at pace, allowing for the addressment of new challenges in CPS. A challenge exists within robotics to secure an effective partnership in a wide range of areas which include shared workspaces and Beyond Visual Line of Sight (BVLOS). Robotic manipulation abilities have improved a robots accessibility via the ability to open doorways, however, challenges exist in how a robot decides if it is safe to move into a new workspace. Current sensing methods are limited to line of sight and are unable to capture data beyond doorways or walls, therefore, a robot is unable to sense if it is safe to open a door. Another limitation exists as robots are unable to detect if a human is within a shared workspace. Therefore, if a human is detected, extended safety precautions can be taken to ensure the safe autonomous operation of a robot. These challenges are represented as safety, trust and resilience, inhibiting the successful advancement of CPS. This paper evaluates the use of frequency modulated continuous wave radar sensing for human detection and through-wall detection to increase situational awareness. The results validate the use of the sensor to detect the difference between a person and infrastructure, and increased situational awareness for navigation via foresight monitoring through walls.      
### 40.Optimal MIMO Combining for Blind Federated Edge Learning with Gradient Sparsification  [ :arrow_down: ](https://arxiv.org/pdf/2203.12957.pdf)
>  We provide the optimal receive combining strategy for federated learning in multiple-input multiple-output (MIMO) systems. Our proposed algorithm allows the clients to perform individual gradient sparsification which greatly improves performance in scenarios with heterogeneous (non i.i.d.) training data. The proposed method beats the benchmark by a wide margin.      
### 41.Personalized incentives as feedback design in generalized Nash equilibrium problems  [ :arrow_down: ](https://arxiv.org/pdf/2203.12948.pdf)
>  We investigate both stationary and time-varying, nonmonotone generalized Nash equilibrium problems that exhibit symmetric interactions among the agents, which are known to be potential. As may happen in practical cases, however, we envision a scenario in which the formal expression of the underlying potential function is not available, and we design a semi-decentralized Nash equilibrium seeking algorithm. In the proposed two-layer scheme, a coordinator iteratively integrates the (possibly noisy and sporadic) agents' feedback to learn the pseudo-gradients of the agents, and then design personalized incentives for them. On their side, the agents receive those personalized incentives, compute a solution to an extended game, and then return feedback measurements to the coordinator. In the stationary setting, our algorithm returns a Nash equilibrium in case the coordinator is endowed with standard learning policies, while it returns a Nash equilibrium up to a constant, yet adjustable, error in the time-varying case. As a motivating application, we consider the ridehailing service provided by several companies with mobility as a service orchestration, necessary to both handle competition among firms and avoid traffic congestion, which is also adopted to run numerical experiments verifying our results.      
### 42.SelfRemaster: Self-Supervised Speech Restoration with Analysis-by-Synthesis Approach Using Channel Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2203.12937.pdf)
>  We present a self-supervised speech restoration method without paired speech corpora. Because the previous general speech restoration method uses artificial paired data created by applying various distortions to high-quality speech corpora, it cannot sufficiently represent acoustic distortions of real data, limiting the applicability. Our model consists of analysis, synthesis, and channel modules that simulate the recording process of degraded speech and is trained with real degraded speech data in a self-supervised manner. The analysis module extracts distortionless speech features and distortion features from degraded speech, while the synthesis module synthesizes the restored speech waveform, and the channel module adds distortions to the speech waveform. Our model also enables audio effect transfer, in which only acoustic distortions are extracted from degraded speech and added to arbitrary high-quality audio. Experimental evaluations with both simulated and real data show that our method achieves significantly higher-quality speech restoration than the previous supervised method, suggesting its applicability to real degraded speech materials.      
### 43.Lahjoita puhetta -- a large-scale corpus of spoken Finnish with some benchmarks  [ :arrow_down: ](https://arxiv.org/pdf/2203.12906.pdf)
>  The Donate Speech campaign has so far succeeded in gathering approximately 3600 hours of ordinary, colloquial Finnish speech into the Lahjoita puhetta (Donate Speech) corpus. The corpus includes over twenty thousand speakers from all the regions of Finland and from all age brackets. The primary goals of the collection were to create a representative, large-scale resource to study spontaneous spoken Finnish and to accelerate the development of language technology and speech-based services. In this paper, we present the collection process and the collected corpus, and showcase its versatility through multiple use cases. The evaluated use cases include: automatic speech recognition of spontaneous speech, detection of age, gender, dialect and topic and metadata analysis. We provide benchmarks for the use cases, as well down loadable, trained baseline systems with open-source code for reproducibility. One further use case is to verify the metadata and transcripts given in this corpus itself, and to suggest artificial metadata and transcripts for the part of the corpus where it is missing.      
### 44.Expression Classification using Concatenation of Deep Neural Network for the 3rd ABAW3 Competition  [ :arrow_down: ](https://arxiv.org/pdf/2203.12899.pdf)
>  For computers to recognize human emotions, expression classification is an equally important problem in the human-computer interaction area. In the 3rd Affective Behavior Analysis In-The-Wild competition, the task of expression classification includes 8 classes including 6 basic expressions of human faces from videos. In this paper, we perform combination representation from RegNet, Attention module, and Transformer Encoder for the expression classification task. We achieve 35.87 \% for F1-score on the validation set of Aff-Wild2 dataset. This result shows the effectiveness of the proposed architecture.      
### 45.Wide band sub-band speech coding using nonlinear prediction  [ :arrow_down: ](https://arxiv.org/pdf/2203.12896.pdf)
>  We compare a wide band sub-band speech coder using ADPCM schemes with linear prediction against the same scheme with nonlinear prediction based on multi-layer perceptrons. Exhaustive results are presented in each band, and the full signal. Our proposed scheme with non-linear neural net prediction outperforms the linear scheme up to 2 dB in SEGSNR. In addition, we propose a simple method based on a non-linearity in order to obtain a synthetic wide band signal from a narrow band signal.      
### 46.A new subband non linear prediction coding algorithm for narrowband speech signal: The nADPCMB MLT coding scheme  [ :arrow_down: ](https://arxiv.org/pdf/2203.12894.pdf)
>  This paper focuses on a newly developed transparent nADPCMB MLT speech coding algorithm. Our coder first decomposes the narrowband speech signal in subbands, a non linear ADPCM scheme is then performed in each subband. The signal subband decomposition is piloted by the equivalent Modulated Lapped Transform (MLT) filter bank. The novelty of this algorithm is the non linear approach, based on neural networks, to subband prediction coding. We have evaluated the performance of the nADPCMB MLT coding algorithm with a session of formal listening based on the five grade impairment scale standardized within ITU - T Recommendation P.800.      
### 47.An Ensemble Approach for Facial Expression Analysis in Video  [ :arrow_down: ](https://arxiv.org/pdf/2203.12891.pdf)
>  Human emotions recognization contributes to the development of human-computer interaction. The machines understanding human emotions in the real world will significantly contribute to life in the future. This paper will introduce the Affective Behavior Analysis in-the-wild (ABAW3) 2022 challenge. The paper focuses on solving the problem of the valence-arousal estimation and action unit detection. For valence-arousal estimation, we conducted two stages: creating new features from multimodel and temporal learning to predict valence-arousal. First, we make new features; the Gated Recurrent Unit (GRU) and Transformer are combined using a Regular Networks (RegNet) feature, which is extracted from the image. The next step is the GRU combined with Local Attention to predict valence-arousal. The Concordance Correlation Coefficient (CCC) was used to evaluate the model.      
### 48.Automatic Speech recognition for Speech Assessment of Preschool Children  [ :arrow_down: ](https://arxiv.org/pdf/2203.12886.pdf)
>  The acoustic and linguistic features of preschool speech are investigated in this study to design an automated speech recognition (ASR) system. Acoustic fluctuation has been highlighted as a significant barrier to developing high-performance ASR applications for youngsters. Because of the epidemic, preschool speech assessment should be conducted online. Accordingly, there is a need for an automatic speech recognition system. We were confronted with new challenges in our cognitive system, including converting meaningless words from speech to text and recognizing word sequence. After testing and experimenting with several models we obtained a 3.1\% phoneme error rate in Persian. Wav2Vec 2.0 is a paradigm that could be used to build a robust end-to-end speech recognition system.      
### 49.Revisiting Digital Twins: Origins, Fundamentals and Practices  [ :arrow_down: ](https://arxiv.org/pdf/2203.12867.pdf)
>  The Digital Twins (DT) has quickly become a hot topic since it was proposed. It not only appears in all kinds of commercial propaganda, but also is widely quoted by academic circles. However, there are misstatements and misuse of the term DT in business and academy. This paper revisits Digital Twins and defines it to be a more advanced system/product/service modelling and simulation environment that combines the most modern Information Communication Technology (ICTs) and engineering mechanisms digitization, and characterized by system/product/service life cycle management, physically geometric visualization, real-time sensing and measurement of system operating conditions, predictability of system performance/safety/lifespan, complete engineering mechanisms-based simulations. The idea of Digital Twins originates from modelling and simulation practices of engineering informatization, including Virtual Manufacturing (VM), Model Predictive Control (MPC), and Building Information Model (BIM). Based on the two-element VM model, we propose a three-element model to represent Digital Twins. Digital Twins does not have its own unique technical characteristics; the existing practices of Digital Twins are extensions of the engineering informatization embracing modern ICTs. These insights clarify the origin of Digital Twins and its technical essentials.      
### 50.Transformer Compressed Sensing via Global Image Tokens  [ :arrow_down: ](https://arxiv.org/pdf/2203.12861.pdf)
>  Convolutional neural networks (CNN) have demonstrated outstanding Compressed Sensing (CS) performance compared to traditional, hand-crafted methods. However, they are broadly limited in terms of generalisability, inductive bias and difficulty to model long distance relationships. Transformer neural networks (TNN) overcome such issues by implementing an attention mechanism designed to capture dependencies between inputs. However, high-resolution tasks typically require vision Transformers (ViT) to decompose an image into patch-based tokens, limiting inputs to inherently local contexts. We propose a novel image decomposition that naturally embeds images into low-resolution inputs. These Kaleidoscope tokens (KD) provide a mechanism for global attention, at the same computational cost as a patch-based approach. To showcase this development, we replace CNN components in a well-known CS-MRI neural network with TNN blocks and demonstrate the improvements afforded by KD. We also propose an ensemble of image tokens, which enhance overall image quality and reduces model size. Supplementary material is available: <a class="link-external link-https" href="https://github.com/uqmarlonbran/TCS.git" rel="external noopener nofollow">this https URL</a>}{<a class="link-external link-https" href="https://github.com/uqmarlonbran/TCS.git" rel="external noopener nofollow">this https URL</a>      
### 51.Matrix Pontryagin principle approach to controllability metrics maximization under sparsity constraints  [ :arrow_down: ](https://arxiv.org/pdf/2203.12828.pdf)
>  Controllability maximization problem under sparsity constraints is a node selection problem that selects inputs that are effective for control in order to minimize the energy to control for desired state. In this paper we discuss the equivalence between the sparsity constrained controllability metrics maximization problems and their convex relaxation. The proof is based on the matrix-valued Pontryagin maximum principle applied to the controllability Lyapunov differential equation.      
### 52.Subjective and Objective Analysis of Streamed Gaming Videos  [ :arrow_down: ](https://arxiv.org/pdf/2203.12824.pdf)
>  The rising popularity of online User-Generated-Content (UGC) in the form of streamed and shared videos, has hastened the development of perceptual Video Quality Assessment (VQA) models, which can be used to help optimize their delivery. Gaming videos, which are a relatively new type of UGC videos, are created when skilled gamers post videos of their gameplay. These kinds of screenshots of UGC gameplay videos have become extremely popular on major streaming platforms like YouTube and Twitch. Synthetically-generated gaming content presents challenges to existing VQA algorithms, including those based on natural scene/video statistics models. Synthetically generated gaming content presents different statistical behavior than naturalistic videos. A number of studies have been directed towards understanding the perceptual characteristics of professionally generated gaming videos arising in gaming video streaming, online gaming, and cloud gaming. However, little work has been done on understanding the quality of UGC gaming videos, and how it can be characterized and predicted. Towards boosting the progress of gaming video VQA model development, we conducted a comprehensive study of subjective and objective VQA models on UGC gaming videos. To do this, we created a novel UGC gaming video resource, called the LIVE-YouTube Gaming video quality (LIVE-YT-Gaming) database, comprised of 600 real UGC gaming videos. We conducted a subjective human study on this data, yielding 18,600 human quality ratings recorded by 61 human subjects. We also evaluated a number of state-of-the-art (SOTA) VQA models on the new database, including a new one, called GAME-VQP, based on both natural video statistics and CNN-learned features. To help support work in this field, we are making the new LIVE-YT-Gaming Database, publicly available through the link: <a class="link-external link-https" href="https://live.ece.utexas.edu/research/LIVE-YT-Gaming/index.html" rel="external noopener nofollow">this https URL</a> .      
### 53.Disentangleing Content and Fine-grained Prosody Information via Hybrid ASR Bottleneck Features for Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2203.12813.pdf)
>  Non-parallel data voice conversion (VC) have achieved considerable breakthroughs recently through introducing bottleneck features (BNFs) extracted by the automatic speech recognition(ASR) model. However, selection of BNFs have a significant impact on VC result. For example, when extracting BNFs from ASR trained with Cross Entropy loss (CE-BNFs) and feeding into neural network to train a VC system, the timbre similarity of converted speech is significantly degraded. If BNFs are extracted from ASR trained using Connectionist Temporal Classification loss (CTC-BNFs), the naturalness of the converted speech may decrease. This phenomenon is caused by the difference of information contained in BNFs. In this paper, we proposed an any-to-one VC method using hybrid bottleneck features extracted from CTC-BNFs and CE-BNFs to complement each other advantages. Gradient reversal layer and instance normalization were used to extract prosody information from CE-BNFs and content information from CTC-BNFs. Auto-regressive decoder and Hifi-GAN vocoder were used to generate high-quality waveform. Experimental results show that our proposed method achieves higher similarity, naturalness, quality than baseline method and reveals the differences between the information contained in CE-BNFs and CTC-BNFs as well as the influence they have on the converted speech.      
### 54.Functional mimicry of Ruffini receptors with Fiber Bragg Gratings and Deep Neural Networks enables a bio-inspired large-area tactile sensitive skin  [ :arrow_down: ](https://arxiv.org/pdf/2203.12752.pdf)
>  Collaborative robots are expected to physically interact with humans in daily living and workplace, including industrial and healthcare settings. A related key enabling technology is tactile sensing, which currently requires addressing the outstanding scientific challenge to simultaneously detect contact location and intensity by means of soft conformable artificial skins adapting over large areas to the complex curved geometries of robot embodiments. In this work, the development of a large-area sensitive soft skin with a curved geometry is presented, allowing for robot total-body coverage through modular patches. The biomimetic skin consists of a soft polymeric matrix, resembling a human forearm, embedded with photonic Fiber Bragg Grating (FBG) transducers, which partially mimics Ruffini mechanoreceptor functionality with diffuse, overlapping receptive fields. A Convolutional Neural Network deep learning algorithm and a multigrid Neuron Integration Process were implemented to decode the FBG sensor outputs for inferring contact force magnitude and localization through the skin surface. Results achieved 35 mN (IQR = 56 mN) and 3.2 mm (IQR = 2.3 mm) median errors, for force and localization predictions, respectively. Demonstrations with an anthropomorphic arm pave the way towards AI-based integrated skins enabling safe human-robot cooperation via machine intelligence.      
### 55.An interactive music infilling interface for pop music composition  [ :arrow_down: ](https://arxiv.org/pdf/2203.12736.pdf)
>  Artificial intelligence (AI) has been widely applied to music generation topics such as continuation, melody/harmony generation, genre transfer and music infilling application. Although with the burst interest to apply AI to music, there are still few interfaces for the musicians to take advantage of the latest progress of the AI technology. This makes those tools less valuable in practice and harder to find its advantage/drawbacks without utilizing them in the real scenario. This work builds a max patch for interactive music infilling application with different levels of control, including track density/polyphony/occupation rate and bar tonal tension control. The user can select the melody/bass/harmony track as the infilling content up to 16 bars. The infilling algorithm is based on the author's previous work, and the interface sends/receives messages to the AI system hosted in the cloud. This interface lowers the barrier of AI technology and can generate different variations of the selected content. Those results can give several alternatives to the musicians' composition, and the interactive process realizes the value of the AI infilling system.      
### 56.Quantitative phase and absorption contrast imaging  [ :arrow_down: ](https://arxiv.org/pdf/2203.12733.pdf)
>  Phase retrieval in its most general form is the problem of reconstructing a complex valued function from phaseless information of some transform of that function. This problem arises in various fields such as X-ray crystallography, electron microscopy, coherent diffractive imaging, astronomy, speech recognition, and quantum mechanics. The mathematical and computational analysis of these problems has a long history and a variety of different algorithms has been proposed in the literature. The performance of which usually depends on the constraints imposed on the sought function and the number of measurements. In this paper, we present an algorithm for coherent diffractive imaging with phaseless measurements. The algorithm accounts for both coherent and incoherent wave propagation and allows for reconstructing absorption as well as phase images that quantify the attenuation and the refraction of the waves when they go through an object. The algorithm requires coherent or partially coherent illumination, and several detectors to record the intensity of the distorted wave that passes through the object under inspection. To obtain enough information for imaging, a series of masks are introduced between the source and the object that create a diversity of illumination patterns.      
### 57.Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2203.12707.pdf)
>  Unpaired image-to-image translation (I2I) is an ill-posed problem, as an infinite number of translation functions can map the source domain distribution to the target distribution. Therefore, much effort has been put into designing suitable constraints, e.g., cycle consistency (CycleGAN), geometry consistency (GCGAN), and contrastive learning-based constraints (CUTGAN), that help better pose the problem. However, these well-known constraints have limitations: (1) they are either too restrictive or too weak for specific I2I tasks; (2) these methods result in content distortion when there is a significant spatial variation between the source and target domains. This paper proposes a universal regularization technique called maximum spatial perturbation consistency (MSPC), which enforces a spatial perturbation function (T ) and the translation operator (G) to be commutative (i.e., TG = GT ). In addition, we introduce two adversarial training components for learning the spatial perturbation function. The first one lets T compete with G to achieve maximum perturbation. The second one lets G and T compete with discriminators to align the spatial variations caused by the change of object size, object distortion, background interruptions, etc. Our method outperforms the state-of-the-art methods on most I2I benchmarks. We also introduce a new benchmark, namely the front face to profile face dataset, to emphasize the underlying challenges of I2I for real-world applications. We finally perform ablation experiments to study the sensitivity of our method to the severity of spatial perturbation and its effectiveness for distribution alignment.      
### 58.Towards Scalable Risk Analysis for Stochastic Systems Using Extreme Value Theory  [ :arrow_down: ](https://arxiv.org/pdf/2203.12689.pdf)
>  We aim to analyze the behaviour of a finite-time stochastic system, whose model is not available, in the context of more rare and harmful outcomes. Standard estimators are not effective in predicting the magnitude of such outcomes due to their rarity. Instead, we leverage Extreme Value Theory (EVT), the study of the long-term behaviour of normalized maxima of random variables. In this letter, we quantify risk using the upper-semideviation $E(\max\{Y-E(Y),0\})$, which is the expected exceedance of a random variable $Y$ above its mean $E(Y)$. To assess more rare and harmful outcomes, we propose an EVT-based estimator for this functional in a given fraction of the worst cases. We show that our estimator enjoys a closed-form representation in terms of a popular functional called the conditional value-at-risk. In experiments, we illustrate the extrapolation power of our estimator when a small number of i.i.d. samples is available ($&lt;$50). Our approach is useful for estimating the risk of finite-time systems when models are inaccessible and data collection is expensive. The numerical complexity does not grow with the size of the state space. This letter initiates a broader pathway for estimating the risk of large-scale systems.      
### 59.Are Evolutionary Algorithms Safe Optimizers?  [ :arrow_down: ](https://arxiv.org/pdf/2203.12622.pdf)
>  We consider a type of constrained optimization problem, where the violation of a constraint leads to an irrevocable loss, such as breakage of a valuable experimental resource/platform or loss of human life. Such problems are referred to as safe optimization problems (SafeOPs). While SafeOPs have received attention in the machine learning community in recent years, there was little interest in the evolutionary computation (EC) community despite some early attempts between 2009 and 2011. Moreover, there is a lack of acceptable guidelines on how to benchmark different algorithms for SafeOPs, an area where the EC community has significant experience in. Driven by the need for more efficient algorithms and benchmark guidelines for SafeOPs, the objective of this paper is to reignite the interest of this problem class in the EC community. To achieve this we (i) provide a formal definition of SafeOPs and contrast it to other types of optimization problems that the EC community is familiar with, (ii) investigate the impact of key SafeOP parameters on the performance of selected safe optimization algorithms, (iii) benchmark EC against state-of-the-art safe optimization algorithms from the machine learning community, and (iv) provide an open-source Python framework to replicate and extend our work.      
