# ArXiv eess --Wed, 23 Mar 2022
### 1.Multiple Models for Discrete-time Adaptive Iterative Learning Control  [ :arrow_down: ](https://arxiv.org/pdf/2203.11892.pdf)
>  This article presents a complete framework for multiple estimation models in the discrete-time Adaptive Iterative Learning Control (ILC) problem. The use of multiple models improves transient response and error convergence in adaptive systems and has not been previously explored in the context of Adaptive ILC. A new control and identification scheme with a single estimation model is presented. This scheme enables the extension to multiple models, unlike existing Adaptive ILC strategies. Next, the fundamentals of the Multiple Models, Switching and Tuning (MMST) methodology are used for multiple estimation models in Adaptive ILC. Two switching strategies are outlined, and convergence is proved for all techniques using the properties of square-summable sequences. Simulation results demonstrate the efficacy of the proposed strategies, with improved convergence using multiple estimation models.      
### 2.Neural System Level Synthesis: Learning over All Stabilizing Policies for Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.11812.pdf)
>  We address the problem of designing stabilizing control policies for nonlinear systems in discrete-time, while minimizing an arbitrary cost function. When the system is linear and the cost is convex, the System Level Synthesis (SLS) approach offers an exact solution based on convex programming. Beyond this case, a globally optimal solution cannot be found in a tractable way, in general. In this paper, we develop a parametrization of all and only the control policies stabilizing a given time-varying nonlinear system in terms of the combined effect of 1) a strongly stabilizing base controller and 2) a stable SLS operator to be freely designed. Based on this result, we propose a Neural SLS (Neur-SLS) approach guaranteeing closed-loop stability during and after parameter optimization, without requiring any constraints to be satisfied. We exploit recent Deep Neural Network (DNN) models based on Recurrent Equilibrium Networks (RENs) to learn over a rich class of nonlinear stable operators, and demonstrate the effectiveness of the proposed approach in numerical examples.      
### 3.Analysis Method of Strapdown Inertial Navigation Error Distribution Based on Covariance Matrix Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2203.11810.pdf)
>  Error distribution analysis is an important assistant technology for the research of SINS(Strapdown Inertial Navigation System). Error distribution result can provide the contribution of different errors to final navigation error, which is helpful for modifying and optimizing SINS. To realize decomposing the navigation error into parts that caused by each error source, the SINS error state space model is established and covariance matrix is decomposed according to error sources. The proposed error distribution analysis method based on 34-dimension SINS error model can quantitatively analyze the contribution to the end navigation error of initial errors, IMU(Inertial Measurement Unit) bias, IMU scale factor errors, mounting errors of gyroscopes and accelerometers, and IMU stochastic errors. The simulations in static condition and single axis rotation condition indict that the distribution result of proposed analysis method accords with the law of error propagation. After trajectory determined, the corresponding error distribution result will be calculated with the proposed method. Compared with the Monte-Carlo method and other method based on covariance matrix, the proposed method uses more complete error model, considers the interaction effect of error sources and can be easily realized with less computation.      
### 4.COSTARICA estimator for rollback-less systems handling in iterative co-simulation algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2203.11752.pdf)
>  Co-simulation is widely used in the industry due to the emergence of modular dynamical models made up of interconnected, black-boxed systems. Several co-simulation algorithms have been developed, each with different properties and different levels of accuracy and robustness. Among them, the most accurate and reliable ones are the iterative ones, although they have a main drawback in common: the involved systems are required to be capable of rollback. The latter denotes the ability of a system to integrate over a co-simulation time step that has already been simulated. Non-rollback-capable system can only go forward in time and every integrated step is definitive. In practice, the industrial modelling and simulation platforms rarely produce rollback-capable systems. This paper proposes a solution that slightly changes the co-simulation methodology and that enables to use iterative co-simulation methods on a modular model which contains non-rollback-capable systems in case the latter represent ordinary differential equations. The idea is to replace such a system by a simplified version, which is used to estimate the results of the integrations instead of integrating the real system. Once the co-simulation method's surrogate iterations on these estimators predict the convergence on the co-simulation step, the non-rollback-capable systems genuinely integrate the step using the estimated solution on the other systems before moving forward, transforming the iterative co-simulation method into a non-iterative one.      
### 5.Gated Recurrent Unit based Autoencoder for Optical Link Fault Diagnosis in Passive Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.11727.pdf)
>  We propose a deep learning approach based on an autoencoder for identifying and localizing fiber faults in passive optical networks. The experimental results show that the proposed method detects faults with 97% accuracy, pinpoints them with an RMSE of 0.18 m and outperforms conventional techniques.      
### 6.Unsupervised Anomaly Detection in Medical Images with a Memory-augmented Multi-level Cross-attentional Masked Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2203.11725.pdf)
>  Unsupervised anomaly detection (UAD) aims to find anomalous images by optimising a detector using a training set that contains only normal images. UAD approaches can be based on reconstruction methods, self-supervised approaches, and Imagenet pre-trained models. Reconstruction methods, which detect anomalies from image reconstruction errors, are advantageous because they do not rely on the design of problem-specific pretext tasks needed by self-supervised approaches, and on the unreliable translation of models pre-trained from non-medical datasets. However, reconstruction methods may fail because they can have low reconstruction errors even for anomalous images. In this paper, we introduce a new reconstruction-based UAD approach that addresses this low-reconstruction error issue for anomalous images. Our UAD approach, the memory-augmented multi-level cross-attentional masked autoencoder (MemMC-MAE), is a transformer-based approach, consisting of a novel memory-augmented self-attention operator for the encoder and a new multi-level cross-attention operator for the decoder. MemMC-MAE masks large parts of the input image during its reconstruction, reducing the risk that it will produce low reconstruction errors because anomalies are likely to be masked and cannot be reconstructed. However, when the anomaly is not masked, then the normal patterns stored in the encoder's memory combined with the decoder's multi-level cross-attention will constrain the accurate reconstruction of the anomaly. We show that our method achieves SOTA anomaly detection and localisation on colonoscopy and Covid-19 Chest X-ray datasets.      
### 7.Convolutional Neural Network to Restore Low-Dose Digital Breast Tomosynthesis Projections in a Variance Stabilization Domain  [ :arrow_down: ](https://arxiv.org/pdf/2203.11722.pdf)
>  Digital breast tomosynthesis (DBT) exams should utilize the lowest possible radiation dose while maintaining sufficiently good image quality for accurate medical diagnosis. In this work, we propose a convolution neural network (CNN) to restore low-dose (LD) DBT projections to achieve an image quality equivalent to a standard full-dose (FD) acquisition. The proposed network architecture benefits from priors in terms of layers that were inspired by traditional model-based (MB) restoration methods, considering a model-based deep learning approach, where the network is trained to operate in the variance stabilization transformation (VST) domain. To accurately control the network operation point, in terms of noise and blur of the restored image, we propose a loss function that minimizes the bias and matches residual noise between the input and the output. The training dataset was composed of clinical data acquired at the standard FD and low-dose pairs obtained by the injection of quantum noise. The network was tested using real DBT projections acquired with a physical anthropomorphic breast phantom. The proposed network achieved superior results in terms of the mean normalized squared error (MNSE), training time and noise spatial correlation compared with networks trained with traditional data-driven methods. The proposed approach can be extended for other medical imaging application that requires LD acquisitions.      
### 8.Location- and Orientation-aware Millimeter Wave Beam Selection for Multi-Panel Antenna Devices  [ :arrow_down: ](https://arxiv.org/pdf/2203.11714.pdf)
>  While initial beam alignment (BA) in millimeter-wave networks has been thoroughly investigated, most research assumes a simplified terminal model based on uniform linear/planar arrays with isotropic antennas. Devices with non-isotropic antenna elements need multiple panels to provide good spherical coverage, and exhaustive search over all beams of all the panels leads to unacceptable overhead. This paper proposes a location- and orientation-aware solution that manages the initial BA for multi-panel devices. We present three different neural network structures that provide efficient BA with a wide range of training dataset sizes, complexity, and feedback message sizes. Our proposed methods outperform the generalized inverse fingerprinting and hierarchical panel-beam selection methods for two considered edge and edge-face antenna placement designs.      
### 9.Accelerating Extremum Seeking Convergence by Richardson Extrapolation Methods  [ :arrow_down: ](https://arxiv.org/pdf/2203.11696.pdf)
>  In this paper, we propose the concept of accelerated convergence that has originally been developed to speed up the convergence of numerical methods for extremum seeking (ES) loops. We demonstrate how the dynamics of ES loops may be analyzed to extract structural information about the generated output of the loop. This information is then used to distil the limit of the loop without having to wait for the system to converge to it.      
### 10.Panoptic segmentation with highly imbalanced semantic labels  [ :arrow_down: ](https://arxiv.org/pdf/2203.11692.pdf)
>  This manuscript describes the panoptic segmentation method we devised for our submission to the CONIC challenge at ISBI 2022. Key features of our method are a weighted loss that we specifically engineered for semantic segmentation of highly imbalanced cell types, and an existing state-of-the art nuclei instance segmentation model, which we combine in a Hovernet-like architecture.      
### 11.End-to-End Learned Block-Based Image Compression with Block-Level Masked Convolutions and Asymptotic Closed Loop Training  [ :arrow_down: ](https://arxiv.org/pdf/2203.11686.pdf)
>  Learned image compression research has achieved state-of-the-art compression performance with auto-encoder based neural network architectures, where the image is mapped via convolutional neural networks (CNN) into a latent representation that is quantized and processed again with CNN to obtain the reconstructed image. CNN operate on entire input images. On the other hand, traditional state-of-the-art image and video compression methods process images with a block-by-block processing approach for various reasons. Very recently, work on learned image compression with block based approaches have also appeared, which use the auto-encoder architecture on large blocks of the input image and introduce additional neural networks that perform intra/spatial prediction and deblocking/post-processing functions. This paper explores an alternative learned block-based image compression approach in which neither an explicit intra prediction neural network nor an explicit deblocking neural network is used. A single auto-encoder neural network with block-level masked convolutions is used and the block size is much smaller (8x8). By using block-level masked convolutions, each block is processed using reconstructed neighboring left and upper blocks both at the encoder and decoder. Hence, the mutual information between adjacent blocks is exploited during compression and each block is reconstructed using neighboring blocks, resolving the need for explicit intra prediction and deblocking neural networks. Since the explored system is a closed loop system, a special optimization procedure, the asymptotic closed loop design, is used with standard stochastic gradient descent based training. The experimental results indicate competitive image compression performance.      
### 12.Piecewise Constant Parameters Identification Under Finite Excitation Condition: Time Alertness Preservation, Exponential Convergence, Robustness and Applications  [ :arrow_down: ](https://arxiv.org/pdf/2203.11685.pdf)
>  The scope of this research is the identification of piecewise constant parameters of linear regression equations under the finite excitation condition. Such an equation is considered as a switched system, which identification usually consists of three main steps: a switching time instant detection, choice of the most appropriate model from the known set or generation of a new one, online adjustment of the chosen model parameters. Compared to the known methods, to make the computational burden lower and simplify the stability analysis, we use only one model to identify all switching states of the regression. So, the proposed identification procedure includes only two main approaches. The first one is a new estimation algorithm to detect switching time and preserve time alertness, which is based on a well-known DREM procedure and ensures adjustable detection delay. Unlike existing solutions, it does not involve an offline operation of data monitoring and stacking. The second one is the adaptive law, which provides element-wise monotonous exponential convergence of the regression parameters to their true values over the time range between two consecutive switches. Its convergence condition is that the regressor is finitely exciting somewhere inside such time interval. The robustness of the proposed identification procedure to the influence of external disturbances is analytically proved. Its effectiveness is demonstrated via numerical experiments, in which both abstract regressions and a second-order plant model are used.      
### 13.Multi-layer Clustering-based Residual Sparsifying Transform for Low-dose CT Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2203.11565.pdf)
>  The recently proposed sparsifying transform models incur low computational cost and have been applied to medical imaging. Meanwhile, deep models with nested network structure reveal great potential for learning features in different layers. In this study, we propose a network-structured sparsifying transform learning approach for X-ray computed tomography (CT), which we refer to as multi-layer clustering-based residual sparsifying transform (MCST) learning. The proposed MCST scheme learns multiple different unitary transforms in each layer by dividing each layer's input into several classes. We apply the MCST model to low-dose CT (LDCT) reconstruction by deploying the learned MCST model into the regularizer in penalized weighted least squares (PWLS) reconstruction. We conducted LDCT reconstruction experiments on XCAT phantom data and Mayo Clinic data and trained the MCST model with 2 (or 3) layers and with 5 clusters in each layer. The learned transforms in the same layer showed rich features while additional information is extracted from representation residuals. Our simulation results demonstrate that PWLS-MCST achieves better image reconstruction quality than the conventional FBP method and PWLS with edge-preserving (EP) regularizer. It also outperformed recent advanced methods like PWLS with a learned multi-layer residual sparsifying transform prior (MARS) and PWLS with a union of learned transforms (ULTRA), especially for displaying clear edges and preserving subtle details.      
### 14.New penalized criteria for smooth non-negative tensor factorization with missing entries  [ :arrow_down: ](https://arxiv.org/pdf/2203.11514.pdf)
>  Tensor factorization models are widely used in many applied fields such as chemometrics, psychometrics, computer vision or communication networks. Real life data collection is often subject to errors, resulting in missing data. Here we focus in understanding how this issue should be dealt with for nonnegative tensor factorization. We investigate several criteria used for non-negative tensor factorization in the case where some entries are missing. In particular we show how smoothness penalties can compensate the presence of missing values in order to ensure the existence of an optimum. This lead us to propose new criteria with efficient numerical optimization algorithms. Numerical experiments are conducted to support our claims.      
### 15.Joint Noise Reduction and Listening Enhancement for Full-End Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2203.11500.pdf)
>  Speech enhancement (SE) methods mainly focus on recovering clean speech from noisy input. In real-world speech communication, however, noises often exist in not only speaker but also listener environments. Although SE methods can suppress the noise contained in the speaker's voice, they cannot deal with the noise that is physically present in the listener side. To address such a complicated but common scenario, we investigate a deep learning-based joint framework integrating noise reduction (NR) with listening enhancement (LE), in which the NR module first suppresses noise and the LE module then modifies the denoised speech, i.e., the output of the NR module, to further improve speech intelligibility. The enhanced speech can thus be less noisy and more intelligible for listeners. Experimental results show that our proposed method achieves promising results and significantly outperforms the disjoint processing methods in terms of various speech evaluation metrics.      
### 16.Safety of Sampled-Data Systems with Control Barrier Functions via Approximate Discrete Time Models  [ :arrow_down: ](https://arxiv.org/pdf/2203.11470.pdf)
>  Control Barrier Functions (CBFs) have been demonstrated to be a powerful tool for safety-critical controller design for nonlinear systems. Existing design paradigms do not address the gap between theory (controller design with continuous time models) and practice (the discrete time sampled implementation of the resulting controllers); this can lead to poor performance and violations of safety for hardware instantiations. We propose an approach to close this gap by synthesizing sampled-data counterparts to these CBF-based controllers using approximate discrete time models and Sampled-Data Control Barrier Functions (SD-CBFs). Using properties of a system's continuous time model, we establish a relationship between SD-CBFs and a notion of practical safety for sampled-data systems. Furthermore, we construct convex optimization-based controllers that formally endow nonlinear systems with safety guarantees in practice. We demonstrate the efficacy of these controllers in simulation.      
### 17.Combined Optimal Routing and Coordination of Connected and Automated Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2203.11408.pdf)
>  In this letter, we consider a transportation network with a 100\% penetration rate of connected and automated vehicles (CAVs), and present an optimal routing approach which takes into account the efficiency achieved in the network by coordinating the CAVs at specific traffic scenarios, e.g., intersections, merging roadways, roundabouts, etc. To derive the optimal route of a travel request, we use the information of the CAVs that have already received a routing solution. This enables each CAV to consider the traffic conditions on the roads. Given the trajectories of CAVs resulting by the routing solutions, the solution of any new travel request determines the optimal travel time at each traffic scenario while satisfying all state, control, and safety constraints. We validate the performance of our framework through numerical simulations. To the best of our knowledge, this is the first attempt to consider the coordination of CAVs in a routing problem.      
### 18.Enhanced Preamble Based MAC Mechanism for IIoT-oriented PLC Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.11404.pdf)
>  In this paper, we propose an enhanced preamble based media access control mechanism (E-PMAC), which can be applied in power line communication (PLC) network for Industrial Internet of Things (IIoT). We introduce detailed technologies used in E-PMAC, including delay calibration mechanism, preamble design, and slot allocation algorithm. With these technologies, E-PMAC is more robust than existing preamble based MAC mechanism (P-MAC). Besides, we analyze the disadvantage of P-MAC in multi-layer networking and design the networking process of E-PMAC to accelerate networking process. We analyze the complexity of networking process in P-MAC and E-PMAC and prove that E-PMAC has lower complexity than P-MAC. Finally, we simulate the single-layer networking and multi-layer networking of E-PMAC, P-MAC, and existing PLC protocol, i.e. , IEEE1901.1. The simulation results indicate that E-PMAC spends much less time in networking than IEEE1901.1 and P-MAC. Finally, with our work, a PLC network based on E-PMAC mechanism can be realized.      
### 19.Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data  [ :arrow_down: ](https://arxiv.org/pdf/2203.11391.pdf)
>  Idiopathic Pulmonary Fibrosis (IPF) is an inexorably progressive fibrotic lung disease with a variable and unpredictable rate of progression. CT scans of the lungs inform clinical assessment of IPF patients and contain pertinent information related to disease progression. In this work, we propose a multi-modal method that uses neural networks and memory banks to predict the survival of IPF patients using clinical and imaging data. The majority of clinical IPF patient records have missing data (e.g. missing lung function tests). To this end, we propose a probabilistic model that captures the dependencies between the observed clinical variables and imputes missing ones. This principled approach to missing data imputation can be naturally combined with a deep survival analysis model. We show that the proposed framework yields significantly better survival analysis results than baselines in terms of concordance index and integrated Brier score. Our work also provides insights into novel image-based biomarkers that are linked to mortality.      
### 20.Robust Model Predictive Control with Polytopic Model Uncertainty through System Level Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2203.11375.pdf)
>  We propose a novel method for robust model predictive control (MPC) of uncertain systems subject to both polytopic model uncertainty and additive disturbances. In our method, we over-approximate the actual uncertainty by a surrogate additive disturbance which simplifies constraint tightening of the robust optimal control problem. Using System Level Synthesis, we can optimize over a robust linear state feedback control policy and the uncertainty over-approximation parameters jointly and in a convex manner. The proposed method is demonstrated to achieve feasible domains close to the maximal robust control invariant set for a wide range of uncertainty parameters and significant improvement in conservatism compared with tube-based MPC through numerical examples.      
### 21.Sensitivity of Single-Pulse Radar Detection to Radar State Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2203.11372.pdf)
>  Mission planners for aircraft operating under threat of detection from ground-based radar systems are often concerned with the probability of detection. Current approaches to path planning in such environments consider the radar state (i.e. radar position and parameters) to be deterministic and known. In practice, there is uncertainty in the radar state which induces uncertainty in the probability of detection. This paper presents a method to incorporate the uncertainty of the radar state in a single-pulse radar detection model. The method linearizes the radar detection model with respect to the the radar state and uses the linearized models to estimate, to the first order, the variance of the probability of detection. The results in this paper validate the linearization using Monte Carlo analysis and illustrate the sensitivity of the probability of detection to radar state uncertainty.      
### 22.Alarm-Based Root Cause Analysis in Industrial Processes Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.11321.pdf)
>  Alarm management systems have become indispensable in modern industry. Alarms inform the operator of abnormal situations, particularly in the case of equipment failures. Due to the interconnections between various parts of the system, each fault can affect other sections of the system operating normally. As a result, the fault propagates through faultless devices, increasing the number of alarms. Hence, the timely detection of the major fault that triggered the alarm by the operator can prevent the following consequences. However, due to the complexity of the system, it is often impossible to find precise relations between the underlying fault and the alarms. As a result, the operator needs support to make an appropriate decision immediately. Modeling alarms based on the historical alarm data can assist the operator in determining the root cause of the alarm. This research aims to model the relations between industrial alarms using historical alarm data in the database. Firstly, alarm data is collected, and alarm tags are sequenced. Then, these sequences are converted to numerical vectors using word embedding. Next, a self-attention-based BiLSTM-CNN classifier is used to learn the structure and relevance between historical alarm data. After training the model, this model is used for online fault detection. Finally, as a case study, the proposed model is implemented in the well-known Tennessee Eastman process, and the results are presented.      
### 23.Forward Link Analysis for Full-Duplex Cellular Networks with Low Resolution ADC/DAC  [ :arrow_down: ](https://arxiv.org/pdf/2203.11281.pdf)
>  In this work, we consider a full-duplex (FD) massive multiple-input multiple-output (MIMO) cellular network with low resolution analog-to-digital converters (ADCs) and digital-to-analog converters (DACs). We propose a unified framework for forward link analysis where matched filter precoders are applied at the FD base stations (BSs) under channel hardening. We derive expressions for the signal-to-quantization-plus-interference-plus-noise ratio (SQINR) for general and special cases. Finally, we quantify effects of quantization error, pilot contamination, and full duplexing for a hexagonal cells lattice on spectral efficiency and cumulative distribution function (CDF).      
### 24.EEG based Emotion Recognition: A Tutorial and Review  [ :arrow_down: ](https://arxiv.org/pdf/2203.11279.pdf)
>  Emotion recognition technology through analyzing the EEG signal is currently an essential concept in Artificial Intelligence and holds great potential in emotional health care, human-computer interaction, multimedia content recommendation, etc. Though there have been several works devoted to reviewing EEG-based emotion recognition, the content of these reviews needs to be updated. In addition, those works are either fragmented in content or only focus on specific techniques adopted in this area but neglect the holistic perspective of the entire technical routes. Hence, in this paper, we review from the perspective of researchers who try to take the first step on this topic. We review the recent representative works in the EEG-based emotion recognition research and provide a tutorial to guide the researchers to start from the beginning. The scientific basis of EEG-based emotion recognition in the psychological and physiological levels is introduced. Further, we categorize these reviewed works into different technical routes and illustrate the theoretical basis and the research motivation, which will help the readers better understand why those techniques are studied and employed. At last, existing challenges and future investigations are also discussed in this paper, which guides the researchers to decide potential future research directions.      
### 25.One-Bit Compressive Sensing: Can We Go Deep and Blind?  [ :arrow_down: ](https://arxiv.org/pdf/2203.11278.pdf)
>  One-bit compressive sensing is concerned with the accurate recovery of an underlying sparse signal of interest from its one-bit noisy measurements. The conventional signal recovery approaches for this problem are mainly developed based on the assumption that an exact knowledge of the sensing matrix is available. In this work, however, we present a novel data-driven and model-based methodology that achieves blind recovery; i.e., signal recovery without requiring the knowledge of the sensing matrix. To this end, we make use of the deep unfolding technique and develop a model-driven deep neural architecture which is designed for this specific task. The proposed deep architecture is able to learn an alternative sensing matrix by taking advantage of the underlying unfolded algorithm such that the resulting learned recovery algorithm can accurately and quickly (in terms of the number of iterations) recover the underlying compressed signal of interest from its one-bit noisy measurements. In addition, due to the incorporation of the domain knowledge and the mathematical model of the system into the proposed deep architecture, the resulting network benefits from enhanced interpretability, has a very small number of trainable parameters, and requires very small number of training samples, as compared to the commonly used black-box deep neural network alternatives for the problem at hand.      
### 26.Contribution of Different Handwriting Modalities to Differential Diagnosis of Parkinson's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2203.11269.pdf)
>  In this paper, we evaluate the contribution of different handwriting modalities to the diagnosis of Parkinson's disease. We analyse on-surface movement, in-air movement and pressure exerted on the tablet surface. Especially in-air movement and pressure-based features have been rarely taken into account in previous studies. We show that pressure and in-air movement also possess information that is relevant for the diagnosis of Parkinson's Disease (PD) from handwriting. In addition to the conventional kinematic and spatio-temporal features, we present a group of the novel features based on entropy and empirical mode decomposition of the handwriting signal. The presented results indicate that handwriting can be used as biomarker for PD providing classification performance around 89% area under the ROC curve (AUC) for PD classification.      
### 27.Domain Knowledge Aids in Signal Disaggregation; the Example of the Cumulative Water Heater  [ :arrow_down: ](https://arxiv.org/pdf/2203.11268.pdf)
>  In this article we present an unsupervised low-frequency method aimed at detecting and disaggregating the power used by Cumulative Water Heaters (CWH) in residential homes. Our model circumvents the inherent difficulty of unsupervised signal disaggregation by using both the shape of a power spike and its time of occurrence to identify the contribution of CWH reliably. Indeed, many CHWs in France are configured to turn on automatically during off-peak hours only, and we are able to use this domain knowledge to aid peak identification despite the low sampling frequency. In order to test our model, we equipped a home with sensors to record the ground-truth consumption of a water heater. We then apply the model to a larger dataset of energy consumption of Hello Watt users consisting of one month of consumption data for 5k homes at 30-minute resolution. In this dataset we successfully identified CWHs in the majority of cases where consumers declared using them. The remaining part is likely due to possible misconfiguration of CWHs, since triggering them during off-peak hours requires specific wiring in the electrical panel of the house. Our model, despite its simplicity, offers promising applications: detection of mis-configured CWHs on off-peak contracts and slow performance degradation.      
### 28.Assessing trade-offs among electrification and grid decarbonization in a clean energy transition: Application to New York State  [ :arrow_down: ](https://arxiv.org/pdf/2203.11263.pdf)
>  A modeling framework is presented to investigate trade-offs among decarbonization from increased low-carbon electricity generation and electrification of heating and vehicles. The model is broadly applicable but relies on high-fidelity parameterization of existing infrastructure and anticipated electrified loads; this study applies it to New York State where detailed data is available. Trade-offs are investigated between end use electrification and renewable energy deployment in terms of supply costs, generation and storage capacities, renewable resource mix, and system operation. Results indicate that equivalent emissions reductions can be achieved at lower costs to the grid by prioritizing electrification with 40-70% low-carbon electricity supply instead of aiming for complete grid decarbonization. With 60% electrification and 50% low-carbon electricity, approximately 1/3 emissions reductions can be achieved at current supply costs; with only 20% electrification, 90% low-carbon electricity is required to achieve the same emissions reductions, resulting in 43% higher grid costs. In addition, three primary cost drivers are identified for a system undergoing decarbonization: (1) decreasing per-unit costs of existing infrastructure with increasing electrified demand, (2) higher in-state generation costs from low-carbon sources relative to gas-based and hydropower generation, and (3) increasing integration costs at high percentages of low-carbon electricity.      
### 29.ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2203.11213.pdf)
>  Glioma is the most common and aggressive brain tumor. Magnetic resonance imaging (MRI) plays a vital role to evaluate tumors for the arrangement of tumor surgery and the treatment of subsequent procedures. However, the manual segmentation of the MRI image is strenuous, which limits its clinical application. With the development of deep learning, a large number of automatic segmentation methods have been developed, but most of them stay in 2D images, which leads to subpar performance. Moreover, the serious voxel imbalance between the brain tumor and the background as well as the different sizes and locations of the brain tumor makes the segmentation of 3D images a challenging problem. Aiming at segmenting 3D MRI, we propose a model for brain tumor segmentation with multiple encoders. The structure contains four encoders and one decoder. The four encoders correspond to the four modalities of the MRI image, perform one-to-one feature extraction, and then merge the feature maps of the four modalities into the decoder. This method reduces the difficulty of feature extraction and greatly improves model performance. We also introduced a new loss function named "Categorical Dice", and set different weights for different segmented regions at the same time, which solved the problem of voxel imbalance. We evaluated our approach using the online BraTS 2020 Challenge verification. Our proposed method can achieve promising results in the validation set compared to the state-of-the-art approaches with Dice scores of 0.70249, 0.88267, and 0.73864 for the intact tumor, tumor core, and enhanced tumor, respectively.      
### 30.Phase Recognition in Contrast-Enhanced CT Scans based on Deep Learning and Random Sampling  [ :arrow_down: ](https://arxiv.org/pdf/2203.11206.pdf)
>  A fully automated system for interpreting abdominal computed tomography (CT) scans with multiple phases of contrast enhancement requires an accurate classification of the phases. This work aims at developing and validating a precise, fast multi-phase classifier to recognize three main types of contrast phases in abdominal CT scans. We propose in this study a novel method that uses a random sampling mechanism on top of deep CNNs for the phase recognition of abdominal CT scans of four different phases: non-contrast, arterial, venous, and others. The CNNs work as a slice-wise phase prediction, while the random sampling selects input slices for the CNN models. Afterward, majority voting synthesizes the slice-wise results of the CNNs, to provide the final prediction at scan level. Our classifier was trained on 271,426 slices from 830 phase-annotated CT scans, and when combined with majority voting on 30% of slices randomly chosen from each scan, achieved a mean F1-score of 92.09% on our internal test set of 358 scans. The proposed method was also evaluated on 2 external test sets: CTPAC-CCRCC (N = 242) and LiTS (N = 131), which were annotated by our experts. Although a drop in performance has been observed, the model performance remained at a high level of accuracy with a mean F1-score of 76.79% and 86.94% on CTPAC-CCRCC and LiTS datasets, respectively. Our experimental results also showed that the proposed method significantly outperformed the state-of-the-art 3D approaches while requiring less computation time for inference.      
### 31.VinDr-Mammo: A large-scale benchmark dataset for computer-aided diagnosis in full-field digital mammography  [ :arrow_down: ](https://arxiv.org/pdf/2203.11205.pdf)
>  Mammography, or breast X-ray, is the most widely used imaging modality to detect cancer and other breast diseases. Recent studies have shown that deep learning-based computer-assisted detection and diagnosis (CADe or CADx) tools have been developed to support physicians and improve the accuracy of interpreting mammography. However, most published datasets of mammography are either limited on sample size or digitalized from screen-film mammography (SFM), hindering the development of CADe and CADx tools which are developed based on full-field digital mammography (FFDM). To overcome this challenge, we introduce VinDr-Mammo - a new benchmark dataset of FFDM for detecting and diagnosing breast cancer and other diseases in mammography. The dataset consists of 5,000 mammography exams, each of which has four standard views and is double read with disagreement (if any) being resolved by arbitration. It is created for the assessment of Breast Imaging Reporting and Data System (BI-RADS) and density at the breast level. In addition, the dataset also provides the category, location, and BI-RADS assessment of non-benign findings. We make VinDr-Mammo publicly available on PhysioNet as a new imaging resource to promote advances in developing CADe and CADx tools for breast cancer screening.      
### 32.Enabling faster and more reliable sonographic assessment of gestational age through machine learning  [ :arrow_down: ](https://arxiv.org/pdf/2203.11903.pdf)
>  Fetal ultrasounds are an essential part of prenatal care and can be used to estimate gestational age (GA). Accurate GA assessment is important for providing appropriate prenatal care throughout pregnancy and identifying complications such as fetal growth disorders. Since derivation of GA from manual fetal biometry measurements (head, abdomen, femur) are operator-dependent and time-consuming, there have been a number of research efforts focused on using artificial intelligence (AI) models to estimate GA using standard biometry images, but there is still room to improve the accuracy and reliability of these AI systems for widescale adoption. To improve GA estimates, without significant change to provider workflows, we leverage AI to interpret standard plane ultrasound images as well as 'fly-to' ultrasound videos, which are 5-10s videos automatically recorded as part of the standard of care before the still image is captured. We developed and validated three AI models: an image model using standard plane images, a video model using fly-to videos, and an ensemble model (combining both image and video). All three were statistically superior to standard fetal biometry-based GA estimates derived by expert sonographers, the ensemble model has the lowest mean absolute error (MAE) compared to the clinical standard fetal biometry (mean difference: -1.51 $\pm$ 3.96 days, 95% CI [-1.9, -1.1]) on a test set that consisted of 404 participants. We showed that our models outperform standard biometry by a more substantial margin on fetuses that were small for GA. Our AI models have the potential to empower trained operators to estimate GA with higher accuracy while reducing the amount of time required and user variability in measurement acquisition.      
### 33.An Optimal Transport Formulation of Bayes' Law for Nonlinear Filtering Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2203.11869.pdf)
>  This paper presents a variational representation of the Bayes' law using optimal transportation theory. The variational representation is in terms of the optimal transportation between the joint distribution of the (state, observation) and their independent coupling. By imposing certain structure on the transport map, the solution to the variational problem is used to construct a Brenier-type map that transports the prior distribution to the posterior distribution for any value of the observation signal. The new formulation is used to derive the optimal transport form of the Ensemble Kalman filter (EnKF) for the discrete-time filtering problem and propose a novel extension of EnKF to the non-Gaussian setting utilizing input convex neural networks. Finally, the proposed methodology is used to derive the optimal transport form of the feedback particle filler (FPF) in the continuous-time limit, which constitutes its first variational construction without explicitly using the nonlinear filtering equation or Bayes' law.      
### 34.Cryptographic switching functions for multiplicative watermarking in cyber-physical systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.11851.pdf)
>  In this paper we present a novel switching function for multiplicative watermarking systems. The switching function is based on the algebraic structure of elliptic curves over finite fields. The resulting function allows for both watermarking generator and remover to define appropriate system parameters, sharing only limited information, namely a private key. Given the definition of the switching function, we prove that the resulting watermarking parameters lead to a stable watermarking scheme.      
### 35.Gain and phase type multipliers for structured feedback robustness  [ :arrow_down: ](https://arxiv.org/pdf/2203.11837.pdf)
>  It is known that the stability of a feedback interconnection of two linear time-invariant systems implies that the graphs of the open-loop systems are quadratically separated. This separation is defined by an object known as the multiplier. The theory of integral quadratic constraints shows that the converse also holds under certain conditions. This paper establishes that if the feedback is robustly stable against certain structured uncertainty, then there always exists a multiplier that takes a corresponding form. In particular, if the feedback is robustly stable to certain gain-type uncertainty, then there exists a corresponding multiplier that is of phase-type, i.e., its diagonal blocks are zeros. These results build on the notion of phases of matrices and systems, which was recently introduced in the field of control. Similarly, if the feedback is robustly stable to certain phase-type uncertainty, then there exists a gain-type multiplier, i.e., its off-diagonal blocks are zeros. The results are meaningfully instructive in the search for a valid multiplier for establishing robust closed-loop stability, and cover the well-known small-gain and the recent small-phase theorems.      
### 36.A New Approach to Improve Learning-based Deepfake Detection in Realistic Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2203.11807.pdf)
>  Deep convolutional neural networks have achieved exceptional results on multiple detection and recognition tasks. However, the performance of such detectors are often evaluated in public benchmarks under constrained and non-realistic situations. The impact of conventional distortions and processing operations found in imaging workflows such as compression, noise, and enhancement are not sufficiently studied. Currently, only a few researches have been done to improve the detector robustness to unseen perturbations. This paper proposes a more effective data augmentation scheme based on real-world image degradation process. This novel technique is deployed for deepfake detection tasks and has been evaluated by a more realistic assessment framework. Extensive experiments show that the proposed data augmentation scheme improves generalization ability to unpredictable data distortions and unseen datasets.      
### 37.AP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network  [ :arrow_down: ](https://arxiv.org/pdf/2203.11799.pdf)
>  Blind-spot network (BSN) and its variants have made significant advances in self-supervised denoising. Nevertheless, they are still bound to synthetic noisy inputs due to less practical assumptions like pixel-wise independent noise. Hence, it is challenging to deal with spatially correlated real-world noise using self-supervised BSN. Recently, pixel-shuffle downsampling (PD) has been proposed to remove the spatial correlation of real-world noise. However, it is not trivial to integrate PD and BSN directly, which prevents the fully self-supervised denoising model on real-world images. We propose an Asymmetric PD (AP) to address this issue, which introduces different PD stride factors for training and inference. We systematically demonstrate that the proposed AP can resolve inherent trade-offs caused by specific PD stride factors and make BSN applicable to practical scenarios. To this end, we develop AP-BSN, a state-of-the-art self-supervised denoising method for real-world sRGB images. We further propose random-replacing refinement, which significantly improves the performance of our AP-BSN without any additional parameters. Extensive studies demonstrate that our method outperforms the other self-supervised and even unpaired denoising methods by a large margin, without using any additional knowledge, e.g., noise level, regarding the underlying unknown noise.      
### 38.A Novel Framework for Assessment of Learning-based Detectors in Realistic Conditions with Application to Deepfake Detection  [ :arrow_down: ](https://arxiv.org/pdf/2203.11797.pdf)
>  Deep convolutional neural networks have shown remarkable results on multiple detection tasks. Despite the significant progress, the performance of such detectors are often assessed in public benchmarks under non-realistic conditions. Specifically, impact of conventional distortions and processing operations such as compression, noise, and enhancement are not sufficiently studied. This paper proposes a rigorous framework to assess performance of learning-based detectors in more realistic situations. An illustrative example is shown under deepfake detection context. Inspired by the assessment results, a data augmentation strategy based on natural image degradation process is designed, which significantly improves the generalization ability of two deepfake detectors.      
### 39.A Perspective on Neural Capacity Estimation: Viability and Reliability  [ :arrow_down: ](https://arxiv.org/pdf/2203.11793.pdf)
>  Recently, several methods have been proposed for estimating the mutual information from sample data using deep neural networks and without the knowledge of closed-form distribution of the data. This class of estimators is referred to as neural mutual information estimators (NMIE). In this paper, we investigate the performance of different NMIE proposed in the literature when applied to the capacity estimation problem. In particular, we study the performance of mutual information neural estimator (MINE), smoothed mutual information lower-bound estimator (SMILE), and directed information neural estimator (DINE). For the NMIE above, capacity estimation relies on two deep neural networks (DNN): (i) one DNN generates samples from a distribution that is learned, and (ii) a DNN to estimate the MI between the channel input and the channel output. We benchmark these NMIE in three scenarios: (i) AWGN channel capacity estimation and (ii) channels with unknown capacity and continuous inputs i.e., optical intensity and peak-power constrained AWGN channel (iii) channels with unknown capacity and a discrete number of mass points i.e., Poisson channel. Additionally, we also (iv) consider the extension to the MAC capacity problem by considering the AWGN and optical MAC models.      
### 40.Autonomous Bikebot Control for Crossing Obstacles with Assistive Leg Impulsive Actuation  [ :arrow_down: ](https://arxiv.org/pdf/2203.11777.pdf)
>  As a single-track mobile platform, bikebot (i.e., bicycle-based robot) has attractive navigation capability to pass through narrow, off-road terrain with high-speed and high-energy efficiency. However, running crossing step-like obstacles creates challenges for intrinsically unstable, underactuated bikebots. This paper presents a novel autonomous bikebot control with assistive leg actuation to navigate crossing obstacles. The proposed design integrates the external/internal convertible-based control with leg-assisted impulse control. The leg-terrain interaction generates assistive impulsive torques to help maintain the navigation and balance capability when running across obstacles. The control performance is analyzed and guaranteed. The experimental results confirm that under the control design, the bikebot can smoothly run crossing multiple step-like obstacles with height more than one third of the wheel radius. The comparison results demonstrate the superior performance than those under only the velocity and steering control without leg assistive impulsive actuation.      
### 41.Estimation of speaker age and height from speech signal using bi-encoder transformer mixture model  [ :arrow_down: ](https://arxiv.org/pdf/2203.11774.pdf)
>  The estimation of speaker characteristics such as age and height is a challenging task, having numerous applications in voice forensic analysis. In this work, we propose a bi-encoder transformer mixture model for speaker age and height estimation. Considering the wide differences in male and female voice characteristics such as differences in formant and fundamental frequencies, we propose the use of two separate transformer encoders for the extraction of specific voice features in the male and female gender, using wav2vec 2.0 as a common-level feature extractor. This architecture reduces the interference effects during backpropagation and improves the generalizability of the model. We perform our experiments on the TIMIT dataset and significantly outperform the current state-of-the-art results on age estimation. Specifically, we achieve root mean squared error (RMSE) of 5.54 years and 6.49 years for male and female age estimation, respectively. Further experiment to evaluate the relative importance of different phonetic types for our task demonstrate that vowel sounds are the most distinguishing for age estimation.      
### 42.AI-enabled Assessment of Cardiac Systolic and Diastolic Function from Echocardiography  [ :arrow_down: ](https://arxiv.org/pdf/2203.11726.pdf)
>  Left ventricular (LV) function is an important factor in terms of patient management, outcome, and long-term survival of patients with heart disease. The most recently published clinical guidelines for heart failure recognise that over reliance on only one measure of cardiac function (LV ejection fraction) as a diagnostic and treatment stratification biomarker is suboptimal. Recent advances in AI-based echocardiography analysis have shown excellent results on automated estimation of LV volumes and LV ejection fraction. However, from time-varying 2-D echocardiography acquisition, a richer description of cardiac function can be obtained by estimating functional biomarkers from the complete cardiac cycle. In this work we propose for the first time an AI approach for deriving advanced biomarkers of systolic and diastolic LV function from 2-D echocardiography based on segmentations of the full cardiac cycle. These biomarkers will allow clinicians to obtain a much richer picture of the heart in health and disease. The AI model is based on the 'nn-Unet' framework and was trained and tested using four different databases. Results show excellent agreement between manual and automated analysis and showcase the potential of the advanced systolic and diastolic biomarkers for patient stratification. Finally, for a subset of 50 cases, we perform a correlation analysis between clinical biomarkers derived from echocardiography and CMR and we show excellent agreement between the two modalities.      
### 43.A Bayesian Approach for Shaft Centre Localisation in Journal Bearings  [ :arrow_down: ](https://arxiv.org/pdf/2203.11719.pdf)
>  It has been shown that ultrasonic techniques work well for online measuring of circumferential oil film thickness profile in journal bearings; unfortunately, they can be limited by their measuring range and unable to capture details of the film all around the bearing circumference. Attempts to model the film thickness over the full range of the bearing rely on deterministic approaches, which assume the observations to be true with absolute certainty. Unaccounted uncertainties of the film thickness may lead to a cascade of inaccurate predictions for subsequent calculations of hydrodynamic parameters. In the present work, a probabilistic framework is proposed to model the film thickness with Gaussian Processes. The results are then used to estimate the location of the bearing shaft under various operational conditions. A further step in the process involves using the newly-constructed dataset to generate likelihood maps displaying the probable location of the shaft centre, given the bearing rotational speed and applied static load. The results offer the possibility to visualise the confidence of the predictions and allow the true location to be found within an area of high probability within the bearing's bore.      
### 44.Scale Fragilities in Localized Consensus Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2203.11708.pdf)
>  We consider distributed consensus in networks where the agents have integrator dynamics of order two or higher ($n\ge 2$). We assume all feedback to be localized in the sense that each agent has a bounded number of neighbors and consider a scaling of the network through the addition of agents {in a modular manner, i.e., without re-tuning controller gains upon addition}. We show that standard consensus algorithms, {which rely on relative state feedback, }are subject to what we term scale fragilities, meaning that stability is lost as the network scales. For high-order agents ($n\ge 3$), we prove that no consensus algorithm with fixed gains can achieve consensus in networks of any size. That is, while a given algorithm may allow a small network to converge, it causes instability if the network grows beyond a certain finite size. This holds in families of network graphs whose algebraic connectivity, that is, the smallest non-zero Laplacian eigenvalue, is decreasing towards zero in network size (e.g. all planar graphs). For second-order consensus ($n = 2$) we prove that the same scale fragility applies to directed graphs that have a complex Laplacian eigenvalue approaching the origin (e.g. directed ring graphs). The proofs for both results rely on Routh-Hurwitz criteria for complex-valued polynomials and hold true for general directed network graphs. We survey classes of graphs subject to these scale fragilities, discuss their scaling constants, and finally prove that a sub-linear scaling of nodal neighborhoods can suffice to overcome the issue.      
### 45.Switching transformations for control of opinion patterns in signed networks: application to dynamic task allocation  [ :arrow_down: ](https://arxiv.org/pdf/2203.11703.pdf)
>  We propose a new design method to control opinion patterns on signed networks of agents making decisions about two options and to switch the network from any opinion pattern to a new desired one. Our method relies on switching transformations, which switch the sign of an agent's opinion at a stable equilibrium by flipping the sign of its local interactions with its neighbors. The global dynamical behavior of the switched network can be predicted rigorously when the original, and thus the witched, networks are structurally balanced. Structural balance ensures that the network dynamics are monotone, which makes the study of the basin of attraction of the various opinion patterns amenable to rigorous analysis through monotone systems theory. We illustrate the utility of the approach through scenarios motivated by multi-robot coordination and dynamic task allocation.      
### 46.Speaker recognition with a MLP classifier and LPCC codebook  [ :arrow_down: ](https://arxiv.org/pdf/2203.11614.pdf)
>  This paper improves the speaker recognition rates of a MLP classifier and LPCC codebook alone, using a linear combination between both methods. In simulations we have obtained an improvement of 4.7% over a LPCC codebook of 32 vectors and 1.5% for a codebook of 128 vectors (error rate drops from 3.68% to 2.1%). Also we propose an efficient algorithm that reduces the computational complexity of the LPCC-VQ system by a factor of 4.      
### 47.Nonlinear prediction with neural nets in ADPCM  [ :arrow_down: ](https://arxiv.org/pdf/2203.11612.pdf)
>  In the last years there has been a growing interest for nonlinear speech models. Several works have been published revealing the better performance of nonlinear techniques, but little attention has been dedicated to the implementation of the nonlinear model into real applications. This work is focused on the study of the behaviour of a nonlinear predictive model based on neural nets, in a speech waveform coder. Our novel scheme obtains an improvement in SEGSNR between 1 and 2 dB for an adaptive quantization ranging from 2 to 5 bits.      
### 48.Analysis of Disfluencies for automatic detection of Mild Cognitive Impartment: a deep learning approach  [ :arrow_down: ](https://arxiv.org/pdf/2203.11606.pdf)
>  The so-called Mild Cognitive Impairment (MCI) or cognitive loss appears in a previous stage before Alzheimer's Disease (AD), but it does not seem sufficiently severe to interfere in independent abilities of daily life, so it usually does not receive an appropriate diagnosis. Its detection is a challenging issue to be addressed by medical specialists. This work presents a novel proposal based on automatic analysis of speech and disfluencies aimed at supporting MCI diagnosis. The approach includes deep learning by means of Convolutional Neural Networks (CNN) and non-linear multifeature modelling. Moreover, to select the most relevant features non-parametric Mann-Whitney U-testt and Support Vector Machine Attribute (SVM) evaluation are used.      
### 49.Distributed Vehicular Dynamic Spectrum Access for Platooning Environments  [ :arrow_down: ](https://arxiv.org/pdf/2203.11600.pdf)
>  In this paper, we propose a distributed Vehicular Dynamic Spectrum Access (VDSA) framework for vehicles operating in platoon formations. Given the potential for significant congestion in licensed frequency bands for vehicular applications such as 5.9 GHz. Our approach proposes to offload part of the intra-platoon data traffic to spectral white-spaces in order to enhance vehicular connectivity in support of on-road operations. To enable VDSA, a Bumblebee-based decision making process is employed which is based on the behavioral models of animals, is employed to provide a means of distributed transmission band selection. Simulation results show the distributed VDSA framework improves the leader packets reception ratio by 5%, thus indicating its potential to increase in reliability of intra-platoon communications.      
### 50.CT-SAT: Contextual Transformer for Sequential Audio Tagging  [ :arrow_down: ](https://arxiv.org/pdf/2203.11573.pdf)
>  Sequential audio event tagging can provide not only the type information of audio events, but also the order information between events and the number of events that occur in an audio clip. Most previous works on audio event sequence analysis rely on connectionist temporal classification (CTC). However, CTC's conditional independence assumption prevents it from effectively learning correlations between diverse audio events. This paper first attempts to introduce Transformer into sequential audio tagging, since Transformers perform well in sequence-related tasks. To better utilize contextual information of audio event sequences, we draw on the idea of bidirectional recurrent neural networks, and propose a contextual Transformer (cTransformer) with a bidirectional decoder that could exploit the forward and backward information of event sequences. Experiments on the real-life polyphonic audio dataset show that, compared to CTC-based methods, the cTransformer can effectively combine the fine-grained acoustic representations from the encoder and coarse-grained audio event cues to exploit contextual information to successfully recognize and predict audio event sequences.      
### 51.Conditional Generative Data Augmentation for Clinical Audio Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2203.11570.pdf)
>  In this work, we propose a novel data augmentation method for clinical audio datasets based on a conditional Wasserstein Generative Adversarial Network with Gradient Penalty (cWGAN-GP), operating on log-mel spectrograms. To validate our method, we created a clinical audio dataset which was recorded in a real-world operating room during Total Hip Arthroplasty (THA) procedures and contains typical sounds which resemble the different phases of the intervention. We demonstrate the capability of the proposed method to generate realistic class-conditioned samples from the dataset distribution and show that training with the generated augmented samples outperforms classical audio augmentation methods in terms of classification accuracy. The performance was evaluated using a ResNet-18 classifier which shows a mean per-class accuracy improvement of 1.51% in a 5-fold cross validation experiment using the proposed augmentation method. Because clinical data is often expensive to acquire, the development of realistic and high-quality data augmentation methods is crucial to improve the robustness and generalization capabilities of learning-based algorithms which is especially important for safety-critical medical applications. Therefore, the proposed data augmentation method is an important step towards improving the data bottleneck for clinical audio-based machine learning systems. The code and dataset will be published upon acceptance.      
### 52.A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2203.11562.pdf)
>  Speech synthesis has come a long way as current text-to-speech (TTS) models can now generate natural human-sounding speech. However, most of the TTS research focuses on using adult speech data and there has been very limited work done on child speech synthesis. This study developed and validated a training pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models using child speech datasets. This approach adopts a multispeaker TTS retuning workflow to provide a transfer-learning pipeline. A publicly available child speech dataset was cleaned to provide a smaller subset of approximately 19 hours, which formed the basis of our fine-tuning experiments. Both subjective and objective evaluations were performed using a pretrained MOSNet for objective evaluation and a novel subjective framework for mean opinion score (MOS) evaluations. Subjective evaluations achieved the MOS of 3.92 for speech intelligibility, 3.85 for voice naturalness, and 3.96 for voice consistency. Objective evaluation using a pretrained MOSNet showed a strong correlation between real and synthetic child voices. The final trained model was able to synthesize child-like speech from reference audio samples as short as 5 seconds.      
### 53.NOS-NOC: A Software Package for Numerical Optimal Control of Nonsmooth Systems  [ :arrow_down: ](https://arxiv.org/pdf/2203.11516.pdf)
>  This letter introduces the open source software package for Nonsmooth Numerical Optimal Control (NOS-NOC). It is a modular tool based on CasADi [Andersson et al., 2019], IPOPT [Wchter and Biegler, 2006] and MATLAB, for numerically solving Optimal Control Problems (OCP) with piecewise smooth systems (PSS). It relies on the recently introduced Finite Elements with Switch Detection [Nurkanovi et al., 2022] which enables high accuracy optimal control and simulation of PSS. The time-freezing reformulation [Nurkanovi et al., 2021], which transforms several classes of systems with state jumps into PSS is supported as well. This enables the treatment of a broad class of nonsmooth systems in a unified way. The algorithms and reformulations yield mathematical programs with complementarity constraints (MPCC). They can be solved with techniques of continuous optimization in a homotopy procedure, without the use of integer variables. The goal of the package is to automate all reformulations and to make nonsmooth optimal control problems practically solvable, without deep expert knowledge.      
### 54.Continuous Optimization for Control of Hybrid Systems with Hysteresis via Time-Freezing  [ :arrow_down: ](https://arxiv.org/pdf/2203.11510.pdf)
>  This article regards numerical optimal control of a class of hybrid systems with hysteresis using solely techniques from nonlinear optimization, without any integer variables. Hysteresis is a rate independent memory effect which often results in severe nonsmoothness in the dynamics. These systems are not simply Piecewise Smooth Systems (PSS); they are a more complicated form of hybrid systems. We introduce a time-freezing reformulation which transforms these systems into a PSS. From the theoretical side, this reformulation opens the door to study systems with hysteresis via the rich tools developed for Filippov systems. From the practical side, it enables the use of the recently developed Finite Elements with Switch Detection [Nurkanovic et al., 2022], which makes high accuracy numerical optimal control of hybrid systems with hysteresis possible.      
### 55.Residual-Guided Non-Intrusive Speech Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2203.11499.pdf)
>  This paper proposes an approach to improve Non-Intrusive speech quality assessment(NI-SQA) based on the residuals between impaired speech and enhanced speech. The difficulty in our task is particularly lack of information, for which the corresponding reference speech is absent. We generate an enhanced speech on the impaired speech to compensate for the absence of the reference audio, then pair the information of residuals with the impaired speech. Compared to feeding the impaired speech directly into the model, residuals could bring some extra helpful information from the contrast in enhancement. The human ear is sensitive to certain noises but different to deep learning model. Causing the Mean Opinion Score(MOS) the model predicted is not enough to fit our subjective sensitive well and causes deviation. These residuals have a close relationship to reference speech and then improve the ability of the deep learning models to predict MOS. During the training phase, experimental results demonstrate that paired with residuals can quickly obtain better evaluation indicators under the same conditions. Furthermore, our final results improved 31.3 percent and 14.1 percent, respectively, in PLCC and RMSE.      
### 56.FrameHopper: Selective Processing of Video Frames in Detection-driven Real-Time Video Analytics  [ :arrow_down: ](https://arxiv.org/pdf/2203.11493.pdf)
>  Detection-driven real-time video analytics require continuous detection of objects contained in the video frames using deep learning models like YOLOV3, EfficientDet. However, running these detectors on each and every frame in resource-constrained edge devices is computationally intensive. By taking the temporal correlation between consecutive video frames into account, we note that detection outputs tend to be overlapping in successive frames. Elimination of similar consecutive frames will lead to a negligible drop in performance while offering significant performance benefits by reducing overall computation and communication costs. The key technical questions are, therefore, (a) how to identify which frames to be processed by the object detector, and (b) how many successive frames can be skipped (called skip-length) once a frame is selected to be processed. The overall goal of the process is to keep the error due to skipping frames as small as possible. We introduce a novel error vs processing rate optimization problem with respect to the object detection task that balances between the error rate and the fraction of frames filtering. Subsequently, we propose an off-line Reinforcement Learning (RL)-based algorithm to determine these skip-lengths as a state-action policy of the RL agent from a recorded video and then deploy the agent online for live video streams. To this end, we develop FrameHopper, an edge-cloud collaborative video analytics framework, that runs a lightweight trained RL agent on the camera and passes filtered frames to the server where the object detection model runs for a set of applications. We have tested our approach on a number of live videos captured from real-life scenarios and show that FrameHopper processes only a handful of frames but produces detection results closer to the oracle solution and outperforms recent state-of-the-art solutions in most cases.      
### 57.Modeling speech recognition and synthesis simultaneously: Encoding and decoding lexical and sublexical semantic information into speech with no direct access to speech data  [ :arrow_down: ](https://arxiv.org/pdf/2203.11476.pdf)
>  Human speakers encode information into raw speech which is then decoded by the listeners. This complex relationship between encoding (production) and decoding (perception) is often modeled separately. Here, we test how decoding of lexical and sublexical semantic information can emerge automatically from raw speech in unsupervised generative deep convolutional networks that combine both the production and perception principle. We introduce, to our knowledge, the most challenging objective in unsupervised lexical learning: an unsupervised network that must learn to assign unique representations for lexical items with no direct access to training data. We train several models (ciwGAN and fiwGAN by [1]) and test how the networks classify raw acoustic lexical items in the unobserved test data. Strong evidence in favor of lexical learning emerges. The architecture that combines the production and perception principles is thus able to learn to decode unique information from raw acoustic data in an unsupervised manner without ever accessing real training data. We propose a technique to explore lexical and sublexical learned representations in the classifier network. The results bear implications for both unsupervised speech synthesis and recognition as well as for unsupervised semantic modeling as language models increasingly bypass text and operate from raw acoustics.      
### 58.RT-Bench: an Extensible Benchmark Framework for the Analysis and Management of Real-Time Applications  [ :arrow_down: ](https://arxiv.org/pdf/2203.11423.pdf)
>  Benchmarking is crucial for testing and validating any system, even more so in real-time systems. Typical real-time applications adhere to well-understood abstractions: they exhibit a periodic behavior, operate on a well-defined working set, and strive for stable response time avoiding non-predicable factors such as page faults. Unfortunately, available benchmark suites fail to reflect key characteristics of real-time applications. Practitioners and researchers must resort to either benchmark heavily approximated real-time environments, or to re-engineer available benchmarks to add -- if possible -- the sought-after features. Additionally, the measuring and logging capabilities provided by most benchmark suites are not tailored "out-of-the-box" to real-time environments, and changing basic parameters such as the scheduling policy often becomes a tiring and error-prone exercise. <br>In this paper, we present RT-bench, an open-source framework adding standard real-time features to virtually any existing benchmark. Furthermore, RT-bench provides an easy-to-use, unified command line interface to customize key aspects of the real-time execution of a set of benchmarks. Our framework is guided by four main criteria: 1) cohesive interface, 2) support for periodic application behavior and deadline semantics, 3) controllable memory footprint, and 4) extensibility and portability. We have integrated within the framework applications from the widely used SD-VBS and IsolBench suites. We showcase a set of use-cases that are representative of typical real-time system evaluation scenarios and that can be easily conducted via RT-Bench.      
### 59.Mobility Equity from a Game-Theoretic Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2203.11421.pdf)
>  In this letter, we consider a multi-modal mobility system of travelers, and propose a game-theoretic framework to efficiently assign each traveler to a mobility service (e.g., different modes of transportation). Our focus in this framework is to maximize the "mobility equity" in the sense of respecting the mobility budgets of the travelers. Each traveler seeks to travel using only one service (e.g., car, bus, train, bike). The services are capacitated and can serve up to a fixed number of travelers at any instant of time. Thus, our problem falls under the category of many-to-one assignment problems, where the goal is to find the conditions that guarantee the stability of assignments. We formulate a linear program of maximizing the mobility equity of travelers and we fully characterize the optimal solution. We also show that our framework under the proposed pricing scheme induces truthfulness from the strategic travelers, while they have incentives to voluntarily participate under informational asymmetry.      
### 60.Robust Pivoting: Exploiting Frictional Stability Using Bilevel Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2203.11412.pdf)
>  Generalizable manipulation requires that robots be able to interact with novel objects and environment. This requirement makes manipulation extremely challenging as a robot has to reason about complex frictional interaction with uncertainty in physical properties of the object. In this paper, we study robust optimization for control of pivoting manipulation in the presence of uncertainties. We present insights about how friction can be exploited to compensate for the inaccuracies in the estimates of the physical properties during manipulation. In particular, we derive analytical expressions for stability margin provided by friction during pivoting manipulation. This margin is then used in a bilevel trajectory optimization algorithm to design a controller that maximizes this stability margin to provide robustness against uncertainty in physical properties of the object. We demonstrate our proposed method using a 6 DoF manipulator for manipulating several different objects.      
### 61.The VoiceMOS Challenge 2022  [ :arrow_down: ](https://arxiv.org/pdf/2203.11389.pdf)
>  We present the first edition of the VoiceMOS Challenge, a scientific event that aims to promote the study of automatic prediction of the mean opinion score (MOS) of synthetic speech. This challenge drew 22 participating teams from academia and industry who tried a variety of approaches to tackle the problem of predicting human ratings of synthesized speech. The listening test data for the main track of the challenge consisted of samples from 187 different text-to-speech and voice conversion systems spanning over a decade of research, and the out-of-domain track consisted of data from more recent systems rated in a separate listening test. Results of the challenge show the effectiveness of fine-tuning self-supervised speech models for the MOS prediction task, as well as the difficulty of predicting MOS ratings for unseen speakers and listeners, and for unseen systems in the out-of-domain setting.      
### 62.Online Joint Optimal Control-Estimation Architecture in Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.11327.pdf)
>  In this paper, we propose an optimal control-estimation architecture for distribution networks, which jointly solves the optimal power flow (OPF) problem and static state estimation (SE) problem through an online gradient-based feedback algorithm. The main objective is to enable a fast and timely interaction between the optimal controllers and state estimators with limited sensor measurements. First, convergence and optimality of the proposed algorithm are analytically established. Then, the proposed gradient-based algorithm is modified by introducing statistical information of the inherent estimation and linearization errors for an improved and robust performance of the online control decisions. Overall, the proposed method eliminates the traditional separation of control and operation, where control and estimation usually operate at distinct layers and different time-scales. Hence, it enables a computationally affordable, efficient and robust online operational framework for distribution networks under time-varying settings.      
### 63.Enhancing Speech Recognition Decoding via Layer Aggregation  [ :arrow_down: ](https://arxiv.org/pdf/2203.11325.pdf)
>  Recently proposed speech recognition systems are designed to predict using representations generated by their top layers, employing greedy decoding which isolates each timestep from the rest of the sequence. Aiming for improved performance, a beam search algorithm is frequently utilized and a language model is incorporated to assist with ranking the top candidates. In this work, we experiment with several speech recognition models and find that logits predicted using the top layers may hamper beam search from achieving optimal results. Specifically, we show that fined-tuned Wav2Vec 2.0 and HuBERT yield highly confident predictions, and hypothesize that the predictions are based on local information and may not take full advantage of the information encoded in intermediate layers. To this end, we perform a layer analysis to reveal and visualize how predictions evolve throughout the inference flow. We then propose a prediction method that aggregates the top M layers, potentially leveraging useful information encoded in intermediate layers and relaxing model confidence. We showcase the effectiveness of our approach via beam search decoding, conducting our experiments on Librispeech test and dev sets and achieving WER, and CER reduction of up to 10% and 22%, respectively.      
### 64.Probabilistically robust stabilizing allocations in uncertain cooperative games  [ :arrow_down: ](https://arxiv.org/pdf/2203.11322.pdf)
>  In this paper we consider multi-agent cooperative games with uncertain value functions for which we establish distribution-free guarantees on the probability of allocation stability, i.e., agents do not have incentives to defect the grand coalition to subcoalitions for unseen realizations of the uncertain parameter. In case the set of stable allocations, the so called core of the game, is empty, we propose a randomized relaxation of the core. We then show that those allocations that belong to this relaxed set can be accompanied by stability guarantees in a probably approximately correct fashion. Finally, numerical experiments corroborate our theoretical findings.      
### 65.On the Effect of Pre-Processing and Model Complexity for Plastic Analysis Using Short-Wave-Infrared Hyper-Spectral Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2203.11209.pdf)
>  The importance of plastic waste recycling is undeniable. In this respect, computer vision and deep learning enable solutions through the automated analysis of short-wave-infrared hyper-spectral images of plastics. In this paper, we offer an exhaustive empirical study to show the importance of efficient model selection for resolving the task of hyper-spectral image segmentation of various plastic flakes using deep learning. We assess the complexity level of generic and specialized models and infer their performance capacity: generic models are often unnecessarily complex. We introduce two variants of a specialized hyper-spectral architecture, PlasticNet, that outperforms several well-known segmentation architectures in both performance as well as computational complexity. In addition, we shed lights on the significance of signal pre-processing within the realm of hyper-spectral imaging. To complete our contribution, we introduce the largest, most versatile hyper-spectral dataset of plastic flakes of four primary polymer types.      
### 66.Hybrid training of optical neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2203.11207.pdf)
>  Optical neural networks are emerging as a promising type of machine learning hardware capable of energy-efficient, parallel computation. Today's optical neural networks are mainly developed to perform optical inference after in silico training on digital simulators. However, various physical imperfections that cannot be accurately modelled may lead to the notorious reality gap between the digital simulator and the physical system. To address this challenge, we demonstrate hybrid training of optical neural networks where the weight matrix is trained with neuron activation functions computed optically via forward propagation through the network. We examine the efficacy of hybrid training with three different networks: an optical linear classifier, a hybrid opto-electronic network, and a complex-valued optical network. We perform a comparative study to in silico training, and our results show that hybrid training is robust against different kinds of static noise. Our platform-agnostic hybrid training scheme can be applied to a wide variety of optical neural networks, and this work paves the way towards advanced all-optical training in machine intelligence.      
### 67.g2pW: A Conditional Weighted Softmax BERT for Polyphone Disambiguation in Mandarin  [ :arrow_down: ](https://arxiv.org/pdf/2203.10430.pdf)
>  Polyphone disambiguation is the most crucial task in Mandarin grapheme-to-phoneme (g2p) conversion. Previous studies have approached this problem using pre-trained language models, restricted output, and extra information from Part-Of-Speech (POS) tagging. Inspired by these strategies, we propose a novel approach, called g2pW, which adapts learnable softmax-weights to condition the outputs of BERT with the polyphonic character of interest and its POS tagging. Rather than using the hard mask as in previous works, our experiments show that learning a soft-weighting function for the candidate phonemes benefits performance. In addition, our proposed g2pW does not require extra pre-trained POS tagging models while using POS tags as auxiliary features since we train the POS tagging model simultaneously with the unified encoder. Experimental results show that our g2pW outperforms existing methods on the public CPP dataset. All codes, model weights, and a user-friendly package are publicly available.      
### 68.Audio-Visual Speech Enhancement using Multimodal Deep Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/1709.00944.pdf)
>  Speech enhancement (SE) aims to reduce noise in speech signals. Most SE techniques focus on addressing audio information only. In this work, inspired by multimodal learning, which utilizes data from different modalities, and the recent success of convolutional neural networks (CNNs) in SE, we propose an audio-visual deep CNN (AVDCNN) SE model, which incorporates audio and visual streams into a unified network model. In the proposed AVDCNN SE model, audio and visual data are first processed using individual CNNs, and then, fused into a joint network to generate enhanced speech at the output layer. The AVDCNN model is trained in an end-to-end manner, and parameters are jointly learned through back-propagation. We evaluate enhanced speech using five objective criteria. Results show that the AVDCNN yields notably better performance, compared with an audio-only CNN-based SE model and two conventional SE approaches, confirming the effectiveness of integrating visual information into the SE process.      
