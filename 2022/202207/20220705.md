# ArXiv eess --Tue, 5 Jul 2022
### 1.Classification of Alzheimer's Disease Using the Convolutional Neural Network (CNN) with Transfer Learning and Weighted Loss  [ :arrow_down: ](https://arxiv.org/pdf/2207.01584.pdf)
>  Alzheimer's disease is a progressive neurodegenerative disorder that gradually deprives the patient of cognitive function and can end in death. With the advancement of technology today, it is possible to detect Alzheimer's disease through Magnetic Resonance Imaging (MRI) scans. So that MRI is the technique most often used for the diagnosis and analysis of the progress of Alzheimer's disease. With this technology, image recognition in the early diagnosis of Alzheimer's disease can be achieved automatically using machine learning. Although machine learning has many advantages, currently the use of deep learning is more widely applied because it has stronger learning capabilities and is more suitable for solving image recognition problems. However, there are still several challenges that must be faced to implement deep learning, such as the need for large datasets, requiring large computing resources, and requiring careful parameter setting to prevent overfitting or underfitting. In responding to the challenge of classifying Alzheimer's disease using deep learning, this study propose the Convolutional Neural Network (CNN) method with the Residual Network 18 Layer (ResNet-18) architecture. To overcome the need for a large and balanced dataset, transfer learning from ImageNet is used and weighting the loss function values so that each class has the same weight. And also in this study conducted an experiment by changing the network activation function to a mish activation function to increase accuracy. From the results of the tests that have been carried out, the accuracy of the model is 88.3 % using transfer learning, weighted loss and the mish activation function. This accuracy value increases from the baseline model which only gets an accuracy of 69.1 %.      
### 2.Spatiotemporal Feature Learning Based on Two-Step LSTM and Transformer for CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2207.01579.pdf)
>  Computed tomography (CT) imaging could be very practical for diagnosing various diseases. However, the nature of the CT images is even more diverse since the resolution and number of the slices of a CT scan are determined by the machine and its settings. Conventional deep learning models are hard to tickle such diverse data since the essential requirement of the deep neural network is the consistent shape of the input data. In this paper, we propose a novel, effective, two-step-wise approach to tickle this issue for COVID-19 symptom classification thoroughly. First, the semantic feature embedding of each slice for a CT scan is extracted by conventional backbone networks. Then, we proposed a long short-term memory (LSTM) and Transformer-based sub-network to deal with temporal feature learning, leading to spatiotemporal feature representation learning. In this fashion, the proposed two-step LSTM model could prevent overfitting, as well as increase performance. Comprehensive experiments reveal that the proposed two-step method not only shows excellent performance but also could be compensated for each other. More specifically, the two-step LSTM model has a lower false-negative rate, while the 2-step Swin model has a lower false-positive rate. In summary, it is suggested that the model ensemble could be adopted for more stable and promising performance in real-world applications.      
### 3.Semi-blind source separation using convolutive transfer function for nonlinear acoustic echo cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2207.01556.pdf)
>  The recently proposed semi-blind source separation (SBSS) method for nonlinear acoustic echo cancellation (NAEC) outperforms adaptive NAEC in attenuating the nonlinear acoustic echo. However, the multiplicative transfer function (MTF) approximation makes it unsuitable for real-time applications especially in highly reverberant environments, and the natural gradient makes it hard to balance well between fast convergence speed and stability. In this paper, we propose two more effective SBSS methods based on auxiliary-function-based independent vector analysis (AuxIVA) and independent low-rank matrix analysis (ILRMA). The convolutive transfer function (CTF) approximation is used instead of MTF so that a long impulse response can be modeled with a short latency. The optimization schemes used in AuxIVA and ILRMA are carefully regularized according to the constrained demixing matrix of NAEC. Experimental results validate significantly better echo cancellation performance of the proposed methods.      
### 4.Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)  [ :arrow_down: ](https://arxiv.org/pdf/2207.01547.pdf)
>  An essential design decision for multilingual Neural Text-To-Speech (NTTS) systems is how to represent input linguistic features within the model. Looking at the wide variety of approaches in the literature, two main paradigms emerge, unified and separate representations. The former uses a shared set of phonetic tokens across languages, whereas the latter uses unique phonetic tokens for each language. In this paper, we conduct a comprehensive study comparing multilingual NTTS systems models trained with both representations. Our results reveal that the unified approach consistently achieves better cross-lingual synthesis with respect to both naturalness and accent. Separate representations tend to have an order of magnitude more tokens than unified ones, which may affect model capacity. For this reason, we carry out an ablation study to understand the interaction of the representation type with the size of the token embedding. We find that the difference between the two paradigms only emerges above a certain threshold embedding size. This study provides strong evidence that unified representations should be the preferred paradigm when building multilingual NTTS systems.      
### 5.Adaptive GLCM sampling for transformer-based COVID-19 detection on CT  [ :arrow_down: ](https://arxiv.org/pdf/2207.01520.pdf)
>  The world has suffered from COVID-19 (SARS-CoV-2) for the last two years, causing much damage and change in people's daily lives. Thus, automated detection of COVID-19 utilizing deep learning on chest computed tomography (CT) scans became promising, which helps correct diagnosis efficiently. Recently, transformer-based COVID-19 detection method on CT is proposed to utilize 3D information in CT volume. However, its sampling method for selecting slices is not optimal. To leverage rich 3D information in CT volume, we propose a transformer-based COVID-19 detection using a novel data curation and adaptive sampling method using gray level co-occurrence matrices (GLCM). To train the model which consists of CNN layer, followed by transformer architecture, we first executed data curation based on lung segmentation and utilized the entropy of GLCM value of every slice in CT volumes to select important slices for the prediction. The experimental results show that the proposed method improve the detection performance with large margin without much difficult modification to the model.      
### 6.Solving Bilevel AC OPF Problems by Smoothing the Complementary Conditions -- Part II: Solution Techniques and Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2207.01509.pdf)
>  This is a second part of the research on AC optimal power flow being used in the lower level of the bilevel strategic bidding or investment models. As an example of a suitable upper-level problem, we observe a strategic bidding of energy storage and propose a novel formulation based on the smoothing technique. After presenting the idea and scope of our work, as well as the model itself and the solution algorithm in the companion paper (Part I), this paper presents a number of existing solution techniques and the proposed one based on smoothing the complementary conditions. The superiority of the proposed algorithm and smoothing techniques is demonstrated in terms of accuracy and computational tractability over multiple transmission networks of different sizes and different OPF models. The results indicate that the proposed approach outperforms all other options in both metrics by a significant margin. This is especially noticeable in the metric of accuracy where out of total 422 optimizations over 9 meshed networks the greatest AC OPF error is 0.023% that is further reduced to 3.3e-4% in the second iteration of our algorithm.      
### 7.Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)  [ :arrow_down: ](https://arxiv.org/pdf/2207.01507.pdf)
>  Training multilingual Neural Text-To-Speech (NTTS) models using only monolingual corpora has emerged as a popular way for building voice cloning based Polyglot NTTS systems. In order to train these models, it is essential to understand how the composition of the training corpora affects the quality of multilingual speech synthesis. In this context, it is common to hear questions such as "Would including more Spanish data help my Italian synthesis, given the closeness of both languages?". Unfortunately, we found existing literature on the topic lacking in completeness in this regard. In the present work, we conduct an extensive ablation study aimed at understanding how various factors of the training corpora, such as language family affiliation, gender composition, and the number of speakers, contribute to the quality of Polyglot synthesis. Our findings include the observation that female speaker data are preferred in most scenarios, and that it is not always beneficial to have more speakers from the target language variant in the training corpus. The findings herein are informative for the process of data procurement and corpora building.      
### 8.GlowVC: Mel-spectrogram space disentangling model for language-independent text-free voice conversion  [ :arrow_down: ](https://arxiv.org/pdf/2207.01454.pdf)
>  In this paper, we propose GlowVC: a multilingual multi-speaker flow-based model for language-independent text-free voice conversion. We build on Glow-TTS, which provides an architecture that enables use of linguistic features during training without the necessity of using them for VC inference. We consider two versions of our model: GlowVC-conditional and GlowVC-explicit. GlowVC-conditional models the distribution of mel-spectrograms with speaker-conditioned flow and disentangles the mel-spectrogram space into content- and pitch-relevant dimensions, while GlowVC-explicit models the explicit distribution with unconditioned flow and disentangles said space into content-, pitch- and speaker-relevant dimensions. We evaluate our models in terms of intelligibility, speaker similarity and naturalness for intra- and cross-lingual conversion in seen and unseen languages. GlowVC models greatly outperform AutoVC baseline in terms of intelligibility, while achieving just as high speaker similarity in intra-lingual VC, and slightly worse in the cross-lingual setting. Moreover, we demonstrate that GlowVC-explicit surpasses both GlowVC-conditional and AutoVC in terms of naturalness.      
### 9.MPC-Based Operation Strategy for Electric Vehicle Aggregators Considering Regulation Markets  [ :arrow_down: ](https://arxiv.org/pdf/2207.01446.pdf)
>  The optimal operation problem of electric vehicle aggregator (EVA) is considered. An EVA can participate in energy and regulation markets with its current and upcoming EVs, thus reducing its total cost of purchasing energy to fulfill EVs' charging requirements. A model predictive control (MPC) based optimization is developed to consider the future arrival of EVs as well as energy and regulation prices. The index of conditional value-at-risk (CVaR) is used to model the risk-averseness of an EVA. Simulations on a 2000-EV test system validate the effectiveness of our work in achieving a lucrative revenue while satisfying the charging requests from EV owners.      
### 10.Representation Learning with Information Theory for COVID-19 Detection  [ :arrow_down: ](https://arxiv.org/pdf/2207.01437.pdf)
>  Successful data representation is a fundamental factor in machine learning based medical imaging analysis. Deep Learning (DL) has taken an essential role in robust representation learning. However, the inability of deep models to generalize to unseen data can quickly overfit intricate patterns. Thereby, we can conveniently implement strategies to aid deep models in discovering useful priors from data to learn their intrinsic properties. Our model, which we call a dual role network (DRN), uses a dependency maximization approach based on Least Squared Mutual Information (LSMI). The LSMI leverages dependency measures to ensure representation invariance and local smoothness. While prior works have used information theory measures like mutual information, known to be computationally expensive due to a density estimation step, our LSMI formulation alleviates the issues of intractable mutual information estimation and can be used to approximate it. Experiments on CT based COVID-19 Detection and COVID-19 Severity Detection benchmarks demonstrate the effectiveness of our method.      
### 11.Physics-informed Deep Learning for Musculoskeletal Modelling: Predicting Muscle Forces and Joint Kinematics from Surface EMG  [ :arrow_down: ](https://arxiv.org/pdf/2207.01435.pdf)
>  Musculoskeletal models have been widely used for detailed biomechanical analysis to characterise various functional impairments given their ability to estimate movement variables (i.e., muscle forces and joint moment) which cannot be readily measured in vivo. Physics-based computational neuromusculoskeletal models can interpret the dynamic interaction between neural drive to muscles, muscle dynamics, body and joint kinematics and kinetics. Still, such set of solutions suffers from slowness, especially for the complex models, hindering the utility in real-time applications. In recent years, data-driven methods has emerged as a promising alternative due to the benefits in speedy and simple implementation, but they cannot reflect the underlying neuromechanical processes. This paper proposes a physics-informed deep learning framework for musculoskeletal modelling, where physics-based domain knowledge is brought into the data-driven model as soft constraints to penalise/regularise the data-driven model. We use the synchronous muscle forces and joint kinematics prediction from surface electromyogram (sEMG) as the exemplar to illustrate the proposed framework. Convolutional neural network (CNN) is employed as the deep neural network to implement the proposed framework. At the same time, the physics law between muscle forces and joint kinematics is used the soft constraint. Experimental validations on two groups of data, including one benchmark dataset and one self-collected dataset from six healthy subjects, are performed. The experimental results demonstrate the effectiveness and robustness of the proposed framework.      
### 12.Krasovskii and Shifted Passivity Based Output Consensus  [ :arrow_down: ](https://arxiv.org/pdf/2207.01430.pdf)
>  Motivated by current sharing in power networks, we consider a class of output consensus (also called agreement) problems for nonlinear systems, where the consensus value is determined by external disturbances, e.g., power demand. This output consensus problem is solved by a simple distributed output feedback controller if a system is either Krasovskii or shifted passive, which is the only essential requirement. The effectiveness of the proposed controller is shown in simulation on an islanded DC power network.      
### 13.Multimodal 4DVarNets for the reconstruction of sea surface dynamics from SST-SSH synergies  [ :arrow_down: ](https://arxiv.org/pdf/2207.01372.pdf)
>  Due to the irregular space-time sampling of sea surface observations, the reconstruction of sea surface dynamics is a challenging inverse problem. While satellite altimetry provides a direct observation of the sea surface height (SSH), which relates to the divergence-free component of sea surface currents, the associated sampling pattern prevents from retrieving fine-scale sea surface dynamics, typically below a 10-day time scale. By contrast, other satellite sensors provide higher-resolution observations of sea surface tracers such as sea surface temperature (SST). Multimodal inversion schemes then arise as an appealing strategy. Though theoretical evidence supports the existence of an explicit relationship between sea surface temperature and sea surface dynamics under specific dynamical regimes, the generalization to the variety of upper ocean dynamical regimes is complex. Here, we investigate this issue from a physics-informed learning perspective. We introduce a trainable multimodal inversion scheme for the reconstruction of sea surface dynamics from multi-source satellite-derived observations. The proposed 4DVarNet schemes combine a variational formulation involving trainable observation and a priori terms with a trainable gradient-based solver. We report an application to the reconstruction of the divergence-free component of sea surface dynamics from satellite-derived SSH and SST data. An observing system simulation experiment for a Gulf Stream region supports the relevance of our approach compared with state-of-the-art schemes. We report relative improvement greater than 50% compared with the operational altimetry product in terms of root mean square error and resolved space-time scales. We discuss further the application and extension of the proposed approach for the reconstruction and forecasting of geophysical dynamics from irregularly-sampled satellite observations.      
### 14.Can Competition Outperform Collaboration? The Role of Malicious Agents  [ :arrow_down: ](https://arxiv.org/pdf/2207.01346.pdf)
>  We investigate a novel approach to resilient distributed optimization with quadratic costs in a Networked Control System prone to exogenous attacks that make agents misbehave. In contrast with commonly adopted filtering strategies, we draw inspiration from a game-theoretic formulation of the consensus problem and argue that adding competition to the mix can improve resilience in the presence of malicious agents. Our intuition is corroborated by analytical and numerical results showing that (i) our strategy reveals a nontrivial performance trade-off between full collaboration and full competition, and (ii) such competitionbased approach can outperform state-of-the-art algorithms based on Mean Subsequence Reduced. Finally, we study impact of communication topology and connectivity on performance, pointing out insights to robust network design.      
### 15.Multi-scale alignment and Spatial ROI Module for COVID-19 Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2207.01345.pdf)
>  Coronavirus Disease 2019 (COVID-19) has spread globally and become a health crisis faced by humanity since first reported. Radiology imaging technologies such as computer tomography (CT) and chest X-ray imaging (CXR) are effective tools for diagnosing COVID-19. However, in CT and CXR images, the infected area occupies only a small part of the image. Some common deep learning methods that integrate large-scale receptive fields may cause the loss of image detail, resulting in the omission of the region of interest (ROI) in COVID-19 images and are therefore not suitable for further processing. To this end, we propose a deep spatial pyramid pooling (D-SPP) module to integrate contextual information over different resolutions, aiming to extract information under different scales of COVID-19 images effectively. Besides, we propose a COVID-19 infection detection (CID) module to draw attention to the lesion area and remove interference from irrelevant information. Extensive experiments on four CT and CXR datasets have shown that our method produces higher accuracy of detecting COVID-19 lesions in CT and CXR images. It can be used as a computer-aided diagnosis tool to help doctors effectively diagnose and screen for COVID-19.      
### 16.Input Sequence and Parameter Estimation in Impulsive Biomedical Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.01325.pdf)
>  A hybrid model for biomedical time series comprising a continuous second-order linear time-invariant system driven by an input sequence of positively weighted Dirac delta-functions is considered. The problem of the joint estimation of the input sequence and the continuous system parameters from output measurements is investigated. A solution that builds upon and refines a previously published least-squares formulation is proposed. Based on a thorough analysis of the properties of the least-squares solution, improvements in terms of accuracy and ease of use are achieved on synthetic data, compared to the original algorithm.      
### 17.Particle Flow Gaussian Particle Filter  [ :arrow_down: ](https://arxiv.org/pdf/2207.01308.pdf)
>  State estimation in non-linear models is performed by tracking the posterior distribution recursively. A plethora of algorithms have been proposed for this task. Among them, the Gaussian particle filter uses a weighted set of particles to construct a Gaussian approximation to the posterior. In this paper, we propose to use invertible particle flow methods, derived under the Gaussian boundary conditions for a flow equation, to generate a proposal distribution close to the posterior. The resultant particle flow Gaussian particle filter (PFGPF) algorithm retains the asymptotic properties of Gaussian particle filters, with the potential for improved state estimation performance in high dimensional spaces. We compare the performance of PFGPF with the particle flow filters and particle flow particle filters in two challenging numerical simulation examples.      
### 18.Assessing the Performance of Automated Prediction and Ranking of Patient Age from Chest X-rays Against Clinicians  [ :arrow_down: ](https://arxiv.org/pdf/2207.01302.pdf)
>  Understanding the internal physiological changes accompanying the aging process is an important aspect of medical image interpretation, with the expected changes acting as a baseline when reporting abnormal findings. Deep learning has recently been demonstrated to allow the accurate estimation of patient age from chest X-rays, and shows potential as a health indicator and mortality predictor. In this paper we present a novel comparative study of the relative performance of radiologists versus state-of-the-art deep learning models on two tasks: (a) patient age estimation from a single chest X-ray, and (b) ranking of two time-separated images of the same patient by age. We train our models with a heterogeneous database of 1.8M chest X-rays with ground truth patient ages and investigate the limitations on model accuracy imposed by limited training data and image resolution, and demonstrate generalisation performance on public data. To explore the large performance gap between the models and humans on these age-prediction tasks compared with other radiological reporting tasks seen in the literature, we incorporate our age prediction model into a conditional Generative Adversarial Network (cGAN) allowing visualisation of the semantic features identified by the prediction model as significant to age prediction, comparing the identified features with those relied on by clinicians.      
### 19.Virtual Nonholonomic Constraints: A Geometric Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.01299.pdf)
>  Virtual constraints are invariant relations imposed on a control system via feedback as opposed to real physical constraints acting on the system. Nonholonomic systems are mechanical systems with non-integrable constraints on the velocities. In this work, we introduce the notion of virtual nonholonomic constraints in a geometric framework. More precisely, it is a controlled invariant distribution associated with an affine connection mechanical control system. We demonstrate the existence and uniqueness of a control law defining a virtual nonholonomic constraint and we characterize the trajectories of the closed-loop system as solutions of a mechanical system associated with an induced constrained connection. Moreover, we characterize the dynamics for nonholonomic systems in terms of virtual nonholonomic constraints, i.e., we characterize when can we obtain nonholonomic dynamics from virtual nonholonomic constraints.      
### 20.FFCNet: Fourier Transform-Based Frequency Learning and Complex Convolutional Network for Colon Disease Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.01287.pdf)
>  Reliable automatic classification of colonoscopy images is of great significance in assessing the stage of colonic lesions and formulating appropriate treatment plans. However, it is challenging due to uneven brightness, location variability, inter-class similarity, and intra-class dissimilarity, affecting the classification accuracy. To address the above issues, we propose a Fourier-based Frequency Complex Network (FFCNet) for colon disease classification in this study. Specifically, FFCNet is a novel complex network that enables the combination of complex convolutional networks with frequency learning to overcome the loss of phase information caused by real convolution operations. Also, our Fourier transform transfers the average brightness of an image to a point in the spectrum (the DC component), alleviating the effects of uneven brightness by decoupling image content and brightness. Moreover, the image patch scrambling module in FFCNet generates random local spectral blocks, empowering the network to learn long-range and local diseasespecific features and improving the discriminative ability of hard samples. We evaluated the proposed FFCNet on an in-house dataset with 2568 colonoscopy images, showing our method achieves high performance outperforming previous state-of-the art methods with an accuracy of 86:35% and an accuracy of 4.46% higher than the backbone. The project page with code is available at <a class="link-external link-https" href="https://github.com/soleilssss/FFCNet" rel="external noopener nofollow">this https URL</a>.      
### 21.Towards Cell-Free Massive MIMO: A Measurement-Based Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.01280.pdf)
>  Cell-free widely distributed massive multiple-input multiple-output (MIMO) systems utilize radio units spread out over a large geographical area. The radio signal of a user equipment (UE) is coherently detected by a subset of radio units (RUs) in the vicinity of the UE and processed jointly at the nearest baseband processing unit (BPU). This architecture promises two orders of magnitude less transmit power, spatial focusing at the UE position for high reliability, and consistent throughput over the coverage area. All these properties have been investigated so far from a theoretical point of view. To the best of our knowledge, this work presents the first empirical radio wave propagation measurements in the form of time-variant channel transfer functions for a linear, widely distributed antenna array with 32 single antenna RUs spread out over a range of 46.5 m. The large aperture allows for valuable insights into the propagation characteristics of cell-free systems. Three different co-located and widely distributed RU configurations and their properties in an urban environment are analyzed in terms of time-variant delay-spread, Doppler spread, path loss and the correlation of the local scattering function over space. For the development of 6G cell-free massive MIMO transceiver algorithms, we analyze properties such as channel hardening, channel aging as well as the signal to interference and noise ratio (SINR). Our empirical evidence supports the promising claims for widely distributed cell-free systems.      
### 22.Masked Self-Supervision for Remaining Useful Lifetime Prediction in Machine Tools  [ :arrow_down: ](https://arxiv.org/pdf/2207.01219.pdf)
>  Prediction of Remaining Useful Lifetime(RUL) in the modern manufacturing and automation workplace for machines and tools is essential in Industry 4.0. This is clearly evident as continuous tool wear, or worse, sudden machine breakdown will lead to various manufacturing failures which would clearly cause economic loss. With the availability of deep learning approaches, the great potential and prospect of utilizing these for RUL prediction have resulted in several models which are designed driven by operation data of manufacturing machines. Current efforts in these which are based on fully-supervised models heavily rely on the data labeled with their RULs. However, the required RUL prediction data (i.e. the annotated and labeled data from faulty and/or degraded machines) can only be obtained after the machine breakdown occurs. The scarcity of broken machines in the modern manufacturing and automation workplace in real-world situations increases the difficulty of getting sufficient annotated and labeled data. In contrast, the data from healthy machines is much easier to be collected. Noting this challenge and the potential for improved effectiveness and applicability, we thus propose (and also fully develop) a method based on the idea of masked autoencoders which will utilize unlabeled data to do self-supervision. In thus the work here, a noteworthy masked self-supervised learning approach is developed and utilized. This is designed to seek to build a deep learning model for RUL prediction by utilizing unlabeled data. The experiments to verify the effectiveness of this development are implemented on the C-MAPSS datasets (which are collected from the data from the NASA turbofan engine). The results rather clearly show that our development and approach here perform better, in both accuracy and effectiveness, for RUL prediction when compared with approaches utilizing a fully-supervised model.      
### 23.CAM/CAD Point Cloud Part Segmentation via Few-Shot Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.01218.pdf)
>  3D part segmentation is an essential step in advanced CAM/CAD workflow. Precise 3D segmentation contributes to lower defective rate of work-pieces produced by the manufacturing equipment (such as computer controlled CNCs), thereby improving work efficiency and attaining the attendant economic benefits. A large class of existing works on 3D model segmentation are mostly based on fully-supervised learning, which trains the AI models with large, annotated datasets. However, the disadvantage is that the resulting models from the fully-supervised learning methodology are highly reliant on the completeness of the available dataset, and its generalization ability is relatively poor to new unknown segmentation types (i.e. further additional novel classes). In this work, we propose and develop a noteworthy few-shot learning-based approach for effective part segmentation in CAM/CAD; and this is designed to significantly enhance its generalization ability and flexibly adapt to new segmentation tasks by using only relatively rather few samples. As a result, it not only reduces the requirements for the usually unattainable and exhaustive completeness of supervision datasets, but also improves the flexibility for real-world applications. As further improvement and innovation, we additionally adopt the transform net and the center loss block in the network. These characteristics serve to improve the comprehension for 3D features of the various possible instances of the whole work-piece and ensure the close distribution of the same class in feature space. Moreover, our approach stores data in the point cloud format that reduces space consumption, and which also makes the various procedures involved have significantly easier read and edit access (thus improving efficiency and effectiveness and lowering costs).      
### 24.Reusing the H.264/AVC deblocking filter for efficient spatio-temporal prediction in video coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.01210.pdf)
>  The prediction step is a very important part of hybrid video codecs for effectively compressing video sequences. While existing video codecs predict either in temporal or in spatial direction only, the compression efficiency can be increased by a combined spatio-temporal prediction. In this paper we propose an algorithm for reusing the H.264/AVC deblocking filter for spatio-temporal prediction. Reusing this highly op timized filter allows for a very low computational complexity of this prediction mode and an average rate reduction of up to 7.2% can be achieved.      
### 25.Multiple Selection Approximation for Improved Spatio-Temporal Prediction in Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.01207.pdf)
>  In this contribution, a novel spatio-temporal prediction algorithm for video coding is introduced. This algorithm exploits temporal as well as spatial redundancies for effectively predicting the signal to be encoded. To achieve this, the algorithm operates in two stages. Initially, motion compensated prediction is applied on the block being encoded. Afterwards this preliminary temporal prediction is refined by forming a joint model of the initial predictor and the spatially adjacent already transmitted blocks. The novel algorithm is able to outperform earlier refinement algorithms in speed and prediction quality. Compared to pure motion compensated prediction, the mean data rate can be reduced by up to 15% and up to 1.16 dB gain in PSNR can be achieved for the considered sequences.      
### 26.Fast orthogonality deficiency compensation for improved frequency selective image extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2207.01205.pdf)
>  The purpose of this paper is to introduce a very efficient algorithm for signal extrapolation. It can widely be used in many applications in image and video communication, e. g. for concealment of block errors caused by transmission errors or for prediction in video coding. The signal extrapolation is performed by extending a signal from a limited number of known samples into areas beyond these samples. Therefore a finite set of orthogonal basis functions is used and the known part of the signal is projected onto them. Since the basis functions are not orthogonal regarding the area of the known samples, the projection does not lead to the real portion a basis function has of the signal. The proposed algorithm efficiently copes with this non-orthogonality resulting in very good objective and visual extrapolation results for edges, smooth areas, as well as structured areas. Compared to an existent implementation, this algorithm has a significantly lower computational complexity without any degradation in quality. The processing time can be reduced by a factor larger than 100.      
### 27.An Extendable Maneuver Management Framework with Fault-Tolerant Mechanism for Vehicle Platoon Control System in Highway Scenario  [ :arrow_down: ](https://arxiv.org/pdf/2207.01167.pdf)
>  Vehicle platoon often face the problem of lack of scalability of maneuvers in practical applications. Once a new scenario is added, the original program may no longer be available. To deal with this problem, this paper introduces a two-dimensional maneuver management framework with a fault-tolerant mechanism on the basis of the proposed hierarchical architecture for the platoon control system. Maneuvers and roles are two dimensions, based on which the management strategies are decoupled. This makes each vehicle in the platoon has the ability to execute management strategies of various maneuvers and the new maneuver could be extended without revising the existing part. The fault-tolerant mechanism is designed as a maneuver triggered by hardware failures to keep safe before taking over. Furthermore, three typical maneuvers are selected for case studies to illustrate how the management strategies in this framework work. Finally, a comprehensive simulation scenario integrating different maneuvers is designed and a real-world implementation using micro-vehicles is conducted. Results show that the propose two-dimensional framework could effectively deal with various maneuvers and satisfy the computational real-time requirements      
### 28.Data-driven design of explicit predictive controllers using model-based priors  [ :arrow_down: ](https://arxiv.org/pdf/2207.01148.pdf)
>  In this paper, we propose a data-driven approach to derive explicit predictive control laws, without requiring any intermediate identification step. The keystone of the presented strategy is the exploitation of available priors on the control law, coming from model-based analysis. Specifically, by leveraging on the knowledge that the optimal predictive controller is expressed as a piecewise affine (PWA) law, we directly optimize the parameters of such an analytical controller from data, instead of running an on-line optimization problem. As the proposed method allows us to automatically retrieve also a model of the closed-loop system, we show that we can apply model-based techniques to perform a stability check prior to the controller deployment. The effectiveness of the proposed strategy is assessed on two benchmark simulation examples, through which we also discuss the use of regularization and its combination with averaging techniques to handle the presence of noise.      
### 29.Learning Noise with Generative Adversarial Networks: Explorations with Classical Random Process Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.01110.pdf)
>  Random noise arising from physical processes is an inherent characteristic of measurements and a limiting factor for most signal processing tasks. Given the recent interest in generative adversarial networks (GANs) for data-driven signal modeling, it is important to determine to what extent GANs can faithfully reproduce noise in target data sets. In this paper, we present an empirical investigation that aims to shed light on this issue for time series. Namely, we examine the ability of two general-purpose time-series GANs, a direct time-series model and an image-based model using a short-time Fourier transform (STFT) representation, to learn a broad range of noise types commonly encountered in electronics and communication systems: band-limited thermal noise, power law noise, shot noise, and impulsive noise. We find that GANs are capable of learning many noise types, although they predictably struggle when the GAN architecture is not well suited to some aspects of the noise, e.g., impulsive time-series with extreme outliers. Our findings provide insights into the capabilities and potential limitations of current approaches to time-series GANs and highlight areas for further research. In addition, our battery of tests provides a useful benchmark to aid the development of deep generative models for time series.      
### 30.Transfer functions of FXLMS-based Multi-channel Multi-tone Active Noise Equalizers  [ :arrow_down: ](https://arxiv.org/pdf/2207.01102.pdf)
>  Multi-channel Multi-tone Active Noise Equalizers can achieve different user-selected noise spectrum profiles even at different space positions. They can apply a different equalization factor at each noise frequency component and each control point. Theoretically, the value of the transfer function at the frequencies where the noise signal has energy is determined by the equalizer configuration. In this work, we show how to calculate these transfer functions with a double aim: to verify that at the frequencies of interest the values imposed by the equalizer settings are obtained, and to characterize the behavior of these transfer functions in the rest of the spectrum, as well as to get clues to predict the convergence behaviour of the algorithm. The information provided thanks to these transfer functions serves as a practical alternative to the cumbersome statistical analysis of convergence, whose results are often of no practical use.      
### 31.A Novel Low Complexity High Resolution Spectrum Hole Detection Technique for Cognitive Radio  [ :arrow_down: ](https://arxiv.org/pdf/2207.01098.pdf)
>  Cognitive radio is a potential solution to meet the upcoming spectrum crunch issue. In a cognitive radio, spectrum holes can be identified using spectrum sensing techniques. A high resolution spectrum hole detection can ensure even the smallest inactive portion in the spectrum is efficiently utilized. In this paper, a spectrum hole detection technique is proposed in which coarse sensing is done initially so as to detect occupied channels simultaneously. Spectrum holes in the occupied band can be efficiently detected using a fine sensing method. A two stage Frequency Response Masking (FRM) filter sandwiched between two Pascal structure based sampling rate converters results in arbitrary variation of bandwidth. This arbitrary variation of bandwidth can be utilized for fine sensing the spectrum such that the spectrum holes can be detected with high resolution. In the proposed method, high resolution in spectrum hole detection can be achieved without increasing the hardware complexity of the design. The hardware complexity of the proposed method is compared with the state of the art and is found to be significantly less      
### 32.Patient-specific modelling, simulation and real time processing for constrictive respiratory diseases  [ :arrow_down: ](https://arxiv.org/pdf/2207.01082.pdf)
>  Asthma is a common chronic disease of the respiratory system causing significant disability and societal burden. It affects over 500 million people worldwide and generates costs exceeding $USD 56 billion in 2011 in the United States. Managing asthma involves controlling symptoms, preventing exacerbations, and maintaining lung function. Improving asthma control affects the daily life of patients and is associated with a reduced risk of exacerbations and lung function impairment, reduces the cost of asthma care and indirect costs associated with reduced productivity. Understanding the complex dynamics of the pulmonary system and the lung's response to disease, injury, and treatment is fundamental to the advancement of Asthma treatment. Computational models of the respiratory system seek to provide a theoretical framework to understand the interaction between structure and function. Their application can improve pulmonary medicine by a patient-specific approach to medicinal methodologies optimizing the delivery given the personalized geometry and personalized ventilation patterns while introducing a patient-specific technique that maximizes drug delivery. A three-fold objective addressed within this dissertation becomes prominent at this point. The first part refers to the comprehension of pulmonary pathophysiology and the mechanics of Asthma and subsequently of constrictive pulmonary conditions in general. The second part refers to the design and implementation of tools that facilitate personalized medicine to improve delivery and effectiveness. Finally, the third part refers to the self-management of the condition, meaning that medical personnel and patients have access to tools and methods that allow the first party to easily track the course of the condition and the second party, i.e. the patient to easily self-manage it alleviating the significant burden from the health system.      
### 33.Training Patch Analysis and Mining Skills for Image Restoration Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.01075.pdf)
>  There have been numerous image restoration methods based on deep convolutional neural networks (CNNs). However, most of the literature on this topic focused on the network architecture and loss functions, while less detailed on the training methods. Hence, some of the works are not easily reproducible because it is required to know the hidden training skills to obtain the same results. To be specific with the training dataset, few works discussed how to prepare and order the training image patches. Moreover, it requires a high cost to capture new datasets to train a restoration network for the real-world scene. Hence, we believe it is necessary to study the preparation and selection of training data. In this regard, we present an analysis of the training patches and explore the consequences of different patch extraction methods. Eventually, we propose a guideline for the patch extraction from given training images.      
### 34.Variational Deep Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2207.01074.pdf)
>  This paper presents a new variational inference framework for image restoration and a convolutional neural network (CNN) structure that can solve the restoration problems described by the proposed framework. Earlier CNN-based image restoration methods primarily focused on network architecture design or training strategy with non-blind scenarios where the degradation models are known or assumed. For a step closer to real-world applications, CNNs are also blindly trained with the whole dataset, including diverse degradations. However, the conditional distribution of a high-quality image given a diversely degraded one is too complicated to be learned by a single CNN. Therefore, there have also been some methods that provide additional prior information to train a CNN. Unlike previous approaches, we focus more on the objective of restoration based on the Bayesian perspective and how to reformulate the objective. Specifically, our method relaxes the original posterior inference problem to better manageable sub-problems and thus behaves like a divide-and-conquer scheme. As a result, the proposed framework boosts the performance of several restoration problems compared to the previous ones. Specifically, our method delivers state-of-the-art performance on Gaussian denoising, real-world noise reduction, blind image super-resolution, and JPEG compression artifacts reduction.      
### 35.DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2207.01063.pdf)
>  The majority of current TTS datasets, which are collections of individual utterances, contain few conversational aspects in terms of both style and metadata. In this paper, we introduce DailyTalk, a high-quality conversational speech dataset designed for Text-to-Speech. We sampled, modified, and recorded 2,541 dialogues from the open-domain dialogue dataset DailyDialog which are adequately long to represent context of each dialogue. During the data construction step, we maintained attributes distribution originally annotated in DailyDialog to support diverse dialogue in DailyTalk. On top of our dataset, we extend prior work as our baseline, where a non-autoregressive TTS is conditioned on historical information in a dialog. We gather metadata so that a TTS model can learn historical dialog information, the key to generating context-aware speech. From the baseline experiment results, we show that DailyTalk can be used to train neural text-to-speech models, and our baseline can represent contextual information. The DailyTalk dataset and baseline code are freely available for academic use with CC-BY-SA 4.0 license.      
### 36.A 16-Channel Low-Power Neural Connectivity Extraction and Phase-Locked Deep Brain Stimulation SoC  [ :arrow_down: ](https://arxiv.org/pdf/2207.01060.pdf)
>  Growing evidence suggests that phase-locked deep brain stimulation (DBS) can effectively regulate abnormal brain connectivity in neurological and psychiatric disorders. This letter therefore presents a low-power SoC with both neural connectivity extraction and phase-locked DBS capabilities. A 16-channel low-noise analog front-end (AFE) records local field potentials (LFPs) from multiple brain regions with precise gain matching. A novel low-complexity phase estimator and neural connectivity processor subsequently enable energy-efficient, yet accurate measurement of the instantaneous phase and cross-regional synchrony measures. Through flexible combination of neural biomarkers such as phase synchrony and spectral energy, a four-channel charge-balanced neurostimulator is triggered to treat various pathological brain conditions. Fabricated in 65nm CMOS, the SoC occupies a silicon area of 2.24mm2 and consumes 60uW, achieving over 60% power saving in neural connectivity extraction compared to the state-of-the-art. Extensive in-vivo measurements demonstrate multi-channel LFP recording, real-time extraction of phase and neural connectivity measures, and phase-locked stimulation in rats.      
### 37.Leveraging Acoustic Contextual Representation by Audio-textual Cross-modal Learning for Conversational ASR  [ :arrow_down: ](https://arxiv.org/pdf/2207.01039.pdf)
>  Leveraging context information is an intuitive idea to improve performance on conversational automatic speech recognition(ASR). Previous works usually adopt recognized hypotheses of historical utterances as preceding context, which may bias the current recognized hypothesis due to the inevitable historicalrecognition errors. To avoid this problem, we propose an audio-textual cross-modal representation extractor to learn contextual representations directly from preceding speech. Specifically, it consists of two modal-related encoders, extracting high-level latent features from speech and the corresponding text, and a cross-modal encoder, which aims to learn the correlation between speech and text. We randomly mask some input tokens and input sequences of each modality. Then a token-missing or modal-missing prediction with a modal-level CTC loss on the cross-modal encoder is performed. Thus, the model captures not only the bi-directional context dependencies in a specific modality but also relationships between different modalities. Then, during the training of the conversational ASR system, the extractor will be frozen to extract the textual representation of preceding speech, while such representation is used as context fed to the ASR decoder through attention mechanism. The effectiveness of the proposed approach is validated on several Mandarin conversation corpora and the highest character error rate (CER) reduction up to 16% is achieved on the MagicData dataset.      
### 38.Facial Image Reconstruction from Functional Magnetic Resonance Imaging via GAN Inversion with Improved Attribute Consistency  [ :arrow_down: ](https://arxiv.org/pdf/2207.01011.pdf)
>  Neuroscience studies have revealed that the brain encodes visual content and embeds information in neural activity. Recently, deep learning techniques have facilitated attempts to address visual reconstructions by mapping brain activity to image stimuli using generative adversarial networks (GANs). However, none of these studies have considered the semantic meaning of latent code in image space. Omitting semantic information could potentially limit the performance. In this study, we propose a new framework to reconstruct facial images from functional Magnetic Resonance Imaging (fMRI) data. With this framework, the GAN inversion is first applied to train an image encoder to extract latent codes in image space, which are then bridged to fMRI data using linear transformation. Following the attributes identified from fMRI data using an attribute classifier, the direction in which to manipulate attributes is decided and the attribute manipulator adjusts the latent code to improve the consistency between the seen image and the reconstructed image. Our experimental results suggest that the proposed framework accomplishes two goals: (1) reconstructing clear facial images from fMRI data and (2) maintaining the consistency of semantic characteristics.      
### 39.Closed Form Expressions of the Nonlinear Interference for UWB Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.01001.pdf)
>  We present a comprehensive closed-form GN/EGN model supporting ultra-wide-band systems spanning 50 THz of optical bandwidth. We show a case-study of 10x100km of SMF where we gradually increase the number of channels across the C,L,S,U,E bands while optimizing launch power.      
### 40.PS$^2$F: Polarized Spiral Point Spread Function for Single-Shot 3D Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.00945.pdf)
>  We propose a compact snapshot monocular depth estimation technique that relies on an engineered point spread function (PSF). Traditional approaches used in microscopic super-resolution imaging, such as the Double-Helix PSF (DHPSF), are ill-suited for scenes that are more complex than a sparse set of point light sources. We show, using the Cramér-Rao lower bound (CRLB), that separating the two lobes of the DHPSF and thereby capturing two separate images leads to a dramatic increase in depth accuracy. A unique property of the phase mask used for generating the DHPSF is that a separation of the phase mask into two halves leads to a spatial separation of the two lobes. We leverage this property to build a compact polarization-based optical setup, where we place two orthogonal linear polarizers on each half of the DHPSF phase mask and then capture the resulting image with a polarization sensitive camera. Results from simulations and a lab prototype demonstrate that our technique achieves up to $50\%$ lower depth error compared to state-of-the-art designs including the DHPSF, and the Tetrapod PSF, with little to no loss in spatial resolution.      
### 41.A Graph Isomorphism Network with Weighted Multiple Aggregators for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.00940.pdf)
>  Speech emotion recognition (SER) is an essential part of human-computer interaction. In this paper, we propose an SER network based on a Graph Isomorphism Network with Weighted Multiple Aggregators (WMA-GIN), which can effectively handle the problem of information confusion when neighbour nodes' features are aggregated together in GIN structure. Moreover, a Full-Adjacent (FA) layer is adopted for alleviating the over-squashing problem, which is existed in all Graph Neural Network (GNN) structures, including GIN. Furthermore, a multi-phase attention mechanism and multi-loss training strategy are employed to avoid missing the useful emotional information in the stacked WMA-GIN layers. We evaluated the performance of our proposed WMA-GIN on the popular IEMOCAP dataset. The experimental results show that WMA-GIN outperforms other GNN-based methods and is comparable to some advanced non-graph-based methods by achieving 72.48% of weighted accuracy (WA) and 67.72% of unweighted accuracy (UA).      
### 42.Multi-Octave Interference Detectors with Sub-Microsecond Response  [ :arrow_down: ](https://arxiv.org/pdf/2207.00937.pdf)
>  High-power interferers are one of the main hurdles in wideband communication channels. To that end, this paper presents a wideband interferer detection method. The presented technique operates by sampling the incoming signal as an input, and produces the frequency and the power readings of the detected interferer. The detection method relies on driving an open circuit stub, where the voltage is proportional to the power of the interferer, and the standing wave pattern is an indicator of its frequency. This approach is feasible over multi-octave bandwidth with a wide power dynamic range. The concept is analyzed for design and optimization, and a prototype is built for a proof-of-concept. The measured results demonstrate the ability to detect an interferer within the 1--16 GHz frequency range, with a power dynamic range between -20 to 20 dBm. The detection concept is also fitted with different types of tunable bandstop filters (BSFs) for automatic detection and suppression of the interferer if its power exceeds a programmable threshold. With a measured response time of 500 ns, the presented method is a technology enabler for wideband receivers.      
### 43.Doubly-Iterative Sparsified MMSE Turbo Equalization for OTFS Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00866.pdf)
>  Currently, orthogonal time frequency space (OTFS) modulation has drawn much attention to reliable communications in high-mobility scenarios. This paper proposes a doubly-iterative sparsified minimum mean square error (DI-S-MMSE) turbo equalizer, which iteratively exchanges the extrinsic information between a soft-input-soft-input (SISO) MMSE estimator and a SISO decoder. Our proposed equalizer does not suffer from short loops and approaches the performance of the near-optimal symbol-wise maximum a posteriori (MAP) algorithm. To exploit the inherent sparsity of OTFS system, we resort to graph theory to investigate the sparsity pattern of the channel matrix, and propose two sparsification guidelines to reduce the complexity of calculating the matrix inverse at the MMSE estimator. Then, we apply two iterative algorithms to MMSE estimation, i.e., the Generalized Minimal Residual (GMRES) and Factorized Sparse Approximate Inverse (FSPAI) algorithms. The former is used at the initial turbo iteration, whose global convergence is proven in our equalizer, while the latter is used at the subsequent turbo iterations with the help of our proposed guidelines. Simulation results demonstrate that our equalizer has a linear order of complexity while the performance loss incurred by the sparsification is only 0.2 dB at $10^{-4}$ bit error rate.      
### 44.Precision Data-enabled Koopman-type Inverse Operators for Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.00855.pdf)
>  The advent of easy access to large amount of data has sparked interest in directly developing the relationships between input and output of dynamic systems. A challenge is that in addition to the applied input and the measured output, the dynamics can also depend on hidden states that are not directly measured. The main contribution of this work is to identify the information needed (in particular, the past history of the output) to remove the hidden state dependence in Koopman-type inverse operators for linear systems. Additionally, it is shown that the time history of the output should be augmented with the instantaneous time derivatives of the output to achieve precision of the inverse operator. This insight into the required output (history and instantaneous derivative) information, to remove the hidden-state dependence and improve the precision of data-enabled inverse operators, is illustrated with an example system.      
### 45.Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.00844.pdf)
>  Medical image synthesis has attracted increasing attention because it could generate missing image data, improving diagnosis and benefits many downstream tasks. However, so far the developed synthesis model is not adaptive to unseen data distribution that presents domain shift, limiting its applicability in clinical routine. This work focuses on exploring domain adaptation (DA) of 3D image-to-image synthesis models. First, we highlight the technical difference in DA between classification, segmentation and synthesis models. Second, we present a novel efficient adaptation approach based on 2D variational autoencoder which approximates 3D distributions. Third, we present empirical studies on the effect of the amount of adaptation data and the key hyper-parameters. Our results show that the proposed approach can significantly improve the synthesis accuracy on unseen domains in a 3D setting. The code is publicly available at <a class="link-external link-https" href="https://github.com/WinstonHuTiger/2D_VAE_UDA_for_3D_sythesis" rel="external noopener nofollow">this https URL</a>      
### 46.Safe Reinforcement Learning for a Robot Being Pursued but with Objectives Covering More Than Capture-avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2207.00842.pdf)
>  Reinforcement Learning (RL) algorithms show amazing performance in recent years, but placing RL in real-world applications such as self-driven vehicles may suffer safety problems. A self-driven vehicle moving to a target position following a learned policy may suffer a vehicle with unpredictable aggressive behaviors or even being pursued by a vehicle following a Nash strategy. To address the safety issue of the self-driven vehicle in this scenario, this paper conducts a preliminary study based on a system of robots. A safe RL framework with safety guarantees is developed for a robot being pursued but with objectives covering more than capture-avoidance. Simulations and experiments are conducted based on the system of robots to evaluate the effectiveness of the developed safe RL framework.      
### 47.CRB for a Generic Near-Field Positioning System Using Three Electric Field Types  [ :arrow_down: ](https://arxiv.org/pdf/2207.00799.pdf)
>  The use of larger antenna arrays at higher frequency bands is envisioned in the beyond 5G networks. This takes advantage of the near-field propagation regime where the wavefront is no longer plane but spherical, bringing both new opportunities and challenges for the high-precision positioning. In this paper, a generic near-field positioning model with different observation capabilities for three electric fields (vector, scalar and overall scalar electric field) is proposed. For these three electric field types, the Cramér-Rao bound (CRB) is adopted to evaluate the achievable estimation accuracy. The expressions of the CRBs using different electric field observations are derived by combining electromagnetic theory with estimation theory. Closed-form expressions can be further obtained if the terminal is located on the central perpendicular line (CPL) of the receiving antenna surface. In addition, the above discussions are extended to the system with multiple distributed receiving antennas under the CPL assumption. The CRBs using various electric fields in this case are derived and the effect of different numbers of receiving antennas on estimation accuracy is investigated. Numerical results are provided to quantify the CRBs and validate the analytical results. Also, the impact of various system parameters, including different electric fields and multiple antennas, on the near-field positioning performance is evaluated.      
### 48.Computational and experimental analysis of the impact of a sphere on a beam and the resulting modal energy distribution  [ :arrow_down: ](https://arxiv.org/pdf/2207.00795.pdf)
>  We consider the common problem setting of an elastic sphere impacting on a flexible beam. In contrast to previous studies, we analyze the modal energy distribution induced by the impact, having in mind the particular application of impact vibration absorbers. Also, the beam is analyzed in the clamped-clamped configuration, in addition to the free-free configuration usually considered. We demonstrate that the designed test rig permits to obtain well-repeatable measurements. The measurements are confronted with predictions obtained using two different approaches, state-of-the-art Finite Element Analysis and a recently developed computational approach involving a reduced-order model. The innovative aspect of the latter approach is to achieve a massless contact boundary using component mode synthesis, which reduces the mathematical model order and numerical oscillations. We show that the novel computational approach reduces the numerical effort by 3-4 orders of magnitude compared to state-of-the-art Finite Element Analysis, without compromising the excellent agreement with the measurements.      
### 49.Prediction and validation of the strongly modulated forced response of two beams undergoing frictional impacts  [ :arrow_down: ](https://arxiv.org/pdf/2207.00793.pdf)
>  We consider two cantilevered beams undergoing frictional impacts at the free end. The beams are designed to be of similar geometry so that they have distinct but close natural frequencies. Under harmonic base excitation near the primary resonance with the higher-frequency fundamental bending mode, the system shows a strongly modulated non-periodic response. The purpose of this work is to analyze to what extent the non-periodic vibro-impact dynamics can be predicted. To this end, we use a recently developed modeling and simulation approach. The approach relies on component mode synthesis, the massless boundary concept and an appropriate time stepping scheme. Unilateral contact and dry friction are modeled as set-valued laws and imposed locally within the spatially resolved contact area. A linear model updating is carried out based on the natural frequencies and damping ratios identified in the regime without impacts. The nonlinear simulation of the steady-state response to forward and backward stepped sine excitation is compared against measurements. The results are in very good agreement, especially in the light of the uncertainty associated with the observed material loss in the contact region and the nonlinear behavior of the clamping.      
### 50.Environment Sensing Considering the Occlusion Effect: A Multi-View Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.00776.pdf)
>  In this paper, we consider the problem of sensing the environment within a wireless cellular framework. Specifically, multiple user equipments (UEs) send sounding signals to one or multiple base stations (BSs) and then a centralized processor retrieves the environmental information from all the channel information obtained at the BS(s). Taking into account the occlusion effect that is common in the wireless context, we make full use of the different views of the environment from different users and/or BS(s), and propose an effective sensing algorithm called GAMP-MVSVR (generalized-approximate-message-passing-based multi-view sparse vector reconstruction). In the proposed algorithm, a multi-layer factor graph is constructed to iteratively estimate the scattering coefficients of the cloud points and their occlusion relationship. In each iteration, the occlusion relationship between the cloud points of the sparse environment is recalculated according to a simple occlusion detection rule, and in turn, used to estimate the scattering coefficients of the cloud points. Our proposed algorithm can achieve improved sensing performance with multi-BS collaboration in addition to the multi-views from the UEs. The simulation results verify its convergence and effectiveness.      
### 51.Computer-assisted Pronunciation Training -- Speech synthesis is almost all you need  [ :arrow_down: ](https://arxiv.org/pdf/2207.00774.pdf)
>  The research community has long studied computer-assisted pronunciation training (CAPT) methods in non-native speech. Researchers focused on studying various model architectures, such as Bayesian networks and deep learning methods, as well as on the analysis of different representations of the speech signal. Despite significant progress in recent years, existing CAPT methods are not able to detect pronunciation errors with high accuracy (only 60\% precision at 40\%-80\% recall). One of the key problems is the low availability of mispronounced speech that is needed for the reliable training of pronunciation error detection models. If we had a generative model that could mimic non-native speech and produce any amount of training data, then the task of detecting pronunciation errors would be much easier. We present three innovative techniques based on phoneme-to-phoneme (P2P), text-to-speech (T2S), and speech-to-speech (S2S) conversion to generate correctly pronounced and mispronounced synthetic speech. We show that these techniques not only improve the accuracy of three machine learning models for detecting pronunciation errors but also help establish a new state-of-the-art in the field. Earlier studies have used simple speech generation techniques such as P2P conversion, but only as an additional mechanism to improve the accuracy of pronunciation error detection. We, on the other hand, consider speech generation to be the first-class method of detecting pronunciation errors. The effectiveness of these techniques is assessed in the tasks of detecting pronunciation and lexical stress errors. Non-native English speech corpora of German, Italian, and Polish speakers are used in the evaluations. The best proposed S2S technique improves the accuracy of detecting pronunciation errors in AUC metric by 41\% from 0.528 to 0.749 compared to the state-of-the-art approach.      
### 52.Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift  [ :arrow_down: ](https://arxiv.org/pdf/2207.00769.pdf)
>  Class distribution plays an important role in learning deep classifiers. When the proportion of each class in the test set differs from the training set, the performance of classification nets usually degrades. Such a label distribution shift problem is common in medical diagnosis since the prevalence of disease vary over location and time. In this paper, we propose the first method to tackle label shift for medical image classification, which effectively adapt the model learned from a single training label distribution to arbitrary unknown test label distribution. Our approach innovates distribution calibration to learn multiple representative classifiers, which are capable of handling different one-dominating-class distributions. When given a test image, the diverse classifiers are dynamically aggregated via the consistency-driven test-time adaptation, to deal with the unknown test label distribution. We validate our method on two important medical image classification tasks including liver fibrosis staging and COVID-19 severity prediction. Our experiments clearly show the decreased model performance under label shift. With our method, model performance significantly improves on all the test datasets with different label shifts for both medical image diagnosis tasks.      
### 53.Face Image Lighting Enhancement Using a 3D Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.00761.pdf)
>  Image enhancement helps to generate balanced lighting distributions over faces. Our goal is to get an illuminance-balanced enhanced face image from a single view. Traditionally, image enhancement methods ignore the 3D geometry of the face or require a complicated multi-view geometry. Other methods cause color tone shifting or over saturation. Inspired by the new research achievements in face alignment and face 3D modeling, we propose an improved face image enhancement method by leveraging 3D face models. Given a face image as input, our method will first estimate its lighting distribution. Then we build an optimization process to refine the distribution. Finally, we generate an illuminance-balanced face image from a single view. Experiments on the FiveK dataset demonstrate that our method performs well and compares favorably with other methods.      
### 54.A Distributionally Robust Resilience Enhancement Strategy for Distribution Grids Considering Decision-Dependent Contingencies  [ :arrow_down: ](https://arxiv.org/pdf/2207.00741.pdf)
>  When performing resilience enhancement for distribution grids, suboptimal strategies induced by misspecified contingency models may lead to unanticipated regrets in retrospective analyses. However, there are two obstacles for reliably modeling uncertain contingencies: 1) decision-dependent uncertainty (DDU) resulting from different line hardening decisions, and 2) distributional ambiguity due to limited outage information under extreme weather events (EWEs). To address these two challenges, this paper constructs scenario-wise decision-dependent ambiguity sets (SWDD-ASs), where the DDU and distributional ambiguity inherent in EWE-induced contingencies are simultaneously captured under each possible EWE scenario. Then, a two-stage trilevel decision-dependent distributionally robust resilient enhancement (DD-DRRE) model is formulated, whose outputs include the optimal line hardening, distributed generation (DG) allocation, and proactive network reconfiguration strategy under the worst-case distributions in SWDD-ASs. Then, the DD-DRRE model are equivalently recast to a MILP-based master problem and multiple scenario-wise subproblems, facilitating the utilization of a customized column-and-constraint generation (C&amp;CG) algorithm. Finally, numerical tests demonstrate a remarkable improvement in the out-of-sample performance of our model, compared to its prevailing stochastic and robust counterparts. Moreover, the potential values of incorporating the ambiguity and distributional information are quantitatively estimated, which can serve as a useful reference for planners with different budgets and risk-aversion levels.      
### 55.UserLibri: A Dataset for ASR Personalization Using Only Text  [ :arrow_down: ](https://arxiv.org/pdf/2207.00706.pdf)
>  Personalization of speech models on mobile devices (on-device personalization) is an active area of research, but more often than not, mobile devices have more text-only data than paired audio-text data. We explore training a personalized language model on text-only data, used during inference to improve speech recognition performance for that user. We experiment on a user-clustered LibriSpeech corpus, supplemented with personalized text-only data for each user from Project Gutenberg. We release this User-Specific LibriSpeech (UserLibri) dataset to aid future personalization research. LibriSpeech audio-transcript pairs are grouped into 55 users from the test-clean dataset and 52 users from test-other. We are able to lower the average word error rate per user across both sets in streaming and nonstreaming models, including an improvement of 2.5 for the harder set of test-other users when streaming.      
### 56.Uncertainty Quantification for Deep Unrolling-Based Computational Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2207.00698.pdf)
>  Deep unrolling is an emerging deep learning-based image reconstruction methodology that bridges the gap between model-based and purely deep learning-based image reconstruction methods. Although deep unrolling methods achieve state-of-the-art performance for imaging problems and allow the incorporation of the observation model into the reconstruction process, they do not provide any uncertainty information about the reconstructed image, which severely limits their use in practice, especially for safety-critical imaging applications. In this paper, we propose a learning-based image reconstruction framework that incorporates the observation model into the reconstruction task and that is capable of quantifying epistemic and aleatoric uncertainties, based on deep unrolling and Bayesian neural networks. We demonstrate the uncertainty characterization capability of the proposed framework on magnetic resonance imaging and computed tomography reconstruction problems. We investigate the characteristics of the epistemic and aleatoric uncertainty information provided by the proposed framework to motivate future research on utilizing uncertainty information to develop more accurate, robust, trustworthy, uncertainty-aware, learning-based image reconstruction and analysis methods for imaging problems. We show that the proposed framework can provide uncertainty information while achieving comparable reconstruction performance to state-of-the-art deep unrolling methods.      
### 57.Optimal Placement of PV Smart Inverters with Volt-VAr Control in Electric Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.00679.pdf)
>  The high R/X ratio of typical distribution systems makes the system voltage vulnerable to the active power injection from distributed energy resources (DERs). Moreover, the intermittent and uncertain nature of the DER generation brings new challenges to the voltage control. This paper proposes a two-stage stochastic optimization strategy to optimally place the PV smart inverters with Volt-VAr capability for distribution systems with high photovoltaic (PV) penetration to mitigate voltage violation issues. The proposed optimization strategy enables a planningstage guide for upgrading the existing PV inverters to smart inverters with Volt-VAr capability while considering the operationstage characteristics of the Volt-VAr control. One advantage of this planning strategy is that it utilizes the local control capability of the smart inverter that requires no communication, thus avoiding issues related to communication delays and failures. Another advantage is that the Volt-VAr control characteristic is internally integrated into the optimization model as a set of constraints, making placement decisions more accurate. The objective of the optimization is to minimize the upgrading cost and the number of the smart inverters required while maintaining the voltage profile within the acceptable range. Case studies on an actual 12.47kV, 9km long Arizona utility feeder have been conducted using OpenDSS to validate the effectiveness of the proposed placement strategy in both static and dynamic simulations. Index Terms      
### 58.Distributed Frequency Control in Power Grids with Low and Time-Varying Inertia  [ :arrow_down: ](https://arxiv.org/pdf/2207.00677.pdf)
>  This paper presents a distributed frequency control method for power grids with high penetration of inverter-connected resources under low and time-varying inertia due to renewable energy (RE). We provide a distributed virtual inertia (VI) allocation method using the distributed subgradient algorithm. We implement our distributed control strategy under full and sparse communication architectures. The distributed full and sparse communication controllers achieve comparable performance to a centralized controller and stabilize the test system within 6~s. We study the sensitivity of the controller performance to varying objective function weights on phase angle versus frequency deviation, gradient step sizes, and allowed rate of change of inertia (RoCoI) coefficients. We observe that the settling time of the states and the controller performance and effort are susceptible to changes in the gradient step size and objective weights on the frequency and angle deviation. While a higher objective weight on angle versus frequency deviations positively affects their settling times, it negatively impacts controller performance and control effort. The impact of this work is to propose distributed control schemes as new mechanisms that act before or in alignment with primary and secondary control to safely regulate the frequency in future power grids.      
### 59.The Importance of the Instantaneous Phase for classification using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.00672.pdf)
>  Large-scale training of Convolutional Neural Networks (CNN) is extremely demanding in terms of computational resources. Also, for specific applications, the standard use of transfer learning also tends to require far more resources than what may be needed. This work examines the impact of using AM-FM representations as input images for CNN classification applications. A comparison was made between AM-FM components combinations and grayscale images as inputs for reduced and complete networks. The results showed that only the phase component produced significant predictions within a simple network. Neither IA or gray scale image were able to induce any learning in the system. Furthermore, the FM results were 7x faster during training and used 123x less parameters compared to state-of-the-art MobileNetV2 architecture, while maintaining comparable performance (AUC of 0.78 vs 0.79).      
### 60.Speaker Diarization and Identification from Single-Channel Classroom Audio Recording Using Virtual Microphones  [ :arrow_down: ](https://arxiv.org/pdf/2207.00660.pdf)
>  Speaker identification in noisy audio recordings, specifically those from collaborative learning environments, can be extremely challenging. There is a need to identify individual students talking in small groups from other students talking at the same time. To solve the problem, we assume the use of a single microphone per student group without any access to previous large datasets for training. <br>This dissertation proposes a method of speaker identification using cross-correlation patterns associated to an array of virtual microphones, centered around the physical microphone. The virtual microphones are simulated by using approximate speaker geometry observed from a video recording. The patterns are constructed based on estimates of the room impulse responses for each virtual microphone. The correlation patterns are then used to identify the speakers. The proposed method is validated with classroom audios and shown to substantially outperform diarization services provided by Google Cloud and Amazon AWS.      
### 61.FSE Compensated Motion Correction for MRI Using Data Driven Methods  [ :arrow_down: ](https://arxiv.org/pdf/2207.00656.pdf)
>  Magnetic Resonance Imaging (MRI) is a widely used medical imaging modality boasting great soft tissue contrast without ionizing radiation, but unfortunately suffers from long acquisition times. Long scan times can lead to motion artifacts, for example due to bulk patient motion such as head movement and periodic motion produced by the heart or lungs. Motion artifacts can degrade image quality and in some cases render the scans nondiagnostic. To combat this problem, prospective and retrospective motion correction techniques have been introduced. More recently, data driven methods using deep neural networks have been proposed. As a large number of publicly available MRI datasets are based on Fast Spin Echo (FSE) sequences, methods that use them for training should incorporate the correct FSE acquisition dynamics. Unfortunately, when simulating training data, many approaches fail to generate accurate motion-corrupt images by neglecting the effects of the temporal ordering of the k-space lines as well as neglecting the signal decay throughout the FSE echo train. In this work, we highlight this consequence and demonstrate a training method which correctly simulates the data acquisition process of FSE sequences with higher fidelity by including sample ordering and signal decay dynamics. Through numerical experiments, we show that accounting for the FSE acquisition leads to better motion correction performance during inference.      
### 62.An Alternative Method for Solving Security-Constraint Unit Commitment with Neural Network Based Battery Degradation Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.00650.pdf)
>  Battery energy storage system (BESS) can effectively mitigate the uncertainty of variable renewable generation and provide flexible ancillary services. However, degradation is a key concern for rechargeable batteries such as the most widely used Lithium-ion battery. A neural network based battery degradation (NNBD) model can accurately quantify the battery degradation. When incorporating the NNBD model into security-constrained unit commitment (SCUC), we can establish a battery degradation based SCUC (BD-SCUC) model that can consider the equivalent battery degradation cost precisely. However, the BD-SCUC may not be solved directly due to high non-linearity of the NNBD model. To address this issue, the NNBD model is linearized by converting the nonlinear activation function at each neuron into linear constraints, which enables BD-SCUC to become a linearized BD-SCUC (L-BD-SCUC) model. Case studies demonstrate the proposed L-BD-SCUC model can be efficiently solved for multiple BESS buses power system day-ahead scheduling problems with the lowest total cost including the equivalent degradation cost and normal operation cost.      
### 63.Synthesis of General Decoupling Networks Using Transmission Lines  [ :arrow_down: ](https://arxiv.org/pdf/2207.00615.pdf)
>  In this paper, we introduce a synthesis technique for transmission line based decoupling networks, which find application in coupled systems such as multiple-antenna systems and antenna arrays. Employing the generalized $\pi$-network and the transmission line analysis technique, we reduce the decoupling network design into simple matrix calculations. The synthesized decoupling network is essentially a generalized $\pi$-network with transmission lines at all branches. The advantage of this proposed decoupling network is that it can be implemented using transmission lines, ensuring better control on loss, performance consistency and higher power handling capability, when compared with lumped components, and can be easily scaled for operation at different frequencies.      
### 64.Deep Learning for Short-term Instant Energy Consumption Forecasting in the Manufacturing Sector  [ :arrow_down: ](https://arxiv.org/pdf/2207.01595.pdf)
>  Electricity is a volatile power source that requires great planning and resource management for both short and long term. More specifically, in the short-term, accurate instant energy consumption forecasting contributes greatly to improve the efficiency of buildings, opening new avenues for the adoption of renewable energy. In that regard, data-driven approaches, namely the ones based on machine learning, are begin to be preferred over more traditional ones since they provide not only more simplified ways of deployment but also state of the art results. In that sense, this work applies and compares the performance of several deep learning algorithms, LSTM, CNN, mixed CNN-LSTM and TCN, in a real testbed within the manufacturing sector. The experimental results suggest that the TCN is the most reliable method for predicting instant energy consumption in the short-term.      
### 65.Interpretable Fusion Analytics Framework for fMRI Connectivity: Self-Attention Mechanism and Latent Space Item-Response Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.01581.pdf)
>  There have been several attempts to use deep learning based on brain fMRI signals to classify cognitive impairment diseases. However, deep learning is a hidden black box model that makes it difficult to interpret the process of classification. To address this issue, we propose a novel analytical framework that interprets the classification result from deep learning processes. We first derive the region of interest (ROI) functional connectivity network (FCN) by embedding functions based on their similar signal patterns. Then, using the self-attention equipped deep learning model, we classify diseases based on their FCN. Finally, in order to interpret the classification results, we employ a latent space item-response interaction network model to identify the significant functions that exhibit distinct connectivity patterns when compared to other diseases. The application of this proposed framework to the four types of cognitive impairment shows that our approach is valid for determining the significant ROI functions.      
### 66.j-Wave: An open-source differentiable wave simulator  [ :arrow_down: ](https://arxiv.org/pdf/2207.01499.pdf)
>  We present an open-source differentiable acoustic simulator, j-Wave, which can solve time-varying and time-harmonic acoustic problems. It supports automatic differentiation, which is a program transformation technique that has many applications, especially in machine learning and scientific computing. j-Wave is composed of modular components that can be easily customized and reused. At the same time, it is compatible with some of the most popular machine learning libraries, such as JAX and TensorFlow. The accuracy of the simulation results for known configurations is evaluated against the widely used k-Wave toolbox and a cohort of acoustic simulation software. j-Wave is available from <a class="link-external link-https" href="https://github.com/ucl-bug/jwave" rel="external noopener nofollow">this https URL</a>.      
### 67.SmartMask- Developing an automated self-care system  [ :arrow_down: ](https://arxiv.org/pdf/2207.01492.pdf)
>  COVID-19 has changed our world and has filled people with fear and anxiety. Everyone has a fear of coming in contact with people having the Coronavirus. In Spite of releasing full lockdowns, there is still a pressing need to maintain social distancing in the short- to medium-term to control the spread of coronavirus. Due to lack of self discipline or obviously pulling down the mask to get some fresh air, might pose a threat when you come near a person showing COVID symptoms. Abiding to WHO guidelines to avoid touching the mask while wearing it, we propose a wearable device for no contact pulling up of mask on face and additionally to implement social distancing with sensors mounted on the device. The SmartMask will detect if we are in the vicinity of any other person and will pull itself up. With sensors for detecting the closeness of objects around you and prompting you to take a proper action or pull the mask automatically. Along with the automated mask we will incorporate a temperature sensor to check vitals of an individual at all times and give an alert to the peers around him. This will ensure social distancing and help in avoiding spread of the virus.      
### 68.State of the Art of Audio- and Video-Based Solutions for AAL  [ :arrow_down: ](https://arxiv.org/pdf/2207.01487.pdf)
>  The report illustrates the state of the art of the most successful AAL applications and functions based on audio and video data, namely (i) lifelogging and self-monitoring, (ii) remote monitoring of vital signs, (iii) emotional state recognition, (iv) food intake monitoring, activity and behaviour recognition, (v) activity and personal assistance, (vi) gesture recognition, (vii) fall detection and prevention, (viii) mobility assessment and frailty recognition, and (ix) cognitive and motor rehabilitation. For these application scenarios, the report illustrates the state of play in terms of scientific advances, available products and research project. The open challenges are also highlighted.      
### 69.Polarization-Dependent Loss of Optical Connectors Measured with High Accuracy (&lt;0.004 dB) after Cancelation of Polarimetric Errors  [ :arrow_down: ](https://arxiv.org/pdf/2207.01481.pdf)
>  State-of-the-art polarimeter calibration is reviewed. Producing many quasi-random polarization states and moving/bending a fiber without changing power allows finding a polarimeter calibration where the degree-of-polarization reaches unity and parasitic polarization-dependent loss is small. Using a polarization scrambler/transformer and a polarimeter a device-under-test can be characterized. Its Mueller matrix can be decomposed into a product of a nondepolarizing Mueller-Jones matrix times a purely depolarizing Mueller matrix. Test polarizations may drift over time. With help of an optical switch the reference device can be measured against an internal reference path. Later, with possibly different test polarizations, the actual device-under-test is measured against the internal reference. Polarization drift and need for repeated reference device measurement are thus overcome. When a patchcord is inserted, connector PDL can be measured, provided that errors are calibrated away, again by fiber moving/bending. Experimentally we have measured PDL with errors &lt;0.004 dB. This easily suffices to measure connector PDL, which is demonstrated. PDL &gt;60 dB was measured when the device under test was a good polarizer. A 20 Mrad/s polarization scrambler with LiNbO3 device generates the test polarizations. The polarimeter can sample at 100 MHz and can store 64M Stokes vectors. During laser frequency scans Mueller matrices can be measured in time intervals as short as 5 us.      
### 70.Simultaneous Contact-Rich Grasping and Locomotion via Distributed Optimization Enabling Free-Climbing for Multi-Limbed Robots  [ :arrow_down: ](https://arxiv.org/pdf/2207.01418.pdf)
>  While motion planning of locomotion for legged robots has shown great success, motion planning for legged robots with dexterous multi-finger grasping is not mature yet. We present an efficient motion planning framework for simultaneously solving locomotion (e.g., centroidal dynamics), grasping (e.g., patch contact), and contact (e.g., gait) problems. To accelerate the planning process, we propose distributed optimization frameworks based on Alternating Direction Methods of Multipliers (ADMM) to solve the original large-scale Mixed-Integer NonLinear Programming (MINLP). The resulting frameworks use Mixed-Integer Quadratic Programming (MIQP) to solve contact and NonLinear Programming (NLP) to solve nonlinear dynamics, which are more computationally tractable and less sensitive to parameters. Also, we explicitly enforce patch contact constraints from limit surfaces with micro-spine grippers. We demonstrate our proposed framework in the hardware experiments, showing that the multi-limbed robot is able to realize various motions including free-climbing at a slope angle 45° with a much shorter planning time.      
### 71.Controlling the Cascade: Kinematic Planning for N-ball Toss Juggling  [ :arrow_down: ](https://arxiv.org/pdf/2207.01414.pdf)
>  Dynamic movements are ubiquitous in human motor behavior as they tend to be more efficient and can solve a broader range of skill domains than their quasi-static counterparts. For decades, robotic juggling tasks have been among the most frequently studied dynamic manipulation problems since the required dynamic dexterity can be scaled to arbitrarily high difficulty. However, successful approaches have been limited to basic juggling skills, indicating a lack of understanding of the required constraints for dexterous toss juggling. We present a detailed analysis of the toss juggling task, identifying the key challenges and formalizing it as a trajectory optimization problem. Building on our state-of-the-art, real-world toss juggling platform, we reach the theoretical limits of toss juggling in simulation, evaluate a resulting real-time controller in environments of varying difficulty and achieve robust toss juggling of up to 17 balls on two anthropomorphic manipulators.      
### 72.Large-scale Robustness Analysis of Video Action Recognition Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.01398.pdf)
>  We have seen a great progress in video action recognition in recent years. There are several models based on convolutional neural network (CNN) with some recent transformer based approaches which provide state-of-the-art performance on existing benchmark datasets. However, large-scale robustness has not been studied for these models which is a critical aspect for real-world applications. In this work we perform a large-scale robustness analysis of these existing models for video action recognition. We mainly focus on robustness against distribution shifts due to real-world perturbations instead of adversarial perturbations. We propose four different benchmark datasets, HMDB-51P, UCF-101P, Kinetics-400P, and SSv2P and study the robustness of six different state-of-the-art action recognition models against 90 different perturbations. The study reveals some interesting findings, 1) transformer based models are consistently more robust against most of the perturbations when compared with CNN based models, 2) Pretraining helps Transformer based models to be more robust to different perturbations than CNN based models, and 3) All of the studied models are robust to temporal perturbation on the Kinetics dataset, but not on SSv2; this suggests temporal information is much more important for action label prediction on SSv2 datasets than on the Kinetics dataset. We hope that this study will serve as a benchmark for future research in robust video action recognition. More details about the project are available at <a class="link-external link-https" href="https://rose-ar.github.io/" rel="external noopener nofollow">this https URL</a>.      
### 73.Task-oriented Self-supervised Learning for Anomaly Detection in Electroencephalography  [ :arrow_down: ](https://arxiv.org/pdf/2207.01391.pdf)
>  Accurate automated analysis of electroencephalography (EEG) would largely help clinicians effectively monitor and diagnose patients with various brain diseases. Compared to supervised learning with labelled disease EEG data which can train a model to analyze specific diseases but would fail to monitor previously unseen statuses, anomaly detection based on only normal EEGs can detect any potential anomaly in new EEGs. Different from existing anomaly detection strategies which do not consider any property of unavailable abnormal data during model development, a task-oriented self-supervised learning approach is proposed here which makes use of available normal EEGs and expert knowledge about abnormal EEGs to train a more effective feature extractor for the subsequent development of anomaly detector. In addition, a specific two branch convolutional neural network with larger kernels is designed as the feature extractor such that it can more easily extract both larger scale and small-scale features which often appear in unavailable abnormal EEGs. The effectively designed and trained feature extractor has shown to be able to extract better feature representations from EEGs for development of anomaly detector based on normal data and future anomaly detection for new EEGs, as demonstrated on three EEG datasets. The code is available at <a class="link-external link-https" href="https://github.com/ironing/EEG-AD" rel="external noopener nofollow">this https URL</a>.      
### 74.Reaching optimal distributed estimation through myopic self-confidence adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2207.01384.pdf)
>  Consider discrete-time linear distributed averaging dynamics, whereby agents in a network start with uncorrelated and unbiased noisy measurements of a common underlying parameter (state of the world) and iteratively update their estimates following a non-Bayesian rule. Specifically, let every agent update her estimate to a convex combination of her own current estimate and those of her neighbors in the network. As a result of this iterative averaging, each agent obtains an asymptotic estimate of the state of the world, and the variance of this individual estimate depends on the matrix of weights the agents assign to self and to the others. We study a game-theoretic multi-objective optimization problem whereby every agent seeks to choose her self-weight in such a convex combination in a way to minimize the variance of her asymptotic estimate of the state of the unknown parameters. Assuming that the relative influence weights assigned by the agents to their neighbors in the network remain fixed and form an irreducible and aperiodic relative influence matrix, we characterize the Pareto frontier of the problem, as well as the set of Nash equilibria in the resulting game.      
### 75.Safe Reinforcement Learning via Confidence-Based Filters  [ :arrow_down: ](https://arxiv.org/pdf/2207.01337.pdf)
>  Ensuring safety is a crucial challenge when deploying reinforcement learning (RL) to real-world systems. We develop confidence-based safety filters, a control-theoretic approach for certifying state safety constraints for nominal policies learned via standard RL techniques, based on probabilistic dynamics models. Our approach is based on a reformulation of state constraints in terms of cost functions, reducing safety verification to a standard RL task. By exploiting the concept of hallucinating inputs, we extend this formulation to determine a "backup" policy that is safe for the unknown system with high probability. Finally, the nominal policy is minimally adjusted at every time step during a roll-out towards the backup policy, such that safe recovery can be guaranteed afterwards. We provide formal safety guarantees, and empirically demonstrate the effectiveness of our approach.      
### 76.Estimating indoor crowd density and movement behavior using WiFi Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.01313.pdf)
>  The fact that almost every person owns a smartphone device that can be precisely located is both empowering and worrying. If methods for accurate tracking of devices (and their owners) via WiFi probing are developed in a responsible way, they could be applied in many different fields, from data security to urban planning. Numerous approaches to data collection and analysis have been covered, some of which use active sensing equipment, while others rely on passive probing, which takes advantage of nearly universal smartphone usage and WiFi network coverage. In this study, we introduce a system that uses WiFi probing technologies aimed at tracking user locations and understanding individual behavior. We built our own devices to passively capture WiFi request probe packets from smartphones, without the phones being connected to the network. The devices were tested at the headquarters of the research sector of the Elm Company. The results of the analyses carried out to estimate the crowd density in offices and the flows of the crowd from one place to another are promising and illustrate the importance of such solutions in indoor and closed spaces.      
### 77.Optimal control in opinion dynamics models: towards a unified framework  [ :arrow_down: ](https://arxiv.org/pdf/2207.01300.pdf)
>  Understanding how individuals change their opinions is essential to mitigate the harmful effect of fake news and stop opinion polarization. Building upon an extremely flexible opinion dynamics model that can capture the key microscopic mechanisms of social influence, the current paper elaborates a framework to find optimal control, which, thus, can be applied to a broad set of opinion dynamics settings. Using a combination of theoretical and computational approaches, I characterize the properties of optimal control and demonstrates how the elaborated framework can be applied to specific examples, which cover the classical situations that garner remarkable attention in the literature on opinion dynamics models: assimilative influence, bounded confidence, and dissimilative influence.      
### 78.CaTT-KWS: A Multi-stage Customized Keyword Spotting Framework based on Cascaded Transducer-Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2207.01267.pdf)
>  Customized keyword spotting (KWS) has great potential to be deployed on edge devices to achieve hands-free user experience. However, in real applications, false alarm (FA) would be a serious problem for spotting dozens or even hundreds of keywords, which drastically affects user experience. To solve this problem, in this paper, we leverage the recent advances in transducer and transformer based acoustic models and propose a new multi-stage customized KWS framework named Cascaded Transducer-Transformer KWS (CaTT-KWS), which includes a transducer based keyword detector, a frame-level phone predictor based force alignment module and a transformer based decoder. Specifically, the streaming transducer module is used to spot keyword candidates in audio stream. Then force alignment is implemented using the phone posteriors predicted by the phone predictor to finish the first stage keyword verification and refine the time boundaries of keyword. Finally, the transformer decoder further verifies the triggered keyword. Our proposed CaTT-KWS framework reduces FA rate effectively without obviously hurting keyword recognition accuracy. Specifically, we can get impressively 0.13 FA per hour on a challenging dataset, with over 90% relative reduction on FA comparing to the transducer based detection model, while keyword recognition accuracy only drops less than 2%.      
### 79.Minimizing Sequential Confusion Error in Speech Command Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.01261.pdf)
>  Speech command recognition (SCR) has been commonly used on resource constrained devices to achieve hands-free user experience. However, in real applications, confusion among commands with similar pronunciations often happens due to the limited capacity of small models deployed on edge devices, which drastically affects the user experience. In this paper, inspired by the advances of discriminative training in speech recognition, we propose a novel minimize sequential confusion error (MSCE) training criterion particularly for SCR, aiming to alleviate the command confusion problem. Specifically, we aim to improve the ability of discriminating the target command from other commands on the basis of MCE discriminative criteria. We define the likelihood of different commands through connectionist temporal classification (CTC). During training, we propose several strategies to use prior knowledge creating a confusing sequence set for similar-sounding command instead of creating the whole non-target command set, which can better save the training resources and effectively reduce command confusion errors. Specifically, we design and compare three different strategies for confusing set construction. By using our proposed method, we can relatively reduce the False Reject Rate~(FRR) by 33.7% at 0.01 False Alarm Rate~(FAR) and confusion errors by 18.28% on our collected speech command set.      
### 80.TMGAN-PLC: Audio Packet Loss Concealment using Temporal Memory Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.01255.pdf)
>  Real-time communications in packet-switched networks have become widely used in daily communication, while they inevitably suffer from network delays and data losses in constrained real-time conditions. To solve these problems, audio packet loss concealment (PLC) algorithms have been developed to mitigate voice transmission failures by reconstructing the lost information. Limited by the transmission latency and device memory, it is still intractable for PLC to accomplish high-quality voice reconstruction using a relatively small packet buffer. In this paper, we propose a temporal memory generative adversarial network for audio PLC, dubbed TMGAN-PLC, which is comprised of a novel nested-UNet generator and the time-domain/frequency-domain discriminators. Specifically, a combination of the nested-UNet and temporal feature-wise linear modulation is elaborately devised in the generator to finely adjust the intra-frame information and establish inter-frame temporal dependencies. To complement the missing speech content caused by longer loss bursts, we employ multi-stage gated vector quantizers to capture the correct content and reconstruct the near-real smooth audio. Extensive experiments on the PLC Challenge dataset demonstrate that the proposed method yields promising performance in terms of speech quality, intelligibility, and PLCMOS.      
### 81.Active-Passive IRS aided Wireless Communication: New Hybrid Architecture and Elements Allocation Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2207.01244.pdf)
>  Intelligent reflecting surface (IRS) has emerged as a promising technology to enhance the wireless communication network coverage and capacity by dynamically controlling the radio signal propagation environment. In contrast to the existing works that considered active or passive IRS only, we propose in this paper a new hybrid active-passive IRS architecture that consists of both active and passive reflecting elements, thus achieving their combined advantages flexibly. Under a practical channel setup with Rician fading where only the statistical channel state information (CSI) is available, we study the hybrid IRS design in a multi-user communication system. Specifically, we formulate an optimization problem to maximize the achievable ergodic capacity of the worst-case user by designing the hybrid IRS beamforming and active/passive elements allocation based on the statistical CSI, subject to various practical constraints on the active-element amplification factor and amplification power consumption, as well as the total active and passive elements deployment budget. To solve this challenging problem, we first approximate the ergodic capacity in a simpler form and then propose an efficient algorithm to solve the problem optimally. Moreover, we show that for the special case with all channels to be line-of-sight (LoS), only active elements need to be deployed when the total deployment budget is sufficiently small, while both active and passive elements should be deployed with a decreasing number ratio when the budget increases and exceeds a certain threshold. Finally, numerical results are presented which demonstrate the performance gains of the proposed hybrid IRS architecture and its optimal design over the conventional schemes with active/passive IRS only under various practical system setups.      
### 82.Intelligent Reflecting Surface Enabled Multi-Target Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.01230.pdf)
>  Besides improving communication performance, intelligent reflecting surfaces (IRSs) are also promising enablers for achieving larger sensing coverage and enhanced sensing quality. Nevertheless, in the absence of a direct path between the base station (BS) and the targets, multi-target sensing is generally very difficult, since IRSs are incapable of proactively transmitting sensing beams or analyzing target information. Moreover, the echoes of different targets reflected via the IRS-established virtual links share the same directionality at the BS. In this paper, we study a wireless system comprising a multi-antenna BS and an IRS for multi-target sensing, where the beamforming vector and the IRS phase shifts are jointly optimized to improve the sensing performance. To meet the different sensing requirements, such as a minimum received power and a minimum sensing frequency, we propose three novel IRS-assisted sensing schemes: Time division (TD) sensing, signature sequence (SS) sensing, and hybrid TD-SS sensing. First, for TD sensing, the sensing tasks are performed in sequence over time. Subsequently, a novel signature sequence (SS) sensing scheme is proposed to improve sensing efficiency by establishing a relationship between directions and SSs. To strike a flexible balance between the beam pattern gain and sensing efficiency, we also propose a general hybrid TD-SS sensing scheme with target grouping, where targets belonging to the same group are sensed simultaneously via SS sensing, while the targets in different groups are assigned to orthogonal time slots. By controlling the number of groups, the hybrid TD-SS sensing scheme can provide a more flexible balance between beam pattern gain and sensing frequency. Moreover, ...      
### 83.Cross-speaker Emotion Transfer Based On Prosody Compensation for End-to-End Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2207.01198.pdf)
>  Cross-speaker emotion transfer speech synthesis aims to synthesize emotional speech for a target speaker by transferring the emotion from reference speech recorded by another (source) speaker. In this task, extracting speaker-independent emotion embedding from reference speech plays an important role. However, the emotional information conveyed by such emotion embedding tends to be weakened in the process to squeeze out the source speaker's timbre information. In response to this problem, a prosody compensation module (PCM) is proposed in this paper to compensate for the emotional information loss. Specifically, the PCM tries to obtain speaker-independent emotional information from the intermediate feature of a pre-trained ASR model. To this end, a prosody compensation encoder with global context (GC) blocks is introduced to obtain global emotional information from the ASR model's intermediate feature. Experiments demonstrate that the proposed PCM can effectively compensate the emotion embedding for the emotional information loss, and meanwhile maintain the timbre of the target speaker. Comparisons with state-of-the-art models show that our proposed method presents obvious superiority on the cross-speaker emotion transfer task.      
### 84.Multi-Modal Multi-Correlation Learning for Audio-Visual Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2207.01197.pdf)
>  In this paper we propose a multi-modal multi-correlation learning framework targeting at the task of audio-visual speech separation. Although previous efforts have been extensively put on combining audio and visual modalities, most of them solely adopt a straightforward concatenation of audio and visual features. To exploit the real useful information behind these two modalities, we define two key correlations which are: (1) identity correlation (between timbre and facial attributes); (2) phonetic correlation (between phoneme and lip motion). These two correlations together comprise the complete information, which shows a certain superiority in separating target speaker's voice especially in some hard cases, such as the same gender or similar content. For implementation, contrastive learning or adversarial training approach is applied to maximize these two correlations. Both of them work well, while adversarial training shows its advantage by avoiding some limitations of contrastive learning. Compared with previous research, our solution demonstrates clear improvement on experimental metrics without additional complexity. Further analysis reveals the validity of the proposed architecture and its good potential for future extension.      
### 85.Photonics-based short-time Fourier transform without high-frequency electronic devices and equipment  [ :arrow_down: ](https://arxiv.org/pdf/2207.01175.pdf)
>  A photonics-based short-time Fourier transform (STFT) system is proposed and experimentally demonstrated based on stimulated Brillouin scattering (SBS) without using high-frequency electronic devices and equipment. The wavelength of a distributed feedback laser diode is periodically swept by using a low-speed periodic sawtooth/triangular driving current. The periodic frequency-sweep optical signal is modulated by the signal under test (SUT) and then injected into a section of SBS medium. The optical signal from another laser diode as the pump wave is reversely injected into the SBS medium. After simply detecting the forward transmission optical signals in a low-speed photodetector, the STFT of the SUT can be implemented. The system is characterized by the absence of any high-frequency electronic devices or equipment. An experiment is performed. The STFT of a variety of RF signals is carried out in a 4-GHz bandwidth. The dynamic frequency resolution is demonstrated to be around 60 MHz.      
### 86.Geometrically-Shaped Multi-Dimensional Modulation Formats in Coherent Optical Transmission Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.01152.pdf)
>  Shaping modulation formats in multi-dimensional (MD) space is an effective approach to harvest spectral efficiency gains in both the additive white Gaussian noise (AWGN) channel and the optical fiber channel. In the first part of this paper, existing MD geometrically-shaped modulations for fiber optical communications are reviewed. It is shown that large gains can be obtained by exploiting correlation in the dimensions or/and by increasing the cardinality of the modulation format. Practical limitations and challenges are also discussed together with efficient solutions. In the second part, we extend the recently proposed four-dimensional (4D) modulation format family based on the constraint of orthant-symmetry to high spectrum efficiencies up to 10 bit/4D-sym by maximizing generalized mutual information for AWGN channel. Reach increases of up to 25% for a multi-span optical fiber transmission system are reported. Lastly,with the help of a recently introduced nonlinear interference (NLI) model, an optimization for designing nonlinear-tolerant 4D modulation formats is introduced for a single-span optical fiber system. Simulation results show that the proposed NLI model-based 4D modulation format could increase the effective SNRs by 0.25 dB with respect to the AWGN channel-optimal 4D modulation format.      
### 87.ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes  [ :arrow_down: ](https://arxiv.org/pdf/2207.01078.pdf)
>  Choosing optimal maskers for existing soundscapes to effect a desired perceptual change via soundscape augmentation is non-trivial due to extensive varieties of maskers and a dearth of benchmark datasets with which to compare and develop soundscape augmentation models. To address this problem, we make publicly available the ARAUS (Affective Responses to Augmented Urban Soundscapes) dataset, which comprises a five-fold cross-validation set and independent test set totaling 25,440 unique subjective perceptual responses to augmented soundscapes presented as audio-visual stimuli. Each augmented soundscape is made by digitally adding "maskers" (bird, water, wind, traffic, construction, or silence) to urban soundscape recordings at fixed soundscape-to-masker ratios. Responses were then collected by asking participants to rate how pleasant, annoying, eventful, uneventful, vibrant, monotonous, chaotic, calm, and appropriate each augmented soundscape was, in accordance with ISO 12913-2:2018. Participants also provided relevant demographic information and completed standard psychological questionnaires. We perform exploratory and statistical analysis of the responses obtained to verify internal consistency and agreement with known results in the literature. Finally, we demonstrate the benchmarking capability of the dataset by training and comparing four baseline models for urban soundscape pleasantness: a low-parameter regression model, a high-parameter convolutional neural network, and two attention-based networks in the literature.      
### 88.Generating gender-ambiguous voices for privacy-preserving speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.01052.pdf)
>  Our voice encodes a uniquely identifiable pattern which can be used to infer private attributes, such as gender or identity, that an individual might wish not to reveal when using a speech recognition service. To prevent attribute inference attacks alongside speech recognition tasks, we present a generative adversarial network, GenGAN, that synthesises voices that conceal the gender or identity of a speaker. The proposed network includes a generator with a U-Net architecture that learns to fool a discriminator. We condition the generator only on gender information and use an adversarial loss between signal distortion and privacy preservation. We show that GenGAN improves the trade-off between privacy and utility compared to privacy-preserving representation learning methods that consider gender information as a sensitive attribute to protect.      
### 89.Auto-Calibrating Admittance Controller for Robust Motion of Robotic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.01033.pdf)
>  We demonstrate an admittance controller with auto-tuning that can be applied for single and multi-point contact robots (e.g., legged robots with point feet or multi-finger grippers). The controller's objective is to track wrench profiles of each contact point while considering the additional torque due to rotational friction. Our admittance controller is adaptive during online operation by using an auto-tuning method that tunes the gains of the controller while following several training objectives that facilitate controller stability, such as tracking the wrench profile as closely as possible, ensuring control outputs that are within force limits that minimize slippage, and avoids kinematic singularity. We demonstrate the robustness of our controller on hardware for both manipulation and locomotion tasks using a multi-limbed climbing robot.      
### 90.Continuous-Time and Event-Triggered Online Optimization for Linear Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.00999.pdf)
>  This paper studies the decentralized online convex optimization problem for heterogeneous linear multi-agent systems. Agents have access to their time-varying local cost functions related to their own outputs, and there are also time-varying coupling inequality constraints among them. The goal of each agent is to minimize the global cost function by selecting appropriate local actions only through communication between neighbors. We design a distributed controller based on the saddle-point method which achieves constant regret bound and sublinear fit bound. In addition, to reduce the communication overhead, we propose an event-triggered communication scheme and show that the constant regret bound and sublinear fit bound are still achieved in the case of discrete communications with no Zeno behavior. A numerical example is provided to verify the proposed algorithms.with no Zeno behavior. A numerical example is provided to verify the proposed algorithms.      
### 91.Towards Error-Resilient Neural Speech Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.00993.pdf)
>  Neural audio coding has shown very promising results recently in the literature to largely outperform traditional codecs but limited attention has been paid on its error resilience. Neural codecs trained considering only source coding tend to be extremely sensitive to channel noises, especially in wireless channels with high error rate. In this paper, we investigate how to elevate the error resilience of neural audio codecs for packet losses that often occur during real-time communications. We propose a feature-domain packet loss concealment algorithm (FD-PLC) for real-time neural speech coding. Specifically, we introduce a self-attention-based module on the received latent features to recover lost frames in the feature domain before the decoder. A hybrid segment-level and frame-level frequency-domain discriminator is employed to guide the network to focus on both the generative quality of lost frames and the continuity with neighbouring frames. Experimental results on several error patterns show that the proposed scheme can achieve better robustness compared with the corresponding error-free and error-resilient baselines. We also show that feature-domain concealment is superior to waveform-domain counterpart as post-processing.      
### 92.Cycle-Interactive Generative Adversarial Network for Robust Unsupervised Low-Light Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2207.00965.pdf)
>  Getting rid of the fundamental limitations in fitting to the paired training data, recent unsupervised low-light enhancement methods excel in adjusting illumination and contrast of images. However, for unsupervised low light enhancement, the remaining noise suppression issue due to the lacking of supervision of detailed signal largely impedes the wide deployment of these methods in real-world applications. Herein, we propose a novel Cycle-Interactive Generative Adversarial Network (CIGAN) for unsupervised low-light image enhancement, which is capable of not only better transferring illumination distributions between low/normal-light images but also manipulating detailed signals between two domains, e.g., suppressing/synthesizing realistic noise in the cyclic enhancement/degradation process. In particular, the proposed low-light guided transformation feed-forwards the features of low-light images from the generator of enhancement GAN (eGAN) into the generator of degradation GAN (dGAN). With the learned information of real low-light images, dGAN can synthesize more realistic diverse illumination and contrast in low-light images. Moreover, the feature randomized perturbation module in dGAN learns to increase the feature randomness to produce diverse feature distributions, persuading the synthesized low-light images to contain realistic noise. Extensive experiments demonstrate both the superiority of the proposed method and the effectiveness of each module in CIGAN.      
### 93.WaferSegClassNet -- A Light-weight Network for Classification and Segmentation of Semiconductor Wafer Defects  [ :arrow_down: ](https://arxiv.org/pdf/2207.00960.pdf)
>  As the integration density and design intricacy of semiconductor wafers increase, the magnitude and complexity of defects in them are also on the rise. Since the manual inspection of wafer defects is costly, an automated artificial intelligence (AI) based computer-vision approach is highly desired. The previous works on defect analysis have several limitations, such as low accuracy and the need for separate models for classification and segmentation. For analyzing mixed-type defects, some previous works require separately training one model for each defect type, which is non-scalable. In this paper, we present WaferSegClassNet (WSCN), a novel network based on encoder-decoder architecture. WSCN performs simultaneous classification and segmentation of both single and mixed-type wafer defects. WSCN uses a "shared encoder" for classification, and segmentation, which allows training WSCN end-to-end. We use N-pair contrastive loss to first pretrain the encoder and then use BCE-Dice loss for segmentation, and categorical cross-entropy loss for classification. Use of N-pair contrastive loss helps in better embedding representation in the latent dimension of wafer maps. WSCN has a model size of only 0.51MB and performs only 0.2M FLOPS. Thus, it is much lighter than other state-of-the-art models. Also, it requires only 150 epochs for convergence, compared to 4,000 epochs needed by a previous work. We evaluate our model on the MixedWM38 dataset, which has 38,015 images. WSCN achieves an average classification accuracy of 98.2% and a dice coefficient of 0.9999. We are the first to show segmentation results on the MixedWM38 dataset. The source code can be obtained from <a class="link-external link-https" href="https://github.com/ckmvigil/WaferSegClassNet" rel="external noopener nofollow">this https URL</a>.      
### 94.Group-Theoretic Wideband Radar Waveform Design  [ :arrow_down: ](https://arxiv.org/pdf/2207.00959.pdf)
>  We investigate the theory of affine groups in the context of designing radar waveforms that obey the desired wideband ambiguity function (WAF). The WAF is obtained by correlating the signal with its time-dilated, Doppler-shifted, and delayed replicas. We consider the WAF definition as a coefficient function of the unitary representation of the group $a\cdot x + b$. This is essentially an algebraic problem applied to the radar waveform design. Prior works on this subject largely analyzed narrow-band ambiguity functions. Here, we show that when the underlying wideband signal of interest is a pulse or pulse train, a tight frame can be built to design that waveform. Specifically, we design the radar signals by minimizing the ratio of bounding constants of the frame in order to obtain lower sidelobes in the WAF. This minimization is performed by building a codebook based on difference sets in order to achieve the Welch bound. We show that the tight frame so obtained is connected with the wavelet transform that defines the WAF.      
### 95.M-Adapter: Modality Adaptation for End-to-End Speech-to-Text Translation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00952.pdf)
>  End-to-end speech-to-text translation models are often initialized with pre-trained speech encoder and pre-trained text decoder. This leads to a significant training gap between pre-training and fine-tuning, largely due to the modality differences between speech outputs from the encoder and text inputs to the decoder. In this work, we aim to bridge the modality gap between speech and text to improve translation quality. We propose M-Adapter, a novel Transformer-based module, to adapt speech representations to text. While shrinking the speech sequence, M-Adapter produces features desired for speech-to-text translation via modelling global and local dependencies of a speech sequence. Our experimental results show that our model outperforms a strong baseline by up to 1 BLEU score on the Must-C En$\rightarrow$DE dataset.\footnote{Our code is available at <a class="link-external link-https" href="https://github.com/mingzi151/w2v2-st" rel="external noopener nofollow">this https URL</a>.}      
### 96.Degradation-Guided Meta-Restoration Network for Blind Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.00943.pdf)
>  Blind super-resolution (SR) aims to recover high-quality visual textures from a low-resolution (LR) image, which is usually degraded by down-sampling blur kernels and additive noises. This task is extremely difficult due to the challenges of complicated image degradations in the real-world. Existing SR approaches either assume a predefined blur kernel or a fixed noise, which limits these approaches in challenging cases. In this paper, we propose a Degradation-guided Meta-restoration network for blind Super-Resolution (DMSR) that facilitates image restoration for real cases. DMSR consists of a degradation extractor and meta-restoration modules. The extractor estimates the degradations in LR inputs and guides the meta-restoration modules to predict restoration parameters for different degradations on-the-fly. DMSR is jointly optimized by a novel degradation consistency loss and reconstruction losses. Through such an optimization, DMSR outperforms SOTA by a large margin on three widely-used benchmarks. A user study including 16 subjects further validates the superiority of DMSR in real-world blind SR tasks.      
### 97.Wireless Channel Prediction in Partially Observed Environments  [ :arrow_down: ](https://arxiv.org/pdf/2207.00934.pdf)
>  Site-specific radio frequency (RF) propagation prediction increasingly relies on models built from visual data such as cameras and LIDAR sensors. When operating in dynamic settings, the environment may only be partially observed. This paper introduces a method to extract statistical channel models, given partial observations of the surrounding environment. We propose a simple heuristic algorithm that performs ray tracing on the partial environment and then uses machine-learning trained predictors to estimate the channel and its uncertainty from features extracted from the partial ray tracing results. It is shown that the proposed method can interpolate between fully statistical models when no partial information is available and fully deterministic models when the environment is completely observed. The method can also capture the degree of uncertainty of the propagation predictions depending on the amount of region that has been explored. The methodology is demonstrated in a robotic navigation application simulated on a set of indoor maps with detailed models constructed using state-of-the-art navigation, simultaneous localization and mapping (SLAM), and computer vision methods.      
### 98.Interference Constrained Beam Alignment for Time-Varying Channels via Kernelized Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2207.00908.pdf)
>  To fully utilize the abundant spectrum resources in millimeter wave (mmWave), Beam Alignment (BA) is necessary for large antenna arrays to achieve large array gains. In practical dynamic wireless environments, channel modeling is challenging due to time-varying and multipath effects. In this paper, we formulate the beam alignment problem as a non-stationary online learning problem with the objective to maximize the received signal strength under interference constraint. In particular, we employ the non-stationary kernelized bandit to leverage the correlation among beams and model the complex beamforming and multipath channel functions. Furthermore, to mitigate interference to other user equipment, we leverage the primal-dual method to design a constrained UCB-type kernelized bandit algorithm. Our theoretical analysis indicates that the proposed algorithm can adaptively adjust the beam in time-varying environments, such that both the cumulative regret of the received signal and constraint violations have sublinear bounds with respect to time. This result is of independent interest for applications such as adaptive pricing and news ranking. In addition, the algorithm assumes the channel is a black-box function and does not require any prior knowledge for dynamic channel modeling, and thus is applicable in a variety of scenarios. We further show that if the information about the channel variation is known, the algorithm will have better theoretical guarantees and performance. Finally, we conduct simulations to highlight the effectiveness of the proposed algorithm.      
### 99.Improving Transformer-based Conversational ASR by Inter-Sentential Attention Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2207.00883.pdf)
>  Transformer-based models have demonstrated their effectiveness in automatic speech recognition (ASR) tasks and even shown superior performance over the conventional hybrid framework. The main idea of Transformers is to capture the long-range global context within an utterance by self-attention layers. However, for scenarios like conversational speech, such utterance-level modeling will neglect contextual dependencies that span across utterances. In this paper, we propose to explicitly model the inter-sentential information in a Transformer based end-to-end architecture for conversational speech recognition. Specifically, for the encoder network, we capture the contexts of previous speech and incorporate such historic information into current input by a context-aware residual attention mechanism. For the decoder, the prediction of current utterance is also conditioned on the historic linguistic information through a conditional decoder framework. We show the effectiveness of our proposed method on several open-source dialogue corpora and the proposed method consistently improved the performance from the utterance-level Transformer-based ASR models.      
### 100.Biological Robots: Perspectives on an Emerging Interdisciplinary Field  [ :arrow_down: ](https://arxiv.org/pdf/2207.00880.pdf)
>  Advances in science and engineering often reveal the limitations of classical approaches initially used to understand, predict, and control phenomena. With progress, conceptual categories must often be re-evaluated to better track recently discovered invariants across disciplines. It is essential to refine frameworks and resolve conflicting boundaries between disciplines such that they better facilitate, not restrict, experimental approaches and capabilities. In this essay, we discuss issues at the intersection of developmental biology, computer science, and robotics. In the context of biological robots, we explore changes across concepts and previously distinct fields that are driven by recent advances in materials, information, and life sciences. Herein, each author provides their own perspective on the subject, framed by their own disciplinary training. We argue that as with computation, certain aspects of developmental biology and robotics are not tied to specific materials; rather, the consilience of these fields can help to shed light on issues of multi-scale control, self-assembly, and relationships between form and function. We hope new fields can emerge as boundaries arising from technological limitations are overcome, furthering practical applications from regenerative medicine to useful synthetic living machines.      
### 101.Energy-efficient User Clustering for UAV-enabled Wireless Networks Using EM Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2207.00873.pdf)
>  Unmanned Aerial Vehicles (UAVs) can be used to provide wireless connectivity to support the existing infrastructure in hot-spots or replace it in cases of destruction. UAV-enabled wireless provides several advantages in network performance due to drone small cells (DSCs) mobility despite the limited onboard energy. However, the problem of resource allocation has added complexity. In this paper, we propose an energy-efficient user clustering mechanism based on Gaussian mixture models (GMM) using a modified Expected-Maximization (EM) algorithm. The algorithm is intended to provide the initial user clustering and drone deployment upon which additional mechanisms can be employed to further enhance the system performance. The proposed algorithm improves the energy efficiency of the system by 25% and link reliability by 18.3% compared to other baseline methods.      
### 102.Hardware architecture for high throughput event visual data filtering with matrix of IIR filters algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2207.00860.pdf)
>  Neuromorphic vision is a rapidly growing field with numerous applications in the perception systems of autonomous vehicles. Unfortunately, due to the sensors working principle, there is a significant amount of noise in the event stream. In this paper we present a novel algorithm based on an IIR filter matrix for filtering this type of noise and a hardware architecture that allows its acceleration using an SoC FPGA. Our method has a very good filtering efficiency for uncorrelated noise - over 99% of noisy events are removed. It has been tested for several event data sets with added random noise. We designed the hardware architecture in such a way as to reduce the utilisation of the FPGA's internal BRAM resources. This enabled a very low latency and a throughput of up to 385.8 MEPS million events per second.The proposed hardware architecture was verified in simulation and in hardware on the Xilinx Zynq Ultrascale+ MPSoC chip on the Mercury+ XU9 module with the Mercury+ ST1 base board.      
### 103.Tree-constrained Pointer Generator with Graph Neural Network Encodings for Contextual Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.00857.pdf)
>  Incorporating biasing words obtained as contextual knowledge is critical for many automatic speech recognition (ASR) applications. This paper proposes the use of graph neural network (GNN) encodings in a tree-constrained pointer generator (TCPGen) component for end-to-end contextual ASR. By encoding the biasing words in the prefix-tree with a tree-based GNN, lookahead for future wordpieces in end-to-end ASR decoding is achieved at each tree node by incorporating information about all wordpieces on the tree branches rooted from it, which allows a more accurate prediction of the generation probability of the biasing words. Systems were evaluated on the Librispeech corpus using simulated biasing tasks, and on the AMI corpus by proposing a novel visual-grounded contextual ASR pipeline that extracts biasing words from slides alongside each meeting. Results showed that TCPGen with GNN encodings achieved about a further 15% relative WER reduction on the biasing words compared to the original TCPGen, with a negligible increase in the computation cost for decoding.      
### 104.Cell-Free Massive MIMO for URLLC: A Finite-Blocklength Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.00856.pdf)
>  We present a general framework for the characterization of the packet error probability achievable in cell-free Massive multiple-input multiple output (MIMO) architectures deployed to support ultra-reliable low lantecy (URLLC) traffic. The framework is general and encompasses both centralized and distributed cell-free architectures, arbitrary fading channels and channel estimation algorithms at both network and user-equipment (UE) sides, as well as arbitrary combing and precoding schemes. The framework is used to perform numerical experiments that clearly show the superiority of cell-free architectures compared to cellular architectures in supporting URLLC traffic in uplink and downlink. Also, they provide the following novel insights into the optimal design of cell-free architectures for URLLC: i) minimum mean square error (MMSE) spatial processing must be used to achieve the URLLC targets; ii) for a given total number of antennas per coverage area, centralized cell-free solutions involving single-antenna access points (APs) offer the best performance in the uplink, thereby highlighting the importance of reducing the average distance between APs and UEs in the URLLC regime; iii) this observation applies also to the downlink, provided that the APs transmit precoded pilots to allow the UEs to estimate accurately the precoded channel.      
### 105.Benchmarks for Industrial Inspection Based on Structured Light  [ :arrow_down: ](https://arxiv.org/pdf/2207.00796.pdf)
>  Robustness and accuracy are two critical metrics for industrial inspection. In this paper, we propose benchmarks that can evaluate the structured light method's performance. Our evaluation metric was learning from a lot of inspection tasks from the factories. The metric we proposed consists of four detailed criteria such as flatness, length, height and sphericity. Then we can judge whether the structured light method/device can be applied to a specified inspection task by our evaluation metric quickly. A structured light device built for TypeC pin needles inspection performance is evaluated via our metrics in the final experimental section.      
### 106.Two-Timescale Design for STAR-RIS Aided NOMA Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.00792.pdf)
>  Simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) have emerged as a promising technology for achieving full-space coverage. Prior works on STAR-RISs mostly assumed the full and instantaneous channel state information (CSI) is available, which, however, is practically difficult to obtain due to the large number of elements. To address it, we investigate STAR-RIS aided NOMA systems, where two efficient two-timescale transmission protocols are proposed for different channel setups to maximize the average sum-rate. Specifically, 1) for line-of-sight (LoS) dominant channels, we propose the beamforming-then-estimate (BTE) Protocol, where the long-term STAR-RIS coefficients are optimized based on the statistical CSI, while the short-term power allocation at the base station (BS) is designed based on the effective channels; 2) for the rich scattering environment, we propose an alternative partition-then-estimate (PTE) Protocol, where the BS determines the long-term STAR-RIS surface-partition strategy; then the BS estimates the instantaneous subsurface channels and designs its power allocation and STAR-RIS phase-shifts accordingly. Simulation results validate the superiority of our proposed transmission protocols as compared to various benchmarks. It is shown that the BTE Protocol outperforms the PTE Protocol when the number of STAR-RIS elements is large and/or the LoS channel components are dominant, and vice versa.      
### 107.Unsupervised Symbolic Music Segmentation using Ensemble Temporal Prediction Errors  [ :arrow_down: ](https://arxiv.org/pdf/2207.00760.pdf)
>  Symbolic music segmentation is the process of dividing symbolic melodies into smaller meaningful groups, such as melodic phrases. We proposed an unsupervised method for segmenting symbolic music. The proposed model is based on an ensemble of temporal prediction error models. During training, each model predicts the next token to identify musical phrase changes. While at test time, we perform a peak detection algorithm to select segment candidates. Finally, we aggregate the predictions of each of the models participating in the ensemble to predict the final segmentation. Results suggest the proposed method reaches state-of-the-art performance on the Essen Folksong dataset under the unsupervised setting when considering F-Score and R-value. We additionally provide an ablation study to better assess the contribution of each of the model components to the final results. As expected, the proposed method is inferior to the supervised setting, which leaves room for improvement in future research considering closing the gap between unsupervised and supervised methods.      
### 108.Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers  [ :arrow_down: ](https://arxiv.org/pdf/2207.00756.pdf)
>  Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.      
### 109.DeltaZ: An Accessible Compliant Delta Robot Manipulator for Research and Education  [ :arrow_down: ](https://arxiv.org/pdf/2207.00721.pdf)
>  This paper presents the DeltaZ robot, a centimeter-scale, low-cost, delta-style robot that allows for a broad range of capabilities and robust functionalities. Current technologies allow DeltaZ to be 3D-printed from soft and rigid materials so that it is easy to assemble and maintain, and lowers the barriers to utilize. Functionality of the robot stems from its three translational degrees of freedom and a closed form kinematic solution which makes manipulation problems more intuitive compared to other manipulators. Moreover, the low cost of the robot presents an opportunity to democratize manipulators for a research setting. We also describe how the robot can be used as a reinforcement learning benchmark. Open-source 3D-printable designs and code are available to the public.      
### 110.Superdirective Antenna Pairs for Energy-Efficient Terahertz Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2207.00697.pdf)
>  Terahertz (THz) communication is widely deemed the next frontier of wireless networks owing to the abundant spectrum resources in the THz band. Whilst THz signals suffer from severe propagation losses, a massive antenna array can be deployed at the base station (BS) to mitigate those losses through beamforming. Nevertheless, a large number of antennas increases the hardware complexity and circuit power consumption, and hence it can lead to poor energy efficiency (EE). To surmount this fundamental problem, we propose a novel array design based on coupled antenna pairs. Specifically, we exploit the mutual coupling between closely spaced antennas to form superdirective pairs. A unique property of them is that all require the same excitation amplitude, and therefore can be driven by a single radio frequency chain akin to conventional phased arrays. Moreover, they facilitate the implementation of multi-port impedance matching, which ensures maximum power transfer for any beamforming angle. After addressing the hardware-related problems of superdirectivity, we show that the number of BS antennas can be effectively reduced without sacrificing the achievable rate. Simulation results showcase that our design offers huge EE gains compared to uncoupled uniform linear arrays, and hence could be a radical solution for future THz systems.      
### 111.Building African Voices  [ :arrow_down: ](https://arxiv.org/pdf/2207.00688.pdf)
>  Modern speech synthesis techniques can produce natural-sounding speech given sufficient high-quality data and compute resources. However, such data is not readily available for many languages. This paper focuses on speech synthesis for low-resourced African languages, from corpus creation to sharing and deploying the Text-to-Speech (TTS) systems. We first create a set of general-purpose instructions on building speech synthesis systems with minimum technological resources and subject-matter expertise. Next, we create new datasets and curate datasets from "found" data (existing recordings) through a participatory approach while considering accessibility, quality, and breadth. We demonstrate that we can develop synthesizers that generate intelligible speech with 25 minutes of created speech, even when recorded in suboptimal environments. Finally, we release the speech data, code, and trained voices for 12 African languages to support researchers and developers.      
### 112.Wirelessly-Controlled Untethered Piezoelectric Planar Soft Robot Capable of Bidirectional Crawling and Rotation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00658.pdf)
>  Electrostatic actuators provide a promising approach to creating soft robotic sheets, due to their flexible form factor, modular integration, and fast response speed. However, their control requires kilo-Volt signals and understanding complex dynamics resulting from force interactions by on-board and environmental effects. In this work, we demonstrate an untethered two-dimensional five-actuator piezoelectric robot powered by batteries and on-board high-voltage circuitry, and controlled through a wireless link. The scalable fabrication approach is based on bonding different functional layers on top of each other (steel foil substrate, actuators, flexible electronics). The robot exhibits a range of controllable motions, including bidirectional crawling (up to ~0.6 cm/s), turning, and in-place rotation (at ~1 degree/s). High-speed videos and control experiments show that the richness of the motion results from the interaction of an asymmetric mass distribution in the robot and the associated dependence of the dynamics on the driving frequency of the piezoelectrics.      
### 113.Feature-selected Graph Spatial Attention Network for Addictive Brain-Networks Identification  [ :arrow_down: ](https://arxiv.org/pdf/2207.00583.pdf)
>  Functional alterations in the relevant neural circuits occur from drug addiction over a certain period. And these significant alterations are also revealed by analyzing fMRI. However, because of fMRI's high dimensionality and poor signal-to-noise ratio, it is challenging to encode efficient and robust brain regional embeddings for both graph-level identification and region-level biomarkers detection tasks between nicotine addiction (NA) and healthy control (HC) groups. In this work, we represent the fMRI of the rat brain as a graph with biological attributes and propose a novel feature-selected graph spatial attention network(FGSAN) to extract the biomarkers of addiction and identify from these brain networks. Specially, a graph spatial attention encoder is employed to capture the features of spatiotemporal brain networks with spatial information. The method simultaneously adopts a Bayesian feature selection strategy to optimize the model and improve classification task by constraining features. Experiments on an addiction-related neural imaging dataset show that the proposed model can obtain superior performance and detect interpretable biomarkers associated with addiction-relevant neural circuits.      
