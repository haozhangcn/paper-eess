# ArXiv eess --Mon, 25 Jul 2022
### 1.Large-Kernel Attention for 3D Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.11225.pdf)
>  Automatic segmentation of multiple organs and tumors from 3D medical images such as magnetic resonance imaging (MRI) and computed tomography (CT) scans using deep learning methods can aid in diagnosing and treating cancer. However, organs often overlap and are complexly connected, characterized by extensive anatomical variation and low contrast. In addition, the diversity of tumor shape, location, and appearance, coupled with the dominance of background voxels, makes accurate 3D medical image segmentation difficult. In this paper, a novel large-kernel (LK) attention module is proposed to address these problems to achieve accurate multi-organ segmentation and tumor segmentation. The advantages of convolution and self-attention are combined in the proposed LK attention module, including local contextual information, long-range dependence, and channel adaptation. The module also decomposes the LK convolution to optimize the computational cost and can be easily incorporated into FCNs such as U-Net. Comprehensive ablation experiments demonstrated the feasibility of convolutional decomposition and explored the most efficient and effective network design. Among them, the best Mid-type LK attention-based U-Net network was evaluated on CT-ORG and BraTS 2020 datasets, achieving state-of-the-art segmentation performance. The performance improvement due to the proposed LK attention module was also statistically validated.      
### 2.Humans plan for the near future to walk economically on uneven terrain  [ :arrow_down: ](https://arxiv.org/pdf/2207.11224.pdf)
>  Humans experience small fluctuations in their gait when walking on uneven terrain. The fluctuations deviate from the steady, energy-minimizing pattern for level walking, and have no obvious organization. But humans often look ahead when they walk, and could potentially plan anticipatory fluctuations for the terrain. Such planning is only sensible if it serves some an objective purpose, such as maintaining constant speed or reducing energy expenditure, that is also attainable within finite planning capacity. Here we show that humans do plan and perform optimal control strategies on uneven terrain. Rather than maintain constant speed, they make purposeful, anticipatory speed adjustments that are consistent with minimizing energy expenditure. A simple optimal control model predicts economical speed fluctuations that agree well with experiments with humans (N = 12) walking on seven different terrain profiles (correlated with model r = 0.517 std. 0.109, P &lt; 0.05 all terrains). Participants made repeatable speed fluctuations starting about seven to eight steps ahead of each terrain feature (up to 7.5 cm height difference each step, up to 16 consecutive features). They need not plan farther ahead, because each leg collision with ground dissipates energy, preventing momentum from persisting indefinitely. About seven to eight steps of continuous look-ahead and working memory thus suffice to practically optimize for any length of terrain. Humans reason about walking in the near future to plan complex optimal control sequences.      
### 3.Improved $α$-GAN architecture for generating 3D connected volumes with an application to radiosurgery treatment planning  [ :arrow_down: ](https://arxiv.org/pdf/2207.11223.pdf)
>  Generative Adversarial Networks (GANs) have gained significant attention in several computer vision tasks for generating high-quality synthetic data. Various medical applications including diagnostic imaging and radiation therapy can benefit greatly from synthetic data generation due to data scarcity in the domain. However, medical image data is typically kept in 3D space, and generative models suffer from the curse of dimensionality issues in generating such synthetic data. In this paper, we investigate the potential of GANs for generating connected 3D volumes. We propose an improved version of 3D $\alpha$-GAN by incorporating various architectural enhancements. On a synthetic dataset of connected 3D spheres and ellipsoids, our model can generate fully connected 3D shapes with similar geometrical characteristics to that of training data. We also show that our 3D GAN model can successfully generate high-quality 3D tumor volumes and associated treatment specifications (e.g., isocenter locations). Similar moment invariants to the training data as well as fully connected 3D shapes confirm that improved 3D $\alpha$-GAN implicitly learns the training data distribution, and generates realistic-looking samples. The capability of improved 3D $\alpha$-GAN makes it a valuable source for generating synthetic medical image data that can help future research in this domain.      
### 4.Human Treelike Tubular Structure Segmentation: A Comprehensive Review and Future Perspectives  [ :arrow_down: ](https://arxiv.org/pdf/2207.11203.pdf)
>  Various structures in human physiology follow a treelike morphology, which often expresses complexity at very fine scales. Examples of such structures are intrathoracic airways, retinal blood vessels, and hepatic blood vessels. Large collections of 2D and 3D images have been made available by medical imaging modalities such as magnetic resonance imaging (MRI), computed tomography (CT), Optical coherence tomography (OCT) and ultrasound in which the spatial arrangement can be observed. Segmentation of these structures in medical imaging is of great importance since the analysis of the structure provides insights into disease diagnosis, treatment planning, and prognosis. Manually labelling extensive data by radiologists is often time-consuming and error-prone. As a result, automated or semi-automated computational models have become a popular research field of medical imaging in the past two decades, and many have been developed to date. In this survey, we aim to provide a comprehensive review of currently publicly available datasets, segmentation algorithms, and evaluation metrics. In addition, current challenges and future research directions are discussed.      
### 5.Proactive Distributed Constraint Optimization of Heterogeneous Incident Vehicle Teams  [ :arrow_down: ](https://arxiv.org/pdf/2207.11132.pdf)
>  Traditionally, traffic incident management (TIM) programs coordinate the deployment of emergency resources to immediate incident requests without accommodating the interdependencies on incident evolutions in the environment. However, ignoring inherent interdependencies on the evolution of incidents in the environment while making current deployment decisions is shortsighted, and the resulting naive deployment strategy can significantly worsen the overall incident delay impact on the network. The interdependencies on incident evolution in the environment, including those between incident occurrences, and those between resource availability in near-future requests and the anticipated duration of the immediate incident request, should be considered through a look-ahead model when making current-stage deployment decisions. This study develops a new proactive framework based on the distributed constraint optimization problem (DCOP) to address the above limitations, overcoming conventional TIM models that cannot accommodate the dependencies in the TIM problem. Furthermore, the optimization objective is formulated to incorporate Unmanned Aerial Vehicles (UAVs). The UAVs' role in TIM includes exploring uncertain traffic conditions, detecting unexpected events, and augmenting information from roadway traffic sensors. Robustness analysis of our model for multiple TIM scenarios shows satisfactory performance using local search exploration heuristics. Overall, our model reports a significant reduction in total incident delay compared to conventional TIM models. With UAV support, we demonstrate a further decrease in the overall incident delay through the shorter response time of emergency vehicles, and a reduction in uncertainties associated with the estimated incident delay impact.      
### 6.Design of Sliding Mode PID Controller with Improved reaching laws for Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.11129.pdf)
>  In this thesis, advanced design technique in sliding mode control (SMC) is presented with focus on PID (Proportional-Integral-Derivative) type Sliding surfaces based Sliding mode control with improved power rate exponential reaching law for Non-linear systems using Modified Particle Swarm Optimization (MPSO). To handle large non-linearities directly, sliding mode controller based on PID-type sliding surface has been designed in this work, where Integral term ensures fast finite convergence time. The controller parameter for various modified structures can be estimated using Modified PSO, which is used as an offline optimization technique. Various reaching law were implemented leading to the proposed improved exponential power rate reaching law, which also improves the finite convergence time. To implement the proposed algorithm, nonlinear mathematical model has to be decrypted without linearizing, and used for the simulation purposes. Their performance is studied using simulations to prove the proposed behavior. The problem of chattering has been overcome by using boundary method and also second order sliding mode method. PI-type sliding surface based second order sliding mode controller with PD surface based SMC compensation is also proposed and implemented. The proposed algorithms have been analyzed using Lyapunov stability criteria. The robustness of the method is provided using simulation results including disturbance and 10% variation in system parameters. Finally process control based hardware is implemented (conical tank system).      
### 7.Integral surface based Second Order Sliding Mode Controller design for Inverted Pendulum with PD SMC Compensation  [ :arrow_down: ](https://arxiv.org/pdf/2207.11128.pdf)
>  Stabilization of a nonlinear single stage inverted pendulum is a complicated control problem, as nonlinearity is present inherently and external factors affect the equilibrium position. In this paper, a PD sliding mode controller is connected with Second order PI (Proportional+Integral) sliding mode controller, which is designed to improve the performance for nonlinear state differential equations with unknown parameters. This paper throws light on the sliding surface design and highlights the important features of multiplexing sliding mode control inputs resulting in robustness and higher convergence of output, through extensive mathematical modeling. Simulations and experimental application is done on the system to evaluate the controller for performance, complexity of implementation and also on the impact of the nonlinear IP system on its stability.      
### 8.Fast strategies for multi-temporal speckle reduction of Sentinel-1 GRD images  [ :arrow_down: ](https://arxiv.org/pdf/2207.11111.pdf)
>  Reducing speckle and limiting the variations of the physical parameters in Synthetic Aperture Radar (SAR) images is often a key-step to fully exploit the potential of such data. Nowadays, deep learning approaches produce state of the art results in single-image SAR restoration. Nevertheless, huge multi-temporal stacks are now often available and could be efficiently exploited to further improve image quality. This paper explores two fast strategies employing a single-image despeckling algorithm, namely SAR2SAR, in a multi-temporal framework. The first one is based on Quegan filter and replaces the local reflectivity pre-estimation by SAR2SAR. The second one uses SAR2SAR to suppress speckle from a ratio image encoding the multi-temporal information under the form of a "super-image", i.e. the temporal arithmetic mean of a time series. Experimental results on Sentinel-1 GRD data show that these two multi-temporal strategies provide improved filtering results while adding a limited computational cost.      
### 9.Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs  [ :arrow_down: ](https://arxiv.org/pdf/2207.11102.pdf)
>  Optical coherence tomography angiography (OCTA) can non-invasively image the eye's circulatory system. In order to reliably characterize the retinal vasculature, there is a need to automatically extract quantitative metrics from these images. The calculation of such biomarkers requires a precise semantic segmentation of the blood vessels. However, deep-learning-based methods for segmentation mostly rely on supervised training with voxel-level annotations, which are costly to obtain. In this work, we present a pipeline to synthesize large amounts of realistic OCTA images with intrinsically matching ground truth labels; thereby obviating the need for manual annotation of training data. Our proposed method is based on two novel components: 1) a physiology-based simulation that models the various retinal vascular plexuses and 2) a suite of physics-based image augmentations that emulate the OCTA image acquisition process including typical artifacts. In extensive benchmarking experiments, we demonstrate the utility of our synthetic data by successfully training retinal vessel segmentation algorithms. Encouraged by our method's competitive quantitative and superior qualitative performance, we believe that it constitutes a versatile tool to advance the quantitative analysis of OCTA images.      
### 10.Multi-temporal speckle reduction with self-supervised deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11095.pdf)
>  Speckle filtering is generally a prerequisite to the analysis of synthetic aperture radar (SAR) images. Tremendous progress has been achieved in the domain of single-image despeckling. Latest techniques rely on deep neural networks to restore the various structures and textures peculiar to SAR images. The availability of time series of SAR images offers the possibility of improving speckle filtering by combining different speckle realizations over the same area. The supervised training of deep neural networks requires ground-truth speckle-free images. Such images can only be obtained indirectly through some form of averaging, by spatial or temporal integration, and are imperfect. Given the potential of very high quality restoration reachable by multi-temporal speckle filtering, the limitations of ground-truth images need to be circumvented. We extend a recent self-supervised training strategy for single-look complex SAR images, called MERLIN, to the case of multi-temporal filtering. This requires modeling the sources of statistical dependencies in the spatial and temporal dimensions as well as between the real and imaginary components of the complex amplitudes. Quantitative analysis on datasets with simulated speckle indicates a clear improvement of speckle reduction when additional SAR images are included. Our method is then applied to stacks of TerraSAR-X images and shown to outperform competing multi-temporal speckle filtering approaches. The code of the trained models is made freely available on the $\href{<a class="link-external link-https" href="https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/" rel="external noopener nofollow">this https URL</a>}{\text{GitLab}}$ of the IMAGES team of the LTCI Lab, Télécom Paris Institut Polytechnique de Paris.      
### 11.Graph Spatio-Spectral Total Variation Model for Hyperspectral Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2207.11050.pdf)
>  The spatio-spectral total variation (SSTV) model has been widely used as an effective regularization of hyperspectral images (HSI) for various applications such as mixed noise removal. However, since SSTV computes local spatial differences uniformly, it is difficult to remove noise while preserving complex spatial structures with fine edges and textures, especially in situations of high noise intensity. To solve this problem, we propose a new TV-type regularization called Graph-SSTV (GSSTV), which generates a graph explicitly reflecting the spatial structure of the target HSI from noisy HSIs and incorporates a weighted spatial difference operator designed based on this graph. Furthermore, we formulate the mixed noise removal problem as a convex optimization problem involving GSSTV and develop an efficient algorithm based on the primal-dual splitting method to solve this problem. Finally, we demonstrate the effectiveness of GSSTV compared with existing HSI regularization models through experiments on mixed noise removal. The source code will be available at <a class="link-external link-https" href="https://www.mdi.c.titech.ac.jp/publications/gsstv" rel="external noopener nofollow">this https URL</a>.      
### 12.Centralized and Decentralized ML-Enabled Integrated Terrestrial and Non-Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11028.pdf)
>  Non-terrestrial networks (NTNs) are a critical enabler of the persistent connectivity vision of sixth-generation networks, as they can service areas where terrestrial infrastructure falls short. However, the integration of these networks with the terrestrial network is laden with obstacles. The dynamic nature of NTN communication scenarios and numerous variables render conventional model-based solutions computationally costly and impracticable for resource allocation, parameter optimization, and other problems. Machine learning (ML)-based solutions, thus, can perform a pivotal role due to their inherent ability to uncover the hidden patterns in time-varying, multi-dimensional data with superior performance and less complexity. Centralized ML (CML) and decentralized ML (DML), named so based on the distribution of the data and computational load, are two classes of ML that are being studied as solutions for the various complications of terrestrial and non-terrestrial networks (TNTN) integration. Both have their benefits and drawbacks under different circumstances, and it is integral to choose the appropriate ML approach for each TNTN integration issue. To this end, this paper goes over the TNTN integration architectures as given in the 3rd generation partnership project standard releases, proposing possible scenarios. Then, the capabilities and challenges of CML and DML are explored from the vantage point of these scenarios.      
### 13.Terahertz Communications for 6G and Beyond Wireless Networks: Challenges, Key Advancements, and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2207.11021.pdf)
>  The unprecedented increase in wireless data traffic, predicted to occur within the next decade, is motivating academia and industries to look beyond contemporary wireless standards and conceptualize the sixth-generation (6G) wireless networks. Among various promising solutions, terahertz (THz) communications (THzCom) is recognized as a highly promising technology for the 6G and beyond era, due to its unique potential to support terabit-per-second transmission in emerging applications. This article delves into key areas for developing end-to-end THzCom systems, focusing on physical, link, and network layers. Specifically, we discuss the areas of THz spectrum management, THz antennas and beamforming, and the integration of other 6G-enabling technologies for THzCom. For each area, we identify the challenges imposed by the unique properties of the THz band. We then present main advancements and outline perspective research directions in each area to stimulate future research efforts for realizing THzCom in 6G and beyond wireless networks.      
### 14.Rapid Lung Ultrasound COVID-19 Severity Scoring with Resource-Efficient Deep Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2207.10998.pdf)
>  Artificial intelligence-based analysis of lung ultrasound imaging has been demonstrated as an effective technique for rapid diagnostic decision support throughout the COVID-19 pandemic. However, such techniques can require days- or weeks-long training processes and hyper-parameter tuning to develop intelligent deep learning image analysis models. This work focuses on leveraging 'off-the-shelf' pre-trained models as deep feature extractors for scoring disease severity with minimal training time. We propose using pre-trained initializations of existing methods ahead of simple and compact neural networks to reduce reliance on computational capacity. This reduction of computational capacity is of critical importance in time-limited or resource-constrained circumstances, such as the early stages of a pandemic. On a dataset of 49 patients, comprising over 20,000 images, we demonstrate that the use of existing methods as feature extractors results in the effective classification of COVID-19-related pneumonia severity while requiring only minutes of training time. Our methods can achieve an accuracy of over 0.93 on a 4-level severity score scale and provides comparable per-patient region and global scores compared to expert annotated ground truths. These results demonstrate the capability for rapid deployment and use of such minimally-adapted methods for progress monitoring, patient stratification and management in clinical practice for COVID-19 patients, and potentially in other respiratory diseases.      
### 15.Two-Port Feedback Analysis On Miller-Compensated Amplifiers  [ :arrow_down: ](https://arxiv.org/pdf/2207.10983.pdf)
>  In this paper, various Miller-compensated amplifiers are analyzed by using the two-port feedback analysis together with the root-locus diagram. The proposed analysis solves problems of Miller theorem/approximation that fail to predict a pole-splitting and that require an impractical assumption that an initial lower frequency pole before connecting a Miller capacitor in a two-stage amplifier should be associated with the input of the amplifier. Since the proposed analysis sheds light on how the closed-loop poles originate from the open-loop poles in the s-plane, it allows the association of the closed-loop poles with the circuit components and thus provides a design insight for frequency compensation. The circuits analyzed are two-stage Miller-compensated amplifiers with and without a current buffer and a three-stage nested Miller-compensated amplifier.      
### 16.Convergence Theory of Generalized Distributed Subgradient Method with Random Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2207.10969.pdf)
>  The distributed subgradient method (DSG) is a widely discussed algorithm to cope with large-scale distributed optimization problems in the arising machine learning applications. Most exisiting works on DSG focus on ideal communication between the cooperative agents such that the shared information between agents is exact and perfect. This assumption, however, could lead to potential privacy concerns and is not feasible when the wireless transmission links are not of good quality. To overcome the challenge, a common approach is to quantize the data locally before transmission, which avoids exposure of raw data and significantly reduces the size of data. Compared with perfect data, quantization poses fundamental challenges on loss of data accuracy, which further impacts the convergence of the algorithms. To settle the problem, we propose a generalized distributed subgradient method with random quantization, which can be intepreted as a two time-scale stochastic approximation method. We provide comprehensive results on the convergence of the algorithm and derive upper bounds on the convergence rates in terms of the quantization bit, stepsizes and the number of network agents. Our results extend the existing results, where only special cases are considered and general conclusions for the convergence rates are missing. Finally, numerical simulations are conducted on linear regression problems to support our theoretical results.      
### 17.DNN-Free Low-Latency Adaptive Speech Enhancement Based on Frame-Online Beamforming Powered by Block-Online FastMNMF  [ :arrow_down: ](https://arxiv.org/pdf/2207.10934.pdf)
>  This paper describes a practical dual-process speech enhancement system that adapts environment-sensitive frame-online beamforming (front-end) with help from environment-free block-online source separation (back-end). To use minimum variance distortionless response (MVDR) beamforming, one may train a deep neural network (DNN) that estimates time-frequency masks used for computing the covariance matrices of sources (speech and noise). Backpropagation-based run-time adaptation of the DNN was proposed for dealing with the mismatched training-test conditions. Instead, one may try to directly estimate the source covariance matrices with a state-of-the-art blind source separation method called fast multichannel non-negative matrix factorization (FastMNMF). In practice, however, neither the DNN nor the FastMNMF can be updated in a frame-online manner due to its computationally-expensive iterative nature. Our DNN-free system leverages the posteriors of the latest source spectrograms given by block-online FastMNMF to derive the current source covariance matrices for frame-online beamforming. The evaluation shows that our frame-online system can quickly respond to scene changes caused by interfering speaker movements and outperformed an existing block-online system with DNN-based beamforming by 5.0 points in terms of the word error rate.      
### 18.XAI based Performance Preserving Adaptive Image Compression for Efficient Satellite Communication  [ :arrow_down: ](https://arxiv.org/pdf/2207.10885.pdf)
>  In the era of multinational cooperation, gathering and analyzing the satellite images are getting easier and more important. Typical procedure of the satellite image analysis include transmission of the bulky image data from satellite to the ground producing significant overhead. To reduce the amount of the transmission overhead while making no harm to the analysis result, we propose a novel image compression scheme RDIC in this paper. RDIC is a reasoning based image compression scheme that compresses an image according to the pixel importance score acquired from the analysis model itself. From the experimental results we showed that our RDIC scheme successfully captures the important regions in an image showing high compression rate and low accuracy loss.      
### 19.Optimizing Image Compression via Joint Learning with Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2207.10869.pdf)
>  High levels of noise usually exist in today's captured images due to the relatively small sensors equipped in the smartphone cameras, where the noise brings extra challenges to lossy image compression algorithms. Without the capacity to tell the difference between image details and noise, general image compression methods allocate additional bits to explicitly store the undesired image noise during compression and restore the unpleasant noisy image during decompression. Based on the observations, we optimize the image compression algorithm to be noise-aware as joint denoising and compression to resolve the bits misallocation problem. The key is to transform the original noisy images to noise-free bits by eliminating the undesired noise during compression, where the bits are later decompressed as clean images. Specifically, we propose a novel two-branch, weight-sharing architecture with plug-in feature denoisers to allow a simple and effective realization of the goal with little computational cost. Experimental results show that our method gains a significant improvement over the existing baseline methods on both the synthetic and real-world datasets. Our source code is available at <a class="link-external link-https" href="https://github.com/felixcheng97/DenoiseCompression" rel="external noopener nofollow">this https URL</a>.      
### 20.On the Spatial Pattern of Input-Output Metrics for a Network Synchronization Process  [ :arrow_down: ](https://arxiv.org/pdf/2207.10868.pdf)
>  A graph-theoretic analysis is undertaken for a compendium of input-output (transfer) metrics of a standard discrete-time linear synchronization model, including lp gains, frequency responses, frequency-band energy, and Markov parameters. We show that these transfer metrics exhibit a spatial degradation, such that they are monotonically nonincreasing along vertex cutsets away from an exogenous input. We use this spatial analysis to characterize signal-to-noise ratios (SNRs) in diffusive networks driven by process noise, and to develop a notion of propagation stability for dynamical networks. Finally, the formal results are illustrated through an example.      
### 21.Learn and Control while Switching: with Guaranteed Stability and Sublinear Regret  [ :arrow_down: ](https://arxiv.org/pdf/2207.10827.pdf)
>  Over-actuated systems often make it possible to achieve specific performances by switching between different subsets of actuators. However, when the system parameters are unknown, transferring authority to different subsets of actuators is challenging due to stability and performance efficiency concerns. This paper presents an efficient algorithm to tackle the so-called "learn and control while switching between different actuating modes" problem in the Linear Quadratic (LQ) setting. Our proposed strategy is constructed upon Optimism in the Face of Uncertainty (OFU) based algorithm equipped with a projection toolbox to keep the algorithm efficient, regret-wise. Along the way, we derive an optimum duration for the warm-up phase, thanks to the existence of a stabilizing neighborhood. The stability of the switched system is also guaranteed by designing a minimum average dwell time. The proposed strategy is proved to have a regret bound of $\mathcal{\bar{O}}\big(\sqrt{T}\big)+\mathcal{O}\big(ns\sqrt{T}\big)$ in horizon $T$ with $(ns)$ number of switches, provably outperforming naively applying the basic OFU algorithm.      
### 22.Explainable AI Algorithms for Vibration Data-based Fault Detection: Use Case-adadpted Methods and Critical Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2207.10732.pdf)
>  Analyzing vibration data using deep neural network algorithms is an effective way to detect damages in rotating machinery at an early stage. However, the black-box approach of these methods often does not provide a satisfactory solution because the cause of classifications is not comprehensible to humans. Therefore, this work investigates the application of explainable AI (XAI) algorithms to convolutional neural networks for vibration-based condition monitoring. For this, various XAI algorithms are applied to classifications based on the Fourier transform as well as the order analysis of the vibration signal. The results are visualized as a function of the revolutions per minute (RPM), in the shape of frequency-RPM maps and order-RPM maps. This allows to assess the saliency given to features which depend on the rotation speed and those with constant frequency. To compare the explanatory power of the XAI methods, investigations are first carried out with a synthetic data set with known class-specific characteristics. Then a real-world data set for vibration-based imbalance classification on an electric motor, which runs at a broad range of rotation speeds, is used. A special focus is put on the consistency for variable periodicity of the data, which translates to a varying rotation speed of a real-world machine. This work aims to show the different strengths and weaknesses of the methods for this use case: GradCAM, LRP and LIME with a new perturbation strategy.      
### 23.Retinex-qDPC: automatic background rectified quantitative differential phase contrast imaging  [ :arrow_down: ](https://arxiv.org/pdf/2207.10669.pdf)
>  The quality of quantitative differential phase contrast reconstruction (qDPC) can be severely degenerated by the mismatch of the background of two oblique illuminated images, yielding problematic phase recovery results. These background mismatches may result from illumination patterns, inhomogeneous media distribution, or other defocusing layers. In previous reports, the background is manually calibrated which is time-consuming, and unstable, since new calibrations are needed if any modification to the optical system was made. It is also impossible to calibrate the background from the defocusing layers, or for high dynamic observation as the background changes over time. To tackle the mismatch of background and increases the experimental robustness, we propose the Retinex-qDPC in which we use the images edge features as data fidelity term yielding L2-Retinex-qDPC and L1-Retinex-qDPC for high background-robustness qDPC reconstruction. The split Bregman method is used to solve the L1-Retinex DPC. We compare both Retinex-qDPC models against state-of-the-art DPC reconstruction algorithms including total-variation regularized qDPC, and isotropic-qDPC using both simulated and experimental data. Results show that the Retinex qDPC can significantly improve the phase recovery quality by suppressing the impact of mismatch background. Within, the L1-Retinex-qDPC is better than L2-Retinex and other state-of-the-art DPC algorithms. In general, the Retinex-qDPC increases the experimental robustness against background illumination without any modification of the optical system, which will benefit all qDPC applications.      
### 24.Improved lightweight identification of agricultural diseases based on MobileNetV3  [ :arrow_down: ](https://arxiv.org/pdf/2207.11238.pdf)
>  At present, the identification of agricultural pests and diseases has the problem that the model is not lightweight enough and difficult to apply. Based on MobileNetV3, this paper introduces the Coordinate Attention block. The parameters of MobileNetV3-large are reduced by 22%, the model size is reduced by 19.7%, and the accuracy is improved by 0.92%. The parameters of MobileNetV3-small are reduced by 23.4%, the model size is reduced by 18.3%, and the accuracy is increased by 0.40%. In addition, the improved MobileNetV3-small was migrated to Jetson Nano for testing. The accuracy increased by 2.48% to 98.31%, and the inference speed increased by 7.5%. It provides a reference for deploying the agricultural pest identification model to embedded devices.      
### 25.Learning Unsupervised Hierarchies of Audio Concepts  [ :arrow_down: ](https://arxiv.org/pdf/2207.11231.pdf)
>  Music signals are difficult to interpret from their low-level features, perhaps even more than images: e.g. highlighting part of a spectrogram or an image is often insufficient to convey high-level ideas that are genuinely relevant to humans. In computer vision, concept learning was therein proposed to adjust explanations to the right abstraction level (e.g. detect clinical concepts from radiographs). These methods have yet to be used for MIR. <br>In this paper, we adapt concept learning to the realm of music, with its particularities. For instance, music concepts are typically non-independent and of mixed nature (e.g. genre, instruments, mood), unlike previous work that assumed disentangled concepts. We propose a method to learn numerous music concepts from audio and then automatically hierarchise them to expose their mutual relationships. We conduct experiments on datasets of playlists from a music streaming service, serving as a few annotated examples for diverse concepts. Evaluations show that the mined hierarchies are aligned with both ground-truth hierarchies of concepts -- when available -- and with proxy sources of concept similarity in the general case.      
### 26.Face editing with GAN -- A Review  [ :arrow_down: ](https://arxiv.org/pdf/2207.11227.pdf)
>  In recent years, Generative Adversarial Networks (GANs) have become a hot topic among researchers and engineers that work with deep learning. It has been a ground-breaking technique which can generate new pieces of content of data in a consistent way. The topic of GANs has exploded in popularity due to its applicability in fields like image generation and synthesis, and music production and composition. GANs have two competing neural networks: a generator and a discriminator. The generator is used to produce new samples or pieces of content, while the discriminator is used to recognize whether the piece of content is real or generated. What makes it different from other generative models is its ability to learn unlabeled samples. In this review paper, we will discuss the evolution of GANs, several improvements proposed by the authors and a brief comparison between the different models. Index Terms generative adversarial networks, unsupervised learning, deep learning.      
### 27.Forest and Water Bodies Segmentation Through Satellite Images Using U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2207.11222.pdf)
>  Global environment monitoring is a task that requires additional attention in the contemporary rapid climate change environment. This includes monitoring the rate of deforestation and areas affected by flooding. Satellite imaging has greatly helped monitor the earth, and deep learning techniques have helped to automate this monitoring process. This paper proposes a solution for observing the area covered by the forest and water. To achieve this task UNet model has been proposed, which is an image segmentation model. The model achieved a validation accuracy of 82.55% and 82.92% for the segmentation of areas covered by forest and water, respectively.      
### 28.Learning to identify cracks on wind turbine blade surfaces using drone-based inspection images  [ :arrow_down: ](https://arxiv.org/pdf/2207.11186.pdf)
>  Wind energy is expected to be one of the leading ways to achieve the goals of the Paris Agreement but it in turn heavily depends on effective management of its operations and maintenance (O&amp;M) costs. Blade failures account for one-third of all O&amp;M costs thus making accurate detection of blade damages, especially cracks, very important for sustained operations and cost savings. Traditionally, damage inspection has been a completely manual process thus making it subjective, error-prone, and time-consuming. Hence in this work, we bring more objectivity, scalability, and repeatability in our damage inspection process, using deep learning, to miss fewer cracks. We build a deep learning model trained on a large dataset of blade damages, collected by our drone-based inspection, to correctly detect cracks. Our model is already in production and has processed more than a million damages with a recall of 0.96. We also focus on model interpretability using class activation maps to get a peek into the model workings. The model not only performs as good as human experts but also better in certain tricky cases. Thus, in this work, we aim to increase wind energy adoption by decreasing one of its major hurdles - the O\&amp;M costs resulting from missing blade failures like cracks.      
### 29.Secure and Lightweight Strong PUF Challenge Obfuscation with Keyed Non-linear FSR  [ :arrow_down: ](https://arxiv.org/pdf/2207.11181.pdf)
>  We propose a secure and lightweight key based challenge obfuscation for strong PUFs. Our architecture is designed to be resilient against learning attacks. Our obfuscation mechanism uses non-linear feedback shift registers (NLFSRs). Responses are directly provided to the user, without error correction or extra post-processing steps. We also discuss the cost of protecting our architecture against power analysis attacks with clock randomization, and Boolean masking. Security against learning attacks is assessed using avalanche criterion, and deep-neural network attacks. We designed a testchip in 65 nm CMOS. When compared to the baseline arbiter PUF implementation, the cost increase of our proposed architecture is 1.27x, and 2.2x when using clock randomization, and Boolean masking, respectively.      
### 30.Motion Planning and Control for Multi Vehicle Autonomous Racing at High Speeds  [ :arrow_down: ](https://arxiv.org/pdf/2207.11136.pdf)
>  This paper presents a multi-layer motion planning and control architecture for autonomous racing, capable of avoiding static obstacles, performing active overtakes, and reaching velocities above 75 $m/s$. The used offline global trajectory generation and the online model predictive controller are highly based on optimization and dynamic models of the vehicle, where the tires and camber effects are represented in an extended version of the basic Pacejka Magic Formula. The proposed single-track model is identified and validated using multi-body motorsport libraries which allow simulating the vehicle dynamics properly, especially useful when real experimental data are missing. The fundamental regularization terms and constraints of the controller are tuned to reduce the rate of change of the inputs while assuring an acceptable velocity and path tracking. The motion planning strategy consists of a Frenét-Frame-based planner which considers a forecast of the opponent produced by a Kalman filter. The planner chooses the collision-free path and velocity profile to be tracked on a 3 seconds horizon to realize different goals such as following and overtaking. The proposed solution has been applied on a Dallara AV-21 racecar and tested at oval race tracks achieving lateral accelerations up to 25 $m/s^{2}$.      
### 31.On Controller Tuning with Time-Varying Bayesian Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2207.11120.pdf)
>  Changing conditions or environments can cause system dynamics to vary over time. To ensure optimal control performance, controllers should adapt to these changes. When the underlying cause and time of change is unknown, we need to rely on online data for this adaptation. In this paper, we will use time-varying Bayesian optimization (TVBO) to tune controllers online in changing environments using appropriate prior knowledge on the control objective and its changes. Two properties are characteristic of many online controller tuning problems: First, they exhibit incremental and lasting changes in the objective due to changes to the system dynamics, e.g., through wear and tear. Second, the optimization problem is convex in the tuning parameters. Current TVBO methods do not explicitly account for these properties, resulting in poor tuning performance and many unstable controllers through over-exploration of the parameter space. We propose a novel TVBO forgetting strategy using Uncertainty-Injection (UI), which incorporates the assumption of incremental and lasting changes. The control objective is modeled as a spatio-temporal Gaussian process (GP) with UI through a Wiener process in the temporal domain. Further, we explicitly model the convexity assumptions in the spatial dimension through GP models with linear inequality constraints. In numerical experiments, we show that our model outperforms the state-of-the-art method in TVBO, exhibiting reduced regret and fewer unstable parameter configurations.      
### 32.Near Real-Time Distributed State Estimation via AI/ML-Empowered 5G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11117.pdf)
>  Fifth-Generation (5G) networks have a potential to accelerate power system transition to a flexible, softwarized, data-driven, and intelligent grid. With their evolving support for Machine Learning (ML)/Artificial Intelligence (AI) functions, 5G networks are expected to enable novel data-centric Smart Grid (SG) services. In this paper, we explore how data-driven SG services could be integrated with ML/AI-enabled 5G networks in a symbiotic relationship. We focus on the State Estimation (SE) function as a key element of the energy management system and focus on two main questions. Firstly, in a tutorial fashion, we present an overview on how distributed SE can be integrated with the elements of the 5G core network and radio access network architecture. Secondly, we present and compare two powerful distributed SE methods based on: i) graphical models and belief propagation, and ii) graph neural networks. We discuss their performance and capability to support a near real-time distributed SE via 5G network, taking into account communication delays.      
### 33.Inference skipping for more efficient real-time speech enhancement with parallel RNNs  [ :arrow_down: ](https://arxiv.org/pdf/2207.11108.pdf)
>  Deep neural network (DNN) based speech enhancement models have attracted extensive attention due to their promising performance. However, it is difficult to deploy a powerful DNN in real-time applications because of its high computational cost. Typical compression methods such as pruning and quantization do not make good use of the data characteristics. In this paper, we introduce the Skip-RNN strategy into speech enhancement models with parallel RNNs. The states of the RNNs update intermittently without interrupting the update of the output mask, which leads to significant reduction of computational load without evident audio artifacts. To better leverage the difference between the voice and the noise, we further regularize the skipping strategy with voice activity detection (VAD) guidance, saving more computational load. Experiments on a high-performance speech enhancement model, dual-path convolutional recurrent network (DPCRN), show the superiority of our strategy over strategies like network pruning or directly training a smaller model. We also validate the generalization of the proposed strategy on two other competitive speech enhancement models.      
### 34.SIM-STEM Lab: Incorporating Compressed Sensing Theory for Fast STEM Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2207.10984.pdf)
>  Recently it has been shown that precise dose control and an increase in the overall acquisition speed of atomic resolution scanning transmission electron microscope (STEM) images can be achieved by acquiring only a small fraction of the pixels in the image experimentally and then reconstructing the full image using an inpainting algorithm. In this paper, we apply the same inpainting approach (a form of compressed sensing) to simulated, sub-sampled atomic resolution STEM images. We find that it is possible to significantly sub-sample the area that is simulated, the number of g-vectors contributing the image, and the number of frozen phonon configurations contributing to the final image while still producing an acceptable fit to a fully sampled simulation. Here we discuss the parameters that we use and how the resulting simulations can be quantifiably compared to the full simulations. As with any Compressed Sensing methodology, care must be taken to ensure that isolated events are not excluded from the process, but the observed increase in simulation speed provides significant opportunities for real time simulations, image classification and analytics to be performed as a supplement to experiments on a microscope to be developed in the future.      
### 35.Head-Related Transfer Function Interpolation from Spatially Sparse Measurements Using Autoencoder with Source Position Conditioning  [ :arrow_down: ](https://arxiv.org/pdf/2207.10967.pdf)
>  We propose a method of head-related transfer function (HRTF) interpolation from sparsely measured HRTFs using an autoencoder with source position conditioning. The proposed method is drawn from an analogy between an HRTF interpolation method based on regularized linear regression (RLR) and an autoencoder. Through this analogy, we found the key feature of the RLR-based method that HRTFs are decomposed into source-position-dependent and source-position-independent factors. On the basis of this finding, we design the encoder and decoder so that their weights and biases are generated from source positions. Furthermore, we introduce an aggregation module that reduces the dependence of latent variables on source position for obtaining a source-position-independent representation of each subject. Numerical experiments show that the proposed method can work well for unseen subjects and achieve an interpolation performance with only one-eighth measurements comparable to that of the RLR-based method.      
### 36.On the Accessibility and Controllability of Statistical Linearization for Stochastic Control: Algebraic Rank Conditions and their Genericity  [ :arrow_down: ](https://arxiv.org/pdf/2207.10944.pdf)
>  Statistical linearization has recently seen a particular surge of interest as a numerically cheap method for robust control of stochastic differential equations. Although it has already been successfully applied to control complex stochastic systems, accessibility and controllability properties of statistical linearization, which are key to make the robust control problem well-posed, have not been investigated yet. In this paper, we bridge this gap by providing sufficient conditions for the accessibility and controllability of statistical linearization. Specifically, we establish simple sufficient algebraic conditions for the accessibility and controllability of statistical linearization, which involve the rank of the Lie algebra generated by the drift only. In addition, we show these latter algebraic conditions are essentially sharp, by means of a counterexample, and that they are generic with respect to the drift and the initial condition.      
### 37.Statistical Hypothesis Testing Based on Machine Learning: Large Deviations Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.10939.pdf)
>  We study the performance -- and specifically the rate at which the error probability converges to zero -- of Machine Learning (ML) classification techniques. Leveraging the theory of large deviations, we provide the mathematical conditions for a ML classifier to exhibit error probabilities that vanish exponentially, say $\sim \exp\left(-n\,I + o(n) \right)$, where $n$ is the number of informative observations available for testing (or another relevant parameter, such as the size of the target in an image) and $I$ is the error rate. Such conditions depend on the Fenchel-Legendre transform of the cumulant-generating function of the Data-Driven Decision Function (D3F, i.e., what is thresholded before the final binary decision is made) learned in the training phase. As such, the D3F and, consequently, the related error rate $I$, depend on the given training set, which is assumed of finite size. Interestingly, these conditions can be verified and tested numerically exploiting the available dataset, or a synthetic dataset, generated according to the available information on the underlying statistical model. In other words, the classification error probability convergence to zero and its rate can be computed on a portion of the dataset available for training. Coherently with the large deviations theory, we can also establish the convergence, for $n$ large enough, of the normalized D3F statistic to a Gaussian distribution. This property is exploited to set a desired asymptotic false alarm probability, which empirically turns out to be accurate even for quite realistic values of $n$. Furthermore, approximate error probability curves $\sim \zeta_n \exp\left(-n\,I \right)$ are provided, thanks to the refined asymptotic derivation (often referred to as exact asymptotics), where $\zeta_n$ represents the most representative sub-exponential terms of the error probabilities.      
### 38.Physics-informed convolutional neural network with bicubic spline interpolation for sound field estimation  [ :arrow_down: ](https://arxiv.org/pdf/2207.10937.pdf)
>  A sound field estimation method based on a physics-informed convolutional neural network (PICNN) using spline interpolation is proposed. Most of the sound field estimation methods are based on wavefunction expansion, making the estimated function satisfy the Helmholtz equation. However, these methods rely only on physical properties; thus, they suffer from a significant deterioration of accuracy when the number of measurements is small. Recent learning-based methods based on neural networks have advantages in estimating from sparse measurements when training data are available. However, since physical properties are not taken into consideration, the estimated function can be a physically infeasible solution. We propose the application of PICNN to the sound field estimation problem by using a loss function that penalizes deviation from the Helmholtz equation. Since the output of CNN is a spatially discretized pressure distribution, it is difficult to directly evaluate the Helmholtz-equation loss function. Therefore, we incorporate bicubic spline interpolation in the PICNN framework. Experimental results indicated that accurate and physically feasible estimation from sparse measurements can be achieved with the proposed method.      
### 39.Delay-Doppler Reversal for OTFS System in Doubly-selective Fading Channels  [ :arrow_down: ](https://arxiv.org/pdf/2207.10910.pdf)
>  The recent proposed orthogonal time frequency space (OTFS) modulation shows signifcant advantages than conventional orthogonal frequency division multiplexing (OFDM) for high mobility wireless communications. However, a challenging problem is the development of effcient receivers for practical OTFS systems with low complexity. In this paper, we propose a novel delay-Doppler reversal (DDR) technology for OTFS system with desired performance and low complexity. We present the DDR technology from a perspective of two-dimensional cascaded channel model, analyze its computational complexity and also analyze its performance gain compared to the direct processing (DP) receiver without DDR. Simulation results demonstrate that our proposed DDR receiver outperforms traditional receivers in doubly-selective fading channels.      
### 40.Cell-Free Massive MIMO with Multi-Antenna Users over Weichselberger Rician Channels  [ :arrow_down: ](https://arxiv.org/pdf/2207.10891.pdf)
>  We consider a cell-free massive multiple-input multiple-output (MIMO) system with multi-antenna access points and user equipments (UEs) over Weichselberger Rician fading channels with random phase-shifts. More specifically, we investigate the uplink spectral efficiency (SE) for two pragmatic processing schemes: 1) the fully centralized processing scheme with global minimum mean square error (MMSE) or maximum ratio (MR) combining; 2) the large-scale fading decoding (LSFD) scheme with local MMSE or MR combining. To improve the system SE performance, we propose a practical uplink precoding scheme based on only the eigenbasis of the UE-side correlation matrices. Moreover, we derive novel closed-form SE expressions for characterizing the LSFD scheme with the MR combining. Numerical results validate the accuracy of our derived expressions and show that the proposed precoding scheme can significantly improve the SE performance compared with the scenario without any precoding scheme.      
### 41.ASR Error Detection via Audio-Transcript entailment  [ :arrow_down: ](https://arxiv.org/pdf/2207.10849.pdf)
>  Despite improved performances of the latest Automatic Speech Recognition (ASR) systems, transcription errors are still unavoidable. These errors can have a considerable impact in critical domains such as healthcare, when used to help with clinical documentation. Therefore, detecting ASR errors is a critical first step in preventing further error propagation to downstream applications. To this end, we propose a novel end-to-end approach for ASR error detection using audio-transcript entailment. To the best of our knowledge, we are the first to frame this problem as an end-to-end entailment task between the audio segment and its corresponding transcript segment. Our intuition is that there should be a bidirectional entailment between audio and transcript when there is no recognition error and vice versa. The proposed model utilizes an acoustic encoder and a linguistic encoder to model the speech and transcript respectively. The encoded representations of both modalities are fused to predict the entailment. Since doctor-patient conversations are used in our experiments, a particular emphasis is placed on medical terms. Our proposed model achieves classification error rates (CER) of 26.2% on all transcription errors and 23% on medical errors specifically, leading to improvements upon a strong baseline by 12% and 15.4%, respectively.      
### 42.Robust and Safe Autonomous Navigation for Systems with Learned SE(3) Hamiltonian Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2207.10840.pdf)
>  Stability and safety are critical properties for successful deployment of automatic control systems. As a motivating example, consider autonomous mobile robot navigation in a complex environment. A control design that generalizes to different operational conditions requires a model of the system dynamics, robustness to modeling errors, and satisfaction of safety \NEWZL{constraints}, such as collision avoidance. This paper develops a neural ordinary differential equation network to learn the dynamics of a Hamiltonian system from trajectory data. The learned Hamiltonian model is used to synthesize an energy-shaping passivity-based controller and analyze its \emph{robustness} to uncertainty in the learned model and its \emph{safety} with respect to constraints imposed by the environment. Given a desired reference path for the system, we extend our design using a virtual reference governor to achieve tracking control. The governor state serves as a regulation point that moves along the reference path adaptively, balancing the system energy level, model uncertainty bounds, and distance to safety violation to guarantee robustness and safety. Our Hamiltonian dynamics learning and tracking control techniques are demonstrated on \Revised{simulated hexarotor and quadrotor robots} navigating in cluttered 3D environments.      
### 43.Characterizing Coherent Integrated Photonic Neural Networks under Imperfections  [ :arrow_down: ](https://arxiv.org/pdf/2207.10835.pdf)
>  Integrated photonic neural networks (IPNNs) are emerging as promising successors to conventional electronic AI accelerators as they offer substantial improvements in computing speed and energy efficiency. In particular, coherent IPNNs use arrays of Mach-Zehnder interferometers (MZIs) for unitary transformations to perform energy-efficient matrix-vector multiplication. However, the underlying MZI devices in IPNNs are susceptible to uncertainties stemming from optical lithographic variations and thermal crosstalk and can experience imprecisions due to non-uniform MZI insertion loss and quantization errors due to low-precision encoding in the tuned phase angles. In this paper, we, for the first time, systematically characterize the impact of such uncertainties and imprecisions (together referred to as imperfections) in IPNNs using a bottom-up approach. We show that their impact on IPNN accuracy can vary widely based on the tuned parameters (e.g., phase angles) of the affected components, their physical location, and the nature and distribution of the imperfections. To improve reliability measures, we identify critical IPNN building blocks that, under imperfections, can lead to catastrophic degradation in the classification accuracy. We show that under multiple simultaneous imperfections, the IPNN inferencing accuracy can degrade by up to 46%, even when the imperfection parameters are restricted within a small range. Our results also indicate that the inferencing accuracy is sensitive to imperfections affecting the MZIs in the linear layers next to the input layer of the IPNN.      
### 44.End-to-End and Self-Supervised Learning for ComParE 2022 Stuttering Sub-Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2207.10817.pdf)
>  In this paper, we present end-to-end and speech embedding based systems trained in a self-supervised fashion to participate in the ACM Multimedia 2022 ComParE Challenge, specifically the stuttering sub-challenge. In particular, we exploit the embeddings from the pre-trained Wav2Vec2.0 model for stuttering detection (SD) on the KSoF dataset. After embedding extraction, we benchmark with several methods for SD. Our proposed self-supervised based SD system achieves a UAR of 36.9% and 41.0% on validation and test sets respectively, which is 31.32% (validation set) and 1.49% (test set) higher than the best (DeepSpectrum) challenge baseline (CBL). Moreover, we show that concatenating layer embeddings with Mel-frequency cepstral coefficients (MFCCs) features further improves the UAR of 33.81% and 5.45% on validation and test sets respectively over the CBL. Finally, we demonstrate that the summing information across all the layers of Wav2Vec2.0 surpasses the CBL by a relative margin of 45.91% and 5.69% on validation and test sets respectively. Grand-challenge: Computational Paralinguistics ChallengE      
### 45.Smart speaker design and implementation with biometric authentication and advanced voice interaction capability  [ :arrow_down: ](https://arxiv.org/pdf/2207.10811.pdf)
>  Advancements in semiconductor technology have reduced dimensions and cost while improving the performance and capacity of chipsets. In addition, advancement in the AI frameworks and libraries brings possibilities to accommodate more AI at the resource-constrained edge of consumer IoT devices. Sensors are nowadays an integral part of our environment which provide continuous data streams to build intelligent applications. An example could be a smart home scenario with multiple interconnected devices. In such smart environments, for convenience and quick access to web-based service and personal information such as calendars, notes, emails, reminders, banking, etc, users link third-party skills or skills from the Amazon store to their smart speakers. Also, in current smart home scenarios, several smart home products such as smart security cameras, video doorbells, smart plugs, smart carbon monoxide monitors, and smart door locks, etc. are interlinked to a modern smart speaker via means of custom skill addition. Since smart speakers are linked to such services and devices via the smart speaker user's account. They can be used by anyone with physical access to the smart speaker via voice commands. If done so, the data privacy, home security and other aspects of the user get compromised. Recently launched, Tensor Cam's AI Camera, Toshiba's Symbio, Facebook's Portal are camera-enabled smart speakers with AI functionalities. Although they are camera-enabled, yet they do not have an authentication scheme in addition to calling out the wake-word. This paper provides an overview of cybersecurity risks faced by smart speaker users due to lack of authentication scheme and discusses the development of a state-of-the-art camera-enabled, microphone array-based modern Alexa smart speaker prototype to address these risks.      
### 46.A Machine Learning Approach for Driver Identification Based on CAN-BUS Sensor Data  [ :arrow_down: ](https://arxiv.org/pdf/2207.10807.pdf)
>  Driver identification is a momentous field of modern decorated vehicles in the controller area network (CAN-BUS) perspective. Many conventional systems are used to identify the driver. One step ahead, most of the researchers use sensor data of CAN-BUS but there are some difficulties because of the variation of the protocol of different models of vehicle. Our aim is to identify the driver through supervised learning algorithms based on driving behavior analysis. To determine the driver, a driver verification technique is proposed that evaluate driving pattern using the measurement of CAN sensor data. In this paper on-board diagnostic (OBD-II) is used to capture the data from the CAN-BUS sensor and the sensors are listed under SAE J1979 statement. According to the service of OBD-II, drive identification is possible. However, we have gained two types of accuracy on a complete data set with 10 drivers and a partial data set with two drivers. The accuracy is good with less number of drivers compared to the higher number of drivers. We have achieved statistically significant results in terms of accuracy in contrast to the baseline algorithm      
### 47.PowerFDNet: Deep Learning-Based Stealthy False Data Injection Attack Detection for AC-model Transmission Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.10805.pdf)
>  Recent studies have demonstrated that smart grids are vulnerable to stealthy false data injection attacks (SFDIAs), as SFDIAs can bypass residual-based bad data detection mechanisms. The SFDIA detection has become one of the focuses of smart grid research. Methods based on deep learning technology have shown promising accuracy in the detection of SFDIAs. However, most existing methods rely on the temporal structure of a sequence of measurements but do not take account of the spatial structure between buses and transmission lines. To address this issue, we propose a spatiotemporal deep network, PowerFDNet, for the SFDIA detection in AC-model power grids. The PowerFDNet consists of two sub-architectures: spatial architecture (SA) and temporal architecture (TA). The SA is aimed at extracting representations of bus/line measurements and modeling the spatial structure based on their representations. The TA is aimed at modeling the temporal structure of a sequence of measurements. Therefore, the proposed PowerFDNet can effectively model the spatiotemporal structure of measurements. Case studies on the detection of SFDIAs on the benchmark smart grids show that the PowerFDNet achieved significant improvement compared with the state-of-the-art SFDIA detection methods. In addition, an IoT-oriented lightweight prototype of size 52 MB is implemented and tested for mobile devices, which demonstrates the potential applications on mobile devices. The trained model will be available at \textit{<a class="link-external link-https" href="https://github.com/FrankYinXF/PowerFDNet" rel="external noopener nofollow">this https URL</a>}.      
### 48.Strategising template-guided needle placement for MR-targeted prostate biopsy  [ :arrow_down: ](https://arxiv.org/pdf/2207.10784.pdf)
>  Clinically significant prostate cancer has a better chance to be sampled during ultrasound-guided biopsy procedures, if suspected lesions found in pre-operative magnetic resonance (MR) images are used as targets. However, the diagnostic accuracy of the biopsy procedure is limited by the operator-dependent skills and experience in sampling the targets, a sequential decision making process that involves navigating an ultrasound probe and placing a series of sampling needles for potentially multiple targets. This work aims to learn a reinforcement learning (RL) policy that optimises the actions of continuous positioning of 2D ultrasound views and biopsy needles with respect to a guiding template, such that the MR targets can be sampled efficiently and sufficiently. We first formulate the task as a Markov decision process (MDP) and construct an environment that allows the targeting actions to be performed virtually for individual patients, based on their anatomy and lesions derived from MR images. A patient-specific policy can thus be optimised, before each biopsy procedure, by rewarding positive sampling in the MDP environment. Experiment results from fifty four prostate cancer patients show that the proposed RL-learned policies obtained a mean hit rate of 93% and an average cancer core length of 11 mm, which compared favourably to two alternative baseline strategies designed by humans, without hand-engineered rewards that directly maximise these clinically relevant metrics. Perhaps more interestingly, it is found that the RL agents learned strategies that were adaptive to the lesion size, where spread of the needles was prioritised for smaller lesions. Such a strategy has not been previously reported or commonly adopted in clinical practice, but led to an overall superior targeting performance when compared with intuitively designed strategies.      
### 49.Data-Driven Stochastic AC-OPF using Gaussian Processes  [ :arrow_down: ](https://arxiv.org/pdf/2207.10781.pdf)
>  In recent years, electricity generation has been responsible for more than a quarter of the greenhouse gas emissions in the US. Integrating a significant amount of renewables into a power grid is probably the most accessible way to reduce carbon emissions from power grids and slow down climate change. Unfortunately, the most accessible renewable power sources, such as wind and solar, are highly fluctuating and thus bring a lot of uncertainty to power grid operations and challenge existing optimization and control policies. The chance-constrained alternating current (AC) optimal power flow (OPF) framework finds the minimum cost generation dispatch maintaining the power grid operations within security limits with a prescribed probability. Unfortunately, the AC-OPF problem's chance-constrained extension is non-convex, computationally challenging, and requires knowledge of system parameters and additional assumptions on the behavior of renewable distribution. Known linear and convex approximations to the above problems, though tractable, are too conservative for operational practice and do not consider uncertainty in system parameters. This paper presents an alternative data-driven approach based on Gaussian process (GP) regression to close this gap. The GP approach learns a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertainty inputs. The latter is then used to determine the solution of CC-OPF efficiently, by accounting for both input and parameter uncertainty. The practical efficiency of the proposed approach using different approximations for GP-uncertainty propagation is illustrated over numerous IEEE test cases.      
### 50.A Proposal for Foley Sound Synthesis Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2207.10760.pdf)
>  "Foley" refers to sound effects that are added to multimedia during post-production to enhance its perceived acoustic properties, e.g., by simulating the sounds of footsteps, ambient environmental sounds, or visible objects on the screen. While foley is traditionally produced by foley artists, there is increasing interest in automatic or machine-assisted techniques building upon recent advances in sound synthesis and generative models. To foster more participation in this growing research area, we propose a challenge for automatic foley synthesis. Through case studies on successful previous challenges in audio and machine learning, we set the goals of the proposed challenge: rigorous, unified, and efficient evaluation of different foley synthesis systems, with an overarching goal of drawing active participation from the research community. We outline the details and design considerations of a foley sound synthesis challenge, including task definition, dataset requirements, and evaluation criteria.      
### 51.STARS Enabled Integrated Sensing and Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.10748.pdf)
>  A simultaneously transmitting and reflecting intelligent surface (STARS) enabled integrated sensing and communications (ISAC) framework is proposed, where the whole space is divided by STARS into a sensing space and a communication space. A novel sensing-at-STARS structure, where dedicated sensors are installed at the STARS, is proposed to address the significant path loss and clutter interference for sensing. The Cramer-Rao bound (CRB) of the 2-dimension (2D) direction-of-arrivals (DOAs) estimation of the sensing target is derived, which is then minimized subject to the minimum communication requirement. A novel approach is proposed to transform the complicated CRB minimization problem into a trackable modified Fisher information matrix (FIM) optimization problem. Both independent and coupled phase-shift models of STARS are investigated: 1) For the independent phase-shift model, to address the coupling of ISAC waveform and STARS coefficient in the modified FIM, an efficient double-loop iterative algorithm based on the penalty dual decomposition (PDD) framework is conceived; 2) For the coupled phase-shift model, based on the PDD framework, a low complexity alternating optimization algorithm is proposed to tackle coupled phase-shift constants by alternatively optimizing amplitude and phase-shift coefficients in closed-form. Finally, the numerical results demonstrate that: 1) STARS significantly outperforms the conventional RIS in CRB under the communication constraints; 2) The coupled phase-shift model achieves comparable performance to the independent one for low communication requirements or sufficient STARS elements; 3) It is more efficient to increase the number of passive elements of STARS rather than the active elements of the sensor; 4) High sensing accuracy can be achieved by STARS using the practical 2D maximum likelihood estimator compared with the conventional RIS.      
### 52.Decision-Feedback Detection for Bidirectional Molecular Relaying with Direct Links  [ :arrow_down: ](https://arxiv.org/pdf/2207.10728.pdf)
>  In this paper, we consider bidirectional relaying between two diffusion-based molecular transceivers (bio-nodes). As opposed to existing literature, we incorporate the effect of direct diffusion links between the nodes and leverage it to improve performance. Assuming network coding type operation at the relay, we devise a detection strategy, based on the maximum-likelihood principle, that combines the signal received from the relay and that received from the direct link. At the same time, since a diffusion-based molecular communication channel is characterized by high inter-symbol interference (ISI), we utilize a decision feedback mechanism to mitigate its effect. Simulation results indicate that the proposed setup incorporating the direct link can achieve notable improvement in error performance over conventional detection schemes that do not exploit the direct link and/or do not attempt to mitigate the effect of ISI.      
### 53.Hardware-Efficient Template-Based Deep CNNs Accelerator Design  [ :arrow_down: ](https://arxiv.org/pdf/2207.10723.pdf)
>  Acceleration of Convolutional Neural Network (CNN) on edge devices has recently achieved a remarkable performance in image classification and object detection applications. This paper proposes an efficient and scalable CNN-based SoC-FPGA accelerator design that takes pre-trained weights with a 16-bit fixed-point quantization and target hardware specification to generate an optimized template capable of achieving higher performance versus resource utilization trade-off. The template analyzed the computational workload, data dependency, and external memory bandwidth and utilized loop tiling transformation along with dataflow modeling to convert convolutional and fully connected layers into vector multiplication between input and output feature maps, which resulted in a single compute unit on-chip. Furthermore, the accelerator was examined among AlexNet, VGG16, and LeNet networks and ran at 200-MHz with a peak performance of 230 GOP/s depending on ZYNQ boards and state-space exploration of different compute unit configurations during simulation and synthesis. Lastly, our proposed methodology was benchmarked against the previous development on Ultra96 for higher performance measurement.      
### 54.Fusing Frame and Event Vision for High-speed Optical Flow for Edge Application  [ :arrow_down: ](https://arxiv.org/pdf/2207.10720.pdf)
>  Optical flow computation with frame-based cameras provides high accuracy but the speed is limited either by the model size of the algorithm or by the frame rate of the camera. This makes it inadequate for high-speed applications. Event cameras provide continuous asynchronous event streams overcoming the frame-rate limitation. However, the algorithms for processing the data either borrow frame like setup limiting the speed or suffer from lower accuracy. We fuse the complementary accuracy and speed advantages of the frame and event-based pipelines to provide high-speed optical flow while maintaining a low error rate. Our bio-mimetic network is validated with the MVSEC dataset showing 19% error degradation at 4x speed up. We then demonstrate the system with a high-speed drone flight scenario where a high-speed event camera computes the flow even before the optical camera sees the drone making it suited for applications like tracking and segmentation. This work shows the fundamental trade-offs in frame-based processing may be overcome by fusing data from other modalities.      
### 55.ME-GAN: Learning Panoptic Electrocardio Representations for Multi-view ECG Synthesis Conditioned on Heart Diseases  [ :arrow_down: ](https://arxiv.org/pdf/2207.10670.pdf)
>  Electrocardiogram (ECG) is a widely used non-invasive diagnostic tool for heart diseases. Many studies have devised ECG analysis models (e.g., classifiers) to assist diagnosis. As an upstream task, researches have built generative models to synthesize ECG data, which are beneficial to providing training samples, privacy protection, and annotation reduction. However, previous generative methods for ECG often neither synthesized multi-view data, nor dealt with heart disease conditions. In this paper, we propose a novel disease-aware generative adversarial network for multi-view ECG synthesis called ME-GAN, which attains panoptic electrocardio representations conditioned on heart diseases and projects the representations onto multiple standard views to yield ECG signals. Since ECG manifestations of heart diseases are often localized in specific waveforms, we propose a new "mixup normalization" to inject disease information precisely into suitable locations. In addition, we propose a view discriminator to revert disordered ECG views into a pre-determined order, supervising the generator to obtain ECG representing correct view characteristics. Besides, a new metric, rFID, is presented to assess the quality of the synthesized ECG signals. Comprehensive experiments verify that our ME-GAN performs well on multi-view ECG signal synthesis with trusty morbid manifestations.      
### 56.Network structural origin of instabilities in large complex systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.07680.pdf)
>  A central issue in the study of large complex network systems, such as power grids, financial networks, and ecological systems, is to understand their response to dynamical perturbations. Recent studies recognize that many real networks show nonnormality and that nonnormality can give rise to reactivity--the capacity of a linearly stable system to amplify its response to perturbations, oftentimes exciting nonlinear instabilities. Here, we identify network structural properties underlying the pervasiveness of nonnormality and reactivity in real directed networks, which we establish using the most extensive data set of such networks studied in this context to date. The identified properties are imbalances between incoming and outgoing network links and paths at each node. Based on this characterization, we develop a theory that quantitatively predicts nonnormality and reactivity and explains the observed pervasiveness. We suggest that these results can be used to design, upgrade, control, and manage networks to avoid or promote network instabilities.      
