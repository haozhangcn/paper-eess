# ArXiv eess --Tue, 26 Jul 2022
### 1.Hosting Capacity Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.12357.pdf)
>  This chapter proposes an evolved concept of "hosting capacity" using the term of "feasible region". Through converting the grid model into a more compact one, "hosting capacity region" not only is promising to further exploit the grid potential for power delivery, but also benefits grid operation feasibility investigation with concise formulas. The hosting capacity region assessment schemes are exploited as well. Facing the derived hosting capacity, originally complicated energy storage optimization problems can be represented algebraically, which is more efficient and friendly for computer processing. Case studies based on a 10.5kV Dutch grid have been implemented, eventually confirming the validity of relevant assessment and optimization methods.      
### 2.Edge-Aware Autoencoder Design for Real-Time Mixture-of-Experts Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2207.12348.pdf)
>  Steered-Mixtures-of-Experts (SMoE) models provide sparse, edge-aware representations, applicable to many use-cases in image processing. This includes denoising, super-resolution and compression of 2D- and higher dimensional pixel data. Recent works for image compression indicate that compression of images based on SMoE models can provide competitive performance to the state-of-the-art. Unfortunately, the iterative model-building process at the encoder comes with excessive computational demands. In this paper we introduce a novel edge-aware Autoencoder (AE) strategy designed to avoid the time-consuming iterative optimization of SMoE models. This is done by directly mapping pixel blocks to model parameters for compression, in spirit similar to recent works on "unfolding" of algorithms, while maintaining full compatibility to the established SMoE framework. With our plug-in AE encoder, we achieve a quantum-leap in performance with encoder run-time savings by a factor of 500 to 1000 with even improved image reconstruction quality. For image compression the plug-in AE encoder has real-time properties and improves RD-performance compared to our previous works.      
### 3.Localization of Coordinated Cyber-Physical Attacks in Power Grids Using Moving Target Defense and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.12339.pdf)
>  As one of the most sophisticated attacks against power grids, coordinated cyber-physical attacks (CCPAs) damage the power grid's physical infrastructure and use a simultaneous cyber attack to mask its effect. This work proposes a novel approach to detect such attacks and identify the location of the line outages (due to the physical attack). The proposed approach consists of three parts. Firstly, moving target defense (MTD) is applied to expose the physical attack by actively perturbing transmission line reactance via distributed flexible AC transmission system (D-FACTS) devices. MTD invalidates the attackers' knowledge required to mask their physical attack. Secondly, convolution neural networks (CNNs) are applied to localize line outage position from the compromised measurements. Finally, model agnostic meta-learning (MAML) is used to accelerate the training speed of CNN following the topology reconfigurations (due to MTD) and reduce the data/retraining time requirements. Simulations are carried out using IEEE test systems. The experimental results demonstrate that the proposed approach can effectively localize line outages in stealthy CCPAs.      
### 4.Cyber-Resilient Frequency Control of Power Grids with Energy Storage Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.12333.pdf)
>  The integration of synchronous generators and energy storage systems operated through communication networks introduces new challenges and vulnerabilities to the electric grid, where cyber attacks can corrupt sensor measurements or control inputs and interrupt functions such as frequency regulation. This paper proposes a defense methodology for the design of resilient operating constraints imposed on each generation and storage unit in order to prevent any attack sequence from driving the system's frequency to unsafe conditions. The resilient operating constraints are found by using ellipsoidal approximations of the reachable set of the power system, leading to a convex optimization problem with linear matrix inequalities. Numerical results in a single-area power system with synchronous generation and energy storage demonstrate how the resilient constraints provide security guarantees against any type of attack affecting frequency measurements or controller setpoints.      
### 5.Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2207.12262.pdf)
>  Sequence-to-Sequence Text-to-Speech architectures that directly generate low level acoustic features from phonetic sequences are known to produce natural and expressive speech when provided with adequate amounts of training data. Such systems can learn and transfer desired speaking styles from one seen speaker to another (in multi-style multi-speaker settings), which is highly desirable for creating scalable and customizable Human-Computer Interaction systems. In this work we explore one-to-many style transfer from a dedicated single-speaker conversational corpus with style nuances and interjections. We elaborate on the corpus design and explore the feasibility of such style transfer when assisted with Voice-Conversion-based data augmentation. In a set of subjective listening experiments, this approach resulted in high-fidelity style transfer with no quality degradation. However, a certain voice persona shift was observed, requiring further improvements in voice conversion.      
### 6.OCTAve: 2D en face Optical Coherence Tomography Angiography Vessel Segmentation in Weakly-Supervised Learning with Locality Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.12238.pdf)
>  While there have been increased researches using deep learning techniques for the extraction of vascular structure from the 2D en face OCTA, for such approach, it is known that the data annotation process on the curvilinear structure like the retinal vasculature is very costly and time consuming, albeit few tried to address the annotation problem. <br>In this work, we propose the application of the scribble-base weakly-supervised learning method to automate the pixel-level annotation. The proposed method, called OCTAve, combines the weakly-supervised learning using scribble-annotated ground truth augmented with an adversarial and a novel self-supervised deep supervision. Our novel mechanism is designed to utilize the discriminative outputs from the discrimination layer of a UNet-like architecture where the Kullback-Liebler Divergence between the aggregate discriminative outputs and the segmentation map predicate is minimized during the training. This combined method leads to the better localization of the vascular structure as shown in our experiments. We validate our proposed method on the large public datasets i.e., ROSE, OCTA-500. The segmentation performance is compared against both state-of-the-art fully-supervised and scribble-based weakly-supervised approaches. The implementation of our work used in the experiments is located at [LINK].      
### 7.Cov3d: Detection of the presence and severity of COVID-19 from CT scans using 3D ResNets  [ :arrow_down: ](https://arxiv.org/pdf/2207.12218.pdf)
>  Deep learning has been used to assist in the analysis of medical imaging. One such use is the classification of Computed Tomography (CT) scans when detecting for COVID-19 in subjects. This paper presents Cov3d, a three dimensional convolutional neural network for detecting the presence and severity of COVID19 from chest CT scans. Trained on the COV19-CT-DB dataset with human expert annotations, it achieves a macro f1 score of 0.9476 on the validation set for the task of detecting the presence of COVID19. For the task of classifying the severity of COVID19, it achieves a macro f1 score of 0.7552. Both results improve on the baseline results of the `AI-enabled Medical Image Analysis Workshop and Covid-19 Diagnosis Competition' (MIA-COV19D) in 2022.      
### 8.Complexity Reduction over Bi-RNN-Based Nonlinearity Mitigation in Dual-Pol Fiber-Optic Communications via a CRNN-Based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.12154.pdf)
>  Bidirectional recurrent neural networks (bi-RNNs), in particular, bidirectional long short term memory (bi-LSTM), bidirectional gated recurrent unit, and convolutional bi-LSTM models have recently attracted attention for nonlinearity mitigation in fiber-optic communication. The recently adopted approaches based on these models, however, incur a high computational complexity which may impede their real-time functioning. In this paper, by addressing the sources of complexity in these methods, we propose a more efficient network architecture, where a convolutional neural network encoder and a unidirectional many-to-one vanilla RNN operate in tandem, each best capturing one set of channel impairments while compensating for the shortcomings of the other. We deploy this model in two different receiver configurations. In one, the neural network is placed after a linear equalization chain and is merely responsible for nonlinearity mitigation; in the other, the neural network is directly placed after the chromatic dispersion compensation and is responsible for joint nonlinearity and polarization mode dispersion compensation. For a 16-QAM 64 GBd dual-polarization optical transmission over 14x80 km standard single-mode fiber, we demonstrate that the proposed hybrid model achieves the bit error probability of the state-of-the-art bi-RNN-based methods with greater than 50% lower complexity, in both receiver configurations.      
### 9.Moving-Horizon State Estimation for Power Networks and Synchronous Generators  [ :arrow_down: ](https://arxiv.org/pdf/2207.12150.pdf)
>  Power network and generators state estimation are usually tackled as separate problems. We propose a dynamic scheme for the simultaneous estimation of the network and the generator states. The estimation is formulated as an optimization problem on a moving-horizon of past observations. The framework is a generalization of static state estimation; it can handle incomplete model knowledge and does not require static network observability by PMUs. The numerical results show an improved estimation accuracy compared to static state estimation. Moreover, accurate estimation of the internal states of generators without PMUs on their terminals can be achieved. Finally, we highlight the capability of the proposed estimator to detect and identify bad data.      
### 10.Label Uncertainty Modeling and Prediction for Speech Emotion Recognition using t-Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2207.12135.pdf)
>  As different people perceive others' emotional expressions differently, their annotation in terms of arousal and valence are per se subjective. To address this, these emotion annotations are typically collected by multiple annotators and averaged across annotators in order to obtain labels for arousal and valence. However, besides the average, also the uncertainty of a label is of interest, and should also be modeled and predicted for automatic emotion recognition. In the literature, for simplicity, label uncertainty modeling is commonly approached with a Gaussian assumption on the collected annotations. However, as the number of annotators is typically rather small due to resource constraints, we argue that the Gaussian approach is a rather crude assumption. In contrast, in this work we propose to model the label distribution using a Student's t-distribution which allows us to account for the number of annotations available. With this model, we derive the corresponding Kullback-Leibler divergence based loss function and use it to train an estimator for the distribution of emotion labels, from which the mean and uncertainty can be inferred. Through qualitative and quantitative analysis, we show the benefits of the t-distribution over a Gaussian distribution. We validate our proposed method on the AVEC'16 dataset. Results reveal that our t-distribution based approach improves over the Gaussian approach with state-of-the-art uncertainty modeling results in speech-based emotion recognition, along with an optimal and even faster convergence.      
### 11.Koopman Form of Nonlinear Systems with Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2207.12132.pdf)
>  The Koopman framework proposes a linear representation of finite dimensional nonlinear systems through a generally infinite dimensional globally linear representation. Originally, the Koopman formalism has been described for autonomous systems and the extension for actuated continuous-time systems with a linear input or a control affine form has only recently been addressed. However, such a derivation for discrete-time systems has not yet been developed. Thus, a particular Koopman form is generally assumed, predominantly a linear time invariant (LTI) model, as it facilitates the use of control techniques such as linear quadratic regulation and model predictive control. However, we show that this assumption is insufficient to capture the dynamics of the underlying nonlinear system. In the present paper, we systematically investigate and analytically derive lifted forms under inputs for a general class of nonlinear systems in both continuous and discrete time, using the fundamental theorem of calculus. We prove that the resulting lifted representations give linear state-space Koopman models where the input matrix becomes state (and input, for the discrete-time case)-dependent, hence it can be seen as a specially structured linear parameter-varying (LPV) description of the underlying system. We also provide error bounds on how much the parameter variation contributes and how well the system behaviour can be approximated by an LTI Koopman representation. The introduced theoretical insight greatly helps for performing proper model structure selection in system identification with Koopman models as well as making a proper choice for LTI or LPV techniques for the control of nonlinear systems through the Koopman approach.      
### 12.A Sample-Based Algorithm for Approximately Testing $r$-Robustness of a Digraph  [ :arrow_down: ](https://arxiv.org/pdf/2207.12110.pdf)
>  One of the intensely studied concepts of network robustness is $r$-robustness, which is a network topology property quantified by an integer $r$. It is required by mean subsequence reduced (MSR) algorithms and their variants to achieve resilient consensus. However, determining $r$-robustness is intractable for large networks. In this paper, we propose a sample-based algorithm to approximately test $r$-robustness of a digraph with $n$ vertices and $m$ edges. For a digraph with a moderate assumption on the minimum in-degree, and an error parameter $0&lt;\epsilon\leq 1$, the proposed algorithm distinguishes $(r+\epsilon n)$-robust graphs from graphs which are not $r$-robust with probability $(1-\delta)$. Our algorithm runs in $\exp(O((\ln{\frac{1}{\epsilon\delta}})/\epsilon^2))\cdot m$ time. The running time is linear in the number of edges if $\epsilon$ is a constant.      
### 13.A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese  [ :arrow_down: ](https://arxiv.org/pdf/2207.12089.pdf)
>  Grapheme-to-phoneme (G2P) conversion is an indispensable part of the Chinese Mandarin text-to-speech (TTS) system, and the core of G2P conversion is to solve the problem of polyphone disambiguation, which is to pick up the correct pronunciation for several candidates for a Chinese polyphonic character. In this paper, we propose a Chinese polyphone BERT model to predict the pronunciations of Chinese polyphonic characters. Firstly, we create 741 new Chinese monophonic characters from 354 source Chinese polyphonic characters by pronunciation. Then we get a Chinese polyphone BERT by extending a pre-trained Chinese BERT with 741 new Chinese monophonic characters and adding a corresponding embedding layer for new tokens, which is initialized by the embeddings of source Chinese polyphonic characters. In this way, we can turn the polyphone disambiguation task into a pre-training task of the Chinese polyphone BERT. Experimental results demonstrate the effectiveness of the proposed model, and the polyphone BERT model obtain 2% (from 92.1% to 94.1%) improvement of average accuracy compared with the BERT-based classifier model, which is the prior state-of-the-art in polyphone disambiguation.      
### 14.A Hybrid Model and Learning-Based Adaptive Navigation Filter  [ :arrow_down: ](https://arxiv.org/pdf/2207.12082.pdf)
>  The fusion between an inertial navigation system and global navigation satellite systems is regularly used in many platforms such as drones, land vehicles, and marine vessels. The fusion is commonly carried out in a model-based extended Kalman filter framework. One of the critical parameters of the filter is the process noise covariance. It is responsible for the real-time solution accuracy, as it considers both vehicle dynamics uncertainty and the inertial sensors quality. In most situations, the process noise is covariance assumed to be constant. Yet, due to vehicle dynamics and sensor measurement variations throughout the trajectory, the process noise covariance is subject to change. To cope with such situations, several adaptive model-based Kalman filters were suggested in the literature. In this paper, we propose a hybrid model and learning-based adaptive navigation filter. We rely on the model-based Kalman filter and design a deep neural network model to tune the momentary system noise covariance matrix, based only on the inertial sensor readings. Once the process noise covariance is learned, it is plugged into the well-established, model-based Kalman filter. After deriving the proposed hybrid framework, field experiment results using a quadrotor are presented and a comparison to model-based adaptive approaches is given. We show that the proposed method obtained an improvement of 25% in the position error. Furthermore, the proposed hybrid learning method can be used in any navigation filter and also in any relevant estimation problem.      
### 15.The Quantum Advantage in Decentralized Control  [ :arrow_down: ](https://arxiv.org/pdf/2207.12075.pdf)
>  It is well known that classical randomization, either through behavioural or mixed strategies does not improve the optimal cost of a team decision problem. We introduce a new class of stochastic strategies for static team decision problems that use quantum entanglement to generate randomness. We demonstrate a numerical example of a static team decision problem and a physically realizable quantum strategy that results in a cost that is strictly lower than the optimal cost achievable through classical strategies, thereby demonstrating the existence of a quantum advantage in decentralized control. We show that the optimal `quantum' cost is given by the optimization of a linear objective over a convex set, but where the convex set is abstractly specified. We provide a computable linear programming relaxation that evaluates lower bounds on the optimal quantum cost. Our investigation introduces a novel decision and control paradigm with an enlarged space of control policies achievable by quantum physical architectures.      
### 16.REPNP: Plug-and-Play with Deep Reinforcement Learning Prior for Robust Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2207.12056.pdf)
>  Image restoration schemes based on the pre-trained deep models have received great attention due to their unique flexibility for solving various inverse problems. In particular, the Plug-and-Play (PnP) framework is a popular and powerful tool that can integrate an off-the-shelf deep denoiser for different image restoration tasks with known observation models. However, obtaining the observation model that exactly matches the actual one can be challenging in practice. Thus, the PnP schemes with conventional deep denoisers may fail to generate satisfying results in some real-world image restoration tasks. We argue that the robustness of the PnP framework is largely limited by using the off-the-shelf deep denoisers that are trained by deterministic optimization. To this end, we propose a novel deep reinforcement learning (DRL) based PnP framework, dubbed RePNP, by leveraging a light-weight DRL-based denoiser for robust image restoration tasks. Experimental results demonstrate that the proposed RePNP is robust to the observation model used in the PnP scheme deviating from the actual one. Thus, RePNP can generate more reliable restoration results for image deblurring and super resolution tasks. Compared with several state-of-the-art deep image restoration baselines, RePNP achieves better results subjective to model deviation with fewer model parameters.      
### 17.Unleashing the potential of price-based congestion management schemes: a unifying approach to compare alternative models under multiple objectives  [ :arrow_down: ](https://arxiv.org/pdf/2207.12041.pdf)
>  A wide range of price-based congestion management schemes were proposed in the literature ranging from marginal cost road pricing to trip based multimodal pricing. The underlying models were formulated under different theoretical assumptions and with varying, and sometimes conflicting objectives. This paper presents a unifying framework under which different approaches can be compared based on their respective assumptions. The unifying modelling framework is referred to as trip pricing model, which extends path-differentiated pricing to multimodal paths, i.e., trips on whatever mode or combinations of modes, and generalizes road pricing schemes (which are shown to be a special case). By setting both positive (tolls) and negative (incentives) prices, revenue-neutral schemes are also shown to be special cases, with no a-priori assumption on which paths/modes to toll/incentivize. For model comparison, the pricing design problem is formulated in a multi-objective optimization framework, which combines traffic efficiency, environmental sustainability, users' acceptance, social and spatial equity, as pricing objectives. The trip pricing scheme is compared with traditional road pricing schemes, and with their revenue-neutral variants. First-best and second-best pricing schemes, designed in single- and multi-objective optimization frameworks, are compared on the Nguyen-Dupuis network. Results suggest a vast potential for multi-objective trip congestion pricing over single-objective road pricing. In addition, the application of both positive and negative prices is shown to significantly increase the expected acceptance of pricing schemes and to preserve efficiency and sustainability objectives. The results should promote the design of more effective pricing policies, i.e., set of pricing rules easier to communicate, based on now available technologies of passengers' tracking and pricing.      
### 18.Unsupervised data selection for Speech Recognition with contrastive loss ratios  [ :arrow_down: ](https://arxiv.org/pdf/2207.12028.pdf)
>  This paper proposes an unsupervised data selection method by using a submodular function based on contrastive loss ratios of target and training data sets. A model using a contrastive loss function is trained on both sets. Then the ratio of frame-level losses for each model is used by a submodular function. By using the submodular function, a training set for automatic speech recognition matching the target data set is selected. Experiments show that models trained on the data sets selected by the proposed method outperform the selection method based on log-likelihoods produced by GMM-HMM models, in terms of word error rate (WER). When selecting a fixed amount, e.g. 10 hours of data, the difference between the results of two methods on Tedtalks was 20.23% WER relative. The method can also be used to select data with the aim of minimising negative transfer, while maintaining or improving on performance of models trained on the whole training set. Results show that the WER on the WSJCAM0 data set was reduced by 6.26% relative when selecting 85% from the whole data set.      
### 19.Non-cascaded Control Barrier Functions for the Safe Control of Quadrotors  [ :arrow_down: ](https://arxiv.org/pdf/2207.12027.pdf)
>  Researchers have developed various cascaded controllers and non-cascaded controllers for the navigation and control of quadrotors in recent years. It is vital to ensure the safety of a quadrotor both in normal state and in abnormal state if a controller tends to make the quadrotor unsafe. To this end, this paper proposes a non-cascaded Control Barrier Function (CBF) for a quadrotor controlled by either cascaded controllers or a non-cascaded controller. Incorporated with a Quadratic Programming (QP), the non-cascaded CBF can simultaneously regulate the magnitude of the total thrust and the torque of the quadrotor determined a controller, so as to ensure the safety of the quadrotor both in normal state and in abnormal state. The non-cascaded CBF establishes a non-conservative forward invariant safe region, in which the controller of a quadrotor is fully or partially effective in the navigation or the pose control of the quadrotor. The non-cascaded CBF is applied to a quadrotor performing trajectory tracking and a quadrotor performing aggressive roll maneuvers in simulations to evaluate the effectiveness of the non-cascaded CBF.      
### 20.Sharing Energy Storage Systems under Net Metering and Time-of-Use Pricing  [ :arrow_down: ](https://arxiv.org/pdf/2207.12022.pdf)
>  Sharing economy has become a socio-economic trend in transportation and housing sectors. It develops business models leveraging underutilized resources. Like those sectors, power grid is also becoming smarter with many flexible resources, and researchers are investigating the impact of sharing resources here as well that can help to reduce cost and extract value. In this work, we investigate sharing of energy storage devices among individual households in a cooperative fashion. Coalitional game theory is used to model the scenario where utility company imposes time-of-use (ToU) price and net metering billing mechanism. The resulting game has a non-empty core and we can develop a cost allocation mechanism in the core with easy to compute analytical formula. A mechanism for sharing the excess energy under the peer to peer network (P2P) is also developed. Thus sharing electricity generated by storage devices among consumers can be effective in this set-up. Our simulation results also validate the theoretical claim.      
### 21.Distributed Coordination of Charging Stations with Shared Energy Storage in a Distribution Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.11983.pdf)
>  Electric vehicle (EV) charging stations have experienced rapid growth, whose impacts on the power grid have become non-negligible. Though charging stations can install battery energy storage to reduce their impacts on the grid, the conventional "one charging station, one battery storage" method may be uneconomical due to the high upfront cost of battery storage. Shared energy storage can be a potential solution. However, effective management of charging stations with shared energy storage in a distribution network is challenging due to the complex coupling, competing interests, and information asymmetry between different agents. To address the aforementioned challenges, this paper proposes a distributed coordination mechanism, with a prediction and a correction step, to guide the behaviors of different agents. In particular, three groups of strategic agents are involved. Each charging station determines the flexible EV charging plan inside it; each shared energy storage operator decides on the amount of energy provided to its connected charging stations; and the distribution system operator monitors power flow in the network. We theoretically prove that the proposed mechanism will converge to the centrally optimal solution. Numerical experiments and comprehensive performance comparisons are conducted to validate the theoretical results and show the advantages of the proposed mechanism.      
### 22.Optimal Utilization of Third-Party Demand Response Resources in Vertically Integrated Utilities: A Game Theoretic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.11975.pdf)
>  This paper studies the optimal mechanisms for the vertically integrated utility to dispatch and incentivize the third-party demand response (DR) providers in its territory. A framework is proposed, with three-layer coupled Stackelberg and simultaneous games, to study the interactions and competitions among the profit-seeking process of the utility, the third-party DR providers, and the individual end users (EUs) in the DR programs. Two coupled single-leader-multiple-followers Stackelberg games with a three-layer structure are proposed to capture the interactions among the utility (modeled in the upper layer), the third-party DR providers (modeled in the middle layer), and the EUs in each DR program (modeled in the lower layer). The competitions among the EUs in each DR program is captured through a non-cooperative simultaneous game. An inconvenience cost function is proposed to model the DR provision willingness and capacity of different EUs. The Stackelberg game between the middle-layer DR provider and the lower-layer EUs is solved by converting the original bi-level programming to a singlelevel programming. This converted single-level programming is embedded in an iterative algorithm toward solving the entire coupled games framework. Case studies are performed on IEEE 34-bus and IEEE 69-bus test systems to illustrate the application of the proposed framework.      
### 23.ConceptBeam: Concept Driven Target Speech Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2207.11964.pdf)
>  We propose a novel framework for target speech extraction based on semantic information, called ConceptBeam. Target speech extraction means extracting the speech of a target speaker in a mixture. Typical approaches have been exploiting properties of audio signals, such as harmonic structure and direction of arrival. In contrast, ConceptBeam tackles the problem with semantic clues. Specifically, we extract the speech of speakers speaking about a concept, i.e., a topic of interest, using a concept specifier such as an image or speech. Solving this novel problem would open the door to innovative applications such as listening systems that focus on a particular topic discussed in a conversation. Unlike keywords, concepts are abstract notions, making it challenging to directly represent a target concept. In our scheme, a concept is encoded as a semantic embedding by mapping the concept specifier to a shared embedding space. This modality-independent space can be built by means of deep metric learning using paired data consisting of images and their spoken captions. We use it to bridge modality-dependent information, i.e., the speech segments in the mixture, and the specified, modality-independent concept. As a proof of our scheme, we performed experiments using a set of images associated with spoken captions. That is, we generated speech mixtures from these spoken captions and used the images or speech signals as the concept specifiers. We then extracted the target speech using the acoustic characteristics of the identified segments. We compare ConceptBeam with two methods: one based on keywords obtained from recognition systems and another based on sound source separation. We show that ConceptBeam clearly outperforms the baseline methods and effectively extracts speech based on the semantic representation.      
### 24.Channel Estimation for LEO Satellite Massive MIMO OFDM Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.11958.pdf)
>  In this paper, we investigate the massive multiple-input multiple-output orthogonal frequency division multiplexing channel estimation for low-earth-orbit satellite communication systems. First, we use the angle-delay domain channel to characterize the space-frequency domain channel. Then, we show that the asymptotic minimum mean square error (MMSE) of the channel estimation can be minimized if the array response vectors of the user terminals (UTs) that use the same pilot are orthogonal. Inspired by this, we design an efficient graph-based pilot allocation strategy to enhance the channel estimation performance. To reduce the computational complexity, we devise a novel two-stage channel estimation (TSCE) approach, in which the received signals at the satellite are manipulated with per-subcarrier space domain processing followed by per-user frequency domain processing. Moreover, the space domain processing of each UT is shown to be identical for all the subcarriers, and an asymptotically optimal vector for the per-subcarrier space domain linear processing is derived. The frequency domain processing can be efficiently implemented by means of the fast Toeplitz system solver. Simulation results show that the proposed TSCE approach can achieve a near performance to the MMSE estimation with much lower complexity.      
### 25.Terahertz-Band Near-Space Communications: From a Physical-Layer Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2207.11945.pdf)
>  Facilitated by rapid technological development of the near-space platform stations (NSPS), near-space communication (NS-COM) is envisioned to play a pivotal role in the space-air-ground integrated network for sixth-generation (6G) communications and beyond. In NS-COM, ultra-broadband wireless connectivity between NSPSs and various airborne/spaceborne platforms is required for a plethora of bandwidth-consuming applications, such as NSPS-based Ad hoc networking, in-flight Internet and relaying technology. However, such requirement seems to contradict with the scarcity of spectrum resources at conventional microwave frequencies, which motivates the exploitation of terahertz (THz) band ranging from 0.1 to 10 THz. Due to huge available bandwidth, the THz signals are capable of supporting ultra-high-rate data transmission for NS-COM over 100 Gb/s, which are naturally suitable for the near-space environment with marginal path loss. To this end, this article provides an extensive investigation on the THz-band NS-COM (THz-NS-COM) from a physical-layer perspective. Firstly, we summarize the potential applications of THz communications in the near-space environment, where the corresponding technical barriers are analyzed. Then the channel characteristics of THz-NS-COM and the corresponding modeling strategies are discussed, respectively. Afterwards, three essential research directions are investigated to surpass the technical barriers of THz-NS-COM, i.e., robust beamforming for ultra-massive antenna array, signal processing algorithms against hybrid distortions, and integrated sensing and communications. Several open problems are also provided to unleash the full potential of THz-NS-COM.      
### 26.Learning a Dual-Mode Speech Recognition Model via Self-Pruning  [ :arrow_down: ](https://arxiv.org/pdf/2207.11906.pdf)
>  There is growing interest in unifying the streaming and full-context automatic speech recognition (ASR) networks into a single end-to-end ASR model to simplify the model training and deployment for both use cases. While in real-world ASR applications, the streaming ASR models typically operate under more storage and computational constraints - e.g., on embedded devices - than any server-side full-context models. Motivated by the recent progress in Omni-sparsity supernet training, where multiple subnetworks are jointly optimized in one single model, this work aims to jointly learn a compact sparse on-device streaming ASR model, and a large dense server non-streaming model, in a single supernet. Next, we present that, performing supernet training on both wav2vec 2.0 self-supervised learning and supervised ASR fine-tuning can not only substantially improve the large non-streaming model as shown in prior works, and also be able to improve the compact sparse streaming model.      
### 27.LEO Satellite Access Network (LEO-SAN) Towards 6G: Challenges and Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2207.11896.pdf)
>  With the rapid development of satellite communication technologies, the space-based access network has been envisioned as a promising complementary part of the future 6G network. Aside from terrestrial base stations, satellite nodes, especially the low-earth-orbit (LEO) satellites, can also serve as base stations for Internet access, and constitute the LEO-satellite-based access network (LEO-SAN). LEO-SAN is expected to provide seamless massive access and extended coverage with high signal quality. However, its practical implementation still faces significant technical challenges, e.g., high mobility and limited budget for communication payloads of LEO satellite nodes. This paper aims at revealing the main technical issues that have not been fully addressed by the existing LEO-SAN designs, from three aspects namely random access, beam management and Doppler-resistant transmission technologies. More specifically, the critical issues of random access in LEO-SAN are discussed regarding low flexibility, long transmission delay, and inefficient handshakes. Then the beam management for LEO-SAN is investigated in complex propagation environments under the constraints of high mobility and limited payload budget. Furthermore, the influence of Doppler shifts on LEO-SAN is explored. Correspondingly, promising technologies to address these challenges are also discussed, respectively. Finally, the future research directions are envisioned.      
### 28.Deep learning based non-contact physiological monitoring in Neonatal Intensive Care Unit  [ :arrow_down: ](https://arxiv.org/pdf/2207.11886.pdf)
>  Preterm babies in the Neonatal Intensive Care Unit (NICU) have to undergo continuous monitoring of their cardiac health. Conventional monitoring approaches are contact-based, making the neonates prone to various nosocomial infections. Video-based monitoring approaches have opened up potential avenues for contactless measurement. This work presents a pipeline for remote estimation of cardiopulmonary signals from videos in NICU setup. We have proposed an end-to-end deep learning (DL) model that integrates a non-learning based approach to generate surrogate ground truth (SGT) labels for supervision, thus refraining from direct dependency on true ground truth labels. We have performed an extended qualitative and quantitative analysis to examine the efficacy of our proposed DL-based pipeline and achieved an overall average mean absolute error of 4.6 beats per minute (bpm) and root mean square error of 6.2 bpm in the estimated heart rate.      
### 29.Near Space Communications (NS-COM): A New Regime in Space-Air-Ground Integrated Network (SAGIN)  [ :arrow_down: ](https://arxiv.org/pdf/2207.11883.pdf)
>  Precipitated by the technological innovations of the near-space platform stations (NSPS), the near space communication (NS-COM) network has emerged as an indispensable part of the next-generation space-air-ground integrated network (SAGIN) that facilitates ubiquitous coverage and broadband data transfer. This paper aims to provide a comprehensive overview of NS-COM. Firstly, we investigate the differences between NS-COM and the existing terrestrial cellular networks as well as satellite-based and unmanned-aerial-vehicle (UAV)-based communication networks, which is followed by a review of the NS-COM development. Then, we explore the unique characteristics of NS-COM regarding the platforms and the propagation environment of the near space. The main issues of NS-COM are identified, resulted from the extremely long transmission distance, limitations of the communication payloads on NSPS and complex atmospheric constitution of the near space. Then various application scenarios of NS-COM are discussed, where the special technical requirements are also revealed, from the physical-layer aspect like transceiver design to the upper-layer aspect like computational offloading and NSPS placement. Furthermore, we investigate the co-existence of NS-COM and ground networks by treating each other as interferers or collaborators. Finally, we list several potential technologies for NS-COM from the perspective of spectrum usage, and highlight their technical challenges for future research.      
### 30.Sparse-based Domain Adaptation Network for OCTA Image Super-Resolution Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2207.11882.pdf)
>  Retinal Optical Coherence Tomography Angiography (OCTA) with high-resolution is important for the quantification and analysis of retinal vasculature. However, the resolution of OCTA images is inversely proportional to the field of view at the same sampling frequency, which is not conducive to clinicians for analyzing larger vascular areas. In this paper, we propose a novel Sparse-based domain Adaptation Super-Resolution network (SASR) for the reconstruction of realistic 6x6 mm2/low-resolution (LR) OCTA images to high-resolution (HR) representations. To be more specific, we first perform a simple degradation of the 3x3 mm2/high-resolution (HR) image to obtain the synthetic LR image. An efficient registration method is then employed to register the synthetic LR with its corresponding 3x3 mm2 image region within the 6x6 mm2 image to obtain the cropped realistic LR image. We then propose a multi-level super-resolution model for the fully-supervised reconstruction of the synthetic data, guiding the reconstruction of the realistic LR images through a generative-adversarial strategy that allows the synthetic and realistic LR images to be unified in the feature domain. Finally, a novel sparse edge-aware loss is designed to dynamically optimize the vessel edge structure. Extensive experiments on two OCTA sets have shown that our method performs better than state-of-the-art super-resolution reconstruction methods. In addition, we have investigated the performance of the reconstruction results on retina structure segmentations, which further validate the effectiveness of our approach.      
### 31.Online Expectation-Maximization Based Frequency and Phase Consensus in Distributed Phased Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2207.11859.pdf)
>  Distributed phased array (DPA) is made up of separate antenna nodes that coordinate with each other to perform a coherent operation. Frequency and phase synchronization is a major challenge in DPA for which a Kalman filtering based decentralized frequency and phase consensus (KF-DFPC) algorithm was proposed in prior work. In the KF-DFPC algorithm, the nodes share their frequencies and phases with their neighbors to reach synchronization. One limitation of the DFPC-based algorithms is that, due to relying on the average consensus protocol which requires undirected networks, they do not converge for directed networks. Since directed networks can be more easily implemented in practice, we propose in this paper a push-sum based frequency and phase consensus ($\text{P}_\text{s}$FPC) algorithm which converges for such networks. The residual phase error of $\text{P}_\text{s}$FPC upon convergence is theoretically derived in this work. Kalman filtering is also integrated with $\text{P}_\text{s}$FPC and the resulting KF-$\text{P}_\text{s}$FPC algorithm further reduces the residual phase error upon convergence. Another limitation of KF-DFPC is that it assumes that the measurement noise and innovation noise covariance matrices are known for KF. Since they may not be known in practice, we develop an online expectation maximization (EM) based algorithm that iteratively computes the maximum likelihood (ML) estimate of the unknown matrices. EM is integrated with KF-$\text{P}_\text{s}$FPC and the resulting algorithm is referred to as the EM-KF-$\text{P}_\text{s}$FPC algorithm. Simulation results are included where the performance of the proposed $\text{P}_\text{s}$FPC-based algorithms is analyzed for different distributed phased arrays and is compared to the DFPC-based algorithms and the earlier proposed hybrid consensus on measurement and consensus on information (HCMCI) algorithm.      
### 32.A Novel ECG Denoising Scheme Using the Ensemble Kalman Filter  [ :arrow_down: ](https://arxiv.org/pdf/2207.11819.pdf)
>  Monitoring of electrocardiogram (ECG) provides vital information as well as any cardiovascular anomalies. Recent advances in the technology of wearable electronics have enabled compact devices to acquire personal physiological signals in the home setting; however, signals are usually contaminated with high level noise. Thus, an efficient ECG filtering scheme is a dire need. In this paper, a novel method using Ensemble Kalman Filter (EnKF) is developed for denoising ECG signals. We also intensively explore various filtering algorithms, including Savitzky-Golay (SG) filter, Ensemble Empirical mode decomposition (EEMD), Normalized Least-Mean-Square (NLMS), Recursive least squares (RLS) filter, Total variation denoising (TVD), Wavelet and extended Kalman filter (EKF) for comparison. Data from the MIT-BIH Noise Stress Test database were used. The proposed methodology shows the average signal to noise ratio (SNR) of 10.96, the Percentage Root Difference of 150.45, and the correlation coefficient of 0.959 from the modified MIT-BIH database with added motion artifacts.      
### 33.Improved Super Resolution of MR Images Using CNNs and Vision Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2207.11748.pdf)
>  State of the art magnetic resonance (MR) image super-resolution methods (ISR) using convolutional neural networks (CNNs) leverage limited contextual information due to the limited spatial coverage of CNNs. Vision transformers (ViT) learn better global context that is helpful in generating superior quality HR images. We combine local information of CNNs and global information from ViTs for image super resolution and output super resolved images that have superior quality than those produced by state of the art methods. We include extra constraints through multiple novel loss functions that preserve structure and texture information from the low resolution to high resolution images.      
### 34.Consensus-based Frequency and Voltage Regulation for Fully Inverter-based Islanded Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2207.11746.pdf)
>  This paper proposes a new distributed consensus-based control method for voltage and frequency control of fully inverter-based islanded microgrids (MGs). The proposed method includes the active power sharing in voltage control to improve the reactive power sharing accuracy and thus generalizes some existing secondary frequency and voltage control methods. Firstly, frequency is regulated by distributed secondary frequency control. Secondly, voltage is regulated by distributed average voltage control and decentralized individual voltage control. It offers a tunable trade-off between voltage regulation, active, and reactive power sharing accuracy. Therefore, it avoids the abuse of sacrificing reactive power sharing accuracy for exact voltage regulation which is a common issue of existing methods. The proposed method is implemented in a distributed way that does not require a prior knowledge of the MG network structure and loads and hence can ensure scalability. Simulation results shows that the proposed controller achieves different compromise between the above three targets under different modes of operation.      
### 35.A Custom IC Layout Generation Engine Based on Dynamic Templates and Grids  [ :arrow_down: ](https://arxiv.org/pdf/2207.11728.pdf)
>  This paper presents an automatic layout generation framework in advanced CMOS technologies. The framework extends the template-and-grid-based layout generation methodology with the following additional techniques applied to produce optimal layouts more effectively. First, layout templates and grids are dynamically created and adjusted during runtime to serve various structural, functional, and design requirements. Virtual instances support the dynamic template-and-grid-based layout generation process. The framework also implements various post-processing functions to handle process-specific requirements efficiently. The post-processing functions include cut/dummy pattern generation and multiple-patterning adjustment. The generator description capability is enhanced with circular grid indexing/slicing and conditional conversion operators. The layout generation framework is applied to various design examples and generates DRC/LVS clean layouts automatically in multiple CMOS technologies.      
### 36.A Novel Decentralized Inverter Control Algorithm for Loss Minimization and LVRT Improvement  [ :arrow_down: ](https://arxiv.org/pdf/2207.11700.pdf)
>  Algorithms that adjust the reactive power injection of converter-connected RES to minimize losses may compromise the converters' fault-ride-through capability. This can become crucial for the reliable operation of the distribution grids, as they could lose valuable resources to support grid voltage at the time they need them the most. This paper explores how two novel loss-minimizing algorithms can both achieve high reduction of the system losses during normal operation and remain connected to support the voltage during faults. The algorithms we propose are decentralized and model-free: they require no communication and no knowledge of the grid topology or the grid location of the converters. Using local information, they control the reactive power injection to minimize the system losses. In this paper, we extend these algorithms to ensure the low voltage ride through (LVRT) capability of the converters, and we integrate them with state-of-the-art Wavelet-CNN-LSTM RES forecasting methods that enhance their performance. We perform extensive simulations on the real-time digital simulation (RTDS) platform, where we demonstrate that the algorithms we propose can achieve a substantial decrease in power losses while remaining compliant with the grid codes for LVRT makes them suitable for the implementation across the distribution system.      
### 37.PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2207.11683.pdf)
>  Deep learning based semi-supervised learning (SSL) methods have achieved strong performance in medical image segmentation, which can alleviate doctors' expensive annotation by utilizing a large amount of unlabeled data. Unlike most existing semi-supervised learning methods, adversarial training based methods distinguish samples from different sources by learning the data distribution of the segmentation map, leading the segmenter to generate more accurate predictions. We argue that the current performance restrictions for such approaches are the problems of feature extraction and learning preference. In this paper, we propose a new semi-supervised adversarial method called Patch Confidence Adversarial Training (PCA) for medical image segmentation. Rather than single scalar classification results or pixel-level confidence maps, our proposed discriminator creates patch confidence maps and classifies them at the scale of the patches. The prediction of unlabeled data learns the pixel structure and context information in each patch to get enough gradient feedback, which aids the discriminator in convergent to an optimal state and improves semi-supervised segmentation performance. Furthermore, at the discriminator's input, we supplement semantic information constraints on images, making it simpler for unlabeled data to fit the expected data distribution. Extensive experiments on the Automated Cardiac Diagnosis Challenge (ACDC) 2017 dataset and the Brain Tumor Segmentation (BraTS) 2019 challenge dataset show that our method outperforms the state-of-the-art semi-supervised methods, which demonstrates its effectiveness for medical image segmentation.      
### 38.FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2207.11678.pdf)
>  The presence of high-density objects such as metal implants and dental fillings can introduce severely streak-like artifacts in computed tomography (CT) images, greatly limiting subsequent diagnosis. Although various deep neural networks-based methods have been proposed for metal artifact reduction (MAR), they usually suffer from poor performance due to limited exploitation of global context in the sinogram domain, secondary artifacts introduced in the image domain, and the requirement of precise metal masks. To address these issues, this paper explores fast Fourier convolution for MAR in both sinogram and image domains, and proposes a Fourier dual-domain network for MAR, termed FD-MAR. Specifically, we first propose a Fourier sinogram restoration network, which can leverage sinogram-wide receptive context to fill in the metal-corrupted region from uncorrupted region and, hence, is robust to the metal trace. Second, we propose a Fourier refinement network in the image domain, which can refine the reconstructed images in a local-to-global manner by exploring image-wide context information. As a result, the proposed FD-MAR can explore the sinogram- and image-wide receptive fields for MAR. By optimizing FD-MAR with a composite loss function, extensive experimental results demonstrate the superiority of the proposed FD-MAR over the state-of-the-art MAR methods in terms of quantitative metrics and visual comparison. Notably, FD-MAR does not require precise metal masks, which is of great importance in clinical routine.      
### 39.Harmonic-Balance Based Power Flow and ZVS Analysis of a Quad-Active Bridge DC-DC Converter  [ :arrow_down: ](https://arxiv.org/pdf/2207.11676.pdf)
>  The power flow control of multi-active bridge converters requires a comprehensive steady-state analysis of the converter and the determination of conditions for zero voltage switching of all switching in the converter which result in minimum switching loss. This paper aims to model and carry out the power flow and Zero Voltage Switching (ZVS) analyses of Quad-active-bridge (QAB) dc-dc converter. The dynamic as well as the steady state analyses of the converter were carried out, thereby determining the phase shifts required to meet commanded load powers. The full equivalent circuit model of the converter which include winding resistances and magnetizing inductances is used rather than the popular lossless star-equivalent circuit model that may introduce significant error in the converter's analysis. The conditions which ensure the converter working in ZVS mode are determined and experimentally verified.      
### 40.An Insight into the Dynamics of a Dual Active Bridge  [ :arrow_down: ](https://arxiv.org/pdf/2207.11672.pdf)
>  This paper aims at analyzing the effect of the zero dynamics of the dual active bridge isolated dc-dc converter on the dynamics of the complete system. It also explains its influence on controller design for the system. To achieve this, the actual model of the system, as well as the first harmonic approximation (FHA) of the model are derived. The ZVS and the stability analysis of the system is done based on the FHA model of the system. The system is shown to be stable for constant output voltage operation for the entire power range while unstable for the constant power load (CPL) operation for load demands close to the system maximum power. It is also shown that the transformer winding current are part of the zero dynamic states and was shown to be always stable regardless of the operating conditions of the system.      
### 41.Collaborative Inference for AI-Empowered IoT Devices  [ :arrow_down: ](https://arxiv.org/pdf/2207.11664.pdf)
>  Artificial intelligence (AI) technologies, and particularly deep learning systems, are traditionally the domain of large-scale cloud servers, which have access to high computational and energy resources. Nonetheless, in Internet-of-Things (IoT) networks, the interface with the real-world is carried out using edge devices that are limited in hardware and can communicate. The conventional approach to provide AI processing to data collected by edge devices involves sending samples to the cloud, at the cost of latency, communication, connectivity, and privacy concerns. Consequently, recent years have witnessed a growing interest in enabling AI-aided inference on edge devices by leveraging their communication capabilities to establish collaborative inference. This article reviews candidate strategies for facilitating the transition of AI to IoT devices via collaboration. We identify the need to operate in different mobility and connectivity constraints as a motivating factor to consider multiple schemes, which can be roughly divided into methods where inference is done remotely, i.e., on the cloud, and those that infer on the edge. We identify the key characteristics of each strategy in terms of inference accuracy, communication latency, privacy, and connectivity requirements, providing a systematic comparison between existing approaches. We conclude by presenting future research challenges and opportunities arising from the concept of collaborative inference.      
### 42.DCT Approximations Based on Chen's Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2207.11638.pdf)
>  In this paper, two 8-point multiplication-free DCT approximations based on the Chen's factorization are proposed and their fast algorithms are also derived. Both transformations are assessed in terms of computational cost, error energy, and coding gain. Experiments with a JPEG-like image compression scheme are performed and results are compared with competing methods. The proposed low-complexity transforms are scaled according to Jridi-Alfalou-Meher algorithm to effect 16- and 32-point approximations. The new sets of transformations are embedded into an HEVC reference software to provide a fully HEVC-compliant video coding scheme. We show that approximate transforms can outperform traditional transforms and state-of-the-art methods at a very low complexity cost.      
### 43.A Scalable Bayesian Persuasion Framework for Epidemic Containment on Heterogeneous Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11578.pdf)
>  During an epidemic, the information available to individuals in the society deeply influences their belief of the epidemic spread, and consequently the preventive measures they take to stay safe from the infection. In this paper, we develop a scalable framework for ascertaining the optimal information disclosure a government must make to individuals in a networked society for the purpose of epidemic containment. This problem of information design problem is complicated by the heterogeneous nature of the society, the positive externalities faced by individuals, and the variety in the public response to such disclosures. We use a networked public goods model to capture the underlying societal structure. Our first main result is a structural decomposition of the government's objectives into two independent components -- a component dependent on the utility function of individuals, and another dependent on properties of the underlying network. Since the network dependent term in this decomposition is unaffected by the signals sent by the government, this characterization simplifies the problem of finding the optimal information disclosure policies. We find explicit conditions, in terms of the risk aversion and prudence, under which no disclosure, full disclosure, exaggeration and downplay are the optimal policies. The structural decomposition results are also helpful in studying other forms of interventions like incentive design and network design.      
### 44.Resource-Efficient HAPS-RIS Enabled Beyond-Cell Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.11576.pdf)
>  In the future, urban regions will encounter a massive number of capacity-hungry devices. Relying solely on terrestrial networks for serving all UEs will be a cost-ineffective approach. Consequently, with the anticipated supply and demand mismatch, several UEs will be unsupported. To offer service to the left-out UEs, we employ an energy-efficient and cost-effective beyond-cell communications approach, which uses reconfigurable intelligent surfaces (RIS) on a high-altitude platform station (HAPS). Particularly, unsupported UEs will be connected to a dedicated control station (CS) through RIS-mounted HAPS. A novel resource-efficient optimization problem is formulated that maximizes the number of connected UEs, while minimizing the total power consumed by the CS and RIS. Since the resulting problem is a mixed-integer nonlinear program (MINLP), a low-complexity two-stage algorithm is developed. Numerical results demonstrate that the proposed algorithm outperforms the benchmark approach in terms of the percentage of connected UEs and the resource-efficiency (RE). Also, the results show that the number of connected UEs is more sensitive to transmit power at the CS than the HAPS size.      
### 45.RIS-Assisted Grant-Free NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2207.11531.pdf)
>  This paper introduces a reconfigurable intelligent surface (RIS)-assisted grant-free non-orthogonal multiple-access (GF-NOMA) scheme. To ensure the power reception disparity required by the power domain NOMA (PD-NOMA), we propose a joint user clustering and RIS assignment/alignment approach that maximizes the network sum rate by judiciously pairing user equipments (UEs) with distinct channel gains, assigning RISs to proper clusters, and aligning RIS phase shifts to the cluster members yielding the highest cluster sum rate. Once UEs are acknowledged with the cluster index, they are allowed to access their resource blocks (RBs) at any time requiring neither further grant acquisitions from the base station (BS) nor power control as all UEs are requested to transmit at the same power. In this way, the proposed approach performs an implicit over-the-air power control with minimal control signaling between BS and UEs, which has shown to deliver up to 20% higher network sum rate than benchmark GF-NOMA and optimal grant-based PD-NOMA schemes depending on the network parameters. The given numerical results also investigate the impact of UE density, RIS deployment, and RIS hardware specifications on the overall performance of the proposed RIS-aided GF-NOMA scheme.      
### 46.Low-complexity CNNs for Acoustic Scene Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.11529.pdf)
>  This paper presents a low-complexity framework for acoustic scene classification (ASC). Most of the frameworks designed for ASC use convolutional neural networks (CNNs) due to their learning ability and improved performance compared to hand-engineered features. However, CNNs are resource hungry due to their large size and high computational complexity. Therefore, CNNs are difficult to deploy on resource constrained devices. This paper addresses the problem of reducing the computational complexity and memory requirement in CNNs. We propose a low-complexity CNN architecture, and apply pruning and quantization to further reduce the parameters and memory. We then propose an ensemble framework that combines various low-complexity CNNs to improve the overall performance. An experimental evaluation of the proposed framework is performed on the publicly available DCASE 2022 Task 1 that focuses on ASC. The proposed ensemble framework has approximately 60K parameters, requires 19M multiply-accumulate operations and improves the performance by approximately 2-4 percentage points compared to the DCASE 2022 Task 1 baseline network.      
### 47.Multi-User Rate Splitting in Optical Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11458.pdf)
>  Optical wireless communication (OWC) has recently received massive interest as a new technology that can support the enormous data traffic increasing on daily basis. Laser-based OWC networks can provide terabits per second (Tbps) aggregate data rates. However, the emerging OWC networks require clusters of optical transmitters to provide uniform coverage for multiple users. In this context, multi-user interference (MUI) is a crucial issue that must be managed efficiently to provide high spectral efficiency. Rate splitting (RS) is proposed as a transmission scheme to serve multiple users simultaneously by splitting the message of a given user into common and private messages, and then, each user decodes the desired message following a certain procedure. In radio frequency (RF) networks, RS provides higher spectral efficiency compared with orthogonal and non-orthogonal transmission schemes. Considering the high density of OWC networks, the performance of RS is limited by the cost of providing channel state information (CSI) at transmitters and by the noise resulting from interference cancellation. In this work, a user-grouping algorithm is proposed and used to form multiple groups, each group contains users spatially clustered. Then, an outer precoder is designed to manage inter-group interference following the methodology of blind interference alignment (BIA), which reduces the requirements of CSI at RF or optical transmitters. For intra-group interference, RS is applied within each group where the users belonging to a given group receive a unique common message on which their private messages are superimposed. Furthermore, an optimization problem is formulated to allocate the power among the private messages intended to all users such that the sum rate of the network is maximized.      
### 48.Novel Improvement for Nonlinear Compatibility of Least Mean Square Adaptive Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2207.11453.pdf)
>  In order to improve the least mean squares (LMS) adaptation algorithm to accommodate the nonlinear transfer function, and to adjust the coefficients of adaptive filter during the actual implement of bias voltage and signal amplitude, methods are proposed and simulated to develop a nonlinear adaptive filter. The inputs to LMS are replaced by the derivatives of traditional inputs, and the step for each training iteration is adaptively controlled by the difference between target signal and actual signal. The simulation utilizes the implementation of Nyquist pulses optical sampling and works as a digital signal processing pre-compensation to reduce influence of the frequency responses on wires and devices. The simulation result shows promising improvement with the modified adaptation algorithm method in tackling Mach Zehnder modulator's non-monotonic transfer function.      
### 49.Convergence in a Repeated Non-atomic Routing Game with Partial Signaling  [ :arrow_down: ](https://arxiv.org/pdf/2207.11415.pdf)
>  We study the following repeated non-atomic routing game. In every round, nature chooses a state in an i.i.d. manner according to a publicly known distribution, which influences link latency functions. The system planner makes private route recommendations to participating agents, which constitute a fixed fraction, according to a publicly known signaling strategy. The participating agents choose between obeying or not obeying the recommendation according to cumulative regret of the participating agent population in the previous round. The non-participating agents choose route according to myopic best response to a calibrated forecast of the routing decisions of the participating agents. We show that, for parallel networks, if the planner's signal strategy satisfies the obedience condition, then, almost surely, the link flows are asymptotically consistent with the Bayes correlated equilibrium induced by the signaling strategy.      
### 50.Computer Vision Aided mmWave Beam Alignment in V2X Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.11409.pdf)
>  Visual information, captured for example by cameras, can effectively reflect the sizes and locations of the environmental scattering objects, and thereby can be used to infer communications parameters like propagation directions, receiver powers, as well as the blockage status. In this paper, we propose a novel beam alignment framework that leverages images taken by cameras installed at the mobile user. Specifically, we utilize 3D object detection techniques to extract the size and location information of the dynamic vehicles around the mobile user, and design a deep neural network (DNN) to infer the optimal beam pair for transceivers without any pilot signal overhead. Moreover, to avoid performing beam alignment too frequently or too slowly, a beam coherence time (BCT) prediction method is developed based on the vision information. This can effectively improve the transmission rate compared with the beam alignment approach with the fixed BCT. Simulation results show that the proposed vision based beam alignment methods outperform the existing LIDAR and vision based solutions, and demand for much lower hardware cost and communication overhead.      
### 51.A Deployable Online Optimization Framework for EV Smart Charging with Real-World Test Cases  [ :arrow_down: ](https://arxiv.org/pdf/2207.11403.pdf)
>  We present a customizable online optimization framework for real-time EV smart charging to be readily implemented at real large-scale charging facilities. Notably, due to real-world constraints, we designed our framework around 3 main requirements. First, the smart charging strategy is readily deployable and customizable for a wide-array of facilities, infrastructure, objectives, and constraints. Second, the online optimization framework can be easily modified to operate with or without user input for energy request amounts and/or departure time estimates which allows our framework to be implemented on standard chargers with 1-way communication or newer chargers with 2-way communication. Third, our online optimization framework outperforms other real-time strategies (including first-come-first-serve, least-laxity-first, earliest-deadline-first, etc.) in multiple real-world test cases with various objectives. We showcase our framework with two real-world test cases with charging session data sourced from SLAC and Google campuses in the Bay Area.      
### 52.Wavelength-Resolution SAR Ground Scene Prediction Based on Image Stack  [ :arrow_down: ](https://arxiv.org/pdf/2207.11400.pdf)
>  This paper presents five different statistical methods for ground scene prediction (GSP) in wavelength-resolution synthetic aperture radar (SAR) images. The GSP image can be used as a reference image in a change detection algorithm yielding a high probability of detection and low false alarm rate. The predictions are based on image stacks, which are composed of images from the same scene acquired at different instants with the same flight geometry. The considered methods for obtaining the ground scene prediction include (i) autoregressive models; (ii) trimmed mean; (iii) median; (iv) intensity mean; and (v) mean. It is expected that the predicted image presents the true ground scene without change and preserves the ground backscattering pattern. The study indicate that the the median method provided the most accurate representation of the true ground. To show the applicability of the GSP, a change detection algorithm was considered using the median ground scene as a reference image. As a result, the median method displayed the probability of detection of $97\%$ and a false alarm rate of 0.11/km$^2, when considering military vehicles concealed in a forest.      
### 53.Low-Complexity Acoustic Echo Cancellation with Neural Kalman Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2207.11388.pdf)
>  The Kalman filter has been adopted in acoustic echo cancellation due to its robustness to double-talk, fast convergence, and good steady-state performance. The performance of Kalman filter is closely related to the estimation accuracy of the state noise covariance and the observation noise covariance. The estimation error may lead to unacceptable results, especially when the echo path suffers abrupt changes, the tracking performance of the Kalman filter could be degraded significantly. In this paper, we propose the neural Kalman filtering (NKF), which uses neural networks to implicitly model the covariance of the state noise and observation noise and to output the Kalman gain in real-time. Experimental results on both synthetic test sets and real-recorded test sets show that, the proposed NKF has superior convergence and re-convergence performance while ensuring low near-end speech degradation comparing with the state-of-the-art model-based methods. Moreover, the model size of the proposed NKF is merely 5.3 K and the RTF is as low as 0.09, which indicates that it can be deployed in low-resource platforms.      
### 54.Split Happens! Imprecise and Negative Information in Gaussian Mixture Random Finite Set Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2207.11356.pdf)
>  In object tracking and state estimation problems, ambiguous evidence such as imprecise measurements and the absence of detections can contain valuable information and thus be leveraged to further refine the probabilistic belief state. In particular, knowledge of a sensor's bounded field-of-view can be exploited to incorporate evidence of where an object was not observed. This paper presents a systematic approach for incorporating knowledge of the field-of-view geometry and position and object inclusion/exclusion evidence into object state densities and random finite set multi-object cardinality distributions. The resulting state estimation problem is nonlinear and solved using a new Gaussian mixture approximation based on recursive component splitting. Based on this approximation, a novel Gaussian mixture Bernoulli filter for imprecise measurements is derived and demonstrated in a tracking problem using only natural language statements as inputs. This paper also considers the relationship between bounded fields-of-view and cardinality distributions for a representative selection of multi-object distributions, which can be used for sensor planning, as is demonstrated through a problem involving a multi-Bernoulli process with up to one-hundred potential objects.      
### 55.On Statistical Modeling of Load in Systems with High Capacity Distributed Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2207.11355.pdf)
>  The emergence of distributed energy resources has led to new challenges in the operation and planning of power networks. Of particular significance is the introduction of a new layer of complexity that manifests in the form of new uncertainties that could severely limit the resiliency and reliability of a modern power system. For example, the increasing adoption of unconventional loads such as plug-in electric vehicles can result in uncertain consumer demand patterns, which are often characterized by random undesirable peaks in energy consumption. In the first half of 2021, the electric vehicle sales increased by nearly 160%, thus accounting for roughly 26% of new sales in the global automotive market. This paper investigates the applicability of generalized mixture models for the statistical representation of aggregated load in systems enhanced with high capacity distributed energy resources such as plug-in electric vehicles.      
### 56.Stationary Cost Nodes in Infinite Horizon LQG-GMFGs  [ :arrow_down: ](https://arxiv.org/pdf/2207.11343.pdf)
>  An analysis of infinite horizon linear quadratic Gaussian (LQG) Mean Field Games is given within the general framework of Graphon Mean Field Games (GMFG) on dense infinite graphs (or networks) introduced in Caines and Huang (2018). For a class of LQG-GMFGs, analytical expressions are derived for the infinite horizon Nash values at the nodes of the infinite graph. Furthermore, under specific conditions on the network and the initial population means, it is shown that the nodes with strict local maximal infinite network degree are also nodes with strict local minimal cost at equilibrium.      
### 57.Impacts of Dynamic Line Ratings on the ERCOT Transmission System  [ :arrow_down: ](https://arxiv.org/pdf/2207.11309.pdf)
>  Grid regulators and participants are paying increasing attention to Dynamic Line Ratings (DLR) as a new approach to address transmission system bottlenecks. In this paper, a thorough comparison of DLR, Ambient Adjusted Ratings (AAR), and the traditional Static Line Ratings (SLR) are conducted on a synthetic ERCOT grid. Estimates of DLR and AAR are calculated using an equation based on heat balance physics, along with high-resolution weather data of temperature and wind velocities. A constraint generation method for contingency screening is developed for solving security-constrained optimal power flow. Numerical results suggest that employing DLR could double the benefits compared to those of AAR relative to SLR, in terms of system costs, renewable curtailment, and emissions.      
### 58.Brain tumor detection using artificial convolutional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11248.pdf)
>  In this paper, a convolutional neural network (CNN) was used to classify NMR images of human brains with 4 different types of tumors: meningioma, glioma and pituitary gland tumors. During the training phase of this project, an accuracy of 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.      
### 59.SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness  [ :arrow_down: ](https://arxiv.org/pdf/2207.12391.pdf)
>  Deep neural network-based image classifications are vulnerable to adversarial perturbations. The image classifications can be easily fooled by adding artificial small and imperceptible perturbations to input images. As one of the most effective defense strategies, adversarial training was proposed to address the vulnerability of classification models, where the adversarial examples are created and injected into training data during training. The attack and defense of classification models have been intensively studied in past years. Semantic segmentation, as an extension of classifications, has also received great attention recently. Recent work shows a large number of attack iterations are required to create effective adversarial examples to fool segmentation models. The observation makes both robustness evaluation and adversarial training on segmentation models challenging. In this work, we propose an effective and efficient segmentation attack method, dubbed SegPGD. Besides, we provide a convergence analysis to show the proposed SegPGD can create more effective adversarial examples than PGD under the same number of attack iterations. Furthermore, we propose to apply our SegPGD as the underlying attack method for segmentation adversarial training. Since SegPGD can create more effective adversarial examples, the adversarial training with our SegPGD can boost the robustness of segmentation models. Our proposals are also verified with experiments on popular Segmentation model architectures and standard segmentation datasets.      
### 60.Toward reliable signals decoding for electroencephalogram: A benchmark study to EEGNeX  [ :arrow_down: ](https://arxiv.org/pdf/2207.12369.pdf)
>  The development of brain-computer interfaces (BCI) has facilitated our study of mental representations in the brain. Neural networks (NNs) have been widely used in BCI due to their decent pattern learning capabilities; however, to our best knowledge, a comprehensive comparison between various neural network models has not been well addressed, due to the interdisciplinary difficulty and case-based study in the domain. Here, we tested the capabilities of common NN architectures in deciphering mental representations from electroencephalogram (EEG) signals, which were recorded in representative classification tasks. In this study, we: 1. Construct 20 mechanism-wise different, typical NN types and their variants on decoding various EEG datasets to show a comprehensive performance comparison regarding their EEG information representation capability. 2. Lighten an efficient pathway based on the analysis results to gradually develop general improvements and propose a novel NN architecture: EEGNeX. 3. We open-sourced all models in an out-of-the-box status, to serve as the benchmark in the BCI community. The performance benchmark contributes as an essential milestone to filling the gap between domains understanding and support for further interdisciplinary studies like analogy investigations between the brain bioelectric signal generation process and NN architecture. All benchmark models and EEGNeX source code is available at:<a class="link-external link-https" href="https://github.com/chenxiachan/EEGNeX" rel="external noopener nofollow">this https URL</a>.      
### 61.OpenRAN Gym: AI/ML Development, Data Collection, and Testing for O-RAN on PAWR Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2207.12362.pdf)
>  Open Radio Access Network (RAN) architectures will enable interoperability, openness and programmable data-driven control in next generation cellular networks. However, developing and testing efficient solutions that generalize across heterogeneous cellular deployments and scales, and that optimize network performance in such diverse environments is a complex task that is still largely unexplored. In this paper we present OpenRAN Gym, a unified, open, and O-RAN-compliant experimental toolbox for data collection, design, prototyping and testing of end-to-end data-driven control solutions for next generation Open RAN systems. OpenRAN Gym extends and combines into a unique solution several software frameworks for data collection of RAN statistics and RAN control, and a lightweight O-RAN near-real-time RAN Intelligent Controller (RIC) tailored to run on experimental wireless platforms. We first provide an overview of the various architectural components of OpenRAN Gym and describe how it is used to collect data and design, train and test artificial intelligence and machine learning O-RAN-compliant applications (xApps) at scale. We then describe in detail how to test the developed xApps on softwarized RANs and provide an example of two xApps developed with OpenRAN Gym that are used to control a network with 7 base stations and 42 users deployed on the Colosseum testbed. Finally, we show how solutions developed with OpenRAN Gym on Colosseum can be exported to real-world, heterogeneous wireless platforms, such as the Arena testbed and the POWDER and COSMOS platforms of the PAWR program. OpenRAN Gym and its software components are open-source and publicly-available to the research community.      
### 62.Unsteady aerodynamic modeling of Aerobat using lifting line theory and Wagner's function  [ :arrow_down: ](https://arxiv.org/pdf/2207.12353.pdf)
>  Flying animals possess highly complex physical characteristics and are capable of performing agile maneuvers using their wings. The flapping wings generate complex wake structures that influence the aerodynamic forces, which can be difficult to model. While it is possible to model these forces using fluid-structure interaction, it is very computationally expensive and difficult to formulate. In this paper, we follow a simpler approach by deriving the aerodynamic forces using a relatively small number of states and presenting them in a simple state-space form. The formulation utilizes Prandtl's lifting line theory and Wagner's function to determine the unsteady aerodynamic forces acting on the wing in a simulation, which then are compared to experimental data of the bat-inspired robot called the Aerobat. The simulated trailing-edge vortex shedding can be evaluated from this model, which then can be analyzed for a wake-based gait design approach to improve the aerodynamic performance of the robot.      
### 63.Tikhonov Regularization of Sphere-Valued Signals  [ :arrow_down: ](https://arxiv.org/pdf/2207.12330.pdf)
>  It is common to have to process signals, whose values are points on the 3-D sphere. We consider a Tikhonov-type regularization model to smoothen or interpolate sphere-valued signals defined on arbitrary graphs. We propose a convex relaxation of this nonconvex problem as a semidefinite program, which is easy to solve numerically and is efficient in practice.      
### 64.FAD: A Chinese Dataset for Fake Audio Detection  [ :arrow_down: ](https://arxiv.org/pdf/2207.12308.pdf)
>  Fake audio detection is a growing concern and some relevant datasets have been designed for research. But there is no standard public Chinese dataset under additive noise conditions. In this paper, we aim to fill in the gap and design a Chinese fake audio detection dataset (FAD) for studying more generalized detection methods. Twelve mainstream speech generation techniques are used to generate fake audios. To simulate the real-life scenarios, three noise datasets are selected for noisy adding at five different signal noise ratios. FAD dataset can be used not only for fake audio detection, but also for detecting the algorithms of fake utterances for audio forensics. Baseline results are presented with analysis. The results that show fake audio detection methods with generalization remain challenging. The FAD dataset is publicly available.      
### 65.A Letter on Progress Made on Husky Carbon: A Legged-Aerial, Multi-modal Platform  [ :arrow_down: ](https://arxiv.org/pdf/2207.12254.pdf)
>  Animals, such as birds, widely use multi-modal locomotion by combining legged and aerial mobility with dominant inertial effects. The robotic biomimicry of this multi-modal locomotion feat can yield ultra-flexible systems in terms of their ability to negotiate their task spaces. The main objective of this paper is to discuss the challenges in achieving multi-modal locomotion, and to report our progress in developing our quadrupedal robot capable of multi-modal locomotion (legged and aerial locomotion), the Husky Carbon. We report the mechanical and electrical components utilized in our robot, in addition to the simulation and experimentation done to achieve our goal in developing a versatile multi-modal robotic platform.      
### 66.Domain Adapting Speech Emotion Recognition modals to real-world scenario with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.12248.pdf)
>  Deep reinforcement learning has been a popular training paradigm as deep learning has gained popularity in the field of machine learning. Domain adaptation allows us to transfer knowledge learnt by a model across domains after a phase of training. The inability to adapt an existing model to a real-world domain is one of the shortcomings of current domain adaptation algorithms. We present a deep reinforcement learning-based strategy for adapting a pre-trained model to a newer domain while interacting with the environment and collecting continual feedback. This method was used on the Speech Emotion Recognition task, which included both cross-corpus and cross-language domain adaption schema. Furthermore, it demonstrates that in a real-world environment, our approach outperforms the supervised learning strategy by 42% and 20% in cross-corpus and cross-language schema, respectively.      
### 67.Resilient Navigation and Path Planning System for High-speed Autonomous Race Car  [ :arrow_down: ](https://arxiv.org/pdf/2207.12232.pdf)
>  This paper describes resilient navigation and planning algorithm for high-speed autonomous race, Indy Autonomous Challenge (IAC). The IAC is a competition with full-scale autonomous race car that can drive up to 290 km/h(180mph). Due to its high-speed and heavy vibration of the race car, GPS/INS system is prone to be degraded. These degraded GPS measurements can cause a critical localization error leading to a serious crashing accidents. To this end, we propose a robust navigation system to implement multi-sensor fusion Kalman filter. In this study, we present how to identify the degradation of measurement based on probabilistic approaches. Based on this approach, we could compute optimal measurement values for Kalman filter correction step. At the same time, we present the other resilient navigation system so that race car can follow the race track in fatal localization failure situations. In addition, this paper also covers the optimal path planning algorithm for obstacle avoidance. To take account for original optimal racing line, obstacles, vehicle dynamics, we propose a road-graph-based path planning algorithm to guarantee our race car drives in-bounded conditions. In the experiments, we will evaluate our designed localization system can handle the degraded data, and sometimes prevent serious crashing accidents while high-speed driving. In addition, we will describe how we successfully completed the obstacle avoidance challenge.      
### 68.Hardware-in-the-loop simulation of a UAV autonomous landing algorithm implemented in SoC FPGA  [ :arrow_down: ](https://arxiv.org/pdf/2207.12198.pdf)
>  This paper presents a system for hardware-in-the-loop (HiL) simulation of unmanned aerial vehicle (UAV) control algorithms implemented on a heterogeneous SoC FPGA computing platforms. The AirSim simulator running on a PC and an Arty Z7 development board with a Zynq SoC chip from AMD Xilinx were used. Communication was carried out via a serial USB link. An application for autonomous landing on a specially marked landing strip was selected as a case study. A landing site detection algorithm was implemented on the Zynq SoC platform. This allowed processing a 1280 x 720 @ 60 fps video stream in real time. Performed tests showed that the system works correctly and there are no delays that could negatively affect the stability of the control. The proposed concept is characterised by relative simplicity and low implementation cost. At the same time, it can be applied to test various types of high-level perception and control algorithms for UAV implemented on embedded platforms. We provide the code developed on GitHub, which includes both Python scripts running on the PC and C code running on Arty Z7.      
### 69.Cross-Modal Contrastive Representation Learning for Audio-to-Image Generation  [ :arrow_down: ](https://arxiv.org/pdf/2207.12121.pdf)
>  Multiple modalities for certain information provide a variety of perspectives on that information, which can improve the understanding of the information. Thus, it may be crucial to generate data of different modality from the existing data to enhance the understanding. In this paper, we investigate the cross-modal audio-to-image generation problem and propose Cross-Modal Contrastive Representation Learning (CMCRL) to extract useful features from audios and use it in the generation phase. Experimental results show that CMCRL enhances quality of images generated than previous research.      
### 70.A Novel Framework for Dataset Generation for profiling Disassembly attacks using Side-Channel Leakages and Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.12068.pdf)
>  Various studies among side-channel attacks have tried to extract information through leakages from electronic devices to reach the instruction flow of some appliances. However, previous methods highly depend on the resolution of traced data. Obtaining low-noise traces is not always feasible in real attack scenarios. This study proposes two deep models to extract low and high-level features from side-channel traces and classify them to related instructions. We aim to evaluate the accuracy of a side-channel attack on low-resolution data with a more robust feature extractor thanks to neural networks. As inves-tigated, instruction flow in real programs is predictable and follows specific distributions. This leads to proposing a LSTM model to estimate these distributions, which could expedite the reverse engineering process and also raise the accuracy. The proposed model for leakage classification reaches 54.58% accuracy on average and outperforms other existing methods on our datasets. Also, LSTM model reaches 94.39% accuracy for instruction prediction on standard implementation of cryptographic algorithms.      
### 71.Multi-Scale Asset Distribution Model for Dynamic Environments  [ :arrow_down: ](https://arxiv.org/pdf/2207.12063.pdf)
>  In many self-organising systems the ability to extract necessary resources from the external environment is essential to the system's growth and survival. Examples include the extraction of sunlight and nutrients in organic plants, of monetary income in business organisations and of mobile robots in swarm intelligence actions. When operating within competitive, ever-changing environments, such systems must distribute their internal assets wisely so as to improve and adapt their ability to extract available resources. As the system size increases, the asset-distribution process often gets organised around a multi-scale control topology. This topology may be static (fixed) or dynamic (enabling growth and structural adaptation) depending on the system's internal constraints and adaptive mechanisms. In this paper, we expand on a plant-inspired asset-distribution model and introduce a more general multi-scale model applicable across a wider range of natural and artificial system domains. We study the impact that the topology of the multi-scale control process has upon the system's ability to self-adapt asset distribution when resource availability changes within the environment. Results show how different topological characteristics and different competition levels between system branches impact overall system profitability, adaptation delays and disturbances when environmental changes occur. These findings provide a basis for system designers to select the most suitable topology and configuration for their particular application and execution environment.      
### 72.Optimizing the Achievable Rate in MIMO Systems Assisted by Multiple Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2207.12047.pdf)
>  In recent years there has been a growing interest in reconfigurable intelligent surfaces (RISs) as enablers for the realization of smart radio propagation environments which can provide performance improvements with low energy consumption in future wireless networks. However, to reap the potential gains of RIS it is crucial to jointly design both the transmit precoder and the phases of the RIS elements. Within this context, in this paper we study the use of multiple RIS panels in a parallel or multi-hop configuration with the aim of assisting a multi-stream multiple-input multiple-output (MIMO) communication. To solve the nonconvex joint optimization problem of the precoder and RIS elements targeted at maximizing the achievable rate, we propose an iterative algorithm based on the monotone accelerated proximal gradient (mAPG) method which includes an extrapolation step for improving the convergence speed and monitoring variables for ensuring sufficient descent of the algorithm. Based on the sufficient descent property we then present a detailed convergence analysis of the algorithm which includes expressions for the step size. Simulation results in different scenarios show that, besides being effective, the proposed approach can often achieve higher rates than other benchmarked schemes.      
### 73.Deep dual stream residual network with contextual attention for pansharpening of remote sensing images  [ :arrow_down: ](https://arxiv.org/pdf/2207.12004.pdf)
>  Pansharpening enhances spatial details of high spectral resolution multispectral images using features of high spatial resolution panchromatic image. There are a number of traditional pansharpening approaches but producing an image exhibiting high spectral and spatial fidelity is still an open problem. Recently, deep learning has been used to produce promising pansharpened images; however, most of these approaches apply similar treatment to both multispectral and panchromatic images by using the same network for feature extraction. In this work, we present present a novel dual attention-based two-stream network. It starts with feature extraction using two separate networks for both images, an encoder with attention mechanism to recalibrate the extracted features. This is followed by fusion of the features forming a compact representation fed into an image reconstruction network to produce a pansharpened image. The experimental results on the Pliades dataset using standard quantitative evaluation metrics and visual inspection demonstrates that the proposed approach performs better than other approaches in terms of pansharpened image quality.      
### 74.An Optimal Motion Planning Framework for Quadruped Jumping  [ :arrow_down: ](https://arxiv.org/pdf/2207.12002.pdf)
>  This paper presents an optimal motion planning framework to generate versatile energy-optimal quadrupedal jumping motions automatically (e.g., flips, spin). The jumping motions via the centroidal dynamics are formulated as a 12-dimensional black-box optimization problem subject to the robot kino-dynamic constraints. Gradient-based approaches offer great success in addressing trajectory optimization (TO), yet, prior knowledge (e.g., reference motion, contact schedule) is required and results in sub-optimal solutions. The new proposed framework first employed a heuristics-based optimization method to avoid these problems. Moreover, a prioritization fitness function is created for heuristics-based algorithms in robot ground reaction force (GRF) planning, enhancing convergence and searching performance considerably. Since heuristics-based algorithms often require significant time, motions are planned offline and stored as a pre-motion library. A selector is designed to automatically choose motions with user-specified or perception information as input. The proposed framework has been successfully validated only with a simple continuously tracking PD controller in an open-source Mini-Cheetah by several challenging jumping motions, including jumping over a window-shaped obstacle with 30 cm height and left-flipping over a rectangle obstacle with 27 cm height.      
### 75.Observer Design for the State Estimation of Epidemic Processes  [ :arrow_down: ](https://arxiv.org/pdf/2207.11977.pdf)
>  Although an appropriate choice of measured state variables may ensure observability, designing state observers for the state estimation of epidemic models remains a challenging task. Epidemic spread is a nonlinear process, often modeled as the law of mass action, which is of a quadratic form; thus, on a compact domain, its Lipschitz constant turns out to be local and relatively large, which renders the Lipschitz-based design criteria of existing observer architectures infeasible. In this paper, a novel observer architecture is proposed for the state estimation of a class of nonlinear systems that encompasses the deterministic epidemic models. The proposed observer offers extra leverage to reduce the influence of nonlinearity in the estimation error dynamics, which is not possible in other Luenberger-like observers. Algebraic Riccati inequalities are derived as sufficient conditions for the asymptotic convergence of the estimation error to zero under local Lipschitz and generalized Lipschitz assumptions. Equivalent linear matrix inequality formulations of the algebraic Riccati inequalities are also provided. The efficacy of the proposed observer design is illustrated by its application on the celebrated SIDARTHE-V epidemic model.      
### 76.Beamforming Analysis and Design for Wideband THz Reconfigurable Intelligent Surface Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.11926.pdf)
>  Reconfigurable intelligent surface (RIS)-aided terahertz (THz) communications have been regarded as a promising candidate for future 6G networks because of its ultra-wide bandwidth and ultra-low power consumption. However, there exists the beam split problem, especially when the base station (BS) or RIS owns the large-scale antennas, which may lead to serious array gain loss. Therefore, in this paper, we investigate the beam split and beamforming design problems in the THz RIS communications. Specifically, we first analyze the beam split effect caused by different RIS sizes, shapes and deployments. On this basis, we apply the fully connected time delayer phase shifter hybrid beamforming architecture at the BS and deploy distributed RISs to cooperatively mitigate the beam split effect. We aim to maximize the achievable sum rate by jointly optimizing the hybrid analog/digital beamforming, time delays at the BS and reflection coefficients at the RISs. To solve the formulated problem, we first design the analog beamforming and time delays based on different RISs physical directions, and then it is transformed into an optimization problem by jointly optimizing the digital beamforming and reflection coefficients. Next, we propose an alternatively iterative optimization algorithm to deal with it. Specifically, for given the reflection coefficients, we propose an iterative algorithm based on the minimum mean square error technique to obtain the digital beamforming. After, we apply LDR and MCQT methods to transform the original problem to a QCQP, which can be solved by ADMM technique to obtain the reflection coefficients. Finally, the digital beamforming and reflection coefficients are obtained via repeating the above processes until convergence. Simulation results verify that the proposed scheme can effectively alleviate the beam split effect and improve the system capacity.      
### 77.Sub-Aperture Feature Adaptation in Single Image Super-resolution Model for Light Field Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2207.11894.pdf)
>  With the availability of commercial Light Field (LF) cameras, LF imaging has emerged as an up and coming technology in computational photography. However, the spatial resolution is significantly constrained in commercial microlens based LF cameras because of the inherent multiplexing of spatial and angular information. Therefore, it becomes the main bottleneck for other applications of light field cameras. This paper proposes an adaptation module in a pretrained Single Image Super Resolution (SISR) network to leverage the powerful SISR model instead of using highly engineered light field imaging domain specific Super Resolution models. The adaption module consists of a Sub aperture Shift block and a fusion block. It is an adaptation in the SISR network to further exploit the spatial and angular information in LF images to improve the super resolution performance. Experimental validation shows that the proposed method outperforms existing light field super resolution algorithms. It also achieves PSNR gains of more than 1 dB across all the datasets as compared to the same pretrained SISR models for scale factor 2, and PSNR gains 0.6 to 1 dB for scale factor 4.      
### 78.Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.11860.pdf)
>  In this paper, we address panoramic semantic segmentation, which provides a full-view and dense-pixel understanding of surroundings in a holistic way. Panoramic segmentation is under-explored due to two critical challenges: (1) image distortions and object deformations on panoramas; (2) lack of annotations for training panoramic segmenters. To tackle these problems, we propose a Transformer for Panoramic Semantic Segmentation (Trans4PASS) architecture. First, to enhance distortion awareness, Trans4PASS, equipped with Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) modules, is capable of handling object deformations and image distortions whenever (before or after adaptation) and wherever (shallow or deep levels) by design. We further introduce the upgraded Trans4PASS+ model, featuring DMLPv2 with parallel token mixing to improve the flexibility and generalizability in modeling discriminative cues. Second, we propose a Mutual Prototypical Adaptation (MPA) strategy for unsupervised domain adaptation. Third, aside from Pinhole-to-Panoramic (Pin2Pan) adaptation, we create a new dataset (SynPASS) with 9,080 panoramic images to explore a Synthetic-to-Real (Syn2Real) adaptation scheme in 360 imagery. Extensive experiments are conducted, which cover indoor and outdoor scenarios, and each of them is investigated with Pin2Pan and Syn2Real regimens. Trans4PASS+ achieves state-of-the-art performances on four domain adaptive panoramic semantic segmentation benchmarks. Code is available at <a class="link-external link-https" href="https://github.com/jamycheung/Trans4PASS" rel="external noopener nofollow">this https URL</a>.      
### 79.Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.11844.pdf)
>  Normalizing flow models have been used successfully for generative image super-resolution (SR) by approximating complex distribution of natural images to simple tractable distribution in latent space through Invertible Neural Networks (INN). These models can generate multiple realistic SR images from one low-resolution (LR) input using randomly sampled points in the latent space, simulating the ill-posed nature of image upscaling where multiple high-resolution (HR) images correspond to the same LR. Lately, the invertible process in INN has also been used successfully by bidirectional image rescaling models like IRN and HCFlow for joint optimization of downscaling and inverse upscaling, resulting in significant improvements in upscaled image quality. While they are optimized for image downscaling too, the ill-posed nature of image downscaling, where one HR image could be downsized to multiple LR images depending on different interpolation kernels and resampling methods, is not considered. A new downscaling latent variable, in addition to the original one representing uncertainties in image upscaling, is introduced to model variations in the image downscaling process. This dual latent variable enhancement is applicable to different image rescaling models and it is shown in extensive experiments that it can improve image upscaling accuracy consistently without sacrificing image quality in downscaled LR images. It is also shown to be effective in enhancing other INN-based models for image restoration applications like image hiding.      
### 80.Towards An Optimal Solution to Place Bistatic Radars for Belt Barrier Coverage with Minimum Cost  [ :arrow_down: ](https://arxiv.org/pdf/2207.11818.pdf)
>  With the rapid growth of threats, sophistication and diversity in the manner of intrusion, traditional belt barrier systems are now faced with a major challenge of providing high and concrete coverage quality to expand the guarding service market. Recent efforts aim at constructing a belt barrier by deploying bistatic radar(s) on a specific line regardless of the limitation on deployment locations, to keep the width of the barrier from going below a specific threshold and the total bistatic radar placement cost is minimized, referred to as the Minimum Cost Linear Placement (MCLP) problem. The existing solutions are heuristic, and their validity is tightly bound by the barrier width parameter that these solutions only work for a fixed barrier width value. In this work, we propose an optimal solution, referred to as the Opt_MCLP, for the "open MCLP problem" that works for full range of the barrier width. Through rigorous theoretical analysis and experimentation, we demonstrate that the proposed algorithms perform well in terms of placement cost reduction and barrier coverage guarantee.      
### 81.Image Denoising Using Convolutional Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2207.11771.pdf)
>  With the inexorable digitalisation of the modern world, every subset in the field of technology goes through major advancements constantly. One such subset is digital images which are ever so popular. Images can not always be as visually pleasing or clear as you would want them to be and are often distorted or obscured with noise. A number of techniques to enhance images have come up as the years passed, all with their own respective pros and cons. In this paper, we look at one such particular technique which accomplishes this task with the help of a neural network model commonly known as an autoencoder. We construct different architectures for the model and compare results in order to decide the one best suited for the task. The characteristics and working of the model are discussed briefly knowing which can help set a path for future research.      
### 82.Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2207.11749.pdf)
>  The separation of single-channel underwater acoustic signals is a challenging problem with practical significance. In view of the signal separation problem with unknown numbers of signals, we propose a solution with a fixed number of output channels, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we modify two algorithms developed for known numbers of signals based on autoencoders, which are highly explainable. We also propose a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The mute channel output is also good.      
### 83.HouseX: A Fine-grained House Music Dataset and its Potential in the Music Industry  [ :arrow_down: ](https://arxiv.org/pdf/2207.11690.pdf)
>  Machine sound classification has been one of the fundamental tasks of music technology. A major branch of sound classification is the classification of music genres. However, though covering most genres of music, existing music genre datasets often do not contain fine-grained labels that indicate the detailed sub-genres of music. In consideration of the consistency of genres of songs in a mixtape or in a DJ (live) set, we have collected and annotated a dataset of house music that provide 4 sub-genre labels, namely future house, bass house, progressive house and melodic house. Experiments show that our annotations well exhibit the characteristics of different categories. Also, we have built baseline models that classify the sub-genre based on the mel-spectrograms of a track, achieving strongly competitive results. Besides, we have put forward a few application scenarios of our dataset and baseline model, with a simulated sci-fi tunnel as a short demo built and rendered in a 3D modeling software, with the colors of the lights automated by the output of our model.      
### 84.Improved Regularization of Event-based Learning by Reversing and Drifting  [ :arrow_down: ](https://arxiv.org/pdf/2207.11659.pdf)
>  Event camera has an enormous potential in challenging scenes for its advantages of high temporal resolution, high dynamic range, low power consumption, and no motion blur. However, event-based learning is hindered by insufficient generalization ability. In this paper, we first analyze the influence of different brightness variations on event data. Then we propose two novel augmentation methods: EventReverse and EventDrift. By reversing and drifting events to their corresponding positions in the spatiotemporal or polarity domain, the proposed methods generate samples affected by different brightness variations, which improves the robustness of event-based learning and results in a better generalization. Extensive experiments on N-CARS, N-Caltech101 and CIFAR10-DVS datasets demonstrate that our method is general and remarkably effective.      
### 85.Clustered Cell-Free Networking: A Graph Partitioning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.11641.pdf)
>  By moving to millimeter wave (mmWave) frequencies, base stations (BSs) will be densely deployed to provide seamless coverage in sixth generation (6G) mobile communication systems, which, unfortunately, leads to severe cell-edge problem. In addition, with massive multiple-input-multiple-output (MIMO) antenna arrays employed at BSs, the beamspace channel is sparse for each user, and thus there is no need to serve all the users in a cell by all the beams therein jointly. Therefore, it is of paramount importance to develop a flexible clustered cell-free networking scheme that can decompose the whole network into a number of weakly interfered small subnetworks operating independently and in parallel. Given a per-user rate constraint for service quality guarantee, this paper aims to maximize the number of decomposed subnetworks so as to reduce the signaling overhead and system complexity as much as possible. By formulating it as a bipartite graph partitioning problem, a rate-constrained network decomposition (RC-NetDecomp) algorithm is proposed, which can smoothly tune the network structure from the current cellular network with simple beam allocation to a fully cooperative network by increasing the required per-user rate. Simulation results demonstrate that the proposed RC-NetDecomp algorithm outperforms existing baselines in terms of average per-user rate, fairness among users and energy efficiency.      
### 86.Prediction Intervals in the Beta Autoregressive Moving Average Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.11628.pdf)
>  In this paper, we propose five prediction intervals for the beta autoregressive moving average model. This model is suitable for modeling and forecasting variables that assume values in the interval $(0,1)$. Two of the proposed prediction intervals are based on approximations considering the normal distribution and the quantile function of the beta distribution. We also consider bootstrap-based prediction intervals, namely: (i) bootstrap prediction errors (BPE) interval; (ii) bias-corrected and acceleration (BCa) prediction interval; and (iii) percentile prediction interval based on the quantiles of the bootstrap-predicted values for two different bootstrapping schemes. The proposed prediction intervals were evaluated according to Monte Carlo simulations. The BCa prediction interval offered the best performance among the evaluated intervals, showing lower coverage rate distortion and small average length. We applied our methodology for predicting the water level of the Cantareira water supply system in So Paulo, Brazil.      
### 87.All you need for horizontal slicing in 5G network  [ :arrow_down: ](https://arxiv.org/pdf/2207.11477.pdf)
>  The telecommunication field has seen unprecedented growth in the last decade that has led to the release of several generations that have been committed to satisfy users by increasing the data rate and reducing the latency, especially in the 5G network. With fully commercialized 5G networks that is already launched in many country, Software-defined network (SDN) and network function virtualization (NFV) will facilitate the implementation of NS. SDN and NFV will serve as the basis for NS, allowing efficient use of both physical and virtual resources. This paper makes it possible to analyze, propose an efficient model, and utilize all of the available resources of the 5G network.      
### 88.Finite-Blocklength RIS-Aided Transmit Beamforming  [ :arrow_down: ](https://arxiv.org/pdf/2207.11444.pdf)
>  This paper considers the downlink of an ultra-reliable low-latency communication (URLLC) system in which a base station (BS) serves multiple single-antenna users in the short (finite) blocklength (FBL) regime with the assistance of a reconfigurable intelligent surface (RIS). In the FBL regime, the users' achievable rates are complex functions of the beamforming vectors and of the RIS's programmable reflecting elements (PREs). We propose the joint design of the transmit beamformers and PREs, the problem of maximizing the geometric mean (GM) of these rates (GM-rate) and show that this aforementioned results are providing fair rate distribution and thus reliable links to all users. A novel computational algorithm is developed, which is based on closed forms to generate improved feasible points, using its execution. The simulations show the merit of our solution.      
### 89.Arbitrary Style Transfer with Structure Enhancement by Combining the Global and Local Loss  [ :arrow_down: ](https://arxiv.org/pdf/2207.11438.pdf)
>  Arbitrary style transfer generates an artistic image which combines the structure of a content image and the artistic style of the artwork by using only one trained network. The image representation used in this method contains content structure representation and the style patterns representation, which is usually the features representation of high-level in the pre-trained classification networks. However, the traditional classification networks were designed for classification which usually focus on high-level features and ignore other features. As the result, the stylized images distribute style elements evenly throughout the image and make the overall image structure unrecognizable. To solve this problem, we introduce a novel arbitrary style transfer method with structure enhancement by combining the global and local loss. The local structure details are represented by Lapstyle and the global structure is controlled by the image depth. Experimental results demonstrate that our method can generate higher-quality images with impressive visual effects on several common datasets, comparing with other state-of-the-art methods.      
### 90.Driver Dojo: A Benchmark for Generalizable Reinforcement Learning for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2207.11432.pdf)
>  Reinforcement learning (RL) has shown to reach super human-level performance across a wide range of tasks. However, unlike supervised machine learning, learning strategies that generalize well to a wide range of situations remains one of the most challenging problems for real-world RL. Autonomous driving (AD) provides a multi-faceted experimental field, as it is necessary to learn the correct behavior over many variations of road layouts and large distributions of possible traffic situations, including individual driver personalities and hard-to-predict traffic events. In this paper we propose a challenging benchmark for generalizable RL for AD based on a configurable, flexible, and performant code base. Our benchmark uses a catalog of randomized scenario generators, including multiple mechanisms for road layout and traffic variations, different numerical and visual observation types, distinct action spaces, diverse vehicle models, and allows for use under static scenario definitions. In addition to purely algorithmic insights, our application-oriented benchmark also enables a better understanding of the impact of design decisions such as action and observation space on the generalizability of policies. Our benchmark aims to encourage researchers to propose solutions that are able to successfully generalize across scenarios, a task in which current RL methods fail. The code for the benchmark is available at <a class="link-external link-https" href="https://github.com/seawee1/driver-dojo" rel="external noopener nofollow">this https URL</a>.      
### 91.Rate-Splitting Multiple Access in Multi-cell Dense Networks: A Stochastic Geometry Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.11430.pdf)
>  In this paper, the potential benefits of applying the Rate-Splitting Multiple Access (RSMA) in multi-cell dense networks are explored. Using tools of stochastic geometry, the sum-rate of RSMA-enhanced multi-cell dense networks is evaluated mathematically based on a Moment Generating Function (MGF) based framework to prove that RSMA is a general and powerful strategy for multi-antenna downlink systems. Further elaboration of the systematic performance metrics is undertaken by developing analytical expressions for area spectral efficiency and sum-rate in the RSMA-enhanced multi-cell dense networks. Based on the tractable expressions, we then offer an optimization framework for energy efficiency in terms of the number of antennas. Additionally, simulation results are shown to verify the accuracy of our analytical results and provide some insightful insights into system design. Analytically, it has been shown that: 1) the sum-rate of RSMA-enhanced multi-cell dense networks is significantly influenced by the power splitting ratio, and there is a unique value that maximizes the sum-rate; 2) the RSMA-enhanced multi-cell dense networks transmission scheme has superior sum-rate performance compared with Non-Orthogonal Multiple Access (NOMA) and Space-Division Multiple Access (SDMA) in a wide range of power splitting ratio; 3) By increasing the number of antennas and BS density in an RSMA-enhanced multi-cell dense network, the area spectral efficiency can be substantially enhanced; 4) As for energy efficiency, there exists an optimal antenna number for maximizing this performance metric.      
### 92.Resolving degeneracies in Google search via quantum stochastic walks  [ :arrow_down: ](https://arxiv.org/pdf/2207.11429.pdf)
>  The internet is one of the most valuable technologies invented to date. Among them, Google is the most widely used search engine. The PageRank algorithm is the backbone of Google search, ranking web pages according to relevance and recency. We employ quantum stochastic walks (QSW) with the hope of bettering the classical PageRank (CPR) algorithm, which is based on classical continuous time random walks (CTRW). We implement QSW via two schemes: only incoherence and dephasing with incoherence. PageRank using QSW with only incoherence or QSW with dephasing and incoherence best resolves degeneracies that are unresolvable via CPR and with a convergence time comparable to that for CPR, which is generally the minimum. For some networks, the two QSW schemes obtain a convergence time lower than CPR and an almost degeneracy-free ranking compared to CPR.      
### 93.A Dual Accelerated Method for Online Stochastic Distributed Averaging: From Consensus to Decentralized Policy Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2207.11425.pdf)
>  Motivated by decentralized sensing and policy evaluation problems, we consider a particular type of distributed optimization problem that involves averaging several stochastic, online observations on a network. We design a dual-based method for this consensus problem with Polyak--Ruppert averaging and analyze its behavior. We show that this algorithm attains an accelerated deterministic error depending optimally on the condition number of the network, and also that it has order-optimal stochastic error. This improves on the guarantees of state-of-the-art distributed optimization algorithms when specialized to this setting, and yields -- among other things -- corollaries for decentralized policy evaluation. Our proofs rely on explicitly studying the evolution of several relevant linear systems, and may be of independent interest. Numerical experiments are provided, which validate our theoretical results and demonstrate that our approach outperforms existing methods in finite-sample scenarios on several natural network topologies.      
### 94.Modeling and Analysis of a Coupled SIS Bi-Virus Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.11414.pdf)
>  The paper deals with the setting where two viruses (say virus~1 and virus~2) coexist in a population, and they are not necessarily mutually exclusive, in the sense that infection due to one virus does not preclude the possibility of simultaneous infection due to the other. We develop a coupled bi-virus susceptible-infected-susceptible (SIS) model from a 4n-state Markov chain model, where n is the number of agents (i.e., individuals or subpopulation) in the population. We identify a sufficient condition for both viruses to eventually die out, and a sufficient condition for the existence, uniqueness and asymptotic stability of the endemic equilibrium of each virus. We establish a sufficient condition and multiple necessary conditions for local exponential convergence to the boundary equilibrium (i.e., one virus persists, the other one dies out) of each virus. Under mild assumptions on the healing rate, we show that there cannot exist a coexisting equilibrium where for each node there is a nonzero fraction infected only by virus 1; a nonzero fraction infected only by virus 2; but no fraction that is infected by both viruses 1 and 2. Likewise, assuming that healing rates are strictly positive, a coexisting equilibrium where for each node there is a nonzero fraction infected by both viruses 1 and 2, but no fraction is infected only by virus 1 (resp. virus 2) does not exist. Further, we provide necessary conditions for the existence of certain other kinds of coexisting equilibria. Finally, we illustrate our theoretical findings using an extensive set of in-depth simulations.      
### 95.Rayleigh Regression Model for Ground Type Detection in SAR Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2207.11397.pdf)
>  This letter proposes a regression model for nonnegative signals. The proposed regression estimates the mean of Rayleigh distributed signals by a structure which includes a set of regressors and a link function. For the proposed model, we present: (i)~parameter estimation; (ii)~large data record results; and (iii)~a detection technique. In this letter, we present closed-form expressions for the score vector and Fisher information matrix. The proposed model is submitted to extensive Monte Carlo simulations and to measured data. The Monte Carlo simulations are used to evaluate the performance of maximum likelihood estimators. Also, an application is performed comparing the detection results of the proposed model with Gaussian-, Gamma-, and Weibull-based regression models in SAR images.      
### 96.A Supervised Tensor Dimension Reduction-Based Prognostics Model for Applications with Incomplete Imaging Data  [ :arrow_down: ](https://arxiv.org/pdf/2207.11353.pdf)
>  This paper proposes a supervised dimension reduction methodology for tensor data which has two advantages over most image-based prognostic models. First, the model does not require tensor data to be complete which expands its application to incomplete data. Second, it utilizes time-to-failure (TTF) to supervise the extraction of low-dimensional features which makes the extracted features more effective for the subsequent prognostic. Besides, an optimization algorithm is proposed for parameter estimation and closed-form solutions are derived under certain distributions.      
### 97.Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities  [ :arrow_down: ](https://arxiv.org/pdf/2207.11345.pdf)
>  As for other forms of AI, speech recognition has recently been examined with respect to performance disparities across different user cohorts. One approach to achieve fairness in speech recognition is to (1) identify speaker cohorts that suffer from subpar performance and (2) apply fairness mitigation measures targeting the cohorts discovered. In this paper, we report on initial findings with both discovery and mitigation of performance disparities using data from a product-scale AI assistant speech recognition system. We compare cohort discovery based on geographic and demographic information to a more scalable method that groups speakers without human labels, using speaker embedding technology. For fairness mitigation, we find that oversampling of underrepresented cohorts, as well as modeling speaker cohort membership by additional input variables, reduces the gap between top- and bottom-performing cohorts, without deteriorating overall recognition accuracy.      
### 98.Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing  [ :arrow_down: ](https://arxiv.org/pdf/2207.11250.pdf)
>  Single-image haze removal is a long-standing hurdle for computer vision applications. Several works have been focused on transferring advances from image classification, detection, and segmentation to the niche of image dehazing, primarily focusing on contrastive learning and knowledge distillation. However, these approaches prove computationally expensive, raising concern regarding their applicability to on-the-edge use-cases. This work introduces a simple, lightweight, and efficient framework for single-image haze removal, exploiting rich "dark-knowledge" information from a lightweight pre-trained super-resolution model via the notion of heterogeneous knowledge distillation. We designed a feature affinity module to maximize the flow of rich feature semantics from the super-resolution teacher to the student dehazing network. In order to evaluate the efficacy of our proposed framework, its performance as a plug-and-play setup to a baseline model is examined. Our experiments are carried out on the RESIDE-Standard dataset to demonstrate the robustness of our framework to the synthetic and real-world domains. The extensive qualitative and quantitative results provided establish the effectiveness of the framework, achieving gains of upto 15\% (PSNR) while reducing the model size by $\sim$20 times.      
