# ArXiv eess --Wed, 27 Jul 2022
### 1.Joint Proportional Fairness Scheduling Using Iterative Search for mmWave Concurrent Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2207.13065.pdf)
>  Millimeter wave (mmWave) will play a significant role as a 5G candidate in facing the growing demand of enormous data rate in the near future. The conventional mmWave standard, IEEE 802.11ad, considers establishing only one mmWave link in wireless local area network (WLAN) to provide multi Gbps data rate. But, mmWave has a tenuous channel which hinders it from providing such rate. Hence, it's necessary to establish multiple mmWave links simultaneously by deploying a multiple number of mmWave access points (APs) in 5G networks. Unfortunately, applying conventional standard without any modifications for mmWave concurrent transmission impedes mmWave APs from selecting optimum mmWave concurrent links. Because IEEE 802.11ad standard associates the user equipment (UEs) to mmWave APs using the link that has the maximum received power without considering mutual interference between simultaneous links. In this paper, a joint proportional fairness scheduling (JPFS) optimization problem for establishing optimum mmWave concurrent transmission links is formulated. And, to find a solution to this non-polynomial (NP) time problem, we use exhaustive search (ES) scheme. Numerical simulation proves the effectiveness of using the ES scheme to improve the system performance.      
### 2.Plug-and-Play Compressed Sensing: Theoretical Guarantees on Exact and Robust Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2207.13031.pdf)
>  In Plug-and-Play (PnP) algorithms, an off-the-shelf denoiser is used for image regularization. PnP yields state-of-the-art results, but its theoretical aspects are not well understood. This work considers the question: Similar to classical compressed sensing (CS), can we theoretically recover the ground-truth via PnP under suitable conditions on the denoiser and the sensing matrix? One hurdle is that since PnP is an algorithmic framework, its solution need not be the minimizer of some objective function. It was recently shown that a convex regularizer $\Phi$ can be associated with a class of linear denoisers such that PnP amounts to solving a convex problem involving $\Phi$. Motivated by this, we consider the PnP analog of CS: minimize $\Phi(x)$ s.t. $Ax=A\xi$, where $A$ is a $m\times n$ random sensing matrix, $\Phi$ is the regularizer associated with a linear denoiser $W$, and $\xi$ is the ground-truth. We prove that if $A$ is Gaussian and $\xi$ is in the range of $W$, then the minimizer is almost surely $\xi$ if $rank(W)\leq m$, and almost never if $rank(W)&gt; m$. Thus, the range of the PnP denoiser acts as a signal prior, and its dimension marks a sharp transition from failure to success of exact recovery. We extend the result to subgaussian sensing matrices, except that exact recovery holds only with high probability. For noisy measurements $b = A \xi + \eta$, we consider a robust formulation: minimize $\Phi(x)$ s.t. $\|Ax-b\|\leq\delta$. We prove that for an optimal solution $x^*$, with high probability the distortion $\|x^*-\xi\|$ is bounded by $\|\eta\|$ and $\delta$ if the number of measurements is large enough. In particular, we can derive the sample complexity of CS as a function of distortion error and success rate. We discuss the extension of these results to random Fourier measurements, perform numerical experiments, and discuss research directions stemming from this work.      
### 3.Topological Optimized Convolutional Visual Recurrent Network for Brain Tumor Segmentation and Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.13021.pdf)
>  In today's world of health care, brain tumor (BT) detection has become a common occurrence. However, the manual BT classification approach is time-consuming and only available at a few diagnostic centres. So Deep Convolutional Neural Network (DCNN) is introduced in the medical field for making accurate diagnoses and aiding in the patient's treatment before surgery. But these networks have problems such as overfitting and being unable to extract necessary features for classification. To overcome these problems, we developed the TDA-IPH and Convolutional Transfer learning and Visual Recurrent learning with Elephant Herding Optimization hyper-parameter tuning (CTVR-EHO) models for BT segmentation and classification. Initially, the Topological Data Analysis based Improved Persistent Homology (TDA-IPH) is designed to segment the BT image. Then, from the segmented image, features are extracted simultaneously using TL via the AlexNet model and Bidirectional Visual Long Short Term Memory (Bi-VLSTM). Elephant Herding Optimization (EHO) is used to tune the hyper parameters of both networks to get an optimal result. Finally, extracted features are concatenated and classified using the softmax activation layer. The simulation result of this proposed CTVR-EHO and TDA-IPH method is analysed based on some metrics such as precision, accuracy, recall, loss, and F score. When compared to other existing BT segmentation and classification models, the proposed CTVR-EHO and TDA-IPH approaches show high accuracy (99.8%), high recall (99.23%), high precision (99.67%), and high F score (99.59%).      
### 4.On the Stability of Electromechanical Switching Devices: An Analytic Study of Hysteretic Switching Behaviors  [ :arrow_down: ](https://arxiv.org/pdf/2207.12993.pdf)
>  Electromagnetic relays and solenoid actuators are commercial devices that generally exhibit bistable behavior. In fact, this is the reason why they are extensively used to switch between two possible configurations in electrical, pneumatic, or hydraulic circuits, among others. Although the state of the art is extensive on modeling, estimation, and control of these electromechanical systems, there are very few works that focus on analysis aspects. In this paper, we present an equilibrium and stability analysis whose main goal is to provide insight into such bistable behavior. The study is based on a hybrid dynamical model of the system also presented in the paper. This model is used to obtain analytic expressions that relate the physical parameters to the switching conditions. The results are extensively discussed and possible practical applications are also proposed.      
### 5.Vision-Aided Blockage Avoidance in UAV-assisted V2X Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.12991.pdf)
>  The blockage is a key challenge for millimeter wave communication systems, since these systems mainly work on line-of-sight (LOS) links, and the blockage can degrade the system performance significantly. It is recently found that visual information, easily obtained by cameras, can be utilized to extract the location and size information of the environmental objects, which can help to infer the communication parameters, such as blockage status. In this paper, we propose a novel vision-aided handover framework for UAV-assisted V2X system, which leverages the images taken by cameras at the mobile station (MS) to choose the direct link or UAV-assisted link to avoid blockage caused by the vehicles on the road. We propose a deep reinforcement learning algorithm to optimize the handover and UAV trajectory policy in order to improve the long-term throughput. Simulations results demonstrate the effectiveness of using visual information to deal with the blockage issues.      
### 6.Learning Series-Parallel Lookup Tables for Efficient Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.12987.pdf)
>  Lookup table (LUT) has shown its efficacy in low-level vision tasks due to the valuable characteristics of low computational cost and hardware independence. However, recent attempts to address the problem of single image super-resolution (SISR) with lookup tables are highly constrained by the small receptive field size. Besides, their frameworks of single-layer lookup tables limit the extension and generalization capacities of the model. In this paper, we propose a framework of series-parallel lookup tables (SPLUT) to alleviate the above issues and achieve efficient image super-resolution. On the one hand, we cascade multiple lookup tables to enlarge the receptive field of each extracted feature vector. On the other hand, we propose a parallel network which includes two branches of cascaded lookup tables which process different components of the input low-resolution images. By doing so, the two branches collaborate with each other and compensate for the precision loss of discretizing input pixels when establishing lookup tables. Compared to previous lookup table-based methods, our framework has stronger representation abilities with more flexible architectures. Furthermore, we no longer need interpolation methods which introduce redundant computations so that our method can achieve faster inference speed. Extensive experimental results on five popular benchmark datasets show that our method obtains superior SISR performance in a more efficient way. The code is available at <a class="link-external link-https" href="https://github.com/zhjy2016/SPLUT" rel="external noopener nofollow">this https URL</a>.      
### 7.Noise reduction in Laguerre-domain discrete delay estimation  [ :arrow_down: ](https://arxiv.org/pdf/2207.12973.pdf)
>  This paper introduces a stochastic framework for a recently proposed discrete-time delay estimation method in Laguerre-domain, i.e. with the delay block input and output signals being represented by the corresponding Laguerre series. A novel Laguerre domain disturbance model is devised, which allows the involved signals to be square-summable sequences and is suitable in a number of important applications. The relation to two commonly used time-domain disturbance models is clarified. Furthermore, by forming the input signal in a certain way, the signal shape of an additive output disturbance can be estimated and utilized for noise reduction. It is demonstrated that a significant improvement in the delay estimation error is achieved when the noise sequence is correlated. The noise reduction approach is applicable to other Laguerre-domain problems than pure delay estimation.      
### 8.Joint Direction-of-Arrival and Time-of-Arrival Estimation with Ultra-wideband Elliptical Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2207.12925.pdf)
>  This paper presents a general technique for the joint Direction-of-Arrival (DoA) and Time-of-Arrival (ToA) estimation in multipath environments. The proposed ultra-wideband technique is based on phase-mode expansions and the use of nearly frequency-invariant elliptical arrays. New possibilities open with the present approach, as not only elliptical, but also circular and linear arrays can be considered with the same implementation. Systematic selection/rejection of signals-of-interest/signals-not-of-interest in smart wireless environments is possible, unlike with previous approaches based on circular arrays. Concentric elliptical arrays of many sizes and eccentricities can be jointly considered, with the subsequent improvement that entails in DoA and ToA detection. This leads to the realization of pseudo-random array patterns; namely, quasi-arbitrary geometries created from the superposition of multiple elliptical arrays. Some simulation and experimental tests (measurements in an anechoic chamber) are carried out for several frequency bands to check the correct performance of the method. The method is proven to give accurate estimations in all tested scenarios, and to be robust against noise, channel nonidealities, position uncertainty in sensor placement and interferences.      
### 9.Assessment of a cost-effective headphone calibration procedure for soundscape evaluations  [ :arrow_down: ](https://arxiv.org/pdf/2207.12899.pdf)
>  To increase the availability and adoption of the soundscape standard, a low-cost calibration procedure for reproduction of audio stimuli over headphones was proposed as part of the global ``Soundscape Attributes Translation Project'' (SATP) for validating ISO/TS~12913-2:2018 perceived affective quality (PAQ) attribute translations. A previous preliminary study revealed significant deviations from the intended equivalent continuous A-weighted sound pressure levels ($L_{\text{A,eq}}$) using the open-circuit voltage (OCV) calibration procedure. For a more holistic human-centric perspective, the OCV method is further investigated here in terms of psychoacoustic parameters, including relevant exceedance levels to account for temporal effects on the same 27 stimuli from the SATP. Moreover, a within-subjects experiment with 36 participants was conducted to examine the effects of OCV calibration on the PAQ attributes in ISO/TS~12913-2:2018. Bland-Altman analysis of the objective indicators revealed large biases in the OCV method across all weighted sound level and loudness indicators; and roughness indicators at \SI{5}{\%} and \SI{10}{\%} exceedance levels. Significant perceptual differences due to the OCV method were observed in about \SI{20}{\%} of the stimuli, which did not correspond clearly with the biased acoustic indicators. A cautioned interpretation of the objective and perceptual differences due to small and unpaired samples nevertheless provide grounds for further investigation.      
### 10.Multimodal Speech Emotion Recognition using Cross Attention with Aligned Audio and Text  [ :arrow_down: ](https://arxiv.org/pdf/2207.12895.pdf)
>  In this paper, we propose a novel speech emotion recognition model called Cross Attention Network (CAN) that uses aligned audio and text signals as inputs. It is inspired by the fact that humans recognize speech as a combination of simultaneously produced acoustic and textual signals. First, our method segments the audio and the underlying text signals into equal number of steps in an aligned way so that the same time steps of the sequential signals cover the same time span in the signals. Together with this technique, we apply the cross attention to aggregate the sequential information from the aligned signals. In the cross attention, each modality is aggregated independently by applying the global attention mechanism onto each modality. Then, the attention weights of each modality are applied directly to the other modality in a crossed way, so that the CAN gathers the audio and text information from the same time steps based on each modality. In the experiments conducted on the standard IEMOCAP dataset, our model outperforms the state-of-the-art systems by 2.66% and 3.18% relatively in terms of the weighted and unweighted accuracy.      
### 11.Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.12866.pdf)
>  In this article gesture recognition and speech recognition applications are implemented on embedded systems with Tiny Machine Learning (TinyML). It features 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The gesture recognition,provides an innovative approach nonverbal communication. It has wide applications in human-computer interaction and sign language. Here in the implementation of hand gesture recognition, TinyML model is trained and deployed from EdgeImpulse framework for hand gesture recognition and based on the hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out the direction of movement of hand. The Speech is a mode of communication. Speech recognition is a way by which the statements or commands of human speech is understood by the computer which reacts accordingly. The main aim of speech recognition is to achieve communication between man and machine. Here in the implementation of speech recognition, TinyML model is trained and deployed from EdgeImpulse framework for speech recognition and based on the keywords pronounced by human, Arduino Nano 33 BLE device having built-in microphone can make an RGB LED glow like red, green or blue based on keyword pronounced. The results of each application are obtained and listed in the results section and given the analysis upon the results.      
### 12.Modeling mandatory and discretionary lane changes using dynamic interaction networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.12793.pdf)
>  A quantitative understanding of dynamic lane-changing (LC) interaction patterns is indispensable for improving the decision-making of autonomous vehicles, especially in mixed traffic with human-driven vehicles. This paper develops a novel framework combining the hidden Markov model and graph structure to identify the difference in dynamic interaction networks between mandatory lane changes (MLC) and discretionary lane changes (DLC). A hidden Markov model is developed to decompose LC interactions into homogenous segments and reveal the temporal properties of these segments. Then, conditional mutual information is used to quantify the interaction intensity, and the graph structure is used to characterize the connectivity between vehicles. Finally, the critical vehicle in each dynamic interaction network is identified. Based on the LC events extracted from the INTERACTION dataset, the proposed analytical framework is applied to modeling MLC and DLC under congested traffic with levels of service E and F. The results show that there are multiple heterogeneous dynamic interaction network structures in an LC process. A comparison of MLC and DLC demonstrates that MLC are more complex, while DLC are more random. The complexity of MLC is attributed to the intense interaction and frequent transition of the interaction network structure, while the random DLC demonstrate no obvious evolution rules and dominant vehicles in interaction networks. The findings in this study are useful for understanding the connectivity structure between vehicles in LC interactions, and for designing appropriate and well-directed driving decision-making models for autonomous vehicles and advanced driver-assistance systems.      
### 13.A Study on the Use of Edge TPUs for Eye Fundus Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.12770.pdf)
>  Medical image segmentation can be implemented using Deep Learning methods with fast and efficient segmentation networks. Single-board computers (SBCs) are difficult to use to train deep networks due to their memory and processing limitations. Specific hardware such as Google's Edge TPU makes them suitable for real time predictions using complex pre-trained networks. In this work, we study the performance of two SBCs, with and without hardware acceleration for fundus image segmentation, though the conclusions of this study can be applied to the segmentation by deep neural networks of other types of medical images. To test the benefits of hardware acceleration, we use networks and datasets from a previous published work and generalize them by testing with a dataset with ultrasound thyroid images. We measure prediction times in both SBCs and compare them with a cloud based TPU system. The results show the feasibility of Machine Learning accelerated SBCs for optic disc and cup segmentation obtaining times below 25 milliseconds per image using Edge TPUs.      
### 14.Comparison of Deep Learning and Machine Learning Models and Frameworks for Skin Lesion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.12715.pdf)
>  The incidence rate for skin cancer has been steadily increasing throughout the world, leading to it being a serious issue. Diagnosis at an early stage has the potential to drastically reduce the harm caused by the disease, however, the traditional biopsy is a labor-intensive and invasive procedure. In addition, numerous rural communities do not have easy access to hospitals and do not prefer visiting one for what they feel might be a minor issue. Using machine learning and deep learning for skin cancer classification can increase accessibility and reduce the discomforting procedures involved in the traditional lesion detection process. These models can be wrapped in web or mobile apps and serve a greater population. In this paper, two such models are tested on the benchmark HAM10000 dataset of common skin lesions. They are Random Forest with Stratified K-Fold Validation, and MobileNetV2 (throughout the rest of the paper referred to as MobileNet). The MobileNet model was trained separately using both TensorFlow and PyTorch frameworks. A side-by-side comparison of both deep learning and machine learning models and a comparison of the same deep learning model on different frameworks for skin lesion diagnosis in a resource-constrained mobile environment has not been conducted before. The results indicate that each of these models fares better at different classification tasks. For greater overall recall, accuracy, and detection of malignant melanoma, the TensorFlow MobileNet was the better choice. However, for detecting noncancerous skin lesions, the PyTorch MobileNet proved to be better. Random Forest was the better algorithm when it came to having a low computational cost with moderate correctness.      
### 15.Real-Time Phase Contrast MRI to quantify Cerebral arterial flow change during variations breathing  [ :arrow_down: ](https://arxiv.org/pdf/2207.12714.pdf)
>  Cerebral arterial blood flow (CABF) can be investigated in few seconds without any synchronization by Real-Time phase contrast. Significant changes in CABF were found between expiration and inspiration during normal breathing of healthy volunteers. Synopsis (100/100) Real-time phase contrast MRI has been applied to investigate cerebral arterial blood flow (CABF) during normal breathing of healthy volunteers. We developed a novel time-domain analysis method to quantify the effect of normal breathing on several parameters of CABF. We found the existence of a delay between the recorded respiratory signal from the belt sensor and the breathing frequency component present in the reconstructed arterial blood flows. During the expiratory, the mean flow rate of CABF increased by 4.4$\pm$1.7%, stroke volume of CABF increased by 9.8$\pm$3.1% and the duration of the cardiac period of CABF increased by 8.1$\pm$3%.      
### 16.Flow 2.0 -a flexible, scalable, cross-platform post-processing software for realtime phase contrast sequences  [ :arrow_down: ](https://arxiv.org/pdf/2207.12712.pdf)
>  Flow 2.0 is an end-to-end easy-of-use software that allows us to quickly, robustly and accurately perform a batch process real-time phase contrast data and multivariate analysis of the effect of respiration on cerebral fluids circulation. Synopsis (99/100) Real-time phase contrast sequences (RT-PC) have potential value as a scientific and clinical tool in quantifying the effects of respiration on cerebral circulation. To simplify its complicated post-processing process, we developed Flow 2.0 software, which provides a complete post-processing workflow including converting DICOM data, image segmentation, image processing, data extraction, background field correction, antialiasing filter, signal processing and analysis and a novel time-domain method for quantifying the effect of respiration on the cerebral circulation. This end-to-end software allows us to quickly, robustly and accurately perform batch process RT-PC and multivariate analysis of the effects of respiration on cerebral circulation.      
### 17.Enabling Grid-Aware Market Participation of Aggregate Flexible Resources  [ :arrow_down: ](https://arxiv.org/pdf/2207.12685.pdf)
>  Increasing integration of distributed energy resources (DERs) within distribution feeders provides unprecedented flexibility at the distribution-transmission interconnection. With the new FERC 2222 order, DER aggregations are allowed to participate in energy market. To enable market participation, these virtual power plants need to provide their generation cost curves. This paper proposes efficient optimization formulations and solution approaches for the characterization of hourly as well as multi-time-step generation cost curves for a distribution system with high penetration of DERs. Network and DER constraints are taken into account when deriving these cost curves, and they enable active distribution systems to bid into the electricity market. The problems of deriving linear and quadratic cost curves are formulated as robust optimization problems and tractable reformulation/solution algorithm are developed to facilitate efficient calculations. The proposed formulations and solution algorithm are validated on a realistic test feeder with high penetration of flexible resources.      
### 18.Key frames assisted hybrid encoding for photorealistic compressive video sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.12627.pdf)
>  Snapshot compressive imaging (SCI) encodes high-speed scene video into a snapshot measurement and then computationally makes reconstructions, allowing for efficient high-dimensional data acquisition. Numerous algorithms, ranging from regularization-based optimization and deep learning, are being investigated to improve reconstruction quality, but they are still limited by the ill-posed and information-deficient nature of the standard SCI paradigm. To overcome these drawbacks, we propose a new key frames assisted hybrid encoding paradigm for compressive video sensing, termed KH-CVS, that alternatively captures short-exposure key frames without coding and long-exposure encoded compressive frames to jointly reconstruct photorealistic video. With the use of optical flow and spatial warping, a deep convolutional neural network framework is constructed to integrate the benefits of these two types of frames. Extensive experiments on both simulations and real data from the prototype we developed verify the superiority of the proposed method.      
### 19.Sizing Co-located Storage for Uncertain Renewable Energy Sold Through Forward Contracts  [ :arrow_down: ](https://arxiv.org/pdf/2207.12619.pdf)
>  In this paper, we propose a high-level Stochastic steady-state model to analyze the value of co-located energy storage systems for wind power producers that participate in an electricity market through Forward or Day Ahead contracts. In particular, we try to find optimal sizing and contracting and stationary operating policies for profit maximization in the long-run. We obtain a stylized model calibrated to actual wind power production and electricity wholesale price data that allows us to asses the value of storage size and perform sensitivity analysis on key parameters such as contract prices, storage cost and storage efficiency.      
### 20.Simulation-based Probabilistic Risk Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2207.12575.pdf)
>  Simulation-based probabilistic risk assessment (SPRA) is a systematic and comprehensive methodology that has been used and refined over the past few decades to evaluate the risks associated with complex systems. SPRA models are well established for cases with considerable data and system behavior information available. In this regard, multiple statistical and probabilistic tools can be used to provide a valuable assessment of dynamic probabilistic risk levels in different applications. This tutorial presents a comprehensive review of SPRA methodologies. Based on the reviewed literature, SPRA methods can be classified into three categories of dynamic probabilistic logic methods, dynamic stochastic analytical models, and hybrid discrete dynamic event and system simulation models. In this tutorial, the key strengths and weaknesses, and suggestions on ways to address real and perceived shortcomings of available SPRA methods are presented and discussed.      
### 21.Seeing Far in the Dark with Patterned Flash  [ :arrow_down: ](https://arxiv.org/pdf/2207.12570.pdf)
>  Flash illumination is widely used in imaging under low-light environments. However, illumination intensity falls off with propagation distance quadratically, which poses significant challenges for flash imaging at a long distance. We propose a new flash technique, named ``patterned flash'', for flash imaging at a long distance. Patterned flash concentrates optical power into a dot array. Compared with the conventional uniform flash where the signal is overwhelmed by the noise everywhere, patterned flash provides stronger signals at sparsely distributed points across the field of view to ensure the signals at those points stand out from the sensor noise. This enables post-processing to resolve important objects and details. Additionally, the patterned flash projects texture onto the scene, which can be treated as a structured light system for depth perception. Given the novel system, we develop a joint image reconstruction and depth estimation algorithm with a convolutional neural network. We build a hardware prototype and test the proposed flash technique on various scenes. The experimental results demonstrate that our patterned flash has significantly better performance at long distances in low-light environments.      
### 22.Distributed HVDC Emergency Power Control; case study Nordic Power System  [ :arrow_down: ](https://arxiv.org/pdf/2207.12567.pdf)
>  Frequency Containment Reserves might be insufficient to provide an appropriate response in the presence of large disturbances and low inertia scenarios. As a solution, this work assesses the supplementary droop frequency-based Emergency Power Control (EPC) from HVDC interconnections, applied in the detailed Nordic Power System model. EPC distribution and factors that determine the EPC performance of an HVDC link are the focus of interest. The main criteria are the maximum Instantaneous Frequency Deviation and used EPC power. The presented methodology is motivated based on the theoretical observation concerning linearized system representation. However, the assessed and proposed properties of interest, such as provided EPC active and reactive power, their ratio, and energy of total loads and losses in the system due to the EPC, concern highly nonlinear system behavior. Finally, based on the obtained study, remarks on the pragmatical importance of the EPC distribution to the frequency nadir limitation are provided.      
### 23.Cooperative Actor-Critic via TD Error Aggregation  [ :arrow_down: ](https://arxiv.org/pdf/2207.12533.pdf)
>  In decentralized cooperative multi-agent reinforcement learning, agents can aggregate information from one another to learn policies that maximize a team-average objective function. Despite the willingness to cooperate with others, the individual agents may find direct sharing of information about their local state, reward, and value function undesirable due to privacy issues. In this work, we introduce a decentralized actor-critic algorithm with TD error aggregation that does not violate privacy issues and assumes that communication channels are subject to time delays and packet dropouts. The cost we pay for making such weak assumptions is an increased communication burden for every agent as measured by the dimension of the transmitted data. Interestingly, the communication burden is only quadratic in the graph size, which renders the algorithm applicable in large networks. We provide a convergence analysis under diminishing step size to verify that the agents maximize the team-average objective function.      
### 24.Deep learning-based algorithm for assessment of knee osteoarthritis severity in radiographs matches performance of radiologists  [ :arrow_down: ](https://arxiv.org/pdf/2207.12521.pdf)
>  A fully-automated deep learning algorithm matched performance of radiologists in assessment of knee osteoarthritis severity in radiographs using the Kellgren-Lawrence grading system. <br>To develop an automated deep learning-based algorithm that jointly uses Posterior-Anterior (PA) and Lateral (LAT) views of knee radiographs to assess knee osteoarthritis severity according to the Kellgren-Lawrence grading system. <br>We used a dataset of 9739 exams from 2802 patients from Multicenter Osteoarthritis Study (MOST). The dataset was divided into a training set of 2040 patients, a validation set of 259 patients and a test set of 503 patients. A novel deep learning-based method was utilized for assessment of knee OA in two steps: (1) localization of knee joints in the images, (2) classification according to the KL grading system. Our method used both PA and LAT views as the input to the model. The scores generated by the algorithm were compared to the grades provided in the MOST dataset for the entire test set as well as grades provided by 5 radiologists at our institution for a subset of the test set. <br>The model obtained a multi-class accuracy of 71.90% on the entire test set when compared to the ratings provided in the MOST dataset. The quadratic weighted Kappa coefficient for this set was 0.9066. The average quadratic weighted Kappa between all pairs of radiologists from our institution who took a part of study was 0.748. The average quadratic-weighted Kappa between the algorithm and the radiologists at our institution was 0.769. <br>The proposed model performed demonstrated equivalency of KL classification to MSK radiologists, but clearly superior reproducibility. Our model also agreed with radiologists at our institution to the same extent as the radiologists with each other. The algorithm could be used to provide reproducible assessment of knee osteoarthritis severity.      
### 25.Optimality and sustainability of hybrid limit cycles in the pollution control problem with regime shifts  [ :arrow_down: ](https://arxiv.org/pdf/2207.12486.pdf)
>  In this paper, we consider the problem of pollution control in a system that undergoes regular regime shifts. We first show that the optimal policy of pollution abatement is periodic as well, and is described by the unique hybrid limit cycle. We next introduce the notion of an environmentally sustainable solution, and demonstrate that such a policy is the only one that yields the best possible trade-off between steadily achieving profit and ensuring environmental preservation. In contrast to that, the policy that is not environmentally sustainable eventually enters stagnation. To further illustrate our findings, we compare the optimal periodic solution with a myopic one. Interestingly enough, the myopic solution yields higher overall payoff in the short-run, but completely fails in the long-run, while the environmentally sustainable policy yields maximal payoff and preserves the environment over the infinite time interval.      
### 26.Color Coding of Large Value Ranges Applied to Meteorological Data  [ :arrow_down: ](https://arxiv.org/pdf/2207.12399.pdf)
>  This paper presents a novel color scheme designed to address the challenge of visualizing data series with large value ranges, where scale transformation provides limited support. We focus on meteorological data, where the presence of large value ranges is common. We apply our approach to meteorological scatterplots, as one of the most common plots used in this domain area. Our approach leverages the numerical representation of mantissa and exponent of the values to guide the design of novel "nested" color schemes, able to emphasize differences between magnitudes. Our user study evaluates the new designs, the state of the art color scales and representative color schemes used in the analysis of meteorological data: ColorCrafter, Viridis, and Rainbow. We assess accuracy, time and confidence in the context of discrimination (comparison) and interpretation (reading) tasks. Our proposed color scheme significantly outperforms the others in interpretation tasks, while showing comparable performances in discrimination tasks.      
### 27.DeFakePro: Decentralized DeepFake Attacks Detection using ENF Authentication  [ :arrow_down: ](https://arxiv.org/pdf/2207.13070.pdf)
>  Advancements in generative models, like Deepfake allows users to imitate a targeted person and manipulate online interactions. It has been recognized that disinformation may cause disturbance in society and ruin the foundation of trust. This article presents DeFakePro, a decentralized consensus mechanism-based Deepfake detection technique in online video conferencing tools. Leveraging Electrical Network Frequency (ENF), an environmental fingerprint embedded in digital media recording, affords a consensus mechanism design called Proof-of-ENF (PoENF) algorithm. The similarity in ENF signal fluctuations is utilized in the PoENF algorithm to authenticate the media broadcasted in conferencing tools. By utilizing the video conferencing setup with malicious participants to broadcast deep fake video recordings to other participants, the DeFakePro system verifies the authenticity of the incoming media in both audio and video channels.      
### 28.Constant Weight Codes with Gabor Dictionaries and Bayesian Decoding for Massive Random Access  [ :arrow_down: ](https://arxiv.org/pdf/2207.13049.pdf)
>  This paper considers a general framework for massive random access based on sparse superposition coding. We provide guidelines for the code design and propose the use of constant-weight codes in combination with a dictionary design based on Gabor frames. The decoder applies an extension of approximate message passing (AMP) by iteratively exchanging soft information between an AMP module that accounts for the dictionary structure, and a second inference module that utilizes the structure of the involved constant-weight code. We apply the encoding structure to (i) the unsourced random access setting, where all users employ a common dictionary, and (ii) to the "sourced" random access setting with user-specific dictionaries. When applied to a fading scenario, the communication scheme essentially operates non-coherently, as channel state information is required neither at the transmitter nor at the receiver. We observe that in regimes of practical interest, the proposed scheme compares favorably with state-of-the art schemes, in terms of the (per-user) energy-per-bit requirement, as well as the number of active users that can be simultaneously accommodated in the system. Importantly, this is achieved with a considerably smaller size of the transmitted codewords, potentially yielding lower latency and bandwidth occupancy, as well as lower implementation complexity.      
### 29.Realization of High Frequency Bidirectional Transceiver (Bitx) Radio  [ :arrow_down: ](https://arxiv.org/pdf/2207.13046.pdf)
>  In this paper, we realize the transmitter of high frequency (HF) bidirectional transceiver radio. A bidirectional transceiver (Bitx) consists of a transmitter and receiver, in which some common circuits are used together by the transmitter and receiver. In addition, Bitx radio uses a single conversion superheterodyne transmitter, which only shifts frequency once in a signal generation process. The measurement result shows that it works at the frequency of 6.8 MHz with 10 MHz BFO, 3,2 MHz VFO, and 50,3 dBm transmit power. The transmitting frequency is obtained from the difference between BFO and VFO oscillator. Bitx radio has a simple circuit and is designed with low-cost components, and it could be an alternative solution for communication in remote areas.      
### 30.Performance of OFDM System against Different Cyclic Prefix Lengths on Multipath Fading Channels  [ :arrow_down: ](https://arxiv.org/pdf/2207.13045.pdf)
>  Orthogonal Frequency Division Multiplexing (OFDM) is a transmission technique that uses several subcarrier frequencies (multicarrier) that are perpendicular to each other (orthogonal). This OFDM modulation technique effectively eliminates Intersymbol Interference (ISI) on a channel caused by the effects of multipath fading. To overcome this weakness, OFDM uses a guard interval (cyclic prefix) inserted in its transmission. This paper analyses the effect of different cyclic prefix lengths on OFDM performance on multipath channels using Matlab. Here, the cyclic prefix length as the main parameter is varied for the different number of subscribers. Meanwhile, Bit Error Rate (BER) and Signal Noise Ratio (SNR) values obtained at the receiver are used to see the system's performance. Based on our simulation results, the best value of BER is obtained using a cyclic prefix length of 1/4 with a Fast Fourier Transform (FFT) size of 512.      
### 31.Learning Generalizable Latent Representations for Novel Degradations in Super Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.12941.pdf)
>  Typical methods for blind image super-resolution (SR) focus on dealing with unknown degradations by directly estimating them or learning the degradation representations in a latent space. A potential limitation of these methods is that they assume the unknown degradations can be simulated by the integration of various handcrafted degradations (e.g., bicubic downsampling), which is not necessarily true. The real-world degradations can be beyond the simulation scope by the handcrafted degradations, which are referred to as novel degradations. In this work, we propose to learn a latent representation space for degradations, which can be generalized from handcrafted (base) degradations to novel degradations. The obtained representations for a novel degradation in this latent space are then leveraged to generate degraded images consistent with the novel degradation to compose paired training data for SR model. Furthermore, we perform variational inference to match the posterior of degradations in latent representation space with a prior distribution (e.g., Gaussian distribution). Consequently, we are able to sample more high-quality representations for a novel degradation to augment the training data for SR model. We conduct extensive experiments on both synthetic and real-world datasets to validate the effectiveness and advantages of our method for blind super-resolution with novel degradations.      
### 32.Cross-Modality Image Registration using a Training-Time Privileged Third Modality  [ :arrow_down: ](https://arxiv.org/pdf/2207.12901.pdf)
>  In this work, we consider the task of pairwise cross-modality image registration, which may benefit from exploiting additional images available only at training time from an additional modality that is different to those being registered. As an example, we focus on aligning intra-subject multiparametric Magnetic Resonance (mpMR) images, between T2-weighted (T2w) scans and diffusion-weighted scans with high b-value (DWI$_{high-b}$). For the application of localising tumours in mpMR images, diffusion scans with zero b-value (DWI$_{b=0}$) are considered easier to register to T2w due to the availability of corresponding features. We propose a learning from privileged modality algorithm, using a training-only imaging modality DWI$_{b=0}$, to support the challenging multi-modality registration problems. We present experimental results based on 369 sets of 3D multiparametric MRI images from 356 prostate cancer patients and report, with statistical significance, a lowered median target registration error of 4.34 mm, when registering the holdout DWI$_{high-b}$ and T2w image pairs, compared with that of 7.96 mm before registration. Results also show that the proposed learning-based registration networks enabled efficient registration with comparable or better accuracy, compared with a classical iterative algorithm and other tested learning-based methods with/without the additional modality. These compared algorithms also failed to produce any significantly improved alignment between DWI$_{high-b}$ and T2w in this challenging application.      
### 33.Bessel Equivariant Networks for Inversion of Transmission Effects in Multi-Mode Optical Fibres  [ :arrow_down: ](https://arxiv.org/pdf/2207.12849.pdf)
>  We develop a new type of model for solving the task of inverting the transmission effects of multi-mode optical fibres through the construction of an $\mathrm{SO}^{+}(2,1)$-equivariant neural network. This model takes advantage of the of the azimuthal correlations known to exist in fibre speckle patterns and naturally accounts for the difference in spatial arrangement between input and speckle patterns. In addition, we use a second post-processing network to remove circular artifacts, fill gaps, and sharpen the images, which is required due to the nature of optical fibre transmission. This two stage approach allows for the inspection of the predicted images produced by the more robust physically motivated equivariant model, which could be useful in a safety-critical application, or by the output of both models, which produces high quality images. Further, this model can scale to previously unachievable resolutions of imaging with multi-mode optical fibres and is demonstrated on $256 \times 256$ pixel images. This is a result of improving the trainable parameter requirement from $\mathcal{O}(N^4)$ to $\mathcal{O}(m)$, where $N$ is pixel size and $m$ is number of fibre modes. Finally, this model generalises to new images, outside of the set of training data classes, better than previous models.      
### 34.Fixed-Time Convergence for a Class of Nonconvex-Nonconcave Min-Max Problems  [ :arrow_down: ](https://arxiv.org/pdf/2207.12845.pdf)
>  This study develops a fixed-time convergent saddle point dynamical system for solving min-max problems under a relaxation of standard convexity-concavity assumption. In particular, it is shown that by leveraging the dynamical systems viewpoint of an optimization algorithm, accelerated convergence to a saddle point can be obtained. Instead of requiring the objective function to be strongly-convex--strongly-concave (as necessitated for accelerated convergence of several saddle-point algorithms), uniform fixed-time convergence is guaranteed for functions satisfying only the two-sided Polyak-Łojasiewicz (PL) inequality. A large number of practical problems, including the robust least squares estimation, are known to satisfy the two-sided PL inequality. The proposed method achieves arbitrarily fast convergence compared to any other state-of-the-art method with linear or even super-linear convergence, as also corroborated in numerical case studies.      
### 35.Data-aided Active User Detection with False Alarm Correction in Grant-Free Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2207.12830.pdf)
>  In most existing grant-free (GF) studies, the two key tasks, namely active user detection (AUD) and payload data decoding, are handled separately. In this paper, a two-step dataaided AUD scheme is proposed, namely the initial AUD step and the false alarm correction step respectively. To implement the initial AUD step, an embedded low-density-signature (LDS) based preamble pool is constructed. In addition, two message passing algorithm (MPA) based initial estimators are developed. In the false alarm correction step, a redundant factor graph is constructed based on the initial active user set, on which MPA is employed for data decoding. The remaining false detected inactive users will be further recognized by the false alarm corrector with the aid of decoded data symbols. Simulation results reveal that both the data decoding performance and the AUD performance are significantly enhanced by more than 1:5 dB at the target accuracy of 10^3 compared with the traditional compressed sensing (CS) based counterparts      
### 36.Generative Extraction of Audio Classifiers for Speaker Identification  [ :arrow_down: ](https://arxiv.org/pdf/2207.12816.pdf)
>  It is perhaps no longer surprising that machine learning models, especially deep neural networks, are particularly vulnerable to attacks. One such vulnerability that has been well studied is model extraction: a phenomenon in which the attacker attempts to steal a victim's model by training a surrogate model to mimic the decision boundaries of the victim model. Previous works have demonstrated the effectiveness of such an attack and its devastating consequences, but much of this work has been done primarily for image and text processing tasks. Our work is the first attempt to perform model extraction on {\em audio classification models}. We are motivated by an attacker whose goal is to mimic the behavior of the victim's model trained to identify a speaker. This is particularly problematic in security-sensitive domains such as biometric authentication. We find that prior model extraction techniques, where the attacker \textit{naively} uses a proxy dataset to attack a potential victim's model, fail. We therefore propose the use of a generative model to create a sufficiently large and diverse pool of synthetic attack queries. We find that our approach is able to extract a victim's model trained on \texttt{LibriSpeech} using queries synthesized with a proxy dataset based off of \texttt{VoxCeleb}; we achieve a test accuracy of 84.41\% with a budget of 3 million queries.      
### 37.Distinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease  [ :arrow_down: ](https://arxiv.org/pdf/2207.12784.pdf)
>  Chronic obstructive pulmonary disease (COPD) causes lung inflammation and airflow blockage leading to a variety of respiratory symptoms; it is also a leading cause of death and affects millions of individuals around the world. Patients often require treatment and hospitalisation, while no cure is currently available. As COPD predominantly affects the respiratory system, speech and non-linguistic vocalisations present a major avenue for measuring the effect of treatment. In this work, we present results on a new COPD dataset of 20 patients, showing that, by employing personalisation through speaker-level feature normalisation, we can distinguish between pre- and post-treatment speech with an unweighted average recall (UAR) of up to 82\,\% in (nested) leave-one-speaker-out cross-validation. We further identify the most important features and link them to pathological voice properties, thus enabling an auditory interpretation of treatment effects. Monitoring tools based on such approaches may help objectivise the clinical status of COPD patients and facilitate personalised treatment plans.      
### 38.An exhaustive variable selection study for linear models of soundscape emotions: rankings and Gibbs analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.12743.pdf)
>  In the last decade, soundscapes have become one of the most active topics in Acoustics, providing a holistic approach to the acoustic environment, which involves human perception and context. Soundscapes-elicited emotions are central and substantially subtle and unnoticed (compared to speech or music). Currently, soundscape emotion recognition is a very active topic in the literature. We provide an exhaustive variable selection study (i.e., a selection of the soundscapes indicators) to a well-known dataset (emo-soundscapes). We consider linear soundscape emotion models for two soundscapes descriptors: arousal and valence. <br>Several ranking schemes and procedures for selecting the number of variables are applied. We have also performed an alternating optimization scheme for obtaining the best sequences keeping fixed a certain number of features. Furthermore, we have designed a novel technique based on Gibbs sampling, which provides a more complete and clear view of the relevance of each variable. Finally, we have also compared our results with the analysis obtained by the classical methods based on p-values. As a result of our study, we suggest two simple and parsimonious linear models of only 7 and 16 variables (within the 122 possible features) for the two outputs (arousal and valence), respectively. The suggested linear models provide very good and competitive performance, with $R^2&gt;0.86$ and $R^2&gt;0.63$ (values obtained after a cross-validation procedure), respectively.      
### 39.Time Majority Voting, a PC-based EEG Classifier for Non-expert Users  [ :arrow_down: ](https://arxiv.org/pdf/2207.12662.pdf)
>  Using Machine Learning and Deep Learning to predict cognitive tasks from electroencephalography (EEG) signals is a rapidly advancing field in Brain-Computer Interfaces (BCI). In contrast to the fields of computer vision and natural language processing, the data amount of these trials is still rather tiny. Developing a PC-based machine learning technique to increase the participation of non-expert end-users could help solve this data collection issue. We created a novel algorithm for machine learning called Time Majority Voting (TMV). In our experiment, TMV performed better than cutting-edge algorithms. It can operate efficiently on personal computers for classification tasks involving the BCI. These interpretable data also assisted end-users and researchers in comprehending EEG tests better.      
### 40.Can Deep Learning Assist Automatic Identification of Layered Pigments From XRF Data?  [ :arrow_down: ](https://arxiv.org/pdf/2207.12651.pdf)
>  X-ray fluorescence spectroscopy (XRF) plays an important role for elemental analysis in a wide range of scientific fields, especially in cultural heritage. XRF imaging, which uses a raster scan to acquire spectra across artworks, provides the opportunity for spatial analysis of pigment distributions based on their elemental composition. However, conventional XRF-based pigment identification relies on time-consuming elemental mapping by expert interpretations of measured spectra. To reduce the reliance on manual work, recent studies have applied machine learning techniques to cluster similar XRF spectra in data analysis and to identify the most likely pigments. Nevertheless, it is still challenging for automatic pigment identification strategies to directly tackle the complex structure of real paintings, e.g. pigment mixtures and layered pigments. In addition, pixel-wise pigment identification based on XRF imaging remains an obstacle due to the high noise level compared with averaged spectra. Therefore, we developed a deep-learning-based end-to-end pigment identification framework to fully automate the pigment identification process. In particular, it offers high sensitivity to the underlying pigments and to the pigments with a low concentration, therefore enabling satisfying results in mapping the pigments based on single-pixel XRF spectrum. As case studies, we applied our framework to lab-prepared mock-up paintings and two 19th-century paintings: Paul Gauguin's Poèmes Barbares (1896) that contains layered pigments with an underlying painting, and Paul Cezanne's The Bathers (1899-1904). The pigment identification results demonstrated that our model achieved comparable results to the analysis by elemental mapping, suggesting the generalizability and stability of our model.      
### 41.A Learning and Control Perspective for Microfinance  [ :arrow_down: ](https://arxiv.org/pdf/2207.12631.pdf)
>  Microfinance in developing areas such as Africa has been proven to improve the local economy significantly. However, many applicants in developing areas cannot provide adequate information required by the financial institution to make a lending decision. As a result, it is challenging for microfinance institutions to assign credit properly based on conventional policies. In this paper, we formulate the decision-making of microfinance into a rigorous optimization-based framework involving learning and control. We propose an algorithm to explore and learn the optimal policy to approve or reject applicants. We provide the conditions under which the algorithms are guaranteed to converge to an optimal one. The proposed algorithm can naturally deal with missing information and systematically tradeoff multiple objectives such as profit maximization, financial inclusion, social benefits, and economic development. Through extensive simulation of both real and synthetic microfinance datasets, we showed our proposed algorithm is superior to existing benchmarks. To the best of our knowledge, this paper is the first to make a connection between microfinance and control and use control-theoretic tools to optimize the policy with a provable guarantee.      
### 42.Time-invariant prefix-free source coding for MIMO LQG control  [ :arrow_down: ](https://arxiv.org/pdf/2207.12614.pdf)
>  In this work we consider discrete-time multiple-input multiple-output (MIMO) linear-quadratic-Gaussian (LQG) control where the feedback consists of variable length binary codewords. To simplify the decoder architecture, we enforce a strict prefix constraint on the codewords. We develop a data compression architecture that provably achieves a near minimum time-average expected bitrate for a fixed constraint on the LQG performance. The architecture conforms to the strict prefix constraint and does not require time-varying lossless source coding, in contrast to the prior art.      
### 43.Structured input-output analysis of stably stratified plane Couette flow  [ :arrow_down: ](https://arxiv.org/pdf/2207.12605.pdf)
>  We employ a recently introduced structured input-output analysis (SIOA) approach to analyze streamwise and spanwise wavelengths of flow structures in stably stratified plane Couette flow. In the low-Reynolds number ($Re$) low-bulk Richardson number ($Ri_b$) spatially intermittent regime, we demonstrate that SIOA predicts high amplification associated with wavelengths corresponding to the characteristic oblique turbulent bands in this regime. SIOA also identifies quasi-horizontal flow structures resembling the turbulent-laminar layers commonly observed in the high-$Re$ high-$Ri_b$ intermittent regime. An SIOA across a range of $Ri_b$ and $Re$ values suggests that the classical Miles-Howard stability criterion ($Ri_b\leq 1/4$) is associated with a change in the most amplified flow structures when Prandtl number is close to one ($Pr\approx 1$). However, for $Pr\ll 1$, the most amplified flow structures are determined by the product $PrRi_b$. For $Pr\gg 1$, SIOA identifies another quasi-horizontal flow structure that we show is principally associated with density perturbations. We further demonstrate the dominance of this density-associated flow structure in the high $Pr$ limit by constructing analytical scaling arguments for the amplification in terms of $Re$ and $Pr$ under the assumptions of unstratified flow (with $Ri_b=0$) and streamwise invariance.      
### 44.Compiler-Aware Neural Architecture Search for On-Mobile Real-time Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.12577.pdf)
>  Deep learning-based super-resolution (SR) has gained tremendous popularity in recent years because of its high image quality performance and wide application scenarios. However, prior methods typically suffer from large amounts of computations and huge power consumption, causing difficulties for real-time inference, especially on resource-limited platforms such as mobile devices. To mitigate this, we propose a compiler-aware SR neural architecture search (NAS) framework that conducts depth search and per-layer width search with adaptive SR blocks. The inference speed is directly taken into the optimization along with the SR loss to derive SR models with high image quality while satisfying the real-time inference requirement. Instead of measuring the speed on mobile devices at each iteration during the search process, a speed model incorporated with compiler optimizations is leveraged to predict the inference latency of the SR block with various width configurations for faster convergence. With the proposed framework, we achieve real-time SR inference for implementing 720p resolution with competitive SR performance (in terms of PSNR and SSIM) on GPU/DSP of mobile platforms (Samsung Galaxy S21).      
### 45.Inter-Frame Compression for Dynamic Point Cloud Geometry Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.12554.pdf)
>  Efficient point cloud compression is essential for applications like virtual and mixed reality, autonomous driving, and cultural heritage. In this paper, we propose a deep learning-based inter-frame encoding scheme for dynamic point cloud geometry compression. We propose a lossy geometry compression scheme that predicts the latent representation of the current frame using the previous frame by employing a novel prediction network. Our proposed network utilizes sparse convolutions with hierarchical multiscale 3D feature learning to encode the current frame using the previous frame. We employ convolution on target coordinates to map the latent representation of the previous frame to the downsampled coordinates of the current frame to predict the current frame's feature embedding. Our framework transmits the residual of the predicted features and the actual features by compressing them using a learned probabilistic factorized entropy model. At the receiver, the decoder hierarchically reconstructs the current frame by progressively rescaling the feature embedding. We compared our model to the state-of-the-art Video-based Point Cloud Compression (V-PCC) and Geometry-based Point Cloud Compression (G-PCC) schemes standardized by the Moving Picture Experts Group (MPEG). Our method achieves more than 91% BD-Rate Bjontegaard Delta Rate) reduction against G-PCC, more than 62% BD-Rate reduction against V-PCC intra-frame encoding mode, and more than 52% BD-Rate savings against V-PCC P-frame-based inter-frame encoding mode using HEVC.      
### 46.Lyapunov stability tests for linear time-delay systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.12462.pdf)
>  An overview of stability conditions in terms of the Lyapunov matrix for time-delay systems is presented. The main results and proof are presented in details for the case of systems with multiple delays. The state of the art, ongoing research and potential extensions to other classes of delay systems are discussed.      
