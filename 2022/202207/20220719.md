# ArXiv eess --Tue, 19 Jul 2022
### 1.Modulo Sampling of FRI Signals  [ :arrow_down: ](https://arxiv.org/pdf/2207.08774.pdf)
>  The dynamic range of an analog-to-digital converter (ADC) is critical during sampling of analog signals. A modulo operation prior to sampling can be used to enhance the effective dynamic range of the ADC. Further, sampling rate of ADC too plays a crucial role and it is desirable to reduce it. Finite-rate-of-innovation (FRI) signal model, which is ubiquitous in many applications, can be used to reduce the sampling rate. In the context of modulo folding for FRI sampling, existing works operate at a very high sampling rate compared to the rate of innovation (RoI) and require a large number of samples compared to the degrees of freedom (DoF) of the FRI signal. Moreover, these approaches use infinite length filters that are practically infeasible. We consider the FRI sampling problem with a compactly supported kernel under the modulo framework. We derive theoretical guarantees and show that FRI signals could be uniquely identified by sampling above the RoI. The number of samples for identifiability is equal to the DoF. We propose a practical algorithm to estimate the FRI parameters from the modulo samples. We show that the proposed approach has the lowest error in estimating the FRI parameters while operating with the lowest number of samples and sampling rates compared to existing techniques. The results are helpful in designing cost-effective, high-dynamic-range ADCs for FRI signals.      
### 2.Integrated Sensing and Communications with Joint Beam Squint and Beam Split for Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2207.08737.pdf)
>  Integrated sensing and communications (ISAC) has attracted tremendous attention for the future 6G wireless communication systems. To improve the transmission rates and sensing accuracy, massive multi-input multi-output (MIMO) technique is leveraged with large transmission bandwidth. However, the growing size of transmission bandwidth and antenna array results in the beam squint effect, which hampers the communications. Moreover, the time overhead of the traditional sensing algorithm is prohibitive for practical systems. In this paper, instead of alleviating the wideband beam squint effect, we take advantage of joint beam squint and beam split effect and propose a novel user directions sensing method integrated with massive MIMO orthogonal frequency division multiplexing (OFDM) systems. Specifically, with the beam squint effect, the BS utilizes the true-time-delay (TTD) lines to steer the beams of different OFDM subcarriers towards different directions simultaneously. The users feedback the subcarrier frequency with the maximum array gain to the BS. Then, the BS calculates the direction based on the subcarrier frequency feedback. Futhermore, the beam split effect introduced by enlarging the inter-antenna spacing is exploited to expand the sensing range. The proposed sensing method operates over frequency-domain, and the intended sensing range is covered by all the subcarriers simultaneously, which reduces the time overhead of the conventional sensing significantly. Simulation results have demonstrated the effectiveness as well as the superior performance of the proposed ISAC scheme.      
### 3.Implementation of Machine Learning-based DER Local Control Schemes on Measurement Devices for Counteracting Communication Failures  [ :arrow_down: ](https://arxiv.org/pdf/2207.08732.pdf)
>  One of the significant challenges linked with the massive integration of distributed energy resources (DER) in the active distribution grids is the uncertainty it brings along. The grid operation becomes more arduous to avoid voltage or thermal violations. While the Optimal Power Flow (OPF) algorithm is vastly discussed in the literature, little attention has been given to the robustness of such centralised implementation, such as the provision of redundant control solutions during a communication failure. This paper aims to implement a machine learning-based algorithm at each Intelligent Electronic Device (IED) that mimics the centralised OPF used during communication failures using IEC 61850 data models. Under normal circumstances, the IEDs communicate for centralised OPF. In addition, the system is trained offline for all operational conditions and the individual look-up tables linking the actual voltages to the DER setpoints are sent to the respective controllers. The regression models allow for the local reconstruction of the DER setpoints, emulating the overall OPF, in case of a communication failure. In addition to the regression control, the paper also explains an offline learning approach for periodic re-training of the regression models. The implementation is experimentally verified using a Hardware-in-the-loop test setup. The tests showed promising results compared to conventional control strategies during communication failures. When properly trained and coordinated, such an intuitive local control approach for each DER could be very beneficial for the bulk power system. This machine learning-based approach could also replace the existing Q(V) control strategies, to better support the bulk power system.      
### 4.On stabilizing reinforcement learning without Lyapunov functions  [ :arrow_down: ](https://arxiv.org/pdf/2207.08730.pdf)
>  Reinforcement learning remains one of the major directions of the contemporary development of control engineering and machine learning. Nice intuition, flexible settings, ease of application are among the many perks of this methodology. From the standpoint of machine learning, the main strength of a reinforcement learning agent is its ability to ``capture" (learn) the optimal behavior in the given environment. Typically, the agent is built on neural networks and it is their approximation abilities that give rise to the above belief. From the standpoint of control engineering, however, reinforcement learning has serious deficiencies. The most significant one is the lack of stability guarantee of the agent-environment closed loop. A great deal of research was and is being made towards stabilizing reinforcement learning. Speaking of stability, the celebrated Lyapunov theory is the de facto tool. It is thus no wonder that so many techniques of stabilizing reinforcement learning rely on the Lyapunov theory in one way or another. In control theory, there is an intricate connection between a stabilizing controller and a Lyapunov function. Employing such a pair seems thus quite attractive to design stabilizing reinforcement learning. However, computation of a Lyapunov function is generally a cumbersome process. In this note, we show how to construct a stabilizing reinforcement learning agent that does not employ such a function at all. We only assume that a Lyapunov function exists, which is a natural thing to do if the given system (read: environment) is stabilizable, but we do not need to compute one.      
### 5.Graph-based Robust Sequential Localization in Obstructed LOS Situations  [ :arrow_down: ](https://arxiv.org/pdf/2207.08646.pdf)
>  This paper presents a factor graph formulation and particle-based sum-product algorithm (SPA) for robust sequential localization in multipath-prone environments. The proposed algorithm jointly performs data association, sequential estimation of a mobile agent position, and adapts all relevant model parameters. We derive a novel non-uniform false alarm (FA) model that captures the delay and amplitude statistics of the multipath radio channel. This model enables the algorithm to indirectly exploit position-related information contained in the MPCs for the estimation of the agent position. Using simulated and real measurements, we demonstrate that the algorithm can provide high-accuracy position estimates even in fully obstructed line-of-sight (OLOS) situations, significantly outperforming the conventional amplitude-information probabilistic data association (AIPDA) filter. We show that the performance of our algorithm constantly attains the posterior Cramer-Rao lower bound (PCRLB), or even succeeds it, due to the additional information contained in the presented FA model.      
### 6.Enhancing HDR Video Compression through CNN-based Effective Bit Depth Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2207.08634.pdf)
>  It is well known that high dynamic range (HDR) video can provide more immersive visual experiences compared to conventional standard dynamic range content. However, HDR content is typically more challenging to encode due to the increased detail associated with the wider dynamic range. In this paper, we improve HDR compression performance using the effective bit depth adaptation approach (EBDA). This method reduces the effective bit depth of the original video content before encoding and reconstructs the full bit depth using a CNN-based up-sampling method at the decoder. In this work, we modify the MFRNet network architecture to enable multiple frame processing, and the new network, multi-frame MFRNet, has been integrated into the EBDA framework using two Versatile Video Coding (VVC) host codecs: VTM 16.2 and the Fraunhofer Versatile Video Encoder (VVenC 1.4.0). The proposed approach was evaluated under the JVET HDR Common Test Conditions using the Random Access configuration. The results show coding gains over both the original VVC VTM 16.2 and VVenC 1.4.0 (w/o EBDA) on JVET HDR tested sequences, with average bitrate savings of 2.9% (over VTM) and 4.8% (against VVenC) based on the Bjontegaard Delta measurement. The source code of multi-frame MFRNet has been released at <a class="link-external link-https" href="https://github.com/fan-aaron-zhang/MF-MFRNet" rel="external noopener nofollow">this https URL</a>.      
### 7.CACTUSS: Common Anatomical CT-US Space for US examinations  [ :arrow_down: ](https://arxiv.org/pdf/2207.08619.pdf)
>  Abdominal aortic aneurysm (AAA) is a vascular disease in which a section of the aorta enlarges, weakening its walls and potentially rupturing the vessel. Abdominal ultrasound has been utilized for diagnostics, but due to its limited image quality and operator dependency, CT scans are usually required for monitoring and treatment planning. Recently, abdominal CT datasets have been successfully utilized to train deep neural networks for automatic aorta segmentation. Knowledge gathered from this solved task could therefore be leveraged to improve US segmentation for AAA diagnosis and monitoring. To this end, we propose CACTUSS: a common anatomical CT-US space, which acts as a virtual bridge between CT and US modalities to enable automatic AAA screening sonography. CACTUSS makes use of publicly available labelled data to learn to segment based on an intermediary representation that inherits properties from both US and CT. We train a segmentation network in this new representation and employ an additional image-to-image translation network which enables our model to perform on real B-mode images. Quantitative comparisons against fully supervised methods demonstrate the capabilities of CACTUSS in terms of Dice Score and diagnostic metrics, showing that our method also meets the clinical requirements for AAA scanning and diagnosis.      
### 8.Rapid and robust synchronization via weak synaptic coupling  [ :arrow_down: ](https://arxiv.org/pdf/2207.08610.pdf)
>  This paper examines how weak synaptic coupling can achieve rapid synchronization in heterogeneous networks. The assumptions aim at capturing the key mathematical properties that make this possible for biophysical networks. In particular, the combination of nodal excitability and synaptic coupling are shown to be essential to the phenomenon.      
### 9.Learning Correspondency in Frequency Domain by a Latent-Space Similarity Loss for Multispectral Pansharpening  [ :arrow_down: ](https://arxiv.org/pdf/2207.08602.pdf)
>  The process of fuse a high spatial resolution (HR) panchromatic (PAN) image and a low spatial resolution (LR) multispectral (MS) image to obtain an HRMS image is known as pansharpening. With the development of convolutional neural networks, the performance of pansharpening methods has been improved, however, the blurry effects and the spectral distortion still exist in their fusion results due to the insufficiency in details learning and the mismatch between the high-frequency (HF) and low-frequency (LF) components. Therefore, the improvements of spatial details at the premise of reducing spectral distortion is still a challenge. In this paper, we propose a frequency-aware network (FAN) together with a novel latent-space similarity loss to address above mentioned problems. FAN is composed of three modules, where the frequency feature extraction module aims to extract features in the frequency domain with the help of discrete wavelet transform (DWT) layers, and the inverse DWT (IDWT) layers are then utilized in the frequency feature fusion module to reconstruct the features. Finally, the fusion results are obtained through the reconstruction module. In order to learn the correspondency, we also propose a latent-space similarity loss to constrain the LF features derived from PAN and MS branches, so that HF features of PAN can reasonably be used to supplement that of MS. Experimental results on three datasets at both reduced- and full-resolution demonstrate the superiority of the proposed method compared with several state-of-the-art pansharpening models, especially for the fusion at full resolution.      
### 10.Data-driven Self-triggered Control via Trajectory Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2207.08596.pdf)
>  Self-triggered control, a well-documented technique for reducing the communication overhead while ensuring desired system performance, is gaining increasing popularity. However, existing methods for self-triggered control require explicit system models that are assumed perfectly known a priori. An end-to-end control paradigm known as data-driven control learns control laws directly from data, and offers a competing alternative to the routine system identification-then-control method. In this context, the present paper puts forth data-driven self-triggered control schemes for unknown linear systems using data collected offline. Specifically, for output feedback control systems, a data-driven model predictive control (MPC) scheme is proposed, which computes a sequence of control inputs while generating a predicted system trajectory. A data-driven self-triggering law is designed using the predicted trajectory, to determine the next triggering time once a new measurement becomes available. For state feedback control systems, instead of capitalizing on MPC to predict the trajectory, a data-fitting problem using the pre-collected input-state data is solved, whose solution is employed to construct the self-triggering mechanism. Both feasibility and stability are established for the proposed self-triggered controllers, which are validated using numerical examples.      
### 11.Probabilistic 5G Indoor Positioning Proof of Concept with Outlier Rejection  [ :arrow_down: ](https://arxiv.org/pdf/2207.08512.pdf)
>  The continuously increasing bandwidth and antenna aperture available in wireless networks laid the foundation for developing competitive positioning solutions relying on communications standards and hardware. However, poor propagation conditions such as non-line of sight (NLOS) and rich multipath still pose many challenges due to outlier measurements that significantly degrade the positioning performance. In this work, we introduce an iterative positioning method that reweights the time of arrival (ToA) and angle of arrival (AoA) measurements originating from multiple locators in order to efficiently remove outliers. In contrast to existing approaches that typically rely on a single locator to set the time reference for the time difference of arrival (TDoA) measurements corresponding to the remaining locators, and whose measurements may be unreliable, the proposed iterative approach does not rely on a reference locator only. The resulting robust position estimate is then used to initialize a computationally efficient gradient search to perform maximum likelihood position estimation. Our proposal is validated with an experimental setup at 3.75 GHz with 5G numerology in an indoor factory scenario, achieving an error of less than 50 cm in 95% of the measurements. To the best of our knowledge, this paper describes the first proof of concept for 5G-based joint ToA and AoA localization.      
### 12.Auto-Positioning in Radio-based Localization Systems: A Bayesian Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.08503.pdf)
>  The application of radio-based positioning systems is ever increasing. In light of the dissemination of the Internet of Things and location-aware communication systems, the demands on localization architectures and amount of possible use cases steadily increases. While traditional radio-based localization is performed by utilizing stationary nodes, whose positions are absolutely referenced, collaborative auto-positioning methods aim to estimate location information without any a-priori knowledge of the node distribution. The usage of auto-positioning decreases the installation efforts of localization systems and therefore allows their market-wide dissemination. Since observations and position information in this scenario are correlated, the uncertainties of all nodes need to be considered. In this paper we propose a discrete Bayesian method based on a multi-dimensional histogram filter to solve the task of robust auto-positioning, allowing to propagate historical positions and estimated position uncertainties, as well as lowering the demands on observation availability when compared to conventional closed-form approaches. The proposed method is validated utilizing different multipath-, outlier and failure-corrupted ranging measurements in a static environment, where we obtain at least 58% higher positioning accuracy compared to a baseline closed-form auto-positioning approach.      
### 13.Distributed Graph Neural Networks for Optimizing Wireless Networks: Message Passing Over-the-Air  [ :arrow_down: ](https://arxiv.org/pdf/2207.08498.pdf)
>  Distributed power allocation is important for interference-limited wireless networks with dense transceiver pairs. In this paper, we aim to design low signaling overhead distributed power allocation schemes by using graph neural networks (GNNs), which are scalable to the number of wireless links. We first apply the message passing neural network (MPNN), a unified framework of GNN, to solve the problem. We show that the signaling overhead grows quadratically as the network size increases. Inspired from the over-the-air computation (AirComp), we then propose an Air-MPNN framework, where the messages from neighboring nodes are represented by the transmit power of pilots and can be aggregated efficiently by evaluating the total interference power. The signaling overhead of Air-MPNN grows linearly as the network size increases, and we prove that Air-MPNN is permutation invariant. To further reduce the signaling overhead, we propose the Air message passing recurrent neural network (Air-MPRNN), where each node utilizes the graph embedding and local state in the previous frame to update the graph embedding in the current frame. Since existing communication systems send a pilot during each frame, Air-MPRNN can be integrated into the existing standards by adjusting pilot power. Simulation results validate the scalability of the proposed frameworks, and show that they outperform the existing power allocation algorithms in terms of sum-rate for various system parameters.      
### 14.Shallow Water Bathymetry Survey using an Autonomous Surface Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2207.08492.pdf)
>  Accurate and cost effective mapping of water bodies has an enormous significance for environmental understanding and navigation. However, the quantity and quality of information we acquire from such environmental features is limited by various factors, including cost, time, security, and the capabilities of existing data collection techniques. Measurement of water depth is an important part of such mapping, particularly in shallow locations that could provide navigational risk or have important ecological functions. Erosion and deposition at these locations, for example, due to storms and erosion, can cause rapid changes that require repeated measurements. In this paper, we describe a low-cost, resilient, unmanned autonomous surface vehicle for bathymetry data collection using side-scan sonar. We discuss the adaptation of equipment and sensors for the collection of navigation, control, and bathymetry data and also give an overview of the vehicle setup. This autonomous surface vehicle has been used to collect bathymetry from the Powai Lake in Mumbai, India.      
### 15.Neural Distributed Image Compression with Cross-Attention Feature Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2207.08489.pdf)
>  We propose a novel deep neural network (DNN) architecture for compressing an image when a correlated image is available as side information only at the decoder side, a special case of the well-known and heavily studied distributed source coding (DSC) problem. In particular, we consider a pair of stereo images, which have overlapping fields of view, captured by a synchronized and calibrated pair of cameras; and therefore, are highly correlated. We assume that one image of the pair is to be compressed and transmitted, while the other image is available only at the decoder. In the proposed architecture, the encoder maps the input image to a latent space using a DNN, quantizes the latent representation, and compresses it losslessly using entropy coding. The proposed decoder extracts useful information common between the images solely from the available side information, as well as a latent representation of the side information. Then, the latent representations of the two images, one received from the encoder, the other extracted locally, along with the locally generated common information, are fed to the respective decoders of the two images. We employ a cross-attention module (CAM) to align the feature maps obtained in the intermediate layers of the respective decoders of the two images, thus allowing better utilization of the side information. We train and demonstrate the effectiveness of the proposed algorithm on various realistic setups, such as KITTI and Cityscape datasets of stereo image pairs. Our results show that the proposed architecture is capable of exploiting the decoder-only side information in a more efficient manner as it outperforms previous works. We also show that the proposed method is able to provide significant gains even in the case of uncalibrated and unsynchronized camera array use cases.      
### 16.Segmenting white matter hyperintensities on isotropic three-dimensional Fluid Attenuated Inversion Recovery magnetic resonance images: A comparison of Deep learning tools on a Norwegian national imaging database  [ :arrow_down: ](https://arxiv.org/pdf/2207.08467.pdf)
>  Automated segmentation of white matter hyperintensities (WMHs) is an essential step in neuroimaging analysis of Magnetic Resonance Imaging (MRI). Fluid Attenuated Inversion Recovery (FLAIR-weighted) is an MRI contrast that is particularly useful to visualize and quantify WMHs, a hallmark of cerebral small vessel disease and Alzheimer's disease (AD). Clinical MRI protocols migrate to a three-dimensional (3D) FLAIR-weighted acquisition to enable high spatial resolution in all three voxel dimensions. The current study details the deployment of deep learning tools to enable automated WMH segmentation and characterization from 3D FLAIR-weighted images acquired as part of a national AD imaging initiative. <br>Among 642 participants (283 male, mean age: (65.18 +/- 9.33) years) from the DDI study, two in-house networks were trained and validated across five national collection sites. Three models were tested on a held-out subset of the internal data from the 642 participants and an external dataset with 29 cases from an international collaborator. These test sets were evaluated independently. Five established WMH performance metrics were used for comparison against ground truth human-in-the-loop segmentation. <br>Results of the three networks tested, the 3D nnU-Net had the best performance with an average dice similarity coefficient score of 0.78 +/- 0.10, performing better than both the in-house developed 2.5D model and the SOTA Deep Bayesian network. <br>With the increasing use of 3D FLAIR-weighted images in MRI protocols, our results suggest that WMH segmentation models can be trained on 3D data and yield WMH segmentation performance that is comparable to or better than state-of-the-art without the need for including T1-weighted image series.      
### 17.Magnetic Field Based Hand Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2207.08464.pdf)
>  Sensor-based 3D hand tracking is still challenging despite the massive exploration of different sensing modalities in the past decades. This work describes the design, implementation, and evaluation of a novel induced magnetic field-based 3D hand tracking system, aiming to address the shortcomings of existing approaches and supply an alternative solution. This system is composed of a set of transmitters for the magnetic field generation, a receiver for field strength sensing, and the Zigbee units for synchronization. In more detail, the transmitters generate the oscillating magnetic fields with a registered sequence, the receiver senses the strength of the induced magnetic field by a customized three axes coil, which is configured as the LC oscillator with the same oscillating frequency so that an induced current shows up when the receiver is located in the field of the generated magnetic field. Five scenarios are explored to evaluate the performance of the proposed system in hand tracking regarding the transmitters deployment: "in front of a whiteboard", "above a table", "in front of and in a shelf", "in front of the waist and chest", and "around the waist". The true-range multilateration method is used to calculate the coordinates of the hand in 3D space. Compared with the ground truth collected by a commercial ultrasound positioning system, the presented magnetic field-based system shows a robust accuracy of around ten centimeters with the transmitters deployed both off-body and on-body(in front of waist and chest), which indicates the feasibility of the proposed sensing modality in 3D hand tracking.      
### 18.Generalized Analysis and Unified Design of EM Skins  [ :arrow_down: ](https://arxiv.org/pdf/2207.08419.pdf)
>  A generalized formulation is derived for the analysis of the field manipulation properties of electromagnetic skins (EMSs) in the working regimes of interest for wireless communications. Based on such a theoretical framework, a unified method for the design of anomalous-reflecting and focusing EMSs is presented. Representative results, from a wide set of numerical experiments, are reported and validated with full-wave HFSS simulations to give the interested readers some insights on the accuracy, the effectiveness, and the computational efficiency of the proposed analysis/synthesis tools.      
### 19.Multi-head Cascaded Swin Transformers with Attention to k-space Sampling Pattern for Accelerated MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2207.08412.pdf)
>  Global correlations are widely seen in human anatomical structures due to similarity across tissues and bones. These correlations are reflected in magnetic resonance imaging (MRI) scans as a result of close-range proton density and T1/T2 parameter. Furthermore, to achieve accelerated MRI, k-space data are undersampled which causes global aliasing artifacts. Convolutional neural network (CNN) models are widely utilized for accelerated MRI reconstruction, but those models are limited in capturing global correlations due to the intrinsic locality of the convolution operation. The self-attention-based transformer models are capable of capturing global correlations among image features, however, the current contributions of transformer models for MRI reconstruction are minute. The existing contributions mostly provide CNN-transformer hybrid solutions and rarely leverage the physics of MRI. In this paper, we propose a physics-based stand-alone (convolution free) transformer model titled, the Multi-head Cascaded Swin Transformers (McSTRA) for accelerated MRI reconstruction. McSTRA combines several interconnected MRI physics-related concepts with the transformer networks: it exploits global MR features via the shifted window self-attention mechanism; it extracts MR features belonging to different spectral components separately using a multi-head setup; it iterates between intermediate de-aliasing and k-space correction via a cascaded network with data consistency in k-space and intermediate loss computations; furthermore, we propose a novel positional embedding generation mechanism to guide self-attention utilizing the point spread function corresponding to the undersampling mask. Our model significantly outperforms state-of-the-art MRI reconstruction methods both visually and quantitatively while depicting improved resolution and removal of aliasing artifacts.      
### 20.ORB-based SLAM accelerator on SoC FPGA  [ :arrow_down: ](https://arxiv.org/pdf/2207.08405.pdf)
>  Simultaneous Localization and Mapping (SLAM) is one of the main components of autonomous navigation systems. With the increase in popularity of drones, autonomous navigation on low-power systems is seeing widespread application. Most SLAM algorithms are computationally intensive and struggle to run in real-time on embedded devices with reasonable accuracy. ORB-SLAM is an open-sourced feature-based SLAM that achieves high accuracy with reduced computational complexity. We propose an SoC based ORB-SLAM system that accelerates the computationally intensive visual feature extraction and matching on hardware. Our FPGA system based on a Zynq-family SoC runs 8.5x, 1.55x and 1.35x faster compared to an ARM CPU, Intel Desktop CPU, and a state-of-the-art FPGA system respectively, while averaging a 2x improvement in accuracy compared to prior work on FPGA.      
### 21.GLEAM: Greedy Learning for Large-Scale Accelerated MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2207.08393.pdf)
>  Unrolled neural networks have recently achieved state-of-the-art accelerated MRI reconstruction. These networks unroll iterative optimization algorithms by alternating between physics-based consistency and neural-network based regularization. However, they require several iterations of a large neural network to handle high-dimensional imaging tasks such as 3D MRI. This limits traditional training algorithms based on backpropagation due to prohibitively large memory and compute requirements for calculating gradients and storing intermediate activations. To address this challenge, we propose Greedy LEarning for Accelerated MRI (GLEAM) reconstruction, an efficient training strategy for high-dimensional imaging settings. GLEAM splits the end-to-end network into decoupled network modules. Each module is optimized in a greedy manner with decoupled gradient updates, reducing the memory footprint during training. We show that the decoupled gradient updates can be performed in parallel on multiple graphical processing units (GPUs) to further reduce training time. We present experiments with 2D and 3D datasets including multi-coil knee, brain, and dynamic cardiac cine MRI. We observe that: i) GLEAM generalizes as well as state-of-the-art memory-efficient baselines such as gradient checkpointing and invertible networks with the same memory footprint, but with 1.3x faster training; ii) for the same memory footprint, GLEAM yields 1.1dB PSNR gain in 2D and 1.8 dB in 3D over end-to-end baselines.      
### 22.Modeling and Control of Multi-Energy Dynamical Systems: Hidden Paths to Decarbonization  [ :arrow_down: ](https://arxiv.org/pdf/2207.08370.pdf)
>  This paper points out some key drawbacks of today's modeling and control underlying hierarchical electric power system operations and planning as the hidden roadblocks on the way to decarbonization. We suggest that these can be overcome by enhancing today's information exchange and control. This can be done by revealing and utilising inherent structure-preserving features of complex physical systems, and, based on this, by establishing multi-layered energy modeling. Each module (component, control area, non-utility-owned entities) can be characterized in terms of its interaction variable, and higher level models can be used to understand the interaction dynamics between different modules. Once the structure is understood, we propose nonlinear energy control for these modules which supports feed-forward self-adaptation to ensure feasible interconnected system. Based on these technology agnostic structures it becomes possible to expand today's Balancing Authorities (BA) to multi-layered interactive intelligent Balancing Authorities (iBAs) and to introduce protocols for flexible utilization of diverse technologies over broad ranges of temporal and spatial conditions.      
### 23.Optimization of stochastic switching buffer network via DC programming  [ :arrow_down: ](https://arxiv.org/pdf/2207.08362.pdf)
>  This letter deals with the optimization problems of stochastic switching buffer networks, where the switching law is governed by Markov process. The dynamical buffer network is introduced, and its application in modeling the car-sharing network is also presented. To address the nonconvexity for getting a solution as close-to-the-global-optimal as possible of the optimization problem, we adopt a succinct but effective nonconvex optimization method called \emph{ DC (difference of convex functions) programming}. By resorting to the log-log convexity of a class of nonlinear functions called posynomials, the optimization problems can be reduced to DC programming problems. Finally, we verify the effectiveness of our results by simulation experiments.      
### 24.Quantized Consensus under Data-Rate Constraints and DoS Attacks: A Zooming-In and Holding Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.08332.pdf)
>  This paper is concerned with the quantized consensus problem for uncertain nonlinear multi-agent systems under data-rate constraints and Denial-of-Service (DoS) attacks. The agents are modeled in strict-feedback form with unknown nonlinear dynamics and external disturbance. Extended state observers (ESOs) are leveraged to estimate agents' total uncertainties along with their states. To mitigate the effects of DoS attacks, a novel dynamic quantization with zooming-in and holding capabilities is proposed. The idea is to zoom-in and hold the variable to be quantized if the system is in the absence and presence of DoS attacks, respectively. The control protocol is given in terms of the outputs of the ESOs and the dynamic-quantization-based encoders and decoders. We show that, for a connected undirected network, the developed control protocol is capable of handling any DoS attacks inducing bounded consecutive packet losses with merely 3-level quantization. The application of the zooming-in and holding approach to known linear multi-agent systems is also discussed.      
### 25.Bayesian Quickest Change Detection of an Intruder in Acknowledgments for Private Remote State Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2207.08329.pdf)
>  For geographically separated cyber-physical systems, state estimation at a remote monitoring or control site is important to ensure stability and reliability of the system. Often for safety or commercial reasons it is necessary to ensure confidentiality of the process state and control information. A current topic of interest is the private transmission of confidential state information. Many transmission encoding schemes rely on acknowledgments, which may be susceptible to interference from an adversary. We consider a stealthy intruder that selectively blocks acknowledgments allowing an eavesdropper to obtain a reliable state estimate defeating an encoding scheme. We utilize Bayesian Quickest Change Detection techniques to quickly detect online the presence of an intruder at both the remote transmitter and receiver.      
### 26.Improving spatial cues for hearables using a parameterized binaural CDR estimator  [ :arrow_down: ](https://arxiv.org/pdf/2207.08314.pdf)
>  We investigate a speech enhancement method based on the binaural coherence-to-diffuse power ratio (CDR), which preserves auditory spatial cues for maskers and a broadside target. Conventional CDR estimators typically rely on a mathematical coherence model of the desired signal and/or diffuse noise field in their formulation, which may influence their accuracy in natural environments. This work proposes a new robust and parameterized directional binaural CDR estimator. The estimator is calculated in the time-frequency domain and is based on a geometrical interpretation of the spatial coherence function between the binaural microphone signals. The binaural performance of the new CDR estimator is compared with three state-of-the-art CDR estimators in cocktail-party-like environments and has shown improvements in terms of several objective speech quality metrics such as PESQ and SRMR. We also discuss the benefits of the parameterizable CDR estimator for varying sound environments and briefly reflect on several informal subjective evaluations using a low-latency real-time framework.      
### 27.A Novel Composite Resilience Indicator for Decentralized Infrastructure Systems (CRI-DS)  [ :arrow_down: ](https://arxiv.org/pdf/2207.08303.pdf)
>  Resilience is a key driver for planning adaptation strategies to mitigate risks due to both natural and anthropogenic hazards. The effectiveness of a resilience-driven decision-making strategy for adapting systems against stressors depends on how resilience is mapped to decision variables. This requires a functional resilience metric, without which it would not be possible to identify the priority needs for improvement, assess changes, or show improvement in post-adaptation resilience. In this paper, we aim to contribute to existing methodologies by proposing a novel model-based resilience assessment strategy while building on the notion of composite resilience indicators. Our proposed indicator is functional, reproducible, and is mapped to adaptation decisions. We propose a framework for integrating the developed resilience functional form into an adaptation decision-making model. To illustrate our approach, we use a case study on assessing resilience of On-Site Wastewater Treatment and Disposal Systems (OSTDS) exposed to risks due to sea-level rise.      
### 28.Non-Parametric Neuro-Adaptive Formation Control  [ :arrow_down: ](https://arxiv.org/pdf/2207.08288.pdf)
>  We develop a learning-based algorithm for the distributed formation control of networked multi-agent systems governed by unknown, nonlinear dynamics. Most existing algorithms either assume certain parametric forms for the unknown dynamic terms or resort to unnecessarily large control inputs in order to provide theoretical guarantees. The proposed algorithm avoids these drawbacks by integrating neural network-based learning with adaptive control in a two-step procedure. In the first step of the algorithm, each agent learns a controller, represented as a neural network, using training data that correspond to a collection of formation tasks and agent parameters. These parameters and tasks are derived by varying the nominal agent parameters and a user-defined formation task to be achieved, respectively. In the second step of the algorithm, each agent incorporates the trained neural network into an online and adaptive control policy in such a way that the behavior of the multi-agent closed-loop system satisfies the user-defined formation task. Both the learning phase and the adaptive control policy are distributed, in the sense that each agent computes its own actions using only local information from its neighboring agents. The proposed algorithm does not use any a priori information on the agents' unknown dynamic terms or any approximation schemes. We provide formal theoretical guarantees on the achievement of the formation task.      
### 29.MLP-GAN for Brain Vessel Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.08265.pdf)
>  Brain vessel image segmentation can be used as a promising biomarker for better prevention and treatment of different diseases. One successful approach is to consider the segmentation as an image-to-image translation task and perform a conditional Generative Adversarial Network (cGAN) to learn a transformation between two distributions. In this paper, we present a novel multi-view approach, MLP-GAN, which splits a 3D volumetric brain vessel image into three different dimensional 2D images (i.e., sagittal, coronal, axial) and then feed them into three different 2D cGANs. The proposed MLP-GAN not only alleviates the memory issue which exists in the original 3D neural networks but also retains 3D spatial information. Specifically, we utilize U-Net as the backbone for our generator and redesign the pattern of skip connection integrated with the MLP-Mixer which has attracted lots of attention recently. Our model obtains the ability to capture cross-patch information to learn global information with the MLP-Mixer. Extensive experiments are performed on the public brain vessel dataset that show our MLP-GAN outperforms other state-of-the-art methods. We release our code at <a class="link-external link-https" href="https://github.com/bxie9/MLP-GAN" rel="external noopener nofollow">this https URL</a>      
### 30.Finite Time Privacy Preserving Quantized Average Consensus with Transmission Stopping  [ :arrow_down: ](https://arxiv.org/pdf/2207.08244.pdf)
>  Due to their flexibility, battery powered or energy-harvesting wireless networks are employed in diverse applications. Securing data transmissions between wireless devises is of critical importance in order to avoid privacy-sensitive user data leakage. In this paper, we focus on the scenario where some nodes are curious (but not malicious) and try to identify the initial states of one (or multiple) other nodes, while some nodes aim to preserve the privacy of their initial states from the curious nodes. We present a privacy preserving finite transmission event-triggered quantized average consensus algorithm. Its operation is suitable for battery-powered or energy-harvesting wireless network since it guarantees (i) efficient (quantized) communication, and (ii) transmission ceasing (which allows preservation of available energy). Furthermore, we present topological conditions under which the proposed algorithm allows nodes to preserve their privacy. We conclude with a comparison of our algorithm against other algorithms in the existing literature.      
### 31.Robust Action Governor for Uncertain Piecewise Affine Systems with Non-convex Constraints and Safe Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.08240.pdf)
>  The action governor is an add-on scheme to a nominal control loop that monitors and adjusts the control actions to enforce safety specifications expressed as pointwise-in-time state and control constraints. In this paper, we introduce the Robust Action Governor (RAG) for systems the dynamics of which can be represented using discrete-time Piecewise Affine (PWA) models with both parametric and additive uncertainties and subject to non-convex constraints. We develop the theoretical properties and computational approaches for the RAG. After that, we introduce the use of the RAG for realizing safe Reinforcement Learning (RL), i.e., ensuring all-time constraint satisfaction during online RL exploration-and-exploitation process. This development enables safe real-time evolution of the control policy and adaptation to changes in the operating environment and system parameters (due to aging, damage, etc.). We illustrate the effectiveness of the RAG in constraint enforcement and safe RL using the RAG by considering their applications to a soft-landing problem of a mass-spring-damper system.      
### 32.Optimal Database Allocation in Finite Time with Efficient Communication and Transmission Stopping over Dynamic Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.08236.pdf)
>  In this paper, we focus on the problem of data sharing over a wireless computer network (i.e., a wireless grid). Given a set of available data, we present a distributed algorithm which operates over a dynamically changing network, and allows each node to calculate the optimal allocation of data in a finite number of time steps. We show that our proposed algorithm (i) converges to the optimal solution in finite time with very high probability, and (ii) once the optimal solution is reached, each node is able to cease transmissions without needing knowledge of a global parameter such as the network diameter. Furthermore, our algorithm (i) operates exclusively with quantized values (i.e., each node processes and transmits quantized information), (ii) relies on event-driven updates, and (iii) calculates the optimal solution in the form of a quantized fraction which avoids errors due to quantization. Finally, we demonstrate the operation, performance, and potential advantages of our algorithm over random dynamic networks.      
### 33.Distributed Finite Time k-means Clustering with Quantized Communucation and Transmission Stopping  [ :arrow_down: ](https://arxiv.org/pdf/2207.08232.pdf)
>  In this paper, we present a distributed algorithm which implements the $k$-means algorithm in a distributed fashion for multi-agent systems with directed communication links. The goal of $k$-means is to partition the network's agents in mutually exclusive sets (groups) such that agents in the same set have (and possibly share) similar information and are able to calculate a representative value for their group.During the operation of our distributed algorithm, each node (i) transmits quantized values in an event-driven fashion, and (ii) exhibits distributed stopping capabilities. Transmitting quantized values leads to more efficient usage of the available bandwidth and reduces the communication bottleneck. Also, in order to preserve available resources, nodes are able to distributively determine whether they can terminate the operation of the proposed algorithm. We characterize the properties of the proposed distributed algorithm and show that its execution (on any static and strongly connected digraph) will partition all agents to mutually exclusive clusters in finite time. We conclude with examples that illustrate the operation, performance, and potential advantages of the proposed algorithm.      
### 34.Unsupervised Medical Image Translation with Adversarial Diffusion Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.08208.pdf)
>  Imputation of missing images via source-to-target modality translation can facilitate downstream tasks in medical imaging. A pervasive approach for synthesizing target images involves one-shot mapping through generative adversarial networks (GAN). Yet, GAN models that implicitly characterize the image distribution can suffer from limited sample fidelity and diversity. Here, we propose a novel method based on adversarial diffusion modeling, SynDiff, for improved reliability in medical image synthesis. To capture a direct correlate of the image distribution, SynDiff leverages a conditional diffusion process to progressively map noise and source images onto the target image. For fast and accurate image sampling during inference, large diffusion steps are coupled with adversarial projections in the reverse diffusion direction. To enable training on unpaired datasets, a cycle-consistent architecture is devised with two coupled diffusion processes to synthesize the target given source and the source given target. Extensive assessments are reported on the utility of SynDiff against competing GAN and diffusion models in multi-contrast MRI and MRI-CT translation. Our demonstrations indicate that SynDiff offers superior performance against competing baselines both qualitatively and quantitatively.      
### 35.Expectation-Maximization Based Defense Mechanism for Distributed Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2207.08194.pdf)
>  Controlling large-scale systems sometimes requires decentralized computation. Communication among agents is crucial to achieving consensus and optimal global behavior. These negotiation mechanisms are sensitive to attacks on those exchanges. This paper proposes an algorithm based on Expectation Maximization to mitigate the effects of attacks in a resource allocation based distributed model predictive control. The performance is assessed through an academic example of the temperature control of multiple rooms under input power constraints.      
### 36.Locational Aspect of Fast Frequency Reserves in Low-Inertia Systems -- Control Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.08188.pdf)
>  This paper evaluates the frequency performance of an AC system when primary frequency response is provided by inverter-based resources located at remote-areas. Due to potentially larger wave propagation constants over longer lines, fast active power response from inverter based resources may have a negative impact on the system frequency response. Within this context, this paper presents a control performance analysis is presented in order to identify limitations for improving the frequency stability when inverter-based resources in remote locations use local frequency measurements. Our results suggest that there exists a trafeoff between disturbance rejection and stability robustness when allocating primary frequency control. In particular, fast frequency control can have a negative impact on the damping ratio of poorly damped electromechanical modes.      
### 37.Mapping Disruption Sources in the Power Grid and Implications for Resilience  [ :arrow_down: ](https://arxiv.org/pdf/2207.08146.pdf)
>  Developing models and metrics that can address resilience against disruptions is vital to ensure power grid reliability and that adequate recovery and adaptation mechanisms are in place. In this paper, we propose a novel disruption mapping approach and apply it to the publicly available U.S. Department of Energy DOE-417 Electric Emergency and Disturbance Report to holistically analyze the origin of anomalous events and their propagation through the cyber, physical and human domains. We show that capturing the disruption process onset has implications for quantifying, mitigating, and reporting power grid resilience.      
### 38.Multi-channel target speech enhancement based on ERB-scaled spatial coherence features  [ :arrow_down: ](https://arxiv.org/pdf/2207.08126.pdf)
>  Recently, speech enhancement technologies that are based on deep learning have received considerable research attention. If the spatial information in microphone signals is exploited, microphone arrays can be advantageous under some adverse acoustic conditions compared with single-microphone systems. However, multichannel speech enhancement is often performed in the short-time Fourier transform (STFT) domain, which renders the enhancement approach computationally expensive. To remedy this problem, we propose a novel equivalent rectangular bandwidth (ERB)-scaled spatial coherence feature that is dependent on the target speaker activity between two ERB bands. Experiments conducted using a four-microphone array in a reverberant environment, which involved speech interference, demonstrated the efficacy of the proposed system. This study also demonstrated that a network that was trained with the ERB-scaled spatial feature was robust against variations in the geometry and number of the microphones in the array.      
### 39.Latency Minimization for mmWave D2D Mobile Edge Computing Systems: Joint Task Allocation and Hybrid Beamforming Design  [ :arrow_down: ](https://arxiv.org/pdf/2207.08123.pdf)
>  Mobile edge computing (MEC) and millimeter wave (mmWave) communications are capable of significantly reducing the network's delay and enhancing its capacity. In this paper we investigate a mmWave and device-to-device (D2D) assisted MEC system, in which user A carries out some computational tasks and shares the results with user B with the aid of a base station (BS). We propose a novel two-timescale joint hybrid beamforming and task allocation algorithm to reduce the system latency whilst cut down the required signaling overhead. Specifically, the high-dimensional analog beamforming matrices are updated in a frame-based manner based on the channel state information (CSI) samples, where each frame consists of a number of time slots, while the low-dimensional digital beamforming matrices and the offloading ratio are optimized more frequently relied on the low-dimensional effective channel matrices in each time slot. A stochastic successive convex approximation (SSCA) based algorithm is developed to design the long-term analog beamforming matrices. As for the short-term variables, the digital beamforming matrices are optimized relying on the innovative penalty-concave convex procedure (penalty-CCCP) for handling the mmWave non-linear transmit power constraint, and the offloading ratio can be obtained via the derived closed-form solution. Simulation results verify the effectiveness of the proposed algorithm by comparing the benchmarks.      
### 40.FloLPIPS: A Bespoke Video Quality Metric for Frame Interpoation  [ :arrow_down: ](https://arxiv.org/pdf/2207.08119.pdf)
>  Video frame interpolation (VFI) serves as a useful tool for many video processing applications. Recently, it has also been applied in the video compression domain for enhancing both conventional video codecs and learning-based compression architectures. While there has been an increased focus on the development of enhanced frame interpolation algorithms in recent years, the perceptual quality assessment of interpolated content remains an open field of research. In this paper, we present a bespoke full reference video quality metric for VFI, FloLPIPS, that builds on the popular perceptual image quality metric, LPIPS, which captures the perceptual degradation in extracted image feature space. In order to enhance the performance of LPIPS for evaluating interpolated content, we re-designed its spatial feature aggregation step by using the temporal distortion (through comparing optical flows) to weight the feature difference maps. Evaluated on the BVI-VFI database, which contains 180 test sequences with various frame interpolation artefacts, FloLPIPS shows superior correlation performance (with statistical significance) with subjective ground truth over 12 popular quality assessors. To facilitate further research in VFI quality assessment, our code is publicly available at <a class="link-external link-https" href="https://danielism97.github.io/FloLPIPS" rel="external noopener nofollow">this https URL</a>.      
### 41.Accelerating Magnetic Resonance Parametric Mapping Using Simultaneously Spatial Patch-based and Parametric Group-based Low-rank Tensors (SMART)  [ :arrow_down: ](https://arxiv.org/pdf/2207.08117.pdf)
>  Quantitative magnetic resonance (MR) parametric mapping is a promising approach for characterizing intrinsic tissue-dependent information. However, long scan time significantly hinders its widespread applications. Recently, low-rank tensor has been employed and demonstrated good performance in accelerating MR parametricmapping. In this study, we propose a novel method that uses spatial patch-based and parametric group-based low rank tensors simultaneously (SMART) to reconstruct images from highly undersampled k-space data. The spatial patch-based low-rank tensor exploits the high local and nonlocal redundancies and similarities between the contrast images in parametric mapping. The parametric group based low-rank tensor, which integrates similar exponential behavior of the image signals, is jointly used to enforce the multidimensional low-rankness in the reconstruction process. In vivo brain datasets were used to demonstrate the validity of the proposed method. Experimental results have demonstrated that the proposed method achieves 11.7-fold and 13.21-fold accelerations in two-dimensional and three-dimensional acquisitions, respectively, with more accurate reconstructed images and maps than several state-of-the-art methods. Prospective reconstruction results further demonstrate the capability of the SMART method in accelerating MR quantitative imaging.      
### 42.BCS-Net: Boundary, Context and Semantic for Automatic COVID-19 Lung Infection Segmentation from CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2207.08114.pdf)
>  The spread of COVID-19 has brought a huge disaster to the world, and the automatic segmentation of infection regions can help doctors to make diagnosis quickly and reduce workload. However, there are several challenges for the accurate and complete segmentation, such as the scattered infection area distribution, complex background noises, and blurred segmentation boundaries. To this end, in this paper, we propose a novel network for automatic COVID-19 lung infection segmentation from CT images, named BCS-Net, which considers the boundary, context, and semantic attributes. The BCS-Net follows an encoder-decoder architecture, and more designs focus on the decoder stage that includes three progressively Boundary-Context-Semantic Reconstruction (BCSR) blocks. In each BCSR block, the attention-guided global context (AGGC) module is designed to learn the most valuable encoder features for decoder by highlighting the important spatial and boundary locations and modeling the global context dependence. Besides, a semantic guidance (SG) unit generates the semantic guidance map to refine the decoder features by aggregating multi-scale high-level features at the intermediate resolution. Extensive experiments demonstrate that our proposed framework outperforms the existing competitors both qualitatively and quantitatively.      
### 43.Robust Vehicle Positioning based on Multi-Epoch and Multi-Antenna TOAs in Harsh Environments  [ :arrow_down: ](https://arxiv.org/pdf/2207.08049.pdf)
>  For radio-based time-of-arrival (TOA) positioning systems applied in harsh environments, obstacles in the surroundings and on the vehicle itself will block the signals from the anchors, reduce the number of available TOA measurements and thus degrade the localization performance. Conventional multi-antenna positioning technique requires a good initialization to avoid local minima, and suffers from location ambiguity due to insufficient number of TOA measurements and/or poor geometry of anchors at a single epoch. A new initialization method based on semidefinite programming (SDP), namely MEMA-SDP, is first designed to address the initialization problem of the MEMA-TOA method. Then, an iterative refinement step is developed to obtain the optimal positioning result based on the MEMA-SDP initialization. We derive the Cramer-Rao lower bound (CRLB) to analyze the accuracy of the new MEMA-TOA method theoretically, and show its superior positioning performance over the conventional single-epoch and multi-antenna (SEMA) localization method. Simulation results in harsh environments demonstrate that i) the new MEMA-SDP provides an initial estimation that is close to the real location, and empirically guarantees the global optimality of the final refined positioning solution, and ii) compared with the conventional SEMA method, the new MEMA-TOA method has higher positioning accuracy without location ambiguity, consistent with the theoretical analysis.      
### 44.Single MR Image Super-Resolution using Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.08036.pdf)
>  Spatial resolution of medical images can be improved using super-resolution methods. Real Enhanced Super Resolution Generative Adversarial Network (Real-ESRGAN) is one of the recent effective approaches utilized to produce higher resolution images, given input images of lower resolution. In this paper, we apply this method to enhance the spatial resolution of 2D MR images. In our proposed approach, we slightly modify the structure of the Real-ESRGAN to train 2D Magnetic Resonance images (MRI) taken from the Brain Tumor Segmentation Challenge (BraTS) 2018 dataset. The obtained results are validated qualitatively and quantitatively by computing SSIM (Structural Similarity Index Measure), NRMSE (Normalized Root Mean Square Error), MAE (Mean Absolute Error), and VIF (Visual Information Fidelity) values.      
### 45.Stability analysis and stabilization of systems with hyperexponential rates  [ :arrow_down: ](https://arxiv.org/pdf/2207.08033.pdf)
>  Hyperexponential stability is investigated for dynamical systems with the use of both, explicit and implicit, Lyapunov function methods. A nonlinear hyperexponential control is designed for stabilizing linear systems. The tuning procedure is formalized in LMI form. Through numeric experiments, it is observed that the proposed hyperexponential control is less sensitive with respect to noises and discretization errors than its finite-time analog. It also demonstrates better performance in the presence of delays as well. Theoretical results are supported by numerical simulations.      
### 46.Analysis of liver cancer detection based on image processing  [ :arrow_down: ](https://arxiv.org/pdf/2207.08032.pdf)
>  Medical imaging is the most important tool for detecting complications in the inner body of medicine. Nowadays, with the development of image processing technology as well as changing the size of photos to higher resolution images in the field of digital medical imaging, there is an efficient and accurate system for segmenting this. Real-world images that for a variety of reasons have poor heterogeneity, noise and contrast are essential. Digital image segmentation in medicine is used for diagnostic and therapeutic analysis, which is very helpful for physicians. In this study, we aim at liver cancer photographs, which aim to more accurately detect the lesion or tumor of the liver because accurate and timely detection of the tumor is very important in the survival and life of the patient.The aim of this paper is to simplify the obnoxious study problems related to the study of MR images. The liver is the second organ most generic involved by metastatic disease being liver cancer one of the prominent causes of death worldwide. Without healthy liver a person cannot survive. It is life threatening disease which is very challenging perceptible for both medical and engineering technologists. Medical image processing is used as a non-invasive method to detect tumours. The chances of survival having liver Tumor highly depends on early detection of Tumor and then classification as cancerous and noncancerous tumours. Image processing techniques for automatic detection of brain are includes pre-processing and enhancement, image segmentation, classification and volume calculation, Poly techniques have been developed for the detection of liver Tumor and different liver toM oR detection algorithms and methodologies utilized for Tumor diagnosis. Novel methodology for the detection and diagnosis of liver Tumor.      
### 47.Analysis of the attack and its solution in wireless sensor networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.08014.pdf)
>  Several years ago, wireless sensor networks were used only by the military. These networks, which have many uses and are subject to limitations, the most important of which is the energy constraint, this energy constraint creates the requirement that the number And the length of the messages exchanged between the sensors is low. Sensor networks do not have a stable topology due to their inaccessible environments, and continually changes with the disappearance or addition of a node, support for the topology is performed in three steps before deployment, after deployment and deployment. . The sensor nodes are scattered across the field and transmitted by a multidimensional connection to the sink. The communication protocol of the sensor networks used by all nodes in the network and sink. The protocol consists of five layers and three levels of management. Sensor networks require a kind of security mechanism due to inaccessibility and protection. Conventional security mechanisms are inefficient due to the inherent limitations of sensor nodes in these networks. Sensor nodes, due to energy and resource constraints, require security requirements such as the confidentiality of data integrity data, authentication, synchronization, etc. Currently, many organizations use wireless sensor networks for purposes such as air, Pollution, Traffic Control and Healthcare Security is the main concern of wireless sensor networks. In this article, I will focus on the types of security attacks and their detection. This article outlines security needs and security attacks in wireless sensor networks. Also, the security criteria in wireless sensor networks are mentioned.      
### 48.Power Hardware-In-the-Loop Testing of a Peer-to-Peer Energy Trading Framework  [ :arrow_down: ](https://arxiv.org/pdf/2207.08009.pdf)
>  This paper demonstrates the value of power hardware-in-the-loop (PHIL) testing for the study of peer-to-peer (P2P) energy trading. P2P has emerged as a promising candidate for coordinating large numbers of distributed energy resources (DER) that pose a risk to network operations if left unmanaged. The existing literature has so far relied on pure software simulations to study DER and distribution networks within this context. This requires the development of simplified models for complex components due to the computational limitations involved. Issues that arise through the operation of physical hardware in real-world applications are therefore neglected. We present PHIL testing as a solution to this problem by exhibiting its ability to capture the complex behaviors of physical DER devices. A high-fidelity PHIL test environment is introduced that combines key hardware elements with a simulated network model to study a P2P trading scenario. The initial findings reveal several underlying challenges of coordinating DER that are not typically discussed in prior works.      
### 49.Exploring The Resilience of Control Execution Skips against False Data Injection Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2207.08005.pdf)
>  Modern Cyber-Physical Systems (CPSs) are often designed as networked, software-based controller implementations which have been found to be vulnerable to network-level and physical level attacks. A number of research works have proposed CPS-specific attack detection schemes as well as techniques for attack resilient controller design. However, such schemes also incur platform-level overheads. In this regard, some recent works have leveraged the use of skips in control execution to enhance the resilience of a CPS against false data injection (FDI) attacks. <br>In this paper, we provide an analytical discussion on when and how skipping a control execution can improve the resilience of the system against FDI attacks while maintaining the control performance requirement. We also propose a methodology to synthesize such optimal control execution patterns. To the best of our knowledge, no previous work has provided any quantitative analysis about the trade-off between attack resilience and control performance for such aperiodic control execution. Finally, we evaluate the proposed method on several safety-critical CPS benchmarks.      
### 50.Intelligent Reflecting Surfaces for the Enhancement of 6G Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2207.07999.pdf)
>  With the advancement of sensing technologies over the years, it has become critical to ensure the seamless connectivity of the Internet of Things (IoT) gadgets. With the advancement of communication technology, cellular networks are increasingly being utilized to link IoT systems. An IRS is a rectangular metasurface made up of a vast number of reflecting components that has recently gained research attention due to its ability to significantly improve the energy and spectral efficiencies of communication networks by modifying wireless transmission environments.      
### 51.A Study of Long-term Energy-mix Optimization Model: A Case Study in Japan  [ :arrow_down: ](https://arxiv.org/pdf/2207.07969.pdf)
>  There is a strong need to reduce greenhouse gas emissions to deal with climate change. In the power sector, changing the power generation method in the medium and long term is needed to reduce greenhouse gas emissions. This paper proposes a long-term energy-mix optimization mod-el to obtain the process of carbon neutrality in the power system. The proposed model models power supply and demand at an hourly granularity and determines the generation capacity that minimizes the long-term energy supply cost. Compared with the models proposed in previous studies, the proposed model can determine the installed capacity to maintain the balance of power supply and demand by adding the capacity of regulation reserve required by fluctuations in the output of variable renewable energy as a constraint condition. A Japan energy mix calculation is reported as a case study of the proposed model. This model can clarify the roadmap to achieving each country's emission reduction target and support the government's decision-making.      
### 52.Assessing the impact of cyber attacks manipulating distributed energy resources on power system operation  [ :arrow_down: ](https://arxiv.org/pdf/2207.07968.pdf)
>  Successful cyber attacks on power systems cause severe disruptions. One possible manipulation strategy is the utilization of distributed energy resources (DERs) to disturb power system operation. In addition to the impact on bulk power system frequency, local cascading effects caused by DER control and protection can increase the severity of this strategy. To investigate these effects, manipulation scenarios including the disconnection as well as the manipulation of active (P) and reactive power (Q) setpoints of DERs are derived. The impact is analyzed using time-domain simulations and quantified using assessment criteria such as voltage band violation and plant protection triggering. Though DER disconnection leads to high amounts of lost P injection the manipulation of Q setpoints offers potential to disconnect additional DERs through local cascading effects. To mitigate the impact of the manipulation scenarios automated tap changer operation as well as a limitation of remotely accessible Q is suitable.      
### 53.Learnable Mixed-precision and Dimension Reduction Co-design for Low-storage Activation  [ :arrow_down: ](https://arxiv.org/pdf/2207.07931.pdf)
>  Recently, deep convolutional neural networks (CNNs) have achieved many eye-catching results. However, deploying CNNs on resource-constrained edge devices is constrained by limited memory bandwidth for transmitting large intermediated data during inference, i.e., activation. Existing research utilizes mixed-precision and dimension reduction to reduce computational complexity but pays less attention to its application for activation compression. To further exploit the redundancy in activation, we propose a learnable mixed-precision and dimension reduction co-design system, which separates channels into groups and allocates specific compression policies according to their importance. In addition, the proposed dynamic searching technique enlarges search space and finds out the optimal bit-width allocation automatically. Our experimental results show that the proposed methods improve 3.54%/1.27% in accuracy and save 0.18/2.02 bits per value over existing mixed-precision methods on ResNet18 and MobileNetv2, respectively.      
### 54.Discriminative Kernel Convolution Network for Multi-Label Ophthalmic Disease Detection on Imbalanced Fundus Image Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2207.07918.pdf)
>  It is feasible to recognize the presence and seriousness of eye disease by investigating the progressions in retinal biological structure. Fundus examination is a diagnostic procedure to examine the biological structure and anomaly of the eye. Ophthalmic diseases like glaucoma, diabetic retinopathy, and cataract are the main reason for visual impairment around the world. Ocular Disease Intelligent Recognition (ODIR-5K) is a benchmark structured fundus image dataset utilized by researchers for multi-label multi-disease classification of fundus images. This work presents a discriminative kernel convolution network (DKCNet), which explores discriminative region-wise features without adding extra computational cost. DKCNet is composed of an attention block followed by a squeeze and excitation (SE) block. The attention block takes features from the backbone network and generates discriminative feature attention maps. The SE block takes the discriminative feature maps and improves channel interdependencies. Better performance of DKCNet is observed with InceptionResnet backbone network for multi-label classification of ODIR-5K fundus images with 96.08 AUC, 94.28 F1-score and 0.81 kappa score. The proposed method splits the common target label for an eye pair based on the diagnostic keyword. Based on these labels oversampling and undersampling is done to resolve class imbalance. To check the biasness of proposed model towards training data, the model trained on ODIR dataset is tested on three publicly available benchmark datasets. It is found to give good performance on completely unseen fundus images also.      
### 55.NOCT: Nonlinear Observability with Constraints and Time Offset  [ :arrow_down: ](https://arxiv.org/pdf/2207.07881.pdf)
>  Nonlinear systems of affine control inputs overarch many sensor fusion instances. Analyzing whether a state variable in such a nonlinear system can be estimated (i.e., observability) informs better estimator design. Among the research on local observability of nonlinear systems, approaches based on differential geometry have attracted much attention for the solid theoretic foundation and suitability to automated deduction. Such approaches usually work with a system model of unconstrained control inputs and assume that the control inputs and observation outputs are timestamped by the same clock. To our knowledge, it has not been shown how to conduct the observability analysis with additional constraints enforced on the system's observations or control inputs. To this end, we propose procedures to convert a system model of affine control inputs with linear constraints into a constraint-free standard model which is apt to be analyzed by the classic observability analysis procedure. Then, the whole analysis procedure is illustrated by applying to the well-studied visual inertial odometry (VIO) system which estimates the camera-IMU relative pose and time offset. The findings about unobservable variables under degenerate motion concur with those obtained with linearized VIO systems in other studies, whereas the findings about observability of time offset extend those in previous studies. These findings are further validated by simulation.      
### 56.Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation  [ :arrow_down: ](https://arxiv.org/pdf/2207.07850.pdf)
>  We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region.      
### 57.Adaptive t-vMF Dice Loss for Multi-class Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.07842.pdf)
>  Dice loss is widely used for medical image segmentation, and many improvement loss functions based on such loss have been proposed. However, further Dice loss improvements are still possible. In this study, we reconsidered the use of Dice loss and discovered that Dice loss can be rewritten in the loss function using the cosine similarity through a simple equation transformation. Using this knowledge, we present a novel t-vMF Dice loss based on the t-vMF similarity instead of the cosine similarity. Based on the t-vMF similarity, our proposed Dice loss is formulated in a more compact similarity loss function than the original Dice loss. Furthermore, we present an effective algorithm that automatically determines the parameter $\kappa$ for the t-vMF similarity using a validation accuracy, called Adaptive t-vMf Dice loss. Using this algorithm, it is possible to apply more compact similarities for easy classes and wider similarities for difficult classes, and we are able to achieve an adaptive training based on the accuracy of the class. Through experiments conducted on four datasets using a five-fold cross validation, we confirmed that the Dice score coefficient (DSC) was further improved in comparison with the original Dice loss and other loss functions.      
### 58.Towards Realistic Statistical Channel Models For Positioning: Evaluating the Impact of Early Clusters  [ :arrow_down: ](https://arxiv.org/pdf/2207.07838.pdf)
>  Physical effects such as reflection, refraction, and diffraction cause a radio signal to arrive from a transmitter to a receiver in multiple replicas that have different amplitude and rotation. Bandwidth-limited signals, such as positioning reference signals, have a limited time resolution. In reality, the signal is often reflected in the close vicinity of a transmitter and receiver, which causes the displacement of the observed peak from the true peak expected according to the line of sight (LOS) geometry between the transmitter and receiver. In this paper, we show that the existing channel model specified for performance evaluation within 3GPP fails to model the above phenomena. As a result, the simulation results deviate significantly from the measured values. Based on our measurement and simulation results, we propose a model for incorporating the signal reflection by obstacles in the vicinity of transmitter or receiver, so that the outcome of the model corresponds to the measurement made in such scenario.      
### 59.Complementary Semi-Deterministic Clusters for Realistic Statistical Channel Models for Positioning  [ :arrow_down: ](https://arxiv.org/pdf/2207.07837.pdf)
>  Positioning benefits from channel models that capture geometric effects and, in particular, from the signal properties of the first arriving path and the spatial consistency of the propagation condition of multiple links. The models that capture the physical effects observed in a realistic deployment scenario are essential for assessing the potential benefits of enhancements in positioning methods. Channel models based on ray-tracing simulations and statistical channel models, which are current state-of-the-art methods employed to evaluate performance of positioning in 3GPP systems, do not fully capture important aspects applicable to positioning. Hence, we propose an extension of existing statistical channel models with semi-deterministic clusters (SDCs). SDCs allow channels to be simulated using three types of clusters: fixed-, specular-, and random-clusters. Our results show that the proposed model aligns with measurements obtained in a real deployment scenario. Thus, our channel models can be used to develop advanced positioning solutions based on machine learning, which enable positioning with centimeter level accuracy in NLOS and multipath scenarios.      
### 60.Distributed Safe Learning and Planning for Multi-robot Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.07824.pdf)
>  This paper considers the problem where a group of mobile robots subject to unknown external disturbances aim to safely reach goal regions. We develop a distributed safe learning and planning algorithm that allows the robots to learn about the external unknown disturbances and safely navigate through the environment via their single trajectories. We use Gaussian process regression for online learning where variance is adopted to quantify the learning uncertainty. By leveraging set-valued analysis, the developed algorithm enables fast adaptation to newly learned models while avoiding collision against the learning uncertainty. Active learning is then applied to return a control policy such that the robots are able to actively explore the unknown disturbances and reach their goal regions in time. Sufficient conditions are established to guarantee the safety of the robots. A set of simulations are conducted for evaluation.      
### 61.Adversarial Reweighting for Speaker Verification Fairness  [ :arrow_down: ](https://arxiv.org/pdf/2207.07776.pdf)
>  We address performance fairness for speaker verification using the adversarial reweighting (ARW) method. ARW is reformulated for speaker verification with metric learning, and shown to improve results across different subgroups of gender and nationality, without requiring annotation of subgroups in the training data. An adversarial network learns a weight for each training sample in the batch so that the main learner is forced to focus on poorly performing instances. Using a min-max optimization algorithm, this method improves overall speaker verification fairness. We present three different ARWformulations: accumulated pairwise similarity, pseudo-labeling, and pairwise weighting, and measure their performance in terms of equal error rate (EER) on the VoxCeleb corpus. Results show that the pairwise weighting method can achieve 1.08% overall EER, 1.25% for male and 0.67% for female speakers, with relative EER reductions of 7.7%, 10.1% and 3.0%, respectively. For nationality subgroups, the proposed algorithm showed 1.04% EER for US speakers, 0.76% for UK speakers, and 1.22% for all others. The absolute EER gap between gender groups was reduced from 0.70% to 0.58%, while the standard deviation over nationality groups decreased from 0.21 to 0.19.      
### 62.Segment-level Metric Learning for Few-shot Bioacoustic Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2207.07773.pdf)
>  Few-shot bioacoustic event detection is a task that detects the occurrence time of a novel sound given a few examples. Previous methods employ metric learning to build a latent space with the labeled part of different sound classes, also known as positive events. In this study, we propose a segment-level few-shot learning framework that utilizes both the positive and negative events during model optimization. Training with negative events, which are larger in volume than positive events, can increase the generalization ability of the model. In addition, we use transductive inference on the validation set during training for better adaptation to novel classes. We conduct ablation studies on our proposed method with different setups on input features, training data, and hyper-parameters. Our final system achieves an F-measure of 62.73 on the DCASE 2022 challenge task 5 (DCASE2022-T5) validation set, outperforming the performance of the baseline prototypical network 34.02 by a large margin. Using the proposed method, our submitted system ranks 2nd in DCASE2022-T5. The code of this paper is fully open-sourced at <a class="link-external link-https" href="https://github.com/haoheliu/DCASE_2022_Task_5" rel="external noopener nofollow">this https URL</a>.      
### 63.ESFPNet: efficient deep learning architecture for real-time lesion segmentation in autofluorescence bronchoscopic video  [ :arrow_down: ](https://arxiv.org/pdf/2207.07759.pdf)
>  Lung cancer tends to be detected at an advanced stage, resulting in a high patient mortality rate. Thus, recent research has focused on early disease detection. Lung cancer generally first appears as lesions developing within the bronchial epithelium of the airway walls. Bronchoscopy is the procedure of choice for effective noninvasive bronchial lesion detection. In particular, autofluorescence bronchoscopy (AFB) discriminates the autofluorescence properties of normal and diseased tissue, whereby lesions appear reddish brown in AFB video frames, while normal tissue appears green. Because recent studies show AFB's ability for high lesion sensitivity, it has become a potentially pivotal method during the standard bronchoscopic airway exam for early-stage lung cancer detection. Unfortunately, manual inspection of AFB video is extremely tedious and error-prone, while limited effort has been expended toward potentially more robust automatic AFB lesion detection and segmentation. We propose a real-time deep learning architecture ESFPNet for robust detection and segmentation of bronchial lesions from an AFB video stream. The architecture features an encoder structure that exploits pretrained Mix Transformer (MiT) encoders and a stage-wise feature pyramid (ESFP) decoder structure. Results from AFB videos derived from lung cancer patient airway exams indicate that our approach gives mean Dice index and IOU values of 0.782 and 0.658, respectively, while having a processing throughput of 27 frames/sec. These values are superior to results achieved by other competing architectures that use Mix transformers or CNN-based encoders. Moreover, the superior performance on the ETIS-LaribPolypDB dataset demonstrates its potential applicability to other domains.      
### 64.Distributed Learning of Neural Lyapunov Functions for Large-Scale Networked Dissipative Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.07731.pdf)
>  This paper considers the problem of characterizing the stability region of a large-scale networked system comprised of dissipative nonlinear subsystems, in a distributed and computationally tractable way. One standard approach to estimate the stability region of a general nonlinear system is to first find a Lyapunov function for the system and characterize its region of attraction as the stability region. However, classical approaches, such as sum-of-squares methods and quadratic approximation, for finding a Lyapunov function either do not scale to large systems or give very conservative estimates for the stability region. In this context, we propose a new distributed learning based approach by exploiting the dissipativity structure of the subsystems. Our approach has two parts: the first part is a distributed approach to learn the storage functions (similar to the Lyapunov functions) for all the subsystems, and the second part is a distributed optimization approach to find the Lyapunov function for the networked system using the learned storage functions of the subsystems. We demonstrate the superior performance of our proposed approach through extensive case studies in microgrid networks.      
### 65.Temporal Forward-Backward Consistency, Not Residual Error, Measures the Prediction Accuracy of Extended Dynamic Mode Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2207.07719.pdf)
>  Extended Dynamic Mode Decomposition (EDMD) is a popular data-driven method to approximate the action of the Koopman operator on a linear function space spanned by a dictionary of functions. The accuracy of EDMD model critically depends on the quality of the particular dictionary's span, specifically on how close it is to being invariant under the Koopman operator. Motivated by the observation that the residual error of EDMD, typically used for dictionary learning, does not encode the quality of the function space and is sensitive to the choice of basis, we introduce the novel concept of consistency index. We show that this measure, based on using EDMD forward and backward in time, enjoys a number of desirable qualities that make it suitable for data-driven modeling of dynamical systems: it measures the quality of the function space, it is invariant under the choice of basis, can be computed in closed form from the data, and provides a tight upper-bound for the relative root mean square error of all function predictions on the entire span of the dictionary.      
### 66.Untrained, physics-informed neural networks for structured illumination microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2207.07705.pdf)
>  In recent years there has been great interest in using deep neural networks (DNN) for super-resolution image reconstruction including for structured illumination microscopy (SIM). While these methods have shown very promising results, they all rely on data-driven, supervised training strategies that need a large number of ground truth images, which is experimentally difficult to realize. For SIM imaging, there exists a need for a flexible, general, and open-source reconstruction method that can be readily adapted to different forms of structured illumination. We demonstrate that we can combine a deep neural network with the forward model of the structured illumination process to reconstruct sub-diffraction images without training data. The resulting physics-informed neural network (PINN) can be optimized on a single set of diffraction limited sub-images and thus doesn't require any training set. We show with simulated and experimental data that this PINN can be applied to a wide variety of SIM methods by simply changing the known illumination patterns used in the loss function and can achieve resolution improvements that match well with theoretical expectations.      
### 67.Localisation And Imaging Methods for Moving Target Ghost Imaging Radar Based On Correlation Intensity Weighting  [ :arrow_down: ](https://arxiv.org/pdf/2207.07649.pdf)
>  Ghost imaging radar is a new system of gaze imaging radar with high detection sensitivity, super-resolution and better anti-interference performance, but the relative motion between the radar system and the target will make the target imaging deteriorate. This paper proposes to perform absolute position localisation of a single target in the field of view by weighting the correlation strength of a single frame image of rough target, and to compensate translation of the reference arm speckle according to the localisation and tracking trajectory to accumulate the rough image into a high quality image. The proposed correlation intensity weighted localization and tracking imaging method has been verified by simulation to be able to locate and image targets in the field of view well.      
### 68.Style Transfer of Audio Effects with Differentiable Signal Processing  [ :arrow_down: ](https://arxiv.org/pdf/2207.08759.pdf)
>  We present a framework that can impose the audio effects and production style from one recording to another by example with the goal of simplifying the audio production process. We train a deep neural network to analyze an input recording and a style reference recording, and predict the control parameters of audio effects used to render the output. In contrast to past work, we integrate audio effects as differentiable operators in our framework, perform backpropagation through audio effects, and optimize end-to-end using an audio-domain loss. We use a self-supervised training strategy enabling automatic control of audio effects without the use of any labeled or paired training data. We survey a range of existing and new approaches for differentiable signal processing, showing how each can be integrated into our framework while discussing their trade-offs. We evaluate our approach on both speech and music tasks, demonstrating that our approach generalizes both to unseen recordings and even to sample rates different than those seen during training. Our approach produces convincing production style transfer results with the ability to transform input recordings to produced recordings, yielding audio effect control parameters that enable interpretability and user interaction.      
### 69.Quality Assessment of Image Super-Resolution: Balancing Deterministic and Statistical Fidelity  [ :arrow_down: ](https://arxiv.org/pdf/2207.08689.pdf)
>  There has been a growing interest in developing image super-resolution (SR) algorithms that convert low-resolution (LR) to higher resolution images, but automatically evaluating the visual quality of super-resolved images remains a challenging problem. Here we look at the problem of SR image quality assessment (SR IQA) in a two-dimensional (2D) space of deterministic fidelity (DF) versus statistical fidelity (SF). This allows us to better understand the advantages and disadvantages of existing SR algorithms, which produce images at different clusters in the 2D space of (DF, SF). Specifically, we observe an interesting trend from more traditional SR algorithms that are typically inclined to optimize for DF while losing SF, to more recent generative adversarial network (GAN) based approaches that by contrast exhibit strong advantages in achieving high SF but sometimes appear weak at maintaining DF. Furthermore, we propose an uncertainty weighting scheme based on content-dependent sharpness and texture assessment that merges the two fidelity measures into an overall quality prediction named the Super Resolution Image Fidelity (SRIF) index, which demonstrates superior performance against state-of-the-art IQA models when tested on subject-rated datasets.      
### 70.Upper Limb Movement Recognition utilising EEG and EMG Signals for Rehabilitative Robotics  [ :arrow_down: ](https://arxiv.org/pdf/2207.08650.pdf)
>  Upper limb movement classification, which maps input signals to the target activities, is one of the crucial areas in the control of rehabilitative robotics. Classifiers are trained for the rehabilitative system to comprehend the desires of the patient whose upper limbs do not function properly. Electromyography (EMG) signals and Electroencephalography (EEG) signals are used widely for upper limb movement classification. By analysing the classification results of the real-time EEG and EMG signals, the system can understand the intention of the user and predict the events that one would like to carry out. Accordingly, it will provide external help to the user to assist one to perform the activities. However, not all users process effective EEG and EMG signals due to the noisy environment. The noise in the real-time data collection process contaminates the effectiveness of the data. Moreover, not all patients process strong EMG signals due to muscle damage and neuromuscular disorder. To address these issues, we would like to propose a novel decision-level multisensor fusion technique. In short, the system will integrate EEG signals with EMG signals, retrieve effective information from both sources to understand and predict the desire of the user, and thus provide assistance. By testing out the proposed technique on a publicly available WAY-EEG-GAL dataset, which contains EEG and EMG signals that were recorded simultaneously, we manage to conclude the feasibility and effectiveness of the novel system.      
### 71.Nature-Inspired Intelligent -Fair Hybrid Precoding in Multiuser Massive Multiple-Input Multiple-Output Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.08588.pdf)
>  This paper proposes a novel nature-inspired $\alpha$-fair hybrid precoding (NI-$\alpha$HP) technique for millimeter-wave multi-user massive multiple-input multiple-output systems. Unlike the existing HP literature, we propose to apply $\alpha$-fairness for maintaining various fairness expectations (e.g., sum-rate maximization, proportional fairness, max-min fairness, etc.). After developing the analog RF beamformer via slow time-varying angular information, the digital baseband (BB) precoder is designed via the reduced-dimensional effective channel matrix seen from the BB-stage. For the $\alpha$-fairness, we derive the optimal digital BB precoder expression with a set of parameters, where optimizing them is an NP-hard problem. Hence, we efficiently optimize the parameters in the digital BB precoder via five nature-inspired intelligent algorithms. Numerical results present that when the sum-rate maximization is the target, the proposed NI-$\alpha$HP technique greatly improves the sum-rate capacity and energy-efficiency performance compared to other benchmarks. Moreover, NI-$\alpha$HP supports different fairness expectations and reduces the rate gap among UEs by varying the fairness level ($\alpha$).      
### 72.Study of the performance and scalability of federated learning for medical imaging with intermittent clients  [ :arrow_down: ](https://arxiv.org/pdf/2207.08581.pdf)
>  Federated learning is a data decentralization privacy-preserving technique used to perform machine or deep learning in a secure way. In this paper we present theoretical aspects about federated learning, such as the presentation of an aggregation operator, different types of federated learning, and issues to be taken into account in relation to the distribution of data from the clients, together with the exhaustive analysis of a use case where the number of clients varies. Specifically, a use case of medical image analysis is proposed, using chest X-ray images obtained from an open data repository. In addition to the advantages related to privacy, improvements in predictions (in terms of accuracy and area under the curve) and reduction of execution times will be studied with respect to the classical case (the centralized approach). Different clients will be simulated from the training data, selected in an unbalanced manner, i.e., they do not all have the same number of data. The results of considering three or ten clients are exposed and compared between them and against the centralized case. Two approaches to follow will be analyzed in the case of intermittent clients, as in a real scenario some clients may leave the training, and some new ones may enter the training. The evolution of the results for the test set in terms of accuracy, area under the curve and execution time is shown as the number of clients into which the original data is divided increases. Finally, improvements and future work in the field are proposed.      
### 73.ManiFeSt: Manifold-based Feature Selection for Small Data Sets  [ :arrow_down: ](https://arxiv.org/pdf/2207.08574.pdf)
>  In this paper, we present a new method for few-sample supervised feature selection (FS). Our method first learns the manifold of the feature space of each class using kernels capturing multi-feature associations. Then, based on Riemannian geometry, a composite kernel is computed, extracting the differences between the learned feature associations. Finally, a FS score based on spectral analysis is proposed. Considering multi-feature associations makes our method multivariate by design. This in turn allows for the extraction of the hidden manifold underlying the features and avoids overfitting, facilitating few-sample FS. We showcase the efficacy of our method on illustrative examples and several benchmarks, where our method demonstrates higher accuracy in selecting the informative features compared to competing methods. In addition, we show that our FS leads to improved classification and better generalization when applied to test data.      
### 74.The Vocal Signature of Social Anxiety: Exploration using Hypothesis-Testing and Machine-Learning Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2207.08534.pdf)
>  Background - Social anxiety (SA) is a common and debilitating condition, negatively affecting life quality even at sub-diagnostic thresholds. We sought to characterize SA's acoustic signature using hypothesis-testing and machine learning (ML) approaches. Methods - Participants formed spontaneous utterances responding to instructions to refuse or consent to commands of alleged peers. Vocal properties (e.g., intensity and duration) of these utterances were analyzed. Results - Our prediction that, as compared to low-SA (n=31), high-SA (n=32) individuals exhibit a less confident vocal speech signature, especially with respect to refusal utterances, was only partially supported by the classical hypothesis-testing approach. However, the results of the ML analyses and specifically the decision tree classifier were consistent with such speech patterns in SA. Using a Gaussian Process (GP) classifier, we were able to distinguish between high- and low-SA individuals with high (75.6%) accuracy and good (.83 AUC) separability. We also expected and found that vocal properties differentiated between refusal and consent utterances. Conclusions - Our findings provide further support for the usefulness of ML approach for the study of psychopathology, highlighting the utility of developing automatic techniques to create behavioral markers of SAD. Clinically, the simplicity and accessibility of these procedures may encourage people to seek professional help.      
### 75.Fully trainable Gaussian derivative convolutional layer  [ :arrow_down: ](https://arxiv.org/pdf/2207.08424.pdf)
>  The Gaussian kernel and its derivatives have already been employed for Convolutional Neural Networks in several previous works. Most of these papers proposed to compute filters by linearly combining one or several bases of fixed or slightly trainable Gaussian kernels with or without their derivatives. In this article, we propose a high-level configurable layer based on anisotropic, oriented and shifted Gaussian derivative kernels which generalize notions encountered in previous related works while keeping their main advantage. The results show that the proposed layer has competitive performance compared to previous works and that it can be successfully included in common deep architectures such as VGG16 for image classification and U-net for image segmentation.      
### 76.Predictive Neural Speech Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.08363.pdf)
>  Neural audio/speech coding has shown its capability to deliver a high quality at much lower bitrates than traditional methods recently. However, existing neural audio/speech codecs employ either acoustic features or learned blind features with a convolutional neural network for encoding, by which there are still temporal redundancies inside encoded features. This paper introduces latent-domain predictive coding into the VQ-VAE framework to fully remove such redundancies and proposes the TF-Codec for low-latency neural speech coding in an end-to-end way. Specifically, the extracted features are encoded conditioned on a prediction from past quantized latent frames so that temporal correlations are further removed. What's more, we introduce a learnable compression on the time-frequency input to adaptively adjust the attention paid on main frequencies and details at different bitrates. A differentiable vector quantization scheme based on distance-to-soft mapping and Gumbel-Softmax is proposed to better model the latent distributions with rate constraint. Subjective results on multilingual speech datasets show that with a latency of 40ms, the proposed TF-Codec at 1kbps can achieve a much better quality than Opus 9kbps and TF-Codec at 3kbps outperforms both EVS 9.6kbps and Opus 12kbps. Numerous studies are conducted to show the effectiveness of these techniques.      
### 77.SepLUT: Separable Image-adaptive Lookup Tables for Real-time Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2207.08351.pdf)
>  Image-adaptive lookup tables (LUTs) have achieved great success in real-time image enhancement tasks due to their high efficiency for modeling color transforms. However, they embed the complete transform, including the color component-independent and the component-correlated parts, into only a single type of LUTs, either 1D or 3D, in a coupled manner. This scheme raises a dilemma of improving model expressiveness or efficiency due to two factors. On the one hand, the 1D LUTs provide high computational efficiency but lack the critical capability of color components interaction. On the other, the 3D LUTs present enhanced component-correlated transform capability but suffer from heavy memory footprint, high training difficulty, and limited cell utilization. Inspired by the conventional divide-and-conquer practice in the image signal processor, we present SepLUT (separable image-adaptive lookup table) to tackle the above limitations. Specifically, we separate a single color transform into a cascade of component-independent and component-correlated sub-transforms instantiated as 1D and 3D LUTs, respectively. In this way, the capabilities of two sub-transforms can facilitate each other, where the 3D LUT complements the ability to mix up color components, and the 1D LUT redistributes the input colors to increase the cell utilization of the 3D LUT and thus enable the use of a more lightweight 3D LUT. Experiments demonstrate that the proposed method presents enhanced performance on photo retouching benchmark datasets than the current state-of-the-art and achieves real-time processing on both GPUs and CPUs.      
### 78.MobileCodec: Neural Inter-frame Video Compression on Mobile Devices  [ :arrow_down: ](https://arxiv.org/pdf/2207.08338.pdf)
>  Realizing the potential of neural video codecs on mobile devices is a big technological challenge due to the computational complexity of deep networks and the power-constrained mobile hardware. We demonstrate practical feasibility by leveraging Qualcomm's technology and innovation, bridging the gap from neural network-based codec simulations running on wall-powered workstations, to real-time operation on a mobile device powered by Snapdragon technology. We show the first-ever inter-frame neural video decoder running on a commercial mobile phone, decoding high-definition videos in real-time while maintaining a low bitrate and high visual quality.      
### 79.An Intelligent Deterministic Scheduling Method for Ultra-Low Latency Communication in Edge Enabled Industrial Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2207.08226.pdf)
>  Edge enabled Industrial Internet of Things (IIoT) platform is of great significance to accelerate the development of smart industry. However, with the dramatic increase in real-time IIoT applications, it is a great challenge to support fast response time, low latency, and efficient bandwidth utilization. To address this issue, Time Sensitive Network (TSN) is recently researched to realize low latency communication via deterministic scheduling. To the best of our knowledge, the combinability of multiple flows, which can significantly affect the scheduling performance, has never been systematically analyzed before. In this article, we first analyze the combinability problem. Then a non-collision theory based deterministic scheduling (NDS) method is proposed to achieve ultra-low latency communication for the time-sensitive flows. Moreover, to improve bandwidth utilization, a dynamic queue scheduling (DQS) method is presented for the best-effort flows. Experiment results demonstrate that NDS/DQS can well support deterministic ultra-low latency services and guarantee efficient bandwidth utilization.      
### 80.EM-Based Estimation and Compensation of Phase Noise in Massive-MIMO Uplink Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.08213.pdf)
>  Phase noise (PN) is a major disturbance in MIMO systems, where the contribution of different oscillators at the transmitter and the receiver side may degrade the overall performance and offset the gains offered by MIMO techniques. This is even more crucial in the case of massive MIMO, since the number of PN sources may increase considerably. In this work, we propose an iterative receiver based on the application of the expectation-maximization algorithm. We consider a massive MIMO framework with a general association of oscillators to antennas, and include other channel disturbances like imperfect channel state information and Rician block fading. At each receiver iteration, given the information on the transmitted symbols, steepest descent is used to estimate the PN samples, with an optimized adaptive step size and a threshold-based stopping rule. The results obtained for several test cases show how the bit error rate and mean square error can benefit from the proposed phase-detection algorithm, even to the point of reaching the same performance as in the case where no PN is present. Further analysis of the results allow to draw some useful trade-offs respecting final performance and consumption of resources.      
### 81.INFWIDE: Image and Feature Space Wiener Deconvolution Network for Non-blind Image Deblurring in Low-Light Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2207.08201.pdf)
>  Under low-light environment, handheld photography suffers from severe camera shake under long exposure settings. Although existing deblurring algorithms have shown promising performance on well-exposed blurry images, they still cannot cope with low-light snapshots. Sophisticated noise and saturation regions are two dominating challenges in practical low-light deblurring. In this work, we propose a novel non-blind deblurring method dubbed image and feature space Wiener deconvolution network (INFWIDE) to tackle these problems systematically. In terms of algorithm design, INFWIDE proposes a two-branch architecture, which explicitly removes noise and hallucinates saturated regions in the image space and suppresses ringing artifacts in the feature space, and integrates the two complementary outputs with a subtle multi-scale fusion network for high quality night photograph deblurring. For effective network training, we design a set of loss functions integrating a forward imaging model and backward reconstruction to form a close-loop regularization to secure good convergence of the deep neural network. Further, to optimize INFWIDE's applicability in real low-light conditions, a physical-process-based low-light noise model is employed to synthesize realistic noisy night photographs for model training. Taking advantage of the traditional Wiener deconvolution algorithm's physically driven characteristics and arisen deep neural network's representation ability, INFWIDE can recover fine details while suppressing the unpleasant artifacts during deblurring. Extensive experiments on synthetic data and real data demonstrate the superior performance of the proposed approach.      
### 82.Federated Learning and catastrophic forgetting in pervasive computing: demonstration in HAR domain  [ :arrow_down: ](https://arxiv.org/pdf/2207.08180.pdf)
>  Federated Learning has been introduced as a new machine learning paradigm enhancing the use of local devices. At a server level, FL regularly aggregates models learned locally on distributed clients to obtain a more general model. In this way, no private data is sent over the network, and the communication cost is reduced. However, current solutions rely on the availability of large amounts of stored data at the client side in order to fine-tune the models sent by the server. Such setting is not realistic in mobile pervasive computing where data storage must be kept low and data characteristic (distribution) can change dramatically. To account for this variability, a solution is to use the data regularly collected by the client to progressively adapt the received model. But such naive approach exposes clients to the well-known problem of catastrophic forgetting. The purpose of this paper is to demonstrate this problem in the mobile human activity recognition context on smartphones.      
### 83.End-to-End Spoken Language Understanding: Performance analyses of a voice command task in a low resource setting  [ :arrow_down: ](https://arxiv.org/pdf/2207.08179.pdf)
>  Spoken Language Understanding (SLU) is a core task in most human-machine interaction systems. With the emergence of smart homes, smart phones and smart speakers, SLU has become a key technology for the industry. In a classical SLU approach, an Automatic Speech Recognition (ASR) module transcribes the speech signal into a textual representation from which a Natural Language Understanding (NLU) module extracts semantic information. Recently End-to-End SLU (E2E SLU) based on Deep Neural Networks has gained momentum since it benefits from the joint optimization of the ASR and the NLU parts, hence limiting the cascade of error effect of the pipeline architecture. However, little is known about the actual linguistic properties used by E2E models to predict concepts and intents from speech input. In this paper, we present a study identifying the signal features and other linguistic properties used by an E2E model to perform the SLU task. The study is carried out in the application domain of a smart home that has to handle non-English (here French) voice commands. The results show that a good E2E SLU performance does not always require a perfect ASR capability. Furthermore, the results show the superior capabilities of the E2E model in handling background noise and syntactic variation compared to the pipeline model. Finally, a finer-grained analysis suggests that the E2E model uses the pitch information of the input signal to identify voice command concepts. The results and methodology outlined in this paper provide a springboard for further analyses of E2E models in speech processing.      
### 84.Source-free Unsupervised Domain Adaptation for Blind Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2207.08124.pdf)
>  Existing learning-based methods for blind image quality assessment (BIQA) are heavily dependent on large amounts of annotated training data, and usually suffer from a severe performance degradation when encountering the domain/distribution shift problem. Thanks to the development of unsupervised domain adaptation (UDA), some works attempt to transfer the knowledge from a label-sufficient source domain to a label-free target domain under domain shift with UDA. However, it requires the coexistence of source and target data, which might be impractical for source data due to the privacy or storage issues. In this paper, we take the first step towards the source-free unsupervised domain adaptation (SFUDA) in a simple yet efficient manner for BIQA to tackle the domain shift without access to the source data. Specifically, we cast the quality assessment task as a rating distribution prediction problem. Based on the intrinsic properties of BIQA, we present a group of well-designed self-supervised objectives to guide the adaptation of the BN affine parameters towards the target domain. Among them, minimizing the prediction entropy and maximizing the batch prediction diversity aim to encourage more confident results while avoiding the trivial solution. Besides, based on the observation that the IQA rating distribution of single image follows the Gaussian distribution, we apply Gaussian regularization to the predicted rating distribution to make it more consistent with the nature of human scoring. Extensive experimental results under cross-domain scenarios demonstrated the effectiveness of our proposed method to mitigate the domain shift.      
### 85.Load Modulation for Backscatter Communication: Channel Capacity and Capacity-Approaching Finite Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2207.08100.pdf)
>  In backscatter communication (BC), a passive tag transmits information by just affecting an external electromagnetic field through load modulation. Thereby, the feed current of the excited tag antenna is modulated by adapting the passive termination load. This paper studies the achievable information rates with a freely adaptable passive load. As a prerequisite, we unify monostatic, bistatic, and ambient BC with circuit-based system modeling. A crucial insight is that channel capacity is described by existing results on peak-power-limited quadrature Gaussian channels, because the steady-state tag current phasor lies on a disk. Consequently, we derive the channel capacity in the case of an unmodulated external field, for a general passive or purely reactive or resistive tag load. We find that modulating both resistance and reactance is crucial for high rates. We discuss the capacity-achieving load statistics, the rate asymptotics, and the capacity of ambient BC in important special cases. <br>Furthermore, we propose a capacity-approaching finite constellation design: a tailored amplitude-and-phase-shift keying on the reflection coefficient. We also demonstrate high rates for very simple loads of just a few switched resistors and capacitors. Finally, we investigate the rate loss from a value-range-constrained load, which is found to be small for moderate constraints.      
### 86.RIS-Assisted MIMO Communication Systems: Model-based versus Autoencoder Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2207.08077.pdf)
>  This paper considers reconfigurable intelligent surface (RIS)-assisted point-to-point multiple-input multiple-output (MIMO) communication systems, where a transmitter communicates with a receiver through an RIS. Based on the main target of reducing the bit error rate (BER) and therefore enhancing the communication reliability, we study different model-based and data-driven (autoencoder) approaches. In particular, we consider a model-based approach that optimizes both active and passive optimization variables. We further propose a novel end-to-end data-driven framework, which leverages the recent advances in machine learning. The neural networks presented for conventional signal processing modules are jointly trained with the channel effects to minimize the bit error detection. Numerical results demonstrate that the proposed data-driven approach can learn to encode the transmitted signal via different channel realizations dynamically. In addition, the data-driven approach not only offers a significant gain in the BER performance compared to the other state-of-the-art benchmarks but also guarantees the performance when perfect channel information is unavailable.      
### 87.Balancing Accuracy and Integrity for Reconfigurable Intelligent Surface-aided Over-the-Air Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.08057.pdf)
>  Over-the-air federated learning (AirFL) allows devices to train a learning model in parallel and synchronize their local models using over-the-air computation. The integrity of AirFL is vulnerable due to the obscurity of the local models aggregated over-the-air. This paper presents a novel framework to balance the accuracy and integrity of AirFL, where multi-antenna devices and base station (BS) are jointly optimized with a reconfigurable intelligent surface (RIS). The key contributions include a new and non-trivial problem jointly considering the model accuracy and integrity of AirFL, and a new framework that transforms the problem into tractable subproblems. Under perfect channel state information (CSI), the new framework minimizes the aggregated model's distortion and retains the local models' recoverability by optimizing the transmit beamformers of the devices, the receive beamformers of the BS, and the RIS configuration in an alternating manner. Under imperfect CSI, the new framework delivers a robust design of the beamformers and RIS configuration to combat non-negligible channel estimation errors. As corroborated experimentally, the novel framework can achieve comparable accuracy to the ideal FL while preserving local model recoverability under perfect CSI, and improve the accuracy when the number of receive antennas is small or moderate under imperfect CSI.      
### 88.Federated Deep Reinforcement Learning for RIS-Assisted Indoor Multi-Robot Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.08056.pdf)
>  Indoor multi-robot communications face two key challenges: one is the severe signal strength degradation caused by blockages (e.g., walls) and the other is the dynamic environment caused by robot mobility. To address these issues, we consider the reconfigurable intelligent surface (RIS) to overcome the signal blockage and assist the trajectory design among multiple robots. Meanwhile, the non-orthogonal multiple access (NOMA) is adopted to cope with the scarcity of spectrum and enhance the connectivity of robots. Considering the limited battery capacity of robots, we aim to maximize the energy efficiency by jointly optimizing the transmit power of the access point (AP), the phase shifts of the RIS, and the trajectory of robots. A novel federated deep reinforcement learning (F-DRL) approach is developed to solve this challenging problem with one dynamic long-term objective. Through each robot planning its path and downlink power, the AP only needs to determine the phase shifts of the RIS, which can significantly save the computation overhead due to the reduced training dimension. Simulation results reveal the following findings: I) the proposed F-DRL can reduce at least 86% convergence time compared to the centralized DRL; II) the designed algorithm can adapt to the increasing number of robots; III) compared to traditional OMA-based benchmarks, NOMA-enhanced schemes can achieve higher energy efficiency.      
### 89.Triple-Block Generalized Inverses for Control Applications with Mixed Consistency Requirements  [ :arrow_down: ](https://arxiv.org/pdf/2207.08027.pdf)
>  Extends previous work on block-partitioned mixed generalized inverses from two subsets of system variables with distinct consistency requirements to three subsets. Does not include any notable theoretical contributions.      
### 90.EEG2Vec: Learning Affective EEG Representations via Variational Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2207.08002.pdf)
>  There is a growing need for sparse representational formats of human affective states that can be utilized in scenarios with limited computational memory resources. We explore whether representing neural data, in response to emotional stimuli, in a latent vector space can serve to both predict emotional states as well as generate synthetic EEG data that are participant- and/or emotion-specific. We propose a conditional variational autoencoder based framework, EEG2Vec, to learn generative-discriminative representations from EEG data. Experimental results on affective EEG recording datasets demonstrate that our model is suitable for unsupervised EEG modeling, classification of three distinct emotion categories (positive, neutral, negative) based on the latent representation achieves a robust performance of 68.49%, and generated synthetic EEG sequences resemble real EEG data inputs to particularly reconstruct low-frequency signal components. Our work advances areas where affective EEG representations can be useful in e.g., generating artificial (labeled) training data or alleviating manual feature extraction, and provide efficiency for memory constrained edge computing applications.      
### 91.Signed Cumulative Distribution Transform for Parameter Estimation of 1-D Signals  [ :arrow_down: ](https://arxiv.org/pdf/2207.07989.pdf)
>  We describe a method for signal parameter estimation using the signed cumulative distribution transform (SCDT), a recently introduced signal representation tool based on optimal transport theory. The method builds upon signal estimation using the cumulative distribution transform (CDT) originally introduced for positive distributions. Specifically, we show that Wasserstein-type distance minimization can be performed simply using linear least squares techniques in SCDT space for arbitrary signal classes, thus providing a global minimizer for the estimation problem even when the underlying signal is a nonlinear function of the unknown parameters. Comparisons to current signal estimation methods using $L_p$ minimization shows the advantage of the method.      
### 92.On the use of impedance detuning for gastrointestinal segment tracking of ingestible capsules  [ :arrow_down: ](https://arxiv.org/pdf/2207.07975.pdf)
>  During their travel through the gastrointestinal tract, ingestible antennas encounter detuning in their impedance response due to varying electromagnetic properties of the surrounding tissues. This paper investigates the possibility of using this impedance detuning to detect in which segment of the gastrointestinal tract - stomach, small intestine, or large intestine - the capsule is located. Meandered dipole antennas operating in the 433 MHz Industrial, Scientific, and Medical Band are designed for this purpose. The antennas conform to the inner surface of 3D-printed polylactic-acid capsules with a shell thickness of 0.6 or 0.4 mm. The impedance response is first optimized numerically in a homogeneous cylindrical phantom with time-averaged electromagnetic properties. The magnitude and the phase of the reflection coefficient are then obtained in different tissues and compared with simulations and measurements. The experimental demonstration is carried out first using tissue-mimicking liquids and then in a recently deceased ex vivo porcine model. The minimum change in the phase between different gastrointestinal tissues was determined to be around 10 degrees in the porcine model, indicating that the changes in the impedance response, particularly the changes in the phase, provide sufficient information to follow the position of the capsule in the gastrointestinal tract.      
### 93.Visually-aware Acoustic Event Detection using Heterogeneous Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2207.07935.pdf)
>  Perception of auditory events is inherently multimodal relying on both audio and visual cues. A large number of existing multimodal approaches process each modality using modality-specific models and then fuse the embeddings to encode the joint information. In contrast, we employ heterogeneous graphs to explicitly capture the spatial and temporal relationships between the modalities and represent detailed information about the underlying signal. Using heterogeneous graph approaches to address the task of visually-aware acoustic event classification, which serves as a compact, efficient and scalable way to represent data in the form of graphs. Through heterogeneous graphs, we show efficiently modelling of intra- and inter-modality relationships both at spatial and temporal scales. Our model can easily be adapted to different scales of events through relevant hyperparameters. Experiments on AudioSet, a large benchmark, shows that our model achieves state-of-the-art performance.      
### 94.CNN-based Euler's Elastica Inpainting with Deep Energy and Deep Image Prior  [ :arrow_down: ](https://arxiv.org/pdf/2207.07921.pdf)
>  Euler's elastica constitute an appealing variational image inpainting model. It minimises an energy that involves the total variation as well as the level line curvature. These components are transparent and make it attractive for shape completion tasks. However, its gradient flow is a singular, anisotropic, and nonlinear PDE of fourth order, which is numerically challenging: It is difficult to find efficient algorithms that offer sharp edges and good rotation invariance. As a remedy, we design the first neural algorithm that simulates inpainting with Euler's Elastica. We use the deep energy concept which employs the variational energy as neural network loss. Furthermore, we pair it with a deep image prior where the network architecture itself acts as a prior. This yields better inpaintings by steering the optimisation trajectory closer to the desired solution. Our results are qualitatively on par with state-of-the-art algorithms on elastica-based shape completion. They combine good rotation invariance with sharp edges. Moreover, we benefit from the high efficiency and effortless parallelisation within a neural framework. Our neural elastica approach only requires 3x3 central difference stencils. It is thus much simpler than other well-performing algorithms for elastica inpainting. Last but not least, it is unsupervised as it requires no ground truth training data.      
### 95.Few-shot bioacoustic event detection at the DCASE 2022 challenge  [ :arrow_down: ](https://arxiv.org/pdf/2207.07911.pdf)
>  Few-shot sound event detection is the task of detecting sound events, despite having only a few labelled examples of the class of interest. This framework is particularly useful in bioacoustics, where often there is a need to annotate very long recordings but the expert annotator time is limited. This paper presents an overview of the second edition of the few-shot bioacoustic sound event detection task included in the DCASE 2022 challenge. A detailed description of the task objectives, dataset, and baselines is presented, together with the main results obtained and characteristics of the submitted systems. This task received submissions from 15 different teams from which 13 scored higher than the baselines. The highest F-score was of 60% on the evaluation set, which leads to a huge improvement over last year's edition. Highly-performing methods made use of prototypical networks, transductive learning, and addressed the variable length of events from all target classes. Furthermore, by analysing results on each of the subsets we can identify the main difficulties that the systems face, and conclude that few-show bioacoustic sound event detection remains an open challenge.      
### 96.Automatic dataset generation for specific object detection  [ :arrow_down: ](https://arxiv.org/pdf/2207.07867.pdf)
>  In the past decade, object detection tasks are defined mostly by large public datasets. However, building object detection datasets is not scalable due to inefficient image collecting and labeling. Furthermore, most labels are still in the form of bounding boxes, which provide much less information than the real human visual system. In this paper, we present a method to synthesize object-in-scene images, which can preserve the objects' detailed features without bringing irrelevant information. In brief, given a set of images containing a target object, our algorithm first trains a model to find an approximate center of the object as an anchor, then makes an outline regression to estimate its boundary, and finally blends the object into a new scene. Our result shows that in the synthesized image, the boundaries of objects blend very well with the background. Experiments also show that SOTA segmentation models work well with our synthesized data.      
### 97.Deep Learning and Its Applications to WiFi Human Sensing: A Benchmark and A Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2207.07859.pdf)
>  WiFi sensing has been evolving rapidly in recent years. Empowered by propagation models and deep learning methods, many challenging applications are realized such as WiFi-based human activity recognition and gesture recognition. However, in contrast to deep learning for visual recognition and natural language processing, no sufficiently comprehensive public benchmark exists. In this paper, we highlight the recent progress on deep learning enabled WiFi sensing, and then propose a benchmark, SenseFi, to study the effectiveness of various deep learning models for WiFi sensing. These advanced models are compared in terms of distinct sensing tasks, WiFi platforms, recognition accuracy, model size, computational complexity, feature transferability, and adaptability of unsupervised learning. It is also regarded as a tutorial for deep learning based WiFi sensing, starting from CSI hardware platform to sensing algorithms. The extensive experiments provide us with experiences in deep model design, learning strategy skills and training techniques for real-world applications. To the best of our knowledge, this is the first benchmark with an open-source library for deep learning in WiFi sensing research. The benchmark codes are available at <a class="link-external link-https" href="https://github.com/CHENXINYAN-sg/WiFi-CSI-Sensing-Benchmark" rel="external noopener nofollow">this https URL</a>.      
### 98.Robust AI Driving Strategy for Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2207.07829.pdf)
>  There has been significant progress in sensing, perception, and localization for automated driving, However, due to the wide spectrum of traffic/road structure scenarios and the long tail distribution of human driver behavior, it has remained an open challenge for an intelligent vehicle to always know how to make and execute the best decision on road given available sensing / perception / localization information. In this chapter, we talk about how artificial intelligence and more specifically, reinforcement learning, can take advantage of operational knowledge and safety reflex to make strategical and tactical decisions. We discuss some challenging problems related to the robustness of reinforcement learning solutions and their implications to the practical design of driving strategies for autonomous vehicles. We focus on automated driving on highway and the integration of reinforcement learning, vehicle motion control, and control barrier function, leading to a robust AI driving strategy that can learn and adapt safely.      
### 99.Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2207.07828.pdf)
>  We propose an effective Structural Prior guided Generative Adversarial Transformer (SPGAT) to solve low-light image enhancement. Our SPGAT mainly contains a generator with two discriminators and a structural prior estimator (SPE). The generator is based on a U-shaped Transformer which is used to explore non-local information for better clear image restoration. The SPE is used to explore useful structures from images to guide the generator for better structural detail estimation. To generate more realistic images, we develop a new structural prior guided adversarial learning method by building the skip connections between the generator and discriminators so that the discriminators can better discriminate between real and fake features. Finally, we propose a parallel windows-based Swin Transformer block to aggregate different level hierarchical features for high-quality image restoration. Experimental results demonstrate that the proposed SPGAT performs favorably against recent state-of-the-art methods on both synthetic and real-world datasets.      
### 100.CHARM: A Hierarchical Deep Learning Model for Classification of Complex Human Activities Using Motion Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2207.07806.pdf)
>  In this paper, we report a hierarchical deep learning model for classification of complex human activities using motion sensors. In contrast to traditional Human Activity Recognition (HAR) models used for event-based activity recognition, such as step counting, fall detection, and gesture identification, this new deep learning model, which we refer to as CHARM (Complex Human Activity Recognition Model), is aimed for recognition of high-level human activities that are composed of multiple different low-level activities in a non-deterministic sequence, such as meal preparation, house chores, and daily routines. CHARM not only quantitatively outperforms state-of-the-art supervised learning approaches for high-level activity recognition in terms of average accuracy and F1 scores, but also automatically learns to recognize low-level activities, such as manipulation gestures and locomotion modes, without any explicit labels for such activities. This opens new avenues for Human-Machine Interaction (HMI) modalities using wearable sensors, where the user can choose to associate an automated task with a high-level activity, such as controlling home automation (e.g., robotic vacuum cleaners, lights, and thermostats) or presenting contextually relevant information at the right time (e.g., reminders, status updates, and weather/news reports). In addition, the ability to learn low-level user activities when trained using only high-level activity labels may pave the way to semi-supervised learning of HAR tasks that are inherently difficult to label.      
### 101.QuaDUE-CCM: Interpretable Distributional Reinforcement Learning using Uncertain Contraction Metrics for Precise Quadrotor Trajectory Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2207.07789.pdf)
>  Accuracy and stability are common requirements for Quadrotor trajectory tracking systems. Designing an accurate and stable tracking controller remains challenging, particularly in unknown and dynamic environments with complex aerodynamic disturbances. We propose a Quantile-approximation-based Distributional-reinforced Uncertainty Estimator (QuaDUE) to accurately identify the effects of aerodynamic disturbances, i.e., the uncertainties between the true and estimated Control Contraction Metrics (CCMs). Taking inspiration from contraction theory and integrating the QuaDUE for uncertainties, our novel CCM-based trajectory tracking framework tracks any feasible reference trajectory precisely whilst guaranteeing exponential convergence. More importantly, the convergence and training acceleration of the distributional RL are guaranteed and analyzed, respectively, from theoretical perspectives. We also demonstrate our system under unknown and diverse aerodynamic forces. Under large aerodynamic forces (&gt;2m/s^2), compared with the classic data-driven approach, our QuaDUE-CCM achieves at least a 56.6% improvement in tracking error. Compared with QuaDRED-MPC, a distributional RL-based approach, QuaDUE-CCM achieves at least a 3 times improvement in contraction rate.      
### 102.Strategic Asset Allocation with Illiquid Alternatives  [ :arrow_down: ](https://arxiv.org/pdf/2207.07767.pdf)
>  We address the problem of strategic asset allocation (SAA) with portfolios that include illiquid alternative asset classes. The main challenge in portfolio construction with illiquid asset classes is that we do not have direct control over our positions, as we do in liquid asset classes. Instead we can only make commitments; the position builds up over time as capital calls come in, and reduces over time as distributions occur, neither of which the investor has direct control over. The effect on positions of our commitments is subject to a delay, typically of a few years, and is also unknown or stochastic. A further challenge is the requirement that we can meet the capital calls, with very high probability, with our liquid assets. <br>We formulate the illiquid dynamics as a random linear system, and propose a convex optimization based model predictive control (MPC) policy for allocating liquid assets and making new illiquid commitments in each period. Despite the challenges of time delay and uncertainty, we show that this policy attains performance surprisingly close to a fictional setting where we pretend the illiquid asset classes are completely liquid, and we can arbitrarily and immediately adjust our positions. In this paper we focus on the growth problem, with no external liabilities or income, but the method is readily extended to handle this case.      
### 103.Input/output-to-state stability of switched systems under restricted switching  [ :arrow_down: ](https://arxiv.org/pdf/2207.07764.pdf)
>  This paper deals with input/output-to-state stability (IOSS) of continuous-time switched nonlinear systems. Given a family of systems, possibly containing unstable dynamics, and a set of restrictions on admissible switches between the subsystems and admissible dwell times on the subsystems, we identify a class of switching signals that obeys these restrictions and preserves stability of the resulting switched system. The primary apparatus for our analysis is multiple Lyapunov-like functions. Input-to-state stability (ISS) and global asymptotic stability (GAS) of switched systems under pre-specified restrictions on switching signals fall as special cases of our results when no outputs (resp., also inputs) are considered.      
### 104.A Review of the Operational Use of UAS in Public Safety Emergency Incidents  [ :arrow_down: ](https://arxiv.org/pdf/2207.07761.pdf)
>  The domain of public safety in the form of search \&amp; rescue, wildland firefighting, structure firefighting, and law enforcement operations have drawn great interest in the field of aerospace engineering, human-robot teaming, autonomous systems, and robotics. However, a divergence exists in the assumptions made in research and how state-of-the-art technologies may realistically transition into an operational capacity. To aid in the alignment between researchers, technologists, and end users, we aim to provide perspective on how small Uncrewed Aerial Systems (sUAS) have been applied in 114 real world incidents as part of a technical rescue team from 2016 to 2021. We highlight the main applications, integration, tasks, and challenges of employing UAS within five primary use cases including searches, evidence collection, SWAT, wildland firefighting, and structure firefighting. Within these use cases, key incidents are featured that provide perspective on the evolving and dynamic nature of UAS tasking during an operation. Finally, we highlight key technical directions for improving the utilization and efficiency of employing aerial technology in all emergency types.      
### 105.Carleman Linearization of Nonlinear Systems and Its Finite-Section Approximations  [ :arrow_down: ](https://arxiv.org/pdf/2207.07755.pdf)
>  The Carleman linearization is one of the mainstream approaches to lift a \linebreak[4] finite-dimensional nonlinear dynamical system into an infinite-dimensional linear system with the promise of providing accurate approximations of the original nonlinear system over larger regions around the equilibrium for longer time horizons with respect to the conventional first-order linearization approach. Finite-section approximations of the lifted system has been widely used to study dynamical and control properties of the original nonlinear system. In this context, some of the outstanding problems are to determine under what conditions, as the finite-section order (i.e., truncation length) increases, the trajectory of the resulting approximate linear system from the finite-section scheme converges to that of the original nonlinear system and whether the time interval over which the convergence happens can be quantified explicitly. In this paper, we provide explicit error bounds for the finite-section approximation and prove that the convergence is indeed exponential with respect to the finite-section order. For a class of nonlinear systems, it is shown that one can achieve exponential convergence over the entire time horizon up to infinity. Our results are practically plausible as our proposed error bound estimates can be used to compute proper truncation lengths for a given application, e.g., determining proper sampling period for model predictive control and reachability analysis for safety verifications. We validate our theoretical findings through several illustrative simulations.      
### 106.Do Not Sleep on Linear Models: Simple and Interpretable Techniques Outperform Deep Learning for Sleep Scoring  [ :arrow_down: ](https://arxiv.org/pdf/2207.07753.pdf)
>  Over the last few years, research in automatic sleep scoring has mainly focused on developing increasingly complex deep learning architectures. However, recently these approaches achieved only marginal improvements, often at the expense of requiring more data and more expensive training procedures. Despite all these efforts and their satisfactory performance, automatic sleep staging solutions are not widely adopted in a clinical context yet. We argue that most deep learning solutions for sleep scoring are limited in their real-world applicability as they are hard to train, deploy, and reproduce. Moreover, these solutions lack interpretability and transparency, which are often key to increase adoption rates. In this work, we revisit the problem of sleep stage classification using classical machine learning. Results show that state-of-the-art performance can be achieved with a conventional machine learning pipeline consisting of preprocessing, feature extraction, and a simple machine learning model. In particular, we analyze the performance of a linear model and a non-linear (gradient boosting) model. Our approach surpasses state-of-the-art (that uses the same data) on two public datasets: Sleep-EDF SC-20 (MF1 0.810) and Sleep-EDF ST (MF1 0.795), while achieving competitive results on Sleep-EDF SC-78 (MF1 0.775) and MASS SS3 (MF1 0.817). We show that, for the sleep stage scoring task, the expressiveness of an engineered feature vector is on par with the internally learned representations of deep learning models. This observation opens the door to clinical adoption, as a representative feature vector allows to leverage both the interpretability and successful track record of traditional machine learning models.      
### 107.Cache Enabled UAV HetNets Access xHaul Coverage Analysis and Optimal Resource Partitioning  [ :arrow_down: ](https://arxiv.org/pdf/2207.06822.pdf)
>  We study an urban wireless network in which cache-enabled UAV-Access points (UAV-APs) and UAV-Base stations (UAV-BSs) are deployed to provide higher throughput and ad-hoc coverage to users on the ground. The cache-enabled UAV-APs route the user data to the core network via either terrestrial base stations (TBSs) or backhaul-enabled UAV-BSs through an xHaul link. First, we derive the association probabilities in the access and xHaul links. Interestingly, we show that to maximize the line-of-sight (LoS) unmanned aerial vehicle (UAV) association, densifying the UAV deployment may not be beneficial after a threshold. Then, we obtain the signal to interference noise ratio (SINR) coverage probability of the typical user in the access link and the tagged UAV-AP in the xHaul link, respectively. The SINR coverage analysis is employed to characterize the successful content delivery probability by jointly considering the probability of successful access and xHaul transmissions and successful cache-hit probability. We numerically optimize the distribution of frequency resources between the access and the xHaul links to maximize the successful content delivery to the users. For a given storage capacity at the UAVs, our study prescribes the network operator optimal bandwidth partitioning factors and dimensioning rules concerning the deployment of the UAV-APs.      
### 108.Dominant Channel Estimation via MIPS for Large-Scale Antenna Systems with One-Bit ADCs  [ :arrow_down: ](https://arxiv.org/pdf/1808.06754.pdf)
>  In large-scale antenna systems, using one-bit analog-to-digital converters (ADCs) has recently become important since they offer significant reductions in both power and cost. However, in contrast to high-resolution ADCs, the coarse quantization of one-bit ADCs results in an irreversible loss of information. In the context of channel estimation, studies have been developed extensively to combat the performance loss incurred by one-bit ADCs. Furthermore, in the field of array signal processing, direction-of-arrival (DOA) estimation combined with one-bit ADCs has gained growing interests recently to minimize the estimation error. In this paper, a channel estimator is proposed for one-bit ADCs where the channels are characterized by their angular geometries, e.g., uniform linear arrays (ULAs). The goal is to estimate the dominant channel among multiple paths. The proposed channel estimator first finds the DOA estimate using the maximum inner product search (MIPS). Then, the channel fading coefficient is estimated using the concavity of the log-likelihood function. The limit inherent in one-bit ADCs is also investigated, which results from the loss of magnitude information.      
