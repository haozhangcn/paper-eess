# ArXiv eess --Thu, 7 Jul 2022
### 1.mu-synthesis-based Generalized Robust Framework for Grid-following and Grid-forming Inverters  [ :arrow_down: ](https://arxiv.org/pdf/2207.02818.pdf)
>  Grid-following and grid-forming inverters are integral components of microgrids and for integration of renewable energy sources with the grid. For grid following inverters, which need to emulate controllable current sources, a significant challenge is to address the large uncertainty of the grid impedance. For grid forming inverters, which need to emulate a controllable voltage source, large uncertainty due to varying loads has to be addressed. In this article, a mu-synthesis-based robust control design methodology, where performance under quantified uncertainty is guaranteed, is developed under a unified approach for both grid-following and grid-forming inverters. The control objectives, while designing the proposed optimal controllers, are: i) reference tracking, disturbance rejection, harmonic compensation capability with sufficient LCL resonance damping under large variations of grid impedance uncertainty for grid-following inverters; ii) reference tracking, disturbance rejection, harmonic compensation capability with enhanced dynamic response under large variations of equivalent loading uncertainty for grid-forming inverters. A combined system-in-the-loop (SIL), controller hardware-in-the-loop (CHIL) and power hardware-in-the-loop (PHIL) based experimental validation on 10 kVA microgrid system with two physical inverter systems is conducted in order to evaluate the efficacy and viability of the proposed controllers.      
### 2.The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.02797.pdf)
>  The manifold hypothesis is a core mechanism behind the success of deep learning, so understanding the intrinsic manifold structure of image data is central to studying how neural networks learn from the data. Intrinsic dataset manifolds and their relationship to learning difficulty have recently begun to be studied for the common domain of natural images, but little such research has been attempted for radiological images. We address this here. First, we compare the intrinsic manifold dimensionality of radiological and natural images. We also investigate the relationship between intrinsic dimensionality and generalization ability over a wide range of datasets. Our analysis shows that natural image datasets generally have a higher number of intrinsic dimensions than radiological images. However, the relationship between generalization ability and intrinsic dimensionality is much stronger for medical images, which could be explained as radiological images having intrinsic features that are more difficult to learn. These results give a more principled underpinning for the intuition that radiological images can be more challenging to apply deep learning to than natural image datasets common to machine learning research. We believe rather than directly applying models developed for natural images to the radiological imaging domain, more care should be taken to developing architectures and algorithms that are more tailored to the specific characteristics of this domain. The research shown in our paper, demonstrating these characteristics and the differences from natural images, is an important first step in this direction.      
### 3.Histopathology DatasetGAN: Synthesizing Large-Resolution Histopathology Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2207.02712.pdf)
>  Self-supervised learning (SSL) methods are enabling an increasing number of deep learning models to be trained on image datasets in domains where labels are difficult to obtain. These methods, however, struggle to scale to the high resolution of medical imaging datasets, where they are critical for achieving good generalization on label-scarce medical image datasets. In this work, we propose the Histopathology DatasetGAN (HDGAN) framework, an extension of the DatasetGAN semi-supervised framework for image generation and segmentation that scales well to large-resolution histopathology images. We make several adaptations from the original framework, including updating the generative backbone, selectively extracting latent features from the generator, and switching to memory-mapped arrays. These changes reduce the memory consumption of the framework, improving its applicability to medical imaging domains. We evaluate HDGAN on a thrombotic microangiopathy high-resolution tile dataset, demonstrating strong performance on the high-resolution image-annotation generation task. We hope that this work enables more application of deep learning models to medical datasets, in addition to encouraging more exploration of self-supervised frameworks within the medical imaging domain.      
### 4.Modeling the HEVC Encoding Energy Using the Encoder Processing Time  [ :arrow_down: ](https://arxiv.org/pdf/2207.02676.pdf)
>  The global significance of energy consumption of video communication renders research on the energy need of video coding an important task. To do so, usually, a dedicated setup is needed that measures the energy of the encoding and decoding system. However, such measurements are costly and complex. To this end, this paper presents the results of an exhaustive measurement series using the x265 encoder implementation of HEVC and analyzes the relation between encoding time and encoding energy. Finally, we introduce a simple encoding energy estimation model which employs the encoding time of a lightweight encoding process to estimate the encoding energy of complex encoding configurations. The proposed model reaches a mean estimation error of 11.35% when averaged over all presets. The results from this work are useful when the encoding energy estimate is required to develop new energy-efficient video compression algorithms.      
### 5.VI-SLAM2tag: Low-Effort Labeled Dataset Collection for Fingerprinting-Based Indoor Localization  [ :arrow_down: ](https://arxiv.org/pdf/2207.02668.pdf)
>  Fingerprinting-based approaches are particularly suitable for deploying indoor positioning systems for pedestrians with minimal infrastructure costs. The accuracy of the method, however, strongly depends on the quality of collected labeled fingerprints within the calibration phase, which is a tedious process when done manually in a static fashion. We present VI-SLAM2tag, a system for auto-labeling of dynamically collected fingerprints using the visual-inertial simultaneous localization and mapping (VI-SLAM) module of ARCore. ARCore occasionally updates its internal coordinate system. Mapping the entire trajectory to a target coordinate system via a single transformation thus results in large drift effects. To solve this, we propose a strategy for determining locally optimal sub-trajectory transformations. Our system is evaluated with respect to the accuracy of the generated position labels using a geodetic tracking system. We achieve an average labeling error of roughly 50 cm for trajectories of up to 15 minutes, which is sufficient for fingerprinting-based localization. We demonstrate this by collecting a multi-floor dataset including WLAN and IMU data and show how it can be used to train neural network based models that achieve a positioning accuracy of roughly 2 m. VI-SLAM2tag and the dataset are made publicly available.      
### 6.User Pairing and Power Allocation for FTN-based SC-NOMA and MIMO-NOMA Systems Considering User Fairness  [ :arrow_down: ](https://arxiv.org/pdf/2207.02653.pdf)
>  Faster than Nyquist (FTN) and non-orthogonal multiple access (NOMA) are two promising paradigms to improve the spectrum efficiency (SE) of communication systems without any extra spectrum resources required. The combination of FTN signaling and NOMA technology is an interesting attempt and has recently been focused on by researchers. In the NOMA technology, user pairing and power allocation are key algorithms that can determine the system capacity. This paper proposes a joint optimal user pairing and power allocation algorithm for the FTN-based single-carrier (SC) NOMA system, considering user fairness. Then, the FTN-based multiple-input-multiple-output (MIMO) NOMA system is studied and a dynamic user pairing and power allocation scheme is presented. In both scenarios, the constraint for user fairness guarantees that the user's SE in the NOMA system is no less than that in the OMA system. Afterward, the performance of the proposed scheme in achievable sum rate (ASR) and quality of service (QoS) is derived and verified. Simulation results show that the proposed user pairing and power allocation can achieve significantly higher ASRs and lower outage probabilities beyond the conventional orthogonal multiple access (OMA) system and the NOMA system with Nyquist-criterion transmission and random user pairing. As far as we know, this paper is the first solution to the issue of user pairing and power allocation for FTN-based NOMA systems.      
### 7.Lightweight Encoder-Decoder Architecture for Foot Ulcer Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.02515.pdf)
>  Continuous monitoring of foot ulcer healing is needed to ensure the efficacy of a given treatment and to avoid any possibility of deterioration. Foot ulcer segmentation is an essential step in wound diagnosis. We developed a model that is similar in spirit to the well-established encoder-decoder and residual convolution neural networks. Our model includes a residual connection along with a channel and spatial attention integrated within each convolution block. A simple patch-based approach for model training, test time augmentations, and majority voting on the obtained predictions resulted in superior performance. Our model did not leverage any readily available backbone architecture, pre-training on a similar external dataset, or any of the transfer learning techniques. The total number of network parameters being around 5 million made it a significantly lightweight model as compared with the available state-of-the-art models used for the foot ulcer segmentation task. Our experiments presented results at the patch-level and image-level. Applied on publicly available Foot Ulcer Segmentation (FUSeg) Challenge dataset from MICCAI 2021, our model achieved state-of-the-art image-level performance of 88.22% in terms of Dice similarity score and ranked second in the official challenge leaderboard. We also showed an extremely simple solution that could be compared against the more advanced architectures.      
### 8.Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies  [ :arrow_down: ](https://arxiv.org/pdf/2207.02495.pdf)
>  There is often a trade-off between performance and latency in streaming automatic speech recognition (ASR). Traditional methods such as look-ahead and chunk-based methods, usually require information from future frames to advance recognition accuracy, which incurs inevitable latency even if the computation is fast enough. A causal model that computes without any future frames can avoid this latency, but its performance is significantly worse than traditional methods. In this paper, we propose corresponding revision strategies to improve the causal model. Firstly, we introduce a real-time encoder states revision strategy to modify previous states. Encoder forward computation starts once the data is received and revises the previous encoder states after several frames, which is no need to wait for any right context. Furthermore, a CTC spike position alignment decoding algorithm is designed to reduce time costs brought by the revision strategy. Experiments are all conducted on Librispeech datasets. Fine-tuning on the CTC-based wav2vec2.0 model, our best method can achieve 3.7/9.2 WERs on test-clean/other sets, which is also competitive with the chunk-based methods and the knowledge distillation methods.      
### 9.Multi-Contrast MRI Segmentation Trained on Synthetic Images  [ :arrow_down: ](https://arxiv.org/pdf/2207.02469.pdf)
>  In our comprehensive experiments and evaluations, we show that it is possible to generate multiple contrast (even all synthetically) and use synthetically generated images to train an image segmentation engine. We showed promising segmentation results tested on real multi-contrast MRI scans when delineating muscle, fat, bone and bone marrow, all trained on synthetic images. Based on synthetic image training, our segmentation results were as high as 93.91\%, 94.11\%, 91.63\%, 95.33\%, for muscle, fat, bone, and bone marrow delineation, respectively. Results were not significantly different from the ones obtained when real images were used for segmentation training: 94.68\%, 94.67\%, 95.91\%, and 96.82\%, respectively.      
### 10.Neural Network Equalization for Asynchronous Multitrack Detection in TDMR  [ :arrow_down: ](https://arxiv.org/pdf/2207.02432.pdf)
>  The advent of multiple readers in magnetic recording opens the possibility of replacing the current industry's single-track detection with the more promising multitrack detection architectures. We have proposed a first solution, a generalized partial-response maximum-likelihood (GPRML) architecture, that extends the conventional PRML paradigm to jointly detect multiple asynchronous tracks. In this paper, we propose to replace the conventional communication-theoretic multiple-input multiple-output equalizer in the GPRML architecture with a neural network equalizer for better adaption to the nonlinearity of the underlying channel. We evaluate the proposed equalization strategy on a realistic two-dimensional magnetic-recording channel, and find that the proposed equalizer outperforms the conventional linear equalizer, by a 35% reduction in the bit-error rate.      
### 11.Modeling and Analysis of Utilizing Cryptocurrency Mining for Demand Flexibility in Electric Energy Systems: A Synthetic Texas Grid Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2207.02428.pdf)
>  The electricity sector is facing the dual challenge of supporting increasing level of demand electrification while substantially reducing its carbon footprint. Among electricity demands, the energy consumption of cryptocurrency mining data centers has witnessed significant growth worldwide. If well-coordinated, these data centers could be tailor-designed to aggressively absorb the increasing uncertainties of energy supply and, in turn, provide valuable grid-level services in the electricity market. In this paper, we study the impact of integrating new cryptocurrency mining loads into Texas power grid and the potential profit of utilizing demand flexibility from cryptocurrency mining facilities in the electricity market. We investigate different demand response programs available for data centers and quantify the annual profit of cryptocurrency mining units participating in these programs. We perform our simulations using a synthetic 2000 bus ERCOT grid model, along with added cryptocurrency mining loads on top of the real-world demand profiles in Texas. Our preliminary results show that depending on the size and location of these new loads, we observe different impacts on the ERCOT electricity market, where they could increase the electricity prices and incur more fluctuations in a highly non-uniform manner.      
### 12.A Two-Step Quasi-Newton Identification Algorithm for Stochastic Systems with Saturated Observations  [ :arrow_down: ](https://arxiv.org/pdf/2207.02422.pdf)
>  This paper investigates the identification and prediction problems for stochastic dynamical systems with saturated observations, which arise from various fields in engineering and social systems, but still lack comprehensive theoretical studies up to now. The main contributions of this paper are: (i) To introduce a two-step Quasi-Newton (TSQN) identification algorithm which is applicable to a typical class of nonlinear stochastic systems with outputs observed under possibly varying saturations. (ii) To establish the convergence of both the parameter estimators and adaptive predictors and to prove the asymptotic normality, under a weakest possible non-persistent excitation (PE) condition, which can be applied to stochastic feedback systems with general non-stationary and correlated system signals or data. (iii) To establish probabilistic estimation error bounds for any given finite number of data, by using either martingale inequalities or Monte Carlo experiments. A numerical example is also provided to illustrated the performance of our identification algorithm.      
### 13.Novel Spectrum Allocation Among Multiple Transmission Windows for Terahertz Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.02401.pdf)
>  This paper presents a novel spectrum allocation strategy for multiuser terahertz (THz) band communication systems when the to-be-allocated spectrum is composed of multiple transmission windows (TWs). This strategy explores the benefits of (i) allowing users to occupy sub-bands with unequal bandwidths and (ii) optimally avoiding using some spectra that exist at the edges of TWs where molecular absorption loss is high. To maximize the aggregated multiuser data rate, we formulate an optimization problem, with the primary focus on spectrum allocation. We then apply transformations and modifications to make the problem computationally tractable, and develop an iterative algorithm based on successive convex approximation to determine the optimal sub-band bandwidth and the unused spectra at the edges of TWs. Using numerical results, we show that a significantly higher data rate can be achieved by changing the sub-band bandwidth, as compared to equal sub-band bandwidth. We also show that a further data rate gain can be obtained by optimally determining the unused spectra at the edges of TWs, as compared to avoiding using pre-defined spectra at the edges of TWs.      
### 14.Learning Apparent Diffusion Coefficient Maps from Undersampled Radial k-Space Diffusion-Weighted MRI in Mice using a Deep CNN-Transformer Model in Conjunction with a Monoexponential Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.02399.pdf)
>  Purpose: To accelerate radially sampled diffusion weighted spin-echo (Rad-DW-SE) acquisition method for generating high quality of apparent diffusion coefficient (ADC) maps. Methods: A deep learning method was developed to generate accurate ADC map reconstruction from undersampled DWI data acquired with the Rad-DW-SE method. The deep learning method integrates convolutional neural networks (CNNs) with vison transformers to generate high quality ADC maps from undersampled DWI data, regularized by a monoexponential ADC model fitting term. A model was trained on DWI data of 147 mice and evaluated on DWI data of 36 mice, with undersampling rates of 4x and 8x. Results: Ablation studies and experimental results have demonstrated that the proposed deep learning model can generate high quality ADC maps from undersampled DWI data, better than alternative deep learning methods under comparison, with their performance quantified on different levels of images, tumors, kidneys, and muscles. Conclusions: The deep learning method with integrated CNNs and transformers provides an effective means to accurately compute ADC maps from undersampled DWI data acquired with the Rad-DW-SE method.      
### 15.A Novel Hybrid Endoscopic Dataset for Evaluating Machine Learning-based Photometric Image Enhancement Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.02396.pdf)
>  Endoscopy is the most widely used medical technique for cancer and polyp detection inside hollow organs. However, images acquired by an endoscope are frequently affected by illumination artefacts due to the enlightenment source orientation. There exist two major issues when the endoscope's light source pose suddenly changes: overexposed and underexposed tissue areas are produced. These two scenarios can result in misdiagnosis due to the lack of information in the affected zones or hamper the performance of various computer vision methods (e.g., SLAM, structure from motion, optical flow) used during the non invasive examination. The aim of this work is two-fold: i) to introduce a new synthetically generated data-set generated by a generative adversarial techniques and ii) and to explore both shallow based and deep learning-based image-enhancement methods in overexposed and underexposed lighting conditions. Best quantitative results (i.e., metric based results), were obtained by the deep-learnnig-based LMSPEC method,besides a running time around 7.6 fps)      
### 16.AutoSpeed: A Linked Autoencoder Approach for Pulse-Echo Speed-of-Sound Imaging for Medical Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2207.02392.pdf)
>  Quantitative ultrasound, e.g., speed-of-sound (SoS) in tissues, provides information about tissue properties that have diagnostic value. Recent studies showed the possibility of extracting SoS information from pulse-echo ultrasound raw data (a.k.a. RF data) using deep neural networks that are fully trained on simulated data. These methods take sensor domain data, i.e., RF data, as input and train a network in an end-to-end fashion to learn the implicit mapping between the RF data domain and SoS domain. However, such networks are prone to overfitting to simulated data which results in poor performance and instability when tested on measured data. We propose a novel method for SoS mapping employing learned representations from two linked autoencoders. We test our approach on simulated and measured data acquired from human breast mimicking phantoms. We show that SoS mapping is possible using linked autoencoders. The proposed method has a Mean Absolute Percentage Error (MAPE) of 2.39% on the simulated data. On the measured data, the predictions of the proposed method are close to the expected values with MAPE of 1.1%. Compared to an end-to-end trained network, the proposed method shows higher stability and reproducibility.      
### 17.A Review of High-Performance Computing and Parallel Techniques Applied to Power Systems Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2207.02388.pdf)
>  The accelerating technological landscape and drive towards net-zero emission made the power system grow in scale and complexity. Serial computational approaches for grid planning and operation struggle to execute necessary calculations within reasonable times. Resorting to high-performance and parallel computing approaches has become paramount. Moreover, the ambitious plans for the future grid and IoT integration make a shift towards utilizing Cloud computing inevitable. This article recounts the dawn of parallel computation and its appearance in power system studies, reviewing the most recent literature and research on exploiting the available computational resources and technologies today. The article starts with a brief introduction to the field. The relevant hardware and paradigms are then explained thoroughly in this article providing a base for the reader to understand the literature. Later, parallel power system studies are reviewed, reciting the study development from older papers up to the 21st century, emphasizing the most impactful work of the last decade. The studies included system stability studies, state estimation and power system operation, and market optimization. The reviews are split into \ac{CPU} based,\ac{GPU} based, and Cloud-based studies. Finally, the state-of-the-art is discussed, highlighting the issue of standardization and the future of computation in power system studies.      
### 18.Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2207.02377.pdf)
>  The acquisition conditions for low-dose and high-dose CT images are usually different, so that the shifts in the CT numbers often occur. Accordingly, unsupervised deep learning-based approaches, which learn the target image distribution, often introduce CT number distortions and result in detrimental effects in diagnostic performance. To address this, here we propose a novel unsupervised learning approach for lowdose CT reconstruction using patch-wise deep metric learning. The key idea is to learn embedding space by pulling the positive pairs of image patches which shares the same anatomical structure, and pushing the negative pairs which have same noise level each other. Thereby, the network is trained to suppress the noise level, while retaining the original global CT number distributions even after the image translation. Experimental results confirm that our deep metric learning plays a critical role in producing high quality denoised images without CT number shift.      
### 19.TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2207.02327.pdf)
>  Diffusion MRI tractography is an advanced imaging technique for quantitative mapping of the brain's structural connectivity. Whole brain tractography (WBT) data contains over hundreds of thousands of individual fiber streamlines (estimated brain connections), and this data is usually parcellated to create compact representations for data analysis applications such as disease classification. In this paper, we propose a novel parcellation-free WBT analysis framework, TractoFormer, that leverages tractography information at the level of individual fiber streamlines and provides a natural mechanism for interpretation of results using the attention mechanism of transformers. TractoFormer includes two main contributions. First, we propose a novel and simple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber spatial relationships and any feature of interest that can be computed from individual fibers (such as FA or MD). Second, we design a network based on vision transformers (ViTs) that includes: 1) data augmentation to overcome model overfitting on small datasets, 2) identification of discriminative fibers for interpretation of results, and 3) ensemble learning to leverage fiber information from different brain regions. In a synthetic data experiment, TractoFormer successfully identifies discriminative fibers with simulated group differences. In a disease classification experiment comparing several methods, TractoFormer achieves the highest accuracy in classifying schizophrenia vs control. Discriminative fibers are identified in left hemispheric frontal and parietal superficial white matter regions, which have previously been shown to be affected in schizophrenia patients.      
### 20.A Deep Ensemble Learning Approach to Lung CT Segmentation for COVID-19 Severity Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2207.02322.pdf)
>  We present a novel deep learning approach to categorical segmentation of lung CTs of COVID-19 patients. Specifically, we partition the scans into healthy lung tissues, non-lung regions, and two different, yet visually similar, pathological lung tissues, namely, ground-glass opacity and consolidation. This is accomplished via a unique, end-to-end hierarchical network architecture and ensemble learning, which contribute to the segmentation and provide a measure for segmentation uncertainty. The proposed framework achieves competitive results and outstanding generalization capabilities for three COVID-19 datasets. Our method is ranked second in a public Kaggle competition for COVID-19 CT images segmentation. Moreover, segmentation uncertainty regions are shown to correspond to the disagreements between the manual annotations of two different radiologists. Finally, preliminary promising correspondence results are shown for our private dataset when comparing the patients' COVID-19 severity scores (based on clinical measures), and the segmented lung pathologies. Code and data are available at our repository: <a class="link-external link-https" href="https://github.com/talbenha/covid-seg" rel="external noopener nofollow">this https URL</a>      
### 21.Interference Aware Path Planning for Mobile Robots in mmWave Multi Cell Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.02275.pdf)
>  The emerging beyond 5G and envisioned 6G wireless networks are considered as key enablers in supporting a diversified set of applications for industrial mobile robots (MRs). The scenario under investigation in this paper relates to mobile robots that autonomously roam in an industrial floor and perform a variety of tasks at different locations whilst utilizing high directivity beamformers in mmWave small cells. In such scenarios, the potential close proximity of mobile robots connected to different base stations, may cause excessive levels of interference having as a net result a decrease in the overall achievable data rate in the network. To resolve this issue, a novel mixed integer linear programming formulation is proposed where the trajectory of the mobile robots is considered jointly with the interference level at different beam sectors. Therefore, creating a low interference path for each mobile robot in the industrial floor. A wide set of numerical investigations reveal that the proposed path planning optimization approach for the mmWave connected mobile robots can improve the overall achievable throughput by up to 31% compared to an interference oblivious scheme, without penalizing the overall travelling time.      
### 22.STOCHOS: Stochastic Opportunistic Maintenance Scheduling For Offshore Wind Farms  [ :arrow_down: ](https://arxiv.org/pdf/2207.02274.pdf)
>  Despite the promising outlook, the numerous economic and environmental benefits of offshore wind energy are still compromised by its high operations and maintenance (O&amp;M) expenditures. On one hand, offshore-specific challenges such as site remoteness, harsh weather, high transportation requirements, and production losses, significantly inflate the total O&amp;M expenses relative to land-based wind farms. On the other hand, the uncertainties in weather conditions, asset degradation, and electricity prices largely constrain the farm operator's ability to determine the time windows at which maintenance is possible, let alone optimal. In response, we propose STOCHOS, short for the stochastic holistic opportunistic scheduler--a maintenance scheduling approach tailored to address the unique challenges and uncertainties in offshore wind farms. Given probabilistic forecasts of key environmental and operational parameters, STOCHOS optimally schedules the offshore maintenance tasks by harnessing the maintenance opportunities that arise due to a combination of favorable weather conditions, on-site maintenance resources, and maximal operating revenues. STOCHOS is formulated as a two-stage stochastic mixed integer linear program, which we solve using a scenario-based rolling horizon algorithm that aligns with the industrial practice. Scenarios are generated using a probabilistic forecasting framework which adequately characterizes the temporal dependence in key input parameters. Evaluated on a real-world case study from the U.S. North Atlantic where several large-scale offshore wind farms are being developed, STOCHOS achieves considerable improvements relative to prevalent maintenance benchmarks, across several O&amp;M metrics, including total cost, downtime, resource utilization, and maintenance interruptions, attesting to its potential merit towards enabling the economic viability of offshore wind energy.      
### 23.A Survey of Recent Machine Learning Solutions for Ship Collision Avoidance and Mission Planning  [ :arrow_down: ](https://arxiv.org/pdf/2207.02767.pdf)
>  Machine Learning (ML) techniques have gained significant traction as a means of improving the autonomy of marine vehicles over the last few years. This article surveys the recent ML approaches utilised for ship collision avoidance (COLAV) and mission planning. Following an overview of the ever-expanding ML exploitation for maritime vehicles, key topics in the mission planning of ships are outlined. Notable papers with direct and indirect applications to the COLAV subject are technically reviewed and compared. Critiques, challenges, and future directions are also identified. The outcome clearly demonstrates the thriving research in this field, even though commercial marine ships incorporating machine intelligence able to perform autonomously under all operating conditions are still a long way off.      
### 24.Towards the Use of Saliency Maps for Explaining Low-Quality Electrocardiograms to End Users  [ :arrow_down: ](https://arxiv.org/pdf/2207.02726.pdf)
>  When using medical images for diagnosis, either by clinicians or artificial intelligence (AI) systems, it is important that the images are of high quality. When an image is of low quality, the medical exam that produced the image often needs to be redone. In telemedicine, a common problem is that the quality issue is only flagged once the patient has left the clinic, meaning they must return in order to have the exam redone. This can be especially difficult for people living in remote regions, who make up a substantial portion of the patients at Portal Telemedicina, a digital healthcare organization based in Brazil. In this paper, we report on ongoing work regarding (i) the development of an AI system for flagging and explaining low-quality medical images in real-time, (ii) an interview study to understand the explanation needs of stakeholders using the AI system at OurCompany, and, (iii) a longitudinal user study design to examine the effect of including explanations on the workflow of the technicians in our clinics. To the best of our knowledge, this would be the first longitudinal study on evaluating the effects of XAI methods on end-users -- stakeholders that use AI systems but do not have AI-specific expertise. We welcome feedback and suggestions on our experimental setup.      
### 25.Channel Estimation in RIS-Assisted MIMO Systems Operating Under Imperfections  [ :arrow_down: ](https://arxiv.org/pdf/2207.02700.pdf)
>  Reconfigurable intelligent surface is a potential technology component of future wireless networks due to its capability of shaping the wireless environment. The promising MIMO systems in terms of extended coverage and enhanced capacity are, however, critically dependent on the accuracy of the channel state information. However, traditional channel estimation schemes are not applicable in RIS-assisted MIMO networks, since passive RISs typically lack the signal processing capabilities that are assumed by channel estimation algorithms. This becomes most problematic when physical imperfections or electronic impairments affect the RIS due to its exposition to different environmental effects or caused by hardware limitations from the circuitry. While these real-world effects are typically ignored in the literature, in this paper we propose efficient channel estimation schemes for RIS-assisted MIMO systems taking different imperfections into account. Specifically, we propose two sets of tensor-based algorithms, based on the parallel factor analysis decomposition schemes. First, by assuming a long-term model in which the RIS imperfections, modeled as unknown phase shifts, are static within the channel coherence time we formulate an iterative alternating least squares (ALS)-based algorithm for the joint estimation of the communication channels and the unknown phase deviations. Next, we develop the short-term imperfection model, which allows both amplitude and phase RIS imperfections to be non-static with respect to the channel coherence time. We propose two iterative ALS-based and closed-form higher order singular value decomposition-based algorithms for the joint estimation of the channels and the unknown impairments. Moreover, we analyze the identifiability and computational complexity of the proposed algorithms and study the effects of various imperfections on the channel estimation quality.      
### 26.Optical fiber for remote sensing with high spatial resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.02683.pdf)
>  The use of optical fiber as sensor as well as transmission medium for sensing data is discussed, enabling the use of optically active sensors without power supply at distances of tens of kilometers. Depending on the interrogation system, a spatial resolution of less than a millimeter can be achieved. The basic sensing principle is optical time-domain reflectometry (OTDR) with direct detection or coherent detection of the Rayleigh back-scattered or Fresnel reflected signal. Spatial resolution is improved by a cross-correlation between the transmitted sequence and the received signals.      
### 27.Kaggle Competition: Cantonese Audio-Visual Speech Recognition for In-car Commands  [ :arrow_down: ](https://arxiv.org/pdf/2207.02663.pdf)
>  With the rise of deep learning and intelligent vehicles, the smart assistant has become an essential in-car component to facilitate driving and provide extra functionalities. In-car smart assistants should be able to process general as well as car-related commands and perform corresponding actions, which eases driving and improves safety. However, in this research field, most datasets are in major languages, such as English and Chinese. There is a huge data scarcity issue for low-resource languages, hindering the development of research and applications for broader communities. Therefore, it is crucial to have more benchmarks to raise awareness and motivate the research in low-resource languages. To mitigate this problem, we collect a new dataset, namely Cantonese In-car Audio-Visual Speech Recognition (CI-AVSR), for in-car speech recognition in the Cantonese language with video and audio data. Together with it, we propose Cantonese Audio-Visual Speech Recognition for In-car Commands as a new challenge for the community to tackle low-resource speech recognition under in-car scenarios.      
### 28.Reconfigurable Refractive Surfaces: An Energy-Efficient Way to Holographic MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2207.02662.pdf)
>  Holographic Multiple Input Multiple Output (HMIMO), which integrates massive antenna elements into a compact space to achieve a spatially continuous aperture, plays an important role in future wireless networks. With numerous antenna elements, it is hard to implement the HMIMO via phased arrays due to unacceptable power consumption. To address this issue, reconfigurable refractive surface (RRS) is an energy efficient enabler of HMIMO since the surface is free of expensive phase shifters. Unlike traditional metasurfaces working as passive relays, the RRS is used as transmit antennas, where the far-field approximation does not hold anymore, urging a new performance analysis framework. In this letter, we first derive the data rate of an RRS-based single-user downlink system, and then compare its power consumption with the phased array. Simulation results verify our analysis and show that the RRS is an energy-efficient way to HMIMO.      
### 29.Cooperative Beamforming for RIS-Aided Cell-Free Massive MIMO Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.02650.pdf)
>  The combination of cell-free massive multiple-input multiple-output (CF-mMIMO) and reconfigurable intelligent surface (RIS) is envisioned as a promising paradigm to improve network capacity and enhance coverage capability. However, to reap full benefits of RIS-aided CF-mMIMO, the main challenge is to efficiently design cooperative beamforming (CBF) at base stations (BSs), RISs, and users. Firstly, we investigate the fractional programing to convert the weighted sum-rate (WSR) maximization problem into a tractable optimization problem. Then, the alternating optimization framework is employed to decompose the transformed problem into a sequence of subproblems, i.e., hybrid BF (HBF) at BSs, passive BF at RISs, and combining at users. In particular, the alternating direction method of multipliers algorithm is utilized to solve the HBF subproblem at BSs. Concretely, the analog BF design with unit-modulus constraints is solved by the manifold optimization (MO) while we obtain a closed-form solution to the digital BF design that is essentially a convex least-square problem. Additionally, the passive BF at RISs and the analog combining at users are designed by primal-dual subgradient and MO methods. Moreover, considering heavy communication costs in conventional CF-mMIMO systems, we propose a partially-connected CF-mMIMO (P-CF-mMIMO) framework to decrease the number of connections among BSs and users. To better compromise WSR performance and network costs, we formulate the BS selection problem in the P-CF-mMIMO system as a binary integer quadratic programming (BIQP) problem, and develop a relaxed linear approximation algorithm to handle this BIQP problem. Finally, numerical results demonstrate superiorities of our proposed algorithms over baseline counterparts.      
### 30.Beam Selection for RIS-Enabled Terahertz Multi-User MIMO Systems via Multi-Task Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.02597.pdf)
>  Reconfigurable intelligent surface (RIS) and hybrid beamforming have been envisioned as promising alternatives to alleviate blockage vulnerability and enhance coverage capability for terahertz (THz) multi-user multiple-input multiple-output systems that suffer from severe propagation attenuation and poor diffraction. Considering that the joint beamforming with large-scale array elements at transceivers and RIS is extremely complicated, the codebook based beamforming can be employed in a computationally efficient manner. However, the codeword selection for analog beamforming is an intractable combinatorial optimization (CO) problem. To this end, an iterative alternating search (IAS) algorithm is developed to achieve a near-optimal sum-rate performance with low computational complexity in contrast with the optimal exhaustive search algorithm. According to the THz channel dataset generated by the IAS algorithm, a multi-task learning based analog beam selection (MTL-ABS) framework is developed to further decrease the computation overhead. Specifically, we take the CO problem as a multi-task classification problem and implement multiple beam selection tasks at transceivers and RIS simultaneously. Remarkably, residual network and self-attention mechanism are used to combat the network degradation and mine intrinsic THz channel features. Finally, blockwise convergence analysis and numerical results demonstrate the effectiveness of the MTL-ABS framework over search based counterparts.      
### 31.Cascaded Deep Hybrid Models for Multistep Household Energy Consumption Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2207.02589.pdf)
>  Sustainability requires increased energy efficiency with minimal waste. The future power systems should thus provide high levels of flexibility iin controling energy consumption. Precise projections of future energy demand/load at the aggregate and on the individual site levels are of great importance for decision makers and professionals in the energy industry. Forecasting energy loads has become more advantageous for energy providers and customers, allowing them to establish an efficient production strategy to satisfy demand. This study introduces two hybrid cascaded models for forecasting multistep household power consumption in different resolutions. The first model integrates Stationary Wavelet Transform (SWT), as an efficient signal preprocessing technique, with Convolutional Neural Networks and Long Short Term Memory (LSTM). The second hybrid model combines SWT with a self-attention based neural network architecture named transformer. The major constraint of using time-frequency analysis methods such as SWT in multistep energy forecasting problems is that they require sequential signals, making signal reconstruction problematic in multistep forecasting applications.The cascaded models can efficiently address this problem through using the recursive outputs. Experimental results show that the proposed hybrid models achieve superior prediction performance compared to the existing multistep power consumption prediction methods. The results will pave the way for more accurate and reliable forecasting of household power consumption.      
### 32.Nanoscale Reconfigurable Intelligent Surface Design and Performance Analysis for Terahertz Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.02563.pdf)
>  Terahertz (THz) communications have been envisioned as a promising enabler to provide ultra-high data transmission for sixth generation (6G) wireless networks. To tackle the blockage vulnerability brought by severe attenuation and poor diffraction of THz waves, a nanoscale reconfigurable intelligent surface (NRIS) is developed to smartly manipulate the propagation directions of incident THz waves. In this paper, the electric properties of the graphene are investigated by revealing the relationship between conductivity and applied voltages, and then an efficient hardware structure of electrically-controlled NRIS is designed based on Fabry-Perot resonance model. Particularly, the phase response of NRIS can be programmed up to 306.82 degrees. To analyze the hardware performance, we jointly design the passive and active beamforming for NRIS aided THz communication system. Particularly, an adaptive gradient descent (A-GD) algorithm is developed to optimize the phase shift matrix of NRIS by dynamically updating the step size during the iterative process. Finally, numerical results demonstrate the effectiveness of our designed hardware architecture as well as the developed algorithm.      
### 33.Axial and radial axonal diffusivities from single encoding strongly diffusion-weighted MRI  [ :arrow_down: ](https://arxiv.org/pdf/2207.02526.pdf)
>  We enable the estimation of the per-axon axial diffusivity from single encoding, strongly diffusion-weighted, pulsed gradient spin echo data. Additionally, we improve the estimation of the per-axon radial diffusivity compared to estimates based on spherical averaging. The use of strong diffusion weightings in magnetic resonance imaging (MRI) allows to approximate the signal in white matter as the sum of the contributions from axons. At the same time, spherical averaging leads to a major simplification of the modeling by removing the need to explicitly account for the unknown orientation distribution of axons. However, the spherically averaged signal acquired at strong diffusion weightings is not sensitive to the axial diffusivity, which cannot therefore be estimated. After revising existing theory, we introduce a new general method for the estimation of both axonal diffusivities at strong diffusion weightings based on zonal harmonics modeling. We additionally show how this could lead to estimates that are free from partial volume bias with, for instance, gray matter. We test the method on publicly available data from the MGH Adult Diffusion Human Connectome project dataset. We report reference values of axonal diffusivities based on 34 subjects, and derive estimates of axonal radii. We address the estimation problem also from the angle of the required data preprocessing, the presence of biases related to modeling assumptions, current limitations, and future possibilities.      
### 34.A Learning System for Motion Planning of Free-Float Dual-Arm Space Manipulator towards Non-Cooperative Object  [ :arrow_down: ](https://arxiv.org/pdf/2207.02464.pdf)
>  Recent years have seen the emergence of non-cooperative objects in space, like failed satellites and space junk. These objects are usually operated or collected by free-float dual-arm space manipulators. Thanks to eliminating the difficulties of modeling and manual parameter-tuning, reinforcement learning (RL) methods have shown a more promising sign in the trajectory planning of space manipulators. Although previous studies demonstrate their effectiveness, they cannot be applied in tracking dynamic targets with unknown rotation (non-cooperative objects). In this paper, we proposed a learning system for motion planning of free-float dual-arm space manipulator (FFDASM) towards non-cooperative objects. Specifically, our method consists of two modules. Module I realizes the multi-target trajectory planning for two end-effectors within a large target space. Next, Module II takes as input the point clouds of the non-cooperative object to estimate the motional property, and then can predict the position of target points on an non-cooperative object. We leveraged the combination of Module I and Module II to track target points on a spinning object with unknown regularity successfully. Furthermore, the experiments also demonstrate the scalability and generalization of our learning system.      
### 35.Highly accurate quantum optimization algorithm for CT image reconstructions based on sinogram patterns  [ :arrow_down: ](https://arxiv.org/pdf/2207.02448.pdf)
>  Computed tomography has been developed as a non-destructive technique for observing internal images of samples. It was difficult to obtain a clean CT image due to various artifacts generated during scanning and limitations of backprojection. Recently, an iterative optimization algorithm has been developed that uses the entire sinogram to make small errors in various artifacts. In this paper, we introduce a method of representing CT images using a combination of qubits. Each qubit variable can represent the internal structure of a real sample by energy optimization. We tested simple image samples in a quantum annealer and a gated model quantum computer.      
### 36.Compute Cost Amortized Transformer for Streaming ASR  [ :arrow_down: ](https://arxiv.org/pdf/2207.02393.pdf)
>  We present a streaming, Transformer-based end-to-end automatic speech recognition (ASR) architecture which achieves efficient neural inference through compute cost amortization. Our architecture creates sparse computation pathways dynamically at inference time, resulting in selective use of compute resources throughout decoding, enabling significant reductions in compute with minimal impact on accuracy. The fully differentiable architecture is trained end-to-end with an accompanying lightweight arbitrator mechanism operating at the frame-level to make dynamic decisions on each input while a tunable loss function is used to regularize the overall level of compute against predictive performance. We report empirical results from experiments using the compute amortized Transformer-Transducer (T-T) model conducted on LibriSpeech data. Our best model can achieve a 60% compute cost reduction with only a 3% relative word error rate (WER) increase.      
### 37.Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI  [ :arrow_down: ](https://arxiv.org/pdf/2207.02390.pdf)
>  Fast MRI aims to reconstruct a high fidelity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at <a class="link-external link-https" href="https://github.com/ayanglab/SDAUT" rel="external noopener nofollow">this https URL</a>.      
### 38.Saturation region of Freeway Networks under Safe Microscopic Ramp Metering  [ :arrow_down: ](https://arxiv.org/pdf/2207.02360.pdf)
>  We consider ramp metering at the microscopic level subject to vehicle safety constraint. The traffic network is abstracted by a ring road with multiple on- and off-ramps. The arrival times of vehicles to the on-ramps, as well as their destination off-ramps are modeled by exogenous stochastic processes. Once a vehicle is released from an on-ramp, it accelerates towards the free flow speed if it is not obstructed by another vehicle; once it gets close to another vehicle, it adopts a safe behavior. The vehicle exits the traffic network once it reaches its destination off-ramp. We design traffic-responsive ramp metering policies which maximize the saturation region of the network. The saturation region of a policy is defined as the set of demands, i.e., arrival rates and the routing matrix, for which the queue lengths at all the on-ramps remain bounded in expectation. The proposed ramp metering policies operate under synchronous cycles during which an on-ramp does not release more vehicles than its queue length at the beginning of the cycle. We provide three policies under which, respectively, each on-ramp (i) pauses release for a time-interval at the end of the cycle, or (ii) modulates the release rate during the cycle, or (iii) adopts a conservative safety criterion for release during the cycle. None of the policies, however, require information about the demand. The saturation region of these policies is characterized by studying stochastic stability of the induced Markov chains, and is proven to be maximal when the merging speed of all on-ramps equals the free flow speed. Simulations are provided to illustrate the performance of the policies.      
### 39.Can We Effectively Use Smart Contracts to Stipulate Time Constraints?  [ :arrow_down: ](https://arxiv.org/pdf/2207.02323.pdf)
>  Smart contracts provide the means to stipulate rules of interaction between mutually distrustful organizations. They encode contractual agreements on the basis of source code, which else need to be contractualized in natural language. While the mediation of contractual agreements via smart contracts is seamless in theory, it requires that the conditions of an interaction are accurately made available in the blockchain. Time is a prominent such condition. In the paper at hand, we empirically measure the consistency of a smart contract to yield equal results on the basis of the time of an interaction and its potentially inaccurate representation in the blockchain. We propose a novel metric called execution accuracy to measure this consistency. We specifically measure the execution accuracy of a time interval-constrained smart contract that executes distinct logic within and without some constraint interval. We run experiments for the local Ganache and Quorum and the public Görli and Rinkeby Ethereum blockchains. Our experiments confirm our intuition that execution accuracy decreases near interval bounds. The novelty of our proposed metric resides in its capacity to quantify this decrease and make distinct blockchains comparable with respect to their capacity to accurately stipulate time contraints.      
### 40.Effectivity of super resolution convolutional neural network for the enhancement of land cover classification from medium resolution satellite images  [ :arrow_down: ](https://arxiv.org/pdf/2207.02301.pdf)
>  In the modern world, satellite images play a key role in forest management and degradation monitoring. For a precise quantification of forest land cover changes, the availability of spatially fine resolution data is a necessity. Since 1972, NASAs LANDSAT Satellites are providing terrestrial images covering every corner of the earth, which have been proved to be a highly useful resource for terrestrial change analysis and have been used in numerous other sectors. However, freely accessible satellite images are, generally, of medium to low resolution which is a major hindrance to the precision of the analysis. Hence, we performed a comprehensive study to prove our point that, enhancement of resolution by Super-Resolution Convolutional Neural Network (SRCNN) will lessen the chance of misclassification of pixels, even under the established recognition methods. We tested the method on original LANDSAT-7 images of different regions of Sundarbans and their upscaled versions which were produced by bilinear interpolation, bicubic interpolation, and SRCNN respectively and it was discovered that SRCNN outperforms the others by a significant amount.      
### 41.Ultra-Low-Bitrate Speech Coding with Pretrained Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2207.02262.pdf)
>  Speech coding facilitates the transmission of speech over low-bandwidth networks with minimal distortion. Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches. While this new generation of codecs is capable of synthesizing high-fidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently. We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias. As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder. Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of $600\,\mathrm{bps}$ that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate. Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.      
### 42.Array Camera Image Fusion using Physics-Aware Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2207.02250.pdf)
>  We demonstrate a physics-aware transformer for feature-based data fusion from cameras with diverse resolution, color spaces, focal planes, focal lengths, and exposure. We also demonstrate a scalable solution for synthetic training data generation for the transformer using open-source computer graphics software. We demonstrate image synthesis on arrays with diverse spectral responses, instantaneous field of view and frame rate.      
### 43.State-Augmented Learnable Algorithms for Resource Management in Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.02242.pdf)
>  We consider resource management problems in multi-user wireless networks, which can be cast as optimizing a network-wide utility function, subject to constraints on the long-term average performance of users across the network. We propose a state-augmented algorithm for solving the aforementioned radio resource management (RRM) problems, where, alongside the instantaneous network state, the RRM policy takes as input the set of dual variables corresponding to the constraints, which evolve depending on how much the constraints are violated during execution. We theoretically show that the proposed state-augmented algorithm leads to feasible and near-optimal RRM decisions. Moreover, focusing on the problem of wireless power control using graph neural network (GNN) parameterizations, we demonstrate the superiority of the proposed RRM algorithm over baseline methods across a suite of numerical experiments.      
### 44.Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets  [ :arrow_down: ](https://arxiv.org/pdf/2207.02238.pdf)
>  The regulatory approval and broad clinical deployment of medical AI have been hampered by the perception that deep learning models fail in unpredictable and possibly catastrophic ways. A lack of statistically rigorous uncertainty quantification is a significant factor undermining trust in AI results. Recent developments in distribution-free uncertainty quantification present practical solutions for these issues by providing reliability guarantees for black-box models on arbitrary data distributions as formally valid finite-sample prediction intervals. Our work applies these new uncertainty quantification methods -- specifically conformal prediction -- to a deep-learning model for grading the severity of spinal stenosis in lumbar spine MRI. We demonstrate a technique for forming ordinal prediction sets that are guaranteed to contain the correct stenosis severity within a user-defined probability (confidence interval). On a dataset of 409 MRI exams processed by the deep-learning model, the conformal method provides tight coverage with small prediction set sizes. Furthermore, we explore the potential clinical applicability of flagging cases with high uncertainty predictions (large prediction sets) by quantifying an increase in the prevalence of significant imaging abnormalities (e.g. motion artifacts, metallic artifacts, and tumors) that could degrade confidence in predictive performance when compared to a random sample of cases.      
### 45.Transfer Learning for Rapid Extraction of Thickness from Optical Spectra of Semiconductor Thin Films  [ :arrow_down: ](https://arxiv.org/pdf/2207.02209.pdf)
>  High-throughput experimentation with autonomous workflows, increasingly used to screen and optimize optoelectronic thin films, requires matching throughput of downstream characterizations. Despite being essential, thickness characterization lags in throughput. Although optical spectroscopic methods, e.g., spectrophotometry, provide quick measurements, a critical bottleneck is the ensuing manual fitting of optical oscillation models to the measured reflection and transmission. This study presents a machine-learning (ML) framework called thicknessML, which rapidly extracts film thickness from spectroscopic reflection and transmission. thicknessML leverages transfer learning to generalize to materials of different underlying optical oscillator models (i.e., different material classes).We demonstrate that thicknessML can extract film thickness from six perovskite samples in a two-stage process: (1) pre-training on a generic simulated dataset of Tauc-Lorentz oscillator, and (2) transfer learning to a simulated perovskite dataset of several literature perovskite refractive indices. Results show a pre-training thickness mean absolute percentage error (MAPE) of 5-7% and an experimental thickness MAPE of 6-19%.      
### 46.ImageBox3: No-Server Tile Serving to Traverse Whole Slide Images on the Web  [ :arrow_down: ](https://arxiv.org/pdf/2207.01734.pdf)
>  Whole slide imaging (WSI) has become the primary modality for digital pathology data. However, due to the size and high-resolution nature of these images, they are generally only accessed in smaller sections or tiles via specialized platforms, most of which require extensive setup and/or costly infrastructure. These platforms typically also need a copy of the images to be locally available to them, potentially causing issues with data governance and provenance. To address these concerns, we developed ImageBox3, an in-browser tiling mechanism to enable zero-footprint traversal of remote WSI data. All computation is performed client-side without compromising user governance, operating public and private images alike as long as the storage service supports HTTP range requests (standard in Cloud storage and most web servers). ImageBox3 thus removes significant hurdles to WSI operation and effective collaboration, allowing for the sort of democratized analytical tools needed to establish participative, FAIR digital pathology data commons. <br>Availability: <br>code - <a class="link-external link-https" href="https://github.com/episphere/imagebox3;" rel="external noopener nofollow">this https URL</a> <br>fig1 (live) - <a class="link-external link-https" href="https://episphere.github.io/imagebox3/demo/scriptTag" rel="external noopener nofollow">this https URL</a> ; <br>fig2 (live) - <a class="link-external link-https" href="https://episphere.github.io/imagebox3/demo/serviceWorker" rel="external noopener nofollow">this https URL</a> ; <br>fig 3 (live) - <a class="link-external link-https" href="https://observablehq.com/@prafulb/imagebox3-in-observable" rel="external noopener nofollow">this https URL</a> .      
