# ArXiv eess --Thu, 21 Jul 2022
### 1.Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.10040.pdf)
>  Image restoration algorithms for atmospheric turbulence are known to be much more challenging to design than traditional ones such as blur or noise because the distortion caused by the turbulence is an entanglement of spatially varying blur, geometric distortion, and sensor noise. Existing CNN-based restoration methods built upon convolutional kernels with static weights are insufficient to handle the spatially dynamical atmospheric turbulence effect. To address this problem, in this paper, we propose a physics-inspired transformer model for imaging through atmospheric turbulence. The proposed network utilizes the power of transformer blocks to jointly extract a dynamical turbulence distortion map and restore a turbulence-free image. In addition, recognizing the lack of a comprehensive dataset, we collect and present two new real-world turbulence datasets that allow for evaluation with both classical objective metrics (e.g., PSNR and SSIM) and a new task-driven metric using text recognition accuracy. Both real testing sets and all related code will be made publicly available.      
### 2.Phenomenon-Signal Model: Formalisation, Graph and Application  [ :arrow_down: ](https://arxiv.org/pdf/2207.09996.pdf)
>  Considering information as the basis of action, it may be of interest to examine the flow and acquisition of information between the actors in traffic. The central question is: Which signals does an automated driving system (which will be referred to as an automaton in the remainder of this paper) in traffic have to receive, decode or send in road traffic in order to act safely and in a manner that is compliant with valid standards. The phenomenon-signal model (PSM) is a method for structuring the problem area and for analysing and describing this very signal flow. The aim of this paper is to explain the basics, the structure and the application of this method.      
### 3.Implementacion de un sistema IoT de bajo costo para el monitoreo de la calidad del aire en El Salvador  [ :arrow_down: ](https://arxiv.org/pdf/2207.09975.pdf)
>  Environmental pollution is a factor that represents a significant health risk. In El Salvador, The entity in charge of monitoring air quality is the ministry of Environment and Natural Resources, currently said ministry only has 3 stations for monitoring air quality throughout the territory of the country. The main objective of this work was the application of internet of things techniques in the design and implementation of a station of remote particulate pollution monitoring in the surrounding air. As well as setting up an Internet of Things (IoT) platform for the deployment of the data collected by the remote station. The development methodology of this research was based on the Model of Reference Architecture for IoT systems, which has as based on the development of system prototypes based on the correct choice of components available and suitable for the specific environment or application. For this case, they were used as inputs for the electronic hardware and Esp32 controller together with a Wemos-Lolin development board together with a sensor of particulate matter contamination PMS5003; by the side of software for the IoT platform was implemented a storage system, graphics and website based on low-cost tools The main result obtained in this work was an IoT prototype of an electronic station that allows monitoring the levels of contamination by the material of particles in the environment, whose data are accessible from any device with internet access through a site web, another result of this work is the configuring an IoT platform or cloud to connect wireless with the electronic station, the storage of the data produced by it and a web visualization stage.      
### 4.Decentralized Bandits with Feedback for Cognitive Radar Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.09904.pdf)
>  Completely decentralized Multi-Player Bandit models have demonstrated high localization accuracy at the cost of long convergence times in cognitive radar networks. Rather than model each radar node as an independent learner, entirely unable to swap information with other nodes in a network, in this work we construct a "central coordinator" to facilitate the exchange of information between radar nodes. We show that in interference-limited spectrum, where the signal to interference plus noise (SINR) ratio for the available bands may vary by location, a cognitive radar network (CRN) is able to use information from a central coordinator to reduce the number of time steps required to attain a given localization error. Importantly, each node is still able to learn separately. We provide a description of a network which has hybrid cognition in both a central coordinator and in each of the cognitive radar nodes, and examine the online machine learning algorithms which can be implemented in this structure.      
### 5.${\mathcal K}$-monotonicity and feedback synthesis for incrementally stable networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.09828.pdf)
>  We discuss the role of monotonicity in enabling numerically tractable modular control design for networked nonlinear systems. We first show that the variational systems of monotone systems can be embedded into positive systems. Utilizing this embedding, we show how to solve a network stabilization problem by enforcing monotonicity and exponential dissipativity of the network sub-components. Such modular approach leads to a design algorithm based on a sequence of linear programming problems.      
### 6.Operating Envelopes under Probabilistic Electricity Demand and Solar Generation Forecasts  [ :arrow_down: ](https://arxiv.org/pdf/2207.09818.pdf)
>  The increasing penetration of distributed energy resources in low-voltage networks is turning end-users from consumers to prosumers. However, the incomplete smart meter rollout and paucity of smart meter data due to the regulatory separation between retail and network service provision make active distribution network management difficult. Furthermore, distribution network operators oftentimes do not have access to real-time smart meter data, which creates an additional challenge. For the lack of better solutions, they use blanket rooftop solar export limits, leading to suboptimal outcomes. To address this, we designed a conditional generative adversarial network (CGAN)-based model to forecast household solar generation and electricity demand, which serves as an input to chance-constrained optimal power flow used to compute fair operating envelopes under uncertainty.      
### 7.Anomaly Detection of Smart Metering System for Power Management with Battery Storage System/Electric Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2207.09784.pdf)
>  A novel smart metering technique capable of anomaly detection was proposed for real-time home power management system. Smart meter data generated in real-time was obtained from 900 households of single apartments. To detect outliers and missing values in smart meter data, a deep learning model, the autoencoder, consisting of a graph convolutional network and bidirectional long short-term memory network, was applied to the smart metering technique. Power management based on the smart metering technique was performed by multi-objective optimization in the presence of a battery storage system and an electric vehicle. The results of the power management employing the proposed smart metering technique indicate a reduction in electricity cost and amount of power supplied by the grid compared to the results of power management without anomaly detection.      
### 8.Stabilised Inverse Flowline Evolution for Anisotropic Image Sharpening  [ :arrow_down: ](https://arxiv.org/pdf/2207.09779.pdf)
>  The central limit theorem suggests Gaussian convolution as a generic blur model for images. Since Gaussian convolution is equivalent to homogeneous diffusion filtering, one way to deblur such images is to diffuse them backwards in time. However, backward diffusion is highly ill-posed. Thus, it requires stabilisation in the model as well as highly sophisticated numerical algorithms. Moreover, sharpening is often only desired across image edges but not along them, since it may cause very irregular contours. This creates the need to model a stabilised anisotropic backward evolution and to devise an appropriate numerical algorithm for this ill-posed process. <br>We address both challenges. First we introduce stabilised inverse flowline evolution (SIFE) as an anisotropic image sharpening flow. Outside extrema, its partial differential equation (PDE) is backward parabolic in gradient direction. Interestingly, it is sufficient to stabilise it in extrema by imposing a zero flow there. We show that morphological derivatives - which are not common in the numerics of PDEs - are ideal for the numerical approximation of SIFE: They effortlessly approximate directional derivatives in gradient direction. Our scheme adapts one-sided morphological derivatives to the underlying image structure. It allows to progress in subpixel accuracy and enables us to prove stability properties. Our experiments show that SIFE allows nonflat steady states and outperforms other sharpening flows.      
### 9.Towards Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription  [ :arrow_down: ](https://arxiv.org/pdf/2207.09747.pdf)
>  Automatic speech recognition (ASR) has progressed significantly in recent years due to large-scale datasets and the paradigm of self-supervised learning (SSL) methods. However, as its counterpart problem in the singing domain, automatic lyric transcription (ALT) suffers from limited data and degraded intelligibility of sung lyrics, which has caused it to develop at a slower pace. To fill in the performance gap between ALT and ASR, we attempt to exploit the similarities between speech and singing. In this work, we propose a transfer-learning-based ALT solution that takes advantage of these similarities by adapting wav2vec 2.0, an SSL ASR model, to the singing domain. We maximize the effectiveness of transfer learning by exploring the influence of different transfer starting points. We further enhance the performance by extending the original CTC model to a hybrid CTC/attention model. Our method surpasses previous approaches by a large margin on various ALT benchmark datasets. Further experiment shows that, with even a tiny proportion of training data, our method still achieves competitive performance.      
### 10.Benchmarking tools for a priori identifiability analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.09745.pdf)
>  The structural identifiability and the observability of a model determine the possibility of inferring its parameters and states by observing its outputs. These properties should be analysed before attempting to calibrate a model. Unfortunately, such \textit{a priori} analysis can be challenging, since it requires symbolic calculations that often have a high computational cost. In recent years a number of software tools have been developed for this task, mostly in the systems biology community but also in other disciplines. These tools have vastly different features and capabilities, and a critical assessment of their performance is still lacking. Here we present a comprehensive study of the computational resources available for analysing structural identifiability. We consider 12 software tools developed in 7 programming languages (Matlab, Maple, Mathematica, Julia, Python, Reduce, and Maxima), and evaluate their performance using a set of 25 case studies created from 21 models. Our results reveal their strengths and weaknesses, provide guidelines for choosing the most appropriate tool for a given problem, and highlight opportunities for future developments.      
### 11.Interpreting Latent Spaces of Generative Models for Medical Images using Unsupervised Methods  [ :arrow_down: ](https://arxiv.org/pdf/2207.09740.pdf)
>  Generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) play an increasingly important role in medical image analysis. The latent spaces of these models often show semantically meaningful directions corresponding to human-interpretable image transformations. However, until now, their exploration for medical images has been limited due to the requirement of supervised data. Several methods for unsupervised discovery of interpretable directions in GAN latent spaces have shown interesting results on natural images. This work explores the potential of applying these techniques on medical images by training a GAN and a VAE on thoracic CT scans and using an unsupervised method to discover interpretable directions in the resulting latent space. We find several directions corresponding to non-trivial image transformations, such as rotation or breast size. Furthermore, the directions show that the generative models capture 3D structure despite being presented only with 2D data. The results show that unsupervised methods to discover interpretable directions in GANs generalize to VAEs and can be applied to medical images. This opens a wide array of future work using these methods in medical image analysis.      
### 12.Optimized processing order for 3D hole filling in video sequences using frequency selective extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09737.pdf)
>  A problem often arising in video communication is the reconstruction of missing or distorted areas in a video sequence. Such holes of unavailable pixels may be caused for example by transmission errors of coded video data or undesired objects like logos. In order to close the holes given neighboring available content, a signal extrapolation has to be performed. The best quality can be achieved, if spatial as well as temporal information is used for the reconstruction. However, the question always is in which order to process the extrapolation to obtain the best result. In this paper, an optimized processing order is introduced for improving the extrapolation quality of Three-dimensional Frequency Selective Extrapolation. Using the proposed optimized order, holes in video sequences can be closed from the outer margin to the center, leading to a higher reconstruction quality, and visually noticeable gains of more than 0.5 dB PSNR are possible.      
### 13.Direct and Residual Subspace Decomposition of Spatial Room Impulse Responses  [ :arrow_down: ](https://arxiv.org/pdf/2207.09733.pdf)
>  Psychoacoustic experiments have shown that directional properties of, in particular, the direct sound, salient reflections, and the late reverberation of an acoustic room response can have a distinct influence on the auditory perception of a given room. Spatial room impulse responses (SRIRs) capture those properties and thus are used for direction-dependent room acoustic analysis and virtual acoustic rendering. This work proposes a subspace method that decomposes SRIRs into a direct part, which comprises the direct sound and the salient reflections, and a residual, to facilitate enhanced analysis and rendering methods by providing individual access to these components. The proposed method is based on the generalized singular value decomposition and interprets the residual as noise that is to be separated from the other components of the reverberation. It utilizes a noise estimate to identify large generalized singular values, which are then attributed to the direct part. By advancing from the end of the SRIR toward the beginning while iteratively updating the noise estimate, the method is able to work with anisotropic and slowly time-varying reverberant sound fields. The proposed method does not require direction-of-arrival estimation of reflections and shows an improved separation of the direct part from the residual compared to an existing approach. A case study with measured SRIRs suggests a high robustness of the method under different acoustic conditions. A reference implementation is provided.      
### 14.Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2207.09732.pdf)
>  The amount of audio data available on public websites is growing rapidly, and an efficient mechanism for accessing the desired data is necessary. We propose a content-based audio retrieval method that can retrieve a target audio that is similar to but slightly different from the query audio by introducing auxiliary textual information which describes the difference between the query and target audio. While the range of conventional content-based audio retrieval is limited to audio that is similar to the query audio, the proposed method can adjust the retrieval range by adding an embedding of the auxiliary text query-modifier to the embedding of the query sample audio in a shared latent space. To evaluate our method, we built a dataset comprising two different audio clips and the text that describes the difference. The experimental results show that the proposed method retrieves the paired audio more accurately than the baseline. We also confirmed based on visualization that the proposed method obtains the shared latent space in which the audio difference and the corresponding text are represented as similar embedding vectors.      
### 15.Spatio-temporal prediction in video coding by non-local means refined motion compensation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09729.pdf)
>  The prediction step is a very important part of hybrid video codecs. In this contribution, a novel spatio-temporal prediction algorithm is introduced. For this, the prediction is carried out in two steps. Firstly, a preliminary temporal prediction is conducted by motion compensation. Afterwards, spatial refinement is carried out for incorporating spatial redundancies from already decoded neighboring blocks. Thereby, the spatial refinement is achieved by applying Non-Local Means de-noising to the union of the motion compensated block and the already decoded blocks. Including the spatial refinement into H.264/AVC, a rate reduction of up to 14 % or respectively a gain of up to 0.7 dB PSNR compared to unrefined motion compensated prediction can be achieved.      
### 16.Spatio-temporal prediction in video coding by best approximation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09727.pdf)
>  Within the scope of this contribution we propose a novel efficient spatio-temporal prediction algorithm for video coding. The algorithm operates in two stages. First, motion compensation is performed on the block to be predicted in order to exploit temporal correlations. Afterwards, in order to exploit spatial correlations, this preliminary estimate is spatially refined by forming a joint model of the motion compensated block and spatially adjacent already decoded blocks. Compared to an earlier refinement algorithm, the novel one only needs very little iteration, leading to a speedup of factor 17. The implementation of this new algorithm into the H.264/AVC leads to a maximum reduction in data rate of up to nearly 13% for the considered sequences.      
### 17.Orthogonality Deficiency Compensation for Improved Frequency Selective Image Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09724.pdf)
>  This paper describes a very efficient algorithm for image signal extrapolation. It can be used for various applications in image and video communication, e.g. the concealment of data corrupted by transmission errors or prediction in video coding. The extrapolation is performed on a limited number of known samples and extends the signal beyond these samples. Therefore the signal from the known samples is iteratively projected onto different basis functions in order to generate a model of the signal. As the basis functions are not orthogonal with respect to the area of the known samples we propose a new extension, the orthogonality deficiency compensation, to cope with the non-orthogonality. Using this extension, very good extrapolation results for structured as well as for smooth areas are achievable. This algorithm improves PSNR up to 2 dB and gives a better visual quality for concealment of block losses compared to extrapolation algorithms existent so far.      
### 18.Lesion detection in contrast enhanced spectral mammography  [ :arrow_down: ](https://arxiv.org/pdf/2207.09692.pdf)
>  Background \&amp; purpose: The recent emergence of neural networks models for the analysis of breast images has been a breakthrough in computer aided diagnostic. This approach was not yet developed in Contrast Enhanced Spectral Mammography (CESM) where access to large databases is complex. This work proposes a deep-learning-based Computer Aided Diagnostic development for CESM recombined images able to detect lesions and classify cases. Material \&amp; methods: A large CESM diagnostic dataset with biopsy-proven lesions was collected from various hospitals and different acquisition systems. The annotated data were split on a patient level for the training (55%), validation (15%) and test (30%) of a deep neural network with a state-of-the-art detection architecture. Free Receiver Operating Characteristic (FROC) was used to evaluate the model for the detection of 1) all lesions, 2) biopsied lesions and 3) malignant lesions. ROC curve was used to evaluate breast cancer classification. The metrics were finally compared to clinical results. Results: For the evaluation of the malignant lesion detection, at high sensitivity (Se&gt;0.95), the false positive rate was at 0.61 per image. For the classification of malignant cases, the model reached an Area Under the Curve (AUC) in the range of clinical CESM diagnostic results. Conclusion: This CAD is the first development of a lesion detection and classification model for CESM images. Trained on a large dataset, it has the potential to be used for helping the management of biopsy decision and for helping the radiologist detecting complex lesions that could modify the clinical treatment.      
### 19.Optimal Path Planning for Connected and Automated Vehicles in Lane-free Traffic with Vehicle Nudging  [ :arrow_down: ](https://arxiv.org/pdf/2207.09670.pdf)
>  The paper presents a movement strategy for Connected and Automated Vehicles (CAVs) in a lane-free traffic environment with vehicle nudging by use of an optimal control approach. State-dependent constraints on control inputs are considered to ensure that the vehicle moves within the road boundaries and to prevent collisions. An objective function, comprising various sub-objectives, is designed, whose minimization leads to vehicle advancement at the desired speed, whenever possible, while avoiding obstacles. A nonlinear optimal control problem (OCP) is formulated for the minimization of the objective function subject to constraints for each vehicle. A computationally efficient Feasible Direction Algorithm (FDA) is called, on event-triggered basis, to compute in real time the numerical solution for finite time horizons within a Model Predictive Control (MPC) framework. The approach is applied to each vehicle on the road, while running simulations on a lane-free ring-road, for a wide range of vehicle densities and different types of vehicles. From the simulations, which create countless driving episodes for each involved vehicle, it is observed that the proposed approach is highly efficient in delivering safe, comfortable and efficient vehicle trajectories, as well as high traffic flow outcomes. The approach is under investigation for further use in various lane-free road infrastructures for CAV traffic.      
### 20.Deep Learning Based Automatic Modulation Recognition: Models, Datasets, and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2207.09647.pdf)
>  Automatic modulation recognition (AMR) detects the modulation scheme of the received signals for further signal processing without needing prior information, and provides the essential function when such information is missing. Recent breakthroughs in deep learning (DL) have laid the foundation for developing high-performance DL-AMR approaches for communications systems. Comparing with traditional modulation detection methods, DL-AMR approaches have achieved promising performance including high recognition accuracy and low false alarms due to the strong feature extraction and classification abilities of deep neural networks. Despite the promising potential, DL-AMR approaches also bring concerns to complexity and explainability, which affect the practical deployment in wireless communications systems. This paper aims to present a review of the current DL-AMR research, with a focus on appropriate DL models and benchmark datasets. We further provide comprehensive experiments to compare the state of the art models for single-input-single-output (SISO) systems from both accuracy and complexity perspectives, and propose to apply DL-AMR in the new multiple-input-multiple-output (MIMO) scenario with precoding. Finally, existing challenges and possible future research directions are discussed.      
### 21.Lens Antenna Arrays-Assisted mmWave MU-MIMO Uplink Transmission: Joint Beam Selection and Phase-Only Beamforming Design  [ :arrow_down: ](https://arxiv.org/pdf/2207.09636.pdf)
>  This paper considers a lens antenna array-assisted millimeter wave (mmWave) multiuser multiple-input multiple-output (MU-MIMO) system. The base station's beam selection matrix and user terminals' phase-only beamformers are jointly designed with the aim of maximizing the uplink sum rate. In order to deal with the formulated mixed-integer optimization problem, a penalty dual decomposition (PDD)-based iterative algorithm is developed via capitalizing on the weighted minimum mean square error (WMMSE), block coordinate descent (BCD), and minorization-maximization (MM) techniques. Moreover, a low-complexity sequential optimization (SO)-based algorithm is proposed at the cost of a slight sum rate performance loss. Numerical results demonstrate that the proposed methods can achieve higher sum rates than state-of-the-art methods.      
### 22.Segmentation of 3D Dental Images Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.09582.pdf)
>  3D image segmentation is a recent and crucial step in many medical analysis and recognition schemes. In fact, it represents a relevant research subject and a fundamental challenge due to its importance and influence. This paper provides a multi-phase Deep Learning-based system that hybridizes various efficient methods in order to get the best 3D segmentation output. First, to reduce the amount of data and accelerate the processing time, the application of Decimate compression technique is suggested and justified. We then use a CNN model to segment dental images into fifteen separated classes. In the end, a special KNN-based transformation is applied for the purpose of removing isolated meshes and of correcting dental forms. Experimentations demonstrate the precision and the robustness of the selected framework applied to 3D dental images within a private clinical benchmark.      
### 23.Low Complexity First: Duration-Centric ISI Mitigation in Molecular Communication via Diffusion  [ :arrow_down: ](https://arxiv.org/pdf/2207.09565.pdf)
>  In this paper, we propose a novel inter-symbol interference (ISI) mitigation scheme for molecular communication via diffusion (MCvD) systems with the optimal detection interval. Its rationale is to exploit the discarded duration (i.e., the symbol duration outside this optimal interval) to relieve ISI in the target system. Following this idea, we formulate an objective function to quantify the impact of the discarded time on bit error rate (BER) performance. Besides, an optimally reusable interval within the discarded duration is derived in closed form, which applies to both the absorbing and passive receivers. Finally, numerical results validate our analysis and show that for the considered MCvD system, significant BER improvements can be achieved by using the derived reusable duration.      
### 24.Unified Grid-Forming Control of PMSG Wind Turbines for Fast Frequency Response and MPPT  [ :arrow_down: ](https://arxiv.org/pdf/2207.09536.pdf)
>  In this work we present a novel dual-port grid-forming control strategy, for permanent magnet synchronous generator wind turbines with back-to-back voltage source converters, that unifies the entire range of functions from maximum power point tracking (MPPT) to providing inertia and fast frequency response without explicit mode switching between grid-following and grid-forming control. The controls impose a well-defined AC voltage at the grid-side converter (GSC) and the machine-side converter (MSC) AC terminals and explicitly stabilize the DC-link capacitor voltage through both GSC and MSC. The wind turbine's kinetic energy storage and curtailment are adjusted through a combination of implicit rotor speed control and pitch angle control and directly determine the operating mode and level of grid support. Moreover, we provide analytical small-signal stability conditions for a simplified system and explicitly characterize the relationship between control gains, curtailment, and the wind turbines steady-state response. Finally, a detailed simulation study is used to validate the results and compared the proposed control with state-of-the-art MPPT control.      
### 25.Chance-Constrained AC Optimal Power Flow for Unbalanced Distribution Grids  [ :arrow_down: ](https://arxiv.org/pdf/2207.09520.pdf)
>  The growing penetration of distributed energy resources (DERs) is leading to continually changing operating conditions, which need to be managed efficiently by distribution grid operators. The intermittent nature of DERs such as solar photovoltaic (PV) systems as well as load forecasting errors not only increase uncertainty in the grid, but also pose significant power quality challenges such as voltage unbalance and voltage magnitude violations. This paper leverages a chance-constrained optimization approach to reduce the impact of uncertainty on distribution grid operation. We first present the chance-constrained optimal power flow (CC-OPF) problem for distribution grids and discuss a reformulation based on constraint tightening that does not require any approximations or relaxations of the three-phase AC power flow equations. We then propose two iterative solution algorithms capable of efficiently solving the reformulation. In the case studies, the performance of both algorithms is analyzed by running simulations on the IEEE 13-bus test feeder using real PV and load measurement data. The simulation results indicate that both methods are able to enforce the chance constraints in in- and out-of-sample evaluations.      
### 26.ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2207.09514.pdf)
>  This paper presents recent progress on integrating speech separation and enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE work, numerous features have been added, including recent state-of-the-art speech enhancement models with their respective training and evaluation recipes. Importantly, a new interface has been designed to flexibly combine speech enhancement front-ends with other tasks, including automatic speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). To showcase such integration, we performed experiments on carefully designed synthetic datasets for noisy-reverberant multi-channel ST and SLU tasks, which can be used as benchmark corpora for future research. In addition to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and single-channel SE approaches. Results show that the integration of SE front-ends with back-end tasks is a promising research direction even for tasks besides ASR, especially in the multi-channel scenario. The code is available online at <a class="link-external link-https" href="https://github.com/ESPnet/ESPnet" rel="external noopener nofollow">this https URL</a>. The multi-channel ST and SLU datasets, which are another contribution of this work, are released on HuggingFace.      
### 27.Comparison of automatic prostate zones segmentation models in MRI images using U-net-like architectures  [ :arrow_down: ](https://arxiv.org/pdf/2207.09483.pdf)
>  Prostate cancer is the second-most frequently diagnosed cancer and the sixth leading cause of cancer death in males worldwide. The main problem that specialists face during the diagnosis of prostate cancer is the localization of Regions of Interest (ROI) containing a tumor tissue. Currently, the segmentation of this ROI in most cases is carried out manually by expert doctors, but the procedure is plagued with low detection rates (of about 27-44%) or overdiagnosis in some patients. Therefore, several research works have tackled the challenge of automatically segmenting and extracting features of the ROI from magnetic resonance images, as this process can greatly facilitate many diagnostic and therapeutic applications. However, the lack of clear prostate boundaries, the heterogeneity inherent to the prostate tissue, and the variety of prostate shapes makes this process very difficult to <a class="link-external link-http" href="http://automate.In" rel="external noopener nofollow">this http URL</a> this work, six deep learning models were trained and analyzed with a dataset of MRI images obtained from the Centre Hospitalaire de Dijon and Universitat Politecnica de Catalunya. We carried out a comparison of multiple deep learning models (i.e. U-Net, Attention U-Net, Dense-UNet, Attention Dense-UNet, R2U-Net, and Attention R2U-Net) using categorical cross-entropy loss function. The analysis was performed using three metrics commonly used for image segmentation: Dice score, Jaccard index, and mean squared error. The model that give us the best result segmenting all the zones was R2U-Net, which achieved 0.869, 0.782, and 0.00013 for Dice, Jaccard and mean squared error, respectively.      
### 28.Automatic Segmentation of Coronal Holes in Solar Images and Solar Prediction Map Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.10070.pdf)
>  Solar image analysis relies on the detection of coronal holes for predicting disruptions to earth's magnetic field. The coronal holes act as sources of solar wind that can reach the earth. Thus, coronal holes are used in physical models for predicting the evolution of solar wind and its potential for interfering with the earth's magnetic field. Due to inherent uncertainties in the physical models, there is a need for a classification system that can be used to select the physical models that best match the observed coronal holes. <br>The physical model classification problem is decomposed into three subproblems. First, he thesis develops a method for coronal hole segmentation. Second, the thesis develops methods for matching coronal holes from different maps. Third, based on the matching results, the thesis develops a physical map classification system. <br>A level-set segmentation method is used for detecting coronal holes that are observed in extreme ultra-violet images (EUVI) and magnetic field images. For validating the segmentation approach, two independent manual segmentations were combined to produce 46 consensus maps. Overall, the level-set segmentation approach produces significant improvements over current approaches. <br>Physical map classification is based on coronal hole matching between the physical maps and (i) the consensus maps (semi-automated), or (ii) the segmented maps (fully-automated). Based on the matching results, the system uses area differences,shortest distances between matched clusters, number and areas of new and missing coronal hole clusters to classify each map. The results indicate that the automated segmentation and classification system performs better than individual humans.      
### 29.Fine-grained Early Frequency Attention for Deep Speaker Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.10006.pdf)
>  Attention mechanisms have emerged as important tools that boost the performance of deep models by allowing them to focus on key parts of learned embeddings. However, current attention mechanisms used in speaker recognition tasks fail to consider fine-grained information items such as frequency bins in input spectral representations used by the deep networks. To address this issue, we propose the novel Fine-grained Early Frequency Attention (FEFA) for speaker recognition in-the-wild. Once integrated into a deep neural network, our proposed mechanism works by obtaining queries from early layers of the network and generating learnable weights to attend to information items as small as the frequency bins in the input spectral representations. To evaluate the performance of FEFA, we use several well-known deep models as backbone networks and integrate our attention module in their pipelines. The overall performance of these networks (with and without FEFA) are evaluated on the VoxCeleb1 dataset, where we observe considerable improvements when FEFA is used.      
### 30.Diffsound: Discrete Diffusion Model for Text-to-sound Generation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09983.pdf)
>  Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 \textit{v.s} 2.786, and the generation speed is five times faster than the AR decoder.      
### 31.Telepresence Video Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2207.09956.pdf)
>  Video conferencing, which includes both video and audio content, has contributed to dramatic increases in Internet traffic, as the COVID-19 pandemic forced millions of people to work and learn from home. Global Internet traffic of video conferencing has dramatically increased Because of this, efficient and accurate video quality tools are needed to monitor and perceptually optimize telepresence traffic streamed via Zoom, Webex, Meet, etc. However, existing models are limited in their prediction capabilities on multi-modal, live streaming telepresence content. Here we address the significant challenges of Telepresence Video Quality Assessment (TVQA) in several ways. First, we mitigated the dearth of subjectively labeled data by collecting ~2k telepresence videos from different countries, on which we crowdsourced ~80k subjective quality labels. Using this new resource, we created a first-of-a-kind online video quality prediction framework for live streaming, using a multi-modal learning framework with separate pathways to compute visual and audio quality predictions. Our all-in-one model is able to provide accurate quality predictions at the patch, frame, clip, and audiovisual levels. Our model achieves state-of-the-art performance on both existing quality databases and our new TVQA database, at a considerably lower computational expense, making it an attractive solution for mobile and embedded systems.      
### 32.A Secure Clustering Protocol with Fuzzy Trust Evaluation and Outlier Detection for Industrial Wireless Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.09936.pdf)
>  Security is one of the major concerns in Industrial Wireless Sensor Networks (IWSNs). To assure the security in clustered IWSNs, this paper presents a secure clustering protocol with fuzzy trust evaluation and outlier detection (SCFTO). Firstly, to deal with the transmission uncertainty in an open wireless medium, an interval type-2 fuzzy logic controller is adopted to estimate the trusts. And then a density based outlier detection mechanism is introduced to acquire an adaptive trust threshold used to isolate the malicious nodes from being cluster heads. Finally, a fuzzy based cluster heads election method is proposed to achieve a balance between energy saving and security assurance, so that a normal sensor node with more residual energy or less confidence on other nodes has higher probability to be the cluster head. Extensive experiments verify that our secure clustering protocol can effectively defend the network against attacks from internal malicious or compromised nodes.      
### 33.Large Scale Radio Frequency Signal Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.09918.pdf)
>  Existing datasets used to train deep learning models for narrowband radio frequency (RF) signal classification lack enough diversity in signal types and channel impairments to sufficiently assess model performance in the real world. We introduce the Sig53 dataset consisting of 5 million synthetically-generated samples from 53 different signal classes and expertly chosen impairments. We also introduce TorchSig, a signals processing machine learning toolkit that can be used to generate this dataset. TorchSig incorporates data handling principles that are common to the vision domain, and it is meant to serve as an open-source foundation for future signals machine learning research. Initial experiments using the Sig53 dataset are conducted using state of the art (SoTA) convolutional neural networks (ConvNets) and Transformers. These experiments reveal Transformers outperform ConvNets without the need for additional regularization or a ConvNet teacher, which is contrary to results from the vision domain. Additional experiments demonstrate that TorchSig's domain-specific data augmentations facilitate model training, which ultimately benefits model performance. Finally, TorchSig supports on-the-fly synthetic data creation at training time, thus enabling massive scale training sessions with virtually unlimited datasets.      
### 34.When Is TTS Augmentation Through a Pivot Language Useful?  [ :arrow_down: ](https://arxiv.org/pdf/2207.09889.pdf)
>  Developing Automatic Speech Recognition (ASR) for low-resource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-to-speech (TTS) systems. However, many low-resource languages do not have quality TTS systems either. We propose an alternative: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS text-speech pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surprisingly, ASR performance can by harmed by increases in measured TTS quality. Application of these findings improves ASR by 64.5\% and 45.0\% character error reduction rate (CERR) respectively for two low-resource languages: GuaranÃ­ and Suba.      
### 35.Beam Alignment for the Cell-Free mmWave Massive MU-MIMO Uplink  [ :arrow_down: ](https://arxiv.org/pdf/2207.09879.pdf)
>  Millimeter-wave (mmWave) cell-free massive multi-user (MU) multiple-input multiple-output (MIMO) systems combine the large bandwidths available at mmWave frequencies with the improved coverage of cell-free systems. However, to combat the high path loss at mmWave frequencies, user equipments (UEs) must form beams in meaningful directions, i.e., to a nearby access point (AP). At the same time, multiple UEs should avoid transmitting to the same AP to reduce MU interference. We propose an interference-aware method for beam alignment (BA) in the cell-free mmWave massive MU-MIMO uplink. In the considered scenario, the APs perform full digital receive beamforming while the UEs perform analog transmit beamforming. We evaluate our method using realistic mmWave channels from a commercial ray-tracer, showing the superiority of the proposed method over omnidirectional transmission as well as over methods that do not take MU interference into account.      
### 36.Learning to Solve Soft-Constrained Vehicle Routing Problems with Lagrangian Relaxation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09860.pdf)
>  Vehicle Routing Problems (VRPs) in real-world applications often come with various constraints, therefore bring additional computational challenges to exact solution methods or heuristic search approaches. The recent idea to learn heuristic move patterns from sample data has become increasingly promising to reduce solution developing costs. However, using learning-based approaches to address more types of constrained VRP remains a challenge. The difficulty lies in controlling for constraint violations while searching for optimal solutions. To overcome this challenge, we propose a Reinforcement Learning based method to solve soft-constrained VRPs by incorporating the Lagrangian relaxation technique and using constrained policy optimization. We apply the method on three common types of VRPs, the Travelling Salesman Problem with Time Windows (TSPTW), the Capacitated VRP (CVRP) and the Capacitated VRP with Time Windows (CVRPTW), to show the generalizability of the proposed method. After comparing to existing RL-based methods and open-source heuristic solvers, we demonstrate its competitive performance in finding solutions with a good balance in travel distance, constraint violations and inference speed.      
### 37.Evaluating the Stability of Deep Image Quality Assessment With Respect to Image Scaling  [ :arrow_down: ](https://arxiv.org/pdf/2207.09856.pdf)
>  Image quality assessment (IQA) is a fundamental metric for image processing tasks (e.g., compression). With full-reference IQAs, traditional IQAs, such as PSNR and SSIM, have been used. Recently, IQAs based on deep neural networks (deep IQAs), such as LPIPS and DISTS, have also been used. It is known that image scaling is inconsistent among deep IQAs, as some perform down-scaling as pre-processing, whereas others instead use the original image size. In this paper, we show that the image scale is an influential factor that affects deep IQA performance. We comprehensively evaluate four deep IQAs on the same five datasets, and the experimental results show that image scale significantly influences IQA performance. We found that the most appropriate image scale is often neither the default nor the original size, and the choice differs depending on the methods and datasets used. We visualized the stability and found that PieAPP is the most stable among the four deep IQAs.      
### 38.Unsupervised energy disaggregation via convolutional sparse coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.09785.pdf)
>  In this work, a method for unsupervised energy disaggregation in private households equipped with smart meters is proposed. This method aims to classify power consumption as active or passive, granting the ability to report on the residents' activity and presence without direct interaction. This lays the foundation for applications like non-intrusive health monitoring of private homes. <br>The proposed method is based on minimizing a suitable energy functional, for which the iPALM (inertial proximal alternating linearized minimization) algorithm is employed, demonstrating that various conditions guaranteeing convergence are satisfied. <br>In order to confirm feasibility of the proposed method, experiments on semi-synthetic test data sets and a comparison to existing, supervised methods are provided.      
### 39.Localization supervision of chest x-ray classifiers using label-specific eye-tracking annotation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09771.pdf)
>  Convolutional neural networks (CNNs) have been successfully applied to chest x-ray (CXR) images. Moreover, annotated bounding boxes have been shown to improve the interpretability of a CNN in terms of localizing abnormalities. However, only a few relatively small CXR datasets containing bounding boxes are available, and collecting them is very costly. Opportunely, eye-tracking (ET) data can be collected in a non-intrusive way during the clinical workflow of a radiologist. We use ET data recorded from radiologists while dictating CXR reports to train CNNs. We extract snippets from the ET data by associating them with the dictation of keywords and use them to supervise the localization of abnormalities. We show that this method improves a model's interpretability without impacting its image-level classification.      
### 40.K-Means Based Constellation Optimization for Index Modulated Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2207.09766.pdf)
>  Reconfigurable intelligent surface (RIS) has recently emerged as a promising technology enabling next-generation wireless networks. In this paper, we develop an improved index modulation (IM) scheme by utilizing RIS to convey information. Specifically, we study an RIS-aided multiple-input single-output (MISO) system, in which the information bits are conveyed by reflection patterns of RIS rather than the conventional amplitude-phase constellation. Furthermore, the K-means algorithm is employed to optimize the reflection constellation to improve the error performance. Also, we propose a generalized Gray coding method for mapping information bits to an appropriate reflection constellation and analytically evaluate the error performance of the proposed scheme by deriving a closed-form expression of the average bit error rate (BER). Finally, numerical results verify the accuracy of our theoretical analysis as well as the substantially improved BER performance of the proposed RIS-based IM scheme.      
### 41.Revisiting data augmentation for subspace clustering  [ :arrow_down: ](https://arxiv.org/pdf/2207.09728.pdf)
>  Subspace clustering is the classical problem of clustering a collection of data samples that approximately lie around several low-dimensional subspaces. The current state-of-the-art approaches for this problem are based on the self-expressive model which represents the samples as linear combination of other samples. However, these approaches require sufficiently well-spread samples for accurate representation which might not be necessarily accessible in many applications. In this paper, we shed light on this commonly neglected issue and argue that data distribution within each subspace plays a critical role in the success of self-expressive models. Our proposed solution to tackle this issue is motivated by the central role of data augmentation in the generalization power of deep neural networks. We propose two subspace clustering frameworks for both unsupervised and semi-supervised settings that use augmented samples as an enlarged dictionary to improve the quality of the self-expressive representation. We present an automatic augmentation strategy using a few labeled samples for the semi-supervised problem relying on the fact that the data samples lie in the union of multiple linear subspaces. Experimental results confirm the effectiveness of data augmentation, as it significantly improves the performance of general self-expressive models.      
### 42.Efficient Meta-Tuning for Content-aware Neural Video Delivery  [ :arrow_down: ](https://arxiv.org/pdf/2207.09691.pdf)
>  Recently, Deep Neural Networks (DNNs) are utilized to reduce the bandwidth and improve the quality of Internet video delivery. Existing methods train corresponding content-aware super-resolution (SR) model for each video chunk on the server, and stream low-resolution (LR) video chunks along with SR models to the client. Although they achieve promising results, the huge computational cost of network training limits their practical applications. In this paper, we present a method named Efficient Meta-Tuning (EMT) to reduce the computational cost. Instead of training from scratch, EMT adapts a meta-learned model to the first chunk of the input video. As for the following chunks, it fine-tunes the partial parameters selected by gradient masking of previous adapted model. In order to achieve further speedup for EMT, we propose a novel sampling strategy to extract the most challenging patches from video frames. The proposed strategy is highly efficient and brings negligible additional cost. Our method significantly reduces the computational cost and achieves even better performance, paving the way for applying neural video delivery techniques to practical applications. We conduct extensive experiments based on various efficient SR architectures, including ESPCN, SRCNN, FSRCNN and EDSR-1, demonstrating the generalization ability of our work. The code is released at \url{<a class="link-external link-https" href="https://github.com/Neural-video-delivery/EMT-Pytorch-ECCV2022" rel="external noopener nofollow">this https URL</a>}.      
### 43.Improving Data Driven Inverse Text Normalization using Data Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.09674.pdf)
>  Inverse text normalization (ITN) is used to convert the spoken form output of an automatic speech recognition (ASR) system to a written form. Traditional handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile neural modeling approaches require quality large-scale spoken-written pair examples in the same or similar domain as the ASR system (in-domain data), to train. Both these approaches require costly and complex annotations. In this paper, we present a data augmentation technique that effectively generates rich spoken-written numeric pairs from out-of-domain textual data with minimal human annotation. We empirically demonstrate that ITN model trained using our data augmentation technique consistently outperform ITN model trained using only in-domain data across all numeric surfaces like cardinal, currency, and fraction, by an overall accuracy of 14.44%.      
### 44.Generalizable and Robust Deep Learning Algorithm for Atrial Fibrillation Diagnosis Across Ethnicities, Ages and Sexes  [ :arrow_down: ](https://arxiv.org/pdf/2207.09667.pdf)
>  To drive health innovation that meets the needs of all and democratize healthcare, there is a need to assess the generalization performance of deep learning (DL) algorithms across various distribution shifts to ensure that these algorithms are robust. This retrospective study is, to the best of our knowledge, the first to develop and assess the generalization performance of a deep learning (DL) model for AF events detection from long term beat-to-beat intervals across ethnicities, ages and sexes. The new recurrent DL model, denoted ArNet2, was developed on a large retrospective dataset of 2,147 patients totaling 51,386 hours of continuous electrocardiogram (ECG). The models generalization was evaluated on manually annotated test sets from four centers (USA, Israel, Japan and China) totaling 402 patients. The model was further validated on a retrospective dataset of 1,730 consecutives Holter recordings from the Rambam Hospital Holter clinic, Haifa, Israel. The model outperformed benchmark state-of-the-art models and generalized well across ethnicities, ages and sexes. Performance was higher for female than male and young adults (less than 60 years old) and showed some differences across ethnicities. The main finding explaining these variations was an impairment in performance in groups with a higher prevalence of atrial flutter (AFL). Our findings on the relative performance of ArNet2 across groups may have clinical implications on the choice of the preferred AF examination method to use relative to the group of interest.      
### 45.Roadmap Towards Responsible AI in Crisis Resilience Management  [ :arrow_down: ](https://arxiv.org/pdf/2207.09648.pdf)
>  Novel data sensing and AI technologies are finding practical use in the analysis of crisis resilience, revealing the need to consider how responsible artificial intelligence (AI) practices can mitigate harmful outcomes and protect vulnerable populations. In this opinion paper, we present a responsible AI roadmap that is embedded in the Crisis Information Management Circle. This roadmap includes six propositions to highlight and address important challenges and considerations specifically related to responsible AI for crisis resilience management. We cover a wide spectrum of interwoven challenges and considerations pertaining to the responsible collection, analysis, sharing and use of information such as equity, fairness, biases, explainability and transparency, accountability, privacy, inter-organizational coordination, and public engagement. Through examining issues around AI systems for crisis resilience management, we dissect the inherent complexities of information management and decision-making in crises and highlight the urgency of responsible AI research and practice. The ideas laid out in this opinion paper are the first attempt in establishing a roadmap for researchers, practitioners, developers, emergency managers, humanitarian organizations, and public officials to address important considerations for responsible AI pertaining to crisis resilience management.      
### 46.EVHA: Explainable Vision System for Hardware Testing and Assurance -- An Overview  [ :arrow_down: ](https://arxiv.org/pdf/2207.09627.pdf)
>  Due to the ever-growing demands for electronic chips in different sectors the semiconductor companies have been mandated to offshore their manufacturing processes. This unwanted matter has made security and trustworthiness of their fabricated chips concerning and caused creation of hardware attacks. In this condition, different entities in the semiconductor supply chain can act maliciously and execute an attack on the design computing layers, from devices to systems. Our attack is a hardware Trojan that is inserted during mask generation/fabrication in an untrusted foundry. The Trojan leaves a footprint in the fabricated through addition, deletion, or change of design cells. In order to tackle this problem, we propose Explainable Vision System for Hardware Testing and Assurance (EVHA) in this work that can detect the smallest possible change to a design in a low-cost, accurate, and fast manner. The inputs to this system are Scanning Electron Microscopy (SEM) images acquired from the Integrated Circuits (ICs) under examination. The system output is determination of IC status in terms of having any defect and/or hardware Trojan through addition, deletion, or change in the design cells at the cell-level. This article provides an overview on the design, development, implementation, and analysis of our defense system.      
### 47.ICRICS: Iterative Compensation Recovery for Image Compressive Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.09594.pdf)
>  Closed-loop architecture is widely utilized in automatic control systems and attain distinguished performance. However, classical compressive sensing systems employ open-loop architecture with separated sampling and reconstruction units. Therefore, a method of iterative compensation recovery for image compressive sensing (ICRICS) is proposed by introducing closed-loop framework into traditional compresses sensing systems. The proposed method depends on any existing approaches and upgrades their reconstruction performance by adding negative feedback structure. Theory analysis on negative feedback of compressive sensing systems is performed. An approximate mathematical proof of the effectiveness of the proposed method is also provided. Simulation experiments on more than 3 image datasets show that the proposed method is superior to 10 competition approaches in reconstruction performance. The maximum increment of average peak signal-to-noise ratio is 4.36 dB and the maximum increment of average structural similarity is 0.034 on one dataset. The proposed method based on negative feedback mechanism can efficiently correct the recovery error in the existing systems of image compressive sensing.      
### 48.A Frequency-Velocity CNN for Developing Near-Surface 2D Vs Images from Linear-Array, Active-Source Wavefield Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2207.09580.pdf)
>  This paper presents a frequency-velocity convolutional neural network (CNN) for rapid, non-invasive 2D shear wave velocity (Vs) imaging of near-surface geo-materials. Operating in the frequency-velocity domain allows for significant flexibility in the linear-array, active-source experimental testing configurations used for generating the CNN input, which are normalized dispersion images. Unlike wavefield images, normalized dispersion images are relatively insensitive to the experimental testing configuration, accommodating various source types, source offsets, numbers of receivers, and receiver spacings. We demonstrate the effectiveness of the frequency-velocity CNN by applying it to a classic near-surface geophysics problem, namely, imaging a two-layer, undulating, soil-over-bedrock interface. This problem was recently investigated in our group by developing a time-distance CNN, which showed great promise but lacked flexibility in utilizing different field-testing configurations. Herein, the new frequency-velocity CNN is shown to have comparable accuracy to the time-distance CNN while providing greater flexibility to handle varied field applications. The frequency-velocity CNN was trained, validated, and tested using 100,000 synthetic near-surface models. The ability of the proposed frequency-velocity CNN to generalize across various acquisition configurations is first tested using synthetic near-surface models with different acquisition configurations from that of the training set, and then applied to experimental field data collected at the Hornsby Bend site in Austin, Texas, USA. When fully developed for a wider range of geological conditions, the proposed CNN may ultimately be used as a rapid, end-to-end alternative for current pseudo-2D surface wave imaging techniques or to develop starting models for full waveform inversion.      
### 49.A review on recent advances in scenario aggregation methods for power system analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.09557.pdf)
>  Worldwide commitments to net zero greenhouse emissions have accelerated investments in renewable energy resources. The requirements for operating and planning power systems are becoming stringent because of the need to take into account the uncertainty associated with renewable generation. Several modeling frameworks that consider the inherent uncertainty in the operation and planning of the power system have been extensively studied. Stochastic optimization has been the most popular approach among these frameworks due to its intuitive representation, especially when formulated using discrete probabilistic scenarios to represent the random variables. Although many scenarios representing all possible uncertain operating conditions would be needed to accurate evaluate stochastic operation and planning models, the size of the scenario set impacts computational complexity, posing a significant tradeoff between uncertainty detail representation and computational tractability. <br>During the last decade, a large body of research has focused on developing new scenario aggregation methods to derive reduced scenario sets that show properties similar to the original scenario set while decreasing computational burden. This review provides an up-to-date, comprehensive classification and analysis of the literature related to scenario aggregation methods for addressing power system optimization problems. First, we present a general framework and the aggregation methodologies. Then, the main studies related to temporal and spatial scenario aggregation are described, followed by a bibliometric analysis of the main publication sources, authors, and application problems. Finally, we provide a numerical analysis and discuss 16 aggregation methods for the transmission expansion planning problem. Finally, recommendations, opportunities, and conclusions are discussed.      
### 50.COVID-19 Detection from Respiratory Sounds with Hierarchical Spectrogram Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2207.09529.pdf)
>  Monitoring of prevalent airborne diseases such as COVID-19 characteristically involve respiratory assessments. While auscultation is a mainstream method for symptomatic monitoring, its diagnostic utility is hampered by the need for dedicated hospital visits. Continual remote monitoring based on recordings of respiratory sounds on portable devices is a promising alternative, which can assist in screening of COVID-19. In this study, we introduce a novel deep learning approach to distinguish patients with COVID-19 from healthy controls given audio recordings of cough or breathing sounds. The proposed approach leverages a novel hierarchical spectrogram transformer (HST) on spectrogram representations of respiratory sounds. HST embodies self-attention mechanisms over local windows in spectrograms, and window size is progressively grown over model stages to capture local to global context. HST is compared against state-of-the-art conventional and deep-learning baselines. Comprehensive demonstrations on a multi-national dataset indicate that HST outperforms competing methods, achieving over 97% area under the receiver operating characteristic curve (AUC) in detecting COVID-19 cases.      
