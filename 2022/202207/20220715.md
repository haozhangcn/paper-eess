# ArXiv eess --Fri, 15 Jul 2022
### 1.A Novel Implementation of Machine Learning for the Efficient, Explainable Diagnosis of COVID-19 from Chest CT  [ :arrow_down: ](https://arxiv.org/pdf/2207.07117.pdf)
>  In a worldwide health crisis as exigent as COVID-19, there has become a pressing need for rapid, reliable diagnostics. Currently, popular testing methods such as reverse transcription polymerase chain reaction (RT-PCR) can have high false negative rates. Consequently, COVID-19 patients are not accurately identified nor treated quickly enough to prevent transmission of the virus. However, the recent rise of medical CT data has presented promising avenues, since CT manifestations contain key characteristics indicative of COVID-19. This study aimed to take a novel approach in the machine learning-based detection of COVID-19 from chest CT scans. First, the dataset utilized in this study was derived from three major sources, comprising a total of 17,698 chest CT slices across 923 patient cases. Image preprocessing algorithms were then developed to reduce noise by excluding irrelevant features. Transfer learning was also implemented with the EfficientNetB7 pre-trained model to provide a backbone architecture and save computational resources. Lastly, several explainability techniques were leveraged to qualitatively validate model performance by localizing infected regions and highlighting fine-grained pixel details. The proposed model attained an overall accuracy of 0.927 and a sensitivity of 0.958. Explainability measures showed that the model correctly distinguished between relevant, critical features pertaining to COVID-19 chest CT images and normal controls. Deep learning frameworks provide efficient, human-interpretable COVID-19 diagnostics that could complement radiologist decisions or serve as an alternative screening tool. Future endeavors may provide insight into infection severity, patient risk stratification, and prognosis.      
### 2.A Neural-Network Framework for the Design of Individualised Hearing-Loss Compensation  [ :arrow_down: ](https://arxiv.org/pdf/2207.07091.pdf)
>  Even though sound processing in the human auditory system is complex and highly non-linear, hearing aids (HAs) still rely on simplified descriptions of auditory processing or hearing loss to restore hearing. Standard HA amplification strategies succeed in restoring inaudibility of faint sounds, but fall short of providing targetted treatments for complex sensorineural deficits. To address this challenge, biophysically realistic models of human auditory processing can be adopted in the design of individualised HA strategies, but these are typically non-differentiable and computationally expensive. Therefore, this study proposes a differentiable DNN framework that can be used to train DNN-based HA models based on biophysical auditory-processing differences between normal-hearing and hearing-impaired models. We investigate the restoration capabilities of our DNN-based hearing-loss compensation for different loss functions, to optimally compensate for a mixed outer-hair-cell (OHC) loss and cochlear-synaptopathy (CS) impairment. After evaluating which trained DNN-HA model yields the best restoration outcomes on simulated auditory responses and speech intelligibility, we applied the same training procedure to two milder hearing-loss profiles with OHC loss or CS alone. Our results show that auditory-processing restoration was possible for all considered hearing-loss cases, with OHC loss proving easier to compensate than CS. Several objective metrics were considered to estimate the expected perceptual benefit after processing, and these simulations hold promise in yielding improved understanding of speech-in-noise for hearing-impaired listeners who use our DNN-HA processing. Since our framework can be tuned to the hearing-loss profiles of individual listeners, we enter an era where truly individualised and DNN-based hearing-restoration strategies can be developed and be tested experimentally.      
### 3.Inverse Resource Rational Based Stochastic Driver Behavior Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.07088.pdf)
>  Human drivers have limited and time-varying cognitive resources when making decisions in real-world traffic scenarios, which often leads to unique and stochastic behaviors that can not be explained by perfect rationality assumption, a widely accepted premise in modeling driving behaviors that presume drivers rationally make decisions to maximize their own rewards under all circumstances. To explicitly address this disadvantage, this study presents a novel driver behavior model that aims to capture the resource rationality and stochasticity of the human driver's behaviors in realistic longitudinal driving scenarios. The resource rationality principle can provide a theoretic framework to better understand the human cognition processes by modeling human's internal cognitive mechanisms as utility maximization subject to cognitive resource limitations, which can be represented as finite and time-varying preview horizons in the context of driving. An inverse resource rational-based stochastic inverse reinforcement learning approach (IRR-SIRL) is proposed to learn a distribution of the planning horizon and cost function of the human driver with a given series of human demonstrations. A nonlinear model predictive control (NMPC) with a time-varying horizon approach is used to generate driver-specific trajectories by using the learned distributions of the planning horizon and the cost function of the driver. The simulation experiments are carried out using human demonstrations gathered from the driver-in-the-loop driving simulator. The results reveal that the proposed inverse resource rational-based stochastic driver model can address the resource rationality and stochasticity of human driving behaviors in a variety of realistic longitudinal driving scenarios.      
### 4.An Efficient Method for Quantifying the Aggregate Flexibility of Plug-in Electric Vehicle Populations  [ :arrow_down: ](https://arxiv.org/pdf/2207.07067.pdf)
>  Plug-in electric vehicles (EVs) are widely recognized as being highly flexible electric loads that can be pooled and controlled via aggregators to provide low-cost energy and ancillary services to wholesale electricity markets. To participate in these markets, an EV aggregator must encode the aggregate flexibility of the population of EVs under their command as a single polytope that is compliant with existing market rules. To this end, we investigate the problem of characterizing the aggregate flexibility set of a heterogeneous population of EVs whose individual flexibility sets are given as convex polytopes in half-space representation. As the exact computation of the aggregate flexibility set -- the Minkowski sum of the individual flexibility sets -- is known to be intractable, we study the problems of computing maximum-volume inner approximations and minimum-volume outer approximations to the aggregate flexibility set by optimizing over affine transformations of a given convex polytope in half-space representation. We show how to conservatively approximate the pair of maximum-volume and minimum-volume set containment problems as linear programs that scale polynomially with the number and dimension of the individual flexibility sets. The class of approximations methods provided in this paper generalizes existing methods from the literature. We illustrate the improvement in approximation accuracy achievable by our methods with numerical experiments.      
### 5.Multi-Agent Deep Reinforcement Learning-Driven Mitigation of Adverse Effects of Cyber-Attacks on Electric Vehicle Charging Station  [ :arrow_down: ](https://arxiv.org/pdf/2207.07041.pdf)
>  An electric vehicle charging station (EVCS) infrastructure is the backbone of transportation electrification. However, the EVCS has myriads of exploitable vulnerabilities in software, hardware, supply chain, and incumbent legacy technologies such as network, communication, and control. These standalone or networked EVCS open up large attack surfaces for the local or state-funded adversaries. The state-of-the-art approaches are not agile and intelligent enough to defend against and mitigate advanced persistent threats (APT). We propose the data-driven model-free distributed intelligence based on multiagent Deep Reinforcement Learning (MADRL)-- Twin Delayed Deep Deterministic Policy Gradient (TD3) -- that efficiently learns the control policy to mitigate the cyberattacks on the controllers of EVCS. Also, we have proposed two additional mitigation methods: the manual/Bruteforce mitigation and the controller clone-based mitigation. The attack model considers the APT designed to malfunction the duty cycles of the EVCS controllers with Type-I low-frequency attack and Type-II constant attack. The proposed model restores the EVCS operation under threat incidence in any/all controllers by correcting the control signals generated by the legacy controllers. Also, the TD3 algorithm provides higher granularity by learning nonlinear control policies as compared to the other two mitigation methods. Index Terms: Cyberattack, Deep Reinforcement Learning(DRL), Electric Vehicle Charging Station, Mitigation.      
### 6.MedFuse: Multi-modal fusion with clinical time-series data and chest X-ray images  [ :arrow_down: ](https://arxiv.org/pdf/2207.07027.pdf)
>  Multi-modal fusion approaches aim to integrate information from different data sources. Unlike natural datasets, such as in audio-visual applications, where samples consist of "paired" modalities, data in healthcare is often collected asynchronously. Hence, requiring the presence of all modalities for a given sample is not realistic for clinical tasks and significantly limits the size of the dataset during training. In this paper, we propose MedFuse, a conceptually simple yet promising LSTM-based fusion module that can accommodate uni-modal as well as multi-modal input. We evaluate the fusion method and introduce new benchmark results for in-hospital mortality prediction and phenotype classification, using clinical time-series data in the MIMIC-IV dataset and corresponding chest X-ray images in MIMIC-CXR. Compared to more complex multi-modal fusion strategies, MedFuse provides a performance improvement by a large margin on the fully paired test set. It also remains robust across the partially paired test set containing samples with missing chest X-ray images. We release our code for reproducibility and to enable the evaluation of competing models in the future.      
### 7.Precision Attitude Stabilization with Intermittent External Torque  [ :arrow_down: ](https://arxiv.org/pdf/2207.06987.pdf)
>  The attitude stabilization of a micro-satellite employing a variable-amplitude cold gas thruster which reflects as a time varying gain on the control input is considered. Existing literature uses a persistence filter based approach that typically leads to large control gains and torque inputs during specific time intervals corresponding to the 'on' phase of the external actuation. This work aims at reducing the transient spikes placed upon the torque commands by the judicious introduction of an additional time varying scaling signal as part of the control law. The time update mechanism for the new scaling factor and overall closed-loop stability are established through a Lyapunov-like analysis. Numerical simulations highlight the various features of this new control algorithm for spacecraft attitude stabilization subject to torque intermittence.      
### 8.Adversarial Examples for Model-Based Control: A Sensitivity Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2207.06982.pdf)
>  We propose a method to attack controllers that rely on external timeseries forecasts as task parameters. An adversary can manipulate the costs, states, and actions of the controllers by forging the timeseries, in this case perturbing the real timeseries. Since the controllers often encode safety requirements or energy limits in their costs and constraints, we refer to such manipulation as an adversarial attack. We show that different attacks on model-based controllers can increase control costs, activate constraints, or even make the control optimization problem infeasible. We use the linear quadratic regulator and convex model predictive controllers as examples of how adversarial attacks succeed and demonstrate the impact of adversarial attacks on a battery storage control task for power grid operators. As a result, our method increases control cost by $8500\%$ and energy constraints by $13\%$ on real electricity demand timeseries.      
### 9.Power Injection Measurements are more Vulnerable to Data Integrity Attacks than Power Flow Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2207.06973.pdf)
>  A novel metric that describes the vulnerability of the measurements in power system to data integrity attacks is proposed. The new metric, coined vulnerability index (VuIx), leverages information theoretic measures to assess the attack effect on the fundamental limits of the disruption and detection tradeoff. The result of computing the VuIx of the measurements in the system yields an ordering of the measurements vulnerability based on the level of exposure to data integrity attacks. This new framework is used to assess the measurements vulnerability of IEEE test systems and it is observed that power injection measurements are overwhelmingly more vulnerable to data integrity attacks than power flow measurements. A detailed numerical evaluation of the VuIx values for IEEE test systems is provided.      
### 10.Learning Representations for CSI Adaptive Quantization and Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2207.06924.pdf)
>  In this work, we propose an efficient method for channel state information (CSI) adaptive quantization and feedback in frequency division duplexing (FDD) systems. Existing works mainly focus on the implementation of autoencoder (AE) neural networks (NNs) for CSI compression, and consider straightforward quantization methods, e.g., uniform quantization, which are generally not optimal. With this strategy, it is hard to achieve a low reconstruction error, especially, when the available number of bits reserved for the latent space quantization is small. To address this issue, we recommend two different methods: one based on a post training quantization and the second one in which the codebook is found during the training of the AE. Both strategies achieve better reconstruction accuracy compared to standard quantization techniques.      
### 11.Pediatric Sleep Scoring In-the-wild from Millions of Multi-channel EEG Signals  [ :arrow_down: ](https://arxiv.org/pdf/2207.06921.pdf)
>  Sleep is critical to the health and development of infants, children, and adolescents, but pediatric sleep is severely under-researched compared to adult sleep in the context of machine learning for health and well-being. Here, we present the first automated pediatric sleep scoring results on a recent large-scale sleep study dataset that was collected during standard clinical care. We develop a transformer-based deep neural network model that learns to classify five sleep stages from millions of multi-channel electroencephalogram (EEG) signals with 78% overall accuracy. Further, we conduct an in-depth analysis of the model performance based on patient demographics and EEG channels.      
### 12.Interference-Limited Ultra-Reliable and Low-Latency Communications: Graph Neural Networks or Stochastic Geometry?  [ :arrow_down: ](https://arxiv.org/pdf/2207.06918.pdf)
>  In this paper, we aim to improve the Quality-of-Service (QoS) of Ultra-Reliability and Low-Latency Communications (URLLC) in interference-limited wireless networks. To obtain time diversity within the channel coherence time, we first put forward a random repetition scheme that randomizes the interference power. Then, we optimize the number of reserved slots and the number of repetitions for each packet to minimize the QoS violation probability, defined as the percentage of users that cannot achieve URLLC. We build a cascaded Random Edge Graph Neural Network (REGNN) to represent the repetition scheme and develop a model-free unsupervised learning method to train it. We analyze the QoS violation probability using stochastic geometry in a symmetric scenario and apply a model-based Exhaustive Search (ES) method to find the optimal solution. Simulation results show that in the symmetric scenario, the QoS violation probabilities achieved by the model-free learning method and the model-based ES method are nearly the same. In more general scenarios, the cascaded REGNN generalizes very well in wireless networks with different scales, network topologies, cell densities, and frequency reuse factors. It outperforms the model-based ES method in the presence of the model mismatch.      
### 13.Experimental emulation for OTFS waveform RF-impairments  [ :arrow_down: ](https://arxiv.org/pdf/2207.06915.pdf)
>  Orthogonal time-frequency space (OTFS) waveform exceeds the challenges that face orthogonal frequency division multiplexing (OFDM) in the high-mobility environment with high time-frequency dispersive channels. Since practical pulse shaping design and RF-impairments effects have a direct impact on waveform behavior, this paper investigates experimental implementation for practical pulse shaping design and RF-impairments that affect OTFS waveform performance and compares them to OFDM waveform as a benchmark. Firstly, the doubly-dispersive channel effect is analyzed, then an experimental framework is established for investigating the RF-impairments include non-linearity, Carrier frequency offset, I/Q-imbalances, DC-offset, and phase noise are considered. The experiments were conducted in a real indoor wireless environment using software-defined radio (SDR)based on the Keysight EXG X-Serie devices. The experimental results validate the accuracy of the theoretical results.      
### 14.An Investigation on Non-Invasive Brain-Computer Interfaces: Emotiv Epoc+ Neuroheadset and Its Effectiveness  [ :arrow_down: ](https://arxiv.org/pdf/2207.06914.pdf)
>  In this study, we illustrate the progress of BCI research and present scores of unveiled contemporary approaches. First, we explore a decoding natural speech approach that is designed to decode human speech directly from the human brain onto a digital screen introduced by Facebook Reality Lab and University of California San Francisco. Then, we study a recently presented visionary project to control the human brain using Brain-Machine Interfaces (BMI) approach. We also investigate well-known electroencephalography (EEG) based Emotiv Epoc+ Neuroheadset to identify six emotional parameters including engagement, excitement, focus, stress, relaxation, and interest using brain signals by experimenting the neuroheadset among three human subjects where we utilize two supervised learning classifiers, Naive Bayes and Linear Regression to show the accuracy and competency of the Epoc+ device and its associated applications in neurotechnological research. We present experimental studies and the demonstration indicates 69% and 62% improved accuracy for the aforementioned classifiers respectively in reading the performance matrices of the participants. We envision that non-invasive, insertable, and low-cost BCI approaches shall be the focal point for not only an alternative for patients with physical paralysis but also understanding the brain that would pave us to access and control the memories and brain somewhere very near.      
### 15.Construction of Lyapunov Functions Using Vector Field Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2207.06912.pdf)
>  In the present paper, a novel vector field decomposition based approach for constructing Lyapunov functions is proposed. For a given dynamical system, if the defining vector field admits a decomposition into two mutually orthogonal vector fields, one of which is curl-free and the other is divergence-free, then the potential function of the former can serve as a Lyapunov function candidate, since its positive definiteness will reflect the stability of the system. Moreover, under some additional conditions, its sublevel sets will give the exact attraction domain of the system. A sufficient condition for the existence of the proposed vector field decomposition is first obtained for $2$-dimensional systems by solving a partial differential equation and then generalized to $n$-dimensional systems. Furthermore, the proposed vector field decomposition always exists for linear systems and can be obtained by solving a specific algebraic Riccati equation.      
### 16.Improving self-supervised pretraining models for epileptic seizure detection from EEG data  [ :arrow_down: ](https://arxiv.org/pdf/2207.06911.pdf)
>  There is abundant medical data on the internet, most of which are unlabeled. Traditional supervised learning algorithms are often limited by the amount of labeled data, especially in the medical domain, where labeling is costly in terms of human processing and specialized experts needed to label them. They are also prone to human error and biased as a select few expert annotators label them. These issues are mitigated by Self-supervision, where we generate pseudo-labels from unlabelled data by seeing the data itself. This paper presents various self-supervision strategies to enhance the performance of a time-series based Diffusion convolution recurrent neural network (DCRNN) model. The learned weights in the self-supervision pretraining phase can be transferred to the supervised training phase to boost the model's prediction capability. Our techniques are tested on an extension of a Diffusion Convolutional Recurrent Neural network (DCRNN) model, an RNN with graph diffusion convolutions, which models the spatiotemporal dependencies present in EEG signals. When the learned weights from the pretraining stage are transferred to a DCRNN model to determine whether an EEG time window has a characteristic seizure signal associated with it, our method yields an AUROC score $1.56\%$ than the current state-of-the-art models on the TUH EEG seizure corpus.      
### 17.Attention mechanisms for physiological signal deep learning: which attention should we take?  [ :arrow_down: ](https://arxiv.org/pdf/2207.06904.pdf)
>  Attention mechanisms are widely used to dramatically improve deep learning model performance in various fields. However, their general ability to improve the performance of physiological signal deep learning model is immature. In this study, we experimentally analyze four attention mechanisms (e.g., squeeze-and-excitation, non-local, convolutional block attention module, and multi-head self-attention) and three convolutional neural network (CNN) architectures (e.g., VGG, ResNet, and Inception) for two representative physiological signal prediction tasks: the classification for predicting hypotension and the regression for predicting cardiac output (CO). We evaluated multiple combinations for performance and convergence of physiological signal deep learning model. Accordingly, the CNN models with the spatial attention mechanism showed the best performance in the classification problem, whereas the channel attention mechanism achieved the lowest error in the regression problem. Moreover, the performance and convergence of the CNN models with attention mechanisms were better than stand-alone self-attention models in both problems. Hence, we verified that convolutional operation and attention mechanisms are complementary and provide faster convergence time, despite the stand-alone self-attention models requiring fewer parameters.      
### 18.Adaptive Attitude Estimation Using a Hybrid Model-Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.06903.pdf)
>  Attitude determination using the smartphone's inertial sensors poses a major challenge due to the sensor low-performance grade and variate nature of the walking pedestrian. In this paper, data-driven techniques are employed to address that challenge. To that end, a hybrid deep learning and model based solution for attitude estimation is proposed. Here, classical model based equations are applied to form an adaptive complementary filter structure. Instead of using constant or model based adaptive weights, the accelerometer weights in each axis are determined by a unique neural network. The performance of the proposed hybrid approach is evaluated relative to popular model based approaches using experimental data.      
### 19.Immunofluorescence Capillary Imaging Segmentation: Cases Study  [ :arrow_down: ](https://arxiv.org/pdf/2207.06861.pdf)
>  Nonunion is one of the challenges faced by orthopedics clinics for the technical difficulties and high costs in photographing interosseous capillaries. Segmenting vessels and filling capillaries are critical in understanding the obstacles encountered in capillary growth. However, existing datasets for blood vessel segmentation mainly focus on the large blood vessels of the body, and the lack of labeled capillary image datasets greatly limits the methodological development and applications of vessel segmentation and capillary filling. Here, we present a benchmark dataset, named IFCIS-155, consisting of 155 2D capillary images with segmentation boundaries and vessel fillings annotated by biomedical experts, and 19 large-scale, high-resolution 3D capillary images. To obtain better images of interosseous capillaries, we leverage state-of-the-art immunofluorescence imaging techniques to highlight the rich vascular morphology of interosseous capillaries. We conduct comprehensive experiments to verify the effectiveness of the dataset and the benchmarking deep learning models (\eg UNet/UNet++ and the modified UNet/UNet++). Our work offers a benchmark dataset for training deep learning models for capillary image segmentation and provides a potential tool for future capillary research. The IFCIS-155 dataset and code are all publicly available at \url{<a class="link-external link-https" href="https://github.com/ncclabsustech/IFCIS-55" rel="external noopener nofollow">this https URL</a>}.      
### 20.Joint UAV Placement and IRS Phase Shift Optimization in Downlink Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.06853.pdf)
>  This study investigates the integration of an intelligent reflecting surface (IRS) into an unmanned aerial vehicle (UAV) platform to utilize the advantages of these leading technologies for sixth-generation communications, e.g., improved spectral and energy efficiency, extended network coverage, and flexible deployment. In particular, we investigate a downlink IRS-UAV system, wherein single-antenna ground users (UEs) are served by a multi-antenna base station (BS). To assist the communication between UEs and the BS, an IRS mounted on a UAV is deployed, in which the direct links are obstructed owing to the complex urban channel characteristics. The beamforming at the BS, phase shift at the IRS, and the 3D placement of the UAV are jointly optimized to maximize the sum rate. Because the optimization variables, particularly the beamforming and IRS phase shift, are highly coupled with each other, the optimization problem is naturally non-convex. To effectively solve the formulated problem, we propose an iterative algorithm that employs block coordinate descent and inner approximation methods. Numerical results demonstrate the effectiveness of our proposed approach for a UAV-mounted IRS system on the sum rate performance over the state-of-the-art technology using the terrestrial counterpart.      
### 21.Adaptive frequency prior for frequency selective reconstruction of images from non-regular subsampling  [ :arrow_down: ](https://arxiv.org/pdf/2207.06797.pdf)
>  Image signals typically are defined on a rectangular two-dimensional grid. However, there exist scenarios where this is not fulfilled and where the image information only is available for a non-regular subset of pixel position. For processing, transmitting or displaying such an image signal, a re-sampling to a regular grid is required. Recently, Frequency Selective Reconstruction (FSR) has been proposed as a very effective sparsity-based algorithm for solving this under-determined problem. For this, FSR iteratively generates a model of the signal in the Fourier-domain. In this context, a fixed frequency prior inspired by the optical transfer function is used for favoring low-frequency content. However, this fixed prior is often too strict and may lead to a reduced reconstruction quality. To resolve this weakness, this paper proposes an adaptive frequency prior which takes the local density of the available samples into account. The proposed adaptive prior allows for a very high reconstruction quality, yielding gains of up to 0.6 dB PSNR over the fixed prior, independently of the density of the available samples. Compared to other state-of-the-art algorithms, visually noticeable gains of several dB are possible.      
### 22.Multiple Selection Extrapolation for Improved Spatial Error Concealment  [ :arrow_down: ](https://arxiv.org/pdf/2207.06795.pdf)
>  This contribution introduces a novel signal extrapolation algorithm and its application to image error concealment. The signal extrapolation is carried out by iteratively generating a model of the signal suffering from distortion. Thereby, the model results from a weighted superposition of two-dimensional basis functions whereas in every iteration step a set of these is selected and the approximation residual is projected onto the subspace they span. The algorithm is an improvement to the Frequency Selective Extrapolation that has proven to be an effective method for concealing lost or distorted image regions. Compared to this algorithm, the novel algorithm is able to reduce the processing time by a factor larger than three, by still preserving the very high extrapolation quality.      
### 23.Adaptive joint spatio-temporal error concealment for video communication  [ :arrow_down: ](https://arxiv.org/pdf/2207.06794.pdf)
>  In the past years, video communication has found its application in an increasing number of environments. Unfortunately, some of them are error-prone and the risk of block losses caused by transmission errors is ubiquitous. To reduce the effects of these block losses, a new spatio-temporal error concealment algorithm is presented. The algorithm uses spatial as well as temporal information for extrapolating the signal into the lost areas. The extrapolation is carried out in two steps, first a preliminary temporal extrapolation is performed which then is used to generate a model of the original signal, using the spatial neighborhood of the lost block. By applying the spatial refinement a significantly higher concealment quality can be achieved resulting in a gain of up to 5.2 dB in PSNR compared to the unrefined underlying pure temporal extrapolation.      
### 24.Proof-of-concept Study of Sparse Processing Particle Image Velocimetry for Real Time Flow Observation  [ :arrow_down: ](https://arxiv.org/pdf/2207.06774.pdf)
>  In this paper, we overview, evaluate, and demonstrate the sparse processing particle image velocimetry (SPPIV) as a real-time flow field estimation method using the particle image velocimetry (PIV), whereas SPPIV was previously proposed with its feasibility study and its real-time demonstration is conducted for the first time in this study. In the wind tunnel test, the PIV measurement and real-time measurement using SPPIV were conducted for the flow velocity field around the NACA0015 airfoil model. The off-line analysis results of the test show that the flow velocity field can be estimated from a small number of processing points by applying SPPIV, and also illustrates the following characteristics of SPPIV. The estimation accuracy improves as the number of processing points increases, whereas the processing time per step increases in proportion to the number of processing points. Therefore, it is necessary to set an optimal number of processing points. In addition, the application of the Kalman filter significantly improves the estimation accuracy with a small number of processing points while suppressing the processing time. When the flow velocity fields with different angles of attack are used as the training data with that of test data, the estimation using SPPIV is found to be reasonable if the difference in angle of attack between the training and test data is equal to or less than 2 deg and the flow phenomena of the training data are similar to that of the test data. For this reason, training data should be prepared at least every 4 deg. Finally, the demonstration of SPPIV as a real-time flow observation was conducted for the first time. In this demonstration, the real-time measurement is found to be possible at a sampling rate of 2000 Hz at 20 or less processing points in the top 10 modes estimation as expected by the off-line analyses.      
### 25.Robot Swarms as Hybrid Systems: Modelling and Verification  [ :arrow_down: ](https://arxiv.org/pdf/2207.06758.pdf)
>  A swarm robotic system consists of a team of robots performing cooperative tasks without any centralized coordination. In principle, swarms enable flexible and scalable solutions; however, designing individual control algorithms that can guarantee a required global behavior is difficult. Formal methods have been suggested by several researchers as a mean to increase confidence in the behavior of the swarm. In this work, we propose to model swarms as hybrid systems and use reachability analysis to verify their properties. We discuss challenges and report on the experience gained from applying hybrid formalisms to the verification of a swarm robotic system.      
### 26.Single-Pixel Image Reconstruction Based on Block Compressive Sensing and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.06746.pdf)
>  Single-pixel imaging (SPI) is a novel imaging technique whose working principle is based on the compressive sensing (CS) theory. In SPI, data is obtained through a series of compressive measurements and the corresponding image is reconstructed. Typically, the reconstruction algorithm such as basis pursuit relies on the sparsity assumption in images. However, recent advances in deep learning have found its uses in reconstructing CS images. Despite showing a promising result in simulations, it is often unclear how such an algorithm can be implemented in an actual SPI setup. In this paper, we demonstrate the use of deep learning on the reconstruction of SPI images in conjunction with block compressive sensing (BCS). We also proposed a novel reconstruction model based on convolutional neural networks that outperforms other competitive CS reconstruction algorithms. Besides, by incorporating BCS in our deep learning model, we were able to reconstruct images of any size above a certain smallest image size. In addition, we show that our model is capable of reconstructing images obtained from an SPI setup while being priorly trained on natural images, which can be vastly different from the SPI images. This opens up opportunity for the feasibility of pretrained deep learning models for CS reconstructions of images from various domain areas.      
### 27.Downlink Analysis and Evaluation of Multi-Beam LEO Satellite Communication in Shadowed Rician Channels  [ :arrow_down: ](https://arxiv.org/pdf/2207.06663.pdf)
>  The extension of wide area wireless connectivity to low-earth orbit (LEO) satellite communication systems demands a fresh look at the effects of in-orbit base stations, sky-to-ground propagation, and cell planning. A multi-beam LEO satellite delivers widespread coverage by forming multiple spot beams that tessellate cells over a given region on the surface of the Earth. In doing so, overlapping spot beams introduce interference when delivering downlink concurrently in the same area using the same frequency spectrum. To permit forecasting of communication system performance, we characterize desired and interference signal powers, along with SNR, INR, SIR, and SINR, under the measurement-backed Shadowed Rician (SR) sky-to-ground channel model. We introduce a minor approximation to the fading order of SR channels that greatly simplifies the PDF and CDF of these quantities and facilitates statistical analyses of LEO satellite systems such as probability of outage. We conclude this paper with an evaluation of multi-beam LEO satellite communication in SR channels of varying intensity fitted from existing measurements. Our numerical results highlight the effects satellite elevation angle has on SNR, INR, and SINR, which brings attention to the variability in system state and potential performance as a satellite traverses across the sky along its orbit.      
### 28.Toward cm-Level Accuracy: Carrier Phase Positioning for IIoT in 5G-Advanced NR Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.06633.pdf)
>  High-precision positioning accuracy is among the key features of the future fifth-generation (5G-advanced) cellular networks to enable a wide variety of commercial, critical, and consumer use cases. 5G new radio (NR) systems have relied on (1) cellular temporal/angular-based positioning methods to provide the indoor environments with a moderate positioning accuracy that is well below the positioning requirements of these future use cases and (2) highly precise satellite carrier phase/code-based positioning methods for the outdoor deployments that are limited by the availability of the satellite coverage. This paper defines the relevant standard mechanisms and algorithms to use the carrier phase cellular-based measurements as a potential solution to achieve a high-precision positioning estimation accuracy in 5G-advanced NR networks. The presented positioning technique is evaluated using high-fidelity system-level simulations for indoor factory (InF) deployment scenarios. The numerical results demonstrate that the presented technique can significantly improve the positioning accuracy compared with the state-of-the-art NR positioning methods. Our findings in this paper also show that the carrier phase method not only provides an indoor complement to the outdoor satellite positioning but also provides an outdoor alternative to the high-precision satellite methods.      
### 29.Perception-Oriented Stereo Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.06617.pdf)
>  Recent studies of deep learning based stereo image super-resolution (StereoSR) have promoted the development of StereoSR. However, existing StereoSR models mainly concentrate on improving quantitative evaluation metrics and neglect the visual quality of super-resolved stereo images. To improve the perceptual performance, this paper proposes the first perception-oriented stereo image super-resolution approach by exploiting the feedback, provided by the evaluation on the perceptual quality of StereoSR results. To provide accurate guidance for the StereoSR model, we develop the first special stereo image super-resolution quality assessment (StereoSRQA) model, and further construct a StereoSRQA database. Extensive experiments demonstrate that our StereoSR approach significantly improves the perceptual quality and enhances the reliability of stereo images for disparity estimation.      
### 30.Approximate synchronization of coupled multi-valued logical networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.06615.pdf)
>  This article deals with the approximate synchronization of two coupled multi-valued logical networks. According to the initial state set from which both systems start, two kinds of approximate synchronization problem, local approximate synchronization and global approximate synchronization, are proposed for the first time. Three new notions: approximate synchronization state set (ASSS), the maximum approximate synchronization basin (MASB) and the shortest approximate synchronization time (SAST) are introduced and analyzed. Based on ASSS, several necessary and sufficient conditions are obtained for approximate synchronization. MASB, the set of all possible initial states, from which the systems are approximately synchronous, is investigated combining with the maximum invariant subset. And the calculation method of the SAST, associated with transient period, is presented. By virtue of MASB, pinning control scheme is investigated to make two coupled systems achieve global approximate synchronization. Furthermore, the related theories are also applied to the complete synchronization problem of $k$-valued ($k\geq2$) logical networks. Finally, four examples are given to illustrate the obtained results.      
### 31.Few-Shot Specific Emitter Identification via Deep Metric Ensemble Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.06592.pdf)
>  Specific emitter identification (SEI) is a highly potential technology for physical layer authentication that is one of the most critical supplement for the upper-layer authentication. SEI is based on radio frequency (RF) features from circuit difference, rather than cryptography. These features are inherent characteristic of hardware circuits, which difficult to counterfeit. Recently, various deep learning (DL)-based conventional SEI methods have been proposed, and achieved advanced performances. However, these methods are proposed for close-set scenarios with massive RF signal samples for training, and they generally have poor performance under the condition of limited training samples. Thus, we focus on few-shot SEI (FS-SEI) for aircraft identification via automatic dependent surveillance-broadcast (ADS-B) signals, and a novel FS-SEI method is proposed, based on deep metric ensemble learning (DMEL). Specifically, the proposed method consists of feature embedding and classification. The former is based on metric learning with complex-valued convolutional neural network (CVCNN) for extracting discriminative features with compact intra-category distance and separable inter-category distance, while the latter is realized by an ensemble classifier. Simulation results show that if the number of samples per category is more than 5, the average accuracy of our proposed method is higher than 98\%. Moreover, feature visualization demonstrates the advantages of our proposed method in both discriminability and generalization. The codes of this paper can be downloaded from GitHub(<a class="link-external link-https" href="https://github.com/BeechburgPieStar/Few-Shot-Specific-Emitter-Identification-via-Deep-Metric-Ensemble-Learning" rel="external noopener nofollow">this https URL</a>)      
### 32.Improving the diagnosis of breast cancer based on biophysical ultrasound features utilizing machine learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.06560.pdf)
>  The improved diagnostic accuracy of ultrasound breast examinations remains an important goal. In this study, we propose a biophysical feature based machine learning method for breast cancer detection to improve the performance beyond a benchmark deep learning algorithm and to furthermore provide a color overlay visual map of the probability of malignancy within a lesion. This overall framework is termed disease specific imaging. Previously, 150 breast lesions were segmented and classified utilizing a modified fully convolutional network and a modified GoogLeNet, respectively. In this study multiparametric analysis was performed within the contoured lesions. Features were extracted from ultrasound radiofrequency, envelope, and log compressed data based on biophysical and morphological models. The support vector machine with a Gaussian kernel constructed a nonlinear hyperplane, and we calculated the distance between the hyperplane and data point of each feature in multiparametric space. The distance can quantitatively assess a lesion, and suggest the probability of malignancy that is color coded and overlaid onto B mode images. Training and evaluation were performed on in vivo patient data. The overall accuracy for the most common types and sizes of breast lesions in our study exceeded 98.0% for classification and 0.98 for an area under the receiver operating characteristic curve, which is more precise than the performance of radiologists and a deep learning system. Further, the correlation between the probability and BI RADS enables a quantitative guideline to predict breast cancer. Therefore, we anticipate that the proposed framework can help radiologists achieve more accurate and convenient breast cancer classification and detection.      
### 33.Body Composition Assessment with Limited Field-of-view Computed Tomography: A Semantic Image Extension Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2207.06551.pdf)
>  Field-of-view (FOV) tissue truncation beyond the lungs is common in routine lung screening computed tomography (CT). This poses limitations for opportunistic CT- based body composition (BC) assessment as key anatomical structures are missing. Traditionally, extending the FOV of CT is considered as a CT reconstruction problem using limited data. However, this approach relies on the projection domain data which might not be available in application. In this work, we formulate the problem from the semantic image extension perspective which only requires image data as inputs. The proposed two-stage method identifies a new FOV border based on the estimated extent of the complete body and imputes missing tissues in the truncated region. The training samples are simulated using CT slices with complete body in FOV, making the model development self-supervised. We evaluate the validity of the proposed method in automatic BC assessment using lung screening CT with limited FOV. The proposed method effectively restores the missing tissues and reduces BC assessment error introduced by FOV tissue truncation. In the BC assessment for a large-scale lung screening CT dataset, this correction improves both the intra-subject consistency and the correlation with anthropometric approximations. The developed method is available at <a class="link-external link-https" href="https://github.com/MASILab/S-EFOV" rel="external noopener nofollow">this https URL</a>.      
### 34.A Deep Learning-Based GPR Forward Solver for Predicting B-Scans of Subsurface Objects  [ :arrow_down: ](https://arxiv.org/pdf/2207.06527.pdf)
>  The forward full-wave modeling of ground-penetrating radar (GPR) facilitates the understanding and interpretation of GPR data. Traditional forward solvers require excessive computational resources, especially when their repetitive executions are needed in signal processing and/or machine learning algorithms for GPR data inversion. To alleviate the computational burden, a deep learning-based 2D GPR forward solver is proposed to predict the GPR B-scans of subsurface objects buried in the heterogeneous soil. The proposed solver is constructed as a bimodal encoder-decoder neural network. Two encoders followed by an adaptive feature fusion module are designed to extract informative features from the subsurface permittivity and conductivity maps. The decoder subsequently constructs the B-scans from the fused feature representations. To enhance the network's generalization capability, transfer learning is employed to fine-tune the network for new scenarios vastly different from those in training set. Numerical results show that the proposed solver achieves a mean relative error of 1.28%. For predicting the B-scan of one subsurface object, the proposed solver requires 12 milliseconds, which is 22,500x less than the time required by a classical physics-based solver.      
### 35.One Model to Unite Them All: Personalized Federated Learning of Multi-Contrast MRI Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2207.06509.pdf)
>  Learning-based MRI translation involves a synthesis model that maps a source-contrast onto a target-contrast image. Multi-institutional collaborations are key to training synthesis models across broad datasets, yet centralized training involves privacy risks. Federated learning (FL) is a collaboration framework that instead adopts decentralized training to avoid sharing imaging data and mitigate privacy concerns. However, FL-trained models can be impaired by the inherent heterogeneity in the distribution of imaging data. On the one hand, implicit shifts in image distribution are evident across sites, even for a common translation task with fixed source-target configuration. Conversely, explicit shifts arise within and across sites when diverse translation tasks with varying source-target configurations are prescribed. To improve reliability against domain shifts, here we introduce the first personalized FL method for MRI Synthesis (pFLSynth). pFLSynth is based on an adversarial model equipped with a mapper that produces latents specific to individual sites and source-target contrasts. It leverages novel personalization blocks that adaptively tune the statistics and weighting of feature maps across the generator based on these latents. To further promote site-specificity, partial model aggregation is employed over downstream layers of the generator while upstream layers are retained locally. As such, pFLSynth enables training of a unified synthesis model that can reliably generalize across multiple sites and translation tasks. Comprehensive experiments on multi-site datasets clearly demonstrate the enhanced performance of pFLSynth against prior federated methods in multi-contrast MRI synthesis.      
### 36.A Data-Efficient Deep Learning Framework for Segmentation and Classification of Histopathology Images  [ :arrow_down: ](https://arxiv.org/pdf/2207.06489.pdf)
>  The current study of cell architecture of inflammation in histopathology images commonly performed for diagnosis and research purposes excludes a lot of information available on the biopsy slide. In autoimmune diseases, major outstanding research questions remain regarding which cell types participate in inflammation at the tissue level,and how they interact with each other. While these questions can be partially answered using traditional methods, artificial intelligence approaches for segmentation and classification provide a much more efficient method to understand the architecture of inflammation in autoimmune disease, holding a great promise for novel insights. In this paper, we empirically develop deep learning approaches that uses dermatomyositis biopsies of human tissue to detect and identify inflammatory cells. Our approach improves classification performance by 26% and segmentation performance by 5%. We also propose a novel post-processing autoencoder architecture that improves segmentation performance by an additional 3%. We have open-sourced our approach and architecture at <a class="link-external link-https" href="https://github.com/pranavsinghps1/DEDL" rel="external noopener nofollow">this https URL</a>      
### 37.Dynamic State Estimation for Load Bus Protection on Inverter-Interfaced Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2207.06474.pdf)
>  Inverter-interfaced microgrids results in challenges when designing protection systems. Traditional time-overcurrent, admittance, and differential protection methods are unsuitable on account of lack of fault current, excessively short lines, or a prohibitive number of protective devices needing to be installed. Current practice is to force all inverters to shut down during fault conditions, weakening resilience and reducing reliability. Dynamic state estimation (DSE), which has been explored for both line protection and load bus protection before, is a potential solution to these challenges to create widely utilizable, highly reliable protection systems. However, it has only been tested for load protection with ideal voltage sources, which do not capture the short-circuit behavior of inverter-interfaced generation, notably low fault current and unbalanced output voltage. This paper aims to extend the state-of-the-art on DSE load protection: the performance of DSE during short-circuit conditions with a grid-forming inverter with current-limiting behavior during fault conditions is investigated.      
### 38.Imaging through the Atmosphere using Turbulence Mitigation Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2207.06465.pdf)
>  Restoring images distorted by atmospheric turbulence is a long-standing problem due to the spatially varying nature of the distortion, nonlinearity of the image formation process, and scarcity of training and testing data. Existing methods often have strong statistical assumptions on the distortion model which in many cases will lead to a limited performance in real-world scenarios as they do not generalize. To overcome the challenge, this paper presents an end-to-end physics-driven approach that is efficient and can generalize to real-world turbulence. On the data synthesis front, we significantly increase the image resolution that can be handled by the SOTA turbulence simulator by approximating the random field via wide-sense stationarity. The new data synthesis process enables the generation of large-scale multi-level turbulence and ground truth pairs for training. On the network design front, we propose the turbulence mitigation transformer (TMT), a two stage U-Net shaped multi-frame restoration network which has a noval efficient self-attention mechanism named temporal channel joint attention (TCJA). We also introduce a new training scheme that is enabled by the new simulator, and we design new transformer units to reduce the memory consumption. Experimental results on both static and dynamic scenes are promising, including various real turbulence scenarios.      
### 39.Optimization of rule-based energy management strategies for hybrid vehicles using dynamic programming  [ :arrow_down: ](https://arxiv.org/pdf/2207.06450.pdf)
>  Reducing energy consumption is a key focus for hybrid electric vehicle (HEV) development. The popular vehicle dynamic model used in many energy management optimization studies does not capture the vehicle dynamics that the in-vehicle measurement system does. However, feedback from the measurement system is what the vehicle controller actually uses to manage energy consumption. Therefore, the optimization solely using the model does not represent what the vehicle controller sees in the vehicle. This paper reports the utility factor-weighted energy consumption using a rule-based strategy under a real-world representative drive cycle. In addition, the vehicle test data was used to perform the optimization approach. By comparing results from both rule-based and optimization-based strategies, the areas for further improving rule-based strategy are discussed. Furthermore, recent development of OBD raises a concern about the increase of energy consumption. This paper investigates the energy consumption increase with extensive OBD usage.      
### 40.Reconstruction of Time-varying Graph Signals via Sobolev Smoothness  [ :arrow_down: ](https://arxiv.org/pdf/2207.06439.pdf)
>  Graph Signal Processing (GSP) is an emerging research field that extends the concepts of digital signal processing to graphs. GSP has numerous applications in different areas such as sensor networks, machine learning, and image processing. The sampling and reconstruction of static graph signals have played a central role in GSP. However, many real-world graph signals are inherently time-varying and the smoothness of the temporal differences of such graph signals may be used as a prior assumption. In the current work, we assume that the temporal differences of graph signals are smooth, and we introduce a novel algorithm based on the extension of a Sobolev smoothness function for the reconstruction of time-varying graph signals from discrete samples. We explore some theoretical aspects of the convergence rate of our Time-varying Graph signal Reconstruction via Sobolev Smoothness (GraphTRSS) algorithm by studying the condition number of the Hessian associated with our optimization problem. Our algorithm has the advantage of converging faster than other methods that are based on Laplacian operators without requiring expensive eigenvalue decomposition or matrix inversions. The proposed GraphTRSS is evaluated on several datasets including two COVID-19 datasets and it has outperformed many existing state-of-the-art methods for time-varying graph signal reconstruction. GraphTRSS has also shown excellent performance on two environmental datasets for the recovery of particulate matter and sea surface temperature signals.      
### 41.Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2207.06418.pdf)
>  Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the WorldStrat dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels and Sentinel2 imagery are CC BY, and the source code and pre-trained models under BSD. The dataset is available at <a class="link-external link-https" href="https://zenodo.org/record/6810792" rel="external noopener nofollow">this https URL</a> and the software package at <a class="link-external link-https" href="https://github.com/worldstrat/worldstrat" rel="external noopener nofollow">this https URL</a> .      
### 42.Changepoint Detection for Real-Time Spectrum Sharing Radar  [ :arrow_down: ](https://arxiv.org/pdf/2207.06409.pdf)
>  Radar must adapt to changing environments, and we propose changepoint detection as a method to do so. In the world of increasingly congested radio frequencies, radars must adapt to avoid interference. Many radar systems employ the prediction action cycle to proactively determine transmission mode while spectrum sharing. This method constructs and implements a model of the environment to predict unused frequencies, and then transmits in this predicted availability. For these selection strategies, performance is directly reliant on the quality of the underlying environmental models. In order to keep up with a changing environment, these models can employ changepoint detection. Changepoint detection is the identification of sudden changes, or changepoints, in the distribution from which data is drawn. This information allows the models to discard "garbage" data from a previous distribution, which has no relation to the current state of the environment. In this work, bayesian online changepoint detection (BOCD) is applied to the sense and predict algorithm to increase the accuracy of its models and improve its performance. In the context of spectrum sharing, these changepoints represent interferers leaving and entering the spectral environment. The addition of changepoint detection allows for dynamic and robust spectrum sharing even as interference patterns change dramatically. BOCD is especially advantageous because it enables online changepoint detection, allowing models to be updated continuously as data are collected. This strategy can also be applied to many other predictive algorithms that create models in a changing environment.      
### 43.ECG beat classification using machine learning and pre-trained convolutional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.06408.pdf)
>  The electrocardiogram (ECG) is routinely used in hospitals to analyze cardiovascular status and health of an individual. Abnormal heart rhythms can be a precursor to more serious conditions including sudden cardiac death. Classifying abnormal rhythms is a laborious process prone to error. Therefore, tools that perform automated classification with high accuracy are highly desirable. The work presented classifies five different types of ECG arrhythmia based on AAMI EC57 standard and using the MIT-BIH data set. These include non-ectopic (normal), supraventricular, ventricular, fusion, and unknown beat. By appropriately transforming pre-processed ECG waveforms into a rich feature space along with appropriate post-processing and utilizing deep convolutional neural networks post fine-tuning and hyperparameter selection, it is shown that highly accurate classification for the five waveform types can be obtained. Performance on the test set indicated higher overall accuracy (98.62%), as well as better performance in classifying each of the five waveforms than hitherto reported in literature.      
### 44.ASUMAN: Age Sense Updating Multiple Access in Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.07094.pdf)
>  We consider a fully-connected wireless gossip network which consists of a source and $n$ receiver nodes. The source updates itself with a Poisson process and also sends updates to the nodes as Poisson arrivals. Upon receiving the updates, the nodes update their knowledge about the source. The nodes gossip the data among themselves in the form of Poisson arrivals to disperse their knowledge about the source. The total gossiping rate is bounded by a constraint. The goal of the network is to be as timely as possible with the source. In this work, we propose ASUMAN, a distributed opportunistic gossiping scheme, where after each time the source updates itself, each node waits for a time proportional to its current age and broadcasts a signal to the other nodes of the network. This allows the nodes in the network which have higher age to remain silent and only the low-age nodes to gossip, thus utilizing a significant portion of the constrained total gossip rate. We calculate the average age for a typical node in such a network with symmetric settings and show that the theoretical upper bound on the age scales as $O(1)$. ASUMAN, with an average age of $O(1)$, offers significant gains compared to a system where the nodes just gossip blindly with a fixed update rate in which case the age scales as $O(\log n)$.      
### 45.Efficient spike encoding algorithms for neuromorphic speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.07073.pdf)
>  Spiking Neural Networks (SNN) are known to be very effective for neuromorphic processor implementations, achieving orders of magnitude improvements in energy efficiency and computational latency over traditional deep learning approaches. Comparable algorithmic performance was recently made possible as well with the adaptation of supervised training algorithms to the context of SNN. However, information including audio, video, and other sensor-derived data are typically encoded as real-valued signals that are not well-suited to SNN, preventing the network from leveraging spike timing information. Efficient encoding from real-valued signals to spikes is therefore critical and significantly impacts the performance of the overall system. To efficiently encode signals into spikes, both the preservation of information relevant to the task at hand as well as the density of the encoded spikes must be considered. In this paper, we study four spike encoding methods in the context of a speaker independent digit classification system: Send on Delta, Time to First Spike, Leaky Integrate and Fire Neuron and Bens Spiker Algorithm. We first show that all encoding methods yield higher classification accuracy using significantly fewer spikes when encoding a bio-inspired cochleagram as opposed to a traditional short-time Fourier transform. We then show that two Send On Delta variants result in classification results comparable with a state of the art deep convolutional neural network baseline, while simultaneously reducing the encoded bit rate. Finally, we show that several encoding methods result in improved performance over the conventional deep learning baseline in certain cases, further demonstrating the power of spike encoding algorithms in the encoding of real-valued signals and that neuromorphic implementation has the potential to outperform state of the art techniques.      
### 46.Intelligent Reflective Surface vs. Mobile Relay-supported NLoS Avoidance in Indoor mmWave Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.07050.pdf)
>  The 6th generation of wireless communication (6G) is envisioned to give rise to various technologies for improving the end-to-end communication performance, where the communication is envisioned to utilize wireless signals in the millimeter wave (mmWave) frequencies and above. Among others, these technologies comprise Intelligent Reflective Surfaces (IRSs) and Mobile Relays (MRs), whose envisaged roles include mitigating the negative effects of Non-Line-of-Sight (NLoS) connectivity, in particular at mmWave and higher frequencies. The core idea behind these technologies is to use cooperative networking where the source sends a signal to a repeater, in this case the IRS or the MR, which is upon reception forwarded to the destination. When comparing the two technologies, it is important to realize that the IRSs are primarily envisioned to be static entities attached to various objects in the environment such as walls and furniture. In contrast, the MRs will feature a higher degree of freedom, as they will be able to position themselves seamlessly in the environment. Based on the above assumptions, we derive an approach for determining the optimal position of the IRS and MR in indoor environments, i.e., the one that maximizes the end-to-end link quality between the source and the destination. We follow by capturing the communication quality indicators for both IRS- and MR-supported NLoS avoidance in indoor mmWave communication in a number of scenarios. Our results show that, from the end-to-end link quality perspective, the MRs generally outperform the IRSs, suggesting their utilization potential for throughput-optimized NLoS avoidance scenarios.      
### 47.A Single Self-Supervised Model for Many Speech Modalities Enables Zero-Shot Modality Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2207.07036.pdf)
>  While audio-visual speech models can yield superior performance and robustness compared to audio-only models, their development and adoption are hindered by the lack of labeled and unlabeled audio-visual data and the cost to deploy one model per modality. In this paper, we present u-HuBERT, a self-supervised pre-training framework that can leverage both multimodal and unimodal speech with a unified masked cluster prediction objective. By utilizing modality dropout during pre-training, we demonstrate that a single fine-tuned model can achieve performance on par or better than the state-of-the-art modality-specific models. Moreover, our model fine-tuned only on audio can perform well with audio-visual and visual speech input, achieving zero-shot modality generalization for speech recognition and speaker verification. In particular, our single model yields 1.2%/1.4%/27.2% speech recognition word error rate on LRS3 with audio-visual/audio/visual input.      
### 48.Multitrack Music Transformer: Learning Long-Term Dependencies in Music with Diverse Instruments  [ :arrow_down: ](https://arxiv.org/pdf/2207.06983.pdf)
>  Existing approaches for generating multitrack music with transformer models have been limited to either a small set of instruments or short music segments. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations for multitrack music. In this work, we propose a compact representation that allows a diverse set of instruments while keeping a short sequence length. Using our proposed representation, we present the Multitrack Music Transformer (MTMT) for learning long-term dependencies in multitrack music. In a subjective listening test, our proposed model achieves competitive quality on unconditioned generation against two baseline models. We also show that our proposed model can generate samples that are twice as long as those produced by the baseline models, and, further, can do so in half the inference time. Moreover, we propose a new measure for analyzing musical self-attentions and show that the trained model learns to pay less attention to notes that form a dissonant interval with the current note, yet attending more to notes that are 4N beats away from current. Finally, our findings provide a novel foundation for future work exploring longer-form multitrack music generation and improving self-attentions for music. All source code and audio samples can be found at <a class="link-external link-https" href="https://salu133445.github.io/mtmt/" rel="external noopener nofollow">this https URL</a> .      
### 49.Proceedings of the ICML 2022 Expressive Vocalizations Workshop and Competition: Recognizing, Generating, and Personalizing Vocal Bursts  [ :arrow_down: ](https://arxiv.org/pdf/2207.06958.pdf)
>  This is the Proceedings of the ICML Expressive Vocalization (ExVo) Competition. The ExVo competition focuses on understanding and generating vocal bursts: laughs, gasps, cries, and other non-verbal vocalizations that are central to emotional expression and communication. ExVo 2022, included three competition tracks using a large-scale dataset of 59,201 vocalizations from 1,702 speakers. The first, ExVo-MultiTask, requires participants to train a multi-task model to recognize expressed emotions and demographic traits from vocal bursts. The second, ExVo-Generate, requires participants to train a generative model that produces vocal bursts conveying ten different emotions. The third, ExVo-FewShot, requires participants to leverage few-shot learning incorporating speaker identity to train a model for the recognition of 10 emotions conveyed by vocal bursts.      
### 50.Sub 8-Bit Quantization of Streaming Keyword Spotting Models for Embedded Chipsets  [ :arrow_down: ](https://arxiv.org/pdf/2207.06920.pdf)
>  We propose a novel 2-stage sub 8-bit quantization aware training algorithm for all components of a 250K parameter feedforward, streaming, state-free keyword spotting model. For the 1st-stage, we adapt a recently proposed quantization technique using a non-linear transformation with tanh(.) on dense layer weights. In the 2nd-stage, we use linear quantization methods on the rest of the network, including other parameters (bias, gain, batchnorm), inputs, and activations. We conduct large scale experiments, training on 26,000 hours of de-identified production, far-field and near-field audio data (evaluating on 4,000 hours of data). We organize our results in two embedded chipset settings: a) with commodity ARM NEON instruction set and 8-bit containers, we present accuracy, CPU, and memory results using sub 8-bit weights (4, 5, 8-bit) and 8-bit quantization of rest of the network; b) with off-the-shelf neural network accelerators, for a range of weight bit widths (1 and 5-bit), while presenting accuracy results, we project reduction in memory utilization. In both configurations, our results show that the proposed algorithm can achieve: a) parity with a full floating point model's operating point on a detection error tradeoff (DET) curve in terms of false detection rate (FDR) at false rejection rate (FRR); b) significant reduction in compute and memory, yielding up to 3 times improvement in CPU consumption and more than 4 times improvement in memory consumption.      
### 51.Online Bayesian Meta-Learning for Cognitive Tracking Radar  [ :arrow_down: ](https://arxiv.org/pdf/2207.06917.pdf)
>  A key component of cognitive radar is the ability to generalize, or achieve consistent performance across a broad class of sensing environments, since aspects of the physical scene may vary over time. This presents a challenge for learning-based waveform selection approaches, since transmission policies which are effective in one scene may be highly suboptimal in another. One way to address this problem is to bias a learning algorithm strategically by exploiting high-level structure across tracking instances, referred to as meta-learning. In this work, we develop an online meta-learning approach for waveform-agile tracking. This approach uses information gained from previous target tracks to speed up and enhance learning in new tracking instances. This results in sample-efficient learning across a class of finite state target channels by exploiting inherent similarity across tracking scenes, attributed to common physical elements such as target type or clutter. We formulate the online waveform selection problem in the framework of Bayesian learning, and provide prior-dependent performance bounds for the meta-learning problem using PAC-Bayes theory. We present a computationally feasible posterior sampling algorithm and study the performance in a simulation study consisting of diverse scenes. Finally, we examine the potential performance benefits and practical challenges associated with online meta-learning for waveform-agile tracking.      
### 52.A Comprehensive Review on Digital Image Watermarking  [ :arrow_down: ](https://arxiv.org/pdf/2207.06909.pdf)
>  The advent of the Internet led to the easy availability of digital data like images, audio, and video. Easy access to multimedia gives rise to the issues such as content authentication, security, copyright protection, and ownership identification. Here, we discuss the concept of digital image watermarking with a focus on the technique used in image watermark embedding and extraction of the watermark. The detailed classification along with the basic characteristics, namely visual imperceptibility, robustness, capacity, security of digital watermarking is also presented in this work. Further, we have also discussed the recent application areas of digital watermarking such as healthcare, remote education, electronic voting systems, and the military. The robustness is evaluated by examining the effect of image processing attacks on the signed content and the watermark recoverability. The authors believe that the comprehensive survey presented in this paper will help the new researchers to gather knowledge in this domain. Further, the comparative analysis can enkindle ideas to improve upon the already mentioned techniques.      
### 53.Open-source software for electrical engineering applications requiring consideration of electrodynamics: elecode  [ :arrow_down: ](https://arxiv.org/pdf/2207.06908.pdf)
>  The work presents elecode, open-source software for various electrical engineering applications that require considering electromagnetic processes. The primary focus of the software is power engineering applications. However, the software does not impose any specific limitations preventing other uses. In contrast to other open-source software based on the Finite Difference Time Domain (FDTD) method, elecode implements various thin wire modeling techniques which allow simulating complex objects consisting of wires. In addition, implemented graphical user interface (GUI) helps modify models conveniently. The software provides auxiliary numerical methods for simulations and measurements of the electrical soil properties, allows conducting lightning-related simulations (including those involving isolation breakdown models), and calculations of grounding characteristics. The part of the code responsible for FDTD simulations is well tested in previous works. Recently, the code was rewritten in order to add a convenient interface for using it as a library, command-line program, or GUI program. Finally, the code was released under an open-source license. The main capabilities of the software are described in the work. Several simulation examples covering main software features are presented. elecode is available at <a class="link-external link-https" href="https://gitlab.com/dmika/elecode" rel="external noopener nofollow">this https URL</a>.      
### 54.Frequency-Encoded Deep Learning with Speed-of-Light Dominated Latency  [ :arrow_down: ](https://arxiv.org/pdf/2207.06883.pdf)
>  The ability of deep neural networks to perform complex tasks more accurately than manually-crafted solutions has created a substantial demand for more complex models processing larger amounts of data. However, the traditional computing architecture has reached a bottleneck in processing performance due to data movement from memory to computing. Considerable efforts have been made towards custom hardware acceleration, among which are optical neural networks (ONNs). These excel at energy efficient linear operations but struggle with scalability and the integration of linear and nonlinear functions. Here, we introduce our multiplicative analog frequency transform optical neural network (MAFT-ONN) that encodes the data in the frequency domain to compute matrix-vector products in a single-shot using a single photoelectric multiplication, and then implements the nonlinear activation for all neurons using a single electro-optic modulator. We experimentally demonstrate a 3-layer DNN with our architecture using a simple hardware setup assembled with commercial components. Additionally, this is the first DNN hardware accelerator suitable for analog inference of temporal waveforms like voice or radio signals, achieving bandwidth-limited throughput and speed-of-light limited latency. Our results demonstrate a highly scalable ONN with a straightforward path to surpassing the current computing bottleneck, in addition to enabling new possibilities for high-performance analog deep learning of temporal waveforms.      
### 55.Data Augmentation for Low-Resource Quechua ASR Improvement  [ :arrow_down: ](https://arxiv.org/pdf/2207.06872.pdf)
>  Automatic Speech Recognition (ASR) is a key element in new services that helps users to interact with an automated system. Deep learning methods have made it possible to deploy systems with word error rates below 5% for ASR of English. However, the use of these methods is only available for languages with hundreds or thousands of hours of audio and their corresponding transcriptions. For the so-called low-resource languages to speed up the availability of resources that can improve the performance of their ASR systems, methods of creating new resources on the basis of existing ones are being investigated. In this paper we describe our data augmentation approach to improve the results of ASR models for low-resource and agglutinative languages. We carry out experiments developing an ASR for Quechua using the wav2letter++ model. We reduced WER by 8.73% through our approach to the base model. The resulting ASR model obtained 22.75% WER and was trained with 99 hours of original resources and 99 hours of synthetic data obtained with a combination of text augmentation and synthetic speech generati      
### 56.Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.06867.pdf)
>  Self-supervised learning (SSL) is seen as a very promising approach with high performance for several speech downstream tasks. Since the parameters of SSL models are generally so large that training and inference require a lot of memory and computational cost, it is desirable to produce compact SSL models without a significant performance degradation by applying compression methods such as knowledge distillation (KD). Although the KD approach is able to shrink the depth and/or width of SSL model structures, there has been little research on how varying the depth and width impacts the internal representation of the small-footprint model. This paper provides an empirical study that addresses the question. We investigate the performance on SUPERB while varying the structure and KD methods so as to keep the number of parameters constant; this allows us to analyze the contribution of the representation introduced by varying the model architecture. Experiments demonstrate that a certain depth is essential for solving content-oriented tasks (e.g. automatic speech recognition) accurately, whereas a certain width is necessary for achieving high performance on several speaker-oriented tasks (e.g. speaker identification). Based on these observations, we identify, for SUPERB, a more compressed model with better performance than previous studies.      
### 57.RSD-GAN: Regularized Sobolev Defense GAN Against Speech-to-Text Adversarial Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2207.06858.pdf)
>  This paper introduces a new synthesis-based defense algorithm for counteracting with a varieties of adversarial attacks developed for challenging the performance of the cutting-edge speech-to-text transcription systems. Our algorithm implements a Sobolev-based GAN and proposes a novel regularizer for effectively controlling over the functionality of the entire generative model, particularly the discriminator network during training. Our achieved results upon carrying out numerous experiments on the victim DeepSpeech, Kaldi, and Lingvo speech transcription systems corroborate the remarkable performance of our defense approach against a comprehensive range of targeted and non-targeted adversarial attacks.      
### 58.Reduction in optimal control with broken symmetry for collision and obstacle avoidance of multi-agent system on Lie groups  [ :arrow_down: ](https://arxiv.org/pdf/2207.06806.pdf)
>  We study the reduction by symmetry for optimality conditions in optimal control problems of left-invariant affine multi-agent control systems, with partial symmetry breaking cost functions for continuous-time and discrete-time systems. We recast the optimal control problem as a constrained variational problem with a partial symmetry breaking Lagrangian and obtain the reduced optimality conditions from a reduced variational principle via symmetry reduction techniques in both settings, continuous-time, and discrete-time. We apply the results to a collision and obstacle avoidance problem for multiple vehicles evolving on $SE(2)$ in the presence of a static obstacle.      
### 59.Semi-supervised cross-lingual speech emotion recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.06767.pdf)
>  Speech emotion recognition (SER) on a single language has achieved remarkable results through deep learning approaches over the last decade. However, cross-lingual SER remains a challenge in real-world applications due to (i) a large difference between the source and target domain distributions, (ii) the availability of few labeled and many unlabeled utterances for the new language. Taking into account previous aspects, we propose a Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition when a few labels from the new language are available. Based on a Convolutional Neural Network (CNN), our method adapts to a new language by exploiting a pseudo-labeling strategy for the unlabeled utterances. In particular, the use of a hard and soft pseudo-labels approach is investigated. We thoroughly evaluate the performance of the method in a speaker-independent setup on both the source and the new language and show its robustness across five languages belonging to different linguistic strains.      
### 60.Two-Pass Low Latency End-to-End Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2207.06670.pdf)
>  End-to-end (E2E) models are becoming increasingly popular for spoken language understanding (SLU) systems and are beginning to achieve competitive performance to pipeline-based approaches. However, recent work has shown that these models struggle to generalize to new phrasings for the same intent indicating that models cannot understand the semantic content of the given utterance. In this work, we incorporated language models pre-trained on unlabeled text data inside E2E-SLU frameworks to build strong semantic representations. Incorporating both semantic and acoustic information can increase the inference time, leading to high latency when deployed for applications like voice assistants. We developed a 2-pass SLU system that makes low latency prediction using acoustic information from the few seconds of the audio in the first pass and makes higher quality prediction in the second pass by combining semantic and acoustic representations. We take inspiration from prior work on 2-pass end-to-end speech recognition systems that attends on both audio and first-pass hypothesis using a deliberation network. The proposed 2-pass SLU system outperforms the acoustic-based SLU model on the Fluent Speech Commands Challenge Set and SLURP dataset and reduces latency, thus improving user experience. Our code and models are publicly available as part of the ESPnet-SLU toolkit.      
### 61.T-RECX: Tiny-Resource Efficient Convolutional Neural Networks with Early-Exit  [ :arrow_down: ](https://arxiv.org/pdf/2207.06613.pdf)
>  Deploying Machine learning (ML) on the milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and IoT. However, the capabilities of tinyML are restricted by strict power and compute constraints. The majority of the contemporary research in tinyML focuses on model compression techniques such as model pruning and quantization to fit ML models on low-end devices. Nevertheless, the improvements in energy consumption and inference time obtained by existing techniques are limited because aggressive compression quickly shrinks model capacity and accuracy. Another approach to improve inference time and/or reduce power while preserving its model capacity is through early-exit networks. These networks place intermediate classifiers along a baseline neural network that facilitate early exit from neural network computation if an intermediate classifier exhibits sufficient confidence in its prediction. Previous work on early-exit networks have focused on large networks, beyond what would typically be used for tinyML applications. In this paper, we discuss the challenges of adding early-exits to state-of-the-art tiny-CNNs and devise an early-exit architecture, T-RECX, that addresses these challenges. In addition, we develop a method to alleviate the effect of network overthinking at the final exit by leveraging the high-level representations learned by the early-exit. We evaluate T-RECX on three CNNs from the MLPerf tiny benchmark suite for image classification, keyword spotting and visual wake word detection tasks. Our results demonstrate that T-RECX improves the accuracy of baseline network and significantly reduces the average inference time of tiny-CNNs. T-RECX achieves 32.58% average reduction in FLOPS in exchange for 1% accuracy across all evaluated models. Also, our techniques increase the accuracy of baseline network in two out of three models we evaluate      
### 62.Virtual stain transfer in histology via cascaded deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.06578.pdf)
>  Pathological diagnosis relies on the visual inspection of histologically stained thin tissue specimens, where different types of stains are applied to bring contrast to and highlight various desired histological features. However, the destructive histochemical staining procedures are usually irreversible, making it very difficult to obtain multiple stains on the same tissue section. Here, we demonstrate a virtual stain transfer framework via a cascaded deep neural network (C-DNN) to digitally transform hematoxylin and eosin (H&amp;E) stained tissue images into other types of histological stains. Unlike a single neural network structure which only takes one stain type as input to digitally output images of another stain type, C-DNN first uses virtual staining to transform autofluorescence microscopy images into H&amp;E and then performs stain transfer from H&amp;E to the domain of the other stain in a cascaded manner. This cascaded structure in the training phase allows the model to directly exploit histochemically stained image data on both H&amp;E and the target special stain of interest. This advantage alleviates the challenge of paired data acquisition and improves the image quality and color accuracy of the virtual stain transfer from H&amp;E to another stain. We validated the superior performance of this C-DNN approach using kidney needle core biopsy tissue sections and successfully transferred the H&amp;E-stained tissue images into virtual PAS (periodic acid-Schiff) stain. This method provides high-quality virtual images of special stains using existing, histochemically stained slides and creates new opportunities in digital pathology by performing highly accurate stain-to-stain transformations.      
### 63.Scheduling Out-of-Coverage Vehicular Communications Using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.06537.pdf)
>  Performance of vehicle-to-vehicle (V2V) communications depends highly on the employed scheduling approach. While centralized network schedulers offer high V2V communication reliability, their operation is conventionally restricted to areas with full cellular network coverage. In contrast, in out-of-cellular-coverage areas, comparatively inefficient distributed radio resource management is used. To exploit the benefits of the centralized approach for enhancing the reliability of V2V communications on roads lacking cellular coverage, we propose VRLS (Vehicular Reinforcement Learning Scheduler), a centralized scheduler that proactively assigns resources for out-of-coverage V2V communications \textit{before} vehicles leave the cellular network coverage. By training in simulated vehicular environments, VRLS can learn a scheduling policy that is robust and adaptable to environmental changes, thus eliminating the need for targeted (re-)training in complex real-life environments. We evaluate the performance of VRLS under varying mobility, network load, wireless channel, and resource configurations. VRLS outperforms the state-of-the-art distributed scheduling algorithm in zones without cellular network coverage by reducing the packet error rate by half in highly loaded conditions and achieving near-maximum reliability in low-load scenarios.      
### 64.High-fidelity intensity diffraction tomography with a non-paraxial multiple-scattering model  [ :arrow_down: ](https://arxiv.org/pdf/2207.06532.pdf)
>  We propose a novel intensity diffraction tomography (IDT) reconstruction algorithm based on the split-step non-paraxial (SSNP) model for recovering the 3D refractive index (RI) distribution of multiple-scattering biological samples. High-quality IDT reconstruction requires high-angle illumination to encode both low- and high- spatial frequency information of the 3D biological sample. We show that our SSNP model can more accurately compute multiple scattering from high-angle illumination compared to paraxial approximation-based multiple-scattering models. We apply this SSNP model to both sequential and multiplexed IDT techniques. We develop a unified reconstruction algorithm for both IDT modalities that is highly computationally efficient and is implemented by a modular automatic differentiation framework. We demonstrate the capability of our reconstruction algorithm on both weakly scattering buccal epithelial cells and strongly scattering live $\textit{C. elegans}$ worms and live $\textit{C. elegans}$ embryos.      
### 65.Gridless Channel Estimation for MmWave Hybrid Massive MIMO Systems with Low-Resolution ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2207.06451.pdf)
>  This paper proposes the Newtonized fully corrective forward greedy selection-cross validation-based (NFCFGS-CV-based) channel estimator for millimeter (mmWave) hybrid massive multiple-input multiple-output (MIMO) systems with low-resolution analog-to-digital converters (ADCs). The proposed NFCFGS algorithm is a gridless compressed sensing (CS) technique that combines the FCFGS and Newtonized orthogonal matching pursuit (NOMP) algorithms. In particular, NFCFGS performs single path estimation over the continuum at each iteration based on the previously estimated paths. The CV technique is adopted as an indicator of termination in the absence of the prior knowledge on the number of paths, which is a model validation technique that prevents overfitting.      
### 66.Optimal control of dielectric elastomer actuated multibody dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.06424.pdf)
>  In this work, a simulation model for the optimal control of dielectric elastomer actuated flexible multibody dynamics systems is presented. The Dielectric Elastomer Actuator (DEA) behaves like a flexible artificial muscles in soft robotics. It is modeled as an electromechanically coupled geometrically exact beam, where the electric charges serve as control variables. The DEA-beam is integrated as an actuator into multibody systems consisting of rigid and flexible components. The model also represents contact interaction via unilateral constraints between the beam actuator and e.g. a rigid body during the grasping process of a soft robot. Specifically for the DEA, a work conjugated electric displacement and strain-like electric variables are derived for the Cosserat beam. With a mathematically concise and physically representative formulation, a reduced free energy function is developed for the beam-DEA. In the optimal control problem, an objective function is minimized while the dynamic balance equations for the multibody system have to be fulfilled together with the complementarity conditions for the contact and boundary conditions. The optimal control problem is solved via a direct transcription method, transforming it into a constrained nonlinear optimization problem. The beam is firstly semidiscretized with 1D finite elements and then the multibody dynamics is temporally discretized with a variational integrator leading to the discrete Euler-Lagrange equations, which are further reduced with the null space projection. The discrete Euler-Lagrange equations and the boundary conditions serve as equality constraints, whereas the contact constraints are treated as inequality constraints in the optimization of the discretized objective. The effectiveness of the developed model is demonstrated by three numerical examples, including a cantilever beam, a soft robotic worm and a soft grasper.      
### 67.Wakeword Detection under Distribution Shifts  [ :arrow_down: ](https://arxiv.org/pdf/2207.06423.pdf)
>  We propose a novel approach for semi-supervised learning (SSL) designed to overcome distribution shifts between training and real-world data arising in the keyword spotting (KWS) task. Shifts from training data distribution are a key challenge for real-world KWS tasks: when a new model is deployed on device, the gating of the accepted data undergoes a shift in distribution, making the problem of timely updates via subsequent deployments hard. Despite the shift, we assume that the marginal distributions on labels do not change. We utilize a modified teacher/student training framework, where labeled training data is augmented with unlabeled data. Note that the teacher does not have access to the new distribution as well. To train effectively with a mix of human and teacher labeled data, we develop a teacher labeling strategy based on confidence heuristics to reduce entropy on the label distribution from the teacher model; the data is then sampled to match the marginal distribution on the labels. Large scale experimental results show that a convolutional neural network (CNN) trained on far-field audio, and evaluated on far-field audio drawn from a different distribution, obtains a 14.3% relative improvement in false discovery rate (FDR) at equal false reject rate (FRR), while yielding a 5% improvement in FDR under no distribution shift. Under a more severe distribution shift from far-field to near-field audio with a smaller fully connected network (FCN) our approach achieves a 52% relative improvement in FDR at equal FRR, while yielding a 20% relative improvement in FDR on the original distribution.      
### 68.MorphoActivation: Generalizing ReLU activation function by mathematical morphology  [ :arrow_down: ](https://arxiv.org/pdf/2207.06413.pdf)
>  This paper analyses both nonlinear activation functions and spatial max-pooling for Deep Convolutional Neural Networks (DCNNs) by means of the algebraic basis of mathematical morphology. Additionally, a general family of activation functions is proposed by considering both max-pooling and nonlinear operators in the context of morphological representations. Experimental section validates the goodness of our approach on classical benchmarks for supervised learning by DCNN.      
### 69.Information Design for Vehicle-to-Vehicle Communication  [ :arrow_down: ](https://arxiv.org/pdf/2207.06411.pdf)
>  The emerging technology of Vehicle-to-Vehicle (V2V) communication over vehicular ad hoc networks promises to improve road safety by allowing vehicles to autonomously warn each other of road hazards. However, research on other transportation information systems has shown that informing only a subset of drivers of road conditions may have a perverse effect of increasing congestion. In the context of a simple (yet novel) model of V2V hazard information sharing, we ask whether partial adoption of this technology can similarly lead to undesirable outcomes. In our model, drivers individually choose how recklessly to behave as a function of information received from other V2V-enabled cars, and the resulting aggregate behavior influences the likelihood of accidents (and thus the information propagated by the vehicular network). We fully characterize the game-theoretic equilibria of this model using our new equilibrium concept. Our model indicates that for a wide range of the parameter space, V2V information sharing surprisingly increases the equilibrium frequency of accidents relative to no V2V information sharing, and that it may increase equilibrium social cost as well.      
### 70.Time-Staging Enhancement of Hybrid System Falsification  [ :arrow_down: ](https://arxiv.org/pdf/1803.03866.pdf)
>  Optimization-based falsification employs stochastic optimization algorithms to search for error input of hybrid systems. In this paper we introduce a simple idea to enhance falsification, namely time staging, that allows the time-causal structure of time-dependent signals to be exploited by the optimizers. Time staging consists of running a falsification solver multiple times, from one interval to another, incrementally constructing an input signal candidate. Our experiments show that time staging can dramatically increase performance in some realistic examples. We also present theoretical results that suggest the kinds of models and specifications for which time staging is likely to be effective.      
