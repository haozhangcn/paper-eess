# ArXiv eess --Mon, 4 Jul 2022
### 1.How can spherical CNNs benefit ML-based diffusion MRI parameter estimation?  [ :arrow_down: ](https://arxiv.org/pdf/2207.00572.pdf)
>  This paper demonstrates spherical convolutional neural networks (S-CNN) offer distinct advantages over conventional fully-connected networks (FCN) at estimating scalar parameters of tissue microstructure from diffusion MRI (dMRI). Such microstructure parameters are valuable for identifying pathology and quantifying its extent. However, current clinical practice commonly acquires dMRI data consisting of only 6 diffusion weighted images (DWIs), limiting the accuracy and precision of estimated microstructure indices. Machine learning (ML) has been proposed to address this challenge. However, existing ML-based methods are not robust to differing dMRI gradient sampling schemes, nor are they rotation equivariant. Lack of robustness to sampling schemes requires a new network to be trained for each scheme, complicating the analysis of data from multiple sources. A possible consequence of the lack of rotational equivariance is that the training dataset must contain a diverse range of microstucture orientations. Here, we show spherical CNNs represent a compelling alternative that is robust to new sampling schemes as well as offering rotational equivariance. We show the latter can be leveraged to decrease the number of training datapoints required.      
### 2.FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.00555.pdf)
>  Large-scale speech self-supervised learning (SSL) has emerged to the main field of speech processing, however, the problem of computational cost arising from its vast size makes a high entry barrier to academia. In addition, existing distillation techniques of speech SSL models compress the model by reducing layers, which induces performance degradation in linguistic pattern recognition tasks such as phoneme recognition (PR). In this paper, we propose FitHuBERT, which makes thinner in dimension throughout almost all model components and deeper in layer compared to prior speech SSL distillation works. Moreover, we employ a time-reduction layer to speed up inference time and propose a method of hint-based distillation for less performance degradation. Our method reduces the model to 23.8% in size and 35.9% in inference time compared to HuBERT. Also, we achieve 12.1% word error rate and 13.3% phoneme error rate on the SUPERB benchmark which is superior than prior work.      
### 3.Ray-Space Motion Compensation for Lenslet Plenoptic Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.00522.pdf)
>  Plenoptic images and videos bearing rich information demand a tremendous amount of data storage and high transmission cost. While there has been much study on plenoptic image coding, investigations into plenoptic video coding have been very limited. We investigate the motion compensation for plenoptic video coding from a slightly different perspective by looking at the problem in the ray-space domain instead of in the conventional pixel domain. Here, we develop a novel motion compensation scheme for lenslet video under two sub-cases of ray-space motion, that is, integer ray-space motion and fractional ray-space motion. The proposed new scheme of light field motion-compensated prediction is designed such that it can be easily integrated into well-known video coding techniques such as HEVC. Experimental results compared to relevant existing methods have shown remarkable compression efficiency with an average gain of 19.63% and a peak gain of 29.1%.      
### 4.Reinforcement Learning Based User-Guided Motion Planning for Human-Robot Collaboration  [ :arrow_down: ](https://arxiv.org/pdf/2207.00492.pdf)
>  Robots are good at performing repetitive tasks in modern manufacturing industries. However, robot motions are mostly planned and preprogrammed with a notable lack of adaptivity to task changes. Even for slightly changed tasks, the whole system must be reprogrammed by robotics experts. Therefore, it is highly desirable to have a flexible motion planning method, with which robots can adapt to specific task changes in unstructured environments, such as production systems or warehouses, with little or no intervention from non-expert personnel. In this paper, we propose a user-guided motion planning algorithm in combination with the reinforcement learning (RL) method to enable robots automatically generate their motion plans for new tasks by learning from a few kinesthetic human demonstrations. To achieve adaptive motion plans for a specific application environment, e.g., desk assembly or warehouse loading/unloading, a library is built by abstracting features of common human demonstrated tasks. The definition of semantical similarity between features in the library and features of a new task is proposed and further used to construct the reward function in RL. The RL policy can automatically generate motion plans for a new task if it determines that new task constraints can be satisfied with the current library and request additional human demonstrations. Multiple experiments conducted on common tasks and scenarios demonstrate that the proposed user-guided RL-assisted motion planning method is effective.      
### 5.Exploring the solution space of linear inverse problems with GAN latent geometry  [ :arrow_down: ](https://arxiv.org/pdf/2207.00460.pdf)
>  Inverse problems consist in reconstructing signals from incomplete sets of measurements and their performance is highly dependent on the quality of the prior knowledge encoded via regularization. While traditional approaches focus on obtaining a unique solution, an emerging trend considers exploring multiple feasibile solutions. In this paper, we propose a method to generate multiple reconstructions that fit both the measurements and a data-driven prior learned by a generative adversarial network. In particular, we show that, starting from an initial solution, it is possible to find directions in the latent space of the generative model that are null to the forward operator, and thus keep consistency with the measurements, while inducing significant perceptual change. Our exploration approach allows to generate multiple solutions to the inverse problem an order of magnitude faster than existing approaches; we show results on image super-resolution and inpainting problems.      
### 6.SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors  [ :arrow_down: ](https://arxiv.org/pdf/2207.00458.pdf)
>  Optical coherence tomography (OCT) is a non-invasive 3D modality widely used in ophthalmology for imaging the retina. Achieving automated, anatomically coherent retinal layer segmentation on OCT is important for the detection and monitoring of different retinal diseases, like Age-related Macular Disease (AMD) or Diabetic Retinopathy. However, the majority of state-of-the-art layer segmentation methods are based on purely supervised deep-learning, requiring a large amount of pixel-level annotated data that is expensive and hard to obtain. With this in mind, we introduce a semi-supervised paradigm into the retinal layer segmentation task that makes use of the information present in large-scale unlabeled datasets as well as anatomical priors. In particular, a novel fully differentiable approach is used for converting surface position regression into a pixel-wise structured segmentation, allowing to use both 1D surface and 2D layer representations in a coupled fashion to train the model. In particular, these 2D segmentations are used as anatomical factors that, together with learned style factors, compose disentangled representations used for reconstructing the input image. In parallel, we propose a set of anatomical priors to improve network training when a limited amount of labeled data is available. We demonstrate on the real-world dataset of scans with intermediate and wet-AMD that our method outperforms state-of-the-art when using our full training set, but more importantly largely exceeds state-of-the-art when it is trained with a fraction of the labeled data.      
### 7.Implicit adaptation of mesh model of transient heat conduction problem  [ :arrow_down: ](https://arxiv.org/pdf/2207.00444.pdf)
>  Considering high-temperature heating, the equations of transient heat conduction model require an adaptation, i.e. the dependence of thermophysical parameters of the model on the temperature is to be identified for each specific material to be heated. This problem is most often solved by approximation of the tabular data on the measurements of the required parameters, which can be found in the literature, by means of regression equations. But, for example, considering the steel heating process, this approach is difficult to be implemented due to the lack of tabular discrete measurements for many grades of steel, such as alloyed ones. In this paper, the new approach is proposed, which is based on a solution of a related variational problem. Its main idea is to substitute the adaptation process in the classical sense (i.e., to find the dependencies of thermophysical parameters on temperature) with 'supervised learning' of a mesh model on the basis of the technological data received from the plant. The equations to adjust the parameters of the transient heat conduction model, which are related to the thermophysical coefficients, have been derived. A numerical experiment is conducted for steel of a particular group of grades, for which enough both technological as well as tabular data are available. As a result, the 'trained' mesh model, which has not received explicitly any information about the physical and chemical properties of the heated substance, demonstrated an average error of 18.820 C, which is quite close to the average error of the model adapted classically on the basis of the tabular data (18.10 C).      
### 8.WNet: A data-driven dual-domain denoising model for sparse-view computed tomography with a trainable reconstruction layer  [ :arrow_down: ](https://arxiv.org/pdf/2207.00400.pdf)
>  Deep learning based solutions are being succesfully implemented for a wide variety of applications. Most notably, clinical use-cases have gained an increased interest and have been the main driver behind some of the cutting-edge data-driven algorithms proposed in the last years. For applications like sparse-view tomographic reconstructions, where the amount of measurement data is small in order to keep acquisition times short and radiation dose low, reduction of the streaking artifacts has prompted the development of data-driven denoising algorithms with the main goal of obtaining diagnostically viable images with only a subset of a full-scan data. We propose WNet, a data-driven dual-domain denoising model which contains a trainable reconstruction layer for sparse-view artifact denoising. Two encoder-decoder networks perform denoising in both sinogram- and reconstruction-domain simultaneously, while a third layer implementing the Filtered Backprojection algorithm is sandwiched between the first two and takes care of the reconstruction operation. We investigate the performance of the network on sparse-view chest CT scans, and we highlight the added benefit of having a trainable reconstruction layer over the more conventional fixed ones. We train and test our network on two clinically relevant datasets and we compare the obtained results with three different types of sparse-view CT denoising and reconstruction algorithms.      
### 9.Learning Subject-Invariant Representations from Speech-Evoked EEG Using Variational Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2207.00323.pdf)
>  The electroencephalogram (EEG) is a powerful method to understand how the brain processes speech. Linear models have recently been replaced for this purpose with deep neural networks and yield promising results. In related EEG classification fields, it is shown that explicitly modeling subject-invariant features improves generalization of models across subjects and benefits classification accuracy. In this work, we adapt factorized hierarchical variational autoencoders to exploit parallel EEG recordings of the same stimuli. We model EEG into two disentangled latent spaces. Subject accuracy reaches 98.96% and 1.60% on respectively the subject and content latent space, whereas binary content classification experiments reach an accuracy of 51.51% and 62.91% on respectively the subject and content latent space.      
### 10.A framework for receding-horizon control in infinite-horizon aggregative games  [ :arrow_down: ](https://arxiv.org/pdf/2207.00305.pdf)
>  A novel modelling framework is proposed for the analysis of aggregative games on an infinite-time horizon, assuming that players are subject to heterogeneous periodic constraints. A new aggregative equilibrium notion is presented and the strategic behaviour of the agents is analysed under a receding horizon paradigm. The evolution of the strategies predicted and implemented by the players over time is modelled through a discrete-time multi-valued dynamical system. By considering Lyapunov stability notions and applying limit and invariance results for set-valued correspondences, necessary conditions are derived for convergence of a receding horizon map to a periodic equilibrium of the aggregative game. This result is achieved for any (feasible) initial condition, thus ensuring implicit adaptivity of the proposed control framework to real-time variations in the number and parameters of players. Design and implementation of the proposed control strategy are discussed and an example of distributed control for data routing is presented, evaluating its performance in simulation.      
### 11.Competitive DER Aggregation for Participation in Wholesale Markets  [ :arrow_down: ](https://arxiv.org/pdf/2207.00290.pdf)
>  The problem of the large-scale aggregation of the behind-the-meter demand and generation resources by a distributed-energy-resource aggregator (DERA) is considered. As a profit-seeking wholesale market participant, a DERA maximizes its profit while providing competitive services to its customers with higher consumer/prosumer surpluses than those offered by the distribution utilities or community choice aggregators. A constrained profit maximization program for aggregating behind-the-meter generation and consumption resources is formulated, from which payment functions for the behind-the-meter consumptions and generations are derived. Also obtained are DERA's bid and offer curves for its participation in the wholesale energy market and the optimal schedule of behind-the-meter resources. It is shown that the proposed DERA's aggregation model can achieve market efficiency equivalent to that when its customers participate individually directly in the wholesale market.      
### 12.Covid-19 Detection Using transfer Learning Approach from Computed Temography Images  [ :arrow_down: ](https://arxiv.org/pdf/2207.00259.pdf)
>  Our main goal in this study is to propose a transfer learning based method for COVID-19 detection from Computed Tomography (CT) images. The transfer learning model used for the task is a pretrained Xception model. Both model architecture and pre-trained weights on ImageNet were used. The resulting modified model was trained with 128 batch size and 224x224, 3 channeled input images, converted from original 512x512, grayscale images. The dataset used is a the COV19-CT-DB. Labels in the dataset include COVID-19 cases and Non-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on the validation partition of the dataset as well as precision recall and macro F1 score were used to measure the performance of the proposed method. The resulting Macro F1 score on the validation set exceeded the baseline model.      
### 13.Distributed Parallel Image Signal Extrapolation Framework using Message Passing Interface  [ :arrow_down: ](https://arxiv.org/pdf/2207.00238.pdf)
>  This paper introduces a framework for distributed parallel image signal extrapolation. Since high-quality image signal processing often comes along with a high computational complexity, a parallel execution is desirable. The proposed framework allows for the application of existing image signal extrapolation algorithms without the need to modify them for a parallel processing. The unaltered application of existing algorithms is achieved by dividing input images into overlapping tiles which are distributed to compute nodes via Message Passing Interface. In order to keep the computational overhead low, a novel image tiling algorithm is proposed. Using this algorithm, a nearly optimum tiling is possible at a very small processing time. For showing the efficacy of the framework, it is used for parallelizing a high-complexity extrapolation algorithm. Simulation results show that the proposed framework has no negative impact on extrapolation quality while at the same time offering good scaling behavior on compute clusters.      
### 14.Optimized and Parallelized Processing Order for Improved Frequency Selective Signal Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00233.pdf)
>  In the recent years, multi-core processor designs have found their way into many computing devices. To exploit the capabilities of such devices in the best possible way, signal processing algorithms have to be adapted to an operation in parallel tasks. In this contribution an optimized processing order is proposed for Frequency Selective Extrapolation, a powerful signal extrapolation algorithm. Using this optimized order, the extrapolation can be carried out in parallel. The algorithm scales very good, resulting in an acceleration of a factor of up to 7.7 for an eight core computer. Additionally, the optimized processing order aims at reducing the propagation of extrapolation errors over consecutive losses. Thus, in addition to the acceleration, a visually noticeable improvement in quality of up to 0.5 dB PSNR can be achieved.      
### 15.Motion Compensated Frequency Selective Extrapolation for Error Concealment in Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.00231.pdf)
>  Although wireless and IP-based access to video content gives a new degree of freedom to the viewers, the risk of severe block losses caused by transmission errors is always present. The purpose of this paper is to present a new method for concealing block losses in erroneously received video sequences. For this, a motion compensated data set is generated around the lost block. Based on this aligned data set, a model of the signal is created that continues the signal into the lost areas. Since spatial as well as temporal informations are used for the model generation, the proposed method is superior to methods that use either spatial or temporal information for concealment. Furthermore it outperforms current state of the art spatio-temporal concealment algorithms by up to 1.4 dB in PSNR.      
### 16.Introducing flexible perovskites to the IoT world using photovoltaic-powered wireless tags  [ :arrow_down: ](https://arxiv.org/pdf/2207.00227.pdf)
>  Billions of everyday objects could become part of the Internet of Things (IoT) by augmentation with low-cost, long-range, maintenance-free wireless sensors. Radio Frequency Identification (RFID) is a low-cost wireless technology that could enable this vision, but it is constrained by short communication range and lack of sufficient energy available to power auxiliary electronics and sensors. Here, we explore the use of flexible perovskite photovoltaic cells to provide external power to semi-passive RFID tags to increase range and energy availability for external electronics such as microcontrollers and digital sensors. Perovskites are intriguing materials that hold the possibility to develop high-performance, low-cost, optically tunable (to absorb different light spectra), and flexible light energy harvesters. Our prototype perovskite photovoltaic cells on plastic substrates have an efficiency of 13% and a voltage of 0.88 V at maximum power under standard testing conditions. We built prototypes of RFID sensors powered with these flexible photovoltaic cells to demonstrate real-world applications. Our evaluation of the prototypes suggests that: i) flexible PV cells are durable up to a bending radius of 5 mm with only a 20 % drop in relative efficiency; ii) RFID communication range increased by 5x, and meets the energy needs (10-350 microwatt) to enable self-powered wireless sensors; iii) perovskite powered wireless sensors enable many battery-less sensing applications (e.g., perishable good monitoring, warehouse automation)      
### 17.On the Performance of Data Compression in Clustered Fog Radio Access Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.00223.pdf)
>  The fog-radio-access-network (F-RAN) has been proposed to address the strict latency requirements, which offloads computation tasks generated in user equipments (UEs) to the edge to reduce the processing latency. However, it incorporates the task transmission latency, which may become the bottleneck of latency requirements. Data compression (DC) has been considered as one of the promising techniques to reduce the transmission latency. By compressing the computation tasks before transmitting, the transmission delay is reduced due to the shrink transmitted data size, and the original computing task can be retrieved by employing data decompressing (DD) at the edge nodes or the centre cloud. Nevertheless, the DC and DD incorporate extra processing latency, and the latency performance has not been investigated in the large-scale DC-enabled F-RAN. Therefore, in this work, the successful data compression probability (SDCP) is defined to analyse the latency performance of the F-RAN. Moreover, to analyse the effect of compression offloading ratio (COR), a novel hybrid compression mode is proposed based on the queueing theory. Based on this, the closed-form result of SDCP in the large-scale DC-enabled F-RAN is derived by combining the Matern cluster process and M/G/1 queueing model, and validated by Monte Carlo simulations. Based on the derived SDCP results, the effects of COR on the SDCP is analysed numerically. The results show that the SDCP with the optimal COR can be enhanced with a maximum value of 0.3 and 0.55 as compared with the cases of compressing all computing tasks at the edge and at the UE, respectively. Moreover, for the system requiring the minimal latency, the proposed hybrid compression mode can alleviate the requirement on the backhaul capacity.      
### 18.Updating Only Encoders Prevents Catastrophic Forgetting of End-to-End ASR Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.00216.pdf)
>  In this paper, we present an incremental domain adaptation technique to prevent catastrophic forgetting for an end-to-end automatic speech recognition (ASR) model. Conventional approaches require extra parameters of the same size as the model for optimization, and it is difficult to apply these approaches to end-to-end ASR models because they have a huge amount of parameters. To solve this problem, we first investigate which parts of end-to-end ASR models contribute to high accuracy in the target domain while preventing catastrophic forgetting. We conduct experiments on incremental domain adaptation from the LibriSpeech dataset to the AMI meeting corpus with two popular end-to-end ASR models and found that adapting only the linear layers of their encoders can prevent catastrophic forgetting. Then, on the basis of this finding, we develop an element-wise parameter selection focused on specific layers to further reduce the number of fine-tuning parameters. Experimental results show that our approach consistently prevents catastrophic forgetting compared to parameter selection from the whole model.      
### 19.Polarized Color Image Denoising using Pocoformer  [ :arrow_down: ](https://arxiv.org/pdf/2207.00215.pdf)
>  Polarized color photography provides both visual textures and object surficial information in one single snapshot. However, the use of the directional polarizing filter array causes extremely lower photon count and SNR compared to conventional color imaging. Thus, the feature essentially leads to unpleasant noisy images and destroys polarization analysis performance. It is a challenge for traditional image processing pipelines owing to the fact that the physical constraints exerted implicitly in the channels are excessively complicated. To address this issue, we propose a learning-based approach to simultaneously restore clean signals and precise polarization information. A real-world polarized color image dataset of paired raw short-exposed noisy and long-exposed reference images are captured to support the learning-based pipeline. Moreover, we embrace the development of vision Transformer and propose a hybrid transformer model for the Polarized Color image denoising, namely PoCoformer, for a better restoration performance. Abundant experiments demonstrate the effectiveness of proposed method and key factors that affect results are analyzed.      
### 20.Range of Motion Sensors for Monitoring Recovery of Total Knee Arthroplasty  [ :arrow_down: ](https://arxiv.org/pdf/2207.00190.pdf)
>  A low-cost, accurate device to measure and record knee range of motion (ROM) is of the essential need to improve confidence in at-home rehabilitation. It is to reduce hospital stay duration and overall medical cost after Total Knee Arthroplasty (TKA) procedures. The shift in Medicare funding from pay-as-you-go to the Bundled Payments for Care Improvement (BPCI) has created a push towards at-home care over extended hospital stays. It has heavily affected TKA patients, who typically undergo physical therapy at the clinic after the procedure to ensure full recovery of ROM. In this paper, we use accelerometers to create a ROM sensor that can be integrated into the post-operative surgical dressing, so that the cost of the sensors can be included in the bundled payments. In this paper, we demonstrate the efficacy of our method in comparison to the baseline computer vision method. Our results suggest that calculating angular displacement from accelerometer sensors demonstrates accurate ROM recordings under both stationary and walking conditions. The device would keep track of angle measurements and alert the patient when certain angle thresholds have been crossed, allowing patients to recover safely at home instead of going to multiple physical therapy sessions. The affordability of our sensor makes it more accessible to patients in need.      
### 21.Improving Disease Classification Performance and Explainability of Deep Learning Models in Radiology with Heatmap Generators  [ :arrow_down: ](https://arxiv.org/pdf/2207.00157.pdf)
>  As deep learning is widely used in the radiology field, the explainability of such models is increasingly becoming essential to gain clinicians' trust when using the models for diagnosis. In this research, three experiment sets were conducted with a U-Net architecture to improve the classification performance while enhancing the heatmaps corresponding to the model's focus through incorporating heatmap generators during training. All of the experiments used the dataset that contained chest radiographs, associated labels from one of the three conditions ("normal", "congestive heart failure (CHF)", and "pneumonia"), and numerical information regarding a radiologist's eye-gaze coordinates on the images. The paper (A. Karargyris and Moradi, 2021) that introduced this dataset developed a U-Net model, which was treated as the baseline model for this research, to show how the eye-gaze data can be used in multi-modal training for explainability improvement. To compare the classification performances, the 95% confidence intervals (CI) of the area under the receiver operating characteristic curve (AUC) were measured. The best method achieved an AUC of 0.913 (CI: 0.860-0.966). The greatest improvements were for the "pneumonia" and "CHF" classes, which the baseline model struggled most to classify, resulting in AUCs of 0.859 (CI: 0.732-0.957) and 0.962 (CI: 0.933-0.989), respectively. The proposed method's decoder was also able to produce probability masks that highlight the determining image parts in model classifications, similarly as the radiologist's eye-gaze data. Hence, this work showed that incorporating heatmap generators and eye-gaze information into training can simultaneously improve disease classification and provide explainable visuals that align well with how the radiologist viewed the chest radiographs when making diagnosis.      
### 22.Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.00156.pdf)
>  We aim to quantitatively measure the practical usability of medical image segmentation models: to what extent, how often, and on which samples a model's predictions can be used/trusted. We first propose a measure, Correctness-Confidence Rank Correlation (CCRC), to capture how predictions' confidence estimates correlate with their correctness scores in rank. A model with a high value of CCRC means its prediction confidences reliably suggest which samples' predictions are more likely to be correct. Since CCRC does not capture the actual prediction correctness, it alone is insufficient to indicate whether a prediction model is both accurate and reliable to use in practice. Therefore, we further propose another method, Usable Region Estimate (URE), which simultaneously quantifies predictions' correctness and reliability of confidence assessments in one estimate. URE provides concrete information on to what extent a model's predictions are usable. In addition, the sizes of usable regions (UR) can be utilized to compare models: A model with a larger UR can be taken as a more usable and hence better model. Experiments on six datasets validate that the proposed evaluation methods perform well, providing a concrete and concise measure for the practical usability of medical image segmentation models. Code is made available at <a class="link-external link-https" href="https://github.com/yizhezhang2000/ure" rel="external noopener nofollow">this https URL</a>.      
### 23.SASV Based on Pre-trained ASV System and Integrated Scoring Module  [ :arrow_down: ](https://arxiv.org/pdf/2207.00150.pdf)
>  Based on the assumption that there is a correlation between anti-spoofing and speaker verification, a Total-Divide-Total integrated Spoofing-Aware Speaker Verification (SASV) system based on pre-trained automatic speaker verification (ASV) system and integrated scoring module is proposed and submitted to the SASV 2022 Challenge. The training and scoring of ASV and anti-spoofing countermeasure (CM) in current SASV systems are relatively independent, ignoring the correlation. In this paper, by leveraging the correlation between the two tasks, an integrated SASV system can be obtained by simply training a few more layers on the basis of the baseline pre-trained ASV subsystem. The features in pre-trained ASV system are utilized for logical access spoofing speech detection. Further, speaker embeddings extracted by the pre-trained ASV system are used to improve the performance of the CM. The integrated scoring module takes the embeddings of the ASV and anti-spoofing branches as input and preserves the correlation between the two tasks through matrix operations to produce integrated SASV scores. Submitted primary system achieved equal error rate (EER) of 3.07\% on the development dataset of the SASV 2022 Challenge and 4.30\% on the evaluation part, which is a 25\% improvement over the baseline systems.      
### 24.A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos  [ :arrow_down: ](https://arxiv.org/pdf/2207.00141.pdf)
>  Breast lesion detection in ultrasound is critical for breast cancer diagnosis. Existing methods mainly rely on individual 2D ultrasound images or combine unlabeled video and labeled 2D images to train models for breast lesion detection. In this paper, we first collect and annotate an ultrasound video dataset (188 videos) for breast lesion detection. Moreover, we propose a clip-level and video-level feature aggregated network (CVA-Net) for addressing breast lesion detection in ultrasound videos by aggregating video-level lesion classification features and clip-level temporal features. The clip-level temporal features encode local temporal information of ordered video frames and global temporal information of shuffled video frames. In our CVA-Net, an inter-video fusion module is devised to fuse local features from original video frames and global features from shuffled video frames, and an intra-video fusion module is devised to learn the temporal information among adjacent video frames. Moreover, we learn video-level features to classify the breast lesions of the original video as benign or malignant lesions to further enhance the final breast lesion detection performance in ultrasound videos. Experimental results on our annotated dataset demonstrate that our CVA-Net clearly outperforms state-of-the-art methods. The corresponding code and dataset are publicly available at \url{<a class="link-external link-https" href="https://github.com/jhl-Det/CVA-Net" rel="external noopener nofollow">this https URL</a>}.      
### 25.Decentralized Signal Temporal Logic Control for Perturbed Interconnected Systems via Assume-Guarantee Contract Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2207.00115.pdf)
>  We develop a novel decentralized control method for a network of perturbed linear systems with dynamical couplings subject to Signal Temporal Logic (STL) specifications. We first transform the STL requirements into set containment problems and then we develop controllers to solve these problems. Our approach is based on treating the couplings between subsystems as disturbances, which are bounded sets that the subsystems negotiate in the form of parametric assume-guarantee contracts. The set containment requirements and parameterized contracts are added to the subsystems' constraints. We introduce a centralized optimization problem to derive the contracts, reachability tubes, and decentralized closed-loop control laws. We show that, when the STL formula is separable with respect to the subsystems, the centralized optimization problem can be solved in a distributed way, which scales to large systems. We present formal theoretical guarantees on robustness of STL satisfaction. The effectiveness of the proposed method is demonstrated via a power network case study.      
### 26.End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology  [ :arrow_down: ](https://arxiv.org/pdf/2207.00095.pdf)
>  Current approaches for classification of whole slide images (WSI) in digital pathology predominantly utilize a two-stage learning pipeline. The first stage identifies areas of interest (e.g. tumor tissue), while the second stage processes cropped tiles from these areas in a supervised fashion. During inference, a large number of tiles are combined into a unified prediction for the entire slide. A major drawback of such approaches is the requirement for task-specific auxiliary labels which are not acquired in clinical routine. We propose a novel learning pipeline for WSI classification that is trainable end-to-end and does not require any auxiliary annotations. We apply our approach to predict molecular alterations for a number of different use-cases, including detection of microsatellite instability in colorectal tumors and prediction of specific mutations for colon, lung, and breast cancer cases from The Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to be competitive with state of the art two-stage pipelines. We believe our approach can facilitate future research in digital pathology and contribute to solve a large range of problems around the prediction of cancer phenotypes, hopefully enabling personalized therapies for more patients in future.      
### 27.A Transfer-Learning Based Ensemble Architecture for ECG Signal Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.00002.pdf)
>  Manual interpretation and classification of ECG signals lack both accuracy and reliability. These continuous time-series signals are more effective when represented as an image for CNN-based classification. A continuous Wavelet transform filter is used here to get corresponding images. In achieving the best result generic CNN architectures lack sufficient accuracy and also have a higher run-time. To address this issue, we propose an ensemble method of transfer learning-based models to classify ECG signals. In our work, two modified VGG-16 models and one InceptionResNetV2 model with added feature extracting layers and ImageNet weights are working as the backbone. After ensemble, we report an increase of 6.36% accuracy than previous MLP-based algorithms. After 5-fold cross-validation with the Physionet dataset, our model reaches an accuracy of 99.98%.      
### 28.Distance-Based Sound Separation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00562.pdf)
>  We propose the novel task of distance-based sound separation, where sounds are separated based only on their distance from a single microphone. In the context of assisted listening devices, proximity provides a simple criterion for sound selection in noisy environments that would allow the user to focus on sounds relevant to a local conversation. We demonstrate the feasibility of this approach by training a neural network to separate near sounds from far sounds in single channel synthetic reverberant mixtures, relative to a threshold distance defining the boundary between near and far. With a single nearby speaker and four distant speakers, the model improves scale-invariant signal to noise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds.      
### 29.QoE-Centric Multi-User mmWave Scheduling: A Beam Alignment and Buffer Predictive Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.00532.pdf)
>  In this paper, we consider the multi-user scheduling problem in millimeter wave (mmWave) video streaming networks, which comprise a streaming server and several users, each requesting a video stream with a different resolution. The main objective is to optimize the long-term average quality of experience (QoE) for all users. We tackle this problem by considering the physical layer characteristics of the mmWave network, including the beam alignment overhead due to pencil-beams. To develop an efficient scheduling policy, we leverage the contextual multi-armed bandit (MAB) models to propose a beam alignment overhead and buffer predictive streaming solution, dubbed B2P-Stream. The proposed B2P-Stream algorithm optimally balances the trade-off between the overhead and users' buffer levels and improves the QoE by reducing the beam alignment overhead for users of higher resolutions. We also provide a theoretical guarantee for our proposed method and prove that it guarantees a sub-linear regret bound. Finally, we examine our proposed framework through extensive simulations. We provide a detailed comparison of the B2P-Stream against uniformly random and Round-robin (RR) policies and show that it outperforms both of them in providing a better QoE and fairness. We also analyze the scalability and robustness of the B2P-Stream algorithm with different network configurations.      
### 30.Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2207.00501.pdf)
>  Diagnosing hematological malignancies requires identification and classification of white blood cells in peripheral blood smears. Domain shifts caused by different lab procedures, staining, illumination, and microscope settings hamper the re-usability of recently developed machine learning methods on data collected from different sites. Here, we propose a cross-domain adapted autoencoder to extract features in an unsupervised manner on three different datasets of single white blood cells scanned from peripheral blood smears. The autoencoder is based on an R-CNN architecture allowing it to focus on the relevant white blood cell and eliminate artifacts in the image. To evaluate the quality of the extracted features we use a simple random forest to classify single cells. We show that thanks to the rich features extracted by the autoencoder trained on only one of the datasets, the random forest classifier performs satisfactorily on the unseen datasets, and outperforms published oracle networks in the cross-domain task. Our results suggest the possibility of employing this unsupervised approach in more complicated diagnosis and prognosis tasks without the need to add expensive expert labels to unseen data.      
### 31.A self-contained karma economy for the dynamic allocation of common resources  [ :arrow_down: ](https://arxiv.org/pdf/2207.00495.pdf)
>  This paper presents karma mechanisms, a novel approach to the repeated allocation of a scarce resource among competing agents over an infinite time. Examples of such resource allocation problems include deciding which trip requests to serve in a ride-hailing platform during peak demand, granting the right of way in intersections, or admitting internet content to a fast channel for improved quality of service. We study a simplified yet insightful formulation of these problems where at every time two agents from a large population get randomly matched to compete over the resource. The intuitive interpretation of a karma mechanism is "If I give in now, I will be rewarded in the future." Agents compete in an auction-like setting where they bid units of karma, which circulates directly among them and is self-contained in the system. We demonstrate that this allows a society of self-interested agents to achieve high levels of efficiency without resorting to a (possibly problematic) monetary pricing of the resource. We model karma mechanisms as dynamic population games, in which agents have private states - their urgency to acquire the resource and how much karma they have - that vary in time based on their strategic decisions. We adopt the stationary Nash equilibrium as the solution concept and prove its existence. We then analyze the performance at the stationary Nash equilibrium numerically. For the case where the agents have homogeneous preferences, we compare different mechanism design choices which allow to strike trade-offs between efficiency and fairness metrics, showing how it is possible to achieve an efficient and ex-post fair allocation when the agents are future aware. Finally, we test the robustness of the mechanisms against heterogeneity in the urgency processes and the future awareness of the agents and propose remedies to some of the observed phenomena via karma redistribution.      
### 32.Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound  [ :arrow_down: ](https://arxiv.org/pdf/2207.00475.pdf)
>  Standard plane (SP) localization is essential in routine clinical ultrasound (US) diagnosis. Compared to 2D US, 3D US can acquire multiple view planes in one scan and provide complete anatomy with the addition of coronal plane. However, manually navigating SPs in 3D US is laborious and biased due to the orientation variability and huge search space. In this study, we introduce a novel reinforcement learning (RL) framework for automatic SP localization in 3D US. Our contribution is three-fold. First, we formulate SP localization in 3D US as a tangent-point-based problem in RL to restructure the action space and significantly reduce the search space. Second, we design an auxiliary task learning strategy to enhance the model's ability to recognize subtle differences crossing Non-SPs and SPs in plane search. Finally, we propose a spatial-anatomical reward to effectively guide learning trajectories by exploiting spatial and anatomical information simultaneously. We explore the efficacy of our approach on localizing four SPs on uterus and fetal brain datasets. The experiments indicate that our approach achieves a high localization accuracy as well as robust performance.      
### 33.Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling  [ :arrow_down: ](https://arxiv.org/pdf/2207.00474.pdf)
>  Ultrasound (US) is widely used for its advantages of real-time imaging, radiation-free and portability. In clinical practice, analysis and diagnosis often rely on US sequences rather than a single image to obtain dynamic anatomical information. This is challenging for novices to learn because practicing with adequate videos from patients is clinically unpractical. In this paper, we propose a novel framework to synthesize high-fidelity US videos. Specifically, the synthesis videos are generated by animating source content images based on the motion of given driving videos. Our highlights are three-fold. First, leveraging the advantages of self- and fully-supervised learning, our proposed system is trained in weakly-supervised manner for keypoint detection. These keypoints then provide vital information for handling complex high dynamic motions in US videos. Second, we decouple content and texture learning using the dual decoders to effectively reduce the model learning difficulty. Last, we adopt the adversarial training strategy with GAN losses for further improving the sharpness of the generated videos, narrowing the gap between real and synthesis videos. We validate our method on a large in-house pelvic dataset with high dynamic motion. Extensive evaluation metrics and user study prove the effectiveness of our proposed method.      
### 34.Stain Isolation-based Guidance for Improved Stain Translation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00431.pdf)
>  Unsupervised and unpaired domain translation using generative adversarial neural networks, and more precisely CycleGAN, is state of the art for the stain translation of histopathology images. It often, however, suffers from the presence of cycle-consistent but non structure-preserving errors. We propose an alternative approach to the set of methods which, relying on segmentation consistency, enable the preservation of pathology structures. Focusing on immunohistochemistry (IHC) and multiplexed immunofluorescence (mIF), we introduce a simple yet effective guidance scheme as a loss function that leverages the consistency of stain translation with stain isolation. Qualitative and quantitative experiments show the ability of the proposed approach to improve translation between the two domains.      
### 35.Semantic Communications for 6G Future Internet: Fundamentals, Applications, and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2207.00427.pdf)
>  With the increasing demand for intelligent services, the sixth-generation (6G) wireless networks will shift from a traditional architecture that focuses solely on high transmission rate to a new architecture that is based on the intelligent connection of everything. Semantic communication (SemCom), a revolutionary architecture that integrates user as well as application requirements and meaning of information into the data processing and transmission, is predicted to become a new core paradigm in 6G. While SemCom is expected to progress beyond the classical Shannon paradigm, several obstacles need to be overcome on the way to a SemCom-enabled smart wireless Internet. In this paper, we first highlight the motivations and compelling reasons of SemCom in 6G. Then, we outline the major 6G visions and key enabler techniques which lay the foundation of SemCom. Meanwhile, we highlight some benefits of SemCom-empowered 6G and present a SemCom-native 6G network architecture. Next, we show the evolution of SemCom from its introduction to classical SemCom related theory and modern AI-enabled SemCom. Following that, focusing on modern SemCom, we classify SemCom into three categories, i.e., semantic-oriented communication, goal-oriented communication, and semantic-aware communication, and introduce three types of semantic metrics. We then discuss the applications, the challenges and technologies related to semantics and communication. Finally, we introduce future research opportunities. In a nutshell, this paper investigates the fundamentals of SemCom, its applications in 6G networks, and the existing challenges and open issues for further direction.      
### 36.NICT's versatile miniaturized lasercom terminals for moving platforms  [ :arrow_down: ](https://arxiv.org/pdf/2207.00423.pdf)
>  With the goal of meeting the diverse requirements of many different types of platforms, ranging from small drones to big satellites, and being applied in a variety of diverse scenarios, ranging from fixed terrestrial links to moving platforms in general, and operating within a wide range of conditions and distances, the Japanese National Institute of Information and Communications Technology (NICT) is currently working towards the development of a series of versatile miniaturized free-space laser-communication terminals. By choosing the appropriate terminal configuration for any given scenario, the basic conditions of operations can be satisfied without the need of customization, and the adaptive design of the terminals can close the gap to achieve an optimum solution that meets the communication requirements. This paper presents NICT's current efforts regarding the development of this series of lasercom terminals and introduces the first prototypes developed for validation and test purposes.      
### 37.Review of 5G capabilities for smart manufacturing  [ :arrow_down: ](https://arxiv.org/pdf/2207.00417.pdf)
>  5G has been defined to address new use cases beyond consumer-focused mobile broadband services. In particular industrial use cases, for example in smart manufacturing, have been addressed in the 5G standardization, so that 5G can support industrial IoT services and wireless industrial networking. To this end, 5G needs to integrate with the industrial network based on Ethernet and TSN. 5G can create new opportunities for smart manufacturing by enabling flexibility and increasing the automation in the production. This paper provides an overview of the use cases and requirements for smart manufacturing that can be addressed with 5G and which are validated in three industrial 5G trial systems. The capabilities of 5G are described for providing non-public networks that support industrial LAN services based on Ethernet and TSN. The 5G radio access network functionality is described and discussed for practical deployments. The paper provides an overview of the current state-of-the-art of using 5G for smart manufacturing.      
### 38.Energy Efficient Routing For Underwater Acoustic Sensor Network Using Genetic Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2207.00416.pdf)
>  In underwater acoustic sensor networks (UWASN), energy-reliable data transmission is a challenging task. This is due to acoustic transmission disturbances caused by excessive noise, exceptionally long propagation delays, a high bit error rate, limited bandwidth capability, and interference. One of the most important issues of UWASN for research is how to extend the life span of data transmission. Data transfer from a source node to a destination node in UWASN is a complicated topic for researchers. Many routing algorithms, such as vector base forwarding and depth base routing, have been developed in past years. We propose a genetic algorithm-based optimization method for improving the energy efficiency of data transmission in the routing path from a source node to a destination node.      
### 39.Artificial Intelligence Techniques for Next-Generation Mega Satellite Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.00414.pdf)
>  Space communications, particularly mega satellite networks, re-emerged as an appealing candidate for next generation networks due to major advances in space launching, electronics, processing power, and miniaturization. However, mega satellite networks rely on numerous underlying and intertwined processes that cannot be truly captured using conventionally used models, due to their dynamic and unique features such as orbital speed, inter-satellite links, short time pass, and satellite footprint, among others. Hence, new approaches are needed to enable the network to proactively adjust to the rapidly varying conditions associated within the link. Artificial intelligence (AI) provides a pathway to capture these processes, analyze their behavior, and model their effect on the network. This article introduces the application of AI techniques for integrated terrestrial satellite networks, particularly mega satellite network communications. It details the unique features of mega satellite networks, and the overarching challenges concomitant with their integration into the current communication infrastructure. Moreover, the article provides insights into state-of-the-art AI techniques across various layers of the communication link. This entails applying AI for forecasting the highly dynamic radio channel, spectrum sensing and classification, signal detection and demodulation, inter-satellite link and satellite access network optimization, and network security. Moreover, future paradigms and the mapping of these mechanisms onto practical networks are outlined.      
### 40.Toward Low-Cost End-to-End Spoken Language Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2207.00352.pdf)
>  Recent advances in spoken language understanding benefited from Self-Supervised models trained on large speech corpora. For French, the LeBenchmark project has made such models available and has led to impressive progress on several tasks including spoken language understanding. These advances have a non-negligible cost in terms of computation time and energy consumption. In this paper, we compare several learning strategies trying to reduce such cost while keeping competitive performance. At the same time we propose an extensive analysis where we measure the cost of our models in terms of training time and electric energy consumption, hopefully promoting a comprehensive evaluation procedure. The experiments are performed on the FSC and MEDIA corpora, and show that it is possible to reduce the learning cost while maintaining state-of-the-art performance and using SSL models.      
### 41.HyperTensioN and Total-order Forward Decomposition optimizations  [ :arrow_down: ](https://arxiv.org/pdf/2207.00345.pdf)
>  Hierarchical Task Networks (HTN) planners generate plans using a decomposition process with extra domain knowledge to guide search towards a planning task. While domain experts develop HTN descriptions, they may repeatedly describe the same preconditions, or methods that are rarely used or possible to be decomposed. By leveraging a three-stage compiler design we can easily support more language descriptions and preprocessing optimizations that when chained can greatly improve runtime efficiency in such domains. In this paper we evaluate such optimizations with the HyperTensioN HTN planner, used in the HTN IPC 2020.      
### 42.Automatic Evaluation of Speaker Similarity  [ :arrow_down: ](https://arxiv.org/pdf/2207.00344.pdf)
>  We introduce a new automatic evaluation method for speaker similarity assessment, that is consistent with human perceptual scores. Modern neural text-to-speech models require a vast amount of clean training data, which is why many solutions switch from single speaker models to solutions trained on examples from many different speakers. Multi-speaker models bring new possibilities, such as a faster creation of new voices, but also a new problem - speaker leakage, where the speaker identity of a synthesized example might not match those of the target speaker. Currently, the only way to discover this issue is through costly perceptual evaluations. In this work, we propose an automatic method for assessment of speaker similarity. For that purpose, we extend the recent work on speaker verification systems and evaluate how different metrics and speaker embeddings models reflect Multiple Stimuli with Hidden Reference and Anchor (MUSHRA) scores. Our experiments show that we can train a model to predict speaker similarity MUSHRA scores from speaker embeddings with 0.96 accuracy and significant correlation up to 0.78 Pearson score at the utterance level.      
### 43.Task-specific Performance Prediction and Acquisition Optimization for Anisotropic X-ray Dark-field Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2207.00332.pdf)
>  Anisotropic X-ray Dark-field Tomography (AXDT) is a recently developed imaging modality that enables the visualization of oriented microstructures using lab-based X-ray grating interferometer setups. While there are very promising application scenarios, for example in materials testing of fibrous composites or in medical diagnosis of brain cell connectivity, AXDT faces challenges in practical applicability due to the complex and time-intensive acquisitions required to fully sample the anisotropic X-ray scattering functions. However, depending on the specific imaging task at hand, a full sampling may not be required, allowing for reduced acquisitions. In this work we are investigating a performance prediction approach for AXDT using task-specific detectability indices. Based on this approach we present a task-driven acquisition optimization method that enables reduced acquisition schemes while keeping the task-specific image quality high. We demonstrate the feasibility and efficacy of the method in experiments with simulated and experimental data.      
### 44.Survey on Wireless Information Energy Transfer (WIET) and Related Applications in 6G Internet of NanoThings (IoNT)  [ :arrow_down: ](https://arxiv.org/pdf/2207.00326.pdf)
>  This article contains an overview of WIET and the related applications in 6G IoNT. Specifically, to explore the following, we: (i) introduce the 6G network along with the implementation challenges, possible techniques, THz communication and related research challenges, (ii) focus on the WIET architecture, and different energy carrying code words for efficient charging through WIET, (iii) discuss IoNT with techniques proposed for communication of nano-devices, and (iv) conduct a detailed literature review to explore the implicational aspects of the WIET in the 6G nano-network. In addition, we also investigate the expected applications of WIET in the 6G IoNT based devices and discuss the WIET implementation challenges in 6G IoNT for the optimal use of the technology. Lastly, we overview the expected design challenges which may occur during the implementation process, and identify the key research challenges which require timely solutions and which are significant to spur further research in this challenging area. Overall, through this survey, we discuss the possibility to maximize the applications of WIET in 6G IoNT.      
### 45.Safe Controlled Invariance for Linear Systems Using Sum-of-Squares Programming  [ :arrow_down: ](https://arxiv.org/pdf/2207.00321.pdf)
>  Safety is closely related to set invariance for dynamical systems. However, synthesizing a safe invariant set and at the same time synthesizing the associated safe controller still remains challenging. In this note we introduce a simple invariance-based method for linear systems with safety guarantee. The proposed method uses sum-of-squares programming.      
### 46.High-resolution Solar Image Reconstruction Based on Non-rigid Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2207.00268.pdf)
>  Suppressing the interference of atmospheric turbulence and obtaining observation data with a high spatial resolution is an issue to be solved urgently for ground observations. One way to solve this problem is to perform a statistical reconstruction of short-exposure speckle images. Combining the rapidity of Shift-Add and the accuracy of speckle masking, this paper proposes a novel reconstruction algorithm-NASIR (Non-rigid Alignment based Solar Image Reconstruction). NASIR reconstructs the phase of the object image at each frequency by building a computational model between geometric distortion and intensity distribution and reconstructs the modulus of the object image on the aligned speckle images by speckle interferometry. We analyzed the performance of NASIR by using the correlation coefficient, power spectrum, and coefficient of variation of intensity profile (CVoIP) in processing data obtained by the NVST (1m New Vacuum Solar Telescope). The reconstruction experiments and analysis results show that the quality of images reconstructed by NASIR is close to speckle masking when the seeing is good, while NASIR has excellent robustness when the seeing condition becomes worse. Furthermore, NASIR reconstructs the entire field of view in parallel in one go, without phase recursion and block-by-block reconstruction, so its computation time is less than half that of speckle masking. Therefore, we consider NASIR is a robust and high-quality fast reconstruction method that can serve as an effective tool for data filtering and quick look.      
### 47.A Functional Architecture for 6G Special Purpose Industrial IoT Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.00264.pdf)
>  Future industrial applications will encompass compelling new use cases requiring stringent performance guarantees over multiple key performance indicators (KPI) such as reliability, dependability, latency, time synchronization, security, etc. Achieving such stringent and diverse service requirements necessitates the design of a special-purpose Industrial Internet of Things (IIoT) network comprising a multitude of specialized functionalities and technological enablers. This article proposes an innovative architecture for such a special-purpose 6G IIoT network incorporating seven functional building blocks categorized into: special-purpose functionalities and enabling technologies. The former consists of Wireless Environment Control, Traffic/Channel Prediction, Proactive Resource Management and End-to-End Optimization functions; whereas the latter includes Synchronization and Coordination, Machine Learning and Artificial Intelligence Algorithms, and Auxiliary Functions. The proposed architecture aims at providing a resource-efficient and holistic solution for the complex and dynamically challenging requirements imposed by future 6G industrial use cases. Selected test scenarios are provided and assessed to illustrate cross-functional collaboration and demonstrate the applicability of the proposed architecture in a wireless IIoT network.      
### 48.snapshot CEST++ : the next snapshot CEST for fast whole-brain APTw imaging at 3T  [ :arrow_down: ](https://arxiv.org/pdf/2207.00261.pdf)
>  CEST suffers from two main problems long acquisitin times or restricted coverage as well as incoherent protocol settings. In this paper we give suggestions on how to optimise your protocol settings fro CEST and present one setting for APT CEST. To increase the coverage while keeping the acquisition time constant we suggest using a spatial temporal Compressed Sensing approach. Finally, 1.8mm isotropic whole brain APT CEST maps can be acquired in a little bit less than 2min with a fully integrated online reconstruction. This will pave the way to an even further clinical use of CEST.      
### 49.Improving Speech Enhancement through Fine-Grained Speech Characteristics  [ :arrow_down: ](https://arxiv.org/pdf/2207.00237.pdf)
>  While deep learning based speech enhancement systems have made rapid progress in improving the quality of speech signals, they can still produce outputs that contain artifacts and can sound unnatural. We propose a novel approach to speech enhancement aimed at improving perceptual quality and naturalness of enhanced signals by optimizing for key characteristics of speech. We first identify key acoustic parameters that have been found to correlate well with voice quality (e.g. jitter, shimmer, and spectral flux) and then propose objective functions which are aimed at reducing the difference between clean speech and enhanced speech with respect to these features. The full set of acoustic features is the extended Geneva Acoustic Parameter Set (eGeMAPS), which includes 25 different attributes associated with perception of speech. Given the non-differentiable nature of these feature computation, we first build differentiable estimators of the eGeMAPS and then use them to fine-tune existing speech enhancement systems. Our approach is generic and can be applied to any existing deep learning based enhancement systems to further improve the enhanced speech signals. Experimental results conducted on the Deep Noise Suppression (DNS) Challenge dataset shows that our approach can improve the state-of-the-art deep learning based enhancement systems.      
### 50.Deep Motion Network for Freehand 3D Ultrasound Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2207.00177.pdf)
>  Freehand 3D ultrasound (US) has important clinical value due to its low cost and unrestricted field of view. Recently deep learning algorithms have removed its dependence on bulky and expensive external positioning devices. However, improving reconstruction accuracy is still hampered by difficult elevational displacement estimation and large cumulative drift. In this context, we propose a novel deep motion network (MoNet) that integrates images and a lightweight sensor known as the inertial measurement unit (IMU) from a velocity perspective to alleviate the obstacles mentioned above. Our contribution is two-fold. First, we introduce IMU acceleration for the first time to estimate elevational displacements outside the plane. We propose a temporal and multi-branch structure to mine the valuable information of low signal-to-noise ratio (SNR) acceleration. Second, we propose a multi-modal online self-supervised strategy that leverages IMU information as weak labels for adaptive optimization to reduce drift errors and further ameliorate the impacts of acceleration noise. Experiments show that our proposed method achieves the superior reconstruction performance, exceeding state-of-the-art methods across the board.      
### 51.Data-Driven Model for Failure Analysis of Internet of Things Devices: A Preliminary Study  [ :arrow_down: ](https://arxiv.org/pdf/2207.00173.pdf)
>  This paper proposes the preliminary study of the data-driven failure analysis model for the internet of things (IoT) devices. This model focus on the impact of data transferring both get and receiving data in class C of Low Power Wide Area Network (LoRaWAN). To set up the network, the authors develop the combination of four several technology parts, including 1) the End Device Gateway Network server of LoRa IoT, 2) an Application server for storing the data in the database, 3) the Dashboard to show and got the command by the user, and 4) the failure analysis model based on Bayesian belief networks which calculate the probability values that collect the data transferring both uplink and downlink on the network connection in this study. In the testing phase, the authors input the separated data into the data-driven failure analysis model to analyze the time and latency of the connection by the concern of the impact and risk of the failure of the overall system. The model will show the probability value of failure. The authors hope to use the results to clarify whether a tested IoT device is suitable for use or not.      
### 52.Turbo: Opportunistic Enhancement for Edge Video Analytics  [ :arrow_down: ](https://arxiv.org/pdf/2207.00172.pdf)
>  Edge computing is being widely used for video analytics. To alleviate the inherent tension between accuracy and cost, various video analytics pipelines have been proposed to optimize the usage of GPU on edge nodes. Nonetheless, we find that GPU compute resources provisioned for edge nodes are commonly under-utilized due to video content variations, subsampling and filtering at different places of a pipeline. As opposed to model and pipeline optimization, in this work, we study the problem of opportunistic data enhancement using the non-deterministic and fragmented idle GPU resources. In specific, we propose a task-specific discrimination and enhancement module and a model-aware adversarial training mechanism, providing a way to identify and transform low-quality images that are specific to a video pipeline in an accurate and efficient manner. A multi-exit model structure and a resource-aware scheduler is further developed to make online enhancement decisions and fine-grained inference execution under latency and GPU resource constraints. Experiments across multiple video analytics pipelines and datasets reveal that by judiciously allocating a small amount of idle resources on frames that tend to yield greater marginal benefits from enhancement, our system boosts DNN object detection accuracy by $7.3-11.3\%$ without incurring any latency costs.      
### 53.Variational Autoencoder Assisted Neural Network Likelihood RSRP Prediction Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.00166.pdf)
>  Measuring customer experience on mobile data is of utmost importance for global mobile operators. The reference signal received power (RSRP) is one of the important indicators for current mobile network management, evaluation and monitoring. Radio data gathered through the minimization of drive test (MDT), a 3GPP standard technique, is commonly used for radio network analysis. Collecting MDT data in different geographical areas is inefficient and constrained by the terrain conditions and user presence, hence is not an adequate technique for dynamic radio environments. In this paper, we study a generative model for RSRP prediction, exploiting MDT data and a digital twin (DT), and propose a data-driven, two-tier neural network (NN) model. In the first tier, environmental information related to user equipment (UE), base stations (BS) and network key performance indicators (KPI) are extracted through a variational autoencoder (VAE). The second tier is designed as a likelihood model. Here, the environmental features and real MDT data features are adopted, formulating an integrated training process. On validation, our proposed model that uses real-world data demonstrates an accuracy improvement of about 20% or more compared with the empirical model and about 10% when compared with a fully connected prediction network.      
### 54.A Resource Allocation Scheme for Energy Demand Management in 6G-enabled Smart Grid  [ :arrow_down: ](https://arxiv.org/pdf/2207.00154.pdf)
>  Smart grid (SG) systems enhance grid resilience and efficient operation, leveraging the bidirectional flow of energy and information between generation facilities and prosumers. For energy demand management (EDM), the SG network requires computing a large amount of data generated by massive Internet-of-things (IoT) sensors and advanced metering infrastructure (AMI) with minimal latency. This paper proposes a deep reinforcement learning (DRL)-based resource allocation scheme in a 6G-enabled SG edge network to offload resource-consuming EDM computation to edge servers. Automatic resource provisioning is achieved by harnessing the computational capabilities of smart meters in the dynamic edge network. To enforce DRL-assisted policies in dense 6G networks, the state information from multiple edge servers is required. However, adversaries can 'poison' such information through false state injection (FSI) attacks, exhausting SG edge computing resources. Toward addressing this issue, we investigate the impact of such FSI attacks with respect to abusive utilization of edge resources, and develop a lightweight FSI detection mechanism based on supervised classifiers. Simulation results demonstrate the efficacy of DRL in dynamic resource allocation, the impact of the FSI attacks, and the effectiveness of the detection technique.      
### 55.An Analytical Study on Functional Split in Martian 3D Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.00153.pdf)
>  As space agencies are planning manned missions to reach Mars, researchers need to pave the way for supporting astronauts during their sojourn. This will also be achieved by providing broadband and low-latency connectivity through wireless network infrastructures. In such a framework, we propose a Martian deployment of a 3-Dimensional (3D) network acting as Cloud Radio Access Network (C-RAN). The scenario consists, mostly, of unmanned aerial vehicles (UAVs) and nanosatellites. Thanks to the thin Martian atmosphere, CubeSats can stably orbit at very-low-altitude. This allows to meet strict delay requirements to split functions of the baseband processing between drones and CubeSats. The detailed analytical study, presented in this paper, confirmed the viability of the proposed 3D architecture, under some constraints and trade-off concerning the involved Space communication infrastructures, that are discussed in detail.      
### 56.UWB patch antenna design and realization in the bandwidth 780 MHz to 4.22 GHz  [ :arrow_down: ](https://arxiv.org/pdf/2207.00152.pdf)
>  The proposed UWB antenna covers mobile communications (GSM, EDG, UMTS(3G), LTE(4G)) and wireless networks (WIFI, WiMAX), within a theoretical bandwidth defined from 780MHz to 4.22GHz. The UWB antenna is designed and realized on a FR-4 substrate with an electrical permittivity of 4.4. It presents a 98.75% average analytical efficiency and an omnidirectional radiation within the previous bandwidth. The impedance excitation port is fixed at 50 Ohm according with the SMA impedance used in the practical part. The measured results are in good agreement with those obtained using CST and ADS softwares. The measured bandwidth, defined from 980MHz to 4.2GHz, presents an efficiency of 94.14%. Furthermore, the practical radiation diagram and the excitation port impedance stay the same as that the simulation one.      
### 57.Space Broadband Access: The Race Has Just Begun  [ :arrow_down: ](https://arxiv.org/pdf/2207.00151.pdf)
>  Recent years have witnessed an exponential growth of the commercial space industry, including rocket launch, satellite network deployment, private space travel, and even extraterrestrial colonization. Several trends are predicted in this unprecedented transition to an era of space-enabled broadband access.      
### 58.Error Analysis of Cooperative NOMA with Practical Constraints: Hardware-Impairment, Imperfect SIC and CSI  [ :arrow_down: ](https://arxiv.org/pdf/2207.00146.pdf)
>  Non-orthogonal multiple access (NOMA) has been a strong candidate to support massive connectivity in future wireless networks. In this regard, its implementation into cooperative relaying, named cooperative-NOMA (CNOMA), has received tremendous attention by researchers. However, most of the existing CNOMA studies have failed to address practical constraints since they assume ideal conditions. Particularly, error performance of CNOMA schemes with imperfections has not been investigated, yet. In this letter, we provide an analytical framework for error performance of CNOMA schemes under practical assumptions where we take into account imperfect successive interference canceler (SIC), imperfect channel estimation (ICSI), and hardware impairments (HWI) at the transceivers. We derive bit error rate (BER) expressions in CNOMA schemes whether the direct links between source and users exist or not which is, to the best of the authors' knowledge, the first study in the open literature. For comparisons, we also provide BER expression for downlink NOMA with practical constraints which has also not been given in literature, yet. The theoretical BER expressions are validated with computer simulations where the perfect-match is observed. Finally, we discuss the effects of the system parameters (e.g., power allocation, HWI level) on the performance of CNOMA schemes to reveal fruitful insights for the society.      
### 59.A Hybrid Energy Harvesting Protocol for Cooperative NOMA: Error Performance Approach  [ :arrow_down: ](https://arxiv.org/pdf/2207.00133.pdf)
>  Cooperative non-orthogonal multiple access (CNOMA) has recently been adapted with energy harvesting (EH) to increase energy efficiency and extend the lifetime of energy-constrained wireless networks. This paper proposes a hybrid EH protocol-assisted CNOMA, which is a combination of the two main existing EH protocols (power splitting (PS) and time switching (TS)). The end-to-end bit error rate (BER) expressions of users in the proposed scheme are obtained over Nakagami-$m$ fading channels. The proposed hybrid EH (HEH) protocol is compared with the benchmark schemes (i.e., existing EH protocols and no EH). Based on the extensive simulations, we reveal that the analytical results match perfectly with simulations which proves the correctness of the derivations. Numerical results also show that the HEH-CNOMA outperforms the benchmarks significantly. In addition, we discuss the optimum value of EH factors to minimize the error probability in HEH-CNOMA and show that an optimum value can be obtained according to channel parameters.      
### 60.Multi-Agent Shape Control with Optimal Transport  [ :arrow_down: ](https://arxiv.org/pdf/2207.00129.pdf)
>  We introduce a method called MASCOT (Multi-Agent Shape Control with Optimal Transport) to compute optimal control solutions of agents with shape/formation/density constraints. For example, we might want to apply shape constraints on the agents -- perhaps we desire the agents to hold a particular shape along the path, or we want agents to spread out in order to minimize collisions. We might also want a proportion of agents to move to one destination, while the other agents move to another, and to do this in the optimal way, i.e. the source-destination assignments should be optimal. In order to achieve this, we utilize the Earth Mover's Distance from Optimal Transport to distribute the agents into their proper positions so that certain shapes can be satisfied. This cost is both introduced in the terminal cost and in the running cost of the optimal control problem.      
### 61.GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00106.pdf)
>  Parkinson's disease (PD) is a neurological disorder that has a variety of observable motor-related symptoms such as slow movement, tremor, muscular rigidity, and impaired posture. PD is typically diagnosed by evaluating the severity of motor impairments according to scoring systems such as the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). Automated severity prediction using video recordings of individuals provides a promising route for non-intrusive monitoring of motor impairments. However, the limited size of PD gait data hinders model ability and clinical potential. Because of this clinical data scarcity and inspired by the recent advances in self-supervised large-scale language models like GPT-3, we use human motion forecasting as an effective self-supervised pre-training task for the estimation of motor impairment severity. We introduce GaitForeMer, Gait Forecasting and impairment estimation transforMer, which is first pre-trained on public datasets to forecast gait movements and then applied to clinical data to predict MDS-UPDRS gait impairment severity. Our method outperforms previous approaches that rely solely on clinical data by a large margin, achieving an F1 score of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we show how public human movement data repositories can assist clinical use cases through learning universal motion representations. The code is available at <a class="link-external link-https" href="https://github.com/markendo/GaitForeMer" rel="external noopener nofollow">this https URL</a> .      
### 62.Class Impression for Data-free Incremental Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.00005.pdf)
>  Standard deep learning-based classification approaches require collecting all samples from all classes in advance and are trained offline. This paradigm may not be practical in real-world clinical applications, where new classes are incrementally introduced through the addition of new data. Class incremental learning is a strategy allowing learning from such data. However, a major challenge is catastrophic forgetting, i.e., performance degradation on previous classes when adapting a trained model to new data. Prior methodologies to alleviate this challenge save a portion of training data require perpetual storage of such data that may introduce privacy issues. Here, we propose a novel data-free class incremental learning framework that first synthesizes data from the model trained on previous classes to generate a \ours. Subsequently, it updates the model by combining the synthesized data with new class data. Furthermore, we incorporate a cosine normalized Cross-entropy loss to mitigate the adverse effects of the imbalance, a margin loss to increase separation among previous classes and new ones, and an intra-domain contrastive loss to generalize the model trained on the synthesized data to real data. We compare our proposed framework with state-of-the-art methods in class incremental learning, where we demonstrate improvement in accuracy for the classification of 11,062 echocardiography cine series of patients.      
### 63.A Multi-stage Framework with Mean Subspace Computation and Recursive Feedback for Online Unsupervised Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2207.00003.pdf)
>  In this paper, we address the Online Unsupervised Domain Adaptation (OUDA) problem and propose a novel multi-stage framework to solve real-world situations when the target data are unlabeled and arriving online sequentially in batches. To project the data from the source and the target domains to a common subspace and manipulate the projected data in real-time, our proposed framework institutes a novel method, called an Incremental Computation of Mean-Subspace (ICMS) technique, which computes an approximation of mean-target subspace on a Grassmann manifold and is proven to be a close approximate to the Karcher mean. Furthermore, the transformation matrix computed from the mean-target subspace is applied to the next target data in the recursive-feedback stage, aligning the target data closer to the source domain. The computation of transformation matrix and the prediction of next-target subspace leverage the performance of the recursive-feedback stage by considering the cumulative temporal dependency among the flow of the target subspace on the Grassmann manifold. The labels of the transformed target data are predicted by the pre-trained source classifier, then the classifier is updated by the transformed data and predicted labels. Extensive experiments on six datasets were conducted to investigate in depth the effect and contribution of each stage in our proposed framework and its performance over previous approaches in terms of classification accuracy and computational speed. In addition, the experiments on traditional manifold-based learning models and neural-network-based learning models demonstrated the applicability of our proposed framework for various types of learning models.      
### 64.MultiEarth 2022 -- The Champion Solution for Image-to-Image Translation Challenge via Generation Models  [ :arrow_down: ](https://arxiv.org/pdf/2207.00001.pdf)
>  The MultiEarth 2022 Image-to-Image Translation challenge provides a well-constrained test bed for generating the corresponding RGB Sentinel-2 imagery with the given Sentinel-1 VV &amp; VH imagery. In this challenge, we designed various generation models and found the SPADE [1] and pix2pixHD [2] models could perform our best results. In our self-evaluation, the SPADE-2 model with L1-loss can achieve 0.02194 MAE score and 31.092 PSNR dB. In our final submission, the best model can achieve 0.02795 MAE score ranked No.1 on the leader board.      
