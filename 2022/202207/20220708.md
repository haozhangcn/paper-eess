# ArXiv eess --Fri, 8 Jul 2022
### 1.Data-driven State of Risk Prediction and Mitigation in Support of the Net-zero Carbon Electric Grid  [ :arrow_down: ](https://arxiv.org/pdf/2207.03472.pdf)
>  An approach for reaching the net-zero carbon electricity grid is to intensify the deployment of distributed renewable generation resources such as photovoltaic (PV) solar and wind generation, complemented with stationary and mobile (electric vehicle) battery energy storage systems (BESS). This paper assumes a scenario where the PV renewable generation and BESS are integrated into a nano-Grid (n-Grid), a prosumer-owned virtual power plant installed at the residential or commercial sites. To be profitable, this distributed energy resource (DER) needs to be managed effectively to support its own load and the wholesale and retail market services. Our paper introduces a risk-based, data-driven approach focused on predicting the State of Risk (SoR) of the utility grid outages. The SoR prediction enables the development of optimal mitigation strategies aiming at reducing the impact of the grid outages by harvesting the n-Grid flexibility. Several data analytics tools to assist the n-Grid operators and aggregators for n-Grid participation in the wholesale market ancillary service products are introduced, and some preliminary implementation results are demonstrated.      
### 2.Linearized Physics-Based Lithium-Ion Battery Model for Power System Economic Studies  [ :arrow_down: ](https://arxiv.org/pdf/2207.03469.pdf)
>  This paper proposes the linearized physics-based model of a lithium-ion battery that can be incorporated into the optimization framework for power system economic studies. The proposed model is a linear approximation of the single particle model and it allows to characterize dynamics of the physical processes inside the battery that impact the battery operation. There is a need for such model as a simplistic power-energy model that is widely employed in operation and planning studies with the lithium-ion battery energy storage system (LIBESS) results in infeasible operation and misleading economic assessment. The proposed linearized model is computationally beneficial compared with a recently used nonlinear physics-based model. The energy arbitrage application is used to assess the advantages of the proposed model over a simple power-energy model.      
### 3.TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2207.03450.pdf)
>  Medical image segmentation is one of the most fundamental tasks concerning medical information analysis. Various solutions have been proposed so far, including many deep learning-based techniques, such as U-Net, FC-DenseNet, etc. However, high-precision medical image segmentation remains a highly challenging task due to the existence of inherent magnification and distortion in medical images as well as the presence of lesions with similar density to normal tissues. In this paper, we propose TFCNs (Transformers for Fully Convolutional denseNets) to tackle the problem by introducing ResLinear-Transformer (RL-Transformer) and Convolutional Linear Attention Block (CLAB) to FC-DenseNet. TFCNs is not only able to utilize more latent information from the CT images for feature extraction, but also can capture and disseminate semantic features and filter non-semantic features more effectively through the CLAB module. Our experimental results show that TFCNs can achieve state-of-the-art performance with dice scores of 83.72\% on the Synapse dataset. In addition, we evaluate the robustness of TFCNs for lesion area effects on the COVID-19 public datasets. The Python code will be made publicly available on <a class="link-external link-https" href="https://github.com/HUANGLIZI/TFCNs" rel="external noopener nofollow">this https URL</a>.      
### 4.Learning to restore images degraded by atmospheric turbulence using uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2207.03447.pdf)
>  Atmospheric turbulence can significantly degrade the quality of images acquired by long-range imaging systems by causing spatially and temporally random fluctuations in the index of refraction of the atmosphere. Variations in the refractive index causes the captured images to be geometrically distorted and blurry. Hence, it is important to compensate for the visual degradation in images caused by atmospheric turbulence. In this paper, we propose a deep learning-based approach for restring a single image degraded by atmospheric turbulence. We make use of the epistemic uncertainty based on Monte Carlo dropouts to capture regions in the image where the network is having hard time restoring. The estimated uncertainty maps are then used to guide the network to obtain the restored image. Extensive experiments are conducted on synthetic and real images to show the significance of the proposed work. Code is available at : <a class="link-external link-https" href="https://github.com/rajeevyasarla/AT-Net" rel="external noopener nofollow">this https URL</a>      
### 5.On Aggregation Performance in Privacy Conscious Hierarchical Flexibility Coordination Schemes  [ :arrow_down: ](https://arxiv.org/pdf/2207.03439.pdf)
>  In this paper we introduce a method for performance quantification of flexibility aggregation in flexibility coordination schemes (FCS), with a focus on privacy preserving hierarchical FCS. The quantification is based on two performance metrics: The aggregation error and the aggregation efficiency. We present the simulation framework and the modelling of one complex type of flexibility providing units (FPUs), namely energy storage systems (ESS). ESS cause intertemporal constraints for flexibility coordination that lead to aggregation errors in case flexibility is aggregated from heterogeneous groups of FPUs. We identify one parameter responsible for the aggregation error to be the power-to-energy ratio of the ESS. A grouping of FPUs using similarity in their power-to-energy ratios is shown to improve the coordination performance. Additionally, we describe the influence of flexibility demand timeseries on the aggregation error, concluding that future assessments of aggregation errors should consider multiple representative demand timeseries, which is a non-trivial task. Finally, we discuss the applicability of the developed method to scenarios of larger system size.      
### 6.A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion  [ :arrow_down: ](https://arxiv.org/pdf/2207.03430.pdf)
>  Multi-modal medical image completion has been extensively applied to alleviate the missing modality issue in a wealth of multi-modal diagnostic tasks. However, for most existing synthesis methods, their inferences of missing modalities can collapse into a deterministic mapping from the available ones, ignoring the uncertainties inherent in the cross-modal relationships. Here, we propose the Unified Multi-Modal Conditional Score-based Generative Model (UMM-CSGM) to take advantage of Score-based Generative Model (SGM) in modeling and stochastically sampling a target probability distribution, and further extend SGM to cross-modal conditional synthesis for various missing-modality configurations in a unified framework. Specifically, UMM-CSGM employs a novel multi-in multi-out Conditional Score Network (mm-CSN) to learn a comprehensive set of cross-modal conditional distributions via conditional diffusion and reverse generation in the complete modality space. In this way, the generation process can be accurately conditioned by all available information, and can fit all possible configurations of missing modalities in a single network. Experiments on BraTS19 dataset show that the UMM-CSGM can more reliably synthesize the heterogeneous enhancement and irregular area in tumor-induced lesions for any missing modalities.      
### 7.Inertia Adequacy in Transient Stability Models for Synthetic Electric Grids  [ :arrow_down: ](https://arxiv.org/pdf/2207.03396.pdf)
>  If a disturbance rocks a low-inertia power system, the frequency decline may be too rapid to arrest before it triggers undesirable responses from generators and loads. In the worst case, this instability could lead to blackout and major equipment damage. Electric utilities, to combat this, study inertia adequacy in systems that are particularly vulnerable. This process, involving detailed transient simulations, usually leads to a notion of a system-wide inertia floor. Ongoing questions in this analysis are in how to set the inertial floor and to what extent the location of frequency control resources matters. This paper proposes a new analysis technique that quantifies theoretical locational rate of change of frequency (ROCOF) as a computationally efficient screening algorithm scalable to large systems. An additional challenge in moving this area forward is the lack of high-quality, public benchmark dynamics cases. This paper presents a synthetic case for such purposes and a methodology for validation, to ensure that it is well suited to inertia adequacy studies to improve electric grid performance.      
### 8.Benefits and Challenges of Dynamic Modelling of Cascading Failures in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2207.03389.pdf)
>  Time-based dynamic models of cascading failures have been recognized as one of the most comprehensive methods of representing detailed cascading information and are often used for benchmarking and validation. This paper provides an overview of the progress in the field of dynamic analysis of cascading failures in power systems and outlines the benefits and challenges of dynamic simulations in future grids. The benefits include the ability to capture temporal characteristics of system dynamics and provide timing information to facilitate control actions for blackout mitigation. The greatest barriers to dynamic modelling of cascading failures are the computational burden, and the extensive but often unavailable data requirements for dynamic representation of a power system. These factors are discussed in detail in this paper and the need for in-depth research into dynamic modelling of cascading failures is highlighted. Furthermore, case studies of dynamic cascading simulation of 200-bus and 2000-bus benchmark systems provide initial guidance for the selection of critical parameters to enhance simulation efficiency. Finally, cross-validation and comparison against a quasi-steady state DC power flow model is performed, with various metrics compared.      
### 9.Speech Emotion: Investigating Model Representations, Multi-Task Learning and Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2207.03334.pdf)
>  Estimating dimensional emotions, such as activation, valence and dominance, from acoustic speech signals has been widely explored over the past few years. While accurate estimation of activation and dominance from speech seem to be possible, the same for valence remains challenging. Previous research has shown that the use of lexical information can improve valence estimation performance. Lexical information can be obtained from pre-trained acoustic models, where the learned representations can improve valence estimation from speech. We investigate the use of pre-trained model representations to improve valence estimation from acoustic speech signal. We also explore fusion of representations to improve emotion estimation across all three emotion dimensions: activation, valence and dominance. Additionally, we investigate if representations from pre-trained models can be distilled into models trained with low-level features, resulting in models with a less number of parameters. We show that fusion of pre-trained model embeddings result in a 79% relative improvement in concordance correlation coefficient CCC on valence estimation compared to standard acoustic feature baseline (mel-filterbank energies), while distillation from pre-trained model embeddings to lower-dimensional representations yielded a relative 12% improvement. Such performance gains were observed over two evaluation sets, indicating that our proposed architecture generalizes across those evaluation sets. We report new state-of-the-art "text-free" acoustic-only dimensional emotion estimation $CCC$ values on two MSP-Podcast evaluation sets.      
### 10.Low-resource Low-footprint Wake-word Detection using Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2207.03331.pdf)
>  As virtual assistants have become more diverse and specialized, so has the demand for application or brand-specific wake words. However, the wake-word-specific datasets typically used to train wake-word detectors are costly to create. In this paper, we explore two techniques to leverage acoustic modeling data for large-vocabulary speech recognition to improve a purpose-built wake-word detector: transfer learning and knowledge distillation. We also explore how these techniques interact with time-synchronous training targets to improve detection latency. Experiments are presented on the open-source "Hey Snips" dataset and a more challenging in-house far-field dataset. Using phone-synchronous targets and knowledge distillation from a large acoustic model, we are able to improve accuracy across dataset sizes for both datasets while reducing latency.      
### 11.Reinforcement Learning for Distributed Transient Frequency Control with Stability and Safety Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2207.03329.pdf)
>  This paper proposes a reinforcement learning-based approach for optimal transient frequency control in power systems with stability and safety guarantees. Building on Lyapunov stability theory and safety-critical control, we derive sufficient conditions on the distributed controller design that ensure the stability and transient frequency safety of the closed-loop system. Our idea of distributed dynamic budget assignment makes these conditions less conservative than those in recent literature, so that they can impose less stringent restrictions on the search space of control policies. We construct neural network controllers that parameterize such control policies and use reinforcement learning to train an optimal one. Simulations on the IEEE 39-bus network illustrate the guaranteed stability and safety properties of the controller along with its significantly improved optimality.      
### 12.State Prediction of Human-in-the-Loop Multi-rotor System with Stochastic Human Behavior Model  [ :arrow_down: ](https://arxiv.org/pdf/2207.03318.pdf)
>  Reachability analysis is a widely used method to analyze the safety of a Human-in-the-Loop Cyber Physical System (HiLCPS). This strategy allows the HiLCPS to respond against an imminent threat in advance by predicting reachable states of the system. However, it could lead to an unnecessarily conservative reachable set if the prediction only relies on the system dynamics without explicitly considering human behavior, and thus the risk might be overestimated. To reduce the conservativeness of the reachability analysis, we present a state prediction method which takes into account a stochastic human behavior model represented as a Gaussian Mixture Model (GMM). In this paper, we focus on the multi-rotor in a near-collision situation. The stochastic human behavior model is trained using experimental data to represent human operators' evasive maneuver. Then, we can retrieve a human control input probability distribution from the trained stochastic human behavior model using the Gaussian Mixture Regression (GMR). The proposed algorithm predicts the probability distribution of the multi-rotor's future state based on the given dynamics and the retrieved human control input probability distribution. Besides, the proposed state prediction method considers the uncertainty of the initial state modeled as a GMM, which yields more robust performance. Human subject experiment results are provided to demonstrate the effectiveness of the proposed algorithm.      
### 13.Do We Really Need Inertia?  [ :arrow_down: ](https://arxiv.org/pdf/2207.03292.pdf)
>  The emphasis on inertia for system stability has been a long-held tradition in conventional grids. The fast and flexible controllability of inverters opens up new possibilities. This letter investigates inertia-free power systems driven by inverter-based resources. We illustrate that by replacing inertia with fast primary control, the first-swing stability region is greatly extended compared to the classic equal area criterion. The new criterion is fully decentralised and independent of network topology, and therefore is readily scalable to complex systems. This enables simplified system operation and new business models of stability services from distributed resources. The findings of the letter are rigorously proved and verified by simulation on the IEEE 68-bus system.      
### 14.Tractable Data Enriched Distributionally Robust Chance-Constrained CVR  [ :arrow_down: ](https://arxiv.org/pdf/2207.03286.pdf)
>  This paper proposes a tractable distributionally robust chance-constrained conservation voltage reduction (DRCC-CVR) method with enriched data-based ambiguity set in unbalanced three-phase distribution systems. The increasing penetration of distributed renewable energy not only brings clean power but also challenges the voltage regulation and energy-saving performance of CVR by introducing high uncertainties to distribution systems. In most cases, the conventional robust optimization methods for CVR only provide conservative solutions. To better consider the impacts of load and PV generation uncertainties on CVR implementation in distribution systems and provide less conservative solutions, this paper develops a data-based DRCC-CVR model with tractable reformulation and data enrichment method. Even though the uncertainties of load and photovoltaic (PV) can be captured by data, the availability of smart meters (SMs) and micro-phasor measurement units (PMUs) is restricted by cost budget. The limited data access may hinder the performance of the proposed DRCC-CVR. Thus, we further present a data enrichment method to statistically recover the high-resolution load and PV generation data from low-resolution data with Gaussian Process Regression (GPR) and Markov Chain (MC) models, which can be used to construct a data-based moment ambiguity set of uncertainty distributions for the proposed DRCC-CVR. Finally, the nonlinear power flow and voltage dependant load models and DRCC with moment-based ambiguity set are reformulated to be computationally tractable and tested on a real distribution feeder in Midwest U. S. to validate the effectiveness and robustness of the proposed method.      
### 15.NESC: Robust Neural End-2-End Speech Coding with GANs  [ :arrow_down: ](https://arxiv.org/pdf/2207.03282.pdf)
>  Neural networks have proven to be a formidable tool to tackle the problem of speech coding at very low bit rates. However, the design of a neural coder that can be operated robustly under real-world conditions remains a major challenge. Therefore, we present Neural End-2-End Speech Codec (NESC) a robust, scalable end-to-end neural speech codec for high-quality wideband speech coding at 3 kbps. The encoder uses a new architecture configuration, which relies on our proposed Dual-PathConvRNN (DPCRNN) layer, while the decoder architecture is based on our previous work Streamwise-StyleMelGAN. Our subjective listening tests on clean and noisy speech show that NESC is particularly robust to unseen conditions and signal perturbations.      
### 16.New perspectives on transient stability between grid-following and grid-forming VSCs  [ :arrow_down: ](https://arxiv.org/pdf/2207.03273.pdf)
>  The grid-following and grid-forming controls in voltage-source converters are considered as different operation modes and the synchronization mechanism of them are studied separately. In this article, the intrinsic relationships between gridfollowing and grid-forming controlled converters are established as follows: 1) the proportional gain of PLL is in inverse proportion to damping; 2) the integral gain of PLL is similar to integral droop; 3) PLL has no practical inertia but acts like grid-forming control in zero inertia cases. Further, a general stability-enhanced method combining damping and inertia is proposed, and the modified energy function is obtained to estimate the region of attraction for the system. Finally, these findings are corroborated by simulation tests with an intuitive conclusion.      
### 17.Fuel savings through missed approach maneuvers based on aircraft reinjection  [ :arrow_down: ](https://arxiv.org/pdf/2207.03262.pdf)
>  Humanity is facing global challenges related to climate change, along with an energetic crisis that urgently requires optimizing any process or system able to improve global economic conditions. Taking fuel as an example, its prices are among the highest increases detected in goods of general use. In this sense, initiatives to mitigate fuel consumption are both welcome and necessary. In the aerial transportation area, fuel costs are critical to the economic viability of companies, and so urgent measures should be adopted to avoid any unnecessary increase of operational costs. In this work we study the costs involved in a standard procedure following a missed approach. In addition, we study the improvements achieved with a fast reinjection scheme proposed in a prior work. Experimental results show that, for a standard A320 aircraft, fuel savings ranging from 55% to 90% can be achieved through our proposed method.      
### 18.Waveform Design and Hybrid Duplex Exploiting Radar Features for Joint Communication and Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.03241.pdf)
>  Joint communication and sensing (JCAS) is a very promising 6G technology, which attracts more and more research attention. Unlike communication, radar has many unique features in terms of waveform criteria, self-interference cancellation (SIC), aperture-dependent resolution, and virtual aperture. This paper proposes a waveform design named max-aperture radar slicing (MaRS) to gain a large time-frequency aperture, which reuses the orthogonal frequency division multiplexing (OFDM) hardware and occupies only a tiny fraction of OFDM resources. The proposed MaRS keeps the radar advantages of constant modulus, zero auto-correlation, and simple SIC. Joint space-time processing algorithms are proposed to recover the range-velocity-angle information from strong clutters. Furthermore, this paper proposes a hybrid-duplex JCAS scheme where communication is half-duplex while radar is full-duplex. In this scheme, the half-duplex communication antenna array is reused, and a small sensing-dedicated antenna array is specially designed. Using these two arrays, a large space-domain aperture is virtually formed to greatly improve the angle resolution. The numerical results show that the proposed MaRS and hybrid-duplex schemes achieve a high sensing resolution with less than 0.4% OFDM resources and gain an almost 100% hit rate for both car and UAV detection at a range up to 1 km.      
### 19.Consistent discretization of finite/fixed-time controllers  [ :arrow_down: ](https://arxiv.org/pdf/2207.03235.pdf)
>  The paper proposes an algorithm for a discretization (sampled-time implementation) of a homogeneous control preserving the finite-time and nearly fixed-time stability property of the original (sampling-free) system. The sampling period is assumed to be constant. Both single-input and multiple-input cases are considered. The robustness (Input-to-State Stability) of the obtained sampled-time control system is studied as well. Theoretical results are supported by numerical simulations.      
### 20.Control of Oscillatory Temperature Field in a Building via Damping Assignment to Nonlinear Koopman Mode  [ :arrow_down: ](https://arxiv.org/pdf/2207.03219.pdf)
>  This paper addresses a control problem on air-conditioning systems in buildings that is regarded as a control practice of nonlinear distributed-parameter systems. Specifically, we consider the design of a controller for suppressing an oscillatory response of in-room temperature field. The main idea in this paper is to apply the emergent theory of Koopman operator and Koopman mode decomposition for nonlinear systems, and to formulate a technique of damping assignment to a nonlinear Koopman mode in a fully data-driven manner. Its effectiveness is examined by numerical simulations guided by measurement of a practical room space.      
### 21.BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.03210.pdf)
>  We propose a method for estimating the bone mineral density (BMD) from a plain x-ray image. Dual-energy X-ray absorptiometry (DXA) and quantitative computed tomography (QCT) provide high accuracy in diagnosing osteoporosis; however, these modalities require special equipment and scan protocols. Measuring BMD from an x-ray image provides an opportunistic screening, which is potentially useful for early diagnosis. The previous methods that directly learn the relationship between x-ray images and BMD require a large training dataset to achieve high accuracy because of large intensity variations in the x-ray images. Therefore, we propose an approach using the QCT for training a generative adversarial network (GAN) and decomposing an x-ray image into a projection of bone-segmented QCT. The proposed hierarchical learning improved the robustness and accuracy of quantitatively decomposing a small-area target. The evaluation of 200 patients with osteoarthritis using the proposed method, which we named BMD-GAN, demonstrated a Pearson correlation coefficient of 0.888 between the predicted and ground truth DXA-measured BMD. Besides not requiring a large-scale training database, another advantage of our method is its extensibility to other anatomical areas, such as the vertebrae and rib bones.      
### 22.Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2207.03180.pdf)
>  Recently, deep-learning-based approaches have been widely studied for deformable image registration task. However, most efforts directly map the composite image representation to spatial transformation through the convolutional neural network, ignoring its limited ability to capture spatial correspondence. On the other hand, Transformer can better characterize the spatial relationship with attention mechanism, its long-range dependency may be harmful to the registration task, where voxels with too large distances are unlikely to be corresponding pairs. In this study, we propose a novel Deformer module along with a multi-scale framework for the deformable image registration task. The Deformer module is designed to facilitate the mapping from image representation to spatial transformation by formulating the displacement vector prediction as the weighted summation of several bases. With the multi-scale framework to predict the displacement fields in a coarse-to-fine manner, superior performance can be achieved compared with traditional and learning-based approaches. Comprehensive experiments on two public datasets are conducted to demonstrate the effectiveness of the proposed Deformer module as well as the multi-scale framework.      
### 23.End-to-end Speech-to-Punctuated-Text Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.03169.pdf)
>  Conventional automatic speech recognition systems do not produce punctuation marks which are important for the readability of the speech recognition results. They are also needed for subsequent natural language processing tasks such as machine translation. There have been a lot of works on punctuation prediction models that insert punctuation marks into speech recognition results as post-processing. However, these studies do not utilize acoustic information for punctuation prediction and are directly affected by speech recognition errors. In this study, we propose an end-to-end model that takes speech as input and outputs punctuated texts. This model is expected to predict punctuation robustly against speech recognition errors while using acoustic information. We also propose to incorporate an auxiliary loss to train the model using the output of the intermediate layer and unpunctuated texts. Through experiments, we compare the performance of the proposed model to that of a cascaded system. The proposed model achieves higher punctuation prediction accuracy than the cascaded system without sacrificing the speech recognition error rate. It is also demonstrated that the multi-task learning using the intermediate output against the unpunctuated text is effective. Moreover, the proposed model has only about 1/7th of the parameters compared to the cascaded system.      
### 24.5G for Railways: the Next Generation Railway Dedicated Communications  [ :arrow_down: ](https://arxiv.org/pdf/2207.03127.pdf)
>  To overcome increasing traffic, provide various new services, further ensure safety and security, significantly improve travel comfort, a new communication system for railways is required. Since 2019, public networks have been evolving to the fifth generation communication (5G) worldwide, whereas the main communication system of railway is still based on the second generation communication (2G). It is thus necessary for railways to replace the current 2G-based technology with the next generation railway dedicated communication system with improved capacity and capability, and the 5G for railways (5G-R) technology is a promising solution for further intelligent railways. This article gives a review of the current developments of the next generation railway communications, followed by a discussion of the typical services that the 5G-R can provide to intelligent railways. Then, main application scenarios of 5G-R are summarized and system configurations are compared. Some key technologies of 5G-R such as network architecture, massive MIMO, millimeter-wave, multiple access scheme, ultra-reliable low latency communication, and advanced video processing are presented and analyzed. Finally, some challenges of 5G-R are highlighted.      
### 25.What Makes for Automatic Reconstruction of Pulmonary Segments  [ :arrow_down: ](https://arxiv.org/pdf/2207.03078.pdf)
>  3D reconstruction of pulmonary segments plays an important role in surgical treatment planning of lung cancer, which facilitates preservation of pulmonary function and helps ensure low recurrence rates. However, automatic reconstruction of pulmonary segments remains unexplored in the era of deep learning. In this paper, we investigate what makes for automatic reconstruction of pulmonary segments. First and foremost, we formulate, clinically and geometrically, the anatomical definitions of pulmonary segments, and propose evaluation metrics adhering to these definitions. Second, we propose ImPulSe (Implicit Pulmonary Segment), a deep implicit surface model designed for pulmonary segment reconstruction. The automatic reconstruction of pulmonary segments by ImPulSe is accurate in metrics and visually appealing. Compared with canonical segmentation methods, ImPulSe outputs continuous predictions of arbitrary resolutions with higher training efficiency and fewer parameters. Lastly, we experiment with different network inputs to analyze what matters in the task of pulmonary segment reconstruction. Our code is available at <a class="link-external link-https" href="https://github.com/M3DV/ImPulSe" rel="external noopener nofollow">this https URL</a>.      
### 26.Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.03050.pdf)
>  Lung nodules can be an alarming precursor to potential lung cancer. Missed nodule detections during chest radiograph analysis remains a common challenge among thoracic radiologists. In this work, we present a multi-task lung nodule detection algorithm for chest radiograph analysis. Unlike past approaches, our algorithm predicts a global-level label indicating nodule presence along with local-level labels predicting nodule locations using a Dual Head Network (DHN). We demonstrate the favorable nodule detection performance that our multi-task formulation yields in comparison to conventional methods. In addition, we introduce a novel Dual Head Augmentation (DHA) strategy tailored for DHN, and we demonstrate its significance in further enhancing global and local nodule predictions.      
### 27.Towards Length-Versatile and Noise-Robust Radio Frequency Fingerprint Identification  [ :arrow_down: ](https://arxiv.org/pdf/2207.03001.pdf)
>  Radio frequency fingerprint identification (RFFI) can classify wireless devices by analyzing the signal distortions caused by the intrinsic hardware impairments. State-of-the-art neural networks have been adopted for RFFI. However, many neural networks, e.g., multilayer perceptron (MLP) and convolutional neural network (CNN), require fixed-size input data. In addition, many IoT devices work in low signal-to-noise ratio (SNR) scenarios but the RFFI performance in such scenarios is rarely investigated. In this paper, we analyze the reason why MLP- and CNN-based RFFI systems are constrained by the input size. To overcome this, we propose four neural networks that can process signals of variable lengths, namely flatten-free CNN, long short-term memory (LSTM) network, gated recurrent unit (GRU) network and transformer. We adopt data augmentation during training which can significantly improve the model's robustness to noise. We compare two augmentation schemes, namely offline and online augmentation. The results show the online one performs better. During the inference, a multi-packet inference approach is further leveraged to improve the classification accuracy in low SNR scenarios. We take LoRa as a case study and evaluate the system by classifying 10 commercial-off-the-shelf LoRa devices in various SNR conditions. The online augmentation can boost the low-SNR classification accuracy by up to 50% and the multi-packet inference approach can further increase the accuracy by over 20%.      
### 28.Towards Receiver-Agnostic and Collaborative Radio Frequency Fingerprint Identification  [ :arrow_down: ](https://arxiv.org/pdf/2207.02999.pdf)
>  Radio frequency fingerprint identification (RFFI) is an emerging device authentication technique, which exploits the hardware characteristics of the RF front-end as device identifiers. RFFI is implemented in the wireless receiver and acts to extract the transmitter impairments and then perform classification. The receiver hardware impairments will actually interfere with the feature extraction process, but its effect and mitigation have not been comprehensively studied. In this paper, we propose a receiver-agnostic RFFI system that is not sensitive to the changes in receiver characteristics; it is implemented by employing adversarial training to learn the receiver-independent features. Moreover, when there are multiple receivers, this functionality can perform collaborative inference to enhance classification accuracy. Finally, we show how it is possible to leverage fine-tuning for further improvement with fewer collected signals. To validate the approach, we have conducted extensive experimental evaluation by applying the approach to a LoRaWAN case study involving ten LoRa devices and 20 software-defined radio (SDR) receivers. The results show that receiver-agnostic training enables the trained neural network to become robust to changes in receiver characteristics. The collaborative inference improves classification accuracy by up to 20% beyond a single-receiver RFFI system and fine-tuning can bring a 40% improvement for under-performing receivers.      
### 29.Impact of Internal Algebraic Variable Treatment on Transient Stability Simulation Performance  [ :arrow_down: ](https://arxiv.org/pdf/2207.02997.pdf)
>  It is a general notion that, in transient stability simulations, reducing the number of algebraic variables for the differential-algebraic equations (DAE) can improve the simulation performance. Many simulation programs split algebraic variables internal to a dynamic model from the full DAE and evaluate them outside each iterative step, using results from the previous iteration. The updated internal variables are then treated as constants when solving for the current iteration. This letter discusses how such a split formulation can impact simulation performance. Case studies using various systems with synchronous generator and converter models demonstrate the impact of the split on the convergence pattern and simulation performance.      
### 30.Context-aware Self-supervised Learning for Medical Images Using Graph Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.02957.pdf)
>  Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufficiently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. We introduce a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. We use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that our approach compares favorably to baseline methods that do not account for the context. We use the learned embedding for staging lung tissue abnormalities related to COVID-19.      
### 31.Virtual staining of defocused autofluorescence images of unlabeled tissue using deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.02946.pdf)
>  Deep learning-based virtual staining was developed to introduce image contrast to label-free tissue sections, digitally matching the histological staining, which is time-consuming, labor-intensive, and destructive to tissue. Standard virtual staining requires high autofocusing precision during the whole slide imaging of label-free tissue, which consumes a significant portion of the total imaging time and can lead to tissue photodamage. Here, we introduce a fast virtual staining framework that can stain defocused autofluorescence images of unlabeled tissue, achieving equivalent performance to virtual staining of in-focus label-free images, also saving significant imaging time by lowering the microscope's autofocusing precision. This framework incorporates a virtual-autofocusing neural network to digitally refocus the defocused images and then transforms the refocused images into virtually stained images using a successive network. These cascaded networks form a collaborative inference scheme: the virtual staining model regularizes the virtual-autofocusing network through a style loss during the training. To demonstrate the efficacy of this framework, we trained and blindly tested these networks using human lung tissue. Using 4x fewer focus points with 2x lower focusing precision, we successfully transformed the coarsely-focused autofluorescence images into high-quality virtually stained H&amp;E images, matching the standard virtual staining framework that used finely-focused autofluorescence input images. Without sacrificing the staining quality, this framework decreases the total image acquisition time needed for virtual staining of a label-free whole-slide image (WSI) by ~32%, together with a ~89% decrease in the autofocusing time, and has the potential to eliminate the laborious and costly histochemical staining process in pathology.      
### 32.ISAC from the Sky: UAV Trajectory Design for Joint Communication and Target Localization  [ :arrow_down: ](https://arxiv.org/pdf/2207.02904.pdf)
>  Unmanned aerial vehicles (UAVs) as aerial base stations (BSs) are able to provide not only the communication service to ground users, but also the sensing functionality to localize targets of interests. In this paper, we consider an airborne integrated sensing and communications (ISAC) system where a UAV, which acts both as a communication BS and a mono-static radar, flies over a given area to transmit downlink signal to a ground communication user. In the meantime, the same transmitted signal is also exploited for mono-static radar sensing. We aim to optimize the UAV trajectory, such that the performance for both communication and sensing (C$\&amp;$S) is explicitly considered. In particular, we first formulate the trajectory design problem into a weighted optimization problem, where a flexible performance trade-off between C$\&amp;$S is achieved. As a step forward, a multi-stage trajectory design approach is proposed to improve the target estimation accuracy. While the resultant optimization problem is difficult to solve directly, we develop an iterative algorithm to obtain a locally optimal solution. Finally, numerical results show that the target estimation error obtained by the trade-off approach is about an order of magnitude better than a communication-only approach with a slight decrease on communication performance.      
### 33.Perfusion imaging in deep prostate cancer detection from mp-MRI: can we take advantage of it?  [ :arrow_down: ](https://arxiv.org/pdf/2207.02854.pdf)
>  To our knowledge, all deep computer-aided detection and diagnosis (CAD) systems for prostate cancer (PCa) detection consider bi-parametric magnetic resonance imaging (bp-MRI) only, including T2w and ADC sequences while excluding the 4D perfusion sequence,which is however part of standard clinical protocols for this diagnostic task. In this paper, we question strategies to integrate information from perfusion imaging in deep neural architectures. To do so, we evaluate several ways to encode the perfusion information in a U-Net like architecture, also considering early versus mid fusion strategies. We compare performance of multiparametric MRI (mp-MRI) models with the baseline bp-MRI model based on a private dataset of 219 mp-MRI exams. Perfusion maps derived from dynamic contrast enhanced MR exams are shown to positively impact segmentation and grading performance of PCa lesions, especially the 3D MR volume corresponding to the maximum slope of the wash-in curve as well as Tmax perfusion maps. The latter mp-MRI models indeed outperform the bp-MRI one whatever the fusion strategy, with Cohen's kappa score of 0.318$\pm$0.019 for the bp-MRI model and 0.378 $\pm$ 0.033 for the model including the maximum slope with a mid fusion strategy, also achieving competitive Cohen's kappa score compared to state of the art.      
### 34.Finding Fallen Objects Via Asynchronous Audio-Visual Integration  [ :arrow_down: ](https://arxiv.org/pdf/2207.03483.pdf)
>  The way an object looks and sounds provide complementary reflections of its physical properties. In many settings cues from vision and audition arrive asynchronously but must be integrated, as when we hear an object dropped on the floor and then must find it. In this paper, we introduce a setting in which to study multi-modal object localization in 3D virtual environments. An object is dropped somewhere in a room. An embodied robot agent, equipped with a camera and microphone, must determine what object has been dropped -- and where -- by combining audio and visual signals with knowledge of the underlying physics. To study this problem, we have generated a large-scale dataset -- the Fallen Objects dataset -- that includes 8000 instances of 30 physical object categories in 64 rooms. The dataset uses the ThreeDWorld platform which can simulate physics-based impact sounds and complex physical interactions between objects in a photorealistic setting. As a first step toward addressing this challenge, we develop a set of embodied agent baselines, based on imitation learning, reinforcement learning, and modular planning, and perform an in-depth analysis of the challenge of this new task.      
### 35.A Model-based Multi-agent Framework to Enable an Agile Response to Supply Chain Disruptions  [ :arrow_down: ](https://arxiv.org/pdf/2207.03460.pdf)
>  Due to the COVID-19 pandemic, the global supply chain is disrupted at an unprecedented scale under uncertain and unknown trends of labor shortage, high material prices, and changing travel or trade regulations. To stay competitive, enterprises desire agile and dynamic response strategies to quickly react to disruptions and recover supply-chain functions. Although both centralized and multi-agent approaches have been studied, their implementation requires prior knowledge of disruptions and agent-rule-based reasoning. In this paper, we introduce a model-based multi-agent framework that enables agent coordination and dynamic agent decision-making to respond to supply chain disruptions in an agile and effective manner. Through a small-scale simulated case study, we showcase the feasibility of the proposed approach under several disruption scenarios that affect a supply chain network differently, and analyze performance trade-offs between the proposed distributed and centralized methods.      
### 36.Stochastic optimal well control in subsurface reservoirs using reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.03456.pdf)
>  We present a case study of model-free reinforcement learning (RL) framework to solve stochastic optimal control for a predefined parameter uncertainty distribution and partially observable system. We focus on robust optimal well control problem which is a subject of intensive research activities in the field of subsurface reservoir management. For this problem, the system is partially observed since the data is only available at well locations. Furthermore, the model parameters are highly uncertain due to sparsity of available field data. In principle, RL algorithms are capable of learning optimal action policies -- a map from states to actions -- to maximize a numerical reward signal. In deep RL, this mapping from state to action is parameterized using a deep neural network. In the RL formulation of the robust optimal well control problem, the states are represented by saturation and pressure values at well locations while the actions represent the valve openings controlling the flow through wells. The numerical reward refers to the total sweep efficiency and the uncertain model parameter is the subsurface permeability field. The model parameter uncertainties are handled by introducing a domain randomisation scheme that exploits cluster analysis on its uncertainty distribution. We present numerical results using two state-of-the-art RL algorithms, proximal policy optimization (PPO) and advantage actor-critic (A2C), on two subsurface flow test cases representing two distinct uncertainty distributions of permeability field. The results were benchmarked against optimisation results obtained using differential evolution algorithm. Furthermore, we demonstrate the robustness of the proposed use of RL by evaluating the learned control policy on unseen samples drawn from the parameter uncertainty distribution that were not used during the training process.      
### 37.Binary Iterative Hard Thresholding Converges with Optimal Number of Measurements for 1-Bit Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2207.03427.pdf)
>  Compressed sensing has been a very successful high-dimensional signal acquisition and recovery technique that relies on linear operations. However, the actual measurements of signals have to be quantized before storing or processing. 1(One)-bit compressed sensing is a heavily quantized version of compressed sensing, where each linear measurement of a signal is reduced to just one bit: the sign of the measurement. Once enough of such measurements are collected, the recovery problem in 1-bit compressed sensing aims to find the original signal with as much accuracy as possible. The recovery problem is related to the traditional "halfspace-learning" problem in learning theory. <br>For recovery of sparse vectors, a popular reconstruction method from 1-bit measurements is the binary iterative hard thresholding (BIHT) algorithm. The algorithm is a simple projected sub-gradient descent method, and is known to converge well empirically, despite the nonconvexity of the problem. The convergence property of BIHT was not theoretically justified, except with an exorbitantly large number of measurements (i.e., a number of measurement greater than $\max\{k^{10}, 24^{48}, k^{3.5}/\epsilon\}$, where $k$ is the sparsity, $\epsilon$ denotes the approximation error, and even this expression hides other factors). In this paper we show that the BIHT algorithm converges with only $\tilde{O}(\frac{k}{\epsilon})$ measurements. Note that, this dependence on $k$ and $\epsilon$ is optimal for any recovery method in 1-bit compressed sensing. With this result, to the best of our knowledge, BIHT is the only practical and efficient (polynomial time) algorithm that requires the optimal number of measurements in all parameters (both $k$ and $\epsilon$). This is also an example of a gradient descent algorithm converging to the correct solution for a nonconvex problem, under suitable structural conditions.      
### 38.Non-Linear Pairwise Language Mappings for Low-Resource Multilingual Acoustic Model Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2207.03391.pdf)
>  Multilingual speech recognition has drawn significant attention as an effective way to compensate data scarcity for low-resource languages. End-to-end (e2e) modelling is preferred over conventional hybrid systems, mainly because of no lexicon requirement. However, hybrid DNN-HMMs still outperform e2e models in limited data scenarios. Furthermore, the problem of manual lexicon creation has been alleviated by publicly available trained models of grapheme-to-phoneme (G2P) and text to IPA transliteration for a lot of languages. In this paper, a novel approach of hybrid DNN-HMM acoustic models fusion is proposed in a multilingual setup for the low-resource languages. Posterior distributions from different monolingual acoustic models, against a target language speech signal, are fused together. A separate regression neural network is trained for each source-target language pair to transform posteriors from source acoustic model to the target language. These networks require very limited data as compared to the ASR training. Posterior fusion yields a relative gain of 14.65% and 6.5% when compared with multilingual and monolingual baselines respectively. Cross-lingual model fusion shows that the comparable results can be achieved without using posteriors from the language dependent ASR.      
### 39.Investigating the Impact of Cross-lingual Acoustic-Phonetic Similarities on Multilingual Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2207.03390.pdf)
>  Multilingual automatic speech recognition (ASR) systems mostly benefit low resource languages but suffer degradation in performance across several languages relative to their monolingual counterparts. Limited studies have focused on understanding the languages behaviour in the multilingual speech recognition setups. In this paper, a novel data-driven approach is proposed to investigate the cross-lingual acoustic-phonetic similarities. This technique measures the similarities between posterior distributions from various monolingual acoustic models against a target speech signal. Deep neural networks are trained as mapping networks to transform the distributions from different acoustic models into a directly comparable form. The analysis observes that the languages closeness can not be truly estimated by the volume of overlapping phonemes set. Entropy analysis of the proposed mapping networks exhibits that a language with lesser overlap can be more amenable to cross-lingual transfer, and hence more beneficial in the multilingual setup. Finally, the proposed posterior transformation approach is leveraged to fuse monolingual models for a target language. A relative improvement of ~8% over monolingual counterpart is achieved.      
### 40.Automated Classification of General Movements in Infants Using a Two-stream Spatiotemporal Fusion Network  [ :arrow_down: ](https://arxiv.org/pdf/2207.03344.pdf)
>  The assessment of general movements (GMs) in infants is a useful tool in the early diagnosis of neurodevelopmental disorders. However, its evaluation in clinical practice relies on visual inspection by experts, and an automated solution is eagerly awaited. Recently, video-based GMs classification has attracted attention, but this approach would be strongly affected by irrelevant information, such as background clutter in the video. Furthermore, for reliability, it is necessary to properly extract the spatiotemporal features of infants during GMs. In this study, we propose an automated GMs classification method, which consists of preprocessing networks that remove unnecessary background information from GMs videos and adjust the infant's body position, and a subsequent motion classification network based on a two-stream structure. The proposed method can efficiently extract the essential spatiotemporal features for GMs classification while preventing overfitting to irrelevant information for different recording environments. We validated the proposed method using videos obtained from 100 infants. The experimental results demonstrate that the proposed method outperforms several baseline models and the existing methods.      
### 41.Monkeypox Skin Lesion Detection Using Deep Learning Models: A Feasibility Study  [ :arrow_down: ](https://arxiv.org/pdf/2207.03342.pdf)
>  The recent monkeypox outbreak has become a public health concern due to its rapid spread in more than 40 countries outside Africa. Clinical diagnosis of monkeypox in an early stage is challenging due to its similarity with chickenpox and measles. In cases where the confirmatory Polymerase Chain Reaction (PCR) tests are not readily available, computer-assisted detection of monkeypox lesions could be beneficial for surveillance and rapid identification of suspected cases. Deep learning methods have been found effective in the automated detection of skin lesions, provided that sufficient training examples are available. However, as of now, such datasets are not available for the monkeypox disease. In the current study, we first develop the ``Monkeypox Skin Lesion Dataset (MSLD)" consisting skin lesion images of monkeypox, chickenpox, and measles. The images are mainly collected from websites, news portals, and publicly accessible case reports. Data augmentation is used to increase the sample size, and a 3-fold cross-validation experiment is set up. In the next step, several pre-trained deep learning models, namely, VGG-16, ResNet50, and InceptionV3 are employed to classify monkeypox and other diseases. An ensemble of the three models is also developed. ResNet50 achieves the best overall accuracy of $82.96(\pm4.57\%)$, while VGG16 and the ensemble system achieved accuracies of $81.48(\pm6.87\%)$ and $79.26(\pm1.05\%)$, respectively. A prototype web-application is also developed as an online monkeypox screening tool. While the initial results on this limited dataset are promising, a larger demographically diverse dataset is required to further enhance the generalizability of these models.      
### 42.Text to Image Synthesis using Stacked Conditional Variational Autoencoders and Conditional Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.03332.pdf)
>  Synthesizing a realistic image from textual description is a major challenge in computer vision. Current text to image synthesis approaches falls short of producing a highresolution image that represent a text descriptor. Most existing studies rely either on Generative Adversarial Networks (GANs) or Variational Auto Encoders (VAEs). GANs has the capability to produce sharper images but lacks the diversity of outputs, whereas VAEs are good at producing a diverse range of outputs, but the images generated are often blurred. Taking into account the relative advantages of both GANs and VAEs, we proposed a new stacked Conditional VAE (CVAE) and Conditional GAN (CGAN) network architecture for synthesizing images conditioned on a text description. This study uses Conditional VAEs as an initial generator to produce a high-level sketch of the text descriptor. This high-level sketch output from first stage and a text descriptor is used as an input to the conditional GAN network. The second stage GAN produces a 256x256 high resolution image. The proposed architecture benefits from a conditioning augmentation and a residual block on the Conditional GAN network to achieve the results. Multiple experiments were conducted using CUB and Oxford-102 dataset and the result of the proposed approach is compared against state-ofthe-art techniques such as StackGAN. The experiments illustrate that the proposed method generates a high-resolution image conditioned on text descriptions and yield competitive results based on Inception and Frechet Inception Score using both datasets      
### 43.Cooperative Backscatter NOMA with Imperfect SIC: Towards Energy Efficient Sum Rate Maximization in Sustainable 6G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2207.03295.pdf)
>  The combination of backscatter communication with non-orthogonal multiple access (NOMA) has the potential to support low-powered massive connections in upcoming sixth-generation (6G) wireless networks. More specifically, backscatter communication can harvest and use the existing RF signals in the atmosphere for communication, while NOMA provides communication to multiple wireless devices over the same frequency and time resources. This paper has proposed a new resource management framework for backscatter-aided cooperative NOMA communication in upcoming 6G networks. In particular, the proposed work has simultaneously optimized the base station's transmit power, relaying node, the reflection coefficient of the backscatter tag, and time allocation under imperfect successive interference cancellation to maximize the sum rate of the system. To obtain an efficient solution for the resource management framework, we have proposed a combination of the bisection method and dual theory, where the sub-gradient method is adopted to optimize the Lagrangian multipliers. Numerical results have shown that the proposed solution provides excellent performance. When the performance of the proposed technique is compared to a brute-forcing search technique that guarantees optimal solution however, is very time-consuming, it was seen that the gap in performance is actually 0\%. Hence, the proposed framework has provided performance equal to a cumbersome brute-force search technique while offering much less complexity. The works in the literature on cooperative NOMA considered equal time distribution for cooperation and direct communication. Our results showed that optimizing the time-division can increase the performance by more than 110\% for high transmission powers.      
### 44.D2HNet: Joint Denoising and Deblurring with Hierarchical Network for Robust Night Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2207.03294.pdf)
>  Night imaging with modern smartphone cameras is troublesome due to low photon count and unavoidable noise in the imaging system. Directly adjusting exposure time and ISO ratings cannot obtain sharp and noise-free images at the same time in low-light conditions. Though many methods have been proposed to enhance noisy or blurry night images, their performances on real-world night photos are still unsatisfactory due to two main reasons: 1) Limited information in a single image and 2) Domain gap between synthetic training images and real-world photos (e.g., differences in blur area and resolution). To exploit the information from successive long- and short-exposure images, we propose a learning-based pipeline to fuse them. A D2HNet framework is developed to recover a high-quality image by deblurring and enhancing a long-exposure image under the guidance of a short-exposure image. To shrink the domain gap, we leverage a two-phase DeblurNet-EnhanceNet architecture, which performs accurate blur removal on a fixed low resolution so that it is able to handle large ranges of blur in different resolution inputs. In addition, we synthesize a D2-Dataset from HD videos and experiment on it. The results on the validation set and real photos demonstrate our methods achieve better visual quality and state-of-the-art quantitative scores. The D2HNet codes, models, and D2-Dataset can be found at <a class="link-external link-https" href="https://github.com/zhaoyuzhi/D2HNet" rel="external noopener nofollow">this https URL</a>.      
### 45.Self-Supervised Learning of Music-Dance Representation through Explicit-Implicit Rhythm Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2207.03190.pdf)
>  Although audio-visual representation has been proved to be applicable in many downstream tasks, the representation of dancing videos, which is more specific and always accompanied by music with complex auditory contents, remains challenging and uninvestigated. Considering the intrinsic alignment between the cadent movement of dancer and music rhythm, we introduce MuDaR, a novel Music-Dance Representation learning framework to perform the synchronization of music and dance rhythms both in explicit and implicit ways. Specifically, we derive the dance rhythms based on visual appearance and motion cues inspired by the music rhythm analysis. Then the visual rhythms are temporally aligned with the music counterparts, which are extracted by the amplitude of sound intensity. Meanwhile, we exploit the implicit coherence of rhythms implied in audio and visual streams by contrastive learning. The model learns the joint embedding by predicting the temporal consistency between audio-visual pairs. The music-dance representation, together with the capability of detecting audio and visual rhythms, can further be applied to three downstream tasks: (a) dance classification, (b) music-dance retrieval, and (c) music-dance retargeting. Extensive experiments demonstrate that our proposed framework outperforms other self-supervised methods by a large margin.      
### 46.Roadside IRS-Aided Vehicular Communication: Efficient Channel Estimation and Low-Complexity Beamforming Design  [ :arrow_down: ](https://arxiv.org/pdf/2207.03157.pdf)
>  Intelligent reflecting surface (IRS) has emerged as a promising technique to control wireless propagation environment for enhancing the communication performance cost-effectively. However, the rapidly time-varying channel in high-mobility communication scenarios such as vehicular communication renders it challenging to obtain the instantaneous channel state information (CSI) efficiently for IRS with a large number of reflecting elements. In this paper, we propose a new roadside IRS-aided vehicular communication system to tackle this challenge. Specifically, by exploiting the symmetrical deployment of IRSs with inter-laced equal intervals on both sides of the road and the cooperation among nearby IRS controllers, we propose a new two-stage channel estimation scheme with off-line and online training, respectively, to obtain the static/time-varying CSI required by the proposed low-complexity passive beamforming scheme efficiently. The proposed IRS beamforming and online channel estimation designs leverage the existing uplink pilots in wireless networks and do not require any change of the existing transmission protocol. Moreover, they can be implemented by each of IRS controllers independently, without the need of any real-time feedback from the user's serving BS. Simulation results show that the proposed designs can efficiently achieve the high IRS passive beamforming gain and thus significantly enhance the achievable communication throughput for high-speed vehicular communications.      
### 47.Uncertainty-Aware Self-supervised Neural Network for Liver $T_{1ρ}$ Mapping with Relaxation Constraint  [ :arrow_down: ](https://arxiv.org/pdf/2207.03105.pdf)
>  $T_{1\rho}$ mapping is a promising quantitative MRI technique for the non-invasive assessment of tissue properties. Learning-based approaches can map $T_{1\rho}$ from a reduced number of $T_{1\rho}$ weighted images, but requires significant amounts of high quality training data. Moreover, existing methods do not provide the confidence level of the $T_{1\rho}$ estimation. To address these problems, we proposed a self-supervised learning neural network that learns a $T_{1\rho}$ mapping using the relaxation constraint in the learning process. Epistemic uncertainty and aleatoric uncertainty are modelled for the $T_{1\rho}$ quantification network to provide a Bayesian confidence estimation of the $T_{1\rho}$ mapping. The uncertainty estimation can also regularize the model to prevent it from learning imperfect data. We conducted experiments on $T_{1\rho}$ data collected from 52 patients with non-alcoholic fatty liver disease. The results showed that our method outperformed the existing methods for $T_{1\rho}$ quantification of the liver using as few as two $T_{1\rho}$-weighted images. Our uncertainty estimation provided a feasible way of modelling the confidence of the self-supervised learning based $T_{1\rho}$ estimation, which is consistent with the reality in liver $T_{1\rho}$ imaging.      
### 48.DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.03081.pdf)
>  In this paper, we propose a multi-objective camera ISP framework that utilizes Deep Reinforcement Learning (DRL) and camera ISP toolbox that consist of network-based and conventional ISP tools. The proposed DRL-based camera ISP framework iteratively selects a proper tool from the toolbox and applies it to the image to maximize a given vision task-specific reward function. For this purpose, we implement total 51 ISP tools that include exposure correction, color-and-tone correction, white balance, sharpening, denoising, and the others. We also propose an efficient DRL network architecture that can extract the various aspects of an image and make a rigid mapping relationship between images and a large number of actions. Our proposed DRL-based ISP framework effectively improves the image quality according to each vision task such as RAW-to-RGB image restoration, 2D object detection, and monocular depth estimation.      
### 49.Visual-Assisted Sound Source Depth Estimation in the Wild  [ :arrow_down: ](https://arxiv.org/pdf/2207.03074.pdf)
>  Depth estimation enables a wide variety of 3D applications, such as robotics, autonomous driving, and virtual reality. Despite significant work in this area, it remains open how to enable accurate, low-cost, high-resolution, and large-range depth estimation. Inspired by the flash-to-bang phenomenon (\ie hearing the thunder after seeing the lightning), this paper develops FBDepth, the first audio-visual depth estimation framework. It takes the difference between the time-of-flight (ToF) of the light and the sound to infer the sound source depth. FBDepth is the first to incorporate video and audio with both semantic features and spatial hints for range estimation. It first aligns correspondence between the video track and audio track to locate the target object and target sound in a coarse granularity. Based on the observation of moving objects' trajectories, FBDepth proposes to estimate the intersection of optical flow before and after the sound production to locate video events in time. FBDepth feeds the estimated timestamp of the video event and the audio clip for the final depth estimation. We use a mobile phone to collect 3000+ video clips with 20 different objects at up to $50m$. FBDepth decreases the Absolute Relative error (AbsRel) by 55\% compared to RGB-based methods.      
### 50.Cross-Scale Vector Quantization for Scalable Neural Speech Coding  [ :arrow_down: ](https://arxiv.org/pdf/2207.03067.pdf)
>  Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.      
### 51.Energy-Efficient Transmit Beamforming and Antenna Selection with Non-Linear PA Efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2207.03052.pdf)
>  This letter studies the energy-efficient design in a downlink multi-antenna multi-user system consisting of a multi-antenna base station (BS) and multiple single-antenna users, by considering the practical non-linear power amplifier (PA) efficiency and the on-off power consumption of radio frequency (RF) chain at each transmit antenna. Under this setup, we jointly optimize the transmit beamforming and antenna on/off selection at the BS to minimize its total power consumption while ensuring the individual signal-to-interference-plus-noise ratio (SINR) constraints at the users. However, due to the non-linear PA efficiency and the on-off RF chain power consumption, the formulated SINR-constrained power minimization problem is highly non-convex and difficult to solve. To tackle this issue, we propose an efficient algorithm to obtain a high-quality solution based on the technique of sequential convex approximation (SCA). We provide numerical results to validate the performance of our proposed design. It is shown that at the optimized solution, the BS tends to activate fewer antennas and use higher power transmission at each antenna to exploit the non-linear PA efficiency.      
### 52.Single-image Defocus Deblurring by Integration of Defocus Map Prediction Tracing the Inverse Problem Computation  [ :arrow_down: ](https://arxiv.org/pdf/2207.03047.pdf)
>  In this paper, we consider the problem in defocus image deblurring. Previous classical methods follow two-steps approaches, i.e., first defocus map estimation and then the non-blind deblurring. In the era of deep learning, some researchers have tried to address these two problems by CNN. However, the simple concatenation of defocus map, which represents the blur level, leads to suboptimal performance. Considering the spatial variant property of the defocus blur and the blur level indicated in the defocus map, we employ the defocus map as conditional guidance to adjust the features from the input blurring images instead of simple concatenation. Then we propose a simple but effective network with spatial modulation based on the defocus map. To achieve this, we design a network consisting of three sub-networks, including the defocus map estimation network, a condition network that encodes the defocus map into condition features, and the defocus deblurring network that performs spatially dynamic modulation based on the condition features. Moreover, the spatially dynamic modulation is based on an affine transform function to adjust the features from the input blurry images. Experimental results show that our method can achieve better quantitative and qualitative evaluation performance than the existing state-of-the-art methods on the commonly used public test datasets.      
### 53.Self-Supervised RF Signal Representation Learning for NextG Signal Classification with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2207.03046.pdf)
>  Deep learning (DL) finds rich applications in the wireless domain to improve spectrum awareness. Typically, the DL models are either randomly initialized following a statistical distribution or pretrained on tasks from other data domains such as computer vision (in the form of transfer learning) without accounting for the unique characteristics of wireless signals. Self-supervised learning enables the learning of useful representations from Radio Frequency (RF) signals themselves even when only limited training data samples with labels are available. We present the first self-supervised RF signal representation learning model and apply it to the automatic modulation recognition (AMR) task by specifically formulating a set of transformations to capture the wireless signal characteristics. We show that the sample efficiency (the number of labeled samples required to achieve a certain accuracy performance) of AMR can be significantly increased (almost an order of magnitude) by learning signal representations with self-supervised learning. This translates to substantial time and cost savings. Furthermore, self-supervised learning increases the model accuracy compared to the state-of-the-art DL methods and maintains high accuracy even when a small set of training data samples is used.      
### 54.Machine Learning based Interference Whitening in 5G NR MIMO Receiver  [ :arrow_down: ](https://arxiv.org/pdf/2207.03010.pdf)
>  We address the problem of computing the interference-plus-noise covariance matrix from a sparsely located demodulation reference signal (DMRS) for spatial domain interference whitening (IW). The IW procedure is critical at the user equipment (UE) to mitigate the co-channel interference in 5G new radio (NR) systems. A supervised learning based algorithm is proposed to compute the covariance matrix with goals of minimizing both the block-error rate (BLER) and the whitening complexity. A single neural network is trained to select an IW option for covariance computation in various interference scenarios consisting of different interference occupancy, signal-to-interference ratio, signal-to-noise ratio, modulation order, coding rate, etc. In interference-dominant scenarios, the proposed algorithm computes the covariance matrix using DMRS in one resource block (RB) due to the frequency selectivity of the interference channel. On the other hand, in noise-dominant scenarios, the covariance matrix is computed from DMRS in entire signal bandwidth. Further, the proposed algorithm approximates the covariance matrix into a diagonal matrix when the spatial correlation of interference-plus-noise is low. This approximation reduces the complexity of whitening from $\mathcal{O}(N^3)$ to $\mathcal{O}(N)$ where $N$ is the number of receiver antennas. Results show that the selection algorithm can minimize the BLER under both trained as well as untrained interference scenarios.      
### 55.Orthogonal Matrix Retrieval with Spatial Consensus for 3D Unknown-View Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2207.02985.pdf)
>  Unknown-view tomography (UVT) reconstructs a 3D density map from its 2D projections at unknown, random orientations. A line of work starting with Kam (1980) employs the method of moments (MoM) with rotation-invariant Fourier features to solve UVT in the frequency domain, assuming that the orientations are uniformly distributed. This line of work includes the recent orthogonal matrix retrieval (OMR) approaches based on matrix factorization, which, while elegant, either require side information about the density that is not available, or fail to be sufficiently robust. In order for OMR to break free from those restrictions, we propose to jointly recover the density map and the orthogonal matrices by requiring that they be mutually consistent. We regularize the resulting non-convex optimization problem by a denoised reference projection and a nonnegativity constraint. This is enabled by the new closed-form expressions for spatial autocorrelation features. Further, we design an easy-to-compute initial density map which effectively mitigates the non-convexity of the reconstruction problem. Experimental results show that the proposed OMR with spatial consensus is more robust and performs significantly better than the previous state-of-the-art OMR approach in the typical low-SNR scenario of 3D UVT.      
### 56.MoRPI: Mobile Robot Pure Inertial Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2207.02982.pdf)
>  Mobile robots are used in industrial, leisure, and military applications. In some situations, a robot navigation solution relies only on inertial sensors and as a consequence, the navigation solution drifts in time. In this paper, we propose the MoRPI framework, a mobile robot pure inertial approach. Instead of travelling in a straight line trajectory, the robot moves in a periodic motion trajectory to enable peak-to-peak estimation. In this manner, instead of performing three integrations to calculate the robot position in a classical inertial solution, an empirical formula is used to estimate the travelled distance. Two types of MoRPI approaches are suggested, where one is based on both accelerometer and gyroscope readings while the other is only on gyroscopes. Closed form analytical solutions are derived to show that MoRPI produces lower position error compared to the classical pure inertial solution. In addition, to evaluate the proposed approach, field experiments were made with a mobile robot equipped with two types of inertial sensors. In total, 143 trajectories with a time duration of 75 minutes were collected and evaluated. The results show the benefits of using our approach. To facilitate further development of the proposed approach, both dataset and code are publicly available at <a class="link-external link-https" href="https://github.com/ansfl/MoRPI" rel="external noopener nofollow">this https URL</a>.      
### 57.Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding  [ :arrow_down: ](https://arxiv.org/pdf/2207.02971.pdf)
>  Conformer has proven to be effective in many speech processing tasks. It combines the benefits of extracting local dependencies using convolutions and global dependencies using self-attention. Inspired by this, we propose a more flexible, interpretable and customizable encoder alternative, Branchformer, with parallel branches for modeling various ranged dependencies in end-to-end speech processing. In each encoder layer, one branch employs self-attention or its variant to capture long-range dependencies, while the other branch utilizes an MLP module with convolutional gating (cgMLP) to extract local relationships. We conduct experiments on several speech recognition and spoken language understanding benchmarks. Results show that our model outperforms both Transformer and cgMLP. It also matches with or outperforms state-of-the-art results achieved by Conformer. Furthermore, we show various strategies to reduce computation thanks to the two-branch architecture, including the ability to have variable inference complexity in a single trained model. The weights learned for merging branches indicate how local and global dependencies are utilized in different layers, which benefits model designing.      
### 58.Variance in Classifying Affective State via Electrocardiogram and Photoplethysmography  [ :arrow_down: ](https://arxiv.org/pdf/2207.02916.pdf)
>  Advances in wearable technology have significantly increased the sensitivity and accuracy of devices for recording physiological signals. Commercial off-the-shelf wearable devices can gather large quantities of physiological data un-obtrusively. This enables momentary assessments of human physiology, which provide valuable insights into an individual's health and psychological state. Leveraging these insights provides significant benefits for human-to-computer interaction and personalised healthcare. This work contributes an analysis of variance occurring in features representative of affective states extracted from electrocardiograms and photoplethysmography; subsequently identifies the cardiac measures most descriptive of affective states from both signals and provides insights into signal and emotion-specific cardiac measures; finally baseline performance for automated affective state detection from physiological signals is established.      
### 59.Physical Interaction and Manipulation of the Environment using Aerial Robots  [ :arrow_down: ](https://arxiv.org/pdf/2207.02856.pdf)
>  The physical interaction of aerial robots with their environment has countless potential applications and is an emerging area with many open challenges. Fully-actuated multirotors have been introduced to tackle some of these challenges. They provide complete control over position and orientation and eliminate the need for attaching a multi-DoF manipulation arm to the robot. However, there are many open problems before they can be used in real-world applications. Researchers have introduced some methods for physical interaction in limited settings. Their experiments primarily use prototype-level software without an efficient path to integration with real-world applications. We describe a new cost-effective solution for integrating these robots with the existing software and hardware flight systems for real-world applications and expand it to physical interaction applications. On the other hand, the existing control approaches for fully-actuated robots assume conservative limits for the thrusts and moments available to the robot. Using conservative assumptions for these already-inefficient robots makes their interactions even less optimal and may even result in many feasible physical interaction applications becoming infeasible. This work proposes a real-time method for estimating the complete set of instantaneously available forces and moments that robots can use to optimize their physical interaction performance. Finally, many real-world applications where aerial robots can improve the existing manual solutions deal with deformable objects. However, the perception and planning for their manipulation is still challenging. This research explores how aerial physical interaction can be extended to deformable objects. It provides a detection method suitable for manipulating deformable one-dimensional objects and introduces a new perspective on planning the manipulation of these objects.      
