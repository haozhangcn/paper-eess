# ArXiv eess --Tue, 8 Sep 2020
### 1.Data-Driven Transferred Energy Management Strategy for Hybrid Electric Vehicles via Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.03289.pdf)
>  Real-time applications of energy management strategies (EMSs) in hybrid electric vehicles (HEVs) are the harshest requirements for researchers and engineers. Inspired by the excellent problem-solving capabilities of deep reinforcement learning (DRL), this paper proposes a real-time EMS via incorporating the DRL method and transfer learning (TL). The related EMSs are derived from and evaluated on the real-world collected driving cycle dataset from Transportation Secure Data Center (TSDC). The concrete DRL algorithm is proximal policy optimization (PPO) belonging to the policy gradient (PG) techniques. For specification, many source driving cycles are utilized for training the parameters of deep network based on PPO. The learned parameters are transformed into the target driving cycles under the TL framework. The EMSs related to the target driving cycles are estimated and compared in different training conditions. Simulation results indicate that the presented transfer DRL-based EMS could effectively reduce time consumption and guarantee control performance.      
### 2.Information: to Harvest, to Have and to Hold  [ :arrow_down: ](https://arxiv.org/pdf/2009.03223.pdf)
>  Signal-to-Noise Ratios (SNRs) and the Shannon-Hartley channel capacity are metrics that help define the loss of known information while transferring data through a noisy channel. These metrics cannot be used for quantifying the opposite process: the harvesting of new information. Correlation functions and correlation coefficients do play an important role in collecting new information from noisy sources. However, Bershad and Rockmore [1974] based their formulas on contradictory a priori assumptions in Real-space and in Fourier-space. Their formulations were subsequently copied literally to the practical science of electron microscopy, where those a priori assumptions now distort most quality metrics in Cryo-EM. Cryo-EM became a great success in recent years [Wiley Award 2017; Nobel prize for Chemistry 2017] and became the method of choice for revealing structures of biological complexes like ribosomes, viruses, or corona-virus spikes, vitally important during the current COVID-19 pandemic. Those early misconceptions now interfere with the objective comparison of independently obtained results. We found that the roots of these problems significantly pre-date those 1970s publications and were already inherent in the original SNR definitions. We here propose novel metrics to assess the amount of information harvested in an experiment, information which is measured in bits. These new metrics assess the total amount of information collected on an object, as well as the information density distribution within that object. The new metrics can be applied everywhere where data is collected, processed, compressed, or compared. As an example, we compare the structures of two recently published SARS-CoV-2 spike proteins. We also introduce new metrics for transducer-quality assessment in many sciences including: cryo-EM, biomedical imaging, microscopy, signal processing, photography, tomography, etc.      
### 3.Resilience and performance of the power grid with high penetration of renewable energy sources: the Balearic Islands as a case study  [ :arrow_down: ](https://arxiv.org/pdf/2009.03217.pdf)
>  We analyze the dynamics of the power grid with a high penetration of renewable energy sources using the ORNL-PSERC-Alaska (OPA) model. In particular we consider the power grid of the Balearic Islands with a high share of solar photovoltaic power as a case study. Day-to-day fluctuations of the solar generation and the use of storage are included in the model. Resilience is analyzed through the blackout distribution and performance is measured as the average fraction of the demand covered by solar power generation. We find that with the present consumption patterns and moderate storage, solar generation can replace conventional power plants without compromising reliability up to $30\%$ of the total installed capacity. We also find that using source redundancy it is possible to cover up to $80\%$ or more of the demand with solar plants, while keeping the risk similar to that with full conventional generation. However this requires oversizing the installed solar power to be at least $2.5$ larger than the average demand. The potential of wind energy is also briefly discussed      
### 4.Temporal optical neurons for serial deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.03213.pdf)
>  Deep learning is able to functionally simulate the human brain and thus, it has attracted considerable interest. Optics-assisted deep learning is a promising approach to improve the forward-propagation speed and reduce the power consumption. However, present methods are based on a parallel processing approach that is inherently ineffective in dealing with serial data signals at the core of information and communication technologies. Here, we propose and demonstrate a serial optical deep learning concept that is specifically designed to directly process high-speed temporal data. By utilizing ultra-short coherent optical pulses as the information carriers, the neurons are distributed at different time slots in a serial pattern, and interconnected to each other through group delay dispersion. A 4-layer serial optical neural network (SONN) was constructed and trained for classification of both analog and digital signals with simulated accuracy rates of over 90% with proper individuality variance rates. Furthermore, we performed a proof-of-concept experiment of a pseudo-3-layer SONN to successfully recognize the ASCII (American Standard Code for Information Interchange) codes of English letters at a data rate of 12 Gbps. This concept represents a novel one-dimensional realization of artificial neural networks, enabling an efficient application of optical deep learning methods to the analysis and processing of serial data signals, while offering a new overall perspective for the temporal signal processing.      
### 5.A New Screening Method for COVID-19 based on Ocular Feature Recognition by Machine Learning Tools  [ :arrow_down: ](https://arxiv.org/pdf/2009.03184.pdf)
>  The Coronavirus disease 2019 (COVID-19) has affected several million people. With the outbreak of the epidemic, many researchers are devoting themselves to the COVID-19 screening system. The standard practices for rapid risk screening of COVID-19 are the CT imaging or RT-PCR (real-time polymerase chain reaction). However, these methods demand professional efforts of the acquisition of CT images and saliva samples, a certain amount of waiting time, and most importantly prohibitive examination fee in some countries. Recently, some literatures have shown that the COVID-19 patients usually accompanied by ocular manifestations consistent with the conjunctivitis, including conjunctival hyperemia, chemosis, epiphora, or increased secretions. After more than four months study, we found that the confirmed cases of COVID-19 present the consistent ocular pathological symbols; and we propose a new screening method of analyzing the eye-region images, captured by common CCD and CMOS cameras, could reliably make a rapid risk screening of COVID-19 with very high accuracy. We believe a system implementing such an algorithm should assist the triage management or the clinical diagnosis. To further evaluate our algorithm and approved by the Ethics Committee of Shanghai public health clinic center of Fudan University, we conduct a study of analyzing the eye-region images of 303 patients (104 COVID-19, 131 pulmonary, and 68 ocular patients), as well as 136 healthy people. Remarkably, our results of COVID-19 patients in testing set consistently present similar ocular pathological symbols; and very high testing results have been achieved in terms of sensitivity and specificity. We hope this study can be inspiring and helpful for encouraging more researches in this topic.      
### 6.DeepOPF+: A Deep Neural Network Approach for DC Optimal Power Flow for Ensuring Feasibility  [ :arrow_down: ](https://arxiv.org/pdf/2009.03147.pdf)
>  Deep Neural Networks (DNNs) approaches for the Optimal Power Flow (OPF) problem received considerable attention recently. A key challenge of these approaches lies in ensuring the feasibility of the predicted solutions to physical system constraints. Due to the inherent approximation errors, the solutions predicted by DNNs may violate the operating constraints, e.g., the transmission line capacities, limiting their applicability in practice. To address this challenge, we develop DeepOPF+ as a DNN approach based on the so-called "preventive" framework. Specifically, we calibrate the generation and transmission line limits used in the DNN training, thereby anticipating approximation errors and ensuring that the resulting predicted solutions remain feasible. We theoretically characterize the calibration magnitude necessary for ensuring universal feasibility. Our DeepOPF+ approach improves over existing DNN-based schemes in that it ensures feasibility and achieves a consistent speed up performance in both light-load and heavy-load regimes. Detailed simulation results on a range of test instances show that the proposed DeepOPF+ generates 100% feasible solutions with minor optimality loss. Meanwhile, it achieves a computational speedup of two orders of magnitude compared to state-of-the-art solvers.      
### 7.An End-to-end Architecture of Online Multi-channel Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2009.03141.pdf)
>  Multi-speaker speech recognition has been one of the keychallenges in conversation transcription as it breaks the singleactive speaker assumption employed by most state-of-the-artspeech recognition systems. Speech separation is consideredas a remedy to this problem. Previously, we introduced a sys-tem, calledunmixing,fixed-beamformerandextraction(UFE),that was shown to be effective in addressing the speech over-lap problem in conversation transcription. With UFE, an inputmixed signal is processed by fixed beamformers, followed by aneural network post filtering. Although promising results wereobtained, the system contains multiple individually developedmodules, leading potentially sub-optimum performance. In thiswork, we introduce an end-to-end modeling version of UFE. Toenable gradient propagation all the way, an attentional selectionmodule is proposed, where an attentional weight is learnt foreach beamformer and spatial feature sampled over space. Ex-perimental results show that the proposed system achieves com-parable performance in an offline evaluation with the originalseparate processing-based pipeline, while producing remark-able improvements in an online evaluation.      
### 8.Edge Learning with Unmanned Ground Vehicle: Joint Path, Energy and Sample Size Planning  [ :arrow_down: ](https://arxiv.org/pdf/2009.03140.pdf)
>  Edge learning (EL), which uses edge computing as a platform to execute machine learning algorithms, is able to fully exploit the massive sensing data generated by Internet of Things (IoT). However, due to the limited transmit power at IoT devices, collecting the sensing data in EL systems is a challenging task. To address this challenge, this paper proposes to integrate unmanned ground vehicle (UGV) with EL. With such a scheme, the UGV could improve the communication quality by approaching various IoT devices. However, different devices may transmit different data for different machine learning jobs and a fundamental question is how to jointly plan the UGV path, the devices' energy consumption, and the number of samples for different jobs? This paper further proposes a graph-based path planning model, a network energy consumption model and a sample size planning model that characterizes F-measure as a function of the minority class sample size. With these models, the joint path, energy and sample size planning (JPESP) problem is formulated as a large-scale mixed integer nonlinear programming (MINLP) problem, which is nontrivial to solve due to the high-dimensional discontinuous variables related to UGV movement. To this end, it is proved that each IoT device should be served only once along the path, thus the problem dimension is significantly reduced. Furthermore, to handle the discontinuous variables, a tabu search (TS) based algorithm is derived, which converges in expectation to the optimal solution to the JPESP problem. Simulation results under different task scenarios show that our optimization schemes outperform the fixed EL and the full path EL schemes.      
### 9.Edge effect removal in Fourier ptychographic microscopy via perfect Fourier transformation (PFT)  [ :arrow_down: ](https://arxiv.org/pdf/2009.03138.pdf)
>  Edge effect may degrade the imaging precision and is caused by the aperiodic image extension of fast Fourier transform (FFT). In this letter, a perfect Fourier transform algorithm termed PFT was reported to remove the artifacts with comparable efficiency to FFT. Although we demonstrated the performance of PFT in Fourier ptychographic microscopy (FPM) only, it can be expanded in any occasion where the conventional FFT is used.      
### 10.Semi-Supervised Deep Learning for Multi-Tissue Segmentation from Multi-Contrast MRI  [ :arrow_down: ](https://arxiv.org/pdf/2009.03128.pdf)
>  Segmentation of thigh tissues (muscle, fat, inter-muscular adipose tissue (IMAT), bone, and bone marrow) from magnetic resonance imaging (MRI) scans is useful for clinical and research investigations in various conditions such as aging, diabetes mellitus, obesity, metabolic syndrome, and their associated comorbidities. Towards a fully automated, robust, and precise quantification of thigh tissues, herein we designed a novel semi-supervised segmentation algorithm based on deep network architectures. Built upon Tiramisu segmentation engine, our proposed deep networks use variational and specially designed targeted dropouts for faster and robust convergence, and utilize multi-contrast MRI scans as input data. In our experiments, we have used 150 scans from 50 distinct subjects from the Baltimore Longitudinal Study of Aging (BLSA). The proposed system made use of both labeled and unlabeled data with high efficacy for training, and outperformed the current state-of-the-art methods with dice scores of 97.52%, 94.61%, 80.14%, 95.93%, and 96.83% for muscle, fat, IMAT, bone, and bone marrow tissues, respectively. Our results indicate that the proposed system can be useful for clinical research studies where volumetric and distributional tissue quantification is pivotal and labeling is a significant issue. To the best of our knowledge, the proposed system is the first attempt at multi-tissue segmentation using a single end-to-end semi-supervised deep learning framework for multi-contrast thigh MRI scans.      
### 11.KoSpeech: Open-Source Toolkit for End-to-End Korean Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2009.03092.pdf)
>  We present KoSpeech, an open-source software, which is modular and extensible end-to-end Korean automatic speech recognition (ASR) toolkit based on the deep learning library PyTorch. Several automatic speech recognition open-source toolkits have been released, but all of them deal with non-Korean languages, such as English (e.g. ESPnet, Espresso). Although AI Hub opened 1,000 hours of Korean speech corpus known as KsponSpeech, there is no established preprocessing method and baseline model to compare model performances. Therefore, we propose preprocessing methods for KsponSpeech corpus and a baseline model for benchmarks. Our baseline model is based on Listen, Attend and Spell (LAS) architecture and ables to customize various training hyperparameters conveniently. By KoSpeech, we hope this could be a guideline for those who research Korean speech recognition. Our baseline model achieved 10.31% character error rate (CER) at KsponSpeech corpus only with the acoustic model. Our source code is available here.      
### 12.Designing sequence set with minimal peak side-lobe level for applications in high resolution RADAR imaging  [ :arrow_down: ](https://arxiv.org/pdf/2009.03081.pdf)
>  Constant modulus sequence set with low peak side-lobe level is a necessity for enhancing the performance of modern active sensing systems like Multiple Input Multiple Output (MIMO) RADARs. In this paper, we consider the problem of designing a constant modulus sequence set by minimizing the peak side-lobe level, which can be cast as a non-convex minimax problem, and propose a Majorization-Minimization technique based iterative monotonic algorithm. The iterative steps of our algorithm are computationally not very demanding and they can be efficiently implemented via Fast Fourier Transform (FFT) operations. We also establish the convergence of our proposed algorithm and discuss the computational and space complexities of the algorithm. Finally, through numerical simulations, we illustrate the performance of our method with the state-of-the-art methods. To highlight the potential of our approach, we evaluate the performance of the sequence set designed via our approach in the context of probing sequence set design for MIMO RADAR angle-range imaging application and show results exhibiting good performance of our method when compared with other commonly used sequence set design approaches.      
### 13.Tuning Rules for Control of Nonlinear Mechanical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.03055.pdf)
>  In this paper, we propose several rules to tune the gains of Passivity-Based Controllers for a class of nonlinear mechanical systems. Such tuning rules aim to prescribe a desired behavior to the closed-loop system, where we are particularly interested in attenuating oscillations and improving the rise time of the transient response. Hence, the resulting controllers stabilize the plant and simultaneously address the performance in terms of oscillations, damping ratio, and rise time of the transient response of the closed-loop system. Moreover, the closed-loop system analysis provides a clear insight into how the kinetic energy, the potential energy, and the damping of the mechanical system are related to its transient response, endowing in this way the tuning rules with a physical interpretation. Additionally, we corroborate the analytical results through the practical implementation of a controller that stabilizes a two-degrees-of-freedom (2DoF) planar manipulator, where the control gains are tuned following the proposed rules.      
### 14.On global convergence of area-constrained formations of hierarchical multi-agent systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.03048.pdf)
>  This paper is concerned with a formation shaping problem for point agents in a two-dimensional space, where control avoids the possibility of reflection ambiguities. One solution for this type of problems was given first for three or four agents by considering a potential function which consists of both the distance error and the signed area terms. Then, by exploiting a hierarchical control strategy with such potential functions, the method was extended to any number of agents recently. However, a specific gain on the signed area term must be employed there, and it does not guarantee the global convergence. To overcome this issue, this paper provides a necessary and sufficient condition for the global convergence, subject to the constraint that the desired formation consists of isosceles triangles only. This clarifies the admissible range of the gain on the signed area for this case. In addition, as for formations consisting of arbitrary triangles, it is shown when high gain on the signed area is admissible for global convergence.      
### 15.Handling actuator magnitude and rate saturation in uncertain over-actuated systems: A modified projection algorithm approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.03024.pdf)
>  This paper proposes a projection algorithm which can be employed to bound actuator signals, in terms of both magnitude and rate, for uncertain systems with redundant actuators. The investigated closed loop control system is assumed to contain an adaptive control allocator to distribute the total control input among actuators. Although conventional control allocation methods can handle actuator rate and magnitude constraints, they cannot consider actuator uncertainty. On the other hand, adaptive allocators manage uncertainty and actuator magnitude limits. The proposed projection algorithm enables adaptive control allocators to handle both magnitude and rate saturation constraints. A mathematically rigorous analysis is provided to show that with the help of the proposed projection algorithm, the performance of the adaptive control allocator can be guaranteed, in terms of error bounds. Simulation results are presented, where the Aero-Data Model In Research Environment (ADMIRE) is used as an over-actuated system, to demonstrate the effectiveness of the proposed method.      
### 16.Towards learned optimal q-space sampling in diffusion MRI  [ :arrow_down: ](https://arxiv.org/pdf/2009.03008.pdf)
>  Fiber tractography is an important tool of computational neuroscience that enables reconstructing the spatial connectivity and organization of white matter of the brain. Fiber tractography takes advantage of diffusion Magnetic Resonance Imaging (dMRI) which allows measuring the apparent diffusivity of cerebral water along different spatial directions. Unfortunately, collecting such data comes at the price of reduced spatial resolution and substantially elevated acquisition times, which limits the clinical applicability of dMRI. This problem has been thus far addressed using two principal strategies. Most of the efforts have been extended towards improving the quality of signal estimation for any, yet fixed sampling scheme (defined through the choice of diffusion-encoding gradients). On the other hand, optimization over the sampling scheme has also proven to be effective. Inspired by the previous results, the present work consolidates the above strategies into a unified estimation framework, in which the optimization is carried out with respect to both estimation model and sampling design {\it concurrently}. The proposed solution offers substantial improvements in the quality of signal estimation as well as the accuracy of ensuing analysis by means of fiber tractography. While proving the optimality of the learned estimation models would probably need more extensive evaluation, we nevertheless claim that the learned sampling schemes can be of immediate use, offering a way to improve the dMRI analysis without the necessity of deploying the neural network used for their estimation. We present a comprehensive comparative analysis based on the Human Connectome Project data. Code and learned sampling designs aviliable at <a class="link-external link-https" href="https://github.com/tomer196/Learned_dMRI" rel="external noopener nofollow">this https URL</a>.      
### 17.Active Disturbance Rejection Control with Sensor Noise Suppressing Observer for DC-DC Buck Power Converters  [ :arrow_down: ](https://arxiv.org/pdf/2009.02948.pdf)
>  The class of active disturbance rejection control (ADRC) algorithms has been shown in the literature to be an interesting alternative to standard control methods in power electronics devices. However, their robustness and stability are often limited in practice by the high-frequency measurement noise, common in industrial applications. In this article, this problem is addressed by replacing the conventional high-gain extended state observer (ESO) with a new cascade observer structure. The presented experimental results, performed on a DC-DC buck power converter system, show that the new cascade ESO design has increased estimation/control performance compared to the standard approach, while effectively suppressing the detrimental effect of sensor noise over-amplification.      
### 18.Deep Learning-Based Single-Ended Objective Quality Measures for Time-Scale Modified Audio  [ :arrow_down: ](https://arxiv.org/pdf/2009.02940.pdf)
>  Objective evaluation of audio processed with Time-Scale Modification (TSM) is seeing a resurgence of interest. Recently, a labelled time-scaled audio dataset was used to train an objective measure for TSM evaluation. This DE measure was an extension of Perceptual Evaluation of Audio Quality, and required reference and test signals. In this paper, two single-ended objective quality measures for time-scaled audio are proposed that do not require a reference signal. Data driven features are created by either a convolutional neural network (CNN) or a bidirectional gated recurrent unit (BGRU) network and fed to a fully-connected network to predict subjective mean opinion scores. The proposed CNN and BGRU measures achieve an average Root Mean Squared Error of 0.608 and 0.576, and a mean Pearson correlation of 0.771 and 0.794, respectively. The proposed measures are used to evaluate TSM algorithms, and comparisons are provided for 16 TSM implementations. The objective measure is available at <a class="link-external link-https" href="https://www.github.com/zygurt/TSM" rel="external noopener nofollow">this https URL</a>.      
### 19.Brain Tumor Survival Prediction using Radiomics Features  [ :arrow_down: ](https://arxiv.org/pdf/2009.02903.pdf)
>  Surgery planning in patients diagnosed with brain tumor is dependent on their survival prognosis. A poor prognosis might demand for a more aggressive treatment and therapy plan, while a favorable prognosis might enable a less risky surgery plan. Thus, accurate survival prognosis is an important step in treatment planning. Recently, deep learning approaches have been used extensively for brain tumor segmentation followed by the use of deep features for prognosis. However, radiomics-based studies have shown more promise using engineered/hand-crafted features. In this paper, we propose a three-step approach for multi-class survival prognosis. In the first stage, we extract image slices corresponding to tumor regions from multiple magnetic resonance image modalities. We then extract radiomic features from these 2D slices. Finally, we train machine learning classifiers to perform the classification. We evaluate our proposed approach on the publicly available BraTS 2019 data and achieve an accuracy of 76.5% and precision of 74.3% using the random forest classifier, which to the best of our knowledge are the highest reported results yet. Further, we identify the most important features that contribute in improving the prediction.      
### 20.A Game Theoretic Analysis of LQG Control under Adversarial Attack  [ :arrow_down: ](https://arxiv.org/pdf/2009.02877.pdf)
>  Motivated by recent works addressing adversarial attacks on deep reinforcement learning, a deception attack on linear quadratic Gaussian control is studied in this paper. In the considered attack model, the adversary can manipulate the observation of the agent subject to a mutual information constraint. The adversarial problem is formulated as a novel dynamic cheap talk game to capture the strategic interaction between the adversary and the agent, the asymmetry of information availability, and the system dynamics. Necessary and sufficient conditions are provided for subgame perfect equilibria to exist in pure strategies and in behavioral strategies; and characteristics of the equilibria and the resulting control rewards are given. The results show that pure strategy equilibria are informative, while only babbling equilibria exist in behavioral strategies. Numerical results are shown to illustrate the impact of strategic adversarial interaction.      
### 21.Kernel Center Adaptation in the Reproducing Kernel Hilbert Space Embedding Method  [ :arrow_down: ](https://arxiv.org/pdf/2009.02867.pdf)
>  The performance of adaptive estimators that employ embedding in reproducing kernel Hilbert spaces (RKHS) depends on the choice of the location of basis kernel centers. Parameter convergence and error approximation rates depend on where and how the kernel centers are distributed in the state-space. In this paper, we develop the theory that relates parameter convergence and approximation rates to the position of kernel centers. We develop criteria for choosing kernel centers in a specific class of systems - ones in which the state trajectory regularly visits the neighborhood of the positive limit set. Two algorithms, based on centroidal Voronoi tessellations and Kohonen self-organizing maps, are derived to choose kernel centers in the RKHS embedding method. Finally, we implement these methods on two practical examples and test their effectiveness.      
### 22.Sufficient Conditions for Parameter Convergence over Embedded Manifolds using Kernel Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2009.02866.pdf)
>  The persistence of excitation (PE) condition is sufficient to ensure parameter convergence in adaptive estimation problems. Recent results on adaptive estimation in reproducing kernel Hilbert spaces (RKHS) introduce PE conditions for RKHS. This paper presents sufficient conditions for PE for the particular class of uniformly embedded reproducing kernel Hilbert spaces (RKHS) defined over smooth Riemannian manifolds. This paper also studies the implications of the sufficient condition in the case when the RKHS is finite or infinite-dimensional. When the RKHS is finite-dimensional, the sufficient condition implies parameter convergence as in the conventional analysis. On the other hand, when the RKHS is infinite-dimensional, the same condition implies that the function estimate error is ultimately bounded by a constant that depends on the approximation error in the infinite-dimensional RKHS. We illustrate the effectiveness of the sufficient condition in a practical example.      
### 23.A Comparison of Virtual Analog Modelling Techniques for Desktop and Embedded Implementations  [ :arrow_down: ](https://arxiv.org/pdf/2009.02833.pdf)
>  We develop a virtual analog model of the Klon Centaur guitar pedal circuit, comparing various circuit modelling techniques. The techniques analyzed include traditional modelling techniques such as nodal analysis and Wave Digital Filters, as well as a machine learning technique using recurrent neural networks. We examine these techniques in the contexts of two use cases: an audio plug-in designed to be run on a consumer-grade desktop computer, and a guitar pedal-style effect running on an embedded device. Finally, we discuss the advantages and disdvantages of each technique for modelling different circuits, and targeting different platforms.      
### 24.Non causal deep learning based dereverberation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02832.pdf)
>  In this paper we demonstrate the effectiveness of non-causal context for mitigating the effects of reverberation in deep-learning-based automatic speech recognition (ASR) systems. First, the value of non-causal context using a non-causal FIR filter is shown by comparing the contributions of previous vs. future information. Second, MLP- and LSTM-based dereverberation networks were trained to confirm the effects of causal and non-causal context when used in ASR systems trained with clean speech. The non-causal deep-learning-based dereverberation provides a 45% relative reduction in word error rate (WER) compared to the popular weighted prediction error (WPE) method in experiments with clean training in the REVERB challenge. Finally, an expanded multicondition training procedure used in combination with a semi-enhanced test utterance generation based on combinations of reverberated and dereverberated signals is proposed to reduce any artifacts or distortion that may be introduced by the non-causal dereverberation methods. The combination of both approaches provided average relative reductions in WER equal to 10.9% and 6.0% when compared to the baseline system obtained with the most recent REVERB challenge recipe without and with WPE, respectively.      
### 25.Libri-Adapt: A New Speech Dataset for Unsupervised Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02814.pdf)
>  This paper introduces a new dataset, Libri-Adapt, to support unsupervised domain adaptation research on speech recognition models. Built on top of the LibriSpeech corpus, Libri-Adapt contains English speech recorded on mobile and embedded-scale microphones, and spans 72 different domains that are representative of the challenging practical scenarios encountered by ASR models. More specifically, Libri-Adapt facilitates the study of domain shifts in ASR models caused by a) different acoustic environments, b) variations in speaker accents, c) heterogeneity in the hardware and platform software of the microphones, and d) a combination of the aforementioned three shifts. We also provide a number of baseline results quantifying the impact of these domain shifts on the Mozilla DeepSpeech2 ASR model.      
### 26.The 2ST-UNet for Pneumothorax Segmentation in Chest X-Rays using ResNet34 as a Backbone for U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2009.02805.pdf)
>  Pneumothorax, also called a collapsed lung, refers to the presence of the air in the pleural space between the lung and chest wall. It can be small (no need for treatment), or large and causes death if it is not identified and treated on time. It is easily seen and identified by experts using a chest X-ray. Although this method is mostly error-free, it is time-consuming and needs expert radiologists. Recently, Computer Vision has been providing great assistance in detecting and segmenting pneumothorax. In this paper, we propose a 2-Stage Training system (2ST-UNet) to segment images with pneumothorax. This system is built based on U-Net with Residual Networks (ResNet-34) backbone that is pre-trained on the ImageNet dataset. We start with training the network at a lower resolution before we load the trained model weights to retrain the network with a higher resolution. Moreover, we utilize different techniques including Stochastic Weight Averaging (SWA), data augmentation, and Test-Time Augmentation (TTA). We use the chest X-ray dataset that is provided by the 2019 SIIM-ACR Pneumothorax Segmentation Challenge, which contains 12,047 training images and 3,205 testing images. Our experiments show that 2-Stage Training leads to better and faster network convergence. Our method achieves 0.8356 mean Dice Similarity Coefficient (DSC) placing it among the top 9% of models with a rank of 124 out of 1,475.      
### 27.CSI-Based Multi-Antenna and Multi-Point Indoor Positioning Using Probability Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2009.02798.pdf)
>  Channel state information (CSI)-based fingerprinting via neural networks (NNs) is a promising approach to enable accurate indoor and outdoor positioning of user equipments (UEs), even under challenging propagation conditions. In this paper, we propose a CSI-based positioning pipeline for wireless LAN MIMO-OFDM systems operating indoors, which relies on NNs that extract a probability map indicating the likelihood of a UE being at a given grid point. We propose methods to fuse these probability maps at a centralized processor, which enables improved positioning accuracy if CSI is acquired at different access points (APs) and extracted from different transmit antennas. To improve positioning accuracy, we propose the design of CSI features that are robust to hardware and system impairments arising in real-world MIMO-OFDM transceivers. We provide experimental results with real-world indoor measurements under line-of-sight (LoS) and non-LoS propagation conditions, and for multi-antenna and multi-AP measurements. Our results demonstrate that probability fusion significantly improves positioning accuracy without requiring exact synchronization between APs and that centimeter-level median distance error is achievable.      
### 28.Perfusion Imaging: A Data Assimilation Approach  [ :arrow_down: ](https://arxiv.org/pdf/2009.02796.pdf)
>  Perfusion imaging (PI) is clinically used to assess strokes and brain tumors. Commonly used PI approaches based on magnetic resonance imaging (MRI) or computed tomography (CT) measure the effect of a contrast agent moving through blood vessels and into tissue. Contrast-agent free approaches, for example, based on intravoxel incoherent motion, also exist, but are so far not routinely used clinically. These methods rely on estimating on the arterial input function (AIF) to approximately model tissue perfusion, neglecting spatial dependencies, and reliably estimating the AIF is also non-trivial, leading to difficulties with standardizing perfusion measures. In this work we therefore propose a data-assimilation approach (PIANO) which estimates the velocity and diffusion fields of an advection-diffusion model that best explains the contrast dynamics. PIANO accounts for spatial dependencies and neither requires estimating the AIF nor relies on a particular contrast agent bolus shape. Specifically, we propose a convenient parameterization of the estimation problem, a numerical estimation approach, and extensively evaluate PIANO. We demonstrate that PIANO can successfully resolve velocity and diffusion field ambiguities and results in sensitive measures for the assessment of stroke, comparing favorably to conventional measures of perfusion.      
### 29.Overview and Evaluation of Sound Event Localization and Detection in DCASE 2019  [ :arrow_down: ](https://arxiv.org/pdf/2009.02792.pdf)
>  Sound event localization and detection is a novel area of research that emerged from the combined interest of analyzing the acoustic scene in terms of the spatial and temporal activity of sounds of interest. This paper presents an overview of the first international evaluation on sound event localization and detection, organized as a task of DCASE 2019 Challenge. A large-scale realistic dataset of spatialized sound events was generated for the challenge, to be used for training of learning-based approaches, and for evaluation of the submissions in an unlabeled subset. %Additionally, a competent baseline was provided to the participants. The overview presents in detail how the systems were evaluated and ranked and the characteristics of the best-performing systems. Common strategies in terms of input features, model architectures, training approaches, exploitation of prior knowledge, and data augmentation are discussed. Since ranking in the challenge was based on individually evaluating localization and event classification performance, part of the overview focuses on presenting metrics for the joint measurement of the two, together with a re-evaluation of submissions using these new metrics. The analysis reveals submissions with balanced performance on classifying sounds correctly close to their original location, and systems being strong on one or both of the two tasks, but not jointly.      
### 30.Edge-variational Graph Convolutional Networks for Uncertainty-aware Disease Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2009.02759.pdf)
>  There is a rising need for computational models that can complementarily leverage data of different modalities while investigating associations between subjects for population-based disease analysis. Despite the success of convolutional neural networks in representation learning for imaging data, it is still a very challenging task. In this paper, we propose a generalizable framework that can automatically integrate imaging data with non-imaging data in populations for uncertainty-aware disease prediction. At its core is a learnable adaptive population graph with variational edges, which we mathematically prove that it is optimizable in conjunction with graph convolutional neural networks. To estimate the predictive uncertainty related to the graph topology, we propose the novel concept of Monte-Carlo edge dropout. Experimental results on four databases show that our method can consistently and significantly improve the diagnostic accuracy for Autism spectrum disorder, Alzheimer's disease, and ocular diseases, indicating its generalizability in leveraging multimodal data for computer-aided diagnosis.      
### 31.System Modelling and Design Aspects of Next Generation High Throughput Satellites  [ :arrow_down: ](https://arxiv.org/pdf/2009.02754.pdf)
>  Future generation wireless networks are targeting the convergence of fixed, mobile and broadcasting systems with the integration of satellite and terrestrial systems towards utilizing their mutual benefits. Satellite Communications (Sat- Com) is envisioned to play a vital role to provide integrated services seamlessly over heterogeneous networks. As compared to terrestrial systems, the design of SatCom systems require a different approach due to differences in terms of wave propagation, operating frequency, antenna structures, interfering sources, limitations of onboard processing, power limitations and transceiver impairments. In this regard, this letter aims to identify and discuss important modeling and design aspects of the next generation High Throughput Satellite (HTS) systems. First, communication models of HTSs including the ones for multibeam and multicarrier satellites, multiple antenna techniques, and for SatCom payloads and antennas are highlighted and discussed. Subsequently, various design aspects of SatCom transceivers including impairments related to the transceiver, payload and channel, and traffic-based coverage adaptation are presented. Finally, some open topics for the design of next generation HTSs are identified and discussed.      
### 32.Simultaneous Energy Harvesting and Gait Recognition using Piezoelectric Energy Harvester  [ :arrow_down: ](https://arxiv.org/pdf/2009.02752.pdf)
>  Piezoelectric energy harvester, which generates electricity from stress or vibrations, is gaining increasing attention as a viable solution to extend battery life in wearables. Recent research further reveals that, besides generating energy, PEH can also serve as a passive sensor to detect human gait power-efficiently because its stress or vibration patterns are significantly influenced by the gait. However, as PEHs are not designed for precise measurement of motion, achievable gait recognition accuracy remains low with conventional classification algorithms. The accuracy deteriorates further when the generated electricity is stored simultaneously. To classify gait reliably while simultaneously storing generated energy, we make two distinct contributions. First, we propose a preprocessing algorithm to filter out the effect of energy storage on PEH electricity signal. Second, we propose a long short-term memory (LSTM) network-based classifier to accurately capture temporal information in gait-induced electricity generation. We prototype the proposed gait recognition architecture in the form factor of an insole and evaluate its gait recognition as well as energy harvesting performance with 20 subjects. Our results show that the proposed architecture detects human gait with 12% higher recall and harvests up to 127% more energy while consuming 38% less power compared to the state-of-the-art.      
### 33.A Convolutional Neural Network-Based Low Complexity Filter  [ :arrow_down: ](https://arxiv.org/pdf/2009.02733.pdf)
>  Convolutional Neural Network (CNN)-based filters have achieved significant performance in video artifacts reduction. However, the high complexity of existing methods makes it difficult to be applied in real usage. In this paper, a CNN-based low complexity filter is proposed. We utilize depth separable convolution (DSC) merged with the batch normalization (BN) as the backbone of our proposed CNN-based network. Besides, a weight initialization method is proposed to enhance the training performance. To solve the well known over smoothing problem for the inter frames, a frame-level residual mapping (RM) is presented. We analyze some of the mainstream methods like frame-level and block-level based filters quantitatively and build our CNN-based filter with frame-level control to avoid the extra complexity and artificial boundaries caused by block-level control. In addition, a novel module called RM is designed to restore the distortion from the learned residuals. As a result, we can effectively improve the generalization ability of the learning-based filter and reach an adaptive filtering effect. Moreover, this module is flexible and can be combined with other learning-based filters. The experimental results show that our proposed method achieves significant BD-rate reduction than H.265/HEVC. It achieves about 1.2% BD-rate reduction and 79.1% decrease in FLOPs than VR-CNN. Finally, the measurement on H.266/VVC and ablation studies are also conducted to ensure the effectiveness of the proposed method.      
### 34.Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2009.02725.pdf)
>  This paper proposes an any-to-many location-relative, sequence-to-sequence (seq2seq) based, non-parallel voice conversion approach. In this approach, we combine a bottle-neck feature extractor (BNE) with a seq2seq based synthesis module. During the training stage, an encoder-decoder based hybrid connectionist-temporal-classification-attention (CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck layer. A BNE is obtained from the phoneme recognizer and is utilized to extract speaker-independent, dense and rich linguistic representations from spectral features. Then a multi-speaker location-relative attention based seq2seq synthesis model is trained to reconstruct spectral features from the bottle-neck features, conditioning on speaker representations for speaker identity control in the generated speech. To mitigate the difficulties of using seq2seq based models to align long sequences, we down-sample the input spectral feature along the temporal dimension and equip the synthesis model with a discretized mixture of logistic (MoL) attention mechanism. Since the phoneme recognizer is trained with large speech recognition data corpus, the proposed approach can conduct any-to-many voice conversion. Objective and subjective evaluations shows that the proposed any-to-many approach has superior voice conversion performance in terms of both naturalness and speaker similarity. Ablation studies are conducted to confirm the effectiveness of feature selection and model design strategies in the proposed approach. The proposed VC approach can readily be extended to support any-to-any VC (also known as one/few-shot VC), and achieve high performance according to objective and subjective evaluations.      
### 35.New Results on Delay Robustness of Consensus Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2009.02714.pdf)
>  Consensus of autonomous agents is a benchmark problem in cooperative control. In this paper, we consider standard continuous-time averaging consensus policies (or Laplacian flows) over time-varying graphs and focus on robustness of consensus against communication delays. Such a robustness has been proved under the assumption of uniform quasi-strong connectivity of the graph. It is known, however, that the uniform connectivity is not necessary for consensus. For instance, in the case of undirected graph and undelayed communication consensus requires a much weaker condition of integral connectivity. In this paper, we show that the latter results remain valid in presence of unknown but bounded communication delays, furthermore, the condition of undirected graph can be substantially relaxed and replaced by the conditions of non-instantaneous type-symmetry. Furthermore, consensus can be proved for any feasible solution of the delay differential inequalities associated to the consensus algorithm. Such inequalities naturally arise in problems of containment control, distributed optimization and models of social dynamics.      
### 36.Deep Learning for Automatic Spleen Length Measurement in Sickle Cell Disease Patients  [ :arrow_down: ](https://arxiv.org/pdf/2009.02704.pdf)
>  Sickle Cell Disease (SCD) is one of the most common genetic diseases in the world. Splenomegaly (abnormal enlargement of the spleen) is frequent among children with SCD. If left untreated, splenomegaly can be life-threatening. The current workflow to measure spleen size includes palpation, possibly followed by manual length measurement in 2D ultrasound imaging. However, this manual measurement is dependent on operator expertise and is subject to intra- and inter-observer variability. We investigate the use of deep learning to perform automatic estimation of spleen length from ultrasound images. We investigate two types of approach, one segmentation-based and one based on direct length estimation, and compare the results against measurements made by human experts. Our best model (segmentation-based) achieved a percentage length error of 7.42%, which is approaching the level of inter-observer variability (5.47%-6.34%). To the best of our knowledge, this is the first attempt to measure spleen size in a fully automated way from ultrasound images.      
### 37.Active Gate Drive with Gate-Drain Discharge Compensation for Voltage Balancing in Series-Connected SiC MOSFETs  [ :arrow_down: ](https://arxiv.org/pdf/2009.02658.pdf)
>  Imbalanced voltage sharing during the turn-off transient is a challenge for series-connected silicon carbide (SiC) MOSFET application. This article first discusses the influence of the gate-drain discharge deviation on the voltage imbalance ratio, and its primary causes are also presented and verified by LTspice simulation. Accordingly, a novel active gate drive, which aims to compensate the discharge difference between devices connected in series, is proposed and analyzed. By only using the original output of the driving IC, the proposed gate drive is realized by implementing an auxiliary circuit on the existing commercial gate drive. Therefore, unlike other active gate drives for balancing control, no extra isolations for power/signal are needed, and the number of the devices in series is unlimited. The auxiliary circuit includes three sub-circuits as a high-bandwidth current sink for regulating switching performance, a relative low-frequency but reliable sampling and control circuit for closed-loop control, and a trigger combining the former and the latter. The operational principle and the design guideline for each part are presented in detail. Experimental results validate the performance of the proposed gate drive and its voltage balancing control algorithm.      
### 38.A Novel Event-based Non-intrusive Load Monitoring Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2009.02656.pdf)
>  Non-intrusive load monitoring (NILM), aims to infer the power profiles of appliances from the aggregated power signal via purely analytical methods. Existing NILM methods are susceptible to various issues such as the noise and transient spikes of the power signal, overshoots at the mode transition times, close consumption values by different appliances, and unavailability of a large training dataset. This paper proposes a novel event-based NILM classification algorithm mitigating these issues. The proposed algorithm (i) filters power consumption signals and accurately detects all events, (ii) extracts specific features of appliances, such as operation modes and their respective power consumption intervals, from their power consumption signals in the training dataset, and (iii) labels with high accuracy each detected event of the aggregated signal with an appliance mode transition. The algorithm is validated using REDD with the results showing its effectiveness to accurately disaggregate low frequency measured data by existing smart meters.      
### 39.FusionNet: Enhanced Beam Prediction for mmWave Communications Using Sub-6GHz Channel and A Few Pilots  [ :arrow_down: ](https://arxiv.org/pdf/2009.02655.pdf)
>  In this paper, we propose a new downlink beamforming strategy for mmWave communications using uplink sub-6GHz channel information and a very few mmWave pilots. Specifically, we design a novel dual-input neural network, called FusionNet, to extract and exploit the features from sub-6GHz channel and a few mmWave pilots to accurately predict mmWave beam. To further improve the beamforming performance and avoid over-fitting, we develop two data pre-processing approaches utilizing channel sparsity and data augmentation. The simulation results demonstrate superior performance and robustness of the proposed strategy compared to the existing one that purely relies on the sub-6GHz information, especially in the low signal-to-noise ratio (SNR) regions.      
### 40.Adaptive and Fast Combined Waveform-Beamforming Design for mmWave Automotive Joint Communication-Radar  [ :arrow_down: ](https://arxiv.org/pdf/2009.02633.pdf)
>  Millimeter-wave (mmWave) joint communication-radar (JCR) will enable high data rate communication and high-resolution radar sensing for applications such as autonomous driving. Prior JCR systems that are based on the mmWave communications hardware, however, suffer from a limited angular field-of-view and low estimation accuracy for radars due to the employed directional communication beam. In this paper, we propose an adaptive and fast combined waveform-beamforming design for the mmWave automotive JCR with a phased-array architecture that permits a trade-off between communication and radar performances. To rapidly estimate the mmWave automotive radar channel in the Doppler-angle domain with a wide field-of-view, our JCR design employs a few circulant shifts of the transmit beamformer and apply two-dimensional partial Fourier compressed sensing technique. We optimize these circulant shifts to achieve minimum coherence in compressed sensing. We evaluate the JCR performance trade-offs using a normalized mean square error (MSE) metric for radar estimation and a distortion MSE metric for data communication, which is analogous to the distortion metric in the rate distortion theory. Additionally, we develop a MSE-based weighted average optimization problem for the adaptive JCR combined waveform-beamforming design. Numerical results demonstrate that our proposed JCR design enables the estimation of short- and medium-range radar channels in the Doppler-angle domain with a low normalized MSE, at the expense of a small degradation in the communication distortion MSE.      
### 41.Preserving Privacy of the Influence Structure in Friedkin-Johnsen Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02627.pdf)
>  The nature of information sharing in common distributed consensus algorithms permits network eavesdroppers to expose sensitive system information. An important parameter within distributed systems, often neglected under the scope of privacy preservation, is the influence structure - the weighting each agent places on the sources of their opinion pool. This paper proposes a local (i.e. computed individually by each agent), time varying mask to prevent the discovery of the influence structure by an external observer with access to the entire information flow, network knowledge and mask formulation. This result is produced through the auxiliary demonstration of the preserved stability of a Friedkin-Johnsen system under a set of generalised conditions. The mask is developed under these constraints and involves perturbing the influence structure by decaying pseudonoise. This paper provides the information matrix of the best influence structure estimate by an eavesdropper lacking a priori knowledge and uses stochastic simulations to analyse the performance of the mask against ranging system hyperparameters.      
### 42.Data-Driven Power Electronic Converter Modeling for Low Inertia Power System Dynamic Studies  [ :arrow_down: ](https://arxiv.org/pdf/2009.02621.pdf)
>  A significant amount of converter-based generation is being integrated into the bulk electric power grid to fulfill the future electric demand through renewable energy sources, such as wind and photovoltaic. The dynamics of converter systems in the overall stability of the power system can no longer be neglected as in the past. Numerous efforts have been made in the literature to derive detailed dynamic models, but using detailed models becomes complicated and computationally prohibitive in large system level studies. In this paper, we use a data-driven, black-box approach to model the dynamics of a power electronic converter. System identification tools are used to identify the dynamic models, while a power amplifier controlled by a real-time digital simulator is used to perturb and control the converter. A set of linear dynamic models for the converter are derived, which can be employed for system level studies of converter-dominated electric grids.      
### 43.Artefact removal in ground truth and noise model deficient sub-cellular nanoscopy images using auto-encoder deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.02617.pdf)
>  Image denoising or artefact removal using deep learning is possible in the availability of supervised training dataset acquired in real experiments or synthesized using known noise models. Neither of the conditions can be fulfilled for nanoscopy (super-resolution optical microscopy) images that are generated from microscopy videos through statistical analysis techniques. Due to several physical constraints, supervised dataset cannot be measured. Due to non-linear spatio-temporal mixing of data and valuable statistics of fluctuations from fluorescent molecules which compete with noise statistics, noise or artefact models in nanoscopy images cannot be explicitly learnt. Therefore, such problem poses unprecedented challenges to deep learning. Here, we propose a robust and versatile simulation-supervised training approach of deep learning auto-encoder architectures for the highly challenging nanoscopy images of sub-cellular structures inside biological samples. We show the proof of concept for one nanoscopy method and investigate the scope of generalizability across structures, noise models, and nanoscopy algorithms not included during simulation-supervised training. We also investigate a variety of loss functions and learning models and discuss the limitation of existing performance metrics for nanoscopy images. We generate valuable insights for this highly challenging and unsolved problem in nanoscopy, and set the foundation for application of deep learning problems in nanoscopy for life sciences.      
### 44.XL-MIMO Energy-Efficient Antenna Selection under Non-Stationary Channels  [ :arrow_down: ](https://arxiv.org/pdf/2009.02616.pdf)
>  Massive multiple-input-multiple-output (M-MIMO) is a key technology for 5G networks. Within this research area, new types of deployment are arising, such as the extremely-large regime (XL- MIMO), where the antenna array at the base station (BS) has extreme dimensions. As a consequence, spatial non-stationary properties appear as the users see only a portion of the antenna array, which is called visibility region (VR). In this challenging transmission-reception scenario, an algorithm to select the appropriate antenna-elements for processing the received signal of a given user in the uplink (UL), as well as to transmit the signal of this user during downlink (DL) is proposed. The advantage of not using all the available antenna-elements at the BS is the computational burden and circuit power consumption reduction, improving the energy efficiency (EE) substantially. Numerical results demonstrate that one can increase the EE without compromising considerably the spectral efficiency (SE). Under few active users scenario, the performance of the XL-MIMO system shows that the EE is maximized using less than 20% of the antenna-elements of the array, without compromising the SE severely.      
### 45.GroupRegNet: A Groupwise One-shot Deep Learning-based 4D Image Registration Method  [ :arrow_down: ](https://arxiv.org/pdf/2009.02613.pdf)
>  Accurate deformable 4-dimensional (4D) (3-dimensional in space and time) medical images registration is essential in a variety of medical applications. Deep learning-based methods have recently gained popularity in this area for the significant lower inference time. However, they suffer from drawbacks of non-optimal accuracy and the requirement of a large amount of training data. A new method named GroupRegNet is proposed to address both limitations. The deformation fields to warp all images in the group into a common template is obtained through one-shot learning. The use of the implicit template reduces bias and accumulated error associated with the specified reference image. The one-shot learning strategy is similar to the conventional iterative optimization method but the motion model and parameters are replaced with a convolutional neural network (CNN) and the weights of the network. GroupRegNet also features a simpler network design and a more straightforward registration process, which eliminates the need to break up the input image into patches. The proposed method was quantitatively evaluated on two public respiratory-binned 4D-CT datasets. The results suggest that GroupRegNet outperforms the latest published deep learning-based methods and is comparable to the top conventional method pTVreg. To facilitate future research, the source code is available at <a class="link-external link-https" href="https://github.com/vincentme/GroupRegNet" rel="external noopener nofollow">this https URL</a>.      
### 46.Semi-supervised Multi-modal Emotion Recognition with Cross-Modal Distribution Matching  [ :arrow_down: ](https://arxiv.org/pdf/2009.02598.pdf)
>  Automatic emotion recognition is an active research topic with wide range of applications. Due to the high manual annotation cost and inevitable label ambiguity, the development of emotion recognition dataset is limited in both scale and quality. Therefore, one of the key challenges is how to build effective models with limited data resource. Previous works have explored different approaches to tackle this challenge including data enhancement, transfer learning, and semi-supervised learning etc. However, the weakness of these existing approaches includes such as training instability, large performance loss during transfer, or marginal improvement. <br>In this work, we propose a novel semi-supervised multi-modal emotion recognition model based on cross-modality distribution matching, which leverages abundant unlabeled data to enhance the model training under the assumption that the inner emotional status is consistent at the utterance level across modalities. <br>We conduct extensive experiments to evaluate the proposed model on two benchmark datasets, IEMOCAP and MELD. The experiment results prove that the proposed semi-supervised learning model can effectively utilize unlabeled data and combine multi-modalities to boost the emotion recognition performance, which outperforms other state-of-the-art approaches under the same condition. The proposed model also achieves competitive capacity compared with existing approaches which take advantage of additional auxiliary information such as speaker and interaction context.      
### 47.A multi-view approach for Mandarin non-native mispronunciation verification  [ :arrow_down: ](https://arxiv.org/pdf/2009.02573.pdf)
>  Traditionally, the performance of non-native mispronunciation verification systems relied on effective phone-level labelling of non-native corpora. In this study, a multi-view approach is proposed to incorporate discriminative feature representations which requires less annotation for non-native mispronunciation verification of Mandarin. Here, models are jointly learned to embed acoustic sequence and multi-source information for speech attributes and bottleneck features. Bidirectional LSTM embedding models with contrastive losses are used to map acoustic sequences and multi-source information into fixed-dimensional embeddings. The distance between acoustic embeddings is taken as the similarity between phones. Accordingly, examples of mispronounced phones are expected to have a small similarity score with their canonical pronunciations. The approach shows improvement over GOP-based approach by +11.23% and single-view approach by +1.47% in diagnostic accuracy for a mispronunciation verification task.      
### 48.Stochastic Channel Models for Massive and XL-MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02570.pdf)
>  In this paper, stochastic channel models for massive MIMO (M-MIMO) and extreme large MIMO (XL- MIMO) system applications are described, evaluated and systematically compared. This work aims to cover new aspects of massive MIMO stochastic channel models in a comprehensive and systematic way. For that, we compare different models, presenting graphically and intuitively the behavior of each model. Each massive MIMO channel model emulates the environment using different methodologies and properties. Using metrics such as capacity, SINR, singular values decomposition (SVD), and condition number, one can understand the influence of each characteristic on the modelling and how it differentiates from other models. Moreover, in new XL-MIMO scenarios, where the near-field and visible region (VR) effects arise, our finding demonstrate that for the two assumed schemes of clusters distribution, the clusters location influences the performance of the conjugate beamforming and zero-forcing (ZF) precoding due to the correlation effect, which have been analysed from the geometric massive MIMO channel models.      
### 49.Max-Fusion U-Net for Multi-Modal Pathology Segmentation with Attention and Dynamic Resampling  [ :arrow_down: ](https://arxiv.org/pdf/2009.02569.pdf)
>  Automatic segmentation of multi-sequence (multi-modal) cardiac MR (CMR) images plays a significant role in diagnosis and management for a variety of cardiac diseases. However, the performance of relevant algorithms is significantly affected by the proper fusion of the multi-modal information. Furthermore, particular diseases, such as myocardial infarction, display irregular shapes on images and occupy small regions at random locations. These facts make pathology segmentation of multi-modal CMR images a challenging task. In this paper, we present the Max-Fusion U-Net that achieves improved pathology segmentation performance given aligned multi-modal images of LGE, T2-weighted, and bSSFP modalities. Specifically, modality-specific features are extracted by dedicated encoders. Then they are fused with the pixel-wise maximum operator. Together with the corresponding encoding features, these representations are propagated to decoding layers with U-Net skip-connections. Furthermore, a spatial-attention module is applied in the last decoding layer to encourage the network to focus on those small semantically meaningful pathological regions that trigger relatively high responses by the network neurons. We also use a simple image patch extraction strategy to dynamically resample training examples with varying spacial and batch sizes. With limited GPU memory, this strategy reduces the imbalance of classes and forces the model to focus on regions around the interested pathology. It further improves segmentation accuracy and reduces the mis-classification of pathology. We evaluate our methods using the Myocardial pathology segmentation (MyoPS) combining the multi-sequence CMR dataset which involves three modalities. Extensive experiments demonstrate the effectiveness of the proposed model which outperforms the related baselines.      
### 50.Semi-supervised Pathology Segmentation with Disentangled Representations  [ :arrow_down: ](https://arxiv.org/pdf/2009.02564.pdf)
>  Automated pathology segmentation remains a valuable diagnostic tool in clinical practice. However, collecting training data is challenging. Semi-supervised approaches by combining labelled and unlabelled data can offer a solution to data scarcity. An approach to semi-supervised learning relies on reconstruction objectives (as self-supervision objectives) that learns in a joint fashion suitable representations for the task. Here, we propose Anatomy-Pathology Disentanglement Network (APD-Net), a pathology segmentation model that attempts to learn jointly for the first time: disentanglement of anatomy, modality, and pathology. The model is trained in a semi-supervised fashion with new reconstruction losses directly aiming to improve pathology segmentation with limited annotations. In addition, a joint optimization strategy is proposed to fully take advantage of the available annotations. We evaluate our methods with two private cardiac infarction segmentation datasets with LGE-MRI scans. APD-Net can perform pathology segmentation with few annotations, maintain performance with different amounts of supervision, and outperform related deep learning methods.      
### 51.A Grant-based Random Access Protocol in Extra-Large Massive MIMO System  [ :arrow_down: ](https://arxiv.org/pdf/2009.02549.pdf)
>  Extra-large massive multiple-input multiple-output (XL-MIMO) systems is a new concept, where spatial non-stationarities allow activate a high number of user equipments (UEs). This paper focuses on a grant-based random access (RA) approach in the novel XL-MIMO channel scenarios. Modifications in the classical Strongest User Collision Resolution (SUCRe) protocol have been aggregated to explore the visibility regions (VRs) overlapping in XL-MIMO. The proposed grant-based RA protocol takes advantage of this new degree of freedom for improving the number of access attempts and accepted UEs. As a result, the proposed grant-based protocol for XL-MIMO systems is capable of reducing latency in the pilot allocation step.      
### 52.Antenna Selection for Improving Energy Efficiency in XL-MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02542.pdf)
>  We consider the recently proposed extra-large scale massive multiple-input multiple-output (XL-MIMO) systems, with some hundreds of antennas serving a smaller number of users. Since the array length is of the same order as the distance to the users, the long-term fading coefficients of a given user vary with the different antennas at the base station (BS). Thus, the signal transmitted by some antennas might reach the user with much more power than that transmitted by some others. From a green perspective, it is not effective to simultaneously activate hundreds or even thousands of antennas, since the power-hungry radio frequency (RF) chains of the active antennas increase significantly the total energy consumption. Besides, a larger number of selected antennas increases the power required by linear processing, such as precoding matrix computation, and short-term channel estimation. In this paper, we propose four antenna selection (AS) approaches to be deployed in XL-MIMO systems aiming at maximizing the total energy efficiency (EE). Besides, employing some simplifying assumptions, we derive a closed-form analytical expression for the EE of the XL-MIMO system, and propose a straightforward iterative method to determine the optimal number of selected antennas able to maximize it. The proposed AS schemes are based solely on long-term fading parameters, thus, the selected antennas set remains valid for a relatively large time/frequency intervals. Comparing the results, we find that the genetic-algorithm based AS scheme usually achieves the best EE performance, although our proposed highest normalized received power AS scheme also achieves very promising EE performance in a simple and straightforward way.      
### 53.A Survey of Deep Learning Architectures for Intelligent Reflecting Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2009.02540.pdf)
>  Deep learning (DL) has ignited a great research interest in the communications and signal processing society for system design in the physical layer of the wireless communications. DL is particularly advantageous for its model-free nature to provide an efficient way solving the complex and computationally intractable problems. Thus, DL-based works has adopted for the design of next generation communication systems, where intelligent reflecting surfaces (IRSs) are envisioned to be a promising tool due to its hardware efficiency. This article provides an overview of recent advances in the development of DL architectures for IRS-assisted wireless systems. We systematically investigate the DL models for the physical layer design problems, such as signal detection, channel estimation and beamforming. A comprehensive analysis is provided for the advantages and the drawbacks of the DL approaches and related design challenges, such as training data collection, communication- and hardware-efficient model training and environment adaptation, and we highlight the possible future research directions for DL-based wireless communication systems.      
### 54.An Algorithmic Approach for Identifying Critical Distance Relays for Transient Stability Studies  [ :arrow_down: ](https://arxiv.org/pdf/2009.02500.pdf)
>  After major disturbances, power system behavior is governed by the dynamic characteristics of its assets and protection schemes. Therefore, modeling protection devices is essential for performing accurate stability studies. Modeling all the protection devices in a bulk power system is an intractable task due to the limitations of current stability software, and the difficulty of maintaining and updating the data for thousands of protection devices. One of the critical protection schemes that is not adequately modeled in stability studies is distance relaying. Therefore, this paper proposes an algorithm that uses two methods to identify the critical distance relays to be modeled in stability studies: (i) apparent impedance monitoring, and (ii) the minimum voltage evaluation (MVE). The algorithm is implemented in Python 3.6 and uses the GE positive sequence load flow analysis (PSLF) software for performing stability studies. The performance of the algorithm is evaluated on the Western Electricity Coordinating Council (WECC) system data representing the 2018 summer-peak load. The results of the case studies representing various types of contingencies show that to have an accurate assessment of system behavior, modeling the critical distance relays identified by the algorithm suffices, and there is no need for modeling all the distance relays.      
### 55.Cross-domain Adaptation with Discrepancy Minimization for Text-independent Forensic Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2009.02444.pdf)
>  Forensic audio analysis for speaker verification offers unique challenges due to location/scenario uncertainty and diversity mismatch between reference and naturalistic field recordings. The lack of real naturalistic forensic audio corpora with ground-truth speaker identity represents a major challenge in this field. It is also difficult to directly employ small-scale domain-specific data to train complex neural network architectures due to domain mismatch and loss in performance. Alternatively, cross-domain speaker verification for multiple acoustic environments is a challenging task which could advance research in audio forensics. In this study, we introduce a CRSS-Forensics audio dataset collected in multiple acoustic environments. We pre-train a CNN-based network using the VoxCeleb data, followed by an approach which fine-tunes part of the high-level network layers with clean speech from CRSS-Forensics. Based on this fine-tuned model, we align domain-specific distributions in the embedding space with the discrepancy loss and maximum mean discrepancy (MMD). This maintains effective performance on the clean set, while simultaneously generalizes the model to other acoustic domains. From the results, we demonstrate that diverse acoustic environments affect the speaker verification performance, and that our proposed approach of cross-domain adaptation can significantly improve the results in this scenario.      
### 56.Interplay Between Resilience and Accuracy in Resilient Vector Consensus in Multi-Agent Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.02417.pdf)
>  In this paper, we study the relationship between resilience and accuracy in the resilient distributed multi-dimensional consensus problem. We consider a network of agents, each of which has a state in $\mathbb{R}^d$. Some agents in the network are adversarial and can change their states arbitrarily. The normal (non-adversarial) agents interact locally and update their states to achieve consensus at some point in the convex hull $\calC$ of their initial states. This objective is achievable if the number of adversaries in the neighborhood of normal agents is less than a specific value, which is a function of the local connectivity and the state dimension $d$. However, to be resilient against adversaries, especially in the case of large $d$, the desired local connectivity is large. We discuss that resilience against adversarial agents can be improved if normal agents are allowed to converge in a bounded region $\calB\supseteq\calC$, which means normal agents converge at some point close to but not necessarily inside $\calC$ in the worst case. The accuracy of resilient consensus can be measured by the Hausdorff distance between $\calB$ and $\calC$. As a result, resilience can be improved at the cost of accuracy. We propose a resilient bounded consensus algorithm that exploits the trade-off between resilience and accuracy by projecting $d$-dimensional states into lower dimensions and then solving instances of resilient consensus in lower dimensions. We analyze the algorithm, present various resilience and accuracy bounds, and also numerically evaluate our results.      
### 57.Online Learning of Parameterized Uncertain Dynamical Environments with Finite-sample Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2009.02390.pdf)
>  We present a novel online learning algorithm for a class of unknown and uncertain dynamical environments that are fully observable. First, we obtain a novel probabilistic characterization of systems whose mean behavior is known but which are subject to additive, unknown subGaussian disturbances. This characterization relies on recent concentration of measure results and is given in terms of ambiguity sets. Second, we extend the results to environments whose mean behavior is also unknown but described by a parameterized class of possible mean behaviors. Our algorithm adapts the ambiguity set dynamically by learning the parametric dependence online, and retaining similar probabilistic guarantees with respect to the additive, unknown disturbance. We illustrate the results on a differential-drive robot subject to environmental uncertainty.      
### 58.Risk-Averse Optimization for Resilience Enhancement of Complex Engineering Systems under Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2009.02351.pdf)
>  With the growth of complexity and extent, large scale interconnected network systems, e.g. transportation networks or infrastructure networks, become more vulnerable towards external disruptions. Hence, managing potential disruptive events during the design, operating, and recovery phase of an engineered system therefore improving the system's resilience is an important yet challenging task. In order to ensure system resilience after the occurrence of failure events, this study proposes a mixed-integer linear programming (MILP) based restoration framework using heterogeneous dispatchable agents. Scenario-based stochastic optimization (SO) technique is adopted to deal with the inherent uncertainties imposed on the recovery process from nature. Moreover, different from conventional SO using deterministic equivalent formulations, additional risk measure is implemented for this study because of the temporal sparsity of the decision making in applications such as the recovery from extreme events. The resulting restoration framework involves a large-scale MILP problem and thus an adequate decomposition technique, i.e. modified Lagrangian dual decomposition, is also employed in order to achieve tractable computational complexity. Case study results based on the IEEE 37-bus test feeder demonstrate the benefits of using the proposed framework for resilience improvement as well as the advantages of adopting SO formulations.      
### 59.An optimal home energy management system for modulating heat pumps and photovoltaic systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.02349.pdf)
>  Efficient residential sector coupling plays a key role in supporting the energy transition. In this study, we analyze the structural properties associated with the optimal control of a home energy management system and the effects of common technological configurations and objectives. We conduct this study by modeling a representative building with a modulating air-sourced heat pump, a photovoltaic (PV) system, a battery, and thermal storage systems for floor heating and hot-water supply. In addition, we allow grid feed-in by assuming fixed feed-in tariffs and consider user comfort. In our numerical analysis, we find that the battery, naturally, is the essential building block for improving self-sufficiency. However, in order to use the PV surplus efficiently grid feed-in is necessary. The commonly considered objective of maximizing self-consumption is not economically viable under the given tariff structure; however, close-to-optimal performance and significant reduction in solution times can be achieved by maximizing self-sufficiency. Based on optimal control and considering seasonal effects, the dominant order of PV distribution and the target states of charge of the storage systems can be derived. Using a rolling horizon approach, the solution time can be reduced to less than 1 min (achieving a time resolution of 1 h per year). By evaluating the value of information, we find that the common value of 24 h for the prediction and control horizons results in unintended but avoidable end-of-horizon effects. Our input data and mixed-integer linear model developed using the Julia JuMP programming language are available in an open-source manner.      
### 60.A Joint Network Optimization Framework to Predict Clinical Severity from Resting State Functional MRI Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.03238.pdf)
>  We propose a novel optimization framework to predict clinical severity from resting state fMRI (rs-fMRI) data. Our model consists of two coupled terms. The first term decomposes the correlation matrices into a sparse set of representative subnetworks that define a network manifold. These subnetworks are modeled as rank-one outer-products which correspond to the elemental patterns of co-activation across the brain; the subnetworks are combined via patient-specific non-negative coefficients. The second term is a linear regression model that uses the patient-specific coefficients to predict a measure of clinical severity. We validate our framework on two separate datasets in a ten fold cross validation setting. The first is a cohort of fifty-eight patients diagnosed with Autism Spectrum Disorder (ASD). The second dataset consists of sixty three patients from a publicly available ASD database. Our method outperforms standard semi-supervised frameworks, which employ conventional graph theoretic and statistical representation learning techniques to relate the rs-fMRI correlations to behavior. In contrast, our joint network optimization framework exploits the structure of the rs-fMRI correlation matrices to simultaneously capture group level effects and patient heterogeneity. Finally, we demonstrate that our proposed framework robustly identifies clinically relevant networks characteristic of ASD.      
### 61.Deepfake detection: humans vs. machines  [ :arrow_down: ](https://arxiv.org/pdf/2009.03155.pdf)
>  Deepfake videos, where a person's face is automatically swapped with a face of someone else, are becoming easier to generate with more realistic results. In response to the threat such manipulations can pose to our trust in video evidence, several large datasets of deepfake videos and many methods to detect them were proposed recently. However, it is still unclear how realistic deepfake videos are for an average person and whether the algorithms are significantly better than humans at detecting them. In this paper, we present a subjective study conducted in a crowdsourcing-like scenario, which systematically evaluates how hard it is for humans to see if the video is deepfake or not. For the evaluation, we used 120 different videos (60 deepfakes and 60 originals) manually pre-selected from the Facebook deepfake database, which was provided in the Kaggle's Deepfake Detection Challenge 2020. For each video, a simple question: "Is face of the person in the video real of fake?" was answered on average by 19 nave subjects. The results of the subjective evaluation were compared with the performance of two different state of the art deepfake detection methods, based on Xception and EfficientNets (B4 variant) neural networks, which were pre-trained on two other large public databases: the Google's subset from FaceForensics++ and the recent Celeb-DF dataset. The evaluation demonstrates that while the human perception is very different from the perception of a machine, both successfully but in different ways are fooled by deepfakes. Specifically, algorithms struggle to detect those deepfake videos, which human subjects found to be very easy to spot.      
### 62.Analysis of Uplink IRS-Assisted NOMA under Nakagami-m Fading via Moments Matching  [ :arrow_down: ](https://arxiv.org/pdf/2009.03133.pdf)
>  This letter investigates the uplink outage performance of intelligent reflecting surface (IRS)-assisted non-orthogonal multiple access (NOMA). We consider the general case where all users have both direct and reflection links, and all links undergo Nakagami-m fading. We approximate the received powers of the NOMA users as Gamma random variables via moments matching. This allows for tractable expressions of the outage under interference cancellation (IC), while being flexible in modeling various propagation environments. Our analysis shows that under certain conditions, the presence of an IRS might degrade the performance of users that have dominant line-of-sight (LOS) to the base station (BS), while users dominated by non-line-of-sight (NLOS) will always benefit from it.      
### 63.Localization and classification of intracranialhemorrhages in CT data  [ :arrow_down: ](https://arxiv.org/pdf/2009.03046.pdf)
>  Intracranial hemorrhages (ICHs) are life-threatening brain injures with a relatively high incidence. In this paper, the automatic algorithm for the detection and classification of ICHs, including localization, is present. The set of binary convolutional neural network-based classifiers with a designed cascade-parallel architecture is used. This automatic system may lead to a distinct decrease in the diagnostic process's duration in acute cases. An average Jaccard coefficient of 53.7 % is achieved on the data from the publicly available head CT dataset CQ500.      
### 64.Secrecy Performance of Terahertz Wireless Links in Rain and Snow  [ :arrow_down: ](https://arxiv.org/pdf/2009.03031.pdf)
>  Wireless communication technique at terahertz (THz) frequencies is regarded as the most potential candidate for future wireless networks due to its wider frequency bandwidth and higher data capacity when compared to that employing radio frequency (RF) and millimeter wave (mmWave). Besides, a THz link can achieve higher security at physical layer when it propagates in clear weather due to its higher directionality, which reduces the possibility of eavesdropping attacks. However, under adverse weather conditions (such as water fog, dust fog, rain and snow), the link degradation due to weather particles and gaseous molecules will affect the link secrecy performance seriously. In this work, we present theoretical investigations on physical layer security of a point-to-point THz link in rain and snow with a potential eavesdropper locating outside of the legitimate link path. Signal degradation due to rain/snow, gaseous attenuation and beam divergence are included in a theoretical model to estimate the link performance. Secrecy capacity of the link with carriers at 140, 220 and 340 GHz is calculated and compared. Insecure regions are also presented and the secrecy performance is analyzed. We find that the THz link suffers least eavesdropping attacks in rain and the maximum data transmission rate decreases for higher carrier frequencies in rain and snow.      
### 65.Soft-Output Finite Alphabet Equalization for mmWAVE Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2009.02990.pdf)
>  Next-generation wireless systems are expected to combine millimeter-wave (mmWave) and massive multi-user multiple-input multiple-output (MU-MIMO) technologies to deliver high data-rates. These technologies require the basestations (BSs) to process high-dimensional data at extreme rates, which results in high power dissipation and system costs. Finite-alphabet equalization has been proposed recently to reduce the power consumption and silicon area of uplink spatial equalization circuitry at the BS by coarsely quantizing the equalization matrix. In this work, we improve upon finite-alphabet equalization by performing unbiased estimation and soft-output computation for coded systems. By simulating a massive MU-MIMO system that uses orthogonal frequency-division multiplexing and per-user convolutional coding, we show that soft-output finite-alphabet equalization delivers competitive error-rate performance using only 1 to 3 bits per entry of the equalization matrix, even for challenging mmWave channels.      
### 66.Alternating Beamforming with Intelligent Reflecting Surface Element Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02875.pdf)
>  Intelligent reflecting surface (IRS) has become a promising technology to aid next generation wireless communication systems. In this paper, we develop an alternating beamforming technique with a novel concept of IRS element allocation for multiple-input multiple-output systems when a base station supports multiple single antenna users aided with a single IRS. Specifically, we allocate each IRS element separately to each user, thus, in the beamforming stage allowing the IRS element only consider a single user at a time. In result to this separation, the complexity is vastly decreased. The proposed beamforming technique tries to maximize the minimum rate of all users with minimal complexity. In the numerical results, we show that the proposed technique is comparable to the convex optimization-based benchmark with sufficiently less complexity.      
### 67.Dynamically Computing Adversarial Perturbations for Recurrent Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.02874.pdf)
>  Convolutional and recurrent neural networks have been widely employed to achieve state-of-the-art performance on classification tasks. However, it has also been noted that these networks can be manipulated adversarially with relative ease, by carefully crafted additive perturbations to the input. Though several experimentally established prior works exist on crafting and defending against attacks, it is also desirable to have theoretical guarantees on the existence of adversarial examples and robustness margins of the network to such examples. We provide both in this paper. We focus specifically on recurrent architectures and draw inspiration from dynamical systems theory to naturally cast this as a control problem, allowing us to dynamically compute adversarial perturbations at each timestep of the input sequence, thus resembling a feedback controller. Illustrative examples are provided to supplement the theoretical discussions.      
### 68.Robust Digital Envelope Estimation Via Geometric Properties of an Arbitrary Real Signal  [ :arrow_down: ](https://arxiv.org/pdf/2009.02860.pdf)
>  The temporal amplitude envelope of a signal is essential for its complete characterization, being the primary information-carrying medium in spoken voice and telecommunications, for example. Envelope detection techniques have applications in areas like health, sound classification and synthesis, seismology and speech recognition. Nevertheless, a general method to digital envelope detection of signals with rich spectral content doesn't exist, as most methods involve manual intervention, in the form of filter design, smoothing, as well as other specific design choices, based on a priori knowledge about the nature of the specific waves under investigation. To address this problem, we propose a framework that uses intrinsic characteristics of a signal to estimate its envelope, completely eliminating the necessity of parameter tuning. The approach here described draws inspiration from geometric concepts to isolate the frontiers and thus estimate the temporal envelope of an arbitrary signal; to that end, alpha-shapes, concave hulls, and discrete curvature are explored. We also define entities, such as a pulse and frontiers, in the context of an arbitrary digital signal, as a means to reduce dimensionality and the complexity of the proposed algorithm. Specifically, a new measure of discrete curvature is used to obtain the average radius of curvature of a discrete wave, serving as a threshold to identify the wave's frontier points. We find the algorithm accurate in the identification of the frontiers of a wide range of digital sound waves with very diverse characteristics, while localizing each pseudo-cycle of the wave in the time domain. The algorithm also compares favourably with classic envelope detection techniques based on filtering and the Hilbert Transform.      
### 69.Unsupervised Wasserstein Distance Guided Domain Adaptation for 3D Multi-Domain Liver Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02831.pdf)
>  Deep neural networks have shown exceptional learning capability and generalizability in the source domain when massive labeled data is provided. However, the well-trained models often fail in the target domain due to the domain shift. Unsupervised domain adaptation aims to improve network performance when applying robust models trained on medical images from source domains to a new target domain. In this work, we present an approach based on the Wasserstein distance guided disentangled representation to achieve 3D multi-domain liver segmentation. Concretely, we embed images onto a shared content space capturing shared feature-level information across domains and domain-specific appearance spaces. The existing mutual information-based representation learning approaches often fail to capture complete representations in multi-domain medical imaging tasks. To mitigate these issues, we utilize Wasserstein distance to learn more complete representation, and introduces a content discriminator to further facilitate the representation disentanglement. Experiments demonstrate that our method outperforms the state-of-the-art on the multi-modality liver segmentation task.      
### 70.Reconfigurable Intelligent Surfaces for Localization: Position and Orientation Error Bounds  [ :arrow_down: ](https://arxiv.org/pdf/2009.02818.pdf)
>  Next-generation cellular networks will witness the creation of smart radio environments (SREs), where walls and objects can be coated with reconfigurable intelligent surfaces (RISs) to strengthen the communication and localization coverage by controlling the reflected multipath. In fact, RISs have been recently introduced not only to overcome communication blockages due to obstacles but also for high-precision localization of mobile users in GPS denied environments, e.g., indoors. Towards this vision, this paper presents the localization performance limits for communication scenarios where a single next-generation NodeB base station (gNB), equipped with multiple-antennas, infers the position and the orientation of the user equipment(UE) in a RIS-assisted SRE. We consider a signal model that is valid also for near-field propagation conditions, as the usually adopted far-field assumption does not always hold, especially for large RISs. For the considered scenario, we derive the Cramer-Rao lower bound (CRLB) for assessing the ultimate localization and orientation performance of synchronous and asynchronous signaling schemes. In addition, we propose a closed-form RIS phase profile that well suits joint communication and localization. We perform extensive numerical results to assess the performance of our scheme for various localization scenarios and RIS phase design. Numerical results show that the proposed scheme can achieve remarkable performance, even in asynchronous signaling and that the proposed phase design approaches the numerical optimal phase design that minimizes the CRLB.      
### 71.A Change-Detection Based Thompson Sampling Framework for Non-Stationary Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2009.02791.pdf)
>  We consider a non-stationary two-armed bandit framework and propose a change-detection based Thompson sampling (TS) algorithm, named TS with change-detection (TS-CD), to keep track of the dynamic environment. The non-stationarity is modeled using a Poisson arrival process, which changes the mean of the rewards on each arrival. The proposed strategy compares the empirical mean of the recent rewards of an arm with the estimate of the mean of the rewards from its history. It detects a change when the empirical mean deviates from the mean estimate by a value larger than a threshold. Then, we characterize the lower bound on the duration of the time-window for which the bandit framework must remain stationary for TS-CD to successfully detect a change when it occurs. Consequently, our results highlight an upper bound on the parameter for the Poisson arrival process, for which the TS-CD achieves asymptotic regret optimality with high probability. Finally, we validate the efficacy of TS-CD by testing it for edge-control of radio access technique (RAT)-selection in a wireless network. Our results show that TS-CD not only outperforms the classical max-power RAT selection strategy but also other actively adaptive and passively adaptive bandit algorithms that are designed for non-stationary environments.      
### 72.A Vision of Self-Evolving Network Management for Future Intelligent Vertical HetNet  [ :arrow_down: ](https://arxiv.org/pdf/2009.02771.pdf)
>  Future integrated terrestrial-aerial-satellite networks will have to exhibit some unprecedented characteristics for the provision of both communications and computation services, and security for a tremendous number of devices with very broad and demanding requirements in an almost-ubiquitous manner. Although 3GPP introduced the concept of self-organization networks (SONs) in 4G and 5G documents to automate network management, even this progressive concept will face several challenges as it may not be sufficiently agile in coping with the immense levels of complexity, heterogeneity, and mobility in the envisioned beyond-5G integrated networks. In the presented vision, we discuss how future integrated networks can be intelligently and autonomously managed to efficiently utilize resources, reduce operational costs, and achieve the targeted Quality of Experience (QoE). We introduce the novel concept of self-evolving networks (SENs) framework, which utilizes artificial intelligence, enabled by machine learning (ML) algorithms, to make future integrated networks fully intelligent and automated with respect to the provision, adaptation, optimization, and management aspects of networking, communications, and computation. To envisage the concept of SEN in future integrated networks, we use the Intelligent Vertical Heterogeneous Network (I-VHetNet) architecture as our reference. The paper discusses five prominent communications and computation scenarios where SEN plays the main role in providing automated network management. Numerical results provide an insight on how the SEN framework improves the performance of future integrated networks. The paper presents the leading enablers and examines the challenges associated with the application of SEN concept in future integrated networks.      
### 73.Finite-Alphabet MMSE Equalization for All-Digital Massive MU-MIMO mmWave Communication  [ :arrow_down: ](https://arxiv.org/pdf/2009.02747.pdf)
>  We propose finite-alphabet equalization, a new paradigm that restricts the entries of the spatial equalization matrix to low-resolution numbers, enabling high-throughput, low-power, and low-cost hardware equalizers. To minimize the performance loss of this paradigm, we introduce FAME, short for finite-alphabet minimum mean-square error (MMSE) equalization, which is able to significantly outperform a naive quantization of the linear MMSE matrix. We develop efficient algorithms to approximately solve the NP-hard FAME problem and showcase that near-optimal performance can be achieved with equalization coefficients quantized to only 1-3 bits for massive multi-user multiple-input multiple-output (MU-MIMO) millimeter-wave (mmWave) systems. We provide very-large scale integration (VLSI) results that demonstrate a reduction in equalization power and area by at least a factor of 3.9x and 5.8x, respectively.      
### 74.On the probabilistic feasibility of solutions in multi-agent optimization problems under uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2009.02706.pdf)
>  We investigate the probabilistic feasibility of randomized solutions to two distinct classes of uncertain multi-agent optimization programs. We first assume that only the constraints of the program are affected by uncertainty, while the cost function is arbitrary. Leveraging recent \emph{a posteriori} developments of the scenario approach, we provide probabilistic guarantees for all feasible solutions of the program under study. This result is particularly useful in cases where numerical difficulties related to the convergence of the solution-seeking algorithm hinder the exact quantification of the optimal solution. Furthermore, it can be applied to cases where the agents' incentives lead to a suboptimal solution, e.g., under a non-cooperative setting. We then focus on optimization programs where the cost function admits an aggregate representation and depends on uncertainty while constraints are deterministic. By exploiting the structure of the program under study and leveraging the so called support rank notion, we provide agent-independent robustness certificates for the optimal solution, i.e., the constructed bound on the probability of constraint violation does not depend on the number of agents, but only on the dimension of the agents' decision. This substantially reduces the number of samples required to achieve a certain level of probabilistic robustness as the number of agents increases. All robustness certificates provided in this paper are distribution-free and can be used alongside any optimization algorithm. Our theoretical results are accompanied by a numerical case study involving a charging control problem of a fleet of electric vehicles.      
### 75.A Generative Adversarial Approach To ECG Synthesis And Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2009.02700.pdf)
>  Generative Adversarial Networks (GAN) are known to produce synthetic data that are difficult to discern from real ones by humans. In this paper we present an approach to use GAN to produce realistically looking ECG signals. We utilize them to train and evaluate a denoising autoencoder that achieves state-of-the-art filtering quality for ECG signals. It is demonstrated that generated data improves the model performance compared to the model trained on real data only. We also investigate an effect of transfer learning by reusing trained discriminator network for denoising model.      
### 76.Weighted Information Filtering, Smoothing, and Out-of-Sequence Measurement Processing  [ :arrow_down: ](https://arxiv.org/pdf/2009.02659.pdf)
>  We consider the problem of state estimation in dynamical systems and propose a different mechanism for handling unmodeled system uncertainties. Instead of injecting random process noise, we assign different weights to measurements so that more recent measurements are assigned more weight. A specific choice of exponentially decaying weight function results in an algorithm with essentially the same recursive structure as the Kalman filter. It differs, however, in the manner in which old and new data are combined. While in the classical KF, the uncertainty associated with the previous estimate is inflated by adding the process noise covariance, in the present case, the uncertainty inflation is done by multiplying the previous covariance matrix by an exponential factor. This difference allows us to solve a larger variety of problems using essentially the same algorithm. We thus propose a unified and optimal, in the least-squares sense, method for filtering, prediction, smoothing and general out-of-sequence updates. All of these tasks require different Kalman-like algorithms when addressed in the classical manner.      
### 77.Deep Ensemble of Weighted Viterbi Decoders for Tail-Biting Convolutional Codes  [ :arrow_down: ](https://arxiv.org/pdf/2009.02591.pdf)
>  Tail-biting convolutional codes extend the classical zero-termination convolutional codes: Both encoding schemes force the equality of start and end states, but under the tail-biting each state is a valid termination. This paper proposes a machine-learning approach to improve the state-of-the-art decoding of tail-biting codes, focusing on the widely employed short length regime as in the LTE standard. This standard also includes a CRC code. <br>First, we parameterize the circular Viterbi algorithm, a baseline decoder that exploits the circular nature of the underlying trellis. An ensemble combines multiple such weighted decoders, each decoder specializes in decoding words from a specific region of the channel words' distribution. A region corresponds to a subset of termination states; the ensemble covers the entire states space. A non-learnable gating satisfies two goals: it filters easily decoded words and mitigates the overhead of executing multiple weighted decoders. The CRC criterion is employed to choose only a subset of experts for decoding purpose. Our method achieves FER improvement of up to 0.75dB over the CVA in the waterfall region for multiple code lengths, adding negligible computational complexity compared to the circular Viterbi algorithm in high SNRs.      
### 78.Deep Sparse Light Field Refocusing  [ :arrow_down: ](https://arxiv.org/pdf/2009.02582.pdf)
>  Light field photography enables to record 4D images, containing angular information alongside spatial information of the scene. One of the important applications of light field imaging is post-capture refocusing. Current methods require for this purpose a dense field of angle views; those can be acquired with a micro-lens system or with a compressive system. Both techniques have major drawbacks to consider, including bulky structures and angular-spatial resolution trade-off. We present a novel implementation of digital refocusing based on sparse angular information using neural networks. This allows recording high spatial resolution in favor of the angular resolution, thus, enabling to design compact and simple devices with improved hardware as well as better performance of compressive systems. We use a novel convolutional neural network whose relatively small structure enables fast reconstruction with low memory consumption. Moreover, it allows handling without re-training various refocusing ranges and noise levels. Results show major improvement compared to existing methods.      
### 79.Structured Sparsity Modeling for Improved Multivariate Statistical Analysis based Fault Isolation  [ :arrow_down: ](https://arxiv.org/pdf/2009.02528.pdf)
>  In order to improve the fault diagnosis capability of multivariate statistical methods, this article introduces a fault isolation method based on structured sparsity modelling. The developed method relies on the reconstruction based contribution analysis and the process structure information can be incorporated into the reconstruction objective function in the form of structured sparsity regularization terms. The structured sparsity terms allow optimal selection of fault variables over structures like blocks or networks of process variables, hence more accurate fault isolation can be achieved. Four structured sparsity terms corresponding to different kinds of process information are considered, namely, partially known sparse support, block sparsity, clustered sparsity and tree-structured sparsity. The optimization problems involving the structured sparsity terms can be solved using the Alternating Multiplier Method (ADMM) algorithm, which is fast and efficient. In addition, the ADMM algorithm can be easily extended in a parallel/distributed way to handle large-scale systems with a large number of variables. Through a simulation example and an application study to a coal-fired power plant, it is verified that the proposed method can better isolate faulty variables by incorporating process structure information.      
### 80.Towards Probabilistic Tensor Canonical Polyadic Decomposition 2.0: Automatic Tensor Rank Learning Using Generalized Hyperbolic Prior  [ :arrow_down: ](https://arxiv.org/pdf/2009.02472.pdf)
>  Tensor rank learning for canonical polyadic decomposition (CPD) has long been deemed as an essential but challenging problem. In particular, since the tensor rank controls the complexity of the CPD model, its inaccurate learning would cause overfitting to noise or underfitting to the signal sources, and even destroy the interpretability of model parameters. However, the optimal determination of a tensor rank is known to be a non-deterministic polynomial-time hard (NP-hard) task. Rather than exhaustively searching for the best tensor rank via trial-and-error experiments, Bayesian inference under the Gaussian-gamma prior was introduced in the context of probabilistic CPD modeling and it was shown to be an effective strategy for automatic tensor rank determination. This triggered flourishing research on other structured tensor CPDs with automatic tensor rank learning. As the other side of the coin, these research works also reveal that the Gaussian-gamma model does not perform well for high-rank tensors or/and low signal-to-noise ratios (SNRs). To overcome these drawbacks, in this paper, we introduce a more advanced generalized hyperbolic (GH) prior to the probabilistic CPD model, which not only includes the Gaussian-gamma model as a special case, but also provides more flexibilities to adapt to different levels of sparsity. Based on this novel probabilistic model, an algorithm is developed under the framework of variational inference, where each update is obtained in a closed-form. Extensive numerical results, using synthetic data and real-world datasets, demonstrate the excellent performance of the proposed method in learning both low as well as high tensor ranks even for low SNR cases.      
### 81.Construction of Periodic Counterexamples to the Discrete-Time Kalman Conjecture  [ :arrow_down: ](https://arxiv.org/pdf/2009.02468.pdf)
>  This paper considers the Lurye system of a discrete-time, linear time-invariant plant in negative feedback with a nonlinearity. Both monotone and slope-restricted nonlinearities are considered. The main result is a procedure to construct destabilizing nonlinearities for the Lurye system. If the plant satisfies a certain phase condition then a monotone nonlinearity can be constructed so that the Lurye system has a non-trivial periodic cycle. Several examples are provided to demonstrate the construction. This represents a contribution for absolute stability analysis since the constructed nonlinearity provides a less conservative upper bound than existing bounds in the literature.      
### 82.Duality bounds for discrete-time Zames-Falb multipliers  [ :arrow_down: ](https://arxiv.org/pdf/2008.11975.pdf)
>  We develop phase limitations for the discrete-time Zames-Falb multipliers based on the separation theorem for Banach spaces. By contrast with their continuous-time counterparts they lead to numerically efficient results that can be computed either in closed form or via a linear program. The closed-form phase limitations are tight in the sense that we can construct multipliers that meet them with equality. We discuss numerical examples where the limitations are stronger than others in the literature. The numerical results complement searches for multipliers in the literature; they allow us to show, by construction, that the set of plants for which a suitable Zames-Falb multiplier exists is non-convex.      
### 83.High-Confidence Attack Detection via Wasserstein-Metric Computations  [ :arrow_down: ](https://arxiv.org/pdf/2003.07880.pdf)
>  This paper considers a sensor attack and fault detection problem for linear cyber-physical systems, which are subject to system noise that can obey an unknown light-tailed distribution. We propose a new threshold-based detection mechanism that employs the Wasserstein metric, and which guarantees system performance with high confidence employing a finite number of measurements. The proposed detector may generate false alarms with a rate $\Delta$ in normal operation, where $\Delta$ can be tuned to be arbitrarily small by means of a benchmark distribution which is part of our mechanism. Thus, the proposed detector is sensitive to sensor attacks and faults which have a statistical behavior that is different from that of the system's noise. We quantify the impact of stealthy attacks---which aim to perturb the system operation while producing false alarms that are consistent with the natural system's noise---via a probabilistic reachable set. To enable tractable implementation of our methods, we propose a linear optimization problem that computes the proposed detection measure and a semidefinite program that produces the proposed reachable set.      
### 84.Data-driven Variable Speed Limit Design with Performance Guarantees for Highways  [ :arrow_down: ](https://arxiv.org/pdf/1911.10184.pdf)
>  This paper studies the data-driven design of variable speed limits for highways subject to uncertainty, including unknown driver actions as well as vehicle arrivals and departures. With accessibility to sample measurements of the uncertain variables, we aim to find the set of speed limits that prevents traffic congestion and an optimum vehicle throughput with high probability. This results into the formulation of a stochastic optimization problem (P), which is intractable due to the unknown distribution of the uncertainty variables. By developing a distributionally robust optimization framework, we present an equivalent and yet tractable reformulation of (P). Further, we propose an efficient algorithm that provides suboptimal data-driven solutions and guarantees congestion-free conditions with high probability. We employ the resulting control method on a traffic simulator to illustrate the effectiveness of this approach.      
### 85.3D Contouring for Breast Tumor in Sonography  [ :arrow_down: ](https://arxiv.org/pdf/1901.09407.pdf)
>  Malignant and benign breast tumors present differently in their shape and size on sonography. Morphological information provided by tumor contours are important in clinical diagnosis. However, ultrasound images contain noises and tissue texture; clinical diagnosis thus highly depends on the experience of physicians. The manual way to sketch three-dimensional (3D) contours of breast tumor is a time-consuming and complicate task. If automatic contouring could provide a precise breast tumor contour that might assist physicians in making an accurate diagnosis. This study presents an efficient method for automatically contouring breast tumors in 3D sonography. The proposed method utilizes an efficient segmentation procedure, i.e. level-set method (LSM), to automatic detect contours of breast tumors. This study evaluates 20 cases comprising ten benign and ten malignant tumors. The results of computer simulation reveal that the proposed 3D segmentation method provides robust contouring for breast tumor on ultrasound images. This approach consistently obtains contours similar to those obtained by manual contouring of the breast tumor and can save much of the time required to sketch precise contours.      
### 86.Data assimilation and online optimization with performance guarantees  [ :arrow_down: ](https://arxiv.org/pdf/1901.07377.pdf)
>  This paper considers a class of real-time stochastic optimization problems dependent on an unknown probability distribution. In the considered scenario, data is streaming frequently while trying to reach a decision. Thus, we aim to devise a procedure that incorporates samples (data) of the distribution sequentially and adjusts decisions accordingly. We approach this problem in a distributionally robust optimization framework and propose a novel Online Data Assimilation Algorithm (ONDA Algorithm) for this purpose. This algorithm guarantees out-of-sample performance of decisions with high probability, and gradually improves the quality of the decisions by incorporating the streaming data. We show that the ONDA Algorithm converges under a sufficiently slow data streaming rate, and provide a criteria for its termination after certain number of data have been collected. Simulations illustrate the results.      
### 87.Convex searches for discrete-time Zames-Falb multipliers  [ :arrow_down: ](https://arxiv.org/pdf/1812.02397.pdf)
>  In this paper we develop and analyse convex searches for Zames--Falb multipliers. We present two different approaches: Infinite Impulse Response (IIR) and Finite Impulse Response (FIR) multipliers. The set of FIR multipliers is complete in that any IIR multipliers can be phase-substituted by an arbitrarily large order FIR multiplier. We show that searches in discrete-time for FIR multipliers are effective even for large orders. As expected, the numerical results provide the best $\ell_{2}$-stability results in the literature for slope-restricted nonlinearities. Finally, we demonstrate that the discrete-time search can provide an effective method to find suitable continuous-time multipliers.      
### 88.Data-driven Variable Speed Limit Design for Highways via Distributionally Robust Optimization  [ :arrow_down: ](https://arxiv.org/pdf/1810.11385.pdf)
>  This paper introduces an optimization problem (P) and a solution strategy to design variable-speed-limit controls for a highway that is subject to traffic congestion and uncertain vehicle arrival and departure. By employing a finite data-set of samples of the uncertain variables, we aim to find a data-driven solution that has a guaranteed out-of-sample performance. In principle, such formulation leads to an intractable problem (P) as the distribution of the uncertainty variable is unknown. By adopting a distributionally robust optimization approach, this work presents a tractable reformulation of (P) and an efficient algorithm that provides a suboptimal solution that retains the out-of-sample performance guarantee. A simulation illustrates the effectiveness of this method.      
### 89.Online data assimilation in distributionally robust optimization  [ :arrow_down: ](https://arxiv.org/pdf/1803.07984.pdf)
>  This paper considers a class of real-time decision making problems to minimize the expected value of a function that depends on a random variable $\xi$ under an unknown distribution $\mathbb{P}$. In this process, samples of $\xi$ are collected sequentially in real time, and the decisions are made, using the real-time data, to guarantee out-of-sample performance. We approach this problem in a distributionally robust optimization framework and propose a novel Online Data Assimilation Algorithm for this purpose. This algorithm guarantees the out-of-sample performance in high probability, and gradually improves the quality of the data-driven decisions by incorporating the streaming data. We show that the Online Data Assimilation Algorithm guarantees convergence under the streaming data, and a criteria for termination of the algorithm after certain number of data has been collected.      
### 90.Analysis of a Modern Voice Morphing Approach using Gaussian Mixture Models for Laryngectomees  [ :arrow_down: ](https://arxiv.org/pdf/1208.1418.pdf)
>  This paper proposes a voice morphing system for people suffering from Laryngectomy, which is the surgical removal of all or part of the larynx or the voice box, particularly performed in cases of laryngeal cancer. A primitive method of achieving voice morphing is by extracting the source's vocal coefficients and then converting them into the target speaker's vocal parameters. In this paper, we deploy Gaussian Mixture Models (GMM) for mapping the coefficients from source to destination. However, the use of the traditional/conventional GMM-based mapping approach results in the problem of over-smoothening of the converted voice. Thus, we hereby propose a unique method to perform efficient voice morphing and conversion based on GMM,which overcomes the traditional-method effects of over-smoothening. It uses a technique of glottal waveform separation and prediction of excitations and hence the result shows that not only over-smoothening is eliminated but also the transformed vocal tract parameters match with the target. Moreover, the synthesized speech thus obtained is found to be of a sufficiently high quality. Thus, voice morphing based on a unique GMM approach has been proposed and also critically evaluated based on various subjective and objective evaluation parameters. Further, an application of voice morphing for Laryngectomees which deploys this unique approach has been recommended by this paper.      
### 91.Text-Independent Speaker Recognition for Low SNR Environments with Encryption  [ :arrow_down: ](https://arxiv.org/pdf/1111.0024.pdf)
>  Recognition systems are commonly designed to authenticate users at the access control levels of a system. A number of voice recognition methods have been developed using a pitch estimation process which are very vulnerable in low Signal to Noise Ratio (SNR) environments thus, these programs fail to provide the desired level of accuracy and robustness. Also, most text independent speaker recognition programs are incapable of coping with unauthorized attempts to gain access by tampering with the samples or reference database. The proposed text-independent voice recognition system makes use of multilevel cryptography to preserve data integrity while in transit or storage. Encryption and decryption follow a transform based approach layered with pseudorandom noise addition whereas for pitch detection, a modified version of the autocorrelation pitch extraction algorithm is used. The experimental results show that the proposed algorithm can decrypt the signal under test with exponentially reducing Mean Square Error over an increasing range of SNR. Further, it outperforms the conventional algorithms in actual identification tasks even in noisy environments. The recognition rate thus obtained using the proposed method is compared with other conventional methods used for speaker identification.      
### 92.Rotation, Scaling and Translation Analysis of Biometric Signature Templates  [ :arrow_down: ](https://arxiv.org/pdf/1110.1208.pdf)
>  Biometric authentication systems that make use of signature verification methods often render optimum performance only under limited and restricted conditions. Such methods utilize several training samples so as to achieve high accuracy. Moreover, several constraints are imposed on the end-user so that the system may work optimally, and as expected. For example, the user is made to sign within a small box, in order to limit their signature to a predefined set of dimensions, thus eliminating scaling. Moreover, the angular rotation with respect to the referenced signature that will be inadvertently introduced as human error, hampers performance of biometric signature verification systems. To eliminate this, traditionally, a user is asked to sign exactly on top of a reference line. In this paper, we propose a robust system that optimizes the signature obtained from the user for a large range of variation in Rotation-Scaling-Translation (RST) and resolves these error parameters in the user signature according to the reference signature stored in the database.      
