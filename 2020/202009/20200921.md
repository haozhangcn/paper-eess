# ArXiv eess --Mon, 21 Sep 2020
### 1.Predicting molecular phenotypes from histopathology images: a transcriptome-wide expression-morphology analysis in breast cancer  [ :arrow_down: ](https://arxiv.org/pdf/2009.08917.pdf)
>  Molecular phenotyping is central in cancer precision medicine, but remains costly and standard methods only provide a tumour average profile. Microscopic morphological patterns observable in histopathology sections from tumours are determined by the underlying molecular phenotype and associated with clinical factors. The relationship between morphology and molecular phenotype has a potential to be exploited for prediction of the molecular phenotype from the morphology visible in histopathology images. <br>We report the first transcriptome-wide Expression-MOrphology (EMO) analysis in breast cancer, where gene-specific models were optimised and validated for prediction of mRNA expression both as a tumour average and in spatially resolved manner. Individual deep convolutional neural networks (CNNs) were optimised to predict the expression of 17,695 genes from hematoxylin and eosin (HE) stained whole slide images (WSIs). Predictions for 9,334 (52.75%) genes were significantly associated with RNA-sequencing estimates (FDR adjusted p-value &lt; 0.05). 1,011 of the genes were brought forward for validation, with 876 (87%) and 908 (90%) successfully replicated in internal and external test data, respectively. Predicted spatial intra-tumour variabilities in expression were validated in 76 genes, out of which 59 (77.6%) had a significant association (FDR adjusted p-value &lt; 0.05) with spatial transcriptomics estimates. These results suggest that the proposed methodology can be applied to predict both tumour average gene expression and intra-tumour spatial expression directly from morphology, thus providing a scalable approach to characterise intra-tumour heterogeneity.      
### 2.AdderSR: Towards Energy Efficient Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2009.08891.pdf)
>  This paper studies the single image super-resolution problem using adder neural networks (AdderNet). Compared with convolutional neural networks, AdderNet utilizing additions to calculate the output features thus avoid massive energy consumptions of conventional multiplications. However, it is very hard to directly inherit the existing success of AdderNet on large-scale image classification to the image super-resolution task due to the different calculation paradigm. Specifically, the adder operation cannot easily learn the identity mapping, which is essential for image processing tasks. In addition, the functionality of high-pass filters cannot be ensured by AdderNet. To this end, we thoroughly analyze the relationship between an adder operation and the identity mapping and insert shortcuts to enhance the performance of SR models using adder networks. Then, we develop a learnable power activation for adjusting the feature distribution and refining details. Experiments conducted on several benchmark models and datasets demonstrate that, our image super-resolution models using AdderNet can achieve comparable performance and visual quality to that of their CNN baselines with an about 2$\times$ reduction on the energy consumption.      
### 3.Classification and Region Analysis of COVID-19 Infection using Lung CT Images and Deep Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2009.08864.pdf)
>  COVID-19 is a global health problem. Consequently, early detection and analysis of the infection patterns are crucial for controlling infection spread as well as devising a treatment plan. This work proposes a two-stage deep Convolutional Neural Networks (CNNs) based framework for delineation of COVID-19 infected regions in Lung CT images. In the first stage, initially, COVID-19 specific CT image features are enhanced using a two-level discrete wavelet transformation. These enhanced CT images are then classified using the proposed custom-made deep CoV-CTNet. In the second stage, the CT images classified as infectious images are provided to the segmentation models for the identification and analysis of COVID-19 infectious regions. In this regard, we propose a novel semantic segmentation model CoV-RASeg, which systematically uses average and max pooling operations in the encoder and decoder blocks. This systematic utilization of max and average pooling operations helps the proposed CoV-RASeg in simultaneously learning both the boundaries and region homogeneity. Moreover, the idea of attention is incorporated to deal with mildly infected regions. The proposed two-stage framework is evaluated on a standard Lung CT image dataset, and its performance is compared with the existing deep CNN models. The performance of the proposed CoV-CTNet is evaluated using Mathew Correlation Coefficient (MCC) measure (0.98) and that of proposed CoV-RASeg using Dice Similarity (DS) score (0.95). The promising results on an unseen test set suggest that the proposed framework has the potential to help the radiologists in the identification and analysis of COVID-19 infected regions.      
### 4.Fused Deep Convolutional Neural Network for Precision Diagnosis of COVID-19 Using Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2009.08831.pdf)
>  With a Coronavirus disease (COVID-19) case count exceeding 10 million worldwide, there is an increased need for a diagnostic capability. The main variables in increasing diagnostic capability are reduced cost, turnaround or diagnosis time, and upfront equipment cost and accessibility. Two candidates for machine learning COVID-19 diagnosis are Computed Tomography (CT) scans and plain chest X-rays. While CT scans score higher in sensitivity, they have a higher cost, maintenance requirement, and turnaround time as compared to plain chest X-rays. The use of portable chest X-radiograph (CXR) is recommended by the American College of Radiology (ACR) since using CT places a massive burden on radiology services. Therefore, X-ray imagery paired with machine learning techniques is proposed a first-line triage tool for COVID-19 diagnostics. In this paper we propose a computer-aided diagnosis (CAD) to accurately classify chest X-ray scans of COVID-19 and normal subjects by fine-tuning several neural networks (ResNet18, ResNet50, DenseNet201) pre-trained on the ImageNet dataset. These neural networks are fused in a parallel architecture and the voting criteria are applied in the final classification decision between the candidate object classes where the output of each neural network is representing a single vote. Several experiments are conducted on the weakly labeled COVID-19-CT-CXR dataset consisting of 263 COVID-19 CXR images extracted from PubMed Central Open Access subsets combined with 25 normal classification CXR images. These experiments show an optimistic result and a capability of the proposed model to outperforming many state-of-the-art algorithms on several measures. Using k-fold cross-validation and a bagging classifier ensemble, we achieve an accuracy of 99.7% and a sensitivity of 100%.      
### 5.Residual Spatial Attention Network for Retinal Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2009.08829.pdf)
>  Reliable segmentation of retinal vessels can be employed as a way of monitoring and diagnosing certain diseases, such as diabetes and hypertension, as they affect the retinal vascular structure. In this work, we propose the Residual Spatial Attention Network (RSAN) for retinal vessel segmentation. RSAN employs a modified residual block structure that integrates DropBlock, which can not only be utilized to construct deep networks to extract more complex vascular features, but can also effectively alleviate the overfitting. Moreover, in order to further improve the representation capability of the network, based on this modified residual block, we introduce the spatial attention (SA) and propose the Residual Spatial Attention Block (RSAB) to build RSAN. We adopt the public DRIVE and CHASE DB1 color fundus image datasets to evaluate the proposed RSAN. Experiments show that the modified residual structure and the spatial attention are effective in this work, and our proposed RSAN achieves the state-of-the-art performance.      
### 6.Disordered complex networks: energy optimal lattices and persistent homology  [ :arrow_down: ](https://arxiv.org/pdf/2009.08811.pdf)
>  Disordered complex networks are of fundamental interest as stochastic models for information transmission over wireless networks. Well-known networks based on the Poisson point process model have limitations vis-a-vis network efficiency, whereas strongly correlated alternatives, such as those based on random matrix spectra (RMT), have tractability and robustness issues. In this work, we demonstrate that network models based on random perturbations of Euclidean lattices interpolate between Poisson and rigidly structured networks, and allow us to achieve the best of both worlds : significantly improve upon the Poisson model in terms of network efficacy measured by the Signal to Interference plus Noise Ratio (abbrv. SINR) and the related concept of coverage probabilities, at the same time retaining a considerable measure of mathematical and computational simplicity and robustness to erasure and noise. <br>We investigate the optimal choice of the base lattice in this model, connecting it to the celebrated problem optimality of Euclidean lattices with respect to the Epstein Zeta function, which is in turn related to notions of lattice energy. This leads us to the choice of the triangular lattice in 2D and face centered cubic lattice in 3D. We demonstrate that the coverage probability decreases with increasing strength of perturbation, eventually converging to that of the Poisson network. In the regime of low disorder, we approximately characterize the statistical law of the coverage function. <br>In 2D, we determine the disorder strength at which the PTL and the RMT networks are the closest measured by comparing their network topologies via a comparison of their Persistence Diagrams . We demonstrate that the PTL network at this disorder strength can be taken to be an effective substitute for the RMT network model, while at the same time offering the advantages of greater tractability.      
### 7.Automatic Differentiation to Simultaneously Identify Nonlinear Dynamics and Extract Noise Probability Distributions from Data  [ :arrow_down: ](https://arxiv.org/pdf/2009.08810.pdf)
>  The sparse identification of nonlinear dynamics (SINDy) is a regression framework for the discovery of parsimonious dynamic models and governing equations from time-series data. As with all system identification methods, noisy measurements compromise the accuracy and robustness of the model discovery procedure. In this work, we develop a variant of the SINDy algorithm that integrates automatic differentiation and recent time-stepping constrained motivated by Rudy et al. for simultaneously (i) denoising the data, (ii) learning and parametrizing the noise probability distribution, and (iii) identifying the underlying parsimonious dynamical system responsible for generating the time-series data. Thus within an integrated optimization framework, noise can be separated from signal, resulting in an architecture that is approximately twice as robust to noise as state-of-the-art methods, handling as much as 40% noise on a given time-series signal and explicitly parametrizing the noise probability distribution. We demonstrate this approach on several numerical examples, from Lotka-Volterra models to the spatio-temporal Lorenz 96 model. Further, we show the method can identify a diversity of probability distributions including Gaussian, uniform, Gamma, and Rayleigh.      
### 8.Deep learning denoising for EOG artifacts removal from EEG signals  [ :arrow_down: ](https://arxiv.org/pdf/2009.08809.pdf)
>  There are many sources of interference encountered in the electroencephalogram (EEG) recordings, specifically ocular, muscular, and cardiac artifacts. Rejection of EEG artifacts is an essential process in EEG analysis since such artifacts cause many problems in EEG signals analysis. One of the most challenging issues in EEG denoising processes is removing the ocular artifacts where Electrooculographic (EOG), and EEG signals have an overlap in both frequency and time domains. In this paper, we build and train a deep learning model to deal with this challenge and remove the ocular artifacts effectively. In the proposed scheme, we convert each EEG signal to an image to be fed to a U-NET model, which is a deep learning model usually used in image segmentation tasks. We proposed three different schemes and made our U-NET based models learn to purify contaminated EEG signals similar to the process used in the image segmentation process. The results confirm that one of our schemes can achieve a reliable and promising accuracy to reduce the Mean square error between the target signal (Pure EEGs) and the predicted signal (Purified EEGs).      
### 9.Improving the spatial resolution of a BOTDA sensor using deconvolution algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2009.08804.pdf)
>  Spatial resolution improvement from an acquired measurement using long pulse is developed for Brillouin optical time domain analysis (BOTDA) systems based on the total variation deconvolution algorithm. The frequency dependency of Brillouin gain temporal envelope is investigated by simulation, and its impact on the recovered results of deconvolution algorithm is thoroughly analyzed. To implement a reliable deconvolution process, differential pulse-width pair (DPP) technique is utilized to effectively eliminate the systematic BFS distortion stemming from the frequency dependency of temporal envelope. The width of the pulse pairs should be larger than 40 ns as is analyzed theoretically and verified experimentally. It has been demonstrated that the proposed method can realize flexible adjustment of spatial resolution with enhanced signal-to-noise ratio (SNR) from an established measurement with long pump pulse. In the experiment, the spatial resolution is increased to 0.5 m and 1 m with high measurement accuracy by using the deconvolution algorithm from the measurement of 60/40 ns DPP signals. Compared with the raw DPP results with the same spatial resolution, 9.2 dB and 8.4 dB SNR improvements are obtained for 0.5 m and 1 m spatial resolution respectively, thanks to the denoising capability of the total variation deconvolution algorithm. The impact of sampling rate on the recovery results is also studied. The proposed sensing system allows for distortion-free Brillouin distributed sensing with higher spatial resolution and enhanced SNR from the conventional DPP setup with long pulse pairs.      
### 10.Automated Stroke Rehabilitation Assessment using Wearable Accelerometers in Free-Living Environments  [ :arrow_down: ](https://arxiv.org/pdf/2009.08798.pdf)
>  Stroke is known as a major global health problem, and for stroke survivors it is key to monitor the recovery levels. However, traditional stroke rehabilitation assessment methods (such as the popular clinical assessment) can be subjective and expensive, and it is also less convenient for patients to visit clinics in a high frequency. To address this issue, in this work based on wearable sensing and machine learning techniques, we developed an automated system that can predict the assessment score in an objective and continues manner. With wrist-worn sensors, accelerometer data was collected from 59 stroke survivors in free-living environments for a duration of 8 weeks, and we aim to map the week-wise accelerometer data (3 days per week) to the assessment score by developing signal processing and predictive model pipeline. To achieve this, we proposed two new features, which can encode the rehabilitation information from both paralysed/non-paralysed sides while suppressing the high-level noises such as irrelevant daily activities. We further developed the longitudinal mixed-effects model with Gaussian process prior (LMGP), which can model the random effects caused by different subjects and time slots (during the 8 weeks). Comprehensive experiments were conducted to evaluate our system on both acute and chronic patients, and the results suggested its effectiveness.      
### 11.Hybrid Digital-Analog Beamforming and MIMO Radar with OTFS Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2009.08785.pdf)
>  Motivated by future automotive applications, we study some joint radar target detection and parameter estimation problems where the transmitter, equipped with a mono-static MIMO radar, wishes to detect multiple targets and then estimate their respective parameters, while simultaneously communicating information data using orthogonal time frequency space (OTFS) modulation. Assuming that the number of radio frequency chains is smaller than the number of antennas over the mmWave frequency band, we design hybrid digital-analog beamforming at the radar transmitter adapted to different operating phases. The first scenario considers a wide angular beam in order to perform the target detection and parameter estimation, while multicasting a common message to all possible active users. The second scenario considers narrow angular beams to send information streams individually to the already detected users and simultaneously keep tracking of their respective parameters. Under this setup, we propose an efficient maximum likelihood scheme combined with hybrid beamforming to jointly perform target detection and parameter estimation. Our numerical results demonstrate that the proposed algorithm is able to reliably detect multiple targets with a sufficient number of antennas and achieves the CramÃ©r-Rao lower bound for radar parameter estimation such as delay, Doppler and angle-of-arrival (AoA).      
### 12.Revisiting Consensus-Based Energy-Management in Smart Grid with Transmission Losses and Directed Communication  [ :arrow_down: ](https://arxiv.org/pdf/2009.08758.pdf)
>  We discovered a deficiency in Algorithm 1 and Theorem 3 of [1]. The algorithm called CEMA aims to solve an energy management problem distributively. However, by means of a counter example, we show that Theorem 2 and 3 of [1] contradict each other in the case of a valid scenario, proving that the suggested algorithm does not always find the optimum. Furthermore, we provide theoretic results, showing that Theorem 3 of [1] does not hold generally. At last, we provide a rectification by adjusting the algorithm and the corresponding proof of Theorem 3.      
### 13.An Analysis by Synthesis Method that Allows Accurate Spatial Modeling of Thickness of Cortical Bone from Clinical QCT  [ :arrow_down: ](https://arxiv.org/pdf/2009.08664.pdf)
>  Osteoporosis is a skeletal disorder that leads to increased fracture risk due to decreased strength of cortical and trabecular bone. Even with state-of-the-art non-invasive assessment methods there is still a high underdiagnosis rate. Quantitative computed tomography (QCT) permits the selective analysis of cortical bone, however the low spatial resolution of clinical QCT leads to an overestimation of the thickness of cortical bone (<a class="link-external link-http" href="http://Ct.Th" rel="external noopener nofollow">this http URL</a>) and bone strength. <br>We propose a novel, model based, fully automatic image analysis method that allows accurate spatial modeling of the thickness distribution of cortical bone from clinical QCT. In an analysis-by-synthesis (AbS) fashion a stochastic scan is synthesized from a probabilistic bone model, the optimal model parameters are estimated using a maximum a-posteriori approach. By exploiting the different characteristics of in-plane and out-of-plane point spread functions of CT scanners the proposed method is able assess the spatial distribution of cortical thickness. <br>The method was evaluated on eleven cadaveric human vertebrae, scanned by clinical QCT and analyzed using standard methods and AbS, both compared to high resolution peripheral QCT (HR-pQCT) as gold standard. While standard QCT based measurements overestimated <a class="link-external link-http" href="http://Ct.Th" rel="external noopener nofollow">this http URL</a>. by 560% and did not show significant correlation with the gold standard ($r^2 = 0.20,\, p = 0.169$) the proposed method eliminated the overestimation and showed a significant tight correlation with the gold standard ($r^2 = 0.98,\, p &lt; 0.0001$) a root mean square error below 10%.      
### 14.On necessary conditions of tracking control for nonlinear systems via contraction analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.08662.pdf)
>  In this paper we address the problem of tracking control of nonlinear systems via contraction analysis. The necessary conditions of the systems which can achieve universal asymptotic tracking are studied under several different cases. We show the links to the well developed control contraction metric, as well as its invariance under dynamic extension. In terms of these conditions, we identify a differentially detectable output, based on which a simple differential controller for trajectory tracking is designed via damping injection. As illustration we apply to electrostatic microactuators.      
### 15.X-DC: Explainable Deep Clustering based on Learnable Spectrogram Templates  [ :arrow_down: ](https://arxiv.org/pdf/2009.08661.pdf)
>  Deep neural networks (DNNs) have achieved substantial predictive performance in various speech processing tasks. Particularly, it has been shown that a monaural speech separation task can be successfully solved with a DNN-based method called deep clustering (DC), which uses a DNN to describe the process of assigning a continuous vector to each time-frequency (TF) bin and measure how likely each pair of TF bins is to be dominated by the same speaker. In DC, the DNN is trained so that the embedding vectors for the TF bins dominated by the same speaker are forced to get close to each other. One concern regarding DC is that the embedding process described by a DNN has a black-box structure, which is usually very hard to interpret. The potential weakness owing to the non-interpretable black-box structure is that it lacks the flexibility of addressing the mismatch between training and test conditions (caused by reverberation, for instance). To overcome this limitation, in this paper, we propose the concept of explainable deep clustering (X-DC), whose network architecture can be interpreted as a process of fitting learnable spectrogram templates to an input spectrogram followed by Wiener filtering. During training, the elements of the spectrogram templates and their activations are constrained to be non-negative, which facilitates the sparsity of their values and thus improves interpretability. The main advantage of this framework is that it naturally allows us to incorporate a model adaptation mechanism into the network thanks to its physically interpretable structure. We experimentally show that the proposed X-DC enables us to visualize and understand the clues for the model to determine the embedding vectors while achieving speech separation performance comparable to that of the original DC models.      
### 16.Single Image Super-Resolution of Noisy 3D Dental CT Images Using Tucker Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2009.08657.pdf)
>  Tensor decomposition has proven to be a strong tool in various 3D image processing tasks such as denoising and super-resolution. In this context, we recently proposed a canonical polyadic decomposition (CPD) based algorithm for single image super-resolution (SISR). The algorithm has shown to be an order of magnitude faster than popular optimization-based techniques. In this work, we investigated the added value brought by Tucker decomposition. While CPD allows a joint implementation of the denoising and deconvolution steps of the SISR model, with Tucker decomposition the denoising is realized first, followed by deconvolution. This way the ill-posedness of the deconvolution caused by noise is partially mitigated. The results achieved using the two different tensor decomposition techniques were compared, and the robustness against noise was investigated. For validation, we used dental images. The superiority of the proposed method is shown in terms of peak signal-to-ratio, structural similarity index, the canal segmentation accuracy, and runtime.      
### 17.Bucket of deep transfer learning features and classification models for melanoma detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.08639.pdf)
>  Malignant melanoma is the deadliest form of skin cancer and, in recent years, is rapidly growing in terms of the incidence worldwide rate. The most effective approach to targeted treatment is early diagnosis. Deep learning algorithms, specifically convolutional neural networks, represent a methodology for the image analysis and representation. They optimize the features design task, essential for an automatic approach on different types of images, including medical. In this paper, we adopted pretrained deep convolutional neural networks architectures for the image representation with purpose to predict skin lesion melanoma. Firstly, we applied a transfer learning approach to extract image features. Secondly, we adopted the transferred learning features inside an ensemble classification context. Specifically, the framework trains individual classifiers on balanced subspaces and combines the provided predictions through statistical measures. Experimental phase on datasets of skin lesion images is performed and results obtained show the effectiveness of the proposed approach with respect to state-of-the-art competitors.      
### 18.Hardware Accelerator for Multi-Head Attention and Position-Wise Feed-Forward in the Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2009.08605.pdf)
>  Designing hardware accelerators for deep neural networks (DNNs) has been much desired. Nonetheless, most of these existing accelerators are built for either convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Recently, the Transformer model is replacing the RNN in the natural language processing (NLP) area. However, because of intensive matrix computations and complicated data flow being involved, the hardware design for the Transformer model has never been reported. In this paper, we propose the first hardware accelerator for two key components, i.e., the multi-head attention (MHA) ResBlock and the position-wise feed-forward network (FFN) ResBlock, which are the two most complex layers in the Transformer. Firstly, an efficient method is introduced to partition the huge matrices in the Transformer, allowing the two ResBlocks to share most of the hardware resources. Secondly, the computation flow is well designed to ensure the high hardware utilization of the systolic array, which is the biggest module in our design. Thirdly, complicated nonlinear functions are highly optimized to further reduce the hardware complexity and also the latency of the entire system. Our design is coded using hardware description language (HDL) and evaluated on a Xilinx FPGA. Compared with the implementation on GPU with the same setting, the proposed design demonstrates a speed-up of 14.6x in the MHA ResBlock, and 3.4x in the FFN ResBlock, respectively. Therefore, this work lays a good foundation for building efficient hardware accelerators for multiple Transformer networks.      
### 19.Sparsity-Aware SSAF Algorithm with Individual Weighting Factors for Acoustic Echo Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2009.08593.pdf)
>  In this paper, we propose and analyze the sparsity-aware sign subband adaptive filtering with individual weighting factors (S-IWF-SSAF) algorithm, and consider its application in acoustic echo cancellation (AEC). Furthermore, we design a joint optimization scheme of the step-size and the sparsity penalty parameter to enhance the S-IWF-SSAF performance in terms of convergence rate and steady-state error. A theoretical analysis shows that the S-IWF-SSAF algorithm outperforms the previous sign subband adaptive filtering with individual weighting factors (IWF-SSAF) algorithm in sparse scenarios. In particular, compared with the existing analysis on the IWF-SSAF algorithm, the proposed analysis does not require the assumptions of large number of subbands, long adaptive filter, and paraunitary analysis filter bank, and matches well the simulated results. Simulations in both system identification and AEC situations have demonstrated our theoretical analysis and the effectiveness of the proposed algorithms.      
### 20.SCREENet: A Multi-view Deep Convolutional Neural Network for Classification of High-resolution Synthetic Mammographic Screening Scans  [ :arrow_down: ](https://arxiv.org/pdf/2009.08563.pdf)
>  Purpose: To develop and evaluate the accuracy of a multi-view deep learning approach to the analysis of high-resolution synthetic mammograms from digital breast tomosynthesis screening cases, and to assess the effect on accuracy of image resolution and training set size. Materials and Methods: In a retrospective study, 21,264 screening digital breast tomosynthesis (DBT) exams obtained at our institution were collected along with associated radiology reports. The 2D synthetic mammographic images from these exams, with varying resolutions and data set sizes, were used to train a multi-view deep convolutional neural network (MV-CNN) to classify screening images into BI-RADS classes (BI-RADS 0, 1 and 2) before evaluation on a held-out set of exams. <br>Results: Area under the receiver operating characteristic curve (AUC) for BI-RADS 0 vs non-BI-RADS 0 class was 0.912 for the MV-CNN trained on the full dataset. The model obtained accuracy of 84.8%, recall of 95.9% and precision of 95.0%. This AUC value decreased when the same model was trained with 50% and 25% of images (AUC = 0.877, P=0.010 and 0.834, P=0.009 respectively). Also, the performance dropped when the same model was trained using images that were under-sampled by 1/2 and 1/4 (AUC = 0.870, P=0.011 and 0.813, P=0.009 respectively). <br>Conclusion: This deep learning model classified high-resolution synthetic mammography scans into normal vs needing further workup using tens of thousands of high-resolution images. Smaller training data sets and lower resolution images both caused significant decrease in performance.      
### 21.Asymptotic Analysis of ADMM for Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2009.08545.pdf)
>  In this paper, we analyze the asymptotic behavior of alternating direction method of multipliers (ADMM) for compressed sensing, where we reconstruct an unknown structured signal from its underdetermined linear measurements. The analytical tool used in this paper is recently developed convex Gaussian min-max theorem (CGMT), which can be applied to various convex optimization problems to obtain its asymptotic error performance. In our analysis of ADMM, we analyze the convex subproblem in the update of ADMM and characterize the asymptotic distribution of the tentative estimate obtained at each iteration. The result shows that the update equations in ADMM can be decoupled into a scalar-valued stochastic process in the asymptotic regime with the large system limit. From the asymptotic result, we can predict the evolution of the error (e.g. mean-square-error (MSE) and symbol error rate (SER)) in ADMM for large-scale compressed sensing problems. Simulation results show that the empirical performance of ADMM and its theoretical prediction are close to each other in sparse vector reconstruction and binary vector reconstruction.      
### 22.Spatio-Temporal Probabilistic Voltage Sensitivity Analysis (ST-PVSA)-A Novel Framework for Hosting Capacity Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2009.08490.pdf)
>  Smart grids are envisioned to accommodate high penetration of distributed photovoltaic (PV) generation, which may cause adverse grid impacts in terms of voltage violations. Therefore, PV Hosting capacity (HC) is being used as a planning tool to determine the maximum PV installation capacity that causes the first voltage violation and above which would require infrastructure upgrades. Traditional methods of HC analysis are computationally complex as they are based on iterative load flow algorithms that require investigation of a large number of scenarios for accurate assessment of PV impacts. This paper first presents a computationally efficient analytical approach to compute the probability distribution of voltage change at a particular node due to random behavior of randomly located multiple distributed PVs. Next, the derived distribution is used to identify voltage violations for various PV penetration levels and subsequently determine the HC of the system without the need to examine multiple scenarios. Results from the proposed spatio-temporal probabilistic voltage sensitivity analysis and the HC are validated via conventional load flow based simulation approach on the IEEE 37 and IEEE 123 node test systems.      
### 23.Hierarchical Multi-Grained Generative Model for Expressive Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2009.08474.pdf)
>  This paper proposes a hierarchical generative model with a multi-grained latent variable to synthesize expressive speech. In recent years, fine-grained latent variables are introduced into the text-to-speech synthesis that enable the fine control of the prosody and speaking styles of synthesized speech. However, the naturalness of speech degrades when these latent variables are obtained by sampling from the standard Gaussian prior. To solve this problem, we propose a novel framework for modeling the fine-grained latent variables, considering the dependence on an input text, a hierarchical linguistic structure, and a temporal structure of latent variables. This framework consists of a multi-grained variational autoencoder, a conditional prior, and a multi-level auto-regressive latent converter to obtain the different time-resolution latent variables and sample the finer-level latent variables from the coarser-level ones by taking into account the input text. Experimental results indicate an appropriate method of sampling fine-grained latent variables without the reference signal at the synthesis stage. Our proposed framework also provides the controllability of speaking style in an entire utterance.      
### 24.GRAC: Self-Guided and Self-Regularized Actor-Critic  [ :arrow_down: ](https://arxiv.org/pdf/2009.08973.pdf)
>  Deep reinforcement learning (DRL) algorithms have successfully been demonstrated on a range of challenging decision making and control tasks. One dominant component of recent deep reinforcement learning algorithms is the target network which mitigates the divergence when learning the Q function. However, target networks can slow down the learning process due to delayed function updates. Another dominant component especially in continuous domains is the policy gradient method which models and optimizes the policy directly. However, when Q functions are approximated with neural networks, their landscapes can be complex and therefore mislead the local gradient. In this work, we propose a self-regularized and self-guided actor-critic method. We introduce a self-regularization term within the TD-error minimization and remove the need for the target network. In addition, we propose a self-guided policy improvement method by combining policy-gradient with zero-order optimization such as the Cross Entropy Method. It helps to search for actions associated with higher Q-values in a broad neighborhood and is robust to local noise in the Q function approximation. These actions help to guide the updates of our actor network. We evaluate our method on the suite of OpenAI gym tasks, achieving or outperforming state of the art in every environment tested.      
### 25.Unitary Learning for Deep Diffractive Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2009.08935.pdf)
>  Realization of deep learning with coherent diffraction has achieved remarkable development nowadays, which benefits on the fact that matrix multiplication can be optically executed in parallel as well as with little power consumption. Coherent optical field propagated in the form of complex-value entity can be manipulated into a task-oriented output with statistical inference. In this paper, we present a unitary learning protocol on deep diffractive neural network, meeting the physical unitary prior in coherent diffraction. Unitary learning is a backpropagation serving to unitary weights update through the gradient translation between Euclidean and Riemannian space. The temporal-space evolution characteristic in unitary learning is formulated and elucidated. Particularly a compatible condition on how to select the nonlinear activations in complex space is unveiled, encapsulating the fundamental sigmoid, tanh and quasi-ReLu in complex space. As a preliminary application, deep diffractive neural network with unitary learning is tentatively implemented on the 2D classification and verification tasks.      
### 26.Optimizing Speech Emotion Recognition using Manta-Ray Based Feature Selection  [ :arrow_down: ](https://arxiv.org/pdf/2009.08909.pdf)
>  Emotion recognition from audio signals has been regarded as a challenging task in signal processing as it can be considered as a collection of static and dynamic classification tasks. Recognition of emotions from speech data has been heavily relied upon end-to-end feature extraction and classification using machine learning models, though the absence of feature selection and optimization have restrained the performance of these methods. Recent studies have shown that Mel Frequency Cepstral Coefficients (MFCC) have been emerged as one of the most relied feature extraction methods, though it circumscribes the accuracy of classification with a very small feature dimension. In this paper, we propose that the concatenation of features, extracted by using different existing feature extraction methods can not only boost the classification accuracy but also expands the possibility of efficient feature selection. We have used Linear Predictive Coding (LPC) apart from the MFCC feature extraction method, before feature merging. Besides, we have performed a novel application of Manta Ray optimization in speech emotion recognition tasks that resulted in a state-of-the-art result in this field. We have evaluated the performance of our model using SAVEE and Emo-DB, two publicly available datasets. Our proposed method outperformed all the existing methods in speech emotion analysis and resulted in a decent result in these two datasets with a classification accuracy of 97.06% and 97.68% respectively.      
### 27.IDA: Improved Data Augmentation Applied to Salient Object Detection  [ :arrow_down: ](https://arxiv.org/pdf/2009.08845.pdf)
>  In this paper, we present an Improved Data Augmentation (IDA) technique focused on Salient Object Detection (SOD). Standard data augmentation techniques proposed in the literature, such as image cropping, rotation, flipping, and resizing, only generate variations of the existing examples, providing a limited generalization. Our method combines image inpainting, affine transformations, and the linear combination of different generated background images with salient objects extracted from labeled data. Our proposed technique enables more precise control of the object's position and size while preserving background information. The background choice is based on an inter-image optimization, while object size follows a uniform random distribution within a specified interval, and the object position is intra-image optimal. We show that our method improves the segmentation quality when used for training state-of-the-art neural networks on several famous datasets of the SOD field. Combining our method with others surpasses traditional techniques such as horizontal-flip in 0.52% for F-measure and 1.19% for Precision. We also provide an evaluation in 7 different SOD datasets, with 9 distinct evaluation metrics and an average ranking of the evaluated methods.      
### 28.Cough Against COVID: Evidence of COVID-19 Signature in Cough Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2009.08790.pdf)
>  Testing capacity for COVID-19 remains a challenge globally due to the lack of adequate supplies, trained personnel, and sample-processing equipment. These problems are even more acute in rural and underdeveloped regions. We demonstrate that solicited-cough sounds collected over a phone, when analysed by our AI model, have statistically significant signal indicative of COVID-19 status (AUC 0.72, t-test,p &lt;0.01,95% CI 0.61-0.83). This holds true for asymptomatic patients as well. Towards this, we collect the largest known(to date) dataset of microbiologically confirmed COVID-19 cough sounds from 3,621 individuals. When used in a triaging step within an overall testing protocol, by enabling risk-stratification of individuals before confirmatory tests, our tool can increase the testing capacity of a healthcare system by 43% at disease prevalence of 5%, without additional supplies, trained personnel, or physical infrastructure      
### 29.The basins of attraction of the global minimizers of non-convex inverse problems with low-dimensional models in infinite dimension  [ :arrow_down: ](https://arxiv.org/pdf/2009.08670.pdf)
>  Non-convex methods for linear inverse problems with low-dimensional models have emerged as an alternative to convex techniques. We propose a theoretical framework where both finite dimensional and infinite dimensional linear inverse problems can be studied. We show how the size of the the basins of attraction of the minimizers of such problems is linked with the number of available measurements. This framework recovers known results about low-rank matrix estimation and off-the-grid sparse spike estimation, and it provides new results for Gaussian mixture estimation from linear measurements. keywords: low-dimensional models, non-convex methods, low-rank matrix recovery, off-the-grid sparse recovery, Gaussian mixture model estimation from linear measurements.      
### 30.Low Density Parity Check Code (LDPC Codes) Overview  [ :arrow_down: ](https://arxiv.org/pdf/2009.08645.pdf)
>  This paper basically expresses the core fundamentals and brief overview of the research of R. G. GALLAGER [1] on Low-Density Parity-Check (LDPC) codes and various parameters related to LDPC codes like, encoding and decoding of LDPC codes, code rate, parity check matrix, tanner graph. We also discuss advantages and applications as well as the usage of LDPC codes in 5G technology. We have simulated encoding and decoding of LDPC codes and have acquired results in terms of BER vs SNR graph in MATLAB software. This report was submitted as an assignment in Nirma University      
### 31.Decentralized Game-Theoretic Control for Dynamic Task Allocation Problems for Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2009.08628.pdf)
>  We propose a decentralized game-theoretic framework for dynamic task allocation problems for multi-agent systems. In our problem formulation, the agents' utilities depend on both the rewards and the costs associated with the successful completion of the tasks assigned to them. The rewards reflect how likely is for the agents to accomplish their assigned tasks whereas the costs reflect the effort needed to complete these tasks (this effort is determined by the solution of corresponding optimal control problems). The task allocation problem considered herein corresponds to a dynamic game whose solution depends on the states of the agents in contrast with classic static (or single-act) game formulations. We propose a greedy solution approach in which the agents negotiate with each other to find a mutually agreeable (or individually rational) task assignment profile based on evaluations of the task utilities that reflect their current states. We illustrate the main ideas of this work by means of extensive numerical simulations.      
### 32.Identification of Abnormal States in Videos of Ants Undergoing Social Phase Change  [ :arrow_down: ](https://arxiv.org/pdf/2009.08626.pdf)
>  Biology is both an important application area and a source of motivation for development of advanced machine learning techniques. Although much attention has been paid to large and complex data sets resulting from high-throughput sequencing, advances in high-quality video recording technology have begun to generate similarly rich data sets requiring sophisticated techniques from both computer vision and time-series analysis. Moreover, just as studying gene expression patterns in one organism can reveal general principles that apply to other organisms, the study of complex social interactions in an experimentally tractable model system, such as a laboratory ant colony, can provide general principles about the dynamics many other social groups. Here, we focus on one such example from the study of reproductive regulation in small laboratory colonies of $\sim$50 Harpgenathos ants. These ants can be artificially induced to begin a $\sim$20 day process of hierarchy reformation. Although the conclusion of this process is conspicuous to a human observer, it is still unclear which behaviors during the transients are contributing to the process. To address this issue, we explore the potential application of One-class Classification (OC) to the detection of abnormal states in ant colonies for which behavioral data is only available for the normal societal conditions during training. Specifically, we build upon the Deep Support Vector Data Description (DSVDD) and introduce the Inner-Outlier Generator (IO-GEN) that synthesizes fake "inner outlier" observations during training that are near the center of the DSVDD data description. We show that IO-GEN increases the reliability of the final OC classifier relative to other DSVDD baselines. This method can be used to screen video frames for which additional human observation is needed.      
### 33.Observers Design for Inertial Navigation Systems: A Brief Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2009.08569.pdf)
>  The design of navigation observers able to simultaneously estimate the position, linear velocity and orientation of a vehicle in a three-dimensional space is crucial in many robotics and aerospace applications. This problem was mainly dealt with using the extended Kalman filter and its variants which proved to be instrumental in many practical applications. Although practically efficient, the lack of strong stability guarantees of these algorithms motivated the emergence of a new class of geometric navigation observers relying on Riemannian geometry tools, leading to provable strong stability properties. The objective of this brief tutorial is to provide an overview of the existing estimation schemes, as well as some recently developed geometric nonlinear observers, for autonomous navigation systems relying on inertial measurement unit (IMU) and landmark measurements.      
### 34.SREC: Proactive Self-Remedy of Energy-Constrained UAV-Based Networks via Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2009.08528.pdf)
>  Energy-aware control for multiple unmanned aerial vehicles (UAVs) is one of the major research interests in UAV based networking. Yet few existing works have focused on how the network should react around the timing when the UAV lineup is changed. In this work, we study proactive self-remedy of energy-constrained UAV networks when one or more UAVs are short of energy and about to quit for charging. We target at an energy-aware optimal UAV control policy which proactively relocates the UAVs when any UAV is about to quit the network, rather than passively dispatches the remaining UAVs after the quit. Specifically, a deep reinforcement learning (DRL)-based self remedy approach, named SREC-DRL, is proposed to maximize the accumulated user satisfaction scores for a certain period within which at least one UAV will quit the network. To handle the continuous state and action space in the problem, the state-of-the-art algorithm of the actor-critic DRL, i.e., deep deterministic policy gradient (DDPG), is applied with better convergence stability. Numerical results demonstrate that compared with the passive reaction method, the proposed SREC-DRL approach shows a $12.12\%$ gain in accumulative user satisfaction score during the remedy period.      
### 35.Smartphone Camera De-identification while Preserving Biometric Utility  [ :arrow_down: ](https://arxiv.org/pdf/2009.08511.pdf)
>  The principle of Photo Response Non Uniformity (PRNU) is often exploited to deduce the identity of the smartphone device whose camera or sensor was used to acquire a certain image. In this work, we design an algorithm that perturbs a face image acquired using a smartphone camera such that (a) sensor-specific details pertaining to the smartphone camera are suppressed (sensor anonymization); (b) the sensor pattern of a different device is incorporated (sensor spoofing); and (c) biometric matching using the perturbed image is not affected (biometric utility). We employ a simple approach utilizing Discrete Cosine Transform to achieve the aforementioned objectives. Experiments conducted on the MICHE-I and OULU-NPU datasets, which contain periocular and facial data acquired using 12 smartphone cameras, demonstrate the efficacy of the proposed de-identification algorithm on three different PRNU-based sensor identification schemes. This work has application in sensor forensics and personal privacy.      
### 36.Automatic deep learning for trend prediction in time series data  [ :arrow_down: ](https://arxiv.org/pdf/2009.08510.pdf)
>  Recently, Deep Neural Network (DNN) algorithms have been explored for predicting trends in time series data. In many real world applications, time series data are captured from dynamic systems. DNN models must provide stable performance when they are updated and retrained as new observations becomes available. In this work we explore the use of automatic machine learning techniques to automate the algorithm selection and hyperparameter optimisation process for trend prediction. We demonstrate how a recent AutoML tool, specifically the HpBandSter framework, can be effectively used to automate DNN model development. Our AutoML experiments found optimal configurations that produced models that compared well against the average performance and stability levels of configurations found during the manual experiments across four data sets.      
### 37.Separating physically distinct mechanisms in complex infrared plasmonic nanostructures via machine learning enhanced electron energy loss spectroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2009.08501.pdf)
>  Low-loss electron energy loss spectroscopy (EELS) has emerged as a technique of choice for exploring the localization of plasmonic phenomena at the nanometer level, necessitating analysis of physical behaviors from 3D spectral data sets. For systems with high localization, linear unmixing methods provide an excellent basis for exploratory analysis, while in more complex systems large numbers of components are needed to accurately capture the true plasmonic response and the physical interpretability of the components becomes uncertain. Here, we explore machine learning based analysis of low-loss EELS data on heterogeneous self-assembled monolayer films of doped-semiconductor nanoparticles, which support infrared resonances. We propose a pathway for supervised analysis of EELS datasets that separate and classify regions of the films with physically distinct spectral responses. The classifications are shown to be robust, to accurately capture the common spatiospectral tropes of the complex nanostructures, and to be transferable between different datasets to allow high-throughput analysis of large areas of the sample. As such, it can be used as a basis for automated experiment workflows based on Bayesian optimization, as demonstrated on the ex situ data. We further demonstrate the use of non-linear autoencoders (AE) combined with clustering in the latent space of the AE yields highly reduced representations of the system response that yield insight into the relevant physics that do not depend on operator input and bias. The combination of these supervised and unsupervised tools provides complementary insight into the nanoscale plasmonic phenomena.      
