# ArXiv eess --Wed, 12 Aug 2020
### 1.Classification of Radio Signals Using Truncated Gaussian Discriminant Analysis of Convolutional Neural Network-Derived Features  [ :arrow_down: ](https://arxiv.org/pdf/2008.04874.pdf)
>  To improve the utility and scalability of distributed radio frequency (RF) sensor and communication networks, reduce the need for convolutional neural network (CNN) retraining, and efficiently share learned information about signals, we examined a supervised bootstrapping approach for RF modulation classification. We show that CNN-bootstrapped features of new and existing modulation classes can be considered as mixtures of truncated Gaussian distributions, allowing for maximumlikelihood-based classification of new classes without retraining the network. In this work, the authors observed classification performance using maximum likelihood estimation of CNNbootstrapped features to be comparable to that of a CNN trained on all classes, even for those classes on which the bootstrapping CNN was not trained. This performance was achieved while reducing the number of parameters needed for new class definition from over 8 million to only 200. Furthermore, some physical features of interest, not directly labeled during training, e.g. signal-to-noise ratio (SNR), can be learned or estimated from these same CNN-derived features. Finally, we show that SNR estimation accuracy is highest when classification accuracy is lowest and therefore can be used to calibrate a confidence in the classification.      
### 2.TextureWGAN: Texture Preserving WGAN with MLE Regularizer for Inverse Problems  [ :arrow_down: ](https://arxiv.org/pdf/2008.04861.pdf)
>  Many algorithms and methods have been proposed for inverse problems particularly with the recent surge of interest in machine learning and deep learning methods. Among all proposed methods, the most popular and effective method is the convolutional neural network (CNN) with mean square error (MSE). This method has been proven effective in super-resolution, image de-noising, and image reconstruction. However, this method is known to over-smooth images due to the nature of MSE. MSE based methods minimize Euclidean distance for all pixels between a baseline image and a generated image by CNN and ignore the spatial information of the pixels such as image texture. In this paper, we proposed a new method based on Wasserstein GAN (WGAN) for inverse problems. We showed that the WGAN-based method was effective to preserve image texture. It also used a maximum likelihood estimation (MLE) regularizer to preserve pixel fidelity. Maintaining image texture and pixel fidelity is the most important requirement for medical imaging. We used Peak Signal to Noise Ratio (PSNR) and Structure Similarity (SSIM) to evaluate the proposed method quantitatively. We also conducted first-order and second-order statistical image texture analysis to assess image texture.      
### 3.An Anytime Algorithm for Reachability on Uncountable MDP  [ :arrow_down: ](https://arxiv.org/pdf/2008.04824.pdf)
>  We provide an algorithm for reachability on Markov decision processes with uncountable state and action spaces, which, under mild assumptions, approximates the optimal value to any desired precision. It is the first such anytime algorithm, meaning that at any point in time it can return the current approximation with its precision. Moreover, it simultaneously is the first algorithm able to utilize \emph{learning} approaches without sacrificing guarantees and it further allows for combination with existing heuristics.      
### 4.A Review on Deep Learning Techniques for the Diagnosis of Novel Coronavirus (COVID-19)  [ :arrow_down: ](https://arxiv.org/pdf/2008.04815.pdf)
>  Novel coronavirus (COVID-19) outbreak, has raised a calamitous situation all over the world and has become one of the most acute and severe ailments in the past hundred years. The prevalence rate of COVID-19 is rapidly rising every day throughout the globe. Although no vaccines for this pandemic have been discovered yet, deep learning techniques proved themselves to be a powerful tool in the arsenal used by clinicians for the automatic diagnosis of COVID-19. This paper aims to overview the recently developed systems based on deep learning techniques using different medical imaging modalities like Computer Tomography (CT) and X-ray. This review specifically discusses the systems developed for COVID-19 diagnosis using deep learning techniques and provides insights on well-known data sets used to train these networks. It also highlights the data partitioning techniques and various performance measures developed by researchers in this field. A taxonomy is drawn to categorize the recent works for proper insight. Finally, we conclude by addressing the challenges associated with the use of deep learning methods for COVID-19 detection and probable future trends in this research area. This paper is intended to provide experts (medical or otherwise) and technicians with new insights into the ways deep learning techniques are used in this regard and how they potentially further works in combatting the outbreak of COVID-19.      
### 5.Full and Reduced Order Observers for Image-based Depth Estimation using Concurrent Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.04809.pdf)
>  In this paper concurrent learning (CL)-based full and reduced order observers for a perspective dynamical system (PDS) are developed. The PDS is a widely used model for estimating the depth of a feature point from a sequence of camera images. Building on the current progress of CL for parameter estimation in adaptive control, a state observer is developed for the PDS model where the inverse depth appears as a time-varying parameter in the dynamics. The data recorded over a sliding time window in the near past is used in the CL term to design the full and the reduced order state observers. A Lyapunov-based stability analysis is carried out to prove the uniformly ultimately bounded (UUB) stability of the developed observers. Simulation results are presented to validate the accuracy and convergence of the developed observers in terms of convergence time, root mean square error (RMSE) and mean absolute percentage error (MAPE) metrics. Real world depth estimation experiments are performed to demonstrate the performance of the observers using aforementioned metrics on a 7-DoF manipulator with an eye-in-hand configuration.      
### 6.3D FLAT: Feasible Learned Acquisition Trajectories for Accelerated MRI  [ :arrow_down: ](https://arxiv.org/pdf/2008.04808.pdf)
>  Magnetic Resonance Imaging (MRI) has long been considered to be among the gold standards of today's diagnostic imaging. The most significant drawback of MRI is long acquisition times, prohibiting its use in standard practice for some applications. Compressed sensing (CS) proposes to subsample the k-space (the Fourier domain dual to the physical space of spatial coordinates) leading to significantly accelerated acquisition. However, the benefit of compressed sensing has not been fully exploited; most of the sampling densities obtained through CS do not produce a trajectory that obeys the stringent constraints of the MRI machine imposed in practice. Inspired by recent success of deep learning based approaches for image reconstruction and ideas from computational imaging on learning-based design of imaging systems, we introduce 3D FLAT, a novel protocol for data-driven design of 3D non-Cartesian accelerated trajectories in MRI. Our proposal leverages the entire 3D k-space to simultaneously learn a physically feasible acquisition trajectory with a reconstruction method. Experimental results, performed as a proof-of-concept, suggest that 3D FLAT achieves higher image quality for a given readout time compared to standard trajectories such as radial, stack-of-stars, or 2D learned trajectories (trajectories that evolve only in the 2D plane while fully sampling along the third dimension). Furthermore, we demonstrate evidence supporting the significant benefit of performing MRI acquisitions using non-Cartesian 3D trajectories over 2D non-Cartesian trajectories acquired slice-wise.      
### 7.Artificial Intelligence to Assist in Exclusion of Coronary Atherosclerosis during CCTA Evaluation of Chest-Pain in the Emergency Department: Preparing an Application for Real-World Use  [ :arrow_down: ](https://arxiv.org/pdf/2008.04802.pdf)
>  Coronary Computed Tomography Angiography (CCTA) evaluation of chest-pain patients in an Emergency Department (ED) is considered appropriate. While a negative CCTA interpretation supports direct patient discharge from an ED, labor-intensive analyses are required, with accuracy in jeopardy from distractions. We describe the development of an Artificial Intelligence (AI) algorithm and workflow for assisting interpreting physicians in CCTA screening for the absence of coronary atherosclerosis. The two-phase approach consisted of (1) Phase 1 - focused on the development and preliminary testing of an algorithm for vessel-centerline extraction classification in a balanced study population (n = 500 with 50% disease prevalence) derived by retrospective random case selection; and (2) Phase 2 - concerned with simulated-clinical Trialing of the developed algorithm on a per-case basis in a more real-world study population (n = 100 with 28% disease prevalence) from an ED chest-pain series. This allowed pre-deployment evaluation of the AI-based CCTA screening application which provides a vessel-by-vessel graphic display of algorithm inference results integrated into a clinically capable viewer. Algorithm performance evaluation used Area Under the Receiver-Operating-Characteristic Curve (AUC-ROC); confusion matrices reflected ground-truth vs AI determinations. The vessel-based algorithm demonstrated strong performance with AUC-ROC = 0.96. In both Phase 1 and Phase 2, independent of disease prevalence differences, negative predictive values at the case level were very high at 95%. The rate of completion of the algorithm workflow process (96% with inference results in 55-80 seconds) in Phase 2 depended on adequate image quality. There is potential for this AI application to assist in CCTA interpretation to help extricate atherosclerosis from chest-pain presentations.      
### 8.ARX Model Identification using Generalized Spectral Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2008.04779.pdf)
>  This article is concerned with the identification of autoregressive with exogenous inputs (ARX) models. Most of the existing approaches like prediction error minimization and state-space framework are widely accepted and utilized for the estimation of ARX models but are known to deliver unbiased and consistent parameter estimates for a correctly supplied guess of input-output orders and delay. <br>In this paper, we propose a novel automated framework which recovers orders, delay, output noise distribution along with parameter estimates. The primary tool utilized in the proposed framework is generalized spectral decomposition. The proposed algorithm systematically estimates all the parameters in two steps. The first step utilizes estimates of the order by examining the generalized eigenvalues, and the second step estimates the parameter from the generalized eigenvectors. Simulation studies are presented to demonstrate the efficacy of the proposed method and are observed to deliver consistent estimates even at low signal to noise ratio (SNR).      
### 9.Constrained Active Classification Using Partially Observable Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2008.04768.pdf)
>  In this work, we study the problem of actively classifying the attributes of dynamical systems characterized as a finite set of Markov decision process (MDP) models. We are interested in finding strategies that actively interact with the dynamical system and observe its reactions so that the attribute of interest is classified efficiently with high confidence. We present a decision-theoretic framework based on partially observable Markov decision processes (POMDPs). The proposed framework relies on assigning a classification belief (a probability distribution) to the attributes of interest. Given an initial belief, confidence level over which a classification decision can be made, a cost bound, safe belief sets, and a finite time horizon, we compute POMDP strategies leading to classification decisions. We present two different algorithms to compute such strategies. The first algorithm computes the optimal strategy exactly by value iteration. To overcome the computational complexity of computing the exact solutions, we propose a second algorithm is based on adaptive sampling to approximate the optimal probability of reaching a classification decision. We illustrate the proposed methodology using examples from medical diagnosis and privacy-preserving advertising.      
### 10.Channel Estimation for Intelligent Reflecting Surface Assisted MIMO Systems: A Tensor Modeling Approach  [ :arrow_down: ](https://arxiv.org/pdf/2008.04766.pdf)
>  Intelligent reflecting surface (IRS) is an emerging technology for future wireless communications including 5G and especially 6G. It consists of a large 2D array of (semi-)passive scattering elements that control the electromagnetic properties of radio-frequency waves so that the reflected signals add coherently at the intended receiver or destructively to reduce co-channel interference. The promised gains of IRS-assisted communications depend on the accuracy of the channel state information. In this paper, we address the receiver design for an IRS-assisted multiple-input multiple-output (MIMO) communication system via a tensor modeling approach aiming at the channel estimation problem using supervised (pilot-assisted) methods. Considering a structured time-domain pattern of pilots and IRS phase shifts, we present two channel estimation methods that rely on a parallel factor (PARAFAC) tensor modeling of the received signals. The first one has a closed-form solution by solving rank-1 matrix approximation problems, while the second one is based on an alternating estimation scheme. Uniqueness issues are discussed and design requirements that guide the choice of the system parameters as well as the structures of the pilot signals and IRS phase shifts are discussed. A performance analysis is also carried out by means of the CramÃ©r-Rao lower bound. Numerical results show the effectiveness of the proposed receivers and highlight the involved tradeoffs, while corroborating their superior performance compared to competing solutions.      
### 11.AtrialJSQnet: A New Framework for Joint Segmentation and Quantification of Left Atrium and Scars Incorporating Spatial and Shape Information  [ :arrow_down: ](https://arxiv.org/pdf/2008.04729.pdf)
>  Left atrial (LA) and atrial scar segmentation from late gadolinium enhanced magnetic resonance imaging (LGE MRI) is an important task in clinical practice. %, to guide ablation therapy and predict treatment results for atrial fibrillation (AF) patients. The automatic segmentation is however still challenging, due to the poor image quality, the various LA shapes, the thin wall, and the surrounding enhanced regions. Previous methods normally solved the two tasks independently and ignored the intrinsic spatial relationship between LA and scars. In this work, we develop a new framework, namely AtrialJSQnet, where LA segmentation, scar projection onto the LA surface, and scar quantification are performed simultaneously in an end-to-end style. We propose a mechanism of shape attention (SA) via an explicit surface projection, to utilize the inherent correlation between LA and LA scars. In specific, the SA scheme is embedded into a multi-task architecture to perform joint LA segmentation and scar quantification. Besides, a spatial encoding (SE) loss is introduced to incorporate continuous spatial information of the target, in order to reduce noisy patches in the predicted segmentation. We evaluated the proposed framework on 60 LGE MRIs from the MICCAI2018 LA challenge. Extensive experiments on a public dataset demonstrated the effect of the proposed AtrialJSQnet, which achieved competitive performance over the state-of-the-art. The relatedness between LA segmentation and scar quantification was explicitly explored and has shown significant performance improvements for both tasks. The code and results will be released publicly once the manuscript is accepted for publication via <a class="link-external link-https" href="https://zmiclab.github.io/projects.html" rel="external noopener nofollow">this https URL</a>.      
### 12.Learning Event-triggered Control from Data through Joint Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.04712.pdf)
>  We present a framework for model-free learning of event-triggered control strategies. Event-triggered methods aim to achieve high control performance while only closing the feedback loop when needed. This enables resource savings, e.g., network bandwidth if control commands are sent via communication networks, as in networked control systems. Event-triggered controllers consist of a communication policy, determining when to communicate, and a control policy, deciding what to communicate. It is essential to jointly optimize the two policies since individual optimization does not necessarily yield the overall optimal solution. To address this need for joint optimization, we propose a novel algorithm based on hierarchical reinforcement learning. The resulting algorithm is shown to accomplish high-performance control in line with resource savings and scales seamlessly to nonlinear and high-dimensional systems. The method's applicability to real-world scenarios is demonstrated through experiments on a six degrees of freedom real-time controlled manipulator. Further, we propose an approach towards evaluating the stability of the learned neural network policies.      
### 13.Implanting Synthetic Lesions for Improving Liver Lesion Segmentation in CT Exams  [ :arrow_down: ](https://arxiv.org/pdf/2008.04690.pdf)
>  The success of supervised lesion segmentation algorithms using Computed Tomography (CT) exams depends significantly on the quantity and variability of samples available for training. While annotating such data constitutes a challenge itself, the variability of lesions in the dataset also depends on the prevalence of different types of lesions. This phenomenon adds an inherent bias to lesion segmentation algorithms that can be diminished, among different possibilities, using aggressive data augmentation methods. In this paper, we present a method for implanting realistic lesions in CT slices to provide a rich and controllable set of training samples and ultimately improving semantic segmentation network performances for delineating lesions in CT exams. Our results show that implanting synthetic lesions not only improves (up to around 12\%) the segmentation performance considering different architectures but also that this improvement is consistent among different image synthesis networks. We conclude that increasing the variability of lesions synthetically in terms of size, density, shape, and position seems to improve the performance of segmentation models for liver lesion segmentation in CT slices.      
### 14.S-vectors: Speaker Embeddings based on Transformer's Encoder for Text-Independent Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2008.04659.pdf)
>  X-vectors have become the standard for speaker-embeddings in automatic speaker verification. X-vectors are obtained using a Time-delay Neural Network (TDNN) with context over several frames. We have explored the use of an architecture built on self-attention which attends to all the features over the entire utterance, and hence better capture speaker-level characteristics. We have used the encoder structure of Transformers, which is built on self-attention, as the base architecture and trained it to do a speaker classification task. In this paper, we have proposed to derive speaker embeddings from the output of the trained Transformer encoder structure after appropriate statistics pooling to obtain utterance level features. We have named the speaker embeddings from this structure as s-vectors. s-vectors outperform x-vectors with a relative improvement of 10% and 15% in % EER when trained on Voxceleb-1 only and Voxceleb-1+2 datasets. We have also investigated the effect of deriving s-vectors from different layers of the model.      
### 15.Transfer Learning for Improving Singing-voice Detection in Polyphonic Instrumental Music  [ :arrow_down: ](https://arxiv.org/pdf/2008.04658.pdf)
>  Detecting singing-voice in polyphonic instrumental music is critical to music information retrieval. To train a robust vocal detector, a large dataset marked with vocal or non-vocal label at frame-level is essential. However, frame-level labeling is time-consuming and labor expensive, resulting there is little well-labeled dataset available for singing-voice detection (S-VD). Hence, we propose a data augmentation method for S-VD by transfer learning. In this study, clean speech clips with voice activity endpoints and separate instrumental music clips are artificially added together to simulate polyphonic vocals to train a vocal/non-vocal detector. Due to the different articulation and phonation between speaking and singing, the vocal detector trained with the artificial dataset does not match well with the polyphonic music which is singing vocals together with the instrumental accompaniments. To reduce this mismatch, transfer learning is used to transfer the knowledge learned from the artificial speech-plus-music training set to a small but matched polyphonic dataset, i.e., singing vocals with accompaniments. By transferring the related knowledge to make up for the lack of well-labeled training data in S-VD, the proposed data augmentation method by transfer learning can improve S-VD performance with an F-score improvement from 89.5% to 93.2%.      
### 16.AHP-Net: adaptive-hyper-parameter deep learning based image reconstruction method for multilevel low-dose CT  [ :arrow_down: ](https://arxiv.org/pdf/2008.04656.pdf)
>  Low-dose CT (LDCT) imaging is desirable in many clinical applications to reduce X-ray radiation dose to patients. Inspired by deep learning (DL), a recent promising direction of model-based iterative reconstruction (MBIR) methods for LDCT is via optimization-unrolling DL-regularized image reconstruction, where pre-defined image prior is replaced by learnable data-adaptive prior. However, LDCT is clinically multilevel, since clinical scans have different noise levels that depend of scanning site, patient size, and clinical task. Therefore, this work aims to develop an adaptive-hyper-parameter DL-based image reconstruction method (AHP-Net) that can handle multilevel LDCT of different noise levels. AHP-Net unrolls a half-quadratic splitting scheme with learnable image prior built on framelet filter bank, and learns a network that automatically adjusts the hyper-parameters for various noise levels. As a result, AHP-Net provides a single universal training model that can handle multilevel LDCT. Extensive experimental evaluations using clinical scans suggest that AHP-Net outperformed conventional MBIR techniques and state-of-the-art deep-learning-based methods for multilevel LDCT of different noise levels.      
### 17.A Simple Cooperative Platooning Controller for Connected Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2008.04650.pdf)
>  Urban traffic congestion is a chronic problem faced by many cities. It is essentially inefficient infrastructure use which results in increased vehicle fuel consumption and emissions. This in turn adds extra costs to commuters and businesses. Addressing this issue is therefore of paramount interest due to the perceived dual benefit. Many technologies were and are being developed. These include adaptive traffic signals, dedicated lanes, etc. This paper presents a simple platooning algorithm that maintains relatively small distances (pre-specified time gap) between consecutive vehicles to enhance mobility, increase transportation capacity and ultimately reduce travel costs. Several dynamic and kinematic constraints governing the motion of vehicles are also accounted for. These include acceleration, velocity, and distance constraints. This developed logic was tested on highways that traverse the downtown area of Los Angeles. Depending on the market penetration rate of connected automated vehicles versus non-connected automated vehicles, a reduction in travel time, delay and fuel consumed across the city can be observed. Vehicles are expected to reach their destination in less travel time, delay and consumed fuel. The reduction percentages range from 0 to 5 percent, 0 to 9.4 percent, and 2.58 to 8.17 percent, respectively.      
### 18.Residual Generation Using Physically-Based Grey-Box Recurrent Neural Networks For Engine Fault Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2008.04644.pdf)
>  Data-driven fault diagnosis is complicated by unknown fault classes and limited training data from different fault realizations. In these situations, conventional multi-class classification approaches are not suitable for fault diagnosis. One solution is the use of anomaly classifiers that are trained using only nominal data. Anomaly classifiers can be used to detect when a fault occurs but give little information about its root cause. Hybrid fault diagnosis methods combining physically-based models and available training data have shown promising results to improve fault classification performance and identify unknown fault classes. Residual generation using grey-box recurrent neural networks can be used for anomaly classification where physical insights about the monitored system are incorporated into the design of the machine learning algorithm. In this work, an automated residual design is developed using a bipartite graph representation of the system model to design grey-box recurrent neural networks and evaluated using a real industrial case study. Data from an internal combustion engine test bench is used to illustrate the potentials of combining machine learning and model-based fault diagnosis techniques.      
### 19.Alzheimer's Dementia Detection from Audio and Text Modalities  [ :arrow_down: ](https://arxiv.org/pdf/2008.04617.pdf)
>  Automatic detection of Alzheimer's dementia by speech processing is enhanced when features of both the acoustic waveform and the content are extracted. Audio and text transcription have been widely used in health-related tasks, as spectral and prosodic speech features, as well as semantic and linguistic content, convey information about various diseases. Hence, this paper describes the joint work of the GTM-UVIGO research group and acceXible startup to the ADDReSS challenge at INTERSPEECH 2020. The submitted systems aim to detect patterns of Alzheimer's disease from both the patient's voice and message transcription. Six different systems have been built and compared: four of them are speech-based and the other two systems are text-based. The x-vector, i-vector, and statistical speech-based functionals features are evaluated. As a lower speaking fluency is a common pattern in patients with Alzheimer's disease, rhythmic features are also proposed. For transcription analysis, two systems are proposed: one uses GloVe word embedding features and the other uses several features extracted by language modelling. Several intra-modality and inter-modality score fusion strategies are investigated. The performance of single modality and multimodal systems are presented. The achieved results are promising, outperforming the results achieved by the ADDReSS's baseline systems.      
### 20.Left Ventricular Wall Motion Estimation by Active Polynomials for Acute Myocardial Infarction Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.04615.pdf)
>  Echocardiogram (echo) is the earliest and the primary tool for identifying regional wall motion abnormalities (RWMA) in order to diagnose myocardial infarction (MI) or commonly known as heart attack. This paper proposes a novel approach, Active Polynomials, which can accurately and robustly estimate the global motion of the Left Ventricular (LV) wall from any echo in a robust and accurate way. The proposed algorithm quantifies the true wall motion occurring in LV wall segments so as to assist cardiologists diagnose early signs of an acute MI. It further enables medical experts to gain an enhanced visualization capability of echo images through color-coded segments along with their "maximum motion displacement" plots helping them to better assess wall motion and LV Ejection-Fraction (LVEF). The outputs of the method can further help echo-technicians to assess and improve the quality of the echocardiogram recording. A major contribution of this study is the first public echo database collection composed by physicians at the Hamad Medical Corporation Hospital in Qatar. The so-called HMC-QU database will serve as the benchmark for the forthcoming relevant studies. The results over the HMC-QU dataset show that the proposed approach can achieve high accuracy, sensitivity and precision in MI detection even though the echo quality is quite poor, and the temporal resolution is low.      
### 21.Multi-modal segmentation of 3D brain scans using neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.04594.pdf)
>  Purpose: To implement a brain segmentation pipeline based on convolutional neural networks, which rapidly segments 3D volumes into 27 anatomical structures. To provide an extensive, comparative study of segmentation performance on various contrasts of magnetic resonance imaging (MRI) and computed tomography (CT) scans. Methods: Deep convolutional neural networks are trained to segment 3D MRI (MPRAGE, DWI, FLAIR) and CT scans. A large database of in total 851 MRI/CT scans is used for neural network training. Training labels are obtained on the MPRAGE contrast and coregistered to the other imaging modalities. The segmentation quality is quantified using the Dice metric for a total of 27 anatomical structures. Dropout sampling is implemented to identify corrupted input scans or low-quality segmentations. Full segmentation of 3D volumes with more than 2 million voxels is obtained in less than 1s of processing time on a graphical processing unit. Results: The best average Dice score is found on $T_1$-weighted MPRAGE ($85.3\pm4.6\,\%$). However, for FLAIR ($80.0\pm7.1\,\%$), DWI ($78.2\pm7.9\,\%$) and CT ($79.1\pm 7.9\,\%$), good-quality segmentation is feasible for most anatomical structures. Corrupted input volumes or low-quality segmentations can be detected using dropout sampling. Conclusion: The flexibility and performance of deep convolutional neural networks enables the direct, real-time segmentation of FLAIR, DWI and CT scans without requiring $T_1$-weighted scans.      
### 22.Surgical Mask Detection with Convolutional Neural Networks and Data Augmentations on Spectrograms  [ :arrow_down: ](https://arxiv.org/pdf/2008.04590.pdf)
>  In many fields of research, labeled datasets are hard to acquire. This is where data augmentation promises to overcome the lack of training data in the context of neural network engineering and classification tasks. The idea here is to reduce model over-fitting to the feature distribution of a small under-descriptive training dataset. We try to evaluate such data augmentation techniques to gather insights in the performance boost they provide for several convolutional neural networks on mel-spectrogram representations of audio data. We show the impact of data augmentation on the binary classification task of surgical mask detection in samples of human voice (ComParE Challenge 2020). Also we consider four varying architectures to account for augmentation robustness. Results show that most of the baselines given by ComParE are outperformed.      
### 23.Why Did the x-Vector System Miss a Target Speaker? Impact of Acoustic Mismatch Upon Target Score on VoxCeleb Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.04578.pdf)
>  Modern automatic speaker verification (ASV) relies heavily on machine learning implemented through deep neural networks. It can be difficult to interpret the output of these black boxes. In line with interpretative machine learning, we model the dependency of ASV detection score upon acoustic mismatch of the enrollment and test utterances. We aim to identify mismatch factors that explain target speaker misses (false rejections). We use distance in the first- and second-order statistics of selected acoustic features as the predictors in a linear mixed effects model, while a standard Kaldi x-vector system forms our ASV black-box. Our results on the VoxCeleb data reveal the most prominent mismatch factor to be in F0 mean, followed by mismatches associated with formant frequencies. Our findings indicate that x-vector systems lack robustness to intra-speaker variations.      
### 24.Bunched LPCNet : Vocoder for Low-cost Neural Text-To-Speech Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.04574.pdf)
>  LPCNet is an efficient vocoder that combines linear prediction and deep neural network modules to keep the computational complexity low. In this work, we present two techniques to further reduce it's complexity, aiming for a low-cost LPCNet vocoder-based neural Text-to-Speech (TTS) System. These techniques are: 1) Sample-bunching, which allows LPCNet to generate more than one audio sample per inference; and 2) Bit-bunching, which reduces the computations in the final layer of LPCNet. With the proposed bunching techniques, LPCNet, in conjunction with a Deep Convolutional TTS (DCTTS) acoustic model, shows a 2.19x improvement over the baseline run-time when running on a mobile device, with a less than 0.1 decrease in TTS mean opinion score (MOS).      
### 25.Neuro-Steered Hearing Devices: Decoding Auditory Attention From the Brain  [ :arrow_down: ](https://arxiv.org/pdf/2008.04569.pdf)
>  People suffering from hearing impairment often have difficulties participating in conversations in so-called `cocktail party' scenarios with multiple people talking simultaneously. Although advanced algorithms exist to suppress background noise in these situations, a hearing device also needs information on which of these speakers the user actually aims to attend to. Recent neuroscientific advances have shown that it is possible to determine the focus of auditory attention from non-invasive neurorecording techniques, such as electroencephalography (EEG). Based on these new insights, a multitude of auditory attention decoding (AAD) algorithms have been proposed, which could, in combination with the appropriate speaker separation algorithms and miniaturized EEG sensor devices, lead to a new generation of so-called neuro-steered hearing devices. In this paper, we address the main signal processing challenges in this field and provide a review and comparative study of state-of-the-art AAD algorithms.      
### 26.Epigraphical Relaxation for Minimizing Layered Non-Proximable Mixed Norms  [ :arrow_down: ](https://arxiv.org/pdf/2008.04565.pdf)
>  This paper proposes an epigraphical relaxation (ERx) technique for non-proximable mixed norm minimization. Mixed norm regularization methods play a central role in signal reconstruction and processing, where their optimization relies on the fact that the proximity operators of the mixed norms can be computed efficiently. To bring out the power of regularization, sophisticated layered modeling of mixed norms that can capture inherent signal structure is a key ingredient, but the proximity operator of such a mixed norm is often unavailable (nonproximable). Our ERx decouples a layered non-proximable mixed norm into a norm and multiple epigraphical constraints. This enables us to handle a wide range of non-proximable mixed norms in optimization, as long as both the proximal operator of the outermost norm and the projection onto each epigraphical constraint are efficiently computable. Moreover, under mild conditions, we prove that ERx does not change the minimizer of the original problem despite relaxing equality constraints into inequality ones. We also develop new regularizers based on ERx: one is decorrelated structure-tensor total variation for color image restoration, and the other is amplitude-spectrum nuclear norm (ASNN) for low-rank amplitude recovery. We examine the power of these regularizers through experiments, which illustrates the utility of ERx.      
### 27.Spectrum and Prosody Conversion for Cross-lingual Voice Conversion with CycleGAN  [ :arrow_down: ](https://arxiv.org/pdf/2008.04562.pdf)
>  Cross-lingual voice conversion aims to change source speaker's voice to sound like that of target speaker, when source and target speakers speak different languages. It relies on non-parallel training data from two different languages, hence, is more challenging than mono-lingual voice conversion. Previous studies on cross-lingual voice conversion mainly focus on spectral conversion with a linear transformation for F0 transfer. However, as an important prosodic factor, F0 is inherently hierarchical, thus it is insufficient to just use a linear method for conversion. We propose the use of continuous wavelet transform (CWT) decomposition for F0 modeling. CWT provides a way to decompose a signal into different temporal scales that explain prosody in different time resolutions. We also propose to train two CycleGAN pipelines for spectrum and prosody mapping respectively. In this way, we eliminate the need for parallel data of any two languages and any alignment techniques. Experimental results show that our proposed Spectrum-Prosody-CycleGAN framework outperforms the Spectrum-CycleGAN baseline in subjective evaluation. To our best knowledge, this is the first study of prosody in cross-lingual voice conversion.      
### 28.Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages  [ :arrow_down: ](https://arxiv.org/pdf/2008.04549.pdf)
>  Recently, sequence-to-sequence models with attention have been successfully applied in Text-to-speech (TTS). These models can generate near-human speech with a large accurately-transcribed speech corpus. However, preparing such a large data-set is both expensive and laborious. To alleviate the problem of heavy data demand, we propose a novel unsupervised pre-training mechanism in this paper. Specifically, we first use Vector-quantization Variational-Autoencoder (VQ-VAE) to ex-tract the unsupervised linguistic units from large-scale, publicly found, and untranscribed speech. We then pre-train the sequence-to-sequence TTS model by using the&lt;unsupervised linguistic units, audio&gt;pairs. Finally, we fine-tune the model with a small amount of&lt;text, audio&gt;paired data from the target speaker. As a result, both objective and subjective evaluations show that our proposed method can synthesize more intelligible and natural speech with the same amount of paired training data. Besides, we extend our proposed method to the hypothesized low-resource languages and verify the effectiveness of the method using objective evaluation.      
### 29.Investigation of End-To-End Speaker-Attributed ASR for Continuous Multi-Talker Recordings  [ :arrow_down: ](https://arxiv.org/pdf/2008.04546.pdf)
>  Recently, an end-to-end (E2E) speaker-attributed automatic speech recognition (SA-ASR) model was proposed as a joint model of speaker counting, speech recognition and speaker identification for monaural overlapped speech. It showed promising results for simulated speech mixtures consisting of various numbers of speakers. However, the model required prior knowledge of speaker profiles to perform speaker identification, which significantly limited the application of the model. In this paper, we extend the prior work by addressing the case where no speaker profile is available. Specifically, we perform speaker counting and clustering by using the internal speaker representations of the E2E SA-ASR model to diarize the utterances of the speakers whose profiles are missing from the speaker inventory. We also propose a simple modification to the reference labels of the E2E SA-ASR training which helps handle continuous multi-talker recordings well. We conduct a comprehensive investigation of the original E2E SA-ASR and the proposed method on the monaural LibriCSS dataset. Compared to the original E2E SA-ASR with relevant speaker profiles, the proposed method achieves a close performance without any prior speaker knowledge. We also show that the source-target attention in the E2E SA-ASR model provides information about the start and end times of the hypotheses.      
### 30.An Intelligent Control Strategy for buck DC-DC Converter via Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.04542.pdf)
>  As a typical switching power supply, the DC-DC converter has been widely applied in DC microgrid. Due to the variation of renewable energy generation, research and design of DC-DC converter control algorithm with outstanding dynamic characteristics has significant theoretical and practical application value. To mitigate the bus voltage stability issue in DC microgrid, an innovative intelligent control strategy for buck DC-DC converter with constant power loads (CPLs) via deep reinforcement learning algorithm is constructed for the first time. In this article, a Markov Decision Process (MDP) model and the deep Q network (DQN) algorithm are defined for DC-DC converter. A model-free based deep reinforcement learning (DRL) control strategy is appropriately designed to adjust the agent-environment interaction through the rewards/penalties mechanism towards achieving converge to nominal voltage. The agent makes approximate decisions by extracting the high-dimensional feature of complex power systems without any prior knowledge. Eventually, the simulation comparison results demonstrate that the proposed controller has stronger self-learning and self-optimization capabilities under the different scenarios.      
### 31.Thick Cloud Removal of Remote Sensing Images Using Temporal Smoothness and Sparsity-Regularized Tensor Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2008.04529.pdf)
>  In remote sensing images, the presence of thick cloud accompanying cloud shadow is a high probability event, which can affect the quality of subsequent processing and limit the scenarios of application. Hence, removing the thick cloud and cloud shadow as well as recovering the cloud-contaminated pixels is indispensable to make good use of remote sensing images. In this paper, a novel thick cloud removal method for remote sensing images based on temporal smoothness and sparsity-regularized tensor optimization (TSSTO) is proposed. The basic idea of TSSTO is that the thick cloud and cloud shadow are not only sparse but also smooth along the horizontal and vertical direction in images while the clean images are smooth along the temporal direction between images. Therefore, the sparsity norm is used to boost the sparsity of the cloud and cloud shadow, and unidirectional total variation (UTV) regularizers are applied to ensure the unidirectional smoothness. This paper utilizes alternation direction method of multipliers to solve the presented model and generate the cloud and cloud shadow element as well as the clean element. The cloud and cloud shadow element is purified to get the cloud area and cloud shadow area. Then, the clean area of the original cloud-contaminated images is replaced to the corresponding area of the clean element. Finally, the reference image is selected to reconstruct details of the cloud area and cloud shadow area using the information cloning method. A series of experiments are conducted both on simulated and real cloud-contaminated images from different sensors and with different resolutions, and the results demonstrate the potential of the proposed TSSTO method for removing cloud and cloud shadow from both qualitative and quantitative viewpoints.      
### 32.Neural PLDA Modeling for End-to-End Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2008.04527.pdf)
>  While deep learning models have made significant advances in supervised classification problems, the application of these models for out-of-set verification tasks like speaker recognition has been limited to deriving feature embeddings. The state-of-the-art x-vector PLDA based speaker verification systems use a generative model based on probabilistic linear discriminant analysis (PLDA) for computing the verification score. Recently, we had proposed a neural network approach for backend modeling in speaker verification called the neural PLDA (NPLDA) where the likelihood ratio score of the generative PLDA model is posed as a discriminative similarity function and the learnable parameters of the score function are optimized using a verification cost. In this paper, we extend this work to achieve joint optimization of the embedding neural network (x-vector network) with the NPLDA network in an end-to-end (E2E) fashion. This proposed end-to-end model is optimized directly from the acoustic features with a verification cost function and during testing, the model directly outputs the likelihood ratio score. With various experiments using the NIST speaker recognition evaluation (SRE) 2018 and 2019 datasets, we show that the proposed E2E model improves significantly over the x-vector PLDA baseline speaker verification system.      
### 33.SAFRON: Stitching Across the Frontier for Generating Colorectal Cancer Histology Images  [ :arrow_down: ](https://arxiv.org/pdf/2008.04526.pdf)
>  Synthetic images can be used for the development and evaluation of deep learning algorithms in the context of limited availability of annotations. In the field of computational pathology where histology images are large and visual context is crucial, synthesis of large tissue images via generative modeling is a challenging task due to memory and computing constraints hindering the generation of large images. To address this challenge, we propose a novel framework named as SAFRON to construct realistic large tissue image tiles from ground truth annotations while preserving morphological features and with minimal boundary artifacts at the seams. To this end, we train the proposed SAFRON framework based on conditional generative adversarial networks on large tissue image tiles from the Colorectal Adenocarcinoma Gland (CRAG) and DigestPath datasets. We demonstrate that our model can generate high quality and realistic image tiles of arbitrary large size after training it on relatively small image patches. We also show that training on synthetic data generated by SAFRON can significantly boost the performance of a standard algorithm for gland segmentation of colorectal cancer tissue images. Sample high resolution images generated using SAFRON are available at the URL:<a class="link-external link-https" href="https://warwick.ac.uk/TIALab/SAFRON" rel="external noopener nofollow">this https URL</a>      
### 34.Acoustic effects of medical, cloth, and transparent face masks on speech signals  [ :arrow_down: ](https://arxiv.org/pdf/2008.04521.pdf)
>  Face masks muffle speech and make communication more difficult, especially for people with hearing loss. This study examines the acoustic attenuation caused by different face masks, including medical, cloth, and transparent masks, using a head-shaped loudspeaker and a live human talker. The results suggest that all masks attenuate frequencies above 1 kHz, that attenuation is greatest in front of the talker, and that there is substantial variation between mask types, especially cloth masks with different materials and weaves. Transparent masks have poor acoustic performance compared to both medical and cloth masks. Most masks have little effect on lapel microphones, suggesting that existing sound reinforcement and assistive listening systems may be effective for verbal communication with masks.      
### 35.A Design of Cooperative Overtaking Based on Complex Lane Detection and Collision Risk Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2008.04505.pdf)
>  Cooperative overtaking is believed to have the capability of improving road safety and traffic efficiency by means of the real-time information exchange between traffic participants, including road infrastructures, nearby vehicles and others. In this paper, we focused on the critical issues of modeling, computation, and analysis of cooperative overtaking and made it playing a key role in the road overtaking area. In detail, for the purpose of extending the awareness of the surrounding environment, the lane markings in front of ego vehicle were detected and modeled with Bezier curve using an onboard camera. While the nearby vehicle positions were obtained through the vehicle-to-vehicle communication scheme making assure of the accuracy of localization. Then, Gaussian-based conflict potential field was proposed to guarantee the overtaking safety, which can quantitatively estimate the oncoming collision danger. To support the proposed method, many experiments were conducted on the human-in-the-loop simulation platform. The results demonstrated that our proposed method achieves better performance, especially in some unpredictable nature road circumstances.      
### 36.ARPM-net: A novel CNN-based adversarial method with Markov Random Field enhancement for prostate and organs at risk segmentation in pelvic CT images  [ :arrow_down: ](https://arxiv.org/pdf/2008.04488.pdf)
>  Purpose: The research is to develop a novel CNN-based adversarial deep learning method to improve and expedite the multi-organ semantic segmentation of CT images, and to generate accurate contours on pelvic CT images. Methods: Planning CT and structure datasets for 110 patients with intact prostate cancer were retrospectively selected and divided for 10-fold cross-validation. The proposed adversarial multi-residual multi-scale pooling Markov Random Field (MRF) enhanced network (ARPM-net) implements an adversarial training scheme. A segmentation network and a discriminator network were trained jointly, and only the segmentation network was used for prediction. The segmentation network integrates a newly designed MRF block into a variation of multi-residual U-net. The discriminator takes the product of the original CT and the prediction/ground-truth as input and classifies the input into fake/real. The segmentation network and discriminator network can be trained jointly as a whole, or the discriminator can be used for fine-tuning after the segmentation network is coarsely trained. Multi-scale pooling layers were introduced to preserve spatial resolution during pooling using less memory compared to atrous convolution layers. An adaptive loss function was proposed to enhance the training on small or low contrast organs. The accuracy of modeled contours was measured with the Dice similarity coefficient (DSC), Average Hausdorff Distance (AHD), Average Surface Hausdorff Distance (ASHD), and relative Volume Difference (VD) using clinical contours as references to the ground-truth. The proposed ARPM-net method was compared to several stateof-the-art deep learning methods.      
### 37.Exploring Aligned Lyrics-Informed Singing Voice Separation  [ :arrow_down: ](https://arxiv.org/pdf/2008.04482.pdf)
>  In this paper, we propose a method of utilizing aligned lyrics as additional information to improve the performance of singing voice separation. We have combined the highway network-based lyrics encoder into Open-unmix separation network and show that the model trained with the aligned lyrics indeed results in a better performance than the model that was not informed. The question now remains whether the increase of performance is actually due to the phonetic contents that lie in the informed aligned lyrics or not. To this end, we investigated the source of performance increase in multifaceted ways by observing the change of performance when incorrect lyrics were given to the model. Experiment results show that the model can use not only just vocal activity information but also the phonetic contents from the aligned lyrics.      
### 38.Transformer with Bidirectional Decoder for Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2008.04481.pdf)
>  Attention-based models have made tremendous progress on end-to-end automatic speech recognition(ASR) recently. However, the conventional transformer-based approaches usually generate the sequence results token by token from left to right, leaving the right-to-left contexts unexploited. In this work, we introduce a bidirectional speech transformer to utilize the different directional contexts simultaneously. Specifically, the outputs of our proposed transformer include a left-to-right target, and a right-to-left target. In inference stage, we use the introduced bidirectional beam search method, which can not only generate left-to-right candidates but also generate right-to-left candidates, and determine the best hypothesis by the score. <br>To demonstrate our proposed speech transformer with a bidirectional decoder(STBD), we conduct extensive experiments on the AISHELL-1 dataset. The results of experiments show that STBD achieves a 3.6\% relative CER reduction(CERR) over the unidirectional speech transformer baseline. Besides, the strongest model in this paper called STBD-Big can achieve 6.64\% CER on the test set, without language model rescoring and any extra data augmentation strategies.      
### 39.PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss  [ :arrow_down: ](https://arxiv.org/pdf/2008.04470.pdf)
>  Neural network applications generally benefit from larger-sized models, but for current speech enhancement models, larger scale networks often suffer from decreased robustness to the variety of real-world use cases beyond what is encountered in training data. We introduce several innovations that lead to better large neural networks for speech enhancement. The novel PoCoNet architecture is a convolutional neural network that, with the use of frequency-positional embeddings, is able to more efficiently build frequency-dependent features in the early layers. A semi-supervised method helps increase the amount of conversational training data by pre-enhancing noisy datasets, improving performance on real recordings. A new loss function biased towards preserving speech quality helps the optimization better match human perceptual opinions on speech quality. Ablation experiments and objective and human opinion metrics show the benefits of the proposed improvements.      
### 40.Fault-Tolerant Control of Degrading Systems with On-Policy Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.04407.pdf)
>  We propose a novel adaptive reinforcement learning control approach for fault tolerant control of degrading systems that is not preceded by a fault detection and diagnosis step. Therefore, \textit{a priori} knowledge of faults that may occur in the system is not required. The adaptive scheme combines online and offline learning of the on-policy control method to improve exploration and sample efficiency, while guaranteeing stable learning. The offline learning phase is performed using a data-driven model of the system, which is frequently updated to track the system's operating conditions. We conduct experiments on an aircraft fuel transfer system to demonstrate the effectiveness of our approach.      
### 41.Comparison of Model Predictive and Reinforcement Learning Methods for Fault Tolerant Control  [ :arrow_down: ](https://arxiv.org/pdf/2008.04403.pdf)
>  A desirable property in fault-tolerant controllers is adaptability to system changes as they evolve during systems operations. An adaptive controller does not require optimal control policies to be enumerated for possible faults. Instead it can approximate one in real-time. We present two adaptive fault-tolerant control schemes for a discrete time system based on hierarchical reinforcement learning. We compare their performance against a model predictive controller in presence of sensor noise and persistent faults. The controllers are tested on a fuel tank model of a C-130 plane. Our experiments demonstrate that reinforcement learning-based controllers perform more robustly than model predictive controllers under faults, partially observable system models, and varying sensor noise levels.      
### 42.GANDALF: Generative Adversarial Networks with Discriminator-Adaptive Loss Fine-tuning for Alzheimer's Disease Diagnosis from MRI  [ :arrow_down: ](https://arxiv.org/pdf/2008.04396.pdf)
>  Positron Emission Tomography (PET) is now regarded as the gold standard for the diagnosis of Alzheimer's Disease (AD). However, PET imaging can be prohibitive in terms of cost and planning, and is also among the imaging techniques with the highest dosage of radiation. Magnetic Resonance Imaging (MRI), in contrast, is more widely available and provides more flexibility when setting the desired image resolution. Unfortunately, the diagnosis of AD using MRI is difficult due to the very subtle physiological differences between healthy and AD subjects visible on MRI. As a result, many attempts have been made to synthesize PET images from MR images using generative adversarial networks (GANs) in the interest of enabling the diagnosis of AD from MR. Existing work on PET synthesis from MRI has largely focused on Conditional GANs, where MR images are used to generate PET images and subsequently used for AD diagnosis. There is no end-to-end training goal. This paper proposes an alternative approach to the aforementioned, where AD diagnosis is incorporated in the GAN training objective to achieve the best AD classification performance. Different GAN lossesare fine-tuned based on the discriminator performance, and the overall training is stabilized. The proposed network architecture and training regime show state-of-the-art performance for three- and four- class AD classification tasks.      
### 43.GANBERT: Generative Adversarial Networks with Bidirectional Encoder Representations from Transformers for MRI to PET synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2008.04393.pdf)
>  Synthesizing medical images, such as PET, is a challenging task due to the fact that the intensity range is much wider and denser than those in photographs and digital renderings and are often heavily biased toward zero. Above all, intensity values in PET have absolute significance, and are used to compute parameters that are reproducible across the population. Yet, usually much manual adjustment has to be made in pre-/post- processing when synthesizing PET images, because its intensity ranges can vary a lot, e.g., between -100 to 1000 in floating point values. To overcome these challenges, we adopt the Bidirectional Encoder Representations from Transformers (BERT) algorithm that has had great success in natural language processing (NLP), where wide-range floating point intensity values are represented as integers ranging between 0 to 10000 that resemble a dictionary of natural language vocabularies. BERT is then trained to predict a proportion of masked values images, where its "next sentence prediction (NSP)" acts as GAN discriminator. Our proposed approach, is able to generate PET images from MRI images in wide intensity range, with no manual adjustments in pre-/post- processing. It is a method that can scale and ready to deploy.      
### 44.Predicting Risk of Developing Diabetic Retinopathy using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2008.04370.pdf)
>  Diabetic retinopathy (DR) screening is instrumental in preventing blindness, but faces a scaling challenge as the number of diabetic patients rises. Risk stratification for the development of DR may help optimize screening intervals to reduce costs while improving vision-related outcomes. We created and validated two versions of a deep learning system (DLS) to predict the development of mild-or-worse ("Mild+") DR in diabetic patients undergoing DR screening. The two versions used either three-fields or a single field of color fundus photographs (CFPs) as input. The training set was derived from 575,431 eyes, of which 28,899 had known 2-year outcome, and the remaining were used to augment the training process via multi-task learning. Validation was performed on both an internal validation set (set A; 7,976 eyes; 3,678 with known outcome) and an external validation set (set B; 4,762 eyes; 2,345 with known outcome). For predicting 2-year development of DR, the 3-field DLS had an area under the receiver operating characteristic curve (AUC) of 0.79 (95%CI, 0.78-0.81) on validation set A. On validation set B (which contained only a single field), the 1-field DLS's AUC was 0.70 (95%CI, 0.67-0.74). The DLS was prognostic even after adjusting for available risk factors (p&lt;0.001). When added to the risk factors, the 3-field DLS improved the AUC from 0.72 (95%CI, 0.68-0.76) to 0.81 (95%CI, 0.77-0.84) in validation set A, and the 1-field DLS improved the AUC from 0.62 (95%CI, 0.58-0.66) to 0.71 (95%CI, 0.68-0.75) in validation set B. The DLSs in this study identified prognostic information for DR development from CFPs. This information is independent of and more informative than the available risk factors.      
### 45.Distributed Personalized Gradient Tracking with Convex Parametric Models  [ :arrow_down: ](https://arxiv.org/pdf/2008.04363.pdf)
>  We present a distributed optimization algorithm for solving online personalized optimization problems over a network of computing and communicating nodes, each of which linked to a specific user. The local objective functions are assumed to have a composite structure and to consist of a known time-varying (engineering) part and an unknown (user-specific) part. Regarding the unknown part, it is assumed to have a known parametric (e.g., quadratic) structure a priori, whose parameters are to be learned along with the evolution of the algorithm. The algorithm is composed of two intertwined components: (i) a dynamic gradient tracking scheme for finding local solution estimates and (ii) a recursive least squares scheme for estimating the unknown parameters via user's noisy feedback on the local solution estimates. The algorithm is shown to exhibit a bounded regret under suitable assumptions. Finally, a numerical example corroborates the theoretical analysis.      
### 46.Leveraging Vehicle Connectivity and Autonomy to Stabilize Flow in Mixed Traffic Conditions: Accounting for Human-driven Vehicle Driver Behavioral Heterogeneity and Perception-reaction Time Delay  [ :arrow_down: ](https://arxiv.org/pdf/2008.04351.pdf)
>  The erratic nature of human driving tends to trigger undesired waves that amplify as successive driver reactions propagate from the errant vehicle to vehicles upstream. Known as phantom jams, this phenomenon has been identified in the literature as one of the main causes of traffic congestion. This paper is based on the premise that vehicle automation and connectivity can help mitigate such jams. In the paper, we design a controller for use in a connected and autonomous vehicle (CAV) to stabilize the flow of human-driven vehicles (HDVs) that are upstream of the CAV, and consequently to lower collision risk in the upstream traffic environment. In modeling the HDV dynamics in the mixed traffic stream, we duly consider HDV driver heterogeneity and the time delays associated with their perception reaction time. We can find that the maximum number of HDVs that a CAV can stabilize is lower when human drivers potential time delay and heterogeneity are considered, compared to the scenario where such are not considered. This result suggests that heterogeneity and time delay in HDV behavior impairs the CAVs capability to stabilize traffic. Therefore, in designing CAV controllers for traffic stabilization, it is essential to consider such uncertainty-related conditions. In our demonstration, we also show that the designed controller can significantly improve both the stability of the mixed traffic stream and the safety of both CAVs and HDVs in the stream. The results are useful for real-time calibration of the model parameters that characterize HDV movements in the mixed stream.      
### 47.An Assessment of the Radio Frequency Electromagnetic Field Exposure from A Massive MIMO 5G Testbed  [ :arrow_down: ](https://arxiv.org/pdf/2008.04345.pdf)
>  Current radiofrequency electromagnetic field (RF-EMF) exposure limits have become a critical concern for fifth-generation (5G) mobile network deployment. Regulation is not harmonized and in certain countries and regions it goes beyond the guidelines set out by the International Commission on Non-Ionizing Radiation Protection (ICNIRP). Using a massive multiple-input-multiple-output (mMIMO) testbed with beamforming capabilities that is capable of mimicking realistic 5G base station (BS) performance, this paper presents an experimental and statistical assessment of its associated RF-EMF exposure within a real-world indoor environment. The mMIMO testbed has up to 128 channels with user-programmable software defined radio (SDR) capability. It could perform zero-forcing precoding after channel state information (CSI) acquisition for different beamforming scenarios with respect to the associated user terminal antenna setups and positions. With 64 active mMIMO transmit antennas, 8 beamforming scenarios have been considered for single-user (SU) and multi-user (MU) downlink communications at different locations. Using a calibrated triaxial isotropic field-probe, the received channel power heat map for each beamforming scenario was acquired and then converted into an RF-EMF heat map. The relevant RF-EMF statistics was evaluated based on the variations of beam profiles and number of users.      
### 48.CG-SENSE revisited: Results from the first ISMRM reproducibility challenge  [ :arrow_down: ](https://arxiv.org/pdf/2008.04308.pdf)
>  Purpose: The aim of this work is to shed light on the issue of reproducibility in MR image reconstruction in the context of a challenge. Participants had to recreate the results of "Advances in sensitivity encoding with arbitrary k-space trajectories" by Pruessmann et al. <br>Methods: The task of the challenge was to reconstruct radially acquired multi-coil k-space data (brain/heart) following the method in the original paper, reproducing its key figures. Results were compared to consolidated reference implementations created after the challenge, accounting for the two most common programming languages used in the submissions (Matlab/Python). <br>Results: Visually, differences between submissions were small. Pixel-wise differences originated from image orientation, assumed field-of-view or resolution. The reference implementations were in good agreement, both visually and in terms of image similarity metrics. <br>Discussion and Conclusion: While the description level of the published algorithm enabled participants to reproduce CG-SENSE in general, details of the implementation varied, e.g., density compensation or Tikhonov regularization. Implicit assumptions about the data lead to further differences, emphasizing the importance of sufficient meta-data accompanying open data sets. Defining reproducibility quantitatively turned out to be non-trivial for this image reconstruction challenge, in the absence of ground-truth results. Typical similarity measures like NMSE of SSIM were misled by image intensity scaling and outlier pixels. Thus, to facilitate reproducibility, researchers are encouraged to publish code and data alongside the original paper. Future methodological papers on MR image reconstruction might benefit from the consolidated reference implementations of CG-SENSE presented here, as a benchmark for methods comparison.      
### 49.Learning to See Through Obstructions with Layered Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2008.04902.pdf)
>  We present a learning-based approach for removing unwanted obstructions, such as window reflections, fence occlusions or raindrops, from a short sequence of images captured by a moving camera. Our method leverages the motion differences between the background and the obstructing elements to recover both layers. Specifically, we alternate between estimating dense optical flow fields of the two layers and reconstructing each layer from the flow-warped images via a deep convolutional neural network. The learning-based layer reconstruction allows us to accommodate potential errors in the flow estimation and brittle assumptions such as brightness consistency. We show that training on synthetically generated data transfers well to real images. Our results on numerous challenging scenarios of reflection and fence removal demonstrate the effectiveness of the proposed method.      
### 50.Channel Leakage and Fundamental Limits of Privacy Leakage for Streaming Data  [ :arrow_down: ](https://arxiv.org/pdf/2008.04893.pdf)
>  In this paper, we first introduce the notion of channel leakage as the minimum mutual information between the channel input and channel output. As its name indicates, channel leakage quantifies the (least) information leakage to the malicious receiver. In a broad sense, it can be viewed as a dual concept of channel capacity, which characterizes the (maximum) information transmission to the targeted receiver. We obtain explicit formulas of channel leakage for the white Gaussian case and colored Gaussian case. We also study the implications of channel leakage in characterizing the fundamental limitations of privacy leakage for streaming data.      
### 51.Common Metrics to Benchmark Human-Machine Teams (HMT): A Review  [ :arrow_down: ](https://arxiv.org/pdf/2008.04855.pdf)
>  A significant amount of work is invested in human-machine teaming (HMT) across multiple fields. Accurately and effectively measuring system performance of an HMT is crucial for moving the design of these systems forward. Metrics are the enabling tools to devise a benchmark in any system and serve as an evaluation platform for assessing the performance, along with the verification and validation, of a system. Currently, there is no agreed-upon set of benchmark metrics for developing HMT systems. Therefore, identification and classification of common metrics are imperative to create a benchmark in the HMT field. The key focus of this review is to conduct a detailed survey aimed at identification of metrics employed in different segments of HMT and to determine the common metrics that can be used in the future to benchmark HMTs. We have organized this review as follows: identification of metrics used in HMTs until now, and classification based on functionality and measuring techniques. Additionally, we have also attempted to analyze all the identified metrics in detail while classifying them as theoretical, applied, real-time, non-real-time, measurable, and observable metrics. We conclude this review with a detailed analysis of the identified common metrics along with their usage to benchmark HMTs.      
### 52.Exposing Deep-faked Videos by Anomalous Co-motion Pattern Detection  [ :arrow_down: ](https://arxiv.org/pdf/2008.04848.pdf)
>  Recent deep learning based video synthesis approaches, in particular with applications that can forge identities such as "DeepFake", have raised great security concerns. Therefore, corresponding deep forensic methods are proposed to tackle this problem. However, existing methods are either based on unexplainable deep networks which greatly degrades the principal interpretability factor to media forensic, or rely on fragile image statistics such as noise pattern, which in real-world scenarios can be easily deteriorated by data compression. In this paper, we propose an fully-interpretable video forensic method that is designed specifically to expose deep-faked videos. To enhance generalizability on videos with various content, we model the temporal motion of multiple specific spatial locations in the videos to extract a robust and reliable representation, called Co-Motion Pattern. Such kind of conjoint pattern is mined across local motion features which is independent of the video contents so that the instance-wise variation can also be largely alleviated. More importantly, our proposed co-motion pattern possesses both superior interpretability and sufficient robustness against data compression for deep-faked videos. We conduct extensive experiments to empirically demonstrate the superiority and effectiveness of our approach under both classification and anomaly detection evaluation settings against the state-of-the-art deep forensic methods.      
### 53.Detecting Urban Dynamics Using Deep Siamese Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2008.04829.pdf)
>  Change detection is a fast-growing discipline in the areas of computer vision and remote sensing. In this work, we designed and developed a variant of convolutional neural network (CNN), known as Siamese CNN to extract features from pairs of Sentinel-2 temporal images of Mekelle city captured at different times and detect changes due to urbanization: buildings and roads. The detection capability of the proposed was measured in terms of overall accuracy (95.8), Kappa measure (72.5), recall (76.5), precision (77.7), F1 measure (77.1). The model has achieved a good performance in terms of most of these measures and can be used to detect changes in Mekelle and other cities at different time horizons undergoing urbanization.      
### 54.Channel Estimation via Direct Calculation and Deep Learning for RIS-Aided mmWave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2008.04704.pdf)
>  This paper proposes a novel reconfigurable intelligent surface (RIS) architecture which enables channel estimation of RIS-assisted millimeter wave (mmWave) systems. More specifically, two channel estimation methods, namely, direct calculation (DC) and deep learning (DL) methods, are proposed to skillfully convert the overall channel estimation into two tasks: the channel estimation and the angle parameter estimation of a small number of active elements. In particular, the direct calculation method calculates the angle parameters directly through the channel estimates of adjacent active elements and, based on it, the DL method reduces the angle offset rate and further improves the accuracy of angle parameter estimation. Compared with the traditional methods, the proposed schemes reduce the complexity of the RIS channel estimation while outperforming the beam training method in terms of minimum square error, achievable rate, and outage probability.      
### 55.Generation expansion planning in the presence of wind power plants using a genetic algorithm model  [ :arrow_down: ](https://arxiv.org/pdf/2008.04703.pdf)
>  One of the essential aspects of power system planning is generation expansion planning (GEP). The purpose of GEP is to enhance construction planning and reduce the costs of installing different types of power plants. This paper proposes a method based on Genetic Algorithm (GA) for GEP in the presence of wind power plants. Since it is desired to integrate the maximum possible wind power production in GEP, the constraints for incorporating different levels of wind energy in power generation are investigated comprehensively. This will allow obtaining the maximum reasonable amount of wind penetration in the network. Besides, due to the existence of different wind regimes, the penetration of strong and weak wind on GEP is assessed. The results show that the maximum utilization of wind power generation capacity could increase the exploitation of more robust wind regimes. Considering the growth of the wind farm industry and the cost reduction for building wind power plants, the sensitivity of GEP to the variations of this cost is investigated. The results further indicate that for a 10% reduction in the initial investment cost of wind power plants, the proposed model estimates that the overall cost will be minimized.      
### 56.PlugSonic: a web- and mobile-based platform for binaural audio and sonic narratives  [ :arrow_down: ](https://arxiv.org/pdf/2008.04638.pdf)
>  PlugSonic is a suite of web- and mobile-based applications for the curation and experience of binaural interactive soundscapes and sonic narratives. It was developed as part of the PLUGGY EU project (Pluggable Social Platform for Heritage Awareness and Participation) and consists of two main applications: PlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, to create and experience binaural soundscapes. The audio processing within PlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while the exploration of soundscapes in a physical space is obtained using Apple's ARKit. In this paper we present the design choices, the user involvement processes and the implementation details. The main goal of PlugSonic is technology democratisation; PlugSonic users - whether institutions or citizens - are all given the instruments needed to create, process and experience 3D soundscapes and sonic narrative; without the need for specific devices, external tools (software and/or hardware), specialised knowledge or custom development. The evaluation, which was conducted with inexperienced users on three tasks - creation, curation and experience - demonstrates how PlugSonic is indeed a simple, effective, yet powerful tool.      
### 57.Security Versus Privacy  [ :arrow_down: ](https://arxiv.org/pdf/2008.04477.pdf)
>  Linear queries can be submitted to a server containing private data. The server provides a response to the queries systematically corrupted using an additive noise to preserve the privacy of those whose data is stored on the server. The measure of privacy is inversely proportional to the trace of the Fisher information matrix. It is assumed that an adversary can inject a false bias to the responses. The measure of the security, capturing the ease of detecting the presence of the false data injection, is the sensitivity of the Kullback-Leiber divergence to the additive bias. An optimization problem for balancing privacy and security is proposed and subsequently solved. It is shown that the level of guaranteed privacy times the level of security equals a constant. Therefore, by increasing the level of privacy, the security guarantees can only be weakened and vice versa. Similar results are developed under the differential privacy framework.      
### 58.Fast Channel Estimation for IRS-Assisted OFDM  [ :arrow_down: ](https://arxiv.org/pdf/2008.04476.pdf)
>  In this letter, we study efficient channel estimation for an intelligent reflecting surface (IRS)-assisted orthogonal frequency division multiplexing (OFDM) system to achieve minimum training time. First, a fast channel estimation scheme with reduced OFDM symbol duration is proposed for arbitrary frequency-selective fading channels. Next, under the typical condition that the IRS-user channel is line-of-sight (LoS) dominant, another fast channel estimation scheme based on the novel concept of sampling-wise IRS reflection variation is proposed. Moreover, the pilot signal and IRS training reflection pattern are jointly optimized for both proposed schemes. Finally, the proposed schemes are compared in terms of training time and channel estimation performance via simulations, as well as against benchmark schemes.      
### 59.Airflow recovery from thoracic and abdominal movements using Synchrosqueezing Transform and Locally Stationary Gaussian Process Regression  [ :arrow_down: ](https://arxiv.org/pdf/2008.04473.pdf)
>  Airflow signal encodes rich information about respiratory system. While the gold standard for measuring airflow is to use a spirometer with an occlusive seal, this is not practical for ambulatory monitoring of patients. Advances in sensor technology have made measurement of motion of the thorax and abdomen feasible with small inexpensive devices, but estimation of airflow from these time series is challenging. We propose to use the nonlinear-type time-frequency analysis tool, synchrosqueezing transform, to properly represent the thoracic and abdominal movement signals as the features, which are used to recover the airflow by the locally stationary Gaussian process. We show that, using a dataset that contains respiratory signals under normal sleep conditions, an accurate prediction can be achieved by fitting the proposed model in the feature space both in the intra- and inter-subject setups. We also apply our method to a more challenging case, where subjects under general anesthesia underwent transitions from pressure support to unassisted ventilation to further demonstrate the utility of the proposed method.      
### 60.Key-Nets: Optical Transformation Convolutional Networks for Privacy Preserving Vision Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2008.04469.pdf)
>  Modern cameras are not designed with computer vision or machine learning as the target application. There is a need for a new class of vision sensors that are privacy preserving by design, that do not leak private information and collect only the information necessary for a target machine learning task. In this paper, we introduce key-nets, which are convolutional networks paired with a custom vision sensor which applies an optical/analog transform such that the key-net can perform exact encrypted inference on this transformed image, but the image is not interpretable by a human or any other key-net. We provide five sufficient conditions for an optical transformation suitable for a key-net, and show that generalized stochastic matrices (e.g. scale, bias and fractional pixel shuffling) satisfy these conditions. We motivate the key-net by showing that without it there is a utility/privacy tradeoff for a network fine-tuned directly on optically transformed images for face identification and object detection. Finally, we show that a key-net is equivalent to homomorphic encryption using a Hill cipher, with an upper bound on memory and runtime that scales quadratically with a user specified privacy parameter. Therefore, the key-net is the first practical, efficient and privacy preserving vision sensor based on optical homomorphic encryption.      
### 61.Cable Estimation-Based Control for Wire-Borne Underactuated Brachiating Robots: A Combined Direct-Indirect Adaptive Robust Approach  [ :arrow_down: ](https://arxiv.org/pdf/2008.04463.pdf)
>  In this paper, we present an online adaptive robust control framework for underactuated brachiating robots traversing flexible cables. Since the dynamic model of a flexible body is unknown in practice, we propose an indirect adaptive estimation scheme to approximate the unknown dynamic effects of the flexible cable as an external force with parametric uncertainties. A boundary layer-based sliding mode control is then designed to compensate for the residual unmodeled dynamics and time-varying disturbances, in which the control gain is updated by an auxiliary direct adaptive control mechanism. Stability analysis and derivation of adaptation laws are carried out through a Lyapunov approach, which formally guarantees the stability and tracking performance of the robot-cable system. Simulation experiments and comparison with a baseline controller show that the combined direct-indirect adaptive robust control framework achieves reliable tracking performance and adaptive system identification, enabling the robot to traverse flexible cables in the presence of unmodeled dynamics, parametric uncertainties and unstructured disturbances.      
### 62.Adaptive music: Automated music composition and distribution  [ :arrow_down: ](https://arxiv.org/pdf/2008.04415.pdf)
>  Creativity, or the ability to produce new useful ideas, is commonly associated to the human being, but there are many other examples in nature where this phenomenon can be observed. Inspired by this fact, in engineering and particularly in computational sciences, many different models have been developed to tackle a number of problems. Music, a form of art broadly present along the human history, is the main field addressed in this thesis, taking advantage of the kind of ideas that bring diversity and creativity to nature and computation. We present Melomics, an algorithmic composition method based on evolutionary search, with a genetic encoding of the solutions that are interpreted in a complex developmental process that leads to music in standard formats. This bioinspired compositional system has exhibited a highly creative power and versatility to produce music of different type, which in many occasions has proven to be indistinguishable from the music made by human composers. The system also has enabled the emergence of a set of completely novel applications: from effective tools that help anyone to easily obtain the precise music they need, to radically new uses like adaptive music for therapy, amusement or many other purposes. It is clear to us that there is yet much research to be done in this field and that countless and new unimaginable uses will derive from it.      
### 63.A Framework of Hierarchical Attacks to Network Controllability  [ :arrow_down: ](https://arxiv.org/pdf/2008.04414.pdf)
>  Network controllability robustness reflects how well a networked dynamical system can maintain its controllability against destructive attacks. This paper investigates the network controllability robustness from the perspective of a malicious attack. A framework of hierarchical attack is proposed, by means of edge- or node-removal attacks. Edges (or nodes) in a target network are classified hierarchically into categories, with different priorities to attack. The category of critical edges (or nodes) has the highest priority to be selected for attack. Extensive experiments on nine synthetic networks and nine real-world networks show the effectiveness of the proposed hierarchical attack strategies for destructing the network controllability. From the protection point of view, this study suggests that the critical edges and nodes should be hidden from the attackers. This finding helps better understand the network controllability and better design robust networks.      
### 64.Bipartite Graph Reasoning GANs for Person Image Generation  [ :arrow_down: ](https://arxiv.org/pdf/2008.04381.pdf)
>  We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the challenging person image generation task. The proposed graph generator mainly consists of two novel blocks that aim to model the pose-to-pose and pose-to-image relations, respectively. Specifically, the proposed Bipartite Graph Reasoning (BGR) block aims to reason the crossing long-range relations between the source pose and the target pose in a bipartite graph, which mitigates some challenges caused by pose deformation. Moreover, we propose a new Interaction-and-Aggregation (IA) block to effectively update and enhance the feature representation capability of both person's shape and appearance in an interactive way. Experiments on two challenging and public datasets, i.e., Market-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN in terms of objective quantitative scores and subjective visual realness. The source code and trained models are available at <a class="link-external link-https" href="https://github.com/Ha0Tang/BiGraphGAN" rel="external noopener nofollow">this https URL</a>.      
### 65.Co-design and Co-simulation for Engineering Systems: Insights from the Sustainable Infrastructure Planning Game  [ :arrow_down: ](https://arxiv.org/pdf/2008.04353.pdf)
>  This paper draws on perspectives from co-design as an integrative and collaborative design activity and co-simulation as a supporting information system to advance engineering design methods for problems of societal significance. Design and implementation of the Sustainable Infrastructure Planning Game provides a prototypical co-design platform that leverages the High Level Architecture co-simulation standard. Three role players create a strategic infrastructure plan for agriculture, water, and energy sectors to meet sustainability objectives for a growing and urbaninzing population in a fictional desert nation. An observational human subject study conducts 15 co-design sessions to understand how information system features influence design outcomes. Results show co-simulation facilitates information exchange critical for discovering and addressing interdependencies across role-specific objectives and frequent data exchange is correlated with achieving joint objectives, highlighting the role of co-simulation in co-design settings. Conclusions reflect on the opportunities and challenges presented by co-simulation in co-design settings to address engineering problems for infrastructure systems and more broadly.      
