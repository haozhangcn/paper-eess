# ArXiv eess --Mon, 3 Aug 2020
### 1.Adversarial Attacks with Multiple Antennas Against Deep Learning-Based Modulation Classifiers  [ :arrow_down: ](https://arxiv.org/pdf/2007.16204.pdf)
>  We consider a wireless communication system, where a transmitter sends signals to a receiver with different modulation types while the receiver classifies the modulation types of the received signals using its deep learning-based classifier. Concurrently, an adversary transmits adversarial perturbations using its multiple antennas to fool the classifier into misclassifying the received signals. From the adversarial machine learning perspective, we show how to utilize multiple antennas at the adversary to improve the adversarial (evasion) attack performance. Two main points are considered while exploiting the multiple antennas at the adversary, namely the power allocation among antennas and the utilization of channel diversity. First, we show that multiple independent adversaries, each with a single antenna cannot improve the attack performance compared to a single adversary with multiple antennas using the same total power. Then, we consider various ways to allocate power among multiple antennas at a single adversary such as allocating power to only one antenna, and proportional or inversely proportional to the channel gain. By utilizing channel diversity, we introduce an attack to transmit the adversarial perturbation through the channel with the largest channel gain at the symbol level. We show that this attack reduces the classifier accuracy significantly compared to other attacks under different channel conditions in terms of channel variance and channel correlation across antennas. Also, we show that the attack success improves significantly as the number of antennas increases at the adversary that can better utilize channel diversity to craft adversarial attacks.      
### 2.Designing Neural Speaker Embeddings with Meta Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.16196.pdf)
>  Neural speaker embeddings trained using classification objectives have demonstrated state-of-the-art performance in multiple applications. Typically, such embeddings are trained on an out-of-domain corpus on a single task e.g., speaker classification, albeit with a large number of classes (speakers). In this work, we reformulate embedding training under the meta-learning paradigm. We redistribute the training corpus as an ensemble of multiple related speaker classification tasks, and learn a representation that generalizes better to unseen speakers. First, we develop an open source toolkit to train x-vectors that is matched in performance with pre-trained Kaldi models for speaker diarization and speaker verification applications. We find that different bottleneck layers in the architecture variedly favor different applications. Next, we use two meta-learning strategies, namely prototypical networks and relation networks, to improve over the x-vector embeddings. Our best performing model achieves a relative improvement of 12.37% and 7.11% in speaker error on the DIHARD II development corpus and the AMI meeting corpus, respectively. We analyze improvements across different domains in the DIHARD corpus. Notably, on the challenging child speech domain, we study the relation between child age and the diarization performance. Further, we show reductions in equal error rate for speaker verification on the SITW corpus (7.68%) and the VOiCES challenge corpus (8.78%). We observe that meta-learning particularly offers benefits in challenging acoustic conditions and recording setups encountered in these corpora. Our experiments illustrate the applicability of meta-learning as a generalized learning paradigm for training deep neural speaker embeddings.      
### 3.Computer-aided Tumor Diagnosis in Automated Breast Ultrasound using 3D Detection Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.16133.pdf)
>  Automated breast ultrasound (ABUS) is a new and promising imaging modality for breast cancer detection and diagnosis, which could provide intuitive 3D information and coronal plane information with great diagnostic value. However, manually screening and diagnosing tumors from ABUS images is very time-consuming and overlooks of abnormalities may happen. In this study, we propose a novel two-stage 3D detection network for locating suspected lesion areas and further classifying lesions as benign or malignant tumors. Specifically, we propose a 3D detection network rather than frequently-used segmentation network to locate lesions in ABUS images, thus our network can make full use of the spatial context information in ABUS images. A novel similarity loss is designed to effectively distinguish lesions from background. Then a classification network is employed to identify the located lesions as benign or malignant. An IoU-balanced classification loss is adopted to improve the correlation between classification and localization task. The efficacy of our network is verified from a collected dataset of 418 patients with 145 benign tumors and 273 malignant tumors. Experiments show our network attains a sensitivity of 97.66% with 1.23 false positives (FPs), and has an area under the curve(AUC) value of 0.8720.      
### 4.Automatic Gain Control Design for Dynamic Visible Light Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.16125.pdf)
>  For dynamic visible light communication (VLC) systems, received optical power fluctuates largely due to the fast movement of VLC terminals. Such fluctuations will cause possible signal clipping and quantization noise at the analog-to-digital converters (ADCs) and thus aggravate signal-to-noise ratio (SNR) and reliability of the communication link. To mitigate this effect, the automatic gain control (AGC) technique is often introduced in the receiver front-end to adaptively adjust the electrical signal strength. In this paper, we provided a VLC front-end analysis considering AGC model. Based on the model, some design principles of AGC are theoretically derivated: the effects of AGC index $m$, AGC maximum gain $g_{max}$ and AGC equilibrium range $DR$. Next, an analog AGC amplifier was carefully implemented and the effectiveness of our AGC modeling was proved through BER experiments. Finally, realtime 25Mb/s on-off-keying (OOK) experiments with AGC function were demonstrated in a dynamic link with different speeds. Experimental results show that the AGC can stabilize the BER performance and improve the system performance in the speed of 1m/s.      
### 5.Predictability and Fairness in Social Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2007.16117.pdf)
>  In many applications, one may benefit from the collaborative collection of data for sensing a physical phenomenon, which is known as social sensing. We show how to make social sensing (1) predictable, in the sense of guaranteeing that the number of queries per participant will be independent of the initial state, in expectation, even when the population of participants varies over time, and (2) fair, in the sense of guaranteeing that the number of queries per participant will be equalised among the participants, in expectation, even when the population of participants varies over time. <br>In a use case, we consider a large, high-density network of participating parked vehicles. When awoken by an administrative centre, this network proceeds to search for moving, missing entities of interest using RFID-based techniques. We regulate the number and geographical distribution of the parked vehicles that are "Switched On" and thus actively searching for the moving entity of interest. In doing so, we seek to conserve vehicular energy consumption while, at the same time, maintaining good geographical coverage of the city such that the moving entity of interest is likely to be located within an acceptable time frame. Which vehicle participants are "Switched On" at any point in time is determined periodically through the use of stochastic techniques. This is illustrated on the example of a missing Alzheimer's patient in Melbourne, Australia.      
### 6.L$^2$C -- Learning to Learn to Compress  [ :arrow_down: ](https://arxiv.org/pdf/2007.16054.pdf)
>  In this paper we present an end-to-end meta-learned system for image compression. Traditional machine learning based approaches to image compression train one or more neural network for generalization performance. However, at inference time, the encoder or the latent tensor output by the encoder can be optimized for each test image. This optimization can be regarded as a form of adaptation or benevolent overfitting to the input content. In order to reduce the gap between training and inference conditions, we propose a new training paradigm for learned image compression, which is based on meta-learning. In a first phase, the neural networks are trained normally. In a second phase, the Model-Agnostic Meta-learning approach is adapted to the specific case of image compression, where the inner-loop performs latent tensor overfitting, and the outer loop updates both encoder and decoder neural networks based on the overfitting performance. Furthermore, after meta-learning, we propose to overfit and cluster the bias terms of the decoder on training image patches, so that at inference time the optimal content-specific bias terms can be selected at encoder-side. Finally, we propose a new probability model for lossless compression, which combines concepts from both multi-scale and super-resolution probability model approaches. We show the benefits of all our proposed ideas via carefully designed experiments.      
### 7.Data-driven Inverter-based Volt/VAr Control for Partially Observable Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.16039.pdf)
>  For active distribution networks (ADNs) integrated with massive inverter-based energy resources, their current models are usually inaccurate or even unknown. Moreover, ADNs are usually partially observable since only a few measurements are available at critical nodes. To provide a practical Volt/Var control (VVC) strategy for such networks, a data-driven VVC method is proposed in this paper. Firstly, the system response policy, approximating the relationship between the control variables and states of monitoring nodes, are estimated by a recursive regression close-form solution. Then, based on current measurements and the newly updated system response policy, an VVC strategy with convergence guarantee is realized. Since the recursive regression solution is embedded in the control stage, a data-driven closed-loop VVC framework is established. The effectiveness of the proposed method is validated in an unbalanced 33-bus distribution system considering nonlinear loads where not only the rapid and self-adaptive voltage regulation is realized but system-wide optimization is achieved.      
### 8.Beamformed Energy Detection in the Presence of an Interferer for Cognitive mmWave Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.15974.pdf)
>  In this paper, we propose beamformed energy detection (BFED) spectrum sensing scheme for a single secondary user (SU) or a cognitive radio (CR). The SU equipped with multiple antennas is used to detect a primary user (PU) transmission in presence of interferer. Saleh-Valenzuela channel model is used to model the mmWave channel for the considered scenario. In the mmWave band due to high attenuation, there are fewer multipaths and the channel is sparse giving rise to fewer direction of arrivals (DoAs). Sensing in only these paths instead of blind energy detection can reap significant benefits. An analog beamforming weight vector is designed such that beamforming gain in the true DoAs of the PU signal is maximized while minimizing interference from the interferer. For this, a single objective function, which is weighted sum of the two objective functions related to beamforming gain and interference, is formed. The proposed sensing scheme is designed under the knowledge of full CSI at the SU for the PU-SU and Interferer-SU channels. However, as the channel information may not be available at the SU, a second BFED sensing scheme is also proposed when no CSI is available, which only tries to estimate the DoAs to reduce the computational complexity. To model the estimates of DoAs, perturbations are added to the true DoAs. The distribution of the test statistics for both the proposed BFED schemes is derived under the null hypothesis so that the threshold of the Neyman-Pearson detector can be found analytically. The performance of both the schemes is also compared with the traditional energy detector for multi-antenna systems.      
### 9.Geometric Total Variation for Image Vectorization, Zooming and Pixel Art Depixelizing  [ :arrow_down: ](https://arxiv.org/pdf/2007.15933.pdf)
>  We propose an original method for vectorizing an image or zooming it at an arbitrary scale. The core of our method relies on the resolution of a geometric variational model and therefore offers theoretic guarantees. More precisely, it associates a total variation energy to every valid triangulation of the image pixels. Its minimization induces a trian-gulation that reflects image gradients. We then exploit this triangulation to precisely locate discontinuities, which can then simply be vectorized or zoomed. This new approach works on arbitrary images without any learning phase. It is particularly appealing for processing images with low quantization like pixel art and can be used for depixelizing such images. The method can be evaluated with an online demonstrator, where users can reproduce results presented here or upload their own images.      
### 10.Large-Scale Noise Characterization of Narrowband Power Line Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.15907.pdf)
>  Noise modeling in power line communications has recently drawn the attention of researchers. However, when characterizing the noise process in narrowband communications, previous works have only focused on small-scale phenomena involving fine-grained details. Nevertheless, the reliability of the communication link is also affected by long-term noise phenomena, that in turn, can be leveraged to maximize transfer rates in higher layers. This paper introduces the concept of large-scale noise characterization for narrowband power line communications and provides a statistical analysis of the long-term trends of power noise. We introduce a statistical description of the noise process in the large-scale time domain based on real field measurements in the FCC band (10 KHz - 490 KHz). The collected data included more than 1.8 billion samples taken from three different locations over a time period of approximately 10 days. The noise samples were statistically analyzed by considering stationarity, autocorrelation, and independence. Finally, we highlight the presence of time series correlations (up to 30 s) and we prove that the noise level variation can be modeled according to a t-location scale distribution, that is characterized by fluctuations bounded to 3dB {\mu}V.      
### 11.Secrecy Outage Probability Analysis for RIS-Assisted NOMA Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.15902.pdf)
>  In this paper, the physical layer security (PLS) for a novel reconfigurable intelligent surface (RIS)-assisted non-orthogonal multiple access (NOMA) system in a multi-user scenario is investigated, where we consider the worst case that the eavesdropper also utilizes the advantage of the RISs. More specifically, we derive analytical results for the secrecy outage probability (SOP). From the numerical results, we observe that the use of RISs can improve the secrecy performance compared to traditional NOMA systems. However, for the worst case that the received signals at the eavesdropper comes from the RISs and source, increasing the number of intelligent elements on the RIS has a negative impact on the secrecy performance. At high SNRs, the system's SOP tends to a constant. Finally, the secrecy performance can be improved through the group selection.      
### 12.A Novel Global Spatial Attention Mechanism in Convolutional Neural Network for Medical Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.15897.pdf)
>  Spatial attention has been introduced to convolutional neural networks (CNNs) for improving both their performance and interpretability in visual tasks including image classification. The essence of the spatial attention is to learn a weight map which represents the relative importance of activations within the same layer or channel. All existing attention mechanisms are local attentions in the sense that weight maps are image-specific. However, in the medical field, there are cases that all the images should share the same weight map because the set of images record the same kind of symptom related to the same object and thereby share the same structural content. In this paper, we thus propose a novel global spatial attention mechanism in CNNs mainly for medical image classification. The global weight map is instantiated by a decision boundary between important pixels and unimportant pixels. And we propose to realize the decision boundary by a binary classifier in which the intensities of all images at a pixel are the features of the pixel. The binary classification is integrated into an image classification CNN and is to be optimized together with the CNN. Experiments on two medical image datasets and one facial expression dataset showed that with the proposed attention, not only the performance of four powerful CNNs which are GoogleNet, VGG, ResNet, and DenseNet can be improved, but also meaningful attended regions can be obtained, which is beneficial for understanding the content of images of a domain.      
### 13.Robust Retinal Vessel Segmentation from a Data Augmentation Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2007.15883.pdf)
>  Retinal vessel segmentation is a fundamental step in screening, diagnosis, and treatment of various cardiovascular and ophthalmic diseases. Robustness is one of the most critical requirements for practical utilization, since the test images may be captured using different fundus cameras, or be affected by various pathological changes. We investigate this problem from a data augmentation perspective, with the merits of no additional training data or inference time. In this paper, we propose two new data augmentation modules, namely, channel-wise random Gamma correction and channel-wise random vessel augmentation. Given a training color fundus image, the former applies random gamma correction on each color channel of the entire image, while the latter intentionally enhances or decreases only the fine-grained blood vessel regions using morphological transformations. With the additional training samples generated by applying these two modules sequentially, a model could learn more invariant and discriminating features against both global and local disturbances. Experimental results on both real-world and synthetic datasets demonstrate that our method can improve the performance and robustness of a classic convolutional neural network architecture. Source codes are available <a class="link-external link-https" href="https://github.com/PaddlePaddle/Research/tree/master/CV/robust_vessel_segmentation" rel="external noopener nofollow">this https URL</a>      
### 14.Residual-CycleGAN based Camera Adaptation for Robust Diabetic Retinopathy Screening  [ :arrow_down: ](https://arxiv.org/pdf/2007.15874.pdf)
>  There are extensive researches focusing on automated diabetic reti-nopathy (DR) detection from fundus images. However, the accuracy drop is ob-served when applying these models in real-world DR screening, where the fun-dus camera brands are different from the ones used to capture the training im-ages. How can we train a classification model on labeled fundus images ac-quired from only one camera brand, yet still achieves good performance on im-ages taken by other brands of cameras? In this paper, we quantitatively verify the impact of fundus camera brands related domain shift on the performance of DR classification models, from an experimental perspective. Further, we pro-pose camera-oriented residual-CycleGAN to mitigate the camera brand differ-ence by domain adaptation and achieve increased classification performance on target camera images. Extensive ablation experiments on both the EyePACS da-taset and a private dataset show that the camera brand difference can signifi-cantly impact the classification performance and prove that our proposed meth-od can effectively improve the model performance on the target domain. We have inferred and labeled the camera brand for each image in the EyePACS da-taset and will publicize the camera brand labels for further research on domain adaptation.      
### 15.Utterance-Wise Meeting Transcription System Using Asynchronous Distributed Microphones  [ :arrow_down: ](https://arxiv.org/pdf/2007.15868.pdf)
>  A novel framework for meeting transcription using asynchronous microphones is proposed in this paper. It consists of audio synchronization, speaker diarization, utterance-wise speech enhancement using guided source separation, automatic speech recognition, and duplication reduction. Doing speaker diarization before speech enhancement enables the system to deal with overlapped speech without considering sampling frequency mismatch between microphones. Evaluation on our real meeting datasets showed that our framework achieved a character error rate (CER) of 28.7 % by using 11 distributed microphones, while a monaural microphone placed on the center of the table had a CER of 38.2 %. We also showed that our framework achieved CER of 21.8 %, which is only 2.1 percentage points higher than the CER in headset microphone-based transcription.      
### 16.A Pyramid Recurrent Network for Predicting Crowdsourced Speech-Quality Ratings of Real-World Signals  [ :arrow_down: ](https://arxiv.org/pdf/2007.15797.pdf)
>  The real-world capabilities of objective speech quality measures are limited since current measures (1) are developed from simulated data that does not adequately model real environments; or they (2) predict objective scores that are not always strongly correlated with subjective ratings. Additionally, a large dataset of real-world signals with listener quality ratings does not currently exist, which would help facilitate real-world assessment. In this paper, we collect and predict the perceptual quality of real-world speech signals that are evaluated by human listeners. We first collect a large quality rating dataset by conducting crowdsourced listening studies on two real-world corpora. We further develop a novel approach that predicts human quality ratings using a pyramid bidirectional long short term memory (pBLSTM) network with an attention mechanism. The results show that the proposed model achieves statistically lower estimation errors than prior assessment approaches, where the predicted scores strongly correlate with human judgments.      
### 17.Inverse NN Modelling of a Piezoelectric Stage with Dominant Variable  [ :arrow_down: ](https://arxiv.org/pdf/2007.15792.pdf)
>  This paper presents an approach for developing a neural network inverse model of a piezoelectric positioning stage, which exhibits rate-dependent, asymmetric hysteresis. It is shown that using both the velocity and the acceleration as inputs results in over-fitting. To overcome this, a rough analytical model of the actuator is derived and by measuring its response to excitation, the velocity signal is identified as the dominant variable. By setting the input space of the neural network to only the dominant variable, an inverse model with good predictive ability is obtained. Training of the network is accomplished using the Levenberg-Marquardt algorithm. Finally, the effectiveness of the proposed approach is experimentally demonstrated.      
### 18.Blending Controllers via Multi-Objective Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2007.15755.pdf)
>  Safety and performance are often two competing objectives in sequential decision-making problems. Existing performant controllers, such as controllers derived from reinforcement learning algorithms, often fall short of safety guarantees. On the contrary, controllers that guarantee safety, such as those derived from classical control theory, require restrictive assumptions and are often conservative in performance. Our goal is to blend a performant and a safe controller to generate a single controller that is safer than the performant and accumulates higher rewards than the safe controller. To this end, we propose a blending algorithm using the framework of contextual multi-armed multi-objective bandits. At each stage, the algorithm observes the environment's current context alongside an immediate reward and cost, which is the underlying safety measure. The algorithm then decides which controller to employ based on its observations. We demonstrate that the algorithm achieves sublinear Pareto regret, a performance measure that models coherence with an expert that always avoids picking the controller with both inferior safety and performance. We derive an upper bound on the loss in individual objectives, which imposes no additional computational complexity. We empirically demonstrate the algorithm's success in blending a safe and a performant controller in a safety-focused testbed, the Safety Gym environment. A statistical analysis of the blended controller's total reward and cost reflects two key takeaways: The blended controller shows a strict improvement in performance compared to the safe controller, and it is safer than the performant controller.      
### 19.Joint DOD and DOA Estimation in Slow-Time MIMO Radar via PARAFAC Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2007.15738.pdf)
>  We develop a new tensor model for slow-time multiple-input multiple output (MIMO) radar and apply it for joint direction-of-departure (DOD) and direction-of-arrival (DOA) estimation. This tensor model aims to exploit the independence of phase modulation matrix and receive array in the received signal for slow-time MIMO radar. Such tensor can be decomposed into two tensors of different ranks, one of which has identical structure to that of the conventional tensor model for MIMO radar, and the other contains all phase modulation values used in the transmit array. We then develop a modification of the alternating least squares algorithm to enable parallel factor decomposition of tensors with extra constants. The Vandermonde structure of the transmit and receive steering matrices (if both arrays are uniform and linear) is then utilized to obtain angle estimates from factor matrices. The multi-linear structure of the received signal is maintained to take advantage of tensor-based angle estimation algorithms, while the shortage of samples in Doppler domain for slow-time MIMO radar is mitigated. As a result, the joint DOD and DOA estimation performance is improved as compared to existing angle estimation techniques for slow-time MIMO radar. Simulation results verify the effectiveness of the proposed method.      
### 20.Detecting Distrust Towards the Skills of a Virtual Assistant Using Speech  [ :arrow_down: ](https://arxiv.org/pdf/2007.15711.pdf)
>  Research has shown that trust is an essential aspect of human-computer interaction directly determining the degree to which the person is willing to use the system. An automatic prediction of the level of trust that a user has on a certain system could be used to attempt to correct potential distrust by having the system take relevant actions like, for example, explaining its actions more thoroughly. In this work, we explore the feasibility of automatically detecting the level of trust that a user has on a virtual assistant (VA) based on their speech. We use a dataset collected for this purpose, containing human-computer speech interactions where subjects were asked to answer various factual questions with the help of a virtual assistant, which they were led to believe was either very reliable or unreliable. We find that the subject's speech can be used to detect which type of VA they were using, which could be considered a proxy for the user's trust toward the VA's abilities, with an accuracy up to 76\%, compared to a random baseline of 50\%. These results are obtained using features that have been previously found useful for detecting speech directed to infants and non-native speakers.      
### 21.Sparse based Super Resolution Multilayer Ultrasonic Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.15691.pdf)
>  In this paper, we model the signal propagation effect in ultrasonic imaging using Huygens principle and use this model to develop sparse signal representation based imaging techniques for a two-layer object immersed in water. Relying on the fact that the image of interest is sparse, we cast such an array based imaging problem as a sparse signal recovery problem and develop two types of imaging methods, one method uses only one transducer to illuminate the region of interest {and for this case the system is modeled as a single input multiple output (SIMO) system. The second method relies on all transducers to transmit ultrasonic waves into the material under test and in this case the system is modeled as a multiple input multiple output (MIMO) system}. We further extend our work to a scenario where the propagation velocity of the wave in the object under test is not known precisely. {We discuss different techniques such as greedy based algorithms as well as $\ell_1$-norm minimization based approach to solve the proposed sparse signal representation based method. We give an assessment of the computational complexity of the $\ell_1$-norm minimization based approach for the SIMO and the MIMO cases. We further point out the superiority of the $\ell_1$-norm minimization based approach over the greedy based algorithms. Then we give a comprehensive analysis of error for both the greedy based approaches as well as the $\ell_1$-norm minimization based technique for both the SIMO and the MIMO cases. The analysis utilizes tools from two powerful branches of modern analysis, \emph{local analysis in Banach spaces} and \emph{concentration of measure}}. We finally apply our methods to experimental data gathered from a solid test sample immersed in water and show that sparse signal recovery based techniques outperform the conventional methods available in the literature.      
### 22.Optimization with Zeroth-Order Oracles in Formation  [ :arrow_down: ](https://arxiv.org/pdf/2007.15680.pdf)
>  In this paper, we consider the optimisation of time varying functions by a network of agents with no gradient information. The proposed a novel method to estimate the gradient at each agent's position using only neighbour information. The gradient estimation is coupled with a formation controller, to minimise gradient estimation error and prevent agent collisions. Convergence results for the algorithm are provided for functions which satisfy the Polyak-Lojasiewicz inequality. Simulations and numerical results are provided to support the theoretical results.      
### 23.Ultra-light deep MIR by trimming lottery tickets  [ :arrow_down: ](https://arxiv.org/pdf/2007.16187.pdf)
>  Current state-of-the-art results in Music Information Retrieval are largely dominated by deep learning approaches. These provide unprecedented accuracy across all tasks. However, the consistently overlooked downside of these models is their stunningly massive complexity, which seems concomitantly crucial to their success. In this paper, we address this issue by proposing a model pruning method based on the lottery ticket hypothesis. We modify the original approach to allow for explicitly removing parameters, through structured trimming of entire units, instead of simply masking individual weights. This leads to models which are effectively lighter in terms of size, memory and number of operations. We show that our proposal can remove up to 90% of the model parameters without loss of accuracy, leading to ultra-light deep MIR models. We confirm the surprising result that, at smaller compression ratios (removing up to 85% of a network), lighter models consistently outperform their heavier counterparts. We exhibit these results on a large array of MIR tasks including audio classification, pitch recognition, chord extraction, drum transcription and onset estimation. The resulting ultra-light deep learning models for MIR can run on CPU, and can even fit on embedded devices with minimal degradation of accuracy.      
### 24.Diet deep generative audio models with structured lottery  [ :arrow_down: ](https://arxiv.org/pdf/2007.16170.pdf)
>  Deep learning models have provided extremely successful solutions in most audio application fields. However, the high accuracy of these models comes at the expense of a tremendous computation cost. This aspect is almost always overlooked in evaluating the quality of proposed models. However, models should not be evaluated without taking into account their complexity. This aspect is especially critical in audio applications, which heavily relies on specialized embedded hardware with real-time constraints. In this paper, we build on recent observations that deep models are highly overparameterized, by studying the lottery ticket hypothesis on deep generative audio models. This hypothesis states that extremely efficient small sub-networks exist in deep models and would provide higher accuracy than larger models if trained in isolation. However, lottery tickets are found by relying on unstructured masking, which means that resulting models do not provide any gain in either disk size or inference time. Instead, we develop here a method aimed at performing structured trimming. We show that this requires to rely on global selection and introduce a specific criterion based on mutual information. First, we confirm the surprising result that smaller models provide higher accuracy than their large counterparts. We further show that we can remove up to 95% of the model weights without significant degradation in accuracy. Hence, we can obtain very light models for generative audio across popular methods such as Wavenet, SING or DDSP, that are up to 100 times smaller with commensurate accuracy. We study the theoretical bounds for embedding these models on Raspberry Pi and Arduino, and show that we can obtain generative models on CPU with equivalent quality as large GPU models. Finally, we discuss the possibility of implementing deep generative audio models on embedded platforms.      
### 25.Paying Per-label Attention for Multi-label Extraction from Radiology Reports  [ :arrow_down: ](https://arxiv.org/pdf/2007.16152.pdf)
>  Training medical image analysis models requires large amounts of expertly annotated data which is time-consuming and expensive to obtain. Images are often accompanied by free-text radiology reports which are a rich source of information. In this paper, we tackle the automated extraction of structured labels from head CT reports for imaging of suspected stroke patients, using deep learning. Firstly, we propose a set of 31 labels which correspond to radiographic findings (e.g. hyperdensity) and clinical impressions (e.g. haemorrhage) related to neurological abnormalities. Secondly, inspired by previous work, we extend existing state-of-the-art neural network models with a label-dependent attention mechanism. Using this mechanism and simple synthetic data augmentation, we are able to robustly extract many labels with a single model, classified according to the radiologist's reporting (positive, uncertain, negative). This approach can be used in further research to effectively extract many labels from medical text.      
### 26.Uncovering the structure of clinical EEG signals with self-supervised learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.16104.pdf)
>  Objective. Supervised learning paradigms are often limited by the amount of labeled data that is available. This phenomenon is particularly problematic in clinically-relevant data, such as electroencephalography (EEG), where labeling can be costly in terms of specialized expertise and human processing time. Consequently, deep learning architectures designed to learn on EEG data have yielded relatively shallow models and performances at best similar to those of traditional feature-based approaches. However, in most situations, unlabeled data is available in abundance. By extracting information from this unlabeled data, it might be possible to reach competitive performance with deep neural networks despite limited access to labels. Approach. We investigated self-supervised learning (SSL), a promising technique for discovering structure in unlabeled data, to learn representations of EEG signals. Specifically, we explored two tasks based on temporal context prediction as well as contrastive predictive coding on two clinically-relevant problems: EEG-based sleep staging and pathology detection. We conducted experiments on two large public datasets with thousands of recordings and performed baseline comparisons with purely supervised and hand-engineered approaches. Main results. Linear classifiers trained on SSL-learned features consistently outperformed purely supervised deep neural networks in low-labeled data regimes while reaching competitive performance when all labels were available. Additionally, the embeddings learned with each method revealed clear latent structures related to physiological and clinical phenomena, such as age effects. Significance. We demonstrate the benefit of self-supervised learning approaches on EEG data. Our results suggest that SSL may pave the way to a wider use of deep learning models on EEG data.      
### 27.Fast computation of all pairs of geodesic distances  [ :arrow_down: ](https://arxiv.org/pdf/2007.16076.pdf)
>  Computing an array of all pairs of geodesic distances between the pixels of an image is time consuming. In the sequel, we introduce new methods exploiting the redundancy of geodesic propagations and compare them to an existing one. We show that our method in which the source point of geodesic propagations is chosen according to its minimum number of distances to the other points, improves the previous method up to 32% and the naive method up to 50% in terms of reduction of the number of operations.      
### 28.A Review on the State of the Art in Non Contact Sensing for COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2007.16063.pdf)
>  COVID-19 disease, caused by SARS-CoV-2, has resulted in a global pandemic recently. With no approved vaccination or treatment, governments around the world have issued guidance to their citizens to remain at home in efforts to control the spread of the disease. The goal of controlling the spread of the virus is to prevent strain on hospital. In this paper, we have focus on how non-invasive methods are being used to detect the COVID-19 and assist healthcare workers in caring for COVID-19 patients. Early detection of the COVID-19 virus can allow for early isolation to prevent further spread. This study outlines the advantages and disadvantages and a breakdown of the methods applied in the current state-of-the-art approaches. In addition, the paper highlights some future research directions, which are required to be explored further to come up with innovative technologies to control this pandemic.      
### 29.Graph signal processing for machine learning: A review and new perspectives  [ :arrow_down: ](https://arxiv.org/pdf/2007.16061.pdf)
>  The effective representation, processing, analysis, and visualization of large-scale structured data, especially those related to complex domains such as networks and graphs, are one of the key questions in modern machine learning. Graph signal processing (GSP), a vibrant branch of signal processing models and algorithms that aims at handling data supported on graphs, opens new paths of research to address this challenge. In this article, we review a few important contributions made by GSP concepts and tools, such as graph filters and transforms, to the development of novel machine learning algorithms. In particular, our discussion focuses on the following three aspects: exploiting data structure and relational priors, improving data and computational efficiency, and enhancing model interpretability. Furthermore, we provide new perspectives on future development of GSP techniques that may serve as a bridge between applied mathematics and signal processing on one side, and machine learning and network science on the other. Cross-fertilization across these different disciplines may help unlock the numerous challenges of complex data analysis in the modern age.      
### 30.Turbo Coded Single User Massive MIMO with Precoding  [ :arrow_down: ](https://arxiv.org/pdf/2007.15959.pdf)
>  Precoding is a method of compensating the channel at the transmitter. This work presents a novel method of data detection in turbo coded single user massive multiple input multiple output (MIMO) systems using precoding. We show via computer simulations that, when precoding is used, re-transmitting the data does not result in significant reduction in bit-error-rate (BER), thus increasing the spectral efficiency, compared to the case without precoding. Moreover, increasing the number of transmit and receive antennas results in improved BER.      
### 31.Neural Style Transfer for Remote Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2007.15920.pdf)
>  The well-known technique outlined in the paper of Leon A. Gatys et al., A Neural Algorithm of Artistic Style, has become a trending topic both in academic literature and industrial applications. Neural Style Transfer (NST) constitutes an essential tool for a wide range of applications, such as artistic stylization of 2D images, user-assisted creation tools and production tools for entertainment applications. The purpose of this study is to present a method for creating artistic maps from satellite images, based on the NST algorithm. This method includes three basic steps (i) application of semantic image segmentation on the original satellite image, dividing its content into classes (i.e. land, water), (ii) application of neural style transfer for each class and (iii) creation of a collage, i.e. an artistic image consisting of a combination of the two stylized image generated on the previous step.      
### 32.Compressively sampling the optical transmission matrix of a multimode fibre  [ :arrow_down: ](https://arxiv.org/pdf/2007.15891.pdf)
>  Measurement of the optical transmission matrix (TM) of an opaque material is an advanced form of space-variant aberration correction. Beyond imaging, TM-based methods are emerging in a range of fields including optical communications, optical micro-manipulation, and optical computing. In many cases the TM is very sensitive to perturbations in the configuration of the scattering medium it represents. Therefore applications often require an up-to-the-minute characterisation of the fragile TM, typically entailing hundreds to thousands of probe measurements. In this work we explore how these measurement requirements can be relaxed using the framework of compressive sensing: incorporation of prior information enables accurate estimation from fewer measurements than the dimensionality of the TM we aim to reconstruct. Examples of such priors include knowledge of a memory effect linking input and output fields, an approximate model of the optical system, or a recent but degraded TM measurement. We demonstrate this concept by reconstructing a full-size TM of a multimode fibre supporting 754 modes at compression ratios down to ~5% with good fidelity. The level of compression achievable is dependent upon the strength of our priors. We show in this case that imaging is still possible using TMs reconstructed at compression ratios down to ~1% (8 probe measurements). This compressive TM sampling strategy is quite general and may be applied to any form of scattering system about which we have some prior knowledge, including diffusers, thin layers of tissue, fibre optics of any known refractive profile, and reflections from opaque walls. These approaches offer a route to measurement of high-dimensional TMs quickly or with access to limited numbers of measurements.      
### 33.Behavioral Economics for Human-in-the-loop Control Systems Design: Overconfidence and the hot hand fallacy  [ :arrow_down: ](https://arxiv.org/pdf/2007.15869.pdf)
>  Successful design of human-in-the-loop control systems requires appropriate models for human decision makers. Whilst most paradigms adopted in the control systems literature hide the (limited) decision capability of humans, in behavioral economics individual decision making and optimization processes are well-known to be affected by perceptual and behavioral biases. Our goal is to enrich control engineering with some insights from behavioral economics research through exposing such biases in control-relevant settings. This paper addresses the following two key questions: 1) How do behavioral biases affect decision making? 2) What is the role played by feedback in human-in-the-loop control systems? Our experimental framework shows how individuals behave when faced with the task of piloting an UAV under risk and uncertainty, paralleling a real-world decision-making scenario. Our findings support the notion of humans in Cyberphysical Systems underlying behavioral biases regardless of -- or even because of -- receiving immediate outcome feedback. We observe substantial shares of drone controllers to act inefficiently through either flying excessively (overconfident) or overly conservatively (underconfident). Furthermore, we observe human-controllers to self-servingly misinterpret random sequences through being subject to a "hot hand fallacy". We advise control engineers to mind the human component in order not to compromise technological accomplishments through human issues.      
### 34.LAVAPilot: Lightweight UAV Trajectory Planner with Situational Awareness for Embedded Autonomy to Track and Locate Radio-tags  [ :arrow_down: ](https://arxiv.org/pdf/2007.15860.pdf)
>  Tracking and locating radio-tagged wildlife is a labor-intensive and time-consuming task necessary in wildlife conservation. In this article, we focus on the problem of achieving embedded autonomy for a resource-limited aerial robot for the task capable of avoiding undesirable disturbances to wildlife. We employ a lightweight sensor system capable of simultaneous (noisy) measurements of radio signal strength information from multiple tags for estimating object locations. We formulate a new lightweight task-based trajectory planning method-LAVAPilot-with a greedy evaluation strategy and a void functional formulation to achieve situational awareness to maintain a safe distance from objects of interest. Conceptually, we embed our intuition of moving closer to reduce the uncertainty of measurements into LAVAPilot instead of employing a computationally intensive information gain based planning strategy. We employ LAVAPilot and the sensor to build a lightweight aerial robot platform with fully embedded autonomy for jointly tracking and planning to track and locate multiple VHF radio collar tags used by conservation biologists. Using extensive Monte Carlo simulation-based experiments, implementations on a single board compute module, and field experiments using an aerial robot platform with multiple VHF radio collar tags, we evaluate our joint planning and tracking algorithms. Further, we compare our method with other information-based planning methods with and without situational awareness to demonstrate the effectiveness of our robot executing LAVAPilot. Our experiments demonstrate that LAVAPilot significantly reduces (by 98.5%) the computational cost of planning to enable real-time planning decisions whilst achieving similar localization accuracy of objects compared to information gain based planning methods, albeit taking a slightly longer time to complete a mission.      
### 35.Strong Stability of Sampled-data Riesz-spectral Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.15846.pdf)
>  Suppose that a continuous-time linear infinite-dimensional system with a static state-feedback controller is strongly stable. We address the following question: If we convert the continuous-time controller to a sampled-data controller by applying an idealized sampler and a zero-order hold, will the resulting sampled-data system be strongly stable for all sufficient small sampling periods? In this paper, we restrict our attention to the situation where the generator of the open-loop system is a Riesz-spectral operator and its point spectrum has a limit point at the origin. We present conditions under which the answer to the above question is affirmative. In the robustness analysis, we show that the sufficient condition for strong stability obtained in the Arendt-Batty-Lyubich-VÅ© theorem is preserved under sampling.      
### 36.Neural Compression and Filtering for Edge-assisted Real-time Object Detection in Challenged Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.15818.pdf)
>  The edge computing paradigm places compute-capable devices - edge servers - at the network edge to assist mobile devices in executing data analysis tasks. Intuitively, offloading compute-intense tasks to edge servers can reduce their execution time. However, poor conditions of the wireless channel connecting the mobile devices to the edge servers may degrade the overall capture-to-output delay achieved by edge offloading. Herein, we focus on edge computing supporting remote object detection by means of Deep Neural Networks (DNNs), and develop a framework to reduce the amount of data transmitted over the wireless link. The core idea we propose builds on recent approaches splitting DNNs into sections - namely head and tail models - executed by the mobile device and edge server, respectively. The wireless link, then, is used to transport the output of the last layer of the head model to the edge server, instead of the DNN input. Most prior work focuses on classification tasks and leaves the DNN structure unaltered. Herein, our focus is on DNNs for three different object detection tasks, which present a much more convoluted structure, and modify the architecture of the network to: (i) achieve in-network compression by introducing a bottleneck layer in the early layers on the head model, and (ii) prefilter pictures that do not contain objects of interest using a convolutional neural network. Results show that the proposed technique represents an effective intermediate option between local and edge computing in a parameter region where these extreme point solutions fail to provide satisfactory performance. We release the code and trained models at <a class="link-external link-https" href="https://github.com/yoshitomo-matsubara/hnd-ghnd-object-detectors" rel="external noopener nofollow">this https URL</a> .      
### 37.Weakly supervised one-stage vision and language disease detection using large scale pneumonia and pneumothorax studies  [ :arrow_down: ](https://arxiv.org/pdf/2007.15778.pdf)
>  Detecting clinically relevant objects in medical images is a challenge despite large datasets due to the lack of detailed labels. To address the label issue, we utilize the scene-level labels with a detection architecture that incorporates natural language information. We present a challenging new set of radiologist paired bounding box and natural language annotations on the publicly available MIMIC-CXR dataset especially focussed on pneumonia and pneumothorax. Along with the dataset, we present a joint vision language weakly supervised transformer layer-selected one-stage dual head detection architecture (LITERATI) alongside strong baseline comparisons with class activation mapping (CAM), gradient CAM, and relevant implementations on the NIH ChestXray-14 and MIMIC-CXR dataset. Borrowing from advances in vision language architectures, the LITERATI method demonstrates joint image and referring expression (objects localized in the image using natural language) input for detection that scales in a purely weakly supervised fashion. The architectural modifications address three obstacles -- implementing a supervised vision and language detection method in a weakly supervised fashion, incorporating clinical referring expression natural language information, and generating high fidelity detections with map probabilities. Nevertheless, the challenging clinical nature of the radiologist annotations including subtle references, multi-instance specifications, and relatively verbose underlying medical reports, ensures the vision language detection task at scale remains stimulating for future investigation.      
### 38.Unidentified Floating Object detection in maritime environment using dictionary learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.15757.pdf)
>  Maritime domain is one of the most challenging scenarios for object detection due to the complexity of the observed scene. In this article, we present a new approach to detect unidentified floating objects in the maritime environment. The proposed approach is capable of detecting floating objects without any prior knowledge of their visual appearance, shape or location. The input image from the video stream is denoised using a visual dictionary learned from a K-SVD algorithm. The denoised image is made of self-similar content. Later, we extract the residual image, which is the difference between the original image and the denoised (self-similar) image. Thus, the residual image contains noise and salient structures (objects). These salient structures can be extracted using an a contrario model. We demonstrate the capabilities of our algorithm by testing it on videos exhibiting varying maritime scenarios.      
### 39.Hearing What You Cannot See: Acoustic Detection Around Corners  [ :arrow_down: ](https://arxiv.org/pdf/2007.15739.pdf)
>  This work proposes to use passive acoustic perception as an additional sensing modality for intelligent vehicles. We demonstrate that approaching vehicles behind blind corners can be detected by sound before such vehicles enter in line-of-sight. We have equipped a hybrid Prius research vehicle with a roof-mounted microphone array, and show on data collected with this sensor setup that wall reflections provide information on the presence and direction of approaching vehicles. A novel method is presented to classify if and from what direction a vehicle is approaching before it is visible, using as input Direction-of-Arrival features that can be efficiently computed from the streaming microphone array data. Since the ego-vehicle position within the local geometry affects the perceived patterns, we systematically study several locations and acoustic environments, and investigate generalization across these environments. With a static ego-vehicle, an accuracy of 92% is achieved on the hidden vehicle classification task, and approaching vehicles are on average detected correctly 2.25 seconds in advance. By stochastic exploring configurations using fewer microphones, we find that on par performance can be achieved with only 7 out of 56 available positions in the array. Finally, we demonstrate positive results on acoustic detection while the vehicle is driving, and study failure cases to identify future research directions.      
### 40.Description of the UPPAAL Models for SRP and CSRP and Verification of their Termination and Consistency Properties  [ :arrow_down: ](https://arxiv.org/pdf/2007.15712.pdf)
>  The IEEE Audio Video Bridging (AVB) Task Group (TG) was created to provide Ethernet with soft real-time guarantees. Later on, the TG was renamed to Time-Sensitive Networking (TSN) and its scope broadened to support hard real-time and critical applications. The Stream Reservation Protocol (SRP) is a key work of the TGs as it allows reserving resources in the network, guaranteeing the required quality of service (QoS). AVB's SRP is based on a distributed architecture, while TSN's is based on centralized ones. The distributed version of SRP is supported and used in TSN. Nevertheless, it was not designed to provide properties that are important for critical applications. Therefore, we propose a new version of SRP with enhanced services called Consistent Stream Reservation Protocol (CSRP). In this document we describe the SRP and CSRP UPPAAL models we developed and the queries we used to verify their termination and consistency properties.      
### 41.PR-NN: RNN-based Detection for Coded Partial-Response Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.15695.pdf)
>  In this paper, we investigate the use of recurrent neural network (RNN)-based detection of magnetic recording channels with inter-symbol interference (ISI). We refer to the proposed detection method, which is intended for recording channels with partial-response equalization, as Partial-Response Neural Network (PR-NN). We train bi-directional gated recurrent units (bi-GRUs) to recover the ISI channel inputs from noisy channel output sequences and evaluate the network performance when applied to continuous, streaming data. The computational complexity of PR-NN during the evaluation process is comparable to that of a Viterbi detector. The recording system on which the experiments were conducted uses a rate-2/3, (1,7) runlength-limited (RLL) code with an E2PR4 partial-response channel target. Experimental results with ideal PR signals show that the performance of PR-NN detection approaches that of Viterbi detection in additive white gaussian noise (AWGN). Moreover, the PR-NN detector outperforms Viterbi detection and achieves the performance of Noise-Predictive Maximum Likelihood (NPML) detection in additive colored noise (ACN) at different channel densities. A PR-NN detector trained with both AWGN and ACN maintains the performance observed under separate training. Similarly, when trained with ACN corresponding to two different channel densities, PR-NN maintains its performance at both densities. Experiments confirm that this robustness is consistent over a wide range of signal-to-noise ratios (SNRs). Finally, PR-NN displays robust performance when applied to a more realistic magnetic recording channel with MMSE-equalized Lorentzian signals.      
### 42.Deep learning for lithological classification of carbonate rock micro-CT images  [ :arrow_down: ](https://arxiv.org/pdf/2007.15693.pdf)
>  In addition to the ongoing development, pre-salt carbonate reservoir characterization remains a challenge, primarily due to inherent geological particularities. These challenges stimulate the use of well-established technologies, such as artificial intelligence algorithms, for image classification tasks. Therefore, this work intends to present an application of deep learning techniques to identify patterns in Brazilian pre-salt carbonate rock microtomographic images, thus making possible lithological classification. Four convolutional neural network models were proposed. The first model includes three convolutional layers followed by fully connected layers and is used as a base model for the following proposals. In the next two models, we replace the max pooling layer with a spatial pyramid pooling and a global average pooling layer. The last model uses a combination of spatial pyramid pooling followed by global average pooling in place of the last pooling layer. All models are compared using original images, when possible, as well as resized images. The dataset consists of 6,000 images from three different classes. The model performances were evaluated by each image individually, as well as by the most frequently predicted class for each sample. According to accuracy, Model 2 trained on resized images achieved the best results, reaching an average of 75.54% for the first evaluation approach and an average of 81.33% for the second. We developed a workflow to automate and accelerate the lithology classification of Brazilian pre-salt carbonate samples by categorizing microtomographic images using deep learning algorithms in a non-destructive way.      
### 43.Model-driven reconstruction for highly-oversampled MRI  [ :arrow_down: ](https://arxiv.org/pdf/2007.15674.pdf)
>  The Nyquist-Shannon theorem states that the information accessible by discrete Fourier protocols saturates when the sampling rate reaches twice the bandwidth of the detected continuous time signal. This maximum rate (the NS-limit) plays a prominent role in Magnetic Resonance Imaging (MRI). Nevertheless, reconstruction methods other than Fourier analysis can extract useful information from data oversampled with respect to the NS-limit, given that relevant prior knowledge is available. Here we present OverSampled MRI (OS-MRI), a method that exploits explicit prior knowledge of the highly controlled physical interactions between electromagnetic fields and the sample spins in MRI systems. Our simulations indicate that OS-MRI can be used for scan acceleration and suppression of noise effects in relevant scenarios by oversampling along frequency-encoded directions, which is innocuous in MRI systems under reasonable conditions. We find situations in which the reconstruction quality can be higher than with NS-limited acquisitions and traditional Fourier reconstruction. Besides, we compare the performance of a variety of encoding pulse sequences as well as image reconstruction protocols, and find that accelerated spiral trajectories in k-space combined with algebraic reconstruction techniques are particularly advantageous.      
### 44.Canopy Density Estimation in Perennial Horticulture Crops Using 3D Spinning LiDAR SLAM  [ :arrow_down: ](https://arxiv.org/pdf/2007.15652.pdf)
>  We propose a 3D ray cloud based method for estimating the canopy density of vineyards and orchards that have been scanned by a vehicle-mounted mobile 3D spinning LiDAR (AgScan3D). The method is composed of two parts. Firstly, the AgScan3D data is processed through a Continuous-Time SLAM algorithm into a globally registered 3D ray cloud. The global ray cloud is a canonical data format (a digital twin) from which we can compare vineyard snapshots over multiple times within a season and across seasons. Secondly, the vineyard rows are automatically extracted from the ray cloud and a novel density calculation is performed to estimate the maximum likelihood canopy densities of the vineyard. This combination of digital twinning, together with the accurate extraction of canopy structure information, allows entire vineyards to be analysed and compared, across the growing season and from year to year. The proposed method is evaluated both in simulation and field experiments. Field experiments were performed at four sites, which varied in vineyard structure and vine management, over two growing seasons, resulting in a total traversal of 160 kilometres, 42.4 scanned hectares of vines with a combined total of approximately 93000 of scanned vines. The field datasets and ray cloud library will be released publicly.      
