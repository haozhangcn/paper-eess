# ArXiv eess --Mon, 13 Jul 2020
### 1.Predicting Bit Error Rate from Meta Information using Random Forests  [ :arrow_down: ](https://arxiv.org/pdf/2007.05503.pdf)
>  With the increasing power of machine learning-based reasoning, the use of meta-information (e.g., digital signal modulation parameters, channel conditions, etc.) to predict the performance of various signal processing techniques has become feasible. One such problem of practical interest is choosing a proper interference mitigation method based on the meta information of the received signal. Since heuristic table-based methods suffer from limited prediction capability for unseen cases, we propose a recommendation system based on the use of Random Forests (RF). Specifically, RF used to predict the Bit-Error-Rate (BER) of all mitigation approaches so as to determine the approach with the best performance. We found RF can predict BER with high accuracy, and its importance factor demonstrates which input attributes matter most. These BER prediction results can also benefit other functions such as adaptive modulation, channel sensing, beaming selection, etc.      
### 2.Joint Information Theoretic Secrecy and Covert Communication in the Presence of an Untrusted User and Warden  [ :arrow_down: ](https://arxiv.org/pdf/2007.05502.pdf)
>  In this paper, we investigate joint information theoretic secrecy and covert communication in a single-input multi-output (SIMO) system where a transmitter (Alice) is communicating with two legitimate users (Bob and Carol). We consider that an untrusted user and a warden node are also present in the network attempting to attack the secure and covert communications to Bob and Carol, respectively. Specifically, Bob requires secure communications such that his messages from Alice are not decoded by the untrusted user, while Carol requires covert communications such that her messages from Alice are not detected by the warden. <br>To do so, we consider that Alice transmits Carol's messages during selected time slots to hide them from the warden while also transmitting Bob's messages in each time slot contentiously. <br>We formulate an optimization problem with the aim of maximizing the average rate subject to a covert communication requirement and a secure communications constraint. <br>Since the proposed optimization problem is non-convex, we utilize successive convex approximation to obtain a tractable solution. We consider practical assumptions that Alice has imperfect knowledge of the warden's location and imperfect channel state information (CSI) of Bob and Carol. <br>Our numerical examples highlight that the imperfect CSI at Carol has a more detrimental impact on the average rate compared to imperfect CSI at Bob.      
### 3.Automatic Detection of COVID-19 Cases on X-ray images Using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.05494.pdf)
>  In recent months the world has been surprised by the rapid advance of COVID-19. In order to face this disease and minimize its socio-economic impacts, in addition to surveillance and treatment, diagnosis is a crucial procedure. However, the realization of this is hampered by the delay and the limited access to laboratory tests, demanding new strategies to carry out case triage. In this scenario, deep learning models are being proposed as a possible option to assist the diagnostic process based on chest X-ray and computed tomography images. Therefore, this research aims to automate the process of detecting COVID-19 cases from chest images, using convolutional neural networks (CNN) through deep learning techniques. The results can contribute to expand access to other forms of detection of COVID-19 and to speed up the process of identifying this disease. All databases used, the codes built, and the results obtained from the models' training are available for open access. This action facilitates the involvement of other researchers in enhancing these models since this can contribute to the improvement of results and, consequently, the progress in confronting COVID-19.      
### 4.Learning from Data to Optimize Control in Precision Farming  [ :arrow_down: ](https://arxiv.org/pdf/2007.05493.pdf)
>  Precision farming is one way of many to meet a 70 percent increase in global demand for agricultural products on current agricultural land by 2050 at reduced need of fertilizers and efficient use of water resources. The catalyst for the emergence of precision farming has been satellite positioning and navigation followed by Internet-of-Things, generating vast information that can be used to optimize farming processes in real-time. Statistical tools from data mining, predictive modeling, and machine learning analyze pattern in historical data, to make predictions about future events as well as intelligent actions. This special issue presents the latest development in statistical inference, machine learning and optimum control for precision farming.      
### 5.XSleepNet: Multi-View Sequential Model for Automatic Sleep Staging  [ :arrow_down: ](https://arxiv.org/pdf/2007.05492.pdf)
>  Automating sleep staging is vital to scale up sleep assessment and diagnosis to millions of people experiencing sleep deprivation and disorders and to enable longitudinal sleep monitoring in home environments. Learning from raw polysomnography signals and their derived time-frequency images has been prevalent. However, learning from multi-view inputs (e.g. both the raw signals and the time-frequency images) for sleep staging is difficult and not well understood. This work proposes a sequence-to-sequence sleep staging model, XSleepNet, that is capable of learning a joint representation from both raw signals and time-frequency images effectively. Since different views often generalize (and overfit) at different rates, the proposed network is trained in such a way that the learning pace on each view is adapted based on their generalization/overfitting behavior. In simple terms, the learning on a particular view is speeded up when it is generalizing well and slowed down when it is overfitting. View-specific generalization/overfitting measures are computed on-the-fly during the training course and used to derive weights to blend the gradients from different views. As a result, the network is able to retain representation power of different views in the joint features which represent the underlying distribution better than those learned by each individual view alone. Furthermore, the XSleepNet architecture is principally designed to gain robustness to the amount of training data and to increase the complementarity between the input views. Experimental results on five databases of different size show that XSleepNet consistently results in better performance than the single-view baselines as well as the multi-view baseline with a simple fusion strategy. Finally, XSleepNet outperforms all prior sleep staging methods and sets new state-of-the-art results on the experimental databases.      
### 6.A Reinforcement Learning Approach for Fast Frequency Control in Low-Inertia Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.05474.pdf)
>  The electric grid is undergoing a major transition from fossil fuel-based power generation to renewable energy sources, typically interfaced to the grid via power electronics. The future power systems are thus expected to face increased control complexity and challenges pertaining to frequency stability due to lower levels of inertia and damping. As a result, the frequency control and development of novel ancillary services is becoming imperative. This paper proposes a data-driven control scheme, based on Reinforcement Learning (RL), for grid-forming Voltage Source Converters (VSCs), with the goal of exploiting their fast response capabilities to provide fast frequency control to the system. A centralized RL-based controller collects generator frequencies and adjusts the VSC power output, in response to a disturbance, to prevent frequency threshold violations. The proposed control scheme is analyzed and its performance evaluated through detailed time-domain simulations of the IEEE 14-bus test system.      
### 7.SIMBA: Specific Identity Markers for Bone Age Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2007.05454.pdf)
>  Bone Age Assessment (BAA) is a task performed by radiologists to diagnose abnormal growth in a child. In manual approaches, radiologists take into account different identity markers when calculating bone age, i.e., chronological age and gender. However, the current automated Bone Age Assessment methods do not completely exploit the information present in the patient's metadata. With this lack of available methods as motivation, we present SIMBA: Specific Identity Markers for Bone Age Assessment. SIMBA is a novel approach for the task of BAA based on the use of identity markers. For this purpose, we build upon the state-of-the-art model, fusing the information present in the identity markers with the visual features created from the original hand radiograph. We then use this robust representation to estimate the patient's relative bone age: the difference between chronological age and bone age. We validate SIMBA on the Radiological Hand Pose Estimation dataset and find that it outperforms previous state-of-the-art methods. SIMBA sets a trend of a new wave of Computer-aided Diagnosis methods that incorporate all of the data that is available regarding a patient. To promote further research in this area and ensure reproducibility we will provide the source code as well as the pre-trained models of SIMBA.      
### 8.Evaluation of Big Data based CNN Models in Classification of Skin Lesions with Melanoma  [ :arrow_down: ](https://arxiv.org/pdf/2007.05446.pdf)
>  This chapter presents a methodology for diagnosis of pigmented skin lesions using convolutional neural networks. The architecture is based on convolu-tional neural networks and it is evaluated using new CNN models as well as re-trained modification of pre-existing CNN models were used. The experi-mental results showed that CNN models pre-trained on big datasets for gen-eral purpose image classification when re-trained in order to identify skin le-sion types offer more accurate results when compared to convolutional neural network models trained explicitly from the dermatoscopic images. The best performance was achieved by re-training a modified version of ResNet-50 convolutional neural network with accuracy equal to 93.89%. Analysis on skin lesion pathology type was also performed with classification accuracy for melanoma and basal cell carcinoma being equal to 79.13% and 82.88%, respectively.      
### 9.Nonlinear Temperature Regulation of Solar Collectors with a Fast Adaptive Polytopic LPV MPC Formulation  [ :arrow_down: ](https://arxiv.org/pdf/2007.05445.pdf)
>  Temperature control in solar collectors is a nonlinear problem: the dynamics of temperature rise vary according to the oil flowing through the collector and to the temperature gradient along the collector area. In this way, this work investigates the formulation of a Model Predictive Control (MPC) application developed within a Linear Parameter Varying (LPV) formalism, which serves as a model of the solar collector process. The proposed system is an adaptive MPC, developed with terminal set constraints and considering the scheduling polytope of the model. At each instant, two Quadratic Programming (QPs) programs are solved: the first considers a backward horizon of N steps to find a virtual model-process tuning variable that defines the best LTI prediction model, considering the vertices of the polytopic system; then, the second QP uses this LTI model to optimize performances along a forward horizon of N steps. The paper ends with a realistic solar collector simulation results, comparing the proposed MPC to other techniques from the literature (linear MPC and robust tube-MPC). Discussions regarding the results, the design procedure and the computational effort for the three methods are presented. It is shown how the proposed MPC design is able to outrank these other standard methods in terms of reference tracking and disturbance rejection.      
### 10.Joint Blind Deconvolution and Robust Principal Component Analysis for Blood Flow Estimation in Medical Ultrasound Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.05428.pdf)
>  This paper addresses the problem of high-resolution Doppler blood flow estimation from an ultrafast sequence of ultrasound images. Formulating the separation of clutter and blood components as an inverse problem has been shown in the literature to be a good alternative to spatio-temporal singular value decomposition (SVD)-based clutter filtering. In particular, a deconvolution step has recently been embedded in such a problem to mitigate the influence of the experimentally measured point spread function (PSF) of the imaging system. Deconvolution was shown in this context to improve the accuracy of the blood flow reconstruction. However, measuring the PSF requires non-trivial experimental setups. To overcome this limitation, we propose herein a blind deconvolution method able to estimate both the blood component and the PSF from Doppler data. Numerical experiments conducted on simulated and in vivo data demonstrate qualitatively and quantitatively the effectiveness of the proposed approach in comparison with the previous method based on experimentally measured PSF and two other state-of-the-art approaches.      
### 11.Fundamental Performance Limits of mm-Wave Cooperative Localization in Linear Topologies  [ :arrow_down: ](https://arxiv.org/pdf/2007.05419.pdf)
>  In applications such as seismic acquisition, the position information of sensor nodes, that are deployed in a linear topology, is desired with sub-meter accuracy in the presence of a limited number of anchor nodes. This can be achieved with antenna arrays via mm-wave cooperative localization, whose performance limits are derived in this letter. The number of anchor nodes is seen to have a stronger impact than the number of antenna elements in the anchor nodes. Succinct closed-form expressions for the position error bound are also obtained for 1-hop and 2-hop cooperative localization, where sub-meter accuracy is perceived over several hundred nodes.      
### 12.Recognition of Instrument-Tissue Interactions in Endoscopic Videos via Action Triplets  [ :arrow_down: ](https://arxiv.org/pdf/2007.05405.pdf)
>  Recognition of surgical activity is an essential component to develop context-aware decision support for the operating room. In this work, we tackle the recognition of fine-grained activities, modeled as action triplets &lt;instrument, verb, target&gt; representing the tool activity. To this end, we introduce a new laparoscopic dataset, CholecT40, consisting of 40 videos from the public dataset Cholec80 in which all frames have been annotated using 128 triplet classes. Furthermore, we present an approach to recognize these triplets directly from the video data. It relies on a module called Class Activation Guide (CAG), which uses the instrument activation maps to guide the verb and target recognition. To model the recognition of multiple triplets in the same frame, we also propose a trainable 3D Interaction Space, which captures the associations between the triplet components. Finally, we demonstrate the significance of these contributions via several ablation studies and comparisons to baselines on CholecT40.      
### 13.Towards accurate simulations of individual speech recognition benefits with real hearing aids with FADE  [ :arrow_down: ](https://arxiv.org/pdf/2007.05378.pdf)
>  Developing and selecting hearing aids is a time consuming process which could be simplified by using objective models. The framework for auditory discrimination experiments (FADE) accurately simulated the benefit of hearing aid algorithms. One simulation with FADE requires several hours of (un)processed signals, which is obstructive when the signals have to be recorded. We propose and evaluate a real-time optimized and data-reduced FADE version ("ROR-FADE") which enables simulations of speech recognition thresholds (SRTs) with about 30 minutes of recorded and potentially processed signals of the (German) matrix sentence test. SRTs were simulated for different noise maskers, degrees of hearing loss, and with simulated hearing aids. At last, speech recognition with three pairs of real hearing aids was simulated. The differences between ROR-FADE and FADE were small for stationary maskers (1 dB), but larger with strongly fluctuating maskers (5 dB). Hearing impairment and hearing aid algorithms seemed to reduced the differences. Hearing aid benefits were found in silence ($\geq$8 dB), in stationary and fluctuating maskers in co-located (stat. 2 dB; fluct. 6 dB), and spatially separated speech and noise signals (stat. $\geq$8 dB; fluct. 8 dB). The simulations were plausible in comparison to data from literature, but a comparison with empirical data is still open. ROR-FADE simulates SRTs in about 30 minutes in any setup that uses matrix sentences. Therefore, it facilitates objective SRT simulations with real devices with unknown signal processing in real environments. Consequently, ROR-FADE could be used for model-based hearing aid fitting or for the development of hearing aids.      
### 14.Effect of Objective Function on Data-Driven Sparse Sensor Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.05377.pdf)
>  The selection problem of an optimal set of sensors estimating the snapshot of high-dimensional data is considered. The objective functions based on various criteria of optimal design are adopted to the greedy method: D-optimality, A-optimality, and E-optimality, which maximizes the determinant, minimize the trace of inverse, and maximize the minimum eigenvalue of the Fisher information matrix, respectively. First, the Fisher information matrix is derived depending on the numbers of latent state variables and sensors. Then, the unified formulation of the objective function based on A-optimality is introduced and proved to be submodular, which provides the lower bound on the performance of the greedy method. Next, the greedy methods based on D-, A-, and E-optimality are applied to randomly generated systems and a practical data set of global climates. The sensors selected by the D-optimality objective function works better than those by A- and E-optimality with regard to the determinant, trace of the inverse, and reconstruction error, while those by A-optimality works the best with regard to the minimum eigenvalue. On the other hand, the performance of sensors selected by the E-optimality objective function is worse for all indices and reconstruction error. This might be because of the lack of submodularity as proved in the paper. The results indicate that the greedy method based on D-optimality is the most suitable for high accurate reconstruction with low computational cost.      
### 15.Semi-supervised Task-driven Data Augmentation for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.05363.pdf)
>  Supervised learning-based segmentation methods typically require a large number of annotated training data to generalize well at test time. In medical applications, curating such datasets is not a favourable option because acquiring a large number of annotated samples from experts is time-consuming and expensive. Consequently, numerous methods have been proposed in the literature for learning with limited annotated examples. Unfortunately, the proposed approaches in the literature have not yet yielded significant gains over random data augmentation for image segmentation, where random augmentations themselves do not yield high accuracy. In this work, we propose a novel task-driven data augmentation method for learning with limited labeled data where the synthetic data generator, is optimized for the segmentation task. The generator of the proposed method models intensity and shape variations using two sets of transformations, as additive intensity transformations and deformation fields. Both transformations are optimized using labeled as well as unlabeled examples in a semi-supervised framework. Our experiments on three medical datasets, namely cardic, prostate and pancreas, show that the proposed approach significantly outperforms standard augmentation and semi-supervised approaches for image segmentation in the limited annotation setting. The code is made publicly available at <a class="link-external link-https" href="https://github.com/krishnabits001/task$" rel="external noopener nofollow">this https URL</a>\_$driven$\_$data$\_$augmentation.      
### 16.Monitoring of Critical Infrastructures by Micro-Motion Estimation: the Mosul Dam Destabilization  [ :arrow_down: ](https://arxiv.org/pdf/2007.05326.pdf)
>  In this paper, authors propose a new procedure to provide a tool for monitoring critical infrastructures. Particularly, through the analysis of COSMO-SkyMed satellite data, a detailed and updated survey is provided, for monitoring the accelerating destabilization process of the Mosul dam, that represents the largest hydraulic facility of Iraq and is located on the Tigris river. The destructive potential of the wave that would be generated, in the event of the dam destruction, could have serious consequences. If the concern for human lives comes first, the concern for cultural heritage protection is not negligible, since several archaeological sites are located around the Mosul dam. The proposed procedure is an in-depth modal assessment based on the micro-motion estimation, through a Doppler sub-apertures tracking and a Multi-Chromatic Analysis (MCA). The method is based initially on the Persistent Scatterers Interferometry (PSI) that is also discussed for completeness and validation. The modal analysis has detected the presence of several areas of resonance that could mean the presence of cracks, and the results have shown that the dam is still in a strong destabilization. Moreover, the dam appears to be divided into two parts: the northern part is accelerating rapidly while the southern part is decelerating and a main crack in this north-south junction is found. The estimated velocities through the PS-InSAR technique show a good agreement with the GNSS in-situ measurements, resulting in a very high correlation coefficient and showing how the proposed procedure works efficiently.      
### 17.A distance-based loss for smooth and continuous skin layer segmentation in optoacoustic images  [ :arrow_down: ](https://arxiv.org/pdf/2007.05324.pdf)
>  Raster-scan optoacoustic mesoscopy (RSOM) is a powerful, non-invasive optical imaging technique for functional, anatomical, and molecular skin and tissue analysis. However, both the manual and the automated analysis of such images are challenging, because the RSOM images have very low contrast, poor signal to noise ratio, and systematic overlaps between the absorption spectra of melanin and hemoglobin. Nonetheless, the segmentation of the epidermis layer is a crucial step for many downstream medical and diagnostic tasks, such as vessel segmentation or monitoring of cancer progression. We propose a novel, shape-specific loss function that overcomes discontinuous segmentations and achieves smooth segmentation surfaces while preserving the same volumetric Dice and IoU. Further, we validate our epidermis segmentation through the sensitivity of vessel segmentation. We found a 20 $\%$ improvement in Dice for vessel segmentation tasks when the epidermis mask is provided as additional information to the vessel segmentation network.      
### 18.ID-Conditioned Auto-Encoder for Unsupervised Anomaly Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.05314.pdf)
>  In this paper, we introduce ID-Conditioned Auto-Encoder for unsupervised anomaly detection. Our method is an adaptation of the Class-Conditioned Auto-Encoder (C2AE) designed for the open-set recognition. Assuming that non-anomalous samples constitute of distinct IDs, we apply Conditioned Auto-Encoder with labels provided by these IDs. Opposed to C2AE, our approach omits the classification subtask and reduces the learning process to the single run. We simplify the learning process further by fixing a constant vector as the target for non-matching labels. We apply our method in the context of sounds for machine condition monitoring. We evaluate our method on the ToyADMOS and MIMII datasets from the DCASE 2020 Challenge Task 2. We conduct an ablation study to indicate which steps of our method influences results the most.      
### 19.Control Hardware-in-the-loop for Voltage Controlled Inverters with Unbalanced and Non-linear Loads in Stand-alone Photovoltaic(PV) Islanded Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2007.05306.pdf)
>  Unbalanced and nonlinear loads connected to microgrids (MG) with local distributed energy resources (DERs) are two of the leading causes of power quality problems. Nonlinearloads introduce voltage and current harmonics, and single-phase loads can cause voltage and current imbalances in a three-phase network. This paper presents a hierarchical control scheme for voltage-controlled photovoltaic (PV) inverters with unbalanced and nonlinear loads in micro-grids. The hierarchical control consists of primary control and voltage compensation control(VCC) and a DC voltage regulator (VR). The primary control scheme controls active and reactive power-sharing and the VCCregulates the unbalanced voltage and harmonics distortion. The effectiveness of the scheme is verified using Opal-RT real-time simulation and experimentally using control hardware-in-the-loop. The voltage distortion at the point of common coupling (PCC)decreased from 6.38 percent to 1.91 percent after compensation, while the unbalanced and harmonic loads are shared proportionally among the DG units.      
### 20.Deep Learning-Based Regression and Classification for Automatic Landmark Localization in Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.05295.pdf)
>  In this study, we propose a fast and accurate method to automatically localize anatomical landmarks in medical images. We employ a global-to-local localization approach using fully convolutional neural networks (FCNNs). First, a global FCNN localizes multiple landmarks through the analysis of image patches, performing regression and classification simultaneously. In regression, displacement vectors pointing from the center of image patches towards landmark locations are determined. In classification, presence of landmarks of interest in the patch is established. Global landmark locations are obtained by averaging the predicted displacement vectors, where the contribution of each displacement vector is weighted by the posterior classification probability of the patch that it is pointing from. Subsequently, for each landmark localized with global localization, local analysis is performed. Specialized FCNNs refine the global landmark locations by analyzing local sub-images in a similar manner, i.e. by performing regression and classification simultaneously and combining the results. Evaluation was performed through localization of 8 anatomical landmarks in CCTA scans, 2 landmarks in olfactory MR scans, and 19 landmarks in cephalometric X-rays. We demonstrate that the method performs similarly to a second observer and is able to localize landmarks in a diverse set of medical images, differing in image modality, image dimensionality, and anatomical coverage.      
### 21.Towards Fast, Flexible and Sensor-Free Control of Standalone PVDG Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.05266.pdf)
>  In this thesis, the problem of fast, effective and low cost control of a Standalone Photovoltaic Distributed Generation (SPVDG) system is considered . On-site generation from these systems is more efficient when the power is transmitted via DC due to elimination of transmission losses and needless energy conversions. The inherent low-inertia of these systems added with fluctuation of output power and uncertain load consumption, calls for advanced control techniques to ensure fast and stable operation during various intermittencies. These techniques are expensive since they demand installation of many sophisticated sensors. The computation power provided by the fast growing IC technology can be utilized to estimate different parameters in a system and reduce the need for expensive sensing equipment. This work provides solutions to problems encountered in the development of faster, more stable and sensor-free voltage control and maximum power point tracking(MPPT) for SPVDG systems with PV and battery.      
### 22.PoCET: a Polynomial Chaos Expansion Toolbox for Matlab  [ :arrow_down: ](https://arxiv.org/pdf/2007.05245.pdf)
>  We introduce PoCET: a free and open-scource Polynomial Chaos Expansion Toolbox for Matlab, featuring the automatic generation of polynomial chaos expansion (PCE) for linear and nonlinear dynamic systems with time-invariant stochastic parameters or initial conditions, as well as several simulation tools. It offers a built-in handling of Gaussian, uniform, and beta probability density functions, projection and collocation-based calculation of PCE coefficients, and the calculation of stochastic moments from a PCE. Efficient algorithms for the calculation of the involved integrals have been designed in order to increase its applicability. PoCET comes with a variety of introductory and instructive examples. Throughout the paper we show how to perform a polynomial chaos expansion on a simple ordinary differential equation using PoCET, as well as how it can be used to solve the more complex task of optimal experimental design.      
### 23.Cross-Attention in Coupled Unmixing Nets for Unsupervised Hyperspectral Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.05230.pdf)
>  The recent advancement of deep learning techniques has made great progress on hyperspectral image super-resolution (HSI-SR). Yet the development of unsupervised deep networks remains challenging for this task. To this end, we propose a novel coupled unmixing network with a cross-attention mechanism, CUCaNet for short, to enhance the spatial resolution of HSI by means of higher-spatial-resolution multispectral image (MSI). Inspired by coupled spectral unmixing, a two-stream convolutional autoencoder framework is taken as backbone to jointly decompose MS and HS data into a spectrally meaningful basis and corresponding coefficients. CUCaNet is capable of adaptively learning spectral and spatial response functions from HS-MS correspondences by enforcing reasonable consistency assumptions on the networks. Moreover, a cross-attention module is devised to yield more effective spatial-spectral information transfer in networks. Extensive experiments are conducted on three widely-used HS-MS datasets in comparison with state-of-the-art HSI-SR models, demonstrating the superiority of the CUCaNet in the HSI-SR application. Furthermore, the codes and datasets will be available at: <a class="link-external link-https" href="https://github.com/danfenghong/ECCV2020_CUCaNet" rel="external noopener nofollow">this https URL</a>.      
### 24.Automatic Segmentation of Non-Tumor Tissues in Glioma MR Brain Images Using Deformable Registration with Partial Convolutional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.05224.pdf)
>  In brain tumor diagnosis and surgical planning, segmentation of tumor regions and accurate analysis of surrounding normal tissues are necessary for physicians. Pathological variability often renders difficulty to register a well-labeled normal atlas to such images and to automatic segment/label surrounding normal brain tissues. In this paper, we propose a new registration approach that first segments brain tumor using a U-Net and then simulates missed normal tissues within the tumor region using a partial convolutional network. Then, a standard normal brain atlas image is registered onto such tumor-removed images in order to segment/label the normal brain tissues. In this way, our new approach greatly reduces the effects of pathological variability in deformable registration and segments the normal tissues surrounding brain tumor well. In experiments, we used MICCAI BraTS2018 T1 tumor images to evaluate the proposed algorithm. By comparing direct registration with the proposed algorithm, the results showed that the Dice coefficient for gray matters was significantly improved for surrounding normal brain tissues.      
### 25.Efficient Unpaired Image Dehazing with Cyclic Perceptual-Depth Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2007.05220.pdf)
>  Image dehazing without paired haze-free images is of immense importance, as acquiring paired images often entails significant cost. However, we observe that previous unpaired image dehazing approaches tend to suffer from performance degradation near depth borders, where depth tends to vary abruptly. Hence, we propose to anneal the depth border degradation in unpaired image dehazing with cyclic perceptual-depth supervision. Coupled with the dual-path feature re-using backbones of the generators and discriminators, our model achieves $\mathbf{20.36}$ Peak Signal-to-Noise Ratio (PSNR) on NYU Depth V2 dataset, significantly outperforming its predecessors with reduced Floating Point Operations (FLOPs).      
### 26.Gated Recurrent Context: Softmax-free attention for Online Encoder-Decoder Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.05214.pdf)
>  Recently, attention-based encoder-decoder (AED) models have shown state-of-the-art performance in automatic speech recognition (ASR). As the original AED models with global attentions are not capable of online inference, various online attention schemes have been developed to reduce ASR latency for better user experience. However, a common limitation of the conventional softmax-based online attention approaches is that they introduce an additional hyperparameter related to the length of the attention window, requiring multiple trials of model training for tuning the hyperparameter. In order to deal with this problem, we propose a novel softmax-free attention method and its modified formulation for online attention, which does not need any additional hyperparameter at the training phase. Through a number of ASR experiments, we demonstrate the tradeoff between the latency and performance of the proposed online attention technique can be controlled by merely adjusting a threshold at the test phase. Furthermore, the proposed methods showed competitive performance to the conventional global and online attentions in terms of word-error-rates (WERs).      
### 27.A Unifying Framework for Adaptive Radar Detection in the Presence of Multiple Alternative Hypotheses  [ :arrow_down: ](https://arxiv.org/pdf/2007.05207.pdf)
>  In this paper, we develop a new elegant framework relying on the Kullback-Leibler Information Criterion to address the design of one-stage adaptive detection architectures for multiple hypothesis testing problems. Specifically, at the design stage, we assume that several alternative hypotheses may be in force and that only one null hypothesis exists. Then, starting from the case where all the parameters are known and proceeding until the case where the adaptivity with respect to the entire parameter set is required, we come up with decision schemes for multiple alternative hypotheses consisting of the sum between the compressed log-likelihood ratio based upon the available data and a penalty term accounting for the number of unknown parameters. The latter rises from suitable approximations of the Kullback-Leibler Divergence between the true and a candidate probability density function. Interestingly, under specific constraints, the proposed decision schemes can share the constant false alarm rate property by virtue of the Invariance Principle. Finally, we show the effectiveness of the proposed framework through the application to examples of practical value in the context of radar detection also in comparison with two-stage competitors. This analysis highlights that the architectures devised within the proposed framework represent an effective means to deal with detection problems where the uncertainty on some parameters leads to multiple alternative hypotheses.      
### 28.OT-driven Multi-Domain Unsupervised Ultrasound Image Artifact Removal using a Single CNN  [ :arrow_down: ](https://arxiv.org/pdf/2007.05205.pdf)
>  Ultrasound imaging (US) often suffers from distinct image artifacts from various sources. Classic approaches for solving these problems are usually model-based iterative approaches that have been developed specifically for each type of artifact, which are often computationally intensive. Recently, deep learning approaches have been proposed as computationally efficient and high performance alternatives. Unfortunately, in the current deep learning approaches, a dedicated neural network should be trained with matched training data for each specific artifact type. This poses a fundamental limitation in the practical use of deep learning for US, since large number of models should be stored to deal with various US image artifacts. Inspired by the recent success of multi-domain image transfer, here we propose a novel, unsupervised, deep learning approach in which a single neural network can be used to deal with different types of US artifacts simply by changing a mask vector that switches between different target domains. Our algorithm is rigorously derived using an optimal transport (OT) theory for cascaded probability measures. Experimental results using phantom and in vivo data demonstrate that the proposed method can generate high quality image by removing distinct artifacts, which are comparable to those obtained by separately trained multiple neural networks.      
### 29.ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.05201.pdf)
>  Optical Coherence Tomography Angiography (OCT-A) is a non-invasive imaging technique, and has been increasingly used to image the retinal vasculature at capillary level resolution. However, automated segmentation of retinal vessels in OCT-A has been under-studied due to various challenges such as low capillary visibility and high vessel complexity, despite its significance in understanding many eye-related diseases. In addition, there is no publicly available OCT-A dataset with manually graded vessels for training and validation. To address these issues, for the first time in the field of retinal image analysis we construct a dedicated Retinal OCT-A SEgmentation dataset (ROSE), which consists of 229 OCT-A images with vessel annotations at either centerline-level or pixel level. This dataset has been released for public access to assist researchers in the community in undertaking research in related topics. Secondly, we propose a novel Split-based Coarse-to-Fine vessel segmentation network (SCF-Net), with the ability to detect thick and thin vessels separately. In the SCF-Net, a split-based coarse segmentation (SCS) module is first introduced to produce a preliminary confidence map of vessels, and a split-based refinement (SRN) module is then used to optimize the shape/contour of the retinal microvasculature. Thirdly, we perform a thorough evaluation of the state-of-the-art vessel segmentation models and our SCF-Net on the proposed ROSE dataset. The experimental results demonstrate that our SCF-Net yields better vessel segmentation performance in OCT-A than both traditional methods and other deep learning methods.      
### 30.Acquiring Measurement Matrices via Deep Basis Pursuit for Sparse Channel Estimation in mmWave Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.05177.pdf)
>  For millimeter-wave (mmWave) massive multiple-input multiple-output (MIMO) systems, the downlink channel state information (CSI) acquisition causes large overhead in a frequency-division duplex system. The overhead of CSI acquisition can be substantially reduced when compressed sensing techniques are employed for channel estimations, owing to the sparsity feature in angular domain. Successful compressed sensing implementations depend on the choice of measurement matrices. Existing compressed sensing approaches widely adopt random matrices as measurement matrices. However, random measurement matrices have been criticized for their suboptimal reconstruction performances. In this paper, a novel data-driven approach is proposed to acquire the measurement matrix to address the shortcomings of random measurement matrices. Given a dataset, a generic framework of deep basis pursuit autoencoder is proposed to optimize the measurement matrix for minimizing reconstruction errors. Under this framework, two specific autoencoder models are constructed using deep unfolding, which is a model-based deep learning technique to acquire data-driven measurement matrices. Compared with random matrices, the acquired data-driven measurement matrices can achieve more accurate reconstructions using fewer measurements, and thus such a design can lead to a higher achievable rate for CSI acquisition in mmWave massive MIMO systems.      
### 31.Rain Streak Removal in a Video to Improve Visibility by TAWL Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2007.05167.pdf)
>  In computer vision applications, the visibility of the video content is crucial to perform analysis for better accuracy. The visibility can be affected by several atmospheric interferences in challenging weather-one of them is the appearance of rain streak. In recent time, rain streak removal achieves lots of interest to the researchers as it has some exciting applications such as autonomous car, intelligent traffic monitoring system, multimedia, etc. In this paper, we propose a novel and simple method by combining three novel extracted features focusing on temporal appearance, wide shape and relative location of the rain streak and we called it TAWL (Temporal Appearance, Width, and Location) method. The proposed TAWL method adaptively uses features from different resolutions and frame rates. Moreover, it progressively processes features from the up-coming frames so that it can remove rain in the real-time. The experiments have been conducted using video sequences with both real rains and synthetic rains to compare the performance of the proposed method against the relevant state-of-the-art methods. The experimental results demonstrate that the proposed method outperforms the state-of-the-art methods by removing more rain streaks while keeping other moving regions.      
### 32.Approximate Time-Optimal Trajectories for Damped Double Integrator in 2D Obstacle Environments under Bounded Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2007.05155.pdf)
>  This article provides extensions to existing path-velocity decomposition based time optimal trajectory planning algorithm \cite{kant1986toward} to scenarios in which agents move in 2D obstacle environment under double integrator dynamics with drag term (damped double integrator). Particularly, we extend the idea of a tangent graph \cite{liu1992path} to $\calC^1$-Tangent graph to find continuously differentiable ($\calC^1$) shortest path between any two points. $\calC^1$-Tangent graph has a continuously differentiable ($\calC^1$) path between any two nodes. We also provide analytical expressions for a near time-optimal velocity profile for an agent moving on these shortest paths under the damped double integrator with bounded acceleration.      
### 33.Localized Motion Artifact Reduction on Brain MRI Using Deep Learning with Effective Data Augmentation Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2007.05149.pdf)
>  In-scanner motion degrades the quality of magnetic resonance imaging (MRI) thereby reducing its utility in the detection of clinically relevant abnormalities. We introduce a deep learning-based MRI artifact reduction model (DMAR) to localize and correct head motion artifacts in brain MRI scans. Our approach integrates the latest advances in object detection and noise reduction in Computer Vision. Specifically, DMAR employs a two-stage approach: in the first, degraded regions are detected using the Single Shot Multibox Detector (SSD), and in the second, the artifacts within the found regions are reduced using a convolutional autoencoder (CAE). We further introduce a set of novel data augmentation techniques to address the high dimensionality of MRI images and the scarcity of available data. As a result, our model was trained on a large synthetic dataset of 217,000 images generated from six whole-brain T1-weighted MRI scans obtained from three subjects. DMAR produces convincing visual results when applied to both synthetic test images and 55 real-world motion-affected slices from 18 subjects from the multi-center Autism Brain Imaging Data Exchange study. Quantitatively, depending on the level of degradation, our model achieves a 14.3%-25.6% reduction in RMSE and a 1.38-2.68 dB gain in PSNR on a 5000-sample set of synthetic images. For real-world scans where the ground-truth is unavailable, our model produces a 3.65% reduction in regional standard deviations of image intensity.      
### 34.Towards Ubiquitous Positioning by Leveraging Reconfigurable Intelligent Surface  [ :arrow_down: ](https://arxiv.org/pdf/2007.05140.pdf)
>  The received signal strength (RSS) based technique is widely utilized for ubiquitous positioning due to its advantage of simple implementability. However, its accuracy is limited because the RSS values of adjacent locations can be very difficult to distinguish. Against this background, we propose the novel RSS-based positioning scheme enabled by reconfigurable intelligent surface (RIS). By modifying the reflection coefficient of the RIS, the propagation channels are programmed in such a way that the differences between the RSS values of adjacent locations can be enlarged to improve the positioning accuracy. New challenge lies in the selection of suitable reflection coefficients for high-accuracy positioning. To tackle this challenge, we formulate the RIS-aided positioning problem and design an iterative algorithm to solve the problem. The effectiveness of the proposed positioning scheme is validated through simulations.      
### 35.Extremum Power Seeking Control of A Hybrid Wind-Solar-Storage DC Power System  [ :arrow_down: ](https://arxiv.org/pdf/2007.05093.pdf)
>  This paper presents a combined power system with a common dc bus that contains solar power, wind power, battery storage, and a constant dc load (CDL). In wind system, an AC-DC uncontrolled rectifier is used at the first stage and the DC-DC converter is controlled by a maximum power point tracker (MPPT) at second stage. In the solar system, two cascaded boost converters are controlled through a sliding mode controller (SMC) to regulate the power flow to the load. A supervisory control strategy is also introduced to maximize the simultaneous energy harvesting from both renewable sources and balance the energy between the sources, battery, and the load. According to the level of power generation available at each renewable energy source, the state of charge in the battery, and the load requirement, the controller results in four contingencies. Simulation results show the accurate operation of the supervisory controller and functionality of the maximum power point tracking algorithm for solar and for wind power.      
### 36.Information-Theoretic Approach to Navigation for Efficient Detection and Classification of Underwater Objects  [ :arrow_down: ](https://arxiv.org/pdf/2007.05072.pdf)
>  This paper addresses an autonomous exploration problem in which a mobile sensor, placed in a previously unseen search area, utilizes an information-theoretic navigation cost function to dynamically select the next sensing action, i.e., location from which to take a measurement, to efficiently detect and classify objects of interest within the area. The information-theoretic cost function proposed in this paper consist of two \textit{information gain} terms, one for detection and localization of objects and the other for sequential classification of the detected objects. We present a novel closed-form representation for the cost function, derived from the definition of mutual information. We evaluate three different policies for choosing the next sensing action: lawn mower, greedy, and non-greedy. For these three policies, we compare the results from our information-theoretic cost functions to the results of other information-theoretic inspired cost functions. Our simulation results show that search efficiency is greater using the proposed cost functions compared to those of the other methods, and that the greedy and non-greedy policies outperform the lawn mower policy.      
### 37.Inferring proximity from Bluetooth Low Energy RSSI with Unscented Kalman Smoothers  [ :arrow_down: ](https://arxiv.org/pdf/2007.05057.pdf)
>  The Covid-19 pandemic has resulted in a variety of approaches for managing infection outbreaks in international populations. One example is mobile phone applications, which attempt to alert infected individuals and their contacts by automatically inferring two key components of infection risk: the proximity to an individual who may be infected, and the duration of proximity. The former component, proximity, relies on Bluetooth Low Energy (BLE) Received Signal Strength Indicator(RSSI) as a distance sensor, and this has been shown to be problematic; not least because of unpredictable variations caused by different device types, device location on-body, device orientation, the local environment and the general noise associated with radio frequency propagation. In this paper, we present an approach that infers posterior probabilities over distance given sequences of RSSI values. Using a single-dimensional Unscented Kalman Smoother (UKS) for non-linear state space modelling, we outline several Gaussian process observation transforms, including: a generative model that directly captures sources of variation; and a discriminative model that learns a suitable observation function from training data using both distance and infection risk as optimisation objective functions. Our results show that good risk prediction can be achieved in $\mathcal{O}(n)$ time on real-world data sets, with the UKS outperforming more traditional classification methods learned from the same training data.      
### 38.Experimental Modeling of Cyclists Fatigue and Recovery Dynamics Enabling Optimal Pacing in a Time Trial  [ :arrow_down: ](https://arxiv.org/pdf/2007.05507.pdf)
>  Improving a cyclist performance during a time-trial effort has been a challenge for sport scientists for several decades. There has been a lot of work on understanding the physiological concepts behind it. The concepts of Critical Power (CP) and Anaerobic Work Capacity (AWC) have been discussed often in recent cycling performance related articles. CP is a power that can be maintained by a cyclist for a long time; meaning pedaling at or below this limit, theoretically, can be continued for infinite amount of time. However, there is a limited source of energy for generating power above CP. This limited energy source is AWC. After burning energy from this tank, a cyclist can recover some by pedaling below CP. In this paper we utilize the concepts of CP and AWC to mathematically model muscle fatigue and recovery of a cyclist. Then, the models are used to formulate an optimal control problem for a time trial effort on a 10.3 km course located in Greenville SC. The course is simulated in a laboratory environment using a CompuTrainer. At the end, the optimal simulation results are compared to the performance of one subject on CompuTrainer.      
### 39.Scientific Discovery by Generating Counterfactuals using Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2007.05500.pdf)
>  Model explanation techniques play a critical role in understanding the source of a model's performance and making its decisions transparent. Here we investigate if explanation techniques can also be used as a mechanism for scientific discovery. We make three contributions: first, we propose a framework to convert predictions from explanation techniques to a mechanism of discovery. Second, we show how generative models in combination with black-box predictors can be used to generate hypotheses (without human priors) that can be critically examined. Third, with these techniques we study classification models for retinal images predicting Diabetic Macular Edema (DME), where recent work showed that a CNN trained on these images is likely learning novel features in the image. We demonstrate that the proposed framework is able to explain the underlying scientific mechanism, thus bridging the gap between the model's performance and human understanding.      
### 40.On the variability of functional connectivity and network measures in source-reconstructed EEG time-series  [ :arrow_down: ](https://arxiv.org/pdf/2007.05395.pdf)
>  The idea to estimate the statistical interdependence among (interacting) EEG signals has motivated numerous researchers to investigate how the resulting networks may reorganize themselves under any conceivable scenario. Even though this idea is not at initial stages, its application is still far to be widespread. One concurrent cause may be related to the proliferation of different approaches that promise to catch the underlying correlation among the (interacting) units. This issue has probably contributed to hinder the comparison among different studies. Not only all these approaches go under the same name (functional connectivity) but they have been often tested and validated using different methods, therefore, making it difficult to understand to what extent they are similar or not. In this study, we aim to compare a set of different approaches commonly used to estimate the functional connectivity on a public EEG dataset representing a possible realistic scenario. Our results show that source-level EEG functional connectivity estimates and the derived network measures display a substantial dependency on the arbitrary choice of the selected connectivity metric. The observed variability reflects ambiguity and concern that should be always discussed when reporting findings based on any connectivity metric.      
### 41.Are pathologist-defined labels reproducible? Comparison of the TUPAC16 mitotic figure dataset with an alternative set of labels  [ :arrow_down: ](https://arxiv.org/pdf/2007.05351.pdf)
>  Pathologist-defined labels are the gold standard for histopathological data sets, regardless of well-known limitations in consistency for some tasks. To date, some datasets on mitotic figures are available and were used for development of promising deep learning-based algorithms. In order to assess robustness of those algorithms and reproducibility of their methods it is necessary to test on several independent datasets. The influence of different labeling methods of these available datasets is currently unknown. To tackle this, we present an alternative set of labels for the images of the auxiliary mitosis dataset of the TUPAC16 challenge. Additional to manual mitotic figure screening, we used a novel, algorithm-aided labeling process, that allowed to minimize the risk of missing rare mitotic figures in the images. All potential mitotic figures were independently assessed by two pathologists. The novel, publicly available set of labels contains 1,999 mitotic figures (+28.80%) and additionally includes 10,483 labels of cells with high similarities to mitotic figures (hard examples). We found significant difference comparing F_1 scores between the original label set (0.549) and the new alternative label set (0.735) using a standard deep learning object detection architecture. The models trained on the alternative set showed higher overall confidence values, suggesting a higher overall label consistency. Findings of the present study show that pathologists-defined labels may vary significantly resulting in notable difference in the model performance. Comparison of deep learning-based algorithms between independent datasets with different labeling methods should be done with caution.      
### 42.A Dynamical Approach to Efficient Eigenvalue Estimation in General Multiagent Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.05340.pdf)
>  We propose a method to efficiently estimate the eigenvalues of any arbitrary, unknown network of interacting dynamical agents. The inputs to our estimation algorithm are measurements about the evolution of the outputs of a subset of agents (potentially one) during a finite time horizon; notably, we do not require knowledge of which agents are contributing to our measurements. We propose an efficient algorithm to exactly recover the eigenvalues corresponding directly to those modes that are recoverable from our measurements. We show how our technique can be applied to networks of multiagent systems with arbitrary dynamics in both continuous- and discrete-time. Finally, we illustrate our results with numerical simulations.      
### 43.TIMELY: Improving Labeling Consistency in Medical Imaging for Cell Type Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.05307.pdf)
>  Diagnosing diseases such as leukemia or anemia requires reliable counts of blood cells. Hematologists usually label and count microscopy images of blood cells manually. In many cases, however, cells in different maturity states are difficult to distinguish, and in combination with image noise and subjectivity, humans are prone to make labeling mistakes. This results in labels that are often not reproducible, which can directly affect the diagnoses. We introduce TIMELY, a probabilistic model that combines pseudotime inference methods with inhomogeneous hidden Markov trees, which addresses this challenge of label inconsistency. We show first on simulation data that TIMELY is able to identify and correct wrong labels with higher precision and recall than baseline methods for labeling correction. We then apply our method to two real-world datasets of blood cell data and show that TIMELY successfully finds inconsistent labels, thereby improving the quality of human-generated labels.      
### 44.A 3D-Hybrid-Shot Spiral Sequence for Hyperpolarized $^{13}$C Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2007.05257.pdf)
>  Purpose: Hyperpolarized imaging experiments have conflicting requirements of high spatial, temporal, and spectral resolution. Spectral-Spatial RF excitation has been shown to form an attractive magnetization-efficient method for hyperpolarized imaging, but the optimum readout strategy is not yet known. <br>Methods: In this work we propose a novel 3D hybrid-shot spiral sequence which features two constant density regions that permit the retrospective reconstruction of either high spatial or high temporal resolution images post hoc, (adaptive spatiotemporal imaging) allowing greater flexibility in acquisition and reconstruction. <br>Results: We have implemented this sequence, both via simulation and on a pre-clinical scanner, to demonstrate its feasibility, in both a 1H phantom and with hyperpolarized 13C pyruvate in vivo. Conclusion: This sequence forms an attractive method for acquiring hyperpolarized imaging datasets, providing adaptive spatiotemporal imaging to ameliorate the conflict of spatial and temporal resolution, with significant potential for clinical translation.      
### 45.Continual Adaptation for Deep Stereo  [ :arrow_down: ](https://arxiv.org/pdf/2007.05233.pdf)
>  Depth estimation from stereo images is carried out with unmatched results by convolutional neural networks trained end-to-end to regress dense disparities. Like for most tasks, this is possible if large amounts of labelled samples are available for training, possibly covering the whole data distribution encountered at deployment time. Being such an assumption systematically met in real applications, the capacity of adapting to any unseen setting becomes of paramount importance. Purposely, we propose a continual adaptation paradigm for deep stereo networks designed to deal with challenging and ever-changing environments. We design a lightweight and modular architecture, Modularly ADaptive Network (MADNet), and formulate Modular ADaptation algorithms(MAD,MAD++) which permit efficient optimization of independent sub-portions of the entire network. In our paradigm the learning signals needed to continuously adapt models online can be sourced from self-supervision via right-to-left image warping or from traditional stereo algorithms. With both sources no other data than the input images being gathered at deployment time are needed.Thus, our network architecture and adaptation algorithms realize the first real-time self-adaptive deep stereo system and pave the way for a new paradigm that can facilitate practical deployment of end-to-end architectures for dense disparity regression.      
### 46.On quotients of Boolean control networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.05200.pdf)
>  In this paper, we focus on the study of quotients of Boolean control networks (BCNs) with the motivation that they might serve as smaller models that still carry enough information about the original network. Given a BCN and an equivalence relation on the state set, we consider a labeled transition system that is generated by the BCN. The resulting quotient transition system then naturally captures the quotient dynamics of the BCN concerned. We therefore develop a method for constructing a Boolean system that behaves equivalently to the resulting quotient transition system. The use of the obtained quotient system for control design is discussed and we show that for BCNs, controller synthesis can be done by first designing a controller for a quotient and subsequently lifting it to the original model. We finally demonstrate the applicability of the proposed techniques on a biological example.      
### 47.Overcoming label noise in audio event detection using sequential labeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.05191.pdf)
>  This paper addresses the noisy label issue in audio event detection (AED) by refining strong labels as sequential labels with inaccurate timestamps removed. In AED, strong labels contain the occurrence of a specific event and its timestamps corresponding to the start and end of the event in an audio clip. The timestamps depend on subjectivity of each annotator, and their label noise is inevitable. Contrary to the strong labels, weak labels indicate only the occurrence of a specific event. They do not have the label noise caused by the timestamps, but the time information is excluded. To fully exploit information from available strong and weak labels, we propose an AED scheme to train with sequential labels in addition to the given strong and weak labels after converting the strong labels into the sequential labels. Using sequential labels consistently improved the performance particularly with the segment-based F-score by focusing on occurrences of events. In the mean-teacher-based approach for semi-supervised learning, including an early step with sequential prediction in addition to supervised learning with sequential labels mitigated label noise and inaccurate prediction of the teacher model and improved the segment-based F-score significantly while maintaining the event-based F-score.      
### 48.Conditioned Time-Dilated Convolutions for Sound Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2007.05183.pdf)
>  Sound event detection (SED) is the task of identifying sound events along with their onset and offset times. A recent, convolutional neural networks based SED method, proposed the usage of depthwise separable (DWS) and time-dilated convolutions. DWS and time-dilated convolutions yielded state-of-the-art results for SED, with considerable small amount of parameters. In this work we propose the expansion of the time-dilated convolutions, by conditioning them with jointly learned embeddings of the SED predictions by the SED classifier. We present a novel algorithm for the conditioning of the time-dilated convolutions which functions similarly to language modelling, and enhances the performance of the these convolutions. We employ the freely available TUT-SED Synthetic dataset, and we assess the performance of our method using the average per-frame $\text{F}_{1}$ score and average per-frame error rate, over the 10 experiments. We achieve an increase of 2\% (from 0.63 to 0.65) at the average $\text{F}_{1}$ score (the higher the better) and a decrease of 3\% (from 0.50 to 0.47) at the error rate (the lower the better).      
### 49.Solving System of Nonlinear Equations with the Genetic Algorithm and Newton's Method  [ :arrow_down: ](https://arxiv.org/pdf/2007.05159.pdf)
>  An implementation and an application of the combination of the genetic algorithm and Newton's method for solving a system of nonlinear equations is presented. The method first uses the advantage of the robustness of the genetic algorithm for guessing the rough location of the roots, then it uses the advantage of a good rate of convergence of Newton's method. An effective application of the method for the positioning problem of multiple small rovers proposed for the use in asteroid exploration is shown.      
### 50.Development and Validation of a Novel Prognostic Model for Predicting AMD Progression Using Longitudinal Fundus Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.05120.pdf)
>  Prognostic models aim to predict the future course of a disease or condition and are a vital component of personalized medicine. Statistical models make use of longitudinal data to capture the temporal aspect of disease progression; however, these models require prior feature extraction. Deep learning avoids explicit feature extraction, meaning we can develop models for images where features are either unknown or impossible to quantify accurately. Previous prognostic models using deep learning with imaging data require annotation during training or only utilize a single time point. We propose a novel deep learning method to predict the progression of diseases using longitudinal imaging data with uneven time intervals, which requires no prior feature extraction. Given previous images from a patient, our method aims to predict whether the patient will progress onto the next stage of the disease. The proposed method uses InceptionV3 to produce feature vectors for each image. In order to account for uneven intervals, a novel interval scaling is proposed. Finally, a Recurrent Neural Network is used to prognosticate the disease. We demonstrate our method on a longitudinal dataset of color fundus images from 4903 eyes with age-related macular degeneration (AMD), taken from the Age-Related Eye Disease Study, to predict progression to late AMD. Our method attains a testing sensitivity of 0.878, a specificity of 0.887, and an area under the receiver operating characteristic of 0.950. We compare our method to previous methods, displaying superior performance in our model. Class activation maps display how the network reaches the final decision.      
### 51.Learnable Hollow Kernels for Anatomical Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.05103.pdf)
>  Segmentation of certain hollow organs, such as the bladder, is especially hard to automate due to their complex geometry, vague intensity gradients in the soft tissues, and a tedious manual process of the data annotation routine. Yet, accurate localization of the walls and the cancer regions in the radiologic images of such organs is an essential step in oncology. To address this issue, we propose a new class of hollow kernels that learn to 'mimic' the contours of the segmented organ, effectively replicating its shape and structural complexity. We train a series of the U-Net-like neural networks using the proposed kernels and demonstrate the superiority of the idea in various spatio-temporal convolution scenarios. Specifically, the dilated hollow-kernel architecture outperforms state-of-the-art spatial segmentation models, whereas the addition of temporal blocks with, e.g., Bi-LSTM, establishes a new multi-class baseline for the bladder segmentation challenge. Our spatio-temporal model based on the hollow kernels reaches the mean dice scores of 0.936, 0.736, and 0.712 for the bladder's inner wall, the outer wall, and the tumor regions, respectively. The results pave the way towards other domain-specific deep learning applications where the shape of the segmented object could be used to form a proper convolution kernel for boosting the segmentation outcome.      
### 52.Automatic Detection of Major Freeway Congestion Events Using Wireless Traffic Sensor Data: A Machine Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.05079.pdf)
>  Monitoring the dynamics of traffic in major corridors can provide invaluable insight for traffic planning purposes. An important requirement for this monitoring is the availability of methods to automatically detect major traffic events and to annotate the abundance of travel data. This paper introduces a machine learning based approach for reliable detection and characterization of highway traffic congestion events from hundreds of hours of traffic speed data. Indeed, the proposed approach is a generic approach for detection of changes in any given time series, which is the wireless traffic sensor data in the present study. The speed data is initially time-windowed by a ten-hour long sliding window and fed into three Neural Networks that are used to detect the existence and duration of congestion events (slowdowns) in each window. The sliding window captures each slowdown event multiple times and results in increased confidence in congestion detection. The training and parameter tuning are performed on 17,483 hours of data that includes 168 slowdown events. This data is collected and labeled as part of the ongoing probe data validation studies at the Center for Advanced Transportation Technologies (CATT) at the University of Maryland. The Neural networks are carefully trained to reduce the chances of over-fitting to the training data. The experimental results show that this approach is able to successfully detect most of the congestion events, while significantly outperforming a heuristic rule-based approach. Moreover, the proposed approach is shown to be more accurate in estimation of the start-time and end-time of the congestion events.      
### 53.A Quick Review on Recent Trends in 3D Point Cloud Data Compression Techniques and the Challenges of Direct Processing in 3D Compressed Domain  [ :arrow_down: ](https://arxiv.org/pdf/2007.05038.pdf)
>  Automatic processing of 3D Point Cloud data for object detection, tracking and segmentation is the latest trending research in the field of AI and Data Science, which is specifically aimed at solving different challenges of autonomous driving cars and getting real time performance. However, the amount of data that is being produced in the form of 3D point cloud (with LiDAR) is very huge, due to which the researchers are now on the way inventing new data compression algorithms to handle huge volumes of data thus generated. However, compression on one hand has an advantage in overcoming space requirements, but on the other hand, its processing gets expensive due to the decompression, which indents additional computing resources. Therefore, it would be novel to think of developing algorithms that can operate/analyse directly with the compressed data without involving the stages of decompression and recompression (required as many times, the compressed data needs to be operated or analyzed). This research field is termed as Compressed Domain Processing. In this paper, we will quickly review few of the recent state-of-the-art developments in the area of LiDAR generated 3D point cloud data compression, and highlight the future challenges of compressed domain processing of 3D point cloud data.      
### 54.StyPath: Style-Transfer Data Augmentation For Robust Histology Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.05008.pdf)
>  The classification of Antibody Mediated Rejection (AMR) in kidney transplant remains challenging even for experienced nephropathologists; this is partly because histological tissue stain analysis is often characterized by low inter-observer agreement and poor reproducibility. One of the implicated causes for inter-observer disagreement is the variability of tissue stain quality between (and within) pathology labs, coupled with the gradual fading of archival sections. Variations in stain colors and intensities can make tissue evaluation difficult for pathologists, ultimately affecting their ability to describe relevant morphological features. Being able to accurately predict the AMR status based on kidney histology images is crucial for improving patient treatment and care. We propose a novel pipeline to build robust deep neural networks for AMR classification based on StyPath, a histological data augmentation technique that leverages a light weight style-transfer algorithm as a means to reduce sample-specific bias. Each image was generated in 1.84 +- 0.03 seconds using a single GTX TITAN V gpu and pytorch, making it faster than other popular histological data augmentation techniques. We evaluated our model using a Monte Carlo (MC) estimate of Bayesian performance and generate an epistemic measure of uncertainty to compare both the baseline and StyPath augmented models. We also generated Grad-CAM representations of the results which were assessed by an experienced nephropathologist; we used this qualitative analysis to elucidate on the assumptions being made by each model. Our results imply that our style-transfer augmentation technique improves histological classification performance (reducing error from 14.8% to 11.5%) and generalization ability.      
### 55.Adaptive-glasses wavefront sensorless full-field OCT for high-resolution retinal imaging over a wide field-of-view  [ :arrow_down: ](https://arxiv.org/pdf/2007.04986.pdf)
>  The highest three-dimensional (3D) resolution possible in in-vivo retinal imaging is achieved by combining optical coherence tomography (OCT) and adaptive optics (AO). However, this combination brings important limitations, such as small field-of-view and complex, cumbersome systems, preventing so far the translation of this technology from the research lab to clinics. In this Letter, we introduce an approach that avoids these limitations by using a multi-actuator adaptive lens just in front of the eye, in a technique we call the adaptive-glasses wavefront sensorless approach. We implemented this approach on our compact full-field OCT (FFOCT) retinal imager without increasing its footprint or optical complexity. The correction of ocular aberrations through the adaptive-glasses approach increased the FFOCT signal-to-noise ratio and enabled us to image different retinal layers with a 3D cellular resolution in a 5 $\times$ 5 field-of-view, without apparent anisoplanatism.      
