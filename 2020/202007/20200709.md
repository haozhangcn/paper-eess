# ArXiv eess --Thu, 9 Jul 2020
### 1.A Quadratic Convex Approximation of Optimal Power Flow in Distribution System with Application in Loss Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2007.04289.pdf)
>  Price is the key to resource allocation. In the electricity market, the price is settled by two steps: i) determine the optimal dispatches for generators; ii) calculate the prices for consumers at different locations. In this paper, a novel quadratic optimal power flow model, namely MDOPF, is proposed to decide the optimal dispatches for distribution systems. The model is proved to be convex if the summation of generation marginal costs is over zero. According to the result of MDOPF, the electricity price can be calculated by two methods, namely marginal loss method (MLM) and loss allocation method (LAM), respectively. The MLM can yields very accurate distribution locational marginal prices (DLMP) if compared with the DLMP solved by ACOPF. While the LAM is aimed to eliminate the over-collected losses caused by DLMP. These two methods are proposed in explicit forms which can release the computational burden. Numerical tests show that the proposed MDOPF model generates very close optimal dispatches if compared with the benchmarks provided by ACOPF.      
### 2.Quantifying and Leveraging Predictive Uncertainty for Medical Image Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2007.04258.pdf)
>  The interpretation of medical images is a challenging task, often complicated by the presence of artifacts, occlusions, limited contrast and more. Most notable is the case of chest radiography, where there is a high inter-rater variability in the detection and classification of abnormalities. This is largely due to inconclusive evidence in the data or subjective definitions of disease appearance. An additional example is the classification of anatomical views based on 2D Ultrasound images. Often, the anatomical context captured in a frame is not sufficient to recognize the underlying anatomy. Current machine learning solutions for these problems are typically limited to providing probabilistic predictions, relying on the capacity of underlying models to adapt to limited information and the high degree of label noise. In practice, however, this leads to overconfident systems with poor generalization on unseen data. To account for this, we propose a system that learns not only the probabilistic estimate for classification, but also an explicit uncertainty measure which captures the confidence of the system in the predicted output. We argue that this approach is essential to account for the inherent ambiguity characteristic of medical images from different radiologic exams including computed radiography, ultrasonography and magnetic resonance imaging. In our experiments we demonstrate that sample rejection based on the predicted uncertainty can significantly improve the ROC-AUC for various tasks, e.g., by 8% to 0.91 with an expected rejection rate of under 25% for the classification of different abnormalities in chest radiographs. In addition, we show that using uncertainty-driven bootstrapping to filter the training data, one can achieve a significant increase in robustness and accuracy.      
### 3.Labelling imaging datasets on the basis of neuroradiology reports: a validation study  [ :arrow_down: ](https://arxiv.org/pdf/2007.04226.pdf)
>  Natural language processing (NLP) shows promise as a means to automate the labelling of hospital-scale neuroradiology magnetic resonance imaging (MRI) datasets for computer vision applications. To date, however, there has been no thorough investigation into the validity of this approach, including determining the accuracy of report labels compared to image labels as well as examining the performance of non-specialist labellers. In this work, we draw on the experience of a team of neuroradiologists who labelled over 5000 MRI neuroradiology reports as part of a project to build a dedicated deep learning-based neuroradiology report classifier. We show that, in our experience, assigning binary labels (i.e. normal vs abnormal) to images from reports alone is highly accurate. In contrast to the binary labels, however, the accuracy of more granular labelling is dependent on the category, and we highlight reasons for this discrepancy. We also show that downstream model performance is reduced when labelling of training reports is performed by a non-specialist. To allow other researchers to accelerate their research, we make our refined abnormality definitions and labelling rules available, as well as our easy-to-use radiology report labelling app which helps streamline this process.      
### 4.The Statistical Characteristics of Power-Spectrum Subband Energy Ratios under Additive Gaussian White Noise  [ :arrow_down: ](https://arxiv.org/pdf/2007.04222.pdf)
>  The power-spectrum subband energy ratio (PSER) has been applied in a variety of fields, but reports on its statistical properties have been limited. As such, this study investigates these characteristics in the presence of additive Gaussian white noise for both pure noise and mixed signals. By analyzing the probability and independence of power spectrum bins, and the relationship between the F and beta distributions, we develop a probability distribution for the PSER. Results showed that in the case of pure noise, the PSER follows a beta distribution. In addition, the probability density function and the quantile exhibited no relationship with the noise variance, only with the number of lines in the power spectrum, that is, PSER is not affected by noise. When Gaussian white noise was mixed with the known signal, the resulting PSER followed a doubly non-central beta distribution. In this case, it was difficult to identify the quantile, as the probability density and cumulative distribution functions were represented by infinite series. However, when a spectral bin did not contain the power spectrum of the known signal, an approximated quantile was found. This quantile is strictly proved to be in agreement with the quantile in the case of pure noise and offers a convenient methodology for identifying valid signals.      
### 5.Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2007.04134.pdf)
>  The intuitive interaction between the audio and visual modalities is valuable for cross-modal self-supervised learning. This concept has been demonstrated for generic audiovisual tasks like video action recognition and acoustic scene classification. However, self-supervision remains under-explored for audiovisual speech. We propose a method to learn self-supervised speech representations from the raw audio waveform. We train a raw audio encoder by combining audio-only self-supervision (by predicting informative audio attributes) with visual self-supervision (by generating talking faces from audio). The visual pretext task drives the audio representations to capture information related to lip movements. This enriches the audio encoder with visual information and the encoder can be used for evaluation without the visual modality. Our method attains competitive performance with respect to existing self-supervised audio features on established isolated word classification benchmarks, and significantly outperforms other methods at learning from fewer labels. Notably, our method also outperforms fully supervised training, thus providing a strong initialization for speech related tasks. Our results demonstrate the potential of multimodal self-supervision in audiovisual speech for learning good audio representations.      
### 6.Machine learning and data analytics for the IoT  [ :arrow_down: ](https://arxiv.org/pdf/2007.04093.pdf)
>  The Internet of Things (IoT) applications have grown in exorbitant numbers, generating a large amount of data required for intelligent data processing. However, the varying IoT infrastructures (i.e., cloud, edge, fog) and the limitations of the IoT application layer protocols in transmitting/receiving messages become the barriers in creating intelligent IoT applications. These barriers prevent current intelligent IoT applications to adaptively learn from other IoT applications. In this paper, we critically review how IoT-generated data are processed for machine learning analysis and highlight the current challenges in furthering intelligent solutions in the IoT environment. Furthermore, we propose a framework to enable IoT applications to adaptively learn from other IoT applications and present a case study in how the framework can be applied to the real studies in the literature. Finally, we discuss the key factors that have an impact on future intelligent applications for the IoT.      
### 7.An adaptive and energy-maximizing control of wave energy converters using extremum-seeking approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.04077.pdf)
>  In this paper, we systematically investigate the feasibility of different extremum-seeking (ES) control schemes to improve the conversion efficiency of wave energy converters (WECs). Continuous-time and model-free ES schemes based on the sliding mode, relay, least-squares gradient, self-driving, and perturbation-based methods are used to improve the mean extracted power of a heaving point absorber subject to regular and irregular waves. This objective is achieved by optimizing the resistive and reactive coefficients of the power take-off (PTO) mechanism using the ES approach. The optimization results are verified against analytical solutions and the extremum of reference-to-output maps. The numerical results demonstrate that except for the self-driving ES algorithm, the other four ES schemes reliably converge for the two-parameter optimization problem, whereas the former is more suitable for optimizing a single-parameter. The results also show that for an irregular sea state, the sliding mode and perturbation-based ES schemes have better convergence to the optimum, in comparison to other ES schemes considered here. The convergence of PTO coefficients towards the performance-optimal values are tested for widely different initial values, in order to avoid bias towards the extremum. We also demonstrate the adaptive capability of ES control by considering a case in which the ES controller adapts to the new extremum automatically amidst changes in the simulated wave conditions.      
### 8.Placing Grid-Forming Converters to Enhance Small Signal Stability of PLL-Integrated Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.03997.pdf)
>  The modern power grid features the high penetration of power converters, which widely employ a phase-locked loop (PLL) for grid synchronization. However, it has been pointed out that PLL can give rise to small-signal instabilities under weak grid conditions. This problem can be potentially resolved by operating the converters in grid-forming mode, namely, without using a PLL. Nonetheless, it has not been theoretically revealed how the placement of grid-forming converters enhances the small-signal stability of power systems integrated with large-scale PLL-based converters. This paper aims at filling this gap. Based on matrix perturbation theory, we explicitly demonstrate that the placement of grid-forming converters is equivalent to increasing the power grid strength and thus improving the small-signal stability of PLL-based converters. Furthermore, we investigate the optimal locations to place grid-forming converters. The analysis in this paper is validated through high-fidelity simulation studies on a modified two-area system. This paper potentially lays the foundation for understanding the interaction between PLL-based (i.e., grid-following) converters and grid-forming converters, and coordinating their placements in future converter-dominated power systems.      
### 9.Designing and Training of A Dual CNN for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2007.03951.pdf)
>  Deep convolutional neural networks (CNNs) for image denoising have recently attracted increasing research interest. However, plain networks cannot recover fine details for a complex task, such as real noisy images. In this paper, we propsoed a Dual denoising Network (DudeNet) to recover a clean image. Specifically, DudeNet consists of four modules: a feature extraction block, an enhancement block, a compression block, and a reconstruction block. The feature extraction block with a sparse machanism extracts global and local features via two sub-networks. The enhancement block gathers and fuses the global and local features to provide complementary information for the latter network. The compression block refines the extracted information and compresses the network. Finally, the reconstruction block is utilized to reconstruct a denoised image. The DudeNet has the following advantages: (1) The dual networks with a parse mechanism can extract complementary features to enhance the generalized ability of denoiser. (2) Fusing global and local features can extract salient features to recover fine details for complex noisy images. (3) A Small-size filter is used to reduce the complexity of denoiser. Extensive experiments demonstrate the superiority of DudeNet over existing current state-of-the-art denoising methods.      
### 10.AUSN: Approximately Uniform Quantization by Adaptively Superimposing Non-uniform Distribution for Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.03903.pdf)
>  Quantization is essential to simplify DNN inference in edge applications. Existing uniform and non-uniform quantization methods, however, exhibit an inherent conflict between the representing range and representing resolution, and thereby result in either underutilized bit-width or significant accuracy drop. Moreover, these methods encounter three drawbacks: i) the absence of a quantitative metric for in-depth analysis of the source of the quantization errors; ii) the limited focus on the image classification tasks based on CNNs; iii) the unawareness of the real hardware and energy consumption reduced by lowering the bit-width. <br>In this paper, we first define two quantitative metrics, i.e., the Clipping Error and rounding error, to analyze the quantization error distribution. We observe that the boundary- and rounding- errors vary significantly across layers, models and tasks. Consequently, we propose a novel quantization method to quantize the weight and activation. The key idea is to Approximate the Uniform quantization by Adaptively Superposing multiple Non-uniform quantized values, namely AUSN. AUSN is consist of a decoder-free coding scheme that efficiently exploits the bit-width to its extreme, a superposition quantization algorithm that can adapt the coding scheme to different DNN layers, models and tasks without extra hardware design effort, and a rounding scheme that can eliminate the well-known bit-width overflow and re-quantization issues. <br>Theoretical analysis~(see Appendix A) and accuracy evaluation on various DNN models of different tasks show the effectiveness and generalization of AUSN. The synthesis~(see Appendix B) results on FPGA show $2\times$ reduction of the energy consumption, and $2\times$ to $4\times$ reduction of the hardware resource.      
### 11.Streaming End-to-End Bilingual ASR Systems with Joint Language Identification  [ :arrow_down: ](https://arxiv.org/pdf/2007.03900.pdf)
>  Multilingual ASR technology simplifies model training and deployment, but its accuracy is known to depend on the availability of language information at runtime. Since language identity is seldom known beforehand in real-world scenarios, it must be inferred on-the-fly with minimum latency. Furthermore, in voice-activated smart assistant systems, language identity is also required for downstream processing of ASR output. In this paper, we introduce streaming, end-to-end, bilingual systems that perform both ASR and language identification (LID) using the recurrent neural network transducer (RNN-T) architecture. On the input side, embeddings from pretrained acoustic-only LID classifiers are used to guide RNN-T training and inference, while on the output side, language targets are jointly modeled with ASR targets. The proposed method is applied to two language pairs: English-Spanish as spoken in the United States, and English-Hindi as spoken in India. Experiments show that for English-Spanish, the bilingual joint ASR-LID architecture matches monolingual ASR and acoustic-only LID accuracies. For the more challenging (owing to within-utterance code switching) case of English-Hindi, English ASR and LID metrics show degradation. Overall, in scenarios where users switch dynamically between languages, the proposed architecture offers a promising simplification over running multiple monolingual ASR models and an LID classifier in parallel.      
### 12.Multi-Resolution Beta-Divergence NMF for Blind Spectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2007.03893.pdf)
>  Blind spectral unmixing is the problem of decomposing the spectrum of a mixed signal or image into a collection of source spectra and their corresponding activations indicating the proportion of each source present in the mixed spectrum. To perform this task, nonnegative matrix factorization (NMF) based on the $\beta$-divergence, referred to as $\beta$-NMF, is a standard and state-of-the art technique. Many NMF-based methods factorize a data matrix that is the result of a resolution trade-off between two adversarial dimensions. Two instrumental examples are (1)~audio spectral unmixing for which the frequency-by-time data matrix is computed with the short-time Fourier transform and is the result of a trade-off between the frequency resolution and the temporal resolution, and (2)~blind hyperspectral unmixing for which the wavelength-by-location data matrix is a trade-off between the number of wavelengths measured and the spatial resolution. In this paper, we propose a new NMF-based method, dubbed multi-resolution $\beta$-NMF (MR-$\beta$-NMF), to address this issue by fusing the information coming from multiple data with different resolutions in order to produce a factorization with high resolutions for all the dimensions. MR-$\beta$-NMF performs a form of nonnegative joint factorization based on the $\beta$-divergence. In order to solve this problem, we propose multiplicative updates based on a majorization-minimization algorithm. We show on numerical experiments that MR-$\beta$-NMF is able to obtain high resolutions in both dimensions for two applications: the joint-factorization of two audio spectrograms, and the hyperspectral and multispectral data fusion problem.      
### 13.Low-dimensional Manifold Constrained Disentanglement Network for Metal Artifact Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2007.03882.pdf)
>  Deep neural network based methods have achieved promising results for CT metal artifact reduction (MAR), most of which use many synthesized paired images for training. As synthesized metal artifacts in CT images may not accurately reflect the clinical counterparts, an artifact disentanglement network (ADN) was proposed with unpaired clinical images directly, producing promising results on clinical datasets. However, without sufficient supervision, it is difficult for ADN to recover structural details of artifact-affected CT images based on adversarial losses only. To overcome these problems, here we propose a low-dimensional manifold (LDM) constrained disentanglement network (DN), leveraging the image characteristics that the patch manifold is generally low-dimensional. Specifically, we design an LDM-DN learning algorithm to empower the disentanglement network through optimizing the synergistic network loss functions while constraining the recovered images to be on a low-dimensional patch manifold. Moreover, learning from both paired and unpaired data, an efficient hybrid optimization scheme is proposed to further improve the MAR performance on clinical datasets. Extensive experiments demonstrate that the proposed LDM-DN approach can consistently improve the MAR performance in paired and/or unpaired learning settings, outperforming competing methods on synthesized and clinical datasets.      
### 14.Fine-grained Vibration Based Sensing Using a Smartphone  [ :arrow_down: ](https://arxiv.org/pdf/2007.03874.pdf)
>  Recognizing surfaces based on their vibration signatures is useful as it can enable tagging of different locations without requiring any additional hardware such as Near Field Communication (NFC) tags. However, previous vibration based surface recognition schemes either use custom hardware for creating and sensing vibration, which makes them difficult to adopt, or use inertial (IMU) sensors in commercial off-the-shelf (COTS) smartphones to sense movements produced due to vibrations, which makes them coarse-grained because of the low sampling rates of IMU sensors. The mainstream COTS smartphones based schemes are also susceptible to inherent hardware based irregularities in vibration mechanism of the smartphones. Moreover, the existing schemes that use microphones to sense vibration are prone to short-term and constant background noises (e.g. intermittent talking, exhaust fan, etc.) because microphones not only capture the sounds created by vibration but also other interfering sounds present in the environment. In this paper, we propose VibroTag, a robust and practical vibration based sensing scheme that works with smartphones with different hardware, can extract fine-grained vibration signatures of different surfaces, and is robust to environmental noise and hardware based irregularities. We implemented VibroTag on two different Android phones and evaluated in multiple different environments where we collected data from 4 individuals for 5 to 20 consecutive days. Our results show that VibroTag achieves an average accuracy of 86.55% while recognizing 24 different locations/surfaces, even when some of those surfaces were made of similar material. VibroTag's accuracy is 37% higher than the average accuracy of 49.25% achieved by one of the state-of-the-art IMUs based schemes, which we implemented for comparison with VibroTag.      
### 15.Global exponential attitude tracking for spacecraft with gyro bias estimation  [ :arrow_down: ](https://arxiv.org/pdf/2007.03825.pdf)
>  This paper addresses the global exponential attitude tracking of a spacecraft when gyro measurements are corrupted by bias. Based on contraction analysis, an exponentially convergent nonlinear observer is designed first to estimate the gyro bias. Relying on this bias estimator and the quaternion logarithm representation of the tracking error, an exponentially globally convergent controller is devised. This controller stabilizes the unique equilibrium of the closed-loop system, where the tracking error is the unit quaternion. For more energy-efficiency and enhancing the robustness in the presence of measurement noise, a hysteretically switching variable as in [1] is incorporated in the control loop and an unwinding-free globally exponentially convergent tracking controller is obtained. Numeric simulations were done to evaluate its performance in terms of tracking errors and energy-efficiency, as well as the robustness to measurement noise and time-varying bias in gyro sensors.      
### 16.Self-supervised Skull Reconstruction in Brain CT Images with Decompressive Craniectomy  [ :arrow_down: ](https://arxiv.org/pdf/2007.03817.pdf)
>  Decompressive craniectomy (DC) is a common surgical procedure consisting of the removal of a portion of the skull that is performed after incidents such as stroke, traumatic brain injury (TBI) or other events that could result in acute subdural hemorrhage and/or increasing intracranial pressure. In these cases, CT scans are obtained to diagnose and assess injuries, or guide a certain therapy and intervention. <br>We propose a deep learning based method to reconstruct the skull defect removed during DC performed after TBI from post-operative CT images. This reconstruction is useful in multiple scenarios, e.g. to support the creation of cranioplasty plates, accurate measurements of bone flap volume and total intracranial volume, important for studies that aim to relate later atrophy to patient outcome. We propose and compare alternative self-supervised methods where an encoder-decoder convolutional neural network (CNN) estimates the missing bone flap on post-operative CTs. The self-supervised learning strategy only requires images with complete skulls and avoids the need for annotated DC images. For evaluation, we employ real and simulated images with DC, comparing the results with other state-of-the-art approaches. The experiments show that the proposed model outperforms current manual methods, enabling reconstruction even in highly challenging cases where big skull defects have been removed during surgery.      
### 17.Real-time Intersection Optimization for Signal Phasing, Timing, and Automated Vehicles' Trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2007.03763.pdf)
>  This study aims to develop a real-time intersection optimization (RIO) control algorithm to efficiently serve traffic of Connected and Automated Vehicles (CAVs) and conventional vehicles (CNVs). This paper extends previous work to consider demand over capacity conditions and trajectory deviations by re-optimizing decisions. To jointly optimize Signal Phase and Timing (SPaT) and departure time of CAVs, we formulated a joint optimization model which is reduced to and solved as a Minimum Cost Flow (MCF) problem. The MCF-based optimization models is embedded into the RIO algorithm to operate the signal controller and to plan the movement of CAVs. Simulation experiments showed 18-22% travel time decrease and up to 12% capacity improvement compared to the base scenario.      
### 18.Transfer Learning for Electricity Price Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2007.03762.pdf)
>  Electricity price forecasting is an essential task for all the deregulated markets of the world. The accurate prediction of the day-ahead electricity prices is an active research field and available data from various markets can be used as an input for forecasting. A collection of models have been proposed for this task, but the fundamental question on how to use the available big data is often neglected. In this paper, we propose to use transfer learning as a tool for utilizing information from other electricity price markets for forecasting. We pre-train a bidirectional Gated Recurrent Units (BGRU) network on source markets and finally do a fine-tuning for the target market. Moreover, we test different ways to use the input data from various markets in the models. Our experiments on five different day-ahead markets indicate that transfer learning improves the performance of electricity price forecasting in a statistically significant manner.      
### 19.Compressive dual-comb spectroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2007.03761.pdf)
>  Broadband, high resolution and rapid measurement of dual-comb spectroscopy (DCS) generates a large amount of data stream. We numerically demonstrate significant data compression of DCS spectra by using a compressive sensing technique. Our simulation shows a compression rate of more than 100 with 3% error in mole fraction estimation of mid-infrared (MIR) DCS of two molecular species in a broadband (~30 THz) and high resolution (~115 MHz) condition. We also demonstrate a massively parallel MIR DCS spectrum of 10 different molecular species can be reconstructed with a compression rate of 10.5 with a transmittance error of 0.003 from the original spectrum.      
### 20.Surveying Off-Board and Extra-Vehicular Monitoring and Progress Towards Pervasive Diagnostics  [ :arrow_down: ](https://arxiv.org/pdf/2007.03759.pdf)
>  We survey the state-of-the-art in offboard diagnostics for vehicles, their occupants, and environments, with particular focus on vibroacoustic approaches. We identify promising application areas including data-driven management for shared mobility and automated fleets, usage-based insurance, and vehicle,occupant, and environmental state and condition monitoring. We close by exploring the particular application of vibroacoustic monitoring to vehicle diagnostics and prognostics and propose the introduction of automated vehicle- and context-specific model selection as a means of improving algorithm performance, e.g. to enable smartphone-resident diagnostics. The described approach may serve as the first step in developing "universal diagnostics" utilizing artificial intelligence, with applicability extending beyond the automotive domain.      
### 21.On Cokriging, Neural Networks, and Spatial Blind Source Separation for Multivariate Spatial Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2007.03747.pdf)
>  Multivariate measurements taken at irregularly sampled locations are a common form of data, for example in geochemical analysis of soil. In practical considerations predictions of these measurements at unobserved locations are of great interest. For standard multivariate spatial prediction methods it is mandatory to not only model spatial dependencies but also cross-dependencies which makes it a demanding task. Recently, a blind source separation approach for spatial data was suggested. When using this spatial blind source separation method prior the actual spatial prediction, modelling of spatial cross-dependencies is avoided, which in turn simplifies the spatial prediction task significantly. In this paper we investigate the use of spatial blind source separation as a pre-processing tool for spatial prediction and compare it with predictions from Cokriging and neural networks in an extensive simulation study as well as a geochemical dataset.      
### 22.Transfer Learning for Brain-Computer Interfaces: A Complete Pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2007.03746.pdf)
>  Transfer learning (TL) has been widely used in electroencephalogram (EEG) based brain-computer interfaces (BCIs) to reduce the calibration effort for a new subject, and demonstrated promising performance. After EEG signal acquisition, a closed-loop EEG-based BCI system also includes signal processing, feature engineering, and classification/regression blocks before sending out the control signal, whereas previous approaches only considered TL in one or two such components. This paper proposes that TL could be considered in all three components (signal processing, feature engineering, and classification/regression). Furthermore, it is also very important to specifically add a data alignment component before signal processing to make the data from different subjects more consistent, and hence to facilitate subsequential TL. Offline calibration experiments on two MI datasets verified our proposal. Especially, integrating data alignment and sophisticated TL approaches can significantly improve the classification performance, and hence greatly reduce the calibration effort.      
### 23.Predictive Analytics for Water Asset Management: Machine Learning and Survival Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2007.03744.pdf)
>  Understanding performance and prioritizing resources for the maintenance of the drinking-water pipe network throughout its life-cycle is a key part of water asset management. Renovation of this vital network is generally hindered by the difficulty or impossibility to gain physical access to the pipes. We study a statistical and machine learning framework for the prediction of water pipe failures. We employ classical and modern classifiers for a short-term prediction and survival analysis to provide a broader perspective and long-term forecast, usually needed for the economic analysis of the renovation. To enrich these models, we introduce new predictors based on water distribution domain knowledge and employ a modern oversampling technique to remedy the high imbalance coming from the few failures observed each year. For our case study, we use a dataset containing the failure records of all pipes within the water distribution network in Barcelona, Spain. The results shed light on the effect of important risk factors, such as pipe geometry, age, material, and soil cover, among others, and can help utility managers conduct more informed predictive maintenance tasks.      
### 24.Digital Twin Enabled Smart Control Engineering as an Industrial AI: A New Framework and A Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2007.03677.pdf)
>  In the way towards Industry 4.0, the complexity of the industrial systems increases due to the presence of multiple agents, Cyber-Physical Systems, distributed sensing, and big data introducing unknown dynamics that affect the production goals of the manufacturing processes. Thus, Digital Twin is a breaking technology corresponding to the capacity of developing a virtual representation of any complex system in order to perform design, analysis, and behavior prediction tasks that enhance the understanding of these systems through new enabling capabilities like real-time analytics, parallel sensing, or Smart Control Engineering. In this paper, a novel framework is proposed for the design and implementation of Digital Twin applications to the development of Smart Control Engineering. The steps of this framework involve system documentation, multidomain simulation, behavioral matching, and real-time monitoring. This framework is applied to develop the Digital Twin for a real-time vision feedback infrared temperature uniformity control. The obtained results show that Digital Twin is a fundamental part of the transformation into Industry 4.0.      
### 25.Information-Based Model Discrimination for Digital Twin Behavioral Matching  [ :arrow_down: ](https://arxiv.org/pdf/2007.03676.pdf)
>  Digital Twin is a breaking technology that allows creating virtual representations of complex physical systems based on updated information of the system and its physical laws. However, making the Digital Twin behavior matching with the real system can be challenging due to the number of unknown parameters in each twin. Its search can be done using optimization-based techniques, producing a family of models based on different system datasets, so, a discrimination criterion is required to determine the best Digital Twin model. This paper presents an information theory-based discrimination criterion to determine the best Digital Twin model resulting from a behavioral matching process. The information gain of a model is employed as a discrimination criterion. Box-Jenkins models are used to define the family of models for each behavioral matching result. The proposed method is compared with other information-based metrics as well as the $\nu$gap metric. As a study case, the discrimination method is applied to the Digital Twin for a real-time vision feedback infrared temperature uniformity control system. Obtained results show that information-based methodologies are useful for selecting an accurate Digital Twin model representing the system among a family of plants      
### 26.Tradespace analysis of GNSS Space Segment Architectures  [ :arrow_down: ](https://arxiv.org/pdf/2007.03675.pdf)
>  Global Navigation Satellite Systems (GNSS) provide ubiquitous, continuous and reliable positioning, navigation and timing information around the world. However, GNSS space segment design decisions have been based on the precursor Global Positioning System (GPS), which was designed in the 70s. This paper revisits those early design decisions in view of major technological advancements and new GNSS environmental threats. The rich tradespace between User Navigation Error (UNE) performance and constellation deployment costs is explored and conclusions are complemented by sensitivity analysis and association rule mining. This study finds that constellations at an orbit altitude of ~2 Earth radii can outperform existing GNSS in terms of cost, robustness and UNE. These insights should be taken into account when designing future generations of GNSS.      
### 27.Reconfigurable Intelligent Surface Empowered Terahertz Communication for LEO Satellite Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.04281.pdf)
>  The revolution in the low earth orbit (LEO) satellite networks will impose changes on their communication models, shifting from the classical bent-pipe architectures to more sophisticated networking platforms. Triggered by the technological advancements in the microelectronics and micro-systems, the terahertz (THz) band emerges as a strong candidate for the associated inter-satellite link (ISL) due to its high data rate promise. Yet, the propagation conditions of the THz band need to be properly modeled and/or controlled by utilizing reconfigurable intelligent surfaces (RISs) to assess their full potential. In this work, we first provide an assessment of the use of THz for the ISL, and quantify the impact of the misalignment fading on the error performance. Then, to compensate for the high path loss associated with the high carrier frequencies, we propose the use of RISs that are mounted on the neighboring satellites to enable signal propagation and to further improve the signal-to-noise ratio (SNR). Based on the mathematical analysis of the problem, we present the closed-form error rate expressions for RIS-assisted ISLs under misalignment fading. Numerical results demonstrate that the proposed RIS empowered THz communication solution reveals significant performance improvement introduced by the usage of RIS.      
### 28.Quantum Fan-out: Circuit Optimizations and Technology Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.04246.pdf)
>  Instruction scheduling is a key compiler optimization in quantum computing, just as it is for classical computing. Current schedulers optimize for data parallelism by allowing simultaneous execution of instructions, as long as their qubits do not overlap. However, on many quantum hardware platforms, instructions on overlapping qubits can be executed simultaneously through __global interactions__. For example, while fan-out in traditional quantum circuits can only be implemented sequentially when viewed at the logical level, global interactions at the physical level allow fan-out to be achieved in one step. We leverage this simultaneous fan-out primitive to optimize circuit synthesis for NISQ (Noisy Intermediate-Scale Quantum) workloads. In addition, we introduce novel quantum memory architectures based on fan-out. <br>Our work also addresses hardware implementation of the fan-out primitive. We perform realistic simulations for trapped ion quantum computers. We also demonstrate experimental proof-of-concept of fan-out with superconducting qubits. We perform depth (runtime) and fidelity estimation for NISQ application circuits and quantum memory architectures under realistic noise models. Our simulations indicate promising results with an asymptotic advantage in runtime, as well as 7--24% reduction in error.      
### 29.Mobility driven Cloud-Fog-Edge Framework for Location-aware Services: A Comprehensive Review  [ :arrow_down: ](https://arxiv.org/pdf/2007.04193.pdf)
>  With the pervasiveness of IoT devices, smart-phones and improvement of location-tracking technologies huge volume of heterogeneous geo-tagged (location specific) data is generated which facilitates several location-aware services. The analytics with this spatio-temporal (having location and time dimensions) datasets provide varied important services such as, smart transportation, emergency services (health-care, national defence or urban planning). While cloud paradigm is suitable for the capability of storage and computation, the major bottleneck is network connectivity loss. In time-critical application, where real-time response is required for emergency service-provisioning, such connectivity issues increases the latency and thus affects the overall quality of system (QoS). To overcome the issue, fog/ edge topology has emerged, where partial computation is carried out in the edge of the network to reduce the delay in communication. Such fog/ edge based system complements the cloud technology and extends the features of the system. This chapter discusses cloud-fog-edge based hierarchical collaborative framework, where several components are deployed to improve the QoS. On the other side. mobility is another critical factor to enhance the efficacy of such location-aware service provisioning. Therefore, this chapter discusses the concerns and challenges associated with mobility-driven cloud-fog-edge based framework to provide several location-aware services to the end-users efficiently.      
### 30.Mastering the working sequence in human-robot collaborative assembly based on reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.04140.pdf)
>  A long-standing goal of the Human-Robot Collaboration (HRC) in manufacturing systems is to increase the collaborative working efficiency. In line with the trend of Industry 4.0 to build up the smart manufacturing system, the Co-robot in the HRC system deserves better designing to be more self-organized and to find the superhuman proficiency by self-learning. Inspired by the impressive machine learning algorithms developed by Google Deep Mind like Alphago Zero, in this paper, the human-robot collaborative assembly working process is formatted into a chessboard and the selection of moves in the chessboard is used to analogize the decision making by both human and robot in the HRC assembly working process. To obtain the optimal policy of working sequence to maximize the working efficiency, the robot is trained with a self-play algorithm based on reinforcement learning, without guidance or domain knowledge beyond game rules. A neural network is also trained to predict the distribution of the priority of move selections and whether a working sequence is the one resulting in the maximum of the HRC efficiency. An adjustable desk assembly is used to demonstrate the proposed HRC assembly algorithm and its efficiency.      
### 31.Energy Optimization in Ultra-Dense Radio Access Networks via Traffic-Aware Cell Switching  [ :arrow_down: ](https://arxiv.org/pdf/2007.04133.pdf)
>  Ultra-dense deployments in 5G, the next generation of cellular networks, are an alternative to provide ultra-high throughput by bringing the users closer to the base stations. On the other hand, 5G deployments must not incur a large increase in energy consumption in order to keep them cost-effective and most importantly to reduce the carbon footprint of cellular networks. We propose a reinforcement learning cell switching algorithm, to minimize the energy consumption in ultra-dense deployments without compromising the quality of service (QoS) experienced by the users. In this regard, the proposed algorithm can intelligently learn which small cells (SCs) to turn off at any given time based on the traffic load of the SCs and the macro cell. To validate the idea, we used the open call detail record (CDR) data set from the city of Milan, Italy, and tested our algorithm against typical operational benchmark solutions. With the obtained results, we demonstrate exactly when and how the proposed algorithm can provide energy savings, and moreover how this happens without reducing QoS of users. Most importantly, we show that our solution has a very similar performance to the exhaustive search, with the advantage of being scalable and less complex.      
### 32.Optimizing Information Freshness via Multiuser Scheduling with Adaptive NOMA/OMA  [ :arrow_down: ](https://arxiv.org/pdf/2007.04072.pdf)
>  This paper considers a wireless network with a base station (BS) conducting timely status updates to multiple clients via adaptive non-orthogonal multiple access (NOMA)/orthogonal multiple access (OMA). Specifically, the BS is able to adaptively switch between NOMA and OMA for the downlink transmission to optimize the information freshness of the network, characterized by the Age of Information (AoI) metric. If the BS chooses OMA, it can only serve one client within each time slot and should decide which client to serve; if the BS chooses NOMA, it can serve more than one client at the same time and needs to decide the power allocated to the served clients. For the simple two-client case, we formulate a Markov Decision Process (MDP) problem and develop the optimal policy for the BS to decide whether to use NOMA or OMA for each downlink transmission based on the instantaneous AoI of both clients. The optimal policy is shown to have a switching-type property with obvious decision switching boundaries. A near-optimal policy with lower computation complexity is also devised. For the more general multi-client scenario, inspired by the proposed near-optimal policy, we formulate a nonlinear optimization problem to determine the optimal power allocated to each client by maximizing the expected AoI drop of the network in each time slot. We resolve the formulated problem by approximating it as a convex optimization problem. We also derive the upper bound of the gap between the approximate convex problem and the original nonlinear, nonconvex problem. Simulation results validate the effectiveness of the adopted approximation. The performance of the adaptive NOMA/OMA scheme by solving the convex optimization is shown to be close to that of max-weight policy solved by exhaustive search...      
### 33.General Framework and Novel Transceiver Architecture based on Hybrid Beamforming for NOMA in Massive MIMO Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.04056.pdf)
>  Massive MIMO and non-orthogonal multiple access (NOMA) are crucial methods for future wireless systems as they provide many advantages over conventional systems. Power domain NOMA methods are investigated in massive MIMO systems, whereas there is little work on integration of code domain NOMA and massive MIMO which is the subject of this study. We propose a general framework employing user-grouping based hybrid beamforming architecture for mm-wave massive MIMO systems where NOMA is considered as an intra-group process. It is shown that classical receivers of sparse code multiple access (SCMA) and multi-user shared access (MUSA) can be directly adapted. Additionally, a novel receiver architecture which is an improvement over classical one is proposed for uplink MUSA. This receiver makes MUSA preferable over SCMA for uplink transmission with lower complexity. We provide a lower bound on achievable information rate (AIR) as a performance measure. We show that code domain NOMA schemes outperform conventional methods with very limited number of radio frequency (RF) chains where users are spatially close to each other. Furthermore, we provide an analysis in terms of bit-error rate and AIR under different code length and overloading scenarios for uplink transmission where flexible structure of MUSA is exploited.      
### 34.Simultaneous Estimation of X-ray Back-Scatter and Forward-Scatter using Multi-Task Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.04018.pdf)
>  Scattered radiation is a major concern impacting X-ray image-guided procedures in two ways. First, back-scatter significantly contributes to patient (skin) dose during complicated interventions. Second, forward-scattered radiation reduces contrast in projection images and introduces artifacts in 3-D reconstructions. While conventionally employed anti-scatter grids improve image quality by blocking X-rays, the additional attenuation due to the anti-scatter grid at the detector needs to be compensated for by a higher patient entrance dose. This also increases the room dose affecting the staff caring for the patient. For skin dose quantification, back-scatter is usually accounted for by applying pre-determined scalar back-scatter factors or linear point spread functions to a primary kerma forward projection onto a patient surface point. However, as patients come in different shapes, the generalization of conventional methods is limited. Here, we propose a novel approach combining conventional techniques with learning-based methods to simultaneously estimate the forward-scatter reaching the detector as well as the back-scatter affecting the patient skin dose. Knowing the forward-scatter, we can correct X-ray projections, while a good estimate of the back-scatter component facilitates an improved skin dose assessment. To simultaneously estimate forward-scatter as well as back-scatter, we propose a multi-task approach for joint back- and forward-scatter estimation by combining X-ray physics with neural networks. We show that, in theory, highly accurate scatter estimation in both cases is possible. In addition, we identify research directions for our multi-task framework and learning-based scatter estimation in general.      
### 35.Guidestar-free image-guided wavefront-shaping  [ :arrow_down: ](https://arxiv.org/pdf/2007.03956.pdf)
>  Optical imaging through scattering media is a fundamental challenge in many applications. Recently, substantial breakthroughs such as imaging through biological tissues and looking around corners have been obtained by the use of wavefront-shaping approaches. However, these require an implanted guide-star for determining the wavefront correction, controlled coherent illumination, and most often raster scanning of the shaped focus. Alternative novel computational approaches that exploit speckle correlations, avoid guide-stars and wavefront control but are limited to small two-dimensional objects contained within the memory-effect correlations range. Here, we present a new concept, image-guided wavefront-shaping, allowing non-invasive, guidestar-free, widefield, incoherent imaging through highly scattering layers, without illumination control. Most importantly, the wavefront-correction is found even for objects that are larger than the memory-effect range, by blindly optimizing image-quality metrics. We demonstrate imaging of extended objects through highly-scattering layers and multi-core fibers, paving the way for non-invasive imaging in various applications, from microscopy to endoscopy.      
### 36.Improving Sound Event Detection In Domestic Environments Using Sound Separation  [ :arrow_down: ](https://arxiv.org/pdf/2007.03932.pdf)
>  Performing sound event detection on real-world recordings often implies dealing with overlapping target sound events and non-target sounds, also referred to as interference or noise. Until now these problems were mainly tackled at the classifier level. We propose to use sound separation as a pre-processing for sound event detection. In this paper we start from a sound separation model trained on the Free Universal Sound Separation dataset and the DCASE 2020 task 4 sound event detection baseline. We explore different methods to combine separated sound sources and the original mixture within the sound event detection. Furthermore, we investigate the impact of adapting the sound separation model to the sound event detection data on both the sound separation and the sound event detection.      
### 37.Training Sound Event Detection On A Heterogeneous Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2007.03931.pdf)
>  Training a sound event detection algorithm on a heterogeneous dataset including both recorded and synthetic soundscapes that can have various labeling granularity is a non-trivial task that can lead to systems requiring several technical choices. These technical choices are often passed from one system to another without being questioned. We propose to perform a detailed analysis of DCASE 2020 task 4 sound event detection baseline with regards to several aspects such as the type of data used for training, the parameters of the mean-teacher or the transformations applied while generating the synthetic soundscapes. Some of the parameters that are usually used as default are shown to be sub-optimal.      
### 38.SiENet: Siamese Expansion Network for Image Extrapolation  [ :arrow_down: ](https://arxiv.org/pdf/2007.03851.pdf)
>  Different from image inpainting, image outpainting has relative less context in the image center to capture and more content at the image border to predict. Therefore, classical encoder-decoder pipeline of existing methods may not predict the outstretched unknown content perfectly. In this paper, a novel two-stage siamese adversarial model for image extrapolation, named Siamese Expansion Network (SiENet) is proposed. In two stages, a novel border sensitive convolution named adaptive filling convolution is designed for allowing encoder to predict the unknown content, alleviating the burden of decoder. Besides, to introduce prior knowledge to network and reinforce the inferring ability of encoder, siamese adversarial mechanism is designed to enable our network to model the distribution of covered long range feature for that of uncovered image feature. The results on four datasets has demonstrated that our method outperforms existing state-of-the-arts and could produce realistic results.      
### 39.Fast Monte Carlo Simulation of Dynamic Power Systems Under Continuous Random Disturbances  [ :arrow_down: ](https://arxiv.org/pdf/2007.03847.pdf)
>  Continuous-time random disturbances from the renewable generation pose a significant impact on power system dynamic behavior. In evaluating this impact, the disturbances must be considered as continuous-time random processes instead of random variables that do not vary with time to ensure accuracy. Monte Carlo simulation (MCs) is a nonintrusive method to evaluate such impact that can be performed on commercial power system simulation software and is easy for power utilities to use, but is computationally cumbersome. Fast samplings methods such as Latin hypercube sampling (LHS) have been introduced to speed up sampling random variables, but yet cannot be applied to sample continuous disturbances. To overcome this limitation, this paper proposes a fast MCs method that enables the LHS to speed up sampling continuous disturbances, which is based on the It process model of the disturbances and the approximation of the It process by functions of independent normal random variables. A case study of the IEEE 39-Bus System shows that the proposed method is 47.6 and 6.7 times faster to converge compared to the traditional MCs in evaluating the expectation and variance of the system dynamic response.      
### 40.Efficient and Parallel Separable Dictionary Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.03800.pdf)
>  Separable, or Kronecker product, dictionaries provide natural decompositions for 2D signals, such as images. In this paper, we describe an algorithm to learn such dictionaries which is highly parallelizable and which reaches sparse representations competitive with the previous state of the art dictionary learning algorithms from the literature. We highlight the performance of the proposed method to sparsely represent image data and for image denoising applications.      
### 41.Acoustic Scene Classification with Spectrogram Processing Strategies  [ :arrow_down: ](https://arxiv.org/pdf/2007.03781.pdf)
>  Recently, convolutional neural networks (CNN) have achieved the state-of-the-art performance in acoustic scene classification (ASC) task. The audio data is often transformed into two-dimensional spectrogram representations, which are then fed to the neural networks. In this paper, we study the problem of efficiently taking advantage of different spectrogram representations through discriminative processing strategies. There are two main contributions. The first contribution is exploring the impact of the combination of multiple spectrogram representations at different stages, which provides a meaningful reference for the effective spectrogram fusion. The second contribution is that the processing strategies in multiple frequency bands and multiple temporal frames are proposed to make fully use of a single spectrogram representation. The proposed spectrogram processing strategies can be easily transferred to any network structures. The experiments are carried out on the DCASE 2020 Task1 datasets, and the results show that our method could achieve the accuracy of 81.8% (official baseline: 54.1%) and 92.1% (official baseline: 87.3%) on the officially provided fold 1 evaluation dataset of Task1A and Task1B, respectively.      
### 42.Electric Vehicle transition in the UK  [ :arrow_down: ](https://arxiv.org/pdf/2007.03745.pdf)
>  This paper provides an overview of the electric vehicle transition in the UK. The spatial disparity in the uptake of BEVs across the different regions is analysed using historic BEV sales. A forecast for future growth in BEVs (ignoring the impact of Covid-19) is performed using an s-curve model. Currently, South East England and Greater London have the highest BEV sales as a percentage of new vehicle sales. The spatial distribution of EV chargers across the different regions is also analysed. The spatial analysis clearly shows the regional disparity in the uptake of EV. South East England has the highest number of public chargers excluding Greater London. However, if we consider the number of EVs in that region, it has the second-lowest ratio of approx. 1 charger per 10 BEV. The lowest ratio being 0.8 in the West Midlands.      
### 43.Learning while Respecting Privacy and Robustness to Distributional Uncertainties and Adversarial Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.03724.pdf)
>  Data used to train machine learning models can be adversarial--maliciously constructed by adversaries to fool the model. Challenge also arises by privacy, confidentiality, or due to legal constraints when data are geographically gathered and stored across multiple learners, some of which may hold even an "anonymized" or unreliable dataset. In this context, the distributionally robust optimization framework is considered for training a parametric model, both in centralized and federated learning settings. The objective is to endow the trained model with robustness against adversarially manipulated input data, or, distributional uncertainties, such as mismatches between training and testing data distributions, or among datasets stored at different workers. To this aim, the data distribution is assumed unknown, and lies within a Wasserstein ball centered around the empirical data distribution. This robust learning task entails an infinite-dimensional optimization problem, which is challenging. Leveraging a strong duality result, a surrogate is obtained, for which three stochastic primal-dual algorithms are developed: i) stochastic proximal gradient descent with an $\epsilon$-accurate oracle, which invokes an oracle to solve the convex sub-problems; ii) stochastic proximal gradient descent-ascent, which approximates the solution of the convex sub-problems via a single gradient ascent step; and, iii) a distributionally robust federated learning algorithm, which solves the sub-problems locally at different workers where data are stored. Compared to the empirical risk minimization and federated learning methods, the proposed algorithms offer robustness with little computation overhead. Numerical tests using image datasets showcase the merits of the proposed algorithms under several existing adversarial attacks and distributional uncertainties.      
### 44.DRIVE: A Digital Network Oracle for Cooperative Intelligent Transportation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.03680.pdf)
>  In a world where Artificial Intelligence revolutionizes inference, prediction and decision-making tasks, Digital Twins emerge as game-changing tools. A case in point is the development and optimization of Cooperative Intelligent Transportation Systems (C-ITSs): a confluence of cyber-physical digital infrastructure and (semi)automated mobility. Herein we introduce Digital Twin for self-dRiving Intelligent VEhicles (DRIVE). The developed framework tackles shortcomings of traditional vehicular and network simulators. It provides a flexible, modular, and scalable implementation to ensure large-scale, city-wide experimentation with a moderate computational cost. The defining feature of our Digital Twin is a unique architecture allowing for submission of sequential queries, to which the Digital Twin provides instantaneous responses with the "state of the world", and hence is an Oracle. With such bidirectional interaction with external intelligent agents and realistic mobility traces, DRIVE provides the environment for development, training and optimization of Machine Learning based C-ITS solutions.      
