# ArXiv eess --Wed, 29 Jul 2020
### 1.Optimal Tree Topology for a Submarine Cable Network With Constrained Internodal Latency  [ :arrow_down: ](https://arxiv.org/pdf/2007.14380.pdf)
>  This paper provides an optimized cable path planning solution for a tree-topology network in an irregular 2D manifold in a 3D Euclidean space, with an application to the planning of submarine cable networks. Our solution method is based on total cost minimization, where the individual cable costs are assumed to be linear to the length of the corresponding submarine cables subject to latency constraints between pairs of nodes. These latency constraints limit the cable length and number of hops between any pair of nodes. Our method combines the Fast Marching Method (FMM) and a new Integer Linear Programming (ILP) formulation for Minimum Spanning Tree (MST) where there are constraints between pairs of nodes. We note that this problem of MST with constraints is NP-complete. Nevertheless, we demonstrate that ILP running time is adequate for the great majority of existing cable systems. For cable systems for which ILP is not able to find the optimal solution within an acceptable time, we propose an alternative heuristic algorithm based on Prim's algorithm. In addition, we apply our FMM/ILP-based algorithm to a real-world cable path planning example and demonstrate that it can effectively find an MST with latency constraints between pairs of nodes.      
### 2.Autosegmental Neural Nets: Should Phones and Tones be Synchronous or Asynchronous?  [ :arrow_down: ](https://arxiv.org/pdf/2007.14351.pdf)
>  Phones, the segmental units of the International Phonetic Alphabet (IPA), are used for lexical distinctions in most human languages; Tones, the suprasegmental units of the IPA, are used in perhaps 70%. Many previous studies have explored cross-lingual adaptation of automatic speech recognition (ASR) phone models, but few have explored the multilingual and cross-lingual transfer of synchronization between phones and tones. In this paper, we test four Connectionist Temporal Classification (CTC)-based acoustic models, differing in the degree of synchrony they impose between phones and tones. Models are trained and tested multilingually in three languages, then adapted and tested cross-lingually in a fourth. Both synchronous and asynchronous models are effective in both multilingual and cross-lingual settings. Synchronous models achieve lower error rate in the joint phone+tone tier, but asynchronous training results in lower tone error rate.      
### 3.A Hybrid Approach to Audio-to-Score Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2007.14333.pdf)
>  Audio-to-score alignment aims at generating an accurate mapping between a performance audio and the score of a given piece. Standard alignment methods are based on Dynamic Time Warping (DTW) and employ handcrafted features. We explore the usage of neural networks as a preprocessing step for DTW-based automatic alignment methods. Experiments on music data from different acoustic conditions demonstrate that this method generates robust alignments whilst being adaptable at the same time.      
### 4.On the use of GNSS for Automatic Detection of Attenuating Environments  [ :arrow_down: ](https://arxiv.org/pdf/2007.14329.pdf)
>  When different radio applications share the same spectrum, the separation by attenuating material is a way to mitigate potential interference. The indoor restriction for WLAN devices in 5150-5350 MHz is an example for a regulatory measure that aims at having WLAN devices operating in an environment that provides sufficient attenuation to enable sharing with other services. In this paper we investigate whether an attenuating environment can be automatically detected without user interaction. Instead of detecting an indoor location, we are directly looking for a detection of an attenuating environment. The basic idea is that signals from global navigation satellite services (GNSS) can be received practically everywhere on earth where there is a view to the sky. Where these signals are attenuated, the receiving device is assumed to be in an attenuating environment. In order to characterize such environment, we evaluate the detectable GNSS satellites and their carrier-to-noise density. Example measurements show that GNSS raw data can help to distinguish between low-attenuating locations and higher-attenuating locations. These measurements were conducted with GNSS receiver in an off-the-shelf Android tablet in order to show the feasibility of the approach.      
### 5.CovMUNET: A Multiple Loss Approach towards Detection of COVID-19 from Chest X-ray  [ :arrow_down: ](https://arxiv.org/pdf/2007.14318.pdf)
>  The recent outbreak of COVID-19 has halted the whole world, bringing a devastating effect on public health, global economy, and educational systems. As the vaccine of the virus is still not available, the most effective way to combat the virus is testing and social distancing. Among all other detection techniques, the Chest X-ray (CXR) based method can be a good solution for its simplicity, rapidity, cost, efficiency, and accessibility. In this paper, we propose CovMUNET, which is a multiple loss deep neural network approach to detect COVID-19 cases from CXR images. Extensive experiments are performed to ensure the robustness of the proposed algorithm and the performance is evaluated in terms of precision, recall, accuracy, and F1-score. The proposed method outperforms the state-of-the-art approaches with an accuracy of 96.97% for 3-class classification (COVID-19 vs normal vs pneumonia) and 99.41% for 2-class classification (COVID vs non-COVID). The proposed neural architecture also successfully detects the abnormality in CXR images.      
### 6.Primal or Dual Terminal Constraints in Economic MPC? -- Comparison and Insights  [ :arrow_down: ](https://arxiv.org/pdf/2007.14306.pdf)
>  This chapter compares different formulations for Economic nonlinear Model Predictive Control (EMPC) which are all based on an established dissipativity assumption on the underlying Optimal Control Problem (OCP). This includes schemes with and without stabilizing terminal constraints, respectively, or with stabilizing terminal costs. We recall that a recently proposed approach based on gradient correcting terminal penalties implies a terminal constraint on the adjoints of the OCP. We analyze the feasibility implications of these dual/adjoint terminal constraints and we compare our findings to approaches with and without primal terminal constraints. Moreover, we suggest a conceptual framework for approximation of the minimal stabilizing horizon length. Finally, we illustrate our findings considering a chemical reactor as an example.      
### 7.Cooperative Internet of UAVs: Distributed Trajectory Design by Multi-agent Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.14297.pdf)
>  Due to the advantages of flexible deployment and extensive coverage, unmanned aerial vehicles (UAVs) have great potential for sensing applications in the next generation of cellular networks, which will give rise to a cellular Internet of UAVs. In this paper, we consider a cellular Internet of UAVs, where the UAVs execute sensing tasks through cooperative sensing and transmission to minimize the age of information (AoI). However, the cooperative sensing and transmission is tightly coupled with the UAVs' trajectories, which makes the trajectory design challenging. To tackle this challenge, we propose a distributed sense-and-send protocol, where the UAVs determine the trajectories by selecting from a discrete set of tasks and a continuous set of locations for sensing and transmission. Based on this protocol, we formulate the trajectory design problem for AoI minimization and propose a compound-action actor-critic (CA2C) algorithm to solve it based on deep reinforcement learning. The CA2C algorithm can learn the optimal policies for actions involving both continuous and discrete variables and is suited for the trajectory design. {Our simulation results show that the CA2C algorithm outperforms four baseline algorithms}. Also, we show that by dividing the tasks, cooperative UAVs can achieve a lower AoI compared to non-cooperative UAVs.      
### 8.Monochrome and Color Polarization Demosaicking Using Edge-Aware Residual Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2007.14292.pdf)
>  A division-of-focal-plane or microgrid image polarimeter enables us to acquire a set of polarization images in one shot. Since the polarimeter consists of an image sensor equipped with a monochrome or color polarization filter array (MPFA or CPFA), the demosaicking process to interpolate missing pixel values plays a crucial role in obtaining high-quality polarization images. In this paper, we propose a novel MPFA demosaicking method based on edge-aware residual interpolation (EARI) and also extend it to CPFA demosaicking. The key of EARI is a new edge detector for generating an effective guide image used to interpolate the missing pixel values. We also present a newly constructed full color-polarization image dataset captured using a 3-CCD camera and a rotating polarizer. Using the dataset, we experimentally demonstrate that our EARI-based method outperforms existing methods in MPFA and CPFA demosaicking.      
### 9.DeepMP for Non-Negative Sparse Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2007.14281.pdf)
>  Non-negative signals form an important class of sparse signals. Many algorithms have already beenproposed to recover such non-negative representations, where greedy and convex relaxed algorithms are among the most popular methods. The greedy techniques are low computational cost algorithms, which have also been modified to incorporate the non-negativity of the representations. One such modification has been proposed for Matching Pursuit (MP) based algorithms, which first chooses positive coefficients and uses a non-negative optimisation technique that guarantees the non-negativity of the coefficients. The performance of greedy algorithms, like all non-exhaustive search methods, suffer from high coherence with the linear generative model, called the dictionary. We here first reformulate the non-negative matching pursuit algorithm in the form of a deep neural network. We then show that the proposed model after training yields a significant improvement in terms of exact recovery performance, compared to other non-trained greedy algorithms, while keeping the complexity low.      
### 10.Efficient adaptation of neural network filter for video compression  [ :arrow_down: ](https://arxiv.org/pdf/2007.14267.pdf)
>  We present an efficient finetuning methodology for neural-network filters which are applied as a postprocessing artifact-removal step in video coding pipelines. The fine-tuning is performed at encoder side to adapt the neural network to the specific content that is being encoded. In order to maximize the PSNR gain and minimize the bitrate overhead, we propose to finetune only the convolutional layers' biases. The proposed method achieves convergence much faster than conventional finetuning approaches, making it suitable for practical applications. The weight-update can be included into the video bitstream generated by the existing video codecs. We show that our method achieves up to 9.7% average BD-rate gain when compared to the state-of-art Versatile Video Coding (VVC) standard codec on 7 test sequences.      
### 11.Intelligent reflecting surface enhanced wideband MIMO-OFDM communications: From practical model to reflection optimization  [ :arrow_down: ](https://arxiv.org/pdf/2007.14243.pdf)
>  Intelligent reflecting surface (IRS) is envisioned as a revolutionary technology for future wireless communication systems since it can intelligently change radio environment and integrate it into wireless communication optimization. However, most recent investigation utilized an ideal IRS reflection model, which is impractical and can cause significant performance degradation in realistic wideband systems. In this work, we first study the amplitude-frequency-phase relationship of reflected signals and present a simplified practical IRS reflection model for wideband signals. Then, an IRS enhanced wideband multiuser multi-input single-output orthogonal frequency division multiplexing (MU-MISO-OFDM) system is investigated. We aim to jointly design the transmit beamformer and IRS reflection to maximize the average sum-rate over all subcarriers. With the aid of the relationship between sum-rate maximization and mean square error (MSE) minimization, the original problem is equivalently transformed into a multi-block/variable problem, which can be solved by classic block coordinate descent (BCD) method. Complexity and convergence for both cases are analyzed or illustrated. Simulation results demonstrate that the proposed algorithm can offer significant average sum-rate enhancement compared to that achieved using the ideal reflection model, which confirms the importance of the use of the practical model for the design of wideband systems.      
### 12.A Probabilistic Approach to Driver Assistance for Delay Reduction at Congested Highway Lane Drops  [ :arrow_down: ](https://arxiv.org/pdf/2007.14232.pdf)
>  This paper proposes an onboard advance warning system based on a probabilistic prediction model that advises vehicles on when to change lanes for an upcoming lane drop. The prediction model estimates the probability of reaching a goal state on the road using one or multiple lane changes. This estimate is based on several traffic-related parameters such as the distribution of inter-vehicle headway distances, as well as driver-related parameters like lane change duration. For an upcoming lane drop, the advance warning system uses the model and vehicle conditions at the moment to continuously estimate the probability of successfully changing lanes under those conditions before reaching the lane end, and advises the driver or autonomous vehicle to change lanes when that probability dips below a certain threshold. In a case study, the proposed system was used on a segment of the I-81 interstate highway with two lane drops - transitioning from four lanes to two lanes - to advise vehicles on avoiding the lane drops. The results show that the proposed system can reduce average delay up to 50% and maximum delay up to 33%, depending on traffic flow and the ratio of vehicles equipped with the advance warning system.      
### 13.Multimodal Integration for Large-Vocabulary Audio-Visual Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.14223.pdf)
>  For many small- and medium-vocabulary tasks, audio-visual speech recognition can significantly improve the recognition rates compared to audio-only systems. However, there is still an ongoing debate regarding the best combination strategy for multi-modal information, which should allow for the translation of these gains to large-vocabulary recognition. While an integration at the level of state-posterior probabilities, using dynamic stream weighting, is almost universally helpful for small-vocabulary systems, in large-vocabulary speech recognition, the recognition accuracy remains difficult to improve. In the following, we specifically consider the large-vocabulary task of the LRS2 database, and we investigate a broad range of integration strategies, comparing early integration and end-to-end learning with many versions of hybrid recognition and dynamic stream weighting. One aspect, which is shown to provide much benefit here, is the use of dynamic stream reliability indicators, which allow for hybrid architectures to strongly profit from the inclusion of visual information whenever the audio channel is distorted even slightly.      
### 14.Detecting and analysing spontaneous oral cancer speech in the wild  [ :arrow_down: ](https://arxiv.org/pdf/2007.14205.pdf)
>  Oral cancer speech is a disease which impacts more than half a million people worldwide every year. Analysis of oral cancer speech has so far focused on read speech. In this paper, we 1) present and 2) analyse a three-hour long spontaneous oral cancer speech dataset collected from YouTube. 3) We set baselines for an oral cancer speech detection task on this dataset. The analysis of these explainable machine learning baselines shows that sibilants and stop consonants are the most important indicators for spontaneous oral cancer speech detection.      
### 15.Hierarchical Control of Multi-Agent Systems using Online Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.14186.pdf)
>  We propose a new reinforcement learning based approach to designing hierarchical linear quadratic regulator (LQR) controllers for heterogeneous linear multi-agent systems with unknown state-space models and separated control objectives. The separation arises from grouping the agents into multiple non-overlapping groups, and defining the control goal as two distinct objectives. The first objective aims to minimize a group-wise block-decentralized LQR function that models group-level mission. The second objective, on the other hand, tries to minimize an LQR function between the average states (centroids) of the groups. Exploiting this separation, we redefine the weighting matrices of the LQR functions in a way that they allow us to decouple their respective algebraic Riccati equations. Thereafter, we develop a reinforcement learning strategy that uses online measurements of the agent states and the average states to learn the respective controllers based on the approximate Riccati equations. Since the first controller is block-decentralized and, therefore, can be learned in parallel, while the second controller is reduced-dimensional due to averaging, the overall design enjoys a significantly reduced learning time compared to centralized reinforcement learning.      
### 16.Low-complexity Point Cloud Filtering for LiDAR by PCA-based Dimension Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2007.14180.pdf)
>  Signals emitted by LiDAR sensors would often be negatively influenced during transmission by rain, fog, dust, atmospheric particles, scattering of light and other influencing factors, causing noises in point cloud images. To address this problem, this paper develops a new noise reduction method to filter LiDAR point clouds, i.e. an adaptive clustering method based on principal component analysis (PCA). Different from the traditional filtering methods that directly process three-dimension (3D) point cloud data, the proposed method uses dimension reduction to generate two-dimension (2D) data by extracting the first principal component and the second principal component of the original data with little information attrition. In the 2D space spanned by two principal components, the generated 2D data are clustered for noise reduction before being restored into 3D. Through dimension reduction and the clustering of the generated 2D data, this method derives low computational complexity, effectively removing noises while retaining details of environmental features. Compared with traditional filtering algorithms, the proposed method has higher precision and recall. Experimental results show a F-score as high as 0.92 with complexity reduced by 50% compared with traditional density-based clustering method.      
### 17.MMSE Channel Estimation for Two-Port Demodulation Reference Signals in New Radio  [ :arrow_down: ](https://arxiv.org/pdf/2007.14168.pdf)
>  Two-port demodulation reference signals (DMRS) have been employed in new radio (NR) recently. In this paper, we firstly propose a minimum mean square error (MMSE) scheme with full priori knowledge (F-MMSE) to achieve the channel estimation of two-port DMRS in NR. When the two ports are assigned to different users, the full priori knowledge of two ports is not easy to be obtained for one user. Then, we present a MMSE scheme with partial priori knowledge (P-MMSE). Finally, numerical results show that the proposed schemes achieve satisfactory channel estimation performance. Moreover, for both mean square error and bit error ratio metrics, the proposed schemes can achieve better performance compared with the classical discrete Fourier transform based channel estimation. Particularly, P-MMSE scheme delivers almost the same performance compared with F-MMSE scheme by a small amount of prior knowledge.      
### 18.Siamese x-vector reconstruction for domain adapted speaker recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.14146.pdf)
>  With the rise of voice-activated applications, the need for speaker recognition is rapidly increasing. The x-vector, an embedding approach based on a deep neural network (DNN), is considered the state-of-the-art when proper end-to-end training is not feasible. However, the accuracy significantly decreases when recording conditions (noise, sample rate, etc.) are mismatched, either between the x-vector training data and the target data or between enrollment and test data. We introduce the Siamese x-vector Reconstruction (SVR) for domain adaptation. We reconstruct the embedding of a higher quality signal from a lower quality counterpart using a lean auxiliary Siamese DNN. We evaluate our method on several mismatch scenarios and demonstrate significant improvement over the baseline.      
### 19.DeScarGAN: Disease-Specific Anomaly Detection with Weak Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2007.14118.pdf)
>  Anomaly detection and localization in medical images is a challenging task, especially when the anomaly exhibits a change of existing structures, e.g., brain atrophy or changes in the pleural space due to pleural effusions. In this work, we present a weakly supervised and detail-preserving method that is able to detect structural changes of existing anatomical structures. In contrast to standard anomaly detection methods, our method extracts information about the disease characteristics from two groups: a group of patients affected by the same disease and a healthy control group. Together with identity-preserving mechanisms, this enables our method to extract highly disease-specific characteristics for a more detailed detection of structural changes. We designed a specific synthetic data set to evaluate and compare our method against state-of-the-art anomaly detection methods. Finally, we show the performance of our method on chest X-ray images. Our method called DeScarGAN outperforms other anomaly detection methods on the synthetic data set and by visual inspection on the chest X-ray image data set.      
### 20.Energy Efficiency and Hover Time Optimization in UAV-based HetNets  [ :arrow_down: ](https://arxiv.org/pdf/2007.14098.pdf)
>  In this paper, we investigate the downlink performance of a three-tier heterogeneous network (HetNet). The objective is to enhance the edge capacity of a macro cell by deploying unmanned aerial vehicles (UAVs) as flying base stations and small cells (SCs) for improving the capacity of indoor users in scenarios such as temporary hotspot regions or during disaster situations where the terrestrial network is either insufficient or out of service. UAVs are energy-constrained devices with a limited flight time, therefore, we formulate a two layer optimization scheme, where we first optimize the power consumption of each tier for enhancing the system energy efficiency (EE) under a minimum quality-of-service (QoS) requirement, which is followed by optimizing the average hover time of UAVs. We obtain the solution to these nonlinear constrained optimization problems by first utilizing the Lagrange multipliers method and then implementing a sub-gradient approach for obtaining convergence. The results show that through optimal power allocation, the system EE improves significantly in comparison to when maximum power is allocated to users (ground cellular users or connected vehicles). The hover time optimization results in increased flight time of UAVs thus providing service for longer durations.      
### 21.A 3D Non-Stationary GBSM for Vehicular Visible Light Communication MISO Channels  [ :arrow_down: ](https://arxiv.org/pdf/2007.14036.pdf)
>  The potential of using visible light communication (VLC) technologies for vehicular communication networks has recently attracted much attention. The underlying VLC channels, as a foundation for the proper design and optimization of vehicular VLC communication systems, have not yet been sufficiently investigated. Vehicular VLC link impairments can have a significant impact on the system performance and capacity. Such impairments include the optical wireless channel distortion and background noise. This paper proposes a novel three-dimensional (3D) regular-shaped geometry-based stochastic model (RS-GBSM) for vehicular VLC multiple-input single-output (MISO) channels. The proposed 3D RS-GBSM combines a two-sphere model and an elliptic-cylinder model. Both the line-of-sight (LoS) and single-bounced (SB) components are considered. The proposed model jointly considers the azimuth and elevation angles by using von-Mises-Fisher (VMF) distribution. Based on the proposed model, the relationship between the communication range and the received optical power is analyzed and validated by simulations. The impact of the elevation angle in the 3D model on the received optical power is investigated by comparing with the received optical power of the corresponding two-dimensional (2D) model. Furthermore, the background noise is also modeled to evaluate the system's signal-to-noise ratio (SNR).      
### 22.UAV-Assisted Intelligent Reflecting Surface Symbiotic Radio System  [ :arrow_down: ](https://arxiv.org/pdf/2007.14029.pdf)
>  This paper investigates a symbiotic unmanned aerial vehicle (UAV)-assisted intelligent reflecting surface (IRS) radio system, where the UAV is leveraged to help the IRS reflect its own signals to the base station (BS), and meanwhile enhance the UAV transmission by passive beamforming at the IRS. First, we consider the problem of maximizing the minimum rate of the IRS by jointly optimizing the UAV trajectory, IRS phase shift matrix, and IRS scheduling, subject to the minimum primary rate requirements of the UAV. We find that conventional relaxation-based methods cannot solve this mixed integer non-convex problem since the minimum primary rate requirements may not be satisfied by the binary reconstruction operation. To address this issue, we first transform the binary constraints into an equivalent series of equality constraints. Then, a penalty-based algorithm is proposed to obtain a high-quality suboptimal solution. Second, we consider the weighted sum-rate maximization problem among all IRS. Although the proposed penalty-based algorithm can also be applied to this problem, it incurs high computational complexity. To reduce its complexity, we first relax the binary variables into continuous variables, and then propose an alternating optimization (AO) method to solve it. We prove that the obtained scheduling results are the same as the binary results from the AO method, which indicates that the primary rate requirements are always satisfied. Numerical results are provided to evaluate the performance of the proposed designs under different setups, as compared with benchmarks.      
### 23.BER Performance of Spatial Modulation Systems Under a Non-Stationary Massive MIMO Channel Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.14027.pdf)
>  In this paper, the bit error rate (BER) performance of spatial modulation (SM) systems is investigated both theoretically and by simulation in a non-stationary Kronecker-based massive multiple-input-multiple-output (MIMO) channel model in multi-user (MU) scenarios. Massive MIMO SM systems are considered in this paper using both a time-division multiple access (TDMA) scheme and a block diagonalization (BD) based precoding scheme, for different system settings. Their performance is compared with a vertical Bell labs layered space-time (V-BLAST) architecture based system and a conventional channel inversion system. It is observed that a higher cluster evolution factor can result in better BER performance of SM systems due to the low correlation among sub-channels. Compared with the BD-SM system, the SM system using the TDMA scheme obtains a better BER performance but with a much lower total system data rate. The BD-MU-SM system achieves the best trade-off between the data rate and the BER performance among all of the systems considered. When compared with the V-BLAST system and the channel inversion system, SM approaches offer advantages in performance for MU massive MIMO systems.      
### 24.Effects of Digital Map on the RT-based Channel Model for UAV mmWave Communications  [ :arrow_down: ](https://arxiv.org/pdf/2007.14020.pdf)
>  Based on the geometry and ray tracing (RT) theory, a millimeter wave (mmWave) channel model and parameter computation method for unmanned aerial vehicle (UAV) assisted air-to-ground (A2G) communications are proposed in this paper. In order to speed up the parameter calculation, a reconstruction process of scene database on the original digital map is developed. Moreover, the effects of reconstruction accuracy on the channel parameter and characteristic are analyzed by extensive simulations at 28 GHz under the campus scene. The simulation and analysis results show that the simplified database can save up to 50% time consumption. However, the difference of statistical properties is slight in the campus scenario.      
### 25.Semiglobal exponential input-to-state stability of sampled-data systems based on approximate discrete-time models  [ :arrow_down: ](https://arxiv.org/pdf/2007.14011.pdf)
>  Several control design strategies for sampled-data systems are based on a discrete-time model. In general, the exact discrete-time model of a nonlinear system is difficult or impossible to obtain, and hence approximate discrete-time models may be employed. Most existing results provide conditions under which the stability of the approximate discrete-time model in closed-loop carries over to the stability of the (unknown) exact discrete-time model but only in a practical sense, meaning that trajectories of the closed-loop system are ensured to converge to a bounded region whose size can be made as small as desired by limiting the maximum sampling period. In addition, some sufficient conditions exist that ensure global exponential stability of an exact model based on an approximate model. However, these conditions may be rather stringent due to the global nature of the result. In this context, our main contribution consists in providing rather mild conditions to ensure semiglobal exponential input-to-state stability of the exact model via an approximate model. The enabling condition, which we name the Robust Equilibrium-Preserving Consistency (REPC) property, is obtained by transforming a previously existing consistency condition into a semiglobal and perturbation-admitting condition. As a second contribution, we show that every explicit and consistent Runge-Kutta model satisfies the REPC condition and hence control design based on such a Runge-Kutta model can be used to ensure semiglobal exponential input-to-state stability of the exact discrete-time model in closed loop.      
### 26.Coupled Convolutional Neural Network with Adaptive Response Function Learning for Unsupervised Hyperspectral Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2007.14007.pdf)
>  Due to the limitations of hyperspectral imaging systems, hyperspectral imagery (HSI) often suffers from poor spatial resolution, thus hampering many applications of the imagery. Hyperspectral super-resolution refers to fusing HSI and MSI to generate an image with both high spatial and high spectral resolutions. Recently, several new methods have been proposed to solve this fusion problem, and most of these methods assume that the prior information of the Point Spread Function (PSF) and Spectral Response Function (SRF) are known. However, in practice, this information is often limited or unavailable. In this work, an unsupervised deep learning-based fusion method - HyCoNet - that can solve the problems in HSI-MSI fusion without the prior PSF and SRF information is proposed. HyCoNet consists of three coupled autoencoder nets in which the HSI and MSI are unmixed into endmembers and abundances based on the linear unmixing model. Two special convolutional layers are designed to act as a bridge that coordinates with the three autoencoder nets, and the PSF and SRF parameters are learned adaptively in the two convolution layers during the training process. Furthermore, driven by the joint loss function, the proposed method is straightforward and easily implemented in an end-to-end training manner. The experiments performed in the study demonstrate that the proposed method performs well and produces robust results for different datasets and arbitrary PSFs and SRFs.      
### 27.Spectral Superresolution of Multispectral Imagery with Joint Sparse and Low-Rank Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.14006.pdf)
>  Extensive attention has been widely paid to enhance the spatial resolution of hyperspectral (HS) images with the aid of multispectral (MS) images in remote sensing. However, the ability in the fusion of HS and MS images remains to be improved, particularly in large-scale scenes, due to the limited acquisition of HS images. Alternatively, we super-resolve MS images in the spectral domain by the means of partially overlapped HS images, yielding a novel and promising topic: spectral superresolution (SSR) of MS imagery. This is challenging and less investigated task due to its high ill-posedness in inverse imaging. To this end, we develop a simple but effective method, called joint sparse and low-rank learning (J-SLoL), to spectrally enhance MS images by jointly learning low-rank HS-MS dictionary pairs from overlapped regions. J-SLoL infers and recovers the unknown hyperspectral signals over a larger coverage by sparse coding on the learned dictionary pair. Furthermore, we validate the SSR performance on three HS-MS datasets (two for classification and one for unmixing) in terms of reconstruction, classification, and unmixing by comparing with several existing state-of-the-art baselines, showing the effectiveness and superiority of the proposed J-SLoL algorithm. Furthermore, the codes and datasets will be available at: <a class="link-external link-https" href="https://github.com/danfenghong/IEEE" rel="external noopener nofollow">this https URL</a>\_TGRS\_J-SLoL, contributing to the RS community.      
### 28.A Non-Stationary VVLC MIMO Channel Model for Street Corner Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2007.13987.pdf)
>  In recent years, the application potential of visible light communication (VLC) technology as an alternative and supplement to radio frequency (RF) technology has attracted people's attention. The study of the underlying VLC channel is the basis for designing the VLC communication system. In this paper, a new non-stationary geometric street corner model is proposed for vehicular VLC (VVLC) multiple-input multiple-output (MIMO) channel. The proposed model takes into account changes in vehicle speed and direction. The category of scatterers includes fixed scatterers and mobile scatterers (MS). Based on the proposed model, we derive the channel impulse response (CIR) and explore the statistical characteristics of the VVLC channel. The channel gain and root mean square (RMS) delay spread of the VVLC channel are studied. In addition, the influence of velocity change on the statistical characteristics of the model is also investigated. The proposed channel model can guide future vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) optical communication system design.      
### 29.Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13975.pdf)
>  The dominant speech separation models are based on complex recurrent or convolution neural network that model speech sequences indirectly conditioning on context, such as passing information through many intermediate states in recurrent neural network, leading to suboptimal separation performance. In this paper, we propose a dual-path transformer network (DPTNet) for end-to-end speech separation, which introduces direct context-awareness in the modeling for speech sequences. By introduces a improved transformer, elements in speech sequences can interact directly, which enables DPTNet can model for the speech sequences with direct context-awareness. The improved transformer in our approach learns the order information of the speech sequences without positional encodings by incorporating a recurrent neural network into the original transformer. In addition, the structure of dual paths makes our model efficient for extremely long speech sequence modeling. Extensive experiments on benchmark datasets show that our approach outperforms the current state-of-the-arts (20.6 dB SDR on the public WSj0-2mix data corpus).      
### 30.Multi-Frequency Multi-Scenario Millimeter Wave MIMO Channel Measurements and Modeling for B5G Wireless Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.13973.pdf)
>  Millimeter wave (mmWave) bands have been utilized for the fifth generation (5G) communication systems and will no doubt continue to be deployed for beyond 5G (B5G). However, the underlying channels are not fully investigated at multifrequency bands and in multi-scenarios by using the same channel sounder, especially for the outdoor, multiple-input multiple-output (MIMO), and vehicle-to-vehicle (V2V) conditions. In this paper, we conduct multi-frequency multi-scenario mmWave MIMO channel measurements with 4*4 antennas at 28, 32, and 39 GHz bands for three cases, i.e., the human body and vehicle blockage measurements, outdoor path loss measurements, and V2V measurements. The channel characteristics, including blockage effect, path loss and coverage range, and non-stationarity and spatial consistency, are thoroughly studied. The blockage model, path loss model, and time-varying channel model are proposed for mmWave MIMO channels. The channel measurement and modeling results will be of great importance for further mmWave communication system deployments in indoor hotspot, outdoor, and vehicular network scenarios for B5G.      
### 31.Deep Reinforcement Learning for Dynamic Spectrum Sensing and Aggregation in Multi-Channel Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.13965.pdf)
>  In this paper, the problem of dynamic spectrum sensing and aggregation is investigated in a wireless network containing N correlated channels, where these channels are occupied or vacant following an unknown joint 2-state Markov model. At each time slot, a single cognitive user with certain bandwidth requirement either stays idle or selects a segment comprising C (C &lt; N) contiguous channels to sense. Then, the vacant channels in the selected segment will be aggregated for satisfying the user requirement. The user receives a binary feedback signal indicating whether the transmission is successful or not (i.e., ACK signal) after each transmission, and makes next decision based on the sensing channel states. Here, we aim to find a policy that can maximize the number of successful transmissions without interrupting the primary users (PUs). The problem can be considered as a partially observable Markov decision process (POMDP) due to without full observation of system environment. We implement a Deep Q-Network (DQN) to address the challenge of unknown system dynamics and computational expenses. The performance of DQN, Q-Learning, and the Improvident Policy with known system dynamics is evaluated through simulations. The simulation results show that DQN can achieve near-optimal performance among different system scenarios only based on partial observations and ACK signals.      
### 32.End-to-End Energy Efficiency Evaluation for B5G Ultra Dense Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.13963.pdf)
>  Energy efficiency (EE) is a major performance metric for fifth generation (5G) and beyond 5G (B5G) wireless communication systems, especially for ultra dense networks. This paper proposes an end-to-end (e2e) power consumption model and studies the energy efficiency for a heterogeneous B5G cellular architecture that separates the indoor and outdoor communication scenarios in ultra dense networks. In this work, massive multiple-input-multiple-output (MIMO) technologies at conventional sub-6 GHz frequencies are used for long-distance outdoor communications. Light-Fidelity (LiFi) and millimeter wave (mmWave) technologies are deployed to provide a high data rate service to indoor users. Whereas, in the referenced nonseparated system, the indoor users communicate with the outdoor massive MIMO macro base station directly. The performance of these two systems are evaluated and compared in terms of the total power consumption and energy efficiency. The results show that the network architecture which separates indoor and outdoor communication can support a higher data rate transmission for less energy consumption, compared to non-separate communication scenario. In addition, the results show that deploying LiFi and mmWave IAPs can enable users to transmit at a higher data rate and further improve the EE.      
### 33.Neural Kalman Filtering for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2007.13962.pdf)
>  Statistical signal processing based speech enhancement methods adopt expert knowledge to design the statistical models and linear filters, which is complementary to the deep neural network (DNN) based methods which are data-driven. In this paper, by using expert knowledge from statistical signal processing for network design and optimization, we extend the conventional Kalman filtering (KF) to the supervised learning scheme, and propose the neural Kalman filtering (NKF) for speech enhancement. Two intermediate clean speech estimates are first produced from recurrent neural networks (RNN) and linear Wiener filtering (WF) separately and are then linearly combined by a learned NKF gain to yield the NKF output. Supervised joint training is applied to NKF to learn to automatically trade-off between the instantaneous linear estimation made by the WF and the long-term non-linear estimation made by the RNN. The NKF method can be seen as using expert knowledge from WF to regularize the RNN estimations to improve its generalization ability to the noise conditions unseen in the training. Experiments in different noisy conditions show that the proposed method outperforms the baseline methods both in terms of objective evaluation metrics and automatic speech recognition (ASR) word error rates (WERs).      
### 34.6G Oriented Wireless Communication Channel Characteristics Analysis and Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.13958.pdf)
>  Based on the vision on the 6G wireless communication network, i.e., global coverage, all spectrums and all applications, we comprehensively survey 6G related wireless channel measurements, channel characteristics, and channel models for all frequency bands and all scenarios. Millimeter wave (mmWave), terahertz (THz), optical band, satellite, unmanned aerial vehicle (UAV), maritime, underwater acoustic, high-speed train (HST), vehicle-to-vehicle (V2V), massive/ ultra-massive multiple-input multiple-output (MIMO), orbital angular momentum (OAM), and industry Internet of things (IoT) communication channels were particularly investigated. The related 6G channel measurement and modeling results were also given. Finally, future research challenges on 6G channel measurements and modeling were pointed out.      
### 35.EasierPath: An Open-source Tool for Human-in-the-loop Deep Learning of Renal Pathology  [ :arrow_down: ](https://arxiv.org/pdf/2007.13952.pdf)
>  Considerable morphological phenotyping studies in nephrology have emerged in the past few years, aiming to discover hidden regularities between clinical and imaging phenotypes. Such studies have been largely enabled by deep learning based image analysis to extract sparsely located targeting objects (e.g., glomeruli) on high-resolution whole slide images (WSI). However, such methods need to be trained using labor-intensive high-quality annotations, ideally labeled by pathologists. Inspired by the recent "human-in-the-loop" strategy, we developed EasierPath, an open-source tool to integrate human physicians and deep learning algorithms for efficient large-scale pathological image quantification as a loop. Using EasierPath, physicians are able to (1) optimize the recall and precision of deep learning object detection outcomes adaptively, (2) seamlessly support deep learning outcomes refining using either our EasierPath or prevalent ImageScope software without changing physician's user habit, and (3) manage and phenotype each object with user-defined classes. As a user case of EasierPath, we present the procedure of curating large-scale glomeruli in an efficient human-in-the-loop fashion (with two loops). From the experiments, the EasierPath saved 57 % of the annotation efforts to curate 8,833 glomeruli during the second loop. Meanwhile, the average precision of glomerular detection was leveraged from 0.504 to 0.620. The EasierPath software has been released as open-source to enable the large-scale glomerular prototyping. The code can be found in <a class="link-external link-https" href="https://github.com/yuankaihuo/EasierPath" rel="external noopener nofollow">this https URL</a>      
### 36.Control Barrier Functions for Abstraction-Free Control Synthesis under Temporal Logic Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2007.13925.pdf)
>  Temporal logic has been widely used to express complex task specifications for cyber-physical systems (CPSs). One way to synthesize a controller for CPS under temporal logic constraints is to first abstract the CPS as a discrete transition system, and then apply formal methods. This approach, however, is computationally demanding and its scalability suffers due to the curse of dimensionality. In this paper, we propose a control barrier function (CBF) approach to abstraction-free control synthesis under a linear temporal logic (LTL) constraint. We first construct the deterministic Rabin automaton of the specification and compute an accepting run. We then compute a sequence of LTL formulae, each of which must be satisfied during a particular time interval, and prove that satisfying the sequence of formulae is sufficient to satisfy the LTL specification. Finally, we compute a control policy for satisfying each formula by constructing an appropriate CBF. We present a quadratic program to compute the controllers, and show the controllers synthesized using the proposed approach guarantees the system to satisfy the LTL specification, provided the quadratic program is feasible at each time step. A numerical case study is presented to demonstrate the proposed approach.      
### 37.Semi-Supervised Learning with Data Augmentation for End-to-End ASR  [ :arrow_down: ](https://arxiv.org/pdf/2007.13876.pdf)
>  In this paper, we apply Semi-Supervised Learning (SSL) along with Data Augmentation (DA) for improving the accuracy of End-to-End ASR. We focus on the consistency regularization principle, which has been successfully applied to image classification tasks, and present sequence-to-sequence (seq2seq) versions of the FixMatch and Noisy Student algorithms. Specifically, we generate the pseudo labels for the unlabeled data on-the-fly with a seq2seq model after perturbing the input features with DA. We also propose soft label variants of both algorithms to cope with pseudo label errors, showing further performance improvements. We conduct SSL experiments on a conversational speech data set with 1.9kh manually transcribed training data, using only 25% of the original labels (475h labeled data). In the result, the Noisy Student algorithm with soft labels and consistency regularization achieves 10.4% word error rate (WER) reduction when adding 475h of unlabeled data, corresponding to a recovery rate of 92%. Furthermore, when iteratively adding 950h more unlabeled data, our best SSL performance is within 5% WER increase compared to using the full labeled training set (recovery rate: 78%).      
### 38.A Probabilistic Spectral Analysis of Multivariate Real-Valued Nonstationary Signals  [ :arrow_down: ](https://arxiv.org/pdf/2007.13855.pdf)
>  A class of multivariate spectral representations for real-valued nonstationary random variables is introduced, which is characterised by a general complex Gaussian distribution. In this way, the temporal signal properties -- harmonicity, wide-sense stationarity and cyclostationarity -- are designated respectively by the mean, Hermitian variance and pseudo-variance of the associated time-frequency representation (TFR). For rigour, the estimators of the TFR distribution parameters are derived within a maximum likelihood framework and are shown to be statistically consistent, owing to the statistical identifiability of the proposed distribution parametrization. By virtue of the assumed probabilistic model, a generalised likelihood ratio test (GLRT) for nonstationarity detection is also proposed. Intuitive examples demonstrate the utility of the derived probabilistic framework for spectral analysis in low-SNR environments.      
### 39.Improving Lesion Segmentation for Diabetic Retinopathy using Adversarial Learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.13854.pdf)
>  Diabetic Retinopathy (DR) is a leading cause of blindness in working age adults. DR lesions can be challenging to identify in fundus images, and automatic DR detection systems can offer strong clinical value. Of the publicly available labeled datasets for DR, the Indian Diabetic Retinopathy Image Dataset (IDRiD) presents retinal fundus images with pixel-level annotations of four distinct lesions: microaneurysms, hemorrhages, soft exudates and hard exudates. We utilize the HEDNet edge detector to solve a semantic segmentation task on this dataset, and then propose an end-to-end system for pixel-level segmentation of DR lesions by incorporating HEDNet into a Conditional Generative Adversarial Network (cGAN). We design a loss function that adds adversarial loss to segmentation loss. Our experiments show that the addition of the adversarial loss improves the lesion segmentation performance over the baseline.      
### 40.Learned Pre-Processing for Automatic Diabetic Retinopathy Detection on Eye Fundus Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.13838.pdf)
>  Diabetic Retinopathy is the leading cause of blindness in the working-age population of the world. The main aim of this paper is to improve the accuracy of Diabetic Retinopathy detection by implementing a shadow removal and color correction step as a preprocessing stage from eye fundus images. For this, we rely on recent findings indicating that application of image dehazing on the inverted intensity domain amounts to illumination compensation. Inspired by this work, we propose a Shadow Removal Layer that allows us to learn the pre-processing function for a particular task. We show that learning the pre-processing function improves the performance of the network on the Diabetic Retinopathy detection task.      
### 41.Deep Learning for Direction of Arrival Estimation via Emulation of Large Antenna Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2007.13824.pdf)
>  We present a MUSIC-based Direction of Arrival (DOA) estimation strategy using small antenna arrays, via employing deep learning for reconstructing the signals of a virtual large antenna array. Not only does the proposed strategy deliver significantly better performance than simply plugging the incoming signals into MUSIC, but surprisingly, the performance is also better than directly using an actual large antenna array with MUSIC for high angle ranges and low test SNR values. We further analyze the best choice for the training SNR as a function of the test SNR, and observe dramatic changes in the behavior of this function for different angle ranges.      
### 42.Supporting Safe Decision Making Through Holistic System-Level Representations &amp; Monitoring -- A Summary and Taxonomy of Self-Representation Concepts for Automated Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2007.13807.pdf)
>  The market introduction of automated vehicles has motivated intense research efforts into the safety of automated vehicle systems. Unlike driver assistance systems, SAE Level 3+ systems are not only responsible for executing (parts of) the dynamic driving task (DDT), but also for monitoring the automation system's performance at all times. Key components to fulfill these surveillance tasks are system monitors which can assess the system's performance at runtime, e.g. to activate fallback modules in case of partial system failures. In order to implement reasonable monitoring strategies for an automated vehicle, holistic system-level approaches are required, which make use of sophisticated internal system models. In this paper we present definitions and an according taxonomy, subsuming such models as a vehicle's self-representation and highlight the terms' roles in a scene and situation representation. Holistic system-level monitoring does not only provide the possibility to use monitors for the activation of fallbacks. In this paper we argue, why holistic system-level monitoring is a crucial step towards higher levels of automation, and give an example how it also enables the system to react to performance loss at a tactical level by providing input for decision making.      
### 43.Efficient minimum word error rate training of RNN-Transducer for end-to-end speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2007.13802.pdf)
>  In this work, we propose a novel and efficient minimum word error rate (MWER) training method for RNN-Transducer (RNN-T). Unlike previous work on this topic, which performs on-the-fly limited-size beam-search decoding and generates alignment scores for expected edit-distance computation, in our proposed method, we re-calculate and sum scores of all the possible alignments for each hypothesis in N-best lists. The hypothesis probability scores and back-propagated gradients are calculated efficiently using the forward-backward algorithm. Moreover, the proposed method allows us to decouple the decoding and training processes, and thus we can perform offline parallel-decoding and MWER training for each subset iteratively. Experimental results show that this proposed semi-on-the-fly method can speed up the on-the-fly method by 6 times and result in a similar WER improvement (3.6%) over a baseline RNN-T model. The proposed MWER training can also effectively reduce high-deletion errors (9.2% WER-reduction) introduced by RNN-T models when EOS is added for endpointer. Further improvement can be achieved if we use a proposed RNN-T rescoring method to re-rank hypotheses and use external RNN-LM to perform additional rescoring. The best system achieves a 5% relative improvement on an English test-set of real far-field recordings and a 11.6% WER reduction on music-domain utterances.      
### 44.SER Analysis for SWIPT-Enabled Differential Decode-and-Forward Relay Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.13740.pdf)
>  In this paper, we analyze the symbol error rate (SER) performance of the simultaneous wireless information and power transfer (SWIPT) enabled three-node differential decode-and-forward (DDF) relay networks, which adopt the power splitting (PS) protocol at the relay. The use of non-coherent differential modulation eliminates the need for sending training symbols to estimate the instantaneous channel state information (CSI) at all network nodes, and therefore improves the power efficiency, as compared with the coherent modulation. However, performance analysis results are not yet available for the state-of-the-art detectors such as the maximum-likelihood detector (MLD) and approximate MLD. Existing works rely on the Monte-Carlo simulation method to show that there exists an optimal PS ratio that minimizes the overall SER. In this work, we propose a near-optimal detector with linear complexity with respect to the modulation size. We derive an approximate SER expression and prove that the proposed detector achieves the full diversity order. Based on our expression, the optimal PS ratio can be accurately estimated without requiring any Monte-Carlo simulation. We also extend the proposed detector and the SER analysis for adopting the time switching (TS) protocol at the relay. Simulation results verify the accuracies of our detector and SER results in various scenarios.      
### 45.Accelerating Federated Learning over Reliability-Agnostic Clients in Mobile Edge Computing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.14374.pdf)
>  Mobile Edge Computing (MEC), which incorporates the Cloud, edge nodes and end devices, has shown great potential in bringing data processing closer to the data sources. Meanwhile, Federated learning (FL) has emerged as a promising privacy-preserving approach to facilitating AI applications. However, it remains a big challenge to optimize the efficiency and effectiveness of FL when it is integrated with the MEC architecture. Moreover, the unreliable nature (e.g., stragglers and intermittent drop-out) of end devices significantly slows down the FL process and affects the global model's quality in such circumstances. In this paper, a multi-layer federated learning protocol called HybridFL is designed for the MEC architecture. HybridFL adopts two levels (the edge level and the cloud level) of model aggregation enacting different aggregation strategies. Moreover, in order to mitigate stragglers and end device drop-out, we introduce regional slack factors into the stage of client selection performed at the edge nodes using a probabilistic approach without identifying or probing the state of end devices (whose reliability is agnostic). We demonstrate the effectiveness of our method in modulating the proportion of clients selected and present the convergence analysis for our protocol. We have conducted extensive experiments with machine learning tasks in different scales of MEC system. The results show that HybridFL improves the FL training process significantly in terms of shortening the federated round length, speeding up the global model's convergence (by up to 12X) and reducing end device energy consumption (by up to 58%)      
### 46.Optimal Probabilistic Motion Planning with Partially Infeasible LTL Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2007.14325.pdf)
>  This paper studies optimal probabilistic motion planning of a mobile agent in an uncertain environment where pre-specified tasks might not be fully realized. The agent's motion is modeled by a probabilistic labeled Markov decision process (MDP). A relaxed product MDP is developed, which allows the agent to revise its motion plan to not strictly follow the desired LTL constraints whenever the task is found to be infeasible. To evaluate the revised motion plan, a utility function composed of violation and implementation cost is developed, where the violation cost function is designed to quantify the differences between the revised and the desired motion plan, and the implementation cost are designed to bias the selection towards cost-efficient plans. Based on the developed utility function, a multi-objective optimization problem is formulated to jointly consider the implementation cost, the violation cost, and the satisfaction probability of tasks. Cost optimization in both prefix and suffix of the agent trajectory is then solved via coupled linear programs. Simulation results are provided to demonstrate its effectiveness.      
### 47.Learning Stable Manoeuvres in Quadruped Robots from Expert Demonstrations  [ :arrow_down: ](https://arxiv.org/pdf/2007.14290.pdf)
>  With the research into development of quadruped robots picking up pace, learning based techniques are being explored for developing locomotion controllers for such robots. A key problem is to generate leg trajectories for continuously varying target linear and angular velocities, in a stable manner. In this paper, we propose a two pronged approach to address this problem. First, multiple simpler policies are trained to generate trajectories for a discrete set of target velocities and turning radius. These policies are then augmented using a higher level neural network for handling the transition between the learned trajectories. Specifically, we develop a neural network-based filter that takes in target velocity, radius and transforms them into new commands that enable smooth transitions to the new trajectory. This transformation is achieved by learning from expert demonstrations. An application of this is the transformation of a novice user's input into an expert user's input, thereby ensuring stable manoeuvres regardless of the user's experience. Training our proposed architecture requires much less expert demonstrations compared to standard neural network architectures. Finally, we demonstrate experimentally these results in the in-house quadruped Stoch 2.      
### 48.Faster Mean-shift: GPU-accelerated Embedding-clustering for Cell Segmentation and Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2007.14283.pdf)
>  Recently, single-stage embedding based deep learning algorithms gain increasing attention in cell segmentation and tracking. Compared with the traditional "segment-then-associate" two-stage approach, a single-stage algorithm not only simultaneously achieves consistent instance cell segmentation and tracking but also gains superior performance when distinguishing ambiguous pixels on boundaries and overlapped objects. However, the deployment of an embedding based algorithm is restricted by slow inference speed (e.g., around 1-2 mins per frame). In this study, we propose a novel Faster Mean-shift algorithm, which tackles the computational bottleneck of embedding based cell segmentation and tracking. Different from previous GPU-accelerated fast mean-shift algorithms, a new online seed optimization policy (OSOP) is introduced to adaptively determine the minimal number of seeds, accelerate computation, and save GPU memory. With both embedding simulation and empirical validation via the four cohorts from the ISBI cell tracking challenge, the proposed Faster Mean-shift algorithm achieved 7-10 times speedup compared to the state-of-the-art embedding based cell instance segmentation and tracking algorithm. Our Faster Mean-shift algorithm also achieved the highest computational speed compared to other GPU benchmarks with optimized memory consumption. The Faster Mean-shift is a plug-and-play model, which can be employed on other pixel embedding based clustering inference for medical image analysis.      
### 49.A Review on Computational Intelligence Techniques in Cloud and Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2007.14215.pdf)
>  Cloud computing (CC) is a centralized computing paradigm that accumulates resources centrally and provides these resources to users through Internet. Although CC holds a large number of resources, it may not be acceptable by real-time mobile applications, as it is usually far away from users geographically. On the other hand, edge computing (EC), which distributes resources to the network edge, enjoys increasing popularity in the applications with low-latency and high-reliability requirements. EC provides resources in a decentralized manner, which can respond to users' requirements faster than the normal CC, but with limited computing capacities. As both CC and EC are resource-sensitive, several big issues arise, such as how to conduct job scheduling, resource allocation, and task offloading, which significantly influence the performance of the whole system. To tackle these issues, many optimization problems have been formulated. These optimization problems usually have complex properties, such as non-convexity and NP-hardness, which may not be addressed by the traditional convex optimization-based solutions. Computational intelligence (CI), consisting of a set of nature-inspired computational approaches, recently exhibits great potential in addressing these optimization problems in CC and EC. This paper provides an overview of research problems in CC and EC and recent progresses in addressing them with the help of CI techniques. Informative discussions and future research trends are also presented, with the aim of offering insights to the readers and motivating new research directions.      
### 50.Online Optimization with Barzilai-Borwein Quasi-Newton Method  [ :arrow_down: ](https://arxiv.org/pdf/2007.14198.pdf)
>  This paper considers the online case for the Barzilai-Borwein quasi-Newton method and presents a regret analysis. To solve online convex optimization problems, sequential decisions are made at each time-step using some algorithm of choice. We use a greedy online gradient algorithm based on Barzilai-Borwein (BB) step sizes and show that the regret obtained from our algorithm is sublinear in time and that the average regret approaches zero. Analysis is presented for the two forms of the Barzilai-Borwein step sizes.      
### 51.Generative networks as inverse problems with fractional wavelet scattering networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.14177.pdf)
>  Deep learning is a hot research topic in the field of machine learning methods and applications. Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) provide impressive image generations from Gaussian white noise, but both of them are difficult to train since they need to train the generator (or encoder) and the discriminator (or decoder) simultaneously, which is easy to cause unstable training. In order to solve or alleviate the synchronous training difficult problems of GANs and VAEs, recently, researchers propose Generative Scattering Networks (GSNs), which use wavelet scattering networks (ScatNets) as the encoder to obtain the features (or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder to generate the image. The advantage of GSNs is the parameters of ScatNets are not needed to learn, and the disadvantage of GSNs is that the expression ability of ScatNets is slightly weaker than CNNs and the dimensional reduction method of Principal Component Analysis (PCA) is easy to lead overfitting in the training of GSNs, and therefore affect the generated quality in the testing process. In order to further improve the quality of generated images while keep the advantages of GSNs, this paper proposes Generative Fractional Scattering Networks (GFRSNs), which use more expressive fractional wavelet scattering networks (FrScatNets) instead of ScatNets as the encoder to obtain the features (or FrScatNet embeddings) and use the similar CNNs of GSNs as the decoder to generate the image. Additionally, this paper develops a new dimensional reduction method named Feature-Map Fusion (FMF) instead of PCA for better keeping the information of FrScatNets and the effect of image fusion on the quality of image generation is also discussed.      
### 52.Accelerating ptychographic reconstructions using spectral initializations  [ :arrow_down: ](https://arxiv.org/pdf/2007.14139.pdf)
>  Ptychography is a promising phase retrieval technique for label-free quantitative phase imaging. In particular, the combination of a synthetic aperture approach with a reference-less setup makes it appealing to the biomedical community. Recent advancements in phase retrieval witnessed the development of spectral methods, in order to accelerate gradient descent algorithms. Using spectral initializations on both simulated and experimental data, for the first time we report 3 times faster ptychographic reconstructions than with a standard gradient descent algorithm. Spectral methods represent a paradigmatic change to develop new theoretical insights as well as experimental implementations of ptychography.      
### 53.Kalman Filter-based Head Motion Prediction for Cloud-based Mixed Reality  [ :arrow_down: ](https://arxiv.org/pdf/2007.14084.pdf)
>  Volumetric video allows viewers to experience highly-realistic 3D content with six degrees of freedom in mixed reality (MR) environments. Rendering complex volumetric videos can require a prohibitively high amount of computational power for mobile devices. A promising technique to reduce the computational burden on mobile devices is to perform the rendering at a cloud server. However, cloud-based rendering systems suffer from an increased interaction (motion-to-photon) latency that may cause registration errors in MR environments. One way of reducing the effective latency is to predict the viewer's head pose and render the corresponding view from the volumetric video in advance. In this paper, we design a Kalman filter for head motion prediction in our cloud-based volumetric video streaming system. We analyze the performance of our approach using recorded head motion traces and compare its performance to an autoregression model for different prediction intervals (look-ahead times). Our results show that the Kalman filter can predict head orientations 0.5 degrees more accurately than the autoregression model for a look-ahead time of 60 ms.      
### 54.Risk-Averse MPC via Visual-Inertial Input and Recurrent Networks for Online Collision Avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2007.14035.pdf)
>  In this paper, we propose an online path planning architecture that extends the model predictive control (MPC) formulation to consider future location uncertainties for safer navigation through cluttered environments. Our algorithm combines an object detection pipeline with a recurrent neural network (RNN) which infers the covariance of state estimates through each step of our MPC's finite time horizon. The RNN model is trained on a dataset that comprises of robot and landmark poses generated from camera images and inertial measurement unit (IMU) readings via a state-of-the-art visual-inertial odometry framework. To detect and extract object locations for avoidance, we use a custom-trained convolutional neural network model in conjunction with a feature extractor to retrieve 3D centroid and radii boundaries of nearby obstacles. The robustness of our methods is validated on complex quadruped robot dynamics and can be generally applied to most robotic platforms, demonstrating autonomous behaviors that can plan fast and collision-free paths towards a goal point.      
### 55.Superpixel Based Graph Laplacian Regularization for Sparse Hyperspectral Unmixing  [ :arrow_down: ](https://arxiv.org/pdf/2007.14033.pdf)
>  An efficient spatial regularization method using superpixel segmentation and graph Laplacian regularization is proposed for sparse hyperspectral unmixing method. A superpixel is defined as a group of structured neighboring pixels which constitutes a homogeneous region. First, we segment the hyperspectral image into many superpixels. Then, a weighted graph in each superpixel is constructed. Each node in the graph represents the spectrum of a pixel and edges connect the similar pixels inside the superpixel. The spatial similarity is investigated in each superpixel using graph Laplacian regularization. A weighted sparsity promoting norm is included in the formulation to sparsify the abundance matrix. Experimental results on simulated and real data sets show the superiority of the proposed algorithm over the well-known algorithms in the literature.      
### 56.Lane-Change Initiation and Planning Approach for Highly Automated Driving on Freeways  [ :arrow_down: ](https://arxiv.org/pdf/2007.14032.pdf)
>  Quantifying and encoding occupants' preferences as an objective function for the tactical decision making of autonomous vehicles is a challenging task. This paper presents a low-complexity approach for lane-change initiation and planning to facilitate highly automated driving on freeways. Conditions under which human drivers find different manoeuvres desirable are learned from naturalistic driving data, eliminating the need for an engineered objective function and incorporation of expert knowledge in form of rules. Motion planning is formulated as a finite-horizon optimisation problem with safety <a class="link-external link-http" href="http://constraints.It" rel="external noopener nofollow">this http URL</a> is shown that the decision model can replicate human drivers' discretionary lane-change decisions with up to 92% accuracy. Further proof of concept simulation of an overtaking manoeuvre is shown, whereby the actions of the simulated vehicle are logged while the dynamic environment evolves as per ground truth data recordings.      
### 57.Self-supervised Neural Audio-Visual Sound Source Localization via Probabilistic Spatial Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2007.13976.pdf)
>  Detecting sound source objects within visual observation is important for autonomous robots to comprehend surrounding environments. Since sounding objects have a large variety with different appearances in our living environments, labeling all sounding objects is impossible in practice. This calls for self-supervised learning which does not require manual labeling. Most of conventional self-supervised learning uses monaural audio signals and images and cannot distinguish sound source objects having similar appearances due to poor spatial information in audio signals. To solve this problem, this paper presents a self-supervised training method using 360 images and multichannel audio signals. By incorporating with the spatial information in multichannel audio signals, our method trains deep neural networks (DNNs) to distinguish multiple sound source objects. Our system for localizing sound source objects in the image is composed of audio and visual DNNs. The visual DNN is trained to localize sound source candidates within an input image. The audio DNN verifies whether each candidate actually produces sound or not. These DNNs are jointly trained in a self-supervised manner based on a probabilistic spatial audio model. Experimental results with simulated data showed that the DNNs trained by our method localized multiple speakers. We also demonstrate that the visual DNN detected objects including talking visitors and specific exhibits from real data recorded in a science museum.      
### 58.1-Bit Massive MIMO Transmission: Embracing Interference with Symbol-Level Precoding  [ :arrow_down: ](https://arxiv.org/pdf/2007.13950.pdf)
>  The deployment of large-scale antenna arrays for cellular base stations (BSs), termed as `Massive MIMO', has been a key enabler for meeting the ever increasing capacity requirement for 5G communication systems and beyond. Despite their promising performance, fully-digital massive MIMO systems require a vast amount of hardware components including radio frequency (RF) chains, power amplifiers, digital-to-analog converters (DACs), etc., resulting in a huge increase in terms of the total power consumption and hardware costs for cellular BSs. Towards both spectrally-efficient and energy-efficient massive MIMO deployment, a number of hardware-limited architectures have been proposed, including hybrid analog-digital (HAD) structures, constant-envelope (CE) transmission, and use of low-resolution DACs. In this paper, we overview the recent interest in improving the error-rate performance of massive MIMO systems deployed with 1-bit DACs through precoding at the symbol level. This line of research goes beyond traditional interference suppression or cancellation techniques by managing interference on a symbol-by-symbol basis. This provides unique opportunities for interference-aware precoding tailored for practical massive MIMO systems. Firstly, we characterise constructive interference (CI) and elaborate on how CI can benefit the 1-bit signal design by exploiting the traditionally undesired multi-user interference as well as the artificial interference from imperfect hardware components. Subsequently, we overview several solutions for 1-bit signal design to illustrate the gains achievable by exploiting CI. Finally, we identify some challenges and future research directions for 1-bit massive MIMO systems that are yet to be explored.      
### 59.Linear Delay-cell Design for Low-energy Delay Multiplication and Accumulation  [ :arrow_down: ](https://arxiv.org/pdf/2007.13895.pdf)
>  A practical deep neural network's (DNN) evaluation involves thousands of multiply-and-accumulate (MAC) operations. To extend DNN's superior inference capabilities to energy constrained devices, architectures and circuits that minimize energy-per-MAC must be developed. In this respect, analog delay-based MAC is advantageous due to reasons both extrinsic and intrinsic to the MAC implementation $-$ (1) lower fixed-point precision (1-8 bits) requirement in a DNN's evaluation, (2) better dynamic range than charge-based accumulation for smaller technology nodes and (3) simpler analog-digital interfaces. Implementing DNNs using delay-based MAC requires mixed-signal delay multipliers that accept digitally stored weights and analog voltages as arguments. To this end, a novel, linearly tune-able delay-cell is proposed, wherein, the delay is realized with an inverted MOS capacitor ($C^*$) steadily discharged from a linearly input-voltage dependent initial charge. The cell is analytically modeled, constraints for its functional validity are determined, and jitter-models are developed. Multiple cells with scaled delays, corresponding to each bit of the digital argument, must be cascaded to form the multiplier. To realize such bit-wise delay-scaling of the cells, a biasing circuit is proposed that generates sub-threshold gate-voltages to scale $C^*$'s discharging rate, and thus area-expensive transistor width-scaling is avoided. On applying the constraints and jitter models to 130nm technology, the minimum optimal $C^*$ was found to be 2 fF and maximum number of bits to be 5. Schematic-level simulations show a worst case energy-consumption close to the state-of-art, and thus, feasibility of the cell.      
### 60.openXDATA: A Tool for Multi-Target Data Generation and Missing Label Completion  [ :arrow_down: ](https://arxiv.org/pdf/2007.13889.pdf)
>  A common problem in machine learning is to deal with datasets with disjoint label spaces and missing labels. In this work, we introduce the openXDATA tool that completes the missing labels in partially labelled or unlabelled datasets in order to generate multi-target data with labels in the joint label space of the datasets. To this end, we designed and implemented the cross-data label completion (CDLC) algorithm that uses a multi-task shared-hidden-layer DNN to iteratively complete the sparse label matrix of the instances from the different datasets. We apply the new tool to estimate labels across four emotion datasets: one labeled with discrete emotion categories (e.g., happy, sad, angry), one labeled with continuous values along arousal and valence dimensions, one with both kinds of labels, and one unlabeled. Testing with drop-out of true labels, we show the ability to estimate both categories and continuous labels for all of the datasets, at rates that approached the ground truth values. openXDATA is available under the GNU General Public License from <a class="link-external link-https" href="https://github.com/fweninger/openXDATA" rel="external noopener nofollow">this https URL</a>.      
### 61.3DMaterialGAN: Learning 3D Shape Representation from Latent Space for Materials Science Applications  [ :arrow_down: ](https://arxiv.org/pdf/2007.13887.pdf)
>  In the field of computer vision, unsupervised learning for 2D object generation has advanced rapidly in the past few years. However, 3D object generation has not garnered the same attention or success as its predecessor. To facilitate novel progress at the intersection of computer vision and materials science, we propose a 3DMaterialGAN network that is capable of recognizing and synthesizing individual grains whose morphology conforms to a given 3D polycrystalline material microstructure. This Generative Adversarial Network (GAN) architecture yields complex 3D objects from probabilistic latent space vectors with no additional information from 2D rendered images. We show that this method performs comparably or better than state-of-the-art on benchmark annotated 3D datasets, while also being able to distinguish and generate objects that are not easily annotated, such as grain morphologies. The value of our algorithm is demonstrated with analysis on experimental real-world data, namely generating 3D grain structures found in a commercially relevant wrought titanium alloy, which were validated through statistical shape comparison. This framework lays the foundation for the recognition and synthesis of polycrystalline material microstructures, which are used in additive manufacturing, aerospace, and structural design applications.      
### 62.se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image Residuals in Synthetic Domains  [ :arrow_down: ](https://arxiv.org/pdf/2007.13866.pdf)
>  Tracking the 6D pose of objects in video sequences is important for robot manipulation. This task, however, introduces multiple challenges: (i) robot manipulation involves significant occlusions; (ii) data and annotations are troublesome and difficult to collect for 6D poses, which complicates machine learning solutions, and (iii) incremental error drift often accumulates in long term tracking to necessitate re-initialization of the object's pose. This work proposes a data-driven optimization approach for long-term, 6D pose tracking. It aims to identify the optimal relative pose given the current RGB-D observation and a synthetic image conditioned on the previous best estimate and the object's model. The key contribution in this context is a novel neural network architecture, which appropriately disentangles the feature encoding to help reduce domain shift, and an effective 3D orientation representation via Lie Algebra. Consequently, even when the network is trained only with synthetic data can work effectively over real images. Comprehensive experiments over benchmarks - existing ones as well as a new dataset with significant occlusions related to object manipulation - show that the proposed approach achieves consistently robust estimates and outperforms alternatives, even though they have been trained with real images. The approach is also the most computationally efficient among the alternatives and achieves a tracking frequency of 90.9Hz.      
### 63.Additive Tensor Decomposition Considering Structural Data Information  [ :arrow_down: ](https://arxiv.org/pdf/2007.13860.pdf)
>  Tensor data with rich structural information becomes increasingly important in process modeling, monitoring, and diagnosis. Here structural information is referred to structural properties such as sparsity, smoothness, low-rank, and piecewise constancy. To reveal useful information from tensor data, we propose to decompose the tensor into the summation of multiple components based on different structural information of them. In this paper, we provide a new definition of structural information in tensor data. Based on it, we propose an additive tensor decomposition (ATD) framework to extract useful information from tensor data. This framework specifies a high dimensional optimization problem to obtain the components with distinct structural information. An alternating direction method of multipliers (ADMM) algorithm is proposed to solve it, which is highly parallelable and thus suitable for the proposed optimization problem. Two simulation examples and a real case study in medical image analysis illustrate the versatility and effectiveness of the ATD framework.      
### 64.Adaptive LiDAR Sampling and Depth Completion using Ensemble Variance  [ :arrow_down: ](https://arxiv.org/pdf/2007.13834.pdf)
>  This work considers the problem of depth completion, with or without image data, where an algorithm may measure the depth of a prescribed limited number of pixels. The algorithmic challenge is to choose pixel positions strategically and dynamically to maximally reduce overall depth estimation error. This setting is realized in daytime or nighttime depth completion for autonomous vehicles with a programmable LiDAR. Our method uses an ensemble of predictors to define a sampling probability over pixels. This probability is proportional to the variance of the predictions of ensemble members, thus highlighting pixels that are difficult to predict. By additionally proceeding in several prediction phases, we effectively reduce redundant sampling of similar pixels. Our ensemble-based method may be implemented using any depth-completion learning algorithm, such as a state-of-the-art neural network, treated as a black box. In particular, we also present a simple and effective Random Forest-based algorithm, and similarly use its internal ensemble in our design. We conduct experiments on the KITTI dataset, using the neural network algorithm of Ma et al. and our Random Forest based learner for implementing our method. The accuracy of both implementations exceeds the state of the art. Compared with a random or grid sampling pattern, our method allows a reduction by a factor of 4-10 in the number of measurements required to attain the same accuracy.      
### 65.Energy Efficient Fog based Healthcare Monitoring Infrastructure  [ :arrow_down: ](https://arxiv.org/pdf/2007.13801.pdf)
>  Recent advances in mobile technologies and cloud computing services have inspired the development of cloud-based real-time health monitoring systems. However, the transfer of health-related data to the cloud contributes to the burden on the networking infrastructures, leading to high latency and increased power consumption. Fog computing is introduced to relieve this burden by bringing services to the users proximity. This study proposes a new fog computing architecture for health monitoring applications based on a Gigabit Passive Optical Network (GPON) access network. An Energy-Efficient Fog Computing (EEFC) model is developed using Mixed Integer Linear Programming (MILP) to optimize the number and location of fog devices at the network edge to process and analyze the health data for energy-efficient fog computing. The performance of the EEFC model at low data rates and high data rates health applications is studied. The outcome of the study reveals that a total energy saving of 36% and 52% are attained via processing and analysis the health data at the fog in comparison to conventional processing and analysis at the central cloud for low data rate application and high data rate application, respectively. We also developed a real-time heuristic; Energy Optimized Fog Computing (EOFC) heuristic, with energy consumption performance approaching the EEFC model. Furthermore, we examined the energy efficiency improvements under different scenarios of devices idle power consumption and traffic volume.      
### 66.Serverless computing for cloud-based power grid emergency generation dispatch  [ :arrow_down: ](https://arxiv.org/pdf/2007.13733.pdf)
>  Operating a modern power grid reliably in case of SCADA/EMS failure or amid difficult times like COVID-19 pandemic is a challenging task for grid operators. In [11], a PMU-based emergency generation dispatch scheme has been proposed to help the system operators with the supply and demand balancing; however, its realization highly relies on the control center infrastructure for computing and communication. This work, rather than using the on-premises server and dispatch communication system, proposes and implements a cloud-centric serverless architecture to ensure the operation continuity regardless of local infrastructure's availability and accessibility. Through its prototype implementation and evaluation at ISO New England, the solution has demonstrated two major advantages. Firstly, the cloud infrastructure is independent and fault-tolerant, providing grid monitoring and control capability even when EMS loses the corresponding functionality or when operators need to work remotely away from the control center. Secondly, the overall design is event-driven using serverless cloud services in response to the SCADA/EMS failure event. Thanks to "serverless", the burden of the server provisioning and maintenance can be avoided from the user side. The cost of using public cloud services for this solution is extremely low since it is architected and implemented based on the event-driven Function-as-a-Service (FaaS) model. This work also develops a comprehensive cyber security mechanism to comply with critical infrastructure requirements for the power grid, which can serve as an exemplary framework for other grid operators to secure their cloud services.      
