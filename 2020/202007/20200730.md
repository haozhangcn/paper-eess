# ArXiv eess --Thu, 30 Jul 2020
### 1.Investigation of Phase Distortion on Perceived Speech Quality for Hearing-impaired Listeners  [ :arrow_down: ](https://arxiv.org/pdf/2007.14986.pdf)
>  Phase serves as a critical component of speech that influences the quality and intelligibility. Current speech enhancement algorithms are beginning to address phase distortions, but the algorithms focus on normal-hearing (NH) listeners. It is not clear whether phase enhancement is beneficial for hearing-impaired (HI) listeners. We investigated the influence of phase distortion on speech quality through a listening study, in which NH and HI listeners provided speech-quality ratings using the MUSHRA procedure. In one set of conditions, the speech was mixed with babble noise at 4 different signal-to-noise ratios (SNRs) from -5 to 10 dB. In another set of conditions, the SNR was fixed at 10 dB and the noisy speech was presented in a simulated reverberant room with T60s ranging from 100 to 1000 ms. The speech level was kept at 65 dB SPL for NH listeners and amplification was applied for HI listeners to ensure audibility. Ideal ratio masking (IRM) was used to simulate speech enhancement. Two objective metrics (i.e., PESQ and HASQI) were utilized to compare subjective and objective ratings. Results indicate that phase distortion has a negative impact on perceived quality for both groups and PESQ is more closely correlated with human ratings.      
### 2.Neural Network-based Reconstruction in Compressed Sensing MRI Without Fully-sampled Training Data  [ :arrow_down: ](https://arxiv.org/pdf/2007.14979.pdf)
>  Compressed Sensing MRI (CS-MRI) has shown promise in reconstructing under-sampled MR images, offering the potential to reduce scan times. Classical techniques minimize a regularized least-squares cost function using an expensive iterative optimization procedure. Recently, deep learning models have been developed that model the iterative nature of classical techniques by unrolling iterations in a neural network. While exhibiting superior performance, these methods require large quantities of ground-truth images and have shown to be non-robust to unseen data. In this paper, we explore a novel strategy to train an unrolled reconstruction network in an unsupervised fashion by adopting a loss function widely-used in classical optimization schemes. We demonstrate that this strategy achieves lower loss and is computationally cheap compared to classical optimization solvers while also exhibiting superior robustness compared to supervised models. Code is available at <a class="link-external link-https" href="https://github.com/alanqrwang/HQSNet" rel="external noopener nofollow">this https URL</a>.      
### 3.On Loss Functions and Recurrency Training for GAN-based Speech Enhancement Systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.14974.pdf)
>  Recent work has shown that it is feasible to use generative adversarial networks (GANs) for speech enhancement, however, these approaches have not been compared to state-of-the-art (SOTA) non GAN-based approaches. Additionally, many loss functions have been proposed for GAN-based approaches, but they have not been adequately compared. In this study, we propose novel convolutional recurrent GAN (CRGAN) architectures for speech enhancement. Multiple loss functions are adopted to enable direct comparisons to other GAN-based systems. The benefits of including recurrent layers are also explored. Our results show that the proposed CRGAN model outperforms the SOTA GAN-based models using the same loss functions and it outperforms other non-GAN based systems, indicating the benefits of using a GAN for speech enhancement. Overall, the CRGAN model that combines an objective metric loss function with the mean squared error (MSE) provides the best performance over comparison approaches across many evaluation metrics.      
### 4.Reliable Tuberculosis Detection using Chest X-ray with Deep Learning, Segmentation and Visualization  [ :arrow_down: ](https://arxiv.org/pdf/2007.14895.pdf)
>  Tuberculosis (TB) is a chronic lung disease that occurs due to bacterial infection and is one of the top 10 leading causes of death. Accurate and early detection of TB is very important, otherwise, it could be life-threatening. In this work, we have detected TB reliably from the chest X-ray images using image pre-processing, data augmentation, image segmentation, and deep-learning classification techniques. Several public databases were used to create a database of 700 TB infected and 3500 normal chest X-ray images for this study. Nine different deep CNNs (ResNet18, ResNet50, ResNet101, ChexNet, InceptionV3, Vgg19, DenseNet201, SqueezeNet, and MobileNet), which were used for transfer learning from their pre-trained initial weights and trained, validated and tested for classifying TB and non-TB normal cases. Three different experiments were carried out in this work: segmentation of X-ray images using two different U-net models, classification using X-ray images, and segmented lung images. The accuracy, precision, sensitivity, F1-score, specificity in the detection of tuberculosis using X-ray images were 97.07 %, 97.34 %, 97.07 %, 97.14 % and 97.36 % respectively. However, segmented lungs for the classification outperformed than whole X-ray image-based classification and accuracy, precision, sensitivity, F1-score, specificity were 99.9 %, 99.91 %, 99.9 %, 99.9 %, and 99.52 % respectively. The paper also used a visualization technique to confirm that CNN learns dominantly from the segmented lung regions results in higher detection accuracy. The proposed method with state-of-the-art performance can be useful in the computer-aided faster diagnosis of tuberculosis.      
### 5.Speed-up Heuristic for an On-Demand Ride-Pooling Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2007.14877.pdf)
>  With ongoing developments in digitalization and advances in the field of autonomous driving, on-demand ride pooling is a mobility service with the potential to disrupt the urban mobility market. Nevertheless, to apply this kind of service successfully efficient algorithms have to be implemented for effective fleet management to exploit the benefits associated with this mobility service. Especially real time computation of finding beneficial assignments is a problem not solved for large problem sizes until today. In this study, we show the importance of using advanced algorithms by comparing a fast, but simple insertion heuristic algorithm with a state-of-the-art multi-step matching algorithm. We test the algorithms in various scenarios based on private vehicle trip OD-data for Munich, Germany. Results indicate that in the tested scenarios by using the multi-step algorithm up to 8$\%$ additional requests could be served while also 10$\%$ additional driven distance could be saved. However, computational time for finding optimal assignments in the advanced algorithm exceeds real time rather fast as problem size increases. Therefore, several aspects to reduce the computational time by decreasing redundant checks of the advanced multi step algorithm are introduced. Finally, a refined vehicle selection heuristic based on three rules is presented to furthermore reduce the computational effort. In the tested scenarios this heuristic can speed up the most cost intensive algorithm step by a factor of over 8, while keeping the number of served requests almost constant and maintaining around 70$\%$ of the driven distance saved in the system. Considering all algorithm steps, an overall speed up of 2.5 could be achieved.      
### 6.Data-driven Predictive Control for Unlocking Building Energy Flexibility: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2007.14866.pdf)
>  Managing supply and demand in the electricity grid is becoming more challenging due to the increasing penetration of variable renewable energy sources. As significant end-use consumers, and through better grid integration, buildings are expected to play an expanding role in the future smart grid. Predictive control allows buildings to better harness available energy flexibility from the building passive thermal mass. However, due to the heterogeneous nature of the building stock, developing computationally tractable control-oriented models, which adequately represent the complex and nonlinear thermal-dynamics of individual buildings, is proving to be a major hurdle. Data-driven predictive control, coupled with the "Internet of Things", holds the promise for a scalable and transferrable approach,with data-driven models replacing traditional physics-based models. This review examines recent work utilising data-driven predictive control for demand side management application with a special focus on the nexus of model development and control integration, which to date, previous reviews have not addressed. Further topics examined include the practical requirements for harnessing passive thermal mass and the issue of feature selection. Current research gaps are outlined and future research pathways are suggested to identify the most promising data-driven predictive control techniques for grid integration of buildings.      
### 7.Rethinking Maximum Flow Problem and Beamforming Design through Brain-inspired Geometric Lens  [ :arrow_down: ](https://arxiv.org/pdf/2007.14859.pdf)
>  Increasing data rate in wireless networks can be accomplished through a two-pronged approach, which are 1) increasing the network flow rate through parallel independent routes and 2) increasing the user's link rate through beamforming codebook adaptation. Mobile relays are utilized to enable achieving these goals given their flexible positioning. First at the network level, we model regularized Laplacian matrices, which are symmetric positive definite (SPD) ones representing relay-dependent network graphs, as points over Riemannian manifolds. Inspired by the geometric classification of different tasks in the brain network, Riemannian metrics, such as Log-Euclidean metric (LEM), are utilized to choose relay positions that result in maximum LEM. Simulation results show that the proposed LEM-based relay positioning algorithm enables parallel routes and achieves maximum network flow rate, as opposed to other metrics (e.g., algebraic connectivity). <br>Second at the link level, we design unique relay-dependent beamforming codebooks aimed to increase data rate over the spatially-correlated fading channels between a given relay and its neighboring users. To do so, we propose a geometric machine learning approach, which utilizes support vector machine (SVM) model to learn an SPD variant of the user's channel over Riemannian manifolds. Consequently, LEM-based Riemannian metric is utilized for classification of different channels, and a matched beamforming codebook is constructed accordingly. Simulation results show that the proposed geometric-based learning model achieves the maximum link rate after a short training period.      
### 8.Unsupervised Generative Adversarial Alignment Representation for Sheet music, Audio and Lyrics  [ :arrow_down: ](https://arxiv.org/pdf/2007.14856.pdf)
>  Sheet music, audio, and lyrics are three main modalities during writing a song. In this paper, we propose an unsupervised generative adversarial alignment representation (UGAAR) model to learn deep discriminative representations shared across three major musical modalities: sheet music, lyrics, and audio, where a deep neural network based architecture on three branches is jointly trained. In particular, the proposed model can transfer the strong relationship between audio and sheet music to audio-lyrics and sheet-lyrics pairs by learning the correlation in the latent shared subspace. We apply CCA components of audio and sheet music to establish new ground truth. The generative (G) model learns the correlation of two couples of transferred pairs to generate new audio-sheet pair for a fixed lyrics to challenge the discriminative (D) model. The discriminative model aims at distinguishing the input which is from the generative model or the ground truth. The two models simultaneously train in an adversarial way to enhance the ability of deep alignment representation learning. Our experimental results demonstrate the feasibility of our proposed UGAAR for alignment representation learning among sheet music, audio, and lyrics.      
### 9.TR-GAN: Topology Ranking GAN with Triplet Loss for Retinal Artery/Vein Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.14852.pdf)
>  Retinal artery/vein (A/V) classification lays the foundation for the quantitative analysis of retinal vessels, which is associated with potential risks of various cardiovascular and cerebral diseases. The topological connection relationship, which has been proved effective in improving the A/V classification performance for the conventional graph based method, has not been exploited by the deep learning based method. In this paper, we propose a Topology Ranking Generative Adversarial Network (TR-GAN) to improve the topology connectivity of the segmented arteries and veins, and further to boost the A/V classification performance. A topology ranking discriminator based on ordinal regression is proposed to rank the topological connectivity level of the ground-truth, the generated A/V mask and the intentionally shuffled mask. The ranking loss is further back-propagated to the generator to generate better connected A/V masks. In addition, a topology preserving module with triplet loss is also proposed to extract the high-level topological features and further to narrow the feature distance between the predicted A/V mask and the ground-truth. The proposed framework effectively increases the topological connectivity of the predicted A/V masks and achieves state-of-the-art A/V classification performance on the publicly available AV-DRIVE dataset.      
### 10.An Uncertainty-aware Transfer Learning-based Framework for Covid-19 Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2007.14846.pdf)
>  The early and reliable detection of COVID-19 infected patients is essential to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not available in many countries and also there are genuine concerns about their reliability and performance. Motivated by these shortcomings, this paper proposes a deep uncertainty-aware transfer learning framework for COVID-19 detection using medical images. Four popular convolutional neural networks (CNNs) including VGG16, ResNet50, DenseNet121, and InceptionResNetV2 are first applied to extract deep features from chest X-ray and computed tomography (CT) images. Extracted features are then processed by different machine learning and statistical modelling techniques to identify COVID-19 cases. We also calculate and report the epistemic uncertainty of classification results to identify regions where the trained models are not confident about their decisions (out of distribution problem). Comprehensive simulation results for X-ray and CT image datasets indicate that linear support vector machine and neural network models achieve the best results as measured by accuracy, sensitivity, specificity, and AUC. Also it is found that predictive uncertainty estimates are much higher for CT images compared to X-ray images.      
### 11.Efficient OCT Image Segmentation Using Neural Architecture Search  [ :arrow_down: ](https://arxiv.org/pdf/2007.14790.pdf)
>  In this work, we propose a Neural Architecture Search (NAS) for retinal layer segmentation in Optical Coherence Tomography (OCT) scans. We incorporate the Unet architecture in the NAS framework as its backbone for the segmentation of the retinal layers in our collected and pre-processed OCT image dataset. At the pre-processing stage, we conduct super resolution and image processing techniques on the raw OCT scans to improve the quality of the raw images. For our search strategy, different primitive operations are suggested to find the down- &amp; up-sampling cell blocks, and the binary gate method is applied to make the search strategy practical for the task in hand. We empirically evaluated our method on our in-house OCT dataset. The experimental results demonstrate that the self-adapting NAS-Unet architecture substantially outperformed the competitive human-designed architecture by achieving 95.4% in mean Intersection over Union metric and 78.7% in Dice similarity coefficient.      
### 12.PDCOVIDNet: A Parallel-Dilated Convolutional Neural Network Architecture for Detecting COVID-19 from Chest X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2007.14777.pdf)
>  The COVID-19 pandemic continues to severely undermine the prosperity of the global health system. To combat this pandemic, effective screening techniques for infected patients are indispensable. There is no doubt that the use of chest X-ray images for radiological assessment is one of the essential screening techniques. Some of the early studies revealed that the patient's chest X-ray images showed abnormalities, which is natural for patients infected with COVID-19. In this paper, we proposed a parallel-dilated convolutional neural network (CNN) based COVID-19 detection system from chest x-ray images, named as Parallel-Dilated COVIDNet (PDCOVIDNet). First, the publicly available chest X-ray collection fully preloaded and enhanced, and then classified by the proposed method. Differing convolution dilation rate in a parallel form demonstrates the proof-of-principle for using PDCOVIDNet to extract radiological features for COVID-19 detection. Accordingly, we have assisted our method with two visualization methods, which are specifically designed to increase understanding of the key components associated with COVID-19 infection. Both visualization methods compute gradients for a given image category related to feature maps of the last convolutional layer to create a class-discriminative region. In our experiment, we used a total of 2,905 chest X-ray images, comprising three cases (such as COVID-19, normal, and viral pneumonia), and empirical evaluations revealed that the proposed method extracted more significant features expeditiously related to the suspected disease. The experimental results demonstrate that our proposed method significantly improves performance metrics: accuracy, precision, recall, and F1 scores reach 96.58%, 96.58%, 96.59%, and 96.58%, respectively, which is comparable or enhanced compared with the state-of-the-art methods.      
### 13.On the unreasonable effectiveness of CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2007.14745.pdf)
>  Deep learning methods using convolutional neural networks (CNN) have been successfully applied to virtually all imaging problems, and particularly in image reconstruction tasks with ill-posed and complicated imaging models. In an attempt to put upper bounds on the capability of baseline CNNs for solving image-to-image problems we applied a widely used standard off-the-shelf network architecture (U-Net) to the "inverse problem" of XOR decryption from noisy data and show acceptable results.      
### 14.Intelligent Reflecting Surface based Passive Information Transmission: A Symbol-Level Precoding Approach  [ :arrow_down: ](https://arxiv.org/pdf/2007.14738.pdf)
>  Intelligent reflecting surfaces (IRS) have been introduced as a potentially revolutionary technology owing to its capability of adaptively controlling the propagation environment in a cost-effective and hardware-efficient fashion. While the application of IRS as a passive reflector for enhancing the performance of wireless communications has been widely investigated, recently the utilization of IRS as a passive transmitter is emerging as a new concept and attracting steadily growing interest. In this paper, we introduce two novel IRS-based passive information transmission systems using advanced symbol-level precoding. One is a pure passive information transmission system, where the IRS operates as a passive transmitter serving multiple receivers by changing the properties of its elements to reflect unmodulated carrier signals, and the other is a joint passive reflection and information transmission system, where the IRS not only enhances transmissions for multiple primary information receivers (PIRs) by passive reflection, but also simultaneously delivers additional information to a secondary information receiver (SIR) by embedding information into the primary signals at the symbol level. Two typical optimization problems, i.e., power minimization and quality-of-service (QoS) balancing, are investigated for the proposed IRS-based passive information transmission systems. Simulation results illustrate the feasibility of IRS-based passive information transmission and the effectiveness of our proposed algorithms.      
### 15.A 128-point Multi-Path SC FFT Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2007.14736.pdf)
>  This paper presents a new radix-2^k multi-path FFT architecture, named MSC FFT, which is based on a single-path radix-2 serial commutator (SC) FFT architecture. The proposed multi-path architecture has a very high hardware utilization that results in a small chip area, while providing high throughput. In addition, the adoption of radix-2^k FFT algorithms allows for simplifying the rotators even further. It is achieved by optimizing the structure of the processing element (PE). The implemented architecture is a 128-point 4-parallel multi-path SC FFT using 90 nm process. Its area and power consumption at 250 MHz are only 0.167 mm2 and 14.81 mW, respectively. Compared with existing works, the proposed design reduces significantly the chip rea and the power consumption, while providing high throughput.      
### 16.Multimodal Spatial Attention Module for Targeting Multimodal PET-CT Lung Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2007.14728.pdf)
>  Multimodal positron emission tomography-computed tomography (PET-CT) is used routinely in the assessment of cancer. PET-CT combines the high sensitivity for tumor detection with PET and anatomical information from CT. Tumor segmentation is a critical element of PET-CT but at present there is not an accurate automated segmentation method. Segmentation tends to be done manually by different imaging experts and it is labor-intensive and prone to errors and inconsistency. Previous automated segmentation methods largely focused on fusing information that is extracted separately from the PET and CT modalities, with the underlying assumption that each modality contains complementary information. However, these methods do not fully exploit the high PET tumor sensitivity that can guide segmentation. In this study, we introduce a multimodal spatial attention module (MSAM) that automatically learns to emphasize regions (spatial areas) related to tumors and suppress normal regions with physiologic high-uptake. The spatial attention maps are subsequently employed to target a convolutional neural network (CNN) for segmentation of areas with higher tumor likelihood. Our MSAM can be applied to common backbone architectures and trained end-to-end. Our experimental results on two clinical PET-CT datasets of non-small cell lung cancer (NSCLC) and soft tissue sarcoma (STS) validate the effectiveness of the MSAM in these different cancer types. We show that our MSAM, with a conventional U-Net backbone, surpasses the state-of-the-art lung tumor segmentation approach by a margin of 7.6% Dice similarity coefficient (DSC).      
### 17.Video compression with low complexity CNN-based spatial resolution adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2007.14726.pdf)
>  It has recently been demonstrated that spatial resolution adaptation can be integrated within video compression to improve overall coding performance by spatially down-sampling before encoding and super-resolving at the decoder. Significant improvements have been reported when convolutional neural networks (CNNs) were used to perform the resolution up-sampling. However, this approach suffers from high complexity at the decoder due to the employment of CNN-based super-resolution. In this paper, a novel framework is proposed which supports the flexible allocation of complexity between the encoder and decoder. This approach employs a CNN model for video down-sampling at the encoder and uses a Lanczos3 filter to reconstruct full resolution at the decoder. The proposed method was integrated into the HEVC HM 16.20 software and evaluated on JVET UHD test sequences using the All Intra configuration. The experimental results demonstrate the potential of the proposed approach, with significant bitrate savings (more than 10%) over the original HEVC HM, coupled with reduced computational complexity at both encoder (29%) and decoder (10%).      
### 18.End-to-End Adversarial White Box Attacks on Music Instrument Classification  [ :arrow_down: ](https://arxiv.org/pdf/2007.14714.pdf)
>  Small adversarial perturbations of input data are able to drastically change performance of machine learning systems, thereby challenging the validity of such systems. We present the very first end-to-end adversarial attacks on a music instrument classification system allowing to add perturbations directly to audio waveforms instead of spectrograms. Our attacks are able to reduce the accuracy close to a random baseline while at the same time keeping perturbations almost imperceptible and producing misclassifications to any desired instrument.      
### 19.Digital biomarkers and artificial intelligence for mass diagnosis of atrial fibrillation in a population sample at risk of sleep disordered breathing  [ :arrow_down: ](https://arxiv.org/pdf/2007.14686.pdf)
>  Atrial fibrillation (AF) is the most prevalent arrhythmia and is associated with a five-fold increase in stroke risk. Many individuals with AF go undetected. These individuals are often asymptomatic. There are ongoing debates on whether mass screening for AF is to be recommended. However, there is incentive in performing screening for specific at risk groups such as individuals suspected of sleep-disordered breathing where an important association between AF and obstructive sleep apnea (OSA) has been demonstrated. We introduce a new methodology leveraging digital biomarkers and recent advances in artificial intelligence (AI) for the purpose of mass AF diagnosis. We demonstrate the value of such methodology in a large population sample at risk of sleep disordered breathing. Four databases, totaling n=3,088 patients and p=26,913 hours of ECG raw data were used. Three of the databases (n=125, p=2,513) were used for training a machine learning model in recognizing AF events from beat-to-beat interval time series. The visit 1 of the sleep heart health study database (SHHS1, n=2,963, p=24,400) consists of overnight polysomnographic (PSG) recordings, and was considered as the test set. In SHHS1, expert inspection identified a total of 70 patients with a prominent AF rhythm. Model prediction on the SHHS1 showed an overall Se=0.97,Sp=0.99,NPV=0.99,PPV=0.67 in classifying individuals with or without prominent AF. PPV was non-inferior (p=0.03) for individuals with an apnea-hypopnea index (AHI) &gt; 15 versus AHI &lt; 15. Over 22% of correctly identified prominent AF rhythm cases were not documented as AF in the SHHS1. Individuals with prominent AF can be automatically diagnosed from an overnight single channel ECG recording, with an accuracy unaffected by the presence of OSA. AF detection from overnight ECG recording revealed a large proportion of undiagnosed AF and may enhance the phenotyping of OSA.      
### 20.Downlink Single-Snapshot Localization and Mapping with a Single-Antenna Receiver  [ :arrow_down: ](https://arxiv.org/pdf/2007.14679.pdf)
>  5G mmWave MIMO systems enable accurate estimation of the user position and mapping of the radio environment using a single snapshot when both the base station (BS) and user are equipped with large antenna arrays. However, massive arrays are initially expected only at the BS side, likely leaving users with one or very few antennas. In this paper, we propose a novel method for single-snapshot localization and mapping in the more challenging case of a user equipped with a single-antenna receiver. The joint maximum likelihood (ML) estimation problem is formulated and its solution formally derived. To avoid the burden of a full-dimensional search over the space of the unknown parameters, we present a novel practical approach that exploits the sparsity of mmWave channels to compute an approximate joint ML estimate. A thorough analysis, including the derivation of the CramÃ©r-Rao lower bounds, reveals that accurate localization and mapping can be achieved also in a MISO setup even when the direct line-of-sight path between the BS and the user is severely attenuated.      
### 21.COVID-19 CT Image Synthesis with a Conditional Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2007.14638.pdf)
>  Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has spread rapidly since December 2019. Real-time reverse transcription polymerase chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits of quick reporting, a low cost, and high sensitivity for the detection of pulmonary infection. Recently, deep-learning-based computer vision methods have demonstrated great promise for use in medical imaging applications, including X-rays, magnetic resonance imaging, and CT imaging. However, training a deep-learning model requires large volumes of data, and medical staff faces a high risk when collecting COVID-19 CT data due to the high infectivity of the disease. Another issue is the lack of experts available for data labeling. In order to meet the data requirements for COVID-19 CT imaging, we propose a CT image synthesis approach based on a conditional generative adversarial network that can effectively generate high-quality and realistic COVID-19 CT images for use in deep-learning-based medical imaging tasks. Experimental results show that the proposed method outperforms other state-of-the-art image synthesis methods with the generated COVID-19 CT images and indicates promising for various machine learning applications including semantic segmentation and classification.      
### 22.Solving Phase Retrieval with a Learned Reference  [ :arrow_down: ](https://arxiv.org/pdf/2007.14621.pdf)
>  Fourier phase retrieval is a classical problem that deals with the recovery of an image from the amplitude measurements of its Fourier coefficients. Conventional methods solve this problem via iterative (alternating) minimization by leveraging some prior knowledge about the structure of the unknown image. The inherent ambiguities about shift and flip in the Fourier measurements make this problem especially difficult; and most of the existing methods use several random restarts with different permutations. In this paper, we assume that a known (learned) reference is added to the signal before capturing the Fourier amplitude measurements. Our method is inspired by the principle of adding a reference signal in holography. To recover the signal, we implement an iterative phase retrieval method as an unrolled network. Then we use back propagation to learn the reference that provides us the best reconstruction for a fixed number of phase retrieval iterations. We performed a number of simulations on a variety of datasets under different conditions and found that our proposed method for phase retrieval via unrolled network and learned reference provides near-perfect recovery at fixed (small) computational cost. We compared our method with standard Fourier phase retrieval methods and observed significant performance enhancement using the learned reference.      
### 23.Transformer based unsupervised pre-training for acoustic representation learning  [ :arrow_down: ](https://arxiv.org/pdf/2007.14602.pdf)
>  Computational audio analysis has become a central issue in associated areas of research and a variety of related applications arised. However, for many acoustic tasks, the labeled data size may be limited. To handle this problem, We propose an unsupervised pre-training method using Transformer based encoder to learn a general and robust high-level representation for all acoustic tasks. Experiments have been conducted on three kinds of acoustic tasks: speech translation, speech emotion recognition and sound event detection. All the experiments have shown that pre-training using its own training data can significantly make the model converge faster and improve the performance. With a larger pre-training data combining MuST-C, Librispeech and ESC-US datasets, for speech translation, the BLEU score can further improve relatively 12.2% on En-De dataset and 8.4% on En-Fr datasets. For sound event detection, the F1 score can further improve absolutely 1.7% on DCASE2018 task5 development set and 2.4% on evaluation set. For speech emotion recognition, the UAR can further improve absolutely 4.3% on IEMOCAP dataset      
### 24.DNN No-Reference PSTN Speech Quality Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2007.14598.pdf)
>  Classic public switched telephone networks (PSTN) are often a black box for VoIP network providers, as they have no access to performance indicators, such as delay or packet loss. Only the degraded output speech signal can be used to monitor the speech quality of these networks. However, the current state-of-the-art speech quality models are not reliable enough to be used for live monitoring. One of the reasons for this is that PSTN distortions can be unique depending on the provider and country, which makes it difficult to train a model that generalizes well for different PSTN networks. In this paper, we present a new open-source PSTN speech quality test set with over 1000 crowdsourced real phone calls. Our proposed no-reference model outperforms the full-reference POLQA and no-reference P.563 on the validation and test set. Further, we analyzed the influence of file cropping on the perceived speech quality and the influence of the number of ratings and training size on the model accuracy.      
### 25.Adaptive Finite-time Disturbance Rejection for Nonlinear Systems using an Experience-Replay based Disturbance Observer  [ :arrow_down: ](https://arxiv.org/pdf/2007.14565.pdf)
>  Control systems are inevitably affected by external disturbances, and a major objective of the control design is to attenuate or eliminate their adverse effects on the system performance. This paper presents a disturbance rejection approach with two main improvements over existing results: 1) it relaxes the requirement of calculating or measuring the state derivatives, which are not available for measurement, and their calculation is corrupted by noise, and 2) it achieves finite-time disturbance rejection and control. To this end, the disturbance is first modeled by an unknown dynamics, and an adaptive disturbance observer is proposed to estimate it. A filtered regressor form is leveraged to model the nonlinear system and the unknown disturbance. It is shown that using this filtered regressor form, the disturbance is estimated using only measured state of the regressor. That is, contrary to the existing results on disturbance rejection, the presented approach does not require the state derivative measurements. To improve the convergence speed of the disturbance estimation, an adaptive law, equipped with experience replay, is presented. The disturbance observer is then augmented with an adaptive integral terminal sliding mode control to assure the finite-time convergence of tracking error to zero. A verifiable rank condition on the history of the past experience used by the experience-replay technique provides a sufficient condition for convergence. Compared to the existing results, neither the knowledge of the disturbance dynamics nor the state derivatives are required, and finite-time stability is guaranteed. A simulation example illustrates the effectiveness of the proposed approach.      
### 26.Accurate 2D soft segmentation of medical image via SoftGAN network  [ :arrow_down: ](https://arxiv.org/pdf/2007.14556.pdf)
>  Accurate 2D lung nodules segmentation from medical Computed Tomography (CT) images is crucial in medical applications. Most current approaches cannot achieve precise segmentation results that preserving both rich edge details description and smooth transition representations between image regions due to the tininess, complexities, and irregularities of lung nodule shapes. To address this issue, we propose a novel Cascaded Generative Adversarial Network (CasGAN) to cope with CT images super-resolution and segmentation tasks, in which the semantic soft segmentation form on precise lesion representation is introduced for the first time according to our knowledge, and lesion edges can be retained accurately after our segmentation that can promote rapid acquisition of high-quality large-scale annotation data based on RECIST weak supervision information. Extensive experiments validate that our CasGAN outperforms the state-of-the-art methods greatly in segmentation quality, which is also robust on the application of medical images beyond lung nodules. Besides, we provide a challenging lung nodules soft segmentation dataset of medical CT images for further studies.      
### 27.A Research Journey of Full-Duplex at University of California from Self-Interference Cancellation to Wireless Network Security  [ :arrow_down: ](https://arxiv.org/pdf/2007.14533.pdf)
>  This article provides an overview of research on full-duplex at the University of California, Riverside, in the past decade. This research was initially focused on self-interference (SI) cancellation, then moved to applications of full-duplex to improve network spectral efficiency, and in recent years advanced to discover full-duplex's potentials for wireless network security. The research on SI cancellation has resulted in both hardware-based SI cancellation results and some advanced theoretical architectures which show promises but are yet to be tested via advanced hardware implementations. The applications of full-duplex for optimized spectral efficiency in ad hoc, cognitive and cellular networks have shown how to optimize power allocation among full-duplex nodes, in their antenna beamspace and over multiple subcarriers. Full-duplex has also been found to be highly beneficial for improving secrecy capacity between legitimate users against eavesdropping. Among the new capabilities that full-duplex provides for network security is an effective anti-eavesdropping channel estimation scheme which is not possible for nodes without full-duplex.      
### 28.Decompose X-ray Images for Bone and Soft Tissue  [ :arrow_down: ](https://arxiv.org/pdf/2007.14510.pdf)
>  Bones are always wrapped by soft tissues. As a result, bones in their X-ray images are obscured and become unclear. In this paper, we tackle this problem and propose a novel task to virtually decompose the soft tissue and bone by image processing algorithms. This task is fundamentally different from segmentation because the decomposed images share the same imaging domain. Our decomposition task is also fundamentally different from the conventional image enhancement. We propose a new mathematical model for such decomposition. Our model is ill-posed and thus it requires some priors. With proper assumptions, our model can be solved by solving a standard Laplace equation. The resulting bone image is theoretically guaranteed to have better contrast than the original input image. Therefore, the details of bones get enhanced and become clearer. Several numerical experiments confirm the effective and efficiency of our method. Our approach is important for clinical diagnosis, surgery planning, recognition, deep learning, etc.      
### 29.AutoClip: Adaptive Gradient Clipping for Source Separation Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.14469.pdf)
>  Clipping the gradient is a known approach to improving gradient descent, but requires hand selection of a clipping threshold hyperparameter. We present AutoClip, a simple method for automatically and adaptively choosing a gradient clipping threshold, based on the history of gradient norms observed during training. Experimental results show that applying AutoClip results in improved generalization performance for audio source separation networks. Observation of the training dynamics of a separation network trained with and without AutoClip show that AutoClip guides optimization into smoother parts of the loss landscape. AutoClip is very simple to implement and can be integrated readily into a variety of applications across multiple domains.      
### 30.Few-Shot Keyword Spotting With Prototypical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.14463.pdf)
>  Recognizing a particular command or a keyword, keyword spotting has been widely used in many voice interfaces such as Amazon's Alexa and Google Home. In order to recognize a set of keywords, most of the recent deep learning based approaches use a neural network trained with a large number of samples to identify certain pre-defined keywords. This restricts the system from recognizing new, user-defined keywords. Therefore, we first formulate this problem as a few-shot keyword spotting and approach it using metric learning. To enable this research, we also synthesize and publish a Few-shot Google Speech Commands dataset. We then propose a solution to the few-shot keyword spotting problem using temporal and dilated convolutions on prototypical networks. Our comparative experimental results demonstrate keyword spotting of new keywords using just a small number of samples.      
### 31.Enhancement of Retinal Fundus Images via Pixel Color Amplification  [ :arrow_down: ](https://arxiv.org/pdf/2007.14456.pdf)
>  We propose a pixel color amplification theory and family of enhancement methods to facilitate segmentation tasks on retinal images. Our novel re-interpretation of the image distortion model underlying dehazing theory shows how three existing priors commonly used by the dehazing community and a novel fourth prior are related. We utilize the theory to develop a family of enhancement methods for retinal images, including novel methods for whole image brightening and darkening. We show a novel derivation of the Unsharp Masking algorithm. We evaluate the enhancement methods as a pre-processing step to a challenging multi-task segmentation problem and show large increases in performance on all tasks, with Dice score increases over a no-enhancement baseline by as much as 0.491. We provide evidence that our enhancement preprocessing is useful for unbalanced and difficult data. We show that the enhancements can perform class balancing by composing them together.      
### 32.Extending LOUPE for K-space Under-sampling Pattern Optimization in Multi-coil MRI  [ :arrow_down: ](https://arxiv.org/pdf/2007.14450.pdf)
>  The previously established LOUPE (Learning-based Optimization of the Under-sampling Pattern) framework for optimizing the k-space sampling pattern in MRI was extended in three folds: firstly, fully sampled multi-coil k-space data from the scanner, rather than simulated k-space data from magnitude MR images in LOUPE, was retrospectively under-sampled to optimize the under-sampling pattern of in-vivo k-space data; secondly, binary stochastic k-space sampling, rather than approximate stochastic k-space sampling of LOUPE during training, was applied together with a straight-through (ST) estimator to estimate the gradient of the threshold operation in a neural network; thirdly, modified unrolled optimization network, rather than modified U-Net in LOUPE, was used as the reconstruction network in order to reconstruct multi-coil data properly and reduce the dependency on training data. Experimental results show that when dealing with the in-vivo k-space data, unrolled optimization network with binary under-sampling block and ST estimator had better reconstruction performance compared to the ones with either U-Net reconstruction network or approximate sampling pattern optimization network, and once trained, the learned optimal sampling pattern worked better than the hand-crafted variable density sampling pattern when deployed with other conventional reconstruction methods.      
### 33.Opacity of Discrete Event Systems with Active Intruder  [ :arrow_down: ](https://arxiv.org/pdf/2007.14960.pdf)
>  Opacity is a security property formalizing the information leakage of a system to an external observer, namely intruder. The conventional opacity that has been studied in the Discrete Event System (DES) literature usually assumes passive intruders, who only observe the behavior of the system. However, in many cybersecurity concerns, such as web service, active intruders, who are capable of influencing the system's behavior beyond passive observations, need to be considered and defended against. We are therefore motivated to extend the opacity notions to handle active intruders. For this, we model the system as a non-deterministic finite-state transducer. It is assumed that the intruder has a full knowledge of the system structure and is capable of interacting with the system by injecting different inputs and observing its responses. In this setup, we first introduce reactive current-state opacity (RCSO) notion characterizing a property that the system does not leak its secret state regardless of how the intruder manipulates the system behavior. We furthermore extend this notion to language-based and initial-state reactive opacity notions, and study the relationship among them. It turns out that all the proposed reactive opacity notions are equivalent to RCSO. We therefore focus on RCSO and study its verification problem. It is shown that the RCSO can be verified by constructing an observer automaton.      
### 34.Spatio-temporal Consistency to Detect Potential Aedes aegypti Breeding Grounds in Aerial Video Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2007.14863.pdf)
>  Every year, the \textit{Aedes aegypti} mosquito infects thousands of people with diseases such as dengue, zika, chikungunya, and urban yellow fever. The main form to combat these diseases is to avoid the transmitter reproduction by searching and eliminating the potential mosquito breeding grounds. In this work, we introduce a comprehensive database of aerial videos recorded with a drone, where all objects of interest are identified by their respective bounding boxes, and describe an object detection system based on deep neural networks. We track the objects by employing phase correlation to obtain the spatial alignment between them along the video frames. By doing so, we are capable of registering the detected objects, minimizing false positives and correcting most false negatives. Using the ResNet-101-FPN as a backbone, it is possible to obtain 0.78 in terms of \textit{F1-score} on the proposed dataset.      
### 35.Parameter identifiability and input-output equations  [ :arrow_down: ](https://arxiv.org/pdf/2007.14787.pdf)
>  Structural parameter identifiability is a property of a differential model with parameters that allows for the parameters to be determined from the model equations in the absence of noise. One of the standard approaches to assessing this problem is via input-output equations and, in particular, characteristic sets of differential ideals. The precise relation between identifiability and input-output identifiability is subtle. The goal of this note is to clarify this relation. The main results are: <br>1) identifiability implies input-output identifiability; <br>2) these notions coincide if the model does not have rational first integrals; <br>3) the field of input-output identifiable functions is generated by the coefficients of a "minimal" characteristic set of the corresponding differential ideal. <br>We expect that some of these facts may be known to the experts in the area, but we are not aware of any articles in which these facts are stated precisely and rigorously proved.      
### 36.Optimized Amplify-and-Forward Relaying for Hierarchical Over-the-Air Computation  [ :arrow_down: ](https://arxiv.org/pdf/2007.14730.pdf)
>  Over-the-air computation (AirComp) is an emerging wireless technique with wide applications (e.g., in distributed edge learning), which can swiftly compute functions of distributed data from different wireless devices (WDs) by exploiting the superposition property of wireless channels. Different from prior works focusing on the AirComp over one single cell in a small area, this paper considers a new hierarchical architecture to enable AirComp in a large area, in which a set of intermediate relays are exploited to help the fusion center to aggregate data from massive WDs for functional computation. In particular, we present a two-phase amplify-and-forward (AF) relaying design for hierarchical AirComp. In the first phase, the WDs simultaneously send their data to the relays, while in the second phase, the relays amplify the received signals and concurrently forward them to the fusion center for aggregation. Under this setup, we minimize the computation distortion measured by the mean squared error (MSE), by jointly optimizing the transmit coefficients at the WDs and relays and the de-noising factor at the fusion center, subject to their individual transmit power constraints. For the highly non-convex MSE minimization problem, we develop an alternating-optimization-based algorithm to obtain a high-quality solution. The optimized solution shows that for each WD, the phase of its transmit coefficient is opposite to that of the composite channel from the WD itself to the relays to the fusion center, such that they can be aligned at the fusion center, and its transmit power follows a regularized composite-channel-inversion structure to strike a balance between minimizing the signal misalignment error and the noise-induced error.      
### 37.Sample Efficient Interactive End-to-End Deep Learning for Self-Driving Cars with Selective Multi-Class Safe Dataset Aggregation  [ :arrow_down: ](https://arxiv.org/pdf/2007.14671.pdf)
>  The objective of this paper is to develop a sample efficient end-to-end deep learning method for self-driving cars, where we attempt to increase the value of the information extracted from samples, through careful analysis obtained from each call to expert driverÅ policy. End-to-end imitation learning is a popular method for computing self-driving car policies. The standard approach relies on collecting pairs of inputs (camera images) and outputs (steering angle, etc.) from an expert policy and fitting a deep neural network to this data to learn the driving policy. Although this approach had some successful demonstrations in the past, learning a good policy might require a lot of samples from the expert driver, which might be resource-consuming. In this work, we develop a novel framework based on the Safe Dateset Aggregation (safe DAgger) approach, where the current learned policy is automatically segmented into different trajectory classes, and the algorithm identifies trajectory segments or classes with the weak performance at each step. Once the trajectory segments with weak performance identified, the sampling algorithm focuses on calling the expert policy only on these segments, which improves the convergence rate. The presented simulation results show that the proposed approach can yield significantly better performance compared to the standard Safe DAgger algorithm while using the same amount of samples from the expert.      
### 38.Modular Transfer Learning with Transition Mismatch Compensation for Excessive Disturbance Rejection  [ :arrow_down: ](https://arxiv.org/pdf/2007.14646.pdf)
>  Underwater robots in shallow waters usually suffer from strong wave forces, which may frequently exceed robot's control constraints. Learning-based controllers are suitable for disturbance rejection control, but the excessive disturbances heavily affect the state transition in Markov Decision Process (MDP) or Partially Observable Markov Decision Process (POMDP). Also, pure learning procedures on targeted system may encounter damaging exploratory actions or unpredictable system variations, and training exclusively on a prior model usually cannot address model mismatch from the targeted system. In this paper, we propose a transfer learning framework that adapts a control policy for excessive disturbance rejection of an underwater robot under dynamics model mismatch. A modular network of learning policies is applied, composed of a Generalized Control Policy (GCP) and an Online Disturbance Identification Model (ODI). GCP is first trained over a wide array of disturbance waveforms. ODI then learns to use past states and actions of the system to predict the disturbance waveforms which are provided as input to GCP (along with the system state). A transfer reinforcement learning algorithm using Transition Mismatch Compensation (TMC) is developed based on the modular architecture, that learns an additional compensatory policy through minimizing mismatch of transitions predicted by the two dynamics models of the source and target tasks. We demonstrated on a pose regulation task in simulation that TMC is able to successfully reject the disturbances and stabilize the robot under an empirical model of the robot system, meanwhile improve sample efficiency.      
### 39.Interpolatory projection technique for Riccati-based feedback stabilization of index-1 descriptor systems  [ :arrow_down: ](https://arxiv.org/pdf/2007.14614.pdf)
>  The aim of the work is to stabilize the unstable index-1 descriptor systems by Riccati-based feedback stabilization via a modified form interpolatory projection-based technique Iterative Rational Krylov Algorithm (IRKA). The basic IRKA is used to find Reduced Order Models (ROMs) for the stable systems conveniently but it is unsuitable for the unstable systems. In the proposed technique, we implement the initial feedback within the construction of the projectors of the IRKA approach. The Riccati solution is estimated from the ROM achieved by IRKA and hence the low-rank feedback matrix is attained. The feedback matrix for the full model is retrieved from the low-rank feedback matrix by the reverse projecting process. Finally, the applicability and efficiency of the proposed method are validated by applying to unstable index-1 descriptor systems. The simulation is done numerically using MATLAB and the achieved results are discussed in both tabular and graphical approaches.      
### 40.Improved Handling of Repeats and Jumps in Audio-Sheet Image Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2007.14580.pdf)
>  This paper studies the problem of automatically generating piano score following videos given an audio recording and raw sheet music images. Whereas previous works focus on synthetic sheet music where the data has been cleaned and preprocessed, we instead focus on developing a system that can cope with the messiness of raw, unprocessed sheet music PDFs from IMSLP. We investigate how well existing systems cope with real scanned sheet music, filler pages and unrelated pieces or movements, and discontinuities due to jumps and repeats. We find that a significant bottleneck in system performance is handling jumps and repeats correctly. In particular, we find that a previously proposed Jump DTW algorithm does not perform robustly when jump locations are unknown a priori. We propose a novel alignment algorithm called Hierarchical DTW that can handle jumps and repeats even when jump locations are not known. It first performs alignment at the feature level on each sheet music line, and then performs a second alignment at the segment level. By operating at the segment level, it is able to encode domain knowledge about how likely a particular jump is. Through carefully controlled experiments on unprocessed sheet music PDFs from IMSLP, we show that Hierarachical DTW significantly outperforms Jump DTW in handling various types of jumps.      
### 41.Camera-Based Piano Sheet Music Identification  [ :arrow_down: ](https://arxiv.org/pdf/2007.14579.pdf)
>  This paper presents a method for large-scale retrieval of piano sheet music images. Our work differs from previous studies on sheet music retrieval in two ways. First, we investigate the problem at a much larger scale than previous studies, using all solo piano sheet music images in the entire IMSLP dataset as a searchable database. Second, we use cell phone images of sheet music as our input queries, which lends itself to a practical, user-facing application. We show that a previously proposed fingerprinting method for sheet music retrieval is far too slow for a real-time application, and we diagnose its shortcomings. We propose a novel hashing scheme called dynamic n-gram fingerprinting that significantly reduces runtime while simultaneously boosting retrieval accuracy. In experiments on IMSLP data, our proposed method achieves a mean reciprocal rank of 0.85 and an average runtime of 0.98 seconds per query.      
### 42.Dreaming: Model-based Reinforcement Learning by Latent Imagination without Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2007.14535.pdf)
>  In the present paper, we propose a decoder-free extension of Dreamer, a leading model-based reinforcement learning (MBRL) method from pixels. Dreamer is a sample- and cost-efficient solution to robot learning, as it is used to train latent state-space models based on a variational autoencoder and to conduct policy optimization by latent trajectory imagination. However, this autoencoding based approach often causes object vanishing, in which the autoencoder fails to perceives key objects for solving control tasks, and thus significantly limiting Dreamer's potential. This work aims to relieve this Dreamer's bottleneck and enhance its performance by means of removing the decoder. For this purpose, we firstly derive a likelihood-free and InfoMax objective of contrastive learning from the evidence lower bound of Dreamer. Secondly, we incorporate two components, (i) independent linear dynamics and (ii) the random crop data augmentation, to the learning scheme so as to improve the training performance. In comparison to Dreamer and other recent model-free reinforcement learning methods, our newly devised Dreamer with InfoMax and without generative decoder (Dreaming) achieves the best scores on 5 difficult simulated robotics tasks, in which Dreamer suffers from object vanishing.      
### 43.An Iterative LQR Controller for Off-Road and On-Road Vehicles using a Neural Network Dynamics Model  [ :arrow_down: ](https://arxiv.org/pdf/2007.14492.pdf)
>  In this work we evaluate Iterative Linear Quadratic Regulator(ILQR) for trajectory tracking of two different kinds of wheeled mobile robots namely Warthog (Fig. 1), an off-road holonomic robot with skid-steering and Polaris GEM e6 [1], a non-holonomic six seater vehicle (Fig. 2). We use multilayer neural network to learn the discrete dynamic model of these robots which is used in ILQR controller to compute the control law. We use model predictive control (MPC) to deal with model imperfections and perform extensive experiments to evaluate the performance of the controller on human driven reference trajectories with vehicle speeds of 3m/s- 4m/s for warthog and 7m/s-10m/s for the Polaris GEM      
### 44.Color-complexity enabled exhaustive color-dots identification and spatial patterns testing in images  [ :arrow_down: ](https://arxiv.org/pdf/2007.14485.pdf)
>  Targeted color-dots with varying shapes and sizes in images are first exhaustively identified, and then their multiscale 2D geometric patterns are extracted for testing spatial uniformness in a progressive fashion. Based on color theory in physics, we develop a new color-identification algorithm relying on highly associative relations among the three color-coordinates: RGB or HSV. Such high associations critically imply low color-complexity of a color image, and renders potentials of exhaustive identification of targeted color-dots of all shapes and sizes. Via heterogeneous shaded regions and lighting conditions, our algorithm is shown being robust, practical and efficient comparing with the popular Contour and OpenCV approaches. Upon all identified color-pixels, we form color-dots as individually connected networks with shapes and sizes. We construct minimum spanning trees (MST) as spatial geometries of dot-collectives of various size-scales. Given a size-scale, the distribution of distances between immediate neighbors in the observed MST is extracted, so do many simulated MSTs under the spatial uniformness assumption. We devise a new algorithm for testing 2D spatial uniformness based on a Hierarchical clustering tree upon all involving MSTs. Our developments are illustrated on images obtained by mimicking chemical spraying via drone in Precision Agriculture.      
### 45.Quickest Detection of Moving Anomalies in Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2007.14475.pdf)
>  The problem of sequentially detecting a moving anomaly which affects different parts of a sensor network with time is studied. Each network sensor is characterized by a non-anomalous and anomalous distribution, governing the generation of sensor data. Initially, the observations of each sensor are generated according to the corresponding non-anomalous distribution. After some unknown but deterministic time instant, a moving anomaly emerges, affecting different sets of sensors as time progresses. As a result, the observations of the affected sensors are generated according to the corresponding anomalous distribution. Our goal is to design a stopping procedure to detect the emergence of the anomaly as quickly as possible, subject to constraints on the frequency of false alarms. The problem is studied in a quickest change detection framework where it is assumed that the evolution of the anomaly is unknown but deterministic. To this end, we propose a modification of Lorden's worst average detection delay metric to account for the trajectory of the anomaly that maximizes the detection delay of a candidate detection procedure. We establish that a Cumulative Sum-type test solves the resulting sequential detection problem exactly when the sensors are homogeneous. For the case of heterogeneous sensors, the proposed detection scheme can be modified to provide a first-order asymptotically optimal algorithm. We conclude by presenting numerical simulations to validate our theoretical analysis.      
### 46.Automated Intracranial Artery Labeling using a Graph Neural Network and Hierarchical Refinement  [ :arrow_down: ](https://arxiv.org/pdf/2007.14472.pdf)
>  Automatically labeling intracranial arteries (ICA) with their anatomical names is beneficial for feature extraction and detailed analysis of intracranial vascular structures. There are significant variations in the ICA due to natural and pathological causes, making it challenging for automated labeling. However, the existing public dataset for evaluation of anatomical labeling is limited. We construct a comprehensive dataset with 729 Magnetic Resonance Angiography scans and propose a Graph Neural Network (GNN) method to label arteries by classifying types of nodes and edges in an attributed relational graph. In addition, a hierarchical refinement framework is developed for further improving the GNN outputs to incorporate structural and relational knowledge about the ICA. Our method achieved a node labeling accuracy of 97.5%, and 63.8% of scans were correctly labeled for all Circle of Willis nodes, on a testing set of 105 scans with both healthy and diseased subjects. This is a significant improvement over available state-of-the-art methods. Automatic artery labeling is promising to minimize manual effort in characterizing the complicated ICA networks and provides valuable information for the identification of geometric risk factors of vascular disease. Our code and dataset are available at <a class="link-external link-https" href="https://github.com/clatfd/GNN-ARTLABEL" rel="external noopener nofollow">this https URL</a>.      
