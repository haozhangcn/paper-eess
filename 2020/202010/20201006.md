# ArXiv eess --Tue, 6 Oct 2020
### 1.Site Diversity in Downlink Optical Satellite Networks Through Ground Station Selection  [ :arrow_down: ](https://arxiv.org/pdf/2010.02176.pdf)
>  Recent advances have shown that satellite communication (SatCom) will be an important enabler for next generation terrestrial networks as it can provide numerous advantages, including global coverage, high speed connectivity, reliability, and instant deployment. An ideal alternative for radio frequency (RF) satellites is its free-space optical (FSO) counterpart. FSO or laser SatCom can mitigate the problems occurring in RF SatCom, while providing important advantages, including reduced mass, lower consumption, better throughput, and lower costs. Furthermore, laser SatCom is inherently resistant to jamming, interception, and interference. Owing to these benefits, this paper focuses on downlink laser SatCom, where the best ground station (GS) is selected among numerous candidates to provide reliable connectivity and maximum site diversity. To quantify the performance of the proposed scheme, we derive closed-form outage probability and ergodic capacity expressions for two different practical GS deployment scenarios. Furthermore, asymptotic analysis is conducted to obtain the overall site diversity gain, and aperture averaging is studied to illustrate the impact of aperture diameter on the overall performance. Finally, important design guidelines that can be useful in the design of practical laser SatComs are outlined.      
### 2.Medical Imaging and Computational Image Analysis in COVID-19 Diagnosis: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2010.02154.pdf)
>  Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. The disease presents with symptoms such as shortness of breath, fever, dry cough, and chronic fatigue, amongst others. Sometimes the symptoms of the disease increase so much they lead to the death of the patients. The disease may be asymptomatic in some patients in the early stages, which can lead to increased transmission of the disease to others. Many studies have tried to use medical imaging for early diagnosis of COVID-19. This study attempts to review papers on automatic methods for medical image analysis and diagnosis of COVID-19. For this purpose, PubMed, Google Scholar, arXiv and medRxiv were searched to find related studies by the end of April 2020, and the essential points of the collected studies were summarised. The contribution of this study is four-fold: 1) to use as a tutorial of the field for both clinicians and technologists, 2) to comprehensively review the characteristics of COVID-19 as presented in medical images, 3) to examine automated artificial intelligence-based approaches for COVID-19 diagnosis based on the accuracy and the method used, 4) to express the research limitations in this field and the methods used to overcome them. COVID-19 reveals signs in medical images can be used for early diagnosis of the disease even in asymptomatic patients. Using automated machine learning-based methods can diagnose the disease with high accuracy from medical images and reduce time, cost and error of diagnostic procedure. It is recommended to collect bulk imaging data from patients in the shortest possible time to improve the performance of COVID-19 automated diagnostic methods.      
### 3.FaultNet: A Deep Convolutional Neural Network for bearing fault classification  [ :arrow_down: ](https://arxiv.org/pdf/2010.02146.pdf)
>  The increased presence of advanced sensors on the production floors has led to collection of datasets that can provide significant insights into machine health. An important and reliable indicator of machine health, vibration signal data can provide us a greater understanding of different faults occurring in mechanical systems. In this work, we analyze vibration signal data of mechanical systems with bearings by combining different signal processing techniques and coupling them machine learning techniques to classify different types of bearing faults. We also highlight the importance of using of different signal processing methods and analyze their effect on bearing fault detection. Apart from the traditional machine learning algorithms we also propose a convolutional neural network FaultNet which can effectively determine the type of bearing fault with a high degree of accuracy. The distinguishing factor of this work is the idea of channels proposed to extract more information from the signal, we have stacked the mean and median channels to raw signal to extract more useful features to classify the signals with greater accuracy      
### 4.Quadratic approximate dynamic programming for scheduling water resources: a case study  [ :arrow_down: ](https://arxiv.org/pdf/2010.02122.pdf)
>  We address the problem of scheduling water resources in a power system via approximate dynamic <a class="link-external link-http" href="http://programming.To" rel="external noopener nofollow">this http URL</a> this goal, we model a finite horizon economic dispatch problemwith convex stage cost and affine dynamics, and consider aquadratic approximation of the value functions. Evaluating theachieved policy entails solving a quadratic program at each timestep, while value function fitting can be cast as a semidefiniteprogram. We test our proposed algorithm on a simplified versionof the Uruguayan power system, achieving a four percent costreduction with respect to the myopic policy      
### 5.Efficient and flexible approach to ptychography using an optimization framework based on automatic differentiation  [ :arrow_down: ](https://arxiv.org/pdf/2010.02074.pdf)
>  Ptychography is a lensless imaging method that allows for wavefront sensing and phase-sensitive microscopy from a set of diffraction patterns. Recently, it has been shown that the optimization task in ptychography can be achieved via automatic differentiation (AD). Here, we propose an open-access AD-based framework implemented with TensorFlow, a popular machine learning library. Using simulations, we show that our AD-based framework performs comparably to a state-of-the-art implementation of the momentum-accelerated ptychographic iterative engine (mPIE) in terms of reconstruction speed and quality. AD-based approaches provide great flexibility, as we demonstrate by setting the reconstruction distance as a trainable parameter. Lastly, we experimentally demonstrate that our framework faithfully reconstructs a biological specimen.      
### 6.Deep Representational Similarity Learning for analyzing neural signatures in task-based fMRI dataset  [ :arrow_down: ](https://arxiv.org/pdf/2010.02012.pdf)
>  Similarity analysis is one of the crucial steps in most fMRI studies. Representational Similarity Analysis (RSA) can measure similarities of neural signatures generated by different cognitive states. This paper develops Deep Representational Similarity Learning (DRSL), a deep extension of RSA that is appropriate for analyzing similarities between various cognitive tasks in fMRI datasets with a large number of subjects, and high-dimensionality -- such as whole-brain images. Unlike the previous methods, DRSL is not limited by a linear transformation or a restricted fixed nonlinear kernel function -- such as Gaussian kernel. DRSL utilizes a multi-layer neural network for mapping neural responses to linear space, where this network can implement a customized nonlinear transformation for each subject separately. Furthermore, utilizing a gradient-based optimization in DRSL can significantly reduce runtime of analysis on large datasets because it uses a batch of samples in each iteration rather than all neural responses to find an optimal solution. Empirical studies on multi-subject fMRI datasets with various tasks -- including visual stimuli, decision making, flavor, and working memory -- confirm that the proposed method achieves superior performance to other state-of-the-art RSA algorithms.      
### 7.Ensembles of Convolutional Neural Networks for pediatric pneumonia diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2010.02007.pdf)
>  Pneumonia is a lung infection that causes 15% of childhood mortality (under 5 years old) around the world. This pathology is mainly caused by viruses or bacteria. Most frequent associated viruses are respiratory syncytial virus (RSV), influenza virus and human parainfluenza virus (HPIV). Most frequent bacteria are: Streptococcus pneumoniae, Haemophilus influenzae, Streptococcus pyogenes and Staphylococcus aureus, and Mycoplasma pneumoniae. Our goal is to classify chest radiographs (X-ray) into two classes: consolidation, which corresponds to alveolar pneumonia, and non-consolidation, corresponding to non-alveolar pneumonia. X-rays imaging analysis is one of the most used methods for pneumonia diagnosis. These clinical images can be analyzed using machine learning methods such as convolutional neural networks (CNN), which learn to extract visual features critical for the classification. However, the usability of these systems is limited in medicine due to the lack of interpretability in the sense that these models do not generate a simple explanation for the predictions they make. To overcome the explainability and interpretability problems of these CNN "black boxes", we have developed an AI explainable approach (XAI) based on heatmaps, which allows highlighting those areas of the image that have been more relevant to generate the final classification. A new CNN model has been designed and trained, using pediatric X-rays (950 samples of children between one month and 16 years old), with two main goals, maximizing the Area Under the Curve (AUC) and the True Positive Rate (TPR) in the dataset. From our experiments, we have obtained some promising results, with a final model that reaches an AUC of 0.81 and a TPR of 0.67, however applying ensemble techniques the performance of the model improved to an AUC of 0.92 and a TPR of 0.76.      
### 8.Automatic Deep Learning System for COVID-19 Infection Quantification in chest CT  [ :arrow_down: ](https://arxiv.org/pdf/2010.01982.pdf)
>  Coronavirus Disease spread globally and infected millions of people quickly, causing high pressure on the health-system facilities. PCR screening is the adopted diagnostic testing method for COVID-19 detection. However, PCR is criticized due its low sensitivity ratios, also, it is time-consuming and manual complicated process. CT imaging proved its ability to detect the disease even for asymptotic patients, which make it a trustworthy alternative for PCR. In addition, the appearance of COVID-19 infections in CT slices, offers high potential to support in disease evolution monitoring using automated infection segmentation methods. However, COVID-19 infection areas include high variations in term of size, shape, contrast and intensity homogeneity, which impose a big challenge on segmentation process. To address these challenges, this paper proposed an automatic deep learning system for COVID-19 infection areas segmentation. The system include different steps to enhance and improve infection areas appearance in the CT slices so they can be learned efficiently using the deep network. The system start prepare the region of interest by segmenting the lung organ, which then undergo edge enhancing diffusion filtering (EED) to improve the infection areas contrast and intensity homogeneity. The proposed FCN is implemented using U-net architecture with modified residual block with concatenation skip connection. The block improves the learning of gradient values by forwarding the infection area features through the network. To demonstrate the generalization and effectiveness of the proposed system, it is trained and tested using many 2D CT slices extracted from diverse datasets from different sources. The proposed system is evaluated using different measures and achieved dice overlapping score of 0.961 and 0.780 for lung and infection areas segmentation, respectively.      
### 9.Automatic CAD-RADS Scoring Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.01963.pdf)
>  Coronary CT angiography (CCTA) has established its role as a non-invasive modality for the diagnosis of coronary artery disease (CAD). The CAD-Reporting and Data System (CAD-RADS) has been developed to standardize communication and aid in decision making based on CCTA findings. The CAD-RADS score is determined by manual assessment of all coronary vessels and the grading of lesions within the coronary artery tree. <br>We propose a bottom-up approach for fully-automated prediction of this score using deep-learning operating on a segment-wise representation of the coronary arteries. The method relies solely on a prior fully-automated centerline extraction and segment labeling and predicts the segment-wise stenosis degree and the overall calcification grade as auxiliary tasks in a multi-task learning setup. <br>We evaluate our approach on a data collection consisting of 2,867 patients. On the task of identifying patients with a CAD-RADS score indicating the need for further invasive investigation our approach reaches an area under curve (AUC) of 0.923 and an AUC of 0.914 for determining whether the patient suffers from CAD. This level of performance enables our approach to be used in a fully-automated screening setup or to assist diagnostic CCTA reading, especially due to its neural architecture design -- which allows comprehensive predictions.      
### 10.Improving Device Directedness Classification of Utterances with Semantic Lexical Features  [ :arrow_down: ](https://arxiv.org/pdf/2010.01949.pdf)
>  User interactions with personal assistants like Alexa, Google Home and Siri are typically initiated by a wake term or wakeword. Several personal assistants feature "follow-up" modes that allow users to make additional interactions without the need of a wakeword. For the system to only respond when appropriate, and to ignore speech not intended for it, utterances must be classified as device-directed or non-device-directed. State-of-the-art systems have largely used acoustic features for this task, while others have used only lexical features or have added LM-based lexical features. We propose a directedness classifier that combines semantic lexical features with a lightweight acoustic feature and show it is effective in classifying directedness. The mixed-domain lexical and acoustic feature model is able to achieve 14% relative reduction of EER over a state-of-the-art acoustic-only baseline model. Finally, we successfully apply transfer learning and semi-supervised learning to the model to improve accuracy even further.      
### 11.A Comparative Study of Existing and New Deep Learning Methods for Detecting Knee Injuries using the MRNet Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2010.01947.pdf)
>  This work presents a comparative study of existing and new techniques to detect knee injuries by leveraging Stanford's MRNet Dataset. All approaches are based on deep learning and we explore the comparative performances of transfer learning and a deep residual network trained from scratch. We also exploit some characteristics of Magnetic Resonance Imaging (MRI) data by, for example, using a fixed number of slices or 2D images from each of the axial, coronal and sagittal planes as well as combining the three planes into one multi-plane network. Overall we achieved a performance of 93.4% AUC on the validation data by using the more recent deep learning architectures and data augmentation strategies. More flexible architectures are also proposed that might help with the development and training of models that process MRIs. We found that transfer learning and a carefully tuned data augmentation strategy were the crucial factors in determining best performance.      
### 12.Unsupervised Region-based Anomaly Detection in Brain MRI with Adversarial Image Inpainting  [ :arrow_down: ](https://arxiv.org/pdf/2010.01942.pdf)
>  Medical segmentation is performed to determine the bounds of regions of interest (ROI) prior to surgery. By allowing the study of growth, structure, and behaviour of the ROI in the planning phase, critical information can be obtained, increasing the likelihood of a successful operation. Usually, segmentations are performed manually or via machine learning methods trained on manual annotations. In contrast, this paper proposes a fully automatic, unsupervised inpainting-based brain tumour segmentation system for T1-weighted MRI. First, a deep convolutional neural network (DCNN) is trained to reconstruct missing healthy brain regions. Then, upon application, anomalous regions are determined by identifying areas of highest reconstruction loss. Finally, superpixel segmentation is performed to segment those regions. We show the proposed system is able to segment various sized and abstract tumours and achieves a mean and standard deviation Dice score of 0.771 and 0.176, respectively.      
### 13.Block Chain and Internet of Nano-Things for Optimizing Chemical Sensing in Smart Farming  [ :arrow_down: ](https://arxiv.org/pdf/2010.01941.pdf)
>  The use of Internet of Things (IoT) with the Internet of Nano Things (IoNT) can further expand decision making systems (DMS) to improve reliability as it provides a new spectrum of more granular level data to make decisions. However, growing concerns such as data security, transparency and processing capability challenge their use in real-world applications. DMS integrated with Block Chain (BC) technology can contribute immensely to overcome such challenges. The use of IoNT and IoT along with BC for making DMS has not yet been investigated. This study proposes a BC-powered IoNT (BC-IoNT) system for sensing chemicals level in the context of farm management. This is a critical application for smart farming, which aims to improve sustainable farm practices through controlled delivery of chemicals. BC-IoNT system includes a novel machine learning model formed by using the Langmuir molecular binding model and the Bayesian theory, and is used as a smart contract for sensing the level of the chemicals. A credit model is used to quantify the traceability and credibility of farms to determine if they are compliant with the chemical standards. The accuracy of detecting the chemicals of the distributed BC-IoNT approach was &gt;90% and the centralized approach was &lt;80%. Also, the efficiency of sensing the level of chemicals depends on the sampling frequency and variability in chemical level among farms.      
### 14.Test-time Unsupervised Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01926.pdf)
>  Convolutional neural networks trained on publicly available medical imaging datasets (source domain) rarely generalise to different scanners or acquisition protocols (target domain). This motivates the active field of domain adaptation. While some approaches to the problem require labeled data from the target domain, others adopt an unsupervised approach to domain adaptation (UDA). Evaluating UDA methods consists of measuring the model's ability to generalise to unseen data in the target domain. In this work, we argue that this is not as useful as adapting to the test set directly. We therefore propose an evaluation framework where we perform test-time UDA on each subject separately. We show that models adapted to a specific target subject from the target domain outperform a domain adaptation method which has seen more data of the target domain but not this specific target subject. This result supports the thesis that unsupervised domain adaptation should be used at test-time, even if only using a single target-domain subject      
### 15.Resolving fast transients with Metal-Oxide gas sensors  [ :arrow_down: ](https://arxiv.org/pdf/2010.01903.pdf)
>  Electronic olfaction can help detect and localise harmful gases and pollutants, but the turbulence of natural environment presents a particular challenge: odor encounters are intermittent, and an effective electronic nose must therefore be able to resolve short odor pulses. The slow responses of the widely-used Metal-Oxide (MOX) gas sensors complicate the task. Here we combine high-resolution data acquisition with a processing method based on Kalman filtering and absolute-deadband sampling to extract fast transients. We find that our system can resolve the precise time of odor onset events, allowing direction estimation with a pair of MOX sensors in stereo-osmic configuration.      
### 16.Non-Linear Self-Interference Cancellation via Tensor Completion  [ :arrow_down: ](https://arxiv.org/pdf/2010.01868.pdf)
>  Non-linear self-interference (SI) cancellation constitutes a fundamental problem in full-duplex communications, which is typically tackled using either polynomial models or neural networks. In this work, we explore the applicability of a recently proposed method based on low-rank tensor completion, called canonical system identification (CSID), to non-linear SI cancellation. Our results show that CSID is very effective in modeling and cancelling the non-linear SI signal and can have lower computational complexity than existing methods, albeit at the cost of increased memory requirements.      
### 17.Direct Signal Separation Via Extraction of Local Frequencies with Adaptive Time-Varying Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2010.01866.pdf)
>  In nature, real-world phenomena that can be formulated as signals (or in terms of time series) are often affected by a number of factors and appear as multi-component modes. The natural approach to understand and process such phenomena is to decompose, or even better, to separate the multi-component signals to their basic building blocks (called sub-signals or time-series components, or fundamental modes). Recently the synchro-squeezing transform (SST) and its variants have been developed for nonstationary signal separation. More recently, a direct method of the time-frequency approach, called signal separation operation (SSO), was introduced for multi-component signal separation. While both SST and SSO are mathematically rigorous on the instantaneous frequency (IF) estimation, SSO avoids the second step of the two-step SST method in signal separation, which depends heavily on the accuracy of the estimated IFs. In the present paper, we solve the signal separation problem by constructing an adaptive signal separation operator (ASSO) for more effective separation of the blind-source multi-component signal, via introducing a time-varying parameter that adapts to local IFs. A recovery scheme is also proposed to extract the signal components one by one, and the time-varying parameter is updated for each component. The proposed method is suitable for engineering implementation, being capable of separating complicated signals into their sub-signals and reconstructing the signal trend directly. Numerical experiments on synthetic and real-world signals are presented to demonstrate our improvement over the previous attempts.      
### 18.JSSS: free Japanese speech corpus for summarization and simplification  [ :arrow_down: ](https://arxiv.org/pdf/2010.01793.pdf)
>  In this paper, we construct a new Japanese speech corpus for speech-based summarization and simplification, "JSSS" (pronounced "j-triple-s"). Given the success of reading-style speech synthesis from short-form sentences, we aim to design more difficult tasks for delivering information to humans. Our corpus contains voices recorded for two tasks that have a role in providing information under constraints: duration-constrained text-to-speech summarization and speaking-style simplification. It also contains utterances of long-form sentences as an optional task. This paper describes how we designed the corpus, which is available on our project page.      
### 19.Ford Highway Driving RTK Dataset: 30,000 km of North American Highways  [ :arrow_down: ](https://arxiv.org/pdf/2010.01774.pdf)
>  There is a growing need for vehicle positioning information to support Advanced Driver Assistance Systems (ADAS), Connectivity (V2X), and Autonomous Driving (AD) features. These range from a need for road determination ($&lt;$5 meters), lane determination ($&lt;$1.5 meters), and determining where the vehicle is within the lane ($&lt;$0.3 meters). This paper presents the Ford Highway Driving RTK (Ford-HDR) dataset. This dataset includes nearly 30,000 km of data collected primarily on North American highways during a driving campaign designed to validate driver assistance features in 2018. This includes data from a representative automotive production GNSS used primarily for turn-by-turn navigation as well as an Inertial Navigation System (INS) which couples two survey-grade GNSS receivers with a tactical grade Inertial Measurement Unit (IMU) to act as ground truth. The latter utilized networked Real-Time Kinematic (RTK) GNSS corrections delivered over a cellular modem in real-time. This dataset is being released into the public domain to spark further research in the community.      
### 20.D3Net: Densely connected multidilated DenseNet for music source separation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01733.pdf)
>  Music source separation involves a large input field to model a long-term dependence of an audio signal. Previous convolutional neural network (CNN) -based approaches address the large input field modeling using sequentially down- and up-sampling feature maps or dilated convolution. In this paper, we claim the importance of a rapid growth of a receptive field and a simultaneous modeling of multi-resolution data in a single convolution layer, and propose a novel CNN architecture called densely connected dilated DenseNet (D3Net). D3Net involves a novel multi-dilated convolution that has different dilation factors in a single layer to model different resolutions simultaneously. By combining the multi-dilated convolution with DenseNet architecture, D3Net avoids the aliasing problem that exists when we naively incorporate the dilated convolution in DenseNet. Experimental results on MUSDB18 dataset show that D3Net achieves state-of-the-art performance with an average signal to distortion ratio (SDR) of 6.01 dB.      
### 21.Deep Reinforcement Learning for Collaborative Edge Computing in Vehicular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.01722.pdf)
>  Mobile edge computing (MEC) is a promising technology to support mission-critical vehicular applications, such as intelligent path planning and safety applications. In this paper, a collaborative edge computing framework is developed to reduce the computing service latency and improve service reliability for vehicular networks. First, a task partition and scheduling algorithm (TPSA) is proposed to decide the workload allocation and schedule the execution order of the tasks offloaded to the edge servers given a computation offloading strategy. Second, an artificial intelligence (AI) based collaborative computing approach is developed to determine the task offloading, computing, and result delivery policy for vehicles. Specifically, the offloading and computing problem is formulated as a Markov decision process. A deep reinforcement learning technique, i.e., deep deterministic policy gradient, is adopted to find the optimal solution in a complex urban transportation network. By our approach, the service cost, which includes computing service latency and service failure penalty, can be minimized via the optimal workload assignment and server selection in collaborative computing. Simulation results show that the proposed AI-based collaborative computing approach can adapt to a highly dynamic environment with outstanding performance.      
### 22.Motion Correction of 3D Dynamic Contrast-Enhanced Ultrasound Imaging without Anatomical Bmode Images  [ :arrow_down: ](https://arxiv.org/pdf/2010.01721.pdf)
>  In conventional 2D DCE-US, motion correction algorithms take advantage of accompanying side-by-side anatomical Bmode images that contain time-stable features. However, current commercial models of 3D DCE-US do not provide side-by-side Bmode images, which makes motion correction challenging. This work introduces a novel motion correction (MC) algorithm for 3D DCE-US and assesses its efficacy when handling clinical data sets. In brief, the algorithm uses a pyramidal approach whereby short temporal windows consisting of 3-6 consecutive frames are created to perform local registrations, which are then registered to a master reference derived from a weighted average of all frames. We evaluated the algorithm in 8 patients with metastatic lesions in the liver using the Philips X6-1 matrix transducer at a frame rate of 1-3 Hz. We assessed improvements in original vs. motion corrected 3D DCE-US cine using: i) frame-to-frame volumetric overlap of segmented lesions, ii) normalized correlation coefficient (NCC) between frames (similarity analysis), and iii) sum of squared errors (SSE), root-mean-squared error (RMSE), and r-squared (R2) quality-of-fit from fitted time-intensity curves (TIC) extracted from a segmented lesion. Overall, results demonstrate significant decreases in 3D DCE-US motion after applying the proposed algorithm. We noted significant improvements in frame-to-frame lesion overlap across all patients, from 68% without correction to 83% with motion correction (p = 0.023). Frame-to-frame similarity as assessed by NCC also significantly improved on two different sets of time points from 0.694 (original cine) to 0.862 (corresponding MC cine) and 0.723 to 0.886. TIC analysis displayed a significant decrease in RMSE (p = 0.018) and a significant increase in R2 goodness-of-fit (p = 0.029) for the patient cohort.      
### 23.Chance-Constrained Controller State and Reference Governor  [ :arrow_down: ](https://arxiv.org/pdf/2010.01710.pdf)
>  The controller state and reference governor (CSRG) is an add-on scheme for nominal closed-loop systems with dynamic controllers which supervises the controller internal state and the reference input to the closed-loop system to enforce pointwise-in-time constraints. By admitting both controller state and reference modifications, the CSRG can achieve an enlarged constrained domain of attraction compared to conventional reference governor schemes where only reference modification is permitted. This paper studies the CSRG for systems subject to stochastic disturbances and chance constraints. We describe the CSRG algorithm in such a stochastic setting and analyze its theoretical properties, including chance-constraint enforcement, finite-time reference convergence, and closed-loop stability. We also present examples illustrating the application of CSRG to constrained aircraft flight control.      
### 24.Spatial Damage Characterization in Self-Sensing Materials via Neural Network-Aided Electrical Impedance Tomography: A Computational Study  [ :arrow_down: ](https://arxiv.org/pdf/2010.01674.pdf)
>  Continuous structural health monitoring (SHM) and integrated nondestructive evaluation (NDE) are important for ensuring the safe operation of high-risk engineering structures. Recently, piezoresistive nanocomposite materials have received much attention for SHM and NDE. These materials are self-sensing because their electrical conductivity changes in response to deformation and damage. Combined with electrical impedance tomography (EIT), it is possible to map deleterious effects. However, EIT suffers from important limitations -- it is computationally expensive, provides indistinct information on damage shape, and can miss multiple damages if they are close together. In this article we apply a novel neural network approach to quantify damage metrics such as size, number, and location from EIT data. This network is trained using a simulation routine calibrated to experimental data for a piezoresistive carbon nanofiber-modified epoxy. Our results show that the network can predict the number of damages with 99.2% accuracy, quantify damage size with respect to the averaged radius at an average of 2.46% error, and quantify damage position with respect to the domain length at an average of 0.89% error. These results are an important first step in translating the combination of self-sensing materials and EIT to real-world SHM and NDE.      
### 25.Surface Agnostic Metrics for Cortical Volume Segmentation and Regression  [ :arrow_down: ](https://arxiv.org/pdf/2010.01669.pdf)
>  The cerebral cortex performs higher-order brain functions and is thus implicated in a range of cognitive disorders. Current analysis of cortical variation is typically performed by fitting surface mesh models to inner and outer cortical boundaries and investigating metrics such as surface area and cortical curvature or thickness. These, however, take a long time to run, and are sensitive to motion and image and surface resolution, which can prohibit their use in clinical settings. In this paper, we instead propose a machine learning solution, training a novel architecture to predict cortical thickness and curvature metrics from T2 MRI images, while additionally returning metrics of prediction uncertainty. Our proposed model is tested on a clinical cohort (Down Syndrome) for which surface-based modelling often fails. Results suggest that deep convolutional neural networks are a viable option to predict cortical metrics across a range of brain development stages and pathologies.      
### 26.KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01663.pdf)
>  Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these "traditional" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely. This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes the U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project our input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation- KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities like ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. The proposed method achieves a better performance as compared to all the recent methods with an additional benefit of fewer parameters and faster convergence. Additionally, we also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. The implementation of KiU-Net can be found here: <a class="link-external link-https" href="https://github.com/jeya-maria-jose/KiU-Net-pytorch" rel="external noopener nofollow">this https URL</a>      
### 27.AFN: Attentional Feedback Network based 3D Terrain Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2010.01626.pdf)
>  Terrain, representing features of an earth surface, plays a crucial role in many applications such as simulations, route planning, analysis of surface dynamics, computer graphics-based games, entertainment, films, to name a few. With recent advancements in digital technology, these applications demand the presence of high-resolution details in the terrain. In this paper, we propose a novel fully convolutional neural network-based super-resolution architecture to increase the resolution of low-resolution Digital Elevation Model (LRDEM) with the help of information extracted from the corresponding aerial image as a complementary modality. We perform the super-resolution of LRDEM using an attention-based feedback mechanism named 'Attentional Feedback Network' (AFN), which selectively fuses the information from LRDEM and aerial image to enhance and infuse the high-frequency features and to produce the terrain realistically. We compare the proposed architecture with existing state-of-the-art DEM super-resolution methods and show that the proposed architecture outperforms enhancing the resolution of input LRDEM accurately and in a realistic manner.      
### 28.AIFNet: Automatic Vascular Function Estimation for Perfusion Analysis Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.01617.pdf)
>  Perfusion imaging is crucial in acute ischemic stroke for quantifying the salvageable penumbra and irreversibly damaged core lesions. As such, it helps clinicians to decide on the optimal reperfusion treatment. In perfusion CT imaging, deconvolution methods are used to obtain clinically interpretable perfusion parameters that allow identifying brain tissue abnormalities. Deconvolution methods require the selection of two reference vascular functions as inputs to the model: the arterial input function (AIF) and the venous output function, with the AIF as the most critical model input. When manually performed, the vascular function selection is time demanding, suffers from poor reproducibility and is subject to the professionals' experience. This leads to potentially unreliable quantification of the penumbra and core lesions and, hence, might harm the treatment decision process. In this work we automatize the perfusion analysis with AIFNet, a fully automatic and end-to-end trainable deep learning approach for estimating the vascular functions. Unlike previous methods using clustering or segmentation techniques to select vascular voxels, AIFNet is directly optimized at the vascular function estimation, which allows to better recognise the time-curve profiles. Validation on the public ISLES18 stroke database shows that AIFNet reaches inter-rater performance for the vascular function estimation and, subsequently, for the parameter maps and core lesion quantification obtained through deconvolution. We conclude that AIFNet has potential for clinical transfer and could be incorporated in perfusion deconvolution software.      
### 29.Real-Time Event Identification Using Deep Graph Learning and PMU Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.01616.pdf)
>  Phasor measurement units (PMUs) are being widely installed on power transmission systems, which provides a unique opportunity to enhance wide-area situational awareness. One key application is to utilize PMU data for real-time event identification. However, taking full advantage of all PMU data is still an open problem. This paper proposes a novel event identification method using multiple PMU measurements and deep graph learning techniques. Unlike previous models that rely on single PMU and ignore the interactive relationships between different PMUs or use multiple PMUs but determine the functional connectivity manually, our method performs data-driven interactive graph inference. Meanwhile, to ensure the optimality of the graph learning procedure, our method learns the interactive graph jointly with the event identification model. Moreover, instead of generating a single statistical graph to represent pair-wise relationships among PMUs during different events, our approach produces different event identification-specific graphs for different power system events, which handles the uncertainty of event location. To test the proposed data-driven approach, a large real dataset from tens of PMU sources and the corresponding event logs have been utilized in this work. The numerical results validate that our method has higher identification accuracy compared to the previous methods.      
### 30.Channel Prediction for mmWave Ground-to-Air Propagation under Blockage  [ :arrow_down: ](https://arxiv.org/pdf/2010.01614.pdf)
>  Ground-to-air (GA) communication using unmanned aerial vehicles (UAVs) has gained popularity in recent years and is expected to be part of 5G networks and beyond. However, the GA links are susceptible to frequent blockages at millimeter wave (mmWave) frequencies. During a link blockage, the channel information cannot be obtained reliably. In this work, we provide a novel method of channel prediction during the GA link blockage at 28 GHz. In our approach, the multipath components (MPCs) along the UAV flight trajectory are arranged into independent path bins based on the minimum Euclidean distance among the channel parameters of the MPCs. After the arrangement, the channel parameters of the MPCs in individual path bins are forecasted during the blockage. An autoregressive model is used for forecasting. The results obtained from ray tracing simulations indicate a close match between the actual and the predicted mmWave channel.      
### 31.Towards Cross-modality Medical Image Segmentation with Online Mutual Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01532.pdf)
>  The success of deep convolutional neural networks is partially attributed to the massive amount of annotated training data. However, in practice, medical data annotations are usually expensive and time-consuming to be obtained. Considering multi-modality data with the same anatomic structures are widely available in clinic routine, in this paper, we aim to exploit the prior knowledge (e.g., shape priors) learned from one modality (aka., assistant modality) to improve the segmentation performance on another modality (aka., target modality) to make up annotation scarcity. To alleviate the learning difficulties caused by modality-specific appearance discrepancy, we first present an Image Alignment Module (IAM) to narrow the appearance gap between assistant and target modality data.We then propose a novel Mutual Knowledge Distillation (MKD) scheme to thoroughly exploit the modality-shared knowledge to facilitate the target-modality segmentation. To be specific, we formulate our framework as an integration of two individual segmentors. Each segmentor not only explicitly extracts one modality knowledge from corresponding annotations, but also implicitly explores another modality knowledge from its counterpart in mutual-guided manner. The ensemble of two segmentors would further integrate the knowledge from both modalities and generate reliable segmentation results on target modality. Experimental results on the public multi-class cardiac segmentation data, i.e., MMWHS 2017, show that our method achieves large improvements on CT segmentation by utilizing additional MRI data and outperforms other state-of-the-art multi-modality learning methods.      
### 32.Shrinkage Strategies for Structure Selection and Identification of Piecewise Affine Models  [ :arrow_down: ](https://arxiv.org/pdf/2010.01520.pdf)
>  We propose two optimization-based heuristics for structure selection and identification of PieceWise Affine (PWA) models with exogenous inputs. The first method determines the number of affine sub-models assuming known model order of the sub-models, while the second approach estimates the model order for a given number of affine sub-models. Both approaches rely on the use of regularization-based shrinking strategies, that are exploited within a coordinate-descent algorithm. This allows us to estimate the structure of the PWA models along with its model parameters. Starting from an over-parameterized model, the key idea is to alternate between an identification step and structure refinement, based on the sparse estimates of the model parameters. The performance of the presented strategies is assessed over two benchmark examples.      
### 33.Energy transmission control for a Grid-connected modern power system Non-Linear loads with a Series Multi-Stage Transformer Voltage Reinjection with controlled converters  [ :arrow_down: ](https://arxiv.org/pdf/2010.01514.pdf)
>  The effective way of energy transmission plays a key factor in improving the overall transmission systems efficiency. Many methods are proposed to control the reactive power flow, voltage fluctuations and power factor improvement, The proposed converter topology gives a much significant improvement in transmission systems performance which includes multistage transformers control with the controlled converters along with the series active filters. The overall control strategy which involves the Multistage Voltage Re-Injection Transformer Controlled Converters (MSVRITCC) to reinject the voltages into the grid to compensate the voltages and remaining parameters and power flow control. The proposed topology improves the grid security, flexibility in reaching the desired load requirements with grid adaptability and reduces THD values into a significant values and made the control of power conditioning circuit flexible and easy to perform the voltage compensations in grid to load connected applications. The binary control is used to trigger the power converter circuits which made the controlling much simpler.      
### 34.Affine Linear Parameter-Varying Embedding of Nonlinear Models with Improved Accuracy and Minimal Overbounding  [ :arrow_down: ](https://arxiv.org/pdf/2010.01500.pdf)
>  In this paper, automated generation of linear parameter-varying (LPV) state-space models to embed the dynamical behavior of nonlinear systems is considered, focusing on the trade-off between scheduling complexity and model accuracy and on the minimization of the conservativeness of the resulting embedding. The LPV state-space model is synthesized with affine scheduling dependency, while the scheduling variables themselves are nonlinear functions of the state and input variables of the original system. The method allows to generate complete or approximative embedding of the nonlinear system model and also it can be used to minimize complexity of existing LPV embeddings. The capabilities of the method are demonstrated on simulation examples and also in an empirical case study where the first-principle motion model of a 3-DOF control moment gyroscope is converted by the proposed method to LPV model with low scheduling complexity. Using the resulting model, a gain-scheduled controller is designed and applied on the gyroscope, demonstrating the efficiency of the developed approach.      
### 35.A Separation Method for Multicomponent Nonstationary Signals with Crossover Instantaneous Frequencies  [ :arrow_down: ](https://arxiv.org/pdf/2010.01498.pdf)
>  In nature and engineering world, the acquired signals are usually affected by multiple complicated factors and appear as multicomponent nonstationary modes. In such and many other situations, it is necessary to separate these signals into a finite number of monocomponents to represent the intrinsic modes and underlying dynamics implicated in the source signals. In this paper, we consider the separation of a multicomponent signal which has crossing instantaneous frequencies (IFs), meaning that some of the components of the signal overlap in the time-frequency domain. We use a kernel function-based complex quadratic phase function to represent a multicomponent signal in the three-dimensional space of time, frequency and chirp rate, to be called the localized quadratic-phase Fourier transform (LQFT). We analyze the error bounds for IF estimation and component recovery with LQFT. In addition, we propose a matched-filter along certain specific time-frequency lines with respect to the chirp rate to make nonstationary signals be further separated and more concentrated in the three-dimensional space of LQFT. Based on the approximation of source signals with linear frequency modulation modes at any local time, we introduce an innovative signal reconstruction algorithm which is suitable for signals with crossing IFs. Moreover, this algorithm decreases component recovery errors when the IFs curves of different components are not crossover, but fast-varying and close to one and other. Numerical experiments on synthetic and real signals show our method is more accurate and consistent in signal separation than the empirical mode decomposition, synchrosqueezing transform, and other approaches.      
### 36.Improving Lesion Detection by exploring bias on Skin Lesion dataset  [ :arrow_down: ](https://arxiv.org/pdf/2010.01485.pdf)
>  All datasets contain some biases, often unintentional, due to how they were acquired and annotated. These biases distort machine-learning models' performance, creating spurious correlations that the models can unfairly exploit, or, contrarily destroying clear correlations that the models could learn. With the popularity of deep learning models, automated skin lesion analysis is starting to play an essential role in the early detection of Melanoma. The ISIC Archive is one of the most used skin lesion sources to benchmark deep learning-based tools. Bissoto et al. experimented with different bounding-box based masks and showed that deep learning models could classify skin lesion images without clinically meaningful information in the input data. Their findings seem confounding since the ablated regions (random rectangular boxes) are not significant. The shape of the lesion is a crucial factor in the clinical characterization of a skin lesion. In that context, we performed a set of experiments that generate shape-preserving masks instead of rectangular bounding-box based masks. A deep learning model trained on these shape-preserving masked images does not outperform models trained on images without clinically meaningful information. That strongly suggests spurious correlations guiding the models. We propose use of general adversarial network (GAN) to mitigate the underlying bias.      
### 37.Async-RED: A Provably Convergent Asynchronous Block Parallel Stochastic Method using Deep Denoising Priors  [ :arrow_down: ](https://arxiv.org/pdf/2010.01446.pdf)
>  Regularization by denoising (RED) is a recently developed framework for solving inverse problems by integrating advanced denoisers as image priors. Recent work has shown its state-of-the-art performance when combined with pre-trained deep denoisers. However, current RED algorithms are inadequate for parallel processing on multicore systems. We address this issue by proposing a new asynchronous RED (ASYNC-RED) algorithm that enables asynchronous parallel processing of data, making it significantly faster than its serial counterparts for large-scale inverse problems. The computational complexity of ASYNC-RED is further reduced by using a random subset of measurements at every iteration. We present complete theoretical analysis of the algorithm by establishing its convergence under explicit assumptions on the data-fidelity and the denoiser. We validate ASYNC-RED on image recovery using pre-trained deep denoisers as priors.      
### 38.autoTICI: Automatic Brain Tissue Reperfusion Scoring on 2D DSA Images of Acute Ischemic Stroke Patients  [ :arrow_down: ](https://arxiv.org/pdf/2010.01432.pdf)
>  The Thrombolysis in Cerebral Infarction (TICI) score is an important metric for reperfusion therapy assessment in acute ischemic stroke. It is commonly used as a technical outcome measure after endovascular treatment (EVT). Existing TICI scores are defined in coarse ordinal grades based on visual inspection, leading to inter- and intra-observer variation. In this work, we present autoTICI, an automatic and quantitative TICI scoring method. First, each digital subtraction angiography (DSA) sequence is separated into four phases (non-contrast, arterial, parenchymal and venous phase) using a multi-path convolutional neural network (CNN), which exploits spatio-temporal features. The network also incorporates sequence level label dependencies in the form of a state-transition matrix. Next, a minimum intensity map (MINIP) is computed using the motion corrected arterial and parenchymal frames. On the MINIP image, vessel, perfusion and background pixels are segmented. Finally, we quantify the autoTICI score as the ratio of reperfused pixels after EVT. On a routinely acquired multi-center dataset, the proposed autoTICI shows good correlation with the extended TICI (eTICI) reference with an average area under the curve (AUC) score of 0.81. The AUC score is 0.90 with respect to the dichotomized eTICI. In terms of clinical outcome prediction, we demonstrate that autoTICI is overall comparable to eTICI.      
### 39.Appliance identification using a histogram post-processing of 2D local binary patterns for smart grid applications  [ :arrow_down: ](https://arxiv.org/pdf/2010.01414.pdf)
>  Identifying domestic appliances in the smart grid leads to a better power usage management and further helps in detecting appliance-level abnormalities. An efficient identification can be achieved only if a robust feature extraction scheme is developed with a high ability to discriminate between different appliances on the smart grid. Accordingly, we propose in this paper a novel method to extract electrical power signatures after transforming the power signal to 2D space, which has more encoding possibilities. Following, an improved local binary patterns (LBP) is proposed that relies on improving the discriminative ability of conventional LBP using a post-processing stage. A binarized eigenvalue map (BEVM) is extracted from the 2D power matrix and then used to post-process the generated LBP representation. Next, two histograms are constructed, namely up and down histograms, and are then concatenated to form the global histogram. A comprehensive performance evaluation is performed on two different datasets, namely the GREEND and WITHED, in which power data were collected at 1 Hz and 44000 Hz sampling rates, respectively. The obtained results revealed the superiority of the proposed LBP-BEVM based system in terms of the identification performance versus other 2D descriptors and existing identification frameworks.      
### 40.Physics-based Reconstruction Methods for Magnetic Resonance Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.01403.pdf)
>  Conventional Magnetic Resonance Imaging (MRI) is hampered by long scan times and only qualitative image contrasts that prohibit a direct comparison between different systems. Instead of assuming a fixed Fourier relationship, model-based reconstructions explicitly model the physical laws that govern the MRI signal generation. By formulating image reconstruction as an inverse problem, quantitative maps of the underlying physical parameters can then be extracted directly from efficiently acquired k-space signals without intermediate image reconstruction -- addressing both shortcomings of conventional MRI at the same time. This review will discuss basic concepts of model-based reconstructions and report about our experience in developing several model-based methods over the last decade using selected examples.      
### 41.CardioXNet: A Novel Lightweight CRNN Framework for Classifying Cardiovascular Diseases from Phonocardiogram Recordings  [ :arrow_down: ](https://arxiv.org/pdf/2010.01392.pdf)
>  The alarmingly high mortality rate and increasing global prevalence of cardiovascular diseases (CVDs) signify the crucial need for early detection schemes. Phonocardiogram(PCG) signals has been historically applied in this domain owing to its simplicity and cost-effectiveness. However, insufficiency of expert physicians and human subjectivity affect the applicability of this technique, especially in the low-resource settings. For resolving this issue, in this paper, we introduce CardioXNet,a novel lightweight CRNN architecture for automatic detection of five classes of cardiac auscultation namely normal, aortic stenosis, mitral stenosis, mitral regurgitation and mitral valve prolapse using raw PCG signal. The process has been automated by the involvement of two learning phases namely, representation learning and sequence residual learning. The first phase mainly focuses on automated feature extraction and it has been implemented in a modular way with three parallel CNN pathways i.e., frequency feature extractor (FFE), pattern extractor (PE) and adaptive feature extractor (AFE). 1D-CNN based FFE and PE respectively learn the coarse and fine-grained features from the PCG while AFE explores the salient features from variable receptive fields involving 2D-CNN based squeezeexpansion. Thus, in the representation learning phase, the network extracts efficient time-invariant features and converges with great rapidity. In the sequential residual learning phase,because of the bidirectional-LSTMs and the skip connection, the network can proficiently extract temporal features. The obtained results demonstrate that the proposed end-to-end architecture yields outstanding performance in all the evaluation metrics compared to the previous state-of-the-art methods with up to 99.6% accuracy, 99.6% precision, 99.6% recall and 99.4% F1-score on an average while being computationally comparable.      
### 42.COVID-19 Classification of X-ray Images Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.01362.pdf)
>  In the midst of the coronavirus disease 2019 (COVID-19) outbreak, chest X-ray (CXR) imaging is playing an important role in the diagnosis and monitoring of patients with COVID-19. Machine learning solutions have been shown to be useful for X-ray analysis and classification in a range of medical contexts. The purpose of this study is to create and evaluate a machine learning model for diagnosis of COVID-19, and to provide a tool for searching for similar patients according to their X-ray scans. In this retrospective study, a classifier was built using a pre-trained deep learning model (ReNet50) and enhanced by data augmentation and lung segmentation to detect COVID-19 in frontal CXR images collected between January 2018 and July 2020 in four hospitals in Israel. A nearest-neighbors algorithm was implemented based on the network results that identifies the images most similar to a given image. The model was evaluated using accuracy, sensitivity, area under the curve (AUC) of receiver operating characteristic (ROC) curve and of the precision-recall (P-R) curve. The dataset sourced for this study includes 2362 CXRs, balanced for positive and negative COVID-19, from 1384 patients (63 +/- 18 years, 552 men). Our model achieved 89.7% (314/350) accuracy and 87.1% (156/179) sensitivity in classification of COVID-19 on a test dataset comprising 15% (350 of 2326) of the original data, with AUC of ROC 0.95 and AUC of the P-R curve 0.94. For each image we retrieve images with the most similar DNN-based image embeddings; these can be used to compare with previous cases.      
### 43.MPC Without the Computational Pain: The Benefits of SLS and Layering in Distributed Control  [ :arrow_down: ](https://arxiv.org/pdf/2010.01292.pdf)
>  The System Level Synthesis (SLS) approach facilitates distributed control of large cyberphysical networks in an easy-to-understand, computationally scalable way. We present a case study motivated by the power grid, with communication constraints, actuator saturation, disturbances, and changing setpoints. This simple but challenging case study necessitates the use of model predictive control (MPC); however, MPC incurs significant online computational cost and often scales poorly to large systems. We overcome these challenges by combining various SLS-based techniques, including SLS-based MPC, in a layered controller. This controller achieves performance that is within 3% of the centralized MPC performance, requires only 5% of the online computational resources of distributed MPC, and scales to systems of arbitrary size. For the unfamiliar reader, we also present a review of the SLS approach and its associated extensions in nonlinear control, MPC, adaptive control, and learning for control.      
### 44.A Control Strategy for Capacity Allocation of Hybrid Energy Storage System Based on Hierarchical Processing of Demand Power  [ :arrow_down: ](https://arxiv.org/pdf/2010.01277.pdf)
>  Pursuing optimal power distribution in hybrid energy storage systems has always been the goal of researchers. Here, HESS is a combination of lithium battery and supercapacitor; this combination has been proven to effectively compensate for some of the deficiencies of lithium batteries as an energy system for electric vehicles. For example, the energy storage system with only lithium batteries cannot provide high power in a short time to meet the high acceleration performance of electric vehicles, and the excessive discharge current will cause the temperature of the battery pack to be too high, which will cause safety problems for the car. This paper proposes an intelligent energy management strategy combining fuzzy controller and improved Savitzky-Golay filter for real-time control. The simulation results show that compared with only using the fuzzy controller, the maximum current of the battery proposed by the strategy is reduced by 2.43%, and the usable cycle life of the battery is increased by 5.01% during the test driving cycle. By simply comparing the latest supercapacitors on the market with the existing supercapacitors in the laboratory, it is possible to roughly predict the performance improvement that the future capacitors may bring in hybrid energy storage systems      
### 45.On the Value of Energy Storage in Generation Cost Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2010.01257.pdf)
>  This work seeks to quantify the benefits of using energy storage toward the reduction of the energy generation cost of a power system. A two-fold optimization framework is provided where the first optimization problem seeks to find the optimal storage schedule that minimizes operational costs. Since the operational cost depends on the storage capacity, a second optimization problem is then formulated with the aim of finding the optimal storage capacity to be deployed. Although, in general, these problems are difficult to solve, we provide a lower bound on the cost savings for a parametrized family of demand profiles. The optimization framework is numerically illustrated using real-world demand data from ISO New England. Numerical results show that energy storage can reduce energy generation costs by at least 2.5 %.      
### 46.Attractor Selection in Nonlinear Energy Harvesting Using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.01255.pdf)
>  Recent research efforts demonstrate that the intentional use of nonlinearity enhances the capabilities of energy harvesting systems. One of the primary challenges that arise in nonlinear harvesters is that nonlinearities can often result in multiple attractors with both desirable and undesirable responses that may co-exist. This paper presents a nonlinear energy harvester which is based on translation-to-rotational magnetic transmission and exhibits coexisting attractors with different levels of electric power output. In addition, a control method using deep reinforcement learning was proposed to realize attractor switching between coexisting attractors with constrained actuation.      
### 47.Placement of UAV-Mounted Mobile Base Station through User Load-Feature K-means Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2010.01236.pdf)
>  Temporary high traffic requests in cellular networks is a challenging problem to address. Recent advances in Unmanned Aerial Vehicles applied to cover these types of traffics. UAV -Mounted Mobile Base Stations placement is a challenging problem to achieve high performance. Different approaches have been proposed; however, user required traffic is not considered in UAV placement. We propose a new feature to apply to K-means clustering to find the optimum clusters. User required traffic is defined as a new feature to assign users to the UAVs. Our simulation results show that UAVs could be placed closer to the high traffic users to achieve higher performance.      
### 48.Latent neural source recovery via transcoding of simultaneous EEG-fMRI  [ :arrow_down: ](https://arxiv.org/pdf/2010.02167.pdf)
>  Simultaneous EEG-fMRI is a multi-modal neuroimaging technique that provides complementary spatial and temporal resolution for inferring a latent source space of neural activity. In this paper we address this inference problem within the framework of transcoding -- mapping from a specific encoding (modality) to a decoding (the latent source space) and then encoding the latent source space to the other modality. Specifically, we develop a symmetric method consisting of a cyclic convolutional transcoder that transcodes EEG to fMRI and vice versa. Without any prior knowledge of either the hemodynamic response function or lead field matrix, the method exploits the temporal and spatial relationships between the modalities and latent source spaces to learn these mappings. We show, for real EEG-fMRI data, how well the modalities can be transcoded from one to another as well as the source spaces that are recovered, all on unseen data. In addition to enabling a new way to symmetrically infer a latent source space, the method can also be seen as low-cost computational neuroimaging -- i.e. generating an 'expensive' fMRI BOLD image from 'low cost' EEG data.      
### 49.Blockchain for Multi-Robot Collaboration to Combat COVID-19 and Future Pandemics  [ :arrow_down: ](https://arxiv.org/pdf/2010.02137.pdf)
>  This conceptual paper overviews how blockchain technology is involving the operation of multi-robot collaboration for combating COVID-19 and future pandemics. Robots are a promising technology for providing many tasks such as spraying, disinfection, cleaning, treating, detecting high body temperature/mask absence, and delivering goods and medical supplies experiencing an epidemic COVID-19. For combating COVID-19, many heterogeneous and homogenous robots are required to perform different tasks for supporting different purposes in the quarantine area. Controlling and decentralizing multi-robot play a vital role in combating COVID-19 by reducing human interaction, monitoring, delivering goods. Blockchain technology can manage multi-robot collaboration in a decentralized fashion, improve the interaction among them to exchange information, share representation, share goals, and trust. We highlight the challenges and provide the tactical solutions enabled by integrating blockchain and multi-robot collaboration to combat COVID-19 pandemic. The framework of our conceptual proposed can increase the intelligence, decentralization, and autonomous operations of connected multi-robot collaboration in the blockchain network. We overview blockchain potential benefits to defining a framework of multi-robot collaboration applications to combat COVID-19 epidemics such as monitoring and outdoor and hospital End to End (E2E) delivery systems. Furthermore, we discuss the challenges and opportunities of integrated blockchain, multi-robot collaboration, and the Internet of Things (IoT) for combating COVID-19 and future pandemics.      
### 50.Convexified Open-Loop Stochastic Optimal Control for Linear Non-Gaussian Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.02101.pdf)
>  We consider stochastic optimal control of linear dynamical systems with additive non-Gaussian disturbance. We propose a novel, sampling-free approach, based on Fourier transformations and convex optimization, to cast the stochastic optimal control problem as a difference-of-convex program. In contrast to existing moment based approaches, our approach invokes higher moments, resulting in less conservatism. We employ piecewise affine approximations and the well-known convex-concave procedure, to efficiently solve the resulting optimization problem via standard conic solvers. We demonstrate that the proposed approach is computationally faster than existing particle based and moment based approaches, without compromising probabilistic safety constraints.      
### 51.TrueImage: A Machine Learning Algorithm to Improve the Quality of Telehealth Photos  [ :arrow_down: ](https://arxiv.org/pdf/2010.02086.pdf)
>  Telehealth is an increasingly critical component of the health care ecosystem, especially due to the COVID-19 pandemic. Rapid adoption of telehealth has exposed limitations in the existing infrastructure. In this paper, we study and highlight photo quality as a major challenge in the telehealth workflow. We focus on teledermatology, where photo quality is particularly important; the framework proposed here can be generalized to other health domains. For telemedicine, dermatologists request that patients submit images of their lesions for assessment. However, these images are often of insufficient quality to make a clinical diagnosis since patients do not have experience taking clinical photos. A clinician has to manually triage poor quality images and request new images to be submitted, leading to wasted time for both the clinician and the patient. We propose an automated image assessment machine learning pipeline, TrueImage, to detect poor quality dermatology photos and to guide patients in taking better photos. Our experiments indicate that TrueImage can reject 50% of the sub-par quality images, while retaining 80% of good quality images patients send in, despite heterogeneity and limitations in the training data. These promising results suggest that our solution is feasible and can improve the quality of teledermatology care.      
### 52.Algorithms for Nonnegative Matrix Factorization with the Kullback-Leibler Divergence  [ :arrow_down: ](https://arxiv.org/pdf/2010.01935.pdf)
>  Nonnegative matrix factorization (NMF) is a standard linear dimensionality reduction technique for nonnegative data sets. In order to measure the discrepancy between the input data and the low-rank approximation, the Kullback-Leibler (KL) divergence is one of the most widely used objective function for NMF. It corresponds to the maximum likehood estimator when the underlying statistics of the observed data sample follows a Poisson distribution, and KL NMF is particularly meaningful for count data sets, such as documents or images. In this paper, we first collect important properties of the KL objective function that are essential to study the convergence of KL NMF algorithms. Second, together with reviewing existing algorithms for solving KL NMF, we propose three new algorithms that guarantee the non-increasingness of the objective function. We also provide a global convergence guarantee for one of our proposed algorithms. Finally, we conduct extensive numerical experiments to provide a comprehensive picture of the performances of the KL NMF algorithms.      
### 53.Neurally Augmented ALISTA  [ :arrow_down: ](https://arxiv.org/pdf/2010.01930.pdf)
>  It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets. In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.      
### 54.Off-the-grid data-driven optimization of sampling schemes in MRI  [ :arrow_down: ](https://arxiv.org/pdf/2010.01817.pdf)
>  We propose a novel learning based algorithm to generate efficient and physically plausible sampling patterns in MRI. This method has a few advantages compared to recent learning based approaches: i) it works off-the-grid and ii) allows to handle arbitrary physical constraints. These two features allow for much more versatility in the sampling patterns that can take advantage of all the degrees of freedom offered by an MRI scanner. The method consists in a high dimensional optimization of a cost function defined implicitly by an algorithm. We propose various numerical tools to address this numerical challenge.      
### 55.High-resolution Piano Transcription with Pedals by Regressing Onsets and Offsets Times  [ :arrow_down: ](https://arxiv.org/pdf/2010.01815.pdf)
>  Automatic music transcription (AMT) is the task of transcribing audio recordings into symbolic representations such as Musical Instrument Digital Interface (MIDI). Recently, neural networks based methods have been applied to AMT, and have achieved state-of-the-art result. However, most of previous AMT systems predict the presence or absence of notes in the frames of audio recordings. The transcription resolution of those systems are limited to the hop size time between adjacent frames. In addition, previous AMT systems are sensitive to the misaligned onsets and offsets labels of audio recordings. For high-resolution evaluation, previous works have not investigated AMT systems evaluated with different onsets and offsets tolerances. For piano transcription, there is a lack of research on building AMT systems with both note and pedal transcription. In this article, we propose a high-resolution AMT system trained by regressing precise times of onsets and offsets. In inference, we propose an algorithm to analytically calculate the precise onsets and offsets times of note and pedal events. We build both note and pedal transcription systems with our high-resolution AMT system. We show that our AMT system is robust to misaligned onsets and offsets labels compared to previous systems. Our proposed system achieves an onset F1 of 96.72% on the MAESTRO dataset, outperforming the onsets and frames system from Google of 94.80%. Our system achieves a pedal onset F1 score of 91.86%, and is the first benchmark result on the MAESTRO dataset. We release the source code of our work at <a class="link-external link-https" href="https://github.com/bytedance/piano" rel="external noopener nofollow">this https URL</a>\_transcription.      
### 56.Painting Outside as Inside: Edge Guided Image Outpainting via Bidirectional Rearrangement with Step-By-Step Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.01810.pdf)
>  Image outpainting is a very intriguing problem as the outside of a given image can be continuously filled by considering as the context of the image. This task has two main challenges. The first is to maintain the spatial consistency in contents of generated regions and the original input. The second is to generate a high-quality large image with a small amount of adjacent information. Conventional image outpainting methods generate inconsistent, blurry, and repeated pixels. To alleviate the difficulty of an outpainting problem, we propose a novel image outpainting method using bidirectional boundary region rearrangement. We rearrange the image to benefit from the image inpainting task by reflecting more directional information. The bidirectional boundary region rearrangement enables the generation of the missing region using bidirectional information similar to that of the image inpainting task, thereby generating the higher quality than the conventional methods using unidirectional information. Moreover, we use the edge map generator that considers images as original input with structural information and hallucinates the edges of unknown regions to generate the image. Our proposed method is compared with other state-of-the-art outpainting and inpainting methods both qualitatively and quantitatively. We further compared and evaluated them using BRISQUE, one of the No-Reference image quality assessment (IQA) metrics, to evaluate the naturalness of the output. The experimental results demonstrate that our method outperforms other methods and generates new images with 360panoramic characteristics.      
### 57.Understanding Catastrophic Overfitting in Single-step Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2010.01799.pdf)
>  Adversarial examples are perturbed inputs that are designed to deceive machine-learning classifiers by adding adversarial perturbations to the original data. Although fast adversarial training have demonstrated both robustness and efficiency, the problem of "catastrophic overfitting" has been observed. It is a phenomenon that, during single-step adversarial training, the robust accuracy against projected gradient descent (PGD) suddenly decreases to 0% after few epochs, whereas the robustness against fast gradient sign method (FGSM) increases to 100%. In this paper, we address three main topics. (i) We demonstrate that catastrophic overfitting occurs in single-step adversarial training because it trains adversarial images with maximum perturbation only, not all adversarial examples in the adversarial direction, which leads to a distorted decision boundary and a highly curved loss surface. (ii) We experimentally prove this phenomenon by proposing a simple method using checkpoints. This method not only prevents catastrophic overfitting, but also overrides the belief that single-step adversarial training is hard to prevent multi-step attacks. (iii) We compare the performance of the proposed method to that obtained in recent works and demonstrate that it provides sufficient robustness to different attacks even after hundreds of training epochs in less time. All code for reproducing the experiments in this paper are at <a class="link-external link-https" href="https://github.com/Harry24k/catastrophic-overfitting" rel="external noopener nofollow">this https URL</a>.      
### 58.Resonant Energy Recycling SRAM Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2010.01767.pdf)
>  Although we may be at the end of Moore's law, lowering chip power consumption is still the primary driving force for the designers. To enable low-power operation, we propose a resonant energy recovery static random access memory (SRAM). We propose the first series resonance scheme to reduce the dynamic power consumption of the SRAM operation. Besides, we identified the requirement of supply boosting of the write buffers for proper resonant operation. We evaluated the resonant 144KB SRAM cache through SPICE and test chip using a commercial 28nm CMOS technology. The experimental results show that the resonant SRAM can save up to 30% dynamic power at 1GHz operating frequency compared to the state-of-the-art design.      
### 59.A Convex Approach to Data-driven Optimal Control via Perron-Frobenius and Koopman Operators  [ :arrow_down: ](https://arxiv.org/pdf/2010.01742.pdf)
>  The paper is about the data-driven computation of optimal control for a class of control affine deterministic nonlinear system. We assume that the control dynamical system model is not available, and the only information about the system dynamics is available in the form of time-series data. We provide a convex formulation for the optimal control problem of the nonlinear system. The convex formulation relies on the duality result in the stability theory of a dynamical system involving density function and Perron-Frobenius operator. The optimal control problem is formulated as an infinite-dimensional convex optimization program. The finite-dimensional approximation of the optimization problem relies on the recent advances made in the data-driven computation of the Koopman operator, which is dual to the Perron-Frobenius operator. Simulation results are presented to demonstrate the application of the developed framework.      
### 60.Lipschitz Bounded Equilibrium Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.01732.pdf)
>  This paper introduces new parameterizations of equilibrium neural networks, i.e. networks defined by implicit equations. This model class includes standard multilayer and residual networks as special cases. The new parameterization admits a Lipschitz bound during training via unconstrained optimization: no projections or barrier functions are required. Lipschitz bounds are a common proxy for robustness and appear in many generalization bounds. Furthermore, compared to previous works we show well-posedness (existence of solutions) under less restrictive conditions on the network weights and more natural assumptions on the activation functions: that they are monotone and slope restricted. These results are proved by establishing novel connections with convex optimization, operator splitting on non-Euclidean spaces, and contracting neural ODEs. In image classification experiments we show that the Lipschitz bounds are very accurate and improve robustness to adversarial attacks.      
### 61.Multi-microphone Complex Spectral Mapping for Utterance-wise and Continuous Speaker Separation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01703.pdf)
>  We propose multi-microphone complex spectral mapping, a simple way of applying deep learning for time-varying non-linear beamforming, for offline utterance-wise and block-online continuous speaker separation in reverberant conditions, aiming at both speaker separation and dereverberation. Assuming a fixed array geometry between training and testing, we train deep neural networks (DNN) to predict the real and imaginary (RI) components of target speech at a reference microphone from the RI components of multiple microphones. We then integrate multi-microphone complex spectral mapping with beamforming and post-filtering to further improve separation, and combine it with frame-level speaker counting for block-online continuous speaker separation (CSS). Although our system is trained on simulated room impulse responses (RIR) based on a fixed number of microphones arranged in a given geometry, it generalizes well to a real array with the same geometry. State-of-the-art separation performance is obtained on the simulated two-talker SMS-WSJ corpus and the real-recorded LibriCSS dataset.      
### 62.Collaborative Tracking and Capture of Aerial Object using UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2010.01588.pdf)
>  This work details the problem of aerial target capture using multiple UAVs. This problem is motivated from the challenge 1 of Mohammed Bin Zayed International Robotic Challenge 2020. The UAVs utilise visual feedback to autonomously detect target, approach it and capture without disturbing the vehicle which carries the target. Multi-UAV collaboration improves the efficiency of the system and increases the chance of capturing the ball robustly in short span of time. In this paper, the proposed architecture is validated through simulation in ROS-Gazebo environment and is further implemented on hardware.      
### 63.New Musical Interfaces and New Music-making Paradigms  [ :arrow_down: ](https://arxiv.org/pdf/2010.01579.pdf)
>  The conception and design of new musical interfaces is a multidisciplinary area that tightly relates technology and artistic creation. In this paper, the author first exposes some of the questions he has posed himself during more than a decade experience as a performer, composer, interface and software designer, and educator. Finally, he illustrates these topics with some examples of his work.      
### 64.Creating Contexts of Creativity: Musical Composition with Modular Components  [ :arrow_down: ](https://arxiv.org/pdf/2010.01578.pdf)
>  This paper describes a series of projects that explore the possibilities of musical expression through the combination of pre-composed, interlocking, modular components. In particular, this paper presents a modular soundtrack recently composed by the author for "Currents of Creativity," a permanent interactive video wall installation at the Pope John Paul II Cultural Center which is slated to open Easter 2001 in Washington, DC.      
### 65.The MATRIX: A Novel Controller for Musical Expression  [ :arrow_down: ](https://arxiv.org/pdf/2010.01577.pdf)
>  The MATRIX (Multipurpose Array of Tactile Rods for Interactive eXpression) is a new musical interface for amateurs and professionals alike. It gives users a 3- dimensional tangible interface to control music using their hands, and can be used in conjunction with a traditional musical instrument and a microphone, or as a stand-alone gestural input device. The surface of the MATRIX acts as a real-time interface that can manipulate the parameters of a synthesis engine or effect algorithm in response to a performer's expressive gestures. One example is to have the rods of the MATRIX control the individual grains of a granular synthesizer, thereby "sonically sculpting" the microstructure of a sound. In this way, the MATRIX provides an intuitive method of manip      
### 66.Body, Clothes, Water, and Toys: Media Towards Natural Music Expressions with Digital Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2010.01576.pdf)
>  In this paper, we introduce our research challenges for creating new musical instruments using everyday-life media with intimate interfaces, such as the self-body, clothes, water and stuffed toys. Various sensor technologies including image processing and general touch sensitive devices are employed to exploit these interaction media. The focus of our effort is to provide user-friendly and enjoyable experiences for new music and sound performances. Multimodality of musical instruments is explored in each attempt. The degree of controllability in the performance and the richness of expressions are also discussed for each installation.      
### 67.Tangible Music Interfaces Using Passive Magnetic Tags  [ :arrow_down: ](https://arxiv.org/pdf/2010.01575.pdf)
>  The technologies behind passive resonant magnetically coupled tags are introduced and their application as a musical controller is illustrated for solo or group performances, interactive installations, and music toys.      
### 68.The Accordiatron: A MIDI Controller For Interactive Music  [ :arrow_down: ](https://arxiv.org/pdf/2010.01574.pdf)
>  The Accordiatron is a new MIDI controller for real-time performance based on the paradigm of a conventional squeeze box or concertina. It translates the gestures of a performer to the standard communication protocol of MIDI, allowing for flexible mappings of performance data to sonic parameters. When used in conjunction with a realtime signal processing environment, the Accordiatron becomes an expressive, versatile musical instrument. A combination of sensory outputs providing both discrete and continuous data gives the subtle expressiveness and control necessary for interactive music.      
### 69.Resonant Processing of Instrumental Sound Controlled by Spatial Position  [ :arrow_down: ](https://arxiv.org/pdf/2010.01572.pdf)
>  We present an acoustic musical instrument played through a resonance model of another sound. The resonance model is controlled in real time as part of the composite instrument. Our implementation uses an electric violin, whose spatial position modifies filter parameters of the resonance model. Simplicial interpolation defines the mapping from spatial position to filter parameters. With some effort, pitch tracking can also control the filter parameters. The individual technologies -- motion tracking, pitch tracking, resonance models -- are easily adapted to other instruments.      
### 70.Input Devices for Musical Expression: Borrowing Tools from HCI  [ :arrow_down: ](https://arxiv.org/pdf/2010.01571.pdf)
>  This paper reviews the existing literature on input device evaluation and design in human-computer interaction (HCI) and discusses possible applications of this knowledge to the design and evaluation of new interfaces for musical expression. Specifically, a set of musical tasks is suggested to allow the evaluation of different existing controllers.      
### 71.Problems and Prospects for Intimate Musical Control of Computers  [ :arrow_down: ](https://arxiv.org/pdf/2010.01570.pdf)
>  In this paper we describe our efforts towards the development of live performance computer-based musical instrumentation. Our design criteria include initial ease of use coupled with a long term potential for virtuosity, minimal and low variance latency, and clear and simple strategies for programming the relationship between gesture and musical result. We present custom controllers and unique adaptations of standard gestural interfaces, a programmable connectivity processor, a communications protocol called Open Sound Control (OSC), and a variety of metaphors for musical control. We further describe applications of our technology to a variety of real musical performances and directions for future research.      
### 72.A Course on Controllers  [ :arrow_down: ](https://arxiv.org/pdf/2010.01569.pdf)
>  Over the last four years, we have developed a series of lectures, labs and project assignments aimed at introducing enough technology so that students from a mix of disciplines can design and build innovative interface devices.      
### 73.Optimal charging scheduling for dynamic dial-a-ride services using electric vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2010.01541.pdf)
>  Coordinating the charging scheduling of electric vehicles for dynamic dial-a-ride services is challenging considering charging queuing delays and stochastic customer demand. We propose a new two-stage solution approach to handle dynamic vehicle charging scheduling to minimize the costs of daily charging operations of the fleet. The approach comprises two components: daily vehicle charging scheduling and online vehicle charger assignment. A new battery replenishment model is proposed to obtain the vehicle charging schedules by minimizing the costs of vehicle daily charging operations while satisfying vehicle driving needs to serve customers. In the second stage, an online vehicle charger assignment model is developed to minimize the total vehicle idle time for charges by considering queuing delays at the level of chargers. We propose an efficient Lagrangian relaxation algorithm to solve the largescale vehicle-charger assignment problem with small optimality gaps. The approach is applied to a realistic dynamic dial a ride service case study in Luxembourg and compared with the nearest charging station charging policy and first come first served minimum charging delay policy under different charging infrastructure scenarios. Our computational results show that the approach can achieve significant savings for the operator in terms of charging waiting times (-74.9%), charging times (-38.6%), and charged energy costs (-27.4%). A sensitivity analysis is conducted to evaluate the impact of the different model parameters, showing the scalability and robustness of the approach in a stochastic environment.      
### 74.Spatial Frequency Bias in Convolutional Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.01473.pdf)
>  As the success of Generative Adversarial Networks (GANs) on natural images quickly propels them into various real-life applications across different domains, it becomes more and more important to clearly understand their limitations. Specifically, understanding GANs' capability across the full spectrum of spatial frequencies, i.e. beyond the low-frequency dominant spectrum of natural images, is critical for assessing the reliability of GAN generated data in any detail-sensitive application (e.g. denoising, filling and super-resolution in medical and satellite images). In this paper, we show that the ability of GANs to learn a distribution is significantly affected by the spatial frequency of the underlying carrier signal, that is, GANs have a bias against learning high spatial frequencies. Crucially, we show that this bias is not merely a result of the scarcity of high frequencies in natural images, rather, it is a systemic bias hindering the learning of high frequencies regardless of their prominence in a dataset. Furthermore, we explain why large-scale GANs' ability to generate fine details on natural images does not exclude them from the adverse effects of this bias. Finally, we propose a method for manipulating this bias with minimal computational overhead. This method can be used to explicitly direct computational resources towards any specific spatial frequency of interest in a dataset, thus extending the flexibility of GANs.      
### 75.Deep Reinforcement Learning for Delay-Oriented IoT Task Scheduling in Space-Air-Ground Integrated Network  [ :arrow_down: ](https://arxiv.org/pdf/2010.01471.pdf)
>  In this paper, we investigate a computing task scheduling problem in space-air-ground integrated network (SAGIN) for delay-oriented Internet of Things (IoT) services. In the considered scenario, an unmanned aerial vehicle (UAV) collects computing tasks from IoT devices and then makes online offloading decisions, in which the tasks can be processed at the UAV or offloaded to the nearby base station or the remote satellite. Our objective is to design a task scheduling policy that minimizes offloading and computing delay of all tasks given the UAV energy capacity constraint. To this end, we first formulate the online scheduling problem as an energy-constrained Markov decision process (MDP). Then, considering the task arrival dynamics, we develop a novel deep risk-sensitive reinforcement learning algorithm. Specifically, the algorithm evaluates the risk, which measures the energy consumption that exceeds the constraint, for each state and searches the optimal parameter weighing the minimization of delay and risk while learning the optimal policy. Extensive simulation results demonstrate that the proposed algorithm can reduce the task processing delay by up to 30% compared to probabilistic configuration methods while satisfying the UAV energy capacity constraint.      
### 76.3D Orientation Field Transform  [ :arrow_down: ](https://arxiv.org/pdf/2010.01453.pdf)
>  The two-dimensional (2D) orientation field transform has been proved to be effective at enhancing 2D contours and curves in images by means of top-down processing. It, however, has no counterpart in three-dimensional (3D) images due to the extremely complicated orientation in 3D compared to 2D. Practically and theoretically, the demand and interest in 3D can only be increasing. In this work, we modularise the concept and generalise it to 3D curves. Different modular combinations are found to enhance curves to different extents and with different sensitivity to the packing of the 3D curves. In principle, the proposed 3D orientation field transform can naturally tackle any dimensions. As a special case, it is also ideal for 2D images, owning simpler methodology compared to the previous 2D orientation field transform. The proposed method is demonstrated with several transmission electron microscopy tomograms ranging from 2D curve enhancement to, the more important and interesting, 3D ones.      
### 77.Uncertainty-Aware Multi-Modal Ensembling for Severity Prediction of Alzheimer's Dementia  [ :arrow_down: ](https://arxiv.org/pdf/2010.01440.pdf)
>  Reliability in Neural Networks (NNs) is crucial in safety-critical applications like healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of NNs in deployment. In this work, we propose an uncertainty-aware boosting technique for multi-modal ensembling to predict Alzheimer's Dementia Severity. The propagation of uncertainty across acoustic, cognitive, and linguistic features produces an ensemble system robust to heteroscedasticity in the data. Weighing the different modalities based on the uncertainty estimates, we experiment on the benchmark ADReSS dataset, a subject-independent and balanced dataset, to show that our method outperforms the state-of-the-art methods while also reducing the overall entropy of the system. This work aims to encourage fair and aware models. The source code is available at <a class="link-external link-https" href="https://github.com/wazeerzulfikar/alzheimers-dementia" rel="external noopener nofollow">this https URL</a>      
### 78.Practical Precoding via Asynchronous Stochastic Successive Convex Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01360.pdf)
>  We consider stochastic optimization of a smooth non-convex loss function with a convex non-smooth regularizer. In the online setting, where a single sample of the stochastic gradient of the loss is available at every iteration, the problem can be solved using the proximal stochastic gradient descent (SGD) algorithm and its variants. However in many problems, especially those arising in communications and signal processing, information beyond the stochastic gradient may be available thanks to the structure of the loss function. Such extra-gradient information is not used by SGD, but has been shown to be useful, for instance in the context of stochastic expectation-maximization, stochastic majorization-minimization, and stochastic successive convex approximation (SCA) approaches. By constructing a stochastic strongly convex surrogates of the loss function at every iteration, the stochastic SCA algorithms can exploit the structural properties of the loss function and achieve superior empirical performance as compared to the SGD. <br>In this work, we take a closer look at the stochastic SCA algorithm and develop its asynchronous variant which can be used for resource allocation in wireless networks. While the stochastic SCA algorithm is known to converge asymptotically, its iteration complexity has not been well-studied, and is the focus of the current work. The insights obtained from the non-asymptotic analysis allow us to develop a more practical asynchronous variant of the stochastic SCA algorithm which allows the use of surrogates calculated in earlier iterations. We characterize precise bound on the maximum delay the algorithm can tolerate, while still achieving the same convergence rate. We apply the algorithm to the problem of linear precoding in wireless sensor networks, where it can be implemented at low complexity but is shown to perform well in practice.      
### 79.Tracking Controller Design for Satellite Attitude Under Unknown Constant Disturbance Using Stable Embedding  [ :arrow_down: ](https://arxiv.org/pdf/2010.01290.pdf)
>  We propose a tracking control law for the fully actuated rigid body system in the presence of any unknown constant disturbance by employing quaternions with the stable embedding technique and Lyapunov stability theory. The stable embedding technique extends the attitude dynamics from the set of unit quaternions to the set of quaternions, which is a Euclidean space, such that the set of unit quaternions is an invariant set of the extended dynamics. Such a stable extension of the system dynamics to a Euclidean space allows us to employ well studied Lyapunov techniques in Euclidean spaces such as LaSalle-Yoshizawa's theorem. A robust tracking control law is proposed for the attitude dynamics subject to unknown constant disturbance and the convergence properties of the tracking control law is rigorously proven. It is demonstrated with the help of numerical simulations that the proposed control law has a remarkable performance even in some challenging situations.      
### 80.Optimal Control of a Soft CyberOctopus Arm  [ :arrow_down: ](https://arxiv.org/pdf/2010.01226.pdf)
>  In this paper, we use the optimal control methodology to control a flexible, elastic Cosserat rod. An inspiration comes from stereotypical movement patterns in octopus arms, which are observed in a variety of manipulation tasks, such as reaching or fetching. To help uncover the mechanisms underlying these observed behaviors, we outline an optimal control-based framework. A single octopus arm is modeled as a Hamiltonian control system, where the continuum mechanics of the arm is captured by the Cosserat rod theory, and internal, distributed muscle forces and couples are considered as controls. First order necessary optimality conditions are derived for an optimal control problem formulated for this infinite dimensional system. Solutions to this problem are obtained numerically by an iterative forward-backward algorithm. The state and adjoint equations are solved in a dynamic simulation environment, setting the stage for studying a broader class of optimal control problems. Trajectories that minimize control effort are demonstrated and qualitatively compared with observed behaviors.      
### 81.Contraction Theory for Dynamical Systems on Hilbert Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2010.01219.pdf)
>  Contraction theory for dynamical systems on Euclidean spaces is well-established. For contractive (resp. semi-contractive) systems, the distance (resp. semi-distance) between any two trajectories decreases exponentially fast. For partially contractive systems, each trajectory converges exponentially fast to an invariant subspace. <br>In this paper, we develop contraction theory on Hilbert spaces. First, for time-invariant systems we establish the existence of a unique globally exponentially stable equilibrium and provide a novel integral condition for contractivity. Second, we introduce the notions of partial and semi-contraction and we provide various sufficient conditions for time-varying and time-invariant systems. Finally, we apply the theory on a classic reaction-diffusion system.      
### 82.Compressing Images by Encoding Their Latent Representations with Relative Entropy Coding  [ :arrow_down: ](https://arxiv.org/pdf/2010.01185.pdf)
>  Variational Autoencoders (VAEs) have seen widespread use in learned image compression. They are used to learn expressive latent representations on which downstream compression methods can operate with high efficiency. Recently proposed 'bits-back' methods can indirectly encode the latent representation of images with codelength close to the relative entropy between the latent posterior and the prior. However, due to the underlying algorithm, these methods can only be used for lossless compression, and they only achieve their nominal efficiency when compressing multiple images simultaneously; they are inefficient for compressing single images. As an alternative, we propose a novel method, Relative Entropy Coding (REC), that can directly encode the latent representation with codelength close to the relative entropy for single images, supported by our empirical results obtained on the Cifar10, ImageNet32 and Kodak datasets. Moreover, unlike previous bits-back methods, REC is immediately applicable to lossy compression, where it is competitive with the state-of-the-art on the Kodak dataset.      
### 83.Adaptive Neural Layer for Globally Filtered Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.01177.pdf)
>  This study is motivated by typical images taken during ultrasonic examinations in the clinic. Their grainy appearance, low resolution, and poor contrast demand an eye of a very qualified expert to discern targets and to spot pathologies. Training a segmentation model on such data is frequently accompanied by excessive pre-processing and image adjustments, with an accumulation of the localization error emerging due to the digital post-filtering artifacts and due to the annotation uncertainty. Each patient case generally requires an individually tuned frequency filter to obtain optimal image contrast and to optimize the segmentation quality. Thus, we aspired to invent an adaptive global frequency-filtering neural layer to "learn" optimal frequency filter for each image together with the weights of the segmentation network itself. Specifically, our model receives the source image in the spatial domain, automatically selects the necessary frequencies from the frequency domain, and transmits the inverse-transform image to the convolutional neural network for concurrent segmentation. In our experiments, such "learnable" filters boosted typical U-Net segmentation performance by 10% and made the training of other popular models (DenseNet and ResNet) almost twice faster. In our experiments, this trait holds both for two public datasets with ultrasonic images (breast and nerves), and for natural images (Caltech birds).      
