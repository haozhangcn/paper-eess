# ArXiv eess --Wed, 28 Oct 2020
### 1.Battery-assisted Electric Vehicle Charging: Data Driven Performance Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2010.14455.pdf)
>  As the number of electric vehicles rapidly increases, their peak demand on the grid becomes one of the major challenges. A battery-assisted charging concept has emerged recently, which allows to accumulate energy during off-peak hours and in-between charging sessions to boost-charge the vehicle at a higher rate than available from the grid. While prior research focused on the design and implementation aspects of battery-assisted charging, its impact at large geographical scales remains largely unexplored. In this paper we analyse to which extent the battery-assisted charging can replace high-speed chargers using a dataset of over 3 million EV charging sessions in both domestic and public setting in the UK. We first develop a discrete-event EV charge model that takes into account battery capacity, grid supply capacity and power output among other parameters. We then run simulations to evaluate the battery-assisted charging performance in terms of delivered energy, charging time and parity with conventional high-speed chargers. The results indicate that in domestic settings battery-assisted charging provides 98% performance parity of high-speed chargers from a standard 3 kW grid connection with a single battery pack. For non-domestic settings, the battery-assisted chargers can provide 92% and 99% performance parity of high-speed chargers with 10 battery packs using 3kW and 7kW grid supply respectively.      
### 2.Heuristic assessment of the economic effects of pandemic control  [ :arrow_down: ](https://arxiv.org/pdf/2010.14452.pdf)
>  Data-driven risk networks describe many complex system dynamics arising in fields such as epidemiology and ecology. They lack explicit dynamics and have multiple sources of cost, both of which are beyond the current scope of traditional control theory. We construct the global risk network by combining the consensus of experts from the World Economic Forum with risk activation data to define its topology and interactions. Many of these risks, including extreme weather, pose significant economic costs when active. We introduce a method for converting network interaction data into continuous dynamics to which we apply optimal control. We contribute the first method for constructing and controlling risk network dynamics based on empirically collected data. We identify seven risks commonly used by governments to control COVID-19 spread and show that many alternative driver risk sets exist with potentially lower cost of control.      
### 3.Optimal model-based trajectory planning with static polygonal constraints  [ :arrow_down: ](https://arxiv.org/pdf/2010.14428.pdf)
>  The main contribution of this paper is a novel method for planning globally optimal trajectories for dynamical systems subject to polygonal constraints. The proposed method is a hybrid trajectory planning approach, which combines graph search, i.e. a discrete roadmap method, with convex optimization, i.e. a complete path method. Contrary to past approaches, which have focused on using simple obstacle approximations, or sub-optimal spatial discretizations, our approach is able to use the exact geometry of polygonal constraints in order to plan optimal trajectories. The performance and flexibility of the proposed method is evaluated via simulations by planning distance-optimal trajectories for a Dubins car model, as well as time-, distance- and energy-optimal trajectories for a marine vehicle.      
### 4.Blind Sound Source Localization based on Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.14420.pdf)
>  This paper presents SSLIDE, Sound Source Localization for Indoors using DEep learning, which applies deep neural networks (DNNs) with encoder-decoder structure to localize sound sources without any prior information about the source candidate locations or source properties. The spatial features of sound signals received by each microphone are extracted and represented as likelihood surfaces for the sound source locations in each point. Our DNN consists of an encoder network followed by two decoders. The encoder obtains a compressed representation of the input likelihoods. One decoder resolves the multipath caused by reverberation, and the other decoder estimates the source location. Experiments show that our method can outperform multiple signal classification (MUSIC), steered response power with phase transform (SRP-PHAT), sparse Bayesian learning (SBL), and a competing convolutional neural network (CNN) approach in the reverberant environment.      
### 5.A Data-Driven Sparse Polynomial Chaos Expansion Method to Assess Probabilistic Total Transfer Capability for Power Systems with Renewables  [ :arrow_down: ](https://arxiv.org/pdf/2010.14358.pdf)
>  The increasing uncertainty level caused by growing renewable energy sources (RES) and aging transmission networks poses a great challenge in the assessment of total transfer capability (TTC) and available transfer capability (ATC). In this paper, a novel data-driven sparse polynomial chaos expansion (DDSPCE) method is proposed for estimating the probabilistic characteristics (e.g., mean, variance, probability distribution) of probabilistic TTC (PTTC). Specifically, the proposed method, requiring no pre-assumed probabilistic distributions of random inputs, exploits data sets directly in estimating the PTTC. Besides, a sparse scheme is integrated to improve the computational efficiency. Numerical studies on the modified IEEE 118-bus system demonstrate that the proposed DDSPCE method can achieve accurate estimation for the probabilistic characteristics of PTTC with a high efficiency. Moreover, numerical results reveal the great significance of incorporating discrete random inputs in PTTC and ATC assessment, which nevertheless was not given sufficient attention.      
### 6.Acoustic echo cancellation with the dual-signal transformation LSTM network  [ :arrow_down: ](https://arxiv.org/pdf/2010.14337.pdf)
>  This paper applies the dual-signal transformation LSTM network (DTLN) to the task of real-time acoustic echo cancellation (AEC). The DTLN combines a short-time Fourier transformation and a learned feature representation in a stacked network approach, which enables robust information processing in the time-frequency and in the time domain, which also includes phase information. The model is only trained on 60~h of real and synthetic echo scenarios. The training setup includes multi-lingual speech, data augmentation, additional noise and reverberation to create a model that should generalize well to a large variety of real-world conditions. The DTLN approach produces state-of-the-art performance on clean and noisy echo conditions reducing acoustic echo and additional noise robustly. The method outperforms the AEC-Challenge baseline by 0.30 in terms of Mean Opinion Score (MOS).      
### 7.Real-time, Software-Defined, GPU-Based Receiver Field Trial  [ :arrow_down: ](https://arxiv.org/pdf/2010.14333.pdf)
>  We demonstrate stable real-time operation of a software-defined, GPU-based receiver over a metropolitan network. Massive parallelization is exploited for implementing direct-detection and coherent Kramers-Kronig detection in real time at 2 and 1 GBaud, respectively.      
### 8.Post Training Uncertainty Calibration of Deep Networks For Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2010.14290.pdf)
>  Neural networks for automated image segmentation are typically trained to achieve maximum accuracy, while less attention has been given to the calibration of their confidence scores. However, well-calibrated confidence scores provide valuable information towards the user. We investigate several post hoc calibration methods that are straightforward to implement, some of which are novel. They are compared to Monte Carlo (MC) dropout. They are applied to neural networks trained with cross-entropy (CE) and soft Dice (SD) losses on BraTS 2018 and ISLES 2018. Surprisingly, models trained on SD loss are not necessarily less calibrated than those trained on CE loss. In all cases, at least one post hoc method improves the calibration. There is limited consistency across the results, so we can't conclude on one method being superior. In all cases, post hoc calibration is competitive with MC dropout. Although average calibration improves compared to the base model, subject-level variance of the calibration remains similar.      
### 9.Physics-Based Deep Learning for Fiber-Optic Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.14258.pdf)
>  We propose a new machine-learning approach for fiber-optic communication systems whose signal propagation is governed by the nonlinear Schrödinger equation (NLSE). Our main observation is that the popular split-step method (SSM) for numerically solving the NLSE has essentially the same functional form as a deep multi-layer neural network; in both cases, one alternates linear steps and pointwise nonlinearities. We exploit this connection by parameterizing the SSM and viewing the linear steps as general linear functions, similar to the weight matrices in a neural network. The resulting physics-based machine-learning model has several advantages over "black-box" function approximators. For example, it allows us to examine and interpret the learned solutions in order to understand why they perform well. As an application, low-complexity nonlinear equalization is considered, where the task is to efficiently invert the NLSE. This is commonly referred to as digital backpropagation (DBP). Rather than employing neural networks, the proposed algorithm, dubbed learned DBP (LDBP), uses the physics-based model with trainable filters in each step and its complexity is reduced by progressively pruning filter taps during gradient descent. Our main finding is that the filters can be pruned to remarkably short lengths-as few as 3 taps/step-without sacrificing performance. As a result, the complexity can be reduced by orders of magnitude in comparison to prior work. By inspecting the filter responses, an additional theoretical justification for the learned parameter configurations is provided. Our work illustrates that combining data-driven optimization with existing domain knowledge can generate new insights into old communications problems.      
### 10.Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment  [ :arrow_down: ](https://arxiv.org/pdf/2010.14233.pdf)
>  Non-autoregressive models greatly improve decoding speed over typical sequence-to-sequence models, but suffer from degraded performance. Infilling and iterative refinement models make up some of this gap by editing the outputs of a non-autoregressive model, but are constrained in the edits that they can make. We propose iterative realignment, where refinements occur over latent alignments rather than output sequence space. We demonstrate this in speech recognition with Align-Refine, an end-to-end Transformer-based model which refines connectionist temporal classification (CTC) alignments to allow length-changing insertions and deletions. Align-Refine outperforms Imputer and Mask-CTC, matching an autoregressive baseline on WSJ at 1/14th the real-time factor and attaining a LibriSpeech test-other WER of 9.0% without an LM. Our model is strong even in one iteration with a shallower decoder.      
### 11.Virtual Alignment Method and its application to the dental prostheses and diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2010.14231.pdf)
>  The recent proposal of a new alignment solution for X-ray tomography, Virtual alignment method (VAM) allowed a more accurate method to remove the possible errors that limit the resolution and clarity of the reconstructed image. In the field of dentistry, the movement of patients during the scanning poses as one of the major factors hindering the final reconstructed image quality. Here, the patient's movement was artificially given to the projection image set and the newly proposed algorithm using the sinogram and the fixed point was applied to the tooth sample to compare the reconstruction image to the actual projection image set. The new alignment method showed promising results by reducing the margin of errors down to a few micrometer, which will allow the production of high-quality dental prostheses with accuracy and precision. We hope that the newly proposed alignment method can be further investigated to be applied more readily in the filed of dentistry ot provide better quality images of patients to make a more accurate diagnosis and prostheses.      
### 12.A Comparison of Discrete Latent Variable Models for Speech Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.14230.pdf)
>  Neural latent variable models enable the discovery of interesting structure in speech audio data. This paper presents a comparison of two different approaches which are broadly based on predicting future time-steps or auto-encoding the input signal. Our study compares the representations learned by vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme recognition performance. Results show that future time-step prediction with vq-wav2vec achieves better performance. The best system achieves an error rate of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge      
### 13.UAV-Assisted and Intelligent Reflecting Surfaces-Supported Terahertz Communications  [ :arrow_down: ](https://arxiv.org/pdf/2010.14223.pdf)
>  In this paper, unmanned aerial vehicles (UAVs) and intelligent reflective surface (IRS) are utilized to support terahertz (THz) communications. <br>To this end, the joint optimization of UAV's trajectory, the phase shift of IRS, the allocation of THz sub-bands, and the power control is investigated to maximize the minimum average achievable rate of all the users. <br>An iteration algorithm based on successive Convex Approximation with the Rate constraint penalty (CAR) is developed to obtain UAV's trajectory, and the IRS phase shift is formulated as a closed-form expression with introduced pricing factors. <br>Simulation results show that the proposed scheme significantly enhances the rate performance of the whole system.      
### 14.Motion Compensated Whole-Heart Coronary Magnetic Resonance Angiography using Focused Navigation (fNAV)  [ :arrow_down: ](https://arxiv.org/pdf/2010.14206.pdf)
>  Background: RSN whole-heart CMRA is a technique that estimates and corrects for respiratory motion. However, RSN has been limited to a 1D rigid correction which is often insufficient for patients with complex respiratory patterns. The goal of this work is therefore to improve the robustness and quality of 3D radial CMRA by incorporating both 3D motion information and nonrigid intra-acquisition correction of the data into a framework called focused navigation (fNAV). Methods: We applied fNAV to 500 data sets from a numerical simulation, 22 healthy volunteers, and 549 cardiac patients. We compared fNAV to RSN and respiratory resolved XD-GRASP reconstructions of the same data and recorded reconstruction times. Motion accuracy was measured as the correlation between fNAV and ground truth for simulations, and fNAV and image registration for in vivo data. Vessel sharpness was measured using Soap-Bubble. Finally, image quality analysis was performed by a blinded expert reviewer who chose the best image for each data set. Results The reconstruction time for fNAV images was significantly higher than RSN (6.1 +/- 2.1 minutes vs 1.4 +/- 0.3, minutes, p&lt;0.025) but significantly lower than XD-GRASP (25.6 +/- 7.1, minutes, p&lt;0.025). There is high correlation between the fNAV, and reference displacement estimates across all data sets (0.73 +/- 0.29). For all data, fNAV lead to significantly sharper vessels than all other reconstructions (p &lt; 0.01). Finally, a blinded reviewer chose fNAV as the best image in 239 out of 571 cases (p = 10-5). Conclusion: fNAV is a promising technique for improving free-breathing 3D radial whole-heart CMRA. This novel approach to respiratory self-navigation can derive 3D nonrigid motion estimations from an acquired 1D signal yielding statistically significant improvement in image sharpness relative to 1D translational correction as well as XD-GRASP reconstructions.      
### 15.Data-driven distributed control: Virtual reference feedback tuning in dynamic networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.14177.pdf)
>  In this paper, the problem of synthesizing a distributed controller from data is considered, with the objective to optimize a model-reference control criterion. We establish an explicit ideal distributed controller that solves the model-reference control problem for a structured reference model. On the basis of input-output data collected from the interconnected system, a virtual experiment setup is constructed which leads to a network identification problem. We formulate a prediction-error identification criterion that has the same global optimum as the model-reference criterion, when the controller class contains the ideal distributed controller. The developed distributed controller synthesis method is illustrated on an academic example network of nine subsystems and the influence of the controller interconnection structure on the achieved closed-loop performance is analyzed.      
### 16.Enhanced Cyber-Physical Security Using Attack-resistant Cyber Nodes and Event-triggered Moving Target Defence  [ :arrow_down: ](https://arxiv.org/pdf/2010.14173.pdf)
>  This paper outlines a cyber-physical authentication strategy to protect power system infrastructure against false data injection (FDI) attacks. We demonstrate that it is feasible to use small, low-cost, yet highly attack-resistant security chips as measurement nodes, enhanced with an event-triggered moving target defence (MTD), to offer effective cyber-physical security. At the cyber layer, the proposed solution is based on the MULTOS Trust-Anchor chip, using an authenticated encryption protocol, offering cryptographically protected and chained reports at up to 12/s. The availability of the trust-anchors, allows the grid controller to delegate aspects of passive anomaly detection, supporting local as well as central alarms. In this context, a distributed event-triggered MTD protocol is implemented at the physical layer to complement cyber side enhancement. This protocol applies a distributed anomaly detection scheme based on Holt-Winters seasonal forecasting in combination with MTD implemented via inductance perturbation. The scheme is shown to be effective at preventing or detecting a wide range of attacks against power system measurement system.      
### 17.Clock-centric Serial Links for the Synchronization of Distributed Readout Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.14164.pdf)
>  Detector readout systems for medium to large scale physics experiments, and instruments in some other fields as well, are generally composed of multiple front-end digitizer boards distributed over a certain area. Often, this hardware has to be synchronized to a common reference clock with minimal skew and low jitter. Today's mainstream solutions to precise clock distribution and deterministic latency messaging rely on the capabilities of high speed serial transceivers (a.k.a. SerDes) embedded in modern Field Programmable Gate Arrays (FPGAs). An alternative option uses distinct clock and data links. This can potentially reach higher synchronization accuracy, at significant hardware expenses. This work reports some first steps to explore a third scheme for clock and synchronous message distribution. Like the standard approach, the same media is used to convey clock and data, but instead of using today's "data-centric" links where the recovered clock is only a by-product of a SerDes, this paper defines and investigates "clock-centric" links where, at the opposite, a clock is carried by the link, and synchronous data are embedded into it by a modulation technique. After defining the concepts and principles of data-centric links, experimental studies are presented. Finally, the merits and limitations of the proposed approach are discussed.      
### 18.Leveraging Location Information for RIS-aided mmWave MIMO Communications  [ :arrow_down: ](https://arxiv.org/pdf/2010.14163.pdf)
>  Location information offered by external positioning systems, e.g., satellite navigation, can be used as prior information in the process of beam alignment and channel parameter estimation for reconfigurable intelligent surface (RIS)-aided millimeter wave (mmWave) multiple-input multiple-output networks. Benefiting from the availability of such prior information, albeit imperfect, the beam alignment and channel parameter estimation processes can be significantly accelerated with less candidate beams explored at all the terminals. We propose a practical channel parameter estimation method via atomic norm minimization, which outperforms the standard beam alignment in terms of both the mean square error and the effective spectrum efficiency for the same training overhead.      
### 19.Secure-by-Construction Optimal Path Planning for Linear Temporal Logic Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2010.14160.pdf)
>  In this paper, we investigate the problem of planning an optimal infinite path for a single robot to achieve a linear temporal logic (LTL) task with security guarantee. We assume that the external behavior of the robot, specified by an output function, can be accessed by a passive intruder (eavesdropper). The security constraint requires that the intruder should never infer that the robot was started from a secret location. We provide a sound and complete algorithmic procedure to solve this problem. Our approach is based on the construction of the twin weighted transition systems (twin-WTS) that tracks a pair of paths having the same observation. We show that the security-aware path planning problem can be effectively solved based on graph search techniques in the product of the twin-WTS and the Büchi automaton representing the LTL formula. The complexity of the proposed planning algorithm is polynomial in the size of the system model. Finally, we illustrate our algorithm by a simple robot planning example.      
### 20.Parallel waveform synthesis based on generative adversarial networks with voicing-aware conditional discriminators  [ :arrow_down: ](https://arxiv.org/pdf/2010.14151.pdf)
>  This paper proposes voicing-aware conditional discriminators for Parallel WaveGAN-based waveform synthesis systems. In this framework, we adopt a projection-based conditioning method that can significantly improve the discriminator's performance. Furthermore, the conventional discriminator is separated into two waveform discriminators for modeling voiced and unvoiced speech. As each discriminator learns the distinctive characteristics of the harmonic and noise components, respectively, the adversarial training process becomes more efficient, allowing the generator to produce more realistic speech waveforms. Subjective test results demonstrate the superiority of the proposed method over the conventional Parallel WaveGAN and WaveNet systems. In particular, our speaker-independently trained model within a FastSpeech 2 based text-to-speech framework achieves the mean opinion scores of 4.20, 4.18, 4.21, and 4.31 for four Japanese speakers, respectively.      
### 21.FragmentVC: Any-to-Any Voice Conversion by End-to-End Extracting and Fusing Fine-Grained Voice Fragments With Attention  [ :arrow_down: ](https://arxiv.org/pdf/2010.14150.pdf)
>  Any-to-any voice conversion aims to convert the voice from and to any speakers even unseen during training, which is much more challenging compared to one-to-one or many-to-many tasks, but much more attractive in real-world scenarios. In this paper we proposed FragmentVC, in which the latent phonetic structure of the utterance from the source speaker is obtained from Wav2Vec 2.0, while the spectral features of the utterance(s) from the target speaker are obtained from log mel-spectrograms. By aligning the hidden structures of the two different feature spaces with a two-stage training process, FragmentVC is able to extract fine-grained voice fragments from the target speaker utterance(s) and fuse them into the desired utterance, all based on the attention mechanism of Transformer as verified with analysis on attention maps, and is accomplished end-to-end. This approach is trained with reconstruction loss only without any disentanglement considerations between content and speaker information and doesn't require parallel data. Objective evaluation based on speaker verification and subjective evaluation with MOS both showed that this approach outperformed SOTA approaches, such as AdaIN-VC and AutoVC.      
### 22.Phase Aware Speech Enhancement using Realisation of Complex-valued LSTM  [ :arrow_down: ](https://arxiv.org/pdf/2010.14122.pdf)
>  Most of the deep learning based speech enhancement (SE) methods rely on estimating the magnitude spectrum of the clean speech signal from the observed noisy speech signal, either by magnitude spectral masking or regression. These methods reuse the noisy phase while synthesizing the time-domain waveform from the estimated magnitude spectrum. However, there have been recent works highlighting the importance of phase in SE. There was an attempt to estimate the complex ratio mask taking phase into account using complex-valued feed-forward neural network (FFNN). But FFNNs cannot capture the sequential information essential for phase estimation. In this work, we propose a realisation of complex-valued long short-term memory (RCLSTM) network to estimate the complex ratio mask (CRM) using sequential information along time. The proposed RCLSTM is designed to process the complex-valued sequences using complex arithmetic, and hence it preserves the dependencies between the real and imaginary parts of CRM and thereby the phase. The proposed method is evaluated on the noisy speech mixtures formed from the Voice-Bank corpus and DEMAND database. When compared to real value based masking methods, the proposed RCLSTM improves over them in several objective measures including perceptual evaluation of speech quality (PESQ), in which it improves by over 4.3%      
### 23.Secure Your Intention: On Notions of Pre-Opacity in Discrete-Event Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.14120.pdf)
>  This paper investigates an important informationflow security property called opacity in partially-observed discrete-event systems. We consider the presence of a passive intruder (eavesdropper) that knows the dynamic model of the system and can use the generated information-flow to infer some "secret" of the system. A system is said to be opaque if it always holds the plausible deniability for its secret. Existing notions of opacity only consider secret either as currently visiting some secret states or as having visited some secret states in the past. In this paper, we investigate information-flow security from a new angle by considering the secret of the system as the intention to execute some particular behavior of importance in the future. To this end, we propose a new class of opacity called pre-opacity that characterizes whether or not the intruder can predict the visit of secret states a certain number of steps ahead before the system actually does so. Depending the prediction task of the intruder, we propose two specific kinds of pre-opacity called K-step instant pre-opacity and K-step trajectory pre-opacity to specify this concept. For each notion of pre-opacity, we provide a necessary and sufficient condition as well as an effective verification algorithm. The complexity for the verification of pre-opacity is exponential in the size of the system as we show that pre-opacity is inherently PSPACE-hard. Finally, we generalize our setting to the case where the secret intention of the system is modeled as executing a particular sequence of events rather than visiting a secret state.      
### 24.Hyperspectral Anomaly Change Detection Based on Auto-encoder  [ :arrow_down: ](https://arxiv.org/pdf/2010.14119.pdf)
>  With the hyperspectral imaging technology, hyperspectral data provides abundant spectral information and plays a more important role in geological survey, vegetation analysis and military reconnaissance. Different from normal change detection, hyperspectral anomaly change detection (HACD) helps to find those small but important anomaly changes between multi-temporal hyperspectral images (HSI). In previous works, most classical methods use linear regression to establish the mapping relationship between two HSIs and then detect the anomalies from the residual image. However, the real spectral differences between multi-temporal HSIs are likely to be quite complex and of nonlinearity, leading to the limited performance of these linear predictors. In this paper, we propose an original HACD algorithm based on auto-encoder (ACDA) to give a nonlinear solution. The proposed ACDA can construct an effective predictor model when facing complex imaging conditions. In the ACDA model, two systematic auto-encoder (AE) networks are deployed to construct two predictors from two directions. The predictor is used to model the spectral variation of the background to obtain the predicted image under another imaging condition. Then mean square error (MSE) between the predictive image and corresponding expected image is computed to obtain the loss map, where the spectral differences of the unchanged pixels are highly suppressed and anomaly changes are highlighted. Ultimately, we take the minimum of the two loss maps of two directions as the final anomaly change intensity map. The experiments results on public "Viareggio 2013" datasets demonstrate the efficiency and superiority over traditional methods.      
### 25.Energy Consumption and Battery Aging Minimization Using a Q-learning Strategy for a Battery/Ultracapacitor Electric Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2010.14115.pdf)
>  Propulsion system electrification revolution has been undergoing in the automotive industry. The electrified propulsion system improves energy efficiency and reduces the dependence on fossil fuel. However, the batteries of electric vehicles experience degradation process during vehicle operation. Research considering both battery degradation and energy consumption in battery/ supercapacitor electric vehicles is still lacking. This study proposes a Q-learning-based strategy to minimize battery degradation and energy consumption. Besides Q-learning, two heuristic energy management methods are also proposed and optimized using Particle Swarm Optimization algorithm. A vehicle propulsion system model is first presented, where the severity factor battery degradation model is considered and experimentally validated with the help of Genetic Algorithm. In the results analysis, Q-learning is first explained with the optimal policy map after learning. Then, the result from a vehicle without ultracapacitor is used as the baseline, which is compared with the results from the vehicle with ultracapacitor using Q-learning, and two heuristic methods as the energy management strategies. At the learning and validation driving cycles, the results indicate that the Q-learning strategy slows down the battery degradation by 13-20% and increases the vehicle range by 1.5-2% compared with the baseline vehicle without ultracapacitor.      
### 26.Full-Duplex Cell-Free mMIMO Systems: Analysis and Decentralized Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2010.14110.pdf)
>  Cell-free (CF) massive multiple-input-multiple-output (mMIMO) deployments are usually investigated with half-duplex (HD) nodes and high-capacity fronthaul links. To leverage the possible gains in throughput and energy efficiency (EE) of full-duplex (FD) communications, we consider a FD CF mMIMO system with practical limited-capacity fronthaul links. We derive closed-form spectral efficiency (SE) lower bounds for this system with maximum-ratio transmission/maximum-ratio combining (MRT/MRC) processing and optimal uniform quantization. We then optimize the weighted sum EE (WSEE) via downlink and uplink power control by using a two-layered approach: the first layer formulates the optimization as a generalized convex program (GCP), while the second layer solves the optimization decentrally using alternating direction method of multipliers. We analytically show that the proposed two-layered formulation yields a Karush-Kuhn-Tucker point of the original WSEE optimization. We numerically show the influence of weights on the individual EE of the users, which demonstrates the utility of WSEE metric to incorporate heterogeneous EE requirements of users. We also show that with low fronthaul capacity, the system requires a higher number of fronthaul quantization bits to achieve high SE and WSEE. For high fronthaul capacity, higher number of bits, however, achieves high SE and a reduced WSEE.      
### 27.Micro-CT Synthesis and Inner Ear Super Resolution via Bayesian Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.14105.pdf)
>  Existing medical image super-resolution methods rely on pairs of low- and high- resolution images to learn a mapping in a fully supervised manner. However, such image pairs are often not available in clinical practice. In this paper, we address super resolution problem in a real-world scenario using unpaired data and synthesize linearly \textbf{eight times} higher resolved Micro-CT images of temporal bone structure, which is embedded in the inner ear. We explore cycle-consistency generative adversarial networks for super-resolution task and equip the translation approach with Bayesian inference. We further introduce \emph{Hu Moment} the evaluation metric to quantify the structure of the temporal bone. We evaluate our method on a public inner ear CT dataset and have seen both visual and quantitative improvement over state-of-the-art deep-learning based methods. In addition, we perform a multi-rater visual evaluation experiment and find that trained experts consistently rate the proposed method highest quality scores among all methods. Implementing our approach as an end-to-end learning task, we are able to quantify uncertainty in the unpaired translation tasks and find that the uncertainty mask can provide structural information of the temporal bone.      
### 28.Triple-view Convolutional Neural Networks for COVID-19 Diagnosis with Chest X-ray  [ :arrow_down: ](https://arxiv.org/pdf/2010.14091.pdf)
>  The Coronavirus Disease 2019 (COVID-19) is affecting increasingly large number of people worldwide, posing significant stress to the health care systems. Early and accurate diagnosis of COVID-19 is critical in screening of infected patients and breaking the person-to-person transmission. Chest X-ray (CXR) based computer-aided diagnosis of COVID-19 using deep learning becomes a promising solution to this end. However, the diverse and various radiographic features of COVID-19 make it challenging, especially when considering each CXR scan typically only generates one single image. Data scarcity is another issue since collecting large-scale medical CXR data set could be difficult at present. Therefore, how to extract more informative and relevant features from the limited samples available becomes essential. To address these issues, unlike traditional methods processing each CXR image from a single view, this paper proposes triple-view convolutional neural networks for COVID-19 diagnosis with CXR images. Specifically, the proposed networks extract individual features from three views of each CXR image, i.e., the left lung view, the right lung view and the overall view, in three streams and then integrate them for joint diagnosis. The proposed network structure respects the anatomical structure of human lungs and is well aligned with clinical diagnosis of COVID-19 in practice. In addition, the labeling of the views does not require experts' domain knowledge, which is needed by many existing methods. The experimental results show that the proposed method achieves state-of-the-art performance, especially in the more challenging three class classification task, and admits wide generality and high flexibility.      
### 29.Jacobi-Style Iteration for Distributed Submodular Maximization  [ :arrow_down: ](https://arxiv.org/pdf/2010.14082.pdf)
>  This paper presents a novel Jacobi-style iteration algorithm for solving the problem of distributed submodular maximization, in which each agent determines its own strategy from a finite set so that the global submodular objective function is jointly maximized. Building on the multi-linear extension of the global submodular function, we expect to achieve the solution from a probabilistic, rather than deterministic, perspective, and thus transfer the considered problem from a discrete domain into a continuous domain. Since it is observed that an unbiased estimation of the gradient of multi-linear extension function~can be obtained by sampling the agents' local decisions, a projected stochastic gradient algorithm is proposed to solve the problem. Our algorithm enables the distributed updates among all individual agents and is proved to asymptotically converge to a desirable equilibrium solution. Such an equilibrium solution is guaranteed to achieve at least 1/2-suboptimal bound, which is comparable to the state-of-art in the literature. Moreover, we further enhance the proposed algorithm by handling the scenario in which agents' communication delays are present. The enhanced algorithmic framework admits a more realistic distributed implementation of our approach. Finally, a movie recommendation task is conducted on a real-world movie rating data set, to validate the numerical performance of the proposed algorithms.      
### 30.Two-Parametric Nyquist Pulses with Better Performance Based on Inverse Hyperbolic Functions  [ :arrow_down: ](https://arxiv.org/pdf/2010.14035.pdf)
>  In this article, three new inter-symbol interference (ISI)-free pulses with enhanced performance compared to the state-of-the-art are proposed and studied in terms of frequency and time domain characteristics. They are based on inverse hyperbolic functions and on the concept of inner and outer functions, which was first introduced by the authors. New pulses are two-parametric, i.e., their design depends only on the roll-off factor and the timing jitter parameter, and they outperform most of the well-known pulses reported in the literature, since they present lower error probability, smaller maximum distortion and wider eye-diagram.      
### 31.Spatio-Temporal Processing for Automatic Vehicle Detection in Wide-Area Aerial Video  [ :arrow_down: ](https://arxiv.org/pdf/2010.14025.pdf)
>  Vehicle detection in aerial videos often requires post-processing to eliminate false detections. This paper presents a spatio-temporal processing scheme to improve automatic vehicle detection performance by replacing the thresholding step of existing detection algorithms with multi-neighborhood hysteresis thresholding for foreground pixel classification. The proposed scheme also performs spatial post-processing, which includes morphological opening and closing to shape and prune the detected objects, and temporal post-processing to further reduce false detections. We evaluate the performance of the proposed spatial processing on two local aerial video datasets and one parking vehicle dataset, and the performance of the proposed spatio-temporal processing scheme on five local aerial video datasets and one public dataset. Experimental evaluation shows that the proposed schemes improve vehicle detection performance for each of the nine algorithms when evaluated on seven datasets. Overall, the use of the proposed spatio-temporal processing scheme improves average F-score to above 0.8 and achieves an average reduction of 83.8% in false positives.      
### 32.Online Security Assessment of Low-Inertia Power Systems: A Real-Time Frequency Stability Tool for the Australian South-West Interconnected System  [ :arrow_down: ](https://arxiv.org/pdf/2010.14016.pdf)
>  In small/medium-sized isolated power networks with low rotational inertia and high penetration of renewables, generation/load contingency events may cause large frequency excursions, potentially leading to cascading failures and even blackouts. Therefore, it is crucial for system operators to be able to monitor the state of the network in real-time and predict the maximum possible frequency deviations at all times. <br>This paper presents a real-time frequency stability (RTFS) tool developed by the Australian Energy Market Operator (AEMO) and operationalized in the control room for the South West Interconnected System (SWIS) to ensure that the available spinning reserve is sufficient and fast enough to arrest frequency excursions under any conditions, and particularly low-inertia ones. To reduce the computational burden and complexity of the different turbine-governor models, a simple first-order lag function with two adjustable variables has been used for each of the generator. These adjustable parameters, along with other key model parameters such as load damping and inertia, have been calibrated against actual frequency response using high-speed fault recorder data from historical events. As demonstrated in several case studies, the real-time tool has proven to be accurate at predicting the trajectory of system frequency after credible contingencies, thus suggesting that similar implementations could be carried out elsewhere while power systems worldwide progress towards lower inertia.      
### 33.Long Short-Term Memory Neuron Equalizer  [ :arrow_down: ](https://arxiv.org/pdf/2010.14009.pdf)
>  In this work we propose a neuromorphic hardware based signal equalizer by based on the deep learning implementation. The proposed neural equalizer is plasticity trainable equalizer which is different from traditional model designed based DFE. A trainable Long Short-Term memory neural network based DFE architecture is proposed for signal recovering and digital implementation is evaluated through FPGA implementation. Constructing with modelling based equalization methods, the proposed approach is compatible to multiple frequency signal equalization instead of single type signal equalization. We shows quantitatively that the neuronmorphic equalizer which is amenable both analog and digital implementation outperforms in different metrics in comparison with benchmarks approaches. The proposed method is adaptable both for general neuromorphic computing or ASIC instruments.      
### 34.Graph Blind Deconvolution with Sparseness Constraint  [ :arrow_down: ](https://arxiv.org/pdf/2010.14002.pdf)
>  We propose a blind deconvolution method for signals on graphs, with the exact sparseness constraint for the original signal. Graph blind deconvolution is an algorithm for estimating the original signal on a graph from a set of blurred and noisy measurements. Imposing a constraint on the number of nonzero elements is desirable for many different applications. This paper deals with the problem with constraints placed on the exact number of original sources, which is given by an optimization problem with an $\ell_0$ norm constraint. We solve this non-convex optimization problem using the ADMM iterative solver. Numerical experiments using synthetic signals demonstrate the effectiveness of the proposed method.      
### 35.One-class learning towards generalized voice spoofing detection  [ :arrow_down: ](https://arxiv.org/pdf/2010.13995.pdf)
>  Human voices can be used to authenticate the identity of the speaker, but the automatic speaker verification (ASV) systems are vulnerable to voice spoofing attacks, such as impersonation, replay, text-to-speech, and voice conversion. Recently, researchers developed anti-spoofing techniques to improve the reliability of ASV systems against spoofing attacks. However, most methods encounter difficulties in detecting unknown attacks in practical use, which often have different statistical distributions from known attacks. In this work, we propose an anti-spoofing system to detect unknown logical access attacks (i.e., synthetic speech) using one-class learning. The key idea is to compact the genuine speech representation and inject an angular margin to separate the spoofing attacks in the embedding space. Our system achieves an equal error rate of 2.19% on the evaluation set of ASVspoof 2019 Challenge, outperforming all existing single systems.      
### 36.Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study  [ :arrow_down: ](https://arxiv.org/pdf/2010.13989.pdf)
>  The quality and accuracy of power system models is critical for simulation-based studies, especially for studying actual stability issues in large-scale systems. With the deployment of wide-area monitoring systems (WAMSs), the high-reporting-rate frequency measurement provides a trustworthy ground truth for validating models in frequency response studies. This paper documented an effort to check, tune, and validate the U.S. power system model based on a WAMS called FNET/GridEye. Four metrics are used to quantitatively compare the simulation results and the actual measurement, including frequency nadir, RoCoF, settling frequency and settling time. After tuning governor deadband and the governor ratio, the model frequency response shows significant improvement and matches well with the event measurement data. This work serves as an example for tuning and validating large-scale power system models.      
### 37.Wearing a MASK: Compressed Representations of Variable-Length Sequences Using Recurrent Neural Tangent Kernels  [ :arrow_down: ](https://arxiv.org/pdf/2010.13975.pdf)
>  High dimensionality poses many challenges to the use of data, from visualization and interpretation, to prediction and storage for historical preservation. Techniques abound to reduce the dimensionality of fixed-length sequences, yet these methods rarely generalize to variable-length sequences. To address this gap, we extend existing methods that rely on the use of kernels to variable-length sequences via use of the Recurrent Neural Tangent Kernel (RNTK). Since a deep neural network with ReLu activation is a Max-Affine Spline Operator (MASO), we dub our approach Max-Affine Spline Kernel (MASK). We demonstrate how MASK can be used to extend principal components analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE) and apply these new algorithms to separate synthetic time series data sampled from second-order differential equations.      
### 38.Impact of Spherical Coordinates Transformation Pre-processing in Deep Convolution Neural Networks for Brain Tumor Segmentation and Survival Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2010.13967.pdf)
>  Pre-processing and Data Augmentation play an important role in Deep Convolutional Neural Networks (DCNN). Whereby several methods aim for standardization and augmentation of the dataset, we here propose a novel method aimed to feed DCNN with spherical space transformed input data that could better facilitate feature learning compared to standard Cartesian space images and volumes. In this work, the spherical coordinates transformation has been applied as a preprocessing method that, used in conjunction with normal MRI volumes, improves the accuracy of brain tumor segmentation and patient overall survival (OS) prediction on Brain Tumor Segmentation (BraTS) Challenge 2020 dataset. The LesionEncoder framework has been then applied to automatically extract features from DCNN models, achieving 0.586 accuracy of OS prediction on the validation data set, which is one of the best results according to BraTS 2020 leaderboard.      
### 39.Recent Developments on ESPnet Toolkit Boosted by Conformer  [ :arrow_down: ](https://arxiv.org/pdf/2010.13956.pdf)
>  In this study, we present recent developments on ESPnet: End-to-End Speech Processing toolkit, which mainly involves a recently proposed architecture called Conformer, Convolution-augmented Transformer. This paper shows the results for a wide range of end-to-end speech processing applications, such as automatic speech recognition (ASR), speech translations (ST), speech separation (SS) and text-to-speech (TTS). Our experiments reveal various training tips and significant performance benefits obtained with the Conformer on different tasks. These results are competitive or even outperform the current state-of-art Transformer models. We are preparing to release all-in-one recipes using open source and publicly available corpora for all the above tasks with pre-trained models. Our aim for this work is to contribute to our research community by reducing the burden of preparing state-of-the-art research environments usually requiring high resources.      
### 40.MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection  [ :arrow_down: ](https://arxiv.org/pdf/2010.13886.pdf)
>  We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.      
### 41.Improved Supervised Training of Physics-Guided Deep Learning Image Reconstruction with Multi-Masking  [ :arrow_down: ](https://arxiv.org/pdf/2010.13868.pdf)
>  Physics-guided deep learning (PG-DL) via algorithm unrolling has received significant interest for improved image reconstruction, including MRI applications. These methods unroll an iterative optimization algorithm into a series of regularizer and data consistency units. The unrolled networks are typically trained end-to-end using a supervised approach. Current supervised PG-DL approaches use all of the available sub-sampled measurements in their data consistency units. Thus, the network learns to fit the rest of the measurements. In this study, we propose to improve the performance and robustness of supervised training by utilizing randomness by retrospectively selecting only a subset of all the available measurements for data consistency units. The process is repeated multiple times using different random masks during training for further enhancement. Results on knee MRI show that the proposed multi-mask supervised PG-DL enhances reconstruction performance compared to conventional supervised PG-DL approaches.      
### 42.Linear Predictive Coding as a Valid Approximation of a Mass Spring Damper Model for Acute Stress Prediction from Computer Mouse Movement  [ :arrow_down: ](https://arxiv.org/pdf/2010.13836.pdf)
>  Prior work demonstrated the potential of using Linear Predictive Coding (LPC) to approximate muscle stiffness and damping from computer mouse (a.k.a. mouse) movement to predict stress levels of users. Theoretically, muscle stiffness in the arm can be estimated using a mass-spring-damper (MSD) biomechanical model of the arm. However, the damping frequency and damping ratio values derived using LPC have not yet been compared with those from the theoretical MSD model. In this work, we demonstrate the damping frequency and damping ratio from LPC are significantly correlated with those from MSD model, thus confirming the validity of using LPC to infer muscle stiffness and damping. We also compare the stress level binary classification performance using the values from LPC and MSD with each other and with neural network-based baselines. We found comparable performance across all conditions demonstrating the efficacy of LPC and MSD model-based stress prediction.      
### 43.Distributed Constraint-Coupled Optimization via Primal Decomposition over Random Time-Varying Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2010.14489.pdf)
>  The paper addresses large-scale, convex optimization problems that need to be solved in a distributed way by agents communicating according to a random time-varying graph. Specifically, the goal of the network is to minimize the sum of local costs, while satisfying local and coupling constraints. Agents communicate according to a time-varying model in which edges of an underlying connected graph are active at each iteration with certain non-uniform probabilities. By relying on a primal decomposition scheme applied to an equivalent problem reformulation, we propose a novel distributed algorithm in which agents negotiate a local allocation of the total resource only with neighbors with active communication links. The algorithm is studied as a subgradient method with block-wise updates, in which blocks correspond to the graph edges that are active at each iteration. Thanks to this analysis approach, we show almost sure convergence to the optimal cost of the original problem and almost sure asymptotic primal recovery without resorting to averaging mechanisms typically employed in dual decomposition schemes. Explicit sublinear convergence rates are provided under the assumption of diminishing and constant step-sizes. Finally, an extensive numerical study on a plug-in electric vehicle charging problem corroborates the theoretical results.      
### 44.Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.14462.pdf)
>  Computational image reconstruction algorithms generally produce a single image without any measure of uncertainty or confidence. Regularized Maximum Likelihood (RML) and feed-forward deep learning approaches for inverse problems typically focus on recovering a point estimate. This is a serious limitation when working with underdetermined imaging systems, where it is conceivable that multiple image modes would be consistent with the measured data. Characterizing the space of probable images that explain the observational data is therefore crucial. In this paper, we propose a variational deep probabilistic imaging approach to quantify reconstruction uncertainty. Deep Probabilistic Imaging (DPI) employs an untrained deep generative model to estimate a posterior distribution of an unobserved image. This approach does not require any training data; instead, it optimizes the weights of a neural network to generate image samples that fit a particular measurement dataset. Once the network weights have been learned, the posterior distribution can be efficiently sampled. We demonstrate this approach in the context of interferometric radio imaging, which is used for black hole imaging with the Event Horizon Telescope.      
### 45.Distributed Primal Decomposition for Large-Scale MILPs  [ :arrow_down: ](https://arxiv.org/pdf/2010.14446.pdf)
>  This paper deals with a distributed Mixed-Integer Linear Programming (MILP) set-up arising in several control applications. Agents of a network aim to minimize the sum of local linear cost functions subject to both individual constraints and a linear coupling constraint involving all the decision variables. A key, challenging feature of the considered set-up is that some components of the decision variables must assume integer values. The addressed MILPs are NP-hard, nonconvex and large-scale. Moreover, several additional challenges arise in a distributed framework due to the coupling constraint, so that feasible solutions with guaranteed suboptimality bounds are of interest. We propose a fully distributed algorithm based on a primal decomposition approach and an appropriate tightening of the coupling constraint. The algorithm is guaranteed to provide feasible solutions in finite time. Moreover, asymptotic and finite-time suboptimality bounds are established for the computed solution. Montecarlo simulations highlight the extremely low suboptimality bounds achieved by the algorithm.      
### 46.Deciding $ω$-Regular Properties on Linear Recurrence Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2010.14432.pdf)
>  We consider the problem of deciding $\omega$-regular properties on infinite traces produced by linear loops. Here we think of a given loop as producing a single infinite trace that encodes information about the signs of program variables at each time step. Formally, our main result is a procedure that inputs a prefix-independent $\omega$-regular property and a sequence of numbers satisfying a linear recurrence, and determines whether the sign description of the sequence (obtained by replacing each positive entry with "$+$", each negative entry with "$-$", and each zero entry with "$0$") satisfies the given property. Our procedure requires that the recurrence be simple, \ie, that the update matrix of the underlying loop be diagonalisable. This assumption is instrumental in proving our key technical lemma: namely that the sign description of a simple linear recurrence sequence is almost periodic in the sense of Muchnik, Semënov, and Ushakov. To complement this lemma, we give an example of a linear recurrence sequence whose sign description fails to be almost periodic. Generalising from sign descriptions, we also consider the verification of properties involving semi-algebraic predicates on program variables.      
### 47.Optimal transport for multi-commodity routing on networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.14377.pdf)
>  We present a model for finding optimal multi-commodity flows on networks based on optimal transport theory. The model relies on solving a dynamical system of equations. We prove that its stationary solution is equivalent to the solution of an optimization problem that generalizes the one-commodity framework. In particular, it generalizes previous results in terms of optimality, scaling, and phase transitions obtained in the one-commodity case. Remarkably, for a suitable range of parameters, the optimal topologies have loops. This is radically different to the one-commodity case, where within an analogous parameter range the optimal topologies are trees. This important result is a consequence of the extension of Kirkchoff's law to the multi-commodity case, which enforces the distinction between fluxes of the different commodities. Our results provide new insights into the nature and properties of optimal network topologies. In particular, they show that loops can arise as a consequence of distinguishing different flow types, and complement previous results where loops, in the one-commodity case, were arising as a consequence of imposing dynamical rules to the sources and sinks or when enforcing robustness to damage. Finally, we provide an efficient implementation for each of the two equivalent numerical frameworks, both of which achieve a computational complexity that is more efficient than that of standard optimization methods based on gradient descent. As a result, our model is not merely abstract but can be efficiently applied to large datasets. We give an example of concrete application by studying the network of the Paris metro.      
### 48.Upsampling artifacts in neural audio synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2010.14356.pdf)
>  A number of recent advances in audio synthesis rely on neural upsamplers, which can introduce undesired artifacts. In computer vision, upsampling artifacts have been studied and are known as checkerboard artifacts (due to their characteristic visual pattern). However, their effect has been overlooked so far in audio processing. Here, we address this gap by studying this problem from the audio signal processing perspective. We first show that the main sources of upsampling artifacts are: (i) the tonal and filtering artifacts introduced by problematic upsampling operators, and (ii) the spectral replicas that emerge while upsampling. We then compare different neural upsamplers, showing that nearest neighbor interpolation upsamplers can be an alternative to the problematic (but state-of-the-art) transposed and subpixel convolutions which are prone to introduce tonal artifacts.      
### 49.Leveraging speaker attribute information using multi task learning for speaker verification and diarization  [ :arrow_down: ](https://arxiv.org/pdf/2010.14269.pdf)
>  Deep speaker embeddings have become the leading method for encoding speaker identity in speaker recognition tasks. The embedding space should ideally capture the variations between all possible speakers, encoding the multiple aspects that make up speaker identity. In this work, utilizing speaker age as an auxiliary variable in US Supreme Court recordings and speaker nationality with VoxCeleb, we show that by leveraging additional speaker attribute information in a multi task learning setting, deep speaker embedding performance can be increased for verification and diarization tasks, achieving a relative improvement of 17.8% in DER and 8.9% in EER for Supreme Court audio compared to omitting the auxiliary task. Experimental code has been made publicly available.      
### 50.Random Shifting Intelligent Reflecting Surface for OTP Encrypted Data Transmission  [ :arrow_down: ](https://arxiv.org/pdf/2010.14268.pdf)
>  In this paper, we propose a novel encrypted data transmission scheme using an intelligent reflecting surface (IRS) to generate secret keys in wireless communication networks. We show that perfectly secure one-time pad (OTP) communications can be established by using a simple random phase shifting of the IRS elements. To maximize the secure transmission rate, we design an optimal time slot allocation algorithm for the IRS secret key generation and the encrypted data transmission phases. Moreover, a theoretical expression of the key generation rate is derived based on Poisson point process (PPP) for the practical scenario when eavesdroppers' channel state information (CSI) is unavailable. Simulation results show that employing our IRS-based scheme can significantly improve the encrypted data transmission performance for a wide-range of wireless channel gains and system parameters.      
### 51.Squeezing value of cross-domain labels: a decoupled scoring approach for speaker verification  [ :arrow_down: ](https://arxiv.org/pdf/2010.14243.pdf)
>  Domain mismatch often occurs in real applications and causes serious performance reduction on speaker verification systems. The common wisdom is to collect cross-domain data and train a multi-domain PLDA model, with the hope to learn a domain-independent speaker subspace. In this paper, we firstly present an empirical study to show that simply adding cross-domain data does not help performance in conditions with enrollment-test mismatch. Careful analysis shows that this striking result is caused by the incoherent statistics between the enrollment and test conditions. Based on this analysis, we present a decoupled scoring approach that can maximally squeeze the value of cross-domain labels and obtain optimal verification scores when the enrollment and test are mismatched. When the statistics are coherent, the new formulation falls back to the conventional PLDA. Experimental results on cross-channel test show that the proposed approach is highly effective and is a principle solution to domain mismatch.      
### 52.Deep generative factorization for speech signal  [ :arrow_down: ](https://arxiv.org/pdf/2010.14242.pdf)
>  Various information factors are blended in speech signals, which forms the primary difficulty for most speech information processing tasks. An intuitive idea is to factorize speech signal into individual information factors (e.g., phonetic content and speaker trait), though it turns out to be highly challenging. This paper presents a speech factorization approach based on a novel factorial discriminative normalization flow model (factorial DNF). Experiments conducted on a two-factor case that involves phonetic content and speaker trait demonstrates that the proposed factorial DNF has powerful capability to factorize speech signals and outperforms several comparative models in terms of information representation and manipulation.      
### 53.New interfaces for musical expression  [ :arrow_down: ](https://arxiv.org/pdf/2010.14228.pdf)
>  The rapid evolution of electronics, digital media, advanced materials, and other areas of technology, is opening up unprecedented opportunities for musical interface inventors and designers. The possibilities afforded by these new technologies carry with them the challenges of a complex and often confusing array of choices for musical composers and performers. New musical technologies are at least partly responsible for the current explosion of new musical forms, some of which are controversial and challenge traditional definitions of music. Alternative musical controllers, currently the leading edge of the ongoing dialogue between technology and musical culture, involve many of the issues covered at past CHI meetings. This workshop brings together interface experts interested in musical controllers and musicians and composers involved in the development of new musical interfaces.      
### 54.Spiking Neural Networks -- Part III: Neuromorphic Communications  [ :arrow_down: ](https://arxiv.org/pdf/2010.14220.pdf)
>  Synergies between wireless communications and artificial intelligence are increasingly motivating research at the intersection of the two fields. On the one hand, the presence of more and more wirelessly connected devices, each with its own data, is driving efforts to export advances in machine learning (ML) from high performance computing facilities, where information is stored and processed in a single location, to distributed, privacy-minded, processing at the end user. On the other hand, ML can address algorithm and model deficits in the optimization of communication protocols. However, implementing ML models for learning and inference on battery-powered devices that are connected via bandwidth-constrained channels remains challenging. This paper explores two ways in which Spiking Neural Networks (SNNs) can help address these open problems. First, we discuss federated learning for the distributed training of SNNs, and then describe the integration of neuromorphic sensing, SNNs, and impulse radio technologies for low-power remote inference.      
### 55.Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns  [ :arrow_down: ](https://arxiv.org/pdf/2010.14217.pdf)
>  Inspired by the operation of biological brains, Spiking Neural Networks (SNNs) have the unique ability to detect information encoded in spatio-temporal patterns of spiking signals. Examples of data types requiring spatio-temporal processing include logs of time stamps, e.g., of tweets, and outputs of neural prostheses and neuromorphic sensors. In this paper, the second of a series of three review papers on SNNs, we first review models and training algorithms for the dominant approach that considers SNNs as a Recurrent Neural Network (RNN) and adapt learning rules based on backpropagation through time to the requirements of SNNs. In order to tackle the non-differentiability of the spiking mechanism, state-of-the-art solutions use surrogate gradients that approximate the threshold activation function with a differentiable function. Then, we describe an alternative approach that relies on probabilistic models for spiking neurons, allowing the derivation of local learning rules via stochastic estimates of the gradient. Finally, experiments are provided for neuromorphic data sets, yielding insights on accuracy and convergence under different SNN models.      
### 56.Spiking Neural Networks -- Part I: Detecting Spatial Patterns  [ :arrow_down: ](https://arxiv.org/pdf/2010.14208.pdf)
>  Spiking Neural Networks (SNNs) are biologically inspired machine learning models that build on dynamic neuronal models processing binary and sparse spiking signals in an event-driven, online, fashion. SNNs can be implemented on neuromorphic computing platforms that are emerging as energy-efficient co-processors for learning and inference. This is the first of a series of three papers that introduce SNNs to an audience of engineers by focusing on models, algorithms, and applications. In this first paper, we first cover neural models used for conventional Artificial Neural Networks (ANNs) and SNNs. Then, we review learning algorithms and applications for SNNs that aim at mimicking the functionality of ANNs by detecting or generating spatial patterns in rate-encoded spiking signals. We specifically discuss ANN-to-SNN conversion and neural sampling. Finally, we validate the capabilities of SNNs for detecting and generating spatial patterns through experiments.      
### 57.Learning Contextual Tag Embeddings for Cross-Modal Alignment of Audio and Tags  [ :arrow_down: ](https://arxiv.org/pdf/2010.14171.pdf)
>  Self-supervised audio representation learning offers an attractive alternative for obtaining generic audio embeddings, capable to be employed into various downstream tasks. Published approaches that consider both audio and words/tags associated with audio do not employ text processing models that are capable to generalize to tags unknown during training. In this work we propose a method for learning audio representations using an audio autoencoder (AAE), a general word embeddings model (WEM), and a multi-head self-attention (MHA) mechanism. MHA attends on the output of the WEM, providing a contextualized representation of the tags associated with the audio, and we align the output of MHA with the output of the encoder of AAE using a contrastive loss. We jointly optimize AAE and MHA and we evaluate the audio representations (i.e. the output of the encoder of AAE) by utilizing them in three different downstream tasks, namely sound, music genre, and music instrument classification. Our results show that employing multi-head self-attention with multiple heads in the tag-based network can induce better learned audio representations.      
### 58.Rule-embedded network for audio-visual voice activity detection in live musical video streams  [ :arrow_down: ](https://arxiv.org/pdf/2010.14168.pdf)
>  Detecting anchor's voice in live musical streams is an important preprocessing for music and speech signal processing. Existing approaches to voice activity detection (VAD) primarily rely on audio, however, audio-based VAD is difficult to effectively focus on the target voice in noisy environments. With the help of visual information, this paper proposes a rule-embedded network to fuse the audio-visual (A-V) inputs to help the model better detect target voice. The core role of the rule in the model is to coordinate the relation between the bi-modal information and use visual representations as the mask to filter out the information of non-target sound. Experiments show that: 1) with the help of cross-modal fusion by the proposed rule, the detection result of A-V branch outperforms that of audio branch; 2) the performance of bi-modal model far outperforms that of audio-only models, indicating that the incorporation of both audio and visual signals is highly beneficial for VAD. To attract more attention to the cross-modal music and audio signal processing, a new live musical video corpus with frame-level label is introduced.      
### 59.Emotion recognition by fusing time synchronous and time asynchronous representations  [ :arrow_down: ](https://arxiv.org/pdf/2010.14102.pdf)
>  In this paper, a novel two-branch neural network model structure is proposed for multimodal emotion recognition, which consists of a time synchronous branch (TSB) and a time asynchronous branch (TAB). To capture correlations between each word and its acoustic realisation, the TSB combines speech and text modalities at each input window frame and then does pooling across time to form a single embedding vector. The TAB, by contrast, provides cross-utterance information by integrating sentence text embeddings from a number of context utterances into another embedding vector. The final emotion classification uses both the TSB and the TAB embeddings. Experimental results on the IEMOCAP dataset demonstrate that the two-branch structure achieves state-of-the-art results in 4-way classification with all common test setups. When using automatic speech recognition (ASR) output instead of manually transcribed reference text, it is shown that the cross-utterance information considerably improves the robustness against ASR errors. Furthermore, by incorporating an extra class for all the other emotions, the final 5-way classification system with ASR hypotheses can be viewed as a prototype for more realistic emotion recognition systems.      
### 60.Universal ASR: Unifying Streaming and Non-Streaming ASR Using a Single Encoder-Decoder Model  [ :arrow_down: ](https://arxiv.org/pdf/2010.14099.pdf)
>  Recently, online end-to-end ASR has gained increasing attention. However, the performance of online systems still lags far behind that of offline systems, with a large gap in quality of recognition. For specific scenarios, we can trade-off between performance and latency, and can train multiple systems with different delays to match the performance and latency requirements of various application scenarios. In this work, in contrast to trading-off between performance and latency, we envisage a single system that can match the needs of different scenarios. We propose a novel architecture, termed Universal ASR that can unify streaming and non-streaming ASR models into one system. The embedded streaming ASR model can configure different delays according to requirements to obtain real-time recognition results, while the non-streaming model is able to refresh the final recognition result for better performance. We have evaluated our approach on the public AISHELL-2 benchmark and an industrial-level 20,000-hour Mandarin speech recognition task. The experimental results show that the Universal ASR provides an efficient mechanism to integrate streaming and non-streaming models that can recognize speech quickly and accurately. On the AISHELL-2 task, Universal ASR comfortably outperforms other state-of-the-art systems.      
### 61.Hamilton-Jacobi Deep Q-Learning for Deterministic Continuous-Time Systems with Lipschitz Continuous Controls  [ :arrow_down: ](https://arxiv.org/pdf/2010.14087.pdf)
>  In this paper, we propose Q-learning algorithms for continuous-time deterministic optimal control problems with Lipschitz continuous controls. Our method is based on a new class of Hamilton-Jacobi-Bellman (HJB) equations derived from applying the dynamic programming principle to continuous-time Q-functions. A novel semi-discrete version of the HJB equation is proposed to design a Q-learning algorithm that uses data collected in discrete time without discretizing or approximating the system dynamics. We identify the condition under which the Q-function estimated by this algorithm converges to the optimal Q-function. For practical implementation, we propose the Hamilton-Jacobi DQN, which extends the idea of deep Q-networks (DQN) to our continuous control setting. This approach does not require actor networks or numerical solutions to optimization problems for greedy actions since the HJB equation provides a simple characterization of optimal controls via ordinary differential equations. We empirically demonstrate the performance of our method through benchmark tasks and high-dimensional linear-quadratic problems.      
### 62.ByteCover: Cover Song Identification via Multi-Loss Training  [ :arrow_down: ](https://arxiv.org/pdf/2010.14022.pdf)
>  We present in this paper ByteCover, which is a new feature learning method for cover song identification (CSI). ByteCover is built based on the classical ResNet model, and two major improvements are designed to further enhance the capability of the model for CSI. In the first improvement, we introduce the integration of instance normalization (IN) and batch normalization (BN) to build IBN blocks, which are major components of our ResNet-IBN model. With the help of the IBN blocks, our CSI model can learn features that are invariant to the changes of musical attributes such as key, tempo, timbre and genre, while preserving the version information. In the second improvement, we employ the BNNeck method to allow a multi-loss training and encourage our method to jointly optimize a classification loss and a triplet loss, and by this means, the inter-class discrimination and intra-class compactness of cover songs, can be ensured at the same time. A set of experiments demonstrated the effectiveness and efficiency of ByteCover on multiple datasets, and in the Da-TACOS dataset, ByteCover outperformed the best competitive system by 20.9\%.      
### 63.On analytic interpolation with non-classical constraints for solving problems in robust control  [ :arrow_down: ](https://arxiv.org/pdf/2010.14018.pdf)
>  In this work we consider robust stabilization of uncertain dynamical systems and show that this can be achieved by solving a non-classically constrained analytic interpolation problem. In particular, this non-classical constraint confines the range of the interpolant, when evaluated on the imaginary axis, to a frequency-dependent set. By considering a sufficient condition for when this interpolation problem has a solution, we derive an approximate solution algorithm that can also be used for controller synthesis. Finally, the theory is illustrated on a numerical example with a plant with uncertain gain, phase, and output delay.      
### 64.Decentralized Attribution of Generative Models  [ :arrow_down: ](https://arxiv.org/pdf/2010.13974.pdf)
>  There have been growing concerns regarding the fabrication of contents through generative models. This paper investigates the feasibility of decentralized attribution of such models. Given a set of generative models learned from the same dataset, attributability is achieved when a public verification service exists to correctly identify the source models for generated content. Attribution allows tracing of machine-generated content back to its source model, thus facilitating IP-protection and content regulation. Existing attribution methods are non-scalable with respect to the number of models and lack theoretical bounds on attributability. This paper studies decentralized attribution, where provable attributability can be achieved by only requiring each model to be distinguishable from the authentic data. Our major contributions are the derivation of the sufficient conditions for decentralized attribution and the design of keys following these conditions. Specifically, we show that decentralized attribution can be achieved when keys are (1) orthogonal to each other, and (2) belonging to a subspace determined by the data distribution. This result is validated on MNIST and CelebA. Lastly, we use these datasets to examine the trade-off between generation quality and robust attributability against adversarial post-processes.      
