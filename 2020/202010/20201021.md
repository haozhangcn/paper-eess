# ArXiv eess --Wed, 21 Oct 2020
### 1.Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.10504.pdf)
>  We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4%/2.6% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7%/3.3%.      
### 2.Analysis of Secondary Effects in Roadside mmWave Backhaul Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.10479.pdf)
>  To achieve high survivability of mmWave wireless backhaul networks, deploying mmWave nodes on regularly-spaced lampposts in urban environment according to a triangular-wave topology is a promising approach, because the primary interference among the links on a path of mmWave nodes (referred to as self interference) can be eliminated, thereby maximizing end-to-end throughput. Besides, another advantage of this topology is the ability to reconfigure it to avoid obstacles that might occur along the roadway. Based on this network architecture, this work provides detailed analyses on the interference caused by secondary effects, which includes side-lobe effects and reflection effects. Through both analytical modeling and extensive evaluations, we show that the interference caused by secondary effects has only a very small impact on the network performance with the considered backhaul network architecture.      
### 3.Robust State of Health Estimation of Lithium-ion Batteries Using Convolutional Neural Network and Random Forest  [ :arrow_down: ](https://arxiv.org/pdf/2010.10452.pdf)
>  The State of Health (SOH) of lithium-ion batteries is directly related to their safety and efficiency, yet effective assessment of SOH remains challenging for real-world applications (e.g., electric vehicle). In this paper, the estimation of SOH (i.e., capacity fading) under partial discharge with different starting and final State of Charge (SOC) levels is investigated. The challenge lies in the fact that partial discharge truncates the data available for SOH estimation, thereby leading to the loss or distortion of common SOH indicators. To address this challenge associated with partial discharge, we explore the convolutional neural network (CNN) to extract indicators for both SOH and changes in SOH ($\Delta$SOH) between two successive charge/discharge cycles. The random forest algorithm is then adopted to produce the final SOH estimate by exploiting the indicators from the CNNs. Performance evaluation is conducted using the partial discharge data with different SOC ranges created from a fast-discharging dataset. The proposed approach is compared with i) a differential analysis-based approach and ii) two CNN-based approaches using only SOH and $\Delta$SOH indicators, respectively. Through comparison, the proposed approach demonstrates improved estimation accuracy and robustness. Sensitivity analysis of the CNN and random forest models further validates that the proposed approach makes better use of the available partial discharge data for SOH estimation.      
### 4.Distributed Radio Frequency Cooperation at the Wavelength Level Using Wireless Phase Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2010.10396.pdf)
>  Coordinating the operations of separate wireless systems at the wavelength level can lead to significant improvements in wireless capabilities. We address a fundamental challenge in distributed radio frequency system cooperation - inter-node phase alignment - which must be accomplished wirelessly, and is particularly challenging when the nodes are in relative motion. We present a solution to this problem that is based on a novel combined high-accuracy ranging and frequency transfer technique. Using this approach, we present the design of the first fully wireless distributed system operating at the wavelength level. We demonstrate the system in the first open-loop coherent distributed beamforming experiment. Internode range estimation to support phase alignment was performed using a two-tone stepped frequency waveform with a single pulse, while a two-tone waveform was used for frequency synchronization, where the oscillator of a secondary node was disciplined to the primary node. In this concept, secondary nodes are equipped with an adjunct self-mixing circuit that is able to extract the reference frequency from the captured synchronization waveform. The approach was implemented on a two-node dynamic system using Ettus X310 software-defined radios, with coherent beamforming at 1.5 GHz. We demonstrate distributed beamforming with greater than 90% of the maximum possible coherent gain throughout the displacement of the secondary node over one full cycle of the beamforming frequency.      
### 5.Monitoring Large Crowds With WiFi: A Privacy-Preserving Approach  [ :arrow_down: ](https://arxiv.org/pdf/2010.10370.pdf)
>  This paper presents a crowd monitoring system based on the passive detection of probe requests. The system meets strict privacy requirements and is suited to monitoring events or buildings with a least a few hundreds of attendees. We present our counting process and an associated mathematical model. From this model, we derive a concentration inequality that highlights the accuracy of our crowd count estimator. Then, we describe our system. We present and discuss our sensor hardware, our computing system architecture, and an efficient implementation of our counting algorithm---as well as its space and time complexity. We also show how our system ensures the privacy of people in the monitored area. Finally, we validate our system using nine weeks of data from a public library endowed with a camera-based counting system, which generates counts against which we compare those of our counting system. This comparison empirically quantifies the accuracy of our counting system, thereby showing it to be suitable for monitoring public areas. Similarly, the concentration inequality provides a theoretical validation of the system.      
### 6.Performance of Dual-Augmented Lagrangian Method and Common Spatial Patterns applied in classification of Motor-Imagery BCI  [ :arrow_down: ](https://arxiv.org/pdf/2010.10359.pdf)
>  Motor-imagery based brain-computer interfaces (MI-BCI) have the potential to become ground-breaking technologies for neurorehabilitation, the reestablishment of non-muscular communication and commands for patients suffering from neuronal disorders and disabilities, but also outside of clinical practice, for video game control and other entertainment purposes. However, due to the noisy nature of the used EEG signal, reliable BCI systems require specialized procedures for features optimization and extraction. This paper compares the two approaches, the Common Spatial Patterns with Linear Discriminant Analysis classifier (CSP-LDA), widely used in BCI for extracting features in Motor Imagery (MI) tasks, and the Dual-Augmented Lagrangian (DAL) framework with three different regularization methods: group sparsity with row groups (DAL-GLR), dual-spectrum (DAL-DS) and l1-norm regularization (DAL-L1). The test has been performed on 7 healthy subjects performing 5 BCI-MI sessions each. The preliminary results show that DAL-GLR method outperforms standard CSP-LDA, presenting 6.9% lower misclassification error (p-value = 0.008) and demonstrate the advantage of DAL framework for MI-BCI.      
### 7.Automotive Radar Interference Mitigation with Unfolded Robust PCA based on Residual Overcomplete Auto-Encoder Blocks  [ :arrow_down: ](https://arxiv.org/pdf/2010.10357.pdf)
>  Deep learning methods for automotive radar interference mitigation can succesfully estimate the amplitude of targets, but fail to recover the phase of the respective targets. In this paper, we propose an efficient and effective technique based on unfolded robust Principal Component Analysis (RPCA) that is able to estimate both amplitude and phase in the presence of interference. Our contribution consists in introducing residual overcomplete auto-encoder (ROC-AE) blocks into the recurrent architecture of unfolded RPCA, which results in a deeper model that significantly outperforms unfolded RPCA as well as other deep learning models.      
### 8.A Novel Self-Packaged DBBPF With Multiple TZs for 5G Applications  [ :arrow_down: ](https://arxiv.org/pdf/2010.10356.pdf)
>  A self-packaged dual-band bandpass filter (DBBPF) with high isolation and low insertion loss (IL) for 5G applications is proposed in this paper. To get high stopband suppression, multiple and controllable transmission zeros (TZs) are produced. This novel DBBPF is designed with a pair of quarter-wavelength stepped-impedance resonators (QSIRs) and a half-wavelength hairpin resonator (HWHR). This DBBPF is excited by a pair of U-shape feed lines, which are designed on G6 to fully excite the resonators and to introduce source/load TZs at the same time. In this letter, the generation of two passbands and TZs will be discussed by separate electric and magnetic coupling paths (SEMCP) and mixed EM coupling analysis. This DBBPF achieves a low IL of 0.85/1.15 dB with the fractional bandwidths (FBW) of 11.0% and 6.9% at the center frequencies of 3.45 GHz and 4.9 GHz for 5G application, respectively. The total size is 0.32{\lambda}g*0.45{\lambda}g. Especially, three controllable TZs are introduced between two passbands.      
### 9.Time-domain Representation of Passband Scattering Parameters  [ :arrow_down: ](https://arxiv.org/pdf/2010.10354.pdf)
>  This paper presents a simple and accurate method for the inclusion of linear, time-invariant (LTI) networks, described by RF frequency-domain data, within equivalent baseband time-domain simulations. The time-domain representation is formulated as an equivalent baseband discrete-time impulse response, which may be convolved with the equivalent baseband form of the input signal, to obtain the corresponding equivalent baseband output. This allows networks which are most accurately described in the frequency domain, such as frequency-dispersive transmission lines, to be efficiently included as part of a transient time-domain simulation.      
### 10.Online adaptive group-wise sparse NPLS for ECoG neural signal decoding  [ :arrow_down: ](https://arxiv.org/pdf/2010.10353.pdf)
>  Objective. Brain-computer interfaces (BCIs) create a new communication pathway between the brain and an effector without neuromuscular activation. BCI experiments highlighted high intra and inter-subjects variability in the BCI decoders. Although BCI model is generally relying on neurological markers generalizable on the majority of subjects, it requires to generate a wide range of neural features to include possible neurophysiological patterns. However, the processing of noisy and high dimensional features, such as brain signals, brings several challenges to overcome such as model calibration issues, model generalization and interpretation problems and hardware related obstacles. Approach. An online adaptive group-wise sparse decoder named Lp-Penalized REW-NPLS algorithm (PREW-NPLS) is presented to reduce the feature space dimension employed for BCI decoding. The proposed decoder was designed to create BCI systems with low computational cost suited for portable applications and tested during offline pseudo-online study based on online closed-loop BCI control of the left and right 3D arm movements of a virtual avatar from the ECoG recordings of a tetraplegic patient. <br>Main results. PREW-NPLS algorithm highlight at least as good decoding performance as REW-NPLS algorithm. However, the decoding performance obtained with PREW-NPLS were achieved thanks to sparse models with up to 64% and 75% of the electrodes set to 0 for the left and right hand models respectively using L1-PREW-NPLS. <br>Significance. The designed solution proposed an online incremental adaptive algorithm suitable for online adaptive decoder calibration which estimate sparse decoding solutions. The PREW-NPLS models are suited for portable applications with low computational power using only small number of electrodes with degrading the decoding performance.      
### 11.Deep Learning for Surface Wave Identification in Distributed Acoustic Sensing Data  [ :arrow_down: ](https://arxiv.org/pdf/2010.10352.pdf)
>  Moving loads such as cars and trains are very useful sources of seismic waves, which can be analyzed to retrieve information on the seismic velocity of subsurface materials using the techniques of ambient noise seismology. This information is valuable for a variety of applications such as geotechnical characterization of the near-surface, seismic hazard evaluation, and groundwater monitoring. However, for such processes to converge quickly, data segments with appropriate noise energy should be selected. Distributed Acoustic Sensing (DAS) is a novel sensing technique that enables acquisition of these data at very high spatial and temporal resolution for tens of kilometers. One major challenge when utilizing the DAS technology is the large volume of data that is produced, thereby presenting a significant Big Data challenge to find regions of useful energy. In this work, we present a highly scalable and efficient approach to process real, complex DAS data by integrating physics knowledge acquired during a data exploration phase followed by deep supervised learning to identify "useful" coherent surface waves generated by anthropogenic activity, a class of seismic waves that is abundant on these recordings and is useful for geophysical imaging. Data exploration and training were done on 130~Gigabytes (GB) of DAS measurements. Using parallel computing, we were able to do inference on an additional 170~GB of data (or the equivalent of 10 days' worth of recordings) in less than 30 minutes. Our method provides interpretable patterns describing the interaction of ground-based human activities with the buried sensors.      
### 12.End-to-end Wireless Path Deployment with Intelligent Surfaces Using Interpretable Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.10351.pdf)
>  Intelligent surfaces exert deterministic control over the wireless propagation phenomenon, enabling novel capabilities in performance, security and wireless power transfer. Such surfaces come in the form of rectangular tiles that cascade to cover large surfaces such as walls, ceilings or building facades. Each tile is addressable and can receive software commands from a controller, manipulating an impinging electromagnetic wave upon it by customizing its reflection direction, focus, polarization and phase. A new problem arises concerning the orchestration of a set of tiles towards serving end-to-end communication objectives. Towards that end, we propose a novel intelligent surface networking algorithm based on interpretable neural networks. Tiles are mapped to neural network nodes and any tile line-of-sight connectivity is expressed as a neural network link. Tile wave manipulation functionalities are captured via geometric reflection with virtually rotatable tile surface norm, thus being able to tunable distribute power impinging upon a tile over the corresponding neural network links, with the corresponding power parts acting as the link weights. A feedforward/backpropagate process optimizes these weights to match ideal propagation outcomes (normalized network power outputs) to wireless user emissions (normalized network power inputs). An interpretation process translates these weights to the corresponding tile wave manipulation functionalities.      
### 13.1.23-Tb/s per Wavelength Single-Waveguide On-Chip Optical Interconnect Enabled by Mode-division Multiplexing  [ :arrow_down: ](https://arxiv.org/pdf/2010.10348.pdf)
>  We experimentally demonstrate a record net capacity per wavelength of 1.23~Tb/s over a single silicon-on-insulator (SOI) multimode waveguide for optical interconnects employing on-chip mode-division multiplexing and 11$\times$11 multiple-in-multiple-out (MIMO) digital signal processing.      
### 14.Sub-Optimality of a Dyadic Adaptive Control Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2010.10329.pdf)
>  The dyadic adaptive control architecture evolved as a solution to the problem of designing control laws for nonlinear systems with unmatched nonlinearities, disturbances and uncertainties. A salient feature of this framework is its ability to work with infinite as well as finite dimensional systems, and with a wide range of control and adaptive laws. In this paper, we consider the case where a control law based on the linear quadratic regulator theory is employed for designing the control law. We benchmark the closed-loop system against standard linear quadratic control laws as well as those based on the state-dependent Riccati equation. We pose the problem of designing a part of the control law as a Nehari problem. We obtain analytical expressions for the bounds on the sub-optimality of the control law.      
### 15.Can Steering Wheel Detect Your Driving Fatigue?  [ :arrow_down: ](https://arxiv.org/pdf/2010.10327.pdf)
>  Automated Driving System (ADS) has attracted increasing attention from both industrial and academic communities due to its potential for increasing the safety, mobility and efficiency of existing transportation systems. The state-of-the-art ADS follows the human-in-the-loop (HITL) design, where the driver's anomalous behaviour is closely monitored by the system. Though many approaches have been proposed for detecting driver fatigue, they largely depend on vehicle driving parameters and facial features, which lacks reliability. Approaches using physiological based sensors (e.g., electroencephalogram or electrocardiogram) are either too clumsy to wear or impractical to install. In this paper, we propose a novel driver fatigue detection method by embedding surface electromyography (sEMG) sensors on a steering wheel. Compared with the existing methods, our approach is able to collect bio-signals in a non-intrusive way and detect driver fatigue at an earlier stage. The experimental results show that our approach outperforms existing methods with the weighted average F1 scores about 90%. We also propose promising future directions to deploy this approach in real-life settings, such as applying multimodal learning using several supplementary sensors.      
### 16.White noise jammer mathematical modelling and simulation  [ :arrow_down: ](https://arxiv.org/pdf/2010.10301.pdf)
>  The radar equation is the fundamental mathematical modelling of the basic function of a radar system. Moreover there are many versions of the radar equation that correspond to particular radar operations, like a low PRF, a high PRF or a surveillance mode. In many cases all these expressions of the radar equation exist in their combined forms giving little information to the actual physics and signal geometry between the radar and target involved in the process. In this case study we divide the radar equation into its major steps and present a descriptive mathematical modelling of the radar and other related equations utilizing the free space loss and the target gain concepts for simulating the effect of a white noise jammer to an adversary radar. We believe that this work will be particularly beneficial to instructors of radar courses and to radar simulation engineers because of its analytical block approach of the main equations related to radar and electronic warfare fields. Finally this work falls under the field of predictive dynamics for radar systems using mathematical modelling techniques.      
### 17.The Detection of Thoracic Abnormalities ChestX-Det10 Challenge Results  [ :arrow_down: ](https://arxiv.org/pdf/2010.10298.pdf)
>  The detection of thoracic abnormalities challenge is organized by the Deepwise AI Lab. The challenge is divided into two rounds. In this paper, we present the results of 6 teams which reach the second round. The challenge adopts the ChestX-Det10 dateset proposed by the Deepwise AI Lab. ChestX-Det10 is the first chest X-Ray dataset with instance-level annotations, including 10 categories of disease/abnormality of 3,543 images. We randomly split all data into 3001 images for training and 542 images for testing.      
### 18.Fisheye lens distortion correction  [ :arrow_down: ](https://arxiv.org/pdf/2010.10295.pdf)
>  A new distortion correction algorithm for fisheye lens with equidistant mapping function is considered in the present study. The algorithm is much more data lossless and accurate than such a classical approach like Brown-Conrady model      
### 19.Automatic multitrack mixing with a differentiable mixing console of neural audio effects  [ :arrow_down: ](https://arxiv.org/pdf/2010.10291.pdf)
>  Applications of deep learning to automatic multitrack mixing are largely unexplored. This is partly due to the limited available data, coupled with the fact that such data is relatively unstructured and variable. To address these challenges, we propose a domain-inspired model with a strong inductive bias for the mixing task. We achieve this with the application of pre-trained sub-networks and weight sharing, as well as with a sum/difference stereo loss function. The proposed model can be trained with a limited number of examples, is permutation invariant with respect to the input ordering, and places no limit on the number of input sources. Furthermore, it produces human-readable mixing parameters, allowing users to manually adjust or refine the generated mix. Results from a perceptual evaluation involving audio engineers indicate that our approach generates mixes that outperform baseline approaches. To the best of our knowledge, this work demonstrates the first approach in learning multitrack mixing conventions from real-world data at the waveform level, without knowledge of the underlying mixing parameters.      
### 20.Towards new forms of particle sensing and manipulation and 3D imaging on a smartphone for healthcare applications  [ :arrow_down: ](https://arxiv.org/pdf/2010.10269.pdf)
>  Close to half of the world population have smartphones, while a typical flagship smartphone today has been integrated with more than 20 smart components and sensors, making a smartphone a highly integrated platform that can potentially mimic the five senses of humans. Recent advancement in achieving high compactness, high performance computing, high flexibility, and multiplexed functionality in smartphones have enabled them for many cutting-edge healthcare applications, such as single-molecule imaging, medical diagnosis, and biosensing, which were conventionally done with bulky and sophisticated devices. Most of the current healthcare applications are developed based on using the photon-sensitive components, such as CMOS sensors, flash &amp; fill lights, lens modules, and LED lights in the screen, leaving the rest of the smart and high-performance sensors rarely explored. In this Perspective, we review recent progresses in advanced sensors in modern smartphones and discuss how those sensors have great, as yet unmet, promise to offer widespread and easy-to-implement solutions to many emerging healthcare applications, including nanoscale sensing, point-of-care testing, pollution monitoring, etc.      
### 21.Synthesis of COVID-19 Chest X-rays using Unpaired Image-to-Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2010.10266.pdf)
>  Motivated by the lack of publicly available datasets of chest radiographs of positive patients with Coronavirus disease 2019 (COVID-19), we build the first-of-its-kind open dataset of synthetic COVID-19 chest X-ray images of high fidelity using an unsupervised domain adaptation approach by leveraging class conditioning and adversarial training. Our contributions are twofold. First, we show considerable performance improvements on COVID-19 detection using various deep learning architectures when employing synthetic images as additional training set. Second, we show how our image synthesis method can serve as a data anonymization tool by achieving comparable detection performance when trained only on synthetic data. In addition, the proposed data generation framework offers a viable solution to the COVID-19 detection in particular, and to medical image classification tasks in general. Our publicly available benchmark dataset consists of 21,295 synthetic COVID-19 chest X-ray images. The insights gleaned from this dataset can be used for preventive actions in the fight against the COVID-19 pandemic.      
### 22.Hierarchical Autoregressive Modeling for Neural Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2010.10258.pdf)
>  Recent work by Marino et al. (2020) showed improved performance in sequential density estimation by combining masked autoregressive flows with hierarchical latent variable models. We draw a connection between such autoregressive generative models and the task of lossy video compression. Specifically, we view recent neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal autoregressive trans-form, and propose avenues for enhancement based on this insight. Comprehensive evaluations on large-scale video data show improved rate-distortion performance over both state-of-the-art neural and conventional video compression methods.      
### 23.Quality of service based radar resource management using deep reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.10210.pdf)
>  An intelligent radar resource management is an essential milestone in the development of a cognitive radar system. The quality of service based resource allocation model (Q-RAM) is a framework allowing for intelligent decision making but classical solutions seem insufficient for real-time application in a modern radar system. In this paper, we present a solution for the Q-RAM radar resource management problem using deep reinforcement learning considerably improving on runtime performance.      
### 24.Identification of The Number of Wireless Channel Taps Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.10193.pdf)
>  In wireless communication systems, identifying the number of channel taps offers an enhanced estimation of the channel impulse response (CIR). In this work, efficient identification of the number of wireless channel taps has been achieved via deep neural networks (DNNs), where we modified an existing DNN and analyzed its convergence performance using only the transmitted and received signals of a wireless system. The displayed results demonstrate that the adopted DNN accomplishes superior performance in identifying the number of channel taps, as compared to an existing algorithm called Spectrum Weighted Identification of Signal Sources (SWISS).      
### 25.Opacity Enforcing Supervisory Control using Non-deterministic Supervisors  [ :arrow_down: ](https://arxiv.org/pdf/2010.10149.pdf)
>  In this paper, we investigate the enforcement of opacity via supervisory control in the context of discrete-event systems. A system is said to be opaque if the intruder, which is modeled as a passive observer, can never infer confidentially that the system is at a secret state. The design objective is to synthesize a supervisor such that the closed-loop system is opaque even when the control policy is publicly known. In this paper, we propose a new approach for enforcing opacity using non-deterministic supervisors. A non-deterministic supervisor is a decision mechanism that provides a set of control decisions at each instant, and randomly picks a specific control decision from the decision set to actually control the plant. Compared with the standard deterministic control mechanism, such a non-deterministic control mechanism can enhance the plausible deniability of the controlled system as the online control decision is a random realization and cannot be implicitly inferred from the control policy. We provide a sound and complete algorithm for synthesizing a non-deterministic opacity-enforcing supervisor. Furthermore, we show that non-deterministic supervisors are strictly more powerful than deterministic supervisors in the sense that there may exist a non-deterministic opacity-enforcing supervisor even when deterministic supervisors cannot enforce opacity.      
### 26.University Operations During a Pandemic: A Flexible Decision Analysis Toolkit  [ :arrow_down: ](https://arxiv.org/pdf/2010.10112.pdf)
>  Modeling infection spread during pandemics is not new, with models using past data to tune simulation parameters for predictions. These help understand the healthcare burden posed by a pandemic and respond accordingly. However, the problem of how college/university campuses should function during a pandemic is new for the following reasons:(i) social contact in colleges are structured and can be engineered for chosen objectives, (ii) the last pandemic to cause such societal disruption was over 100 years ago, when higher education was not a critical part of society, (ii) not much was known about causes of pandemics, and hence effective ways of safe operations were not known, and (iii) today with distance learning, remote operation of an academic institution is possible. Our approach is unique in presenting a flexible simulation system, containing a suite of model libraries, one for each major component. The system integrates agent based modeling (ABM) and stochastic network approach, and models the interactions among individual entities, e.g., students, instructors, classrooms, residences, etc. in great detail. For each decision to be made, the system can be used to predict the impact of various choices, and thus enable the administrator to make informed decisions. While current approaches are good for infection modeling, they lack accuracy in social contact modeling. Our ABM approach, combined with ideas from Network Science, presents a novel approach to contact modeling. A detailed case study of the University of Minnesota's Sunrise Plan is presented. For each decisions made, its impact was assessed, and results used to get a measure of confidence. We believe this flexible tool can be a valuable asset for various kinds of organizations to assess their infection risks in pandemic-time operations, including middle and high schools, factories, warehouses, and small/medium sized businesses.      
### 27.Modified QPSK Partition Algorithm Based on MAP Estimation for Probabilistically-Shaped 16-QAM  [ :arrow_down: ](https://arxiv.org/pdf/2010.10106.pdf)
>  Probabilistic shaping (PS) is investigated as a potential technique to approach the Shannon limit. However, it has been proved that conventional carrier phase recovery (CPR) algorithm designed for uniform distribution may have extra penalty in PS systems. In this paper, we find that the performance of QPSK partition algorithm is degenerated when PS is implemented. To solve this issue, a modified QPSK partition algorithm that jointly optimizes the amplitude decision threshold and filter weight is proposed, where the optimization of decision threshold is based on maximum a posterior probability (MAP) estimation. Different from the conventional decision methods which commonly use Euclidean distance metric, the MAP-based decision introduces the statistical characteristics of the received signals to obtain an accurate amplitude partition. In addition, the filter weight is optimized for different decision thresholds to enhance the tolerance of ASE-induced phase noise. We verify the feasibility of the proposed algorithm in a 56 GBaud PS 16-ary quadrature amplitude modulation (16-QAM) system. The proposed algorithm reduces the error of phase noise estimation by nearly half. Compared with conventional QPSK partition, the proposed algorithm could narrow the gap with theoretical mutual information (MI) by more than 0.1 bit/symbol. The channel capacity is increased by 4.2%, 4.3% and 3.6% with signal-to-noise ratio (SNR) from 8 dB to 10 dB respectively. These observations show that the proposed algorithm is a promising method to relieve the penalty of QPSK partition algorithm in PS systems.      
### 28.Analysis of Markov Jump Processes under Terminal Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2010.10096.pdf)
>  Many probabilistic inference problems such as stochastic filtering or the computation of rare event probabilities require model analysis under initial and terminal constraints. We propose a solution to this bridging problem for the widely used class of population-structured Markov jump processes. The method is based on a state-space lumping scheme that aggregates states in a grid structure. The resulting approximate bridging distribution is used to iteratively refine relevant and truncate irrelevant parts of the state-space. This way the algorithm learns a well-justified finite-state projection yielding guaranteed lower bounds for the system behavior under endpoint constraints. We demonstrate the method's applicability to a wide range of problems such as Bayesian inference and the analysis of rare events.      
### 29.Deep Learning-Based Optimal RIS Interaction Exploiting Previously Sampled Channel Correlations  [ :arrow_down: ](https://arxiv.org/pdf/2010.10087.pdf)
>  The reconfigurable intelligent surface (RIS) technology has attracted interest due to its promising coverage and spectral efficiency features. However, some challenges need to be addressed to realize this technology in practice. One of the main challenges is the configuration of reflecting coefficients without the need for beam training overhead or massive channel estimation. Earlier works used estimated channel information with deep learning algorithms to design RIS reflection matrices. Although these works can reduce the beam training overhead, still they overlook existing correlations in the previously sampled channels. In this paper, different from existing works, we propose to exploit the correlation in the previously sampled channels to estimate RIS interaction more reliably. We use a deep multi-layer perceptron for this purpose. Simulation results reveal performance improvements achieved by the proposed algorithm.      
### 30.Fractional-order Modeling of the Arterial Compliance: An Alternative Surrogate Measure of the Arterial Stiffness  [ :arrow_down: ](https://arxiv.org/pdf/2010.10058.pdf)
>  Recent studies have demonstrated the advantages of fractional-order calculus tools for probing the viscoelastic properties of collagenous tissue, characterizing the arterial blood flow and red cell membrane mechanics, and modeling the aortic valve cusp. In this article, we present a novel lumped-parameter equivalent circuit models of the apparent arterial compliance using a fractional-order capacitor (FOC). FOC, which generalizes capacitors and resistors, displays a fractional-order behavior that can capture both elastic and viscous properties through a power-law formulation. The proposed framework describes the dynamic relationship between the blood pressure input and blood volume, using linear fractional-order differential equations. The results show that the proposed models present reasonable fit performance with in-silico data of more than 4,000 subjects. Additionally, strong correlations have been identified between the fractional-order parameter estimates and the central hemodynamic determinants as well as pulse wave velocity indexes. Therefore, fractional-order based paradigm of arterial compliance shows prominent potential as an alternative tool in the analysis of arterial stiffness.      
### 31.An Efficient Algorithm for Device Detection and Channel Estimation in Asynchronous IoT Systems  [ :arrow_down: ](https://arxiv.org/pdf/2010.09979.pdf)
>  A great amount of endeavour has recently been devoted to the joint device activity detection and channel estimation problem in massive machine-type communications. This paper targets at two practical issues along this line that have not been addressed before: asynchronous transmission from uncoordinated users and efficient algorithms for real-time implementation in systems with a massive number of devices. Specifically, this paper considers a practical system where the preamble sent by each active device is delayed by some unknown number of symbols due to the lack of coordination. We manage to cast the problem of detecting the active devices and estimating their delay and channels into a group LASSO problem. Then, a block coordinate descent algorithm is proposed to solve this problem globally, where the closed-form solution is available when updating each block of variables with the other blocks of variables being fixed, thanks to the special structure of our interested problem. Our analysis shows that the overall complexity of the proposed algorithm is low, making it suitable for real-time application.      
### 32.Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution  [ :arrow_down: ](https://arxiv.org/pdf/2010.09960.pdf)
>  Keyword Spotting (KWS) plays a vital role in human-computer interaction for smart on-device terminals and service robots. It remains challenging to achieve the trade-off between small footprint and high accuracy for KWS task. In this paper, we explore the application of multi-scale temporal modeling to the small-footprint keyword spotting task. We propose a multi-branch temporal convolution module (MTConv), a CNN block consisting of multiple temporal convolution filters with different kernel sizes, which enriches temporal feature space. Besides, taking advantage of temporal and depthwise convolution, a temporal efficient neural network (TENet) is designed for KWS system. Based on the purposed model, we replace standard temporal convolution layers with MTConvs that can be trained for better performance. While at the inference stage, the MTConv can be equivalently converted to the base convolution architecture, so that no extra parameters and computational costs are added compared to the base model. The results on Google Speech Command Dataset show that one of our models trained with MTConv performs the accuracy of 96.8% with only 100K parameters.      
### 33.Sampling theory of bandlimited continuous-time graph signals  [ :arrow_down: ](https://arxiv.org/pdf/2010.09952.pdf)
>  A continuous-time graph signal can be viewed as a continuous time-series of graph signals. It generalizes both the classical continuous-time signal and ordinary graph signal. In this paper, we consider the sampling theory of bandlimited continuous-time graph signals. We describe an explicit procedure to determine a discrete sampling set for perfect signal recovery. Moreover, in analogous to the Nyquist-Shannon sampling theorem, we give an explicit formula for the minimal sample rate.      
### 34.Metrics for Aerial, Urban LiDAR Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2010.09951.pdf)
>  This paper introduces new density and accuracy metrics for aerial point clouds that address the complexity and objectives of modern, dense laser scans of urban scenes. Specifically considered is the practice of overlapping flight passes to reduce the occlusions and achieve the vertical density needed for twenty-first-century use cases (e.g. curb and window detection). The application of these metrics to a quartet of recent urban flyovers demonstrates their relevance by establishing (1) the efficacy of considering sensor position and wall height when predicting vertical density; (2) that cross-pass registration accounts for a disproportionate amount of the vertical surface error (but not horizontal) and provides a meaningful parameter to compare high-density, urban point clouds; and (3) that compared to horizontal density and accuracy, the vertical counterparts are disproportionately impacted (positively for density and negatively for accuracy) in modern, optimized flight missions.      
### 35.Planning a Reference Constellation for Radiometric Cross-Calibration of Commercial Earth Observing Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2010.09946.pdf)
>  The Earth Observation planning community has access to tools that can propagate orbits and compute coverage of Earth observing imagers with customizable shapes and orientation, model the expected Earth Reflectance at various bands, epochs and directions, generate simplified instrument performance metrics for imagers and radars, and schedule single and multiple spacecraft payload operations. We are working toward integrating existing tools to design a planner that allows commercial small spacecraft to assess the opportunities for cross-calibration of their sensors against current satellite to be calibrated, specifications of the reference instruments, sensor stability, allowable latency between calibration measurements, differences in viewing and solar geometry between calibration measurements, etc. The planner would output cross-calibration opportunities for every reference target pair as a function of flexible user-defined parameters. We use a preliminary version of this planner to inform the design of a constellation of transfer radiometers that can serve as stable, radiometric references for commercial sensors to cross-calibrate with. We propose such a constellation for either vicarious cross-calibration using pre-selected sites, or top of the atmosphere (TOA) cross-calibration globally. Results from the calibration planner applied to a subset of informed architecture designs show that a 4 sat constellation provides multiple calibration opportunities within half a day planning horizon, for Cubesat sensors deployed into a typical rideshare orbits. While such opportunities are available for cross calibration image pairs within 5 deg of solar or view directions, and with-in an hour (for TOA) and less than a day (vicariously), the planner allows us to identify many more by relaxing user-defined restrictions.      
### 36.Autonomous Scheduling of Agile Spacecraft Constellations with Delay Tolerant Networking for Reactive Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.09940.pdf)
>  Small spacecraft now have precise attitude control systems available commercially, allowing them to slew in 3 degrees of freedom, and capture images within short notice. When combined with appropriate software, this agility can significantly increase response rate, revisit time and coverage. In prior work, we have demonstrated an algorithmic framework that combines orbital mechanics, attitude control and scheduling optimization to plan the time-varying, full-body orientation of agile, small spacecraft in a constellation. The proposed schedule optimization would run at the ground station autonomously, and the resultant schedules uplinked to the spacecraft for execution. The algorithm is generalizable over small steerable spacecraft, control capability, sensor specs, imaging requirements, and regions of interest. In this article, we modify the algorithm to run onboard small spacecraft, such that the constellation can make time-sensitive decisions to slew and capture images autonomously, without ground control. We have developed a communication module based on Delay/Disruption Tolerant Networking (DTN) for onboard data management and routing among the satellites, which will work in conjunction with the other modules to optimize the schedule of agile communication and steering. We then apply this preliminary framework on representative constellations to simulate targeted measurements of episodic precipitation events and subsequent urban floods. The command and control efficiency of our agile algorithm is compared to non-agile (11.3x improvement) and non-DTN (21% improvement) constellations.      
### 37.Circular Convolution and Product Theorem for Affine Discrete Fractional Fourier Transform  [ :arrow_down: ](https://arxiv.org/pdf/2010.09882.pdf)
>  The Fractional Fourier Transform is a ubiquitous signal processing tool in basic and applied sciences. The Fractional Fourier Transform generalizes every property and application of the Fourier Transform. Despite the practical importance of the discrete fractional Fourier transform, its applications in digital communications have been elusive. The convolution property of the discrete Fourier transform plays a vital role in designing multi-carrier modulation systems. Here we report a closed-form affine discrete fractional Fourier transform and we show the circular convolution property for it. The proposed approach is versatile and generalizes the discrete Fourier transform and can find applications in Fourier based signal processing tools.      
### 38.Compressing Colour Images with Joint Inpainting and Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2010.09866.pdf)
>  Inpainting-based codecs store sparse, quantised pixel data directly and decode by interpolating the discarded image parts. This interpolation can be used simultaneously for efficient coding by predicting pixel data to be stored. Such joint inpainting and prediction approaches yield good results with simple components such as regular grids and Shepard interpolation on grey value images, but they lack a dedicated mode for colour images. Therefore, we evaluate different approaches for inpainting-based colour compression. Inpainting operators are able to reconstruct a large range of colours from a small colour palette of the known pixels. We exploit this with a luma preference mode which uses higher sparsity in YCbCr colour channels than in the brightness channel. Furthermore, we propose the first full vector quantisation mode for an inpainting-based codec that stores only a small codebook of colours. Our experiments reveal that both colour extensions yield significant improvements.      
### 39.Cramer-Rao Lower Bounds for Visible Light Communication based Vehicle Localization Methods  [ :arrow_down: ](https://arxiv.org/pdf/2010.09858.pdf)
>  Recent works on visible light communication (VLC) based vehicle localization methods using VLC signals from head/tail LED lights for positioning report cm-level accuracy and near-kHz rates, which can enable collision avoidance and platooning for safer autonomous driving. However, existing analyses of these methods are not comparable and are inconclusive since they assume different system models and consider limited simulations. In this paper, we analyze the theoretical performances of four state-of-the-art (SoA) methods which measure physical VLC system parameters (e.g., propagation distance) using phase-difference-of-arrival (PDoA), roundtrip-time-of-flight (RToF), and single/dual receiver angle-of-arrival (AoA1/AoA2) characteristics of received signals respectively, and estimate position based on these parameters and geometric relations (e.g., triangulation). Specifically, we derive the Cramer-Rao lower bound (CRLB) on positioning accuracy for each method with respect to measured parameters, simulate their parameter measurement procedures under realistic driving scenarios, and evaluate their CRLBs using these simulated measurements. Results show that the current SoA PDoA based method fails to provide cm-level accuracy under any realistic scenario considering the &lt;1 MHz bandwidth of current head/tail LED lights. The AoA1 method provides cm-level accuracy for close range platooning, but its high-sensitivity geometry causes accuracy degradation at high distances. RToF and AoA2 methods provide cm-level accuracy under low to moderate VLC channel noise for all scenarios within 10 m distance, theoretically proving the feasibility of VLC based vehicle localization for collision avoidance and platooning.      
### 40.Social Hierarchy-based Distributed Economic Model Predictive Control of Floating Offshore Wind Farms  [ :arrow_down: ](https://arxiv.org/pdf/2010.09814.pdf)
>  This paper implements a recently developed social hierarchy-based distributed economic model predictive control (DEMPC) algorithm in floating offshore wind farms for the purpose of power maximization. The controller achieves this objective using the concept of yaw and induction-based turbine repositioning (YITuR), which minimizes the overlap areas between adjacent floating wind turbine rotors in real-time to minimize the wake effect. Floating wind farm dynamics and performance are predicted numerically using FOWFSim-Dyn. To ensure fast decision-making by the DEMPC algorithm, feed-forward neural networks are used to estimate floating wind turbine dynamics during the process of dynamic optimization. For simulated wind farms with layouts ranging from 1-by-2 to 1-by-5, an increase of 20% in energy production is predicted when using YITuR instead of greedy operation. Increased variability in wind speed and direction is also studied and is shown to diminish controller performance due to rising errors in neural network predictions.      
### 41.Low-frequency Selection Switch based Cell-to-Cell Battery Voltage Equalizer with Reduced Switch Count  [ :arrow_down: ](https://arxiv.org/pdf/2010.09789.pdf)
>  A selection switch based cell-to-cell voltage equalizer requires only one dual-port dc-dc converter shared by all the cells. A cell-to-cell voltage equalizer is proposed that utilizes a capacitively level-shifted Cuk converter and low-frequency cell selection switches. The absence of isolation transformer and diodes in the equalizer leads to high efficiency, and the use of low-frequency selection switches significantly reduces the cost of the drive circuits. A low-frequency cell selection network is proposed using bipolar voltage buses, where the switch count is almost half, compared to the existing low-frequency cell-to-cell equalizers for the case of a large number of cells. A novel approach for cell voltage recovery compensation is proposed, which reduces the number of operations of the selection switches and the equalization time. The proposed equalizer is implemented with relays and verified with an 8-cell Li-ion stack. The developed prototype shows the efficiency of over 90\% and good voltage balancing performance during charging, discharging, and varying load conditions. Experimental results also show about one order of magnitude reduction in the number of relay switchings and a significant reduction in equalization time using the proposed voltage compensation.      
### 42.Regret-optimal control in dynamic environments  [ :arrow_down: ](https://arxiv.org/pdf/2010.10473.pdf)
>  We consider the control of linear time-varying dynamical systems from the perspective of regret minimization. Unlike most prior work in this area, we focus on the problem of designing an online controller which competes with the best dynamic sequence of control actions selected in hindsight, instead of the best controller in some specific class of controllers. This formulation is attractive when the environment changes over time and no single controller achieves good performance over the entire time horizon. We derive the structure of the regret-optimal online controller via a novel reduction to $H_{\infty}$ control and present a clean data-dependent bound on its regret. We also present numerical simulations which confirm that our regret-optimal controller significantly outperforms the $H_2$ and $H_{\infty}$ controllers in dynamic environments.      
### 43.Investigating Cross-Domain Losses for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2010.10468.pdf)
>  Recent years have seen a surge in the number of available frameworks for speech enhancement (SE) and recognition. Whether model-based or constructed via deep learning, these frameworks often rely in isolation on either time-domain signals or time-frequency (TF) representations of speech data. In this study, we investigate the advantages of each set of approaches by separately examining their impact on speech intelligibility and quality. Furthermore, we combine the fragmented benefits of time-domain and TF speech representations by introducing two new cross-domain SE frameworks. A quantitative comparative analysis against recent model-based and deep learning SE approaches is performed to illustrate the merit of the proposed frameworks.      
### 44.Compressed Super-Resolution of Positive Sources  [ :arrow_down: ](https://arxiv.org/pdf/2010.10461.pdf)
>  Atomic norm minimization is a convex optimization framework to recover point sources from a subset of their low-pass observations, or equivalently the underlying frequencies of a spectrally-sparse signal. When the amplitudes of the sources are positive, a positive atomic norm can be formulated, and exact recovery can be ensured without imposing a separation between the sources, as long as the number of observations is greater than the number of sources. However, the classic formulation of the atomic norm requires to solve a semidefinite program involving a linear matrix inequality of a size on the order of the signal dimension, which can be prohibitive. In this letter, we introduce a novel "compressed" semidefinite program, which involves a linear matrix inequality of a reduced dimension on the order of the number of sources. We guarantee the tightness of this program under certain conditions on the operator involved in the dimensionality reduction. Finally, we apply the proposed method to direction finding over sparse arrays based on second-order statistics and achieve significant computational savings.      
### 45.Distributed ADMM over directed graphs  [ :arrow_down: ](https://arxiv.org/pdf/2010.10421.pdf)
>  We consider the problem of minimizing the sum of convex functions, where each function is privately known to an agent which is part of a communication network. We consider the case where the communication links in the network are directed. Assuming that the network is strongly connected and the objective functions are strongly convex with Lipschitz-continuous gradients, we propose an ADMM algorithm to solve the optimization problem in a distributed manner. We show that if the parameters of the algorithm are chosen appropriately, then the primal-dual iterates of the algorithm converge to their unique optimal points at a geometric rate. Through numerical examples, we observe that the performance of our algorithm is comparable with some state-of-the-art algorithms for solving distributed optimization problems over directed graphs. In particular, we observe that its performance is superior to a distributed dual-ascent algorithm proposed recently.      
### 46.Interpretable Deep Learning for Automatic Diagnosis of 12-lead Electrocardiogram  [ :arrow_down: ](https://arxiv.org/pdf/2010.10328.pdf)
>  Electrocardiogram (ECG) is a widely used reliable, non-invasive approach for cardiovascular disease diagnosis. With the rapid growth of ECG examinations and the insufficiency of cardiologists, accurate and automatic diagnosis of ECG signals has become a hot research topic. Deep learning methods have demonstrated promising results in predictive healthcare tasks. In this paper, we developed a deep neural network for multi-label classification of cardiac arrhythmias in 12-lead ECG recordings. Experiments on a public 12-lead ECG dataset showed the effectiveness of our method. The proposed model achieved an average area under the receiver operating characteristic curve (AUC) of 0.970 and an average F1 score of 0.813. The deep model showed superior performance than 4 machine learning methods learned from extracted expert features. Besides, the deep models trained on single-lead ECGs produce lower performance than using all 12 leads simultaneously. The best-performing leads are lead I, aVR, and V5 among 12 leads. Finally, we employed the SHapley Additive exPlanations (SHAP) method to interpret the model's behavior at both patient level and population level. Our code is freely available at <a class="link-external link-https" href="https://github.com/onlyzdd/ecg-diagnosis" rel="external noopener nofollow">this https URL</a>.      
### 47.User-Number Threshold-based Base Station On/Off Control for Maximizing Coverage Probability  [ :arrow_down: ](https://arxiv.org/pdf/2010.10282.pdf)
>  In this study, we investigate the operation of user-number threshold-based base station (BS) on/off control, in which the BS turns off when the number of active users is less than a specific threshold value. This paper presents a space-based analysis of the BS on/off control system to which a stochastic geometric approach is applied. In particular, we derive the approximated closed-form expression of the coverage probability of a homogeneous network (HomNet) with the user-number threshold-based on/off control. Moreover, the optimal user-number threshold for maximizing the coverage probability is analytically derived. In addition to HomNet, we also derive the overall coverage probability and the optimal user-number thresholds for a heterogeneous network (HetNet). The results show that HetNet, the analysis of which seems intractable, can be analyzed in the form of a linear combination of HomNets with weighted densities. In addition, the optimal user-number threshold of each tier is obtained independently of other tiers. The modeling and analysis presented in this paper are not only limited to the case of user-number threshold-based on/off control, but also applicable to other novel on/off controls with minor modifications. Finally, by comparing with the simulated results, the theoretical contributions of this study are validated.      
### 48.Leveraging the structure of musical preference in content-aware music recommendation  [ :arrow_down: ](https://arxiv.org/pdf/2010.10276.pdf)
>  State-of-the-art music recommendation systems are based on collaborative filtering, which predicts a user's interest from his listening habits and similarities with other users' profiles. These approaches are agnostic to the song content, and therefore face the cold-start problem: they cannot recommend novel songs without listening history. To tackle this issue, content-aware recommendation incorporates information about the songs that can be used for recommending new items. Most methods falling in this category exploit either user-annotated tags, acoustic features or deeply-learned features. Consequently, these content features do not have a clear musicological meaning, thus they are not necessarily relevant from a musical preference perspective. In this work, we propose instead to leverage a model of musical preference which originates from the field of music psychology. From low-level acoustic features we extract three factors (arousal, valence and depth), which accurately describe musical taste. Then we integrate those into a collaborative filtering framework for content-aware music recommendation. Experiments conducted on large-scale data show that this approach is able to address the cold-start problem, while using a compact and meaningful set of musical features.      
### 49.Phase recovery with Bregman divergences for audio source separation  [ :arrow_down: ](https://arxiv.org/pdf/2010.10255.pdf)
>  Time-frequency audio source separation is usually achieved by estimating the short-time Fourier transform (STFT) magnitude of each source, and then applying a phase recovery algorithm to retrieve time-domain signals. In particular, the multiple input spectrogram inversion (MISI) algorithm has shown good performance in several recent works. This algorithm minimizes a quadratic reconstruction error between magnitude spectrograms. However, this loss does not properly account for some perceptual properties of audio, and alternative discrepancy measures such as beta-divergences have been preferred in many settings. In this paper, we propose to reformulate phase recovery in audio source separation as a minimization problem involving Bregman divergences. To optimize the resulting objective, we derive a projected gradient descent algorithm. Experiments conducted on a speech enhancement task show that this approach outperforms MISI for several alternative losses, which highlights their relevance for audio source separation applications.      
### 50.Micro CT Image-Assisted Cross Modality Super-Resolution of Clinical CT Images Utilizing Synthesized Training Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2010.10207.pdf)
>  This paper proposes a novel, unsupervised super-resolution (SR) approach for performing the SR of a clinical CT into the resolution level of a micro CT ($\mu$CT). The precise non-invasive diagnosis of lung cancer typically utilizes clinical CT data. Due to the resolution limitations of clinical CT (about $0.5 \times 0.5 \times 0.5$ mm$^3$), it is difficult to obtain enough pathological information such as the invasion area at alveoli level. On the other hand, $\mu$CT scanning allows the acquisition of volumes of lung specimens with much higher resolution ($50 \times 50 \times 50 \mu {\rm m}^3$ or higher). Thus, super-resolution of clinical CT volume may be helpful for diagnosis of lung cancer. Typical SR methods require aligned pairs of low-resolution (LR) and high-resolution (HR) images for training. Unfortunately, obtaining paired clinical CT and $\mu$CT volumes of human lung tissues is infeasible. Unsupervised SR methods are required that do not need paired LR and HR images. In this paper, we create corresponding clinical CT-$\mu$CT pairs by simulating clinical CT images from $\mu$CT images by modified CycleGAN. After this, we use simulated clinical CT-$\mu$CT image pairs to train an SR network based on SRGAN. Finally, we use the trained SR network to perform SR of the clinical CT images. We compare our proposed method with another unsupervised SR method for clinical CT images named SR-CycleGAN. Experimental results demonstrate that the proposed method can successfully perform SR of clinical CT images of lung cancer patients with $\mu$CT level resolution, and quantitatively and qualitatively outperformed conventional method (SR-CycleGAN), improving the SSIM (structure similarity) form 0.40 to 0.51.      
### 51.Replacing Human Audio with Synthetic Audio for On-device Unspoken Punctuation Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2010.10203.pdf)
>  We present a novel multi-modal unspoken punctuation prediction system for the English language which combines acoustic and text features. We demonstrate for the first time, that by relying exclusively on synthetic data generated using a prosody-aware text-to-speech system, we can outperform a model trained with expensive human audio recordings on the unspoken punctuation prediction problem. Our model architecture is well suited for on-device use. This is achieved by leveraging hash-based embeddings of automatic speech recognition text output in conjunction with acoustic features as input to a quasi-recurrent neural network, keeping the model size small and latency low.      
### 52.Tongji University Undergraduate Team for the VoxCeleb Speaker Recognition Challenge2020  [ :arrow_down: ](https://arxiv.org/pdf/2010.10145.pdf)
>  In this report, we discribe the submission of Tongji University undergraduate team to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020 at Interspeech 2020. We applied the RSBU-CW module to the ResNet34 framework to improve the denoising ability of the network and better complete the speaker verification task in a complex environment.We trained two variants of ResNet,used score fusion and data-augmentation methods to improve the performance of the model. Our fusion of two selected systems for the CLOSE track achieves 0.2973 DCF and 4.9700\% EER on the challenge evaluation set.      
### 53.Integrating LEO Satellites and Multi-UAV Reinforcement Learning for Hybrid FSO/RF Non-Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2010.10138.pdf)
>  A mega-constellation of low-altitude earth orbit (LEO) satellites (SATs) and burgeoning unmanned aerial vehicles (UAVs) are promising enablers for high-speed and long-distance communications in beyond fifth-generation (5G) systems. Integrating SATs and UAVs within a non-terrestrial network (NTN), in this article we investigate the problem of forwarding packets between two faraway ground terminals through SAT and UAV relays using either millimeter-wave (mmWave) radio-frequency (RF) or free-space optical (FSO) link. Towards maximizing the communication efficiency, the real-time associations with orbiting SATs and the moving trajectories of UAVs should be optimized with suitable FSO/RF links, which is challenging due to the time-varying network topology and a huge number of possible control actions. To overcome the difficulty, we lift this problem to multi-agent deep reinforcement learning (MARL) with a novel action dimensionality reduction technique. Simulation results corroborate that our proposed SAT-UAV integrated scheme achieves 1.99x higher end-to-end sum throughput compared to a benchmark scheme with fixed ground relays. While improving the throughput, our proposed scheme also aims to reduce the UAV control energy, yielding 2.25x higher energy efficiency than a baseline method only maximizing the throughput. Lastly, thanks to utilizing hybrid FSO/RF links, the proposed scheme achieves up to 62.56x higher peak throughput and 21.09x higher worst-case throughput than the cases utilizing either RF or FSO links, highlighting the importance of co-designing SAT-UAV associations, UAV trajectories, and hybrid FSO/RF links in beyond-5G NTNs.      
### 54.Towards an Automatic Analysis of CHO-K1 Suspension Growth in Microfluidic Single-cell Cultivation  [ :arrow_down: ](https://arxiv.org/pdf/2010.10124.pdf)
>  Motivation: Innovative microfluidic systems carry the promise to greatly facilitate spatio-temporal analysis of single cells under well-defined environmental conditions, allowing novel insights into population heterogeneity and opening new opportunities for fundamental and applied biotechnology. Microfluidics experiments, however, are accompanied by vast amounts of data, such as time series of microscopic images, for which manual evaluation is infeasible due to the sheer number of samples. While classical image processing technologies do not lead to satisfactory results in this domain, modern deep learning technologies such as convolutional networks can be sufficiently versatile for diverse tasks, including automatic cell tracking and counting as well as the extraction of critical parameters, such as growth rate. However, for successful training, current supervised deep learning requires label information, such as the number or positions of cells for each image in a series; obtaining these annotations is very costly in this setting. Results: We propose a novel Machine Learning architecture together with a specialized training procedure, which allows us to infuse a deep neural network with human-powered abstraction on the level of data, leading to a high-performing regression model that requires only a very small amount of labeled data. Specifically, we train a generative model simultaneously on natural and synthetic data, so that it learns a shared representation, from which a target variable, such as the cell count, can be reliably estimated.      
### 55.Video Reconstruction by Spatio-Temporal Fusion of Blurred-Coded Image Pair  [ :arrow_down: ](https://arxiv.org/pdf/2010.10052.pdf)
>  Learning-based methods have enabled the recovery of a video sequence from a single motion-blurred image or a single coded exposure image. Recovering video from a single motion-blurred image is a very ill-posed problem and the recovered video usually has many artifacts. In addition to this, the direction of motion is lost and it results in motion ambiguity. However, it has the advantage of fully preserving the information in the static parts of the scene. The traditional coded exposure framework is better-posed but it only samples a fraction of the space-time volume, which is at best 50% of the space-time volume. Here, we propose to use the complementary information present in the fully-exposed (blurred) image along with the coded exposure image to recover a high fidelity video without any motion ambiguity. Our framework consists of a shared encoder followed by an attention module to selectively combine the spatial information from the fully-exposed image with the temporal information from the coded image, which is then super-resolved to recover a non-ambiguous high-quality video. The input to our algorithm is a fully-exposed and coded image pair. Such an acquisition system already exists in the form of a Coded-two-bucket (C2B) camera. We demonstrate that our proposed deep learning approach using blurred-coded image pair produces much better results than those from just a blurred image or just a coded image.      
### 56.On the Adversarial Robustness of LASSO Based Feature Selection  [ :arrow_down: ](https://arxiv.org/pdf/2010.10045.pdf)
>  In this paper, we investigate the adversarial robustness of feature selection based on the $\ell_1$ regularized linear regression model, namely LASSO. In the considered model, there is a malicious adversary who can observe the whole dataset, and then will carefully modify the response values or the feature matrix in order to manipulate the selected features. We formulate the modification strategy of the adversary as a bi-level optimization problem. Due to the difficulty of the non-differentiability of the $\ell_1$ norm at the zero point, we reformulate the $\ell_1$ norm regularizer as linear inequality constraints. We employ the interior-point method to solve this reformulated LASSO problem and obtain the gradient information. Then we use the projected gradient descent method to design the modification strategy. In addition, We demonstrate that this method can be extended to other $\ell_1$ based feature selection methods, such as group LASSO and sparse group LASSO. Numerical examples with synthetic and real data illustrate that our method is efficient and effective.      
### 57.Blind Federated Edge Learning  [ :arrow_down: ](https://arxiv.org/pdf/2010.10030.pdf)
>  We study federated edge learning (FEEL), where wireless edge devices, each with its own dataset, learn a global model collaboratively with the help of a wireless access point acting as the parameter server (PS). At each iteration, wireless devices perform local updates using their local data and the most recent global model received from the PS, and send their local updates to the PS over a wireless fading multiple access channel (MAC). The PS then updates the global model according to the signal received over the wireless MAC, and shares it with the devices. Motivated by the additive nature of the wireless MAC, we propose an analog `over-the-air' aggregation scheme, in which the devices transmit their local updates in an uncoded fashion. Unlike recent literature on over-the-air edge learning, here we assume that the devices do not have channel state information (CSI), while the PS has imperfect CSI. Instead, the PS is equipped multiple antennas to alleviate the destructive effect of the channel, exacerbated due to the lack of perfect CSI. We design a receive beamforming scheme at the PS, and show that it can compensate for the lack of perfect CSI when the PS has a sufficient number of antennas. We also derive the convergence rate of the proposed algorithm highlighting the impact of the lack of perfect CSI, as well as the number of PS antennas. Both the experimental results and the convergence analysis illustrate the performance improvement of the proposed algorithm with the number of PS antennas, where the wireless fading MAC becomes deterministic despite the lack of perfect CSI when the PS has a sufficiently large number of antennas.      
### 58.Bernstein polynomial-based transcription method for solving optimal trajectory generation problems  [ :arrow_down: ](https://arxiv.org/pdf/2010.09992.pdf)
>  This paper presents a method and an open-source implementation, Bernstein/Bzier Optimal Trajectories (BeBOT), for the generation of trajectories for autonomous system operations. The proposed method is based on infinite dimensional optimal control formulations of trajectory generation problems. By approximating the trajectories using Bernstein polynomials, these problems can be transcribed as nonlinear programming problems, which can then be solved using off-the-shelf solvers. Bernstein polynomials possess favorable geometric properties that enable the trajectory planner to efficiently evaluate and enforce constraints along the vehicles' trajectories, including maximum speed and angular rates, minimum distance between trajectories and between the vehicles and obstacles. By virtue of these properties, feasibility and safety constraints typically imposed in autonomous vehicle operations can be enforced and guaranteed independently on the order of the polynomials. Thus, the trajectory generation algorithm can efficiently generate feasible and collision-free trajectories, and can be deployed for real-time safety critical applications in complex environments and for multiple vehicle missions.      
### 59.Power pooling: An adaptive pooling function for weakly labelled sound event detection  [ :arrow_down: ](https://arxiv.org/pdf/2010.09985.pdf)
>  Access to large corpora with strongly labelled sound events is expensive and difficult in engineering applications. Much research turns to address the problem of how to detect both the types and the timestamps of sound events with weak labels that only specify the types. This task can be treated as a multiple instance learning (MIL) problem, and the key to it is the design of a pooling function. In this paper, we propose an adaptive power pooling function which can automatically adapt to various sound sources. On two public datasets, the proposed power pooling function outperforms the state-of-the-art linear softmax pooling on both coarsegrained and fine-grained metrics. Notably, it improves the event-based F1 score (which evaluates the detection of event onsets and offsets) by 11.4% and 10.2% relative on the two datasets. While this paper focuses on sound event detection applications, the proposed method can be applied to MIL tasks in other domains.      
### 60.The Effect of Spectrogram Reconstruction on Automatic Music Transcription: An Alternative Approach to Improve Transcription Accuracy  [ :arrow_down: ](https://arxiv.org/pdf/2010.09969.pdf)
>  Most of the state-of-the-art automatic music transcription (AMT) models break down the main transcription task into sub-tasks such as onset prediction and offset prediction and train them with onset and offset labels. These predictions are then concatenated together and used as the input to train another model with the pitch labels to obtain the final transcription. We attempt to use only the pitch labels (together with spectrogram reconstruction loss) and explore how far this model can go without introducing supervised sub-tasks. In this paper, we do not aim at achieving state-of-the-art transcription accuracy, instead, we explore the effect that spectrogram reconstruction has on our AMT model. Our proposed model consists of two U-nets: the first U-net transcribes the spectrogram into a posteriorgram, and a second U-net transforms the posteriorgram back into a spectrogram. A reconstruction loss is applied between the original spectrogram and the reconstructed spectrogram to constrain the second U-net to focus only on reconstruction. We train our model on three different datasets: MAPS, MAESTRO, and MusicNet. Our experiments show that adding the reconstruction loss can generally improve the note-level transcription accuracy when compared to the same model without the reconstruction part. Moreover, it can also boost the frame-level precision to be higher than the state-of-the-art models. The feature maps learned by our U-net contain gridlike structures (not present in the baseline model) which implies that with the presence of the reconstruction loss, the model is probably trying to count along both the time and frequency axis, resulting in a higher note-level transcription accuracy.      
### 61.BIRD: Big Impulse Response Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2010.09930.pdf)
>  This paper introduces BIRD, the Big Impulse Response Dataset. This open dataset consists of 100,000 multichannel room impulse responses (RIRs) generated from simulations using the Image Method, making it the largest multichannel open dataset currently available. These RIRs can be used toperform efficient online data augmentation for scenarios that involve two microphones and multiple sound sources. The paper also introduces use cases to illustrate how BIRD can perform data augmentation with existing speech corpora.      
### 62.Optimality vs Stability Trade-off in Ensemble Kalman Filters  [ :arrow_down: ](https://arxiv.org/pdf/2010.09920.pdf)
>  This paper is concerned with optimality and stability analysis of a family of ensemble Kalman filter (EnKF) algorithms. EnKF is commonly used as an alternative to the Kalman filter for high-dimensional problems, where storing the covariance matrix is computationally expensive. The algorithm consists of an ensemble of interacting particles driven by a feedback control law. The control law is designed such that, in the linear Gaussian setting and asymptotic limit of infinitely many particles, the mean and covariance of the particles follow the exact mean and covariance of the Kalman filter. The problem of finding a control law that is exact does not have a unique solution, reminiscent of the problem of finding a transport map between two distributions. A unique control law can be identified by introducing control cost functions, that are motivated by the optimal transportation problem or Schrdinger bridge problem. The objective of this paper is to study the relationship between optimality and long-term stability of a family of exact control laws. Remarkably, the control law that is optimal in the optimal transportation sense leads to an EnKF algorithm that is not stable.      
### 63.Teleoperated aerial manipulator and its avatar. Part 1: Communication, system's interconnection, control, and virtual world  [ :arrow_down: ](https://arxiv.org/pdf/2010.09903.pdf)
>  The tasks that an aerial manipulator can perform are incredibly diverse. However, nowadays the technology is not completely developed to achieve complex tasks autonomously. That's why we propose a human-in-the-loop system that can control a semi-autonomous aerial manipulator to accomplish these kinds of tasks. Furthermore, motivated by the growing trend of virtual reality systems, together with teleoperation, we develop a system composed of: an aerial manipulator model programmed in PX4 and modeled in Gazebo, a virtual reality immersion with an interactive controller, and the interconnection between the aforementioned systems via the Internet. This research is the first part of a broader project. In this part, we present experiments in the software in the loop simulation. The code of this work is liberated on our GitHub page. Also, a video shows the conducted experiments.      
### 64.Multi-Window Data Augmentation Approach for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2010.09895.pdf)
>  We present a novel, Multi-window Data Augmentation (MWA-SER), approach for speech emotion recognition. MWA-SER is a unimodal approach that focuses on two key concepts; designing the speech augmentation method to generate additional data samples and building the deep learning models to recognize the underlying emotion of an audio signal. We propose a novel multi-window augmentation method to extract more audio features from the speech signal by employing multiple window sizes into the audio feature extraction process. We show that our proposed augmentation method with minimally extracted features combined with a deep learning model improves the performance of speech emotion recognition. We demonstrate the performance of our MWA-SER approach on the IEMOCAP corpus and show that our approach outperforms previous methods, exhibiting 65% accuracy and 73% weighted average precision, a 6% and a 9% absolute improvements on accuracy and weighted average precision, respectively. We also demonstrate that with the minimum number of features (34), our model outperforms other models that use more than 900 features with higher modeling complexity. Furthermore, we also evaluate our model by replacing the "happy" category of emotion with "excited". To the best of our knowledge, our approach achieves state-of-the-art results with 66% accuracy and 68% weighted average precision, which is an 11% and a 14% absolute improvement on accuracy and weighted average precision, respectively.      
### 65.Comparative Analysis of Control Barrier Functions and Artificial Potential Fields for Obstacle Avoidance  [ :arrow_down: ](https://arxiv.org/pdf/2010.09819.pdf)
>  Artificial potential fields (APFs) and their variants have been a staple for collision avoidance of mobile robots and manipulators for almost 40 years. Its model-independent nature, ease of implementation, and real-time performance have played a large role in its continued success over the years. Control barrier functions (CBFs), on the other hand, are a more recent development, commonly used to guarantee safety for nonlinear systems in real-time in the form of a filter on a nominal controller. In this paper, we address the connections between APFs and CBFs. At a theoretic level, we prove that APFs are a special case of CBFs: given a APF one obtains a CBFs, while the converse is not true. Additionally, we prove that CBFs obtained from APFs have additional beneficial properties and can be applied to nonlinear systems. Practically, we compare the performance of APFs and CBFs in the context of obstacle avoidance on simple illustrative examples and for a quadrotor, both in simulation and on hardware using onboard sensing. These comparisons demonstrate that CBFs outperform APFs.      
### 66.Information Transfer as a Framework for Optimized Phase Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2010.09786.pdf)
>  In order to efficiently image a non-absorbing sample (a phase object), dedicated phase contrast optics are required. Typically, these optics are designed with the assumption that the sample is weakly scattering, implying a linear relation between a sample's phase and its transmission function. In the strongly scattering, non-linear case, the standard optics are ineffective and the transfer functions used to characterize them are uninformative. We use the Fisher Information (FI) to assess the efficiency of various phase imaging schemes and to calculate an Information Transfer Function (ITF). We show that a generalized version of Zernike phase contrast is efficient given sufficient foreknowledge of the sample. We show that with no foreknowledge, a random sensing measurement yields a significant fraction of the available information. Finally, we introduce a generalized approach to common path interferometry which can be optimized to prioritize sensitivity to particular sample features. Each of these measurements can be performed using Fourier lenses and phase masks.      
### 67.SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2010.09776.pdf)
>  Multi-agent interaction is a fundamental aspect of autonomous driving in the real world. Despite more than a decade of research and development, the problem of how to competently interact with diverse road users in diverse scenarios remains largely unsolved. Learning methods have much to offer towards solving this problem. But they require a realistic multi-agent simulator that generates diverse and competent driving interactions. To meet this need, we develop a dedicated simulation platform called SMARTS (Scalable Multi-Agent RL Training School). SMARTS supports the training, accumulation, and use of diverse behavior models of road users. These are in turn used to create increasingly more realistic and diverse interactions that enable deeper and broader research on multi-agent interaction. In this paper, we describe the design goals of SMARTS, explain its basic architecture and its key features, and illustrate its use through concrete multi-agent experiments on interactive scenarios. We open-source the SMARTS platform and the associated benchmark tasks and evaluation metrics to encourage and empower research on multi-agent learning for autonomous driving.      
### 68.Fast Optimization with Zeroth-Order Feedback in Distributed, Multi-User MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2006.05445.pdf)
>  In this paper, we develop a gradient-free optimization methodology for efficient resource allocation in Gaussian MIMO multiple access channels. Our approach combines two main ingredients: (i) an entropic semidefinite optimization based on matrix exponential learning (MXL); and (ii) a one-shot gradient estimator which achieves low variance through the reuse of past information. This novel algorithm, which we call gradient-free MXL algorithm with callbacks (MXL0$^{+}$), retains the convergence speed of gradient-based methods while requiring minimal feedback per iteration$-$a single scalar. In more detail, in a MIMO multiple access channel with $K$ users and $M$ transmit antennas per user, the MXL0$^{+}$ algorithm achieves $\epsilon$-optimality within $\text{poly}(K,M)/\epsilon^2$ iterations (on average and with high probability), even when implemented in a fully distributed, asynchronous manner. For cross-validation, we also perform a series of numerical experiments in medium- to large-scale MIMO networks under realistic channel conditions. Throughout our experiments, the performance of MXL0$^{+}$ matches$-$and sometimes exceeds$-$that of gradient-based MXL methods, all the while operating with a vastly reduced communication overhead. In view of these findings, the MXL0$^{+}$ algorithm appears to be uniquely suited for distributed massive MIMO systems where gradient calculations can become prohibitively expensive.      
### 69.Lyapunov functions for nabla discrete fractional order systems  [ :arrow_down: ](https://arxiv.org/pdf/1812.11368.pdf)
>  This paper focuses on the fractional difference of Lyapunov functions related to Riemann-Liouville, Caputo and Grunwald-Letnikov definitions. A new way of building Lyapunov functions is introduced and then five inequalities are derived for each definition. With the help of the developed inequalities, the sufficient conditions can be obtained to guarantee the asymptotic stability of the nabla discrete fractional order nonlinear systems. Finally, three illustrative examples are presented to demonstrate the validity and feasibility of the proposed theoretical results.      
