# ArXiv eess --Tue, 29 Dec 2020
### 1.Adversarial Machine Learning in Wireless Communications using RF Data: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.14392.pdf)
>  Machine learning provides effective means to learn from spectrum data and solve complex tasks involved in wireless communications. Supported by recent advances in computational resources and algorithmic designs, deep learning has found success in performing various wireless communication tasks such as signal recognition and spectrum sensing. However, machine learning in general and deep learning in particular has recently been found vulnerable to manipulations in training and test times giving rise to a field of study called Adversarial Machine Learning (AML). Although AML has been extensively studied in other data domains such as computer vision and natural language processing, research for AML in the wireless communications domain is in its early stage. This paper presents a comprehensive review of the latest research efforts focused on AML in wireless communications while accounting for the unique characteristics of wireless systems. First, the necessary background on the various types of AML attacks is provided. Then, a holistic survey of the works developing the AML attacks and the corresponding defense mechanisms in the wireless domain is presented. Finally, recent research trends are identified and the future outlook for AML as a new attack surface for wireless communications is described.      
### 2.Probabilistic electric load forecasting through Bayesian Mixture Density Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.14389.pdf)
>  Probabilistic load forecasting (PLF) is a key component in the extended tool-chain required for efficient management of smart energy grids. Neural networks are widely considered to achieve improved prediction performances, supporting highly flexible mappings of complex relationships between the target and the conditioning variables set. However, obtaining comprehensive predictive uncertainties from such black-box models is still a challenging and unsolved problem. In this work, we propose a novel PLF approach, framed on Bayesian Mixture Density Networks. Both aleatoric and epistemic uncertainty sources are encompassed within the model predictions, inferring general conditional densities, depending on the input features, within an end-to-end training framework. To achieve reliable and computationally scalable estimators of the posterior distributions, both Mean Field variational inference and deep ensembles are integrated. Experiments have been performed on household short-term load forecasting tasks, showing the capability of the proposed method to achieve robust performances in different operating conditions.      
### 3.Online Photometric Calibration of Automatic Gain Thermal Infrared Cameras  [ :arrow_down: ](https://arxiv.org/pdf/2012.14292.pdf)
>  Thermal infrared cameras are increasingly being used in various applications such as robot vision, industrial inspection and medical imaging, thanks to their improved resolution and portability. However, the performance of traditional computer vision techniques developed for electro-optical imagery does not directly translate to the thermal domain due to two major reasons: these algorithms require photometric assumptions to hold, and methods for photometric calibration of RGB cameras cannot be applied to thermal-infrared cameras due to difference in data acquisition and sensor phenomenology. In this paper, we take a step in this direction, and introduce a novel algorithm for online photometric calibration of thermal-infrared cameras. Our proposed method does not require any specific driver/hardware support and hence can be applied to any commercial off-the-shelf thermal IR camera. We present this in the context of visual odometry and SLAM algorithms, and demonstrate the efficacy of our proposed system through extensive experiments for both standard benchmark datasets, and real-world field tests with a thermal-infrared camera in natural outdoor environments.      
### 4.A Differential-Cascaded Paradigm for Control of Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.14251.pdf)
>  This paper focuses on developing a new paradigm motivated by investigating the consensus problem of networked Lagrangian systems with time-varying delay and switching topologies. We present adaptive controllers with piecewise continuous or arbitrary times differentiable control torques for realizing consensus of Lagrangian systems, extending the results in the literature. This specific study motivates the formulation of a new paradigm referred to as forwardstepping, which is shown to be a systematic tool for solving various nonlinear control problems. One distinctive point associated with forwardstepping is that the order of the reference dynamics is typically specified to be equal to or higher than that of the original nonlinear system, and the reference dynamics and the nonlinear system are governed by a differential/dynamic-cascaded structure. The order invariance or increment of the specified reference dynamics with respect to the nonlinear system and their differential/dynamic-cascaded structure expands significantly the design freedom and thus facilitates the seeking of solutions to many nonlinear control problems which would otherwise often be intractable.      
### 5.Lesion Net -- Skin Lesion Segmentation Using Coordinate Convolution and Deep Residual Units  [ :arrow_down: ](https://arxiv.org/pdf/2012.14249.pdf)
>  Skin lesions segmentation is an important step in the process of automated diagnosis of the skin melanoma. However, the accuracy of segmenting melanomas skin lesions is quite a challenging task due to less data for training, irregular shapes, unclear boundaries, and different skin colors. Our proposed approach helps in improving the accuracy of skin lesion segmentation. Firstly, we have introduced the coordinate convolutional layer before passing the input image into the encoder. This layer helps the network to decide on the features related to translation invariance which further improves the generalization capacity of the model. Secondly, we have leveraged the properties of deep residual units along with the convolutional layers. At last, instead of using only cross-entropy or Dice-loss, we have combined the two-loss functions to optimize the training metrics which helps in converging the loss more quickly and smoothly. After training and validating the proposed model on ISIC 2018 (60% as train set + 20% as validation set), we tested the robustness of our trained model on various other datasets like ISIC 2018 (20% as test-set) ISIC 2017, 2016 and PH2 dataset. The results show that the proposed model either outperform or at par with the existing skin lesion segmentation methods.      
### 6.Combining CNN and Hybrid Active Contours for Head and Neck Tumor Segmentation in CT and PET images  [ :arrow_down: ](https://arxiv.org/pdf/2012.14207.pdf)
>  Automatic segmentation of head and neck tumors plays an important role in radiomics analysis. In this short paper, we propose an automatic segmentation method for head and neck tumors from PET and CT images based on the combination of convolutional neural networks (CNNs) and hybrid active contours. Specifically, we first introduce a multi-channel 3D U-Net to segment the tumor with the concatenated PET and CT images. Then, we estimate the segmentation uncertainty by model ensembles and define a segmentation quality score to select the cases with high uncertainties. Finally, we develop a hybrid active contour model to refine the high uncertainty cases. Our method ranked second place in the MICCAI 2020 HECKTOR challenge with average Dice Similarity Coefficient, precision, and recall of 0.752, 0.838, and 0.717, respectively.      
### 7.Screening COVID-19 Based on CT/CXR Images &amp; Building a Publicly Available CT-scan Dataset of COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2012.14204.pdf)
>  The rapid outbreak of COVID-19 threatens humans life all around the world. Due to insufficient diagnostic infrastructures, developing an accurate, efficient, inexpensive, and quick diagnostic tool is of great importance. As chest radiography, such as chest X-ray (CXR) and CT computed tomography (CT), is a possible way for screening COVID-19, developing an automatic image classification tool is immensely helpful for detecting the patients with COVID-19. To date, researchers have proposed several different screening methods; however, none of them could achieve a reliable and highly sensitive performance yet. The main drawbacks of current methods are the lack of having enough training data, low generalization performance, and a high rate of false-positive detection. To tackle such limitations, this study firstly builds a large-size publicly available CT-scan dataset, consisting of more than 13k CT-images of more than 1000 individuals, in which 8k images are taken from 500 patients infected with COVID-19. Secondly, we propose a deep learning model for screening COVID-19 using our proposed CT dataset and report the baseline results. Finally, we extend the proposed CT model for screening COVID-19 from CXR images using a transfer learning approach. The experimental results show that the proposed CT and CXR methods achieve the AUC scores of 0.886 and 0.984 respectively.      
### 8.On Liveness Enforcement of Distributed Petri Net Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.14199.pdf)
>  This paper considers the liveness enforcement problem in a class of Petri nets (PNs) modeling distributed systems called Synchronized Sequential Processes (SSP). This class of PNs is defined as a set of mono-marked state machines (sequential machines, called also agents) cooperating in a distributed way through buffers. These buffers could model intermediate products in a production system or information channel in a healthcare system but they should be destination private to an agent. The designed controller for liveness enforcement should preserve this important property characteristic to the distributed systems. The approach in this paper is based on the construction of a control PN that is an abstraction of the relations of the T-semiflows and buffers. The control PN will evolve in parallel with the system, avoiding the firing of transitions that may lead the system to livelock. An algorithm to compute this control PN is presented. Moreover, in order to ensure the liveness of control PN, another algorithm is proposed allowing the firing of local T-semiflow in the correct proportion. Finally, an algorithm for guiding the system evolution is also proposed.      
### 9.Perception Consistency Ultrasound Image Super-resolution via Self-supervised CycleGAN  [ :arrow_down: ](https://arxiv.org/pdf/2012.14142.pdf)
>  Due to the limitations of sensors, the transmission medium and the intrinsic properties of ultrasound, the quality of ultrasound imaging is always not ideal, especially its low spatial resolution. To remedy this situation, deep learning networks have been recently developed for ultrasound image super-resolution (SR) because of the powerful approximation capability. However, most current supervised SR methods are not suitable for ultrasound medical images because the medical image samples are always rare, and usually, there are no low-resolution (LR) and high-resolution (HR) training pairs in reality. In this work, based on self-supervision and cycle generative adversarial network (CycleGAN), we propose a new perception consistency ultrasound image super-resolution (SR) method, which only requires the LR ultrasound data and can ensure the re-degenerated image of the generated SR one to be consistent with the original LR image, and vice versa. We first generate the HR fathers and the LR sons of the test ultrasound LR image through image enhancement, and then make full use of the cycle loss of LR-SR-LR and HR-LR-SR and the adversarial characteristics of the discriminator to promote the generator to produce better perceptually consistent SR results. The evaluation of PSNR/IFC/SSIM, inference efficiency and visual effects under the benchmark CCA-US and CCA-US datasets illustrate our proposed approach is effective and superior to other state-of-the-art methods.      
### 10.Analysis of Macula on Color Fundus Images Using Heightmap Reconstruction Through Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.14140.pdf)
>  For medical diagnosis based on retinal images, a clear understanding of 3D structure is often required but due to the 2D nature of images captured, we cannot infer that information. However, by utilizing 3D reconstruction methods, we can recover the height information of the macula area on a fundus image which can be helpful for diagnosis and screening of macular disorders. Recent approaches have used shading information for heightmap prediction but their output was not accurate since they ignored the dependency between nearby pixels and only utilized shading information. Additionally, other methods were dependent on the availability of more than one image of the retina which is not available in practice. In this paper, motivated by the success of Conditional Generative Adversarial Networks(cGANs) and deeply supervised networks, we propose a novel architecture for the generator which enhances the details and the quality of output by progressive refinement and the use of deep supervision to reconstruct the height information of macula on a color fundus image. Comparisons on our own dataset illustrate that the proposed method outperforms all of the state-of-the-art methods in image translation and medical image translation on this particular task. Additionally, perceptual studies also indicate that the proposed method can provide additional information for ophthalmologists for diagnosis.      
### 11.Cascaded Convolutional Neural Network for Automatic Myocardial Infarction Segmentation from Delayed-Enhancement Cardiac MRI  [ :arrow_down: ](https://arxiv.org/pdf/2012.14128.pdf)
>  Automatic segmentation of myocardial contours and relevant areas like infraction and no-reflow is an important step for the quantitative evaluation of myocardial infarction. In this work, we propose a cascaded convolutional neural network for automatic myocardial infarction segmentation from delayed-enhancement cardiac MRI. We first use a 2D U-Net to focus on the intra-slice information to perform a preliminary segmentation. After that, we use a 3D U-Net to utilize the volumetric spatial information for a subtle segmentation. Our method is evaluated on the MICCAI 2020 EMIDEC challenge dataset and achieves average Dice score of 0.8786, 0.7124 and 0.7851 for myocardium, infarction and no-reflow respectively, outperforms all the other teams of the segmentation contest.      
### 12.3D Axial-Attention for Lung Nodule Classification  [ :arrow_down: ](https://arxiv.org/pdf/2012.14117.pdf)
>  Purpose: In recent years, Non-Local based methods have been successfully applied to lung nodule classification. However, these methods offer 2D attention or a limited 3D attention to low-resolution feature maps. Moreover, they still depend on a convenient local filter such as convolution as full 3D attention is expensive to compute and requires a big dataset, which might not be available. Methods: We propose to use 3D Axial-Attention, which requires a fraction of the computing power of a regular Non-Local network. Additionally, we solve the position invariant problem of the Non-Local network by proposing adding 3D positional encoding to shared embeddings. Results: We validated the proposed method on the LIDC-IDRI dataset by following a rigorous experimental setup using only nodules annotated by at least three radiologists. Our results show that the 3D Axial-Attention model achieves state-of-the-art performance on all evaluation metrics including AUC and Accuracy. Conclusions: The proposed model provides full 3D attention effectively, which can be used in all layers without the need for local filters. The experimental results show the importance of full 3D attention for classifying lung nodules.      
### 13.Diagnosis/Prognosis of COVID-19 Images: Challenges, Opportunities, and Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.14106.pdf)
>  The novel Coronavirus disease, COVID-19, has rapidly and abruptly changed the world as we knew in 2020. It becomes the most unprecedent challenge to analytic epidemiology in general and signal processing theories in specific. Given its high contingency nature and adverse effects across the world, it is important to develop efficient processing/learning models to overcome this pandemic and be prepared for potential future ones. In this regard, medical imaging plays an important role for the management of COVID-19. Human-centered interpretation of medical images is, however, tedious and can be subjective. This has resulted in a surge of interest to develop Radiomics models for analysis and interpretation of medical images. Signal Processing (SP) and Deep Learning (DL) models can assist in development of robust Radiomics solutions for diagnosis/prognosis, severity assessment, treatment response, and monitoring of COVID-19 patients. In this article, we aim to present an overview of the current state, challenges, and opportunities of developing SP/DL-empowered models for diagnosis (screening/monitoring) and prognosis (outcome prediction and severity assessment) of COVID-19 infection. More specifically, the article starts by elaborating the latest development on the theoretical framework of analytic epidemiology and hypersignal processing for COVID-19. Afterwards, imaging modalities and Radiological characteristics of COVID-19 are discussed. SL/DL-based Radiomic models specific to the analysis of COVID-19 infection are then described covering the following four domains: Segmentation of COVID-19 lesions; Predictive models for outcome prediction; Severity assessment, and; Diagnosis/classification models. Finally, open problems and opportunities are presented in detail.      
### 14.Real time multibody modeling and simulation of a scaled bogie test rig  [ :arrow_down: ](https://arxiv.org/pdf/2012.14076.pdf)
>  In wheel rail adhesion studies, most of the test rigs used are simplified designs such as a single wheel or wheelset, but the results may not be accurate. Alternatively, representing the complex system by using a full vehicle model provides accurate results but may incur complexity in design. To trade off accuracy over complexity, a bogie model can be the optimum selection. Furthermore, only a real time model can replicate its physical counterpart in the time domain. Developing such a model requires broad expertise and appropriate software and hardware. A few published works are available which deal with real time modeling. However, the influence of the control system has not been included in those works. To address these issues, a real-time scaled bogie test rig including the control system is essential. Therefore, a 1:4 scaled bogie roller rig is developed to study the adhesion between wheel and roller contact. To compare the performances obtained from the scaled bogie test rig and to expand the test applications, a numerical simulation model of that scaled bogie test rig is developed using Gensys multibody software. This model is the complete model of the test rig which delivers more precise results. To exactly represent the physical counterpart system in the time domain, a real-time scaled bogie test rig (RT SBTR) is developed after four consecutive stages. Then, to simulate the RT-SBTR to solve the internal state equations and functions representing the physical counterpart system in equal or less than actual time, the real-time simulation environment is prepared in two stages. To such end, the computational time improved from 4 times slower than real time to 2 times faster than real time. Finally, the real time scaled bogie model is also incorporated with the braking control system which slightly reduces the computational performances without affecting real time capability.      
### 15.On the Performance of One-Bit DoA Estimation via Sparse Linear Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2012.14051.pdf)
>  Direction of Arrival (DoA) estimation using Sparse Linear Arrays (SLAs) has recently gained considerable attention in array processing thanks to their capability to provide enhanced degrees of freedom in resolving uncorrelated source signals. Additionally, deployment of one-bit Analog-to-Digital Converters (ADCs) has emerged as an important topic in array processing, as it offers both a low-cost and a low-complexity implementation. In this paper, we study the problem of DoA estimation from one-bit measurements received by an SLA. Specifically, we first investigate the identifiability conditions for the DoA estimation problem from one-bit SLA data and establish an equivalency with the case when DoAs are estimated from infinite-bit unquantized measurements. Towards determining the performance limits of DoA estimation from one-bit quantized data, we derive a pessimistic approximation of the corresponding CramÃ©r-Rao Bound (CRB). This pessimistic CRB is then used as a benchmark for assessing the performance of one-bit DoA estimators. We also propose a new algorithm for estimating DoAs from one-bit quantized data. We investigate the analytical performance of the proposed method through deriving a closed-form expression for the covariance matrix of the asymptotic distribution of the DoA estimation errors and show that it outperforms the existing algorithms in the literature. Numerical simulations are provided to validate the analytical derivations and corroborate the resulting performance improvement.      
### 16.Building Multi lingual TTS using Cross Lingual Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2012.14039.pdf)
>  In this paper we propose a new cross-lingual Voice Conversion (VC) approach which can generate all speech parameters (MCEP, LF0, BAP) from one DNN model using PPGs (Phonetic PosteriorGrams) extracted from inputted speech using several ASR acoustic models. Using the proposed VC method, we tried three different approaches to build a multilingual TTS system without recording a multilingual speech corpus. A listening test was carried out to evaluate both speech quality (naturalness) and voice similarity between converted speech and target speech. The results show that Approach 1 achieved the highest level of naturalness (3.28 MOS on a 5-point scale) and similarity (2.77 MOS).      
### 17.Scale-free Protocol Design for Output and Regulated Output Synchronization of Heterogeneous Multi-agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.14032.pdf)
>  In this paper, we consider scalable output and regulated output synchronization problems for heterogeneous networks of right-invertible linear agents based on localized information exchange where in the case of regulated output synchronization, the reference trajectory is generated by a so-called exosystem. We assume that all the agents are introspective, meaning that they have access to their own local measurements. We propose a scale-free linear protocol for each agent to achieve output and regulated output synchronizations. These protocols are designed solely based on agent models and they need no information about communication graph and the number of agents or other agent models information.      
### 18.Towards Understanding Sensor and Control Nodes Selection in Nonlinear Dynamic Systems: Lyapunov Theory Meets Branch-and-Bound  [ :arrow_down: ](https://arxiv.org/pdf/2012.14020.pdf)
>  Sensor and actuator selection problems (SASP) are some of the core problems in dynamic systems design and control. These problems correspond to determining the optimal selection of sensors (measurements) or actuators (control nodes) such that certain estimation/control objectives can be achieved. While the literature on SASP is indeed inveterate, the vast majority of the work focuses on linear(ized) representation of the network dynamics, resulting in the placements of sensors or actuators (SA) that are valid for confined operating regions. As an alternative, herein we propose a new general framework for addressing SASP in nonlinear dynamic systems (NDS), assuming that the inputs and outputs are linearly coupled with the nonlinear dynamics. This is investigated through (i) classifying and parameterizing the NDS into various nonlinear function sets, (ii) utilizing rich Lyapunov theoretic formulations, and (iii) designing a new customized branch-and-bound (BnB) algorithm that exploits problem structure of the SASP. The newly designed BnB routines are computationally more attractive than the standard one and also directly applicable to solve SASP for linear systems. In contrast with contemporary approaches from the literature, our approach is suitable for finding the optimal SA combination for stable/unstable NDS that ensures stabilization of estimation error and closed-loop dynamics through a simple linear feedback control policy.      
### 19.Generalized Categorisation of Digital Pathology Whole Image Slides using Unsupervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.13955.pdf)
>  This project aims to break down large pathology images into small tiles and then cluster those tiles into distinct groups without the knowledge of true labels, our analysis shows how difficult certain aspects of clustering tumorous and non-tumorous cells can be and also shows that comparing the results of different unsupervised approaches is not a trivial task. The project also provides a software package to be used by the digital pathology community, that uses some of the approaches developed to perform unsupervised unsupervised tile classification, which could then be easily manually labelled. <br>The project uses a mixture of techniques ranging from classical clustering algorithms such as K-Means and Gaussian Mixture Models to more complicated feature extraction techniques such as deep Autoencoders and Multi-loss learning. Throughout the project, we attempt to set a benchmark for evaluation using a few measures such as completeness scores and cluster plots. <br>Throughout our results we show that Convolutional Autoencoders manages to slightly outperform the rest of the approaches due to its powerful internal representation learning abilities. Moreover, we show that Gaussian Mixture models produce better results than K-Means on average due to its flexibility in capturing different clusters. We also show the huge difference in the difficulties of classifying different types of pathology textures.      
### 20.Learning Generalized Spatial-Temporal Deep Feature Representation for No-Reference Video Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2012.13936.pdf)
>  In this work, we propose a no-reference video quality assessment method, aiming to achieve high-generalization capability in cross-content, -resolution and -frame rate quality prediction. In particular, we evaluate the quality of a video by learning effective feature representations in spatial-temporal domain. In the spatial domain, to tackle the resolution and content variations, we impose the Gaussian distribution constraints on the quality features. The unified distribution can significantly reduce the domain gap between different video samples, resulting in a more generalized quality feature representation. Along the temporal dimension, inspired by the mechanism of visual perception, we propose a pyramid temporal aggregation module by involving the short-term and long-term memory to aggregate the frame-level quality. Experiments show that our method outperforms the state-of-the-art methods on cross-dataset settings, and achieves comparable performance on intra-dataset configurations, demonstrating the high-generalization capability of the proposed method.      
### 21.A Downlink Puncturing Scheme for Simultaneous Transmission of URLLC and eMBB Traffic by Exploiting Data Similarity  [ :arrow_down: ](https://arxiv.org/pdf/2012.13923.pdf)
>  Ultra Reliable and Low Latency Communications (URLLC) is deemed to be an essential service in 5G systems and beyond to accommodate a wide range of emerging applications with stringent latency and reliability requirements. Coexistence of URLLC alongside other service categories calls for developing spectrally efficient multiplexing techniques. Specifically, coupling URLLC and conventional enhanced Mobile BroadBand (eMBB) through superposition/puncturing naturally arises as a promising option due to the tolerance of the latter in terms of latency and reliability. The idea here is to transmit URLLC packets over resources occupied by ongoing eMBB transmissions while minimizing the impact on the eMBB transmissions. In this paper, we propose a novel downlink URLLC-eMBB multiplexing technique that exploits possible similarities among URLLC and eMBB symbols, with the objective of reducing the size of the punctured eMBB symbols. We propose that the base station scans the eMBB traffic' symbol sequences and punctures those that have the highest symbol similarity with that of the URLLC users to be served. As the eMBB and URLLC may use different constellation sizes, we introduce the concept of symbol region similarity to accommodate the different constellations. We assess the performance of the proposed scheme analytically, where we derive closed-form expressions for the symbol error rate (SER) of the eMBB and URLLC services. {We also derive an expression for the eMBB loss function due to puncturing in terms of the eMBB SER}. We demonstrate through numerical and simulation results the efficacy of the proposed scheme where we show that 1) the eMBB spectral efficiency is improved by puncturing fewer symbols, 2) the SER and reliability performance of eMBB are improved, and 3) the URLLC data is accommodated within the specified delay constraint while maintaining its reliability.      
### 22.WHU-Hi: UAV-borne hyperspectral with high spatial resolution (H2) benchmark datasets for hyperspectral image classification  [ :arrow_down: ](https://arxiv.org/pdf/2012.13920.pdf)
>  Classification is an important aspect of hyperspectral images processing and application. At present, the researchers mostly use the classic airborne hyperspectral imagery as the benchmark dataset. However, existing datasets suffer from three bottlenecks: (1) low spatial resolution; (2) low labeled pixels proportion; (3) low degree of subclasses distinction. In this paper, a new benchmark dataset named the Wuhan UAV-borne hyperspectral image (WHU-Hi) dataset was built for hyperspectral image classification. The WHU-Hi dataset with a high spectral resolution (nm level) and a very high spatial resolution (cm level), which we refer to here as H2 imager. Besides, the WHU-Hi dataset has a higher pixel labeling ratio and finer subclasses. Some start-of-art hyperspectral image classification methods benchmarked the WHU-Hi dataset, and the experimental results show that WHU-Hi is a challenging dataset. We hope WHU-Hi dataset can become a strong benchmark to accelerate future research.      
### 23.Structure-Aware Layer Decomposition Learning Based on Gaussian Convolution Model for Inverse Halftoning  [ :arrow_down: ](https://arxiv.org/pdf/2012.13894.pdf)
>  Layer decomposition to separate an input image into base and detail layers has been steadily used for image restoration. Existing residual networks based on an additive model require residual layers with a small output range for fast convergence and visual quality improvement. However, in inverse halftoning, homogenous dot patterns hinder a small output range from the residual layers. Therefore, a new layer decomposition network based on the Gaussian convolution model (GCM) and structure-aware deblurring strategy is presented to achieve residual learning for both the base and detail layers. For the base layer, a new GCM-based residual subnetwork is presented. The GCM utilizes a statistical distribution, in which the image difference between a blurred continuous-tone image and a blurred halftoned image with a Gaussian filter can result in a narrow output range. Subsequently, the GCM-based residual subnetwork uses a Gaussian-filtered halftoned image as input and outputs the image difference as residual, thereby generating the base layer, i.e., the Gaussian-blurred continuous-tone image. For the detail layer, a new structure-aware residual deblurring subnetwork (SARDS) is presented. To remove the Gaussian blurring of the base layer, the SARDS uses the predicted base layer as input and outputs the deblurred version. To more effectively restore image structures such as lines and texts, a new image structure map predictor is incorporated into the deblurring network to induce structure-adaptive learning. This paper provides a method to realize the residual learning of both the base and detail layers based on the GCM and SARDS. In addition, it is verified that the proposed method surpasses state-of-the-art methods based on U-Net, direct deblurring networks, and progressively residual networks.      
### 24.Histogram Matching Augmentation for Domain Adaptation with Application to Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.13871.pdf)
>  Convolutional Neural Networks (CNNs) have achieved high accuracy for cardiac structure segmentation if training cases and testing cases are from the same distribution. However, the performance would be degraded if the testing cases are from a distinct domain (e.g., new MRI scanners, clinical centers). In this paper, we propose a histogram matching (HM) data augmentation method to eliminate the domain gap. Specifically, our method generates new training cases by using HM to transfer the intensity distribution of testing cases to existing training cases. The proposed method is quite simple and can be used in a plug-and-play way in many segmentation tasks. The method is evaluated on MICCAI 2020 M\&amp;Ms challenge, and achieves average Dice scores of 0.9051, 0.8405, and 0.8749, and Hausdorff Distances of 9.996, 12.49, and 12.68 for the left ventricular, myocardium, and right ventricular, respectively. Our results rank the third place in MICCAI 2020 M\&amp;Ms challenge. The code and trained models are publicly available at \url{<a class="link-external link-https" href="https://github.com/JunMa11/HM_DataAug" rel="external noopener nofollow">this https URL</a>}.      
### 25.Target Detection within Nonhomogeneous Clutter via Total Bregman Divergence-Based Matrix Information Geometry Detectors  [ :arrow_down: ](https://arxiv.org/pdf/2012.13861.pdf)
>  Information divergences are commonly used to measure the dissimilarity of two elements on a statistical manifold. Differentiable manifolds endowed with different divergences may possess different geometric properties, which can result in totally different performances in many practical applications. In this paper, we propose a total Bregman divergence-based matrix information geometry (TBD-MIG) detector and apply it to detect targets emerged into nonhomogeneous clutter. In particular, each sample data is assumed to be modeled as a Hermitian positive-definite (HPD) matrix and the clutter covariance matrix is estimated by the TBD mean of a set of secondary HPD matrices. We then reformulate the problem of signal detection as discriminating two points on the HPD matrix manifold. Three TBD-MIG detectors, referred to as the total square loss, the total log-determinant and the total von Neumann MIG detectors, are proposed, and they can achieve great performances due to their power of discrimination and robustness to interferences. Simulations show the advantage of the proposed TBD-MIG detectors in comparison with the geometric detector using an affine invariant Riemannian metric as well as the adaptive matched filter in nonhomogeneous clutter.      
### 26.A Distributed Optimization Approach to the Multi-Regional Day-Ahead Clearing Process in Electricity Markets  [ :arrow_down: ](https://arxiv.org/pdf/2012.13852.pdf)
>  The implementation of electricity markets based on locational marginal pricing in a multi-settlement process has allowed wholesale competition, with pricing mechanisms that incentivize the optimal allocation of generation, transmission, and demand-response resources. While efficiency and reliability gains have been achieved in the US at a regional level, the lack of adequate inter-regional coordination mechanisms has limited broader gains. The shortcomings of cross-border coordination of electricity markets become more apparent as the industry transitions towards higher penetration of renewable resources, which tend to be concentrated in certain regions of the country, usually distant from load pockets. In addition to allowing market participants to respond to economic signals beyond the market region where their assets reside, a coordinated market solution would allow the extension of market mechanisms such as financial transmission rights and capacity contracts to cover transactions across borders; thereby, reducing price uncertainty around the investment in new generation resources. This paper presents a distributed solution to the unit commitment problem that allows for full coordination of the market clearing process across interconnected electricity markets.      
### 27.Evaluation and Comparison of Edge-Preserving Filters  [ :arrow_down: ](https://arxiv.org/pdf/2012.13778.pdf)
>  Edge-preserving filters play an essential role in some of the most basic tasks of computational photography, such as abstraction, tonemapping, detail enhancement and texture removal, to name a few. The abundance and diversity of smoothing operators, accompanied by a lack of methodology to evaluate output quality and/or perform an unbiased comparison between them, could lead to misunderstanding and potential misuse of such methods. This paper introduces a systematic methodology for evaluating and comparing such operators and demonstrates it on a diverse set of published edge-preserving filters. Additionally, we present a common baseline along which a comparison of different operators can be achieved and use it to determine equivalent parameter mappings between methods. Finally, we suggest some guidelines for objective comparison and evaluation of edge-preserving filters.      
### 28.Resilient Consensus Against Epidemic Malicious Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2012.13757.pdf)
>  This paper addresses novel consensus problems for multi-agent systems operating in a pandemic environment where infectious diseases are spreading. The dynamics of the diseases follows the susceptible-infected-recovered (SIR) model, where the infection induces faulty behaviors in the agents and affects their state values. To ensure resilient consensus among the noninfectious agents, the difficulty is that the number of infectious agents changes over time. We assume that a high-level policy maker announces the level of infection in realtime, which can be adopted by the agents for their preventative measures. It is demonstrated that this problem can be formulated as resilient consensus in the presence of the socalled mobile malicious models, where the mean subsequence reduced (MSR) algorithms are known to be effective. We characterize sufficient conditions on the network structures for different policies regarding the announced infection levels and the strength of the pandemic. Numerical simulations are carried out for random graphs to verify the effectiveness of our approach.      
### 29.Android Based Low Cost Sitting Posture Monitoring System  [ :arrow_down: ](https://arxiv.org/pdf/2012.13687.pdf)
>  Back pain is one of the leading causes of disability-adjusted life year globally and the most common cause of low back pain is poor sitting posture. There are several actions that can be adopted proactively to avoid poor sitting posture induced back pain including behavioral change, regular exercise, and use of an ergonomic chair. However, these are either expensive and/or difficult to execute for prolonged periods. Sitting posture monitoring systems continuously observe the sitting pattern of a person in real-time and give feedback/alert poor sitting posture is observed. In this study, a real-time posture monitoring system has been designed and a functional prototype has been developed using simple electrical elements and an android application. Appropriate position of the sensor in the spine to measure the degree of bending and the threshold sensor values for good posture sittings have been determined based on the results from healthy volunteers of different ages and height. The android application continuously monitors the degree of bending and provides vibration when the bending reaches the threshold of bad posture or the duration of the sitting crosses the clinically recommended time limit to prevent prolonged sitting. Positive user feedback has been received in terms of comfortability, effectiveness, and satisfaction levels. The manufacturing cost of the developed monitoring system is minimal compared to the available expensive systems in the market and the cost would further go down if it is produced in bulk. This device efficiently monitors the sitting posture pattern to prevent back pain and within the affordable price range for the people from middle to under-developed countries.      
### 30.An Evidential Reasoning Based Approach to Building Node Selection Criterion for Network Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2012.13684.pdf)
>  A reasonable node selection criterion (NSC) is crucial for the network reduction in power systems. In contrast to the previous works that only consider structure property, this paper proposes a comprehensive and quantitative NSC considering both structural and electrical properties. The proposed NSC is developed by employing the evidential reasoning approach, in which the quasi-one-hot encoding is used to determine the evaluation grades of different criteria or attributes. Then, different criteria are combined through the multi-evidence reasoning. Eventually, the utility evaluation is used to derive the quantitative NSC. Besides, the ER can be readily extended to multiple criteria while considering the uncertainty in the evaluation process simultaneously. The reduced models with higher accuracy can be built by combining the proposed NSC with the existing model reduction algorithms. The case studies on a 30-node power grid substantiate the practicality of the proposed NSC.      
### 31.Millimeter Wave Sensing: A Review of Application Pipelines and Building Blocks  [ :arrow_down: ](https://arxiv.org/pdf/2012.13664.pdf)
>  The increasing bandwidth requirement of new wireless applications has lead to standardization of the millimeter wave spectrum for high-speed wireless communication. The millimeter wave spectrum is part of 5G and covers frequencies between 30 and 300 GHz corresponding to wavelengths ranging from 10 to 1 mm. Although millimeter wave is often considered as a communication medium, it has also proved to be an excellent 'sensor', thanks to its narrow beams, operation across a wide bandwidth, and interaction with atmospheric constituents. In this paper, which is to the best of our knowledge the first review that completely covers millimeter wave sensing application pipelines, we provide a comprehensive overview and analysis of different basic application pipeline building blocks, including hardware, algorithms, analytical models, and model evaluation techniques. The review also provides a taxonomy that highlights different millimeter wave sensing application domains. By performing a thorough analysis, complying with the systematic literature review methodology and reviewing 165 papers, we not only extend previous investigations focused only on communication aspects of the millimeter wave technology and using millimeter wave technology for active imaging, but also highlight scientific and technological challenges and trends, and provide a future perspective for applications of millimeter wave as a sensing technology.      
### 32.COVIDX: Computer-aided diagnosis of Covid-19 and its severity prediction with raw digital chest X-ray images  [ :arrow_down: ](https://arxiv.org/pdf/2012.13605.pdf)
>  Coronavirus disease (COVID-19) is a contagious infection caused by severe acute respiratory syndrome coronavirus-2 (SARS-COV-2) and it has infected and killed millions of people across the globe. In the absence of specific drugs or vaccines for the treatment of COVID-19 and the limitation of prevailing diagnostic techniques, there is a requirement for some alternate automatic screening systems that can be used by the physicians to quickly identify and isolate the infected patients. A chest X-ray (CXR) image can be used as an alternative modality to detect and diagnose the COVID-19. In this study, we present an automatic COVID-19 diagnostic and severity prediction (COVIDX) system that uses deep feature maps from CXR images to diagnose COVID-19 and its severity prediction. The proposed system uses a three-phase classification approach (healthy vs unhealthy, COVID-19 vs Pneumonia, and COVID-19 severity) using different shallow supervised classification algorithms. We evaluated COVIDX not only through 10-fold cross2 validation and by using an external validation dataset but also in real settings by involving an experienced radiologist. In all the evaluation settings, COVIDX outperforms all the existing stateof-the-art methods designed for this purpose. We made COVIDX easily accessible through a cloud-based webserver and python code available at <a class="link-external link-https" href="https://sites.google.com/view/wajidarshad/software" rel="external noopener nofollow">this https URL</a> and <a class="link-external link-https" href="https://github.com/wajidarshad/covidx" rel="external noopener nofollow">this https URL</a>, respectively.      
### 33.Deep Learning Methods for Screening Pulmonary Tuberculosis Using Chest X-rays  [ :arrow_down: ](https://arxiv.org/pdf/2012.13582.pdf)
>  Tuberculosis (TB) is a contagious bacterial airborne disease, and is one of the top 10 causes of death worldwide. According to the World Health Organization (WHO), around 1.8 billion people are infected with TB and 1.6 million deaths were reported in 2018. More importantly,95% of cases and deaths were from developing countries. Yet, TB is a completely curable disease through early diagnosis. To achieve this goal one of the key requirements is efficient utilization of existing diagnostic technologies, among which chest X-ray is the first line of diagnostic tool used for screening for active TB. The presented deep learning pipeline consists of three different state of the art deep learning architectures, to generate, segment and classify lung X-rays. Apart from this image preprocessing, image augmentation, genetic algorithm based hyper parameter tuning and model ensembling were used to to improve the diagnostic process. We were able to achieve classification accuracy of 97.1% (Youden's index-0.941,sensitivity of 97.9% and specificity of 96.2%) which is a considerable improvement compared to the existing work in the literature. In our work, we present an highly accurate, automated TB screening system using chest X-rays, which would be helpful especially for low income countries with low access to qualified medical professionals.      
### 34.LSTM-Aided Hybrid Random Access Scheme for 6G Heterogeneous MTC Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.13537.pdf)
>  An LSTM-aided hybrid random access scheme (LSTMH-RA) is proposed to support diverse quality of service (QoS) requirements in 6G MTC heterogeneous networks where URLLC and mMTC devices coexist. This scheme employs an attention-based LSTM prediction model to predict the number of active URLLC devices, determines the parameters of the multi-user detection algorithm dynamically, and then allows URLLC devices to access the network via a two-step contention-free access procedure, to meet latency and reliability access requirements; mMTC devices access the network via a contentionbased TA-aided access mechanism to meet massive access requirement. We analyze the successful access probability of the LSTMH-RA scheme. Numerical results show that, compared to the benchmark schemes, the LSTMH-RA scheme significantly improves the successful access probability, and satisfies the diverse QoS requirements of URLLC and mMTC devices      
### 35.Channel Estimation for Practical IRS-Assisted OFDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.13521.pdf)
>  Intelligent reflecting surface (IRS), composed of a large number of hardware-efficient passive elements, is deemed as a potential technique for future wireless communications since it can adaptively enhance the propagation environment. In order to effectively utilize IRS to achieve promising beamforming gains, the problem of channel state information (CSI) acquisition needs to be carefully considered. However, most recent works assume to employ an ideal IRS, i.e., each reflecting element has constant amplitude, variable phase shifts, as well as the same response for the signals with different frequencies, which will cause severe estimation error due to the mismatch between the ideal IRS and the practical one. In this paper, we study channel estimation in practical IRS-aided orthogonal frequency division multiplexing (OFDM) systems with discrete phase shifts. Different from the prior works which assume that IRS has an ideal reflection model, we perform channel estimation by considering amplitude-phase shift-frequency relationship for the response of practical IRS. Aiming at minimizing normalized-mean-square-error (NMSE) of the estimated channel, a novel IRS time-varying reflection pattern is designed by leveraging the alternating optimization (AO) algorithm for the case of using low-resolution phase shifters. Moreover, for the high-resolution IRS cases, we provide another practical reflection pattern scheme to further reduce the complexity. Simulation results demonstrate the necessity of considering practical IRS model for channel estimation and the effectiveness of our proposed channel estimation methods.      
### 36.Multi-channel Multi-frame ADL-MVDR for Target Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2012.13442.pdf)
>  Many purely neural network based speech separation approaches have been proposed that greatly improve objective assessment scores, but they often introduce nonlinear distortions that are harmful to automatic speech recognition (ASR). Minimum variance distortionless response (MVDR) filters strive to remove nonlinear distortions, however, these approaches either are not optimal for removing residual (linear) noise, or they are unstable when used jointly with neural networks. In this study, we propose a multi-channel multi-frame (MCMF) all deep learning (ADL)-MVDR approach for target speech separation, which extends our preliminary multi-channel ADL-MVDR approach. The MCMF ADL-MVDR handles different numbers of microphone channels in one framework, where it addresses linear and nonlinear distortions. Spatio-temporal cross correlations are also fully utilized in the proposed approach. The proposed system is evaluated using a Mandarin audio-visual corpora and is compared with several state-of-the-art approaches. Experimental results demonstrate the superiority of our proposed framework under different scenarios and across several objective evaluation metrics, including ASR performance.      
### 37.Lip-reading with Hierarchical Pyramidal Convolution and Self-Attention  [ :arrow_down: ](https://arxiv.org/pdf/2012.14360.pdf)
>  In this paper, we propose a novel deep learning architecture to improving word-level lip-reading. On the one hand, we first introduce the multi-scale processing into the spatial feature extraction for lip-reading. Specially, we proposed hierarchical pyramidal convolution (HPConv) to replace the standard convolution in original module, leading to improvements over the model's ability to discover fine-grained lip movements. On the other hand, we merge information in all time steps of the sequence by utilizing self-attention, to make the model pay more attention to the relevant frames. These two advantages are combined together to further enhance the model's classification power. Experiments on the Lip Reading in the Wild (LRW) dataset show that our proposed model has achieved 86.83% accuracy, yielding 1.53% absolute improvement over the current state-of-the-art. We also conducted extensive experiments to better understand the behavior of the proposed model.      
### 38.WiFresh: Age-of-Information from Theory to Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2012.14337.pdf)
>  Emerging applications, such as smart factories and fleets of drones, increasingly rely on sharing time-sensitive information for monitoring and control. In such application domains, it is essential to keep information fresh, as outdated information loses its value and can lead to system failures and safety risks. The Age-of-Information is a performance metric that captures how fresh the information is from the perspective of the destination. <br>In this paper, we show that as the congestion in the wireless network increases, the Age-of-Information degrades sharply, leading to outdated information at the destination. Leveraging years of theoretical research, we propose WiFresh: an unconventional architecture that achieves near optimal information freshness in wireless networks of any size, even when the network is overloaded. Our experimental results show that WiFresh can improve information freshness by two orders of magnitude when compared to an equivalent standard WiFi network. We propose and realize two strategies for implementing WiFresh: one at the MAC layer using hardware-level programming and another at the Application layer using Python.      
### 39.Causal Inference in Geosciences with Kernel Sensitivity Maps  [ :arrow_down: ](https://arxiv.org/pdf/2012.14303.pdf)
>  Establishing causal relations between random variables from observational data is perhaps the most important challenge in today's Science. In remote sensing and geosciences this is of special relevance to better understand the Earth's system and the complex and elusive interactions between processes. In this paper we explore a framework to derive cause-effect relations from pairs of variables via regression and dependence estimation. We propose to focus on the sensitivity (curvature) of the dependence estimator to account for the asymmetry of the forward and inverse densities of approximation residuals. Results in a large collection of 28 geoscience causal inference problems demonstrate the good capabilities of the method.      
### 40.UWB Propagation Characteristics of Human-to-Robot Communication in Automated Collaborative Warehouse  [ :arrow_down: ](https://arxiv.org/pdf/2012.14278.pdf)
>  Propagation of UWB Gaussian signal in a model of automated collaborative warehouse is analyzed using ray tracing method. The transmitting antenna is placed on the human body while the received power profiles in warehouse containing empty racks and racks filled with different loads are calculated. This gives rise to estimation of safe communication range between humans and robots.      
### 41.Lattice-Free MMI Adaptation Of Self-Supervised Pretrained Acoustic Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.14252.pdf)
>  In this work, we propose lattice-free MMI (LFMMI) for supervised adaptation of self-supervised pretrained acoustic model. We pretrain a Transformer model on thousand hours of untranscribed Librispeech data followed by supervised adaptation with LFMMI on three different datasets. Our results show that fine-tuning with LFMMI, we consistently obtain relative WER improvements of 10% and 35.3% on the clean and other test sets of Librispeech (100h), 10.8% on Switchboard (300h), and 4.3% on Swahili (38h) and 4.4% on Tagalog (84h) compared to the baseline trained only with supervised data.      
### 42.Longitudinal diffusion MRI analysis using Segis-Net: a single-step deep-learning framework for simultaneous segmentation and registration  [ :arrow_down: ](https://arxiv.org/pdf/2012.14230.pdf)
>  This work presents a single-step deep-learning framework for longitudinal image analysis, coined Segis-Net. To optimally exploit information available in longitudinal data, this method concurrently learns a multi-class segmentation and nonlinear registration. Segmentation and registration are modeled using a convolutional neural network and optimized simultaneously for their mutual benefit. An objective function that optimizes spatial correspondence for the segmented structures across time-points is proposed. We applied Segis-Net to the analysis of white matter tracts from N=8045 longitudinal brain MRI datasets of 3249 elderly individuals. Segis-Net approach showed a significant increase in registration accuracy, spatio-temporal segmentation consistency, and reproducibility comparing with two multistage pipelines. This also led to a significant reduction in the sample-size that would be required to achieve the same statistical power in analyzing tract-specific measures. Thus, we expect that Segis-Net can serve as a new reliable tool to support longitudinal imaging studies to investigate macro- and microstructural brain changes over time.      
### 43.An Image Encryption Scheme Based on Chaotic Logarithmic Map and Key Generation using Deep CNN  [ :arrow_down: ](https://arxiv.org/pdf/2012.14156.pdf)
>  A secure and reliable image encryption scheme is presented in this study. The encryption scheme hereby introduces a novel chaotic log-map, deep convolution neural network (CNN) model for key generation, and bit reversion operation for the manipulation process. Thanks to the sensitive key generation, initial values and control parameters are produced for the hyperchaotic log-map, and thus a diverse chaotic sequence is achieved for encrypting operations. The scheme then encrypts the images by scrambling and manipulating the pixels of images through four operations: permutation, DNA encoding, diffusion, and bit reversion. The encryption scheme is precisely examined for the well-known images in terms of various analyses such as keyspace, key sensitivity, information entropy, histogram, correlation, differential attack, noisy attack, and cropping attack. To corroborate the scheme, the visual and numerical results are even compared with available outcomes of the state of the art. Therefore, the proposed log-map based image encryption scheme is successfully verified and validated by the superior absolute and comparative results.      
### 44.From Point to Space: 3D Moving Human Pose Estimation Using Commodity WiFi  [ :arrow_down: ](https://arxiv.org/pdf/2012.14066.pdf)
>  In this paper, we present Wi-Mose, the first 3D moving human pose estimation system using commodity WiFi. Previous WiFi-based works have achieved 2D and 3D pose estimation. These solutions either capture poses from one perspective or construct poses of people who are at a fixed point, preventing their wide adoption in daily scenarios. To reconstruct 3D poses of people who move throughout the space rather than a fixed point, we fuse the amplitude and phase into Channel State Information (CSI) images which can provide both pose and position information. Besides, we design a neural network to extract features that are only associated with poses from CSI images and then convert the features into key-point coordinates. Experimental results show that Wi-Mose can localize key-point with 29.7mm and 37.8mm Procrustes analysis Mean Per Joint Position Error (P-MPJPE) in the Line of Sight (LoS) and Non-Line of Sight (NLoS) scenarios, respectively, achieving higher performance than the state-of-the-art method. The results indicate that Wi-Mose can capture high-precision 3D human poses throughout the space.      
### 45.Aerial Imagery Pile burn detection using Deep Learning: the FLAME dataset  [ :arrow_down: ](https://arxiv.org/pdf/2012.14036.pdf)
>  Wildfires are one of the costliest and deadliest natural disasters in the US, causing damage to millions of hectares of forest resources and threatening the lives of people and animals. Of particular importance are risks to firefighters and operational forces, which highlights the need for leveraging technology to minimize danger to people and property. FLAME (Fire Luminosity Airborne-based Machine learning Evaluation) offers a dataset of aerial images of fires along with methods for fire detection and segmentation which can help firefighters and researchers to develop optimal fire management strategies. This paper provides a fire image dataset collected by drones during a prescribed burning piled detritus in an Arizona pine forest. The dataset includes video recordings and thermal heatmaps captured by infrared cameras. The captured videos and images are annotated and labeled frame-wise to help researchers easily apply their fire detection and modeling algorithms. The paper also highlights solutions to two machine learning problems: (1) Binary classification of video frames based on the presence [and absence] of fire flames. An Artificial Neural Network (ANN) method is developed that achieved a 76% classification accuracy. (2) Fire detection using segmentation methods to precisely determine fire borders. A deep learning method is designed based on the U-Net up-sampling and down-sampling approach to extract a fire mask from the video frames. Our FLAME method approached a precision of 92% and a recall of 84%. Future research will expand the technique for free burning broadcast fire using thermal images.      
### 46.Modeling, Vibration Control, and Trajectory Tracking of a Kinematically Constrained Planar Hybrid Cable-Driven Parallel Robot  [ :arrow_down: ](https://arxiv.org/pdf/2012.14029.pdf)
>  This paper presents a kinematically constrained planar hybrid cable-driven parallel robot (HCDPR) for warehousing applications as well as other potential applications such as rehabilitation. The proposed HCDPR can harness the strengths and benefits of serial and cable-driven parallel robots. Based on this robotic platform, the goal in this paper is to develop an integrated control system to reduce vibrations and improve the trajectory accuracy and performance of the HCDPR, including deriving kinematic and dynamic equations, proposing solutions for redundancy resolution and optimization of stiffness, and developing two motion and vibration control strategies (controllers I and II). Finally, different case studies are conducted to evaluate the control performance, and the results show that the controller II can achieve the goal better.      
### 47.Time-Scale-Chirp_rate Operator for Recovery of Non-stationary Signal Components with Crossover Instantaneous Frequency Curves  [ :arrow_down: ](https://arxiv.org/pdf/2012.14010.pdf)
>  The objective of this paper is to introduce an innovative approach for the recovery of non-stationary signal components with possibly cross-over instantaneous frequency (IF) curves from a multi-component blind-source signal. The main idea is to incorporate a chirp rate parameter with the time-scale continuous wavelet-like transformation, by considering the quadratic phase representation of the signal components. Hence-forth, even if two IF curves cross, the two corresponding signal components can still be separated and recovered, provided that their chirp rates are different. In other words, signal components with the same IF value at any time instant could still be recovered. To facilitate our presentation, we introduce the notion of time-scale-chirp_rate (TSC-R) recovery transform or TSC-R recovery operator to develop a TSC-R theory for the 3-dimensional space of time, scale, chirp rate. Our theoretical development is based on the approximation of the non-stationary signal components with linear chirps and applying the proposed adaptive TSC-R transform to the multi-component blind-source signal to obtain fairly accurate error bounds of IF estimations and signal components recovery. Several numerical experimental results are presented to demonstrate the out-performance of the proposed method over all existing time-frequency and time-scale approaches in the published literature, particularly for non-stationary source signals with crossover IFs.      
### 48.Deep Learning Based Intelligent Inter-Vehicle Distance Control for 6G Enabled Cooperative Autonomous Driving  [ :arrow_down: ](https://arxiv.org/pdf/2012.13817.pdf)
>  Research on the sixth generation cellular networks (6G) is gaining huge momentum to achieve ubiquitous wireless connectivity. Connected autonomous driving (CAV) is a critical vertical envisioned for 6G, holding great potentials of improving road safety, road and energy efficiency. However the stringent service requirements of CAV applications on reliability, latency and high speed communications will present big challenges to 6G networks. New channel access algorithms and intelligent control schemes for connected vehicles are needed for 6G supported CAV. In this paper, we investigated 6G supported cooperative driving, which is an advanced driving mode through information sharing and driving coordination. Firstly we quantify the delay upper bounds of 6G vehicle to vehicle (V2V) communications with hybrid communication and channel access technologies. A deep learning neural network is developed and trained for fast computation of the delay bounds in real time operations. Then, an intelligent strategy is designed to control the inter-vehicle distance for cooperative autonomous driving. Furthermore, we propose a Markov Chain based algorithm to predict the parameters of the system states, and also a safe distance mapping method to enable smooth vehicular speed changes. The proposed algorithms are implemented in the AirSim autonomous driving platform. Simulation results show that the proposed algorithms are effective and robust with safe and stable cooperative autonomous driving, which greatly improve the road safety, capacity and efficiency.      
### 49.Stability-Certified Reinforcement Learning via Spectral Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2012.13744.pdf)
>  In this article, two types of methods from different perspectives based on spectral normalization are described for ensuring the stability of the system controlled by a neural network. The first one is that the L2 gain of the feedback system is bounded less than 1 to satisfy the stability condition derived from the small-gain theorem. While explicitly including the stability condition, the first method may provide an insufficient performance on the neural network controller due to its strict stability condition. To overcome this difficulty, the second one is proposed, which improves the performance while ensuring the local stability with a larger region of attraction. In the second method, the stability is ensured by solving linear matrix inequalities after training the neural network controller. The spectral normalization proposed in this article improves the feasibility of the a-posteriori stability test by constructing tighter local sectors. The numerical experiments show that the second method provides enough performance compared with the first one while ensuring enough stability compared with the existing reinforcement learning algorithms.      
### 50.Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies  [ :arrow_down: ](https://arxiv.org/pdf/2012.13736.pdf)
>  Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at <a class="link-external link-https" href="https://github.com/pshams55/GAN-Case-Study" rel="external noopener nofollow">this https URL</a>.      
### 51.Inception-Based Network and Multi-Spectrogram Ensemble Applied For Predicting Respiratory Anomalies and Lung Diseases  [ :arrow_down: ](https://arxiv.org/pdf/2012.13699.pdf)
>  This paper presents an inception-based deep neural network for detecting lung diseases using respiratory sound input. Recordings of respiratory sound collected from patients are firstly transformed into spectrograms where both spectral and temporal information are well presented, referred to as front-end feature extraction. These spectrograms are then fed into the proposed network, referred to as back-end classification, for detecting whether patients suffer from lung-relevant diseases. Our experiments, conducted over the ICBHI benchmark meta-dataset of respiratory sound, achieve competitive ICBHI scores of 0.53/0.45 and 0.87/0.85 regarding respiratory anomaly and disease detection, respectively.      
### 52.Deep Learning Framework Applied for Predicting Anomaly of Respiratory Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2012.13668.pdf)
>  This paper proposes a robust deep learning framework used for classifying anomaly of respiratory cycles. Initially, our framework starts with front-end feature extraction step. This step aims to transform the respiratory input sound into a two-dimensional spectrogram where both spectral and temporal features are well presented. Next, an ensemble of C- DNN and Autoencoder networks is then applied to classify into four categories of respiratory anomaly cycles. In this work, we conducted experiments over 2017 Internal Conference on Biomedical Health Informatics (ICBHI) benchmark dataset. As a result, we achieve competitive performances with ICBHI average score of 0.49, ICBHI harmonic score of 0.42.      
### 53.Fundamental Limits on Energy-Delay-Accuracy of In-memory Architectures in Inference Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.13645.pdf)
>  This paper obtains fundamental limits on the computational precision of in-memory computing architectures (IMCs). An IMC noise model and associated SNR metrics are defined and their interrelationships analyzed to show that the accuracy of IMCs is fundamentally limited by the compute SNR ($\text{SNR}_{\text{a}}$) of its analog core, and that activation, weight and output precision needs to be assigned appropriately for the final output SNR $\text{SNR}_{\text{T}} \rightarrow \text{SNR}_{\text{a}}$. The minimum precision criterion (MPC) is proposed to minimize the ADC precision. Three in-memory compute models - charge summing (QS), current summing (IS) and charge redistribution (QR) - are shown to underlie most known IMCs. Noise, energy and delay expressions for the compute models are developed and employed to derive expressions for the SNR, ADC precision, energy, and latency of IMCs. The compute SNR expressions are validated via Monte Carlo simulations in a 65 nm CMOS process. For a 512 row SRAM array, it is shown that: 1) IMCs have an upper bound on their maximum achievable $\text{SNR}_{\text{a}}$ due to constraints on energy, area and voltage swing, and this upper bound reduces with technology scaling for QS-based architectures; 2) MPC enables $\text{SNR}_{\text{T}} \rightarrow \text{SNR}_{\text{a}}$ to be realized with minimal ADC precision; 3) QS-based (QR-based) architectures are preferred for low (high) compute SNR scenarios.      
### 54.Echo Chambers and Segregation in Social Networks: Markov Bridge Models and Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2012.13643.pdf)
>  This paper deals with the modeling and estimation of the sociological phenomena called echo chambers and segregation in social networks. Specifically, we present a novel community-based graph model that represents the emergence of segregated echo chambers as a Markov bridge process. A Markov bridge is a one-dimensional Markov random field that facilitates modeling the formation and disassociation of communities at deterministic times which is important in social networks with known timed events. We justify the proposed model with six real world examples and examine its performance on a recent Twitter dataset. We provide model parameter estimation algorithm based on maximum likelihood and, a Bayesian filtering algorithm for recursively estimating the level of segregation using noisy samples obtained from the network. Numerical results indicate that the proposed filtering algorithm outperforms the conventional hidden Markov modeling in terms of the mean-squared error. The proposed filtering method is useful in computational social science where data-driven estimation of the level of segregation from noisy data is required.      
### 55.Real-Time Adaptive Velocity Optimization for Autonomous Electric Cars at the Limits of Handling  [ :arrow_down: ](https://arxiv.org/pdf/2012.13586.pdf)
>  With the evolution of self-driving cars, autonomous racing series like Roborace and the Indy Autonomous Challenge are rapidly attracting growing attention. Researchers participating in these competitions hope to subsequently transfer their developed functionality to passenger vehicles, in order to improve self-driving technology for reasons of safety, and due to environmental and social benefits. The race track has the advantage of being a safe environment where challenging situations for the algorithms are permanently created. To achieve minimum lap times on the race track, it is important to gather and process information about external influences including, e.g., the position of other cars and the friction potential between the road and the tires. Furthermore, the predicted behavior of the ego-car's propulsion system is crucial for leveraging the available energy as efficiently as possible. In this paper, we therefore present an optimization-based velocity planner, mathematically formulated as a multi-parametric Sequential Quadratic Problem (mpSQP). This planner can handle a spatially and temporally varying friction coefficient, and transfer a race Energy Strategy (ES) to the road. It further handles the velocity-profile-generation task for performance and emergency trajectories in real time on the vehicle's Electronic Control Unit (ECU).      
### 56.Toward Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework  [ :arrow_down: ](https://arxiv.org/pdf/2012.13567.pdf)
>  A conventional brain-computer interface (BCI) requires a complete data gathering, training, and calibration phase for each user before it can be used. This preliminary phase is time-consuming and should be done under the supervision of technical experts commonly in laboratories for the BCI to function properly. In recent years, a number of subject-independent (SI) BCIs have been developed. However, there are many problems preventing them from being used in real-world BCI applications. A lower accuracy than the subject-dependent (SD) approach and a relatively high run-time of models with a large number of model parameters are the most important ones. Therefore, a real-world BCI application would greatly benefit from a compact subject-independent BCI framework, ready to use immediately after the user puts it on, and suitable for low-power edge-computing and applications in the emerging area of internet of things (IoT). We propose a novel subject-independent BCI framework named CCSPNet (Convolutional Common Spatial Pattern Network) that is trained on the motor imagery (MI) paradigm of a large-scale EEG signals database consisting of 400 trials for every 54 subjects performing two-class hand-movement MI tasks. The proposed framework applies a wavelet kernel convolutional neural network (WKCNN) and a temporal convolutional neural network (TCNN) in order to represent and extract the diverse frequency behavior and spectral patterns of EEG signals. The convolutional layers outputs go through a CSP algorithm for class discrimination and spatial feature extraction. The number of CSP features is reduced by a dense neural network, and the final class label is determined by an LDA. The final SD and SI classification accuracies of the proposed framework match the best results obtained on the largest motor-imagery dataset present in the BCI literature, with 99.993 percent fewer model parameters.      
### 57.PDRS: A Fast Non-iterative Scheme for Massive Grant-free Access in Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2012.13550.pdf)
>  Grant-free multiple-input multiple-output (MIMO) usually employs non-orthogonal pilots for joint user detection and channel estimation. However, existing methods are too complex for massive grant-free access in massive MIMO. This letter proposes pilot detection reference signal (PDRS) to greatly reduce the complexity. In PDRS scheme, no iteration is required. Direct weight estimation is also proposed to calculate combining weights without channel estimation. After combining, PDRS recovery errors are used to decide the pilot activity. The simulation results show that the proposed grant-free scheme performs good with a complexity reduced by orders of magnitude.      
### 58.Distributional Ground Truth: Non-Redundant Crowdsourcing Data Quality Control in UI Labeling Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2012.13546.pdf)
>  HCI increasingly employs Machine Learning and Image Recognition, in particular for visual analysis of user interfaces (UIs). A popular way for obtaining human-labeled training data is Crowdsourcing, typically using the quality control methods ground truth and majority consensus, which necessitate redundancy in the outcome. In our paper we propose a non-redundant method for prediction of crowdworkers' output quality in web UI labeling tasks, based on homogeneity of distributions assessed with two-sample Kolmogorov-Smirnov test. Using a dataset of about 500 screenshots with over 74,000 UI elements located and classified by 11 trusted labelers and 298 Amazon Mechanical Turk crowdworkers, we demonstrate the advantage of our approach over the baseline model based on mean Time-on-Task. Exploring different dataset partitions, we show that with the trusted set size of 17-27% UIs our "distributional ground truth" model can achieve R2s of over 0.8 and help to obviate the ancillary work effort and expenses.      
### 59.A GCICA Grant-Free Random Access Scheme for M2M Communications in Crowded Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.13539.pdf)
>  A high success rate of grant-free random access scheme is proposed to support massive access for machine-to-machine communications in massive multipleinput multiple-output systems. This scheme allows active user equipments (UEs) to transmit their modulated uplink messages along with super pilots consisting of multiple sub-pilots to a base station (BS). Then, the BS performs channel state information (CSI) estimation and uplink message decoding by utilizing a proposed graph combined clustering independent component analysis (GCICA) decoding algorithm, and then employs the estimated CSIs to detect active UEs by utilizing the characteristic of asymptotic favorable propagation of massive MIMO channel. We call this proposed scheme as GCICA based random access (GCICA-RA) scheme. We analyze the successful access probability, missed detection probability, and uplink throughput of the GCICA-RA scheme. Numerical results show that, the GCICA-RA scheme significantly improves the successful access probability and uplink throughput, decreases missed detection probability, and provides low CSI estimation error at the same time.      
### 60.Performance of Dual-Hop Relaying for THz-RF Wireless Link  [ :arrow_down: ](https://arxiv.org/pdf/2012.13505.pdf)
>  The use of Terahertz (THz) frequency bands for data transmissions between the core network and an access point can be promising for next generation wireless systems. In this paper, we analyze the performance of a dual-hop relaying for THz-RF wireless link for backhaul applications. Considering the $\alpha-\mu$ fading channel and a statistical model of pointing errors, we derive a novel closed-form expression of the cumulative distribution function (CDF) of the signal-to-noise ratio (SNR) for the THz link, which is also valid for non-integer values of $\mu$. Using the CDF, we derive analytical expressions of the end-to-end SNR and lower bound on ergodic capacity of a decode-and-forward (DF) assisted THz-RF relaying in terms of system parameters. Using analytical results of the direct link and computer simulations, we demonstrate that the THz-RF relaying is a viable technology for wireless backhaul, providing a significant increase of almost $25 \%$ in the spectral efficiency, compared to the direct transmissions.      
### 61.LMMSE Processing for Cell-free Massive MIMO with Radio Stripes and MRC Fronthaul  [ :arrow_down: ](https://arxiv.org/pdf/2012.13504.pdf)
>  Cell-free massive MIMO provides ubiquitous connectivity for multiple users, and implementation using radio stripes is very efficient. Compared with collocated massive MIMO, the major cost includes fronthaul overheads and AP hardware. Maximum ratio combination (MRC) achieves a low fronthaul loading and low-cost AP, but the performance is bad. This letter proposes to implement a quasi-LMMSE (Q-LMMSE) processing using MRC fronthaul design. Q-LMMSE is derived from a standard LMMSE, which gains interference information from MRC signal via singular value decomposition. Simulation results show that the proposed Q-LMMSE increases the spectral efficiency by several times using same MRC fronthaul.      
### 62.A Cascaded Residual UNET for Fully Automated Segmentation of Prostate and Peripheral Zone in T2-weighted 3D Fast Spin Echo Images  [ :arrow_down: ](https://arxiv.org/pdf/2012.13501.pdf)
>  Multi-parametric MR images have been shown to be effective in the non-invasive diagnosis of prostate cancer. Automated segmentation of the prostate eliminates the need for manual annotation by a radiologist which is time consuming. This improves efficiency in the extraction of imaging features for the characterization of prostate tissues. In this work, we propose a fully automated cascaded deep learning architecture with residual blocks, Cascaded MRes-UNET, for segmentation of the prostate gland and the peripheral zone in one pass through the network. The network yields high Dice scores ($0.91\pm.02$), precision ($0.91\pm.04$), and recall scores ($0.92\pm.03$) in prostate segmentation compared to manual annotations by an experienced radiologist. The average difference in total prostate volume estimation is less than 5%.      
### 63.Real-Time Optimization of the Current Steering for Visual Prosthesis  [ :arrow_down: ](https://arxiv.org/pdf/2012.13467.pdf)
>  Current steering on a multi-electrode array is commonly used to shape the electric field in the neural tissue in order to improve selectivity and efficacy of stimulation. Previously, simulations of the electric field in tissue required separate computation for each set of the stimulation parameters. Not only is this approach to modeling time-consuming and very difficult with a large number of electrodes, it is incompatible with real-time optimization of the current steering for practical applications. We present a framework for efficient computation of the electric field in the neural tissue based on superposition of the fields from a pre-calculated basis. Such linear algebraic framework enables optimization of the current steering for any targeted electric field in real time. For applications to retinal prosthetics, we demonstrate how the stimulation depth can be optimized for each patient based on the retinal thickness and separation from the array, while maximizing the lateral confinement of the electric field essential for spatial resolution.      
### 64.Road Traffic Monitoring using DSRC Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.13448.pdf)
>  A wide variety of sensor technologies are nowadays used for traffic monitoring applications. Since most of these technologies rely on wired infrastructure, the installation and maintenance costs limit the deployment of the traffic monitoring systems. In this paper, we introduce a traffic monitoring approach that exploits dedicated short-range communications (DSRC) signals sent in a vehicular network and machine learning techniques. We verify the feasibility of the proposed approach with extensive simulations and real-world experiments at an intersection. We first simulate wireless channels under realistic traffic conditions using a ray-tracing simulator and a traffic simulator. Next, we conduct experiments in a real-world environment and collect DSRC messages transmitted from a roadside unit (RSU). The results show that we are able to separate different traffic intensities with an accuracy of 96.3\% and 87.6\% on the simulation and experimental data, respectively. We also estimate the number of vehicles on the road with a weighted mean absolute percentage error (WMAPE) of 10.7\% and 19.7\% on simulation and experimental data, respectively. The proposed approach is suitable to be deployed alongside the current monitoring systems to improve the performance of the systems without requiring additional investment in infrastructure.      
### 65.Timely Tracking of Infection Status of Individuals in a Population  [ :arrow_down: ](https://arxiv.org/pdf/2012.13393.pdf)
>  We consider real-time timely tracking of infection status (e.g., covid-19) of individuals in a population. In this work, a health care provider wants to detect infected people as well as people who recovered from the disease as quickly as possible. In order to measure the timeliness of the tracking process, we use the long-term average difference between the actual infection status of the people and their real-time estimate by the health care provider based on the most recent test results. We first find an analytical expression for this average difference for given test rates, and given infection and recovery rates of people. Next, we propose an alternating minimization based algorithm to minimize this average difference. We observe that if the total test rate is limited, instead of testing all members of the population equally, only a portion of the population is tested based on their infection and recovery rates. We also observe that increasing the total test rate helps track the infection status better. In addition, an increased population size increases diversity of people with different infection and recovery rates, which may be exploited to spend testing capacity more efficiently, thereby improving the system performance. Finally, depending on the health care provider's preferences, test rate allocation can be altered to detect either the infected people or the recovered people more quickly.      
### 66.Multilinear Control Systems Theory  [ :arrow_down: ](https://arxiv.org/pdf/1905.08783.pdf)
>  In this paper, we provide a system theoretic treatment of a new class of multilinear time-invariant (MLTI) systems in which the states, inputs and outputs are tensors, and the system evolution is governed by multilinear operators. The MLTI system representation is based on the Einstein product and even-order paired tensors. There is a particular tensor unfolding which gives rise to an isomorphism from this tensor space to the general linear group, i.e. the group of invertible matrices. By leveraging this unfolding operation, one can extend classical linear time-invariant (LTI) system notions including stability, reachability and observability to MLTI systems. While the unfolding based formulation is a powerful theoretical construct, the computational advantages of MLTI systems can only be fully realized while working with the tensor form, where hidden patterns/structures can be exploited for efficient representations and computations. Along these lines, we establish new results which enable one to express tensor unfolding based stability, reachability and observability criteria in terms of more standard notions of tensor ranks/decompositions. In addition, we develop a generalized CANDECOMP/PARAFAC decomposition and tensor train decomposition based model reduction framework, which can significantly reduce the number of MLTI system parameters. We demonstrate our framework with numerical examples.      
