# ArXiv eess --Tue, 22 Dec 2020
### 1.Deep Learning in Detection and Diagnosis of Covid-19 using Radiology Modalities: A Systematic Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.11577.pdf)
>  Purpose: Early detection and diagnosis of Covid-19 and accurate separation of patients with non-Covid-19 cases at the lowest cost and in the early stages of the disease are one of the main challenges in the epidemic of Covid-19. Concerning the novelty of the disease, the diagnostic methods based on radiological images suffer shortcomings despite their many uses in diagnostic centers. Accordingly, medical and computer researchers tended to use machine-learning models to analyze radiology images. <br>Methods: Present systematic review was conducted by searching three databases of PubMed, Scopus, and Web of Science from November 1, 2019, to July 20, 2020 Based on a search strategy, the keywords were Covid-19, Deep learning, Diagnosis and Detection leading to the extraction of 168 articles that ultimately, 37 articles were selected as the research population by applying inclusion and exclusion criteria. Result: This review study provides an overview of the current state of all models for the detection and diagnosis of Covid-19 through radiology modalities and their processing based on deep learning. According to the finding, Deep learning Based models have an extraordinary capacity to achieve an accurate and efficient system for the detection and diagnosis of Covid-19, which using of them in the processing of CT-Scan and X-Ray images, would lead to a significant increase in sensitivity and specificity values. <br>Conclusion: The Application of Deep Learning (DL) in the field of Covid-19 radiologic image processing leads to the reduction of false-positive and negative errors in the detection and diagnosis of this disease and provides an optimal opportunity to provide fast, cheap, and safe diagnostic services to patients.      
### 2.Reflective Parametric Frequency Selective Limiters with sub-dB Loss and $μ$Watts Power Thresholds  [ :arrow_down: ](https://arxiv.org/pdf/2012.11546.pdf)
>  This article describes the design methodology to achieve reflective diode-based parametric frequency selective limiters (pFSLs) with low power thresholds ($P_{th}$) and sub-dB insertion-loss values ($IL^{s.s}$) for driving power levels ($P_{in}$) lower than $P_{th}$. In addition, we present the measured performance of a reflective pFSL designed through the discussed methodology and assembled on a FR-4 printed circuit board (PCB). Thanks to its optimally engineered dynamics, the built pFSL can operate around $\sim$2.1 GHz while exhibiting record-low $P_{th}$ (-3.4 dBm) and $IL^{s.s}$ (0.94 dB) values. Furthermore, while the pFSL can selectively attenuate undesired signals with power ranging from -3.4 dBm to 13 dBm, it provides a strong suppression level (IS &gt; 12.0 dB) even when driven by much higher $P_{in}$ values approaching 28 dBm. Such measured performance metrics demonstrate how the unique nonlinear dynamics of parametric-based FSLs can be leveraged through components and systems compatible with conventional chip-scale manufacturing processes in order to increase the resilience to electromagnetic interference (EMI), even of wireless radios designed for a low-power consumption and consequently characterized by a narrow dynamic range.      
### 3.An Efficient and Incentive-Compatible Mechanism for Energy Storage Markets  [ :arrow_down: ](https://arxiv.org/pdf/2012.11540.pdf)
>  A key obstacle to increasing renewable energy penetration in the power grid is the lack of utility-scale storage capacity. Transportation electrification has the potential to overcome this obstacle since Electric Vehicles (EVs) that are not in transit can provide battery storage as a service to the grid. This is referred to as EV-Power grid integration, and could potentially be a key milestone in the pathway to decarbonize the electricity and the transportation sectors. We first show that if EV-Power grid integration is not done carefully, then contrary to improving the cost efficiency of operating the grid, it could in fact be counterproductive to it. This fundamentally occurs due to two phenomena operating in tandem - the randomness of EV usage patterns and the possibility of strategic behavior by EV operators. We present a market-based solution to address this issue. Specifically, we develop a mechanism for energy storage markets using which the system operator can efficiently integrate a fleet of strategic EVs with random usage patterns into the grid, utilize them for storage, and satisfy the demand at minimum possible cost.      
### 4.Monotone Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2012.11533.pdf)
>  Maximal monotonicity is explored as a generalization of the linear theory of passivity, which allows for algorithmic system analysis of an important physical property. The theory is developed for nonlinear 1-port circuits, modelled as port interconnections of the four fundamental elements: resistors, capacitors, inductors and memristors. An algorithm for computing the steady state periodic behavior of such a circuit is presented.      
### 5.A Shift-insensitive Full Reference Image Quality Assessment Model Based on Quadratic Sum of Gradient Magnitude and LOG signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.11525.pdf)
>  Image quality assessment that aims at estimating the subject quality of images, builds models to evaluate the perceptual quality of the image in different applications. Based on the fact that the human visual system (HVS) is highly sensitive to structural information, the edge information extraction is widely applied in different IQA metrics. According to previous studies, the image gradient magnitude (GM) and the Laplacian of Gaussian (LOG) operator are two efficient structural features in IQA tasks. However, most of the IQA metrics achieve good performance only when the distorted image is totally registered with the reference image, but fail to perform on images with small translations. In this paper, we propose an FR-IQA model with the quadratic sum of the GM and the LOG signals, which obtains good performance in image quality estimation considering shift-insensitive property for not well-registered reference and distortion image pairs. Experimental results show that the proposed model works robustly on three large scale subjective IQA databases which contain a variety of distortion types and levels, and stays in the state-of-the-art FR-IQA models no matter for single distortion type or across whole database. Furthermore, we validated that the proposed metric performs better with shift-insensitive property compared with the CW-SSIM metric that is considered to be shift-insensitive IQA so far. Meanwhile, the proposed model is much simple than the CW-SSIM, which is efficient for applications.      
### 6.Single module identifiability in linear dynamic networks with partial excitation and measurement  [ :arrow_down: ](https://arxiv.org/pdf/2012.11414.pdf)
>  Identifiability of a single module in a network of transfer functions is determined by the question whether a particular transfer function in the network can be uniquely distinguished within a network model set, on the basis of data. Whereas previous research has focused on the situations that all network signals are either excited or measured, we develop generalized analysis results for the situation of partial measurement and partial excitation. As identifiability conditions typically require a sufficient number of external excitation signals, this work introduces a novel network model structure such that excitation from unmeasured noise signals is included, which leads to less conservative identifiability conditions than relying on measured excitation signals only. More importantly, graphical conditions are developed to verify global and generic identifiability of a single module based on the topology of the dynamic network. Depending on whether the input or the output of the module can be measured, we present four identifiability conditions which cover all possible situations in single module identification. These conditions further lead to synthesis approaches for allocating excitation signals and selecting measured signals, to warrant single module identifiability. In addition, if the identifiability conditions are satisfied, indirect identification methods are developed to provide a consistent estimate of the module. All the obtained results are also extended to identifiability of multiple modules in the network.      
### 7.Gradient-free training of autoencoders for non-differentiable communication channels  [ :arrow_down: ](https://arxiv.org/pdf/2012.11227.pdf)
>  Training of autoencoders using the backpropagation algorithm is challenging for non-differential channel models or in an experimental environment where gradients cannot be computed. In this paper, we study a gradient-free training method based on the cubature Kalman filter. To numerically investigate the method, the autoencoder is employed to perform geometric constellation shaping on a non-differentiable communication channel that includes: laser phase noise, additive white Gaussian noise and blind phase search-based phase noise compensation. Our results indicate that the autoencoder can be successfully optimized using the proposed training method. We also show that the learned constellations are more robust to residual phase noise with respect to standard constellation schemes such as Quadratude Amplitude Modulation and Iterative Polar Modulation for the considered conditions.      
### 8.Automatic Diagnosis of Pneumothorax from Chest Radiographs: A Systematic Literature Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.11214.pdf)
>  Among various medical imaging tools, chest radiographs are the most important and widely used diagnostic tool for detection of thoracic pathologies. Researches are being carried out in order to propose robust automatic diagnostic tool for detection of pathologies from chest radiographs. Artificial Intelligence techniques especially deep learning methodologies have been found to be giving promising results in automating the field of medicine. Lot of research has been done for automatic and fast detection of pneumothorax from chest radiographs while proposing several frameworks based on artificial intelligence and machine learning techniques. This study summarizes the existing literature for the automatic detection of pneumothorax from chest x-rays along with describing the available chest radiographs datasets. The comparative analysis of the literature is also provided in terms of goodness and limitations of the existing literature along with highlighting the research gaps which need to be further explored. The paper provides a brief overview of the present work for pneumothorax detection for helping the researchers in selection of optimal approach for future research.      
### 9.A Multi-View Dynamic Fusion Framework: How to Improve the Multimodal Brain Tumor Segmentation from Multi-Views?  [ :arrow_down: ](https://arxiv.org/pdf/2012.11211.pdf)
>  When diagnosing the brain tumor, doctors usually make a diagnosis by observing multimodal brain images from the axial view, the coronal view and the sagittal view, respectively. And then they make a comprehensive decision to confirm the brain tumor based on the information obtained from multi-views. Inspired by this diagnosing process and in order to further utilize the 3D information hidden in the dataset, this paper proposes a multi-view dynamic fusion framework to improve the performance of brain tumor segmentation. The proposed framework consists of 1) a multi-view deep neural network architecture, which represents multi learning networks for segmenting the brain tumor from different views and each deep neural network corresponds to multi-modal brain images from one single view and 2) the dynamic decision fusion method, which is mainly used to fuse segmentation results from multi-views as an integrate one and two different fusion methods, the voting method and the weighted averaging method, have been adopted to evaluate the fusing process. Moreover, the multi-view fusion loss, which consists of the segmentation loss, the transition loss and the decision loss, is proposed to facilitate the training process of multi-view learning networks so as to keep the consistency of appearance and space, not only in the process of fusing segmentation results, but also in the process of training the learning network. \par By evaluating the proposed framework on BRATS 2015 and BRATS 2018, it can be found that the fusion results from multi-views achieve a better performance than the segmentation result from the single view and the effectiveness of proposed multi-view fusion loss has also been proved. Moreover, the proposed framework achieves a better segmentation performance and a higher efficiency compared to other counterpart methods.      
### 10.An Integrated Human-physical Framework for Control of Power Grids  [ :arrow_down: ](https://arxiv.org/pdf/2012.11208.pdf)
>  In this paper, we bridge two disciplines: systems &amp; control and environmental psychology. We develop second order Behavior and Personal norm (BP) based models (which are consistent with some studies on opinion dynamics) for describing and predicting human activities related to the final use of energy, where psychological variables, financial incentives and social interactions are considered. Based on these models, we develop a human-physical system (HPS) framework consisting of three layers: (i) human behavior, (ii) personal norms and (iii) the physical system (i.e., an AC power grid). Then, we formulate a social-physical welfare optimization problem and solve it by designing a primal-dual controller, which generates the optimal incentives to humans and the control inputs to the power grid. Finally, we assess in simulation the proposed models and approaches.      
### 11.Knowledge-Driven Machine Learning: Concept, Model and Case Study on Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2012.11178.pdf)
>  The power of big data and machine learning has been drastically demonstrated in many fields during the past twenty years which somehow leads to the vague even false understanding that the huge amount of precious human knowledge accumulated to date no longer seems to matter. In this paper, we are pioneering to propose the knowledge-driven machine learning(KDML) model to exhibit that knowledge can play an important role in machine learning tasks. KDML takes advantage of domain knowledge to processes the input data by space transforming without any training which enable the space of input and the output data of the neural networks to be identical, so that we can simplify the machine learning network structure and reduce training costs significantly. Channel estimation problems considering the time selective and frequency selective fading in wireless communications are taken as a case study, where we choose least square(LS) and minimum meansquare error(MMSE) as knowledge module and Long Short Term Memory(LSTM) as learning module. The performance obtained by KDML channel estimator obviously outperforms that of knowledge processing or conventional machine learning, respectively. Our work sheds light on the new area of machine learning and knowledge processing.      
### 12.Unsupervised Cross-Lingual Speech Emotion Recognition Using DomainAdversarial Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.11174.pdf)
>  By using deep learning approaches, Speech Emotion Recog-nition (SER) on a single domain has achieved many excellentresults. However, cross-domain SER is still a challenging taskdue to the distribution shift between source and target <a class="link-external link-http" href="http://domains.In" rel="external noopener nofollow">this http URL</a> this work, we propose a Domain Adversarial Neural Net-work (DANN) based approach to mitigate this distribution shiftproblem for cross-lingual SER. Specifically, we add a languageclassifier and gradient reversal layer after the feature extractor toforce the learned representation both language-independent andemotion-meaningful. Our method is unsupervised, i. e., labelson target language are not required, which makes it easier to ap-ply our method to other languages. Experimental results showthe proposed method provides an average absolute improve-ment of 3.91% over the baseline system for arousal and valenceclassification task. Furthermore, we find that batch normaliza-tion is beneficial to the performance gain of DANN. Thereforewe also explore the effect of different ways of data combinationfor batch normalization.      
### 13.A Comprehensive Survey of Machine Learning Based Localization with Wireless Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.11171.pdf)
>  The last few decades have witnessed a growing interest in location-based services. Using localization systems based on Radio Frequency (RF) signals has proven its efficacy for both indoor and outdoor applications. However, challenges remain with respect to both complexity and accuracy of such systems. Machine Learning (ML) is one of the most promising methods for mitigating these problems, as ML (especially deep learning) offers powerful practical data-driven tools that can be integrated into localization systems. In this paper, we provide a comprehensive survey of ML-based localization solutions that use RF signals. The survey spans different aspects, ranging from the system architectures, to the input features, the ML methods, and the datasets. <br>A main point of the paper is the interaction between the domain knowledge arising from the physics of localization systems, and the various ML approaches. Besides the ML methods, the utilized input features play a major role in shaping the localization solution; we present a detailed discussion of the different features and what could influence them, be it the underlying wireless technology or standards or the preprocessing techniques. A detailed discussion is dedicated to the different ML methods that have been applied to localization problems, discussing the underlying problem and the solution structure. Furthermore, we summarize the different ways the datasets were acquired, and then list the publicly available ones. Overall, the survey categorizes and partly summarizes insights from almost 400 papers in this field. <br>This survey is self-contained, as we provide a concise review of the main ML and wireless propagation concepts, which shall help the researchers in either field navigate through the surveyed solutions, and suggested open problems.      
### 14.Electric Vehicle Aggregator as an Automatic Reserves Provider in the European Market Setting  [ :arrow_down: ](https://arxiv.org/pdf/2012.11158.pdf)
>  Shift of the power system generation from the fossil to the variable renewable sources prompted the system operators to search for new sources of flexibility, that is, new reserve providers. With the introduction of electric vehicles, smart charging emerged as one of the relevant solutions. However, electric vehicle aggregators face the uncertainty of reserve activation on one side and electric vehicle availability on the other. These uncertainty can have a negative effect on both the aggregators' profitability and their users' comfort.State-of-the art literature mostly neglects the reserve activation or the related uncertainty. Also, they rarely model European markets or use real balancing data. This paper introduces a new method for modeling the reserve activation uncertainty based on actual data for the European power system. Three electric vehicle scheduling models were designed and tested: the deterministic, the stochastic and the robust one. The results demonstrate that the current deterministic approaches inaccurately represent the activation uncertainty and that the proposed models that consider uncertainty, both the stochastic and the robust one, substantially improve the results.      
### 15.Explainable Machine Learning based Transform Coding for High Efficiency Intra Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2012.11152.pdf)
>  Machine learning techniques provide a chance to explore the coding performance potential of transform. In this work, we propose an explainable transform based intra video coding to improve the coding efficiency. Firstly, we model machine learning based transform design as an optimization problem of maximizing the energy compaction or decorrelation capability. The explainable machine learning based transform, i.e., Subspace Approximation with Adjusted Bias (Saab) transform, is analyzed and compared with the mainstream Discrete Cosine Transform (DCT) on their energy compaction and decorrelation capabilities. Secondly, we propose a Saab transform based intra video coding framework with off-line Saab transform learning. Meanwhile, intra mode dependent Saab transform is developed. Then, Rate Distortion (RD) gain of Saab transform based intra video coding is theoretically and experimentally analyzed in detail. Finally, three strategies on integrating the Saab transform and DCT in intra video coding are developed to improve the coding efficiency. Experimental results demonstrate that the proposed 8$\times$8 Saab transform based intra video coding can achieve Bjønteggard Delta Bit Rate (BDBR) from -1.19% to -10.00% and -3.07% on average as compared with the mainstream 8$\times$8 DCT based coding scheme.      
### 16.Resting-state EEG sex classification using selected brain connectivity representation  [ :arrow_down: ](https://arxiv.org/pdf/2012.11105.pdf)
>  Effective analysis of EEG signals for potential clinical applications remains a challenging task. So far, the analysis and conditioning of EEG have largely remained sex-neutral. This paper employs a machine learning approach to explore the evidence of sex effects on EEG signals, and confirms the generality of these effects by achieving successful sex prediction of resting-state EEG signals. We have found that the brain connectivity represented by the coherence between certain sensor channels are good predictors of sex.      
### 17.Optimized 2D CA-CFAR for Drone-Mounted Radar Signal Processing Using Integral Image Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2012.11077.pdf)
>  Buried survivor detection in the post-disaster environment by employing radar as sensor is an appealing approach. However, the implementation in the real field is challenging especially for large observation missions. Mounting the radar on the flying drone is the most promising solution. In this case, since the limitations of drones such as low computer specification and limited power resources, an efficient radar data processing is crucially required. Hence, this paper study about the implementation of the integral image technique to optimize the computation of the signal processing step of ultra-wideband impulse radar signatures. The evaluation was held on the single board computer mounted on the developed multisensory drone. The results confirm that the developed method can relatively reduce the data processing time.      
### 18.Experimental Study of Through-the-Wall Respiration Sign Detection Using Ultra-wideband Impulse Radar  [ :arrow_down: ](https://arxiv.org/pdf/2012.11044.pdf)
>  The through-the-wall human being detection and localization in the complex environments by using the radar system has many possible applications such as for law enforcement, and search and rescue missions in disaster-stricken areas. This paper discusses the experimental study of ultra-wideband impulse radar for obscured human respiration detection through spectrum analysis. We investigate the effect of the range of human-wall and radar-wall to the peak factor value of human respiration sign power spectrum. This result will be useful as a reference for analyzing the possibility of the sign to be detected by the radar with respect to the distance from both sides. According to the experimental results, longer both distance increases gradually the peak factor value. However, the peak became narrower. Besides, at a very short distance between areas and walls, the peak factor became very small because of the strong reflection of both objects. Therefore, an adaptive threshold technique and background suppression are needed in order to maintain the detection performance against any possible conditions.      
### 19.Parameter Identification for Digital Fabrication: A Gaussian Process Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.11022.pdf)
>  Tensioned cable nets can be used as supporting structures for the efficient construction of lightweight building elements, such as thin concrete shell structures. To guarantee important mechanical properties of the latter, the tolerances on deviations of the tensioned cable net geometry from the desired target form are very tight. Therefore, the form needs to be readjusted on the construction site. In order to employ model-based optimization techniques, the precise identification of important uncertain model parameters of the cable net system is required. This paper proposes the use of Gaussian process regression to learn the function that maps the cable net geometry to the uncertain parameters. In contrast to previously proposed methods, this approach requires only a single form measurement for the identification of the cable net model parameters. This is beneficial since measurements of the cable net form on the construction site are very expensive. For the training of the Gaussian processes, simulated data is efficiently computed via convex programming. The effectiveness of the proposed method and the impact of the precise identification of the parameters on the form of the cable net are demonstrated in numerical experiments on a quarter-scale prototype of a roof structure.      
### 20.Recent Developments in Detection of Central Serous Retinopathy through Imaging and Artificial Intelligence Techniques A Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.10961.pdf)
>  The Central Serous Retinopathy (CSR) is a major significant disease responsible for causing blindness and vision loss among numerous people across the globe. This disease is also known as the Central Serous Chorioretinopathy (CSC) occurs due to the accumulation of watery fluids behind the retina. The detection of CSR at an early stage allows taking preventive measures to avert any impairment to the human eye. Traditionally, several manual detection methods were developed for observing CSR, but they were proven to be inaccurate, unreliable, and time-consuming. Consequently, the research community embarked on seeking automated solutions for CSR detection. With the advent of modern technology in the 21st century, Artificial Intelligence (AI) techniques are immensely popular in numerous research fields including the automated CSR detection. This paper offers a comprehensive review of various advanced technologies and researches, contributing to the automated CSR detection in this scenario. Additionally, it discusses the benefits and limitations of many classical imaging methods ranging from Optical Coherence Tomography (OCT) and the Fundus imaging, to more recent approaches like AI based Machine/Deep Learning techniques. Study primary objective is to analyze and compare many Artificial Intelligence (AI) algorithms that have efficiently achieved automated CSR detection using OCT imaging. Furthermore, it describes various retinal datasets and strategies proposed for CSR assessment and accuracy. Finally, it is concluded that the most recent Deep Learning (DL) classifiers are performing accurate, fast, and reliable detection of CSR.      
### 21.MA-Unet: An improved version of Unet based on multi-scale and attention mechanism for medical image segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.10952.pdf)
>  Although convolutional neural networks (CNNs) are promoting the development of medical image semantic segmentation, the standard model still has some shortcomings. First, the feature mapping from the encoder and decoder sub-networks in the skip connection operation has a large semantic difference. Second, the remote feature dependence is not effectively modeled. Third, the global context information of different scales is ignored. In this paper, we try to eliminate semantic ambiguity in skip connection operations by adding attention gates (AGs), and use attention mechanisms to combine local features with their corresponding global dependencies, explicitly model the dependencies between channels and use multi-scale predictive fusion to utilize global information at different scales. Compared with other state-of-the-art segmentation networks, our model obtains better segmentation performance while introducing fewer parameters.      
### 22.Domain-adaptive Fall Detection Using Deep Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2012.10911.pdf)
>  Fall detection (FD) systems are important assistive technologies for healthcare that can detect emergency fall events and alert caregivers. However, it is not easy to obtain large-scale annotated fall events with various specifications of sensors or sensor positions, during the implementation of accurate FD systems. Moreover, the knowledge obtained through machine learning has been restricted to tasks in the same domain. The mismatch between different domains might hinder the performance of FD systems. Cross-domain knowledge transfer is very beneficial for machine-learning based FD systems to train a reliable FD model with well-labeled data in new environments. In this study, we propose domain-adaptive fall detection (DAFD) using deep adversarial training (DAT) to tackle cross-domain problems, such as cross-position and cross-configuration. The proposed DAFD can transfer knowledge from the source domain to the target domain by minimizing the domain discrepancy to avoid mismatch problems. The experimental results show that the average F1score improvement when using DAFD ranges from 1.5% to 7% in the cross-position scenario, and from 3.5% to 12% in the cross-configuration scenario, compared to using the conventional FD model without domain adaptation training. The results demonstrate that the proposed DAFD successfully helps to deal with cross-domain problems and to achieve better detection performance.      
### 23.Hierarchical Structure Design and Primary Energy Dispatching Strategy of Grid Energy Router  [ :arrow_down: ](https://arxiv.org/pdf/2012.10874.pdf)
>  As a core device of energy Internet, the energy router is deployed to manage energy flow between the renewable energy and electric grid. In this paper, a hierarchical structure of grid energy router is proposed to greatly facilitate peer-to-peer energy sharing among energy routers. It can be placed at critical buses to make active distribution networks develop into multiple interconnected prosumer-based autonomous systems. To alleviate the mismatch between the medium-time dispatch and device-level control caused by the forecast error of distributed generation, a bi-level primary energy dispatching strategy is proposed to fully utilize the energy buffer of multiple grid energy routers. The power variation in short-time scale is well suppressed by sharing energy buffer in the upper-level control, and the energy buffer is further optimized to better absorb the variation. Combining measured information, the lower-level control is designed to track the optimized instruction of energy buffer in real-time scale, which is a distributed process. The power flow constraint is assumed to be handled by medium-time dispatch, and the current constraint of the device is only taken into consideration. Finally, simulation results demonstrate the effectiveness of proposed hierarchical structure and primary energy dispatching strategy of the grid energy router.      
### 24.Robust Alignment of Multi-Exposed Images with Saturated Regions  [ :arrow_down: ](https://arxiv.org/pdf/2012.10872.pdf)
>  It is challenging to align multi-exposed images due to large illumination variations, especially in presence of saturated regions. In this paper, a novel image alignment algorithm is proposed to cope with the multi-exposed images with saturated regions. Specifically, the multi-exposed images are first normalized by using intensity mapping functions (IMFs) in consideration of saturated pixels. Then, the normalized images are coded by using the local binary pattern (LBP). Finally, the coded images are aligned by formulating an optimization problem by using a differentiable Hamming distance. Experimental results show that the proposed algorithm outperforms state-of-the-art alignment methods for multi-exposed images in terms of alignment accuracy and robustness to exposure values.      
### 25.Constructing and Evaluating an Explainable Model for COVID-19 Diagnosis from Chest X-rays  [ :arrow_down: ](https://arxiv.org/pdf/2012.10787.pdf)
>  In this paper, our focus is on constructing models to assist a clinician in the diagnosis of COVID-19 patients in situations where it is easier and cheaper to obtain X-ray data than to obtain high-quality images like those from CT scans. Deep neural networks have repeatedly been shown to be capable of constructing highly predictive models for disease detection directly from image data. However, their use in assisting clinicians has repeatedly hit a stumbling block due to their black-box nature. Some of this difficulty can be alleviated if predictions were accompanied by explanations expressed in clinically relevant terms. In this paper, deep neural networks are used to extract domain-specific features(morphological features like ground-glass opacity and disease indications like pneumonia) directly from the image data. Predictions about these features are then used to construct a symbolic model (a decision tree) for the diagnosis of COVID-19 from chest X-rays, accompanied with two kinds of explanations: visual (saliency maps, derived from the neural stage), and textual (logical descriptions, derived from the symbolic stage). A radiologist rates the usefulness of the visual and textual explanations. Our results demonstrate that neural models can be employed usefully in identifying domain-specific features from low-level image data; that textual explanations in terms of clinically relevant features may be useful; and that visual explanations will need to be clinically meaningful to be useful.      
### 26.One-Bit Target Detection in Collocated MIMO Radar and Performance Degradation Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.10780.pdf)
>  Target detection is an important problem in multiple-input multiple-output (MIMO) radar. Many existing target detection algorithms were proposed without taking into consideration the quantization error caused by analog-to-digital converters (ADCs). This paper addresses the problem of target detection for MIMO radar with one-bit ADCs and derives a Rao's test-based detector. The proposed method has several appealing features: 1) it is a closed-form detector; 2) it allows us to handle sign measurements straightforwardly; 3) there are closed-form approximations of the detector's distributions, which allow us to theoretically evaluate its performance. Moreover, the closed-form distributions allow us to study the performance degradation due to the one-bit ADCs, yielding an approximate $2$ dB loss in the low-signal-to-noise-ratio (SNR) regime compared to $\infty$-bit ADCs. Simulation results are included to showcase the advantage of the proposed detector and validate the accuracy of the theoretical results.      
### 27.Consolidated Dataset and Metrics for High-Dynamic-Range Image Quality  [ :arrow_down: ](https://arxiv.org/pdf/2012.10758.pdf)
>  Increasing popularity of high-dynamic-range (HDR) image and video content brings the need for metrics that could predict the severity of image impairments as seen on displays of different brightness levels and dynamic range. Such metrics should be trained and validated on a sufficiently large subjective image quality dataset to ensure robust performance. As the existing HDR quality datasets are limited in size, we created a Unified Photometric Image Quality dataset (UPIQ) with over 4,000 images by realigning and merging existing HDR and standard-dynamic-range (SDR) datasets. The realigned quality scores share the same unified quality scale across all datasets. Such realignment was achieved by collecting additional cross-dataset quality comparisons and re-scaling data with a psychometric scaling method. Images in the proposed dataset are represented in absolute photometric and colorimetric units, corresponding to light emitted from a display. We use the new dataset to retrain existing HDR metrics and show that the dataset is sufficiently large for training deep architectures. We show the utility of the dataset on brightness aware image compression.      
### 28.DCCRGAN: Deep Complex Convolution Recurrent Generator Adversarial Network for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2012.10732.pdf)
>  Generative adversarial network (GAN) still exists some problems in dealing with speech enhancement (SE) task. Some GAN-based systems adopt the same structure from Pixel-to-Pixel directly without special optimization. The importance of the generator network has not been fully explored. Other related researches change the generator network but operate in the time-frequency domain, which ignores the phase mismatch problem. In order to solve these problems, a deep complex convolution recurrent GAN (DCCRGAN) structure is proposed in this paper. The complex module builds the correlation between magnitude and phase of the waveform and has been proved to be effective. The proposed structure is trained in an end-to-end way. Different LSTM layers are used in the generator network to sufficiently explore the speech enhancement performance of DCCRGAN. The experimental results confirm that the proposed DCCRGAN outperforms the state-of-the-art GAN-based SE systems.      
### 29.CCML: A Novel Collaborative Learning Model for Classification of Remote Sensing Images with Noisy Multi-Labels  [ :arrow_down: ](https://arxiv.org/pdf/2012.10715.pdf)
>  The development of accurate methods for multi-label classification (MLC) of remote sensing (RS) images is one of the most important research topics in RS. Deep Convolutional Neural Networks (CNNs) based methods have triggered substantial performance gains in RS MLC problems, requiring a large number of reliable training images annotated by multiple land-cover class labels. Collecting such data is time-consuming and costly. To address this problem, the publicly available thematic products, which can include noisy labels, can be used for annotating RS images with zero-labeling cost. However, multi-label noise (which can be associated with wrong as well as missing label annotations) can distort the learning process of the MLC algorithm, resulting in inaccurate predictions. The detection and correction of label noise are challenging tasks, especially in a multi-label scenario, where each image can be associated with more than one label. To address this problem, we propose a novel Consensual Collaborative Multi-Label Learning (CCML) method to alleviate the adverse effects of multi-label noise during the training phase of the CNN model. CCML identifies, ranks, and corrects noisy multi-labels in RS images based on four main modules: 1) group lasso module; 2) discrepancy module; 3) flipping module; and 4) swap module. The task of the group lasso module is to detect the potentially noisy labels assigned to the multi-labeled training images, and the discrepancy module ensures that the two collaborative networks learn diverse features, while obtaining the same predictions. The flipping module is designed to correct the identified noisy multi-labels, while the swap module task is devoted to exchanging the ranking information between two networks. Our code is publicly available online: <a class="link-external link-http" href="http://www.noisy-labels-in-rs.org" rel="external noopener nofollow">this http URL</a>      
### 30.GPU acceleration of a patient-specific airway image segmentation and its assessment  [ :arrow_down: ](https://arxiv.org/pdf/2012.10684.pdf)
>  Image segmentation plays an important role in computer vision, object detection, traffic control, and video surveillance. Typically, it is a critical step in the 3D reconstruction of a specific organ in medical image processing which unveils the detailed tomography of organ, tumor, and nerve, and thus helping to improve the quality of surgical pathology. However, there may be high computational requirements in it. With the advent of GPUs, more complex and realistic models can be simulated, but the deployment of these facilities also requires a huge amount of capital. As a consequence, how to make good use of these computational resource is essential to GPU computing. This study discusses the image segmentation of 3D airway reconstruction, identifies the computing-intensive task, and parallelizes the algorithm of image segmentation in order to obtain theoretical maximum speedup in terms of the benchmark ratio of GPU to CPU. There are five steps involved, which are the image acquisition, pre-processing, segmentation, reconstruction, and object recognition. It is worth to note that it takes 85\% of time on segmentation. This study successfully accelerates the image segmentation of 3D airway reconstruction by optimizing the memory usage, grid and block setting and multiple GPUs communication, thereby gaining a total speedup of 61.8 on two GPUs (Nvidia K40).      
### 31.Deep Reinforcement Learning for Joint Spectrum and Power Allocation in Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.10682.pdf)
>  A wireless network operator typically divides the radio spectrum it possesses into a number of subbands. In a cellular network those subbands are then reused in many cells. To mitigate co-channel interference, a joint spectrum and power allocation problem is often formulated to maximize a sum-rate objective. The best known algorithms for solving such problems generally require instantaneous global channel state information and a centralized optimizer. In fact those algorithms have not been implemented in practice in large networks with time-varying subbands. Deep reinforcement learning algorithms are promising tools for solving complex resource management problems. A major challenge here is that spectrum allocation involves discrete subband selection, whereas power allocation involves continuous variables. In this paper, a learning framework is proposed to optimize both discrete and continuous decision variables. Specifically, two separate deep reinforcement learning algorithms are designed to be executed and trained simultaneously to maximize a joint objective. Simulation results show that the proposed scheme outperforms both the state-of-the-art fractional programming algorithm and a previous solution based on deep reinforcement learning.      
### 32.A Game Theoretic Perspective into the Coexistence of WiFi and NR-U at the 6GHz Unlicensed Bands  [ :arrow_down: ](https://arxiv.org/pdf/2012.10644.pdf)
>  We study the behaviour of WiFi and 5G cellular networks as they exploit the recently unlocked 6 GHz spectrum for unlicensed access, while conforming to the constraints imposed by the incumbent users. We use tools from stochastic geometry to derive the theoretical performance metrics for users of each radio access technology, which helps us in capturing the aggregate behaviour of the network in a snapshot. We propose a framework where the portions of cellular and WiFi networks are grouped together to form entities. These entities interact to satisfy their Quality of Service demands, by playing a non-cooperative game. The action of an entity corresponds to the fraction of its network elements (WiFi access point and cellular base stations) operating in the 6 GHz band. Due to the decentralized nature of the entities, we find the Nash equilibrium using distributed Best Response Algorithm, where each entity takes actions without any centralized scheduling. The results demonstrate how the system parameters affect the performance of a network at equilibrium and highlight the throughput gains of the networks as a result of using the 6 GHz bands, which offer considerably larger bandwidths. The proposed framework is flexible and can be used to model a variety of scenarios for feasibility and performance assessment of the networks involved.      
### 33.Partition of Unity Methods for Signal Processing on Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2012.10636.pdf)
>  Partition of unity methods (PUMs) on graphs are simple and highly adaptive auxiliary tools for graph signal processing. Based on a greedy-type metric clustering and augmentation scheme, we show how a partition of unity can be generated in an efficient way on graphs. We investigate how PUMs can be combined with a local graph basis function (GBF) approximation method in order to obtain low-cost global interpolation or classification schemes. From a theoretical point of view, we study necessary prerequisites for the partition of unity such that global error estimates of the PUM follow from corresponding local ones. Finally, properties of the PUM as cost-efficiency and approximation accuracy are investigated numerically.      
### 34.Virtual Source Synthetic Aperture for Accurate Lateral Displacement Estimation in Ultrasound Elastography  [ :arrow_down: ](https://arxiv.org/pdf/2012.10562.pdf)
>  Ultrasound elastography is an emerging noninvasive imaging technique wherein pathological alterations can be visualized by revealing the mechanical properties of the tissue. Estimating tissue displacement in all directions is required to accurately estimate the mechanical properties. Despite capabilities of elastography techniques in estimating displacement in both axial and lateral directions, estimation of axial displacement is more accurate than lateral direction due to higher sampling frequency, higher resolution and having a carrier signal propagating in the axial direction. Among different ultrasound imaging techniques, Synthetic Aperture (SA) has better lateral resolution than others, but it is not commonly used for ultrasound elastography due to its limitation in imaging depth of field. Virtual source synthetic aperture (VSSA) imaging is a technique to implement synthetic aperture beamforming on the focused transmitted data to overcome limitation of SA in depth of field while maintaining the same lateral resolution as SA. Besides lateral resolution, VSSA has the capability of increasing sampling frequency in the lateral direction without interpolation. In this paper, we utilize VSSA to perform beamforming to enable higher resolution and sampling frequency in the lateral direction. The beamformed data is then processed using our recently published elastography technique, OVERWIND [1]. Simulation and experimental results show substantial improvement in estimation of lateral displacements.      
### 35.Atlas-ISTN: Joint Segmentation, Registration and Atlas Construction with Image-and-Spatial Transformer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.10533.pdf)
>  Deep learning models for semantic segmentation are able to learn powerful representations for pixel-wise predictions, but are sensitive to noise at test time and do not guarantee a plausible topology. Image registration models on the other hand are able to warp known topologies to target images as a means of segmentation, but typically require large amounts of training data, and have not widely been benchmarked against pixel-wise segmentation models. We propose Atlas-ISTN, a framework that jointly learns segmentation and registration on 2D and 3D image data, and constructs a population-derived atlas in the process. Atlas-ISTN learns to segment multiple structures of interest and to register the constructed, topologically consistent atlas labelmap to an intermediate pixel-wise segmentation. Additionally, Atlas-ISTN allows for test time refinement of the model's parameters to optimize the alignment of the atlas labelmap to an intermediate pixel-wise segmentation. This process both mitigates for noise in the target image that can result in spurious pixel-wise predictions, as well as improves upon the one-pass prediction of the model. Benefits of the Atlas-ISTN framework are demonstrated qualitatively and quantitatively on 2D synthetic data and 3D cardiac computed tomography and brain magnetic resonance image data, out-performing both segmentation and registration baseline models. Atlas-ISTN also provides inter-subject correspondence of the structures of interest, enabling population-level shape and motion analysis.      
### 36.Semantic Audio-Visual Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2012.11583.pdf)
>  Recent work on audio-visual navigation assumes a constantly-sounding target and restricts the role of audio to signaling the target's spatial placement. We introduce semantic audio-visual navigation, where objects in the environment make sounds consistent with their semantic meanings (e.g., toilet flushing, door creaking) and acoustic envents are sporadic or short in duration. We propose a transformer-based model to tackle this new semantic AudioGoal task, incorporating an inferred goal descriptor that captures both spatial and semantic properties of the target. Our model's persistent multimodal memory enables it to reach the goal even long after the acoustic event stops. In support of the new task, we also expand the SoundSpaces audio simulation platform to provide semantically grounded object sounds for an array of objects in Matterport3D. Our method strongly outperforms existing audio-visual navigation methods by learning to associate semantic, acoustic, and visual cues.      
### 37.A scalable control design for grid-forming inverters in microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2012.11556.pdf)
>  Microgrids are increasingly recognized as a key technology for the integration of distributed energy resources into the power network, allowing local clusters of load and distributed energy resources to operate autonomously. However, microgrid operation brings new challenges of its own, especially in islanded operation as frequency and voltage control is no longer provided by large rotating machines. Instead, the power converters in the microgrid must coordinate to regulate the frequency and voltage and ensure stability. We consider the problem of designing controllers to achieve these objectives. Using passivity theory to derive decentralized stability conditions for the microgrid, we propose a controller design method for grid-forming inverters. For the analysis we use higher order models for the inverters and also advanced dynamic models for the lines with an arbitrarily large number of states. By satisfying the decentralized condition formulated, plug-and-play operation can be achieved with guaranteed stability, and performance can also be improved by incorporating this condition as a constraint in corresponding optimization problems formulated. In addition, our control design can improve the power sharing properties of the microgrid compared to previous non-droop approaches. Finally, realistic simulations confirm that the controller design improves the stability and performance of the power network.      
### 38.A Frequency And Phase Attention Based Deep Learning Framework For Partial Discharge Detection On Insulated Overhead Conductors  [ :arrow_down: ](https://arxiv.org/pdf/2012.11532.pdf)
>  Partial discharges are known as indicators of degradation of insulation systems.The reliability and selectivity of methods to detect internal partial discharges in the covered conductors are dictated by the level of background noise. The background noise distorts the pattern of partial discharges (PD-pattern) and decreases the capability of detection methods to recognize the features of PD-pattern corresponding to the degradation of an insulation system. This paper proposes a deep learning based framework with novel implementation of frequency and phase attention layers to detect partial discharge pattern on insulated overhead conductors.The introduced phase and frequency attention layers finds the discriminative regions responsible for PD activity in the spectograms of the signals.      
### 39.A Meta-Learning Approach to the Optimal Power Flow Problem Under Topology Reconfigurations  [ :arrow_down: ](https://arxiv.org/pdf/2012.11524.pdf)
>  Recently, there has been a surge of interest in adopting deep neural networks (DNNs) for solving the optimal power flow (OPF) problem in power systems. Computing optimal generation dispatch decisions using a trained DNN takes significantly less time when compared to using conventional optimization solvers. However, a major drawback of existing work is that the machine learning models are trained for a specific system topology. Hence, the DNN predictions are only useful as long as the system topology remains unchanged. Changes to the system topology (initiated by the system operator) would require retraining the DNN, which incurs significant training overhead and requires an extensive amount of training data (corresponding to the new system topology). To overcome this drawback, we propose a DNN-based OPF predictor that is trained using a meta-learning (MTL) approach. The key idea behind this approach is to find a common initialization vector that enables fast training for any system topology. The developed OPF-predictor is validated through simulations using benchmark IEEE bus systems. The results show that the MTL approach achieves significant training speeds-ups and requires only a few gradient steps with a few data samples to achieve high OPF prediction accuracy.      
### 40.Closing the Loop: A High-Performance Connectivity Solution for Realizing Wireless Closed-Loop Control in Industrial IoT Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.11504.pdf)
>  High-performance real-time wireless connectivity is at the heart of the Industrial Internet-of-Things (IIoT). Realizing wireless closed-loop control is crucial for various mission-critical IIoT systems. Existing wireless technologies fall short of meeting the stringent performance requirements of closed-loop control. This paper presents a novel wireless solution, called \textsf{GALLOP}, for realizing closed-loop control over multi-hop or single-hop networks. \textsf{GALLOP} adopts a pragmatic design approach for addressing the challenges of wireless closed-loop control. Key design aspects of \textsf{GALLOP} include control-aware bi-directional scheduling for cyclic exchange, robust retransmission techniques based on cooperative multi-user diversity and low-overhead signaling for scalable operation. \textsf{GALLOP} has been specifically designed for control loops that are closed over the whole network with dynamics on the order of few milliseconds. Performance evaluation based on extensive system-level simulations and hardware implementation on a Bluetooth 5 testbed demonstrates that \textsf{GALLOP} provides high-performance connectivity with very low and deterministic latency, very high reliability and high scalability, to meet the stringent requirements of wireless closed-loop control for versatile IIoT applications.      
### 41.Blurring Fools the Network -- Adversarial Attacks by Feature Peak Suppression and Gaussian Blurring  [ :arrow_down: ](https://arxiv.org/pdf/2012.11442.pdf)
>  Existing pixel-level adversarial attacks on neural networks may be deficient in real scenarios, since pixel-level changes on the data cannot be fully delivered to the neural network after camera capture and multiple image preprocessing steps. In contrast, in this paper, we argue from another perspective that gaussian blurring, a common technique of image preprocessing, can be aggressive itself in specific occasions, thus exposing the network to real-world adversarial attacks. We first propose an adversarial attack demo named peak suppression (PS) by suppressing the values of peak elements in the features of the data. Based on the blurring spirit of PS, we further apply gaussian blurring to the data, to investigate the potential influence and threats of gaussian blurring to performance of the network. Experiment results show that PS and well-designed gaussian blurring can form adversarial attacks that completely change classification results of a well-trained target network. With the strong physical significance and wide applications of gaussian blurring, the proposed approach will also be capable of conducting real world attacks.      
### 42.Pulse-and-Glide Driving with Drivability Constraints: A Pontryagin Approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.11435.pdf)
>  This paper uses Pontryagin methods to analyze pulse-and-glide driving for different nominal vehicle speeds. There is significant literature supporting the fact that pulse-and-glide has the potential to reduce fuel consumption compared to driving at a constant speed, but the benefits at a variety of nominal speeds remain relatively unexplored. Building on the literature, we formulate a speed trajectory optimization problem where the objective is a linearly-scalarized Pareto combination of fuel consumption and average vehicle speed, in addition to a quadratic penalty on jerk. By analyzing this optimization problem using Pontryagin methods, we show that (i) for each nominal speed, there is a critical penalty on jerk below which the optimal solution is pulse-and-glide, (ii) without any penalty on jerk, the optimal pulse-and-glide trajectory switches infinitely fast, and (iii) above a critical nominal velocity, the optimal solution is steady-speed driving rather than pulse-and-glide, regardless of the penalty on jerk.      
### 43.Dual-energy CT Reconstruction from Dual Quarter Scans  [ :arrow_down: ](https://arxiv.org/pdf/2012.11374.pdf)
>  Compared with conventional single-energy computed tomography (CT), dual-energy CT (DECT) provides better material differentiation but most DECT imaging systems require dual full-angle projection data at different X-ray spectra. Relaxing the requirement of data acquisition is a particularly attractive research to promote the applications of DECT in a wide range of imaging areas. In this work, we design a novel DECT imaging scheme with dual quarter scans and propose an efficient method to reconstruct the desired DECT images from dual limited-angle projection data, which enables DECT on imaging configurations with half-scan and largely reduces scanning angles and radiation doses. We first study the characteristics of image artifacts under dual quarter scans scheme, and find that the directional limited-angle artifacts of DECT images are complementarily distributed in image domain because the corresponding X-rays of high- and low-energy scans are orthogonal. Inspired by this finding, a fusion CT image is generated by integrating the limited-angle DECT images of dual quarter scans. This strategy largely reduces the limited-angle artifacts and preserves the image edges and inner structures. Utilizing the capability of neural network in the modeling of nonlinear problem, a novel Anchor network with single-entry double-out architecture is designed in this work to yield the desired DECT images from the generated fusion CT image. Experimental results on the simulated and real data verify the effectiveness of the proposed method.      
### 44.Analysis of Safe Ultrawideband Human-Robot Communication in Automated Collaborative Warehouse  [ :arrow_down: ](https://arxiv.org/pdf/2012.11345.pdf)
>  The paper presents the propagation analysis of ultrawideband Gaussian signal in an automated collaborative warehouse environment where human and robots communicate to ensure that mutual collisions do not occur. The warehouse racks are principally modeled as clusters of metallic (PEC) parallelepipeds, with dimensions chosen to approximate the realistic warehouse. The signal propagation is analyzed using a ray tracing software, with the goal to calculate the path loss profile for different representative scenarios and antenna polarizations. The influence of the rack surface roughness onto propagation is also analyzed. The guidelines for optimum antenna positions on humans and robots for safe communication are proposed according to the simulations results.      
### 45.Real-time Aggregate Flexibility via Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.11261.pdf)
>  Aggregators have emerged as crucial tools for the coordination of distributed, controllable loads. To be used effectively, an aggregator must be able to communicate the available flexibility of the loads they control, as known as the aggregate flexibility to a system operator. However, most of existing aggregate flexibility measures often are slow-timescale estimations and much less attention has been paid to real-time coordination between an aggregator and a system operator to allow the system operator to send control signals to the aggregator that lead to optimization of system-level objectives, such as online cost minimization, and do not violate private constraints of the loads, such as satisfying specific load demands. In this paper, we present a design of real-time aggregate flexibility feedback based on maximization of entropy, termed the maximum entropy feedback (MEF). The design provides a concise and informative signal that can be used by the system operator to perform online cost minimization, while provably satisfying the constraints of the loads. In addition to deriving analytic properties of the MEF, we show that it can be generated efficiently using reinforcement learning and used as a penalty term in model predictive control (MPC), which gives a novel algorithm -- the penalized predictive control (PPC). The benefits of the PPC are (1). Efficient Communication. An operator running PPC does not need to know the exact states and constraints of the loads on the aggregator's side, but only the MEF sent by the aggregator. (2). Fast Computation. The PPC is an unconstrained online optimization and it often has much less number of variables than the optimization formulation of an MPC. (3). Lower Costs. We illustrate the efficacy of the PPC using a dataset from an adaptive electric vehicle charging network and show that PPC outperforms classical MPC by achieving lower costs.      
### 46.Improving J-divergence of brain connectivity states by graph Laplacian denoising  [ :arrow_down: ](https://arxiv.org/pdf/2012.11240.pdf)
>  Functional connectivity (FC) can be represented as a network, and is frequently used to better understand the neural underpinnings of complex tasks such as motor imagery (MI) detection in brain-computer interfaces (BCIs). However, errors in the estimation of connectivity can affect the detection performances. In this work, we address the problem of denoising common connectivity estimates to improve the detectability of different connectivity states. Specifically, we propose a denoising algorithm that acts on the network graph Laplacian, which leverages recent graph signal processing results. Further, we derive a novel formulation of the Jensen divergence for the denoised Laplacian under different states. Numerical simulations on synthetic data show that the denoising method improves the Jensen divergence of connectivity patterns corresponding to different task conditions. Furthermore, we apply the Laplacian denoising technique to brain networks estimated from real EEG data recorded during MI-BCI experiments. Using our novel formulation of the J-divergence, we are able to quantify the distance between the FC networks in the motor imagery and resting states, as well as to understand the contribution of each Laplacian variable to the total J-divergence between two states. Experimental results on real MI-BCI EEG data demonstrate that the Laplacian denoising improves the separation of motor imagery and resting mental states, and shortens the time interval required for connectivity estimation. We conclude that the approach shows promise for the robust detection of connectivity states while being appealing for implementation in real-time BCI applications.      
### 47.DAQ: Distribution-Aware Quantization for Deep Image Super-Resolution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.11230.pdf)
>  Quantizing deep convolutional neural networks for image super-resolution substantially reduces their computational costs. However, existing works either suffer from a severe performance drop in ultra-low precision of 4 or lower bit-widths, or require a heavy fine-tuning process to recover the performance. To our knowledge, this vulnerability to low precisions relies on two statistical observations of feature map values. First, distribution of feature map values varies significantly per channel and per input image. Second, feature maps have outliers that can dominate the quantization error. Based on these observations, we propose a novel distribution-aware quantization scheme (DAQ) which facilitates accurate training-free quantization in ultra-low precision. A simple function of DAQ determines dynamic range of feature maps and weights with low computational burden. Furthermore, our method enables mixed-precision quantization by calculating the relative sensitivity of each channel, without any training process involved. Nonetheless, quantization-aware training is also applicable for auxiliary performance gain. Our new method outperforms recent training-free and even training-based quantization methods to the state-of-the-art image super-resolution networks in ultra-low precision.      
### 48.Communication and Localization with Extremely Large Lens Antenna Array  [ :arrow_down: ](https://arxiv.org/pdf/2012.11161.pdf)
>  Achieving high-rate communication with accurate localization and wireless environment sensing has emerged as an important trend of beyond-fifth and sixth generation cellular systems. Extension of the antenna array to an extremely large scale is a potential technology for achieving such goals. However, the super massive operating antennas significantly increases the computational complexity of the system. Motivated by the inherent advantages of lens antenna arrays in reducing system complexity, we consider communication and localization problems with an \uline{ex}tremely large \uline{lens} antenna array, which we call "ExLens". Since radiative near-field property emerges in the setting, we derive the closed-form array response of the lens antenna array with spherical wave, which includes the array response obtained on the basis of uniform plane wave as a special case. Our derivation result reveals a window effect for energy focusing property of ExLens, which indicates that ExLens has great potential in position sensing and multi-user communication. We also propose an effective method for location and channel parameters estimation, which is able to achieve the localization performance close to the Cramér-Rao lower bound. Finally, we examine the multi-user communication performance of ExLens that serves coexisting near-field and far-field users. Numerical results demonstrate the effectiveness of the proposed channel estimation method and show that ExLens with a minimum mean square error receiver achieves significant spectral efficiency gains and complexity-and-cost reductions compared with a uniform linear array.      
### 49.Multi-stream Convolutional Neural Network with Frequency Selection for Robust Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2012.11159.pdf)
>  Speaker verification aims to verify whether an input speech corresponds to the claimed speaker, and conventionally, this kind of system is deployed based on single-stream scenario, wherein the feature extractor operates in full frequency range. In this paper, we hypothesize that machine can learn enough knowledge to do classification task when listening to partial frequency range instead of full frequency range, which is so called frequency selection technique, and further propose a novel framework of multi-stream Convolutional Neural Network (CNN) with this technique for speaker verification tasks. The proposed framework accommodates diverse temporal embeddings generated from multiple streams to enhance the robustness of acoustic modeling. For the diversity of temporal embeddings, we consider feature augmentation with frequency selection, which is to manually segment the full-band of frequency into several sub-bands, and the feature extractor of each stream can select which sub-bands to use as target frequency domain. Different from conventional single-stream solution wherein each utterance would only be processed for one time, in this framework, there are multiple streams processing it in parallel. The input utterance for each stream is pre-processed by a frequency selector within specified frequency range, and post-processed by mean normalization. The normalized temporal embeddings of each stream will flow into a pooling layer to generate fused embeddings. We conduct extensive experiments on VoxCeleb dataset, and the experimental results demonstrate that multi-stream CNN significantly outperforms single-stream baseline with 20.53 % of relative improvement in minimum Decision Cost Function (minDCF).      
### 50.Adjust-free adversarial example generation in speech recognition using evolutionary multi-objective optimization under black-box condition  [ :arrow_down: ](https://arxiv.org/pdf/2012.11138.pdf)
>  This paper proposes a black-box adversarial attack method to automatic speech recognition systems. Some studies have attempted to attack neural networks for speech recognition; however, these methods did not consider the robustness of generated adversarial examples against timing lag with a target speech. The proposed method in this paper adopts Evolutionary Multi-objective Optimization (EMO)that allows it generating robust adversarial examples under black-box scenario. Experimental results showed that the proposed method successfully generated adjust-free adversarial examples, which are sufficiently robust against timing lag so that an attacker does not need to take the timing of playing it against the target speech.      
### 51.Optimal Decoding of Convolutional Codes using a Linear State Space Control Formulation  [ :arrow_down: ](https://arxiv.org/pdf/2012.11095.pdf)
>  The equivalence of a systematic convolutional encoder as linear state-space control system is first realized and presented through an example. Then, utilizing this structure, a new optimal state-sequence estimator is derived, in the spirit of the Viterbi algorithm. Afterwords, a novel way to perform optimal decoding is achieved, named the Bowyer Decoder, which is a fully deterministic decoder in that the full FSM is known to the decoding algorithm.      
### 52.Data-Driven Geometric System Identification for Shape-Underactuated Dissipative Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.11064.pdf)
>  The study of systems whose movement is both geometric and dissipative offers an opportunity to quickly both identify models and optimize motion. Here, the geometry indicates reduction of the dynamics by environmental homogeneity while the dissipative nature minimizes the role of second order (inertial) features in the dynamics. In this work, we extend the tools of geometric system identification to "Shape-Underactuated Dissipative Systems (SUDS)" -- systems whose motions are kinematic, but whose actuation is restricted to a subset of the body shape coordinates. A large class of SUDS includes highly damped robots with series elastic actuators, and many soft robots. We validate the predictive quality of the models using simulations of a variety of viscous swimming systems. For a large class of SUDS, we show how the shape velocity actuation inputs can be directly converted into torque inputs suggesting that, e.g., systems with soft pneumatic actuators or dielectric elastomers, could be controlled in this way. Based on fundamental assumptions in the physics, we show how our model complexity scales linearly with the number of passive shape coordinates. This offers a large reduction on the number of trials needed to identify the system model from experimental data, and may reduce overfitting. The sample efficiency of our method suggests its use in modeling, control, and optimization in robotics, and as a tool for the study of organismal motion in friction dominated regimes.      
### 53.A Bayesian methodology for localising acoustic emission sources in complex structures  [ :arrow_down: ](https://arxiv.org/pdf/2012.11058.pdf)
>  In the field of structural health monitoring (SHM), the acquisition of acoustic emissions to localise damage sources has emerged as a popular approach. Despite recent advances, the task of locating damage within composite materials and structures that contain non-trivial geometrical features, still poses a significant challenge. Within this paper, a Bayesian source localisation strategy that is robust to these complexities is presented. Under this new framework, a Gaussian process is first used to learn the relationship between source locations and the corresponding difference-in-time-of-arrival values for a number of sensor pairings. As an acoustic emission event with an unknown origin is observed, a mapping is then generated that quantifies the likelihood of the emission location across the surface of the structure. The new probabilistic mapping offers multiple benefits, leading to a localisation strategy that is more informative than deterministic predictions or single-point estimates with an associated confidence bound. The performance of the approach is investigated on a structure with numerous complex geometrical features and demonstrates a favourable performance in comparison to other similar localisation methods.      
### 54.Study of Energy-Efficient Distributed RLS-based Learning with Coarsely Quantized Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.10939.pdf)
>  In this work, we present an energy-efficient distributed learning framework using coarsely quantized signals for Internet of Things (IoT) networks. In particular, we develop a distributed quantization-aware recursive least squares (DQA-RLS) algorithm that can learn parameters in an energy-efficient fashion using signals quantized with few bits while requiring a low computational cost. Numerical results assess the DQA-RLS algorithm against existing techniques for a distributed parameter estimation task where IoT devices operate in a peer-to-peer mode.      
### 55.Multi-Head Linear Attention Generative Adversarial Network for Thin Cloud Removal  [ :arrow_down: ](https://arxiv.org/pdf/2012.10898.pdf)
>  In remote sensing images, the existence of the thin cloud is an inevitable and ubiquitous phenomenon that crucially reduces the quality of imageries and limits the scenarios of application. Therefore, thin cloud removal is an indispensable procedure to enhance the utilization of remote sensing images. Generally, even though contaminated by thin clouds, the pixels still retain more or less surface information. Hence, different from thick cloud removal, thin cloud removal algorithms normally concentrate on inhibiting the cloud influence rather than substituting the cloud-contaminated pixels. Meanwhile, considering the surface features obscured by the cloud are usually similar to adjacent areas, the dependency between each pixel of the input is useful to reconstruct contaminated areas. In this paper, to make full use of the dependencies between pixels of the image, we propose a Multi-Head Linear Attention Generative Adversarial Network (MLAGAN) for Thin Cloud Removal. The MLA-GAN is based on the encoding-decoding framework consisting of multiple attention-based layers and deconvolutional layers. Compared with six deep learning-based thin cloud removal benchmarks, the experimental results on the RICE1 and RICE2 datasets demonstrate that the proposed framework MLA-GAN has dominant advantages in thin cloud removal.      
### 56.Memory Approximate Message Passing  [ :arrow_down: ](https://arxiv.org/pdf/2012.10861.pdf)
>  Approximate message passing (AMP) is a low-cost iterative parameter-estimation technique for certain high-dimensional linear systems with non-Gaussian distributions. However, AMP only applies to the independent identically distributed (IID) transform matrices, but may become unreliable for other matrix ensembles, especially for ill-conditioned ones. To handle this difficulty, orthogonal/vector AMP (OAMP/VAMP) was proposed for general unitarily-invariant matrices, including IID matrices and partial orthogonal matrices. However, the Bayes-optimal OAMP/VAMP requires high-complexity linear minimum mean square error (MMSE) estimator. This limits the application of OAMP/VAMP to large-scale systems. <br>To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a low-complexity memory AMP (MAMP) for unitarily-invariant matrices. MAMP is consisted of an orthogonal non-linear estimator (NLE) for denoising (same as OAMP/VAMP), and an orthogonal long-memory matched filter (MF) for interference suppression. Orthogonal principle is used to guarantee the asymptotic Gaussianity of estimation errors in MAMP. A state evolution is derived to asymptotically characterize the performance of MAMP. The relaxation parameters and damping vector in MAMP are analytically optimized based on the state evolution to guarantee and improve the convergence. We show that MAMP has comparable complexity to AMP. Furthermore, we prove that for all unitarily-invariant matrices, the optimized MAMP converges to the high-complexity OAMP/VAMP, and thus is Bayes-optimal if it has a unique fixed point. Finally, simulations are provided to verify the validity and accuracy of the theoretical results.      
### 57.Visual Speech Enhancement Without A Real Visual Stream  [ :arrow_down: ](https://arxiv.org/pdf/2012.10852.pdf)
>  In this work, we re-think the task of speech enhancement in unconstrained real-world environments. Current state-of-the-art methods use only the audio stream and are limited in their performance in a wide range of real-world noises. Recent works using lip movements as additional cues improve the quality of generated speech over "audio-only" methods. But, these methods cannot be used for several applications where the visual stream is unreliable or completely absent. We propose a new paradigm for speech enhancement by exploiting recent breakthroughs in speech-driven lip synthesis. Using one such model as a teacher network, we train a robust student network to produce accurate lip movements that mask away the noise, thus acting as a "visual noise filter". The intelligibility of the speech enhanced by our pseudo-lip approach is comparable (&lt; 3% difference) to the case of using real lips. This implies that we can exploit the advantages of using lip movements even in the absence of a real video stream. We rigorously evaluate our model using quantitative metrics as well as human evaluations. Additional ablation studies and a demo video on our website containing qualitative comparisons and results clearly illustrate the effectiveness of our approach. We provide a demo video which clearly illustrates the effectiveness of our proposed approach on our website: \url{<a class="link-external link-http" href="http://cvit.iiit.ac.in/research/projects/cvit-projects/visual-speech-enhancement-without-a-real-visual-stream" rel="external noopener nofollow">this http URL</a>}. The code and models are also released for future research: \url{<a class="link-external link-https" href="https://github.com/Sindhu-Hegde/pseudo-visual-speech-denoising" rel="external noopener nofollow">this https URL</a>}.      
### 58.Image-based Intraluminal Contact Force Monitoring in Robotic Vascular Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2012.10762.pdf)
>  Embolization, stroke, ischaemic lesion, and perforation remain significant concerns in endovascular interventions. Sensing catheter interaction inside the artery is advantageous to minimize such complications and enhances navigation safety. Intraluminal information is currently limited due to the lack of intravascular contact sensing technologies. We present monitoring of the intraluminal catheter interaction with the arterial wall using an image-based estimation approach within vascular robotic navigation. The proposed image-based method employs continuous finite element simulation of the catheter motion using imaging data to estimate multi-point forces along catheter-vessel interaction. We implemented imaging algorithms to detect and track contacts and compute catheter pose measurements. The catheter model is constructed based on the nonlinear beam element and flexural rigidity distribution. During remote cannulation of aortic arteries, intraluminal monitoring achieved tracking local contact forces, building contour map of force on the arterial wall, and estimating structural stress of catheter. Shape estimation error was within 2% range. Results suggest that high-risk intraluminal forces may happen even in low insertion forces. The presented online monitoring tool delivers insight into the intraluminal behavior of catheters and is well-suited for intraoperative visual guidance of clinicians, robotic control vascular system, and optimizing interventional device design.      
### 59.Analysis of NARXNN for State of Charge Estimation for Li-ion Batteries on various Drive Cycles  [ :arrow_down: ](https://arxiv.org/pdf/2012.10725.pdf)
>  Electric Vehicles (EVs) are rapidly increasing in popularity as they are environment friendly. Lithium Ion batteries are at the heart of EV technology and contribute to most of the weight and cost of an EV. State of Charge (SOC) is a very important metric which helps to predict the range of an EV. There is a need to accurately estimate available battery capacity in a battery pack such that the available range in a vehicle can be determined. There are various techniques available to estimate SOC. In this paper, a data driven approach is selected and a Nonlinear Autoregressive Network with Exogenous Inputs Neural Network (NARXNN) is explored to accurately estimate SOC. NARXNN has been shown to be superior to conventional Machine Learning techniques available in the literature. The NARXNN model is developed and tested on various EV Drive Cycles like LA92, US06, UDDS and HWFET to test its performance on real world scenarios. The model is shown to outperform conventional statistical machine learning methods and achieve a Mean Squared Error (MSE) in the 1e-5 range.      
### 60.Model-Based Actor-Critic with Chance Constraint for Stochastic System  [ :arrow_down: ](https://arxiv.org/pdf/2012.10716.pdf)
>  Safety constraints are essential for reinforcement learning (RL) applied in real-world situations. Chance constraints are suitable to represent the safety requirements in stochastic systems. Most existing RL methods with chance constraints have a low convergence rate, and only learn a conservative policy. In this paper, we propose a model-based chance constrained actor-critic (CCAC) algorithm which can efficiently learn a safe and non-conservative policy. Different from existing methods that optimize a conservative lower bound, CCAC directly solves the original chance constrained problems, where the objective function and safe probability is simultaneously optimized with adaptive weights. In order to improve the convergence rate, CCAC utilizes the gradient of dynamic model to accelerate policy optimization. The effectiveness of CCAC is demonstrated by an aggressive car-following task. Experiments indicate that compared with previous methods, CCAC improves the performance by 57.6% while guaranteeing safety, with a five times faster convergence rate.      
### 61.Non-uniform FIR Digital Filter Bank for Hearing Aid Application Using Frequency Response Masking Technique: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.10663.pdf)
>  Hearing aid is an electroacoustic device used to selectively amplify the audio sounds with an aim to make speech more intelligible for a hearing impaired person. Filter bank is one of the important parts of digital hearing aid where the sub band gains of each filter can be tuned to compensate an individuals unique hearing loss pattern. As the human perception is based on the logarithmic scale, nonuniform filter bank outperforms uniform filter bank. The main advantage of nonuniform filer bank is that it requires less number of sub-band filters, hence resulted in low hardware complexity and cost. Much effort has been devoted to design these nonuniform filter banks for hearing aid applications. This paper aimed to provide a review of previous researches based on nonuniform finite impulse response (FIR) digital filter bank for hearing aid application using frequency response masking (FRM) technique. By reviewing filter banks, we try to find the difference between fixed and variable band filter bank and to give an insight about which method is more suitable for matching most common types of hearing loss. Papers which involved methods of design, theoretical computation and simulation results of filter bank have been reviewed.      
### 62.Dense Multiscale Feature Fusion Pyramid Networks for Object Detection in UAV-Captured Images  [ :arrow_down: ](https://arxiv.org/pdf/2012.10643.pdf)
>  Although much significant progress has been made in the research field of object detection with deep learning, there still exists a challenging task for the objects with small size, which is notably pronounced in UAV-captured images. Addressing these issues, it is a critical need to explore the feature extraction methods that can extract more sufficient feature information of small objects. In this paper, we propose a novel method called Dense Multiscale Feature Fusion Pyramid Networks(DMFFPN), which is aimed at obtaining rich features as much as possible, improving the information propagation and reuse. Specifically, the dense connection is designed to fully utilize the representation from the different convolutional layers. Furthermore, cascade architecture is applied in the second stage to enhance the localization capability. Experiments on the drone-based datasets named VisDrone-DET suggest a competitive performance of our method.      
### 63.Identifying Invariant Texture Violation for Robust Deepfake Detection  [ :arrow_down: ](https://arxiv.org/pdf/2012.10580.pdf)
>  Existing deepfake detection methods have reported promising in-distribution results, by accessing published large-scale dataset. However, due to the non-smooth synthesis method, the fake samples in this dataset may expose obvious artifacts (e.g., stark visual contrast, non-smooth boundary), which were heavily relied on by most of the frame-level detection methods above. As these artifacts do not come up in real media forgeries, the above methods can suffer from a large degradation when applied to fake images that close to reality. To improve the robustness for high-realism fake data, we propose the Invariant Texture Learning (InTeLe) framework, which only accesses the published dataset with low visual quality. Our method is based on the prior that the microscopic facial texture of the source face is inevitably violated by the texture transferred from the target person, which can hence be regarded as the invariant characterization shared among all fake images. To learn such an invariance for deepfake detection, our InTeLe introduces an auto-encoder framework with different decoders for pristine and fake images, which are further appended with a shallow classifier in order to separate out the obvious artifact-effect. Equipped with such a separation, the extracted embedding by encoder can capture the texture violation in fake images, followed by the classifier for the final pristine/fake prediction. As a theoretical guarantee, we prove the identifiability of such an invariance texture violation, i.e., to be precisely inferred from observational data. The effectiveness and utility of our method are demonstrated by promising generalization ability from low-quality images with obvious artifacts to fake images with high realism.      
### 64.Predictor Antenna: A Technique to Boost the Performance of Moving Relays  [ :arrow_down: ](https://arxiv.org/pdf/2012.10537.pdf)
>  In future wireless systems, a large number of users may access wireless networks via moving relays (MRs) installed on top of public transportation vehicles. One of the main challenges of MRs is the rapid channel variation which may make the channel estimation, and its following procedures, difficult. To overcome these issues, different methods have been proposed, among which predictor antenna (PA) is a candidate technology. The PA system refers to a setup with two sets of antennas on top of a vehicle, where the PA(s) positioned at the front of the vehicle is used to predict the channel state information which is required for data transmission to the receive antennas (RAs) aligned behind the PA. In this paper, we introduce the concept and the potentials of PA systems. Moreover, summarizing the initial field trials for PA systems and the 3GPP attempts on moving relays, we compare the performance of different PA and non-PA methods for vehicle communications in both urban and rural areas with the PA setup backhauled through terrestrial or satellite technology, respectively. As we show, a dedicated PA deployment has the potential to improve the backhaul performance of MRs.      
### 65.Computer Vision based Tomography of Structures Using 3D Digital Image Correlation  [ :arrow_down: ](https://arxiv.org/pdf/2012.10516.pdf)
>  Internal properties of a sample can be observed by medical imaging tools, such as ultrasound devices, magnetic resonance imaging (MRI) and optical coherence tomography (OCT) which are based on relying on changes in material density or chemical composition [1-21]. As a preliminary investigation, the feasibility to detect interior defects inferred from the discrepancy in elasticity modulus distribution of a three-dimensional heterogeneous sample using only surface full-field measurements and finite element model updating as an inverse optimization algorithm without any assumption about local homogeneities and also the elasticity modulus distribution is investigated. Recently, the authors took advantages of the digital image correlation technique as a full field measurement in constitutive property identification of a full-scale steel component [22-27]. To the extension of previous works, in this brief technical note, the new idea intended at recovering unseen volumetric defect distributions within the interior of three-dimensional heterogeneous space of the structural component using 3D-Digital Image Correlation for structural identification [28-57]. As a proof of concept, the results of this paper illustrate the potential to identify invisible internal defect by the proposed computer vision technique establishes the potential for new opportunities to characterize internal heterogeneous materials for their mechanical property distribution and condition state.      
### 66.Reinforcement Learning based Multi-Robot Classification via Scalable Communication Structure  [ :arrow_down: ](https://arxiv.org/pdf/2012.10480.pdf)
>  In the multi-robot collaboration domain, training with Reinforcement Learning (RL) can become intractable, and performance starts to deteriorate drastically as the number of robots increases. In this work, we proposed a distributed multi-robot learning architecture with a scalable communication structure capable of learning a robust communication policy for time-varying communication topology. We construct the communication structure with Long-Short Term Memory (LSTM) cells and star graphs, in which the computational complexity of the proposed learning algorithm scales linearly with the number of robots and suitable for application with a large number of robots. The proposed methodology is validated with a map classification problem in the simulated environment. It is shown that the proposed architecture achieves a comparable classification accuracy with the centralized methods, maintains high performance with various numbers of robots without additional training cost, and robust to hacking and loss of the robots in the network.      
