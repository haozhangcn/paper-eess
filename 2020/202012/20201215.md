# ArXiv eess --Tue, 15 Dec 2020
### 1.Reconstructing unseen modalities and pathology with an efficient Recurrent Inference Machine  [ :arrow_down: ](https://arxiv.org/pdf/2012.07819.pdf)
>  Objective: To allow efficient learning using the Recurrent Inference Machine (RIM) for image reconstruction whereas not being strictly dependent on the training data distribution so that unseen modalities and pathologies are still accurately recovered. Methods: Theoretically, the RIM learns to solve the inverse problem of accelerated-MRI reconstruction whereas being robust to variable imaging conditions. The efficiency and generalization capabilities with different training datasets were studied, as well as recurrent network units with decreasing complexity: the Gated Recurrent Unit (GRU), the Minimal Gated Unit (MGU), and the Independently Recurrent Neural Network (IndRNN), to reduce inference times. Validation was performed against Compressed Sensing (CS) and further assessed based on data unseen during training. A pathology study was conducted by reconstructing simulated white matter lesions and prospectively undersampled data of a Multiple Sclerosis patient. Results: Training on a single modality of 3T $T_1$-weighted brain data appeared sufficient to also reconstruct 7T $T_{2}^*$-weighted brain and 3T $T_2$-weighted knee data. The IndRNN is an efficient recurrent unit, reducing inference time by 68\% compared to CS, whereas maintaining performance. The RIM was able to reconstruct lesions unseen during training more accurately than CS when trained on $T_2$-weighted knee data. Training on $T_1$-weighted brain data and on combined data slightly enhanced the signal compared to CS. Conclusion: The RIM is efficient when decreasing its complexity, which reduces the inference time, whereas still being able to reconstruct data and pathology that was unseen during training.      
### 2.A Low-Loss 1-4 GHz Optically-Controlled Silicon Plasma Switch  [ :arrow_down: ](https://arxiv.org/pdf/2012.07818.pdf)
>  This paper presents a low-loss optically-controlled inline RF switch suitable for L- and S-band applications. Under 1.5 W laser power, the switch exhibits a measured ON-state insertion loss of less than 0.33 dB and return loss better than 20 dB across the band. The measured OFF-state isolation ranges from 27 dB at 1 GHz to 17 dB at 4 GHz. The switch comprises a single silicon chiplet excited by a 915-nm laser fiber which creates electron-hole pairs, thereby exciting the ON-state silicon plasma. An optical fiber is guided through the bottom of the RF substrate to illuminate the chiplet, which bridges a 1.075-mm microstrip line gap. To the best of our knowledge, this is the lowest-loss silicon plasma switch demonstrated today.      
### 3.Non-linear State-space Model Identification from Video Data using Deep Encoders  [ :arrow_down: ](https://arxiv.org/pdf/2012.07721.pdf)
>  Identifying systems with high-dimensional inputs and outputs, such as systems measured by video streams, is a challenging problem with numerous applications in robotics, autonomous vehicles and medical imaging. In this paper, we propose a novel non-linear state-space identification method starting from high-dimensional input and output data. Multiple computational and conceptual advances are combined to handle the high-dimensional nature of the data. An encoder function, represented by a neural network, is introduced to learn a reconstructability map to estimate the model states from past inputs and outputs. This encoder function is jointly learned with the dynamics. Furthermore, multiple computational improvements, such as an improved reformulation of multiple shooting and batch optimization, are proposed to keep the computational time under control when dealing with high-dimensional and large datasets. We apply the proposed method to a video stream of a simulated environment of a controllable ball in a unit box. The simulation study shows low simulation error with excellent long term prediction for the obtained model using the proposed method.      
### 4.Water Level Estimation Using Sentinel-1 Synthetic Aperture Radar Imagery And Digital Elevation Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.07627.pdf)
>  Hydropower dams and reservoirs have been identified as major factors that are redefining hydrological cycles. Monitoring the information of water in reservoirs helps in planning and managing water resources as well as drought and flood forecasting by anomaly detection. In this paper, we utilize the high-resolution Sentinel-1 dataset together with Digital Elevation Model datasets to monitor reservoirs' water levels. We proposed a novel water level extracting method achieving an error of under a meter.      
### 5.Demonstration of CAOS Spectral Imager Design and Advanced High Dynamic Range CAOS Camera Modes  [ :arrow_down: ](https://arxiv.org/pdf/2012.07622.pdf)
>  In the first part of the paper, a CAOS line camera is introduced for spectral imaging of one dimensional (1-D) or line targets. The proposed spectral camera uses both a diffraction grating as well as a cylindrical lens optics system to provide line imaging along the line pixels direction of the image axis and Fourier transforming operations in the orthogonal direction to provide line pixels optical spectrum analysis. The imager incorporates the Digital Micro-mirror Device (DMD)-based Coded Access Optical Sensor (CAOS) structure. The design includes a line-by-line scan option to enable two dimensional (2-D) spectral imaging. For the first time, demonstrated is line style spectral imaging using a 2850 K color temperature white light target illumination source along with visible band color bandpass filters and a moving mechanical pinhole to simulate a line target with individual pixels along 1-D that have unique spectral content. A ~412 nm to ~732 nm input target spectrum is measured using a 38 by 52 CAOS pixels spatial sampling grid providing a test image line of 38 pixels with each pixel providing a designed spectral resolution of ~6.2 nm. The spectral image is generated using the robust Code Division Multiple Access (CDMA) mode of the camera. The second part of the paper demonstrates for the first time the High Dynamic Range (HDR) operation of the Frequency Division Multiple Access (FDMA)-Time Division Multiple Access (TDMA) mode of the CAOS camera. The FDMA-TDMA mode also feature HDR recovery like the Frequency Modulation (FM)-TDMA mode, although at a much faster imaging rate and a higher Signal-to-Noise Ratio (SNR) as more than one CAOS pixel is extracted at a time.      
### 6.Pointing Error Analysis of Optically Pre-Amplified Pulse Position Modulation Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2012.07545.pdf)
>  We present analytical results on the effect of pointing errors on the average bit-error probability (ABEP) of optically pre-amplified pulse-position modulation (PPM) receivers. The results show that the beam width plays a key role in the ABEP and that a significant power penalty is introduced by utilizing sub-optimal widths, especially when pointing errors incorporate a jitter component. We also present the optimisation the beam width for a number of pointing error scenarios and show that increasing beam widths are required as additional signal energy becomes available. The optimal beam width is also affected by the PPM modulation order and the optical noise modes, with more energy efficient (higher modulation order and lower noise) systems allowing for broader beams.      
### 7.Is FFT Fast Enough for Beyond-5G Communications?  [ :arrow_down: ](https://arxiv.org/pdf/2012.07497.pdf)
>  In this work, we consider the complexity and throughput limits of the Fast Fourier Transform (FFT) algorithm having in mind the unprecedented number of points (subcarriers) $N$ expected in future waveforms. Based on the spectro-computational analysis, we verify that the FFT complexity to process an $N$-subcarrier symbol grows faster than the number of bits in the symbol. Thus, the useful throughput of FFT nullifies as $N$ grows. Also, because FFT demands $N$ to be a power of two $2^i$ (for some $i&gt;0$), the spectrum widening causes the FFT complexity to grow exponentially on $i$, i.e. $O(2^ii)$. To overcome these limitations, we propose the Parameterized DFT (PDFT) algorithm, which builds on the parameterized complexity technique and the classic $O(N^2)$ DFT algorithm to replace an $N$-point DFT into $N/n$ ($n&gt;0$) smaller $n$-point DFTs. By setting $n=\Theta(1)$, we get a $O(N)$ algorithm whose resulting waveform matches OFDM in its vectorized form (i.e., Vector OFDM) but with the $N=2^i$ constraint relaxed. Besides, we also show that PDFT becomes multiplierless for $n=2$, requiring only $\Theta(N)$ complex sums. We believe our results constitute a relevant step towards the practical deployment of future extremely wide multicarrier signals.      
### 8.AV Taris: Online Audio-Visual Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.07467.pdf)
>  In recent years, Automatic Speech Recognition (ASR) technology has approached human-level performance on conversational speech under relatively clean listening conditions. In more demanding situations involving distant microphones, overlapped speech, background noise, or natural dialogue structures, the ASR error rate is at least an order of magnitude higher. The visual modality of speech carries the potential to partially overcome these challenges and contribute to the sub-tasks of speaker diarisation, voice activity detection, and the recovery of the place of articulation, and can compensate for up to 15dB of noise on average. This article develops AV Taris, a fully differentiable neural network model capable of decoding audio-visual speech in real time. We achieve this by connecting two recently proposed models for audio-visual speech integration and online speech recognition, namely AV Align and Taris. We evaluate AV Taris under the same conditions as AV Align and Taris on one of the largest publicly available audio-visual speech datasets, LRS2. Our results show that AV Taris is superior to the audio-only variant of Taris, demonstrating the utility of the visual modality to speech recognition within the real time decoding framework defined by Taris. Compared to an equivalent Transformer-based AV Align model that takes advantage of full sentences without meeting the real-time requirement, we report an absolute degradation of approximately 3% with AV Taris. As opposed to the more popular alternative for online speech recognition, namely the RNN Transducer, Taris offers a greatly simplified fully differentiable training pipeline. As a consequence, AV Taris has the potential to popularise the adoption of Audio-Visual Speech Recognition (AVSR) technology and overcome the inherent limitations of the audio modality in less optimal listening conditions.      
### 9.Lagrangian Reachtubes: The Next Generation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07458.pdf)
>  We introduce LRT-NG, a set of techniques and an associated toolset that computes a reachtube (an over-approximation of the set of reachable states over a given time horizon) of a nonlinear dynamical system. LRT-NG significantly advances the state-of-the-art Langrangian Reachability and its associated tool LRT. From a theoretical perspective, LRT-NG is superior to LRT in three ways. First, it uses for the first time an analytically computed metric for the propagated ball which is proven to minimize the ball's volume. We emphasize that the metric computation is the centerpiece of all bloating-based techniques. Secondly, it computes the next reachset as the intersection of two balls: one based on the Cartesian metric and the other on the new metric. While the two metrics were previously considered opposing approaches, their joint use considerably tightens the reachtubes. Thirdly, it avoids the "wrapping effect" associated with the validated integration of the center of the reachset, by optimally absorbing the interval approximation in the radius of the next ball. From a tool-development perspective, LRT-NG is superior to LRT in two ways. First, it is a standalone tool that no longer relies on CAPD. This required the implementation of the Lohner method and a Runge-Kutta time-propagation method. Secondly, it has an improved interface, allowing the input model and initial conditions to be provided as external input files. Our experiments on a comprehensive set of benchmarks, including two Neural ODEs, demonstrates LRT-NG's superior performance compared to LRT, CAPD, and Flow*.      
### 10.On Low-Rank Hankel Matrix Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2012.07433.pdf)
>  The low-complexity assumption in linear systems can often be expressed as rank deficiency in data matrices with generalized Hankel structure. This makes it possible to denoise the data by estimating the underlying structured low-rank matrix. However, standard low-rank approximation approaches are not guaranteed to perform well in estimating the noise-free matrix. In this paper, recent results in matrix denoising by singular value shrinkage are reviewed. A novel approach is proposed to solve the low-rank Hankel matrix denoising problem by using an iterative algorithm in structured low-rank approximation modified with data-driven singular value shrinkage. It is shown numerically in both the input-output trajectory denoising and the impulse response denoising problems, that the proposed method performs the best in terms of estimating the noise-free matrix among existing algorithms of low-rank matrix approximation and denoising.      
### 11.Stability Analysis of Nash Equilibrium for 2-Agent Loss-Aversion-Based Noncooperative Switched Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.07416.pdf)
>  The stability property of the loss-aversion-based noncooperative switched systems with quadratic payoffs is investigated. In this system, each agent adopts the lower sensitivity parameter in the myopic pseudo-gradient dynamics for the case of losing utility than gaining utility, and both systems' dynamics and switching events (conditions) are depending on agents' payoff functions. Sufficient conditions under which agents' state converges towards the Nash equilibrium are derived in accordance with the location of the Nash equilibrium. In the analysis, the mode transition sequence and interesting phenomena which we call flash switching instants are characterized. Finally, we present several numerical examples to illustrate the properties of our results.      
### 12.Fiber Nonlinearity Mitigation by Short-Length Probabilistic Constellation Shaping for Pilot-Aided Signaling  [ :arrow_down: ](https://arxiv.org/pdf/2012.07388.pdf)
>  Probabilistic constellation shaping (PCS) offers a significant performance improvement over uniform signaling. It was recently discovered that long blocks are not required to achieve maximum shaping gain when transmitting over the nonlinear fiber channel because short-length PCS effectively mitigates fiber nonlinear interference (NLI). The reason for this behavior is that short-length PCS implicitly induces some temporal properties in the shaped transmit sequence that are beneficial for the fiber-optic channel. To achieve robust data-aided digital signal processing of high-order QAM, periodic quaternary phase shift keying pilots are typically inserted into the high-order QAM transmit sequence. In this work, we investigate in simulations the effect of such pilot-aided signaling on NLI mitigation. Albeit modifying the temporal properties of the shaped transmit sequence, a pilot rate of 1/32 is found to not alter the beneficial effects of short-length PCS. The operation meaning of this finding is that even with pilot-aided signaling, long PCS block lengths are not required for maximum shaping gain.      
### 13.REDAT: Accent-Invariant Representation for End-to-End ASR by Domain Adversarial Training with Relabeling  [ :arrow_down: ](https://arxiv.org/pdf/2012.07353.pdf)
>  Accents mismatching is a critical problem for end-to-end ASR. This paper aims to address this problem by building an accent-robust RNN-T system with domain adversarial training (DAT). We unveil the magic behind DAT and provide, for the first time, a theoretical guarantee that DAT learns accent-invariant representations. We also prove that performing the gradient reversal in DAT is equivalent to minimizing the Jensen-Shannon divergence between domain output distributions. Motivated by the proof of equivalence, we introduce reDAT, a novel technique based on DAT, which relabels data using either unsupervised clustering or soft labels. Experiments on 23K hours of multi-accent data show that DAT achieves competitive results over accent-specific baselines on both native and non-native English accents but up to 13% relative WER reduction on unseen accents; our reDAT yields further improvements over DAT by 3% and 8% relatively on non-native accents of American and British English.      
### 14.Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2012.07331.pdf)
>  The goal of audio captioning is to translate input audio into its description using natural language. One of the problems in audio captioning is the lack of training data due to the difficulty in collecting audio-caption pairs by crawling the web. In this study, to overcome this problem, we propose to use a pre-trained large-scale language model. Since an audio input cannot be directly inputted into such a language model, we utilize guidance captions retrieved from a training dataset based on similarities that may exist in different audio. Then, the caption of the audio input is generated by using a pre-trained language model while referring to the guidance captions. Experimental results show that (i) the proposed method has succeeded to use a pre-trained language model for audio captioning, and (ii) the oracle performance of the pre-trained model-based caption generator was clearly better than that of the conventional method trained from scratch.      
### 15.Energy Efficient Competitive Resource Allocation in MIMO networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.07312.pdf)
>  This paper considers the competitive resource allocation problem in Multiple-Input Multiple-Output (MIMO) interfering channels, when users maximize their energy efficiency. Considering each transmitter-receiver pair as a selfish player, conditions on the existence and uniqueness of the Nash equilibrium of the underlying noncooperative game are obtained. A decentralized asynchronous algorithm is proven to converge towards this equilibrium under the same conditions. Two frameworks are considered for the analysis of this game. On the one hand, the game is rephrased as a Quasi-Variational Inequality (QVI). On the other hand, the best response of the players is analyzed in light of the contraction mappings. For this problem, the contraction approach is shown to lead to tighter results than the QVI one. When specializing the obtained results to OFDM networks, the obtained conditions appear to significantly outperform state-of-the-art works, and to lead to much simpler decentralized algorithms. Numerical results finally assess the obtained conditions in different settings.      
### 16.Compositional Construction of Control Barrier Functions for Continuous-Time Stochastic Hybrid Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.07296.pdf)
>  In this paper, we propose a compositional framework for the construction of control barrier functions for networks of continuous-time stochastic hybrid systems enforcing complex logic specifications expressed by finite-state automata. The proposed scheme is based on a notion of so-called pseudo-barrier functions computed for subsystems, by employing which one can synthesize hybrid controllers for interconnected systems enforcing complex specifications over a finite-time horizon. Particularly, we first leverage sufficient small-gain type conditions to compositionally construct control barrier functions for interconnected systems based on the corresponding pseudo-barrier functions computed for subsystems. Then, using the constructed control barrier functions, we provide probabilistic guarantees on the satisfaction of given complex specifications in a bounded time horizon. In this respect, we decompose the given complex specification to simpler reachability tasks based on automata representing the complements of original finite-state automata. We then provide systematic approaches to solve those simpler reachability tasks by computing corresponding pseudo-barrier functions. Two different systematic techniques are provided based on (i) the sum-of-squares (SOS) optimization program and (ii) counter-example guided inductive synthesis (CEGIS) to search for pseudo-barrier functions of subsystems while synthesizing local controllers. We demonstrate the effectiveness of our proposed results by applying them to two large-scale physical case studies including a temperature regulation in a circular network of 1000 rooms and a fully-interconnected Kuramoto network of 100 nonlinear oscillators.      
### 17.Group Communication with Context Codec for Ultra-Lightweight Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07291.pdf)
>  Ultra-lightweight model design is an important topic for the deployment of existing speech enhancement and source separation techniques on low-resource platforms. Various lightweight model design paradigms have been proposed in recent years; however, most models still suffer from finding a balance between model size, model complexity, and model performance. In this paper, we propose the group communication with context codec (GC3) design to decrease both model size and complexity without sacrificing the model performance. Group communication splits a high-dimensional feature into groups of low-dimensional features and applies a module to capture the inter-group dependency. A model can then be applied to the groups in parallel with a significantly smaller width. A context codec is applied to decrease the length of a sequential feature, where a context encoder compresses the temporal context of local features into a single feature representing the global characteristics of the context, and a context decoder decompresses the transformed global features back to the context features. Experimental results show that GC3 can achieve on par or better performance than a wide range of baseline architectures with as small as 2.5% model size.      
### 18.Robust Downlink Transmit Optimization under Quantized Channel Feedback via the Strong Duality for QCQP  [ :arrow_down: ](https://arxiv.org/pdf/2012.07289.pdf)
>  Consider a robust multiple-input single-output downlink beamforming optimization problem in a frequency division duplexing system. The base station (BS) sends training signals to the users, and every user estimates the channel coefficients, quantizes the gain and the direction of the estimated channel and sends them back to the BS. Suppose that the channel state information at the transmitter is imperfectly known mainly due to the channel direction quantization errors, channel estimation errors and outdated channel effects. The actual channel is modeled as in an uncertainty set composed of two inequality homogeneous and one equality inhomogeneous quadratic constraints, in order to account for the aforementioned errors and effects. Then the transmit power minimization problem is formulated subject to robust signal-to-noise-plus-interference ratio constraints. Each robust constraint is transformed equivalently into a quadratic matrix inequality (QMI) constraint with respect to the beamforming vectors. The transformation is accomplished by an equivalent phase rotation process and the strong duality result for a quadratically constrained quadratic program. The minimization problem is accordingly turned into a QMI problem, and the problem is solved by a restricted linear matrix inequality relaxation with additional valid convex constraints. Simulation results are presented to demonstrate the performance of the proposed method, and show the efficiency of the restricted relaxation.      
### 19.Relieving the Need for Bi-Level Decision-Making for Optimal Retail Pricing via Online Meta-Prediction of Data-Driven Demand Response of HVAC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.07275.pdf)
>  Price-based demand response (DR) of heating, ventilating, and air-conditioning (HVAC) systems is a challenging task, requiring comprehensive models to represent the building thermal dynamics and game theoretic interactions among participants. This paper proposes an online learning-based strategy for a distribution system operator (DSO) to determine optimal electricity prices, considering the optimal DR of HVAC systems in commercial buildings. An artificial neural network (ANN) is trained with building energy data and represented using an explicit set of linear and nonlinear equations, without physics-based model parameters. An optimization problem for price-based DR is then formulated using this equation set and repeatedly solved offline, producing data on optimal DR schedules for various conditions of electricity prices and building thermal environments. Another ANN is then trained online to directly predict DR schedules for day-ahead electricity prices, which is referred to as meta-prediction (MP). By replacing the DR optimization problem with the MP-enabled ANN, an optimal electricity pricing strategy can be implemented using a single-level decision-making structure, which is simpler and more practical than a bi-level one. In simulation case studies, the proposed single-level strategy is verified to successfully reflect the game theoretic relations between the DSO and commercial building operators, so that they effectively exploit the operational flexibility of the HVAC systems to make the DR application profitable, while ensuring the grid voltage stability and occupants thermal comfort.      
### 20.Data-driven Control of an LCC HVDC System for Real-time Frequency Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07273.pdf)
>  Recent advances in data sensing and processing technologies enable data-driven control of high-voltage direct-current (HVDC) systems for improving the operational stability of interfacing power grids. This paper proposes an optimal data-driven control strategy for an HVDC system with line-commutated converters (LCCs), wherein the dc-link voltage and current are optimally regulated at distinct HVDC terminals to improve frequency regulation (FR) in both rectifier- and inverter-side grids. Each HVDC converter is integrated with feedback loops for regulation of grid frequency and dc-link voltage in a localized manner. For optimal FR in both-side grids, a data-driven model of the HVDC-linked grids is then developed to design a data-driven linear quadratic Gaussian (LQG) regulator, which is incorporated with the converter feedback loops. Case studies on two different LCC HVDC systems are performed using the data-driven models, which are validated via comparisons with physics-based models and comprehensive MATLAB/SIMULINK models. The results of the case studies confirm that the optimal data-driven control strategy successfully exploits the fast dynamics of HVDC converters; moreover, cooperation of the HVDC system and synchronous generators in both-side grids is achieved, improving real-time FR under various HVDC system specifications, LQG parameters, and inertia emulation and droop control conditions.      
### 21.Developing an Analytical Model of Frequency and Voltage Variations for Dynamic Reconfiguration  [ :arrow_down: ](https://arxiv.org/pdf/2012.07268.pdf)
>  This paper develops a new analytical model to estimate real-time variations in grid frequency and voltages resulting from dynamic network reconfiguration (DNR). In the proposed model, switching operations are considered as discrete variations in an admittance matrix, leading to step variations in node injection currents. The network model with discrete admittance variations is then integrated with dynamic models of synchronous generators and voltage-dependent loads, enabling analysis of the dynamic grid operations initiated by the DNR. Case studies are performed to validate the proposed model via comparison with a conventional model and a comprehensive MATLAB/SIMULINK model.      
### 22.Multi-SpectroGAN: High-Diversity and High-Fidelity Spectrogram Generation with Adversarial Style Combination for Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2012.07267.pdf)
>  While generative adversarial networks (GANs) based neural text-to-speech (TTS) systems have shown significant improvement in neural speech synthesis, there is no TTS system to learn to synthesize speech from text sequences with only adversarial feedback. Because adversarial feedback alone is not sufficient to train the generator, current models still require the reconstruction loss compared with the ground-truth and the generated mel-spectrogram directly. In this paper, we present Multi-SpectroGAN (MSG), which can train the multi-speaker model with only the adversarial feedback by conditioning a self-supervised hidden representation of the generator to a conditional discriminator. This leads to better guidance for generator training. Moreover, we also propose adversarial style combination (ASC) for better generalization in the unseen speaking style and transcript, which can learn latent representations of the combined style embedding from multiple mel-spectrograms. Trained with ASC and feature matching, the MSG synthesizes a high-diversity mel-spectrogram by controlling and mixing the individual speaking styles (e.g., duration, pitch, and energy). The result shows that the MSG synthesizes a high-fidelity mel-spectrogram, which has almost the same naturalness MOS score as the ground-truth mel-spectrogram.      
### 23.Optimal Voltage and Current Control of an HVDC System to Improve Real-Time Frequency Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07265.pdf)
>  High-voltage direct-current (HVDC) systems for constant or intermittent power delivery have recently been developed further to support grid frequency regulation (GFR). This paper proposes a new control strategy for a line-commutated converter-based (LCC) HVDC system, wherein the DC-link voltage and current are optimally regulated to improve real-time GFR in both rectifier- and inverter-side AC networks. A dynamic model of an LCC HVDC system is developed using the DC voltage and current as input variables, and is integrated with feedback loops for inertia emulation and droop control. A linear quadratic Gaussian (LQG) controller is also designed for optimal secondary frequency control, while mitigating conflict between the droop controllers of the HVDC converters. An eigenvalue analysis is then conducted, focusing on the effects of model parameters and controller gains on the proposed strategy. Simulation case studies are also performed using the Jeju-Haenam HVDC system as a test bed. The results of the case study confirm that the proposed strategy enables the HVDC system to improve GFR, in coordination with generators in both-side grids, by exploiting the fast dynamics of HVDC converters. The proposed strategy is also effective under various conditions for the LQG weighting coefficients, inertia emulation, and droop control.      
### 24.Learning Hybrid Representations for Automatic 3D Vessel Centerline Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2012.07262.pdf)
>  Automatic blood vessel extraction from 3D medical images is crucial for vascular disease diagnoses. Existing methods based on convolutional neural networks (CNNs) may suffer from discontinuities of extracted vessels when segmenting such thin tubular structures from 3D images. We argue that preserving the continuity of extracted vessels requires to take into account the global geometry. However, 3D convolutions are computationally inefficient, which prohibits the 3D CNNs from sufficiently large receptive fields to capture the global cues in the entire image. In this work, we propose a hybrid representation learning approach to address this challenge. The main idea is to use CNNs to learn local appearances of vessels in image crops while using another point-cloud network to learn the global geometry of vessels in the entire image. In inference, the proposed approach extracts local segments of vessels using CNNs, classifies each segment based on global geometry using the point-cloud network, and finally connects all the segments that belong to the same vessel using the shortest-path algorithm. This combination results in an efficient, fully-automatic and template-free approach to centerline extraction from 3D images. We validate the proposed approach on CTA datasets and demonstrate its superior performance compared to both traditional and CNN-based baselines.      
### 25.IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07261.pdf)
>  Optical coherence tomography angiography (OCTA) is a novel imaging modality that allows a micron-level resolution to present the three-dimensional structure of the retinal vascular. In our previous work, a 3D-to-2D image projection network (IPN) was proposed for retinal vessel (RV) and foveal avascular zone (FAZ) segmentations in OCTA images. One of its advantages is that the segmentation results are directly from the original volumes without using any projection images and retinal layer segmentation. In this work, we propose image projection network V2 (IPN-V2), extending IPN by adding a plane perceptron to enhance the perceptron ability in the horizontal direction. We also propose IPN-V2+, as a supplement of the IPN-V2, by introducing a global retraining process to overcome the "checkerboard effect". Besides, we propose a new multi-modality dataset, dubbed OCTA-500. It contains 500 subjects with two field of view (FOV) types, including OCT and OCTA volumes, six types of projections, four types of text labels and two types of pixel-level labels. The dataset contains more than 360K images with a size of about 80GB. To the best of our knowledge, it is currently the largest OCTA dataset with the abundant information. Finally, we perform a thorough evaluation of the performance of IPN-V2 on the OCTA-500 dataset. The experimental results demonstrate that our proposed IPN-V2 performs better than IPN and other deep learning methods in RV segmentation and FAZ segmentation.      
### 26.Few Shot Adaptive Normalization Driven Multi-Speaker Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2012.07252.pdf)
>  The style of the speech varies from person to person and every person exhibits his or her own style of speaking that is determined by the language, geography, culture and other factors. Style is best captured by prosody of a signal. High quality multi-speaker speech synthesis while considering prosody and in a few shot manner is an area of active research with many real-world applications. While multiple efforts have been made in this direction, it remains an interesting and challenging problem. In this paper, we present a novel few shot multi-speaker speech synthesis approach (FSM-SS) that leverages adaptive normalization architecture with a non-autoregressive multi-head attention model. Given an input text and a reference speech sample of an unseen person, FSM-SS can generate speech in that person's style in a few shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK and LibriTTS datasets, using multiple quantitative metrics that measure generated speech distortion and MoS, along with speaker embedding analysis of the generated speech vs the actual speech samples.      
### 27.Accurate Cell Segmentation in Digital Pathology Images via Attention Enforced Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.07237.pdf)
>  Automatic cell segmentation is an essential step in the pipeline of computer-aided diagnosis (CAD), such as the detection and grading of breast cancer. Accurate segmentation of cells can not only assist the pathologists to make a more precise diagnosis, but also save much time and labor. However, this task suffers from stain variation, cell inhomogeneous intensities, background clutters and cells from different tissues. To address these issues, we propose an Attention Enforced Network (AENet), which is built on spatial attention module and channel attention module, to integrate local features with global dependencies and weight effective channels adaptively. Besides, we introduce a feature fusion branch to bridge high-level and low-level features. Finally, the marker controlled watershed algorithm is applied to post-process the predicted segmentation maps for reducing the fragmented regions. In the test stage, we present an individual color normalization method to deal with the stain variation problem. We evaluate this model on the MoNuSeg dataset. The quantitative comparisons against several prior methods demonstrate the superiority of our approach.      
### 28.D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application to Skin Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07206.pdf)
>  Medical image segmentation annotations suffer from inter/intra-observer variations even among experts due to intrinsic differences in human annotators and ambiguous boundaries. Leveraging a collection of annotators' opinions for an image is an interesting way of estimating a gold standard. Although training deep models in a supervised setting with a single annotation per image has been extensively studied, generalizing their training to work with data sets containing multiple annotations per image remains a fairly unexplored problem. In this paper, we propose an approach to handle annotators' disagreements when training a deep model. To this end, we propose an ensemble of Bayesian fully convolutional networks (FCNs) for the segmentation task by considering two major factors in the aggregation of multiple ground truth annotations: (1) handling contradictory annotations in the training data originating from inter-annotator disagreements and (2) improving confidence calibration through the fusion of base models predictions. We demonstrate the superior performance of our approach on the ISIC Archive and explore the generalization performance of our proposed method by cross-data set evaluation on the PH2 and DermoFit data sets.      
### 29.Self-supervised Text-independent Speaker Verification using Prototypical Momentum Contrastive Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.07178.pdf)
>  In this study, we investigate self-supervised representation learning for speaker verification (SV). First, we examine a simple contrastive learning approach (SimCLR) with a momentum contrastive (MoCo) learning framework, where the MoCo speaker embedding system utilizes a queue to maintain a large set of negative examples. We show that better speaker embeddings can be learned by momentum contrastive learning. Next, alternative augmentation strategies are explored to normalize extrinsic speaker variabilities of two random segments from the same speech utterance. Specifically, augmentation in the waveform largely improves the speaker representations for SV tasks. The proposed MoCo speaker embedding is further improved when a prototypical memory bank is introduced, which encourages the speaker embeddings to be closer to their assigned prototypes with an intermediate clustering step. In addition, we generalize the self-supervised framework to a semi-supervised scenario where only a small portion of the data is labeled. Comprehensive experiments on the Voxceleb dataset demonstrate that our proposed self-supervised approach achieves competitive performance compared with existing techniques, and can approach fully supervised results with partially labeled data.      
### 30.Robust Segmentation of Optic Disc and Cup from Fundus Images Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.07128.pdf)
>  Optic disc (OD) and optic cup (OC) are regions of prominent clinical interest in a retinal fundus image. They are the primary indicators of a glaucomatous condition. With the advent and success of deep learning for healthcare research, several approaches have been proposed for the segmentation of important features in retinal fundus images. We propose a novel approach for the simultaneous segmentation of the OD and OC using a residual encoder-decoder network (REDNet) based regional convolutional neural network (RCNN). The RED-RCNN is motivated by the Mask RCNN (MRCNN). Performance comparisons with the state-of-the-art techniques and extensive validations on standard publicly available fundus image datasets show that RED-RCNN has superior performance compared with MRCNN. RED-RCNN results in Sensitivity, Specificity, Accuracy, Precision, Dice and Jaccard indices of 95.64%, 99.9%, 99.82%, 95.68%, 95.64%, 91.65%, respectively, for OD segmentation, and 91.44%, 99.87%, 99.83%, 85.67%, 87.48%, 78.09%, respectively, for OC segmentation. Further, we perform two-stage glaucoma severity grading using the cup-to-disc ratio (CDR) computed based on the obtained OD/OC segmentation. The superior segmentation performance of RED-RCNN over MRCNN translates to higher accuracy in glaucoma severity grading.      
### 31.Forecasting Daily Primary Three-Hour Net Load Ramps in the CAISO System  [ :arrow_down: ](https://arxiv.org/pdf/2012.07117.pdf)
>  The deepening penetration of variable energy resources creates unprecedented challenges for system operators (SOs). An issue that merits special attention is the precipitous net load ramps, which require SOs to have flexible capacity at their disposal so as to maintain the supply-demand balance at all times. In the judicious procurement and deployment of flexible capacity, a tool that forecasts net load ramps may be of great assistance to SOs. To this end, we propose a methodology to forecast the magnitude and start time of daily primary three-hour net load ramps. We perform an extensive analysis so as to identify the factors that influence net load and draw on the identified factors to develop a forecasting methodology that harnesses the long short-term memory model. We demonstrate the effectiveness of the proposed methodology on the CAISO system using comparative assessments with selected benchmarks based on various evaluation metrics.      
### 32.CHS-Net: A Deep learning approach for hierarchical segmentation of COVID-19 infected CT images  [ :arrow_down: ](https://arxiv.org/pdf/2012.07079.pdf)
>  The pandemic of novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19 has been spreading worldwide, causing rampant loss of lives. Medical imaging such as computed tomography (CT), X-ray, etc., plays a significant role in diagnosing the patients by presenting the visual representation of the functioning of the organs. However, for any radiologist analyzing such scans is a tedious and time-consuming task. The emerging deep learning technologies have displayed its strength in analyzing such scans to aid in the faster diagnosis of the diseases and viruses such as COVID-19. In the present article, an automated deep learning based model, COVID-19 hierarchical segmentation network (CHS-Net) is proposed that functions as a semantic hierarchical segmenter to identify the COVID-19 infected regions from lungs contour via CT medical imaging. The CHS-Net is developed with the two cascaded residual attention inception U-Net (RAIU-Net) models where first generates lungs contour maps and second generates COVID-19 infected regions. RAIU-Net comprises of a residual inception U-Net model with spectral spatial and depth attention network (SSD), consisting of contraction and expansion phases of depthwise separable convolutions and hybrid pooling (max and spectral pooling) to efficiently encode and decode the semantic and varying resolution information. The CHS-Net is trained with the segmentation loss function that is the weighted average of binary cross entropy loss and dice loss to penalize false negative and false positive predictions. The approach is compared with the recently proposed research works on the basis of standard metrics, it is observed that the proposed approach outperformed the recently proposed approaches and effectively segments the COVID-19 infected regions in the lungs.      
### 33.Adaptive User Pairing for Downlink NOMA System with Imperfect SIC  [ :arrow_down: ](https://arxiv.org/pdf/2012.07034.pdf)
>  Non-orthogonal multiple access (NOMA) has been recognized as a key driving technology for the fifth generation (5G) and beyond 5G cellular networks. For a practical dowlink NOMA system with imperfect successive interference cancellation (SIC), we derive bounds on channel coefficients and power allocation factors between NOMA users to achieve higher rates than an equivalent orthogonal multiple access (OMA) system. We propose an adaptive user pairing (A-UP) algorithm for NOMA systems. Through extensive simulations, we show that NOMA with imperfect SIC is not always superior to OMA. Further, the proposed A-UP algorithm results in better performance than state-of-the-art NOMA pairing algorithms in presence of SIC imperfections.      
### 34.Data-Driven Dispatchable Regions with Valid Boundaries for Renewable Power Generation: Concept and Construction  [ :arrow_down: ](https://arxiv.org/pdf/2012.07009.pdf)
>  The dispatchable region of volatile renewable power generation (RPG) quantifies how much uncertainty the power system can tackle at a given operating point. State-of-the-art DR research has studied how system operation constraints influence the DR, but seldom considered the effect of the distributional features of RPG outputs. The resultant DR is generally described by a large number of boundaries, and the most critical information about the actual ranges of RPG outputs that the system can accept is often suppressed. To make DR methods friendly to users and obtain critical information, a novel DR for RPGs is proposed which takes both operation constraints and distributional features of RPG outputs into account. Compared to the traditional DR, the proposed DR is enclosed by a small number of valid boundaries that restrict the actual output ranges of RPG and is thus simpler to use. The procedure for constructing the proposed DR entails progressively searching valid boundaries by exploiting the historical RPG output data and is formulated as a mixed-integer linear program. A parallel solution paradigm is also developed to expedite the construction procedure when using a large historical dataset of RPG outputs. Simulation tests on the IEEE 30-bus and 118-bus systems verify the effectiveness of the proposed DR and the efficiency of the proposed algorithm.      
### 35.Radial Deformation Emplacement in Power Transformers Using Long Short-Term Memory Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.06982.pdf)
>  A power transformer winding is usually subject to mechanical stress and tension because of improper transportation or operation. Radial deformation (RD) is an example of mechanical stress that can impact power transformer operation through short circuit faults and insulation damages. Frequency response analysis (FRA) is a well-known method to diagnose mechanical defects in transformers. Despite the precision of FRA, the interpretation of the calculated frequency response curves is not straightforward and requires complex calculations. In this paper, a deep learning algorithm called long short-term memory (LSTM) is used as a feature extraction technique to locate RD faults in their early stages. The experimental results verify the effectiveness of the proposed method in the diagnosis and locating of RD defects.      
### 36.fMRI-Kernel Regression: A Kernel-based Method for Pointwise Statistical Analysis of rs-fMRI for Population Studies  [ :arrow_down: ](https://arxiv.org/pdf/2012.06972.pdf)
>  Due to the spontaneous nature of resting-state fMRI (rs-fMRI) signals, cross-subject comparison and therefore, group studies of rs-fMRI are challenging. Most existing group comparison methods use features extracted from the fMRI time series, such as connectivity features, independent component analysis (ICA), and functional connectivity density (FCD) methods. However, in group studies, especially in the case of spectrum disorders, distances to a single atlas or a representative subject do not fully reflect the differences between subjects that may lie on a multi-dimensional spectrum. Moreover, there may not exist an individual subject or even an average atlas in such cases that is representative of all subjects. Here we describe an approach that measures pairwise distances between the synchronized rs-fMRI signals of pairs of subjects instead of to a single reference point. We also present a method for fMRI data comparison that leverages this generated pairwise feature to establish a radial basis function kernel matrix. This kernel matrix is used in turn to perform kernel regression of rs-fMRI to a clinical variable such as a cognitive or neurophysiological performance score of interest. This method opens a new pointwise analysis paradigm for fMRI data. We demonstrate the application of this method by performing a pointwise analysis on the cortical surface using rs-fMRI data to identify cortical regions associated with variability in ADHD index. While pointwise analysis methods are common in anatomical studies such as cortical thickness analysis and voxel- and tensor-based morphometry and its variants, such a method is lacking for rs-fMRI and could improve the utility of rs-fMRI for group studies. The method presented in this paper is aimed at filling this gap.      
### 37.Comparative Analysis of Methods for Cloud Segmentation in Infrared Images  [ :arrow_down: ](https://arxiv.org/pdf/2012.06930.pdf)
>  The increasing penetration of Photovoltaic (PV) systems in the power network makes the grid vulnerable to the projection of cloud shadows over PV systems. Real-time segmentation of clouds in infrared (IR) images is important to reduce the impact of noise in the short-term forecast of Global Solar Irradiance (GSI). This investigation presents a comparison between discriminate and generative models for cloud segmentation. Markov Random Fields (MRF), which add information from neighboring pixels to the prior, are included among the analyzed generative models. This investigation includes an evaluation of the performance of supervised and unsupervised learning methods in cloud segmentation. The discriminate models are solved in the primal formulation to make them feasible in real-time applications. The performances are compared using the j-statistic. Preprocessing of IR images to remove stationary artifacts increases the overall performances in all of the analyzed methods. The inclusion of features from neighboring pixels in the feature vectors leads to an improvement in the performances in some of the cases. The MRFs achieve the best performance in both unsupervised and supervised generative models. The discriminate models solved in the primal yield a dramatically lower computing time along with high performance in the segmentation. The performances of the generative models are comparable to those of the discriminate models when proper preprocessing is applied to the IR images.      
### 38.DEAAN: Disentangled Embedding and Adversarial Adaptation Network for Robust Speaker Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06896.pdf)
>  Despite speaker verification has achieved significant performance improvement with the development of deep neural networks, domain mismatch is still a challenging problem in this field. In this study, we propose a novel framework to disentangle speaker-related and domain-specific features and apply domain adaptation on the speaker-related feature space solely. Instead of performing domain adaptation directly on the feature space where domain information is not removed, using disentanglement can efficiently boost adaptation performance. To be specific, our model's input speech from the source and target domains is first encoded into different latent feature spaces. The adversarial domain adaptation is conducted on the shared speaker-related feature space to encourage the property of domain-invariance. Further, we minimize the mutual information between speaker-related and domain-specific features for both domains to enforce the disentanglement. Experimental results on the VOiCES dataset demonstrate that our proposed framework can effectively generate more speaker-discriminative and domain-invariant speaker representations with a relative 20.3% reduction of EER compared to the original ResNet-based system.      
### 39.Highly accurate closed-form approximation for the probability of detection of Weibull fluctuating targets in non-coherent detectors  [ :arrow_down: ](https://arxiv.org/pdf/2012.06878.pdf)
>  In this paper, we derive a highly accurate approximation for the probability of detection (PD) of a non-coherent detector operating with Weibull fluctuation targets. To do so, we assume a pulse-to-pulse decorrelation during the coherent processing interval (CPI). Specifically, the proposed approximation is given in terms of: i) a closed-form expression derived in terms of the Fox's H-function, for which we also provide a portable and efficient MATHEMATICA routine; and ii) a fast converging series obtained through a comprehensive calculus of residues. Both solutions are fast and provide very accurate results. In particular, our series representation, besides being a more tractable solution, also exhibits impressive savings in computational load and computation time compared to previous studies. Numerical results and Monte-Carlo simulations corroborated the validity of our expressions.      
### 40.Interactive Radiotherapy Target Delineation with 3D-Fused Context Propagation  [ :arrow_down: ](https://arxiv.org/pdf/2012.06873.pdf)
>  Gross tumor volume (GTV) delineation on tomography medical imaging is crucial for radiotherapy planning and cancer diagnosis. Convolutional neural networks (CNNs) has been predominated on automatic 3D medical segmentation tasks, including contouring the radiotherapy target given 3D CT volume. While CNNs may provide feasible outcome, in clinical scenario, double-check and prediction refinement by experts is still necessary because of CNNs' inconsistent performance on unexpected patient cases. To provide experts an efficient way to modify the CNN predictions without retrain the model, we propose 3D-fused context propagation, which propagates any edited slice to the whole 3D volume. By considering the high-level feature maps, the radiation oncologists would only required to edit few slices to guide the correction and refine the whole prediction volume. Specifically, we leverage the backpropagation for activation technique to convey the user editing information backwardly to the latent space and generate new prediction based on the updated and original feature. During the interaction, our proposed approach reuses the extant extracted features and does not alter the existing 3D CNN model architectures, avoiding the perturbation on other predictions. The proposed method is evaluated on two published radiotherapy target contouring datasets of nasopharyngeal and esophageal cancer. The experimental results demonstrate that our proposed method is able to further effectively improve the existing segmentation prediction from different model architectures given oncologists' interactive inputs.      
### 41.Flexibility-constrained Operation Scheduling of Active Distribution Networks with Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2012.06855.pdf)
>  Regarding the variability of renewable energy sources (RESs), especially in the operation time periods, their high penetration faces the net load pattern of the power system with a major ramp rate challenge. Although employing a market-based mechanism by the independent system operator (ISO) can address this challenge, it may not be possible to handle this challenge in all networks. In such networks, the required flexibility can be supplied by decreasing the ramp rate of the purchased power of distribution companies (Discos) from the market since their net load has an important impact on the system's netload. Therefore, a flexibility-constrained operation problem for the distribution networks with distributed energy resources (DERs) and microgrids (MGs) is proposed in this paper to decrease the ramp-rate of the Disco's purchased power from the market. This problem is formulated using a bi-level two-stage stochastic model where the problem of the Disco and the MGs are modeled as the upper-level and lower-level problems, respectively. The proposed model is applied to the IEEE 33-bus standard test network with three MGs. The results show the effectiveness of the proposed model to decrease the ramp-rate of the Disco's purchased power from the market.      
### 42.Deep Reinforcement Learning for Tropical Air Free-Cooled Data Center Control  [ :arrow_down: ](https://arxiv.org/pdf/2012.06834.pdf)
>  Air free-cooled data centers (DCs) have not existed in the tropical zone due to the unique challenges of year-round high ambient temperature and relative humidity (RH). The increasing availability of servers that can tolerate higher temperatures and RH due to the regulatory bodies' prompts to raise DC temperature setpoints sheds light upon the feasibility of air free-cooled DCs in tropics. However, due to the complex psychrometric dynamics, operating the air free-cooled DC in tropics generally requires adaptive control of supply air condition to maintain the computing performance and reliability of the servers. This paper studies the problem of controlling the supply air temperature and RH in a free-cooled tropical DC below certain thresholds. To achieve the goal, we formulate the control problem as Markov decision processes and apply deep reinforcement learning (DRL) to learn the control policy that minimizes the cooling energy while satisfying the requirements on the supply air temperature and RH. We also develop a constrained DRL solution for performance improvements. Extensive evaluation based on real data traces collected from an air free-cooled testbed and comparisons among the unconstrained and constrained DRL approaches as well as two other baseline approaches show the superior performance of our proposed solutions.      
### 43.Improving EEG Decoding via Clustering-based Multi-task Feature Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06813.pdf)
>  Accurate electroencephalogram (EEG) pattern decoding for specific mental tasks is one of the key steps for the development of brain-computer interface (BCI), which is quite challenging due to the considerably low signal-to-noise ratio of EEG collected at the brain scalp. Machine learning provides a promising technique to optimize EEG patterns toward better decoding accuracy. However, existing algorithms do not effectively explore the underlying data structure capturing the true EEG sample distribution, and hence can only yield a suboptimal decoding accuracy. To uncover the intrinsic distribution structure of EEG data, we propose a clustering-based multi-task feature learning algorithm for improved EEG pattern decoding. Specifically, we perform affinity propagation-based clustering to explore the subclasses (i.e., clusters) in each of the original classes, and then assign each subclass a unique label based on a one-versus-all encoding strategy. With the encoded label matrix, we devise a novel multi-task learning algorithm by exploiting the subclass relationship to jointly optimize the EEG pattern features from the uncovered subclasses. We then train a linear support vector machine with the optimized features for EEG pattern decoding. Extensive experimental studies are conducted on three EEG datasets to validate the effectiveness of our algorithm in comparison with other state-of-the-art approaches. The improved experimental results demonstrate the outstanding superiority of our algorithm, suggesting its prominent performance for EEG pattern decoding in BCI applications.      
### 44.A Novel Method  [ :arrow_down: ](https://arxiv.org/pdf/2012.06803.pdf)
>  Parameter selection is one of the most important parts for nearly all the control strategies. Traditionally, controller parameters are chosen by utilizing trial and error, which is always tedious and time consuming. Moreover, such method is highly dependent on the experience of researchers, which means that it is hard to be popularized. In this light, this paper proposes a novel parameter searching approach by utilizing uniform design (UD) algorithm. By which the satisfactory controller parameters under a performance index could be selected. In this end, two simulation examples are conducted to verify the effectiveness of proposed scheme. Simulation results show that this novel approach, as compared to other intelligent tuning algorithms, excels in efficiency and time saving.      
### 45.Light-Weight 1-D Convolutional Neural Network Architecture for Mental Task Identification and Classification Based on Single-Channel EEG  [ :arrow_down: ](https://arxiv.org/pdf/2012.06782.pdf)
>  Mental task identification and classification using single/limited channel(s) electroencephalogram (EEG) signals in real-time play an important role in the design of portable brain-computer interface (BCI) and neurofeedback (NFB) systems. However, the real-time recorded EEG signals are often contaminated with noises such as ocular artifacts (OAs) and muscle artifacts (MAs), which deteriorate the hand-crafted features extracted from EEG signal, resulting inadequate identification and classification of mental tasks. Therefore, we investigate the use of recent deep learning techniques which do not require any manual feature extraction or artifact suppression step. In this paper, we propose a light-weight one-dimensional convolutional neural network (1D-CNN) architecture for mental task identification and classification. The robustness of the proposed architecture is evaluated using artifact-free and artifact-contaminated EEG signals taken from two publicly available databases (i.e, Keirn and Aunon ($K$) database and EEGMAT ($E$) database) and in-house ($R$) database recorded using single-channel neurosky mindwave mobile 2 (MWM2) EEG headset in performing not only mental/non-mental binary task classification but also different mental/mental multi-tasks classification. Evaluation results demonstrate that the proposed architecture achieves the highest subject-independent classification accuracy of $99.7\%$ and $100\%$ for multi-class classification and pair-wise mental tasks classification respectively in database $K$. Further, the proposed architecture achieves subject-independent classification accuracy of $99\%$ and $98\%$ in database $E$ and the recorded database $R$ respectively. Comparative performance analysis demonstrates that the proposed architecture outperforms existing approaches not only in terms of classification accuracy but also in robustness against artifacts.      
### 46.Generative Adversarial Networks for Automatic Polyp Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.06771.pdf)
>  This paper aims to contribute in bench-marking the automatic polyp segmentation problem using generative adversarial networks framework. Perceiving the problem as an image-to-image translation task, conditional generative adversarial networks are utilized to generate masks conditioned by the images as inputs. Both generator and discriminator are convolution neural networks based. The model achieved 0.4382 on Jaccard index and 0.611 as F2 score.      
### 47.HI-Net: Hyperdense Inception 3D UNet for Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2012.06760.pdf)
>  The brain tumor segmentation task aims to classify tissue into the whole tumor (WT), tumor core (TC), and enhancing tumor (ET) classes using multimodel MRI images. Quantitative analysis of brain tumors is critical for clinical decision making. While manual segmentation is tedious, time-consuming, and subjective, this task is at the same time very challenging to automatic segmentation methods. Thanks to the powerful learning ability, convolutional neural networks (CNNs), mainly fully convolutional networks, have shown promising brain tumor segmentation. This paper further boosts the performance of brain tumor segmentation by proposing hyperdense inception 3D UNet (HI-Net), which captures multi-scale information by stacking factorization of 3D weighted convolutional layers in the residual inception block. We use hyper dense connections among factorized convolutional layers to extract more contexual information, with the help of features reusability. We use a dice loss function to cope with class imbalances. We validate the proposed architecture on the multi-modal brain tumor segmentation challenges (BRATS) 2020 testing dataset. Preliminary results on the BRATS 2020 testing set show that achieved by our proposed approach, the dice (DSC) scores of ET, WT, and TC are 0.79457, 0.87494, and 0.83712, respectively.      
### 48.Gap Reduced Minimum Error Robust Simultaneous Estimation For Unstable Nano Air Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2012.06756.pdf)
>  This paper proposes a novel Gap Reduced Minimum Error Robust Simultaneous (GRMERS) estimator for resource-constrained Nano Aerial Vehicle (NAV) that enables a single estimator to provide simultaneous and robust estimation for a given N unstable and uncertain NAV plant models. The estimated full state feedback enables a stable flight for NAV. The GRMERS estimator is implemented utilizing a Minimum Error Robust Simultaneous (MERS) estimator and Gap Reducing (GR) compensators. The MERS estimator provides robust simultaneous estimation with minimal largest worst-case estimation error even in the presence of a bounded energy exogenous disturbance signal. The GR compensators reduce the gap between the graphs of N linear plant models to decrease the estimation error generated by the MERS estimator. A sufficient condition for the existence of a simultaneous estimator is established using LMIs and robust estimation theory. Further, MERS estimator and GR compensator design are formulated as non-convex tractable optimization problems and are solved using the population-based genetic algorithms. The performance of the GRMERS estimator consisting of MERS estimator and GR compensators from the population-based genetic algorithms is validated through simulation studies. The study results indicate that a single GRMERS estimator can produce state estimates with reduced errors for all flight conditions. The results indicate that the single GRMERS estimator is robust than the individually designed H inifinity filters.      
### 49.Towards Connected Unmanned Aerial System: A Channel Modeling Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2012.06707.pdf)
>  Unmanned aerial vehicles (UAVs) have attracted increasing interest in integrating them in next-generation wireless communication. It has reached a consensus that unmanned aerial system (UAS) plays an intensely crucial role in the emerging aerial networks. Concerning connected UAV, a multitude of communication scenarios are involved, such as intercommunications between UAVs and interactions with the ground user equipment (UE), the cellular base station (BS), and the ground station (GS), etc. In this article, we pour attention into the essentials of corresponding air-to-air (A2A) and air-to-ground (A2G) links in the above scenarios from the perspective of channel modeling. We first extract new characteristics related to UAV channel modeling, compared with traditional cellular and vehicular channels. We then provide a brief review of A2A and A2G channel statistics and models based on existing works. Following that, we show the impacts of the environment and frequency on A2G channel characterizations with the aid of measurements. Besides, we employ a realistic channel model to show how it influences communication performance. Finally, potential directions are discussed for paving the way to more accurate and effective UAV channel modeling.      
### 50.Coordinated Frequency and Voltage Regulation of Grid-Following and Grid-Forming Inverters  [ :arrow_down: ](https://arxiv.org/pdf/2012.06685.pdf)
>  In this work, we propose a peer-to-peer secondary control for voltage and frequency regulation in islanded microgrids to facilitate high inverter-based distributed energy resource (DER) integration in the distribution grid. It has been a major challenge to achieve various performance objectives simultaneously in an inverter-driven microgrid, i.e., system frequency restoration, real power sharing, voltage regulation and circulating var mitigation. We show that the challenge can be addressed in even microgrids with 100% penetration of inverters, without the support of any synchronous generators or the bulk power system. In specific, we propose a novel control scheme to coordinate a fleet of grid-forming and grid-following inverters by using a leader-follower consensus algorithm to achieve these objectives simultaneously. Several use cases examining the effects of microgrid switching events (topology change) and communication degradation are presented to demonstrate the effectiveness of the proposed control architecture.      
### 51.Focus optimization in a Computational Confocal Microscope  [ :arrow_down: ](https://arxiv.org/pdf/2012.06677.pdf)
>  We consider the numerical optimization of performance for a computational extension of a confocal microscope. Using a system where the pinhole detector is replaced with a detector array, we seek to exploit this additional information for each point in the scan. We derive an optimal estimate of the light at focus which minimizes the contribution of out-of-focus light. We estimate the amount of improvement that would be theoretically possible in point-scanning and line-scanning systems and demonstrate with simulation. We find that even with a large degree of regularization, a significant improvement is possible, especially for line-scanning systems.      
### 52.Exploiting the Dual-Tree Complex Wavelet Transform for Ship Wake Detection in SAR Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2012.06663.pdf)
>  In this paper, we analyse synthetic aperture radar (SAR) images of the sea surface using an inverse problem formulation whereby Radon domain information is enhanced in order to accurately detect ship wakes. This is achieved by promoting linear features in the images. For the inverse problem-solving stage, we propose a penalty function, which combines the dual-tree complex wavelet transform (DT-CWT) with the non-convex Cauchy penalty function. The solution to this inverse problem is based on the forward-backward (FB) splitting algorithm to obtain enhanced images in the Radon domain. The proposed method achieves the best results and leads to significant improvement in terms of various performance metrics, compared to state-of-the-art ship wake detection methods. The accuracy of detecting ship wakes in SAR images with different frequency bands and spatial resolution reaches more than 90%, which clearly demonstrates an accuracy gain of 7% compared to the second-best approach.      
### 53.DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization  [ :arrow_down: ](https://arxiv.org/pdf/2012.06659.pdf)
>  Recent success in speech representation learning enables a new way to leverage unlabeled data to train speech recognition model. In speech representation learning, a large amount of unlabeled data is used in a self-supervised manner to learn a feature representation. Then a smaller amount of labeled data is used to train a downstream ASR system using the new feature representations. Based on our previous work DeCoAR and inspirations from other speech representation learning, we propose DeCoAR 2.0, a Deep Contextualized Acoustic Representation with vector quantization. We introduce several modifications over the DeCoAR: first, we use Transformers in encoding module instead of LSTMs; second, we introduce a vector quantization layer between encoder and reconstruction modules; third, we propose an objective that combines the reconstructive loss with vector quantization diversity loss to train speech representations. Our experiments show consistent improvements over other speech representations in different data-sparse scenarios. Without fine-tuning, a light-weight ASR model trained on 10 hours of LibriSpeech labeled data with DeCoAR 2.0 features outperforms the model trained on the full 960-hour dataset with filterbank features.      
### 54.A Simulation Study to Evaluate the Performance of the Cauchy Proximal Operator in Despeckling SAR Images of the Sea Surface  [ :arrow_down: ](https://arxiv.org/pdf/2012.06657.pdf)
>  The analysis of ocean surface is widely performed using synthetic aperture radar (SAR) imagery as it yields information for wide areas under challenging weather conditions, during day or night, etc. Speckle noise constitutes however the main reason for reduced performance in applications such as classification, ship detection, target tracking and so on. This paper presents an investigation into the despeckling of SAR images of the ocean that include ship wake structures, via sparse regularisation using the Cauchy proximal operator. We propose a closed-form expression for calculating the proximal operator for the Cauchy prior, which makes it applicable in generic proximal splitting algorithms. In our experiments, we simulate SAR images of moving vessels and their wakes. The performance of the proposed method is evaluated in comparison to the L1 and TV norm regularisation functions. The results show a superior performance of the proposed method for all the utilised images generated.      
### 55.Noisy Linear Convergence of Stochastic Gradient Descent for CV@R Statistical Learning under Polyak-ojasiewicz Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2012.07785.pdf)
>  Conditional Value-at-Risk ($\mathrm{CV@R}$) is one of the most popular measures of risk, which has been recently considered as a performance criterion in supervised statistical learning, as it is related to desirable operational features in modern applications, such as safety, fairness, distributional robustness, and prediction error stability. However, due to its variational definition, $\mathrm{CV@R}$ is commonly believed to result in difficult optimization problems, even for smooth and strongly convex loss functions. We disprove this statement by establishing noisy (i.e., fixed-accuracy) linear convergence of stochastic gradient descent for sequential $\mathrm{CV@R}$ learning, for a large class of not necessarily strongly-convex (or even convex) loss functions satisfying a set-restricted Polyak-Lojasiewicz inequality. This class contains all smooth and strongly convex losses, confirming that classical problems, such as linear least squares regression, can be solved efficiently under the $\mathrm{CV@R}$ criterion, just as their risk-neutral versions. Our results are illustrated numerically on such a risk-aware ridge regression task, also verifying their validity in practice.      
### 56.Calibrating Path Choices and Train Capacities for Urban Rail Transit Simulation Models Using Smart Card Data  [ :arrow_down: ](https://arxiv.org/pdf/2012.07731.pdf)
>  Transit network simulation models are often used for performance and retrospective analysis of urban rail systems, taking advantage of the availability of extensive automated fare collection (AFC) and automated vehicle location (AVL) data. Important inputs to such models, in addition to origin-destination flows, include passenger path choices and train capacity. Train capacity, which has often been overlooked in the literature, is an important input that exhibits a lot of variability. The paper proposes a simulation-based optimization (SBO) framework to calibrate path choices and train capacity for urban rail systems using AFC and AVL data. The calibration is formulated as an optimization problem with a black-box objective function. Seven algorithms from four branches of SBO solving methods are evaluated. The algorithms are evaluated using an experimental design that includes five scenarios, representing different degrees of path choice randomness and crowding sensitivity. Data from the Hong Kong Mass Transit Railway (MTR) system is used as a case study. The data is used to generate synthetic observations used as "ground truth". The results show that the response surface methods (particularly Constrained Optimization using Response Surfaces) have consistently good performance under all scenarios. The proposed approach drives large-scale simulation applications for monitoring and planning.      
### 57.Localization Attack by Precoder Feedback Overhearing in 5G Networks and Countermeasures  [ :arrow_down: ](https://arxiv.org/pdf/2012.07727.pdf)
>  In fifth-generation (5G) cellular networks, users feed back to the base station the index of the precoder (from a codebook) to be used for downlink transmission. The precoder is strongly related to the user channel and in turn to the user position within the cell. We propose a method by which an external attacker determines the user position by passively overhearing this unencrypted layer-2 feedback signal. The attacker first builds a map of fed back precoder indices in the cell. Then, by overhearing the precoder index fed back by the victim user, the attacker finds its position on the map. We focus on the type-I single-panel codebook, which today is the only mandatory solution in the 3GPP standard. We analyze the attack and assess the obtained localization accuracy against various parameters. We analyze the localization error of a simplified precoder feedback model and describe its asymptotic localization precision. We also propose a mitigation against our attack, wherein the user randomly selects the precoder among those providing the highest rate. Simulations confirm that the attack can achieve a high localization accuracy, which is significantly reduced when the mitigation solution is adopted, at the cost of a negligible rate degradation.      
### 58.Nonlinear state-space identification using deep encoder networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.07697.pdf)
>  Nonlinear state-space identification for dynamical systems is most often performed by minimizing the simulation error to reduce the effect of model errors. This optimization problem becomes computationally expensive for large datasets. Moreover, the problem is also strongly non-convex, often leading to sub-optimal parameter estimates. This paper introduces a method that approximates the simulation loss by splitting the data set into multiple independent sections similar to the multiple shooting method. This splitting operation allows for the use of stochastic gradient optimization methods which scale well with data set size and has a smoothing effect on the non-convex cost function. The main contribution of this paper is the introduction of an encoder function to estimate the initial state at the start of each section. The encoder function estimates the initial states using a feed-forward neural network starting from historical input and output samples. The efficiency and performance of the proposed state-space encoder method is illustrated on two well-known benchmarks where, for instance, the method achieves the lowest known simulation error on the Wiener--Hammerstein benchmark.      
### 59.System identification of biophysical neuronal models  [ :arrow_down: ](https://arxiv.org/pdf/2012.07691.pdf)
>  After sixty years of quantitative biophysical modeling of neurons, the identification of neuronal dynamics from input-output data remains a challenging problem, primarily due to the inherently nonlinear nature of excitable behaviors. By reformulating the problem in terms of the identification of an operator with fading memory, we explore a simple approach based on a parametrization given by a series interconnection of Generalized Orthonormal Basis Functions (GOBFs) and static Artificial Neural Networks. We show that GOBFs are particularly well-suited to tackle the identification problem, and provide a heuristic for selecting GOBF poles which addresses the ultra-sensitivity of neuronal behaviors. The method is illustrated on the identification of a bursting model from the crab stomatogastric ganglion.      
### 60.An efficient Quasi-Newton method for nonlinear inverse problems via learned singular values  [ :arrow_down: ](https://arxiv.org/pdf/2012.07676.pdf)
>  Solving complex optimization problems in engineering and the physical sciences requires repetitive computation of multi-dimensional function derivatives. Commonly, this requires computationally-demanding numerical differentiation such as perturbation techniques, which ultimately limits the use for time-sensitive applications. In particular, in nonlinear inverse problems Gauss-Newton methods are used that require iterative updates to be computed from the Jacobian. Computationally more efficient alternatives are Quasi-Newton methods, where the repeated computation of the Jacobian is replaced by an approximate update. Here we present a highly efficient data-driven Quasi-Newton method applicable to nonlinear inverse problems. We achieve this, by using the singular value decomposition and learning a mapping from model outputs to the singular values to compute the updated Jacobian. This enables a speed-up expected of Quasi-Newton methods without accumulating roundoff errors, enabling time-critical applications and allowing for flexible incorporation of prior knowledge necessary to solve ill-posed problems. We present results for the highly non-linear inverse problem of electrical impedance tomography with experimental data.      
### 61.Deep Neural Networks for COVID-19 Detection and Diagnosis using Images and Acoustic-based Techniques: A Recent Review  [ :arrow_down: ](https://arxiv.org/pdf/2012.07655.pdf)
>  The new coronavirus disease (COVID-19) has been declared a pandemic since March 2020 by the World Health Organization. It consists of an emerging viral infection with respiratory tropism that could develop atypical pneumonia. Experts emphasize the importance of early detection of those who have the COVID-19 virus. In this way, patients will be isolated from other people and the spread of the virus can be prevented. For this reason, it has become an area of interest to develop early diagnosis and detection methods to ensure a rapid treatment process and prevent the virus from spreading. Since the standard testing system is time-consuming and not available for everyone, alternative early-screening techniques have become an urgent need. In this study, the approaches used in the detection of COVID-19 based on deep learning (DL) algorithms, which have been popular in recent years, have been comprehensively discussed. The advantages and disadvantages of different approaches used in literature are examined in detail. The Computed Tomography of the chest and X-ray images give a rich representation of the patient's lung that is less time-consuming and allows an efficient viral pneumonia detection using the DL algorithms. The first step is the pre-processing of these images to remove noise. Next, deep features are extracted using multiple types of deep models (pre-trained models, generative models, generic neural networks, etc). Finally, the classification is performed using the obtained features to decide whether the patient is infected by coronavirus or it is another lung disease. In this study, we also give a brief review of the latest applications of cough analysis to early screen the COVID-19, and human mobility estimation to limit its spread.      
### 62.WDNet: Watermark-Decomposition Network for Visible Watermark Removal  [ :arrow_down: ](https://arxiv.org/pdf/2012.07616.pdf)
>  Visible watermarks are widely-used in images to protect copyright ownership. Analyzing watermark removal helps to reinforce the anti-attack techniques in an adversarial way. Current removal methods normally leverage image-to-image translation techniques. Nevertheless, the uncertainty of the size, shape, color and transparency of the watermarks set a huge barrier for these methods. To combat this, we combine traditional watermarked image decomposition into a two-stage generator, called Watermark-Decomposition Network (WDNet), where the first stage predicts a rough decomposition from the whole watermarked image and the second stage specifically centers on the watermarked area to refine the removal results. The decomposition formulation enables WDNet to separate watermarks from the images rather than simply removing them. We further show that these separated watermarks can serve as extra nutrients for building a larger training dataset and further improving removal performance. Besides, we construct a large-scale dataset named CLWD, which mainly contains colored watermarks, to fill the vacuum of colored watermark removal dataset. Extensive experiments on the public gray-scale dataset LVW and CLWD consistently show that the proposed WDNet outperforms the state-of-the-art approaches both in accuracy and efficiency.      
### 63.Lyapunov Conditions for Uniform Asymptotic Output Stability and a Relaxation of Barbalat's Lemma  [ :arrow_down: ](https://arxiv.org/pdf/2012.07607.pdf)
>  Asymptotic output stability (AOS) is an interesting property when addressing control applications in which not all state variables are requested to converge to the origin. AOS is often established by invoking classical tools such as Barbashin-Krasovskii-LaSalle's invariance principle or Barbalat's lemma. Nevertheless, none of these tools allow to predict whether the output convergence is uniform on bounded sets of initial conditions, which may lead to practical issues related to convergence speed and robustness. The contribution of this paper is twofold. First, we provide a testable sufficient condition under which this uniform convergence holds. Second, we provide an extension of Barbalat's lemma, which relaxes the uniform continuity requirement. Both these results are first stated in a finite-dimensional context and then extended to infinite-dimensional systems. We provide academic examples to illustrate the usefulness of these results and show that they can be invoked to establish uniform AOS for systems under adaptive control.      
### 64.Biomechanical modelling of brain atrophy through deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.07596.pdf)
>  We present a proof-of-concept, deep learning (DL) based, differentiable biomechanical model of realistic brain deformations. Using prescribed maps of local atrophy and growth as input, the network learns to deform images according to a Neo-Hookean model of tissue deformation. The tool is validated using longitudinal brain atrophy data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, and we demonstrate that the trained model is capable of rapidly simulating new brain deformations with minimal residuals. This method has the potential to be used in data augmentation or for the exploration of different causal hypotheses reflecting brain growth and atrophy.      
### 65.Towards unsupervised phone and word segmentation using self-supervised vector-quantized neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.07551.pdf)
>  We investigate segmenting and clustering speech into low-bitrate phone-like sequences without supervision. We specifically constrain pretrained self-supervised vector-quantized (VQ) neural networks so that blocks of contiguous feature vectors are assigned to the same code, thereby giving a variable-rate segmentation of the speech into discrete units. Two segmentation methods are considered. In the first, features are greedily merged until a prespecified number of segments are reached. The second uses dynamic programming to optimize a squared error with a penalty term to encourage fewer but longer segments. We show that these VQ segmentation methods can be used without alteration across a wide range of tasks: unsupervised phone segmentation, ABX phone discrimination, same-different word discrimination, and as inputs to a symbolic word segmentation algorithm. The penalized method generally performs best. While results are only comparable to the state-of-the-art in some cases, in all tasks a reasonable competing approach is outperformed at a substantially lower bitrate.      
### 66.Bayesian Learning for Deep Neural Network Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07460.pdf)
>  A key task for speech recognition systems is to reduce the mismatch between the training and evaluation data that is often attributable to speaker differences. To this end, speaker adaptation techniques play a vital role to reduce the mismatch. Model-based speaker adaptation approaches often require sufficient amounts of target speaker data to ensure robustness. When the amount of speaker level data is limited, speaker adaptation is prone to overfitting and poor generalization. To address the issue, this paper proposes a full Bayesian learning based DNN speaker adaptation framework to model speaker-dependent (SD) parameter uncertainty given limited speaker specific adaptation data. This framework is investigated in three forms of model based DNN adaptation techniques: Bayesian learning of hidden unit contributions (BLHUC), Bayesian parameterized activation functions (BPAct), and Bayesian hidden unit bias vectors (BHUB). In all three Bayesian adaptation methods, deterministic SD parameters are replaced by latent variable posterior distributions to be learned for each speaker, whose parameters are efficiently estimated using a variational inference based approach. Experiments conducted on 300-hour speed perturbed Switchboard corpus trained LF-MMI factored TDNN/CNN-TDNN systems featuring i-vector speaker adaptation suggest the proposed Bayesian adaptation approaches consistently outperform the adapted systems using deterministic parameters on the NIST Hub5'00 and RT03 evaluation sets in both unsupervised test time speaker adaptation and speaker adaptive training. The efficacy of the proposed Bayesian adaptation techniques is further demonstrated in a comparison against the state-of-the-art performance obtained on the same task using the most recent hybrid and end-to-end systems reported in the literature.      
### 67.DSM Refinement with Deep Encoder-Decoder Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.07427.pdf)
>  3D city models can be generated from aerial images. However, the calculated DSMs suffer from noise, artefacts, and data holes that have to be manually cleaned up in a time-consuming process. This work presents an approach that automatically refines such DSMs. The key idea is to teach a neural network the characteristics of urban area from reference data. In order to achieve this goal, a loss function consisting of an L1 norm and a feature loss is proposed. These features are constructed using a pre-trained image classification network. To learn to update the height maps, the network architecture is set up based on the concept of deep residual learning and an encoder-decoder structure. The results show that this combination is highly effective in preserving the relevant geometric structures while removing the undesired artefacts and noise.      
### 68.Towards localisation of keywords in speech using weak supervision  [ :arrow_down: ](https://arxiv.org/pdf/2012.07396.pdf)
>  Developments in weakly supervised and self-supervised models could enable speech technology in low-resource settings where full transcriptions are not available. We consider whether keyword localisation is possible using two forms of weak supervision where location information is not provided explicitly. In the first, only the presence or absence of a word is indicated, i.e. a bag-of-words (BoW) labelling. In the second, visual context is provided in the form of an image paired with an unlabelled utterance; a model then needs to be trained in a self-supervised fashion using the paired data. For keyword localisation, we adapt a saliency-based method typically used in the vision domain. We compare this to an existing technique that performs localisation as a part of the network architecture. While the saliency-based method is more flexible (it can be applied without architectural restrictions), we identify a critical limitation when using it for keyword localisation. Of the two forms of supervision, the visually trained model performs worse than the BoW-trained model. We show qualitatively that the visually trained model sometimes locate semantically related words, but this is not consistent. While our results show that there is some signal allowing for localisation, it also calls for other localisation methods better matched to these forms of weak supervision.      
### 69.Holographic MIMO Communications Under Spatially-Stationary Scattering  [ :arrow_down: ](https://arxiv.org/pdf/2012.07389.pdf)
>  Holographic MIMO is a spatially-constrained MIMO system with a massive number of antennas, possibly thought of, in its ultimate form, as a spatially-continuous electromagnetic aperture. Accurate and tractable channel modeling is critical to understanding the full potential of this technology. This paper considers arbitrary spatially-stationary scattering and provides a 4D plane-wave representation in Cartesian coordinates, which captures the essence of electromagnetic propagation and allows to evaluate the capacity of Holographic MIMO systems with rectangular volumetric arrays. The developed framework generalizes the virtual channel representation, which was originally developed for uniform linear arrays.      
### 70.A comparison of self-supervised speech representations as input features for unsupervised acoustic word embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2012.07387.pdf)
>  Many speech processing tasks involve measuring the acoustic similarity between speech segments. Acoustic word embeddings (AWE) allow for efficient comparisons by mapping speech segments of arbitrary duration to fixed-dimensional vectors. For zero-resource speech processing, where unlabelled speech is the only available resource, some of the best AWE approaches rely on weak top-down constraints in the form of automatically discovered word-like segments. Rather than learning embeddings at the segment level, another line of zero-resource research has looked at representation learning at the short-time frame level. Recent approaches include self-supervised predictive coding and correspondence autoencoder (CAE) models. In this paper we consider whether these frame-level features are beneficial when used as inputs for training to an unsupervised AWE model. We compare frame-level features from contrastive predictive coding (CPC), autoregressive predictive coding and a CAE to conventional MFCCs. These are used as inputs to a recurrent CAE-based AWE model. In a word discrimination task on English and Xitsonga data, all three representation learning approaches outperform MFCCs, with CPC consistently showing the biggest improvement. In cross-lingual experiments we find that CPC features trained on English can also be transferred to Xitsonga.      
### 71.Safe Reinforcement Learning with Stability &amp; Safety Guarantees Using Robust MPC  [ :arrow_down: ](https://arxiv.org/pdf/2012.07369.pdf)
>  Reinforcement Learning offers tools to optimize policies based on the data obtained from the real system subject to the policy. While the potential of Reinforcement Learning is well understood, many critical aspects still need to be tackled. One crucial aspect is the issue of safety and stability. Recent publications suggest the use of Nonlinear Model Predictive Control techniques in combination with Reinforcement Learning as a viable and theoretically justified approach to tackle these problems. In particular, it has been suggested that robust MPC allows for making formal stability and safety claims in the context of Reinforcement Learning. However, a formal theory detailing how safety and stability can be enforced through the parameter updates delivered by the Reinforcement Learning tools is still lacking. This paper addresses this gap. The theory is developed for the generic robust MPC case, and further detailed in the robust tube-based linear MPC case, where the theory is fairly easy to deploy in practice.      
### 72.Classification of ALS patients based on acoustic analysis of sustained vowel phonations  [ :arrow_down: ](https://arxiv.org/pdf/2012.07347.pdf)
>  Amyotrophic lateral sclerosis (ALS) is incurable neurological disorder with rapidly progressive course. Common early symptoms of ALS are difficulty in swallowing and speech. However, early acoustic manifestation of speech and voice symptoms is very variable, that making their detection very challenging, both by human specialists and automatic systems. This study presents an approach to voice assessment for automatic system that separates healthy people from patients with ALS. In particular, this work focus on analysing of sustain phonation of vowels /a/ and /i/ to perform automatic classification of ALS patients. A wide range of acoustic features such as MFCC, formants, jitter, shimmer, vibrato, PPE, GNE, HNR, etc. were analysed. We also proposed a new set of acoustic features for characterizing harmonic structure of the vowels. Calculation of these features is based on pitch synchronized voice analysis. A linear discriminant analysis (LDA) was used to classify the phonation produced by patients with ALS and those by healthy individuals. Several algorithms of feature selection were tested to find optimal feature subset for LDA model. The study's experiments show that the most successful LDA model based on 32 features picked out by LASSO feature selection algorithm attains 99.7% accuracy with 99.3% sensitivity and 99.9% specificity. Among the classifiers with a small number of features, we can highlight LDA model with 5 features, which has 89.0% accuracy (87.5% sensitivity and 90.4% specificity).      
### 73.Moving Object Captured with Pink Noise Pattern in Computational Ghost Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.07284.pdf)
>  We develop and experimentally demonstrate an imaging method based on the pink noise pattern in the computational ghost imaging (CGI) system, which has a strong ability to photograph moving objects. To examine its unique ability and scope of application, the object oscillates with variable amplitude in horizontal axis, and the result via commonly used white noise are also measured as a comparison. We show that our method can image the object when the white noise method fails. In addition, our method uses less number of patterns, and enhances the signal-to-noise ratio (SNR) to a great extent.      
### 74.Sub-Nyquist computational ghost imaging with orthonormalized colored noise pattern  [ :arrow_down: ](https://arxiv.org/pdf/2012.07250.pdf)
>  Computational ghost imaging generally requires lots of patterns to obtain a high-quality image. The colored noise speckle pattern was recently proposed to substitute the white noise pattern in a variety of noisy environments and gives a significant signal-to-noise ratio enhancement even with a limited number of patterns. We propose and experimentally demonstrate here an orthonormalization approach based on the colored noise patterns to achieve sub-Nyquist computational ghost imaging. We tested the reconstructed image in quality indicators such as the contrast-to-noise ratio, the mean square error, the peak signal to noise ratio, and the correlation coefficient. The results suggest that our method can provide high-quality images while using a sampling ratio an order lower than the conventional methods.      
### 75.A Feature Weighted Mixed Naive Bayes Model for Monitoring Anomalies in the Fan System of a Thermal Power Plant  [ :arrow_down: ](https://arxiv.org/pdf/2012.07230.pdf)
>  With the increasing intelligence and integration, a great number of two-valued variables (generally stored in the form of 0 or 1 value) often exist in large-scale industrial processes. However, these variables cannot be effectively handled by traditional monitoring methods such as LDA, PCA and PLS. Recently, a mixed hidden naive Bayesian model (MHNBM) is developed for the first time to utilize both two-valued and continuous variables for abnormality monitoring. Although MHNBM is effective, it still has some shortcomings that need to be improved. For MHNBM, the variables with greater correlation to other variables have greater weights, which cannot guarantee greater weights are assigned to the more discriminating variables. In addition, the conditional probability must be computed based on the historical data. When the training data is scarce, the conditional probability between continuous variables tends to be uniformly distributed, which affects the performance of MHNBM. Here a novel feature weighted mixed naive Bayes model (FWMNBM) is developed to overcome the above shortcomings. For FWMNBM, the variables that are more correlated to the class have greater weights, which makes the more discriminating variables contribute more to the model. At the same time, FWMNBM does not have to calculate the conditional probability between variables, thus it is less restricted by the number of training data samples. Compared with MHNBM, FWMNBM has better performance, and its effectiveness is validated through the numerical cases of a simulation example and a practical case of Zhoushan thermal power plant (ZTPP), China.      
### 76.Intelligent Reflecting Surface Aided Full-Duplex Communication: Passive Beamforming and Deployment Design  [ :arrow_down: ](https://arxiv.org/pdf/2012.07218.pdf)
>  This paper investigates the passive beamforming and deployment design for an intelligent reflecting surface (IRS) aided full-duplex (FD) wireless system, where an FD access point (AP) communicates with an uplink (UL) user and a downlink (DL) user simultaneously over the same time-frequency dimension with the help of IRS. Under this setup, we consider three deployment cases: 1) two distributed IRSs placed near the UL user and DL user, respectively; 2) one centralized IRS placed near the DL user; 3) one centralized IRS placed near the UL user. In each case, we aim to minimize the weighted sum transmit power consumption of the AP and UL user by jointly optimizing their transmit power and the passive reflection coefficients at the IRS (or IRSs), subject to the UL and DL users' rate constraints and the uni-modulus constraints on the IRS reflection coefficients. First, we analyze the minimum transmit power required in the IRS-aided FD system under each deployment scheme, and compare it with that of the corresponding half-duplex (HD) system. We show that the FD system outperforms its HD counterpart for all IRS deployment schemes, while the distributed deployment further outperforms the other two centralized deployment schemes. Next, we transform the challenging power minimization problem into an equivalent but more tractable form and propose an efficient algorithm to solve it based on the block coordinate descent (BCD) method. Finally, numerical results are presented to validate our analysis as well as the efficacy of the proposed passive beamforming design.      
### 77.Mixed interpolatory and inference non-intrusive reduced order modeling with application to pollutants dispersion  [ :arrow_down: ](https://arxiv.org/pdf/2012.07126.pdf)
>  On the basis of input-output time-domain data collected from a complex simulator, this paper proposes a constructive methodology to infer a reduced-order linear, bilinear or quadratic time invariant dynamical model reproducing the underlying phenomena. The approach is essentially based on linear dynamical systems and approximation theory. More specifically, it sequentially involves the interpolatory Pencil and Loewner framework, known to be both very versatile and scalable to large-scale data sets, and a linear least square problem involving the raw data and reduced internal variables. With respect to intrusive methods, no prior knowledge on the operator is needed. In addition, compared to the traditional non-intrusive operator inference ones, the proposed approach alleviates the need of measuring the original full-order model internal variables. It is thus applicable to a wider application range than standard intrusive and non-intrusive methods. The rationale is successfully applied on a large eddy simulation of a pollutants dispersion case over an airport area involving multi-scale and multi-physics dynamical phenomena. Despite the simplicity of the resulting low complexity model, the proposed approach shows satisfactory results to predict the pollutants plume pattern while being significantly faster to simulate.      
### 78.Vision Based Adaptation to Kernelized Synergies for Human Inspired Robotic Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2012.07046.pdf)
>  Humans in contrast to robots are excellent in performing fine manipulation tasks owing to their remarkable dexterity and sensorimotor organization. Enabling robots to acquire such capabilities, necessitates a framework that not only replicates the human behaviour but also integrates the multi-sensory information for autonomous object interaction. To address such limitations, this research proposes to augment the previously developed kernelized synergies framework with visual perception to automatically adapt to the unknown objects. The kernelized synergies, inspired from humans, retain the same reduced subspace for object grasping and manipulation. To detect object in the scene, a simplified perception pipeline is used that leverages the RANSAC algorithm with Euclidean clustering and SVM for object segmentation and recognition respectively. Further, the comparative analysis of kernelized synergies with other state of art approaches is made to confirm their flexibility and effectiveness on the robotic manipulation tasks. The experiments conducted on the robot hand confirm the robustness of modified kernelized synergies framework against the uncertainties related to the perception of environment.      
### 79.Statistical CSI-based Design for Reconfigurable Intelligent Surface-aided Massive MIMO Systems with Direct Links  [ :arrow_down: ](https://arxiv.org/pdf/2012.07030.pdf)
>  This paper investigates the performance of reconfigurable intelligent surface (RIS)-aided massive multiple-input multiple-output (MIMO) systems with direct links, and the phase shifts of the RIS are designed based on the statistical channel state information (CSI). We first derive the closed-form expression of the uplink ergodic data rate. Then, based on the derived expression, we use the genetic algorithm (GA) to solve the sum data rate maximization problem. With low-complexity maximal-ratio combination (MRC) and low-overhead statistical CSI-based scheme, we validate that the RIS can still bring significant performance gains to traditional massive MIMO systems.      
### 80.Efficient Online Trajectory Planning for Integrator Chain Dynamics using Polynomial Elimination  [ :arrow_down: ](https://arxiv.org/pdf/2012.07029.pdf)
>  Providing smooth reference trajectories can effectively increase performance and accuracy of tracking control applications while overshoot and unwanted vibrations are reduced. Trajectory planning computations can often be simplified significantly by transforming the system dynamics into decoupled integrator chains using methods such as feedback linearization, differential flatness or the controller canonical form. We present an efficient method to plan time optimal trajectories for integrator chains subject to derivative bound constraints. Therefore, an algebraic precomputation algorithm formulates the necessary conditions for time optimality in form of a set of polynomial systems, followed by a symbolic polynomial elimination using Grbner bases. A fast online algorithm then plans the trajectories by calculating the roots of the decomposed polynomial systems. These roots describe the switching time instants of the input signal and the full trajectory simply follows by multiple integration. This method presents a systematic way to compute time optimal trajectories exactly via algebraic calculations without numerical approximation iterations. It is applied to various trajectory types with different continuity order, asymmetric derivative bounds and non-rest initial and final states.      
### 81.Split then Refine: Stacked Attention-guided ResUNets for Blind Single Image Visible Watermark Removal  [ :arrow_down: ](https://arxiv.org/pdf/2012.07007.pdf)
>  Digital watermark is a commonly used technique to protect the copyright of medias. Simultaneously, to increase the robustness of watermark, attacking technique, such as watermark removal, also gets the attention from the community. Previous watermark removal methods require to gain the watermark location from users or train a multi-task network to recover the background indiscriminately. However, when jointly learning, the network performs better on watermark detection than recovering the texture. Inspired by this observation and to erase the visible watermarks blindly, we propose a novel two-stage framework with a stacked attention-guided ResUNets to simulate the process of detection, removal and refinement. In the first stage, we design a multi-task network called SplitNet. It learns the basis features for three sub-tasks altogether while the task-specific features separately use multiple channel attentions. Then, with the predicted mask and coarser restored image, we design RefineNet to smooth the watermarked region with a mask-guided spatial attention. Besides network structure, the proposed algorithm also combines multiple perceptual losses for better quality both visually and numerically. We extensively evaluate our algorithm over four different datasets under various settings and the experiments show that our approach outperforms other state-of-the-art methods by a large margin. The code is available at <a class="link-external link-http" href="http://github.com/vinthony/deep-blind-watermark-removal" rel="external noopener nofollow">this http URL</a>.      
### 82.Joint Hardware Design and Capacity Analysis for Intelligent Reflecting Surface Enabled Terahertz MIMO Communications  [ :arrow_down: ](https://arxiv.org/pdf/2012.06993.pdf)
>  Terahertz (THz) communications have been envisioned as a promising enabler to provide ultra-high data transmission for sixth generation (6G) wireless networks. To tackle the blockage vulnerability brought by severe path attenuation and poor diffraction of THz waves, an intelligent reflecting surface (IRS) is put forward to smartly control the incident THz waves by adjusting the phase shifts. In this paper, we firstly design an efficient hardware structure of graphene-based IRS with phase response up to 306.82 degrees. Subsequently, to characterize the capacity of the IRS-enabled THz multiple-input multiple-output (MIMO) system, an adaptive gradient descent (A-GD) algorithm is developed by dynamically updating the step size during the iterative process, which is determined by the second-order Taylor expansion formulation. In contrast with conventional gradient descent (C-GD) algorithm with fixed step size, the A-GD algorithm evidently improves the achievable rate performance. However, both A-GD algorithm and C-GD algorithm inherit the unacceptable complexity. Then a low complexity alternating optimization (AO) algorithm is proposed by alternately optimizing the precoding matrix by a column-by-column (CBC) algorithm and the phase shift matrix of the IRS by a linear search algorithm. Ultimately, the numerical results demonstrate the effectiveness of the designed hardware structure and the considered algorithms.      
### 83.Spontaneous Emotion Recognition from Facial Thermal Images  [ :arrow_down: ](https://arxiv.org/pdf/2012.06973.pdf)
>  One of the key research areas in computer vision addressed by a vast number of publications is the processing and understanding of images containing human faces. The most often addressed tasks include face detection, facial landmark localization, face recognition and facial expression analysis. Other, more specialized tasks such as affective computing, the extraction of vital signs from videos or analysis of social interaction usually require one or several of the aforementioned tasks that have to be performed. In our work, we analyze that a large number of tasks for facial image processing in thermal infrared images that are currently solved using specialized rule-based methods or not solved at all can be addressed with modern learning-based approaches. We have used USTC-NVIE database for training of a number of machine learning algorithms for facial landmark localization.      
### 84.Network-Cognizant Time-Coupled Aggregate Flexibility of Distribution Systems Under Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2012.06947.pdf)
>  Increasing integration of distributed energy resources (DERs) within distribution feeders provides unprecedented flexibility at the distribution-transmission interconnection. To exploit this flexibility and to use the capacity potential of aggregate DERs, feasible substation power injection trajectories need to be efficiently characterized. This paper provides an ellipsoidal inner approximation of the set of feasible power injection trajectories at the substation such that for any point in the set, there exists a feasible disaggregation strategy of DERs for any load uncertainty realization. The problem is formulated as one of finding the robust maximum volume ellipsoid inside the flexibility region under uncertainty. Though the problem is NP-hard even in the deterministic case, this paper derives novel approximations of the resulting adaptive robust optimization problem based on optimal second-stage policies. The proposed approach yields less conservative flexibility characterization than existing flexibility region approximation formulations. The efficacy of the proposed method is demonstrated on a realistic distribution feeder.      
### 85.VoxSRC 2020: The Second VoxCeleb Speaker Recognition Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2012.06867.pdf)
>  We held the second installment of the VoxCeleb Speaker Recognition Challenge in conjunction with Interspeech 2020. The goal of this challenge was to assess how well current speaker recognition technology is able to diarise and recognize speakers in unconstrained or `in the wild' data. It consisted of: (i) a publicly available speaker recognition and diarisation dataset from YouTube videos together with ground truth annotation and standardised evaluation software; and (ii) a virtual public challenge and workshop held at Interspeech 2020. This paper outlines the challenge, and describes the baselines, methods used, and results. We conclude with a discussion of the progress over the first installment of the challenge.      
### 86.Tutoring Reinforcement Learning via Feedback Control  [ :arrow_down: ](https://arxiv.org/pdf/2012.06863.pdf)
>  We introduce a control-tutored reinforcement learning (CTRL) algorithm. The idea is to enhance tabular learning algorithms by means of a control strategy with limited knowledge of the system model. By tutoring the learning process, the learning rate can be substantially reduced. We use the classical problem of stabilizing an inverted pendulum as a benchmark to numerically illustrate the advantages and disadvantages of the approach.      
### 87.High Order Local Directional Pattern Based Pyramidal Multi-structure for Robust Face Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.06838.pdf)
>  Derived from a general definition of texture in a local neighborhood, local directional pattern (LDP) encodes the directional information in the small local 3x3 neighborhood of a pixel, which may fail to extract detailed information especially during changes in the input image due to illumination variations. Therefore, in this paper we introduce a novel feature extraction technique that calculates the nth order direction variation patterns, named high order local directional pattern (HOLDP). The proposed HOLDP can capture more detailed discriminative information than the conventional LDP. Unlike the LDP operator, our proposed technique extracts nth order local information by encoding various distinctive spatial relationships from each neighborhood layer of a pixel in the pyramidal multi-structure way. Then we concatenate the feature vector of each neighborhood layer to form the final HOLDP feature vector. The performance evaluation of the proposed HOLDP algorithm is conducted on several publicly available face databases and observed the superiority of HOLDP under extreme illumination conditions.      
### 88.Anomaly detection through latent space restoration using vector-quantized variational autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2012.06765.pdf)
>  We propose an out-of-distribution detection method that combines density and restoration-based approaches using Vector-Quantized Variational Auto-Encoders (VQ-VAEs). The VQ-VAE model learns to encode images in a categorical latent space. The prior distribution of latent codes is then modelled using an Auto-Regressive (AR) model. We found that the prior probability estimated by the AR model can be useful for unsupervised anomaly detection and enables the estimation of both sample and pixel-wise anomaly scores. The sample-wise score is defined as the negative log-likelihood of the latent variables above a threshold selecting highly unlikely codes. Additionally, out-of-distribution images are restored into in-distribution images by replacing unlikely latent codes with samples from the prior model and decoding to pixel space. The average L1 distance between generated restorations and original image is used as pixel-wise anomaly score. We tested our approach on the MOOD challenge datasets, and report higher accuracies compared to a standard reconstruction-based approach with VAEs.      
### 89.Less Is More: Improved RNN-T Decoding Using Limited Label Context and Path Merging  [ :arrow_down: ](https://arxiv.org/pdf/2012.06749.pdf)
>  End-to-end models that condition the output label sequence on all previously predicted labels have emerged as popular alternatives to conventional systems for automatic speech recognition (ASR). Since unique label histories correspond to distinct models states, such models are decoded using an approximate beam-search process which produces a tree of hypotheses. <br>In this work, we study the influence of the amount of label context on the model's accuracy, and its impact on the efficiency of the decoding process. We find that we can limit the context of the recurrent neural network transducer (RNN-T) during training to just four previous word-piece labels, without degrading word error rate (WER) relative to the full-context baseline. Limiting context also provides opportunities to improve the efficiency of the beam-search process during decoding by removing redundant paths from the active beam, and instead retaining them in the final lattice. This path-merging scheme can also be applied when decoding the baseline full-context model through an approximation. Overall, we find that the proposed path-merging scheme is extremely effective allowing us to improve oracle WERs by up to 36% over the baseline, while simultaneously reducing the number of model evaluations by up to 5.3% without any degradation in WER.      
### 90.Generating Adversarial Disturbances for Controller Verification  [ :arrow_down: ](https://arxiv.org/pdf/2012.06695.pdf)
>  We consider the problem of generating maximally adversarial disturbances for a given controller assuming only blackbox access to it. We propose an online learning approach to this problem that adaptively generates disturbances based on control inputs chosen by the controller. The goal of the disturbance generator is to minimize regret versus a benchmark disturbance-generating policy class, i.e., to maximize the cost incurred by the controller as well as possible compared to the best possible disturbance generator in hindsight (chosen from a benchmark policy class). In the setting where the dynamics are linear and the costs are quadratic, we formulate our problem as an online trust region (OTR) problem with memory and present a new online learning algorithm (MOTR) for this problem. We prove that this method competes with the best disturbance generator in hindsight (chosen from a rich class of benchmark policies that includes linear-dynamical disturbance generating policies). We demonstrate our approach on two simulated examples: (i) synthetically generated linear systems, and (ii) generating wind disturbances for the popular PX4 controller in the AirSim simulator. On these examples, we demonstrate that our approach outperforms several baseline approaches, including $H_{\infty}$ disturbance generation and gradient-based methods.      
### 91.How to Train your Quadrotor: A Framework for Consistently Smooth and Responsive Flight Control via Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06656.pdf)
>  We focus on the problem of reliably training Reinforcement Learning (RL) models (agents) for stable low-level control in embedded systems and test our methods on a high-performance, custom-built quadrotor platform. A common but often under-studied problem in developing RL agents for continuous control is that the control policies developed are not always smooth. This lack of smoothness can be a major problem when learning controllers %intended for deployment on real hardware as it can result in control instability and hardware failure. Issues of noisy control are further accentuated when training RL agents in simulation due to simulators ultimately being imperfect representations of reality - what is known as the reality gap. To combat issues of instability in RL agents, we propose a systematic framework, `REinforcement-based transferable Agents through Learning' (RE+AL), for designing simulated training environments which preserve the quality of trained agents when transferred to real platforms. RE+AL is an evolution of the Neuroflight infrastructure detailed in technical reports prepared by members of our research group. Neuroflight is a state-of-the-art framework for training RL agents for low-level attitude control. RE+AL improves and completes Neuroflight by solving a number of important limitations that hindered the deployment of Neuroflight to real hardware. We benchmark RE+AL on the NF1 racing quadrotor developed as part of Neuroflight. We demonstrate that RE+AL significantly mitigates the previously observed issues of smoothness in RL agents. Additionally, RE+AL is shown to consistently train agents that are flight-capable and with minimal degradation in controller quality upon transfer. RE+AL agents also learn to perform better than a tuned PID controller, with better tracking errors, smoother control and reduced power consumption.      
### 92.Regularizing Action Policies for Smooth Control with Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06644.pdf)
>  A critical problem with the practical utility of controllers trained with deep Reinforcement Learning (RL) is the notable lack of smoothness in the actions learned by the RL policies. This trend often presents itself in the form of control signal oscillation and can result in poor control, high power consumption, and undue system wear. We introduce Conditioning for Action Policy Smoothness (CAPS), an effective yet intuitive regularization on action policies, which offers consistent improvement in the smoothness of the learned state-to-action mappings of neural network controllers, reflected in the elimination of high-frequency components in the control signal. Tested on a real system, improvements in controller smoothness on a quadrotor drone resulted in an almost 80% reduction in power consumption while consistently training flight-worthy controllers. Project website: <a class="link-external link-http" href="http://ai.bu.edu/caps" rel="external noopener nofollow">this http URL</a>      
