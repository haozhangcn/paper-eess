# ArXiv eess --Mon, 14 Dec 2020
### 1.Unified Multi-Rate Control: from Low Level Actuation to High Level Planning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06558.pdf)
>  In this paper we present a hierarchical multi-rate control architecture for nonlinear autonomous systems operating in partially observable environments. Control objectives are expressed using syntactically co-safe Linear Temporal Logic (LTL) specifications and the nonlinear system is subject to state and input constraints. At the highest level of abstraction, we model the system-environment interaction using a discrete Mixed Observable Markov Decision Problem (MOMDP), where the environment states are partially observed. The high level control policy is used to update the constraint sets and cost function of a Model Predictive Controller (MPC) which plans a reference trajectory. Afterwards, the MPC planned trajectory is fed to a low-level high-frequency tracking controller, which leverages Control Barrier Functions (CBFs) to guarantee bounded tracking errors. Our strategy is based on model abstractions of increasing complexity and layers running at different frequencies. We show that the proposed hierarchical multi-rate control architecture maximizes the probability of satisfying the high-level specifications while guaranteeing state and input constraint satisfaction. Finally, we tested the proposed strategy in simulations and experiments on examples inspired by the Mars exploration mission, where only partial environment observations are available.      
### 2.AIforCOVID: predicting the clinical outcomes in patients with COVID-19 applying AI to chest-X-rays. An Italian multicentre study  [ :arrow_down: ](https://arxiv.org/pdf/2012.06531.pdf)
>  Recent epidemiological data report that worldwide more than 53 million people have been infected by SARS-CoV-2, resulting in 1.3 million deaths. The disease has been spreading very rapidly and few months after the identification of the first infected, shortage of hospital resources quickly became a problem. In this work we investigate whether chest X-ray (CXR) can be used as a possible tool for the early identification of patients at risk of severe outcome, like intensive care or death. CXR is a radiological technique that compared to computed tomography (CT) it is simpler, faster, more widespread and it induces lower radiation dose. We present a dataset including data collected from 820 patients by six Italian hospitals in spring 2020 during the first COVID-19 emergency. The dataset includes CXR images, several clinical attributes and clinical outcomes. We investigate the potential of artificial intelligence to predict the prognosis of such patients, distinguishing between severe and mild cases, thus offering a baseline reference for other researchers and practitioners. To this goal, we present three approaches that use features extracted from CXR images, either handcrafted or automatically by convolutional neuronal networks, which are then integrated with the clinical data. Exhaustive evaluation shows promising performance both in 10-fold and leave-one-centre-out cross-validation, implying that clinical data and images have the potential to provide useful information for the management of patients and hospital resources.      
### 3.Quality-of-Transmission Estimation in Physical Impairment Aware Flexible Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.06477.pdf)
>  In this thesis the creation of nonlinear interference noise (NLIN) in the context of impairment aware flexible optical networks is investigated to estimate transmission quality. In particular, the nonlinear interference of neighboring channels (interferer) during transmission on a channel under test is studied. The modulation format of the interferer, the accumulated chromatic dispersion of the interferer, the span length and channel spacing are identified as the parameter influencing the generation of NLIN in the context of flexible optical networks. Estimation of the NLIN is done based on the evaluation of numerical simulation results and compared to recent analytical transmission models, namely the Gaussian noise model and the enhanced Gaussian noise model. In addition to nonlinear noise power, the simulations also yield information about the phase- and circular-noise contributions as well as the correlation time of the phase noise. The results are evaluated in the context of the pulse-collision picture and the influence on transmission quality is discussed.      
### 4.Context Matters: Graph-based Self-supervised Representation Learning for Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2012.06457.pdf)
>  Supervised learning method requires a large volume of annotated datasets. Collecting such datasets is time-consuming and expensive. Until now, very few annotated COVID-19 imaging datasets are available. Although self-supervised learning enables us to bootstrap the training by exploiting unlabeled data, the generic self-supervised methods for natural images do not sufficiently incorporate the context. For medical images, a desirable method should be sensitive enough to detect deviation from normal-appearing tissue of each anatomical region; here, anatomy is the context. We introduce a novel approach with two levels of self-supervised representation learning objectives: one on the regional anatomical level and another on the patient-level. We use graph neural networks to incorporate the relationship between different anatomical regions. The structure of the graph is informed by anatomical correspondences between each patient and an anatomical atlas. In addition, the graph representation has the advantage of handling any arbitrarily sized image in full resolution. Experiments on large-scale Computer Tomography (CT) datasets of lung images show that our approach compares favorably to baseline methods that do not account for the context. We use the learnt embedding to quantify the clinical progression of COVID-19 and show that our method generalizes well to COVID-19 patients from different hospitals. Qualitative results suggest that our model can identify clinically relevant regions in the images.      
### 5.Low-Dose CT Reconstruction Using Deep Generative Regularization Prior  [ :arrow_down: ](https://arxiv.org/pdf/2012.06448.pdf)
>  Low-dose CT imaging requires reconstruction from noisy indirect measurements which can be defined as an ill-posed linear inverse problem. In addition to conventional FBP method in CT imaging, recent compressed sensing based methods exploit handcrafted priors which are mostly simplistic and hard to determine. More recently, deep learning (DL) based methods have become popular in medical imaging field. In CT imaging, DL based methods try to learn a function that maps low-dose images to normal-dose images. Although the results of these methods are promising, their success mostly depends on the availability of high-quality massive datasets. In this study, we proposed a method that does not require any training data or a learning process. Our method exploits such an approach that deep convolutional neural networks (CNNs) generate patterns easier than the noise, therefore randomly initialized generative neural networks can be suitable priors to be used in regularizing the reconstruction. In the experiments, the proposed method is implemented with different loss function variants. Both analytical CT phantoms and real-world CT images are used with different views. Conventional FBP method, a popular iterative method (SART), and TV regularized SART are used in the comparisons. We demonstrated that our method with different loss function variants outperforms the other methods both qualitatively and quantitatively.      
### 6.Uncertainty-driven refinement of tumor-core segmentation using 3D-to-2D networks with label uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2012.06436.pdf)
>  The BraTS dataset contains a mixture of high-grade and low-grade gliomas, which have a rather different appearance: previous studies have shown that performance can be improved by separated training on low-grade gliomas (LGGs) and high-grade gliomas (HGGs), but in practice this information is not available at test time to decide which model to use. By contrast with HGGs, LGGs often present no sharp boundary between the tumor core and the surrounding edema, but rather a gradual reduction of tumor-cell density. <br>Utilizing our 3D-to-2D fully convolutional architecture, DeepSCAN, which ranked highly in the 2019 BraTS challenge and was trained using an uncertainty-aware loss, we separate cases into those with a confidently segmented core, and those with a vaguely segmented or missing core. Since by assumption every tumor has a core, we reduce the threshold for classification of core tissue in those cases where the core, as segmented by the classifier, is vaguely defined or missing. <br>We then predict survival of high-grade glioma patients using a fusion of linear regression and random forest classification, based on age, number of distinct tumor components, and number of distinct tumor cores. <br>We present results on the validation dataset of the Multimodal Brain Tumor Segmentation Challenge 2020 (segmentation and uncertainty challenge), and on the testing set, where the method achieved 4th place in Segmentation, 1st place in uncertainty estimation, and 1st place in Survival prediction.      
### 7.Local-mean preserving post-processing step for non-negativity enforcement in PET imaging: application to $^{90}$Y-PET  [ :arrow_down: ](https://arxiv.org/pdf/2012.06432.pdf)
>  In a low-statistics PET imaging context, the positive bias in regions of low activity is a burning issue. To overcome this problem, algorithms without the built-in non-negativity constraint may be used. They allow negative voxels in the image to reduce, or even to cancel the bias. However, such algorithms increase the variance and are difficult to interpret, since negative radioactive concentrations have no physical meaning. Here, we propose a post-processing strategy to remove negative intensities while preserving the local mean activities. Our idea is to transfer the negative intensities to neighboring voxels, so that the mean of the image is preserved. The proposed post-processing algorithm solves a linear programming problem with a specific symmetric structure, and the solution can be computed in a very efficient way. Acquired data from an yttrium-90 phantom show that on images produced by a non-constrained algorithm, a much lower variance in the cold area is obtained after the post-processing step.      
### 8.Extended Full Block S-Procedure for Distributed Control of Interconnected Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.06398.pdf)
>  This paper proposes a novel method for distributed controller synthesis of homogeneous interconnected systems consisting of identical subsystems. The objective of the designed controller is to minimize the L2-gain of the performance channel. The proposed method is an extended formulation of the Full Block S-Procedure (FBSP) where we introduce an additional set of variables. This allows relaxing the block-diagonal structural assumptions on the Lyapunov and multiplier matrices required for distributed control design, which reduces conservatism w.r.t most existing approaches. We show how to decompose the proposed extended FBSP into small synthesis conditions, of the size of one individual subsystem.      
### 9.6G Wireless Channel Measurements and Models: Trends and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2012.06381.pdf)
>  In this article, we first present our vision on the application scenarios, performance metrics, and potential key technologies of the sixth generation (6G) wireless communication networks. Then, 6G wireless channel measurements, characteristics, and models are comprehensively surveyed for all frequency bands and all scenarios, focusing on millimeter wave (mmWave), terahertz (THz), and optical wireless communication channels under all spectrums, satellite, unmanned aerial vehicle (UAV), maritime, and underwater acoustic communication channels under global coverage scenarios, and high-speed train (HST), vehicle-to-vehicle (V2V), ultra-massive multiple-input multiple-output (MIMO), orbital angular momentum (OAM), and industry Internet of things (IoT) communication channels under full application scenarios. Future research challenges on 6G channel measurements, a general standard 6G channel model framework, channel measurements and models for intelligent reflection surface (IRS) based 6G technologies, and artificial intelligence (AI) enabled channel measurements and models are also given.      
### 10.Descattering and Density Reconstruction in Polyenergetic Tomography using Locally-Learned Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.06348.pdf)
>  We present a method for descattering and quantitative density reconstruction in polyenergetic X-ray computed tomography (CT) based on fitting local models of scatter. X-ray CT is widely used in medical and industrial applications. If not accounted for during reconstruction, X-ray scatter creates a loss of contrast and introduces severe image artifacts including cupping, shading, and streaks. Even when these qualitative artifacts are not apparent, scatter poses a major obstacle in obtaining accurate quantitative radiographic reconstructions. Our approach to estimating scatter is to generate a training set of radiographs with and without scatter using particle transport simulation software. We then learn a locally adaptive model, i.e., one comprised of many models, each fit to a local neighborhood of the training data. We use this scatter model inside an iterative descattering algorithm and reconstruct densities from the corrected data. Our experiments on monoenergetic and polyenergetic data show that, when applied locally, even simple, linear models are highly-effective at estimating scatter. Further, descattering approaches based on these local models can reduce the effect of scatter on density reconstruction by more than half.      
### 11.Distant Domain Transfer Learning for Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.06346.pdf)
>  Medical image processing is one of the most important topics in the field of the Internet of Medical Things (IoMT). Recently, deep learning methods have carried out state-of-the-art performances on medical image tasks. However, conventional deep learning have two main drawbacks: 1) insufficient training data and 2) the domain mismatch between the training data and the testing data. In this paper, we propose a distant domain transfer learning (DDTL) method for medical image classification. Moreover, we apply our methods to a recent issue (Coronavirus diagnose). Several current studies indicate that lung Computed Tomography (CT) images can be used for a fast and accurate COVID-19 diagnosis. However, the well-labeled training data cannot be easily accessed due to the novelty of the disease and a number of privacy policies. Moreover, the proposed method has two components: Reduced-size Unet Segmentation model and Distant Feature Fusion (DFF) classification model. It is related to a not well-investigated but important transfer learning problem, termed Distant Domain Transfer Learning (DDTL). DDTL aims to make efficient transfers even when the domains or the tasks are entirely different. In this study, we develop a DDTL model for COVID-19 diagnose using unlabeled Office-31, Catech-256, and chest X-ray image data sets as the source data, and a small set of COVID-19 lung CT as the target data. The main contributions of this study: 1) the proposed method benefits from unlabeled data collected from distant domains which can be easily accessed, 2) it can effectively handle the distribution shift between the training data and the testing data, 3) it has achieved 96\% classification accuracy, which is 13\% higher classification accuracy than "non-transfer" algorithms, and 8\% higher than existing transfer and distant transfer algorithms.      
### 12.State-of-the-art Machine Learning MRI Reconstruction in 2020: Results of the Second fastMRI Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2012.06318.pdf)
>  Accelerating MRI scans is one of the principal outstanding problems in the MRI research community. Towards this goal, we hosted the second fastMRI competition targeted towards reconstructing MR images with subsampled k-space data. We provided participants with data from 7,299 clinical brain scans (de-identified via a HIPAA-compliant procedure by NYU Langone Health), holding back the fully-sampled data from 894 of these scans for challenge evaluation purposes. In contrast to the 2019 challenge, we focused our radiologist evaluations on pathological assessment in brain images. We also debuted a new Transfer track that required participants to submit models evaluated on MRI scanners from outside the training set. We received 19 submissions from eight different groups. Results showed one team scoring best in both SSIM scores and qualitative radiologist evaluations. We also performed analysis on alternative metrics to mitigate the effects of background noise and collected feedback from the participants to inform future challenges. Lastly, we identify common failure modes across the submissions, highlighting areas of need for future research in the MRI reconstruction community.      
### 13.Signal processing with a distribution of graph operators  [ :arrow_down: ](https://arxiv.org/pdf/2012.06296.pdf)
>  In this paper, we develop a signal processing framework of a network without explicit knowledge of the network topology. Instead, we make use of knowledge on the distribution of operators on the network. This makes the framework flexible and useful when accurate knowledge of graph topology is unavailable. Moreover, the usual graph signal processing is a special case of our framework by using the delta distribution. The main elements of the theory include Fourier transform, theory of filtering and sampling.      
### 14.Blind Monaural Source Separation on Heart and Lung Sounds Based on Periodic-Coded Deep Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2012.06275.pdf)
>  Auscultation is the most efficient way to diagnose cardiovascular and respiratory diseases. To reach accurate diagnoses, a device must be able to recognize heart and lung sounds from various clinical situations. However, the recorded chest sounds are mixed by heart and lung sounds. Thus, effectively separating these two sounds is critical in the pre-processing stage. Recent advances in machine learning have progressed on monaural source separations, but most of the well-known techniques require paired mixed sounds and individual pure sounds for model training. As the preparation of pure heart and lung sounds is difficult, special designs must be considered to derive effective heart and lung sound separation techniques. In this study, we proposed a novel periodicity-coded deep auto-encoder (PC-DAE) approach to separate mixed heart-lung sounds in an unsupervised manner via the assumption of different periodicities between heart rate and respiration rate. The PC-DAE benefits from deep-learning-based models by extracting representative features and considers the periodicity of heart and lung sounds to carry out the separation. We evaluated PC-DAE on two datasets. The first one includes sounds from the Student Auscultation Manikin (SAM), and the second is prepared by recording chest sounds in real-world conditions. Experimental results indicate that PC-DAE outperforms several well-known separations works in terms of standardized evaluation metrics. Moreover, waveforms and spectrograms demonstrate the effectiveness of PC-DAE compared to existing approaches. It is also confirmed that by using the proposed PC-DAE as a pre-processing stage, the heart sound recognition accuracies can be notably boosted. The experimental results confirmed the effectiveness of PC-DAE and its potential to be used in clinical applications.      
### 15.Linearization-Based Quantized Stabilization of Nonlinear Systems Under DoS Attacks  [ :arrow_down: ](https://arxiv.org/pdf/2012.06273.pdf)
>  Motivated by recent security issues in cyber-physical systems, this technical note studies the stabilization problem of networked control systems under Denial-of-Service (DoS) attacks. In particular, we consider to stabilize a nonlinear system with limited data rate via linearization. We employ a deterministic DoS attack model constrained in terms of attacks' frequency and duration, allowing us to cover a large class of potential attacks. To achieve asymptotic stabilization, we propose a resilient dynamic quantizer in the sense that it does not saturate in the presence of packet losses caused by DoS attacks. A sufficient condition for stability is derived by restricting the average DoS frequency and duration. In addition, because of the locality of linearization, we explicitly investigate an estimate of the region of attraction, which can be expected to be reduced depending on the strength of DoS attacks. A simulation example is presented for demonstration of our results.      
### 16.On the Observability and Controllability of Large-Scale IoT Networks: Reducing Number of Unmatched Nodes via Link Addition  [ :arrow_down: ](https://arxiv.org/pdf/2012.06198.pdf)
>  In this paper, we study large-scale networks in terms of observability and controllability. In particular, we compare the number of unmatched nodes in two main types of Scale-Free (SF) networks: the Barab{รก}si-Albert (BA) model and the Holme-Kim (HK) model. Comparing the two models based on theory and simulation, we discuss the possible relation between clustering coefficient and the number of unmatched nodes. In this direction, we propose a new algorithm to reduce the number of unmatched nodes via link addition. The results are significant as one can reduce the number of unmatched nodes and therefore number of embedded sensors/actuators in, for example, an IoT network. This may significantly reduce the cost of controlling devices or monitoring cost in large-scale systems.      
### 17.Point-to-Point Communication in Integrated Satellite-Aerial Networks: State-of-the-art and Future Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2012.06182.pdf)
>  This paper overviews point-to-point (P2P) links for integrated satellite-aerial networks, which are envisioned to be among the key enablers of the sixth-generation (6G) of wireless networks vision. The paper first outlines the unique characteristics of such integrated large-scale complex networks, often denoted by spatial networks, and focuses on two particular space-air infrastructures, namely, satellites networks and high-altitude platforms (HAPs). The paper then classifies the connecting P2P communications links as satellite-to-satellite links at the same layer (SSLL), satellite-to-satellite links at different layers (SSLD), and HAP-to-HAP links (HHL). The paper overviews each layer of such spatial networks separately, and highlights the possible natures of the connecting links (i.e., radio-frequency or free-space optics) with a dedicated overview to the existing link-budget results. The paper, afterwards, presents the prospective merit of realizing such an integrated satellite-HAP network towards providing broadband services in under-served and remote areas. Finally, the paper sheds light on several future research directions in the context of spatial networks, namely large-scale network optimization, intelligent offloading, smart platforms, energy efficiency, multiple access schemes, and distributed spatial networks.      
### 18.Iterative Geometry Calibration from Distance Estimates for Wireless Acoustic Sensor Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.06142.pdf)
>  In this paper we present an approach to geometry calibration in wireless acoustic sensor networks, whose nodes are assumed to be equipped with a compact microphone array. The proposed approach solely works with estimates of the distances between acoustic sources and the nodes that record these sources. It consists of an iterative weighted least squares localization procedure, which is initialized by multidimensional scaling. Alongside the sensor node locations, also the positions of the acoustic sources are estimated. Furthermore, we derive the Cramer-Rao lower bound (CRLB) for source and sensor position estimation, and show by simulation that the estimator is efficient.      
### 19.An algorithm for onset detection of linguistic segments in continuous electroencephalogram signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.06075.pdf)
>  A Brain Computer Interface based on imagined words can decode the word a subject is thinking on through brain signals to control an external device. In order to build a fully asynchronous Brain Computer Interface based on imagined words in electroencephalogram signals as source, we need to solve the problem of detecting the onset of the imagined words. Although there has been some research in this field, the problem has not been fully solved. In this paper we present an approach to solve this problem by using values from statistics, information theory and chaos theory as features to correctly identify the onset of imagined words in a continuous signal. On detecting the onsets of imagined words, the highest True Positive Rate achieved by our approach was obtained using features based on the generalized Hurst exponent, this True Positive Rate was 0.69 and 0.77 with a timing error tolerance region of 3 and 4 seconds respectively.      
### 20.Power System Dynamic State Estimation Using Extended and Unscented Kalman Filters  [ :arrow_down: ](https://arxiv.org/pdf/2012.06069.pdf)
>  Accurate estimation of power system dynamics is very important for the enhancement of power system reliability, resilience, security, and stability of power system. With the increasing integration of inverter-based distributed energy resources, the knowledge of power system dynamics has become more necessary and critical than ever before for proper control and operation of the power system. Although recent advancement of measurement devices and the transmission technologies have reduced the measurement and transmission error significantly, these measurements are still not completely free from the measurement noises. Therefore, the noisy measurements need to be filtered to obtain the accurate power system operating dynamics. In this work, the power system dynamic states are estimated using extended Kalman filter (EKF) and unscented Kalman filter (UKF). We have performed case studies on Western Electricity Coordinating Council (WECC)'s $3$-machine $9$-bus system and New England $10$-machine $39$-bus. The results show that the UKF and EKF can accurately estimate the power system dynamics. The comparative performance of EKF and UKF for the tested case is also provided. Other Kalman filtering techniques alongwith the machine learning-based estimator will be updated inthis report soon.All the sources code including Newton Raphsonpower flow, admittance matrix calculation, EKF calculation, andUKF calculation are publicly available in Github      
### 21.On-Request Wireless Charging and Partial Computation Offloading In Multi-Access Edge Computing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.06017.pdf)
>  Wireless charging coupled with computation offloading in edge networks offers a promising solution for realizing power-hungry and computation intensive applications on user devices. We consider a multi-access edge computing (MEC) system with collocated MEC server and base-station/access point, each equipped with a massive MIMO antenna array, supporting multiple users requesting data computation and wireless charging. The goal is to minimize the energy consumption for computation offloading and maximize the received energy at the user from wireless charging. The proposed solution is a novel two-stage algorithm employing nested primal-dual and linear programming techniques to perform data partitioning and time allocation for computation offloading and design the optimal energy beamforming for wireless charging, all within MEC-AP transmit power and latency constraints. Algorithm results show that optimal energy beamforming significantly outperforms other schemes such as isotropic or directed charging without beam power allocation. Compared to binary offloading, data partition in partial offloading leads to lower energy consumption and more charging time, and hence offers better wireless charging performance. The charged energy over an extended period of time both with and without computation offloading can be substantial. Opportunistic wireless charging from MEC-AP thus offers a viable untethered approach for supplying energy to user-devices.      
### 22.Nonlinear Regression with a Convolutional Encoder-Decoder for Remote Monitoring of Surface Electrocardiograms  [ :arrow_down: ](https://arxiv.org/pdf/2012.06003.pdf)
>  We propose the Nonlinear Regression Convolutional Encoder-Decoder (NRCED), a novel framework for mapping a multivariate input to a multivariate output. In particular, we implement our algorithm within the scope of 12-lead surface electrocardiogram (ECG) reconstruction from intracardiac electrograms (EGM) and vice versa. The goal of performing this task is to allow for improved point-of-care monitoring of patients with an implanted device to treat cardiac pathologies. We will achieve this goal with 12-lead ECG reconstruction and by providing a new diagnostic tool for classifying atypical heartbeats. The algorithm is evaluated on a dataset retroactively collected from 14 patients. Correlation coefficients calculated between the reconstructed and the actual ECG show that the proposed NRCED method represents an efficient, accurate, and superior way to synthesize a 12-lead ECG. We can also achieve the same reconstruction accuracy with only one EGM lead as input. We also tested the model in a non-patient specific way and saw a reasonable correlation coefficient. The model was also executed in the reverse direction to produce EGM signals from a 12-lead ECG and found that the correlation was comparable to the forward direction. Lastly, we analyzed the features learned in the model and determined that the model learns an overcomplete basis of our 12-lead ECG space. We then use this basis of features to create a new diagnostic tool for identifying atypical and diseased heartbeats. This resulted in a ROC curve with an associated area under the curve value of 0.98, demonstrating excellent discrimination between the two classes.      
### 23.A tuning algorithm for a sliding mode controller of buildings with ATMD  [ :arrow_down: ](https://arxiv.org/pdf/2012.05966.pdf)
>  This paper proposes an automatic tuning algorithm for a sliding mode controller (SMC) based on the Ackermann's formula, that attenuates the structural vibrations of a seismically excited building equipped with an Active Tuned Mass Damper (ATMD) mounted on its top floor. The switching gain and sliding surface of the SMC are designed through the proposed tuning algorithm to suppress the structural vibrations by minimizing either the top floor displacement or the control force applied to the ATMD. Moreover, the tuning algorithm selects the SMC parameters to guarantee the following closed-loop characteristics: 1) the transient responses of the structure and the ATMD are sufficiently fast and damped; and 2) the control force, as well as the displacements and velocities of the building and ATMD are within acceptable limits under the frequency band of the earthquake excitation. The proposed SMC shows robustness against the unmodeled dynamics such as the friction of the damper. Experimental results on a reduced scale structure permits demonstrating the efficiency of the tuning algorithm for the SMC, which is compared with the traditional Linear Quadratic Regulator (LQR).      
### 24.3D Scattering Tomography by Deep Learning with Architecture Tailored to Cloud Fields  [ :arrow_down: ](https://arxiv.org/pdf/2012.05960.pdf)
>  We present 3DeepCT, a deep neural network for computed tomography, which performs 3D reconstruction of scattering volumes from multi-view images. Our architecture is dictated by the stationary nature of atmospheric cloud fields. The task of volumetric scattering tomography aims at recovering a volume from its 2D projections. This problem has been studied extensively, leading, to diverse inverse methods based on signal processing and physics models. However, such techniques are typically iterative, exhibiting high computational load and long convergence time. We show that 3DeepCT outperforms physics-based inverse scattering methods in term of accuracy as well as offering a significant orders of magnitude improvement in computational time. To further improve the recovery accuracy, we introduce a hybrid model that combines 3DeepCT and physics-based method. The resultant hybrid technique enjoys fast inference time and improved recovery performance.      
### 25.Online Joint Topology Identification and Signal Estimation with Inexact Proximal Online Gradient Descent  [ :arrow_down: ](https://arxiv.org/pdf/2012.05957.pdf)
>  Identifying the topology that underlies a set of time series is useful for tasks such as prediction, denoising, and data completion. Vector autoregressive (VAR) model based topologies capture dependencies among time series, and are often inferred from observed spatio-temporal data. When the data are affected by noise and/or missing samples, the tasks of topology identification and signal recovery (reconstruction) have to be performed jointly. Additional challenges arise when i) the underlying topology is time-varying, ii) data become available sequentially, and iii) no delay is tolerated. To overcome these challenges, this paper proposes two online algorithms to estimate the VAR model-based topologies. The proposed algorithms have constant complexity per iteration, which makes them interesting for big data scenarios. They also enjoy complementary merits in terms of complexity and performance. A performance guarantee is derived for one of the algorithms in the form of a dynamic regret bound. Numerical tests are also presented, showcasing the ability of the proposed algorithms to track the time-varying topologies with missing data in an online fashion.      
### 26.Ensemble of Discriminators for Domain Adaptation in Multiple Sound Source 2D Localization  [ :arrow_down: ](https://arxiv.org/pdf/2012.05908.pdf)
>  This paper introduces an ensemble of discriminators that improves the accuracy of a domain adaptation technique for the localization of multiple sound sources. Recently, deep neural networks have led to promising results for this task, yet they require a large amount of labeled data for training. Recording and labeling such datasets is very costly, especially because data needs to be diverse enough to cover different acoustic conditions. In this paper, we leverage acoustic simulators to inexpensively generate labeled training samples. However, models trained on synthetic data tend to perform poorly with real-world recordings due to the domain mismatch. For this, we explore two domain adaptation methods using adversarial learning for sound source localization which use labeled synthetic data and unlabeled real data. We propose a novel ensemble approach that combines discriminators applied at different feature levels of the localization model. Experiments show that our ensemble discrimination method significantly improves the localization performance without requiring any label from the real data.      
### 27.Synergistic Integration of Optical and Microwave Satellite Data for Crop Yield Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2012.05905.pdf)
>  Developing accurate models of crop stress, phenology and productivity is of paramount importance, given the increasing need of food. Earth observation remote sensing data provides a unique source of information to monitor crops in a temporally resolved and spatially explicit way. In this study, we propose the combination of multisensor (optical and microwave) remote sensing data for crop yield estimation and forecasting using two novel approaches. We first propose the lag between Enhanced Vegetation Index derived from MODIS and Vegetation Optical Depth derived from SMAP as a new joint metric combining the information from the two satellite sensors in a unique feature or descriptor. Our second approach avoids summarizing statistics and uses machine learning to combine full time series of EVI and VOD. This study considers two statistical methods, a regularized linear regression and its nonlinear extension called kernel ridge regression to directly estimate the county-level surveyed total production, as well as individual yields of the major crops grown in the region: corn, soybean and wheat. The study area includes the US Corn Belt, and we use agricultural survey data from the National Agricultural Statistics Service (USDA-NASS) for year 2015 for quantitative assessment.      
### 28.Effects of Unsteady Heat Transfer on Behaviour of Commercial Hydro-Pneumatic Accumulators  [ :arrow_down: ](https://arxiv.org/pdf/2012.06526.pdf)
>  Hydraulic accumulators play a central role as energy storage in nearly all fluid power systems. The accumulators serve as pulsation dampers or energy storage devices in hydro-pneumatic suspensions. The energy carrying gas is compressed and decompressed, often periodically. Heat transfer to the outside significantly determines the transfer behaviour of the accumulator since heat transfer changes the thermodynamic state of the enclosed gas. The accumulators operating mode ranges from isothermal to adiabatic. Simulating fluid power systems adequately requires knowledge of the transfer behaviour of the accumulators and therefore of the heat transfer. The Engineer's approach to model heat transfer in technical system is Newton's law. However, research shows, that in harmonically oscillating gas volumes, heat flux and bulk temperature difference change their phase. Newton's law is incapable of representing this physical phenomenon. We performed measurements on two sizes of commercial membrane accumulators. Experimental data confirm the failure of Newton's approach. Instead the heat transfer can be modelled with an additional rate dependent term and independently of the accumulator's size. Correlation equations for the heat transfer and the correct accumulator transfer behaviour are given.      
### 29.On Training Effective Reinforcement Learning Agents for Real-time Power Grid Operation and Control  [ :arrow_down: ](https://arxiv.org/pdf/2012.06458.pdf)
>  Deriving fast and effectively coordinated control actions remains a grand challenge affecting the secure and economic operation of today's large-scale power grid. This paper presents a novel artificial intelligence (AI) based methodology to achieve multi-objective real-time power grid control for real-world implementation. State-of-the-art off-policy reinforcement learning (RL) algorithm, soft actor-critic (SAC) is adopted to train AI agents with multi-thread offline training and periodic online training for regulating voltages and transmission losses without violating thermal constraints of lines. A software prototype was developed and deployed in the control center of SGCC Jiangsu Electric Power Company that interacts with their Energy Management System (EMS) every 5 minutes. Massive numerical studies using actual power grid snapshots in the real-time environment verify the effectiveness of the proposed approach. Well-trained SAC agents can learn to provide effective and subsecond control actions in regulating voltage profiles and reducing transmission losses.      
### 30.Learning How to Trade-Off Safety with Agility Using Deep Covariance Estimation for Perception Driven UAV Motion Planning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06410.pdf)
>  We investigate how to utilize predictive models for selecting appropriate motion planning strategies based on perception uncertainty estimation for agile unmanned aerial vehicle (UAV) navigation tasks. Although there are variety of motion planning and perception algorithms for such tasks, the impact of perception uncertainty is not explicitly handled in many of the current motion algorithms, which leads to performance loss in real-life scenarios where the measurement are often noisy due to external disturbances. We develop a novel framework for embedding perception uncertainty to high level motion planning management, in order to select the best available motion planning approach for the currently estimated perception uncertainty. We estimate the uncertainty in visual inputs using a deep neural network (CovNet) that explicitly predicts the covariance of the current measurements. Next, we train a high level machine learning model for predicting the lowest cost motion planning algorithm given the current estimate of covariance as well as the UAV states. We demonstrate on both real-life data and drone racing simulations that our approach, named uncertainty driven motion planning switcher (UDS) yields the safest and fastest trajectories among compared alternatives. Furthermore, we show that the developed approach learns how to trade-off safety with agility by switching to motion planners that leads to more agile trajectories when the estimated covariance is high and vice versa.      
### 31.Hierarchical coupled routing-charging model of electric vehicles, stations and grid operators  [ :arrow_down: ](https://arxiv.org/pdf/2012.06392.pdf)
>  Electric Vehicles' (EVs) growing number has various consequences, from reducing greenhouse gas emissions and local pollution to altering traffic congestion and electricity consumption. More specifically, decisions of operators from both the transportation and the electrical systems are coupled due to EVs' decisions. Thus, decision-making requires a model of several interdependent operators and of EVs' both driving and charging behaviors. Such a model is suggested for the electrical system in the context of commuting, which has a typical trilevel structure. At the lower level of the model, a congestion game between different types of vehicles gives which driving paths and charging stations (or hubs) commuters choose, depending on travel duration and consumption costs. At the middle level, a Charging Service Operator sets the charging prices at the hubs to maximize the difference between EV charging revenues and electricity supplying costs, which are decided by the Electrical Network Operator at the upper level of the model, whose goal is to reduce grid costs. This trilevel optimization problem is solved using an optimistic iterative bilevel algorithm and simulated annealing. The sensitivity of this trilevel model to exogenous parameters such as the EV penetration and an incentive from a transportation operator is illustrated on realistic urban networks.      
### 32.Nonlinear Distribution Regression for Remote Sensing Applications  [ :arrow_down: ](https://arxiv.org/pdf/2012.06377.pdf)
>  In many remote sensing applications one wants to estimate variables or parameters of interest from observations. When the target variable is available at a resolution that matches the remote sensing observations, standard algorithms such as neural networks, random forests or Gaussian processes are readily available to relate the two. However, we often encounter situations where the target variable is only available at the group level, i.e. collectively associated to a number of remotely sensed observations. This problem setting is known in statistics and machine learning as {\em multiple instance learning} or {\em distribution regression}. This paper introduces a nonlinear (kernel-based) method for distribution regression that solves the previous problems without making any assumption on the statistics of the grouped data. The presented formulation considers distribution embeddings in reproducing kernel Hilbert spaces, and performs standard least squares regression with the empirical means therein. A flexible version to deal with multisource data of different dimensionality and sample sizes is also presented and evaluated. It allows working with the native spatial resolution of each sensor, avoiding the need of match-up procedures. Noting the large computational cost of the approach, we introduce an efficient version via random Fourier features to cope with millions of points and groups.      
### 33.Steering the aggregative behavior of noncooperative agents: a nudge framework  [ :arrow_down: ](https://arxiv.org/pdf/2012.06376.pdf)
>  This paper considers the problem of steering the aggregative behavior of a population of noncooperative price-taking agents towards a desired behavior. Different from conventional pricing schemes where the price is fully available for design, we consider the scenario where a system regulator broadcasts a price prediction signal that can be different from the actual price incurred by the agents. The resulting reliability issues are taken into account by including trust dynamics in our model, implying that the agents will not blindly follow the signal sent by the regulator, but rather follow it based on the history of its accuracy, i.e, its deviation from the actual price. We present several nudge mechanisms to generate suitable price prediction signals that are able to steer the aggregative behavior of the agents to stationary as well as temporal desired aggregative behaviors. We provide analytical convergence guarantees for the resulting multi-components models. In particular, we prove that the proposed nudge mechanisms earn and maintain full trust of the agents, and the aggregative behavior converges to the desired one. The analytical results are complemented by a numerical case study of coordinated charging of plug-in electric vehicles.      
### 34.Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2012.06341.pdf)
>  System identification aims to build models of dynamical systems from data. Traditionally, choosing the model requires the designer to balance between two goals of conflicting nature; the model must be rich enough to capture the system dynamics, but not so flexible that it learns spurious random effects from the dataset. It is typically observed that model validation performance follows a U-shaped curve as the model complexity increases. Recent developments in machine learning and statistics, however, have observed situations where a "double-descent" curve subsumes this U-shaped model-performance curve. With a second decrease in performance occurring beyond the point where the model has reached the capacity of interpolating - i.e., (near) perfectly fitting - the training data. To the best of our knowledge, however, such phenomena have not been studied within the context of the identification of dynamic systems. The present paper aims to answer the question: "Can such a phenomenon also be observed when estimating parameters of dynamic systems?" We show the answer is yes, verifying such behavior experimentally both for artificially generated and real-world datasets.      
### 35.Analysis of Feature Representations for Anomalous Sound Detection  [ :arrow_down: ](https://arxiv.org/pdf/2012.06282.pdf)
>  In this work, we thoroughly evaluate the efficacy of pretrained neural networks as feature extractors for anomalous sound detection. In doing so, we leverage the knowledge that is contained in these neural networks to extract semantically rich features (representations) that serve as input to a Gaussian Mixture Model which is used as a density estimator to model normality. We compare feature extractors that were trained on data from various domains, namely: images, environmental sounds and music. Our approach is evaluated on recordings from factory machinery such as valves, pumps, sliders and fans. All of the evaluated representations outperform the autoencoder baseline with music based representations yielding the best performance in most cases. These results challenge the common assumption that closely matching the domain of the feature extractor and the downstream task results in better downstream task performance.      
### 36.Acoustic Leak Detection in Water Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.06280.pdf)
>  In this work, we present a general procedure for acoustic leak detection in water networks that satisfies multiple real-world constraints such as energy efficiency and ease of deployment. Based on recordings from seven contact microphones attached to the water supply network of a municipal suburb, we trained several shallow and deep anomaly detection models. Inspired by how human experts detect leaks using electronic sounding-sticks, we use these models to repeatedly listen for leaks over a predefined decision horizon. This way we avoid constant monitoring of the system. While we found the detection of leaks in close proximity to be a trivial task for almost all models, neural network based approaches achieve better results at the detection of distant leaks.      
### 37.Video Camera Identification from Sensor Pattern Noise with a Constrained ConvNet  [ :arrow_down: ](https://arxiv.org/pdf/2012.06277.pdf)
>  The identification of source cameras from videos, though it is a highly relevant forensic analysis topic, has been studied much less than its counterpart that uses images. In this work we propose a method to identify the source camera of a video based on camera specific noise patterns that we extract from video frames. For the extraction of noise pattern features, we propose an extended version of a constrained convolutional layer capable of processing color inputs. Our system is designed to classify individual video frames which are in turn combined by a majority vote to identify the source camera. We evaluated this approach on the benchmark VISION data set consisting of 1539 videos from 28 different cameras. To the best of our knowledge, this is the first work that addresses the challenge of video camera identification on a device level. The experiments show that our approach is very promising, achieving up to 93.1% accuracy while being robust to the WhatsApp and YouTube compression techniques. This work is part of the EU-funded project 4NSEEK focused on forensics against child sexual abuse.      
### 38.Dual Control for Exploitation and Exploration (DCEE) in Autonomous Search  [ :arrow_down: ](https://arxiv.org/pdf/2012.06276.pdf)
>  This paper proposes an optimal autonomous search framework, namely Dual Control for Exploration and Exploitation (DCEE), for a target at unknown location in an unknown environment. Source localisation is to find sources of atmospheric hazardous material release in a partially unknown environment. This paper proposes a control theoretic approach to this autonomous search problem. To cope with an unknown target location, at each step, the target location is estimated by Bayesian inference. Then a control action is taken to minimise the error between future robot position and the hypothesised future estimation of the target location. The latter is generated by hypothesised measurements at the corresponding future robot positions (due to the control action) with the current estimation of the target location as a prior. It shows that this approach can take into account both the error between the next robot position and the estimate of the target location, and the uncertainty of the estimate. This approach is further extended to the case with not only an unknown source location, but also an unknown local environment (e.g. wind speed and direction). Different from current information theoretic approaches, this new control theoretic approach achieves the optimal trade-off between exploitation and exploration in a unknown environment with an unknown target by driving the robot moving towards estimated target location while reducing its estimation uncertainty. This scheme is implemented using particle filtering on a mobile robot. Simulation and experimental studies demonstrate promising performance of the proposed approach. The relationships between the proposed approach, informative path planning, dual control, and classic model predictive control are discussed and compared.      
### 39.Motion Mappings for Continuous Bilateral Teleoperation  [ :arrow_down: ](https://arxiv.org/pdf/2012.06268.pdf)
>  Mapping operator motions to a robot is a key problem in teleoperation. Due to differences between workspaces, such as object locations, it is particularly challenging to derive smooth motion mappings that fulfill different goals (e.g. picking objects with different poses on the two sides or passing through key points). Indeed, most state-of-the-art methods rely on mode switches, leading to a discontinuous, low-transparency experience. In this paper, we propose a unified formulation for position, orientation and velocity mappings based on the poses of objects of interest in the operator and robot workspaces. We apply it in the context of bilateral teleoperation. Two possible implementations to achieve the proposed mappings are studied: an iterative approach based on locally-weighted translations and rotations, and a neural network approach. Evaluations are conducted both in simulation and using two torque-controlled Franka Emika Panda robots. Our results show that, despite longer training times, the neural network approach provides faster mapping evaluations and lower interaction forces for the operator, which are crucial for continuous, real-time teleoperation.      
### 40.Reconfigurable Intelligent Surface Based Hybrid Precoding for THz Communications  [ :arrow_down: ](https://arxiv.org/pdf/2012.06261.pdf)
>  Terahertz (THz) communication has been considered as a promising technology to provide ultra-high-speed rates for future 6G wireless systems. To alleviate the severe propagation attenuation in THz communication systems, massive multiple-input multiple-output (MIMO) with hybrid precoding can be used for beamforming to provide high array gains. In this paper, we propose a reconfigurable intelligent surface (RIS)-based hybrid precoding architecture for THz communication, where the energy-efficient RIS rather than the energy-hungry phased array is used to realize the analog beamforming of the hybrid precoding. Then, we investigate the hybrid precoding problem to maximize the sum-rate for the proposed RIS-based hybrid precoding architecture. Due to the non-convex constraint of discrete phase shifts by considering the practical hardware implementation of RIS, this sum-rate maximization problem is challenging to solve. Thus, we reformulate it as a parallel deep neural network (DNN)-based classification problem, which can be solved by the proposed deep learning-based multiple discrete classification (DL-MDC) hybrid precoding scheme with low complexity. Simulation results show that the proposed scheme works well both in theoretical Saleh-Valenzuela channel model and practical 3GPP channel model, and it can reduce the runtime significantly with a negligible performance loss compared with existing iterative search algorithms.      
### 41.Improved Robustness to Disfluencies in RNN-Transducer Based Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.06259.pdf)
>  Automatic Speech Recognition (ASR) based on Recurrent Neural Network Transducers (RNN-T) is gaining interest in the speech community. We investigate data selection and preparation choices aiming for improved robustness of RNN-T ASR to speech disfluencies with a focus on partial words. For evaluation we use clean data, data with disfluencies and a separate dataset with speech affected by stuttering. We show that after including a small amount of data with disfluencies in the training set the recognition accuracy on the tests with disfluencies and stuttering improves. Increasing the amount of training data with disfluencies gives additional gains without degradation on the clean data. We also show that replacing partial words with a dedicated token helps to get even better accuracy on utterances with disfluencies and stutter. The evaluation of our best model shows 22.5% and 16.4% relative WER reduction on those two evaluation sets.      
### 42.Soft Compression for Lossless Image Coding  [ :arrow_down: ](https://arxiv.org/pdf/2012.06240.pdf)
>  Soft compression is a lossless image compression method, which is committed to eliminating coding redundancy and spatial redundancy at the same time by adopting locations and shapes of codebook to encode an image from the perspective of information theory and statistical distribution. In this paper, we propose a new concept, compressible indicator function with regard to image, which gives a threshold about the average number of bits required to represent a location and can be used for revealing the performance of soft compression. We investigate and analyze soft compression for binary image, gray image and multi-component image by using specific algorithms and compressible indicator value. It is expected that the bandwidth and storage space needed when transmitting and storing the same kind of images can be greatly reduced by applying soft compression.      
### 43.Information flow and error scaling for fully-quantum control  [ :arrow_down: ](https://arxiv.org/pdf/2012.06234.pdf)
>  The optimally designed control of quantum systems is playing an increasingly important role to engineer novel and more efficient quantum technologies. Here, in the scenario represented by controlling an arbitrary quantum system via the interaction with an another optimally initialized auxiliary quantum system, we show that the quantum channel capacity sets the scaling behaviour of the optimal control error. Specifically, we prove that the minimum control error is ensured by maximizing the quantum capacity of the channel mapping the initial control state into the target state of the controlled system, i.e., optimizing the quantum information flow from the controller to the system to be controlled. Analytical results, supported by numerical evidences, are provided when the systems and the controller are either qubits or single Bosonic modes and can be applied to a very large class of platforms for controllable quantum devices.      
### 44.Exploring wav2vec 2.0 on speaker verification and language identification  [ :arrow_down: ](https://arxiv.org/pdf/2012.06185.pdf)
>  Wav2vec 2.0 is a recently proposed self-supervised framework for speech representation learning. It follows a two-stage training process of pre-training and fine-tuning, and performs well in speech recognition tasks especially ultra-low resource cases. In this work, we attempt to extend self-supervised framework to speaker verification and language identification. First, we use some preliminary experiments to indicate that wav2vec 2.0 can capture the information about the speaker and language. Then we demonstrate the effectiveness of wav2vec 2.0 on the two tasks respectively. For speaker verification, we obtain a new state-of-the-art result, Equal Error Rate (EER) of 3.61% on the VoxCeleb1 dataset. For language identification, we obtain an EER of 12.02% on 1 second condition and an EER of 3.47% on full-length condition of the AP17-OLR dataset. Finally, we utilize one model to achieve the unified modeling by the multi-task learning for the two tasks.      
### 45.Reduced-Order Nonlinear Observers via Contraction Analysis and Convex Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2012.06158.pdf)
>  In this paper, we propose a new approach to design globally convergent reduced-order observers for nonlinear control systems via contraction analysis and convex optimization. Despite the fact that contraction is a concept naturally suitable for state estimation, the existing solutions are either local or relatively conservative when applying to physical systems. To address this, we show that this problem can be translated into an off-line search for a coordinate transformation after which the dynamics is (transversely) contracting. The obtained sufficient condition consists of some easily verifiable differential inequalities, which, on one hand, identify a very general class of "detectable" nonlinear systems, and on the other hand, can be expressed as computationally efficient convex optimization, making the design procedure more systematic. Connections with some well-established approaches and concepts are also clarified in the paper. Finally, we illustrate the proposed method with several numerical and physical examples, including polynomial, mechanical, electromechanical and biochemical systems.      
### 46.Learning Omni-frequency Region-adaptive Representations for Real Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2012.06131.pdf)
>  Traditional single image super-resolution (SISR) methods that focus on solving single and uniform degradation (i.e., bicubic down-sampling), typically suffer from poor performance when applied into real-world low-resolution (LR) images due to the complicated realistic degradations. The key to solving this more challenging real image super-resolution (RealSR) problem lies in learning feature representations that are both informative and content-aware. In this paper, we propose an Omni-frequency Region-adaptive Network (ORNet) to address both challenges, here we call features of all low, middle and high frequencies omni-frequency features. Specifically, we start from the frequency perspective and design a Frequency Decomposition (FD) module to separate different frequency components to comprehensively compensate the information lost for real LR image. Then, considering the different regions of real LR image have different frequency information lost, we further design a Region-adaptive Frequency Aggregation (RFA) module by leveraging dynamic convolution and spatial attention to adaptively restore frequency components for different regions. The extensive experiments endorse the effective, and scenario-agnostic nature of our OR-Net for RealSR.      
### 47.A Review of Hidden Markov Models and Recurrent Neural Networks for Event Detection and Localization in Biomedical Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.06104.pdf)
>  Biomedical signals carry signature rhythms of complex physiological processes that control our daily bodily activity. The properties of these rhythms indicate the nature of interaction dynamics among physiological processes that maintain a homeostasis. Abnormalities associated with diseases or disorders usually appear as disruptions in the structure of the rhythms which makes isolating these rhythms and the ability to differentiate between them, indispensable. Computer aided diagnosis systems are ubiquitous nowadays in almost every medical facility and more closely in wearable technology, and rhythm or event detection is the first of many intelligent steps that they perform. How these rhythms are isolated? How to develop a model that can describe the transition between processes in time? Many methods exist in the literature that address these questions and perform the decoding of biomedical signals into separate rhythms. In here, we demystify the most effective methods that are used for detection and isolation of rhythms or events in time series and highlight the way in which they were applied to different biomedical signals and how they contribute to information fusion. The key strengths and limitations of these methods are also discussed as well as the challenges encountered with application in biomedical signals.      
### 48.Fairness-Oriented Multiple RISs-Aided MmWave Transmission: Stochastic Optimization Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2012.06103.pdf)
>  In millimeter wave (mmWave) systems, it is challenging to ensure the reliable connectivity of communications due to its sensitivity to the presence of blockages. In order to improve the robustness of the mmWave system under the presence of the random blockages, multiple reconfigurable intelligent surfaces (RISs) are deployed to enhance the spatial diversity gain, and robust beamforming is then designed based on a stochastic optimization for minimizing the maximum outage probability among multiple users to ensure the fairness. Under the stochastic optimization framework, we adopt the stochastic majorization--minimization (SMM) method and the stochastic successive convex approximation (SSCA) method to construct deterministic surrogate problems at each iteration for new channel realizations, and obtain the closed-form solutions of the precoding matrix at the base station (BS) and the passive beamforming vectors at the RISs. Both stochastic optimization methods have been proved to converge to the set of stationary points of the original stochastic problems. Finally, simulation results show that the proposed robust beamforming in the RIS-aided system can effectively compensate for the performance loss caused by the presence of the random blockages, especially at high blockage probability, compared with the benchmark solutions.      
### 49.Single-pixel Tracking and Imaging under Weak Illumination  [ :arrow_down: ](https://arxiv.org/pdf/2012.06091.pdf)
>  Under weak illumination, tracking and imaging moving object turns out to be hard. By spatially collecting the signal, single pixel imaging schemes promise the capability of image reconstruction from low photon flux. However, due to the requirement on large number of samplings, how to clearly image moving objects is an essential problem for such schemes. Here we present a principle of single pixel tracking and imaging method. Velocity vector of the object is obtained from temporal correlation of the bucket signals in a typical computational ghost imaging system. Then the illumination beam is steered accordingly. Taking the velocity into account, both trajectory and clear image of the object are achieved during its evolution. Since tracking is achieved with bucket signals independently, this scheme is valid for capturing moving object as fast as its displacement within the interval of every sampling keeps larger than the resolution of the optical system. Experimentally, our method works well with the average number of detected photons down to 1.88 photons/speckle.      
### 50.Mesoscopic photogrammetry with an unstabilized phone camera  [ :arrow_down: ](https://arxiv.org/pdf/2012.06044.pdf)
>  We present a feature-free photogrammetric technique that enables quantitative 3D mesoscopic (mm-scale height variation) imaging with tens-of-micron accuracy from sequences of images acquired by a smartphone at close range (several cm) under freehand motion without additional hardware. Our end-to-end, pixel-intensity-based approach jointly registers and stitches all the images by estimating a coaligned height map, which acts as a pixel-wise radial deformation field that orthorectifies each camera image to allow homographic registration. The height maps themselves are reparameterized as the output of an untrained encoder-decoder convolutional neural network (CNN) with the raw camera images as the input, which effectively removes many reconstruction artifacts. Our method also jointly estimates both the camera's dynamic 6D pose and its distortion using a nonparametric model, the latter of which is especially important in mesoscopic applications when using cameras not designed for imaging at short working distances, such as smartphone cameras. We also propose strategies for reducing computation time and memory, applicable to other multi-frame registration problems. Finally, we demonstrate our method using sequences of multi-megapixel images captured by an unstabilized smartphone on a variety of samples (e.g., painting brushstrokes, circuit board, seeds).      
### 51.Data-Driven System Identification of Linear Quantum Systems Coupled to Time-Varying Coherent Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2012.06040.pdf)
>  In this paper, we develop a system identification algorithm to identify a model for unknown linear quantum systems driven by time-varying coherent states, based on empirical single-shot continuous homodyne measurement data of the system's output. The proposed algorithm identifies a model that satisfies the physical realizability conditions for linear quantum systems, challenging constraints not encountered in classical (non-quantum) linear system identification. Numerical examples on a multiple-input multiple-output optical cavity model are presented to illustrate an application of the identification algorithm.      
### 52.SensiX: A Platform for Collaborative Machine Learning on the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2012.06035.pdf)
>  The emergence of multiple sensory devices on or near a human body is uncovering new dynamics of extreme edge computing. In this, a powerful and resource-rich edge device such as a smartphone or a Wi-Fi gateway is transformed into a personal edge, collaborating with multiple devices to offer remarkable sensory al eapplications, while harnessing the power of locality, availability, and proximity. Naturally, this transformation pushes us to rethink how to construct accurate, robust, and efficient sensory systems at personal edge. For instance, how do we build a reliable activity tracker with multiple on-body IMU-equipped devices? While the accuracy of sensing models is improving, their runtime performance still suffers, especially under this emerging multi-device, personal edge environments. Two prime caveats that impact their performance are device and data variabilities, contributed by several runtime factors, including device availability, data quality, and device placement. To this end, we present SensiX, a personal edge platform that stays between sensor data and sensing models, and ensures best-effort inference under any condition while coping with device and data variabilities without demanding model engineering. SensiX externalises model execution away from applications, and comprises of two essential functions, a translation operator for principled mapping of device-to-device data and a quality-aware selection operator to systematically choose the right execution path as a function of model accuracy. We report the design and implementation of SensiX and demonstrate its efficacy in developing motion and audio-based multi-device sensing systems. Our evaluation shows that SensiX offers a 7-13% increase in overall accuracy and up to 30% increase across different environment dynamics at the expense of 3mW power overhead.      
### 53.Performance-Weighed Policy Sampling for Meta-Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.06016.pdf)
>  This paper discusses an Enhanced Model-Agnostic Meta-Learning (E-MAML) algorithm that generates fast convergence of the policy function from a small number of training examples when applied to new learning tasks. Built on top of Model-Agnostic Meta-Learning (MAML), E-MAML maintains a set of policy parameters learned in the environment for previous tasks. We apply E-MAML to developing reinforcement learning (RL)-based online fault tolerant control schemes for dynamic systems. The enhancement is applied when a new fault occurs, to re-initialize the parameters of a new RL policy that achieves faster adaption with a small number of samples of system behavior with the new fault. This replaces the random task sampling step in MAML. Instead, it exploits the extant previously generated experiences of the controller. The enhancement is sampled to maximally span the parameter space to facilitate adaption to the new fault. We demonstrate the performance of our approach combining E-MAML with proximal policy optimization (PPO) on the well-known cart pole example, and then on the fuel transfer system of an aircraft.      
### 54.A Recursive Method for Real-Time Waveform Fitting with Background Noise Rejection  [ :arrow_down: ](https://arxiv.org/pdf/2012.05937.pdf)
>  We present here a technique for developing a high-throughput algorithm to fit a combination of template pulse shapes while simultaneously subtracting parameterized background noise. By convolving the psuedoinverse of the least-squares fit design matrix along a regularly sampled waveform trace, the time evolution of the fit parameters for each basis function can be determined in real-time. We approximate these sliding linear fit response functions using piecewise polynomials, and develop an FPGA-friendly algorithm to be implemented in high sample-rate data acquisition systems. This is a robust universal filter that compares well to common filters optimized for energy calibration/resolution, as well as filters optimized for timing performance, even when significant noise components are present.      
### 55.Data-driven Method for Estimating Aircraft Mass from Quick Access Recorder using Aircraft Dynamics and Multilayer Perceptron Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2012.05907.pdf)
>  Accurate aircraft-mass estimation is critical to airlines from the safety-management and performance-optimization viewpoints. Overloading an aircraft with passengers and baggage might result in a safety hazard. In contrast, not fully utilizing an aircraft's payload-carrying capacity undermines its operational efficiency and airline profitability. However, accurate determination of the aircraft mass for each operating flight is not feasible because it is impractical to weigh each aircraft component, including the payload. The existing methods for aircraft-mass estimation are dependent on the aircraft- and engine-performance parameters, which are usually considered proprietary information. Moreover, the values of these parameters vary under different operating conditions while those of others might be subject to large estimation errors. This paper presents a data-driven method involving use of the quick access recorder (QAR)-a digital flight-data recorder-installed on all aircrafts to record the initial aircraft climb mass during each flight. The method requires users to select appropriate parameters among several thousand others recorded by the QAR using physical models. The selected data are subsequently processed and provided as input to a multilayer perceptron neural network for building the model for initial-climb aircraft-mass prediction. Thus, the proposed method offers the advantages of both the model-based and data-driven approaches for aircraft-mass estimation. Because this method does not explicitly rely on any aircraft or engine parameter, it is universally applicable to all aircraft types. In this study, the proposed method was applied to a set of Boeing 777-300ER aircrafts, the results of which demonstrated reasonable accuracy. Airlines can use this tool to better utilize aircraft's payload.      
