# ArXiv eess --Wed, 9 Dec 2020
### 1.Successive Nonnegative Projection Algorithm for Linear Quadratic Mixtures  [ :arrow_down: ](https://arxiv.org/pdf/2012.04612.pdf)
>  In this work, we tackle the problem of hyperspectral (HS) unmixing by departing from the usual linear model and focusing on a Linear-Quadratic (LQ) one. The proposed algorithm, referred to as Successive Nonnegative Projection Algorithm for Linear Quadratic mixtures (SNPALQ), extends the Successive Nonnegative Projection Algorithm (SNPA), designed to address the unmixing problem under a linear model. By explicitly modeling the product terms inherent to the LQ model along the iterations of the SNPA scheme, the nonlinear contributions in the mixing are mitigated, thus improving the separation quality. The approach is shown to be relevant in a realistic numerical experiment.      
### 2.Let's Vibrate with Vibration: Augmenting Structural Engineering with Low-Cost Vibration Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2012.04605.pdf)
>  Using low-cost piezoelectric sensors to sense structural vibration exhibits great potential in augmenting structural engineering, which is yet to be explored in the literature to the best of our knowledge. Examples of such unexplored augmentation include classifying diverse structures (such as building, flyover, foot over-bridge, etc.), and relating the extent of vibration generated at different height of a structure and the associated height. Accordingly, to explore these cases, we develop a low-cost piezoelectric sensor-based vibration sensing system aiming to remotely collect real vibration data from diversified civil structures. We dig into our collected sensed data to classify five different types of structures through rigorous statistical and machine-learning-based analyses. Our analyses achieve a classification accuracy of up to 97% with an F1 score of 0.97. Nonetheless, in the rarely explored time domain, our analyses reveal a novel modality of relation between vibration generated at different heights of a structure and the associated height, which was explored in the frequency domain earlier in the literature with expensive sensors.      
### 3.Gradient-based Automatic Look-Up Table Generator for Atmospheric Radiative Transfer Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.04598.pdf)
>  Atmospheric correction of Earth Observation data is one of the most critical steps in the data processing chain of a satellite mission for successful remote sensing applications. Atmospheric Radiative Transfer Models (RTM) inversion methods are typically preferred due to their high accuracy. However, the execution of RTMs on a pixel-per-pixel basis is impractical due to their high computation time, thus large multi-dimensional look-up tables (LUTs) are precomputed for their later interpolation. To further reduce the RTM computation burden and the error in LUT interpolation, we have developed a method to automatically select the minimum and optimal set of nodes to be included in a LUT. We present the gradient-based automatic LUT generator algorithm (GALGA) which relies on the notion of an acquisition function that incorporates (a) the Jacobian evaluation of an RTM, and (b) information about the multivariate distribution of the current nodes. We illustrate the capabilities of GALGA in the automatic construction and optimization of MODerate resolution atmospheric TRANsmission (MODTRAN) LUTs for several input dimensions. Our results indicate that, when compared to a pseudo-random homogeneous distribution of the LUT nodes, GALGA reduces (1) the LUT size by $\sim$75\% and (2) the maximum interpolation relative errors by 0.5\% It is concluded that automatic LUT design might benefit from the methodology proposed in GALGA to reduce computation time and interpolation errors.      
### 4.Mapping Leaf Area Index with a Smartphone and Gaussian Processes  [ :arrow_down: ](https://arxiv.org/pdf/2012.04596.pdf)
>  Leaf area index (LAI) is a key biophysical parameter used to determine foliage cover and crop growth in environmental studies. Smartphones are nowadays ubiquitous sensor devices with high computational power, moderate cost, and high-quality sensors. A smartphone app, called PocketLAI, was recently presented and tested for acquiring ground LAI estimates. In this letter, we explore the use of state-of-the-art nonlinear Gaussian process regression (GPR) to derive spatially explicit LAI estimates over rice using ground data from PocketLAI and Landsat 8 imagery. GPR has gained popularity in recent years because of their solid Bayesian foundations that offers not only high accuracy but also confidence intervals for the retrievals. We show the first LAI maps obtained with ground data from a smartphone combined with advanced machine learning. This work compares LAI predictions and confidence intervals of the retrievals obtained with PocketLAI to those obtained with classical instruments, such as digital hemispheric photography (DHP) and LI-COR LAI-2000. This letter shows that all three instruments got comparable result but the PocketLAI is far cheaper. The proposed methodology hence opens a wide range of possible applications at moderate cost.      
### 5.Fuzzy model identification based on mixture distribution analysis for bearings remaining useful life estimation using small training data set  [ :arrow_down: ](https://arxiv.org/pdf/2012.04589.pdf)
>  The research work presented in this paper proposes a data-driven modeling method for bearings remaining useful life estimation based on Takagi-Sugeno (T-S) fuzzy inference system (FIS). This method allows identifying the parameters of a classic T-S FIS, starting with a small quantity of data. In this work, we used the vibration signals data from a small number of bearings over an entire period of run-to-failure. The FIS model inputs are features extracted from the vibration signals data observed periodically on the training bearings. The number of rules and the input parameters of each rule of the FIS model are identified using the subtractive clustering method. Furthermore, we propose to use the maximum likelihood method of mixture distribution analysis to calculate the parameters of clusters on the time axis and the probability corresponding to rules on degradation stages. Based on this result, we identified the output parameters of each rule using a weighted least square estimation. We then benchmarked the proposed method with some existing methods from the literature, through numerical experiments conducted on available datasets to highlight its effectiveness.      
### 6.Hierarchical Residual Attention Network for Single Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2012.04578.pdf)
>  Convolutional neural networks are the most successful models in single image super-resolution. Deeper networks, residual connections, and attention mechanisms have further improved their performance. However, these strategies often improve the reconstruction performance at the expense of considerably increasing the computational cost. This paper introduces a new lightweight super-resolution model based on an efficient method for residual feature and attention aggregation. In order to make an efficient use of the residual features, these are hierarchically aggregated into feature banks for posterior usage at the network output. In parallel, a lightweight hierarchical attention mechanism extracts the most relevant features from the network into attention banks for improving the final output and preventing the information loss through the successive operations inside the network. Therefore, the processing is split into two independent paths of computation that can be simultaneously carried out, resulting in a highly efficient and effective model for reconstructing fine details on high-resolution images from their low-resolution counterparts. Our proposed architecture surpasses state-of-the-art performance in several datasets, while maintaining relatively low computation and memory footprint.      
### 7.Efficient model selection in switching linear dynamic systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.04543.pdf)
>  The computational resources required for the optimal switching Kalman Filter (SKF) increase exponentially with the number of modes, an impediment that limits its utility especially for estimating a high-dimensional state variable in real-time. In this paper, a systematic graph representation for a switching linear dynamic system (SLDS) is proposed along with hierarchical clustering to reduce the cardinality of the set of switching modes offline before collecting the measurements. It is shown that the induced error caused by model complexity reduction can be quantified using the proposed graph framework. The simulation results verify that clustering based on the proposed framework can effectively reduce the model complexity and approximately quantify the effect of model complexity reduction on the estimation error.      
### 8.Quantification of mismatch error in randomly switching linear state-space models  [ :arrow_down: ](https://arxiv.org/pdf/2012.04542.pdf)
>  Switching Kalman Filters (SKF) are well known for their ability to solve the piecewise linear dynamic system estimation problem using the standard Kalman Filter (KF). Practical SKFs are heuristic, approximate filters that are not guaranteed to have optimal performance and require more computational resources than a single mode KF. On the other hand, applying a single mode mismatched KF to a switching linear dynamic system (SLDS) results in erroneous estimation. This paper aims to quantify the average error an SKF can eliminate compared to a mismatched, single mode KF in a known SLDS before collecting measurements. Mathematical derivations for the first and second moments of the estimators errors are provided and compared. One can use these derivations to quantify the average performance of filters beforehand and decide which filter to run in operation to have the best performance in terms of estimation error and computation complexity. We further provide simulation results that verify our mathematical derivations.      
### 9.Frequency Sub-Sampling of Ultrasound Non-Destructive Measurements: Acquisition, Reconstruction and Performance  [ :arrow_down: ](https://arxiv.org/pdf/2012.04534.pdf)
>  In ultrasound nondestructive testing, a widespread approach is to take synthetic aperture measurements from the surface of a specimen to detect and locate defects within it. Based on these measurements, imaging is usually performed using the Synthetic Aperture Focusing Technique (SAFT). However, SAFT is sub-optimal in terms of resolution and requires oversampling in time domain to obtain a fine grid for the Delay-and-Sum (DAS). On the other hand, parametric reconstruction algorithms give better resolution, but their usage for imaging becomes computationally expensive due to the size of the parameter space and the large amount of measurement data in realistic 3-D scenarios. In the literature, the remedies to this are twofold: First, the amount of measurement data can be reduced using state of the art sub-Nyquist sampling approaches to measure Fourier coefficients instead of time domain samples. Second, parametric reconstruction algorithms mostly rely on matrix-vector operations that can be implemented efficiently by exploiting the underlying model structure. In this paper, we propose and compare different strategies to choose the Fourier coefficients to be measured. Their asymptotic performance is compared by numerically evaluating the CramÃ©r-Rao-Bound for the localizability of the defect coordinates. These subsampling strategies are then combined with an $\ell_1$-minimization scheme to compute 3-D reconstructions from the low-rate measurements. Compared to conventional DAS, this allows us to formulate a fully physically motivated forward model. To enable this, the projection operations of the forward model matrix are implemented matrix-free by exploiting the underlying 2-level Toeplitz structure. Finally, we show that high resolution reconstructions from as low as a single Fourier coefficient per scan are possible based on simulated data as well as on measurements.      
### 10.Intersymbol and Intercarrier Interference in OFDM Systems: Unified Formulation and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2012.04527.pdf)
>  A unified matrix formulation is presented for the analysis of intersymbol and intercarrier interference in orthogonal frequency-division multiplexing (OFDM) systems. The proposed formulation relies on six parameters and allows studying various schemes, including those with windowing in the transmitter and/or in the receiver (called windowed OFDM systems), which may add cyclic suffix and/or cyclic prefix (CP), besides the conventional CP-OFDM. The proposed framework encompasses seven different OFDM systems. It considers the overlap-and-add procedure performed in the transmitter of windowed OFDM systems, being jointly formulated with the channel convolution. The intersymbol and intercarrier interference, caused when the order of the channel impulse response is higher than the number of CP samples, is characterized. A new equivalent channel matrix that is useful for calculating both the received signal and the interference power is defined and characterized. Unlike previous works, this new channel matrix has no restrictions on the length of the channel impulse response, which means that the study is not constrained to the particular case of two or three data blocks interfering in the received signal. Theoretical expressions for the powers of three different kinds of interference are derived. These expressions allow calculating the signal-to-interference-plus-noise ratio, useful for computing the data rate of each OFDM system. The proposed formulation is applied to realistic examples, showing its effectiveness through comparisons based on numerical performance assessments of the considered OFDM systems.      
### 11.Bayesian Learning of LF-MMI Trained Time Delay Neural Networks for Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2012.04494.pdf)
>  Discriminative training techniques define state-of-the-art performance for automatic speech recognition systems. However, they are inherently prone to overfitting, leading to poor generalization performance when using limited training data. In order to address this issue, this paper presents a full Bayesian framework to account for model uncertainty in sequence discriminative training of factored TDNN acoustic models. Several Bayesian learning based TDNN variant systems are proposed to model the uncertainty over weight parameters and choices of hidden activation functions, or the hidden layer outputs. Efficient variational inference approaches using a few as one single parameter sample ensure their computational cost in both training and evaluation time comparable to that of the baseline TDNN systems. Statistically significant word error rate (WER) reductions of 0.4%-1.8% absolute (5%-11% relative) were obtained over a state-of-the-art 900 hour speed perturbed Switchboard corpus trained baseline LF-MMI factored TDNN system using multiple regularization methods including F-smoothing, L2 norm penalty, natural gradient, model averaging and dropout, in addition to i-Vector plus learning hidden unit contribution (LHUC) based speaker adaptation and RNNLM rescoring. Consistent performance improvements were also obtained on a 450 hour HKUST conversational Mandarin telephone speech recognition task. On a third cross domain adaptation task requiring rapidly porting a 1000 hour LibriSpeech data trained system to a small DementiaBank elderly speech corpus, the proposed Bayesian TDNN LF-MMI systems outperformed the baseline system using direct weight fine-tuning by up to 2.5\% absolute WER reduction.      
### 12.Transfer Learning for Human Activity Recognition using Representational Analysis of Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2012.04479.pdf)
>  Human activity recognition (HAR) research has increased in recent years due to its applications in mobile health monitoring, activity recognition, and patient rehabilitation. The typical approach is training a HAR classifier offline with known users and then using the same classifier for new users. However, the accuracy for new users can be low with this approach if their activity patterns are different than those in the training data. At the same time, training from scratch for new users is not feasible for mobile applications due to the high computational cost and training time. To address this issue, we propose a HAR transfer learning framework with two components. First, a representational analysis reveals common features that can transfer across users and user-specific features that need to be customized. Using this insight, we transfer the reusable portion of the offline classifier to new users and fine-tune only the rest. Our experiments with five datasets show up to 43% accuracy improvement and 66% training time reduction when compared to the baseline without using transfer learning. Furthermore, measurements on the Nvidia Jetson Xavier-NX hardware platform reveal that the power and energy consumption decrease by 43% and 68%, respectively, while achieving the same or higher accuracy as training from scratch.      
### 13.Privacy-Preserving Synthetic Smart Meters Data  [ :arrow_down: ](https://arxiv.org/pdf/2012.04475.pdf)
>  Power consumption data is very useful as it allows to optimize power grids, detect anomalies and prevent failures, on top of being useful for diverse research purposes. However, the use of power consumption data raises significant privacy concerns, as this data usually belongs to clients of a power company. As a solution, we propose a method to generate synthetic power consumption samples that faithfully imitate the originals, but are detached from the clients and their identities. Our method is based on Generative Adversarial Networks (GANs). Our contribution is twofold. First, we focus on the quality of the generated data, which is not a trivial task as no standard evaluation methods are available. Then, we study the privacy guarantees provided to members of the training set of our neural network. As a minimum requirement for privacy, we demand our neural network to be robust to membership inference attacks, as these provide a gateway for further attacks in addition to presenting a privacy threat on their own. We find that there is a compromise to be made between the privacy and the performance provided by the algorithm.      
### 14.Multi-temporal and multi-source remote sensing image classification by nonlinear relative normalization  [ :arrow_down: ](https://arxiv.org/pdf/2012.04469.pdf)
>  Remote sensing image classification exploiting multiple sensors is a very challenging problem: data from different modalities are affected by spectral distortions and mis-alignments of all kinds, and this hampers re-using models built for one image to be used successfully in other scenes. In order to adapt and transfer models across image acquisitions, one must be able to cope with datasets that are not co-registered, acquired under different illumination and atmospheric conditions, by different sensors, and with scarce ground references. Traditionally, methods based on histogram matching have been used. However, they fail when densities have very different shapes or when there is no corresponding band to be matched between the images. An alternative builds upon \emph{manifold alignment}. Manifold alignment performs a multidimensional relative normalization of the data prior to product generation that can cope with data of different dimensionality (e.g. different number of bands) and possibly unpaired examples. Aligning data distributions is an appealing strategy, since it allows to provide data spaces that are more similar to each other, regardless of the subsequent use of the transformed data. In this paper, we study a methodology that aligns data from different domains in a nonlinear way through {\em kernelization}. We introduce the Kernel Manifold Alignment (KEMA) method, which provides a flexible and discriminative projection map, exploits only a few labeled samples (or semantic ties) in each domain, and reduces to solving a generalized eigenvalue problem. We successfully test KEMA in multi-temporal and multi-source very high resolution classification tasks, as well as on the task of making a model invariant to shadowing for hyperspectral imaging.      
### 15.Active Learning Methods for Efficient Hybrid Biophysical Variable Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2012.04468.pdf)
>  Kernel-based machine learning regression algorithms (MLRAs) are potentially powerful methods for being implemented into operational biophysical variable retrieval schemes. However, they face difficulties in coping with large training datasets. With the increasing amount of optical remote sensing data made available for analysis and the possibility of using a large amount of simulated data from radiative transfer models (RTMs) to train kernel MLRAs, efficient data reduction techniques will need to be implemented. Active learning (AL) methods enable to select the most informative samples in a dataset. This letter introduces six AL methods for achieving optimized biophysical variable estimation with a manageable training dataset, and their implementation into a Matlab-based MLRA toolbox for semi-automatic use. The AL methods were analyzed on their efficiency of improving the estimation accuracy of leaf area index and chlorophyll content based on PROSAIL simulations. Each of the implemented methods outperformed random sampling, improving retrieval accuracy with lower sampling rates. Practically, AL methods open opportunities to feed advanced MLRAs with RTM-generated training data for development of operational retrieval models.      
### 16.On the Optimal Deployment of Power Beacons for Massive Wireless Energy Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2012.04467.pdf)
>  Wireless energy transfer (WET) is emerging as an enabling green technology for Internet of Things (IoT) networks. WET allows the IoT devices to wirelessly recharge their batteries with energy from external sources such as dedicated radio frequency transmitters called power beacons (PBs). In this paper, we investigate the optimal deployment of PBs that guarantees a network-wide energy outage constraint. Optimal positions for the PBs are determined by maximizing the average incident power for the worst location in the service area since no information about the sensor deployment is provided. Such network planning guarantees the fairest harvesting performance for all the IoT devices. Numerical simulations evidence that our proposed optimization framework improves the energy supply reliability compared to benchmark schemes. Additionally, we show that although both, the number of deployed PBs and the number of antennas per PB, introduce performance improvements, the former has a dominant role. Finally, our proposal allows to extend the coverage area while keeping the total power budget fixed, which additionally reduces the level of electromagnetic radiation in the vicinity of PBs.      
### 17.Adversarial Disentanglement of Speaker Representation for Attribute-Driven Privacy Preservation  [ :arrow_down: ](https://arxiv.org/pdf/2012.04454.pdf)
>  With the increasing interest over speech technologies, numerous Automatic Speaker Verification (ASV) systems are employed to perform person identification. In the latter context, the systems rely on neural embeddings as a speaker representation. Nonetheless, such representations may contain privacy sensitive information about the speakers (e.g. age, sex, ethnicity, ...). In this paper, we introduce the concept of attribute-driven privacy preservation that enables a person to hide one or a few personal aspects to the authentication component. As a first solution we define an adversarial autoencoding method that disentangles a given speaker attribute from its neural representation. The proposed approach is assessed with a focus on the sex attribute. Experiments carried out using the VoxCeleb data sets have shown that the defined model enables the manipulation (i.e. variation or hiding) of this attribute while preserving good ASV performance.      
### 18.Quadratic Regularization of Data-Enabled Predictive Control: Theory and Application to Power Converter Experiments  [ :arrow_down: ](https://arxiv.org/pdf/2012.04434.pdf)
>  Data-driven control that circumvents the process of system identification by providing optimal control inputs directly from system data has attracted renewed attention in recent years. In this paper, we focus on understanding the effects of the regularization on the data-enabled predictive control (DeePC) algorithm. We provide theoretical motivation and interpretation for including a quadratic regularization term. Our analysis shows that the quadratic regularization term leads to robust and optimal solutions with regards to disturbances affecting the data. Moreover, when the input/output constraints are inactive, the quadratic regularization leads to a closed-form solution of the DeePC algorithm and thus enables fast calculations. On this basis, we propose a framework for data-driven synchronization and power regulations of power converters, which is tested by high-fidelity simulations and experiments.      
### 19.Meta Learning-based MIMO Detectors: Design, Simulation, and Experimental Test  [ :arrow_down: ](https://arxiv.org/pdf/2012.04379.pdf)
>  Deep neural networks (NNs) have exhibited considerable potential for efficiently balancing the performance and complexity of multiple-input and multiple-output (MIMO) detectors. We propose a receiver framework that enables efficient online training by leveraging the following simple observation: although NN parameters should adapt to channels, not all of them are channel-sensitive. In particular, we use a deep unfolded NN structure that represents iterative algorithms in signal detection and channel decoding modules as multi layer deep feed forward networks. An expectation propagation (EP) module, called EPNet, is established for signal detection by unfolding the EP algorithm and rendering the damping factors trainable. An unfolded turbo decoding module, called TurboNet, is used for channel decoding. This component decodes the turbo code, where trainable NN units are integrated into the traditional max-log-maximum a posteriori decoding procedure. We demonstrate that TurboNet is robust for channels and requires only one off-line training. Therefore, only a few damping factors in EPNet must be re-optimized online. An online training mechanism based on meta learning is then developed. Here, the optimizer, which is implemented by long short-term memory NNs, is trained to update damping factors efficiently by using a small training set such that they can quickly adapt to new environments. Simulation results indicate that the proposed receiver significantly outperforms traditional receivers and that the online learning mechanism can quickly adapt to new environments. Furthermore, an over-the-air platform is presented to demonstrate the significant robustness of the proposed receiver in practical deployment.      
### 20.Interpretable deep learning regression for breast density estimation on MRI  [ :arrow_down: ](https://arxiv.org/pdf/2012.04336.pdf)
>  Breast density, which is the ratio between fibroglandular tissue (FGT) and total breast volume, can be assessed qualitatively by radiologists and quantitatively by computer algorithms. These algorithms often rely on segmentation of breast and FGT volume. In this study, we propose a method to directly assess breast density on MRI, and provide interpretations of these assessments. <br>We assessed breast density in 506 patients with breast cancer using a regression convolutional neural network (CNN). The input for the CNN were slices of breast MRI of 128 x 128 voxels, and the output was a continuous density value between 0 (fatty breast) and 1 (dense breast). We used 350 patients to train the CNN, 75 for validation, and 81 for independent testing. We investigated why the CNN came to its predicted density using Deep SHapley Additive exPlanations (SHAP). <br>The density predicted by the CNN on the testing set was significantly correlated with the ground truth densities (N = 81 patients, Spearman's rho = 0.86, P &lt; 0.001). When inspecting what the CNN based its predictions on, we found that voxels in FGT commonly had positive SHAP-values, voxels in fatty tissue commonly had negative SHAP-values, and voxels in non-breast tissue commonly had SHAP-values near zero. This means that the prediction of density is based on the structures we expect it to be based on, namely FGT and fatty tissue. <br>To conclude, we presented an interpretable deep learning regression method for breast density estimation on MRI with promising results.      
### 21.Constrained Optimal Tracking Control of Unknown Systems: A Multi-Step Linear Programming Approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.04318.pdf)
>  We study the problem of optimal state-feedback tracking control for unknown discrete-time deterministic systems with input constraints. To handle input constraints, state-of-art methods utilize a certain nonquadratic stage cost function, which is sometimes limiting real systems. Furthermore, it is well known that Policy Iteration (PI) and Value Iteration (VI), two widely used algorithms in data-driven control, offer complementary strengths and weaknesses. In this work, a two-step transformation is employed, which converts the constrained-input optimal tracking problem to an unconstrained augmented optimal regulation problem, and allows the consideration of general stage cost functions. Then, a novel multi-step VI algorithm based on Q-learning and linear programming is derived. The proposed algorithm improves the convergence speed of VI, avoids the requirement for an initial stabilizing control policy of PI, and computes a constrained optimal feedback controller without the knowledge of a system model and stage cost function. Simulation studies demonstrate the reliability and performance of the proposed approach.      
### 22.Leading Cruise Control in Mixed Traffic Flow: System Modeling, Controllability, and String Stability  [ :arrow_down: ](https://arxiv.org/pdf/2012.04313.pdf)
>  Connected and autonomous vehicles (CAVs) have great potential to improve road transportation systems. Most existing strategies for CAVs' longitudinal control focus on downstream traffic conditions, but neglect the impact of CAVs' behaviors on upstream traffic flow. In this paper, we introduce a notion of Leading Cruise Control (LCC), in which the CAV maintains car-following operations adapting to the states of its preceding vehicles, and also aims to lead the motion of its following vehicles. Specifically, by controlling the CAV, LCC aims to attenuate downstream traffic perturbations and smooth upstream traffic flow actively. We first present the dynamical modeling of LCC, with a focus on three fundamental scenarios: car-following, free-driving, and Connected Cruise Control. Then, the analysis of controllability, observability, and head-to-tail string stability reveals the feasibility and potential of LCC in improving mixed traffic flow performance. Extensive numerical studies validate that the capability of CAVs in dissipating traffic perturbations is further strengthened when incorporating the information of the vehicles behind into the CAV's control.      
### 23.Channel Gain Cartography via Mixture of Experts  [ :arrow_down: ](https://arxiv.org/pdf/2012.04290.pdf)
>  In order to estimate the channel gain (CG) between the locations of an arbitrary transceiver pair across a geographic area of interest, CG maps can be constructed from spatially distributed sensor measurements. Most approaches to build such spectrum maps are location-based, meaning that the input variable to the estimating function is a pair of spatial locations. The performance of such maps depends critically on the ability of the sensors to determine their positions, which may be drastically impaired if the positioning pilot signals are affected by multi-path channels. An alternative location-free approach was recently proposed for spectrum power maps, where the input variable to the maps consists of features extracted from the positioning signals, instead of location estimates. The location-based and the location-free approaches have complementary merits. In this work, apart from adapting the location-free features for the CG maps, a method that can combine both approaches is proposed in a mixture-of-experts framework.      
### 24.Raw Image Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2012.04264.pdf)
>  Deep learning-based blind image deblurring plays an essential role in solving image blur since all existing kernels are limited in modeling the real world blur. Thus far, researchers focus on powerful models to handle the deblurring problem and achieve decent results. For this work, in a new aspect, we discover the great opportunity for image enhancement (e.g., deblurring) directly from RAW images and investigate novel neural network structures benefiting RAW-based learning. However, to the best of our knowledge, there is no available RAW image deblurring dataset. Therefore, we built a new dataset containing both RAW images and processed sRGB images and design a new model to utilize the unique characteristics of RAW images. The proposed deblurring model, trained solely from RAW images, achieves the state-of-art performance and outweighs those trained on processed sRGB images. Furthermore, with fine-tuning, the proposed model, trained on our new dataset, can generalize to other sensors. Additionally, by a series of experiments, we demonstrate that existing deblurring models can also be improved by training on the RAW images in our new dataset. Ultimately, we show a new venue for further opportunities based on the devised novel raw-based deblurring method and the brand-new Deblur-RAW dataset.      
### 25.Benchmarking Resource Usage of Underlying Datatypes of Apache Spark  [ :arrow_down: ](https://arxiv.org/pdf/2012.04192.pdf)
>  The purpose of this paper is to examine how resource usage of an analytic is affected by the different underlying datatypes of Spark analytics - Resilient Distributed Datasets (RDDs), Datasets, and DataFrames. The resource usage of an analytic is explored as a viable and preferred alternative of benchmarking big data analytics instead of the current common benchmarking performed using execution time. The run time of an analytic is shown to not be guaranteed to be a reproducible metric since many external factors to the job can affect the execution time. Instead, metrics readily available through Spark including peak execution memory are used to benchmark the resource usage of these different datatypes in common applications of Spark analytics, such as counting, caching, repartitioning, and KMeans.      
### 26.Topology Identification under Spatially Correlated Noise  [ :arrow_down: ](https://arxiv.org/pdf/2012.04175.pdf)
>  In this article, we address the problem of reconstructing the topology of a linear dynamic model (LDM), where the nodes are excited with spatially correlated exogenous noise sources, from its time series data. We transform the given LDM into an LDM with hidden nodes, where the hidden nodes are characterized using maximal cliques in the correlation graph and all the nodes are excited by uncorrelated noise. Then, using the sparse $+$ low-rank matrix decomposition of the imaginary part of the inverse power spectral density matrix--computed solely from the time series observation of the true LDM--, we reconstruct the topology along with the correlation graph.      
### 27.Set-Membership Filtering-Based Leader-Follower Synchronization of Discrete-time Linear Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.04133.pdf)
>  In this paper, a set-membership filtering-based leader-follower synchronization protocol for discrete-time linear multi-agent systems is proposed wherein the aim is to make the agents synchronize with a leader. The agents, governed by identical high-order discrete-time linear dynamics, are subject to unknown-but-bounded input disturbances. In terms of its own state information, each agent only has access to measured outputs that are corrupted with unknown-but-bounded output disturbances. Also, the initial states of the agents are unknown. To deal with all these unknowns (or uncertainties), a set-membership filter (or state estimator), having the `correction-prediction' form of a standard Kalman filter, is formulated. We consider each agent to be equipped with this filter that estimates the state of the agent and consider the agents to be able to share the state estimate information with the neighbors locally. The corrected state estimates of the agents are utilized in the local control law design for synchronization. Under appropriate conditions, the global disagreement error between the agents and the leader is shown to be bounded. An upper bound on the norm of the global disagreement error is calculated and shown to be monotonically decreasing. Finally, two simulation examples are included to illustrate the effectiveness of the proposed set-membership filter and the proposed leader-follower synchronization protocol.      
### 28.CEL-Net: Continuous Exposure for Extreme Low-Light Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2012.04112.pdf)
>  Deep learning methods for enhancing dark images learn a mapping from input images to output images with pre-determined discrete exposure levels. Often, at inference time the input and optimal output exposure levels of the given image are different from the seen ones during training. As a result the enhanced image might suffer from visual distortions, such as low contrast or dark areas. We address this issue by introducing a deep learning model that can continuously generalize at inference time to unseen exposure levels without the need to retrain the model. To this end, we introduce a dataset of 1500 raw images captured in both outdoor and indoor scenes, with five different exposure levels and various camera parameters. Using the dataset, we develop a model for extreme low-light imaging that can continuously tune the input or output exposure level of the image to an unseen one. We investigate the properties of our model and validate its performance, showing promising results.      
### 29.An unsupervised capacity identification approach based on Sobol' indices  [ :arrow_down: ](https://arxiv.org/pdf/2012.04091.pdf)
>  In many ranking problems, some particular aspects of the addressed situation should be taken into account in the aggregation process. An example is the presence of correlations between criteria, which may introduce bias in the derived ranking. In these cases, aggregation functions based on a capacity may be used to overcome this inconvenience, such as the Choquet integral or the multilinear model. The adoption of such strategies requires a stage to estimate the parameters of these aggregation operators. This task may be difficult in situations in which we do not have either further information about these parameters or preferences given by the decision maker. Therefore, the aim of this paper is to deal with such situations through an unsupervised approach for capacity identification based on the multilinear model. Our goal is to estimate a capacity that can mitigate the bias introduced by correlations in the decision data and, therefore, to provide a fairer result. The viability of our proposal is attested by numerical experiments with synthetic data      
### 30.Muticriteria decision making based on independent component analysis: A preliminary investigation considering the TOPSIS approach  [ :arrow_down: ](https://arxiv.org/pdf/2012.04085.pdf)
>  This work proposes the application of independent component analysis to the problem of ranking different alternatives by considering criteria that are not necessarily statistically independent. In this case, the observed data (the criteria values for all alternatives) can be modeled as mixtures of latent variables. Therefore, in the proposed approach, we perform ranking by means of the TOPSIS approach and based on the independent components extracted from the collected decision data. Numerical experiments attest the usefulness of the proposed approach, as they show that working with latent variables leads to better results compared to already existing methods      
### 31.SCUBA: An In-Device Multiplexed Protocol for Sidelink Communication on Unlicensed Bands  [ :arrow_down: ](https://arxiv.org/pdf/2012.04074.pdf)
>  Device-to-device communication (D2D) is a key enabler for connecting devices together to form the Internet of Things (IoT). A growing issue with IoT networks is the increasing number of IoT devices congesting the spectral resources of the cellular bands. Operating D2D in unlicensed band alleviates this issue by offloading network traffic from the licensed bands, while also reducing the associated licensing costs. To this end, we present a new low-cost radio access technology (RAT) protocol, called Sidelink Communications on Unlicensed BAnds (SCUBA), which can be implemented on cellular devices such that it coexists with the legacy cellular protocol by operating as a secondary RAT in a time division duplex manner using the existing radio hardware. SCUBA is compatible on different types of cellular devices including the low-complexity half-duplex frequency division duplex machine type communication (MTC) user equipments. SCUBA provides flexible sidelink (SL) latency and battery life tradeoff using a discontinuous reception procedure, which ensures that it is applicable across a wide range of use cases. We prove the effectiveness of our protocol with analyses and simulation results of the medium access control layer of SCUBA using different types of MTC traffic for both SL and the underlying cellular communication.      
### 32.A Distributed Economic Model Predictive Control Design for a Transactive Energy Market Platform in Lebanon, NH  [ :arrow_down: ](https://arxiv.org/pdf/2012.04058.pdf)
>  The electricity distribution system is fundamentally changing due to the widespread adoption of variable renewable energy resources (VREs), network-enabled digital physical devices, and active consumer engagement. VREs are uncertain and intermittent in nature and pose various technical challenges to power systems control and operations thus limiting their penetration. Engaging the demand-side with control structures that leverage the benefits of integral social and retail market engagement from individual electricity consumers through active community-level coordination serves as a control lever that could support the greater adoption of VREs. This paper presents a Distributed Economic Model Predictive control (DEMPC) algorithm for the electric power distribution system using the augmented lagrangian alternating direction inexact newton (ALADIN) algorithm. Specifically, this DEMPC solves the Alternating Current Optimal Power Flow (ACOPF) problem over a receding time-horizon. In addition, it employs a social welfare maximization of the ACOPF to capture consumer preferences through explicit use of time-varying utility functions. The DEMPC formulation of the ACOPF applied in this work is novel as it addresses the inherent dynamic characteristics of the grid and scales with the explosion of actively controlled devices on the demand-side. The paper demonstrates the simulation methodology on a 13-node Lebanon NH distribution feeder.      
### 33.Real-Time Formal Verification of Autonomous Systems With An FPGA  [ :arrow_down: ](https://arxiv.org/pdf/2012.04011.pdf)
>  Hamilton-Jacobi reachability analysis is a powerful technique used to verify the safety of autonomous systems. This method is very good at handling non-linear system dynamics with disturbances and flexible set representations. A drawback to this approach is that it suffers from the curse of dimensionality, which prevents real-time deployment on safety-critical systems. In this paper, we show that a customized hardware design on a Field Programmable Gate Array (FPGA) could accelerate 4D grid-based Hamilton-Jacobi (HJ) reachability analysis up to 16 times compared to an optimized implementation and 142 times compared to MATLAB ToolboxLS on a 16-thread CPU. Our design can overcome the complex data access pattern while taking advantage of the parallel nature of the HJ PDE computation. Because of this, we are able to achieve real-time formal verification with a 4D car model by re-solving the HJ PDE at a frequency of 5Hz on the FPGA as the environment changes. The latency of our computation is deterministic, which is crucial for safetycritical systems. Our approach presented here can be applied to different systems dynamics, and moreover, potentially leveraged for higher dimensions systems. We also demonstrate obstacle avoidance with a robot car in a changing environment.      
### 34.Deep Networks to Automatically Detect Late-activating Regions of the Heart  [ :arrow_down: ](https://arxiv.org/pdf/2012.04000.pdf)
>  This paper presents a novel method to automatically identify late-activating regions of the left ventricle from cine Displacement Encoding with Stimulated Echo (DENSE) MR images. We develop a deep learning framework that identifies late mechanical activation in heart failure patients by detecting the Time to the Onset of circumferential Shortening (TOS). In particular, we build a cascade network performing end-to-end (i) segmentation of the left ventricle to analyze cardiac function, (ii) prediction of TOS based on spatiotemporal circumferential strains computed from displacement maps, and (iii) 3D visualization of delayed activation maps. Our approach results in dramatic savings of manual labors and computational time over traditional optimization-based algorithms. To evaluate the effectiveness of our method, we run tests on cardiac images and compare with recent related works. Experimental results show that the proposed approach provides fast prediction of TOS with improved accuracy.      
### 35.Real-Time Dynamic Optimal Power Flow in Electric Vehicles Considering the Lifetime of the Components in the E-Powertrain  [ :arrow_down: ](https://arxiv.org/pdf/2012.03936.pdf)
>  Different types of energy sources (e.g., batteries, supercapacitors, fuel cells) can be utilized in electric vehicles to store and provide energy in the e-powertrain through power electronic devices [1-6]. The lifetime of the components in the e-powertrain depends on their load profile [7,8]. For instance, the lifetime of a battery highly depends on the depth of discharge and the number of charge/discharge cycles [9-13]. The lifetime of an inverter mostly depends on the variations in the active-reactive power passing through it. This means that the expended life cost of the components can be decreased by allocating an optimal share of the total power to each energy source and power electronic device at an optimal time instance. In addition, the driving range of a vehicle can be prolonged by decreasing the energy loss i.e., operating the components in their high-efficiency region. Therefore, it is necessary to perform Optimal Power Flow (OPF) in the operation of the e-powertrain.      
### 36.I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at Pitch  [ :arrow_down: ](https://arxiv.org/pdf/2012.04572.pdf)
>  Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difficult for these audio distances, suggesting significant progress can be made in self-supervised audio learning by improving current losses.      
### 37.Bayesian Image Reconstruction using Deep Generative Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.04567.pdf)
>  Machine learning models are commonly trained end-to-end and in a supervised setting, using paired (input, output) data. Classical examples include recent super-resolution methods that train on pairs of (low-resolution, high-resolution) images. However, these end-to-end approaches require re-training every time there is a distribution shift in the inputs (e.g., night images vs daylight) or relevant latent variables (e.g., camera blur or hand motion). In this work, we leverage state-of-the-art (SOTA) generative models (here StyleGAN2) for building powerful image priors, which enable application of Bayes' theorem for many downstream reconstruction tasks. Our method, called Bayesian Reconstruction through Generative Models (BRGM), uses a single pre-trained generator model to solve different image restoration tasks, i.e., super-resolution and in-painting, by combining it with different forward corruption models. We demonstrate BRGM on three large, yet diverse, datasets that enable us to build powerful priors: (i) 60,000 images from the Flick Faces High Quality dataset \cite{karras2019style} (ii) 240,000 chest X-rays from MIMIC III and (iii) a combined collection of 5 brain MRI datasets with 7,329 scans. Across all three datasets and without any dataset-specific hyperparameter tuning, our approach yields state-of-the-art performance on super-resolution, particularly at low-resolution levels, as well as inpainting, compared to state-of-the-art methods that are specific to each reconstruction task. We will make our code and pre-trained models available online.      
### 38.The Maintenance Location Choice Problem for Railway Rolling Stock  [ :arrow_down: ](https://arxiv.org/pdf/2012.04565.pdf)
>  Due to increasing railway use, the capacity at railway yards and maintenance locations is becoming limiting to accommodate existing rolling stock. To reduce capacity issues at maintenance locations during nighttime, railway undertakings consider performing more daytime maintenance, but the choice at which locations personnel needs to be stationed for daytime maintenance is not straightforward. Among other things, it depends on the planned rolling stock circulation and the maintenance activities that need to be performed. This paper presents the Maintenance Location Choice Problem (MLCP) and provides a Mixed Integer Linear Programming model for this problem. The model demonstrates that for a representative rolling stock circulation from the Dutch railways a substantial amount of maintenance activities can be performed during daytime. Also, it is shown that the location choice delivered by the model is robust under various time horizons and rolling stock circulations. Moreover, the running time for optimizing the model is considered acceptable for planning purposes.      
### 39.Quantifying vegetation biophysical variables from imaging spectroscopy data: a review on retrieval methods  [ :arrow_down: ](https://arxiv.org/pdf/2012.04555.pdf)
>  An unprecedented spectroscopic data stream will soon become available with forthcoming Earth-observing satellite missions equipped with imaging spectroradiometers. This data stream will open up a vast array of opportunities to quantify a diversity of biochemical and structural vegetation properties. The processing requirements for such large data streams require reliable retrieval techniques enabling the spatiotemporally explicit quantification of biophysical variables. With the aim of preparing for this new era of Earth observation, this review summarizes the state-of-the-art retrieval methods that have been applied in experimental imaging spectroscopy studies inferring all kinds of vegetation biophysical variables. Identified retrieval methods are categorized into: (1) parametric regression, including vegetation indices, shape indices, and spectral transformations; (2) non-parametric regression, including linear and non-linear machine learning regression algorithms; (3) physically-based, including inversion of radiative transfer models (RTMs) using numerical optimization and look-up-table approaches; and (4) hybrid regression methods, which combine RTM simulations with machine learning regression methods. For each of these categories, an overview of widely applied methods with application to mapping vegetation properties is given. In view of processing imaging spectroscopy data, a critical aspect involves the challenge of dealing with spectral multicollinearity. The ability to provide robust estimates, retrieval uncertainties, and acceptable retrieval processing speed are other important aspects in view of operational processing. Recommendations towards new-generation spectroscopy-based processing chains for the operational production of biophysical variables are given.      
### 40.CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2012.04551.pdf)
>  We introduce single-shot X-ray tomography that aims to estimate the target image from a single cone-beam projection measurement. This linear inverse problem is extremely under-determined since the measurements are far fewer than the number of unknowns. Moreover, it is more challenging than conventional tomography where a sufficiently large number of projection angles forms the measurements, allowing for a simple inversion process. However, single-shot tomography becomes less severe if the target image is only composed of known shapes. Hence, the shape prior transforms a linear ill-posed image estimation problem to a non-linear problem of estimating the roto-translations of the shapes. In this paper, we circumvent the non-linearity by using a dictionary of possible roto-translations of the shapes. We propose a convex program CoShaRP to recover the dictionary-coefficients successfully. CoShaRP relies on simplex-type constraint and can be solved quickly using a primal-dual algorithm. The numerical experiments show that CoShaRP recovers shapes stably from moderately noisy measurements.      
### 41.GMM-Based Generative Adversarial Encoder Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.04525.pdf)
>  While GAN is a powerful model for generating images, its inability to infer a latent space directly limits its use in applications requiring an encoder. Our paper presents a simple architectural setup that combines the generative capabilities of GAN with an encoder. We accomplish this by combining the encoder with the discriminator using shared weights, then training them simultaneously using a new loss term. We model the output of the encoder latent space via a GMM, which leads to both good clustering using this latent space and improved image generation by the GAN. Our framework is generic and can be easily plugged into any GAN strategy. In particular, we demonstrate it both with Vanilla GAN and Wasserstein GAN, where in both it leads to an improvement in the generated images in terms of both the IS and FID scores. Moreover, we show that our encoder learns a meaningful representation as its clustering results are competitive with the current GAN-based state-of-the-art in clustering.      
### 42.A Geometric Framework for Pitch Estimation on Acoustic Musical Signals  [ :arrow_down: ](https://arxiv.org/pdf/2012.04517.pdf)
>  This paper presents a geometric approach to pitch estimation (PE)-an important problem in Music Information Retrieval (MIR), and a precursor to a variety of other problems in the field. Though there exist a number of highly-accurate methods, both mono-pitch estimation and multi-pitch estimation (particularly with unspecified polyphonic timbre) prove computationally and conceptually challenging. A number of current techniques, whilst incredibly effective, are not targeted towards eliciting the underlying mathematical structures that underpin the complex musical patterns exhibited by acoustic musical signals. Tackling the approach from both a theoretical and experimental perspective, we present a novel framework, a basis for further work in the area, and results that (whilst not state of the art) demonstrate relative efficacy. The framework presented in this paper opens up a completely new way to tackle PE problems, and may have uses both in traditional analytical approaches, as well as in the emerging machine learning (ML) methods that currently dominate the literature.      
### 43.Digital Gimbal: End-to-end Deep Image Stabilization with Learnable Exposure Times  [ :arrow_down: ](https://arxiv.org/pdf/2012.04515.pdf)
>  Mechanical image stabilization using actuated gimbals enables capturing long-exposure shots without suffering from blur due to camera motion. These devices, however, are often physically cumbersome and expensive, limiting their widespread use. In this work, we propose to digitally emulate a mechanically stabilized system from the input of a fast unstabilized camera. To exploit the trade-off between motion blur at long exposures and low SNR at short exposures, we train a CNN that estimates a sharp high-SNR image by aggregating a burst of noisy short-exposure frames, related by unknown motion. We further suggest learning the burst's exposure times in an end-to-end manner, thus balancing the noise and blur across the frames. We demonstrate this method's advantage over the traditional approach of deblurring a single image or denoising a fixed-exposure burst.      
### 44.Reduced motion artifacts and speed improvements in enhanced line-scanning fiber bundle endomicroscopy  [ :arrow_down: ](https://arxiv.org/pdf/2012.04492.pdf)
>  Significance: Confocal laser scanning enables optical sectioning in fiber bundle endomicroscopy but limits the frame rate. To be able to better explore tissue morphology it is useful to stitch sequentially acquired frames into a mosaic. However, low frame rates limit the maximum probe translation speed. Line-scanning confocal endomicroscopy provides higher frame rates, but residual out-of-focus light degrades images. Subtraction based approaches can suppress this residue at the expense of introducing motion artifacts. <br>Aim: To generate high frame rate endomicroscopy images with improved optical sectioning, we develop a high-speed subtraction method that only requires the acquisition of a single camera frame. <br>Approach: The rolling shutter of a CMOS camera acts as both the aligned and offset detector slits required for subtraction-based sectioning enhancement. Two images of the bundle are formed on different regions of the camera, allowing both images to be acquired simultaneously. <br>Results: We confirm improved optical sectioning compared to conventional line-scanning, particularly far from focus, and show that motion artifacts are not introduced. We demonstrate high-speed mosaicing at frame rates of up to 240 Hz. <br>Conclusion: High-speed acquisition of optically sectioned images using the new subtraction based approach leads to improved mosaicing at high frame rates.      
### 45.Discrete Signal Processing on Meet/Join Lattices  [ :arrow_down: ](https://arxiv.org/pdf/2012.04358.pdf)
>  A lattice is a partially ordered set supporting a meet (or join) operation that returns the largest lower bound (smallest upper bound) of two elements. Just like graphs, lattices are a fundamental structure that occurs across domains including social data analysis, natural language processing, computational chemistry and biology, and database theory. In this paper we introduce discrete-lattice signal processing (DLSP), an SP framework for data, or signals, indexed by such lattices. We use the meet (or join) to define a shift operation and derive associated notions of filtering, Fourier basis and transform, and frequency response. We show that the spectrum of a lattice signal inherits the lattice structure of the signal domain and derive a sampling theorem. Finally, we show two prototypical applications: spectral analysis of formal concept lattices in social science and sampling and Wiener filtering of multiset lattices in combinatorial auctions. Formal concept lattices are a compressed representation of relations between objects and attributes. Since relations are equivalent to bipartite graphs and hypergraphs, DLSP offers a form of Fourier analysis for these structures.      
### 46.Overcomplete Representations Against Adversarial Videos  [ :arrow_down: ](https://arxiv.org/pdf/2012.04262.pdf)
>  Adversarial robustness of deep neural networks is an extensively studied problem in the literature and various methods have been proposed to defend against adversarial images. However, only a handful of defense methods have been developed for defending against attacked videos. In this paper, we propose a novel Over-and-Under complete restoration network for Defending against adversarial videos (OUDefend). Most restoration networks adopt an encoder-decoder architecture that first shrinks spatial dimension then expands it back. This approach learns undercomplete representations, which have large receptive fields to collect global information but overlooks local details. On the other hand, overcomplete representations have opposite properties. Hence, OUDefend is designed to balance local and global features by learning those two representations. We attach OUDefend to target video recognition models as a feature restoration block and train the entire network end-to-end. Experimental results show that the defenses focusing on images may be ineffective to videos, while OUDefend enhances robustness against different types of adversarial videos, ranging from additive attacks, multiplicative attacks to physically realizable attacks.      
### 47.An Efficient Analyses of the Behavior of One Dimensional Chaotic Maps using 0-1 Test and Three State Test  [ :arrow_down: ](https://arxiv.org/pdf/2012.04156.pdf)
>  In this paper, a rigorous analysis of the behavior of the standard logistic map, Logistic Tent system (LTS), Logistic-Sine system (LSS) and Tent-Sine system (TSS) is performed using 0-1 test and three state test (3ST). In this work, it has been proved that the strength of the chaotic behavior is not uniform. Through extensive experiment and analysis, the strong and weak chaotic regions of LTS, LSS and TSS have been identified. This would enable researchers using these maps, to have better choices of control parameters as key values, for stronger encryption. In addition, this paper serves as a precursor to stronger testing practices in cryptosystem research, as Lyapunov exponent alone has been shown to fail as a true representation of the chaotic nature of a map.      
### 48.f2IMU-R: Pedestrian Navigation by Low-cost Foot-Mounted Dual IMUs and Inter-foot Ranging  [ :arrow_down: ](https://arxiv.org/pdf/2012.04143.pdf)
>  Foot-mounted inertial sensors become popular in many indoor or GPS-denied applications, including but not limited to medical monitoring, gait analysis, soldier and first responder positioning. However, the foot-mounted inertial navigation relies largely on the aid of Zero Velocity Update (ZUPT) and has encountered inherent problems such as heading drift. This paper implements a pedestrian navigation system based on dual foot-mounted low-cost inertial measurement units (IMU) and inter-foot ultrasonic ranging. The observability analysis of the system is performed to investigate the roles of the ZUPT measurement and the foot-to-foot ranging measurement in improving the state estimability. A Kalman-based estimation algorithm is mechanized in the Earth frame, rather than in the common local-level frame, which is found to be effective in depressing the linearization error in Kalman filtering. An ellipsoid constraint in the Earth frame is also proposed to further restrict the height drift. Simulation and real field experiments show that the proposed method has better robustness and positioning accuracy (about 0.1-0.2% travelled distance) than the traditional pedestrian navigation schemes do.      
### 49.Deep Energy-Based NARX Models  [ :arrow_down: ](https://arxiv.org/pdf/2012.04136.pdf)
>  This paper is directed towards the problem of learning nonlinear ARX models based on system input--output data. In particular, our interest is in learning a conditional distribution of the current output based on a finite window of past inputs and outputs. To achieve this, we consider the use of so-called energy-based models, which have been developed in allied fields for learning unknown distributions based on data. This energy-based model relies on a general function to describe the distribution, and here we consider a deep neural network for this purpose. The primary benefit of this approach is that it is capable of learning both simple and highly complex noise models, which we demonstrate on simulated and experimental data.      
### 50.Complementary Capabilities of Photoacoustic Imaging to Existing Optical Ocular Imaging Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2012.04101.pdf)
>  In this chapter, we will give a brief overview of fundus photography, SLO, and OCT while discussing photoacoustic imaging potential as the next major ocular imaging modality.      
### 51.Frame-level SpecAugment for Deep Convolutional Neural Networks in Hybrid ASR Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.04094.pdf)
>  Inspired by SpecAugment -- a data augmentation method for end-to-end ASR systems, we propose a frame-level SpecAugment method (f-SpecAugment) to improve the performance of deep convolutional neural networks (CNN) for hybrid HMM based ASR systems. Similar to the utterance level SpecAugment, f-SpecAugment performs three transformations: time warping, frequency masking, and time masking. Instead of applying the transformations at the utterance level, f-SpecAugment applies them to each convolution window independently during training. We demonstrate that f-SpecAugment is more effective than the utterance level SpecAugment for deep CNN based hybrid models. We evaluate the proposed f-SpecAugment on 50-layer Self-Normalizing Deep CNN (SNDCNN) acoustic models trained with up to 25000 hours of training data. We observe f-SpecAugment reduces WER by 0.5-4.5% relatively across different ASR tasks for four languages. As the benefits of augmentation techniques tend to diminish as training data size increases, the large scale training reported is important in understanding the effectiveness of f-SpecAugment. Our experiments demonstrate that even with 25k training data, f-SpecAugment is still effective. We also demonstrate that f-SpecAugment has benefits approximately equivalent to doubling the amount of training data for deep CNNs.      
### 52.Invertibility Conditions for the Admittance Matrices of Balanced Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.04087.pdf)
>  The admittance matrix encodes the network topology and electrical parameters of a power system in order to relate the current injection and voltage phasors. Since admittance matrices are central to many power engineering analyses, their characteristics are important subjects of theoretical studies. This paper focuses on the key characteristic of invertibility. Previous literature has presented an invertibility condition for admittance matrices. This paper first identifies and fixes a technical issue in the proof of this previously presented invertibility condition. This paper then extends this previous work by deriving new conditions that are applicable to a broader class of systems with lossless branches and transformers with off-nominal tap ratios.      
### 53.Efficient Attitude Estimators: A Tutorial and Survey  [ :arrow_down: ](https://arxiv.org/pdf/2012.04075.pdf)
>  Inertial sensors based on micro-electromechanical systems (MEMS) technology, such as accelerometers and angular rate sensors, are cost-effective solutions used in inertial navigation systems in a broad spectrum of applications that estimate position, velocity and orientation of a system with respect to an inertial reference frame. The task of an orientation filter is to compute an optimal solution for the attitude state, consisting of roll, pitch and yaw, through the fusion of angular rate, accelerometer, and magnetometer measurements. The aim of this paper is threefold: first, it serves researchers and practitioners in the signal processing community seeking the most appropriate attitude estimators that fulfills their needs, shedding light on the drawbacks and the advantages of a wide variety of designs. Second, it serves as a survey and tutorial for existing estimator designs in the literature, assessing their design aspects and components, and dissecting their hidden details for the benefit of researchers. Third, a comprehensive list of algorithms is discussed for a fully functional inertial navigation system, starting from the navigation equations and ending with the filter equations, keeping in mind their suitability for power limited embedded processors. The source code of all algorithms is published, with the aim of it being an out-of-box solution for researchers in the field. The reader will take away the following concepts from this article: understand the key concepts of an inertial navigation system; be able to implement and test a complete stand alone solution; be able to evaluate and understand different algorithms; understand the trade-offs between different filter architectures and techniques; and understand efficient embedded processing techniques, trends and opportunities.      
### 54.Design and Analysis of Uplink and Downlink Communications for Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.04057.pdf)
>  Communication has been known to be one of the primary bottlenecks of federated learning (FL), and yet existing studies have not addressed the efficient communication design, particularly in wireless FL where both uplink and downlink communications have to be considered. In this paper, we focus on the design and analysis of physical layer quantization and transmission methods for wireless FL. We answer the question of what and how to communicate between clients and the parameter server and evaluate the impact of the various quantization and transmission options of the updated model on the learning performance. We provide new convergence analysis of the well-known FedAvg under non-i.i.d. dataset distributions, partial clients participation, and finite-precision quantization in uplink and downlink communications. These analyses reveal that, in order to achieve an O(1/T) convergence rate with quantization, transmitting the weight requires increasing the quantization level at a logarithmic rate, while transmitting the weight differential can keep a constant quantization level. Comprehensive numerical evaluation on various real-world datasets reveals that the benefit of a FL-tailored uplink and downlink communication design is enormous - a carefully designed quantization and transmission achieves more than 98% of the floating-point baseline accuracy with fewer than 10% of the baseline bandwidth, for majority of the experiments on both i.i.d. and non-i.i.d. datasets. In particular, 1-bit quantization (3.1% of the floating-point baseline bandwidth) achieves 99.8% of the floating-point baseline accuracy at almost the same convergence rate on MNIST, representing the best known bandwidth-accuracy tradeoff to the best of the authors' knowledge.      
### 55.The Spectral-Domain $\mathcal{W}_2$ Wasserstein Distance for Elliptical Processes and the Spectral-Domain Gelbrich Bound  [ :arrow_down: ](https://arxiv.org/pdf/2012.04023.pdf)
>  In this short note, we introduce the spectral-domain $\mathcal{W}_2$ Wasserstein distance for elliptical stochastic processes in terms of their power spectra. We also introduce the spectral-domain Gelbrich bound for processes that are not necessarily elliptical.      
### 56.Battery Model Calibration with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2012.04010.pdf)
>  Lithium-Ion (Li-I) batteries have recently become pervasive and are used in many physical assets. To enable a good prediction of the end of discharge of batteries, detailed electrochemical Li-I battery models have been developed. Their parameters are typically calibrated before they are taken into operation and are typically not re-calibrated during operation. However, since battery performance is affected by aging, the reality gap between the computational battery models and the real physical systems leads to inaccurate predictions. A supervised machine learning algorithm would require an extensive representative training dataset mapping the observation to the ground truth calibration parameters. This may be infeasible for many practical applications. In this paper, we implement a Reinforcement Learning-based framework for reliably and efficiently inferring calibration parameters of battery models. The framework enables real-time inference of the computational model parameters in order to compensate the reality-gap from the observations. Most importantly, the proposed methodology does not need any labeled data samples, (samples of observations and the ground truth calibration parameters). Furthermore, the framework does not require any information on the underlying physical model. The experimental results demonstrate that the proposed methodology is capable of inferring the model parameters with high accuracy and high robustness. While the achieved results are comparable to those obtained with supervised machine learning, they do not rely on the ground truth information during training.      
### 57.Patterns, anticipation and participatory futures  [ :arrow_down: ](https://arxiv.org/pdf/2012.03736.pdf)
>  Patterns embody repeating phenomena, and, as such, they are partly but not fully detachable from their context. 'Design patterns' and 'pattern languages' are established methods for working with patterns. They have been applied in architecture, software engineering, and other design fields, but have so far seen little application in the field of future studies. We reimagine futures discourse and anticipatory practices using pattern methods. We focus specifically on processes for coordinating distributed projects, integrating multiple voices, and on play that builds capability to face what's yet to come. One of the advantages of the method as a whole is that it deals with local knowledge and does not subsume everything within one overall 'global' strategy, while nevertheless offering a way to communicate between contexts and disciplines.      
