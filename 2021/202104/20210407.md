# ArXiv eess --Wed, 7 Apr 2021
### 1.Adaptive Variants of Optimal Feedback Policies  [ :arrow_down: ](https://arxiv.org/pdf/2104.02709.pdf)
>  We combine adaptive control directly with optimal or near-optimal value functions to enhance stability and closed-loop performance in systems with parametric uncertainties. Leveraging the fundamental result that a value function is also a control Lyapunov function (CLF), combined with the fact that direct adaptive control can be immediately used once a CLF is known, we prove asymptotic closed-loop convergence of adaptive feedback controllers derived from optimization-based policies. Both matched and unmatched parametric variations are addressed, where the latter exploits a new technique based on adaptation rate scaling. The results may have particular resonance in machine learning for dynamical systems, where nominal feedback controllers are typically optimization-based but need to remain effective (beyond mere robustness) in the presence of significant but structured variations in parameters.      
### 2.Time-Robust Control for STL Specifications  [ :arrow_down: ](https://arxiv.org/pdf/2104.02677.pdf)
>  We present a robust control framework for time-critical systems in which satisfying real-time constraints is of utmost importance for the safety of the system. Signal Temporal Logic (STL) provides a formal means to express a variety of real-time constraints over signals and is suited for planning and control purposes as it allows us to reason about the time robustness of such constraints. The time robustness of STL particularly quantifies the extent to which timing uncertainties can be tolerated without violating real-time specifications. In this paper, we first pose a control problem in which we aim to find an optimal input sequence to a control system that maximizes the time robustness of an STL constraint. We then propose a Mixed Integer Linear Program (MILP) encoding and provide correctness guarantees and a complexity analysis of the encoding. We also show in two case studies that maximizing STL time robustness allows to account for timing uncertainties of the underlying control system.      
### 3.How Decentral Smart Grid Control limits non-Gaussian power grid frequency fluctuations  [ :arrow_down: ](https://arxiv.org/pdf/2104.02657.pdf)
>  Frequency fluctuations in power grids, caused by unpredictable renewable energy sources, consumer behavior and trading, need to be balanced to ensure stable grid operation. Standard smart grid solutions to mitigate large frequency excursions are based on centrally collecting data and give rise to security and privacy concerns. Furthermore, control of fluctuations is often tested by employing Gaussian perturbations. Here, we demonstrate that power grid frequency fluctuations are in general non-Gaussian, implying that large excursions are more likely than expected based on Gaussian modeling. We consider real power grid frequency measurements from Continental Europe and compare them to stochastic models and predictions based on Fokker-Planck equations. Furthermore, we review a decentral smart grid control scheme to limit these fluctuations. In particular, we derive a scaling law of how decentralized control actions reduce the magnitude of frequency fluctuations and demonstrate the power of these theoretical predictions using a test grid. Overall, we find that decentral smart grid control may reduce grid frequency excursions due to both Gaussian and non-Gaussian power fluctuations and thus offers an alternative pathway for mitigating fluctuation-induced risks.      
### 4.A Unified Passivity-Based Framework for Control of Modular Islanded AC Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2104.02637.pdf)
>  Voltage and frequency control in an islanded AC microgrid (ImGs) amount to stabilizing an a priori unknown ImG equilibrium induced by loads and changes in topology. This paper puts forth a unified control framework which, while guaranteeing such stability, allows for modular ImGs interconnecting multiple subsystems, that is, dynamic RLC lines, nonlinear constant impedance, current, power (ZIP) and exponential (EXP) loads, and inverter-based distributed generation units (DGUs) controlled with different types of primary controllers. The underlying idea of the framework is based one equilibrium-independent passivity (EIP) of the ImG subsystems, which enables stability certificates of ImG equilibria without their explicit knowledge. In order to render DGUs EIP, we propose a decentralized controller synthesis algorithm based on port-Hamiltonian systems (PHSs). We also show that EIP, being the key to stability, provides a general framework which can embrace other solutions available in the literature. Furthermore, we provide a novel argument based on LaSalle's theorem for proving asymptotic voltage and frequency stability. Finally, we analyze the impact of actuator saturation on the stability results by exploiting the inherent EIP properties of the PHS DGU model. Theoretical findings are backed up by realistic simulations based on the CIGRE benchmark for medium voltage networks.      
### 5.I-ODA, Real-World Multi-modal Longitudinal Data for OphthalmicApplications  [ :arrow_down: ](https://arxiv.org/pdf/2104.02609.pdf)
>  Data from clinical real-world settings is characterized by variability in quality, machine-type, setting, and source. One of the primary goals of medical computer vision is to develop and validate artificial intelligence (AI) based algorithms on real-world data enabling clinical translations. However, despite the exponential growth in AI based applications in healthcare, specifically in ophthalmology, translations to clinical settings remain challenging. Limited access to adequate and diverse real-world data inhibits the development and validation of translatable algorithms. In this paper, we present a new multi-modal longitudinal ophthalmic imaging dataset, the Illinois Ophthalmic Database Atlas (I-ODA), with the goal of advancing state-of-the-art computer vision applications in ophthalmology, and improving upon the translatable capacity of AI based applications across different clinical settings. We present the infrastructure employed to collect, annotate, and anonymize images from multiple sources, demonstrating the complexity of real-world retrospective data and its limitations. I-ODA includes 12 imaging modalities with a total of 3,668,649 ophthalmic images of 33,876 individuals from the Department of Ophthalmology and Visual Sciences at the Illinois Eye and Ear Infirmary of the University of Illinois Chicago (UIC) over the course of 12 years.      
### 6.Pathological Image Segmentation with Noisy Labels  [ :arrow_down: ](https://arxiv.org/pdf/2104.02602.pdf)
>  Segmentation of pathological images is essential for accurate disease diagnosis. The quality of manual labels plays a critical role in segmentation accuracy; yet, in practice, the labels between pathologists could be inconsistent, thus confusing the training process. In this work, we propose a novel label re-weighting framework to account for the reliability of different experts' labels on each pixel according to its surrounding features. We further devise a new attention heatmap, which takes roughness as prior knowledge to guide the model to focus on important regions. Our approach is evaluated on the public Gleason 2019 datasets. The results show that our approach effectively improves the model's robustness against noisy labels and outperforms state-of-the-art approaches.      
### 7.A Wave Trapping Dispersive Delay Structure Based on Substrate Integrated Waveguide (SIW)  [ :arrow_down: ](https://arxiv.org/pdf/2104.02601.pdf)
>  A dispersive delay structure (DDS) is a device that creates a specified group delay function over frequency bandwidth. In this paper, the idea of trapping the electromagnetic waves in a feedback loop is proposed to achieve higher group delay swing. The proposed structure includes two transmission lines and two junctions that act as power divider/combiner. The transmission lines can be either dispersive or non-dispersive. The group delay in the proposed configuration is proportional to the injected power into the feedback loop. So, an increase in the group delay swing can be done by improving the junctions or by adding more feedback loops to the structure. Based on this configuration, a novel DDS, using the substrate integrated waveguide (SIW) transmission line, is introduced. According to the experimental results, the proposed device with a relatively compact size creates high group delay swings (9 ns), and its insertion loss is about 7 dB.      
### 8.Safety-Critical Control of Stochastic Systems using Stochastic Control Barrier Functions  [ :arrow_down: ](https://arxiv.org/pdf/2104.02585.pdf)
>  Control barrier functions have been widely used for synthesizing safety-critical controls, often via solving quadratic programs. However, the existence of Gaussian-type noise may lead to unsafe actions and result in severe consequences. In this paper, we study systems modeled by stochastic differential equations (SDEs) driven by Brownian motions. We propose a notion of stochastic control barrier functions (SCBFs)and show that SCBFs can significantly reduce the control efforts, especially in the presence of noise, compared to stochastic reciprocal control barrier functions (SRCBFs), and offer a less conservative estimation of safety probability, compared to stochastic zeroing control barrier functions (SZCBFs). Based on this less conservative probabilistic estimation for the proposed notion of SCBFs, we further extend the results to handle high relative degree safety constraints using high-order SCBFs. We demonstrate that the proposed SCBFs achieve good trade-offs of performance and control efforts, both through theoretical analysis and numerical simulations.      
### 9.Limitations and Improvements of the Intelligent Driver Model (IDM)  [ :arrow_down: ](https://arxiv.org/pdf/2104.02583.pdf)
>  This contribution analyzes the widely used and well-known "intelligent driver model" (briefly IDM), which is a second order car-following model governed by a system of ordinary differential equations. Although this model was intensively studied in recent years for properly capturing traffic phenomena and driver braking behavior, a rigorous study of the well-posedness of solutions has, to our knowledge, never been performed. First it is shown that, for a specific class of initial data, the vehicles' velocities become negative or even diverge to \(-\infty\) in finite time, both undesirable properties for a car-following model. Various modifications of the IDM are then proposed in order to avoid such ill-posedness. The theoretical remediation of the model, rather than post facto by ad-hoc modification of code implementations, allows a more sound numerical implementation and preservation of the model features. Indeed, to avoid inconsistencies and ensure dynamics close to the one of the original model, one may need to inspect and clean large input data, which may result practically impossible for large-scale simulations. Although well-posedness issues occur only for specific initial data, this may happen frequently when different traffic scenarios are analyzed, and especially in presence of lane-changing, on ramps and other network components as it is the case for most commonly used micro-simulators. On the other side, it is shown that well-posedness can be guaranteed by straight-forward improvements, such as those obtained by slightly changing the acceleration to prevent the velocity from becoming negative.      
### 10.Speaker embeddings by modeling channel-wise correlations  [ :arrow_down: ](https://arxiv.org/pdf/2104.02571.pdf)
>  Speaker embeddings extracted with deep 2D convolutional neural networks are typically modeled as projections of first and second order statistics of channel-frequency pairs onto a linear layer, using either average or attentive pooling along the time axis. In this paper we examine an alternative pooling method, where pairwise correlations between channels for given frequencies are used as statistics. The method is inspired by style-transfer methods in computer vision, where the style of an image, modeled by the matrix of channel-wise correlations, is transferred to another image, in order to produce a new image having the style of the first and the content of the second. By drawing analogies between image style and speaker characteristics, and between image content and phonetic sequence, we explore the use of such channel-wise correlations features to train a ResNet architecture in an end-to-end fashion. Our experiments on VoxCeleb demonstrate the effectiveness of the proposed pooling method in speaker recognition.      
### 11.Self Calibration of Scalar and Vector Sensor Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2104.02561.pdf)
>  In this work, we consider the problem of joint calibration and direction-of-arrival (DOA) estimation using sensor arrays. This joint estimation problem is referred to as self calibration. Unlike many previous iterative approaches, we propose geometry independent convex optimization algorithms for jointly estimating the sensor gain and phase errors as well as the source DOAs. We derive these algorithms based on both the conventional element-space data model and the covariance data model. We focus on sparse and regular arrays formed using scalar sensors as well as vector sensors. The developed algorithms are obtained by transforming the underlying bilinear calibration model into a linear model, and subsequently by using standard convex relaxation techniques to estimate the unknown parameters. Prior to the algorithm discussion, we also derive identifiability conditions for the existence of a unique solution to the self calibration problem. To demonstrate the effectiveness of the developed techniques, numerical experiments and comparisons to the state-of-the-art methods are provided. Finally, the results from an experiment that was performed in an anechoic chamber using an acoustic vector sensor array are presented to demonstrate the usefulness of the proposed self calibration techniques.      
### 12.Temporal-Logic-Based Intermittent, Optimal, and Safe Continuous-Time Learning for Trajectory Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2104.02547.pdf)
>  In this paper, we develop safe reinforcement-learning-based controllers for systems tasked with accomplishing complex missions that can be expressed as linear temporal logic specifications, similar to those required by search-and-rescue missions. We decompose the original mission into a sequence of tracking sub-problems under safety constraints. We impose the safety conditions by utilizing barrier functions to map the constrained optimal tracking problem in the physical space to an unconstrained one in the transformed space. Furthermore, we develop policies that intermittently update the control signal to solve the tracking sub-problems with reduced burden in the communication and computation resources. Subsequently, an actor-critic algorithm is utilized to solve the underlying Hamilton-Jacobi-Bellman equations. Finally, we support our proposed framework with stability proofs and showcase its efficacy via simulation results.      
### 13.Extraction of a computer-certified ODE solver  [ :arrow_down: ](https://arxiv.org/pdf/2104.02536.pdf)
>  Reliably determining system trajectories is essential in many analysis and control design approaches. To this end, an initial value problem has to be usually solved via numerical algorithms which rely on a certain software realization. Because software realizations can be error-prone, proof assistants may be used to verify the underlying mathematical concepts and corresponding algorithms. In this work we present a computer-certified formalization of the solution of the initial value problem of ordinary differential equations. The concepts are performed in the framework of constructive analysis and the proofs are written in the \texttt{Minlog} proof system. We show the extraction of a program, which solves an ODE numerically and provide some possible optimization regarding the efficiency. Finally, we provide numerical experiments to demonstrate how programs of a certain high level of abstraction can be obtained efficiently. The presented concepts may also be viewed as a part of preliminary work for the development of formalized nonlinear control theory, hence offering the possibility of computer-assisted controller design and program extraction for the controller implementation.      
### 14.LT-LM: a novel non-autoregressive language model for single-shot lattice rescoring  [ :arrow_down: ](https://arxiv.org/pdf/2104.02526.pdf)
>  Neural network-based language models are commonly used in rescoring approaches to improve the quality of modern automatic speech recognition (ASR) systems. Most of the existing methods are computationally expensive since they use autoregressive language models. We propose a novel rescoring approach, which processes the entire lattice in a single call to the model. The key feature of our rescoring policy is a novel non-autoregressive Lattice Transformer Language Model (LT-LM). This model takes the whole lattice as an input and predicts a new language score for each arc. Additionally, we propose the artificial lattices generation approach to incorporate a large amount of text data in the LT-LM training process. Our single-shot rescoring performs orders of magnitude faster than other rescoring methods in our experiments. It is more than 300 times faster than pruned RNNLM lattice rescoring and N-best rescoring while slightly inferior in terms of WER.      
### 15.Searching Efficient Model-guided Deep Network for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2104.02525.pdf)
>  Neural architecture search (NAS) has recently reshaped our understanding on various vision tasks. Similar to the success of NAS in high-level vision tasks, it is possible to find a memory and computationally efficient solution via NAS with highly competent denoising performance. However, the optimization gap between the super-network and the sub-architectures has remained an open issue in both low-level and high-level vision. In this paper, we present a novel approach to filling in this gap by connecting model-guided design with NAS (MoD-NAS) and demonstrate its application into image denoising. Specifically, we propose to construct a new search space under model-guided framework and develop more stable and efficient differential search strategies. MoD-NAS employs a highly reusable width search strategy and a densely connected search block to automatically select the operations of each layer as well as network width and depth via gradient descent. During the search process, the proposed MoG-NAS is capable of avoiding mode collapse due to the smoother search space designed under the model-guided framework. Experimental results on several popular datasets show that our MoD-NAS has achieved even better PSNR performance than current state-of-the-art methods with fewer parameters, lower number of flops, and less amount of testing time.      
### 16.An Initial Investigation for Detecting Partially Spoofed Audio  [ :arrow_down: ](https://arxiv.org/pdf/2104.02518.pdf)
>  All existing databases of spoofed speech contain attack data that is spoofed in its entirety. In practice, it is entirely plausible that successful attacks can be mounted with utterances that are only partially spoofed. By definition, partially-spoofed utterances contain a mix of both spoofed and bona fide segments, which will likely degrade the performance of countermeasures trained with entirely spoofed utterances. This hypothesis raises the obvious question: 'Can we detect partially-spoofed audio?' This paper introduces a new database of partially-spoofed data, named PartialSpoof, to help address this question. This new database enables us to investigate and compare the performance of countermeasures on both utterance- and segmental- level labels. Experimental results using the utterance-level labels reveal that the reliability of countermeasures trained to detect fully-spoofed data is found to degrade substantially when tested with partially-spoofed data, whereas training on partially-spoofed data performs reliably in the case of both fully- and partially-spoofed utterances. Additional experiments using segmental-level labels show that spotting injected spoofed segments included in an utterance is a much more challenging task even if the latest countermeasure models are used.      
### 17.Low Complexity Joint Impairment Mitigation of I/Q Modulator and PA Using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.02512.pdf)
>  Neural networks (NNs) for multiple hardware impairments mitigation of a realistic direct conversion transmitter are impractical due to high computational complexity. We propose two methods to reduce complexity without significant performance penalty. We first propose a novel attention residual learning NN, referred to as attention residual real-valued time-delay neural network (ARDEN), where trainable neuron-wise shortcut connections between the input and output layers allow to keep the attention always active. Furthermore, we implement a NN pruning algorithm that gradually removes connections corresponding to minimal weight magnitudes in each layer. Simulation and experimental results show that ARDEN with pruning achieves better performance for compensating frequency-dependent quadrature imbalance and power amplifier nonlinearity than other NN-based and Volterra-based models, while requiring less or similar complexity.      
### 18.Nonlinear Model Based Guidance with Deep Learning Based Target Trajectory Prediction Against Aerial Agile Attack Patterns  [ :arrow_down: ](https://arxiv.org/pdf/2104.02491.pdf)
>  In this work, we propose a novel missile guidance algorithm that combines deep learning based trajectory prediction with nonlinear model predictive control. Although missile guidance and threat interception is a well-studied problem, existing algorithms' performance degrades significantly when the target is pulling high acceleration attack maneuvers while rapidly changing its direction. We argue that since most threats execute similar attack maneuvers, these nonlinear trajectory patterns can be processed with modern machine learning methods to build high accuracy trajectory prediction algorithms. We train a long short-term memory network (LSTM) based on a class of simulated structured agile attack patterns, then combine this predictor with quadratic programming based nonlinear model predictive control (NMPC). Our method, named nonlinear model based predictive control with target acceleration predictions (NMPC-TAP), significantly outperforms compared approaches in terms of miss distance, for the scenarios where the target/threat is executing agile maneuvers.      
### 19.Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models  [ :arrow_down: ](https://arxiv.org/pdf/2104.02481.pdf)
>  Convolutional neural networks are showing promise in the automatic diagnosis of thoracic pathologies on chest x-rays. Their black-box nature has sparked many recent works to explain the prediction via input feature attribution methods (aka saliency methods). However, input feature attribution methods merely identify the importance of input regions for the prediction and lack semantic interpretation of model behavior. In this work, we first identify the semantics associated with internal units (feature maps) of the network. We proceed to investigate the following questions; Does a regression model that is only trained with COVID-19 severity scores implicitly learn visual patterns associated with thoracic pathologies? Does a network that is trained on weakly labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover, we investigate the effect of pretraining and data imbalance on the interpretability of learned features. In addition to the analysis, we propose semantic attribution to semantically explain each prediction. We present our findings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8) and COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset). The Code is publicly available.      
### 20.Speaker Diarization using Two-pass Leave-One-Out Gaussian PLDA Clustering of DNN Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2104.02469.pdf)
>  Many modern systems for speaker diarization, such as the recently-developed VBx approach, rely on clustering of DNN speaker embeddings followed by resegmentation. Two problems with this approach are that the DNN is not directly optimized for this task, and the parameters need significant retuning for different applications. We have recently presented progress in this direction with a Leave-One-Out Gaussian PLDA (LGP) clustering algorithm and an approach to training the DNN such that embeddings directly optimize performance of this scoring method. This paper presents a new two-pass version of this system, where the second pass uses finer time resolution to significantly improve overall performance. For the Callhome corpus, we achieve the first published error rate below 4\% without any task-dependent parameter tuning. We also show significant progress towards a robust single solution for multiple diarization tasks.      
### 21.Fast Design Space Exploration of Nonlinear Systems: Part II  [ :arrow_down: ](https://arxiv.org/pdf/2104.02464.pdf)
>  Nonlinear system design is often a multi-objective optimization problem involving search for a design that satisfies a number of predefined constraints. The design space is typically very large since it includes all possible system architectures with different combinations of components composing each architecture. In this article, we address nonlinear system design space exploration through a two-step approach encapsulated in a framework called Fast Design Space Exploration of Nonlinear Systems (ASSENT). In the first step, we use a genetic algorithm to search for system architectures that allow discrete choices for component values or else only component values for a fixed architecture. This step yields a coarse design since the system may or may not meet the target specifications. In the second step, we use an inverse design to search over a continuous space and fine-tune the component values with the goal of improving the value of the objective function. We use a neural network to model the system response. The neural network is converted into a mixed-integer linear program for active learning to sample component values efficiently. We illustrate the efficacy of ASSENT on problems ranging from nonlinear system design to design of electrical circuits. Experimental results show that ASSENT achieves the same or better value of the objective function compared to various other optimization techniques for nonlinear system design by up to 54%. We improve sample efficiency by 6-10x compared to reinforcement learning based synthesis of electrical circuits.      
### 22.Bias Correction in Deterministic Policy Gradient Using Robust MPC  [ :arrow_down: ](https://arxiv.org/pdf/2104.02413.pdf)
>  In this paper, we discuss the deterministic policy gradient using the Actor-Critic methods based on the linear compatible advantage function approximator, where the input spaces are continuous. When the policy is restricted by hard constraints, the exploration may not be Centred or Isotropic (non-CI). As a result, the policy gradient estimation can be biased. We focus on constrained policies based on Model Predictive Control (MPC) schemes and to address the bias issue, we propose an approximate Robust MPC approach accounting for the exploration. The RMPC-based policy ensures that a Centered and Isotropic (CI) exploration is approximately feasible. A posterior projection is used to ensure its exact feasibility, we formally prove that this approach does not bias the gradient estimation.      
### 23.ProsoBeast Prosody Annotation Tool  [ :arrow_down: ](https://arxiv.org/pdf/2104.02397.pdf)
>  The labelling of speech corpora is a laborious and time-consuming process. The ProsoBeast Annotation Tool seeks to ease and accelerate this process by providing an interactive 2D representation of the prosodic landscape of the data, in which contours are distributed based on their similarity. This interactive map allows the user to inspect and label the utterances. The tool integrates several state-of-the-art methods for dimensionality reduction and feature embedding, including variational autoencoders. The user can use these to find a good representation for their data. In addition, as most of these methods are stochastic, each can be used to generate an unlimited number of different prosodic maps. The web app then allows the user to seamlessly switch between these alternative representations in the annotation process. Experiments with a sample prosodically rich dataset have shown that the tool manages to find good representations of varied data and is helpful both for annotation and label correction. The tool is released as free software for use by the community.      
### 24.Newton-Type Optimal Thresholding Algorithms for Sparse Optimization Problems  [ :arrow_down: ](https://arxiv.org/pdf/2104.02371.pdf)
>  Sparse signals can be possibly reconstructed by an algorithm which merges a traditional nonlinear optimization method and a certain thresholding technique. Different from existing thresholding methods, a novel thresholding technique referred to as the optimal $k$-thresholding was recently proposed by Zhao [SIAM J Optim, 30(1), pp. 31-55, 2020]. This technique simultaneously performs the minimization of an error metric for the problem and thresholding of the iterates generated by the classic gradient method. In this paper, we propose the so-called Newton-type optimal $k$-thresholding (NTOT) algorithm which is motivated by the appreciable performance of both Newton-type methods and the optimal $k$-thresholding technique for signal recovery. The guaranteed performance (including convergence) of the proposed algorithms are shown in terms of suitable choices of the algorithmic parameters and the restricted isometry property (RIP) of the sensing matrix which has been widely used in the analysis of compressive sensing algorithms. The simulation results based on synthetic signals indicate that the proposed algorithms are stable and efficient for signal recovery.      
### 25.Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2104.02370.pdf)
>  This paper describes the IDLab submission for the text-independent task of the Short-duration Speaker Verification Challenge 2021 (SdSVC-21). This speaker verification competition focuses on short duration test recordings and cross-lingual trials, along with the constraint of limited availability of in-domain DeepMine Farsi training data. Currently, both Time Delay Neural Networks (TDNNs) and ResNets achieve state-of-the-art results in speaker verification. These architectures are structurally very different and the construction of hybrid networks looks a promising way forward. We introduce a 2D convolutional stem in a strong ECAPA-TDNN baseline to transfer some of the strong characteristics of a ResNet based model to this hybrid CNN-TDNN architecture. Similarly, we incorporate absolute frequency positional encodings in an SE-ResNet34 architecture. These learnable feature map biases along the frequency axis offer this architecture a straightforward way to exploit frequency positional information. We also propose a frequency-wise variant of Squeeze-Excitation (SE) which better preserves frequency-specific information when rescaling the feature maps. Both modified architectures significantly outperform their corresponding baseline on the SdSVC-21 evaluation data and the original VoxCeleb1 test set. A four system fusion containing the two improved architectures achieved a third place in the final SdSVC-21 Task 2 ranking.      
### 26.LEAP Submission for the Third DIHARD Diarization Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2104.02359.pdf)
>  The LEAP submission for DIHARD-III challenge is described in this paper. The proposed system is composed of a speech bandwidth classifier, and diarization systems fine-tuned for narrowband and wideband speech separately. We use an end-to-end speaker diarization system for the narrowband conversational telephone speech recordings. For the wideband multi-speaker recordings, we use a neural embedding based clustering approach, similar to the baseline system. The embeddings are extracted from a time-delay neural network (called x-vectors) followed by the graph based path integral clustering (PIC) approach. The LEAP system showed 24% and 18% relative improvements for Track-1 and Track-2 respectively over the baseline system provided by the organizers. This paper describes the challenge submission, the post-evaluation analysis and improvements observed on the DIHARD-III dataset.      
### 27.Bounded Inputs Total Energy Shaping for Mechanical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.02337.pdf)
>  Designing control systems with bounded input is a practical consideration since realizable physical systems are limited by the saturation of actuators. The actuators' saturation degrades the performance of the control system, and in extreme cases, the stability of the closed-loop system may be lost. However, actuator saturation is typically neglected in the design of control systems, with compensation being made in the form of over-designing the actuator or by post-analyzing the resulting system to ensure acceptable performance. The bounded input control of fully actuated systems has been investigated in multiple studies, but it is not generalized for under actuated mechanical systems. This article proposes a systematic framework for finding the upper bound of control effort in underactuated systems, based on interconnection and the damping assignment passivity based control (IDA-PBC) approach. The proposed method also offers design variables for the control law to be tuned, considering the actuator's limit. The major difficulty in finding the control input upper bounds is the velocity dependent kinetic energy related terms. Thus, the upper bound of velocity is computed using a suitable Lyapunov candidate as a function of closed-loop system parameters. The validity and application of the proposed method are investigated in detail through two benchmark systems.      
### 28.Pyramid U-Net for Retinal Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2104.02333.pdf)
>  Retinal blood vessel can assist doctors in diagnosis of eye-related diseases such as diabetes and hypertension, and its segmentation is particularly important for automatic retinal image analysis. However, it is challenging to segment these vessels structures, especially the thin capillaries from the color retinal image due to low contrast and ambiguousness. In this paper, we propose pyramid U-Net for accurate retinal vessel segmentation. In pyramid U-Net, the proposed pyramid-scale aggregation blocks (PSABs) are employed in both the encoder and decoder to aggregate features at higher, current and lower levels. In this way, coarse-to-fine context information is shared and aggregated in each block thus to improve the location of capillaries. To further improve performance, two optimizations including pyramid inputs enhancement and deep pyramid supervision are applied to PSABs in the encoder and decoder, respectively. For PSABs in the encoder, scaled input images are added as extra inputs. While for PSABs in the decoder, scaled intermediate outputs are supervised by the scaled segmentation labels. Extensive evaluations show that our pyramid U-Net outperforms the current state-of-the-art methods on the public DRIVE and CHASE-DB1 datasets.      
### 29.Detecting English Speech in the Air Traffic Control Voice Communication  [ :arrow_down: ](https://arxiv.org/pdf/2104.02332.pdf)
>  We launched a community platform for collecting the ATC speech world-wide in the ATCO2 project. Filtering out unseen non-English speech is one of the main components in the data processing pipeline. The proposed English Language Detection (ELD) system is based on the embeddings from Bayesian subspace multinomial model. It is trained on the word confusion network from an ASR system. It is robust, easy to train, and light weighted. We achieved 0.0439 equal-error-rate (EER), a 50% relative reduction as compared to the state-of-the-art acoustic ELD system based on x-vectors, in the in-domain scenario. Further, we achieved an EER of 0.1352, a 33% relative reduction as compared to the acoustic ELD, in the unseen language (out-of-domain) condition. We plan to publish the evaluation dataset from the ATCO2 project.      
### 30.Brain Tumors Classification for MR images based on Attention Guided Deep Learning Model  [ :arrow_down: ](https://arxiv.org/pdf/2104.02331.pdf)
>  In the clinical diagnosis and treatment of brain tumors, manual image reading consumes a lot of energy and time. In recent years, the automatic tumor classification technology based on deep learning has entered people's field of vision. Brain tumors can be divided into primary and secondary intracranial tumors according to their source. However, to our best knowledge, most existing research on brain tumors are limited to primary intracranial tumor images and cannot classify the source of the tumor. In order to solve the task of tumor source type classification, we analyze the existing technology and propose an attention guided deep convolution neural network (CNN) model. Meanwhile, the method proposed in this paper also effectively improves the accuracy of classifying the presence or absence of tumor. For the brain MR dataset, our method can achieve the average accuracy of 99.18% under ten-fold cross-validation for identifying the presence or absence of tumor, and 83.38% for classifying the source of tumor. Experimental results show that our method is consistent with the method of medical experts. It can assist doctors in achieving efficient clinical diagnosis of brain tumors.      
### 31.Self-Supervised Learning based CT Denoising using Pseudo-CT Image Pairs  [ :arrow_down: ](https://arxiv.org/pdf/2104.02326.pdf)
>  Recently, Self-supervised learning methods able to perform image denoising without ground truth labels have been proposed. These methods create low-quality images by adding random or Gaussian noise to images and then train a model for denoising. Ideally, it would be beneficial if one can generate high-quality CT images with only a few training samples via self-supervision. However, the performance of CT denoising is generally limited due to the complexity of CT noise. To address this problem, we propose a novel self-supervised learning-based CT denoising method. In particular, we train pre-train CT denoising and noise models that can predict CT noise from Low-dose CT (LDCT) using available LDCT and Normal-dose CT (NDCT) pairs. For a given test LDCT, we generate Pseudo-LDCT and NDCT pairs using the pre-trained denoising and noise models and then update the parameters of the denoising model using these pairs to remove noise in the test LDCT. To make realistic Pseudo LDCT, we train multiple noise models from individual images and generate the noise using the ensemble of noise models. We evaluate our method on the 2016 AAPM Low-Dose CT Grand Challenge dataset. The proposed ensemble noise model can generate realistic CT noise, and thus our method significantly improves the denoising performance existing denoising models trained by supervised- and self-supervised learning.      
### 32.NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling  [ :arrow_down: ](https://arxiv.org/pdf/2104.02321.pdf)
>  In this work, we introduce NU-Wave, the first neural audio upsampling model to produce waveforms of sampling rate 48kHz from coarse 16kHz or 24kHz inputs, while prior works could generate only up to 16kHz. NU-Wave is the first diffusion probabilistic model for audio super-resolution which is engineered based on neural vocoders. NU-Wave generates high-quality audio that achieves high performance in terms of signal-to-noise ratio (SNR), log-spectral distance (LSD), and accuracy of the ABX test. In all cases, NU-Wave outperforms the baseline models despite the substantially smaller model capacity (3.0M parameters) than baselines (5.4-21%). The audio samples of our model are available at <a class="link-external link-https" href="https://mindslab-ai.github.io/nuwave" rel="external noopener nofollow">this https URL</a>, and the code will be made available soon.      
### 33.Hyperspectral Image Denoising Based On Multi-Stream Denoising Network  [ :arrow_down: ](https://arxiv.org/pdf/2104.02304.pdf)
>  Hyperspectral images (HSIs) have been widely applied in many fields, such as military, agriculture, and environment monitoring. Nevertheless, HSIs commonly suffer from various types of noise during acquisition. Therefore, denoising is critical for HSI analysis and applications. In this paper, we propose a novel blind denoising method for HSIs based on Multi-Stream Denoising Network (MSDNet). Our network consists of the noise estimation subnetwork and denoising subnetwork. In the noise estimation subnetwork, a multiscale fusion module is designed to capture the noise from different scales. Then, the denoising subnetwork is utilized to obtain the final denoising image. The proposed MSDNet can obtain robust noise level estimation, which is capable of improving the performance of HSI denoising. Extensive experiments on HSI dataset demonstrate that the proposed method outperforms four closely related methods.      
### 34.Disentangled Non-Local Network for Hyperspectral and LiDAR Data Classification  [ :arrow_down: ](https://arxiv.org/pdf/2104.02302.pdf)
>  As the ground objects become increasingly complex, the classification results obtained by single source remote sensing data can hardly meet the application requirements. In order to tackle this limitation, we propose a simple yet effective attention fusion model based on Disentangled Non-local (DNL) network for hyperspectral and LiDAR data joint classification task. In this model, according to the spectral and spatial characteristics of HSI and LiDAR, a multiscale module and a convolutional neural network (CNN) are used to capture the spectral and spatial characteristics respectively. In addition, the extracted HSI and LiDAR features are fused through some operations to obtain the feature information more in line with the real situation. Finally, the above three data are fed into different branches of the DNL module, respectively. Extensive experiments on Houston dataset show that the proposed network is superior and more effective compared to several of the most advanced baselines in HSI and LiDAR joint classification missions.      
### 35.A clinical validation of VinDr-CXR, an AI system for detecting abnormal chest radiographs  [ :arrow_down: ](https://arxiv.org/pdf/2104.02256.pdf)
>  Computer-Aided Diagnosis (CAD) systems for chest radiographs using artificial intelligence (AI) have recently shown a great potential as a second opinion for radiologists. The performances of such systems, however, were mostly evaluated on a fixed dataset in a retrospective manner and, thus, far from the real performances in clinical practice. In this work, we demonstrate a mechanism for validating an AI-based system for detecting abnormalities on X-ray scans, VinDr-CXR, at the Phu Tho General Hospital{a provincial hospital in the North of Vietnam. The AI system was directly integrated into the Picture Archiving and Communication System (PACS) of the hospital after being trained on a fixed annotated dataset from other sources. The performance of the system was prospectively measured by matching and comparing the AI results with the radiology reports of 6,285 chest X-ray examinations extracted from the Hospital Information System (HIS) over the last two months of 2020. The normal/abnormal status of a radiology report was determined by a set of rules and served as the ground truth. Our system achieves an F1 score{the harmonic average of the recall and the precision{of 0.653 (95% CI 0.635, 0.671) for detecting any abnormalities on chest X-rays. Despite a significant drop from the in-lab performance, this result establishes a high level of confidence in applying such a system in real-life situations.      
### 36.In-Line Image Transformations for Imbalanced, Multiclass Computer Vision Classification of Lung Chest X-Rays  [ :arrow_down: ](https://arxiv.org/pdf/2104.02238.pdf)
>  Artificial intelligence (AI) is disrupting the medical field as advances in modern technology allow common household computers to learn anatomical and pathological features that distinguish between healthy and disease with the accuracy of highly specialized, trained physicians. Computer vision AI applications use medical imaging, such as lung chest X-Rays (LCXRs), to facilitate diagnoses by providing second-opinions in addition to a physician's or radiologist's interpretation. Considering the advent of the current Coronavirus disease (COVID-19) pandemic, LCXRs may provide rapid insights to indirectly aid in infection containment, however generating a reliably labeled image dataset for a novel disease is not an easy feat, nor is it of highest priority when combating a global pandemic. Deep learning techniques such as convolutional neural networks (CNNs) are able to select features that distinguish between healthy and disease states for other lung pathologies; this study aims to leverage that body of literature in order to apply image transformations that would serve to balance the lack of COVID-19 LCXR data. Furthermore, this study utilizes a simple CNN architecture for high-performance multiclass LCXR classification at 94 percent accuracy.      
### 37.Particle MPC for Uncertain and Learning-Based Control  [ :arrow_down: ](https://arxiv.org/pdf/2104.02213.pdf)
>  As robotic systems move from highly structured environments to open worlds, incorporating uncertainty from dynamics learning or state estimation into the control pipeline is essential for robust performance. In this paper we present a nonlinear particle model predictive control (PMPC) approach to control under uncertainty, which directly incorporates any particle-based uncertainty representation, such as those common in robotics. Our approach builds on scenario methods for MPC, but in contrast to existing approaches, which either constrain all or only the first timestep to share actions across scenarios, we investigate the impact of a \textit{partial consensus horizon}. Implementing this optimization for nonlinear dynamics by leveraging sequential convex optimization, our approach yields an efficient framework that can be tuned to the particular information gain dynamics of a system to mitigate both over-conservatism and over-optimism. We investigate our approach for two robotic systems across three problem settings: time-varying, partially observed dynamics; sensing uncertainty; and model-based reinforcement learning, and show that our approach improves performance over baselines in all settings.      
### 38.Insight about Detection, Prediction and Weather Impact of Coronavirus (Covid-19) using Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2104.02173.pdf)
>  The world is facing a tough situation due to the catastrophic pandemic caused by novel coronavirus (COVID-19). The number people affected by this virus are increasing exponentially day by day and the number has already crossed 6.4 million. As no vaccine has been discovered yet, the early detection of patients and isolation is the only and most effective way to reduce the spread of the virus. Detecting infected persons from chest X-Ray by using Deep Neural Networks, can be applied as a time and laborsaving solution. In this study, we tried to detect Covid-19 by classification of Covid-19, pneumonia and normal chest X-Rays. We used five different Convolutional Pre-Trained Neural Network models (VGG16, VGG19, Xception, InceptionV3 and Resnet50) and compared their performance. VGG16 and VGG19 shows precise performance in classification. Both models can classify between three kinds of X-Rays with an accuracy over 92%. Another part of our study was to find the impact of weather factors (temperature, humidity, sun hour and wind speed) on this pandemic using Decision Tree Regressor. We found that temperature, humidity and sun-hour jointly hold 85.88% impact on escalation of Covid-19 and 91.89% impact on death due to Covid-19 where humidity has 8.09% impact on death. We also tried to predict the death of an individual based on age, gender, country, and location due to COVID-19 using the LogisticRegression, which can predict death of an individual with a model accuracy of 94.40%.      
### 39.Strategy Synthesis for Partially-known Switched Stochastic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.02172.pdf)
>  We present a data-driven framework for strategy synthesis for partially-known switched stochastic systems. The properties of the system are specified using linear temporal logic (LTL) over finite traces (LTLf), which is as expressive as LTL and enables interpretations over finite behaviors. The framework first learns the unknown dynamics via Gaussian process regression. Then, it builds a formal abstraction of the switched system in terms of an uncertain Markov model, namely an Interval Markov Decision Process (IMDP), by accounting for both the stochastic behavior of the system and the uncertainty in the learning step. Then, we synthesize a strategy on the resulting IMDP that maximizes the satisfaction probability of the LTLf specification and is robust against all the uncertainties in the abstraction. This strategy is then refined into a switching strategy for the original stochastic system. We show that this strategy is near-optimal and provide a bound on its distance (error) to the optimal strategy. We experimentally validate our framework on various case studies, including both linear and non-linear switched stochastic systems.      
### 40.State Constrained Stochastic Optimal Control Using LSTMs  [ :arrow_down: ](https://arxiv.org/pdf/2104.02135.pdf)
>  In this paper, we propose a new methodology for state constrained stochastic optimal control (SOC) problems. The solution is based on past work in solving SOC problems using forward-backward stochastic differential equations (FBSDE). Our approach in solving the FBSDE utilizes a deep neural network (DNN), specifically Long Short-Term Memory (LSTM) networks. LSTMs are chosen to solve the FBSDE to address the curse of dimensionality, non-linearities, and long time horizons. In addition, the state constraints are incorporated using a hard penalty function, resulting in a controller that respects the constraint boundaries. Numerical instability that would be introduced by the penalty function is dealt with through an adaptive update scheme. The control design methodology is applicable to a large class of control problems. The performance and scalability of our proposed algorithm are demonstrated by numerical simulations.      
### 41.End-to-End Speaker-Attributed ASR with Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2104.02128.pdf)
>  This paper presents our recent effort on end-to-end speaker-attributed automatic speech recognition, which jointly performs speaker counting, speech recognition and speaker identification for monaural multi-talker audio. Firstly, we thoroughly update the model architecture that was previously designed based on a long short-term memory (LSTM)-based attention encoder decoder by applying transformer architectures. Secondly, we propose a speaker deduplication mechanism to reduce speaker identification errors in highly overlapped regions. Experimental results on the LibriSpeechMix dataset shows that the transformer-based architecture is especially good at counting the speakers and that the proposed model reduces the speaker-attributed word error rate by 47% over the LSTM-based baseline. Furthermore, for the LibriCSS dataset, which consists of real recordings of overlapped speech, the proposed model achieves concatenated minimum-permutation word error rates of 11.9% and 16.3% with and without target speaker profiles, respectively, both of which are the state-of-the-art results for LibriCSS with the monaural setting.      
### 42.Scaling to Many Languages with a Triaged Multilingual Text-Dependent and Text-Independent Speaker Verification System  [ :arrow_down: ](https://arxiv.org/pdf/2104.02125.pdf)
>  In this work we study some of the challenges associated with scaling speaker recognition systems to multiple languages. To the best of our knowledge, this is the first study of speaker verification systems at the scale of 46 languages. Training models for each of the many languages can be time and energy demanding in addition to costly. Low resource languages present additional difficulties. The problem is framed from the perspective of using a smart speaker device with interactions consisting of a wake-up keyword (text-dependent) followed by a speech query (text-independent). <br>We examine the use of a hybrid setup consisting of multilingual text-dependent and text-independent components. Experimental evidence suggests that training on multiple languages can generalize to unseen varieties while maintaining performance on seen varieties. We also found that it can reduce computational requirements for training models by an order of magnitude. Furthermore, during model inference on English data, we observe that leveraging a triage framework can reduce the number of calls to the more computationally expensive text-independent system by 73% (and reduce latency by 60%) while maintaining an EER no worse than the text-independent setup.      
### 43.Optimization of vertically mounted agrivoltaic systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.02124.pdf)
>  Agrivoltaic systems represent a key technology for reaching sustainable development goals reducing the completion of land for food versus land for energy. Moreover, agrivoltaic systems are at the centre of the nexus between electricity production, crop production, and irrigation water saving. In this study, an optimization model for vertically mounted agrivoltaic systems with bifacial photovoltaic models is developed. The model combines three main submodels: solar radiation and shadings, photovoltaic, and crop yield. Validation of the submodels is performed showing good agreement with measured data and commercial software. The optimization model is set as multi objective to explore the trade-offs between competing agrivoltaic key performance indicators. The results shows that the row distance between bifacial photovoltaic modules structure affects significantly the photosyntetically active radiation distribution by reducing the crop yield of potato and oat of about 50% by passing from 20 m to 5 m. The implementation of agrivoltaic system for the investigated crops at the chosen location shows a land equivalent ratio above 1.2 that justify the technology for reaching the country sustainability goals.      
### 44.A Generalized LinDistFlow Model for Power Flow Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2104.02118.pdf)
>  This paper proposes a new linear power flow model for distribution system with accurate voltage magnitude estimates. The new model can be seen as a generalization of LinDistFlow model to multiphase distribution system with generic network topology (radial or meshed) around arbitrary linearization point. We have shown that the approximation quality of the proposed model strictly dominates that of the fixed-point linearization (FPL) method, a popular linear power flow model for distribution system analysis, when both are linearized around zero injection point. Numerical examples using standard IEEE test feeders are provided to illustrate the effectiveness of the proposed model as well as the improvement in accuracy over existing methods when linearized around non-zero injection points.      
### 45.Robust Tube-Based Decentralized Nonlinear Model Predictive Control of an Autonomous Tractor-Trailer System  [ :arrow_down: ](https://arxiv.org/pdf/2104.02063.pdf)
>  This paper addresses the trajectory tracking problem of an autonomous tractor-trailer system by using a decentralized control approach. A fully decentralized model predictive controller is designed in which interactions between subsystems are neglected and assumed to be perturbations to each other. In order to have a robust design, a tube-based approach is proposed to handle the differences between the nominal model and real system. Nonlinear moving horizon estimation is used for the state and parameter estimation after each new measurement, and the estimated values are fed to robust tube-based decentralized nonlinear model predictive controller. The proposed control scheme is capable of driving the tractor-trailer system to any desired trajectory ensuring high control accuracy and robustness against neglected subsystem interactions and environmental disturbances. The experimental results show an accurate trajectory tracking performance on a bumpy grass field.      
### 46.Toward Generating Synthetic CT Volumes using a 3D-Conditional Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2104.02060.pdf)
>  We present a novel conditional Generative Adversarial Network (cGAN) architecture that is capable of generating 3D Computed Tomography scans in voxels from noisy and/or pixelated approximations and with the potential to generate full synthetic 3D scan volumes. We believe conditional cGAN to be a tractable approach to generate 3D CT volumes, even though the problem of generating full resolution deep fakes is presently impractical due to GPU memory limitations. We present results for autoencoder, denoising, and depixelating tasks which are trained and tested on two novel COVID19 CT datasets. Our evaluation metrics, Peak Signal to Noise ratio (PSNR) range from 12.53 - 46.46 dB, and the Structural Similarity index ( SSIM) range from 0.89 to 1.      
### 47.Distributed Deep Reinforcement Learning for Collaborative Spectrum Sharing  [ :arrow_down: ](https://arxiv.org/pdf/2104.02059.pdf)
>  Spectrum sharing among users is a fundamental problem in the management of any wireless network. In this paper, we discuss the problem of distributed spectrum collaboration without central management under general unknown channels. Since the cost of communication, coordination and control is rapidly increasing with the number of devices and the expanding bandwidth used there is an obvious need to develop distributed techniques for spectrum collaboration where no explicit signaling is used. In this paper, we combine game-theoretic insights with deep Q-learning to provide a novel asymptotically optimal solution to the spectrum collaboration problem. We propose a deterministic distributed deep reinforcement learning(D3RL) mechanism using a deep Q-network (DQN). It chooses the channels using the Q-values and the channel loads while limiting the options available to the user to a few channels with the highest Q-values and among those, it selects the least loaded channel. Using insights from both game theory and combinatorial optimization we show that this technique is asymptotically optimal for large overloaded networks. The selected channel and the outcome of the successful transmission are fed back into the learning of the deep Q-network to incorporate it into the learning of the Q-values. We also analyzed performance to understand the behavior of D3RL in differ      
### 48.Localizing Visual Sounds the Hard Way  [ :arrow_down: ](https://arxiv.org/pdf/2104.02691.pdf)
>  The objective of this work is to localize sound sources that are visible in a video without using manual annotations. Our key technical contribution is to show that, by training the network to explicitly discriminate challenging image fragments, even for images that do contain the object emitting the sound, we can significantly boost the localization performance. We do so elegantly by introducing a mechanism to mine hard samples and add them to a contrastive learning formulation automatically. We show that our algorithm achieves state-of-the-art performance on the popular Flickr SoundNet dataset. Furthermore, we introduce the VGG-Sound Source (VGG-SS) benchmark, a new set of annotations for the recently-introduced VGG-Sound dataset, where the sound sources visible in each video clip are explicitly marked with bounding box annotations. This dataset is 20 times larger than analogous existing ones, contains 5K videos spanning over 200 categories, and, differently from Flickr SoundNet, is video-based. On VGG-SS, we also show that our algorithm achieves state-of-the-art performance against several baselines.      
### 49.UNBLOCK: Low Complexity Transient Blockage Recovery for Mobile mm-Wave Devices  [ :arrow_down: ](https://arxiv.org/pdf/2104.02658.pdf)
>  Directional radio beams are used in the mm-Wave band to combat the high path loss. The mm-Wave band also suffers from high penetration losses from drywall, wood, glass, concrete, etc., and also the human body. Hence, as a mobile user moves, the Line of Sight (LoS) path between the mobile and the Base Station (BS) can be blocked by objects interposed in the path, causing loss of the link. A mobile with a lost link will need to be re-acquired as a new user by initial access, a process that can take up to a second, causing disruptions to applications. UNBLOCK is a protocol that allows a mobile to recover from transient blockages, such as those caused by a human hand or another human walking into the line of path or other temporary occlusions by objects, which typically disappear within the order of $100$ ms, without having to go through re-acquisition. UNBLOCK is based on extensive experimentation in office type environments which has shown that while a LoS path is blocked, there typically exists a Non-LoS path, i.e., a reflected path through scatterers, with a loss within about $10$ dB of the LoS path. UNBLOCK proactively keeps such a NLoS path in reserve, to be used when blockage happens, typically without any warning. UNBLOCK uses this NLoS path to maintain time-synchronization with the BS until the blockage disappears, as well as to search for a better NLoS path if available. When the transient blockage disappears, it reestablishes LoS communication at the epochs that have been scheduled by the BS for communication with the mobile.      
### 50.Collaborative Learning to Generate Audio-Video Jointly  [ :arrow_down: ](https://arxiv.org/pdf/2104.02656.pdf)
>  There have been a number of techniques that have demonstrated the generation of multimedia data for one modality at a time using GANs, such as the ability to generate images, videos, and audio. However, so far, the task of multi-modal generation of data, specifically for audio and videos both, has not been sufficiently well-explored. Towards this, we propose a method that demonstrates that we are able to generate naturalistic samples of video and audio data by the joint correlated generation of audio and video modalities. The proposed method uses multiple discriminators to ensure that the audio, video, and the joint output are also indistinguishable from real-world samples. We present a dataset for this task and show that we are able to generate realistic samples. This method is validated using various standard metrics such as Inception Score, Frechet Inception Distance (FID) and through human evaluation.      
### 51.A Modified Convolutional Network for Auto-encoding based on Pattern Theory Growth Function  [ :arrow_down: ](https://arxiv.org/pdf/2104.02651.pdf)
>  This brief paper reports the shortcoming of a variant of convolutional neural network whose components are developed based on the pattern theory framework.      
### 52.Weakly-supervised Audio-visual Sound Source Detection and Separation  [ :arrow_down: ](https://arxiv.org/pdf/2104.02606.pdf)
>  Learning how to localize and separate individual object sounds in the audio channel of the video is a difficult task. Current state-of-the-art methods predict audio masks from artificially mixed spectrograms, known as Mix-and-Separate framework. We propose an audio-visual co-segmentation, where the network learns both what individual objects look and sound like, from videos labeled with only object labels. Unlike other recent visually-guided audio source separation frameworks, our architecture can be learned in an end-to-end manner and requires no additional supervision or bounding box proposals. Specifically, we introduce weakly-supervised object segmentation in the context of sound separation. We also formulate spectrogram mask prediction using a set of learned mask bases, which combine using coefficients conditioned on the output of object segmentation , a design that facilitates separation. Extensive experiments on the MUSIC dataset show that our proposed approach outperforms state-of-the-art methods on visually guided sound source separation and sound denoising.      
### 53.Principal Component Analysis Applied to Gradient Fields in Band Gap Optimization Problems for Metamaterials  [ :arrow_down: ](https://arxiv.org/pdf/2104.02588.pdf)
>  A promising technique for the spectral design of acoustic metamaterials is based on the formulation of suitable constrained nonlinear optimization problems. Unfortunately, the straightforward application of classical gradient-based iterative optimization algorithms to the numerical solution of such problems is typically highly demanding, due to the complexity of the underlying physical models. Nevertheless, supervised machine learning techniques can reduce such a computational effort, e.g., by replacing the original objective functions of such optimization problems with more-easily computable approximations. In this framework, the present article describes the application of a related unsupervised machine learning technique, namely, principal component analysis, to approximate the gradient of the objective function of a band gap optimization problem for an acoustic metamaterial, with the aim of making the successive application of a gradient-based iterative optimization algorithm faster. Numerical results show the effectiveness of the proposed method.      
### 54.Comparing CTC and LFMMI for out-of-domain adaptation of wav2vec 2.0 acoustic model  [ :arrow_down: ](https://arxiv.org/pdf/2104.02558.pdf)
>  In this work, we investigate if the wav2vec 2.0 self-supervised pretraining helps mitigate the overfitting issues with connectionist temporal classification (CTC) training to reduce its performance gap with flat-start lattice-free MMI (E2E-LFMMI) for automatic speech recognition with limited training data. Towards that objective, we use the pretrained wav2vec 2.0 BASE model and fine-tune it on three different datasets including out-of-domain (Switchboard) and cross-lingual (Babel) scenarios. Our results show that for supervised adaptation of the wav2vec 2.0 model, both E2E-LFMMI and CTC achieve similar results; significantly outperforming the baselines trained only with supervised data. Fine-tuning the wav2vec 2.0 model with E2E-LFMMI and CTC we obtain the following relative WER improvements over the supervised baseline trained with E2E-LFMMI. We get relative improvements of 40% and 44% on the clean-set and 64% and 58% on the test set of Librispeech (100h) respectively. On Switchboard (300h) we obtain relative improvements of 33% and 35% respectively. Finally, for Babel languages, we obtain relative improvements of 26% and 23% on Swahili (38h) and 18% and 17% on Tagalog (84h) respectively.      
### 55.Fourier Image Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2104.02555.pdf)
>  Transformer architectures show spectacular performance on NLP tasks and have recently also been used for tasks such as image completion or image classification. Here we propose to use a sequential image representation, where each prefix of the complete sequence describes the whole image at reduced resolution. Using such Fourier Domain Encodings (FDEs), an auto-regressive image completion task is equivalent to predicting a higher resolution output given a low-resolution input. Additionally, we show that an encoder-decoder setup can be used to query arbitrary Fourier coefficients given a set of Fourier domain observations. We demonstrate the practicality of this approach in the context of computed tomography (CT) image reconstruction. In summary, we show that Fourier Image Transformer (FIT) can be used to solve relevant image analysis tasks in Fourier space, a domain inherently inaccessible to convolutional architectures.      
### 56.Optimal Transport-based Adaptation in Dysarthric Speech Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2104.02535.pdf)
>  In many real-world applications, the mismatch between distributions of training data (source) and test data (target) significantly degrades the performance of machine learning algorithms. In speech data, causes of this mismatch include different acoustic environments or speaker characteristics. In this paper, we address this issue in the challenging context of dysarthric speech, by multi-source domain/speaker adaptation (MSDA/MSSA). Specifically, we propose the use of an optimal-transport based approach, called MSDA via Weighted Joint Optimal Transport (MSDA-WDJOT). We confront the mismatch problem in dysarthria detection for which the proposed approach outperforms both the Baseline and the state-of-the-art MSDA models, improving the detection accuracy of 0.9% over the best competitor method. We then employ MSDA-WJDOT for dysarthric speaker adaptation in command speech recognition. This provides a Command Error Rate relative reduction of 16% and 7% over the baseline and the best competitor model, respectively. Interestingly, MSDA-WJDOT provides a similarity score between the source and the target, i.e. between speakers in this case. We leverage this similarity measure to define a Dysarthric and Healthy score of the target speaker and diagnose the dysarthria with an accuracy of 95%.      
### 57.Resolution-enhanced OCT and expanded framework of information capacity and resolution in coherent imaging  [ :arrow_down: ](https://arxiv.org/pdf/2104.02531.pdf)
>  Spatial resolution in optical microscopy has traditionally been treated as a fixed parameter of the optical system. Here, we present an approach to enhance transverse resolution in beam-scanned optical coherence tomography (OCT) beyond its aberration-free resolution limit, without any modification to the optical system. Based on the theorem of invariance of information capacity, resolution-enhanced (RE)-OCT navigates the exchange of information between resolution and signal-to-noise ratio (SNR) by exploiting efficient noise suppression via coherent averaging and a simple computational bandwidth expansion procedure. We demonstrate a resolution enhancement of 1.5 times relative to the aberration-free limit while maintaining comparable SNR in silicone phantom. We show that RE-OCT can significantly enhance the visualization of fine microstructural features in collagen gel and ex vivo mouse brain. Beyond RE-OCT, our analysis in the spatial-frequency domain leads to an expanded framework of information capacity and resolution in coherent imaging that contributes new implications to the theory of coherent imaging. RE-OCT can be readily implemented on most OCT systems worldwide, immediately unlocking information that is beyond their current imaging capabilities, and so has the potential for widespread impact in the numerous areas in which OCT is utilized, including the basic sciences and translational medicine.      
### 58.Neural Process for Black-Box Model Optimization Under Bayesian Framework  [ :arrow_down: ](https://arxiv.org/pdf/2104.02487.pdf)
>  There are a large number of optimization problems in physical models where the relationships between model parameters and outputs are unknown or hard to track. These models are named as black-box models in general because they can only be viewed in terms of inputs and outputs, without knowledge of the internal workings. Optimizing the black-box model parameters has become increasingly expensive and time consuming as they have become more complex. Hence, developing effective and efficient black-box model optimization algorithms has become an important task. One powerful algorithm to solve such problem is Bayesian optimization, which can effectively estimates the model parameters that lead to the best performance, and Gaussian Process (GP) has been one of the most widely used surrogate model in Bayesian optimization. However, the time complexity of GP scales cubically with respect to the number of observed model outputs, and GP does not scale well with large parameter dimension either. Consequently, it has been challenging for GP to optimize black-box models that need to query many observations and/or have many parameters. To overcome the drawbacks of GP, in this study, we propose a general Bayesian optimization algorithm that employs a Neural Process (NP) as the surrogate model to perform black-box model optimization, namely, Neural Process for Bayesian Optimization (NPBO). In order to validate the benefits of NPBO, we compare NPBO with four benchmark approaches on a power system parameter optimization problem and a series of seven benchmark Bayesian optimization problems. The results show that the proposed NPBO performs better than the other four benchmark approaches on the power system parameter optimization problem and competitively on the seven benchmark problems.      
### 59.Machine Learning based COVID-19 Detection from Smartphone Recordings: Cough, Breath and Speech  [ :arrow_down: ](https://arxiv.org/pdf/2104.02477.pdf)
>  We present an experimental investigation into the automatic detection of COVID-19 from smartphone recordings of coughs, breaths and speech. This type of screening is attractive because it is non-contact, does not require specialist medical expertise or laboratory facilities and can easily be deployed on inexpensive consumer hardware. We base our experiments on two datasets, Coswara and ComParE, containing recordings of coughing, breathing and speech from subjects around the globe. We have considered seven machine learning classifiers and all of them are trained and evaluated using leave-p-out cross-validation. For the Coswara data, the highest AUC of 0.92 was achieved using a Resnet50 architecture on breaths. For the ComParE data, the highest AUC of 0.93 was achieved using a k-nearest neighbours (KNN) classifier on cough recordings after selecting the best 12 features using sequential forward selection (SFS) and the highest AUC of 0.91 was also achieved on speech by a multilayer perceptron (MLP) when using SFS to select the best 23 features. We conclude that among all vocal audio, coughs carry the strongest COVID-19 signature followed by breath and speech. Although these signatures are not perceivable by human ear, machine learning based COVID-19 detection is possible from vocal audio recorded via smartphone.      
### 60.Solving Large Scale Quadratic Constrained Basis Pursuit  [ :arrow_down: ](https://arxiv.org/pdf/2104.02475.pdf)
>  Inspired by alternating direction method of multipliers and the idea of operator splitting, we propose a efficient algorithm for solving large-scale quadratically constrained basis pursuit. Experimental results show that the proposed algorithm can achieve 50~~100 times speedup when compared with the baseline interior point algorithm implemented in CVX.      
### 61.All-Optical Image Identification with Programmable Matrix Transformation  [ :arrow_down: ](https://arxiv.org/pdf/2104.02474.pdf)
>  An optical neural network is proposed and demonstrated with programmable matrix transformation and nonlinear activation function of photodetection (square-law detection). Based on discrete phase-coherent spatial modes, the dimensionality of programmable optical matrix operations is 30~37, which is implemented by spatial light modulators. With this architecture, all-optical classification tasks of handwritten digits, objects and depth images are performed on the same platform with high accuracy. Due to the parallel nature of matrix multiplication, the processing speed of our proposed architecture is potentially as high as7.4T~74T FLOPs per second (with 10~100GHz detector)      
### 62.Depth Evaluation for Metal Surface Defects by Eddy Current Testing using Deep Residual Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.02472.pdf)
>  Eddy current testing (ECT) is an effective technique in the evaluation of the depth of metal surface defects. However, in practice, the evaluation primarily relies on the experience of an operator and is often carried out by manual inspection. In this paper, we address the challenges of automatic depth evaluation of metal surface defects by virtual of state-of-the-art deep learning (DL) techniques. The main contributions are three-fold. Firstly, a highly-integrated portable ECT device is developed, which takes advantage of an advanced field programmable gate array (Zynq-7020 system on chip) and provides fast data acquisition and in-phase/quadrature demodulation. Secondly, a dataset, termed as MDDECT, is constructed using the ECT device by human operators and made openly available. It contains 48,000 scans from 18 defects of different depths and lift-offs. Thirdly, the depth evaluation problem is formulated as a time series classification problem, and various state-of-the-art 1-d residual convolutional neural networks are trained and evaluated on the MDDECT dataset. A 38-layer 1-d ResNeXt achieves an accuracy of 93.58% in discriminating the surface defects in a stainless steel sheet. The depths of the defects vary from 0.3 mm to 2.0 mm in a resolution of 0.1 mm. In addition, results show that the trained ResNeXt1D-38 model is immune to lift-off signals.      
### 63.A Facial Feature Discovery Framework for Race Classification Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2104.02471.pdf)
>  Race classification is a long-standing challenge in the field of face image analysis. The investigation of salient facial features is an important task to avoid processing all face parts. Face segmentation strongly benefits several face analysis tasks, including ethnicity and race classification. We propose a raceclassification algorithm using a prior face segmentation framework. A deep convolutional neural network (DCNN) was used to construct a face segmentation model. For training the DCNN, we label face images according to seven different classes, that is, nose, skin, hair, eyes, brows, back, and mouth. The DCNN model developed in the first phase was used to create segmentation results. The probabilistic classification method is used, and probability maps (PMs) are created for each semantic class. We investigated five salient facial features from among seven that help in race classification. Features are extracted from the PMs of five classes, and a new model is trained based on the DCNN. We assessed the performance of the proposed race classification method on four standard face datasets, reporting superior results compared with previous studies.      
### 64.On Performance Loss of DOA Measurement Using Massive MIMO Receiver with Mixed-ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2104.02447.pdf)
>  High hardware cost and high power consumption of massive multiple-input and multiple output (MIMO) are still two challenges for the future wireless communications including beyond 5G. Adopting the low-resolution analog-to-digital converter (ADC) is viewed as a promising solution. Additionally, the direction of arrival (DOA) estimation is an indispensable technology for beam alignment and tracking in massive MIMO systems. Thus, in this paper, the performance of DOA estimation for massive MIMO receive array with mixed-ADC structure is first investigated, where one part of radio frequency (RF) chains are connected with high-resolution ADCs and the remaining ones are connected with low-resolution ADCs. Moreover, the Cramer-Rao lower bound (CRLB) for this architecture is derived based on the additive quantization noise model approximation for the effect of low-resolution ADCs. Then, the root-MUSIC method is designed for such a receive structure. Eventually, a performance loss factor and the associated energy efficiency factor is defined for analysis in detail. Simulation results find that a mixed-ADC architecture can strike a good balance among RMSE performance, circuit cost and energy efficiency. More importantly, just 1-4 bits of low-resolution ADCs can achieve a satisfactory performance for DOA measurement.      
### 65.MPC-based Reinforcement Learning for Economic Problems with Application to Battery Storage  [ :arrow_down: ](https://arxiv.org/pdf/2104.02411.pdf)
>  In this paper, we are interested in optimal control problems with purely economic costs, which often yield optimal policies having a (nearly) bang-bang structure. We focus on policy approximations based on Model Predictive Control (MPC) and the use of the deterministic policy gradient method to optimize the MPC closed-loop performance in the presence of unmodelled stochasticity or model error. When the policy has a (nearly) bang-bang structure, we observe that the policy gradient method can struggle to produce meaningful steps in the policy parameters. To tackle this issue, we propose a homotopy strategy based on the interior-point method, providing a relaxation of the policy during the learning. We investigate a specific well-known battery storage problem, and show that the proposed method delivers a homogeneous and faster learning than a classical policy gradient approach.      
### 66.Using Voice and Biofeedback to Predict User Engagement during Requirements Interviews  [ :arrow_down: ](https://arxiv.org/pdf/2104.02410.pdf)
>  Capturing users engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collect and analyze users feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in bespoke software development, or in contexts in which one needs to gather information from specific users. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this paper, we propose to utilize biometric data, in terms of physiological and voice features, to complement interviews with information about the engagement of the user on the discussed product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users' engagement by training supervised machine learning algorithms on biometric data, and that voice features alone can be sufficiently effective. The performance of the prediction algorithms is maximised when pre-processing the training data with the synthetic minority oversampling technique (SMOTE). The results of our work suggest that biofeedback and voice analysis can be used to facilitate prioritization of requirements oriented to product improvement, and to steer the interview based on users' engagement. Furthermore, the usage of voice features can be particularly helpful for emotion-aware requirements elicitation in remote communication, either performed by human analysts or voice-based chatbots.      
### 67.Magnetization Transfer-Mediated MR Fingerprinting  [ :arrow_down: ](https://arxiv.org/pdf/2104.02406.pdf)
>  Purpose: Magnetization transfer (MT) and inhomogeneous MT (ihMT) contrasts are used in MRI to provide information about macromolecular tissue content. In particular, MT is sensitive to macromolecules and ihMT appears to be specific to myelinated tissue. This study proposes a technique to characterize MT and ihMT properties from a single acquisition, producing both semiquantitative contrast ratios, and quantitative parameter maps. <br>Theory and Methods: Building upon previous work that uses multiband radiofrequency (RF) pulses to efficiently generate ihMT contrast, we propose a cyclic-steady-state approach that cycles between multiband and single-band pulses to boost the achieved contrast. Resultant time-variable signals are reminiscent of a magnetic resonance fingerprinting (MRF) acquisition, except that the signal fluctuations are entirely mediated by magnetization transfer effects. A dictionary-based low-rank inversion method is used to reconstruct the resulting images and to produce both semiquantitative MT ratio (MTR) and ihMT ratio (ihMTR) maps, as well as quantitative parameter estimates corresponding to an ihMT tissue model. <br>Results: Phantom and in vivo brain data acquired at 1.5T demonstrate the expected contrast trends, with ihMTR maps showing contrast more specific to white matter (WM), as has been reported by others. Quantitative estimation of semisolid fraction and dipolar T1 was also possible and yielded measurements consistent with literature values in the brain. <br>Conclusions: By cycling between multiband and single-band pulses, an entirely magnetization transfer mediated 'fingerprinting' method was demonstrated. This proof-of-concept approach can be used to generate semiquantitative maps and quantitatively estimate some macromolecular specific tissue parameters.      
### 68.Towards Consistent Hybrid HMM Acoustic Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2104.02387.pdf)
>  High-performance hybrid automatic speech recognition (ASR) systems are often trained with clustered triphone outputs, and thus require a complex training pipeline to generate the clustering. The same complex pipeline is often utilized in order to generate an alignment for use in frame-wise cross-entropy training. In this work, we propose a flat-start factored hybrid model trained by modeling the full set of triphone states explicitly without relying on clustering methods. This greatly simplifies the training of new models. Furthermore, we study the effect of different alignments used for Viterbi training. Our proposed models achieve competitive performance on the Switchboard task compared to systems using clustered triphones and other flat-start models in the literature.      
### 69.Using Kalman Filter The Right Way: Noise Estimation Is Not Optimal  [ :arrow_down: ](https://arxiv.org/pdf/2104.02372.pdf)
>  Determining the noise parameters of a Kalman Filter (KF) has been researched for decades. The research focuses on estimation of the noise under various conditions, since noise estimation is considered equivalent to errors minimization. However, we show that even a seemingly small violation of KF assumptions can significantly modify the effective noise, breaking the equivalence between the tasks and making noise estimation a highly sub-optimal strategy. In particular, whoever tests a new learning-based algorithm in comparison to a (variant of) KF with standard parameters tuning, essentially conducts an unfair comparison between an optimized algorithm and a non-optimized one. We suggest a method (based on Cholesky decomposition) to apply gradient-based optimization efficiently to the symmetric and positive-definite (SPD) parameters of KF, so that KF can be optimized similarly to common neural networks. The benefits of this method are demonstrated for both Radar tracking and video tracking. For Radar tracking we also show how a non-linear neural-network-based model can seem to reduce the tracking errors significantly compared to a KF - and how this reduction entirely vanishes once the KF is optimized. Through a detailed case-study, we also demonstrate that KF requires non-trivial design-decisions to be made, and that parameters optimization makes KF more robust to these decisions.      
### 70.SimMBM Channel Simulator for Media-Based Modulation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.02336.pdf)
>  Media-based modulation (MBM), exploiting rich scattering properties of transmission environments via different radiation patterns of a single reconfigurable antenna (RA), has brought new insights into future communication systems. In this study, considering this innovative transmission principle, we introduce the realistic, two-dimensional (2D), and open-source SimMBM channel simulator to support various applications of MBM systems at sub-6 GHz frequency band in different environments.      
### 71.Taming Adversarial Robustness via Abstaining  [ :arrow_down: ](https://arxiv.org/pdf/2104.02334.pdf)
>  In this work, we consider a binary classification problem and cast it into a binary hypothesis testing framework, where the observations can be perturbed by an adversary. To improve the adversarial robustness of a classifier, we include an abstaining option, where the classifier abstains from taking a decision when it has low confidence about the prediction. We propose metrics to quantify the nominal performance of a classifier with abstaining option and its robustness against adversarial perturbations. We show that there exist a tradeoff between the two metrics regardless of what method is used to choose the abstaining region. Our results imply that the robustness of a classifier with abstaining can only be improved at the expense of its nominal performance. Further, we provide necessary conditions to design the abstaining region for a 1-dimensional binary classification problem. We validate our theoretical results on the MNIST dataset, where we numerically show that the tradeoff between performance and robustness also exist for the general multi-class classification problems.      
### 72.Efficient Video Compression via Content-Adaptive Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2104.02322.pdf)
>  Video compression is a critical component of Internet video delivery. Recent work has shown that deep learning techniques can rival or outperform human-designed algorithms, but these methods are significantly less compute and power-efficient than existing codecs. This paper presents a new approach that augments existing codecs with a small, content-adaptive super-resolution model that significantly boosts video quality. Our method, SRVC, encodes video into two bitstreams: (i) a content stream, produced by compressing downsampled low-resolution video with the existing codec, (ii) a model stream, which encodes periodic updates to a lightweight super-resolution neural network customized for short segments of the video. SRVC decodes the video by passing the decompressed low-resolution video frames through the (time-varying) super-resolution model to reconstruct high-resolution video frames. Our results show that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of H.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep learning-based video compression scheme. SRVC runs at 90 frames per second on a NVIDIA V100 GPU.      
### 73.MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for Discriminative Music Modeling on Raw Waveforms  [ :arrow_down: ](https://arxiv.org/pdf/2104.02309.pdf)
>  In this work, we aim to improve the expressive capacity of waveform-based discriminative music networks by modeling both sequential (temporal) and hierarchical information in an efficient end-to-end architecture. We present MuSLCAT, or Multi-scale and Multi-level Convolutional Attention Transformer, a novel architecture for learning robust representations of complex music tags directly from raw waveform recordings. We also introduce a lightweight variant of MuSLCAT called MuSLCAN, short for Multi-scale and Multi-level Convolutional Attention Network. Both MuSLCAT and MuSLCAN model features from multiple scales and levels by integrating a frontend-backend architecture. The frontend targets different frequency ranges while modeling long-range dependencies and multi-level interactions by using two convolutional attention networks with attention-augmented convolution (AAC) blocks. The backend dynamically recalibrates multi-scale and level features extracted from the frontend by incorporating self-attention. The difference between MuSLCAT and MuSLCAN is their backend components. MuSLCAT's backend is a modified version of BERT. While MuSLCAN's is a simple AAC block. We validate the proposed MuSLCAT and MuSLCAN architectures by comparing them to state-of-the-art networks on four benchmark datasets for music tagging and genre recognition. Our experiments show that MuSLCAT and MuSLCAN consistently yield competitive results when compared to state-of-the-art waveform-based models yet require considerably fewer parameters.      
### 74.Binary Neural Network for Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2104.02306.pdf)
>  Although deep neural networks are successful for many tasks in the speech domain, the high computational and memory costs of deep neural networks make it difficult to directly deploy highperformance Neural Network systems on low-resource embedded devices. There are several mechanisms to reduce the size of the neural networks i.e. parameter pruning, parameter quantization, etc. This paper focuses on how to apply binary neural networks to the task of speaker verification. The proposed binarization of training parameters can largely maintain the performance while significantly reducing storage space requirements and computational costs. Experiment results show that, after binarizing the Convolutional Neural Network, the ResNet34-based network achieves an EER of around 5% on the Voxceleb1 testing dataset and even outperforms the traditional real number network on the text-dependent dataset: Xiaole while having a 32x memory saving.      
### 75.Exploration of Hardware Acceleration Methods for an XNOR Traffic Signs Classifier  [ :arrow_down: ](https://arxiv.org/pdf/2104.02303.pdf)
>  Deep learning algorithms are a key component of many state-of-the-art vision systems, especially as Convolutional Neural Networks (CNN) outperform most solutions in the sense of accuracy. To apply such algorithms in real-time applications, one has to address the challenges of memory and computational complexity. To deal with the first issue, we use networks with reduced precision, specifically a binary neural network (also known as XNOR). To satisfy the computational requirements, we propose to use highly parallel and low-power FPGA devices. In this work, we explore the possibility of accelerating XNOR networks for traffic sign classification. The trained binary networks are implemented on the ZCU 104 development board, equipped with a Zynq UltraScale+ MPSoC device using two different approaches. Firstly, we propose a custom HDL accelerator for XNOR networks, which enables the inference with almost 450 fps. Even better results are obtained with the second method - the Xilinx FINN accelerator - enabling to process input images with around 550 frame rate. Both approaches provide over 96% accuracy on the test set.      
### 76.Hyperspectral and LiDAR data classification based on linear self-attention  [ :arrow_down: ](https://arxiv.org/pdf/2104.02301.pdf)
>  An efficient linear self-attention fusion model is proposed in this paper for the task of hyperspectral image (HSI) and LiDAR data joint classification. The proposed method is comprised of a feature extraction module, an attention module, and a fusion module. The attention module is a plug-and-play linear self-attention module that can be extensively used in any model. The proposed model has achieved the overall accuracy of 95.40\% on the Houston dataset. The experimental results demonstrate the superiority of the proposed method over other state-of-the-art models.      
### 77.Flexi-Transducer: Optimizing Latency, Accuracy and Compute forMulti-Domain On-Device Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2104.02232.pdf)
>  Often, the storage and computational constraints of embeddeddevices demand that a single on-device ASR model serve multiple use-cases / domains. In this paper, we propose aFlexibleTransducer(FlexiT) for on-device automatic speech recognition to flexibly deal with multiple use-cases / domains with different accuracy and latency requirements. Specifically, using a single compact model, FlexiT provides a fast response for voice commands, and accurate transcription but with more latency for dictation. In order to achieve flexible and better accuracy and latency trade-offs, the following techniques are used. Firstly, we propose using domain-specific altering of segment size for Emformer encoder that enables FlexiT to achieve flexible de-coding. Secondly, we use Alignment Restricted RNNT loss to achieve flexible fine-grained control on token emission latency for different domains. Finally, we add a domain indicator vector as an additional input to the FlexiT model. Using the combination of techniques, we show that a single model can be used to improve WERs and real time factor for dictation scenarios while maintaining optimal latency for voice commands use-cases      
### 78.Intelligent Building Control Systems for Thermal Comfort and Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted Techniques  [ :arrow_down: ](https://arxiv.org/pdf/2104.02214.pdf)
>  Building operations represent a significant percentage of the total primary energy consumed in most countries due to the proliferation of Heating, Ventilation and Air-Conditioning (HVAC) installations in response to the growing demand for improved thermal comfort. Reducing the associated energy consumption while maintaining comfortable conditions in buildings are conflicting objectives and represent a typical optimization problem that requires intelligent system design. Over the last decade, different methodologies based on the Artificial Intelligence (AI) techniques have been deployed to find the sweet spot between energy use in HVAC systems and suitable indoor comfort levels to the occupants. This paper performs a comprehensive and an in-depth systematic review of AI-based techniques used for building control systems by assessing the outputs of these techniques, and their implementations in the reviewed works, as well as investigating their abilities to improve the energy-efficiency, while maintaining thermal comfort conditions. This enables a holistic view of (1) the complexities of delivering thermal comfort to users inside buildings in an energy-efficient way, and (2) the associated bibliographic material to assist researchers and experts in the field in tackling such a challenge. Among the 20 AI tools developed for both energy consumption and comfort control, functions such as identification and recognition patterns, optimization, predictive control. Based on the findings of this work, the application of AI technology in building control is a promising area of research and still an ongoing, i.e., the performance of AI-based control is not yet completely satisfactory. This is mainly due in part to the fact that these algorithms usually need a large amount of high-quality real-world data, which is lacking in the building or, more precisely, the energy sector.      
### 79.Generation of new exciting regressors for consistent on-line estimation for a scalar parameter  [ :arrow_down: ](https://arxiv.org/pdf/2104.02210.pdf)
>  In this paper the problem of estimation of a single parameter from a linear regression equation in the absence of sufficient excitation in the regressor is addressed. A novel procedure to generate a new exciting regressor is proposed. The superior performance of a classical gradient estimator using this new regressor, instead of the original one, is illustrated with comprehensive simulations.      
### 80.Dissecting User-Perceived Latency of On-Device E2E Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2104.02207.pdf)
>  As speech-enabled devices such as smartphones and smart speakers become increasingly ubiquitous, there is growing interest in building automatic speech recognition (ASR) systems that can run directly on-device; end-to-end (E2E) speech recognition models such as recurrent neural network transducers and their variants have recently emerged as prime candidates for this task. Apart from being accurate and compact, such systems need to decode speech with low user-perceived latency (UPL), producing words as soon as they are spoken. This work examines the impact of various techniques -- model architectures, training criteria, decoding hyperparameters, and endpointer parameters -- on UPL. Our analyses suggest that measures of model size (parameters, input chunk sizes), or measures of computation (e.g., FLOPS, RTF) that reflect the model's ability to process input frames are not always strongly correlated with observed UPL. Thus, conventional algorithmic latency measurements might be inadequate in accurately capturing latency observed when models are deployed on embedded devices. Instead, we find that factors affecting token emission latency, and endpointing behavior significantly impact on UPL. We achieve the best trade-off between latency and word error rate when performing ASR jointly with endpointing, and using the recently proposed alignment regularization.      
### 81.Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2104.02194.pdf)
>  How to leverage dynamic contextual information in end-to-end speech recognition has remained an active research area. Previous solutions to this problem were either designed for specialized use cases that did not generalize well to open-domain scenarios, did not scale to large biasing lists, or underperformed on rare long-tail words. We address these limitations by proposing a novel solution that combines shallow fusion, trie-based deep biasing, and neural network language model contextualization. These techniques result in significant 19.5% relative Word Error Rate improvement over existing contextual biasing approaches and 5.4%-9.3% improvement compared to a strong hybrid baseline on both open-domain and constrained contextualization tasks, where the targets consist of mostly rare long-tail words. Our final system remains lightweight and modular, allowing for quick modification without model re-training.      
### 82.Automatic Micro-Expression Apex Frame Spotting using Local Binary Pattern from Six Intersection Planes  [ :arrow_down: ](https://arxiv.org/pdf/2104.02149.pdf)
>  Facial expressions are one of the most effective ways for non-verbal communications, which can be expressed as the Micro-Expression (ME) in the high-stake situations. The MEs are involuntary, rapid, and, subtle, and they can reveal real human intentions. However, their feature extraction is very challenging due to their low intensity and very short duration. Although Local Binary Pattern from Three Orthogonal Plane (LBP-TOP) feature extractor is useful for the ME analysis, it does not consider essential information. To address this problem, we propose a new feature extractor called Local Binary Pattern from Six Intersection Planes (LBP-SIPl). This method extracts LBP code on six intersection planes, and then it combines them. Results show that the proposed method has superior performance in apex frame spotting automatically in comparison with the relevant methods on the CASME database. Simulation results show that, using the proposed method, the apex frame has been spotted in 43% of subjects in the CASME database, automatically. Also, the mean absolute error of 1.76 is achieved, using our novel proposed method.      
### 83.Feedback linearization of nonlinear differential-algebraic control systems  [ :arrow_down: ](https://arxiv.org/pdf/2104.02141.pdf)
>  In this paper, we study feedback linearization problems for nonlinear differential-algebraic control systems (DACSs). We consider two kinds of feedback equivalences, namely, the external feedback equivalence, which is defined (locally) on the whole generalized state space, and the internal feedback equivalence, which is defined on the locally maximal controlled invariant submanifold (i.e., on the set where solutions exist). Necessary and sufficient conditions are given for the locally internal and the locally external feedback linearizability of DACSs with the help of a notion called the explicitation with driving variables, which attaches a class of ordinary differential equation control systems (ODECSs) to a given DACS. We show that the feedback linearizability of a DACS is closely related to the involutivity of the linearizability distributions of the explicitation systems. Finally, we apply our results of feedback linearization of DACSs to an academical example and a constrained mechanical system.      
### 84.A predefined-time first-order exact differentiator based on time-varying gains  [ :arrow_down: ](https://arxiv.org/pdf/2104.02140.pdf)
>  Recently, a first-order differentiator based on time-varying gains was introduced in the literature, in its non recursive form, for a class of differentiable signals $y(t)$, satisfying $|\ddot{y}(t)|\leq L(t-t_0)$, for a known function $L(t-t_0)$, such that $\frac{1}{L(t-t_0)}\left|\frac{d {L}(t-t_0)}{dt}\right|\leq M$ with a known constant $M$. It has been shown that such differentiator is globally finite-time convergent. In this paper, we redesign such an algorithm, using time base generators (a class of time-varying gains), to obtain a differentiator algorithm for the same class of signals, with guaranteed convergence before a desired time, i.e., with fixed-time convergence with an a priori user-defined upper bound for the settling time. Thus, our approach can be applied for scenarios under time-constraints. <br>We present numerical examples exposing the contribution with respect to state-of-the-art algorithms.      
### 85.Dopamine Transporter SPECT Image Classification for Neurodegenerative Parkinsonism via Diffusion Maps and Machine Learning Classifiers  [ :arrow_down: ](https://arxiv.org/pdf/2104.02066.pdf)
>  Neurodegenerative parkinsonism can be assessed by dopamine transporter single photon emission computed tomography (DaT-SPECT). Although generating images is time-consuming, these images can show interobserver variability and they have been visually interprete by nuclear medicine physicians to date. Accordingly, this study aims to provide an automatic and robust method based on Diffusion Maps and machine learning classifiers to classify the SPECT images into two types, namely Normal and Abnormal DaT-SPECT image groups. In the proposed method, the 3D images of N patients are mapped to an N by N pairwise distance matrix and training set are embedded into a low-dimensional space by using diffusion maps. Moreover, we use Nystrm's out-of-sample extension, which embeds new sample points as the testing set in the reduced space. Testing samples in the embedded space are then classified into two types through the ensemble classifier with Linear Discriminant Analysis (LDA) and voting procedure through twenty-five-fold cross-validation results. The feasibility of the method is demonstrated via Parkinsonism Progression Markers Initiative (PPMI) dataset of 1097 subjects and a clinical cohort from Kaohsiung Chang Gung Memorial Hospital (KCGMH-TW) of 630 patients. We compare performances using Diffusion Maps with those of three alternative manifold methods for dimension reduction, namely Locally Linear Embedding (LLE), Isomorphic Mapping Algorithm (Isomap), and Kernel Principal Component Analysis (Kernel PCA). We also compare results using through 2D and 3D CNN methods. The diffusion maps method has an average accuracy of 98% from the PPMI and 90% from the KCGMH-TW dataset with twenty-five fold cross-validation results. It outperforms the other three methods concerning the overall accuracy and the robustness in the training and testing samples.      
