# ArXiv eess --Wed, 8 Dec 2021
### 1.Image Enhancement via Bilateral Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.03888.pdf)
>  Nowadays, due to advanced digital imaging technologies and internet accessibility to the public, the number of generated digital images has increased dramatically. Thus, the need for automatic image enhancement techniques is quite apparent. In recent years, deep learning has been used effectively. Here, after introducing some recently developed works on image enhancement, an image enhancement system based on convolutional neural networks is presented. Our goal is to make an effective use of two available approaches, convolutional neural network and bilateral grid. In our approach, we increase the training data and the model dimensions and propose a variable rate during the training process. The enhancement results produced by our proposed method, while incorporating 5 different experts, show both quantitative and qualitative improvements as compared to other available methods.      
### 2.Training end-to-end speech-to-text models on mobile phones  [ :arrow_down: ](https://arxiv.org/pdf/2112.03871.pdf)
>  Training the state-of-the-art speech-to-text (STT) models in mobile devices is challenging due to its limited resources relative to a server environment. In addition, these models are trained on generic datasets that are not exhaustive in capturing user-specific characteristics. Recently, on-device personalization techniques have been making strides in mitigating the problem. Although many current works have already explored the effectiveness of on-device personalization, the majority of their findings are limited to simulation settings or a specific smartphone. In this paper, we develop and provide a detailed explanation of our framework to train end-to-end models in mobile phones. To make it simple, we considered a model based on connectionist temporal classification (CTC) loss. We evaluated the framework on various mobile phones from different brands and reported the results. We provide enough evidence that fine-tuning the models and choosing the right hyperparameter values is a trade-off between the lowest WER achievable, training time on-device, and memory consumption. Hence, this is vital for a successful deployment of on-device training onto a resource-limited environment like mobile phones. We use training sets from speakers with different accents and record a 7.6% decrease in average word error rate (WER). We also report the associated computational cost measurements with respect to time, memory usage, and cpu utilization in mobile phones in real-time.      
### 3.Accurate parameter estimation using scan-specific unsupervised deep learning for relaxometry and MR fingerprinting  [ :arrow_down: ](https://arxiv.org/pdf/2112.03815.pdf)
>  We propose an unsupervised convolutional neural network (CNN) for relaxation parameter estimation. This network incorporates signal relaxation and Bloch simulations while taking advantage of residual learning and spatial relations across neighboring voxels. Quantification accuracy and robustness to noise is shown to be significantly improved compared to standard parameter estimation methods in numerical simulations and in vivo data for multi-echo T2 and T2* mapping. The combination of the proposed network with subspace modeling and MR fingerprinting (MRF) from highly undersampled data permits high quality T1 and T2 mapping.      
### 4.Learning nonlinear feedforward: a Gaussian Process Approach Applied to a Printer with Friction  [ :arrow_down: ](https://arxiv.org/pdf/2112.03805.pdf)
>  Feedforward control is essential to achieving good tracking performance in positioning systems. The aim of this paper is to develop an identification strategy for inverse models of systems with nonlinear dynamics of unknown structure using input-output data, which directly delivers feedforward signals for a-priori unknown tasks. To this end, inverse systems are regarded as noncausal nonlinear finite impulse response (NFIR) systems and modeled as a Gaussian Process with a stationary kernel function that imposes properties such as smoothness and periodicity. The approach is validated experimentally on a consumer printer with friction and shown to lead to improved tracking performance with respect to linear feedforward.      
### 5.Differentially Private $K$-means Clustering Applied to Meter Data Analysis and Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2112.03801.pdf)
>  The proliferation of smart meters has resulted in a large amount of data being generated. It is increasingly apparent that methods are required for allowing a variety of stakeholders to leverage the data in a manner that preserves the privacy of the consumers. The sector is scrambling to define policies, such as the so called '15/15 rule', to respond to the need. However, the current policies fail to adequately guarantee privacy. In this paper, we address the problem of allowing third parties to apply $K$-means clustering, obtaining customer labels and centroids for a set of load time series by applying the framework of differential privacy. We leverage the method to design an algorithm that generates differentially private synthetic load data consistent with the labeled data. We test our algorithm's utility by answering summary statistics such as average daily load profiles for a 2-dimensional synthetic dataset and a real-world power load dataset.      
### 6.Danna-Sep: Unite to separate them all  [ :arrow_down: ](https://arxiv.org/pdf/2112.03752.pdf)
>  Deep learning-based music source separation has gained a lot of interest in the last decades. Most of the existing methods operate with either spectrograms or waveforms. Spectrogram based models learn suitable masks for separating magnitude spectrogram into different sources, and waveform-based models directly generate waveforms of individual sources. The two types of models have complementary strengths; the former is superior given harmonic sources such as vocals, while the latter demonstrates better results for percussion and bass instruments. In this work, we improved upon the state-of-the-art (SoTA) models and successfully combined the best of both worlds. The backbones of the proposed framework, dubbed Danna-Sep, are two spectrogram-based models including a modified X-UMX and U-Net, and an enhanced Demucs as the waveform-based model. Given an input of mixture, we linearly combined respective outputs from the three models to obtain the final result. We showed in the experiments that, despite its simplicity, Danna-Sep surpassed the SoTA models by a large margin in terms of Source-to-Distortion Ratio.      
### 7.On the Kurtosis of Modulation Formats for Characterizing the Nonlinear Fiber Propagation  [ :arrow_down: ](https://arxiv.org/pdf/2112.03745.pdf)
>  Knowing only two high-order statistical moments of modulation symbols, often represented by the fourth moment called "kurtosis", the overestimation of nonlinear interference (NLI) in a Gaussian noise (GN) model due to Gaussian signaling assumption can be corrected through an enhanced GN (EGN) model. However, in some modern optical communication systems where the transmitted modulation symbols are statistically correlated, such as in systems that use probabilistic constellation shaping (PCS) with finite-length sphere shaping, the kurtosis-based EGN model produces significant inaccuracies in analytical prediction of NLI. In this paper, we show that for correlated modulation symbols, the NLI can be more accurately estimated by substituting a statistical measure called windowed kurtosis into the EGN model, instead of the conventional kurtosis. Remarkably, the optimal window length for windowed kurtosis is found to be consistent with the self-phase modulation (SPM) and cross-phase modulation (XPM) characteristic times in various system configurations. The findings can be used in practice to analytically evaluate and design NLI-tolerant modulation formats.      
### 8.Image Compressed Sensing Using Non-local Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2112.03712.pdf)
>  Deep network-based image Compressed Sensing (CS) has attracted much attention in recent years. However, the existing deep network-based CS schemes either reconstruct the target image in a block-by-block manner that leads to serious block artifacts or train the deep network as a black box that brings about limited insights of image prior knowledge. In this paper, a novel image CS framework using non-local neural network (NL-CSNet) is proposed, which utilizes the non-local self-similarity priors with deep network to improve the reconstruction quality. In the proposed NL-CSNet, two non-local subnetworks are constructed for utilizing the non-local self-similarity priors in the measurement domain and the multi-scale feature domain respectively. Specifically, in the subnetwork of measurement domain, the long-distance dependencies between the measurements of different image blocks are established for better initial reconstruction. Analogically, in the subnetwork of multi-scale feature domain, the affinities between the dense feature representations are explored in the multi-scale space for deep reconstruction. Furthermore, a novel loss function is developed to enhance the coupling between the non-local representations, which also enables an end-to-end training of NL-CSNet. Extensive experiments manifest that NL-CSNet outperforms existing state-of-the-art CS methods, while maintaining fast computational speed.      
### 9.Efficient joint noise removal and multi exposure fusion  [ :arrow_down: ](https://arxiv.org/pdf/2112.03701.pdf)
>  Multi-exposure fusion (MEF) is a technique for combining different images of the same scene acquired with different exposure settings into a single image. All the proposed MEF algorithms combine the set of images, somehow choosing from each one the part with better exposure. <br>We propose a novel multi-exposure image fusion chain taking into account noise removal. The novel method takes advantage of DCT processing and the multi-image nature of the MEF problem. We propose a joint fusion and denoising strategy taking advantage of spatio-temporal patch selection and collaborative 3D thresholding. The overall strategy permits to denoise and fuse the set of images without the need of recovering each denoised exposure image, leading to a very efficient procedure.      
### 10.Noise Distribution Adaptive Self-Supervised Image Denoising using Tweedie Distribution and Score Matching  [ :arrow_down: ](https://arxiv.org/pdf/2112.03696.pdf)
>  Tweedie distributions are a special case of exponential dispersion models, which are often used in classical statistics as distributions for generalized linear models. Here, we reveal that Tweedie distributions also play key roles in modern deep learning era, leading to a distribution independent self-supervised image denoising formula without clean reference images. Specifically, by combining with the recent Noise2Score self-supervised image denoising approach and the saddle point approximation of Tweedie distribution, we can provide a general closed-form denoising formula that can be used for large classes of noise distributions without ever knowing the underlying noise distribution. Similar to the original Noise2Score, the new approach is composed of two successive steps: score matching using perturbed noisy images, followed by a closed form image denoising formula via distribution-independent Tweedie's formula. This also suggests a systematic algorithm to estimate the noise model and noise parameters for a given noisy image data set. Through extensive experiments, we demonstrate that the proposed method can accurately estimate noise models and parameters, and provide the state-of-the-art self-supervised image denoising performance in the benchmark dataset and real-world dataset.      
### 11.Hard Sample Aware Noise Robust Learning for Histopathology Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2112.03694.pdf)
>  Deep learning-based histopathology image classification is a key technique to help physicians in improving the accuracy and promptness of cancer diagnosis. However, the noisy labels are often inevitable in the complex manual annotation process, and thus mislead the training of the classification model. In this work, we introduce a novel hard sample aware noise robust learning method for histopathology image classification. To distinguish the informative hard samples from the harmful noisy ones, we build an easy/hard/noisy (EHN) detection model by using the sample training history. Then we integrate the EHN into a self-training architecture to lower the noise rate through gradually label correction. With the obtained almost clean dataset, we further propose a noise suppressing and hard enhancing (NSHE) scheme to train the noise robust model. Compared with the previous works, our method can save more clean samples and can be directly applied to the real-world noisy dataset scenario without using a clean subset. Experimental results demonstrate that the proposed scheme outperforms the current state-of-the-art methods in both the synthetic and real-world noisy datasets. The source code and data are available at <a class="link-external link-https" href="https://github.com/bupt-ai-cz/HSA-NRL/" rel="external noopener nofollow">this https URL</a>.      
### 12.Data-Driven Controllability Analysis and Stabilization for Linear Descriptor Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.03665.pdf)
>  For a parameter-unknown linear discrete-time descriptor system, three implementable experiments are devised and corresponding data are collected in this paper. Then data-based conditions are given to identify the system's type and controllability. Since many data-driven control methods assume that the investigated system is a controllable normal system, this work will verify the assumption and guarantee that those methods will not be misused. Furthermore, if it is a descriptor system, inspired by the equivalent stabilizability between a nominal descriptor system and its slow subsystem, a data-based decomposing method is proposed to transfer the nominal system into its slow-fast subsystems' form. Then a state feedback controller for the slow subsystem is presented based on persistently exciting input and state sequences. In this way, the stability of the nominal descriptor system is also guaranteed. Compared with other related research, we do not require any conservative assumptions or prior knowledge about the system.      
### 13.Secure learning-based MPC via garbled circuit  [ :arrow_down: ](https://arxiv.org/pdf/2112.03654.pdf)
>  Encrypted control seeks confidential controller evaluation in cloud-based or networked systems. Many existing approaches build on homomorphic encryption (HE) that allow simple mathematical operations to be carried out on encrypted data. Unfortunately, HE is computationally demanding and many control laws (in particular non-polynomial ones) cannot be efficiently implemented with this technology. <br>We show in this paper that secure two-party computation using garbled circuits provides a powerful alternative to HE for encrypted control. More precisely, we present a novel scheme that allows to efficiently implement (non-polynomial) max-out neural networks with one hidden layer in a secure fashion. These networks are of special interest for control since they allow, in principle, to exactly describe piecewise affine control laws resulting from, e.g., linear model predictive control (MPC). However, exact fits require high-dimensional preactivations of the neurons. Fortunately, we illustrate that even low-dimensional learning-based approximations are sufficiently accurate for linear MPC. In addition, these approximations can be securely evaluated using garbled circuit in less than 100~ms for our numerical example. Hence, our approach opens new opportunities for applying encrypted control.      
### 14.VeHIF: An Accessible Vegetation High-Impedance Fault Data Set Format  [ :arrow_down: ](https://arxiv.org/pdf/2112.03651.pdf)
>  High-impedance faults are a challenging problem in power distribution systems. They often do not trigger protection devices and can result in serious hazards such as igniting fires when in contact with vegetation. The current research field dedicated to studying these faults is extensive but suffers from a constraining bottleneck of a lack of real experimental data. Many works set to detect and localize such faults rely on high-impedance fault low-fidelity models, and the lack of public data sets makes it impractical to have objective performance benchmarks. This letter describes and proposes a format for a data set of more than 900 vegetation high-impedance faults funded by the Victorian Government in Australia recorded in high-sampling resolution. The original data set is public, but it was made available through an obscure format that limits its accessibility. The presented format in this letter uses the standard hierarchical data format (HDF5), which makes it easily accessible in many languages such as MATLAB, Python, C++, and more. The data set compiler and visualizer script are also provided in the work repository.      
### 15.Evaluating Generic Auto-ML Tools for Computational Pathology  [ :arrow_down: ](https://arxiv.org/pdf/2112.03622.pdf)
>  Image analysis tasks in computational pathology are commonly solved using convolutional neural networks (CNNs). The selection of a suitable CNN architecture and hyperparameters is usually done through exploratory iterative optimization, which is computationally expensive and requires substantial manual work. The goal of this article is to evaluate how generic tools for neural network architecture search and hyperparameter optimization perform for common use cases in computational pathology. For this purpose, we evaluated one on-premises and one cloud-based tool for three different classification tasks for histological images: tissue classification, mutation prediction, and grading. <br>We found that the default CNN architectures and parameterizations of the evaluated AutoML tools already yielded classification performance on par with the original publications. Hyperparameter optimization for these tasks did not substantially improve performance, despite the additional computational effort. However, performance varied substantially between classifiers obtained from individual AutoML runs due to non-deterministic effects. <br>Generic CNN architectures and AutoML tools could thus be a viable alternative to manually optimizing CNN architectures and parametrizations. This would allow developers of software solutions for computational pathology to focus efforts on harder-to-automate tasks such as data curation.      
### 16.Hybrid Controlled User Association and Resource Management for Energy-Efficient Green RANs with Limited Fronthaul  [ :arrow_down: ](https://arxiv.org/pdf/2112.03611.pdf)
>  To alleviate green house effect, high network energy efficiency (EE) has increasingly become an important research target in wireless green communications. Therefore, the investigation for resource management to mitigate the co-tier interference in the small cell network (SCN) is provided. Moreover, with the merits of cloud radio access network (C-RAN), small cell base stations (SBSs) can be decomposed of a central small cell (CSC) and remote small cells (RSCs). To achieve the coordination, the split medium access control (MAC) based functional splitting is adopted with scheduler deployed at CSCs and retransmission functions left at RSCs. However, limited fronthaul has a compelling impact at RSCs due to requirements of user quality-of-service (QoS). Accordingly, a traffic control-based user association and resource allocation (TURA) scheme is proposed for a centralized resource management. To deal with the infeasibility to control all RSCs by CSC, we propose a hybrid controlled user and resource management (HARM) scheme. A CSC performs TURA for RSCs to mitigate intra-group interference within localized C-RANs, whereas the CSCs among separate C-RANs conduct cooperative resource competition (CRC) game for alleviating inter-group interference. Based on regret-based learning algorithm, the proposed schemes are analytically proved to reach the correlated equilibrium (CE). Simulation results have validated the effect of traffic control in TURA scheme and the convergence of CRC. Moreover, the comparison of the proposed TURA, HARM, and CRC schemes with the benchmark is revealed. It is observed that the TURA scheme outperforms the other schemes under ideal fronthaul control, whilst the proposed HARM scheme can sustain EE performance considering feasible implementation.      
### 17.Spiking Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.03565.pdf)
>  Spikes and rhythms organize control and communication in the animal world, in contrast to the bits and clocks of digital technology. As continuous-time signals that can be counted, spikes have a mixed nature. This paper reviews ongoing efforts to develop a control theory of spiking systems. The central thesis is that the mixed nature of spiking results from a mixed feedback principle, and that a control theory of mixed feedback can be grounded in the operator theoretic concept of maximal monotonicity. As a nonlinear generalization of passivity, maximal monotonicity acknowledges at once the physics of electrical circuits, the algorithmic tractability of convex optimization, and the feedback control theory of incremental passivity. We discuss the relevance of a theory of spiking control systems in the emerging age of event-based technology.      
### 18.Optimal Scheduling of Energy Storage for Power System with Capability of Sensing Short-term Future PV Power Production  [ :arrow_down: ](https://arxiv.org/pdf/2112.03551.pdf)
>  Constant rise in energy consumption that comes with the population growth and introduction of new technologies has posed critical issues such as efficient energy management on the consumer side. That has elevated the importance of the use of renewable energy sources, particularly photovoltaic (PV) system and wind turbine. This work models and discusses design options based on the hybrid power system of grid and battery storage. The effects of installed capacity on renewable penetration (RP) and cost of electricity (COE) are investigated for each modality. For successful operation of hybrid power system and electricity trading in power market, accurate predictions of PV power production and load demand are taken into account. A machine learning (ML) model is introduced for scheduling, and predicting variations of the PV power production and load demand. Fitness of the ML model shows, when employing a linear regression model, the mean squared error (MSE) of 0.000012, root mean square error (RMSE) of 0.003560 and R2 of 0.999379. Using predicted PV power production and load demand, reduction of electricity cost is 37.5 % when PV and utility grid are utilized, and is 43.06% when PV, utility grid, and storage system are utilized.      
### 19.A theory of optimal convex regularization for low-dimensional recovery  [ :arrow_down: ](https://arxiv.org/pdf/2112.03540.pdf)
>  We consider the problem of recovering elements of a low-dimensional model from under-determined linear measurements. To perform recovery, we consider the minimization of a convex regularizer subject to a data fit constraint. Given a model, we ask ourselves what is the "best" convex regularizer to perform its recovery. To answer this question, we define an optimal regularizer as a function that maximizes a compliance measure with respect to the model. We introduce and study several notions of compliance. We give analytical expressions for compliance measures based on the best-known recovery guarantees with the restricted isometry property. These expressions permit to show the optimality of the {\ell} 1-norm for sparse recovery and of the nuclear norm for low-rank matrix recovery for these compliance measures. We also investigate the construction of an optimal convex regularizer using the example of sparsity in levels.      
### 20.Learning Pixel-Adaptive Weights for Portrait Photo Retouching  [ :arrow_down: ](https://arxiv.org/pdf/2112.03536.pdf)
>  Portrait photo retouching is a photo retouching task that emphasizes human-region priority and group-level consistency. The lookup table-based method achieves promising retouching performance by learning image-adaptive weights to combine 3-dimensional lookup tables (3D LUTs) and conducting pixel-to-pixel color transformation. However, this paradigm ignores the local context cues and applies the same transformation to portrait pixels and background pixels when they exhibit the same raw RGB values. In contrast, an expert usually conducts different operations to adjust the color temperatures and tones of portrait regions and background regions. This inspires us to model local context cues to improve the retouching quality explicitly. Firstly, we consider an image patch and predict pixel-adaptive lookup table weights to precisely retouch the center pixel. Secondly, as neighboring pixels exhibit different affinities to the center pixel, we estimate a local attention mask to modulate the influence of neighboring pixels. Thirdly, the quality of the local attention mask can be further improved by applying supervision, which is based on the affinity map calculated by the groundtruth portrait mask. As for group-level consistency, we propose to directly constrain the variance of mean color components in the Lab space. Extensive experiments on PPR10K dataset verify the effectiveness of our method, e.g. on high-resolution photos, the PSNR metric receives over 0.5 gains while the group-level consistency metric obtains at least 2.1 decreases.      
### 21.A Time-domain Generalized Wiener Filter for Multi-channel Speech Separation  [ :arrow_down: ](https://arxiv.org/pdf/2112.03533.pdf)
>  Frequency-domain neural beamformers are the mainstream methods for recent multi-channel speech separation models. Despite their well-defined behaviors and the effectiveness, such frequency-domain beamformers still have the limitations of a bounded oracle performance and the difficulties of designing proper networks for the complex-valued operations. In this paper, we propose a time-domain generalized Wiener filter (TD-GWF), an extension to the conventional frequency-domain beamformers that has higher oracle performance and only involves real-valued operations. We also provide discussions on how TD-GWF can be connected to conventional frequency-domain beamformers. Experiment results show that a significant performance improvement can be achieved by replacing frequency-domain beamformers by the TD-GWF in the recently proposed sequential neural beamforming pipelines.      
### 22.Distributed Containment Reference Signal for Nonholonomic Planar Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2112.03523.pdf)
>  Cooperative of multiple nonholonomic vehicles can be converted into tracking problems of a single-vehicle. The reference trajectory design within distributed features for each vehicle in the group is addressed in this note. The motivation is that nonholonomic vehicles cannot achieve asymptotical stabilization of non-feasible reference signals, and modifications about the virtual reference trajectory design are needed. Reduced-order design and time-varying technique, and some simple geometry tricks are applied to derive the dynamic reference trajectory.      
### 23.$N$-Timescale Stochastic Approximation: Stability and Convergence  [ :arrow_down: ](https://arxiv.org/pdf/2112.03515.pdf)
>  This paper presents the first sufficient conditions that guarantee the stability and almost sure convergence of $N$-timescale stochastic approximation (SA) iterates. It extends the existing results on One-timescale and Two-timescale SA iterates with a martingale noise to $N$ timescale recursions using the ordinary differential equation (ODE) based method. As an application of our results, we study SA algorithms with an added heavy ball momentum term in the context of Gradient Temporal Difference (GTD) algorithms. We show that, when the momentum parameters are chosen in a certain way, the schemes are stable and convergent to the same solution using our proposed results.      
### 24.Constrained Resource Allocation Problems in Communications: An Information-assisted Approach  [ :arrow_down: ](https://arxiv.org/pdf/2112.03512.pdf)
>  We consider a class of resource allocation problems given a set of unconditional constraints whose objective function satisfies Bellman's optimality principle. Such problems are ubiquitous in wireless communication, signal processing, and networking. These constrained combinatorial optimization problems are, in general, NP-Hard. This paper proposes two algorithms to solve this class of problems using a dynamic programming framework assisted by an information-theoretic measure. We demonstrate that the proposed algorithms ensure optimal solutions under carefully chosen conditions and use significantly reduced computational resources. We substantiate our claims by solving the power-constrained bit allocation problem in 5G massive Multiple-Input Multiple-Output receivers using the proposed approach.      
### 25.Model-free Nearly Optimal Control of Constrained-Input Nonlinear Systems Based on Synchronous Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.03510.pdf)
>  In this paper a novel model-free algorithm is proposed. This algorithm can learn the nearly optimal control law of constrained-input systems from online data without requiring any a priori knowledge of system dynamics. Based on the concept of generalized policy iteration method, there are two neural networks (NNs), namely actor and critic NN to approximate the optimal value function and optimal policy. The stability of closed-loop systems and the convergence of weights are also guaranteed by Lyapunov analysis.      
### 26.Force control of grinding based on frequency analysis  [ :arrow_down: ](https://arxiv.org/pdf/2112.03463.pdf)
>  Hysteresis-induced drift is a major issue in the detection of force induced during grinding and cutting operations. In this paper, we propose an external force estimation method based on the Mel spectrogram of the force obtained from a force sensor. We focus on the frequent strong correlation between the vibration frequency and the external force in operations with periodic vibrations. The frequency information is found to be more effective for an accurate force estimation than the amplitude in cases with large noise caused by vibration. We experimentally demonstrate that the force estimation method that combines the Mel spectrogram with a neural network is robust against drift.      
### 27.RSBNet: One-Shot Neural Architecture Search for A Backbone Network in Remote Sensing Image Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.03456.pdf)
>  Recently, a massive number of deep learning based approaches have been successfully applied to various remote sensing image (RSI) recognition tasks. However, most existing advances of deep learning methods in the RSI field heavily rely on the features extracted by the manually designed backbone network, which severely hinders the potential of deep learning models due the complexity of RSI and the limitation of prior knowledge. In this paper, we research a new design paradigm for the backbone architecture in RSI recognition tasks, including scene classification, land-cover classification and object detection. A novel one-shot architecture search framework based on weight-sharing strategy and evolutionary algorithm is proposed, called RSBNet, which consists of three stages: Firstly, a supernet constructed in a layer-wise search space is pretrained on a self-assembled large-scale RSI dataset based on an ensemble single-path training strategy. Next, the pre-trained supernet is equipped with different recognition heads through the switchable recognition module and respectively fine-tuned on the target dataset to obtain task-specific supernet. Finally, we search the optimal backbone architecture for different recognition tasks based on the evolutionary algorithm without any network training. Extensive experiments have been conducted on five benchmark datasets for different recognition tasks, the results show the effectiveness of the proposed search paradigm and demonstrate that the searched backbone is able to flexibly adapt different RSI recognition tasks and achieve impressive performance.      
### 28.Hybrid guiding: A multi-resolution refinement approach for semantic segmentation of gigapixel histopathological images  [ :arrow_down: ](https://arxiv.org/pdf/2112.03455.pdf)
>  Histopathological cancer diagnostics has become more complex, and the increasing number of biopsies is a challenge for most pathology laboratories. Thus, development of automatic methods for evaluation of histopathological cancer sections would be of value. In this study, we used 624 whole slide images (WSIs) of breast cancer from a Norwegian cohort. We propose a cascaded convolutional neural network design, called H2G-Net, for semantic segmentation of gigapixel histopathological images. The design involves a detection stage using a patch-wise method, and a refinement stage using a convolutional autoencoder. To validate the design, we conducted an ablation study to assess the impact of selected components in the pipeline on tumour segmentation. Guiding segmentation, using hierarchical sampling and deep heatmap refinement, proved to be beneficial when segmenting the histopathological images. We found a significant improvement when using a refinement network for postprocessing the generated tumour segmentation heatmaps. The overall best design achieved a Dice score of 0.933 on an independent test set of 90 WSIs. The design outperformed single-resolution approaches, such as cluster-guided, patch-wise high-resolution classification using MobileNetV2 (0.872) and a low-resolution U-Net (0.874). In addition, segmentation on a representative x400 WSI took ~58 seconds, using only the CPU. The findings demonstrate the potential of utilizing a refinement network to improve patch-wise predictions. The solution is efficient and does not require overlapping patch inference or ensembling. Furthermore, we showed that deep neural networks can be trained using a random sampling scheme that balances on multiple different labels simultaneously, without the need of storing patches on disk. Future work should involve more efficient patch generation and sampling, as well as improved clustering.      
### 29.Robust Speech Representation Learning via Flow-based Embedding Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2112.03454.pdf)
>  Over the recent years, various deep learning-based methods were proposed for extracting a fixed-dimensional embedding vector from speech signals. Although the deep learning-based embedding extraction methods have shown good performance in numerous tasks including speaker verification, language identification and anti-spoofing, their performance is limited when it comes to mismatched conditions due to the variability within them unrelated to the main task. In order to alleviate this problem, we propose a novel training strategy that regularizes the embedding network to have minimum information about the nuisance attributes. To achieve this, our proposed method directly incorporates the information bottleneck scheme into the training process, where the mutual information is estimated using the main task classifier and an auxiliary normalizing flow network. The proposed method was evaluated on different speech processing tasks and showed improvement over the standard training strategy in all experimentation.      
### 30.Dynamic imaging using Motion-Compensated SmooThness Regularization on Manifolds (MoCo-SToRM)  [ :arrow_down: ](https://arxiv.org/pdf/2112.03380.pdf)
>  We introduce an unsupervised motion-compensated reconstruction scheme for high-resolution free-breathing pulmonary MRI. We model the image frames in the time series as the deformed version of the 3D template image volume. We assume the deformation maps to be points on a smooth manifold in high-dimensional space. Specifically, we model the deformation map at each time instant as the output of a CNN-based generator that has the same weight for all time-frames, driven by a low-dimensional latent vector. The time series of latent vectors account for the dynamics in the dataset, including respiratory motion and bulk motion. The template image volume, the parameters of the generator, and the latent vectors are learned directly from the k-t space data in an unsupervised fashion. Our experimental results show improved reconstructions compared to state-of-the-art methods, especially in the context of bulk motion during the scans.      
### 31.Structured learning of safety guarantees for the control of uncertain dynamical systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.03347.pdf)
>  Approaches to keeping a dynamical system within state constraints typically rely on a model-based safety condition to limit the control signals. In the face of significant modeling uncertainty, the system can suffer from important performance penalties due to the safety condition becoming overly conservative. Machine learning can be employed to reduce the uncertainty around the system dynamics, and allow for higher performance. In this article, we propose the safe uncertainty learning principle, and argue that the learning must be properly structured to preserve safety guarantees. For instance, robust safety conditions are necessary, and they must be initialized with conservative uncertainty bounds prior to learning. Also, the uncertainty bounds should only be tightened if the collected data sufficiently capture the future system behavior. To support the principle, two example problems are solved with control barrier functions: a lane-change controller for an autonomous vehicle, and an adaptive cruise controller. This work offers a way to evaluate if machine learning preserves safety guarantees during the control of uncertain dynamical systems. It also highlights challenging aspects of learning for control.      
### 32.Learning-based synthesis of robust linear time-invariant controllers  [ :arrow_down: ](https://arxiv.org/pdf/2112.03345.pdf)
>  Recent advances in learning for control allow to synthesize controllers from learned system dynamics and maintain robust stability guarantees. However, no approach is well-suited for training linear time-invariant (LTI) controllers using arbitrary learned models of the dynamics. This article introduces a method to do so. It uses a robust control framework to derive robust stability criteria. It also uses simulated policy rollouts to obtain gradients on the controller parameters, which serve to improve the closed-loop performance. By formulating the stability criteria as penalties with computable gradients, they can be used to guide the controller parameters toward robust stability during gradient descent. The approach is flexible as it does not restrict the type of learned model for the simulated rollouts. The robust control framework ensures that the controller is already robustly stabilizing when first implemented on the actual system and no data is yet collected. It also ensures that the system stays stable in the event of a shift in dynamics, given the system behavior remains within assumed uncertainty bounds. We demonstrate the approach by synthesizing a controller for simulated autonomous lane change maneuvers. This work thus presents a flexible approach to learning robustly stabilizing LTI controllers that take advantage of modern machine learning techniques.      
### 33.Neural Energy Casimir Control  [ :arrow_down: ](https://arxiv.org/pdf/2112.03339.pdf)
>  The energy Casimir method is an effective controller design approach to stabilize port-Hamiltonian systems at a desired equilibrium. However, its application relies on finding suitable Casimir and Lyapunov functions, which are generally intractable. In this paper, we propose a neural network-based framework to learn these functions. We show how to achieve equilibrium assignment by adding suitable regularization terms in the optimization cost. We also propose a parameterization of Casimir functions for reducing the training complexity. Moreover, the distance between the equilibrium point of the learned Lyapunov function and the desired equilibrium point is analyzed, which indicates that for small suboptimality gaps, the error decreases linearly with respect to the training loss. Our methods are backed up by simulations on a pendulum system.      
### 34.Quality control for more reliable integration of deep learning-based image segmentation into medical workflows  [ :arrow_down: ](https://arxiv.org/pdf/2112.03277.pdf)
>  Machine learning algorithms underpin modern diagnostic-aiding software, which has proved valuable in clinical practice, particularly in radiology. However, inaccuracies, mainly due to the limited availability of clinical samples for training these algorithms, hamper their wider applicability, acceptance, and recognition amongst clinicians. We present an analysis of state-of-the-art automatic quality control (QC) approaches that can be implemented within these algorithms to estimate the certainty of their outputs. We validated the most promising approaches on a brain image segmentation task identifying white matter hyperintensities (WMH) in magnetic resonance imaging data. WMH are a correlate of small vessel disease common in mid-to-late adulthood and are particularly challenging to segment due to their varied size, and distributional patterns. Our results show that the aggregation of uncertainty and Dice prediction were most effective in failure detection for this task. Both methods independently improved mean Dice from 0.82 to 0.84. Our work reveals how QC methods can help to detect failed segmentation cases and therefore make automatic segmentation more reliable and suitable for clinical practice.      
### 35.Organ localisation using supervised and semi supervised approaches combining reinforcement learning with imitation learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.03276.pdf)
>  Computer aided diagnostics often requires analysis of a region of interest (ROI) within a radiology scan, and the ROI may be an organ or a suborgan. Although deep learning algorithms have the ability to outperform other methods, they rely on the availability of a large amount of annotated data. Motivated by the need to address this limitation, an approach to localisation and detection of multiple organs based on supervised and semi-supervised learning is presented here. It draws upon previous work by the authors on localising the thoracic and lumbar spine region in CT images. The method generates six bounding boxes of organs of interest, which are then fused to a single bounding box. The results of experiments on localisation of the Spleen, Left and Right Kidneys in CT Images using supervised and semi supervised learning (SSL) demonstrate the ability to address data limitations with a much smaller data set and fewer annotations, compared to other state-of-the-art methods. The SSL performance was evaluated using three different mixes of labelled and unlabelled data (i.e.30:70,35:65,40:60) for each of lumbar spine, spleen left and right kidneys respectively. The results indicate that SSL provides a workable alternative especially in medical imaging where it is difficult to obtain annotated data.      
### 36.A Contrastive Distillation Approach for Incremental Semantic Segmentation in Aerial Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.03814.pdf)
>  Incremental learning represents a crucial task in aerial image processing, especially given the limited availability of large-scale annotated datasets. A major issue concerning current deep neural architectures is known as catastrophic forgetting, namely the inability to faithfully maintain past knowledge once a new set of data is provided for retraining. Over the years, several techniques have been proposed to mitigate this problem for image classification and object detection. However, only recently the focus has shifted towards more complex downstream tasks such as instance or semantic segmentation. Starting from incremental-class learning for semantic segmentation tasks, our goal is to adapt this strategy to the aerial domain, exploiting a peculiar feature that differentiates it from natural images, namely the orientation. In addition to the standard knowledge distillation approach, we propose a contrastive regularization, where any given input is compared with its augmented version (i.e. flipping and rotations) in order to minimize the difference between the segmentation features produced by both inputs. We show the effectiveness of our solution on the Potsdam dataset, outperforming the incremental baseline in every test. Code available at: <a class="link-external link-https" href="https://github.com/edornd/contrastive-distillation" rel="external noopener nofollow">this https URL</a>.      
### 37.New Lower Bounds on the Capacity of Optical Fiber Channels via Optimized Shaping and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2112.03796.pdf)
>  Constellation shaping is a practical and effective technique to improve the performance and the rate adaptivity of optical communication systems. In principle, it could also be used to mitigate the impact of nonlinear effects, possibly increasing the information rate beyond the current limit dictated by fiber nonlinearity. However, this appealing idea is frustrated by the difficulty of designing an effective shaping strategy that takes into account the nonlinearity and long memory of the fiber channel, as well as the possible interplay with other nonlinearity mitigation strategies. As a result, only little progress has been made so far, while the optimal shaping distribution and the ultimate channel capacity remain unknown. In this work, we describe a novel technique to optimize the shaping distribution in a very general setting and high-dimensional space. For a simplified block-memoryless nonlinear optical channel, the capacity lower bound obtained by the proposed technique can be expressed analytically, establishing the conditions for an unbounded growth of capacity with power. In a more realistic scenario, the technique can be implemented by a rejection sampling algorithm driven by a suitable cost function, and the corresponding achievable information rate estimated numerically. The combination of the proposed technique with an improved (non-Gaussian) decoding metric yields a new capacity lower bound for the dual-polarization WDM channel.      
### 38.Algorithms based on operator-averaged operators  [ :arrow_down: ](https://arxiv.org/pdf/2112.03790.pdf)
>  A class of algorithms comprised by certain semismooth Newton and active-set methods is able to solve convex minimization problems involving sparsity-inducing regularizers very rapidly; the speed advantage of methods from this class is a consequence of their ability to benefit from the sparsity of the corresponding solutions by solving smaller inner problems than conventional methods. The convergence properties of such conventional methods (e.g., the forward-backward and the proximal-Newton ones) can be studied very elegantly under the framework of iterations of scalar-averaged operators - this is not the case for the aforementioned class. However, we show in this work that by instead considering operator-averaged operators, one can indeed study methods of that class, and also to derive algorithms outside of it that may be more convenient to implement than existing ones. Additionally, we present experiments whose results suggest that methods based on operator-averaged operators achieve substantially faster convergence than conventional ones.      
### 39.Outpatient Diversion using Real-Time Length-of-Stay Predictions  [ :arrow_down: ](https://arxiv.org/pdf/2112.03761.pdf)
>  In this work, we show how real-time length-of-stay (LOS) predictions can be used to divert outpatients from their assigned facility to other facilities with lesser congestion. We illustrate the implementation of this diversion mechanism for two primary health centers (PHCs), wherein we divert patients from their assigned PHC to the other PHC based on their predicted LOSs in both facilities. We develop a discrete-event simulation model of patient flow operations at these two PHCs in an Indian district and observe significantly longer LOSs at one of the PHCs due to disparities in the patient loads across both PHCs. We first determine the expected LOS of the patient at the point in time at which they are expected to arrive at a PHC using system state information recorded at the current time at the PHC in question. The real-time LOS predictions are generated by estimating patient wait times on a real-time basis at the queueing subsystems within the PHC. We then divert the patient to the appropriate PHC on the basis of the predicted LOS estimates at both PHCs, and show through simulation that the proposed framework leads to more equitable utilization of resources involved in provision of outpatient care.      
### 40.Bridging the Model-Reality Gap with Lipschitz Network Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2112.03756.pdf)
>  As robots venture into the real world, they are subject to unmodeled dynamics and disturbances. Traditional model-based control approaches have been proven successful in relatively static and known operating environments. However, when an accurate model of the robot is not available, model-based design can lead to suboptimal and even unsafe behaviour. In this work, we propose a method that bridges the model-reality gap and enables the application of model-based approaches even if dynamic uncertainties are present. In particular, we present a learning-based model reference adaptation approach that makes a robot system, with possibly uncertain dynamics, behave as a predefined reference model. In turn, the reference model can be used for model-based controller design. In contrast to typical model reference adaptation control approaches, we leverage the representative power of neural networks to capture highly nonlinear dynamics uncertainties and guarantee stability by encoding a certifying Lipschitz condition in the architectural design of a special type of neural network called the Lipschitz network. Our approach applies to a general class of nonlinear control-affine systems even when our prior knowledge about the true robot system is limited. We demonstrate our approach in flying inverted pendulum experiments, where an off-the-shelf quadrotor is challenged to balance an inverted pendulum while hovering or tracking circular trajectories.      
### 41.Does Proprietary Software Still Offer Protection of Intellectual Property in the Age of Machine Learning? -- A Case Study using Dual Energy CT Data  [ :arrow_down: ](https://arxiv.org/pdf/2112.03678.pdf)
>  In the domain of medical image processing, medical device manufacturers protect their intellectual property in many cases by shipping only compiled software, i.e. binary code which can be executed but is difficult to be understood by a potential attacker. In this paper, we investigate how well this procedure is able to protect image processing algorithms. In particular, we investigate whether the computation of mono-energetic images and iodine maps from dual energy CT data can be reverse-engineered by machine learning methods. Our results indicate that both can be approximated using only one single slice image as training data at a very high accuracy with structural similarity greater than 0.98 in all investigated cases.      
### 42.Applications of the Frenet Frame to Electric Circuits  [ :arrow_down: ](https://arxiv.org/pdf/2112.03633.pdf)
>  The paper discusses the relationships between electrical quantities, such as voltages, currents, and frequency, and geometrical ones, namely curvature and torsion. The proposed approach is based on the Frenet frame utilized in differential geometry and provides a general framework for the definition of the time derivative of electrical quantities in stationary as well as transient conditions. As a byproduct, the proposed approach unifies and generalizes the time- and phasor-domain frameworks. Other noteworthy results are a new interpretation of the link between frequency and the time derivatives of voltage and current; and a definition of the rate of change of frequency that includes the novel concept of "torsional frequency." Several numerical examples based on balanced, unbalanced, harmonically-distorted and transient voltages illustrate the findings of the paper.      
### 43.Parallel Discrete Convolutions on Adaptive Particle Representations of Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.03592.pdf)
>  We present data structures and algorithms for native implementations of discrete convolution operators over Adaptive Particle Representations (APR) of images on parallel computer architectures. The APR is a content-adaptive image representation that locally adapts the sampling resolution to the image signal. It has been developed as an alternative to pixel representations for large, sparse images as they typically occur in fluorescence microscopy. It has been shown to reduce the memory and runtime costs of storing, visualizing, and processing such images. This, however, requires that image processing natively operates on APRs, without intermediately reverting to pixels. Designing efficient and scalable APR-native image processing primitives, however, is complicated by the APR's irregular memory structure. Here, we provide the algorithmic building blocks required to efficiently and natively process APR images using a wide range of algorithms that can be formulated in terms of discrete convolutions. We show that APR convolution naturally leads to scale-adaptive algorithms that efficiently parallelize on multi-core CPU and GPU architectures. We quantify the speedups in comparison to pixel-based algorithms and convolutions on evenly sampled data. We achieve pixel-equivalent throughputs of up to 1 TB/s on a single Nvidia GeForce RTX 2080 gaming GPU, requiring up to two orders of magnitude less memory than a pixel-based implementation.      
### 44.Neural Networks for Infectious Diseases Detection: Prospects and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2112.03571.pdf)
>  Artificial neural network (ANN) ability to learn, correct errors, and transform a large amount of raw data into useful medical decisions for treatment and care have increased its popularity for enhanced patient safety and quality of care. Therefore, this paper reviews the critical role of ANNs in providing valuable insights for patients' healthcare decisions and efficient disease diagnosis. We thoroughly review different types of ANNs presented in the existing literature that advanced ANNs adaptation for complex applications. Moreover, we also investigate ANN's advances for various disease diagnoses and treatments such as viral, skin, cancer, and COVID-19. Furthermore, we propose a novel deep Convolutional Neural Network (CNN) model called ConXNet for improving the detection accuracy of COVID-19 disease. ConXNet is trained and tested using different datasets, and it achieves more than 97% detection accuracy and precision, which is significantly better than existing models. Finally, we highlight future research directions and challenges such as complexity of the algorithms, insufficient available data, privacy and security, and integration of biosensing with ANNs. These research directions require considerable attention for improving the scope of ANNs for medical diagnostic and treatment applications.      
### 45.Control Parameters Considered Harmful: Detecting Range Specification Bugs in Drone Configuration Modules via Learning-Guided Search  [ :arrow_down: ](https://arxiv.org/pdf/2112.03511.pdf)
>  In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.      
### 46.Cadence: A Practical Time-series Partitioning Algorithm for Unlabeled IoT Sensor Streams  [ :arrow_down: ](https://arxiv.org/pdf/2112.03360.pdf)
>  Timeseries partitioning is an essential step in most machine-learning driven, sensor-based IoT applications. This paper introduces a sample-efficient, robust, time-series segmentation model and algorithm. We show that by learning a representation specifically with the segmentation objective based on maximum mean discrepancy (MMD), our algorithm can robustly detect time-series events across different applications. Our loss function allows us to infer whether consecutive sequences of samples are drawn from the same distribution (null hypothesis) and determines the change-point between pairs that reject the null hypothesis (i.e., come from different distributions). We demonstrate its applicability in a real-world IoT deployment for ambient-sensing based activity recognition. Moreover, while many works on change-point detection exist in the literature, our model is significantly simpler and matches or outperforms state-of-the-art methods. We can fully train our model in 9-93 seconds on average with little variation in hyperparameters for data across different applications.      
### 47.Audio Deepfake Perceptions in College Going Populations  [ :arrow_down: ](https://arxiv.org/pdf/2112.03351.pdf)
>  Deepfake is content or material that is generated or manipulated using AI methods, to pass off as real. There are four different deepfake types: audio, video, image and text. In this research we focus on audio deepfakes and how people perceive it. There are several audio deepfake generation frameworks, but we chose MelGAN which is a non-autoregressive and fast audio deepfake generating framework, requiring fewer parameters. This study tries to assess audio deepfake perceptions among college students from different majors. This study also answers the question of how their background and major can affect their perception towards AI generated deepfakes. We also analyzed the results based on different aspects of: grade level, complexity of the grammar used in the audio clips, length of the audio clips, those who knew the term deepfakes and those who did not, as well as the political angle. It is interesting that the results show when an audio clip has a political connotation, it can affect what people think about whether it is real or fake, even if the content is fairly similar. This study also explores the question of how background and major can affect perception towards deepfakes.      
### 48.Smart Metering System Capable of Anomaly Detection by Bi-directional LSTM Autoencoder  [ :arrow_down: ](https://arxiv.org/pdf/2112.03275.pdf)
>  Anomaly detection is concerned with a wide range of applications such as fault detection, system monitoring, and event detection. Identifying anomalies from metering data obtained from smart metering system is a critical task to enhance reliability, stability, and efficiency of the power system. This paper presents an anomaly detection process to find outliers observed in the smart metering system. In the proposed approach, bi-directional long short-term memory (BiLSTM) based autoencoder is used and finds the anomalous data point. It calculates the reconstruction error through autoencoder with the non-anomalous data, and the outliers to be classified as anomalies are separated from the non-anomalous data by predefined threshold. Anomaly detection method based on the BiLSTM autoencoder is tested with the metering data corresponding to 4 types of energy sources electricity/water/heating/hot water collected from 985 households.      
### 49.A Deep-Learning Intelligent System Incorporating Data Augmentation for Short-Term Voltage Stability Assessment of Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.03265.pdf)
>  Facing the difficulty of expensive and trivial data collection and annotation, how to make a deep learning-based short-term voltage stability assessment (STVSA) model work well on a small training dataset is a challenging and urgent problem. Although a big enough dataset can be directly generated by contingency simulation, this data generation process is usually cumbersome and inefficient; while data augmentation provides a low-cost and efficient way to artificially inflate the representative and diversified training datasets with label preserving transformations. In this respect, this paper proposes a novel deep-learning intelligent system incorporating data augmentation for STVSA of power systems. First, due to the unavailability of reliable quantitative criteria to judge the stability status for a specific power system, semi-supervised cluster learning is leveraged to obtain labeled samples in an original small dataset. Second, to make deep learning applicable to the small dataset, conditional least squares generative adversarial networks (LSGAN)-based data augmentation is introduced to expand the original dataset via artificially creating additional valid samples. Third, to extract temporal dependencies from the post-disturbance dynamic trajectories of a system, a bi-directional gated recurrent unit with attention mechanism based assessment model is established, which bi-directionally learns the significant time dependencies and automatically allocates attention weights. The test results demonstrate the presented approach manages to achieve better accuracy and a faster response time with original small datasets. Besides classification accuracy, this work employs statistical measures to comprehensively examine the performance of the proposal.      
### 50.Modeling Demand Flexibility of RES-based Virtual Power Plants  [ :arrow_down: ](https://arxiv.org/pdf/2112.03261.pdf)
>  In this paper, an approach to evaluate the benefits of demand flexibility for Virtual Power Plants (VPPs) is presented. The flexible demands chosen in this study are part of a renewable energy source-based VPP that participates in Day-Ahead Market (DAM) and Intra-Day Market (IDM) and has dispatchable and non-dispatchable assets. A demand model with bi-level flexibility is proposed: the first level is associated with DAM, whereas the second level is related to IDM sessions. Simulations are carried out considering a 12-node network to ascertain the eventual impacts of modeling demand flexibility on VPP operation. The market structure considered in the case study resembles the different trading floors in the Spanish electricity market. Results obtained show that the proposed demand flexibility scheme increases the overall profit of the VPP, as well as the revenues of the demand owners without disrupting consumers' comfort.      
