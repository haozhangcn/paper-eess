# ArXiv eess --Thu, 9 Dec 2021
### 1.Self-Supervised Speaker Verification with Simple Siamese Network and Self-Supervised Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2112.04459.pdf)
>  Training speaker-discriminative and robust speaker verification systems without speaker labels is still challenging and worthwhile to explore. In this study, we propose an effective self-supervised learning framework and a novel regularization strategy to facilitate self-supervised speaker representation learning. Different from contrastive learning-based self-supervised learning methods, the proposed self-supervised regularization (SSReg) focuses exclusively on the similarity between the latent representations of positive data pairs. We also explore the effectiveness of alternative online data augmentation strategies on both the time domain and frequency domain. With our strong online data augmentation strategy, the proposed SSReg shows the potential of self-supervised learning without using negative pairs and it can significantly improve the performance of self-supervised speaker representation learning with a simple Siamese network architecture. Comprehensive experiments on the VoxCeleb datasets demonstrate that our proposed self-supervised approach obtains a 23.4% relative improvement by adding the effective self-supervised regularization and outperforms other previous works.      
### 2.Data-driven stochastic model predictive control  [ :arrow_down: ](https://arxiv.org/pdf/2112.04439.pdf)
>  We propose a novel data-driven stochastic model predictive control (MPC) algorithm to control linear time-invariant systems with additive stochastic disturbances in the dynamics. The scheme centers around repeated predictions and computations of optimal control inputs based on a non-parametric representation of the space of all possible trajectories, using the fundamental lemma from behavioral systems theory. This representation is based on a single measured input-state-disturbance trajectory generated by persistently exciting inputs and does not require any further identification step. Based on stochastic MPC ideas, we enforce the satisfaction of state constraints with a pre-specified probability level, allowing for a systematic trade-off between control performance and constraint satisfaction. The proposed data-driven stochastic MPC algorithm enables efficient control where robust methods are too conservative, which we demonstrate in a simulation example.      
### 3.Which images to label for few-shot medical landmark detection?  [ :arrow_down: ](https://arxiv.org/pdf/2112.04386.pdf)
>  The success of deep learning methods relies on the availability of well-labeled large-scale datasets. However, for medical images, annotating such abundant training data often requires experienced radiologists and consumes their limited time. Few-shot learning is developed to alleviate this burden, which achieves competitive performances with only several labeled data. However, a crucial yet previously overlooked problem in few-shot learning is about the selection of template images for annotation before learning, which affects the final performance. We herein propose a novel Sample Choosing Policy (SCP) to select "the most worthy" images for annotation, in the context of few-shot medical landmark detection. SCP consists of three parts: 1) Self-supervised training for building a pre-trained deep model to extract features from radiological images, 2) Key Point Proposal for localizing informative patches, and 3) Representative Score Estimation for searching the most representative samples or templates. The advantage of SCP is demonstrated by various experiments on three widely-used public datasets. For one-shot medical landmark detection, its use reduces the mean radial errors on Cephalometric and HandXray datasets by 14.2% (from 3.595mm to 3.083mm) and 35.5% (4.114mm to 2.653mm), respectively.      
### 4.Adaptive R-Peak Detection on Wearable ECG Sensors for High-Intensity Exercise  [ :arrow_down: ](https://arxiv.org/pdf/2112.04369.pdf)
>  Objective: Continuous monitoring of biosignals via wearable sensors has quickly expanded in the medical and wellness fields. At rest, automatic detection of vital parameters is generally accurate. However, in conditions such as high-intensity exercise, sudden physiological changes occur to the signals, compromising the robustness of standard algorithms. Methods: Our method, called BayeSlope, is based on unsupervised learning, Bayesian filtering, and non-linear normalization to enhance and correctly detect the R peaks according to their expected positions in the ECG. Furthermore, as BayeSlope is computationally heavy and can drain the device battery quickly, we propose an online design that adapts its robustness to sudden physiological changes, and its complexity to the heterogeneous resources of modern embedded platforms. This method combines BayeSlope with a lightweight algorithm, executed in cores with different capabilities, to reduce the energy consumption while preserving the accuracy. Results: BayeSlope achieves an F1 score of 99.3% in experiments during intense cycling exercise with 20 subjects. Additionally, the online adaptive process achieves an F1 score of 99% across five different exercise intensities, with a total energy consumption of 1.55+-0.54~mJ. Conclusion: We propose a highly accurate and robust method, and a complete energy-efficient implementation in a modern ultra-low-power embedded platform to improve R peak detection in challenging conditions, such as during high-intensity exercise. Significance: The experiments show that BayeSlope outperforms a state-of-the-art algorithm up to 8.4% in F1 score, while our online adaptive method can reach energy savings up to 38.7% on modern heterogeneous wearable platforms.      
### 5.On the use of Anderson acceleration in hierarchical control  [ :arrow_down: ](https://arxiv.org/pdf/2112.04299.pdf)
>  This paper investigates the use of fixed-point Anderson acceleration method (AA) to a recently proposed hierarchical control framework. Due to its model-free property, the AA-based resulting hierarchical framework becomes more generic since no mathematical model of the subsystems at the lower layer is required at the upper coordinator layer. Numerical results are proposed to evaluate the effectiveness of this approach. The paper also presents a modified version of the original hierarchical approach that involves the AA in hierarchical control.      
### 6.User Activity Detection and Channel Estimation of Spatially Correlated Channels via AMP in Massive MTC  [ :arrow_down: ](https://arxiv.org/pdf/2112.04295.pdf)
>  This paper addresses the problem of joint user identification and channel estimation (JUICE) for grant-free access in massive machine-type communications (mMTC). We consider the JUICE under a spatially correlated fading channel model as that reflects the main characteristics of the practical multiple-input multiple-output channels. We formulate the JUICE as a sparse recovery problem in a multiple measurement vector setup and present a solution based on the approximate message passing (AMP) algorithm that takes into account the channel spatial correlation. Using the state evolution, we provide a detailed theoretical analysis on the activity detection performance of AMP-based JUICE by deriving closed-from expressions for the probabilities of miss detection and false alarm. The simulation experiments show that the performance predicted by the theoretical analysis matches the one obtained by the numerical results.      
### 7.An empirical study on V2X radio coverage using leaky coaxial cables in road crash barriers  [ :arrow_down: ](https://arxiv.org/pdf/2112.04277.pdf)
>  For current and future automated driving functions, the radio availability of broadband hybrid networking services (e.g. digital broadcasting, mobile radio, dedicated short range communication) is a prerequisite for continuous V2X information exchange. The supply focus for this is explicitly the road route with its lanes. The application of antenna-based solutions for such longitudinal radio cells with hybrid telematics services is expensive from the installation point of view and can only be adapted to new future telematics standards with great effort. A more suitable solution for such longitudinally shaped radio cells for road routes are leaky coaxial cables (LCX), which are already successfully used for tunnel solutions, for example. The paper discusses the installation and radio implementation of broadband LCX solutions (up to 6 GHz) in terms of simulation and surveying. The integration of the LCX into the crash barrier is favored due to low installation effort and easy upgradeability. An installation was realized on an automotive test fields, where preliminary empirical results for radio simulation and coverage were obtained. Based on the simulations and evaluation measurements, it can be shown that the propagated coverage approach is sustainable over all radiated services. Further solution approaches such as the direct insertion of LCX into the roadway and the derivation of vehicle location information are discussed in the outlook of the paper.      
### 8.Implicit Neural Representations for Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2112.04267.pdf)
>  Recently Implicit Neural Representations (INRs) gained attention as a novel and effective representation for various data types. Thus far, prior work mostly focused on optimizing their reconstruction performance. This work investigates INRs from a novel perspective, i.e., as a tool for image compression. To this end, we propose the first comprehensive compression pipeline based on INRs including quantization, quantization-aware retraining and entropy coding. Encoding with INRs, i.e. overfitting to a data sample, is typically orders of magnitude slower. To mitigate this drawback, we leverage meta-learned initializations based on MAML to reach the encoding in fewer gradient updates which also generally improves rate-distortion performance of INRs. We find that our approach to source compression with INRs vastly outperforms similar prior work, is competitive with common compression algorithms designed specifically for images and closes the gap to state-of-the-art learned approaches based on Rate-Distortion Autoencoders. Moreover, we provide an extensive ablation study on the importance of individual components of our method which we hope facilitates future research on this novel approach to image compression.      
### 9.Prediction-Aware Quality Enhancement of VVC Using CNN  [ :arrow_down: ](https://arxiv.org/pdf/2112.04225.pdf)
>  The upcoming video coding standard, Versatile Video Coding (VVC), has shown great improvement compared to its predecessor, High Efficiency Video Coding (HEVC), in terms of bitrate saving. Despite its substantial performance, compressed videos might still suffer from quality degradation at low bitrates due to coding artifacts such as blockiness, blurriness and ringing. In this work, we exploit Convolutional Neural Networks (CNN) to enhance quality of VVC coded frames after decoding in order to reduce low bitrate artifacts. The main contribution of this work is the use of coding information from the compressed bitstream. More precisely, the prediction information of intra frames is used for training the network in addition to the reconstruction information. The proposed method is applied on both luminance and chrominance components of intra coded frames of VVC. Experiments on VVC Test Model (VTM) show that, both in low and high bitrates, the use of coding information can improve the BD-rate performance by about 1% and 6% for luma and chroma components, respectively.      
### 10.Learning over All Stabilizing Nonlinear Controllers for a Partially-Observed Linear System  [ :arrow_down: ](https://arxiv.org/pdf/2112.04219.pdf)
>  We propose a parameterization of nonlinear output feedback controllers for linear dynamical systems based on a recently developed class of neural network called the recurrent equilibrium network (REN), and a nonlinear version of the Youla parameterization. Our approach guarantees the closed-loop stability of partially observable linear dynamical systems without requiring any constraints to be satisfied. This significantly simplifies model fitting as any unconstrained optimization procedure can be applied whilst still maintaining stability. We demonstrate our method on reinforcement learning tasks with both exact and approximate gradient methods. Simulation studies show that our method is significantly more scalable and significantly outperforms other approaches in the same problem setting.      
### 11.$\boldsymbolγ$-Net: Superresolving SAR Tomographic Inversion via Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.04211.pdf)
>  Synthetic aperture radar tomography (TomoSAR) has been extensively employed in 3-D reconstruction in dense urban areas using high-resolution SAR acquisitions. Compressive sensing (CS)-based algorithms are generally considered as the state of the art in super-resolving TomoSAR, in particular in the single look case. This superior performance comes at the cost of extra computational burdens, because of the sparse reconstruction, which cannot be solved analytically and we need to employ computationally expensive iterative solvers. In this paper, we propose a novel deep learning-based super-resolving TomoSAR inversion approach, $\boldsymbol{\gamma}$-Net, to tackle this challenge. $\boldsymbol{\gamma}$-Net adopts advanced complex-valued learned iterative shrinkage thresholding algorithm (CV-LISTA) to mimic the iterative optimization step in sparse reconstruction. Simulations show the height estimate from a well-trained $\boldsymbol{\gamma}$-Net approaches the Cramér-Rao lower bound while improving the computational efficiency by 1 to 2 orders of magnitude comparing to the first-order CS-based methods. It also shows no degradation in the super-resolution power comparing to the state-of-the-art second-order TomoSAR solvers, which are much more computationally expensive than the first-order methods. Specifically, $\boldsymbol{\gamma}$-Net reaches more than $90\%$ detection rate in moderate super-resolving cases at 25 measurements at 6dB SNR. Moreover, simulation at limited baselines demonstrates that the proposed algorithm outperforms the second-order CS-based method by a fair margin. Test on real TerraSAR-X data with just 6 interferograms also shows high-quality 3-D reconstruction with high-density detected double scatterers.      
### 12.Beam Squint in Ultra-wideband mmWave Systems: RF Lens Array vs. Phase-Shifter-Based Array  [ :arrow_down: ](https://arxiv.org/pdf/2112.04188.pdf)
>  In this article, we discuss the potential of radio frequency (RF) lens for ultra-wideband millimeter-wave (mmWave) systems. In terms of the beam squint, we compare the proposed RF lens antenna with the phase shifter-based array for hybrid beamforming. To reduce the complexities for fully digital beamforming, researchers have come up with RF lens-based hybrid beamforming. The use of mmWave systems, however, causes an increase in bandwidth, which gives rise to the beam squint phenomenon. We first find the causative factors for beam squint in the dielectric RF lens antenna. Based on the beamforming gain at each frequency, we verify that, in a specific situation, RF lens can be free of the beam squint effect. We use 3D electromagnetic analysis software to numerically interpret the beam squint of each antenna type. Based on the results, we present the degraded spectral efficiency by system-level simulations with 3D indoor ray tracing. Finally, to verify our analysis, we fabricate an actual RF lens antenna and demonstrate the real performance using a mmWave, NI PXIe, software-defined radio system.      
### 13.A study on native American English speech recognition by Indian listeners with varying word familiarity level  [ :arrow_down: ](https://arxiv.org/pdf/2112.04151.pdf)
>  In this study, listeners of varied Indian nativities are asked to listen and recognize TIMIT utterances spoken by American speakers. We have three kinds of responses from each listener while they recognize an utterance: 1. Sentence difficulty ratings, 2. Speaker difficulty ratings, and 3. Transcription of the utterance. From these transcriptions, word error rate (WER) is calculated and used as a metric to evaluate the similarity between the recognized and the original sentences.The sentences selected in this study are categorized into three groups: Easy, Medium and Hard, based on the frequency ofoccurrence of the words in them. We observe that the sentence, speaker difficulty ratings and the WERs increase from easy to hard categories of sentences. We also compare the human speech recognition performance with that using three automatic speech recognition (ASR) under following three combinations of acoustic model (AM) and language model(LM): ASR1) AM trained with recordings from speakers of Indian origin and LM built on TIMIT text, ASR2) AM using recordings from native American speakers and LM built ontext from LIBRI speech corpus, and ASR3) AM using recordings from native American speakers and LM build on LIBRI speech and TIMIT text. We observe that HSR performance is similar to that of ASR1 whereas ASR3 achieves the best performance. Speaker nativity wise analysis shows that utterances from speakers of some nativity are more difficult to recognize by Indian listeners compared to few other nativities      
### 14.Reverse image filtering using total derivative approximation and accelerated gradient descent  [ :arrow_down: ](https://arxiv.org/pdf/2112.04121.pdf)
>  In this paper, we address a new problem of reversing the effect of an image filter, which can be linear or nonlinear. The assumption is that the algorithm of the filter is unknown and the filter is available as a black box. We formulate this inverse problem as minimizing a local patch-based cost function and use total derivative to approximate the gradient which is used in gradient descent to solve the problem. We analyze factors affecting the convergence and quality of the output in the Fourier domain. We also study the application of accelerated gradient descent algorithms in three gradient-free reverse filters, including the one proposed in this paper. We present results from extensive experiments to evaluate the complexity and effectiveness of the proposed algorithm. Results demonstrate that the proposed algorithm outperforms the state-of-the-art in that (1) it is at the same level of complexity as that of the fastest reverse filter, but it can reverse a larger number of filters, and (2) it can reverse the same list of filters as that of the very complex reverse filter, but its complexity is much smaller.      
### 15.Supplementary Feedforward Voltage Control in a Reconfigurable Distribution Network  [ :arrow_down: ](https://arxiv.org/pdf/2112.04086.pdf)
>  Network reconfiguration (NR) has attracted much attention due to its ability to convert conventional distribution networks (DNs) into self-healing grids. This paper proposes a new strategy for real-time voltage regulation (VR) in a reconfigurable DN, whereby optimal feedforward control of synchronous and inverter-based distributed generators (DGs) is achieved in coordination with the operation of feeder line switches (SWs). This enables preemptive compensation of upcoming deviations in DN voltages caused by NR-aided load restoration. A robust optimization problem is formulated using a dynamic analytical model of NR to design the feedforward voltage controllers (FVCs) that minimize voltage deviations with respect to the H infinity norm. Errors in the estimates of DG parameters and load demands are reflected in the design of optimal FVCs through polytopic uncertainty modeling, further improving the robustness of the proposed VR strategy. Small-signal analysis and case studies are conducted, demonstrating the effectiveness of the optimal robust FVCs in improving real-time VR when NR is activated for load restoration. The performances of the proposed FVCs are also verified under various operating conditions of a reconfigurable DN, characterized principally by SW operations, network parameter errors, and communication time delays.      
### 16.Nanogrid Power Management Based on Fuzzy Logic Controller  [ :arrow_down: ](https://arxiv.org/pdf/2112.04068.pdf)
>  The transport industry is undergoing unprecedented technological change. In the automotive sector, the voices of progress are, among other things, linked to the partial or total electrification of vehicles. Thanks to bi-directional charging technology, electric vehicles (EV) could significantly improve their homeowners' energy balance. For islanded household buildings where a supplementary turbine generator and photovoltaic (PV) generator are the main supplies, it is crucial to minimize the running costs and maximize the use of the PV power. In this paper, a supervisory controller based on fuzzy logic is proposed to utilize the battery in the EV, while parking, as an interactive element to replace the auxiliary turbine and to assure that the battery power and energy do not exceed their design limits and maintaining a stable power flow. The nanogrid considered in this paper consists of a PV, EV battery, load and a turbine supplementary unit. The fuzzy logic controller alters the AC bus frequency, which is used by the local controllers of the parallel units to curtail the power generated by the PV or to supplement the power from the turbine unit. The proposed FLC performance is verified by Matlab simulation.      
### 17.Nuclei Segmentation in Histopathology Images using Deep Learning with Local and Global Views  [ :arrow_down: ](https://arxiv.org/pdf/2112.03998.pdf)
>  Digital pathology is one of the most significant developments in modern medicine. Pathological examinations are the gold standard of medical protocols and play a fundamental role in diagnosis. Recently, with the advent of digital scanners, tissue histopathology slides can now be digitized and stored as digital images. As a result, digitized histopathological tissues can be used in computer-aided image analysis programs and machine learning techniques. Detection and segmentation of nuclei are some of the essential steps in the diagnosis of cancers. Recently, deep learning has been used for nuclei segmentation. However, one of the problems in deep learning methods for nuclei segmentation is the lack of information from out of the patches. This paper proposes a deep learning-based approach for nuclei segmentation, which addresses the problem of misprediction in patch border areas. We use both local and global patches to predict the final segmentation map. Experimental results on the Multi-organ histopathology dataset demonstrate that our method outperforms the baseline nuclei segmentation and popular segmentation models.      
### 18.Testing for Causal Influence using a Partial Coherence Statistic  [ :arrow_down: ](https://arxiv.org/pdf/2112.03987.pdf)
>  In this paper we explore partial coherence as a tool for evaluating causal influence of one signal sequence on another. In some cases the signal sequence is sampled from a time- or space-series. The key idea is to establish a connection between questions of causality and questions of partial coherence. Once this connection is established, then a scale-invariant partial coherence statistic is used to resolve the question of causality. This coherence statistic is shown to be a likelihood ratio, and its null distribution is shown to be a Wilks Lambda. It may be computed from a composite covariance matrix or from its inverse, the information matrix. Numerical experiments demonstrate the application of partial coherence to the resolution of causality. Importantly, the method is model-free, depending on no generative model for causality.      
### 19.Tailored neural networks for learning optimal value functions in MPC  [ :arrow_down: ](https://arxiv.org/pdf/2112.03975.pdf)
>  Learning-based predictive control is a promising alternative to optimization-based MPC. However, efficiently learning the optimal control policy, the optimal value function, or the Q-function requires suitable function approximators. Often, artificial neural networks (ANN) are considered but choosing a suitable topology is also non-trivial. Against this background, it has recently been shown that tailored ANN allow, in principle, to exactly describe the optimal control policy in linear MPC by exploiting its piecewise affine structure. In this paper, we provide a similar result for representing the optimal value function and the Q-function that are both known to be piecewise quadratic for linear MPC.      
### 20.BT-Unet: A self-supervised learning framework for biomedical image segmentation using Barlow Twins with U-Net models  [ :arrow_down: ](https://arxiv.org/pdf/2112.03916.pdf)
>  Deep learning has brought the most profound contribution towards biomedical image segmentation to automate the process of delineation in medical imaging. To accomplish such task, the models are required to be trained using huge amount of annotated or labelled data that highlights the region of interest with a binary mask. However, efficient generation of the annotations for such huge data requires expert biomedical analysts and extensive manual effort. It is a tedious and expensive task, while also being vulnerable to human error. To address this problem, a self-supervised learning framework, BT-Unet is proposed that uses the Barlow Twins approach to pre-train the encoder of a U-Net model via redundancy reduction in an unsupervised manner to learn data representation. Later, complete network is fine-tuned to perform actual segmentation. The BT-Unet framework can be trained with a limited number of annotated samples while having high number of unannotated samples, which is mostly the case in real-world problems. This framework is validated over multiple U-Net models over diverse datasets by generating scenarios of a limited number of labelled samples using standard evaluation metrics. With exhaustive experiment trials, it is observed that the BT-Unet framework enhances the performance of the U-Net models with significant margin under such circumstances.      
### 21.GraDIRN: Learning Iterative Gradient Descent-based Energy Minimization for Deformable Image Registration  [ :arrow_down: ](https://arxiv.org/pdf/2112.03915.pdf)
>  We present a Gradient Descent-based Image Registration Network (GraDIRN) for learning deformable image registration by embedding gradient-based iterative energy minimization in a deep learning framework. Traditional image registration algorithms typically use iterative energy-minimization optimization to find the optimal transformation between a pair of images, which is time-consuming when many iterations are needed. In contrast, recent learning-based methods amortize this costly iterative optimization by training deep neural networks so that registration of one pair of images can be achieved by fast network forward pass after training. Motivated by successes in image reconstruction techniques that combine deep learning with the mathematical structure of iterative variational energy optimization, we formulate a novel registration network based on multi-resolution gradient descent energy minimization. The forward pass of the network takes explicit image dissimilarity gradient steps and generalized regularization steps parameterized by Convolutional Neural Networks (CNN) for a fixed number of iterations. We use auto-differentiation to derive the forward computational graph for the explicit image dissimilarity gradient w.r.t. the transformation, so arbitrary image dissimilarity metrics and transformation models can be used without complex and error-prone gradient derivations. We demonstrate that this approach achieves state-of-the-art registration performance while using fewer learnable parameters through extensive evaluations on registration tasks using 2D cardiac MR images and 3D brain MR images.      
### 22.Dyadic Sex Composition and Task Classification Using fNIRS Hyperscanning Data  [ :arrow_down: ](https://arxiv.org/pdf/2112.03911.pdf)
>  Hyperscanning with functional near-infrared spectroscopy (fNIRS) is an emerging neuroimaging application that measures the nuanced neural signatures underlying social interactions. Researchers have assessed the effect of sex and task type (e.g., cooperation versus competition) on inter-brain coherence during human-to-human interactions. However, no work has yet used deep learning-based approaches to extract insights into sex and task-based differences in an fNIRS hyperscanning context. This work proposes a convolutional neural network-based approach to dyadic sex composition and task classification for an extensive hyperscanning dataset with $N = 222$ participants. Inter-brain signal similarity computed using dynamic time warping is used as the input data. The proposed approach achieves a maximum classification accuracy of greater than $80$ percent, thereby providing a new avenue for exploring and understanding complex brain behavior.      
### 23.Everything at Once -- Multi-modal Fusion Transformer for Video Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2112.04446.pdf)
>  Multi-modal learning from video data has seen increased attention recently as it allows to train semantically meaningful embeddings without human annotation enabling tasks like zero-shot retrieval and classification. In this work, we present a multi-modal, modality agnostic fusion transformer approach that learns to exchange information between multiple modalities, such as video, audio, and text, and integrate them into a joined multi-modal representation to obtain an embedding that aggregates multi-modal temporal information. We propose to train the system with a combinatorial loss on everything at once, single modalities as well as pairs of modalities, explicitly leaving out any add-ons such as position or modality encoding. At test time, the resulting model can process and fuse any number of input modalities. Moreover, the implicit properties of the transformer allow to process inputs of different lengths. To evaluate the proposed approach, we train the model on the large scale HowTo100M dataset and evaluate the resulting embedding space on four challenging benchmark datasets obtaining state-of-the-art results in zero-shot video retrieval and zero-shot video action localization.      
### 24.Real Time Integration Centre of Mass (riCOM) Reconstruction for 4D-STEM  [ :arrow_down: ](https://arxiv.org/pdf/2112.04442.pdf)
>  A real-time image reconstruction method for scanning transmission electron microscopy (STEM) is proposed. The method uses the concept of integrated centre of mass (iCOM) and creates a live-updated image based on diffraction patterns collected at each probe position during scanning. It is shown that the method has similar characteristics to the traditional iCOM approach. However, by reformulating the integration method, the reconstruction process can be divided into sub-units, with each unit requiring only information from a single probe position, such that the resulting image can be updated each time a new probe position is visited without storing any intermediate diffraction patterns. As a certain position in the image is only influenced by its surrounding pixels in the immediate vicinity, the image update provides interpretable images being build up while the scanning is being performed. The results show clearer features at higher spatial frequency, such as atomic column positions. It is also demonstrated that common post processing methods, such as a band pass filter, can be directly integrated in the real time processing flow. By comparing with other reconstruction methods, it is shown that the proposed method can produce high quality reconstructions with good noise robustness at extremely low memory and computational requirements. We further present an efficient, interactive open source implementation of the concept, compatible with frame-based, as well as event-based camera/file types. The proposed method provides the attractive feature of immediate feedback that microscope operators have become used to for e.g. conventional HAADF STEM imaging allowing for rapid decision making and fine tuning to obtain the best possible images for beam sensitive samples at the lowest possible dose.      
### 25.Audio-Visual Synchronisation in the wild  [ :arrow_down: ](https://arxiv.org/pdf/2112.04432.pdf)
>  In this paper, we consider the problem of audio-visual synchronisation applied to videos `in-the-wild' (ie of general classes beyond speech). As a new task, we identify and curate a test set with high audio-visual correlation, namely VGG-Sound Sync. We compare a number of transformer-based architectural variants specifically designed to model audio and visual signals of arbitrary length, while significantly reducing memory requirements during training. We further conduct an in-depth analysis on the curated dataset and define an evaluation metric for open domain audio-visual synchronisation. We apply our method on standard lip reading speech benchmarks, LRS2 and LRS3, with ablations on various aspects. Finally, we set the first benchmark for general audio-visual synchronisation with over 160 diverse classes in the new VGG-Sound Sync video dataset. In all cases, our proposed model outperforms the previous state-of-the-art by a significant margin.      
### 26.Training Robust Zero-Shot Voice Conversion Models with Self-supervised Features  [ :arrow_down: ](https://arxiv.org/pdf/2112.04424.pdf)
>  Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker characteristic of an utterance to match an unseen target speaker without relying on parallel training data. Recently, self-supervised learning of speech representation has been shown to produce useful linguistic units without using transcripts, which can be directly passed to a VC model. In this paper, we showed that high-quality audio samples can be achieved by using a length resampling decoder, which enables the VC model to work in conjunction with different linguistic feature extractors and vocoders without requiring them to operate on the same sequence length. We showed that our method can outperform many baselines on the VCTK dataset. Without modifying the architecture, we further demonstrated that a) using pairs of different audio segments from the same speaker, b) adding a cycle consistency loss, and c) adding a speaker classification loss can help to learn a better speaker embedding. Our model trained on LibriTTS using these techniques achieves the best performance, producing audio samples transferred well to the target speaker's voice, while preserving the linguistic content that is comparable with actual human utterances in terms of Character Error Rate.      
### 27.On the Average Mutual Information of MIMO Keyhole Channels with Finite Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2112.04415.pdf)
>  This letter studies the average mutual information (AMI) of keyhole multiple-input multiple-output (MIMO) systems having finite input signals. At first, the AMI of single-stream transmission is investigated under two cases where the state information at the transmitter (CSIT) is available or not. Then, the derived results are further extended to the case of multi-stream transmission. For the sake of providing more system insights, asymptotic analyses are performed in the regime of high signal-to-noise ratio (SNR), which suggests that the high-SNR AMI converges to some constant with its rate of convergence determined by the diversity order. All the results are validated by numerical simulations and are in excellent agreement.      
### 28.Simple Fair Power Allocation for NOMA-Based Visible Light Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.04410.pdf)
>  Non-orthogonal multiple access (NOMA) in the power-domain has been recognized as a promising technique to overcome the bandwidth limitations of current visible light communication (VLC) systems. In this letter, we investigate the power allocation (PA) problem in an NOMA-VLC system under high signal-to-noise-ratio (SNR) regime. A simple fair power allocation strategy (SFPA) is proposed to ensure equitable allocation of transmission resources in a multi-user scenario. SFPA requires minimal channel state information (CSI), making it less prone to channel estimation errors. Results show that NOMA with SFPA provides fairer and higher achievable rates per user (up to 79.5\% higher in the studied setup), without significantly compromising the overall system performance.      
### 29.Generalization Error Bounds for Iterative Recovery Algorithms Unfolded as Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.04364.pdf)
>  Motivated by the learned iterative soft thresholding algorithm (LISTA), we introduce a general class of neural networks suitable for sparse reconstruction from few linear measurements. By allowing a wide range of degrees of weight-sharing between the layers, we enable a unified analysis for very different neural network types, ranging from recurrent ones to networks more similar to standard feedforward neural networks. Based on training samples, via empirical risk minimization we aim at learning the optimal network parameters and thereby the optimal network that reconstructs signals from their low-dimensional linear measurements. We derive generalization bounds by analyzing the Rademacher complexity of hypothesis classes consisting of such deep networks, that also take into account the thresholding parameters. We obtain estimates of the sample complexity that essentially depend only linearly on the number of parameters and on the depth. We apply our main result to obtain specific generalization bounds for several practical examples, including different algorithms for (implicit) dictionary learning, and convolutional neural networks.      
### 30.COSMIC: fast closed-form identification from large-scale data for LTV systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.04355.pdf)
>  We introduce a closed-form method for identification of discrete-time linear time-variant systems from data, formulating the learning problem as a regularized least squares problem where the regularizer favors smooth solutions within a trajectory. We develop a closed-form algorithm with guarantees of optimality and with a complexity that increases linearly with the number of instants considered per trajectory. The COSMIC algorithm achieves the desired result even in the presence of large volumes of data. Our method solved the problem using two orders of magnitude less computational power than a general purpose convex solver and was about 3 times faster than a Stochastic Block Coordinate Descent especially designed method. Computational times of our method remained in the order of magnitude of the second even for 10k and 100k time instants, where the general purpose solver crashed. To prove its applicability to real world systems, we test with spring-mass-damper system and use the estimated model to find the optimal control path. Our algorithm was applied to both a Low Fidelity and Functional Engineering Simulators for the Comet Interceptor mission, that requires precise pointing of the on-board cameras in a fast dynamics environment. Thus, this paper provides a fast alternative to classical system identification techniques for linear time-variant systems, while proving to be a solid base for applications in the Space industry and a step forward to the incorporation of algorithms that leverage data in such a safety-critical environment.      
### 31.Survey of charging scheduling, fleet management, and location planning of charging stations for electrified demand-responsive transport systems: methodologies and recent developments  [ :arrow_down: ](https://arxiv.org/pdf/2112.04221.pdf)
>  The accelerated electrification of transport systems with EVs has brought new challenges for charging scheduling, fleet management, and charging infrastructure location and configuration planning. In this review, we have provided a systematic review of the recent development in strategic, tactical, and operational decisions for demand responsive transport system planning using electric vehicles (EV-DRT). We have summarized recent developments in mathematical modeling approaches and identified future research directions. A list of existing open-access datasets, numerical test instances, and software are provided for future research in EV-DRT and related problems.      
### 32.Learning music audio representations via weak language supervision  [ :arrow_down: ](https://arxiv.org/pdf/2112.04214.pdf)
>  Audio representations for music information retrieval are typically learned via supervised learning in a task-specific fashion. Although effective at producing state-of-the-art results, this scheme lacks flexibility with respect to the range of applications a model can have and requires extensively annotated datasets. In this work, we pose the question of whether it may be possible to exploit weakly aligned text as the only supervisory signal to learn general-purpose music audio representations. To address this question, we design a multimodal architecture for music and language pre-training (MuLaP) optimised via a set of proxy tasks. Weak supervision is provided in the form of noisy natural language descriptions conveying the overall musical content of the track. After pre-training, we transfer the audio backbone of the model to a set of music audio classification and regression tasks. We demonstrate the usefulness of our approach by comparing the performance of audio representations produced by the same audio backbone with different training strategies and show that our pre-training method consistently achieves comparable or higher scores on all tasks and datasets considered. Our experiments also confirm that MuLaP effectively leverages audio-caption pairs to learn representations that are competitive with audio-only and cross-modal self-supervised methods in the literature.      
### 33.Learnable Faster Kernel-PCA for Nonlinear Fault Detection: Deep Autoencoder-Based Realization  [ :arrow_down: ](https://arxiv.org/pdf/2112.04193.pdf)
>  Kernel principal component analysis (KPCA) is a well-recognized nonlinear dimensionality reduction method that has been widely used in nonlinear fault detection tasks. As a kernel trick-based method, KPCA inherits two major problems. First, the form and the parameters of the kernel function are usually selected blindly, depending seriously on trial-and-error. As a result, there may be serious performance degradation in case of inappropriate selections. Second, at the online monitoring stage, KPCA has much computational burden and poor real-time performance, because the kernel method requires to leverage all the offline training data. In this work, to deal with the two drawbacks, a learnable faster realization of the conventional KPCA is proposed. The core idea is to parameterize all feasible kernel functions using the novel nonlinear DAE-FE (deep autoencoder based feature extraction) framework and propose DAE-PCA (deep autoencoder based principal component analysis) approach in detail. The proposed DAE-PCA method is proved to be equivalent to KPCA but has more advantage in terms of automatic searching of the most suitable nonlinear high-dimensional space according to the inputs. Furthermore, the online computational efficiency improves by approximately 100 times compared with the conventional KPCA. With the Tennessee Eastman (TE) process benchmark, the effectiveness and superiority of the proposed method is illustrated.      
### 34.Mobile BCI dataset of scalp- and ear-EEGs with ERP and SSVEP paradigms while standing, walking, and running  [ :arrow_down: ](https://arxiv.org/pdf/2112.04176.pdf)
>  We present a mobile dataset obtained from electroencephalography (EEG) of the scalp and around the ear as well as from locomotion sensors by 24 participants moving at four different speeds while performing two brain-computer interface (BCI) tasks. The data were collected from 32-channel scalp-EEG, 14-channel ear-EEG, 4-channel electrooculography, and 9-channel inertial measurement units placed at the forehead, left ankle, and right ankle. The recording conditions were as follows: standing, slow walking, fast walking, and slight running at speeds of 0, 0.8, 1.6, and 2.0m/s, respectively. For each speed, two different BCI paradigms, event-related potential and steady-state visual evoked potential, were recorded. To evaluate the signal quality, scalp- and ear-EEG data were qualitatively and quantitatively validated during each speed. We believe that the dataset will facilitate BCIs in diverse mobile environments to analyze brain activities and evaluate the performance quantitatively for expanding the use of practical BCIs.      
### 35.Learning Linear Models Using Distributed Iterative Hessian Sketching  [ :arrow_down: ](https://arxiv.org/pdf/2112.04101.pdf)
>  This work considers the problem of learning the Markov parameters of a linear system from observed data. Recent non-asymptotic system identification results have characterized the sample complexity of this problem in the single and multi-rollout setting. In both instances, the number of samples required in order to obtain acceptable estimates can produce optimization problems with an intractably large number of decision variables for a second-order algorithm. We show that a randomized and distributed Newton algorithm based on Hessian-sketching can produce $\epsilon$-optimal solutions and converges geometrically. Moreover, the algorithm is trivially parallelizable. Our results hold for a variety of sketching matrices and we illustrate the theory with numerical examples.      
### 36.KoopmanizingFlows: Diffeomorphically Learning Stable Koopman Operators  [ :arrow_down: ](https://arxiv.org/pdf/2112.04085.pdf)
>  We propose a novel framework for constructing linear time-invariant (LTI) models for data-driven representations of the Koopman operator for a class of stable nonlinear dynamics. The Koopman operator (generator) lifts a finite-dimensional nonlinear system to a possibly infinite-dimensional linear feature space. To utilize it for modeling, one needs to discover finite-dimensional representations of the Koopman operator. Learning suitable features is challenging, as one needs to learn LTI features that are both Koopman-invariant (evolve linearly under the dynamics) as well as relevant (spanning the original state) - a generally unsupervised learning task. For a theoretically well-founded solution to this problem, we propose learning Koopman-invariant coordinates by composing a diffeomorphic learner with a lifted aggregate system of a latent linear model. Using an unconstrained parameterization of stable matrices along with the aforementioned feature construction, we learn the Koopman operator features without assuming a predefined library of functions or knowing the spectrum, while ensuring stability regardless of the operator approximation accuracy. We demonstrate the superior efficacy of the proposed method in comparison to a state-of-the-art method on the well-known LASA handwriting dataset.      
### 37.Scalable 3D Semantic Segmentation for Gun Detection in CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2112.03917.pdf)
>  With the increased availability of 3D data, the need for solutions processing those also increased rapidly. However, adding dimension to already reliably accurate 2D approaches leads to immense memory consumption and higher computational complexity. These issues cause current hardware to reach its limitations, with most methods forced to reduce the input resolution drastically. Our main contribution is a novel deep 3D semantic segmentation method for gun detection in baggage CT scans that enables fast training and low video memory consumption for high-resolution voxelized volumes. We introduce a moving pyramid approach that utilizes multiple forward passes at inference time for segmenting an instance.      
