# ArXiv eess --Tue, 14 Dec 2021
### 1.Actuated Reflector-Based Three-dimensional Ultrasound Imaging with Adaptive-Delay Synthetic Aperture Focusing  [ :arrow_down: ](https://arxiv.org/pdf/2112.06866.pdf)
>  Three-dimensional (3D) ultrasound (US) imaging addresses the limitation in field-of-view (FOV) in conventional two-dimensional (2D) US imaging by providing 3D viewing of the anatomy. 3D US imaging has been extensively adapted for diagnosis and image-guided surgical intervention. However, conventional approaches to implement 3D US imaging require either expensive and sophisticated 2D array transducers, or external actuation mechanisms to move a one-dimensional array mechanically. Here, we propose a 3D US imaging mechanism using actuated acoustic reflector instead of the sensor elements for volume acquisition with significantly extended 3D FOV, which can be implemented with simple hardware and compact size. To improve image quality on the elevation plane, we introduce an adaptive-delay synthetic aperture focusing (AD-SAF) method for elevation beamforming. We first evaluated the proposed imaging mechanism and AD-SAF with simulated point targets and cysts targets. Results of point targets suggested improved image quality on the elevation plane, and results of cysts targets demonstrated a potential to improve 3D visualization of human anatomy. We built a prototype imaging system that has a 3D FOV of 38 mm (lateral) by 38 mm (elevation) by 50 mm (axial) and collected data in imaging experiments with phantoms. Experimental data showed consistency with simulation results. The AD-SAF method enhanced quantifying the cyst volume size in the breast mimicking phantom compared to without elevation beamforming. These results suggested that the proposed 3D US imaging mechanism could potentially be applied in clinical scenarios.      
### 2.Automatic Multi-Class Cardiovascular Magnetic Resonance Image Quality Assessment using Unsupervised Domain Adaptation in Spatial and Frequency Domains  [ :arrow_down: ](https://arxiv.org/pdf/2112.06806.pdf)
>  Population imaging studies rely upon good quality medical imagery before downstream image quantification. This study provides an automated approach to assess image quality from cardiovascular magnetic resonance (CMR) imaging at scale. We identify four common CMR imaging artefacts, including respiratory motion, cardiac motion, Gibbs ringing, and aliasing. The model can deal with images acquired in different views, including two, three, and four-chamber long-axis and short-axis cine CMR images. Two deep learning-based models in spatial and frequency domains are proposed. Besides recognising these artefacts, the proposed models are suitable to the common challenges of not having access to data labels. An unsupervised domain adaptation method and a Fourier-based convolutional neural network are proposed to overcome these challenges. We show that the proposed models reliably allow for CMR image quality assessment. The accuracies obtained for the spatial model in supervised and weakly supervised learning are 99.41+0.24 and 96.37+0.66 for the UK Biobank dataset, respectively. Using unsupervised domain adaptation can somewhat overcome the challenge of not having access to the data labels. The maximum achieved domain gap coverage in unsupervised domain adaptation is 16.86%. Domain adaptation can significantly improve a 5-class classification task and deal with considerable domain shift without data labels. Increasing the speed of training and testing can be achieved with the proposed model in the frequency domain. The frequency-domain model can achieve the same accuracy yet 1.548 times faster than the spatial model. This model can also be used directly on k-space data, and there is no need for image reconstruction.      
### 3.On Stability, Ancillary Services, Operation, and Security of Smart Inverters  [ :arrow_down: ](https://arxiv.org/pdf/2112.06787.pdf)
>  This paper presents some recent trends in the research of grid-interactive inverters. Particularly, this paper focuses on stability, ancillary services, operation, and security of single and multiple inverters in the modern power grid. A grid-interactive inverter performs as a controllable interface between the distributed energy sources and the power grid. High-penetration of inverters in a power distribution system can create some technical challenges on the power quality, as well as voltage and frequency control of the system. In particular, a weak grid can lead to voltage oscillation and consequently instability. Moreover, the power grid is moving toward becoming a cyber-physical system in which smart inverters can exchange information for power marketing and economic dispatching. This puts the inverters at the risk of insecure operations. Hence, security enhancement has become another primary concern. Finally, the grid-interactive inverters are operated proportional power-sharing while operating together with many inverters. Recent research on coordinated operations is also discussed in this paper.      
### 4.Hformer: Hybrid CNN-Transformer for Fringe Order Prediction in Phase Unwrapping of Fringe Projection  [ :arrow_down: ](https://arxiv.org/pdf/2112.06759.pdf)
>  Recently, deep learning has attracted more and more attention in phase unwrapping of fringe projection three-dimensional (3D) measurement, with the aim to improve the performance leveraging the powerful Convolutional Neural Network (CNN) models. In this paper, for the first time (to the best of our knowledge), we introduce the Transformer into the phase unwrapping which is different from CNN and propose Hformer model dedicated to phase unwrapping via fringe order prediction. The proposed model has a hybrid CNN-Transformer architecture that is mainly composed of backbone, encoder and decoder to take advantage of both CNN and Transformer. Encoder and decoder with cross attention are designed for the fringe order prediction. Experimental results show that the proposed Hformer model achieves better performance in fringe order prediction compared with the CNN models such as U-Net and DCNN. Moreover, ablation study on Hformer is made to verify the improved feature pyramid networks (FPN) and testing strategy with flipping in the predicted fringe order. Our work opens an alternative way to deep learning based phase unwrapping methods, which are dominated by CNN in fringe projection 3D measurement.      
### 5.An Open Source Representation for the NYS Electric Grid to Support Power Grid and Market Transition Studies  [ :arrow_down: ](https://arxiv.org/pdf/2112.06756.pdf)
>  Under the increasing need to decarbonize energy systems, there is coupled acceleration in connection of distributed and intermittent renewable resources in power grids. To support this transition, researchers and other stakeholders are embarking on detailed studies and analyses of the evolution of this complex system, which require a validated representation of the essential characteristics of the power grid that is accurate for a specific region of interest. For example, the Climate Leadership and Community Protection Act (CLCPA) in New York State (NYS) sets ambitious targets for the transformation of the energy system, opening many interesting research and analysis questions. To provide a platform for these analyses, this paper presents an overview of the current NYS power grid and develops an open-source (<a class="link-external link-https" href="https://github.com/AndersonEnergyLab-Cornell/NYgrid" rel="external noopener nofollow">this https URL</a>) baseline model using publicly available data. The proposed model is validated with real data for power flow and Locational Marginal Prices (LMPs), demonstrating the feasibility, functionality, and consistency of the model. The model is easily adjustable and customizable for various analyses of future configurations and scenarios that require spatiotemporal information about the NYS power grid with data access to all the available historical data and serves as a practical system for general methods and algorithms testing.      
### 6.Topological Signal Processing over Cell Complexes  [ :arrow_down: ](https://arxiv.org/pdf/2112.06709.pdf)
>  The Topological Signal Processing (TSP) framework has been recently developed to analyze signals defined over simplicial complexes, i.e. topological spaces represented by finite sets of elements that are closed under inclusion of subsets [1]. However, the same inclusion property represents sometimes a too rigid assumption that prevents the application of simplicial complexes to many cases of interest. The goal of this paper is to extend TSP to the analysis of signals defined over cell complexes, which represent a generalization of simplicial complexes, as they are not restricted to satisfy the inclusion property. In particular, the richer topological structure of cell complexes enables them to reveal cycles of any order, as representative of data features. We propose an efficient method to infer the topology of cell complexes from data by showing how their use enables sparser edge signal representations than simplicial-based methods. Furthermore, we show how to design optimal finite impulse response (FIR) filters operating on solenoidal and irrotational signals in order to minimize the approximation error with respect to the desired spectral masks.      
### 7.Hypernet-Ensemble Learning of Segmentation Probability for Medical Image Segmentation with Ambiguous Labels  [ :arrow_down: ](https://arxiv.org/pdf/2112.06693.pdf)
>  Despite the superior performance of Deep Learning (DL) on numerous segmentation tasks, the DL-based approaches are notoriously overconfident about their prediction with highly polarized label probability. This is often not desirable for many applications with the inherent label ambiguity even in human annotations. This challenge has been addressed by leveraging multiple annotations per image and the segmentation uncertainty. However, multiple per-image annotations are often not available in a real-world application and the uncertainty does not provide full control on segmentation results to users. In this paper, we propose novel methods to improve the segmentation probability estimation without sacrificing performance in a real-world scenario that we have only one ambiguous annotation per image. We marginalize the estimated segmentation probability maps of networks that are encouraged to under-/over-segment with the varying Tversky loss without penalizing balanced segmentation. Moreover, we propose a unified hypernetwork ensemble method to alleviate the computational burden of training multiple networks. Our approaches successfully estimated the segmentation probability maps that reflected the underlying structures and provided the intuitive control on segmentation for the challenging 3D medical image segmentation. Although the main focus of our proposed methods is not to improve the binary segmentation performance, our approaches marginally outperformed the state-of-the-arts. The codes are available at \url{<a class="link-external link-https" href="https://github.com/sh4174/HypernetEnsemble" rel="external noopener nofollow">this https URL</a>}.      
### 8.Statistics of the Effective Massive MIMO Channel in Correlated Rician Fading  [ :arrow_down: ](https://arxiv.org/pdf/2112.06692.pdf)
>  Massive MIMO base stations use multiple spatial diversity branches, which are often assumed to be uncorrelated in theoretical work. Correlated branches are considered seldom since they are mathematically less tractable. For correlated Rician fading, only the first- and second-order moments have been explored. To describe propagation environments more accurately, full distribution functions are needed. <br>This manuscript provides these functions for the maximum ratio combining effective channel, a quadratic form of a random complex normal channel vector. Its mean vector and covariance matrix are based on a plane wave model incorporating array geometry, antenna element pattern, power angular spectra and power delay profiles. Closed-form approximations of the distribution functions are presented, to allow the fast evaluation of many real-world scenarios. <br>The statistical framework is used to show that low-directivity antenna elements provide better performance in angular constricted Rician fading with off-axis incidence than high-directivity elements. Moreover, two base station array layouts are compared, showing that a half-circle array illuminates a cell more evenly than a uniform linear array. With the full distribution functions available, performance can be compared over the full range of received powers and not only based on the average SNR.      
### 9.MIMO Radar Transmit Beampattern Shaping for Spectrally Dense Environments  [ :arrow_down: ](https://arxiv.org/pdf/2112.06670.pdf)
>  Designing unimodular waveforms with a desired beampattern, spectral occupancy and orthogonality level is of vital importance in the next generation Multiple-Input Multiple-Output (MIMO) radar systems. Motivated by this fact, in this paper, we propose a framework for shaping the beampattern in MIMO radar systems under the constraints simultaneously ensuring unimodularity, desired spectral occupancy and orthogonality of the designed waveform. In this manner, the proposed framework is the most comprehensive approach for MIMO radar waveform design focusing on beampattern shaping. The problem formulation leads to a non-convex quadratic fractional programming. We propose an effective iterative to solve the problem, where each iteration is composed of a Semi-Definite Programming (SDP) followed by eigenvalue decomposition. Some numerical simulations are provided to illustrate the superior performance of our proposed over the state-of-the-art.      
### 10.Long-Term Benefits for Renewables Integration of Network Boosters for Corrective Grid Security  [ :arrow_down: ](https://arxiv.org/pdf/2112.06667.pdf)
>  Using corrective actions to overcome network loading when single lines fail has the potential to free up network capacity that is otherwise underused in preventive N-1 security strategies. We investigate the impact on renewable integration of a corrective network security strategy, whereby storage or other flexibility assets are used to correct overloading that results from line outages. In a 50-bus model of the German power system utilizing these flexibility assets, so-called network boosters (NB), we find significant cost savings for the integration of renewable energy of up to 0.85 billion euros per year. While previous literature has focused on the potential savings in the short-term operation, we focus on the long-term benefits in systems with high shares of renewable energy sources, where the capacities and dispatch of generation and NB are optimised. We demonstrate the benefits of NB for various shares of renewable energy, NB and flexibility costs, as well as different allowed levels of temporary overloading the lines in both (i) a sequential model, where long-run generation investments are optimised separately from the NB capacities, and (ii) a simultaneous model, where generation is co-optimised with NB investment so that mixed preventive-corrective approaches are possible.      
### 11.You Can Wash Better: Daily Handwashing Assessment with Smartwatches  [ :arrow_down: ](https://arxiv.org/pdf/2112.06657.pdf)
>  We propose UWash, an intelligent solution upon smartwatches, to assess handwashing for the purpose of raising users' awareness and cultivating habits in high-quality handwashing. UWash can identify the onset/offset of handwashing, measure the duration of each gesture, and score each gesture as well as the entire procedure in accordance with the WHO guidelines. Technically, we address the task of handwashing assessment as the semantic segmentation problem in computer vision, and propose a lightweight UNet-like network, only 496KBits, to achieve it effectively. Experiments over 51 subjects show that UWash achieves the accuracy of 92.27\% on sample-wise handwashing gesture recognition, $&lt;$0.5 \textit{seconds} error in onset/offset detection, and $&lt;$5 out of 100 \textit{points} error in scoring in the user-dependent setting, while remains promising in the cross-user evaluation and in the cross-user-cross-location evaluation.      
### 12.mREAL-GAN: Generating Multiple Residential Electrical Appliance Load Profiles with Inter-Dependencies using a Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2112.06656.pdf)
>  In this paper, we introduce mREAL-GAN, a generative adversarial network (GAN) for the parallel generation of multiple residential electrical appliance load (mREAL) profiles. mREAL-GAN is intended for use in community-scale low-voltage network analysis, and represents a departure from previous methods for this purpose, which break the generation of appliance load profiles into several steps and largely model each appliance independently. Instead, mREAL-GAN models appliance load profiles in an end-to-end manner, and generates multiple appliance load profiles in parallel in a way that captures inter-dependencies. We show that mREAL-GAN generates load profiles for individual appliance-types with greater fidelity than a popular example of previous methods, and demonstrate its ability to capture inter-dependencies between appliances.      
### 13.Towards Open-World EEG Decoding via Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.06654.pdf)
>  Electroencephalogram (EEG) decoding aims to identify the perceptual, semantic, and cognitive content of neural processing based on non-invasively measured brain activity. Traditional EEG decoding methods have achieved moderate success when applied to data acquired in static, well-controlled lab environments. However, an open-world environment is a more realistic setting, where situations affecting EEG recordings can emerge unexpectedly, significantly weakening the robustness of existing methods. In recent years, deep learning (DL) has emerged as a potential solution for such problems due to its superior capacity in feature extraction. It overcomes the limitations of defining `handcrafted' features or features extracted using shallow architectures, but typically requires large amounts of costly, expertly-labelled data - something not always obtainable. Combining DL with domain-specific knowledge may allow for development of robust approaches to decode brain activity even with small-sample data. Although various DL methods have been proposed to tackle some of the challenges in EEG decoding, a systematic tutorial overview, particularly for open-world applications, is currently lacking. This article therefore provides a comprehensive survey of DL methods for open-world EEG decoding, and identifies promising research directions to inspire future studies for EEG decoding in real-world applications.      
### 14.DriPP: Driven Point Processes to Model Stimuli Induced Patterns in M/EEG Signals  [ :arrow_down: ](https://arxiv.org/pdf/2112.06652.pdf)
>  The quantitative analysis of non-invasive electrophysiology signals from electroencephalography (EEG) and magnetoencephalography (MEG) boils down to the identification of temporal patterns such as evoked responses, transient bursts of neural oscillations but also blinks or heartbeats for data cleaning. Several works have shown that these patterns can be extracted efficiently in an unsupervised way, e.g., using Convolutional Dictionary Learning. This leads to an event-based description of the data. Given these events, a natural question is to estimate how their occurrences are modulated by certain cognitive tasks and experimental manipulations. To address it, we propose a point process approach. While point processes have been used in neuroscience in the past, in particular for single cell recordings (spike trains), techniques such as Convolutional Dictionary Learning make them amenable to human studies based on EEG/MEG signals. We develop a novel statistical point process model-called driven temporal point processes (DriPP)-where the intensity function of the point process model is linked to a set of point processes corresponding to stimulation events. We derive a fast and principled expectation-maximization (EM) algorithm to estimate the parameters of this model. Simulations reveal that model parameters can be identified from long enough signals. Results on standard MEG datasets demonstrate that our methodology reveals event-related neural responses-both evoked and induced-and isolates non-task specific temporal patterns.      
### 15.Accoustate: Auto-annotation of IMU-generated Activity Signatures under Smart Infrastructure  [ :arrow_down: ](https://arxiv.org/pdf/2112.06651.pdf)
>  Human activities within smart infrastructures generate a vast amount of IMU data from the wearables worn by individuals. Many existing studies rely on such sensory data for human activity recognition (HAR); however, one of the major bottlenecks is their reliance on pre-annotated or labeled data. Manual human-driven annotations are neither scalable nor efficient, whereas existing auto-annotation techniques heavily depend on video signatures. Still, video-based auto-annotation needs high computation resources and has privacy concerns when the data from a personal space, like a smart-home, is transferred to the cloud. This paper exploits the acoustic signatures generated from human activities to label the wearables' IMU data at the edge, thus mitigating resource requirement and data privacy concerns. We utilize acoustic-based pre-trained HAR models for cross-modal labeling of the IMU data even when two individuals perform simultaneous but different activities under the same environmental context. We observe that non-overlapping acoustic gaps exist with a high probability during the simultaneous activities performed by two individuals in the environment's acoustic context, which helps us resolve the overlapping activity signatures to label them individually. A principled evaluation of the proposed approach on two real-life in-house datasets further augmented to create a dual occupant setup, shows that the framework can correctly annotate a significant volume of unlabeled IMU data from both individuals with an accuracy of $\mathbf{82.59\%}$ ($\mathbf{\pm 17.94\%}$) and $\mathbf{98.32\%}$ ($\mathbf{\pm 3.68\%}$), respectively, for a workshop and a kitchen environment.      
### 16.Deep learning in automated ultrasonic NDE -- developments, axioms and opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2112.06650.pdf)
>  The analysis of ultrasonic NDE data has traditionally been addressed by a trained operator manually interpreting data with the support of rudimentary automation tools. Recently, many demonstrations of deep learning (DL) techniques that address individual NDE tasks (data acquisition, data pre-processing, defect detection, and defect characterisation) have started to emerge in the research community. These methods have the potential to offer high flexibility, efficiency, and accuracy subject to the availability of sufficient training data; moreover, they enable the automation of complex processes that span one or more NDE steps (e.g. detection, characterisation, and sizing). There is, however, a lack of consensus on the direction and requirements that these new methods should follow. These elements are critical to help achieve full artificial intelligence driven automation of ultrasonic NDE so that the research community, industry, and regulatory bodies embrace them. This paper reviews the state-of-the-art of autonomous ultrasonic NDE enabled by DL methodologies. The review is organised by the NDE tasks that are addressed by means of DL approaches. Key remaining challenges for each task are noted. Basic axiomatic principles for DL methods in NDE are identified based on the literature review, relevant international regulations, and current industrial needs. By placing DL methods in the context of general NDE automation levels, this paper aims to provide a roadmap for future research and development in the area.      
### 17.Contactless Electrocardiogram Monitoring with Millimeter Wave Radar  [ :arrow_down: ](https://arxiv.org/pdf/2112.06639.pdf)
>  The electrocardiogram (ECG) has always been an important measurement scheme to assess and diagnose cardiovascular diseases. However, the role of ECG monitoring in clinical and daily-life practice are limited by the intrusive equipment and inconvenient manual operation. Here we report the development of a prototype deep learning millimeter radar system to measure cardiac activity and reconstruct ECG without any contact in. The system provides hybrid pipeline of signal processing and deep learning that consists of cardiac activity measurements algorithm from RF signal and interpretable neural network for ECG reconstruction which incorporate domain knowledge of radio frequency (RF) signal and physiological models. The experimental results show that our contactless ECG measurements can timing the Q-peaks, R-peaks, S-peaks, T-peaks, R-R intervals with median error of 14ms, 3ms, 8ms, 10ms, 3ms respectively. In addition, the morphology analysis shows that our results achieve the median Pearson-Correlation of 90% and median Root-Mean-Square-Error of 0.081mv compared to the ground truth ECG. These results indicate that the system enables the potential of contactless, long-time continuous and accurate ECG monitoring, which could facilitate its use in a variety of clinical and daily-life environments.      
### 18.Efficient Training of Volterra Series-Based Pre-distortion Filter Using Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.06637.pdf)
>  We present a simple, efficient "direct learning" approach to train Volterra series-based digital pre-distortion filters using neural networks. We show its superior performance over conventional training methods using a 64-QAM 64-GBaud simulated transmitter with varying transmitter nonlinearity and noisy conditions.      
### 19.gACSON software for automated segmentation and morphology analyses of myelinated axons in 3D electron microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2112.06476.pdf)
>  Background and Objective: Advances in electron microscopy (EM) now allow three-dimensional (3D) imaging of hundreds of micrometers of tissue with nanometer-scale resolution, providing new opportunities to study the ultrastructure of the brain. In this work, we introduce a freely available gACSON software for visualization, segmentation, assessment, and morphology analysis of myelinated axons in 3D-EM volumes of brain tissue samples. Methods: The gACSON software is equipped with a graphical user interface (GUI). It automatically segments the intra-axonal space of myelinated axons and their corresponding myelin sheaths and allows manual segmentation, proofreading, and interactive correction of the segmented components. gACSON analyzes the morphology of myelinated axons, such as axonal diameter, axonal eccentricity, myelin thickness, or g-ratio. Results: We illustrate the use of gACSON by segmenting and analyzing myelinated axons in six 3D-EM volumes of rat somatosensory cortex after sham surgery or traumatic brain injury (TBI). Our results suggest that the equivalent diameter of myelinated axons in somatisensory cortex was decreased in TBI animals five months after the injury. Conclusions: Our results indicate that gACSON is a valuable tool for visualization, segmentation, assessment, and morphology analysis of myelinated axons in 3D-EM volumes. gACSON is freely available at <a class="link-external link-https" href="https://github.com/AndreaBehan/g-ACSON" rel="external noopener nofollow">this https URL</a> under the MIT license.      
### 20.LC-FDNet: Learned Lossless Image Compression with Frequency Decomposition Network  [ :arrow_down: ](https://arxiv.org/pdf/2112.06417.pdf)
>  Recent learning-based lossless image compression methods encode an image in the unit of subimages and achieve comparable performances to conventional non-learning algorithms. However, these methods do not consider the performance drop in the high-frequency region, giving equal consideration to the low and high-frequency areas. In this paper, we propose a new lossless image compression method that proceeds the encoding in a coarse-to-fine manner to separate and process low and high-frequency regions differently. We initially compress the low-frequency components and then use them as additional input for encoding the remaining high-frequency region. The low-frequency components act as a strong prior in this case, which leads to improved estimation in the high-frequency area. In addition, we design the frequency decomposition process to be adaptive to color channel, spatial location, and image characteristics. As a result, our method derives an image-specific optimal ratio of low/high-frequency components. Experiments show that the proposed method achieves state-of-the-art performance for benchmark high-resolution datasets.      
### 21.Consensus-Based Distributed Filtering with Fusion Step Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2112.06395.pdf)
>  For consensus on measurement-based distributed filtering (CMDF), through infinite consensus fusion operations during each sampling interval, each node in the sensor network can achieve optimal filtering performance with centralized filtering. However, due to the limited communication resources in physical systems, the number of fusion steps cannot be infinite. To deal with this issue, the present paper analyzes the performance of CMDF with finite consensus fusion operations. First, by introducing a modified discrete-time algebraic Riccati equation and several novel techniques, the convergence of the estimation error covariance matrix of each sensor is guaranteed under a collective observability condition. In particular, the steady-state covariance matrix can be simplified as the solution to a discrete-time Lyapunov equation. Moreover, the performance degradation induced by reduced fusion frequency is obtained in closed form, which establishes an analytical relation between the performance of the CMDF with finite fusion steps and that of centralized filtering. Meanwhile, it provides a trade-off between the filtering performance and the communication cost. Furthermore, it is shown that the steady-state estimation error covariance matrix exponentially converges to the centralized optimal steady-state matrix with fusion operations tending to infinity during each sampling interval. Finally, the theoretical results are verified with illustrative numerical experiments.      
### 22.Dynamic Exploitation Gaussian Bare-Bones Bat Algorithm for Optimal Reactive Power Dispatch to Improve the Safety and Stability of Power System  [ :arrow_down: ](https://arxiv.org/pdf/2112.06382.pdf)
>  In this paper, a novel Gaussian bare-bones bat algorithm (GBBBA) and its modified version named as dynamic exploitation Gaussian bare-bones bat algorithm (DeGBBBA) are proposed for solving optimal reactive power dispatch (ORPD) problem. The optimal reactive power dispatch (ORPD) plays a fundamental role in ensuring stable, secure, reliable as well as economical operation of the power system. The ORPD problem is formulated as a complex and nonlinear optimization problem of mixed integers including both discrete and continuous control variables. Bat algorithm (BA) is one of the most popular metaheuristic algorithms which mimics the echolocation of the microbats and which has also outperformed some other metaheuristic algorithms in solving various optimization problems. Nevertheless, the standard BA may fail to balance exploration and exploitation for some optimization problems and hence it may often fall into local optima. The proposed GBBBA employs the Gaussian distribution in updating the bat positions in an effort to mitigate the premature convergence problem associated with the standard BA. The GBBBA takes advantages of Gaussian sampling which begins from exploration and continues to exploitation. DeGBBBA is an advanced variant of GBBBA in which a modified Gaussian distribution is introduced so as to allow the dynamic adaptation of exploitation and exploitation in the proposed algorithm. Both GBBBA and DeGBBBA are used to determine the optimal settings of generator bus voltages, tap setting transformers and shunt reactive sources in order to minimize the active power loss, total voltage deviations and voltage stability index. Simulation results show that GBBBA and DeGBBBA are robust and effective in solving the ORPD problem.      
### 23.Reconfigurable Holographic Surfaces for Future Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2112.06372.pdf)
>  Future wireless communications look forward to constructing a ubiquitous intelligent information network with high data rates through cost-efficient devices. Benefiting from the tunability and programmability of metamaterials, the reconfigurable holographic surface (RHS) composed of numerous metamaterial radiation elements is developed as a promising solution to fulfill such challenging visions. The RHS is more likely to serve as an ultra-thin and lightweight surface antenna integrated with the transceiver to generate beams with desirable directions by leveraging the holographic principle. This is different from reconfigurable intelligent surfaces (RISs) widely used as passive relays due to the reflection characteristic. In this article, we investigate RHS-aided wireless communications. Starting with a basic introduction of the RHS including its hardware structure, holographic principle, and fabrication methodologies, we propose a hybrid beamforming scheme for RHS-aided multi-user communication systems. A joint sum-rate maximization algorithm is then developed where the digital beamforming performed at the base station and the holographic beamforming performed at the RHS are optimized iteratively. Furthermore, key challenges in RHS-aided wireless communications are also discussed.      
### 24.Time-of-use Pricing for Energy Storage Investment  [ :arrow_down: ](https://arxiv.org/pdf/2112.06358.pdf)
>  Time-of-use (ToU) pricing is widely used by the electricity utility to shave peak load. Such a pricing scheme provides users with incentives to invest in behind-the-meter energy storage and to shift peak load towards low-price intervals. However, without considering the implication on energy storage investment, an improperly designed ToU pricing scheme may lead to significant welfare loss, especially when users over-invest the storage, which leads to new energy consumption peaks. In this paper, we will study how to design a social-optimum ToU pricing scheme by explicitly considering its impact on storage investment. We model the interactions between the utility and users as a two-stage optimization problem. To resolve the challenge of asymmetric information due to users' private storage cost, we propose a ToU pricing scheme based on different storage types and the aggregate demand per type. Each user does not need to reveal his private cost information. We can further compute the optimal ToU pricing with only a linear complexity. Simulations based on real-world data show that the suboptimality gap of our proposed ToU pricing, compared with the social optimum achieved under complete information, is less than 5%.      
### 25.Knowledge-based optimal irrigation scheduling of agro-hydrological systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.06354.pdf)
>  The typical agricultural irrigation scheduler provides information on how much to irrigate and when to irrigate. The accurate and effective scheduler decision for a large agricultural field is still an open research problem. In this work, we address the high dimensionality of the agricultural field and propose a systematic approach to provide optimum irrigation amount and irrigation time for three-dimensional agro-hydrological systems. The water dynamics of the agro-hydrological system are represented using a cylindrical three-dimensional Richards Equation. We introduce a structure-preserving model reduction technique to decrease the dimension of the system model. Using the reduced model, the optimization-based closed-loop scheduler is designed in model predictive control (MPC) environment. The closed-loop approach can handle weather disturbances and provide improved yield and water conservation. The primary objective of the proposed scheduler is to ensure maximum yield, minimum water consumption and maximize the time between the two irrigation events, which results in less electricity usage. The proposed approach is applied to three different scenarios to show the effectiveness and superiority of the proposed framework.      
### 26.LSTM-based model predictive control with discrete inputs for irrigation scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2112.06352.pdf)
>  The development of well-devised irrigation scheduling methods is desirable from the perspectives of plant quality and water conservation. In this article, a model predictive control (MPC) with discrete actuators is developed for irrigation scheduling, where a long short-term memory (LSTM) model of the soil-water-atmosphere system is used to evaluate the objective of ensuring optimal water uptake in crops while minimizing total water consumption and irrigation costs. A heuristic method involving a sigmoid function is used in this framework to enhance the computational efficiency of the scheduler. The scheduling scheme is applied to homogeneous and spatially variable fields and the results indicate that the LSTM-based MPC with discrete actuators is able to prescribe optimal or near-optimal irrigation schedules that are typical of irrigation practice.      
### 27.DPICT: Deep Progressive Image Compression Using Trit-Planes  [ :arrow_down: ](https://arxiv.org/pdf/2112.06334.pdf)
>  We propose the deep progressive image compression using trit-planes (DPICT) algorithm, which is the first learning-based codec supporting fine granular scalability (FGS). First, we transform an image into a latent tensor using an analysis network. Then, we represent the latent tensor in ternary digits (trits) and encode it into a compressed bitstream trit-plane by trit-plane in the decreasing order of significance. Moreover, within each trit-plane, we sort the trits according to their rate-distortion priorities and transmit more important information first. Since the compression network is less optimized for the cases of using fewer trit-planes, we develop a postprocessing network for refining reconstructed images at low rates. Experimental results show that DPICT outperforms conventional progressive codecs significantly, while enabling FGS transmission.      
### 28.The Role of Distributed Energy Resources in Distribution System Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2112.06317.pdf)
>  With increasing levels of distributed energy resources (DERs) connected to the grid, it is important to understand the role that DERs can play in post-disaster restoration. In this paper, we propose a two-step optimization method to identify and implement an optimal restoration schedule under different DER operating scenarios. We investigate how the presence and geographical distribution of DERs change the optimal restoration order, and assess the impacts on customers with and without DERs. In our case study using the IEEE 123 single phase distribution system, we find that optimal restoration order changes significantly when DERs are concentrated in one part of the grid. We also observe that the presence of DERs generally reduces the energy not served across all customers and can help prioritize grid reconnection of customers without DERs.      
### 29.Attention based Broadly Self-guided Network for Low light Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2112.06226.pdf)
>  During the past years,deep convolutional neural networks have achieved impressive success in low-light Image Enhancement.Existing deep learning methods mostly enhance the ability of feature extraction by stacking network structures and deepening the depth of the network.which causes more runtime cost on single <a class="link-external link-http" href="http://image.In" rel="external noopener nofollow">this http URL</a> order to reduce inference time while fully extracting local features and global features.Inspired by SGN,we propose a Attention based Broadly self-guided network (ABSGN) for real world low-light image Enhancement.such a broadly strategy is able to handle the noise at different exposures.The proposed network is validated by many mainstream benchmark.Additional experimental results show that the proposed network outperforms most of state-of-the-art low-light image Enhancement solutions.      
### 30.Improving Performance of Federated Learning based Medical Image Analysis in Non-IID Settings using Image Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2112.06194.pdf)
>  Federated Learning (FL) is a suitable solution for making use of sensitive data belonging to patients, people, companies, or industries that are obligatory to work under rigid privacy constraints. FL mainly or partially supports data privacy and security issues and provides an alternative to model problems facilitating multiple edge devices or organizations to contribute a training of a global model using a number of local data without having them. Non-IID data of FL caused from its distributed nature presents a significant performance degradation and stabilization skews. This paper introduces a novel method dynamically balancing the data distributions of clients by augmenting images to address the non-IID data problem of FL. The introduced method remarkably stabilizes the model training and improves the model's test accuracy from 83.22% to 89.43% for multi-chest diseases detection of chest X-ray images in highly non-IID FL setting. The results of IID, non-IID and non-IID with proposed method federated trainings demonstrated that the proposed method might help to encourage organizations or researchers in developing better systems to get values from data with respect to data privacy not only for healthcare but also other fields.      
### 31.Learning Token-based Representation for Image Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2112.06159.pdf)
>  In image retrieval, deep local features learned in a data-driven manner have been demonstrated effective to improve retrieval performance. To realize efficient retrieval on large image database, some approaches quantize deep local features with a large codebook and match images with aggregated match kernel. However, the complexity of these approaches is non-trivial with large memory footprint, which limits their capability to jointly perform feature learning and aggregation. To generate compact global representations while maintaining regional matching capability, we propose a unified framework to jointly learn local feature representation and aggregation. In our framework, we first extract deep local features using CNNs. Then, we design a tokenizer module to aggregate them into a few visual tokens, each corresponding to a specific visual pattern. This helps to remove background noise, and capture more discriminative regions in the image. Next, a refinement block is introduced to enhance the visual tokens with self-attention and cross-attention. Finally, different visual tokens are concatenated to generate a compact global representation. The whole framework is trained end-to-end with image-level labels. Extensive experiments are conducted to evaluate our approach, which outperforms the state-of-the-art methods on the Revisited Oxford and Paris datasets.      
### 32.Two New Stenosis Detection Methods of Coronary Angiograms  [ :arrow_down: ](https://arxiv.org/pdf/2112.06149.pdf)
>  Coronary angiography is the "gold standard" for diagnosing coronary artery disease (CAD). At present, the methods for detecting and evaluating coronary artery stenosis cannot satisfy the clinical needs, e.g., there is no prior study of detecting stenoses in prespecified vessel segments, which is necessary in clinical practice. Two vascular stenosis detection methods are proposed to assist the diagnosis. The first one is an automatic method, which can automatically extract the entire coronary artery tree and mark all the possible stenoses. The second one is an interactive method. With this method, the user can choose any vessel segment to do further analysis of its stenoses. Experiments show that the proposed methods are robust for angiograms with various vessel structures. The precision, sensitivity, and $F_1$ score of the automatic stenosis detection method are 0.821, 0.757, and 0.788, respectively. Further investigation proves that the interactive method can provide a more precise outcome of stenosis detection, and our quantitative analysis is closer to reality. The proposed automatic method and interactive method are effective and can complement each other in clinical practice. The first method can be used for preliminary screening, and the second method can be used for further quantitative analysis. We believe the proposed solution is more suitable for the clinical diagnosis of CAD.      
### 33.Continuous Human Action Detection Based on Wearable Inertial Data  [ :arrow_down: ](https://arxiv.org/pdf/2112.06091.pdf)
>  Human action detection is a hot topic, which is widely used in video surveillance, human machine interface, healthcare monitoring, gaming, dancing training and musical instrument teaching. As inertial sensors are low cost, portable, and having no operating space, it is suitable to detect human action. In real-world applications, actions that are of interest appear among actions of non interest without pauses in between. Recognizing and detecting actions of interests from continuous action streams is more challenging and useful for real applications. Based on inertial sensor and C-MHAD smart TV gesture recognition dataset, this paper utilized different inertial sensor feature formats, then compared the performance with different deep neural network structures according to these feature formats. Experiment results show the best performance was achieved by image based inertial feature with convolution neural network, which got 51.1% F1 score.      
### 34.Spatial Graph Convolutional Neural Network via Structured Subdomain Adaptation and Domain Adversarial Learning for Bearing Fault Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2112.06033.pdf)
>  Unsupervised domain adaptation (UDA) has shown remarkable results in bearing fault diagnosis under changing working conditions in recent years. However, most UDA methods do not consider the geometric structure of the data. Furthermore, the global domain adaptation technique is commonly applied, which ignores the relation between subdomains. This paper addresses mentioned challenges by presenting the novel deep subdomain adaptation graph convolution neural network (DSAGCN), which has two key characteristics: First, graph convolution neural network (GCNN) is employed to model the structure of data. Second, adversarial domain adaptation and local maximum mean discrepancy (LMMD) methods are applied concurrently to align the subdomain's distribution and reduce structure discrepancy between relevant subdomains and global domains. CWRU and Paderborn bearing datasets are used to validate the DSAGCN method's efficiency and superiority between comparison models. The experimental results demonstrate the significance of aligning structured subdomains along with domain adaptation methods to obtain an accurate data-driven model in unsupervised fault diagnosis.      
### 35.Unsupervised Image to Image Translation for Multiple Retinal Pathology Synthesis in Optical Coherence Tomography Scans  [ :arrow_down: ](https://arxiv.org/pdf/2112.06031.pdf)
>  Image to Image Translation (I2I) is a challenging computer vision problem used in numerous domains for multiple tasks. Recently, ophthalmology became one of the major fields where the application of I2I is increasing rapidly. One such application is the generation of synthetic retinal optical coherence tomographic (OCT) scans. Existing I2I methods require training of multiple models to translate images from normal scans to a specific pathology: limiting the use of these models due to their complexity. To address this issue, we propose an unsupervised multi-domain I2I network with pre-trained style encoder that translates retinal OCT images in one domain to multiple domains. We assume that the image splits into domain-invariant content and domain-specific style codes, and pre-train these style codes. The performed experiments show that the proposed model outperforms state-of-the-art models like MUNIT and CycleGAN synthesizing diverse pathological scans.      
### 36.Optimization of Residual Convolutional Neural Network for Electrocardiogram Classification  [ :arrow_down: ](https://arxiv.org/pdf/2112.06024.pdf)
>  The interpretation of the electrocardiogram (ECG) gives clinical information and helps in the assessing of the heart function. There are distinct ECG patterns associated with a specific class of arrythmia. The convolutional neural network is actually one of the most applied deep learning algorithms in ECG processing. However, with deep learning models there are many more hyperparameters to tune. Selecting an optimum or best hyperparameter for the convolutional neural network algorithm is challenging. Often, we end up tuning the model manually with different possible range of values until a best fit model is obtained. Automatic hyperparameters tuning using Bayesian optimization (BO) and evolutionary algorithms brings a solution to the harbor manual configuration. In this paper, we propose to optimize the Recurrent one Dimensional Convolutional Neural Network model (R-1D-CNN) with two levels. At the first level, a residual convolutional layer and one-dimensional convolutional neural layers are trained to learn patient-specific ECG features over which the multilayer perceptron layers can learn to produce the final class vectors of each input. This level is manual and aims to lower the search space. The second level is automatic and based on proposed algorithm based BO. Our proposed optimized R-1D-CNN architecture is evaluated on two publicly available ECG Datasets. The experimental results display that the proposed algorithm based BO achieves an optimum rate of 99.95\%, while the baseline model achieves 99.70\% for the MIT-BIH database. Moreover, experiments demonstrate that the proposed architecture fine-tuned with BO achieves a higher accuracy than the other proposed architectures. Our architecture achieves a good result compared to previous works and based on different experiments.      
### 37.An Overview of Signal Processing Techniques for RIS/IRS-aided Wireless Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.05989.pdf)
>  In the past as well as present wireless communication systems, the wireless propagation environment is regarded as an uncontrollable black box that impairs the received signal quality, and its negative impacts are compensated for by relying on the design of various sophisticated transmission/reception schemes. However, the improvements through applying such schemes operating at two endpoints (i.e., transmitter and receiver) only are limited even after five generations of wireless systems. Reconfigurable intelligent surface (RIS) or intelligent reflecting surface (IRS) have emerged as a new and revolutionary technology that can configure the wireless environment in a favorable manner by properly tuning the phase shifts of a large number of quasi passive and low-cost reflecting elements, thus standing out as a promising candidate technology for the next-/sixth-generation (6G) wireless system. However, to reap the performance benefits promised by RIS/IRS, efficient signal processing techniques are crucial, for a variety of purposes such as channel estimation, transmission design, radio localization, and so on. In this paper, we provide a comprehensive overview of recent advances on RIS/IRS-aided wireless systems from the signal processing perspective. We also highlight promising research directions that are worthy of investigation in the future.      
### 38.Iterative Distributed Model Predictive Control in the Presence of Coupling State Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2112.05965.pdf)
>  In this paper, a distributed model predictive controller (MPC) is developed for the control of distributed dynamic systems with bounded dynamic couplings subject to state constraints, coupling state constraints and input constraints. For linear dynamics, dynamic couplings can be bounded due to coupling constraints. In the proposed control scheme, all subsystems solve their respective local optimization problem in parallel requiring only neighbor-to-neighbor communication. Consistency constraints, which ensure that a subsystem's actual state trajectory remains in the neighborhood of its previously communicated reference trajectory, are employed to bound the uncertainties resulting from the simultaneous evaluation of the local optimization problems. The disturbances caused by these uncertainties are handled with a robust tube-based MPC approach. Due to the bounded dynamic couplings among the subsystems, an iterative procedure can be invoked to overcome the restrictions on the degrees of freedom of the local optimization problems caused by the consistency constraints. In the end, the controller design procedure is illustrated with an analytically tractable example, and the algorithm's applicability is demonstrated with a simulation.      
### 39.Applying a System Dynamics Approach for the Pharmaceutical Industry: Simulation and Optimization of the Quality Control Process  [ :arrow_down: ](https://arxiv.org/pdf/2112.05951.pdf)
>  As countries interact more and more, technology gains a decisive role in facilitating today's increased need for interconnection. At the same time, systems, becoming more advanced as technology progresses, feed each other and can produce highly complex and unpredictable results. However, with this ever-increasing need for interconnected operations, complex problems arise that need to be effectively tackled. This need extends far beyond the scientific and mechanical fields, covering every aspect of life. Systemic Thinking Philosophy and the System Dynamics methodology now seem to be more relevant than ever and their practical implementation in real-life industrial cases has started to become a trend. Companies that decide to implement such approaches can achieve significant improvements to the effectiveness of their operations and gain a competitive advantage. This research, influenced by the Systemic Thinking Philosophy, applies a System Dynamics approach in practice by improving the quality control process of a pharmaceutical company. The process is modeled, simulated, analyzed, and improvements are performed to achieve more effective and efficient operations. The results show that all these steps led to a successful identification and optimization of the critical factors, and a significant process improvement was achieved.      
### 40.Circuit Characterization of IRS to Control Beamforming Design for Efficient Wireless Communication  [ :arrow_down: ](https://arxiv.org/pdf/2112.05944.pdf)
>  Intelligent reflecting surface (IRS) has emerged as a transforming solution to enrich wireless communications by efficiently reconfiguring the propagation environment. In this paper, a novel IRS circuit characterization model is proposed for practical beamforming design incorporating various electrical parameters of the meta-surface unit cell. Specifically, we have modelled the IRS control parameters, phase shift (PS) and reflection amplitude (RA) at the communication receiver, in addition to the circuit level parameter, variable effective capacitance $C$ of IRS unit cell. We have obtained closed-form expressions of PS, RA and $C$ in terms of transmission frequency of signal incident to IRS and various electrical parameters of IRS circuit, with a novel touch towards an accurate analytical model for a better beamforming design perspective. Numerical results demonstrate the efficacy of the proposed characterization thereby providing key design insights.      
### 41.Cascode Cross-Coupled Stage High-Speed Dynamic Comparator in 65 nm  [ :arrow_down: ](https://arxiv.org/pdf/2112.05924.pdf)
>  Dynamic comparators are the core of high-speed, high-resolution analog-to-digital converters (ADCs) used for communication applications. Most of the dynamic comparators attain high-speed operation only for sufficiently high input difference voltages. The comparator performance degrades at small input difference voltages due to a limited pre-amplifier gain, which is undesirable for high-speed, high-resolution ADCs. To overcome this drawback, a cascode cross-coupled dynamic comparator is presented. The proposed comparator improves the differential gain of the pre-amplifier and reduces the common-mode voltage seen by the latch, which leads to a much faster regeneration at small input difference voltages. The proposed comparator is designed, simulated, and compared with the state-of-the-art techniques in 65 nm CMOS technology. The results demonstrate that the proposed comparator achieves a delay of 46.5 ps at 1 mV input difference, and a supply of 1.1 V.      
### 42.Simultaneous Localization and Mapping: Through the Lens of Nonlinear Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2112.05921.pdf)
>  Simultaneous Localization and Mapping (SLAM) algorithms perform visual-inertial estimation via filtering or batch optimization methods. Empirical evidence suggests that filtering algorithms are computationally faster, while optimization methods are more accurate. This work presents an optimization-based framework that unifies these approaches, and allows users to flexibly implement different design choices, e.g., the number and types of variables maintained in the algorithm at each time. We prove that filtering methods correspond to specific design choices in our generalized framework. We then reformulate the Multi-State Constrained Kalman Filter (MSCKF), implement the reformulation on challenging image sequence datasets in simulation, and contrast its performance with that of sliding window based filters. Using these results, we explain the relative performance characteristics of these two classes of algorithms in the context of our algorithm. Finally, we illustrate that under different design choices, the empirical performance of our algorithm interpolates between those of state-of-the-art approaches.      
### 43.Personalized Highway Pilot Assist Considering Leading Vehicle's Lateral Behaviours  [ :arrow_down: ](https://arxiv.org/pdf/2112.05913.pdf)
>  Highway pilot assist has become the front line of competition in advanced driver assistance systems. The increasing requirements on safety and user acceptance are calling for personalization in the development process of such systems. Inspired by a finding on drivers' car-following preferences on lateral direction, a personalized highway pilot assist algorithm is proposed, which consists of an Intelligent Driver Model (IDM) based speed control model and a novel lane-keeping model considering the leading vehicle's lateral movement. A simulated driving experiment is conducted to analyse driver gaze and lane-keeping Behaviours in free-driving and following driving scenario. Drivers are clustered into two driving style groups referring to their driving Behaviours affected by the leading vehicle, and then the personalization parameters for every specific subject driver are optimized. The proposed algorithm is validated through driver-in-the-loop experiment based on a moving-base simulator. Results show that, compared with the un-personalized algorithms, the personalized highway pilot algorithm can significantly reduce the mental workload and improve user acceptance of the assist functions.      
### 44.Automated assessment of disease severity of COVID-19 using artificial intelligence with synthetic chest CT  [ :arrow_down: ](https://arxiv.org/pdf/2112.05900.pdf)
>  Background: Triage of patients is important to control the pandemic of coronavirus disease 2019 (COVID-19), especially during the peak of the pandemic when clinical resources become extremely limited. <br>Purpose: To develop a method that automatically segments and quantifies lung and pneumonia lesions with synthetic chest CT and assess disease severity in COVID-19 patients. <br>Materials and Methods: In this study, we incorporated data augmentation to generate synthetic chest CT images using public available datasets (285 datasets from "Lung Nodule Analysis 2016"). The synthetic images and masks were used to train a 2D U-net neural network and tested on 203 COVID-19 datasets to generate lung and lesion segmentations. Disease severity scores (DL: damage load; DS: damage score) were calculated based on the segmentations. Correlations between DL/DS and clinical lab tests were evaluated using Pearson's method. A p-value &lt; 0.05 was considered as statistical significant. <br>Results: Automatic lung and lesion segmentations were compared with manual annotations. For lung segmentation, the median values of dice similarity coefficient, Jaccard index and average surface distance, were 98.56%, 97.15% and 0.49 mm, respectively. The same metrics for lesion segmentation were 76.95%, 62.54% and 2.36 mm, respectively. Significant (p &lt;&lt; 0.05) correlations were found between DL/DS and percentage lymphocytes tests, with r-values of -0.561 and -0.501, respectively. <br>Conclusion: An AI system that based on thoracic radiographic and data augmentation was proposed to segment lung and lesions in COVID-19 patients. Correlations between imaging findings and clinical lab tests suggested the value of this system as a potential tool to assess disease severity of COVID-19.      
### 45.Distributed Graph Learning with Smooth Data Priors  [ :arrow_down: ](https://arxiv.org/pdf/2112.05887.pdf)
>  Graph learning is often a necessary step in processing or representing structured data, when the underlying graph is not given explicitly. Graph learning is generally performed centrally with a full knowledge of the graph signals, namely the data that lives on the graph nodes. However, there are settings where data cannot be collected easily or only with a non-negligible communication cost. In such cases, distributed processing appears as a natural solution, where the data stays mostly local and all processing is performed among neighbours nodes on the communication graph. We propose here a novel distributed graph learning algorithm, which permits to infer a graph from signal observations on the nodes under the assumption that the data is smooth on the target graph. We solve a distributed optimization problem with local projection constraints to infer a valid graph while limiting the communication costs. Our results show that the distributed approach has a lower communication cost than a centralised algorithm without compromising the accuracy in the inferred graph. It also scales better in communication costs with the increase of the network size, especially for sparse networks.      
### 46.Directed Speech Separation for Automatic Speech Recognition of Long Form Conversational Speech  [ :arrow_down: ](https://arxiv.org/pdf/2112.05863.pdf)
>  Many of the recent advances in speech separation are primarily aimed at synthetic mixtures of short audio utterances with high degrees of overlap. These datasets significantly differ from the real conversational data and hence, the models trained and evaluated on these datasets do not generalize to real conversational scenarios. Another issue with using most of these models for long form speech is the nondeterministic ordering of separated speech segments due to either unsupervised clustering for time-frequency masks or Permutation Invariant training (PIT) loss. This leads to difficulty in accurately stitching homogenous speaker segments for downstream tasks like Automatic Speech Recognition (ASR). In this paper, we propose a speaker conditioned separator trained on speaker embeddings extracted directly from the mixed signal. We train this model using a directed loss which regulates the order of the separated segments. With this model, we achieve significant improvements on Word error rate (WER) for real conversational data without the need for an additional re-stitching step.      
### 47.Economic MPC-based planning for marine vehicles: Tuning safety and energy efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2112.05844.pdf)
>  Energy efficiency and safety are two critical objectives for marine vehicles operating in environments with obstacles, and they generally conflict with each other. In this paper, we propose a novel online motion planning method of marine vehicles which can make trade-offs between the two design objectives based on the framework of economic model predictive control (EMPC). Firstly, the feasible trajectory with the most safety margin is designed and utilized as tracking reference. Secondly, the EMPC-based receding horizon motion planning algorithm is designed, in which the practical consumed energy and safety measure (i.e., the distance between the planning trajectory and the reference) are considered. Experimental results verify the effectiveness and feasibility of the proposed method.      
### 48.Learning distributed channel access policies for networked estimation: data-driven optimization in the mean-field regime  [ :arrow_down: ](https://arxiv.org/pdf/2112.05837.pdf)
>  The problem of communicating sensor measurements over shared networks is prevalent in many modern large-scale distributed systems such as cyber-physical systems, wireless sensor networks, and the internet of things. Due to bandwidth constraints, the system designer must jointly design decentralized medium access transmission and estimation policies that accommodate a very large number of devices in extremely contested environments such that the collection of all observations is reproduced at the destination with the best possible fidelity. We formulate a remote estimation problem in the mean-field regime where a very large number of sensors communicate their observations to an access point, or base station, under a strict constraint on the maximum fraction of transmitting devices. We show that in the mean-field regime, this problem exhibits a structure that enables tractable optimization algorithms. More importantly, we obtain a data-driven learning scheme that admits a finite sample-complexity guarantee on the performance of the resulting estimation system under minimal assumptions on the data's probability density function.      
### 49.Exposure-Referred Signal-to-Noise Ratio for Digital Image Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2112.05817.pdf)
>  The signal-to-noise ratio (SNR) of a digital image sensor is typically defined as the ratio between the mean over the standard deviation of the sensor's output, thus known as the output-referred SNR. For sensors with a large full-well capacity, the output-referred SNR demonstrates the well-known linear response in the log-log scale. However, as the input exposure approaches the full-well capacity, the vanishing randomness of the saturated pixel will cause this output-referred SNR to artificially go to infinity. Since modern digital image sensors have a small pitch and hence a small full-well capacity, the shortcomings of the output-referred SNR motivated the development of a theoretical concept known as the exposure-referred SNR, first reported in some sensors and computer vision papers in the 1990's and more since 2010. Some intuitions of the exposure-referred SNR have been discussed in the past, but little is known how the exposure-referred SNR can be rigorously derived. <br>Recognizing the significance of such an analysis to all present and future small pixels, this paper presents a theoretical analysis to justify the definition and answer four questions: (1) What is the correct definition of SNR? (2) How is the output-referred SNR related to the exposure-referred SNR? (3) For simple noise models, the SNRs can be analytically derived, but for complex noise models, how to numerically compute the SNR? (4) What utilities can the exposure-referred SNR bring to solving imaging tasks? New theoretical results are shown to confirm the validity of the exposure-referred SNR for image sensors of any bit-depth and full-well capacity.      
### 50.A Label Correction Algorithm Using Prior Information for Automatic and Accurate Geospatial Object Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.05794.pdf)
>  Thousands of scanned historical topographic maps contain valuable information covering long periods of time, such as how the hydrography of a region has changed over time. Efficiently unlocking the information in these maps requires training a geospatial objects recognition system, which needs a large amount of annotated data. Overlapping geo-referenced external vector data with topographic maps according to their coordinates can annotate the desired objects' locations in the maps automatically. However, directly overlapping the two datasets causes misaligned and false annotations because the publication years and coordinate projection systems of topographic maps are different from the external vector data. We propose a label correction algorithm, which leverages the color information of maps and the prior shape information of the external vector data to reduce misaligned and false annotations. The experiments show that the precision of annotations from the proposed algorithm is 10% higher than the annotations from a state-of-the-art algorithm. Consequently, recognition results using the proposed algorithm's annotations achieve 9% higher correctness than using the annotations from the state-of-the-art algorithm.      
### 51.Pre-training and Fine-tuning Transformers for fMRI Prediction Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2112.05761.pdf)
>  We present the TFF Transformer framework for the analysis of functional Magnetic Resonance Imaging (fMRI) data. TFF employs a transformer-based architecture and a two-phase training approach. First, self-supervised training is applied to a collection of fMRI scans, where the model is trained for the reconstruction of 3D volume data. Second, the pre-trained model is fine-tuned on specific tasks, utilizing ground truth labels. Our results show state-of-the-art performance on a variety of fMRI tasks, including age and gender prediction, as well as schizophrenia recognition.      
### 52.Learning Representations with Contrastive Self-Supervised Learning for Histopathology Applications  [ :arrow_down: ](https://arxiv.org/pdf/2112.05760.pdf)
>  Unsupervised learning has made substantial progress over the last few years, especially by means of contrastive self-supervised learning. The dominating dataset for benchmarking self-supervised learning has been ImageNet, for which recent methods are approaching the performance achieved by fully supervised training. The ImageNet dataset is however largely object-centric, and it is not clear yet what potential those methods have on widely different datasets and tasks that are not object-centric, such as in digital pathology. While self-supervised learning has started to be explored within this area with encouraging results, there is reason to look closer at how this setting differs from natural images and ImageNet. In this paper we make an in-depth analysis of contrastive learning for histopathology, pin-pointing how the contrastive objective will behave differently due to the characteristics of histopathology data. We bring forward a number of considerations, such as view generation for the contrastive objective and hyper-parameter tuning. In a large battery of experiments, we analyze how the downstream performance in tissue classification will be affected by these considerations. The results point to how contrastive learning can reduce the annotation effort within digital pathology, but that the specific dataset characteristics need to be considered. To take full advantage of the contrastive learning objective, different calibrations of view generation and hyper-parameters are required. Our results pave the way for realizing the full potential of self-supervised learning for histopathology applications.      
### 53.Edge-Enhanced Dual Discriminator Generative Adversarial Network for Fast MRI with Parallel Imaging Using Multi-view Information  [ :arrow_down: ](https://arxiv.org/pdf/2112.05758.pdf)
>  In clinical medicine, magnetic resonance imaging (MRI) is one of the most important tools for diagnosis, triage, prognosis, and treatment planning. However, MRI suffers from an inherent slow data acquisition process because data is collected sequentially in k-space. In recent years, most MRI reconstruction methods proposed in the literature focus on holistic image reconstruction rather than enhancing the edge information. This work steps aside this general trend by elaborating on the enhancement of edge information. Specifically, we introduce a novel parallel imaging coupled dual discriminator generative adversarial network (PIDD-GAN) for fast multi-channel MRI reconstruction by incorporating multi-view information. The dual discriminator design aims to improve the edge information in MRI reconstruction. One discriminator is used for holistic image reconstruction, whereas the other one is responsible for enhancing edge information. An improved U-Net with local and global residual learning is proposed for the generator. Frequency channel attention blocks (FCA Blocks) are embedded in the generator for incorporating attention mechanisms. Content loss is introduced to train the generator for better reconstruction quality. We performed comprehensive experiments on Calgary-Campinas public brain MR dataset and compared our method with state-of-the-art MRI reconstruction methods. Ablation studies of residual learning were conducted on the MICCAI13 dataset to validate the proposed modules. Results show that our PIDD-GAN provides high-quality reconstructed MR images, with well-preserved edge information. The time of single-image reconstruction is below 5ms, which meets the demand of faster processing.      
### 54.Enhancing Multi-Scale Implicit Learning in Image Super-Resolution with Integrated Positional Encoding  [ :arrow_down: ](https://arxiv.org/pdf/2112.05756.pdf)
>  Is the center position fully capable of representing a pixel? There is nothing wrong to represent pixels with their centers in a discrete image representation, but it makes more sense to consider each pixel as the aggregation of signals from a local area in an image super-resolution (SR) context. Despite the great capability of coordinate-based implicit representation in the field of arbitrary-scale image SR, this area's nature of pixels is not fully considered. To this end, we propose integrated positional encoding (IPE), extending traditional positional encoding by aggregating frequency information over the pixel area. We apply IPE to the state-of-the-art arbitrary-scale image super-resolution method: local implicit image function (LIIF), presenting IPE-LIIF. We show the effectiveness of IPE-LIIF by quantitative and qualitative evaluations, and further demonstrate the generalization ability of IPE to larger image scales and multiple implicit-based methods. Code will be released.      
### 55.Information Prebuilt Recurrent Reconstruction Network for Video Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2112.05755.pdf)
>  The video super-resolution (VSR) method based on the recurrent convolutional network has strong temporal modeling capability for video sequences. However, the input information received by different recurrent units in the unidirectional recurrent convolutional network is unbalanced. Early reconstruction frames receive less temporal information, resulting in fuzzy or artifact results. Although the bidirectional recurrent convolution network can alleviate this problem, it greatly increases reconstruction time and computational complexity. It is also not suitable for many application scenarios, such as online super-resolution. To solve the above problems, we propose an end-to-end information prebuilt recurrent reconstruction network (IPRRN), consisting of an information prebuilt network (IPNet) and a recurrent reconstruction network (RRNet). By integrating sufficient information from the front of the video to build the hidden state needed for the initially recurrent unit to help restore the earlier frames, the information prebuilt network balances the input information difference before and after without backward propagation. In addition, we demonstrate a compact recurrent reconstruction network, which has significant improvements in recovery quality and time efficiency. Many experiments have verified the effectiveness of our proposed network, and compared with the existing state-of-the-art methods, our method can effectively achieve higher quantitative and qualitative evaluation performance.      
### 56.PyTorch Connectomics: A Scalable and Flexible Segmentation Framework for EM Connectomics  [ :arrow_down: ](https://arxiv.org/pdf/2112.05754.pdf)
>  We present PyTorch Connectomics (PyTC), an open-source deep-learning framework for the semantic and instance segmentation of volumetric microscopy images, built upon PyTorch. We demonstrate the effectiveness of PyTC in the field of connectomics, which aims to segment and reconstruct neurons, synapses, and other organelles like mitochondria at nanometer resolution for understanding neuronal communication, metabolism, and development in animal brains. PyTC is a scalable and flexible toolbox that tackles datasets at different scales and supports multi-task and semi-supervised learning to better exploit expensive expert annotations and the vast amount of unlabeled data during training. Those functionalities can be easily realized in PyTC by changing the configuration options without coding and adapted to other 2D and 3D segmentation tasks for different tissues and imaging modalities. Quantitatively, our framework achieves the best performance in the CREMI challenge for synaptic cleft segmentation (outperforms existing best result by relatively 6.1$\%$) and competitive performance on mitochondria and neuronal nuclei segmentation. Code and tutorials are publicly available at <a class="link-external link-https" href="https://connectomics.readthedocs.io" rel="external noopener nofollow">this https URL</a>.      
### 57.Specificity-Preserving Federated Learning for MR Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2112.05752.pdf)
>  Federated learning (FL) can be used to improve data privacy and efficiency in magnetic resonance (MR) image reconstruction by enabling multiple institutions to collaborate without needing to aggregate local data. However, the domain shift caused by different MR imaging protocols can substantially degrade the performance of FL models. Recent FL techniques tend to solve this by enhancing the generalization of the global model, but they ignore the domain-specific features, which may contain important information about the device properties and be useful for local reconstruction. In this paper, we propose a specificity-preserving FL algorithm for MR image reconstruction (FedMRI). The core idea is to divide the MR reconstruction model into two parts: a globally shared encoder to obtain a generalized representation at the global level, and a client-specific decoder to preserve the domain-specific properties of each client, which is important for collaborative reconstruction when the clients have unique distribution. Moreover, to further boost the convergence of the globally shared encoder when a domain shift is present, a weighted contrastive regularization is introduced to directly correct any deviation between the client and server during optimization. Extensive experiments demonstrate that our FedMRI's reconstructed results are the closest to the ground-truth for multi-institutional data, and that it outperforms state-of-the-art FL methods.      
### 58.Interpretable Design of Reservoir Computing Networks using Realization Theory  [ :arrow_down: ](https://arxiv.org/pdf/2112.06891.pdf)
>  The reservoir computing networks (RCNs) have been successfully employed as a tool in learning and complex decision-making tasks. Despite their efficiency and low training cost, practical applications of RCNs rely heavily on empirical design. In this paper, we develop an algorithm to design RCNs using the realization theory of linear dynamical systems. In particular, we introduce the notion of $\alpha$-stable realization, and provide an efficient approach to prune the size of a linear RCN without deteriorating the training accuracy. Furthermore, we derive a necessary and sufficient condition on the irreducibility of number of hidden nodes in linear RCNs based on the concepts of controllability and observability matrices. Leveraging the linear RCN design, we provide a tractable procedure to realize RCNs with nonlinear activation functions. Finally, we present numerical experiments on forecasting time-delay systems and chaotic systems to validate the proposed RCN design methods and demonstrate their efficacy.      
### 59.Mean-square-error-based secondary source placement in sound field synthesis with prior information on desired field  [ :arrow_down: ](https://arxiv.org/pdf/2112.06774.pdf)
>  A method of optimizing secondary source placement in sound field synthesis is proposed. Such an optimization method will be useful when the allowable placement region and available number of loudspeakers are limited. We formulate a mean-square-error-based cost function, incorporating the statistical properties of possible desired sound fields, for general linear-least-squares-based sound field synthesis methods, including pressure matching and (weighted) mode matching, whereas most of the current methods are applicable only to the pressure-matching method. An efficient greedy algorithm for minimizing the proposed cost function is also derived. Numerical experiments indicated that a high reproduction accuracy can be achieved by the placement optimized by the proposed method compared with the empirically used regular placement.      
### 60.Computational bioacoustics with deep learning: a review and roadmap  [ :arrow_down: ](https://arxiv.org/pdf/2112.06725.pdf)
>  Animal vocalisations and natural soundscapes are fascinating objects of study, and contain valuable evidence about animal behaviours, populations and ecosystems. They are studied in bioacoustics and ecoacoustics, with signal processing and analysis an important component. Computational bioacoustics has accelerated in recent decades due to the growth of affordable digital sound recording devices, and to huge progress in informatics such as big data, signal processing and machine learning. Methods are inherited from the wider field of deep learning, including speech and image processing. However, the tasks, demands and data characteristics are often different from those addressed in speech or music analysis. There remain unsolved problems, and tasks for which evidence is surely present in many acoustic signals, but not yet realised. In this paper I perform a review of the state of the art in deep learning for computational bioacoustics, aiming to clarify key concepts and identify and analyse knowledge gaps. Based on this, I offer a subjective but principled roadmap for computational bioacoustics with deep learning: topics that the community should aim to address, in order to make the most of future developments in AI and informatics, and to use audio data in answering zoological and ecological questions.      
### 61.PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-modeing Unit Training for Robust Uyghur E2E Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.06721.pdf)
>  Consonant and vowel reduction are often encountered in Uyghur speech, which might cause performance degradation in Uyghur automatic speech recognition (ASR). Our recently proposed learning strategy based on masking, Phone Masking Training (PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT achieves remarkably improvements, there still exists room for further gains due to the granularity mismatch between masking unit of PMT (phoneme) and modeling unit (word-piece). To boost the performance of PMT, we propose multi-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The idea of MMUT framework is to split the Encoder into two parts including acoustic feature sequences to phoneme-level representation (AF-to-PLR) and phoneme-level representation to word-piece-level representation (PLR-to-WPLR). It allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss to learn the rich phoneme-level context information brought by PMT. Experi-mental results on Uyghur ASR show that the proposed approaches improve significantly, outperforming the pure PMT (reduction WER from 24.0 to 23.7 on Read-Test and from 38.4 to 36.8 on Oral-Test respectively). We also conduct experiments on the 960-hour Librispeech benchmark using ESPnet1, which achieves about 10% relative WER reduction on all the test sets without LM fusion comparing with the latest official ESPnet1 pre-trained model.      
### 62.N-SfC: Robust and Fast Shape Estimation from Caustic Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.06705.pdf)
>  This paper deals with the highly challenging problem of reconstructing the shape of a refracting object from a single image of its resulting caustic. Due to the ubiquity of transparent refracting objects in everyday life, reconstruction of their shape entails a multitude of practical applications. The recent Shape from Caustics (SfC) method casts the problem as the inverse of a light propagation simulation for synthesis of the caustic image, that can be solved by a differentiable renderer. However, the inherent complexity of light transport through refracting surfaces currently limits the practicability with respect to reconstruction speed and robustness. To address these issues, we introduce Neural-Shape from Caustics (N-SfC), a learning-based extension that incorporates two components into the reconstruction pipeline: a denoising module, which alleviates the computational cost of the light transport simulation, and an optimization process based on learned gradient descent, which enables better convergence using fewer iterations. Extensive experiments demonstrate the effectiveness of our neural extensions in the scenario of quality control in 3D glass printing, where we significantly outperform the current state-of-the-art in terms of computational speed and final surface error.      
### 63.Bi-directional Beamforming Feedback-based Firmware-agnostic WiFi Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2112.06695.pdf)
>  In the field of WiFi sensing, as an alternative sensing source of the channel state information (CSI) matrix, the use of a beamforming feedback matrix (BFM)that is a right singular matrix of the CSI matrix has attracted significant interest owing to its wide availability regarding the underlying WiFi systems. In the IEEE 802.11ac/ax standard, the station (STA) transmits a BFM to an access point (AP), which uses the BFM for precoded multiple-input and multiple-output communications. In addition, in the same way, the AP transmits a BFM to the STA, and the STA uses the received BFM. Regarding BFM-based sensing, extensive real-world experiments were conducted as part of this study, and two key insights were reported: Firstly, this report identified a potential issue related to accuracy in existing uni-directional BFM-based sensing frameworks that leverage only BFMs transmitted for the AP or STA. Such uni-directionality introduces accuracy concerns when there is a sensing capability gap between the uni-directional BFMs for the AP and STA. Thus, this report experimentally evaluates the sensing ability disparity between the uni-directional BFMs, and shows that the BFMs transmitted for an AP achieve higher sensing accuracy compared to the BFMs transmitted from the STA when the sensing target values are estimated depending on the angle of departure of the AP. Secondly, to complement the sensing gap, this paper proposes a bi-directional sensing framework, which simultaneously leverages the BFMs transmitted from the AP and STA. The experimental evaluations reveal that bi-directional sensing achieves higher accuracy than uni-directional sensing in terms of the human localization task.      
### 64.A Review: Challenges and Opportunities for Artificial Intelligence and Robotics in the Offshore Wind Sector  [ :arrow_down: ](https://arxiv.org/pdf/2112.06620.pdf)
>  A global trend in increasing wind turbine size and distances from shore is emerging within the rapidly growing offshore wind farm market. In the UK, the offshore wind sector produced its highest amount of electricity in the UK in 2019, a 19.6% increase on the year before. Currently, the UK is set to increase production further, targeting a 74.7% increase of installed turbine capacity as reflected in recent Crown Estate leasing rounds. With such tremendous growth, the sector is now looking to Robotics and Artificial Intelligence (RAI) in order to tackle lifecycle service barriers as to support sustainable and profitable offshore wind energy production. Today, RAI applications are predominately being used to support short term objectives in operation and maintenance. However, moving forward, RAI has the potential to play a critical role throughout the full lifecycle of offshore wind infrastructure, from surveying, planning, design, logistics, operational support, training and decommissioning. This paper presents one of the first systematic reviews of RAI for the offshore renewable energy sector. The state-of-the-art in RAI is analyzed with respect to offshore energy requirements, from both industry and academia, in terms of current and future requirements. Our review also includes a detailed evaluation of investment, regulation and skills development required to support the adoption of RAI. The key trends identified through a detailed analysis of patent and academic publication databases provide insights to barriers such as certification of autonomous platforms for safety compliance and reliability, the need for digital architectures for scalability in autonomous fleets, adaptive mission planning for resilient resident operations and optimization of human machine interaction for trusted partnerships between people and autonomous assistants.      
### 65.Detecting Emotion Carriers by Combining Acoustic and Lexical Representations  [ :arrow_down: ](https://arxiv.org/pdf/2112.06603.pdf)
>  Personal narratives (PN) - spoken or written - are recollections of facts, people, events, and thoughts from one's own experience. Emotion recognition and sentiment analysis tasks are usually defined at the utterance or document level. However, in this work, we focus on Emotion Carriers (EC) defined as the segments (speech or text) that best explain the emotional state of the narrator ("loss of father", "made me choose"). Once extracted, such EC can provide a richer representation of the user state to improve natural language understanding and dialogue modeling. In previous work, it has been shown that EC can be identified using lexical features. However, spoken narratives should provide a richer description of the context and the users' emotional state. In this paper, we leverage word-based acoustic and textual embeddings as well as early and late fusion techniques for the detection of ECs in spoken narratives. For the acoustic word-level representations, we use Residual Neural Networks (ResNet) pretrained on separate speech emotion corpora and fine-tuned to detect EC. Experiments with different fusion and system combination strategies show that late fusion leads to significant improvements for this task.      
### 66.RIS-Aided Cell-Free Massive MIMO Systems: Joint Design of Transmit Beamforming and Phase Shifts  [ :arrow_down: ](https://arxiv.org/pdf/2112.06593.pdf)
>  This paper studies RIS-aided cell-free massive MIMO systems, where multiple RISs are deployed to assist the communication between multiple access points (APs) and multiple users, with either continuous or discrete phase shifts at the RISs. We formulate the max-min fairness problem that maximizes the minimum achievable rate among all users by jointly optimizing the transmit beamforming at active APs and the phase shifts at passive RISs, subject to power constraints at the APs. To address such a challenging problem, we first study the special single-user scenario and propose an algorithm that can transform the optimization problem into semidefinite program (SDP) or integer linear program (ILP) for the cases of continuous and discrete phase shifts, respectively. By solving the resulting SDP and ILP, we first obtain the optimal phase shifts, and then design the optimal transmit beamforming accordingly. To solve the optimization problem for the multi-user scenario and continuous phase shifts at RISs, we extend the single-user algorithm and propose an alternating optimization algorithm, which can first decompose the max-min fairness problem into two subproblems related to transmit beamforming and phase shifts, and then transform the two subproblems into second-order-cone program and SDP, respectively. For the multi-user scenario and discrete phase shifts, the max-min fairness problem is shown to be a mixed-integer non-linear program (MINLP). To tackle it, we design a ZF-based successive refinement algorithm, which can find a suboptimal transmit beamforming and phase shifts by means of alternating optimization. Numerical results show that compared with benchmark schemes of random phase shifts and without using RISs, the proposed algorithms can significantly increase the minimum achievable rate among all users, especially when the number of reflecting elements at each RIS is large.      
### 67.Ensemble CNN Networks for GBM Tumors Segmentation using Multi-parametric MRI  [ :arrow_down: ](https://arxiv.org/pdf/2112.06554.pdf)
>  Glioblastomas are the most aggressive fast-growing primary brain cancer which originate in the glial cells of the brain. Accurate identification of the malignant brain tumor and its sub-regions is still one of the most challenging problems in medical image segmentation. The Brain Tumor Segmentation Challenge (BraTS) has been a popular benchmark for automatic brain glioblastomas segmentation algorithms since its initiation. In this year's challenge, BraTS 2021 provides the largest multi-parametric (mpMRI) dataset of 2,000 pre-operative patients. In this paper, we propose a new aggregation of two deep learning frameworks namely, DeepSeg and nnU-Net for automatic glioblastoma recognition in pre-operative mpMRI. Our ensemble method obtains Dice similarity scores of 92.00, 87.33, and 84.10 and Hausdorff Distances of 3.81, 8.91, and 16.02 for the enhancing tumor, tumor core, and whole tumor regions on the BraTS 2021 validation set, individually. These Experimental findings provide evidence that it can be readily applied clinically and thereby aiding in the brain cancer prognosis, therapy planning, and therapy response monitoring.      
### 68.Detecting Audio Adversarial Examples with Logit Noising  [ :arrow_down: ](https://arxiv.org/pdf/2112.06443.pdf)
>  Automatic speech recognition (ASR) systems are vulnerable to audio adversarial examples that attempt to deceive ASR systems by adding perturbations to benign speech signals. Although an adversarial example and the original benign wave are indistinguishable to humans, the former is transcribed as a malicious target sentence by ASR systems. Several methods have been proposed to generate audio adversarial examples and feed them directly into the ASR system (over-line). Furthermore, many researchers have demonstrated the feasibility of robust physical audio adversarial examples(over-air). To defend against the attacks, several studies have been proposed. However, deploying them in a real-world situation is difficult because of accuracy drop or time overhead. In this paper, we propose a novel method to detect audio adversarial examples by adding noise to the logits before feeding them into the decoder of the ASR. We show that carefully selected noise can significantly impact the transcription results of the audio adversarial examples, whereas it has minimal impact on the transcription results of benign audio waves. Based on this characteristic, we detect audio adversarial examples by comparing the transcription altered by logit noising with its original transcription. The proposed method can be easily applied to ASR systems without any structural changes or additional training. The experimental results show that the proposed method is robust to over-line audio adversarial examples as well as over-air audio adversarial examples compared with state-of-the-art detection methods.      
### 69.Competitive Car Racing with Multiple Vehicles using a Parallelized Optimization with Safety Guarantee  [ :arrow_down: ](https://arxiv.org/pdf/2112.06435.pdf)
>  This paper presents a novel planning and control strategy for competing with multiple vehicles in a car racing scenario. The proposed racing strategy switches between two modes. When there are no surrounding vehicles, a learning-based model predictive control (MPC) trajectory planner is used to guarantee that the ego vehicle achieves better lap timing. When the ego vehicle is competing with other surrounding vehicles to overtake, an optimization-based planner generates multiple dynamically-feasible trajectories through parallel computation. Each trajectory is optimized under a MPC formulation with different homotopic Bezier-curve reference paths lying laterally between surrounding vehicles. The time-optimal trajectory among these different homotopic trajectories is selected and a low-level MPC controller with obstacle avoidance constraints is used to guarantee system safety-critical performance. The proposed algorithm has the capability to generate collision-free trajectories and track them while enhancing the lap timing performance with steady low computational complexity, outperforming existing approaches in both timing and performance for a car racing environment. To demonstrate the performance of our racing strategy, we simulate with multiple randomly generated moving vehicles on the track and test the ego vehicle's overtake maneuvers.      
### 70.Human-like Driving Decision at Unsignalized Intersections Based on Game Theory  [ :arrow_down: ](https://arxiv.org/pdf/2112.06415.pdf)
>  Unsignalized intersection driving is challenging for automated vehicles. For safe and efficient performances, the diverse and dynamic behaviors of interacting vehicles should be considered. Based on a game-theoretic framework, a human-like payoff design methodology is proposed for the automated decision at unsignalized intersections. Prospect Theory is introduced to map the objective collision risk to the subjective driver payoffs, and the driving style can be quantified as a tradeoff between safety and speed. To account for the dynamics of interaction, a probabilistic model is further introduced to describe the acceleration tendency of drivers. Simulation results show that the proposed decision algorithm can describe the dynamic process of two-vehicle interaction in limit cases. Statistics of uniformly-sampled cases simulation indicate that the success rate of safe interaction reaches 98%, while the speed efficiency can also be guaranteed. The proposed approach is further applied and validated in four-vehicle interaction scenarios at a four-arm intersection.      
### 71.CSI Feedback with Model-Driven Deep Learning of Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.06405.pdf)
>  In order to achieve reliable communication with a high data rate of massive multiple-input multiple-output (MIMO) systems in frequency division duplex (FDD) mode, the estimated channel state information (CSI) at the receiver needs to be fed back to the transmitter. However, the feedback overhead becomes exorbitant with the increasing number of antennas. In this paper, a two stages low rank (TSLR) CSI feedback scheme for millimeter wave (mmWave) massive MIMO systems is proposed to reduce the feedback overhead based on model-driven deep learning. Besides, we design a deep iterative neural network, named FISTA-Net, by unfolding the fast iterative shrinkage thresholding algorithm (FISTA) to achieve more efficient CSI feedback. Moreover, a shrinkage thresholding network (ST-Net) is designed in FISTA-Net based on the attention mechanism, which can choose the threshold adaptively. Simulation results show that the proposed TSLR CSI feedback scheme and FISTA-Net outperform the existing algorithms in various scenarios.      
### 72.A Cluster-Based Weighted Feature Similarity Moving Target Tracking Algorithm for Automotive FMCW Radar  [ :arrow_down: ](https://arxiv.org/pdf/2112.06388.pdf)
>  We studied a target tracking algorithm based on millimeter-wave (MMW) radar in an autonomous driving environment. Aiming at the cluster matching in the target tracking stage, a new weighted feature similarity algorithm is proposed, which increases the matching rate of the same target in adjacent frames under strong environmental noise and multiple interference targets. For autonomous driving scenarios, we constructed a method that uses its motion parameters to extract and correct the trajectory of a moving target, which solves the problem of moving target detection and trajectory correction during vehicle movement. Finally, the feasibility of the proposed method was verified by a series of experiments in autonomous driving environments. The results verify the high recognition accuracy and low positional error of the method.      
### 73.Improving Speech Recognition on Noisy Speech via Speech Enhancement with Multi-Discriminators CycleGAN  [ :arrow_down: ](https://arxiv.org/pdf/2112.06309.pdf)
>  This paper presents our latest investigations on improving automatic speech recognition for noisy speech via speech enhancement. We propose a novel method named Multi-discriminators CycleGAN to reduce noise of input speech and therefore improve the automatic speech recognition performance. Our proposed method leverages the CycleGAN framework for speech enhancement without any parallel data and improve it by introducing multiple discriminators that check different frequency areas. Furthermore, we show that training multiple generators on homogeneous subset of the training data is better than training one generator on all the training data. We evaluate our method on CHiME-3 data set and observe up to 10.03% relatively WER improvement on the development set and up to 14.09% on the evaluation set.      
### 74.Image-to-Height Domain Translation for Synthetic Aperture Sonar  [ :arrow_down: ](https://arxiv.org/pdf/2112.06307.pdf)
>  Observations of seabed texture with synthetic aperture sonar are dependent upon several factors. In this work, we focus on collection geometry with respect to isotropic and anisotropic textures. The low grazing angle of the collection geometry, combined with orientation of the sonar path relative to anisotropic texture, poses a significant challenge for image-alignment and other multi-view scene understanding frameworks. We previously proposed using features captured from estimated seabed relief to improve scene understanding. While several methods have been developed to estimate seabed relief via intensity, no large-scale study exists in the literature. Furthermore, a dataset of coregistered seabed relief maps and sonar imagery is nonexistent to learn this domain translation. We address these problems by producing a large simulated dataset containing coregistered pairs of seabed relief and intensity maps from two unique sonar data simulation techniques. We apply three types of models, with varying complexity, to translate intensity imagery to seabed relief: a Gaussian Markov Random Field approach (GMRF), a conditional Generative Adversarial Network (cGAN), and UNet architectures. Methods are compared in reference to the coregistered simulated datasets using L1 error. Additionally, predictions on simulated and real SAS imagery are shown. Finally, models are compared on two datasets of hand-aligned SAS imagery and evaluated in terms of L1 error across multiple aspects in comparison to using intensity. Our comprehensive experiments show that the proposed UNet architectures outperform the GMRF and pix2pix cGAN models on seabed relief estimation for simulated and real SAS imagery.      
### 75.HerosNet: Hyperspectral Explicable Reconstruction and Optimal Sampling Deep Network for Snapshot Compressive Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2112.06238.pdf)
>  Hyperspectral imaging is an essential imaging modality for a wide range of applications, especially in remote sensing, agriculture, and medicine. Inspired by existing hyperspectral cameras that are either slow, expensive, or bulky, reconstructing hyperspectral images (HSIs) from a low-budget snapshot measurement has drawn wide attention. By mapping a truncated numerical optimization algorithm into a network with a fixed number of phases, recent deep unfolding networks (DUNs) for spectral snapshot compressive sensing (SCI) have achieved remarkable success. However, DUNs are far from reaching the scope of industrial applications limited by the lack of cross-phase feature interaction and adaptive parameter adjustment. In this paper, we propose a novel Hyperspectral Explicable Reconstruction and Optimal Sampling deep Network for SCI, dubbed HerosNet, which includes several phases under the ISTA-unfolding framework. Each phase can flexibly simulate the sensing matrix and contextually adjust the step size in the gradient descent step, and hierarchically fuse and interact the hidden states of previous phases to effectively recover current HSI frames in the proximal mapping step. Simultaneously, a hardware-friendly optimal binary mask is learned end-to-end to further improve the reconstruction performance. Finally, our HerosNet is validated to outperform the state-of-the-art methods on both simulation and real datasets by large margins.      
### 76.Joint Sensing, Communication, and Computation Resource Allocation for Cooperative Perception in Fog-Based Vehicular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.06224.pdf)
>  To enlarge the perception range and reliability of individual autonomous vehicles, cooperative perception has been received much attention. However, considering the high volume of shared messages, limited bandwidth and computation resources in vehicular networks become bottlenecks. In this paper, we investigate how to balance the volume of shared messages and constrained resources in fog-based vehicular networks. To this end, we first characterize sum satisfaction of cooperative perception taking account of its spatial-temporal value and latency performance. Next, the sensing block message, communication resource block, and computation resource are jointly allocated to maximize the sum satisfaction of cooperative perception, while satisfying the maximum latency and sojourn time constraints of vehicles. Owing to its non-convexity, we decouple the original problem into two separate sub-problems and devise corresponding solutions. Simulation results demonstrate that our proposed scheme can effectively boost the sum satisfaction of cooperative perception compared with existing baselines.      
### 77.Visualising and Explaining Deep Learning Models for Speech Quality Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2112.06219.pdf)
>  Estimating quality of transmitted speech is known to be a non-trivial task. While traditionally, test participants are asked to rate the quality of samples; nowadays, automated methods are available. These methods can be divided into: 1) intrusive models, which use both, the original and the degraded signals, and 2) non-intrusive models, which only require the degraded signal. Recently, non-intrusive models based on neural networks showed to outperform signal processing based models. However, the advantages of deep learning based models come with the cost of being more challenging to interpret. To get more insight into the prediction models the non-intrusive speech quality prediction model NISQA is analyzed in this paper. NISQA is composed of a convolutional neural network (CNN) and a recurrent neural network (RNN). The task of the CNN is to compute relevant features for the speech quality prediction on a frame level, while the RNN models time-dependencies between the individual speech frames. Different explanation algorithms are used to understand the automatically learned features of the CNN. In this way, several interpretable features could be identified, such as the sensitivity to noise or strong interruptions. On the other hand, it was found that multiple features carry redundant information.      
### 78.Robust Transmission Design for RIS-Aided Communications with Both Transceiver Hardware Impairments and Imperfect CSI  [ :arrow_down: ](https://arxiv.org/pdf/2112.06207.pdf)
>  Reconfigurable intelligent surface (RIS) or intelligent reflecting surface (IRS) has recently been envisioned as one of the most promising technologies in the future sixth-generation (6G) communications. In this paper, we consider the joint optimization of the transmit beamforming at the base station (BS) and the phase shifts at the RIS for an RIS-aided wireless communication system with both hardware impairments and imperfect channel state information (CSI). Specifically, we assume both the BS-user channel and the BS-RIS-user channel are imperfect due to the channel estimation error, and we consider the channel estimation error under the statistical CSI error model. Then, the transmit power of the BS is minimized, subject to the outage probability constraint and the unit-modulus constraints on the reflecting elements. By using Bernstein-type inequality and semidefinite relaxation (SDR) to reformulate the constraints, we transform the optimization problem into a semidefinite programming (SDP) problem. Numerical results show that the proposed robust design algorithm can ensure communication quality of the user in the presence of both hardware impairments and imperfect CSI.      
### 79.Learning Nigerian accent embeddings from speech: preliminary results based on SautiDB-Naija corpus  [ :arrow_down: ](https://arxiv.org/pdf/2112.06199.pdf)
>  This paper describes foundational efforts with SautiDB-Naija, a novel corpus of non-native (L2) Nigerian English speech. We describe how the corpus was created and curated as well as preliminary experiments with accent classification and learning Nigerian accent embeddings. The initial version of the corpus includes over 900 recordings from L2 English speakers of Nigerian languages, such as Yoruba, Igbo, Edo, Efik-Ibibio, and Igala. We further demonstrate how fine-tuning on a pre-trained model like wav2vec can yield representations suitable for related speech tasks such as accent classification. SautiDB-Naija has been published to Zenodo for general use under a flexible Creative Commons License.      
### 80.Real-world challenges for reinforcement learning in building control  [ :arrow_down: ](https://arxiv.org/pdf/2112.06127.pdf)
>  Building upon prior research that highlighted the need for standardizing environments for building control research, and inspired by recently introduced benchmarks for real life reinforcement learning control, here we propose a non-exhaustive nine real world challenges for reinforcement learning building controller. We argue that building control research should be expressed in this framework in addition to providing a standardized environment for repeatability. Advanced controllers such as model predictive control and reinforcement learning control have both advantages and disadvantages that prevent them from being implemented in real world buildings. Comparisons between the two are seldom, and often biased. By focusing on the benchmark problems and challenges, we can investigate the performance of the controllers under a variety of situations and generate a fair comparison. Lastly, we call for a more interdisciplinary effort of the research community to address the real world challenges, and unlock the potentials of advanced building controllers.      
### 81.NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector  [ :arrow_down: ](https://arxiv.org/pdf/2112.06102.pdf)
>  Vertebrate retinas are highly-efficient in processing trivial visual tasks such as detecting moving objects, yet a complex task for modern computers. The detection of object motion is done by specialised retinal ganglion cells named Object-motion-sensitive ganglion cells (OMS-GC). OMS-GC process continuous signals and generate spike patterns that are post-processed by the Visual Cortex. The Neuromorphic Hybrid Spiking Motion Detector (NeuroHSMD) proposed in this work accelerates the HSMD algorithm using Field-Programmable Gate Arrays (FPGAs). The Hybrid Spiking Motion Detector (HSMD) algorithm was the first hybrid algorithm to enhance dynamic background subtraction (DBS) algorithms with a customised 3-layer spiking neural network (SNN) that generates OMS-GC spiking-like responses. The NeuroHSMD algorithm was compared against the HSMD algorithm, using the same 2012 change detection (CDnet2012) and 2014 change detection (CDnet2014) benchmark datasets. The results show that the NeuroHSMD has produced the same results as the HSMD algorithm in real-time without degradation of quality. Moreover, the NeuroHSMD proposed in this paper was completely implemented in Open Computer Language (OpenCL) and therefore is easily replicated in other devices such as Graphical Processor Units (GPUs) and clusters of Central Processor Units (CPUs).      
### 82.Early Stopping for Deep Image Prior  [ :arrow_down: ](https://arxiv.org/pdf/2112.06074.pdf)
>  Deep image prior (DIP) and its variants have showed remarkable potential for solving inverse problems in computer vision, without any extra training data. Practical DIP models are often substantially overparameterized. During the fitting process, these models learn mostly the desired visual content first, and then pick up the potential modeling and observational noise, i.e., overfitting. Thus, the practicality of DIP often depends critically on good early stopping (ES) that captures the transition period. In this regard, the majority of DIP works for vision tasks only demonstrates the potential of the models -- reporting the peak performance against the ground truth, but provides no clue about how to operationally obtain near-peak performance without access to the groundtruth. In this paper, we set to break this practicality barrier of DIP, and propose an efficient ES strategy, which consistently detects near-peak performance across several vision tasks and DIP variants. Based on a simple measure of dispersion of consecutive DIP reconstructions, our ES method not only outpaces the existing ones -- which only work in very narrow domains, but also remains effective when combined with a number of methods that try to mitigate the overfitting. The code is available at <a class="link-external link-https" href="https://github.com/sun-umn/Early_Stopping_for_DIP" rel="external noopener nofollow">this https URL</a>.      
### 83.Perceptual Loss with Recognition Model for Single-Channel Enhancement and Robust ASR  [ :arrow_down: ](https://arxiv.org/pdf/2112.06068.pdf)
>  Single-channel speech enhancement approaches do not always improve automatic recognition rates in the presence of noise, because they can introduce distortions unhelpful for recognition. Following a trend towards end-to-end training of sequential neural network models, several research groups have addressed this problem with joint training of front-end enhancement module with back-end recognition module. While this approach ensures enhancement outputs are helpful for recognition, the enhancement model can overfit to the training data, weakening the recognition model in the presence of unseen noise. To address this, we used a pre-trained acoustic model to generate a perceptual loss that makes speech enhancement more aware of the phonetic properties of the signal. This approach keeps some benefits of joint training, while alleviating the overfitting problem. Experiments on Voicebank + DEMAND dataset for enhancement show that this approach achieves a new state of the art for some objective enhancement scores. In combination with distortion-independent training, our approach gets a WER of 2.80\% on the test set, which is more than 20\% relative better recognition performance than joint training, and 14\% relative better than distortion-independent mask training.      
### 84.U-shaped Transformer with Frequency-Band Aware Attention for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2112.06052.pdf)
>  The state-of-the-art speech enhancement has limited performance in speech estimation accuracy. Recently, in deep learning, the Transformer shows the potential to exploit the long-range dependency in speech by self-attention. Therefore, it is introduced in speech enhancement to improve the speech estimation accuracy from a noise mixture. However, to address the computational cost issue in Transformer with self-attention, the axial attention is the option i.e., to split a 2D attention into two 1D attentions. Inspired by the axial attention, in the proposed method we calculate the attention map along both time- and frequency-axis to generate time and frequency sub-attention maps. Moreover, different from the axial attention, the proposed method provides two parallel multi-head attentions for time- and frequency-axis. Furthermore, it is proven in the literature that the lower frequency-band in speech, generally, contains more desired information than the higher frequency-band, in a noise mixture. Therefore, the frequency-band aware attention is proposed i.e., high frequency-band attention (HFA), and low frequency-band attention (LFA). The U-shaped Transformer is also first time introduced in the proposed method to further improve the speech estimation accuracy. The extensive evaluations over four public datasets, confirm the efficacy of the proposed method.      
### 85.Behavior measures are predicted by how information is encoded in an individual's brain  [ :arrow_down: ](https://arxiv.org/pdf/2112.06048.pdf)
>  Similar to how differences in the proficiency of the cardiovascular and musculoskeletal system predict an individual's athletic ability, differences in how the same brain region encodes information across individuals may explain their behavior. However, when studying how the brain encodes information, researchers choose different neuroimaging tasks (e.g., language or motor tasks), which can rely on processing different types of information and can modulate different brain regions. We hypothesize that individual differences in how information is encoded in the brain are task-specific and predict different behavior measures. We propose a framework using encoding-models to identify individual differences in brain encoding and test if these differences can predict behavior. We evaluate our framework using task functional magnetic resonance imaging data. Our results indicate that individual differences revealed by encoding-models are a powerful tool for predicting behavior, and that researchers should optimize their choice of task and encoding-model for their behavior of interest.      
### 86.Control-Tutored Reinforcement Learning: Towards the Integration of Data-Driven and Model-Based Control  [ :arrow_down: ](https://arxiv.org/pdf/2112.06018.pdf)
>  We present an architecture where a feedback controller derived on an approximate model of the environment assists the learning process to enhance its data efficiency. This architecture, which we term as Control-Tutored Q-learning (CTQL), is presented in two alternative flavours. The former is based on defining the reward function so that a Boolean condition can be used to determine when the control tutor policy is adopted, while the latter, termed as probabilistic CTQL (pCTQL), is instead based on executing calls to the tutor with a certain probability during learning. Both approaches are validated, and thoroughly benchmarked against Q-Learning, by considering the stabilization of an inverted pendulum as defined in OpenAI Gym as a representative problem.      
### 87.Nonsmooth Control Barrier Function Design of Continuous Constraints for Network Connectivity Maintenance  [ :arrow_down: ](https://arxiv.org/pdf/2112.05935.pdf)
>  This paper considers the problem of maintaining global connectivity of a multi-robot system while executing a desired coordination task. Our approach builds on optimization-based feedback design formulations, where the nominal cost function and constraints encode desirable control objectives for the resulting input. We take advantage of the flexibility provided by control barrier functions to produce additional constraints that guarantee that the resulting optimization-based controller is continuous and maintains network connectivity. Our solution uses the algebraic connectivity of the multi-robot interconnection topology as a control barrier function and critically embraces its nonsmooth nature. The technical treatment combines elements from set-valued theory, nonsmooth analysis, and algebraic graph theory to imbue the proposed constraints with regularity properties so that they can be smoothly combined with other control constraints. We provide simulations and experimental results illustrating the effectiveness and continuity of the proposed approach in a resource gathering problem.      
### 88.Efficient Device Scheduling with Multi-Job Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.05928.pdf)
>  Recent years have witnessed a large amount of decentralized data in multiple (edge) devices of end-users, while the aggregation of the decentralized data remains difficult for machine learning jobs due to laws or regulations. Federated Learning (FL) emerges as an effective approach to handling decentralized data without sharing the sensitive raw data, while collaboratively training global machine learning models. The servers in FL need to select (and schedule) devices during the training process. However, the scheduling of devices for multiple jobs with FL remains a critical and open problem. In this paper, we propose a novel multi-job FL framework to enable the parallel training process of multiple jobs. The framework consists of a system model and two scheduling methods. In the system model, we propose a parallel training process of multiple jobs, and construct a cost model based on the training time and the data fairness of various devices during the training process of diverse jobs. We propose a reinforcement learning-based method and a Bayesian optimization-based method to schedule devices for multiple jobs while minimizing the cost. We conduct extensive experimentation with multiple jobs and datasets. The experimental results show that our proposed approaches significantly outperform baseline approaches in terms of training time (up to 8.67 times faster) and accuracy (up to 44.6% higher).      
### 89.Federated Reinforcement Learning at the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2112.05908.pdf)
>  Modern cyber-physical architectures use data collected from systems at different physical locations to learn appropriate behaviors and adapt to uncertain environments. However, an important challenge arises as communication exchanges at the edge of networked systems are costly due to limited resources. This paper considers a setup where multiple agents need to communicate efficiently in order to jointly solve a reinforcement learning problem over time-series data collected in a distributed manner. This is posed as learning an approximate value function over a communication network. An algorithm for achieving communication efficiency is proposed, supported with theoretical guarantees, practical implementations, and numerical evaluations. The approach is based on the idea of communicating only when sufficiently informative data is collected.      
### 90.Hybrid Neural Networks for On-device Directional Hearing  [ :arrow_down: ](https://arxiv.org/pdf/2112.05893.pdf)
>  On-device directional hearing requires audio source separation from a given direction while achieving stringent human-imperceptible latency requirements. While neural nets can achieve significantly better performance than traditional beamformers, all existing models fall short of supporting low-latency causal inference on computationally-constrained wearables. We present DeepBeam, a hybrid model that combines traditional beamformers with a custom lightweight neural net. The former reduces the computational burden of the latter and also improves its generalizability, while the latter is designed to further reduce the memory and computational overhead to enable real-time and low-latency operations. Our evaluation shows comparable performance to state-of-the-art causal inference models on synthetic data while achieving a 5x reduction of model size, 4x reduction of computation per second, 5x reduction in processing time and generalizing better to real hardware data. Further, our real-time hybrid model runs in 8 ms on mobile CPUs designed for low-power wearable devices and achieves an end-to-end latency of 17.5 ms.      
### 91.Revisiting the Boundary between ASR and NLU in the Age of Conversational Dialog Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.05842.pdf)
>  As more users across the world are interacting with dialog agents in their daily life, there is a need for better speech understanding that calls for renewed attention to the dynamics between research in automatic speech recognition (ASR) and natural language understanding (NLU). We briefly review these research areas and lay out the current relationship between them. In light of the observations we make in this paper, we argue that (1) NLU should be cognizant of the presence of ASR models being used upstream in a dialog system's pipeline, (2) ASR should be able to learn from errors found in NLU, (3) there is a need for end-to-end datasets that provide semantic annotations on spoken input, (4) there should be stronger collaboration between ASR and NLU research communities.      
### 92.Sequence-level self-learning with multiple hypotheses  [ :arrow_down: ](https://arxiv.org/pdf/2112.05826.pdf)
>  In this work, we develop new self-learning techniques with an attention-based sequence-to-sequence (seq2seq) model for automatic speech recognition (ASR). For untranscribed speech data, the hypothesis from an ASR system must be used as a label. However, the imperfect ASR result makes unsupervised learning difficult to consistently improve recognition performance especially in the case that multiple powerful teacher models are unavailable. In contrast to conventional unsupervised learning approaches, we adopt the \emph{multi-task learning} (MTL) framework where the $n$-th best ASR hypothesis is used as the label of each task. The seq2seq network is updated through the MTL framework so as to find the common representation that can cover multiple hypotheses. By doing so, the effect of the \emph{hard-decision} errors can be alleviated. <br>We first demonstrate the effectiveness of our self-learning methods through ASR experiments in an accent adaptation task between the US and British English speech. Our experiment results show that our method can reduce the WER on the British speech data from 14.55\% to 10.36\% compared to the baseline model trained with the US English data only. Moreover, we investigate the effect of our proposed methods in a federated learning scenario.      
### 93.Building a great multi-lingual teacher with sparsely-gated mixture of experts for speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.05820.pdf)
>  The sparsely-gated Mixture of Experts (MoE) can magnify a network capacity with a little computational complexity. In this work, we investigate how multi-lingual Automatic Speech Recognition (ASR) networks can be scaled up with a simple routing algorithm in order to achieve better accuracy. More specifically, we apply the sparsely-gated MoE technique to two types of networks: Sequence-to-Sequence Transformer (S2S-T) and Transformer Transducer (T-T). We demonstrate through a set of ASR experiments on multiple language data that the MoE networks can reduce the relative word error rates by 16.5% and 4.7% with the S2S-T and T-T, respectively. Moreover, we thoroughly investigate the effect of the MoE on the T-T architecture in various conditions: streaming mode, non-streaming mode, the use of language ID and the label decoder with the MoE.      
