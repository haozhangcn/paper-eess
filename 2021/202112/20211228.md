# ArXiv eess --Tue, 28 Dec 2021
### 1.Infant Brain Age Classification: 2D CNN Outperforms 3D CNN in Small Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2112.13811.pdf)
>  Determining if the brain is developing normally is a key component of pediatric neuroradiology and neurology. Brain magnetic resonance imaging (MRI) of infants demonstrates a specific pattern of development beyond simply myelination. While radiologists have used myelination patterns, brain morphology and size characteristics to determine age-adequate brain maturity, this requires years of experience in pediatric neuroradiology. With no standardized criteria, visual estimation of the structural maturity of the brain from MRI before three years of age remains dominated by inter-observer and intra-observer variability. A more objective estimation of brain developmental age could help physicians identify many neurodevelopmental conditions and diseases earlier and more reliably. Such data, however, is naturally hard to obtain, and the observer ground truth not much of a gold standard due to subjectivity of assessment. In this light, we explore the general feasibility to tackle this task, and the utility of different approaches, including two- and three-dimensional convolutional neural networks (CNN) that were trained on a fusion of T1-weighted, T2-weighted, and proton density (PD) weighted sequences from 84 individual subjects divided into four age groups from birth to 3 years of age. In the best performing approach, we achieved an accuracy of 0.90 [95% CI:0.86-0.94] using a 2D CNN on a central axial thick slab. We discuss the comparison to 3D networks and show how the performance compares to the use of only one sequence (T1w). In conclusion, despite the theoretical superiority of 3D CNN approaches, in limited-data situations, such approaches are inferior to simpler architectures. The code can be found in <a class="link-external link-https" href="https://github.com/shabanian2018/Age_MRI-Classification" rel="external noopener nofollow">this https URL</a>      
### 2.Design, Dynamics, and Dissipation of a Torsional-Magnetic Spring Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2112.13806.pdf)
>  We present an analytical and experimental study of torsional magnetic mechanism where the restoring torque is due to magnetic field interactions between rotating and fixed permanent magnets. The oscillator consists of a ball bearing-supported permanent magnet, called the rotor, placed between two fixed permanent magnets called the stators. Perturbing the rotor from its equilibrium angle induces a restoring magnetic torque whose effect is modeled as a torsional spring. This restoring effect is accompanied by dissipation mechanisms arising from structural viscoelasticity, air and electromagnetic damping, as well as friction in the ball bearings. To investigate the system dynamics, we constructed an experimental setup capable of mechanical, electrical and magnetic measurements. For various rotor-stator gaps in this setup, we validated an analytical model that assumes viscous and dry (Coulomb) damping during the rotor free response. Moreover, we forced the rotor by a neighboring electromagnetic coil into high amplitude oscillations. We observed unusual resonator nonlinearity: at large rotor-stator gaps, the oscillations are softening; at reduced gaps, the oscillations stiffen-then-soften. The developed reduced-order models capture the nonlinear effects of the rotor-to-stator and the rotor-to-coil distances. These magnetic oscillators are promising in low-frequency electromagnetic signal transmission and in designing magneto-elastic metamaterials with tailorable nonlinearity.      
### 3.Improving the Performance of Backward Chained Behavior Trees using Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.13744.pdf)
>  In this letter we show how to improve the performance of backward chained behavior trees (BTs) using reinforcement learning (RL). BTs represent a hierarchical and modular way of combining control policies into higher level control policies. Backward chaining is a design principle that leads to BTs that combine reactivity with goal directed actions in a structured way. The backward chained structure has also enabled convergence proofs for BTs, identifying a set of local conditions that lead to the convergence of all trajectories to a set of desired goal states. The key idea of this letter is to improve performance of backward chained BTs by using the conditions identified in the theoretical convergence proof to setup RL problems for individual controllers. In particular, the analysis identified important subgoals, so-called active constraint conditions (ACCs), that should be preserved in order to avoid having to go back and address them again. We propose to setup the RL problems to not only achieve each immediate subgoal, but also avoid violating the identified ACCs. The resulting performance improvement can be seen to depend on how often ACC violations occured before the update, and how much effort was needed to re-achieve them. The proposed approach is illustrated in a dynamic simulation environment.      
### 4.Radiomic biomarker extracted from PI-RADS 3 patients support more eìcient and robust prostate cancer diagnosis: a multi-center study  [ :arrow_down: ](https://arxiv.org/pdf/2112.13686.pdf)
>  Prostate Imaging Reporting and Data System (PI-RADS) based on multi-parametric MRI classiêes patients into 5 categories (PI-RADS 1-5) for routine clinical diagnosis guidance. However, there is no consensus on whether PI-RADS 3 patients should go through biopsies. Mining features from these hard samples (HS) is meaningful for physicians to achieve accurate diagnoses. Currently, the mining of HS biomarkers is insuìcient, and the eéectiveness and robustness of HS biomarkers for prostate cancer diagnosis have not been explored. In this study, biomarkers from diéerent data distributions are constructed. Results show that HS biomarkers can achieve better performances in diéerent data distributions.      
### 5.Self-normalized Classification of Parkinson's Disease DaTscan Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.13637.pdf)
>  Classifying SPECT images requires a preprocessing step which normalizes the images using a normalization region. The choice of the normalization region is not standard, and using different normalization regions introduces normalization region-dependent variability. This paper mathematically analyzes the effect of the normalization region to show that normalized-classification is exactly equivalent to a subspace separation of the half rays of the images under multiplicative equivalence. Using this geometry, a new self-normalized classification strategy is proposed. This strategy eliminates the normalizing region altogether. The theory is used to classify DaTscan images of 365 Parkinson's disease (PD) subjects and 208 healthy control (HC) subjects from the Parkinson's Progression Marker Initiative (PPMI). The theory is also used to understand PD progression from baseline to year 4.      
### 6.Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN  [ :arrow_down: ](https://arxiv.org/pdf/2112.13626.pdf)
>  Translational brain research using Magnetic Resonance Imaging (MRI) is becoming increasingly popular as animal models are an essential part of scientific studies and ultra-high-field scanners become more available. Some drawbacks of MRI are MRI scanner availability, and the time needed to perform a full scanning session (it usually takes over 30 minutes). Data protection laws and 3R ethical rule also make it difficult to create large data sets for training Deep Learning models. Generative Adversarial Networks (GAN) have been shown capable of performing data augmentation with higher quality than other techniques. In this work, the alpha-GAN architecture is used to test its ability to generate realistic 3D MRI scans of the rat brain. As far as the authors are aware, this is the first time an approach based on GANs is used for data augmentation in preclinical data. The generated scans are evaluated using various qualitative and quantitative metrics. A Turing test performed by 4 experts has shown that the generated scans can trick almost any expert. The generated scans were also used to evaluate their impact on the performance of an existing deep learning model developed for rat brain segmentation of white matter, grey matter, and cerebrospinal fluid. The models were compared using the Dice score. The best results for the segmentation of whole brain and white matter were achieved when 174 real scans and 348 synthetic ones were used, with improvements of 0.0172 and 0.0129. The use of 174 real scans and 87 synthetic ones led to improvements of 0.0038 and 0.0764 of grey matter and cerebrospinal fluid segmentation. Thus, by using the proposed new normalisation layer and loss functions, it was possible to improve the realism of the generated rat MRI scans and it was demonstrated that using the data generated improved the segmentation model more than using conventional data augmentation.      
### 7.Over-the-Air Multi-Task Federated Learning Over MIMO Interference Channel  [ :arrow_down: ](https://arxiv.org/pdf/2112.13603.pdf)
>  With the explosive growth of data and wireless devices, federated learning (FL) has emerged as a promising technology for large-scale intelligent systems. Utilizing the analog superposition of electromagnetic waves, over-the-air computation is an appealing approach to reduce the burden of communication in the FL model aggregation. However, with the urgent demand for intelligent systems, the training of multiple tasks with over-the-air computation further aggravates the scarcity of communication resources. This issue can be alleviated to some extent by training multiple tasks simultaneously with shared communication resources, but the latter inevitably brings about the problem of inter-task interference. In this paper, we study over-the-air multi-task FL (OA-MTFL) over the multiple-input multiple-output (MIMO) interference channel. We propose a novel model aggregation method for the alignment of local gradients for different devices, which alleviates the straggler problem that exists widely in over-the-air computation due to the channel heterogeneity. We establish a unified communication-computation analysis framework for the proposed OA-MTFL scheme by considering the spatial correlation between devices, and formulate an optimization problem of designing transceiver beamforming and device selection. We develop an algorithm by using alternating optimization (AO) and fractional programming (FP) to solve this problem, which effectively relieves the impact of inter-task interference on the FL learning performance. We show that due to the use of the new model aggregation method, device selection is no longer essential to our scheme, thereby avoiding the heavy computational burden caused by implementing device selection. The numerical results demonstrate the correctness of the analysis and the outstanding performance of the proposed scheme.      
### 8.Smart Grid Architecture with High Proportion of Energy Utilization  [ :arrow_down: ](https://arxiv.org/pdf/2112.13598.pdf)
>  DC power generation is an emerging trend and has been preferred due to its low cost and low in system power losses within local distribution grid. The theme of this paper is the indigenous design of a DC standalone micro grid which will stabilize the fluctuating power generated from hybrid energy resources and manage its power distribution. The power is distributed through a DC distribution line and converted to the required AC or DC form by converters placed near loads. This project includes power generation, distribution and management strategies for a sustainable micro grid primarily powered by wind and solar energy. DC transmittable power can increase the system efficiency up to 10% as compared to AC.      
### 9.Depth estimation of endoscopy using sim-to-real transfer  [ :arrow_down: ](https://arxiv.org/pdf/2112.13595.pdf)
>  In order to use the navigation system effectively, distance information sensors such as depth sensors are essential. Since depth sensors are difficult to use in endoscopy, many groups propose a method using convolutional neural networks. In this paper, the ground truth of the depth image and the endoscopy image is generated through endoscopy simulation using the colon model segmented by CT colonography. Photo-realistic simulation images can be created using a sim-to-real approach using cycleGAN for endoscopy images. By training the generated dataset, we propose a quantitative endoscopy depth estimation network. The proposed method represents a better-evaluated score than the existing unsupervised training-based results.      
### 10.Survey on Stabilization of Nonlinear Systems via state/output feedback control  [ :arrow_down: ](https://arxiv.org/pdf/2112.13588.pdf)
>  This survey paper deals with the stabilization of nonlinear systems by analyzing the controlling method in terms of state feedback and output feedback. A brief overview of some literature on how the feedback controller of some dynamic systems in real applications like robots can be applicable is introduced. The aim is to give a direction about the methods of how to solve the state and output feedback stabilization problems for nonlinear systems based on feedback design methods and other theorems like the Lyapunov stability theorem. The feedback controllers can be constructed to ensure the origin of the nonlinear system is whether asymptotically stable or not. The paper is purposely focused on directing the theoretical results regarding the presence of such a feedback controller in stabilization. Keywords: Nonlinear systems, stability, Feedback stabilization, State Feedback, Output Feedback      
### 11.Task-specific Optimization of Virtual Channel Linear Prediction-based Speech Dereverberation Front-End for Far-Field Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2112.13569.pdf)
>  Developing a single-microphone speech denoising or dereverberation front-end for robust automatic speaker verification (ASV) in noisy far-field speaking scenarios is challenging. To address this problem, we present a novel front-end design that involves a recently proposed extension of the weighted prediction error (WPE) speech dereverberation algorithm, the virtual acoustic channel expansion (VACE)-WPE. It is demonstrated experimentally in this study that unlike the conventional WPE algorithm, the VACE-WPE can be explicitly trained to cancel out both late reverberation and background noise. To build the front-end, the VACE-WPE is first independently (pre)trained to produce "noisy" dereverberated signals. Subsequently, given a pretrained speaker embedding model, the VACE-WPE is additionally fine-tuned within a task-specific optimization (TSO) framework, causing the speaker embedding extracted from the processed signal to be similar to that extracted from the "noise-free" target signal. Moreover, to extend the application of the proposed front-end to more general, unconstrained "in-the-wild" ASV scenarios beyond controlled far-field conditions, we propose a distortion regularization method for the VACE-WPE within the TSO framework. The effectiveness of the proposed approach is verified on both far-field and in-the-wild ASV benchmarks, demonstrating its superiority over fully neural front-ends and other TSO methods in various cases.      
### 12.DAM-AL: Dilated Attention Mechanism with Attention Loss for 3D Infant Brain Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2112.13559.pdf)
>  While Magnetic Resonance Imaging (MRI) has played an essential role in infant brain analysis, segmenting MRI into a number of tissues such as gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) is crucial and complex due to the extremely low intensity contrast between tissues at around 6-9 months of age as well as amplified noise, myelination, and incomplete volume. In this paper, we tackle those limitations by developing a new deep learning model, named DAM-AL, which contains two main contributions, i.e., dilated attention mechanism and hard-case attention loss. Our DAM-AL network is designed with skip block layers and atrous block convolution. It contains both channel-wise attention at high-level context features and spatial attention at low-level spatial structural features. Our attention loss consists of two terms corresponding to region information and hard samples attention. Our proposed DAM-AL has been evaluated on the infant brain iSeg 2017 dataset and the experiments have been conducted on both validation and testing sets. We have benchmarked DAM-AL on Dice coefficient and ASD metrics and compared it with state-of-the-art methods.      
### 13.Classification of Histopathology Images of Lung Cancer Using Convolutional Neural Network (CNN)  [ :arrow_down: ](https://arxiv.org/pdf/2112.13553.pdf)
>  Cancer is the uncontrollable cell division of abnormal cells inside the human body, which can spread to other body organs. It is one of the non-communicable diseases (NCDs) and NCDs accounts for 71% of total deaths worldwide whereas lung cancer is the second most diagnosed cancer after female breast cancer. Cancer survival rate of lung cancer is only 19%. There are various methods for the diagnosis of lung cancer, such as X-ray, CT scan, PET-CT scan, bronchoscopy and biopsy. However, to know the subtype of lung cancer based on the tissue type H and E staining is widely used, where the staining is done on the tissue aspirated from a biopsy. Studies have reported that the type of histology is associated with prognosis and treatment in lung cancer. Therefore, early and accurate detection of lung cancer histology is an urgent need and as its treatment is dependent on the type of histology, molecular profile and stage of the disease, it is most essential to analyse the histopathology images of lung cancer. Hence, to speed up the vital process of diagnosis of lung cancer and reduce the burden on pathologists, Deep learning techniques are used. These techniques have shown improved efficacy in the analysis of histopathology slides of cancer. Several studies reported the importance of convolution neural networks (CNN) in the classification of histopathological pictures of various cancer types such as brain, skin, breast, lung, colorectal cancer. In this study tri-category classification of lung cancer images (normal, adenocarcinoma and squamous cell carcinoma) are carried out by using ResNet 50, VGG-19, Inception_ResNet_V2 and DenseNet for the feature extraction and triplet loss to guide the CNN such that it increases inter-cluster distance and reduces intra-cluster distance.      
### 14.DPCCN: Densely-Connected Pyramid Complex Convolutional Network for Robust Speech Separation And Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2112.13520.pdf)
>  In recent years, a number of time-domain speech separation methods have been proposed. However, most of them are very sensitive to the environments and wide domain coverage tasks. In this paper, from the time-frequency domain perspective, we propose a densely-connected pyramid complex convolutional network, termed DPCCN, to improve the robustness of speech separation under complicated conditions. Furthermore, we generalize the DPCCN to target speech extraction (TSE) by integrating a new specially designed speaker encoder. Moreover, we also investigate the robustness of DPCCN to unsupervised cross-domain TSE tasks. A Mixture-Remix approach is proposed to adapt the target domain acoustic characteristics for fine-tuning the source model. We evaluate the proposed methods not only under noisy and reverberant in-domain condition, but also in clean but cross-domain conditions. Results show that for both speech separation and extraction, the DPCCN-based systems achieve significantly better performance and robustness than the currently dominating time-domain methods, especially for the cross-domain tasks. Particularly, we find that the Mixture-Remix fine-tuning with DPCCN significantly outperforms the TD-SpeakerBeam for unsupervised cross-domain TSE, with around 3.5 dB performance improvement on target domain test set, without any source domain performance degradation.      
### 15.MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer  [ :arrow_down: ](https://arxiv.org/pdf/2112.13513.pdf)
>  Pancreatic cancer is one of the most malignant cancers in the world, which deteriorates rapidly with very high mortality. The rapid on-site evaluation (ROSE) technique innovates the workflow by immediately analyzing the fast stained cytopathological images with on-site pathologists, which enables faster diagnosis in this time-pressured process. However, the wider expansion of ROSE diagnosis has been hindered by the lack of experienced pathologists. To overcome this problem, we propose a hybrid high-performance deep learning model to enable the automated workflow, thus freeing the occupation of the valuable time of pathologists. By firstly introducing the Transformer block into this field with our particular multi-stage hybrid design, the spatial features generated by the convolutional neural network (CNN) significantly enhance the Transformer global modeling. Turning multi-stage spatial features as global attention guidance, this design combines the robustness from the inductive bias of CNN with the sophisticated global modeling power of Transformer. A dataset of 4240 ROSE images is collected to evaluate the method in this unexplored field. The proposed multi-stage hybrid Transformer (MSHT) achieves 95.68% in classification accuracy, which is distinctively higher than the state-of-the-art models. Facing the need for interpretability, MSHT outperforms its counterparts with more accurate attention regions. The results demonstrate that the MSHT can distinguish cancer samples accurately at an unprecedented image scale, laying the foundation for deploying automatic decision systems and enabling the expansion of ROSE in clinical practice. The code and records are available at: <a class="link-external link-https" href="https://github.com/sagizty/Multi-Stage-Hybrid-Transformer" rel="external noopener nofollow">this https URL</a>.      
### 16.Under-Approximate Reachability Analysis for a Class of Linear Uncertain Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.13503.pdf)
>  Under-approximations of reachable sets and tubes have received recent research attention due to their important roles in control synthesis and verification. Available under-approximation methods designed for continuous-time linear systems typically assume the ability to compute transition matrices and their integrals exactly, which is not feasible in general. In this note, we attempt to overcome this drawback for a class of linear time-invariant (LTI) systems, where we propose a novel method to under-approximate finite-time forward reachable sets and tubes utilizing approximations of the matrix exponential. In particular, we consider the class of continuous-time LTI systems with an identity input matrix and uncertain initial and input values belonging to full dimensional sets that are affine transformations of closed unit balls. The proposed method yields computationally efficient under-approximations of reachable sets and tubes with first order convergence guarantees in the sense of the Hausdorff distance. To illustrate its performance, the proposed method is implemented in three numerical examples, where linear systems of dimensions ranging between 2 and 200 are considered.      
### 17.Sinogram upsampling using Primal-Dual UNet for undersampled CT and radial MRI reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2112.13443.pdf)
>  CT and MRI are two widely used clinical imaging modalities for non-invasive diagnosis. However, both of these modalities come with certain problems. CT uses harmful ionising radiation, and MRI suffers from slow acquisition speed. Both problems can be tackled by undersampling, such as sparse sampling. However, such undersampled data leads to lower resolution and introduces artefacts. Several techniques, including deep learning based methods, have been proposed to reconstruct such data. However, the undersampled reconstruction problem for these two modalities was always considered as two different problems and tackled separately by different research works. This paper proposes a unified solution for both sparse CT and undersampled radial MRI reconstruction, achieved by applying Fourier transform-based pre-processing on the radial MRI and then reconstructing both modalities using sinogram upsampling combined with filtered back-projection. The Primal-Dual network is a deep learning based method for reconstructing sparsely-sampled CT data. This paper introduces Primal-Dual UNet, which improves the Primal-Dual network in terms of accuracy and reconstruction speed. The proposed method resulted in an average SSIM of 0.932 while performing sparse CT reconstruction for fan-beam geometry with a sparsity level of 16, achieving a statistically significant improvement over the previous model, which resulted in 0.919. Furthermore, the proposed model resulted in 0.903 and 0.957 average SSIM while reconstructing undersampled brain and abdominal MRI data with an acceleration factor of 16 - statistically significant improvements over the original model, which resulted in 0.867 and 0.949. Finally, this paper shows that the proposed network not only improves the overall image quality, but also improves the image quality for the regions-of-interest; as well as generalises better in presence of a needle.      
### 18.Over-the-Air Computation with DFT-spread OFDM for Federated Edge Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.13439.pdf)
>  In this study, we propose an over-the-air computation (AirComp) scheme for federated edge learning (FEEL) without channel state information (CSI) at the edge devices (EDs) or the edge server (ES). The proposed scheme relies on non-coherent communication techniques for achieving distributed training by majority vote (MV). In this work, the votes, i.e., the signs of the local gradients, from the EDs are represented with the pulse-position modulation (PPM) symbols constructed with discrete Fourier transform (DFT)-spread orthogonal frequency division multiplexing (OFDM) (DFT-s-OFDM). By taking the delay spread and time-synchronization errors into account, the MV at the ES is obtained with an energy detector. Hence, the proposed scheme does not require CSI at the EDs and ES. We also prove the convergence of the distributed training when the MV is obtained with the proposed scheme under fading channel. Through simulations, we show that the proposed scheme provides a high test accuracy in fading channels while resulting in lower peak-to-mean envelope power ratio (PMEPR) symbols.      
### 19.Deep Curriculum Learning for PolSAR Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2112.13426.pdf)
>  Following the great success of curriculum learning in the area of machine learning, a novel deep curriculum learning method proposed in this paper, entitled DCL, particularly for the classification of fully polarimetric synthetic aperture radar (PolSAR) data. This method utilizes the entropy-alpha target decomposition method to estimate the degree of complexity of each PolSAR image patch before applying it to the convolutional neural network (CNN). Also, an accumulative mini-batch pacing function is used to introduce more difficult patches to CNN.Experiments on the widely used data set of AIRSAR Flevoland reveal that the proposed curriculum learning method can not only increase classification accuracy but also lead to faster training convergence.      
### 20.Two-layer nonlinear control of DC-DC buck converters with meshed network topology  [ :arrow_down: ](https://arxiv.org/pdf/2112.13385.pdf)
>  In this paper, we analyse the behaviour of a buck converter network that contains arbitrary, up to mild regularity assumptions, loads. Our analysis of the network begins with the study of the current dynamics; we propose a novel Lyapunov function for the current in closed-loop with a bounded integrator. We leverage on these results to analyse the interaction properties between voltages and bounded currents as well as between node voltages and to propose a two-layer optimal controller that keeps network voltages within a compact neighbourhood of the nominal operational voltage. We analyse the stability of the closed loop system in two ways: one considering the interconnection properties which yields a weaker ISS type property and a second that contemplates the network in closed loop with a distributed optimal controller. For the latter, we propose a novel distributed way of controlling a Laplacian network using neighbouring information which results in asymptotic stability. We demonstrate our results in a meshed topology network containing 6 power converters, each converter feeding an individual constant power load with values chaning arbitrarily within a pre-specified range.      
### 21.A Trained Regularization Approach Based on Born Iterative Method for Electromagnetic Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2112.13367.pdf)
>  A trained-based Born iterative method (TBIM) is developed for electromagnetic imaging (EMI) applications. The proposed TBIM consists of a nested loop; the outer loop executes TBIM iteration steps, while the inner loop executes a trained iterative shrinkage thresholding algorithm (TISTA). The applied TISTA runs linear Landweber iterations implemented with a trained regularization network designed based on U-net architecture. A normalization process was imposed in TISTA that made TISTA training applicable within the proposed TBIM. The iterative utilization of the regularization network in TISTA is a bottleneck that demands high memory allocation through the training process. Therefore TISTA within each TBIM step was trained separately. The TISTA regularization network in each TBIM step was initialized using the weights from the previous TBIM step. The above approach achieved high-quality image restoration after running few TBIM steps while maintained low memory allocation through the training process. The proposed framework can be extended to Newton or quasi-Newton schemes, where within each Newton iteration, a linear ill-posed problem is optimized that differs from one example to another. The numerical results illustrated in this work show the superiority of the proposed TBIM compared to the conventional sparse-based Born iterative method (SBIM).      
### 22.AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2112.13366.pdf)
>  In this paper we present AIDA, which is an active inference-based agent that iteratively designs a personalized audio processing algorithm through situated interactions with a human client. The target application of AIDA is to propose on-the-spot the most interesting alternative values for the tuning parameters of a hearing aid (HA) algorithm, whenever a HA client is not satisfied with their HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of optimal (acoustic) context-aware Bayesian trial design. In computational terms, AIDA is realized as an active inference-based agent with an Expected Free Energy criterion for trial design. This type of architecture is inspired by neuro-economic models on efficient (Bayesian) trial design in brains and implies that AIDA comprises generative probabilistic models for acoustic signals and user responses. We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive filters and a user response model based on a Gaussian Process Classifier. The full AIDA agent has been implemented in a factor graph for the generative model and all tasks (parameter learning, acoustic context classification, trial design, etc.) are realized by variational message passing on the factor graph. All verification and validation experiments and demonstrations are freely accessible at our GitHub repository.      
### 23.Learning Cross-Scale Prediction for Efficient Neural Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2112.13309.pdf)
>  In this paper, we present the first neural video codec that can compete with the latest coding standard H.266/VVC in terms of sRGB PSNR on UVG dataset for the low-latency mode. Existing neural hybrid video coding approaches rely on optical flow or Gaussian-scale flow for prediction, which cannot support fine-grained adaptation to diverse motion content. Towards more content-adaptive prediction, we propose a novel cross-scale prediction module that achieves more effective motion compensation. Specifically, on the one hand, we produce a reference feature pyramid as prediction sources, then transmit cross-scale flows that leverage the feature scale to control the precision of prediction. On the other hand, we introduce the mechanism of weighted prediction into the scenario of prediction with a single reference frame, where cross-scale weight maps are transmitted to synthesize a fine prediction result. In addition to the cross-scale prediction module, we further propose a multi-stage quantization strategy, which improves the rate-distortion performance with no extra computational penalty during inference. We show the encouraging performance of our efficient neural video codec (ENVC) on several common benchmark datasets and analyze in detail the effectiveness of every important component.      
### 24.Deep-learned speckle pattern and its application to ghost imaging  [ :arrow_down: ](https://arxiv.org/pdf/2112.13293.pdf)
>  In this paper, we present a method for speckle pattern design using deep learning. The speckle patterns possess unique features after experiencing convolutions in Speckle-Net, our well-designed framework for speckle pattern generation. We then apply our method to the computational ghost imaging system. The standard deep learning-assisted ghost imaging methods use the network to recognize the reconstructed objects or imaging algorithms. In contrast, this innovative application optimizes the illuminating speckle patterns via Speckle-Net with specific sampling ratios. Our method, therefore, outperforms the other techniques for ghost imaging, particularly its ability to retrieve high-quality images with extremely low sampling ratios. It opens a new route towards nontrivial speckle generation by referring to a standard loss function on specified objectives with the modified deep neural network. It also has great potential for applications in the fields of dynamic speckle illumination microscopy, structured illumination microscopy, x-ray imaging, photo-acoustic imaging, and optical lattices.      
### 25.Artifact Reduction in Fundus Imaging using Cycle Consistent Adversarial Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.13264.pdf)
>  Fundus images are very useful in identifying various ophthalmic disorders. However, due to the presence of artifacts, the visibility of the retina is severely affected. This may result in misdiagnosis of the disorder which may lead to more complicated problems. Since deep learning is a powerful tool to extract patterns from data without much human intervention, they can be applied to image-to-image translation problems. An attempt has been made in this paper to automatically rectify such artifacts present in the images of the fundus. We use a CycleGAN based model which consists of residual blocks to reduce the artifacts in the images. Significant improvements are seen when compared to the existing techniques.      
### 26.Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2112.13227.pdf)
>  Although equirectangular projection (ERP) is a convenient form to store omnidirectional images (also known as 360-degree images), it is neither equal-area nor conformal, thus not friendly to subsequent visual communication. In the context of image compression, ERP will over-sample and deform things and stuff near the poles, making it difficult for perceptually optimal bit allocation. In conventional 360-degree image compression, techniques such as region-wise packing and tiled representation are introduced to alleviate the over-sampling problem, achieving limited success. In this paper, we make one of the first attempts to learn deep neural networks for omnidirectional image compression. We first describe parametric pseudocylindrical representation as a generalization of common pseudocylindrical map projections. A computationally tractable greedy method is presented to determine the (sub)-optimal configuration of the pseudocylindrical representation in terms of a novel proxy objective for rate-distortion performance. We then propose pseudocylindrical convolutions for 360-degree image compression. Under reasonable constraints on the parametric representation, the pseudocylindrical convolution can be efficiently implemented by standard convolution with the so-called pseudocylindrical padding. To demonstrate the feasibility of our idea, we implement an end-to-end 360-degree image compression system, consisting of the learned pseudocylindrical representation, an analysis transform, a non-uniform quantizer, a synthesis transform, and an entropy model. Experimental results on $19,790$ omnidirectional images show that our method achieves consistently better rate-distortion performance than the competing methods. Moreover, the visual quality by our method is significantly improved for all images at all bitrates.      
### 27.Network-Aware 5G Edge Computing for Object Detection: Augmenting Wearables to "See'' More, Farther and Faster  [ :arrow_down: ](https://arxiv.org/pdf/2112.13194.pdf)
>  Advanced wearable devices are increasingly incorporating high-resolution multi-camera systems. As state-of-the-art neural networks for processing the resulting image data are computationally demanding, there has been growing interest in leveraging fifth generation (5G) wireless connectivity and mobile edge computing for offloading this processing to the cloud. To assess this possibility, this paper presents a detailed simulation and evaluation of 5G wireless offloading for object detection within a powerful, new smart wearable called VIS4ION, for the Blind-and-Visually Impaired (BVI). The current VIS4ION system is an instrumented book-bag with high-resolution cameras, vision processing and haptic and audio feedback. The paper considers uploading the camera data to a mobile edge cloud to perform real-time object detection and transmitting the detection results back to the wearable. To determine the video requirements, the paper evaluates the impact of video bit rate and resolution on object detection accuracy and range. A new street scene dataset with labeled objects relevant to BVI navigation is leveraged for analysis. The vision evaluation is combined with a detailed full-stack wireless network simulation to determine the distribution of throughputs and delays with real navigation paths and ray-tracing from new high-resolution 3D models in an urban environment. For comparison, the wireless simulation considers both a standard 4G-Long Term Evolution (LTE) carrier and high-rate 5G millimeter-wave (mmWave) carrier. The work thus provides a thorough and realistic assessment of edge computing with mmWave connectivity in an application with both high bandwidth and low latency requirements.      
### 28.A Comprehensive Review of Myoelectric Prosthesis Control  [ :arrow_down: ](https://arxiv.org/pdf/2112.13192.pdf)
>  Prosthetic hands can be used to support upper-body amputees. Myoelectric prosthesis, one of the externally-powered active prosthesis categories, requires proper processing units in addition to recording electrodes and instrumentation amplifiers. In this paper, the following myoelectric prosthesis control methods were discussed in detail: On-off and finite-state, proportional, direct, and posture, simultaneous, classification and regression-based control, and deep learning methods. Myoelectric control performance indices, such as completion time and rate, throughput, lag, and path length, were reviewed. The advantages and disadvantages of the control methods were also discussed. Some of myoelectric prosthesis control's significant challenges are comfort, durability, cost, the application of under-sampled signals, and electrode shift. Moreover, the proposed algorithms must be usually tuned after each don and doff, which is not comfortable for the users. Real-time simultaneous and proportional myoelectric control, resampling human's arm, has brought much attention. However, increasing the degree of freedom reduces the overall performance. Applying a 3D printed prosthesis arm and under-sampled electromyographic signals could reduce the fabrication cost and improve the application of such methods in practice. There are many technological and clinical challenges in this area to reduce the prosthesis rejection rate.      
### 29.DSRGAN: Detail Prior-Assisted Perceptual Single Image Super-Resolution via Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.13191.pdf)
>  The generative adversarial network (GAN) is successfully applied to study the perceptual single image superresolution (SISR). However, the GAN often tends to generate images with high frequency details being inconsistent with the real ones. Inspired by conventional detail enhancement algorithms, we propose a novel prior knowledge, the detail prior, to assist the GAN in alleviating this problem and restoring more realistic details. The proposed method, named DSRGAN, includes a well designed detail extraction algorithm to capture the most important high frequency information from images. Then, two discriminators are utilized for supervision on image-domain and detail-domain restorations, respectively. The DSRGAN merges the restored detail into the final output via a detail enhancement manner. The special design of DSRGAN takes advantages from both the model-based conventional algorithm and the data-driven deep learning network. Experimental results demonstrate that the DSRGAN outperforms the state-of-the-art SISR methods on perceptual metrics and achieves comparable results in terms of fidelity metrics simultaneously. Following the DSRGAN, it is feasible to incorporate other conventional image processing algorithms into a deep learning network to form a model-based deep SISR.      
### 30.TeraHertz Band Communication: An Old Problem Revisited and Research Directions for the Next Decade  [ :arrow_down: ](https://arxiv.org/pdf/2112.13187.pdf)
>  TeraHertz (THz) band communications are envisioned as a key technology for 6G and Beyond. As a fundamental wireless infrastructure, THz communication can boost abundant promising applications. In 2014, our team published two comprehensive roadmaps for the development and progress of THz communication networks [1], [2], which helped the research community to start research on this subject afterwards. In particular, this topic became very important and appealing to the research community due to 6G wireless systems design and development in recent years. Many papers are getting published covering different aspects of wireless systems using the THz band. With this paper, our aim is looking back to the last decade and revisiting the old problems and pointing out what has been achieved in the research community so far. Furthermore, in this paper still to be investigated new research challenges for the THz band communication systems are presented by covering diverse subtopics such as from perspectives of devices, channel behavior, communication and networking problems, physical testbeds and demonstration systems. The key aspects presented in this paper will enable THz communications as a pillar of 6G and Beyond wireless systems in the next decade.      
### 31.On the Feasibility of 4.9 GHz Public Safety Band as Spectrum Option for Internet of Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2112.13170.pdf)
>  There is an unprecedented impetus on the advancement of internet of vehicles (IoV). The vehicle-to-everything (V2X) communication is well acknowledged as the key technology in constitution of the IoV. Nevertheless, the spectrum for V2X communication is undergoing a massive change in the United States: a majority of the bandwidth has been reallocated to Wi-Fi leaving even less than a half of the bandwidth for V2X. This motivates investigation of other candidate spectrum bands for operation of V2X communication as an urgent effort to guarantee efficient operations of IoV. To this line, this paper studies the feasibility of sharing the 4.9 GHz public safety band between the incumbent systems and V2X users.      
### 32.Ultrasound Speckle Suppression and Denoising using MRI-derived Normalizing Flow Priors  [ :arrow_down: ](https://arxiv.org/pdf/2112.13110.pdf)
>  Ultrasonography offers an inexpensive, widely-accessible and compact medical imaging solution. However, compared to other imaging modalities such as CT and MRI, ultrasound images notoriously suffer from strong speckle noise, which originates from the random interference of sub-wavelength scattering. This deteriorates ultrasound image quality and makes interpretation challenging. We here propose a new unsupervised ultrasound speckle reduction and image denoising method based on maximum-a-posteriori estimation with deep generative priors that are learned from high-quality MRI images. To model the generative tissue reflectivity prior, we exploit normalizing flows, which in recent years have shown to be very powerful in modeling signal priors across a variety of applications. To facilitate generaliation, we factorize the prior and train our flow model on patches from the NYU fastMRI (fully-sampled) dataset. This prior is then used for inference in an iterative denoising scheme. We first validate the utility of our learned priors on noisy MRI data (no prior domain shift), and then turn to evaluating performance on both simulated and in-vivo ultrasound images from the PICMUS and CUBDL datasets. The results show that the method outperforms other (unsupervised) ultrasound denoising methods (NLM and OBNLM) both quantitatively and qualitatively.      
### 33.Generalized Wasserstein Dice Loss, Test-time Augmentation, and Transformers for the BraTS 2021 challenge  [ :arrow_down: ](https://arxiv.org/pdf/2112.13054.pdf)
>  Brain tumor segmentation from multiple Magnetic Resonance Imaging (MRI) modalities is a challenging task in medical image computation. The main challenges lie in the generalizability to a variety of scanners and imaging protocols. In this paper, we explore strategies to increase model robustness without increasing inference time. Towards this aim, we explore finding a robust ensemble from models trained using different losses, optimizers, and train-validation data split. Importantly, we explore the inclusion of a transformer in the bottleneck of the U-Net architecture. While we find transformer in the bottleneck performs slightly worse than the baseline U-Net in average, the generalized Wasserstein Dice loss consistently produces superior results. Further, we adopt an efficient test time augmentation strategy for faster and robust inference. Our final ensemble of seven 3D U-Nets with test-time augmentation produces an average dice score of 89.4% and an average Hausdorff 95% distance of 10.0 mm when evaluated on the BraTS 2021 testing dataset. Our code and trained models are publicly available at <a class="link-external link-https" href="https://github.com/LucasFidon/TRABIT_BraTS2021" rel="external noopener nofollow">this https URL</a>.      
### 34.Total Energy Shaping with Neural Interconnection and Damping Assignment -- Passivity Based Control  [ :arrow_down: ](https://arxiv.org/pdf/2112.12999.pdf)
>  In this work we exploit the universal approximation property of Neural Networks (NNs) to design interconnection and damping assignment (IDA) passivity-based control (PBC) schemes for fully-actuated mechanical systems in the port-Hamiltonian (pH) framework. To that end, we transform the IDA-PBC method into a supervised learning problem that solves the partial differential matching equations, and fulfills equilibrium assignment and Lyapunov stability conditions. A main consequence of this, is that the output of the learning algorithm has a clear control-theoretic interpretation in terms of passivity and Lyapunov stability. The proposed control design methodology is validated for mechanical systems of one and two degrees-of-freedom via numerical simulations.      
### 35.Machine Learning-based Efficient Ventricular Tachycardia Detection Model of ECG Signal  [ :arrow_down: ](https://arxiv.org/pdf/2112.12956.pdf)
>  In primary diagnosis and analysis of heart defects, an ECG signal plays a significant role. This paper presents a model for the prediction of ventricular tachycardia arrhythmia using noise filtering, a unique set of ECG features, and a machine learning-based classifier model. Before signal feature extraction, we detrend and denoise the signal to eliminate the noise for detecting features properly. After that necessary features have been extracted and necessary parameters related to these features are measured. Using these parameters, we prepared one efficient multiclass classifier model using a machine learning approach that can classify different types of ventricular tachycardia arrhythmias efficiently. Our results indicate that Logistic regression and Decision tree-based models are the most efficient machine learning models for detecting ventricular tachycardia arrhythmia. In order to diagnose heart diseases and find care for a patient, an early, reliable diagnosis of different types of arrhythmia is necessary. By implementing our proposed method, this work deals with the problem of reducing the misclassification of the critical signal related to ventricular tachycardia very efficiently. Experimental findings demonstrate satisfactory enhancements and demonstrate high resilience to the algorithm that we have proposed. With this assistance, doctors can assess this type of arrhythmia of a patient early and take the right decision at the proper time.      
### 36.Supraventricular Tachycardia Detection and Classification Model of ECG signal Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.12953.pdf)
>  Investigation on the electrocardiogram (ECG) signals is an essential way to diagnose heart disease since the ECG process is noninvasive and easy to use. This work presents a supraventricular arrhythmia prediction model consisting of a few stages, including filtering of noise, a unique collection of ECG characteristics, and automated learning classifying model to classify distinct types, depending on their severity. We de-trend and de-noise a signal to reduce noise to better determine functionality before extractions are performed. After that, we present one R-peak detection method and Q-S detection method as a part of necessary feature extraction. Next parameters are computed that correspond to these features. Using these characteristics, we have developed a classification model based on machine learning that can successfully categorize different types of supraventricular tachycardia. Our findings suggest that decision-tree-based models are the most efficient machine learning models for supraventricular tachycardia arrhythmia. Among all the machine learning models, this model most efficiently lowers the crucial signal misclassification of supraventricular tachycardia. Experimental results indicate satisfactory improvements and demonstrate a superior efficiency of the proposed approach with 97% accuracy.      
### 37.Accuracy and Application Scope Analysis for Linearized Branch Flow Model in Radial Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.12942.pdf)
>  An in-depth analysis of linearized branch flow (LBF) model considering current injection and absolute value of impedance is proposed in this paper. The form of LBF model is based on two equations: the current injection to meet KCL and the voltage drop to meet KVL. By representing the absolute value of complex load power with the current injection, LBF model is much simpler than alternating current power flow (ACPF) model. The results on theoretical analysis and numerical studies show that LBF exhibits the high accuracy in bus voltage magnitude but a poor performance in branch flow. Moreover, LBF is also compared with fast decoupled linearized power flow (FDLPF) model to verify its efficiency, thus proving its superiority for fast evaluation of large-scale distribution systems with high accuracy in voltage magnitude. Finally, this paper analyzes three factors to lower LBF's errors of branch flow, as well as LBF's possible application scope.      
### 38.LFT Representation of a Class of Nonlinear Systems: A Data-Driven Approach  [ :arrow_down: ](https://arxiv.org/pdf/2112.12856.pdf)
>  This paper focuses on developing a method to obtain an uncertain linear fractional transformation (LFT) system that adequately captures the dynamics of a nonlinear time-invariant system over some desired envelope. First, the nonlinear system is approximated as a polynomial nonlinear state-space (PNLSS) system, and a linear parameter-varying (LPV) representation of the PNLSS model is obtained. To reduce the potentially large number of scheduling parameters in the resulting LPV system, an approach based on the cascade feedforward neural network (CFNN) is proposed. We account for the approximation and reduction errors through the addition of a norm-bounded, causal, dynamic uncertainty in the LFT system. Falsification is used in a novel way to perform guided simulations for deriving a norm bound on the dynamic uncertainty and generating data for training the CFNN. Then, robustness analysis using integral quadratic constraint theory is carried out to choose an LFT representation that leads to a computationally tractable analysis problem and useful analysis results. Finally, the proposed approach is used to obtain an LFT representation for the nonlinear equations of motion of a fixed-wing unmanned aircraft system.      
### 39.Self-Attention Generative Adversarial Network for Iterative Reconstruction of CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.12810.pdf)
>  Computed tomography (CT) uses X-ray measurements taken from sensors around the body to generate tomographic images of the human body. Conventional reconstruction algorithms can be used if the X-ray data are adequately sampled and of high quality; however, concerns such as reducing dose to the patient, or geometric limitations on data acquisition, may result in low quality or incomplete data. Images reconstructed from these data using conventional methods are of poor quality, due to noise and other artifacts. The aim of this study is to train a single neural network to reconstruct high-quality CT images from noisy or incomplete CT scan data, including low-dose, sparse-view, and limited-angle scenarios. To accomplish this task, we train a generative adversarial network (GAN) as a signal prior, to be used in conjunction with the iterative simultaneous algebraic reconstruction technique (SART) for CT data. The network includes a self-attention block to model long-range dependencies in the data. We compare our Self-Attention GAN for CT image reconstruction with several state-of-the-art approaches, including denoising cycle GAN, CIRCLE GAN, and a total variation superiorized algorithm. Our approach is shown to have comparable overall performance to CIRCLE GAN, while outperforming the other two approaches.      
### 40.Degree-of-Freedom of Modulating Information in the Phases of Reconfigurable Intelligent Surface  [ :arrow_down: ](https://arxiv.org/pdf/2112.13787.pdf)
>  This paper investigates the information theoretical limit of a reconfigurable intelligent surface (RIS) aided communication scenario in which the RIS and the transmitter either jointly or independently send information to the receiver. The RIS is an emerging technology that uses a large number of passive reflective elements with adjustable phases to intelligently reflect the transmit signal to the intended receiver. While most previous studies of the RIS focus on its ability to beamform and to boost the received signal-to-noise ratio (SNR), this paper shows that if the information data stream is also available at the RIS and can be modulated through the adjustable phases at the RIS, significant improvement in the {degree-of-freedom} (DoF) of the overall channel is possible. For example, for an RIS system in which the signals are reflected from a transmitter with $M$ antennas to a receiver with $K$ antennas through an RIS with $N$ reflective elements, assuming no direct path between the transmitter and the receiver, joint transmission of the transmitter and the RIS can achieve a DoF of $\min(M+\frac{N}{2}-\frac{1}{2},N,K)$ as compared to the DoF of $\min(M,K)$ for the conventional multiple-input multiple-output (MIMO) channel. This result is obtained by establishing a connection between the RIS system and the MIMO channel with phase noises and by using results for characterizing the information dimension under projection. The result is further extended to the case with a direct path between the transmitter and the receiver, and also to the multiple access scenario, in which the transmitter and the RIS send independent information. Finally, this paper proposes a symbol-level precoding approach for modulating data through the phases of the RIS, and provides numerical simulation results to verify the theoretical DoF results.      
### 41.Dynamic Time Warping Clustering to Discover Socio-Economic Characteristics in Smart Water Meter Data  [ :arrow_down: ](https://arxiv.org/pdf/2112.13778.pdf)
>  Socio-economic characteristics are influencing the temporal and spatial variability of water demand - the biggest source of uncertainties within water distribution system modeling. Improving our knowledge on these influences can be utilized to decrease demand uncertainties. This paper aims to link smart water meter data to socio-economic user characteristics by applying a novel clustering algorithm that uses a dynamic time warping metric on daily demand patterns. The approach is tested on simulated and measured single family home datasets. We show that the novel algorithm performs better compared to commonly used clustering methods, both, in finding the right number of clusters as well as assigning patterns correctly. Additionally, the methodology can be used to identify outliers within clusters of demand patterns. Furthermore, this study investigates which socio-economic characteristics (e.g. employment status, number of residents) are prevalent within single clusters and, consequently, can be linked to the shape of the cluster's barycenters. In future, the proposed methods in combination with stochastic demand models can be used to fill data-gaps in hydraulic models.      
### 42.Wholesale Electricity Price Forecasting using Integrated Long-term Recurrent Convolutional Network Model  [ :arrow_down: ](https://arxiv.org/pdf/2112.13681.pdf)
>  Electricity price is a key factor affecting the decision-making for all market participants. Accurate forecasting of electricity prices is very important and is also very challenging since electricity price is highly volatile due to various factors. This paper proposes an integrated long-term recurrent convolutional network (ILRCN) model to predict electricity prices considering the majority contributing attributes to the market price as input. The proposed ILRCN model combines the functionalities of convolutional neural network and long short-term memory (LSTM) algorithm along with the proposed novel conditional error correction term. The combined ILRCN model can identify the linear and non-linear behavior within the input data. We have used ERCOT wholesale market price data along with load profile, temperature, and other factors for the Houston region to illustrate the proposed model. The performance of the proposed ILRCN electricity price forecasting model is verified using performance/evaluation metrics like mean absolute error and accuracy. Case studies reveal that the proposed ILRCN model is accurate and efficient in electricity price forecasting as compared to the support vector machine (SVM) model, fully-connected neural network model, LSTM model and the LRCN model without the conditional error correction stage.      
### 43.Personalized Lane Change Decision Algorithm Using Deep Reinforcement Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2112.13646.pdf)
>  To develop driving automation technologies for human, a human-centered methodology should be adopted for ensured safety and satisfactory user experience. Automated lane change decision in dense highway traffic is challenging, especially when considering the personalized preferences of different drivers. To fulfill human driver centered decision algorithm development, we carry out driver-in-the-loop experiments on a 6-Degree-of-Freedom driving simulator. Based on the analysis of the lane change data by drivers of three specific styles,personalization indicators are selected to describe the driver preferences in lane change decision. Then a deep reinforcement learning (RL) approach is applied to design human-like agents for automated lane change decision, with refined reward and loss functions to capture the driver preferences.The trained RL agents and benchmark agents are tested in a two-lane highway driving scenario, and by comparing the agents with the specific drivers at the same initial states of lane change, the statistics show that the proposed algorithm can guarantee higher consistency of lane change decision preferences. The driver personalization indicators and the proposed RL-based lane change decision algorithm are promising to contribute in automated lane change system developing.      
### 44.Multibeam Free Space Optics Receiver Enabled by a Programmable Photonic Mesh  [ :arrow_down: ](https://arxiv.org/pdf/2112.13644.pdf)
>  Free-space optics (FSO) is an attractive technology to meet the ever-growing demand for wireless bandwidth in next generation networks. To increase the spectral efficiency of FSO links, transmission over spatial division multiplexing (SDM) can be exploited, where orthogonal light beams have to be shaped according to suitable amplitude, phase, and polarization profiles. In this work, we show that a programmable photonic circuits, consisting of a silicon photonic mesh of tunable Mach-Zehnder Interferometers (MZIs) can be used as an adaptive multibeam receiver for a FSO communication link. The circuit can self-configure to simultaneously receive and separate, with negligible mutual crosstalk, signals carried by orthogonal FSO beams sharing the same wavelength and polarization. This feature is demonstrated on signal pairs either arriving at the receiver from orthogonal directions (direction-diversity) or being shaped according to different orthogonal spatial modes (mode-diversity), even in the presence of some mixing during propagation. The performance of programmable mesh as an adaptive multibeam receiver is assessed by means of data channel transmission at 10 Gbit/s a wavelength of 1550 nm, but the optical bandwidth of the receiver (&gt;40 nm) allows its use at much higher data rates as well as in wavelength-division multiplexing SDM communication links.      
### 45.Permutation Matrix Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2112.13630.pdf)
>  We propose a novel scheme that allows MIMO system to modulate a set of permutation matrices to send more information bits, extending our initial work on the topic. This system is called Permutation Matrix Modulation (PMM). The basic idea is to employ a permutation matrix as a precoder and treat it as a modulated symbol. We continue the evolution of index modulation in MIMO by adopting all-antenna activation and obtaining a set of unique symbols from altering the positions of the antenna transmit power. We provide the analysis of the achievable rate of PMM under Gaussian Mixture Model (GMM) distribution and evaluate the numerical results by comparing it with the other existing systems. The result shows that PMM outperforms the existing systems under a fair parameter setting. We also present a way to attain the optimal achievable rate of PMM by solving a maximization problem via interior-point method. A low complexity detection scheme based on zero-forcing (ZF) is proposed, and maximum likelihood (ML) detection is discussed. We demonstrate the trade-off between simulation of the symbol error rate (SER) and the computational complexity where ZF performs worse in the SER simulation but requires much less computational complexity than ML.      
### 46.Estimating Parameters of the Tree Root in Heterogeneous Soil Environments via Mask-Guided Multi-Polarimetric Integration Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2112.13494.pdf)
>  Ground-penetrating radar (GPR) has been used as a non-destructive tool for tree root inspection. Estimating root-related parameters from GPR radargrams greatly facilitates root health monitoring and imaging. However, the task of estimating root-related parameters is challenging as the root reflection is a complex function of multiple root parameters and root orientations. Existing methods can only estimate a single root parameter at a time without considering the influence of other parameters and root orientations, resulting in limited estimation accuracy under different root conditions. In addition, soil heterogeneity introduces clutter in GPR radargrams, making the data processing and interpretation even harder. To address these issues, a novel neural network architecture, called mask-guided multi-polarimetric integration neural network (MMI-Net), is proposed to automatically and simultaneously estimate multiple root-related parameters in heterogeneous soil environments. The MMI-Net includes two sub-networks: a MaskNet that predicts a mask to highlight the root reflection area to eliminate interfering environmental clutter, and a ParaNet that uses the predicted mask as guidance to integrate, extract, and emphasize informative features in multi-polarimetric radargrams for accurate estimation of five key root-related parameters. The parameters include the root depth, diameter, relative permittivity, horizontal and vertical orientation angles. Experimental results demonstrate that the proposed MMI-Net achieves high estimation accuracy in these root-related parameters. This is the first work that takes the combined contributions of root parameters and spatial orientations into account and simultaneously estimates multiple root-related parameters. The data and code implemented in the paper can be found at <a class="link-external link-https" href="https://haihan-sun.github.io/GPR.html" rel="external noopener nofollow">this https URL</a>.      
### 47.Leaderless Consensus of Heterogeneous Multiple Euler-Lagrange Systems with Unknown Disturbance  [ :arrow_down: ](https://arxiv.org/pdf/2112.13484.pdf)
>  This paper studies the leaderless consensus problem of {heterogeneous} multiple networked Euler-Lagrange systems subject to persistent disturbances with unknown constant biases, amplitudes, initial phases, and frequencies. The main characteristic of this study is that none of the agents has information of a common reference model or of a common reference trajectory. Therefore, the agents must simultaneously and in a distributed way: achieve consensus to a common reference model (group model); achieve consensus to a common reference trajectory; {and} reject the unknown disturbances. We show that this is possible via a suitable combination of techniques of distributed `observers', internal model principle, and adaptive regulation. The proposed design generalizes recent results on group model learning, which have been studied for linear agents over undirected networks. In this work, group model learning is achieved for Euler-Lagrange dynamics over directed networks in the presence of persistent unknown disturbances.      
### 48.Learning Optimization Proxies for Large-Scale Security-Constrained Economic Dispatch  [ :arrow_down: ](https://arxiv.org/pdf/2112.13469.pdf)
>  The Security-Constrained Economic Dispatch (SCED) is a fundamental optimization model for Transmission System Operators (TSO) to clear real-time energy markets while ensuring reliable operations of power grids. In a context of growing operational uncertainty, due to increased penetration of renewable generators and distributed energy resources, operators must continuously monitor risk in real-time, i.e., they must quickly assess the system's behavior under various changes in load and renewable production. Unfortunately, systematically solving an optimization problem for each such scenario is not practical given the tight constraints of real-time operations. To overcome this limitation, this paper proposes to learn an optimization proxy for SCED, i.e., a Machine Learning (ML) model that can predict an optimal solution for SCED in milliseconds. Motivated by a principled analysis of the market-clearing optimizations of MISO, the paper proposes a novel ML pipeline that addresses the main challenges of learning SCED solutions, i.e., the variability in load, renewable output and production costs, as well as the combinatorial structure of commitment decisions. A novel Classification-Then-Regression architecture is also proposed, to further capture the behavior of SCED solutions. Numerical experiments are reported on the French transmission system, and demonstrate the approach's ability to produce, within a time frame that is compatible with real-time operations, accurate optimization proxies that produce relative errors below $0.6\%$.      
### 49.Bilingual Speech Recognition by Estimating Speaker Geometry from Video Data  [ :arrow_down: ](https://arxiv.org/pdf/2112.13463.pdf)
>  Speech recognition is very challenging in student learning environments that are characterized by significant cross-talk and background noise. To address this problem, we present a bilingual speech recognition system that uses an interactive video analysis system to estimate the 3D speaker geometry for realistic audio simulations. We demonstrate the use of our system in generating a complex audio dataset that contains significant cross-talk and background noise that approximate real-life classroom recordings. We then test our proposed system with real-life recordings. <br>In terms of the distance of the speakers from the microphone, our interactive video analysis system obtained a better average error rate of 10.83% compared to 33.12% for a baseline approach. Our proposed system gave an accuracy of 27.92% that is 1.5% better than Google Speech-to-text on the same dataset. In terms of 9 important keywords, our approach gave an average sensitivity of 38% compared to 24% for Google Speech-to-text, while both methods maintained high average specificity of 90% and 92%. <br>On average, sensitivity improved from 24% to 38% for our proposed approach. On the other hand, specificity remained high for both methods (90% to 92%).      
### 50.Retrieving Effective Acoustic Impedance and Refractive Index for Size Mismatch Samples  [ :arrow_down: ](https://arxiv.org/pdf/2112.13453.pdf)
>  In this paper, we have presented an analytical solution to extract the effective properties of acoustic metamaterials from the measured complex transmission and reflection coefficients when the metamaterial and impedance tube have different sizes. We have first modeled this problem as a bilayer metamaterial located inside a duct and treated the air gap as a separate domain. Then we have mathematically proved that the effective properties of acoustic metamaterial can be obtained by solving a set of eight linear equations when the dimensions are known. Finally, we have evaluated the proposed method with results from numerical simulations. It is shown that the proposed method can calculate the effective refractive index and impedance with an error of below 1\%. This method provides an efficient approach to analyzing the effective properties of acoustic metamaterials of various sizes.      
### 51.Acoustic scene classification using auditory datasets  [ :arrow_down: ](https://arxiv.org/pdf/2112.13450.pdf)
>  The approach used not only challenges some of the fundamental mathematical techniques used so far in early experiments of the same trend but also introduces new scopes and new horizons for interesting results. The physics governing spectrograms have been optimized in the project along with exploring how it handles the intense requirements of the problem at hand. Major contributions and developments brought under the light, through this project involve using better mathematical techniques and problem-specific machine learning methods. Improvised data analysis and data augmentation for audio datasets like frequency masking and random frequency-time stretching are used in the project and hence are explained in this paper. In the used methodology, the audio transforms principle were also tried and explored, and indeed the insights gained were used constructively in the later stages of the project. Using a deep learning principle is surely one of them. Also, in this paper, the potential scopes and upcoming research openings in both short and long term tunnel of time has been presented. Although much of the results gained are domain-specific as of now, they are surely potent enough to produce novel solutions in various different domains of diverse backgrounds.      
### 52.A CNN-BiLSTM Model with Attention Mechanism for Earthquake Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2112.13444.pdf)
>  Earthquakes, as natural phenomena, have continuously caused damage and loss of human life historically. Earthquake prediction is an essential aspect of any society's plans and can increase public preparedness and reduce damage to a great extent. Nevertheless, due to the stochastic character of earthquakes and the challenge of achieving an efficient and dependable model for earthquake prediction, efforts have been insufficient thus far, and new methods are required to solve this problem. Aware of these issues, this paper proposes a novel prediction method based on attention mechanism (AM), convolution neural network (CNN), and bi-directional long short-term memory (BiLSTM) models, which can predict the number and maximum magnitude of earthquakes in each area of mainland China-based on the earthquake catalog of the region. This model takes advantage of LSTM and CNN with an attention mechanism to better focus on effective earthquake characteristics and produce more accurate predictions. Firstly, the zero-order hold technique is applied as pre-processing on earthquake data, making the model's input data more proper. Secondly, to effectively use spatial information and reduce dimensions of input data, the CNN is used to capture the spatial dependencies between earthquake data. Thirdly, the Bi-LSTM layer is employed to capture the temporal dependencies. Fourthly, the AM layer is introduced to highlight its important features to achieve better prediction performance. The results show that the proposed method has better performance and generalize ability than other prediction methods.      
### 53.Energy-Efficient Trajectory Design for UAV-Aided Maritime Data Collection in Wind  [ :arrow_down: ](https://arxiv.org/pdf/2112.13396.pdf)
>  Unmanned aerial vehicles (UAVs), especially fixed-wing ones that withstand strong winds, have great potential for oceanic exploration and research. This paper studies a UAV-aided maritime data collection system with a fixed-wing UAV dispatched to collect data from marine buoys. We aim to minimize the UAV's energy consumption in completing the task by jointly optimizing the communication time scheduling among the buoys and the UAV's flight trajectory subject to wind effect. The conventional successive convex approximation (SCA) method can provide efficient sub-optimal solutions for collecting small/moderate data volume, whereas the solution heavily relies on trajectory initialization and has not explicitly considered wind effect, while the computational/trajectory complexity both become prohibitive for the task with large data volume. To this end, we propose a new cyclical trajectory design framework with tailored initialization algorithm that can handle arbitrary data volume efficiently, as well as a hybrid offline-online (HO2) design that leverages convex stochastic programming (CSP) offline based on wind statistics, and refines the solution by adapting online to real-time wind velocity. Numerical results show that our optimized trajectory can better adapt to various setups with different target data volume and buoys' topology as well as various wind speed/direction/variance compared with benchmark schemes.      
### 54.Stop Line Aided Cooperative Positioning of Connected Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2112.13369.pdf)
>  This paper develops a stop line aided cooperative positioning framework for connected vehicles, which creatively utilizes the location of the stop-line to achieve the positioning enhancement for a vehicular ad-hoc network (VANET) in intersection scenarios via Vehicle-to-Vehicle (V2V) communication. Firstly, a self-positioning correction scheme for the first stopped vehicle is presented, which applied the stop line information as benchmarks to correct the GNSS/INS positioning results. Then, the local observations of each vehicle are fused with the position estimates of other vehicles and the inter-vehicle distance measurements by using an extended Kalman filter (EKF). In this way, the benefits of the first stopped vehicle are extended to the whole VANET. Such a cooperative inertial navigation (CIN) framework can greatly improve the positioning performance of the VANET. Finally, experiments in Beijing show the effectiveness of the proposed stop line aided cooperative positioning framework.      
### 55.Novel Hybrid DNN Approaches for Speaker Verification in Emotional and Stressful Talking Environments  [ :arrow_down: ](https://arxiv.org/pdf/2112.13353.pdf)
>  In this work, we conducted an empirical comparative study of the performance of text-independent speaker verification in emotional and stressful environments. This work combined deep models with shallow architecture, which resulted in novel hybrid classifiers. Four distinct hybrid models were utilized: deep neural network-hidden Markov model (DNN-HMM), deep neural network-Gaussian mixture model (DNN-GMM), Gaussian mixture model-deep neural network (GMM-DNN), and hidden Markov model-deep neural network (HMM-DNN). All models were based on novel implemented architecture. The comparative study used three distinct speech datasets: a private Arabic dataset and two public English databases, namely, Speech Under Simulated and Actual Stress (SUSAS) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). The test results of the aforementioned hybrid models demonstrated that the proposed HMM-DNN leveraged the verification performance in emotional and stressful environments. Results also showed that HMM-DNN outperformed all other hybrid models in terms of equal error rate (EER) and area under the curve (AUC) evaluation metrics. The average resulting verification system based on the three datasets yielded EERs of 7.19%, 16.85%, 11.51%, and 11.90% based on HMM-DNN, DNN-HMM, DNN-GMM, and GMM-DNN, respectively. Furthermore, we found that the DNN-GMM model demonstrated the least computational complexity compared to all other hybrid models in both talking environments. Conversely, the HMM-DNN model required the greatest amount of training time. Findings also demonstrated that EER and AUC values depended on the database when comparing average emotional and stressful performances.      
### 56.Novel Dual-Channel Long Short-Term Memory Compressed Capsule Networks for Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.13350.pdf)
>  Recent analysis on speech emotion recognition has made considerable advances with the use of MFCCs spectrogram features and the implementation of neural network approaches such as convolutional neural networks (CNNs). Capsule networks (CapsNet) have gained gratitude as alternatives to CNNs with their larger capacities for hierarchical representation. To address these issues, this research introduces a text-independent and speaker-independent SER novel architecture, where a dual-channel long short-term memory compressed-CapsNet (DC-LSTM COMP-CapsNet) algorithm is proposed based on the structural features of CapsNet. Our proposed novel classifier can ensure the energy efficiency of the model and adequate compression method in speech emotion recognition, which is not delivered through the original structure of a CapsNet. Moreover, the grid search approach is used to attain optimal solutions. Results witnessed an improved performance and reduction in the training and testing running time. The speech datasets used to evaluate our algorithm are: Arabic Emirati-accented corpus, English speech under simulated and actual stress corpus, English Ryerson audio-visual database of emotional speech and song corpus, and crowd-sourced emotional multimodal actors dataset. This work reveals that the optimum feature extraction method compared to other known methods is MFCCs delta-delta. Using the four datasets and the MFCCs delta-delta, DC-LSTM COMP-CapsNet surpasses all the state-of-the-art systems, classical classifiers, CNN, and the original CapsNet. Using the Arabic Emirati-accented corpus, our results demonstrate that the proposed work yields average emotion recognition accuracy of 89.3% compared to 84.7%, 82.2%, 69.8%, 69.2%, 53.8%, 42.6%, and 31.9% based on CapsNet, CNN, support vector machine, multi-layer perceptron, k-nearest neighbor, radial basis function, and naive Bayes, respectively.      
### 57.Itô-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives  [ :arrow_down: ](https://arxiv.org/pdf/2112.13339.pdf)
>  Denoising Diffusion Probabilistic Models (DDPMs) have been attracting attention recently as a new challenger to popular deep neural generative models including GAN, VAE, etc. However, DDPMs have a disadvantage that they often require a huge number of refinement steps during the synthesis. To address this problem, this paper proposes a new DDPM sampler based on a second-order numerical scheme for stochastic differential equations (SDEs), while the conventional sampler is based on a first-order numerical scheme. In general, it is not easy to compute the derivatives that are required in higher-order numerical schemes. However, in the case of DDPM, this difficulty is alleviated by the trick which the authors call "ideal derivative substitution". The newly derived higher-order sampler was applied to both image and speech generation tasks, and it is experimentally observed that the proposed sampler could synthesize plausible images and audio signals in relatively smaller number of refinement steps.      
### 58.Imaging through scattering media via spatial-temporal encoded pattern illumination  [ :arrow_down: ](https://arxiv.org/pdf/2112.13303.pdf)
>  Optical imaging through scattering media is a long-standing challenge. Although many approaches have been developed to focus light or image objects through scattering media, they are either invasive, restricted to stationary or slowly-moving media, or require high-resolution cameras and complex algorithms to retrieve the images. Here we introduce a computational imaging technique that can overcome these restrictions by exploiting spatial-temporal encoded patterns (STEP). We present non-invasive imaging through scattering media with a single-pixel photodetector. We show that the method is insensitive to the motions of media. We further demonstrate that our image reconstruction algorithm is much more efficient than correlation-based algorithms for single-pixel imaging, which may allow fast imaging in currently unreachable scenarios.      
### 59.Learning Linear Complementarity Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.13284.pdf)
>  This paper investigates the learning, or system identification, of a class of piecewise-affine dynamical systems known as linear complementarity systems (LCSs). We propose a violation-based loss which enables efficient learning of the LCS parameterization, without prior knowledge of the hybrid mode boundaries, using gradient-based methods. The proposed violation-based loss incorporates both dynamics prediction loss and a novel complementarity - violation loss. We show several properties attained by this loss formulation, including its differentiability, the efficient computation of first- and second-order derivatives, and its relationship to the traditional prediction loss, which strictly enforces complementarity. We apply this violation-based loss formulation to learn LCSs with tens of thousands of (potentially stiff) hybrid modes. The results demonstrate a state-of-the-art ability to identify piecewise-affine dynamics, outperforming methods which must differentiate through non-smooth linear complementarity problems.      
### 60.Fitting nonlinear models to continuous oxygen data with oscillatory signal variations via a loss based on DynamicTime Warping  [ :arrow_down: ](https://arxiv.org/pdf/2112.13283.pdf)
>  High throughput experimental systems play an important role in bioprocess development, as they provide an efficient way of analysing different experimental conditions and perform strain discrimination in previous phases to the industrial scale production. In the millilitre scale, these systems are combinations of parallel mini-bioreactors, liquid handling robots and automated workflows for data handling and model based operation. For successfully monitoring cultivation conditions and improving the overall process quality by model-based approaches, a proper model identification is crucial. However, the quality and amount of measurements makes this task challenging considering the complexity of the bio-processes. TheDissolved Oxygen Tension is often the only measurement which is available online, and therefore, a good understanding of the errors in this signal is important for performing a robust estimation.Some of the expected errors will provoke uncertainties in the time-domain of the measurement, and in those cases, the common Weighted Least Squares estimation procedure can fail providing good results. Moreover, these errors will have even a larger effect in the fed-batch phase where bolus feeding is applied, as this generates fast dynamic responses in the signal. In the present work, an insilico study of the performance of Weighted Least Squares estimator is analysed when the expected time-uncertainties are present in the oxygen signal. As an alternative, a loss based on the Dynamic Time Warping measure is proposed. The results show how this latter procedure outperforms the former reconstructing the oxygen signal, and in addition, returns less biased parameter estimates.      
### 61.Interference Nulling Using Reconfigurable Intelligent Surface  [ :arrow_down: ](https://arxiv.org/pdf/2112.13261.pdf)
>  This paper investigates the interference nulling capability of reconfigurable intelligent surface (RIS) in a multiuser environment where multiple single-antenna transceivers communicate simultaneously in a shared spectrum. From a theoretical perspective, we show that when the channels between the RIS and the transceivers have line-of-sight and the direct paths are blocked, it is possible to adjust the phases of the RIS elements to null out all the interference completely and to achieve the maximum $K$ degrees-of-freedom (DoF) in the overall $K$-user interference channel, provided that the number of RIS elements exceeds some finite value that depends on $K$. Algorithmically, for any fixed channel realization we formulate the interference nulling problem as a feasibility problem, and propose an alternating projection algorithm to efficiently solve the resulting nonconvex problem with local convergence guarantee. Numerical results show that the proposed alternating projection algorithm can null all the interference if the number of RIS elements is only slightly larger than a threshold of $2K(K-1)$. For the practical sum-rate maximization objective, this paper proposes to use the zero-forcing solution obtained from alternating projection as an initial point for subsequent Riemannian conjugate gradient optimization, and shows that it has a significant performance advantage over random initializations. For the objective of maximizing the minimum rate, this paper proposes a subgradient projection method which is capable of achieving good performance at low complexity.      
### 62.A Fast Row-Stochastic Decentralized Optimization Method Over Directed Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2112.13257.pdf)
>  In this paper, we introduce a fast row-stochastic decentralized algorithm, referred to as FRSD, to solve consensus optimization problems over directed communication graphs. The proposed algorithm only utilizes row-stochastic weights, leading to certain practical advantages over those requiring column-stochastic weights. Thus, in contrast to the majority of existing methods, FRSD does not employ a gradient tracking technique, rather it uses a novel momentum term. Under the assumption that each node-specific function is smooth and strongly convex, we show that FRSD admits constant step-size and momentum parameters such that the iterate sequence converges linearly to the optimal consensus solution. In the numerical tests, we compare FRSD with other state-of-the-art methods, which use row-stochastic and/or column-stochastic weights.      
### 63.Cyberattack Detection in Large-Scale Smart Grids using Chebyshev Graph Convolutional Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.13166.pdf)
>  As a highly complex and integrated cyber-physical system, modern power grids are exposed to cyberattacks. False data injection attacks (FDIAs), specifically, represent a major class of cyber threats to smart grids by targeting the measurement data's integrity. Although various solutions have been proposed to detect those cyberattacks, the vast majority of the works have ignored the inherent graph structure of the power grid measurements and validated their detectors only for small test systems with less than a few hundred buses. To better exploit the spatial correlations of smart grid measurements, this paper proposes a deep learning model for cyberattack detection in large-scale AC power grids using Chebyshev Graph Convolutional Networks (CGCN). By reducing the complexity of spectral graph filters and making them localized, CGCN provides a fast and efficient convolution operation to model the graph structural smart grid data. We numerically verify that the proposed CGCN based detector surpasses the state-of-the-art model by 7.86 in detection rate and 9.67 in false alarm rate for a large-scale power grid with 2848 buses. It is notable that the proposed approach detects cyberattacks under 4 milliseconds for a 2848-bus system, which makes it a good candidate for real-time detection of cyberattacks in large systems.      
### 64.Enabling Real-time On-chip Audio Super Resolution for Bone Conduction Microphones  [ :arrow_down: ](https://arxiv.org/pdf/2112.13156.pdf)
>  Voice communication using the air conduction microphone in noisy environments suffers from the degradation of speech audibility. Bone conduction microphones (BCM) are robust against ambient noises but suffer from limited effective bandwidth due to their sensing mechanism. Although existing audio super resolution algorithms can recover the high frequency loss to achieve high-fidelity audio, they require considerably more computational resources than available in low-power hearable devices. This paper proposes the first-ever real-time on-chip speech audio super resolution system for BCM. To accomplish this, we built and compared a series of lightweight audio super resolution deep learning models. Among all these models, ATS-UNet is the most cost-efficient because the proposed novel Audio Temporal Shift Module (ATSM) reduces the network's dimensionality while maintaining sufficient temporal features from speech audios. Then we quantized and deployed the ATS-UNet to low-end ARM micro-controller units for real-time embedded prototypes. Evaluation results show that our system achieved real-time inference speed on Cortex-M7 and higher quality than the baseline audio super resolution method. Finally, we conducted a user study with ten experts and ten amateur listeners to evaluate our method's effectiveness to human ears. Both groups perceived a significantly higher speech quality with our method when compared to the solutions with the original BCM or air conduction microphone with cutting-edge noise reduction algorithms.      
### 65.Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures  [ :arrow_down: ](https://arxiv.org/pdf/2112.13150.pdf)
>  The manuscript describes fast and scalable architectures and associated algorithms for computing convolutions and cross-correlations. The basic idea is to map 2D convolutions and cross-correlations to a collection of 1D convolutions and cross-correlations in the transform domain. This is accomplished through the use of the Discrete Periodic Radon Transform (DPRT) for general kernels and the use of SVD-LU decompositions for low-rank kernels. The approach uses scalable architectures that can be fitted into modern FPGA and Zynq-SOC devices. Based on different types of available resources, for $P\times P$ blocks, 2D convolutions and cross-correlations can be computed in just $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a trade-off between performance and required numbers and types of resources. We provide implementations of the proposed architectures using modern programmable devices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required resources, we show that the proposed approaches significantly outperform current methods.      
### 66.Fast and Scalable Computation of the Forward and Inverse Discrete Periodic Radon Transform  [ :arrow_down: ](https://arxiv.org/pdf/2112.13149.pdf)
>  The Discrete Periodic Radon Transform (DPRT) has been extensively used in applications that involve image reconstructions from projections. This manuscript introduces a fast and scalable approach for computing the forward and inverse DPRT that is based on the use of: (i) a parallel array of fixed-point adder trees, (ii) circular shift registers to remove the need for accessing external memory components when selecting the input data for the adder trees, (iii) an image block-based approach to DPRT computation that can fit the proposed architecture to available resources, and (iv) fast transpositions that are computed in one or a few clock cycles that do not depend on the size of the input image. As a result, for an $N\times N$ image ($N$ prime), the proposed approach can compute up to $N^{2}$ additions per clock cycle. Compared to previous approaches, the scalable approach provides the fastest known implementations for different amounts of computational resources. For example, for a $251\times 251$ image, for approximately $25\%$ fewer flip-flops than required for a systolic implementation, we have that the scalable DPRT is computed 36 times faster. For the fastest case, we introduce optimized architectures that can compute the DPRT and its inverse in just $2N+\left\lceil \log_{2}N\right\rceil+1$ and $2N+3\left\lceil \log_{2}N\right\rceil+B+2$ cycles respectively, where $B$ is the number of bits used to represent each input pixel. On the other hand, the scalable DPRT approach requires more 1-bit additions than for the systolic implementation and provides a trade-off between speed and additional 1-bit additions. All of the proposed DPRT architectures were implemented in VHDL and validated using an FPGA implementation.      
### 67.Invertible Network for Unpaired Low-light Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2112.13107.pdf)
>  Existing unpaired low-light image enhancement approaches prefer to employ the two-way GAN framework, in which two CNN generators are deployed for enhancement and degradation separately. However, such data-driven models ignore the inherent characteristics of transformation between the low and normal light images, leading to unstable training and artifacts. Here, we propose to leverage the invertible network to enhance low-light image in forward process and degrade the normal-light one inversely with unpaired learning. The generated and real images are then fed into discriminators for adversarial learning. In addition to the adversarial loss, we design various loss functions to ensure the stability of training and preserve more image details. Particularly, a reversibility loss is introduced to alleviate the over-exposure problem. Moreover, we present a progressive self-guided enhancement process for low-light images and achieve favorable performance against the SOTAs.      
### 68.Noninvasive Fetal Electrocardiography: Models, Technologies and Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2112.13021.pdf)
>  The fetal electrocardiogram (fECG) was first recorded from the maternal abdominal surface in the early 1900s. During the past fifty years, the most advanced electronics technologies and signal processing algorithms have been used to convert noninvasive fetal electrocardiography into a reliable technology for fetal cardiac monitoring. In this chapter, the major signal processing techniques, which have been developed for the modeling, extraction and analysis of the fECG from noninvasive maternal abdominal recordings are reviewed and compared with one another in detail. The major topics of the chapter include: 1) the electrophysiology of the fECG from the signal processing viewpoint, 2) the mathematical model of the maternal volume conduction media and the waveform models of the fECG acquired from body surface leads, 3) the signal acquisition requirements, 4) model-based techniques for fECG noise and interference cancellation, including adaptive filters and semi-blind source separation techniques, and 5) recent algorithmic advances for fetal motion tracking and online fECG extraction from few number of channels.      
### 69.Joint Activity Detection and Channel Estimation in Cell-Free Massive MIMO Networks with Massive Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2112.13013.pdf)
>  Cell-free massive MIMO is one of the key technologies for future wireless communications, in which users are simultaneously and jointly served by all access points (APs). In this paper, we investigate the minimum mean square error (MMSE) estimation of effective channel coefficients in cell-free massive MIMO systems with massive connectivity. To facilitate the theoretical analysis, only single measurement vector (SMV) based MMSE estimation is considered in this paper, i.e., the MMSE estimation is performed based on the received pilot signals at each AP separately. Inspired by the decoupling principle of replica symmetric postulated MMSE estimation of sparse signal vectors with independent and identically distributed (i.i.d.) non-zero components, we develop the corresponding decoupling principle for the SMV based MMSE estimation of sparse signal vectors with independent and non-identically distributed (i.n.i.d.) non-zero components, which plays a key role in the theoretical analysis of SMV based MMSE estimation of the effective channel coefficients in cell-free massive MIMO systems with massive connectivity. Subsequently, based on the obtained decoupling principle of MMSE estimation, likelihood ratio test and the optimal fusion rule, we perform user activity detection based on the received pilot signals at only one AP, or cooperation among the entire set of APs for centralized or distributed detection. Via theoretical analysis, we show that the error probabilities of both centralized and distributed detection tend to zero when the number of APs tends to infinity while the asymptotic ratio between the number of users and pilots is kept constant. We also investigate the asymptotic behavior of oracle estimation in cell-free massive MIMO systems with massive connectivity via random matrix theory.      
### 70.US-GAN: On the importance of Ultimate Skip Connection for Facial Expression Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2112.13002.pdf)
>  Recent studies have shown impressive results in multi-domain image-to-image translation for facial expression synthesis. While effective, these methods require a large number of labelled samples for plausible results. Their performance significantly degrades when we train them on smaller datasets. To address this limitation, in this work, we present US-GAN, a smaller and effective method for synthesizing plausible expressions by employing notably smaller datasets. The proposed method comprises of encoding layers, single residual block, decoding layers and an ultimate skip connection that links the input image to an output image. It has three times lesser parameters as compared to state-of-the-art facial expression synthesis methods. Experimental results demonstrate the quantitative and qualitative effectiveness of our proposed method. In addition, we also show that an ultimate skip connection is sufficient for recovering rich facial and overall color details of the input face image that a larger state-of-the-art model fails to recover.      
### 71.Integrating Physics-Based Modeling with Machine Learning for Lithium-Ion Batteries  [ :arrow_down: ](https://arxiv.org/pdf/2112.12979.pdf)
>  Mathematical modeling of lithium-ion batteries (LiBs) is a primary challenge in advanced battery management. This paper proposes two new frameworks to integrate a physics-based model with machine learning to achieve high-precision modeling for LiBs. The frameworks are characterized by informing the machine learning model of the state information of the physical model, enabling a deep integration between physics and machine learning. Based on the frameworks, a series of hybrid models are constructed, through combining an electrochemical model and an equivalent circuit model, respectively, with a feedforward neural network. The hybrid models are relatively parsimonious in structure and can provide considerable predictive accuracy under a broad range of C-rates, as shown by extensive simulations and experiments. The study further expands to conduct aging-aware hybrid modeling, leading to the design of a hybrid model conscious of the state-of-health to make prediction. Experiments show that the model has high predictive accuracy throughout a LiB's cycle life.      
### 72.RIS-Assisted Multihop FSO/RF Hybrid System for Vehicular Communications over Generalized Fading  [ :arrow_down: ](https://arxiv.org/pdf/2112.12944.pdf)
>  Reconfigurable intelligent surface (RIS) is a promising technology to avoid signal blockage by creating line-of-sight (LOS) connectivity for free-space optical (FSO) and radio frequency (RF) wireless systems. There is limited research on the use of multiple RIS between a source and destination for wireless communications. This paper analyzes the performance of a RIS-assisted multi-hop transmission for vehicular communications by employing multiple RIS to enable LOS communication and reliable connectivity for a hybrid FSO and RF system. We develop an analytical framework to derive statistical results of the signal-to-noise ratio (SNR) of a multi-RIS communication system over general fading models. We use decode-and-forward (DF) and fixed-gain (FG) relaying protocols to mix multi-RIS transmissions over RF and FSO technologies, and derive probability density and distribution functions for both the relaying schemes by considering independent and non-identical double generalized gamma (dGG) distribution models for vehicular RF transmissions and atmospheric turbulence for FSO system combined with zero-boresight pointing errors. We analyze the performance of a moving vehicle connected to one of the RIS modules by deriving exact analytical expressions of the outage probability, average bit-error rate (BER), and ergodic capacity in terms of Fox's H-function. We present asymptotic analysis and diversity order of the outage probability in the high SNR regime to provide a better insight into the system performance. We use computer simulations to demonstrate the effect of multiple RIS modules, fading parameters, and pointing errors on the RIS-aided multi-hop transmissions for the considered vehicular communication system.      
### 73.Backhaul-Aware Drone Base Station Placement and Resource Management for FSO based Drone Assisted Mobile Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.12883.pdf)
>  In drone assisted mobile networks, drones mounted small cell base stations (DBSs) are responsively and flexibly deployed over any Places of Interest (PoI), such as sporadic hotspots and disaster-struck areas, where the existing mobile network infrastructure is unable to provide wireless coverage. Here, a DBS is a relay node to relay traffic between a nearby macro base station (MBS) and the users. In addition, Free-space optics (FSO) is applied as the backhauling solution to significantly increase the capacity of the backhaul link between an MBS and a DBS in a drone assisted mobile network. Most of the existing DBS placement solutions assume the FSO based backhaul link provides sufficient link capacity, which may not be true, especially when a DBS is placed far away from an MBS (e.g., &gt; 10 km in disaster-struck areas) or in a bad weather condition. In this paper, we formulate a problem to jointly optimize bandwidth allocation and DBS placement by considering the FSO based backhaul link capacity constraint. A Backhaul awaRe bandwidth allOcAtion and DBS placement (BROAD) algorithm is designed to efficiently solve the problem, and the performance of the algorithm is demonstrated via extensive simulations.      
### 74.Faster Deep Ensemble Averaging for Quantification of DNA Damage from Comet Assay Images With Uncertainty Estimates  [ :arrow_down: ](https://arxiv.org/pdf/2112.12839.pdf)
>  Several neurodegenerative diseases involve the accumulation of cellular DNA damage. Comet assays are a popular way of estimating the extent of DNA damage. Current literature on the use of deep learning to quantify DNA damage presents an empirical approach to hyper-parameter optimization and does not include uncertainty estimates. Deep ensemble averaging is a standard approach to estimating uncertainty but it requires several iterations of network training, which makes it time-consuming. Here we present an approach to quantify the extent of DNA damage that combines deep learning with a rigorous and comprehensive method to optimize the hyper-parameters with the help of statistical tests. We also use an architecture that allows for a faster computation of deep ensemble averaging and performs statistical tests applicable to networks using transfer learning. We applied our approach to a comet assay dataset with more than 1300 images and achieved an $R^2$ of 0.84, where the output included the confidence interval for each prediction. The proposed architecture is an improvement over the current approaches since it speeds up the uncertainty estimation by 30X while being statistically more rigorous.      
### 75.Joint Detection and Localization of Stealth False Data Injection Attacks in Smart Grids using Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2104.11846.pdf)
>  False data injection attacks (FDIA) are a main category of cyber-attacks threatening the security of power systems. Contrary to the detection of these attacks, less attention has been paid to identifying the attacked units of the grid. To this end, this work jointly studies detecting and localizing the stealth FDIA in power grids. Exploiting the inherent graph topology of power systems as well as the spatial correlations of measurement data, this paper proposes an approach based on the graph neural network (GNN) to identify the presence and location of the FDIA. The proposed approach leverages the auto-regressive moving average (ARMA) type graph filters (GFs) which can better adapt to sharp changes in the spectral domain due to their rational type filter composition compared to the polynomial type GFs such as Chebyshev. To the best of our knowledge, this is the first work based on GNN that automatically detects and localizes FDIA in power systems. Extensive simulations and visualizations show that the proposed approach outperforms the available methods in both detection and localization of FDIA for different IEEE test systems. Thus, the targeted areas can be identified and preventive actions can be taken before the attack impacts the grid.      
