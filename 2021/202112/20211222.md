# ArXiv eess --Wed, 22 Dec 2021
### 1.Covert Communications via Adversarial Machine Learning and Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2112.11414.pdf)
>  By moving from massive antennas to antenna surfaces for software-defined wireless systems, the reconfigurable intelligent surfaces (RISs) rely on arrays of unit cells to control the scattering and reflection profiles of signals, mitigating the propagation loss and multipath attenuation, and thereby improving the coverage and spectral efficiency. In this paper, covert communication is considered in the presence of the RIS. While there is an ongoing transmission boosted by the RIS, both the intended receiver and an eavesdropper individually try to detect this transmission using their own deep neural network (DNN) classifiers. The RIS interaction vector is designed by balancing two (potentially conflicting) objectives of focusing the transmitted signal to the receiver and keeping the transmitted signal away from the eavesdropper. To boost covert communications, adversarial perturbations are added to signals at the transmitter to fool the eavesdropper's classifier while keeping the effect on the receiver low. Results from different network topologies show that adversarial perturbation and RIS interaction vector can be jointly designed to effectively increase the signal detection accuracy at the receiver while reducing the detection accuracy at the eavesdropper to enable covert communications.      
### 2.Waveform-Defined Privacy: A Signal Solution to Protect Wireless Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2112.11409.pdf)
>  Wireless signals are commonly used for communications. Emerging applications are giving new functions to wireless signals, in which wireless sensing is the most attractive one. Channel state information (CSI) is not only the parameter for channel equalization in communications but also the indicator for wireless sensing. However, due to the broadcast nature of wireless signals, eavesdroppers can easily capture legitimate user signals and violate user privacy by measuring CSI. Moreover, the advancement of hardware simplifies illegal eavesdropping since smart devices can track over-the-air signals through walls. Therefore, this work considers a waveform-defined privacy (WDP) solution that can hide CSI phase information and therefore protect user privacy. Besides, the proposed waveform solution achieves better performance due to the use of a unique modulation mechanism. Additionally, by tuning a waveform parameter, the waveform can also enhance communication security.      
### 3.A novel approach for the automated segmentation and volume quantification of cardiac fats on computed tomography  [ :arrow_down: ](https://arxiv.org/pdf/2112.11381.pdf)
>  The deposits of fat on the surroundings of the heart are correlated to several health risk factors such as atherosclerosis, carotid stiffness, coronary artery calcification, atrial fibrillation and many others. These deposits vary unrelated to obesity, which reinforces its direct segmentation for further quantification. However, manual segmentation of these fats has not been widely deployed in clinical practice due to the required human workload and consequential high cost of physicians and technicians. In this work, we propose a unified method for an autonomous segmentation and quantification of two types of cardiac fats. The segmented fats are termed epicardial and mediastinal, and stand apart from each other by the pericardium. Much effort was devoted to achieve minimal user intervention. The proposed methodology mainly comprises registration and classification algorithms to perform the desired segmentation. We compare the performance of several classification algorithms on this task, including neural networks, probabilistic models and decision tree algorithms. Experimental results of the proposed methodology have shown that the mean accuracy regarding both epicardial and mediastinal fats is 98.5% (99.5% if the features are normalized), with a mean true positive rate of 98.0%. In average, the Dice similarity index was equal to 97.6%.      
### 4.Data-Driven Outage Restoration Time Prediction via Transfer Learning with Cluster Ensembles  [ :arrow_down: ](https://arxiv.org/pdf/2112.11374.pdf)
>  This paper develops a data-driven approach to accurately predict the restoration time of outages under different scales and factors. To achieve the goal, the proposed method consists of three stages. First, given the unprecedented amount of data collected by utilities, a sparse dictionary-based ensemble spectral clustering (SDESC) method is proposed to decompose historical outage datasets, which enjoys good computational efficiency and scalability. Specifically, each outage sample is represented by a linear combination of a small number of selected dictionary samples using a density-based method. Then, the dictionary-based representation is utilized to perform the spectral analysis to group the data samples with similar features into the same subsets. In the second stage, a knowledge-transfer-added restoration time prediction model is trained for each subset by combining weather information and outage-related features. The transfer learning technology is introduced with the aim of dealing with the underestimation problem caused by data imbalance in different subsets, thus improving the model performance. Furthermore, to connect unseen outages with the learned outage subsets, a t-distributed stochastic neighbor embedding-based strategy is applied. The proposed method fully builds on and is also tested on a large real-world outage dataset from a utility provider with a time span of six consecutive years. The numerical results validate that our method has high prediction accuracy while showing good stability against real-world data limitations.      
### 5.Waveform-Defined Security: A Low-Cost Framework for Secure Communications  [ :arrow_down: ](https://arxiv.org/pdf/2112.11350.pdf)
>  Communication security could be enhanced at physical layer but at the cost of complex algorithms and redundant hardware, which would render traditional physical layer security (PLS) techniques unsuitable for use with resource-constrained communication systems. This work investigates a waveform-defined security (WDS) framework, which differs fundamentally from traditional PLS techniques used in today's systems. The framework is not dependent on channel conditions such as signal power advantage and channel state information (CSI). Therefore, the framework is more reliable than channel dependent beamforming and artificial noise (AN) techniques. In addition, the framework is more than just increasing the cost of eavesdropping. By intentionally tuning waveform patterns to weaken signal feature diversity and enhance feature similarity, eavesdroppers will not be able to identify correctly signal formats. The wrong classification of signal formats would result in subsequent detection errors even when an eavesdropper uses brute-force detection techniques. To get a robust WDS framework, three impact factors, namely training data feature, oversampling factor and bandwidth compression factor (BCF) offset, are investigated. An optimal WDS waveform pattern is obtained at the end after a joint study of the three factors. To ensure a valid eavesdropping model, artificial intelligence (AI) dependent signal classifiers are designed followed by optimal performance achievable signal detectors. To show the compatibility in available communication systems, the WDS framework is successfully integrated in IEEE 802.11a with nearly no adding computational complexity. Finally, a low-cost software-defined radio (SDR) experiment is designed to verify the feasibility of the WDS framework in resource-constrained communications.      
### 6.Dynamic Edge Computing empowered by Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2112.11269.pdf)
>  In this paper, we propose a novel algorithm for energy-efficient, low-latency dynamic mobile edge computing (MEC), in the context of beyond 5G networks endowed with Reconfigurable Intelligent Surfaces (RISs). In our setting, new computing requests are continuously generated by a set of devices and are handled through a dynamic queueing system. Building on stochastic optimization tools, we devise a dynamic learning algorithm that jointly optimizes the allocation of radio resources (i.e., power, transmission rates, sleep mode and duty cycle), computation resources (i.e., CPU cycles), and RIS reflectivity parameters (i.e., phase shifts), while guaranteeing a target performance in terms of average end-to-end (E2E) delay. The proposed strategy is dynamic, since it performs a low-complexity optimization on a per-slot basis while dealing with time-varying radio channels and task arrivals, whose statistics are unknown. The presence and optimization of RISs helps boosting the performance of dynamic MEC, thanks to the capability to shape and adapt the wireless propagation environment. Numerical results assess the performance in terms of service delay, learning, and adaptation capabilities of the proposed strategy for RIS empowered MEC.      
### 7.Multiple Time Series Fusion Based on LSTM An Application to CAP A Phase Classification Using EEG  [ :arrow_down: ](https://arxiv.org/pdf/2112.11218.pdf)
>  Biomedical decision making involves multiple signal processing, either from different sensors or from different channels. In both cases, information fusion plays a significant role. A deep learning based electroencephalogram channels' feature level fusion is carried out in this work for the electroencephalogram cyclic alternating pattern A phase classification. Channel selection, fusion, and classification procedures were optimized by two optimization algorithms, namely, Genetic Algorithm and Particle Swarm Optimization. The developed methodologies were evaluated by fusing the information from multiple electroencephalogram channels for patients with nocturnal frontal lobe epilepsy and patients without any neurological disorder, which was significantly more challenging when compared to other state of the art works. Results showed that both optimization algorithms selected a comparable structure with similar feature level fusion, consisting of three electroencephalogram channels, which is in line with the CAP protocol to ensure multiple channels' arousals for CAP detection. Moreover, the two optimized models reached an area under the receiver operating characteristic curve of 0.82, with average accuracy ranging from 77% to 79%, a result which is in the upper range of the specialist agreement. The proposed approach is still in the upper range of the best state of the art works despite a difficult dataset, and has the advantage of providing a fully automatic analysis without requiring any manual procedure. Ultimately, the models revealed to be noise resistant and resilient to multiple channel loss.      
### 8.Discrete fully probabilistic design: a tool to design control policies from examples  [ :arrow_down: ](https://arxiv.org/pdf/2112.11210.pdf)
>  We present a discretized design that expounds an algorithm recently introduced in Gagliardi and Russo (2021) to synthesize control policies from examples for constrained, possibly stochastic and nonlinear, systems. The constraints do not need to be fulfilled in the possibly noisy example data, which in turn might be collected from a system that is different from the one under control. For this discretized design, we discuss a number of properties and give a design pipeline. The design, which we term as discrete fully probabilistic design, is benchmarked numerically on an example that involves controlling an inverted pendulum with actuation constraints starting from data collected from a physically different pendulum that does not satisfy the system-specific actuation constraints.      
### 9.High-Resolution Programmable Scattering for Wireless Coverage Enhancement: An Indoor Field Trial Campaign  [ :arrow_down: ](https://arxiv.org/pdf/2112.11194.pdf)
>  This paper presents a multi-bit reconfigurable intelligent surface with high-resolution beam steering capability in the azimuthal plane for deployment at sub-6 Gigahertz (GHz) band. Field trials in realistic indoor deployments have been carried out, with coverage enhancement performance ascertained for three common wireless communication scenarios. Namely, serving users in an open lobby with mixed line of sight and non-line of sight conditions, communication via a junction between long corridors, and a multi-floor scenario with propagation via windows. This work explores the potential for reconfigurable intelligent surface (RIS) deployment to mitigate non-line of sight effects in an indoor wireless communications. In a single transmitter, single receiver non-line of sight link, received power improvement of as much as 40 dB is shown to be achievable by suitable placement of an RIS, with an instantaneous bandwidth of at least 100 MHz possible over a 3 to 4.5 GHz range. In addition, the effects of phase resolution on the optimal power reception for the multi-bit RIS have been experimentally verified, with a 2.65 dB improvement compared to a 1-bit case.      
### 10.RC-Net: A Convolutional Neural Network for Retinal Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2112.11078.pdf)
>  Over recent years, increasingly complex approaches based on sophisticated convolutional neural network architectures have been slowly pushing performance on well-established benchmark datasets. In this paper, we take a step back to examine the real need for such complexity. We present RC-Net, a fully convolutional network, where the number of filters per layer is optimized to reduce feature overlapping and complexity. We also used skip connections to keep spatial information loss to a minimum by keeping the number of pooling operations in the network to a minimum. Two publicly available retinal vessel segmentation datasets were used in our experiments. In our experiments, RC-Net is quite competitive, outperforming alternatives vessels segmentation methods with two or even three orders of magnitude less trainable parameters.      
### 11.Leveraging Image Complexity in Macro-Level Neural Network Design for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2112.11065.pdf)
>  Recent progress in encoder-decoder neural network architecture design has led to significant performance improvements in a wide range of medical image segmentation tasks. However, state-of-the-art networks for a given task may be too computationally demanding to run on affordable hardware, and thus users often resort to practical workarounds by modifying various macro-level design aspects. Two common examples are downsampling of the input images and reducing the network depth to meet computer memory constraints. In this paper we investigate the effects of these changes on segmentation performance and show that image complexity can be used as a guideline in choosing what is best for a given dataset. We consider four statistical measures to quantify image complexity and evaluate their suitability on ten different public datasets. For the purpose of our experiments we also propose two new encoder-decoder architectures representing shallow and deep networks that are more memory efficient than currently popular networks. Our results suggest that median frequency is the best complexity measure in deciding about an acceptable input downsampling factor and network depth. For high-complexity datasets, a shallow network running on the original images may yield better segmentation results than a deep network running on downsampled images, whereas the opposite may be the case for low-complexity images.      
### 12.Is electricity storage systems in the Netherlands indispensable or doable? Testing electricity storage business models with exploratory agent-based modeling  [ :arrow_down: ](https://arxiv.org/pdf/2112.11035.pdf)
>  Electricity storage systems (ESS) are hailed by many scholars and practitioners as a key element of the future electricity systems and a key step toward the transition to renewables . Nonetheless, the global speed of ESS implementation is relatively slow, and among possible reasons is the lack of viable business models. We developed an agent-based model to simulate the behavior of ESS within the Dutch electricity market. We adopted an exploratory modeling analysis (EMA) approach to investigate the effects of two specific business models on the value of ESS from the perspective of both investors and the government under uncertainties in the ESS technical and economics characteristics, and uncertainties in market conditions and regulations. Our results show ESS is not profitable in most scenarios, and generally wholesale arbitrage business model leads to more profit than reserve capacity. In addition, ESS economic and technical characteristics play more important roles in the value of ESS than market conditions, and carbon pricing.      
### 13.RSSI prediction using Machine Learning models  [ :arrow_down: ](https://arxiv.org/pdf/2112.10957.pdf)
>  In this study, we present a method to predict the Received signal strength indication (RSSI) in an area of the base station. Traditional attenuated wave propagation models are often time consuming as well as computationally complex, depending on the unique factors of the medium. This study focuses on providing a solution to predict signal quality using coordinate values of many points in the considering area. We apply machine learning models such as linear regression, Support Vector Machine (SVM) or Decision tree model, to directly predict the RSSI of many points in the range of a base station without computing the complex parameters of the attenuated propagation model. The effectiveness of RSSI prediction was evaluated by mean square error (MSE) and mean absolute error (MAE). The stage of training and testing machine learning models in the research uses data that are the actual measurement results during the research process.      
### 14.Augmented Contrastive Self-Supervised Learning for Audio Invariant Representations  [ :arrow_down: ](https://arxiv.org/pdf/2112.10950.pdf)
>  Improving generalization is a major challenge in audio classification due to labeled data scarcity. Self-supervised learning (SSL) methods tackle this by leveraging unlabeled data to learn useful features for downstream classification tasks. In this work, we propose an augmented contrastive SSL framework to learn invariant representations from unlabeled data. Our method applies various perturbations to the unlabeled input data and utilizes contrastive learning to learn representations robust to such perturbations. Experimental results on the Audioset and DESED datasets show that our framework significantly outperforms state-of-the-art SSL and supervised learning methods on sound/event classification tasks.      
### 15.District Cooling System Control for Providing Operating Reserve based on Safe Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.10949.pdf)
>  Heating, ventilation, and air conditioning (HVAC) systems are well proved to be capable to provide operating reserve for power systems. As a type of large-capacity and energy-efficient HVAC system (up to 100 MW), district cooling system (DCS) is emerging in modern cities and has huge potential to be regulated as a flexible load. However, strategically controlling a DCS to provide flexibility is challenging, because one DCS services multiple buildings with complex thermal dynamics and uncertain cooling demands. Improper control may lead to significant thermal discomfort and even deteriorate the power system's operation security. To address the above issues, we propose a model-free control strategy based on the deep reinforcement learning (DRL) without the requirement of accurate system model and uncertainty distribution. To avoid damaging "trial &amp; error" actions that may violate the system's operation security during the training process, we further propose a safe layer combined to the DRL to guarantee the satisfaction of critical constraints, forming a safe-DRL scheme. Moreover, after providing operating reserve, DCS increases power and tries to recover all the buildings' temperature back to set values, which may probably cause an instantaneous peak-power rebound and bring a secondary impact on power systems. Therefore, we design a self-adaption reward function within the proposed safe-DRL scheme to constrain the peak-power effectively. Numerical studies based on a realistic DCS demonstrate the effectiveness of the proposed methods.      
### 16.HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.10775.pdf)
>  Multiple medical institutions collaboratively training a model using federated learning (FL) has become a promising solution for maximizing the potential of data-driven models, yet the non-independent and identically distributed (non-iid) data in medical images is still an outstanding challenge in real-world practice. The feature heterogeneity caused by diverse scanners or protocols introduces a drift in the learning process, in both local (client) and global (server) optimizations, which harms the convergence as well as model performance. Many previous works have attempted to address the non-iid issue by tackling the drift locally or globally, but how to jointly solve the two essentially coupled drifts is still unclear. In this work, we concentrate on handling both local and global drifts and introduce a new harmonizing framework called HarmoFL. First, we propose to mitigate the local update drift by normalizing amplitudes of images transformed into the frequency domain to mimic a unified imaging setting, in order to generate a harmonized feature space across local clients. Second, based on harmonized features, we design a client weight perturbation guiding each local model to reach a flat optimum, where a neighborhood area of the local optimal solution has a uniformly low loss. Without any extra communication cost, the perturbation assists the global model to optimize towards a converged optimal solution by aggregating several local flat optima. We have theoretically analyzed the proposed method and empirically conducted extensive experiments on three medical image classification and segmentation tasks, showing that HarmoFL outperforms a set of recent state-of-the-art methods with promising convergence behavior.      
### 17.Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2112.11442.pdf)
>  We propose to deliberate the hypothesis alignment of a streaming RNN-T model with the previously proposed Align-Refine non-autoregressive decoding method and its improved versions. The method performs a few refinement steps, where each step shares a transformer decoder that attends to both text features (extracted from alignments) and audio features, and outputs complete updated alignments. The transformer decoder is trained with the CTC loss which facilitates parallel greedy decoding, and performs full-context attention to capture label dependencies. We improve Align-Refine by introducing cascaded encoder that captures more audio context before refinement, and alignment augmentation which enforces learning label dependency. We show that, conditioned on hypothesis alignments of a streaming RNN-T model, our method obtains significantly more accurate recognition results than the first-pass RNN-T, with only small amount of model parameters.      
### 18.Mixed Precision Low-bit Quantization of Neural Network Language Models for Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.11438.pdf)
>  State-of-the-art language models (LMs) represented by long-short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming increasingly complex and expensive for practical applications. Low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. Current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of LMs to quantization errors. To this end, novel mixed precision neural network LM quantization methods are proposed in this paper. The optimal local precision choices for LSTM-RNN and Transformer based neural LMs are automatically learned using three techniques. The first two approaches are based on quantization sensitivity metrics in the form of either the KL-divergence measured between full precision and quantized LMs, or Hessian trace weighted quantization perturbation that can be approximated efficiently using matrix free techniques. The third approach is based on mixed precision neural architecture search. In order to overcome the difficulty in using gradient descent methods to directly estimate discrete quantized weights, alternating direction methods of multipliers (ADMM) are used to efficiently train quantized LMs. Experiments were conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed perturbation, i-Vector and learning hidden unit contribution (LHUC) based speaker adaptation on two tasks: Switchboard telephone speech and AMI meeting transcription. The proposed mixed precision quantization techniques achieved "lossless" quantization on both tasks, by producing model size compression ratios of up to approximately 16 times over the full precision LSTM and Transformer baseline LMs, while incurring no statistically significant word error rate increase.      
### 19.Voice Quality and Pitch Features in Transformer-Based Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.11391.pdf)
>  Jitter and shimmer measurements have shown to be carriers of voice quality and prosodic information which enhance the performance of tasks like speaker recognition, diarization or automatic speech recognition (ASR). However, such features have been seldom used in the context of neural-based ASR, where spectral features often prevail. In this work, we study the effects of incorporating voice quality and pitch features altogether and separately to a Transformer-based ASR model, with the intuition that the attention mechanisms might exploit latent prosodic traits. For doing so, we propose separated convolutional front-ends for prosodic and spectral features, showing that this architectural choice yields better results than simple concatenation of such pitch and voice quality features to mel-spectrogram filterbanks. Furthermore, we find mean Word Error Rate relative reductions of up to 5.6% with the LibriSpeech benchmark. Such findings motivate further research on the application of prosody knowledge for increasing the robustness of Transformer-based ASR.      
### 20.Safeguarding test signals for acoustic measurement using arbitrary sounds  [ :arrow_down: ](https://arxiv.org/pdf/2112.11373.pdf)
>  We propose a simple method to measure acoustic responses using any sounds by converting them suitable for measurement. This method enables us to use music pieces for measuring acoustic conditions. It is advantageous to measure such conditions without annoying test sounds to listeners. In addition, applying the underlying idea of simultaneous measurement of multiple paths provides practically valuable features. For example, it is possible to measure deviations (temporally stable, random, and time-varying) and the impulse response while reproducing slightly modified contents under target conditions. The key idea of the proposed method is to add relatively small deterministic signals that sound like noise to the original sounds. We call the converted sounds safeguarded test signals.      
### 21.ISS-Based Robustness to Various Neglected Damping Mechanisms for the 1-D Wave PDE  [ :arrow_down: ](https://arxiv.org/pdf/2112.11287.pdf)
>  This paper is devoted to the study of the robustness properties of the 1-D wave equation for an elastic vibrating string under four different damping mechanisms that are usually neglected in the study of the wave equation: (i) friction with the surrounding medium of the string (or viscous damping), (ii) thermoelastic phenomena (or thermal damping), (iii) internal friction of the string (or Kelvin-Voigt damping), and (iv) friction at the free end of the string (the so-called passive damper). The passive damper is also the simplest boundary feedback law that guarantees exponential stability for the string. We study robustness with respect to distributed inputs and boundary disturbances in the context of Input-to-State Stability (ISS). By constructing appropriate ISS Lyapunov functionals, we prove the ISS property expressed in various spatial norms.      
### 22.Generating Photo-realistic Images from LiDAR Point Clouds with Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.11245.pdf)
>  We examined the feasibility of generative adversarial networks (GANs) to generate photo-realistic images from LiDAR point clouds. For this purpose, we created a dataset of point cloud image pairs and trained the GAN to predict photorealistic images from LiDAR point clouds containing reflectance and distance information. Our models learned how to predict realistically looking images from just point cloud data, even images with black cars. Black cars are difficult to detect directly from point clouds because of their low level of reflectivity. This approach might be used in the future to perform visual object recognition on photorealistic images generated from LiDAR point clouds. In addition to the conventional LiDAR system, a second system that generates photorealistic images from LiDAR point clouds would run simultaneously for visual object recognition in real-time. In this way, we might preserve the supremacy of LiDAR and benefit from using photo-realistic images for visual object recognition without the usage of any camera. In addition, this approach could be used to colorize point clouds without the usage of any camera images.      
### 23.Human Activity Recognition (HAR) in Smart Homes  [ :arrow_down: ](https://arxiv.org/pdf/2112.11232.pdf)
>  Generally, Human Activity Recognition (HAR) consists of monitoring and analyzing the behavior of one or more persons in order to deduce their activity. In a smart home context, the HAR consists of monitoring daily activities of the residents. Thanks to this monitoring, a smart home can offer home assistance services to improve quality of life, autonomy and health of their residents, especially for elderly and dependent people.      
### 24.Attention-Based Sensor Fusion for Human Activity Recognition Using IMU Signals  [ :arrow_down: ](https://arxiv.org/pdf/2112.11224.pdf)
>  Human Activity Recognition (HAR) using wearable devices such as smart watches embedded with Inertial Measurement Unit (IMU) sensors has various applications relevant to our daily life, such as workout tracking and health monitoring. In this paper, we propose a novel attention-based approach to human activity recognition using multiple IMU sensors worn at different body locations. Firstly, a sensor-wise feature extraction module is designed to extract the most discriminative features from individual sensors with Convolutional Neural Networks (CNNs). Secondly, an attention-based fusion mechanism is developed to learn the importance of sensors at different body locations and to generate an attentive feature representation. Finally, an inter-sensor feature extraction module is applied to learn the inter-sensor correlations, which are connected to a classifier to output the predicted classes of activities. The proposed approach is evaluated using five public datasets and it outperforms state-of-the-art methods on a wide variety of activity categories.      
### 25.An Autonomous Self-Incremental Learning Approach for Detection of Cyber Attacks on Unmanned Aerial Vehicles (UAVs)  [ :arrow_down: ](https://arxiv.org/pdf/2112.11219.pdf)
>  As the technological advancement and capabilities of automated systems have increased drastically, the usage of unmanned aerial vehicles for performing human-dependent tasks without human indulgence has also spiked. Since unmanned aerial vehicles are heavily dependent on Information and Communication Technology, they are highly prone to cyber-attacks. With time more advanced and new attacks are being developed and employed. However, the current Intrusion detection system lacks detection and classification of new and unknown attacks. Therefore, for having an autonomous and reliable operation of unmanned aerial vehicles, more robust and automated cyber detection and protection schemes are needed. To address this, we have proposed an autonomous self-incremental learning architecture, capable of detecting known and unknown cyber-attacks on its own without any human interference. In our approach, we have combined signature-based detection along with anomaly detection in such a way that the signature-based detector autonomously updates its attack classes with the help of an anomaly detector. To achieve this, we have implemented an incremental learning approach, updating our model to incorporate new classes without forgetting the old ones. To validate the applicability and effectiveness of our proposed architecture, we have implemented it in a trial scenario and then compared it with the traditional offline learning approach. Moreover, our anomaly-based detector has achieved a 100% detection rate for attacks.      
### 26.Predicting Defects in Laser Powder Bed Fusion using in-situ Thermal Imaging Data and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.11212.pdf)
>  Variation in the local thermal history during the laser powder bed fusion (LPBF) process in additive manufacturing (AM) can cause microporosity defects. in-situ sensing has been proposed to monitor the AM process to minimize defects, but the success requires establishing a quantitative relationship between the sensing data and the porosity, which is especially challenging for a large number of variables and computationally costly. In this work, we develop machine learning (ML) models that can use in-situ thermographic data to predict the microporosity of LPBF stainless steel materials. This work considers two identified key features from the thermal histories: the time above the apparent melting threshold (/tau) and the maximum radiance (T_{max}). These features are computed, stored for each voxel in the built material, are used as inputs. The binary state of each voxel, either defective or normal, is the output. Different ML models are trained and tested for the binary classification task. In addition to using the thermal features of each voxel to predict its own state, the thermal features of neighboring voxels are also included as inputs. This is shown to improve the prediction accuracy, which is consistent with thermal transport physics around each voxel contributing to its final state. Among the models trained, the F1 scores on test sets reach above 0.96 for random forests. Feature importance analysis based on the ML models shows that T_{max}is more important to the voxel state than /tau. The analysis also finds that the thermal history of the voxels above the present voxel is more influential than those beneath it.      
### 27.Achievable Rate Maximization for Underlay Spectrum Sharing MIMO System with Intelligent Reflecting Surface  [ :arrow_down: ](https://arxiv.org/pdf/2112.11181.pdf)
>  We consider the achievable rate maximization problem for intelligent reflecting surface (IRS) assisted multiple-input multiple-output systems in an underlay spectrum sharing scenario, subject to interference power constraints at primary users. The formulated non-convex optimization problem is challenging to solve due to its non-convexity as well as coupling design variables in the constraints. Different from existing works that are mostly based on alternating optimization (AO), we propose a penalty dual decomposition based gradient projection (PDDGP) algorithm to solve this problem. We also provide a convergence proof and a complexity analysis for the proposed algorithm. We benchmark the proposed algorithm against two known solutions, namely a minimum mean-square error based AO algorithm and an inner approximation method with block coordinate descent. Specifically, the complexity of the proposed algorithm is $O(N_I^2)$ while that of the two benchmark methods is $O(N_I^3)$, where $N_I$ is the number of IRS elements. Moreover, numerical results show that the proposed PDDGP algorithm yields considerably higher achievable rate than the benchmark solutions.      
### 28.Self-Supervised Learning based Monaural Speech Enhancement with Complex-Cycle-Consistent  [ :arrow_down: ](https://arxiv.org/pdf/2112.11142.pdf)
>  Recently, self-supervised learning (SSL) techniques have been introduced to solve the monaural speech enhancement problem. Due to the lack of using clean phase information, the enhancement performance is limited in most SSL methods. Therefore, in this paper, we propose a phase-aware self-supervised learning based monaural speech enhancement method. The latent representations of both amplitude and phase are studied in two decoders of the foundation autoencoder (FAE) with only a limited set of clean speech signals independently. Then, the downstream autoencoder (DAE) learns a shared latent space between the clean speech and mixture representations with a large number of unseen mixtures. A complex-cycle-consistent (CCC) mechanism is proposed to minimize the reconstruction loss between the amplitude and phase domains. Besides, it is noticed that if the speech features are extracted as the multi-resolution spectra, the desired information distributed in spectra of different scales can be studied to further boost the performance. The NOISEX and DAPS corpora are used to generate mixtures with different interferences to evaluate the efficacy of the proposed method. It is highlighted that the clean speech and mixtures fed in FAE and DAE are not paired. Both ablation and comparison experimental results show that the proposed method clearly outperforms the state-of-the-art approaches.      
### 29.Melody Harmonization with Controllable Harmonic Rhythm  [ :arrow_down: ](https://arxiv.org/pdf/2112.11122.pdf)
>  Melody harmonization, namely generating a chord progression for a user-given melody, remains a challenging task to this day. Although previous neural network-based systems can effectively generate an appropriate chord progression for a melody, few studies focus on controllable melody harmonization, and none of them can generate flexible harmonic rhythms. To achieve harmonic rhythm-controllable melody harmonization, we propose AutoHarmonizer, a neural network-based melody harmonization system that can generate denser or sparser chord progressions with the use of a new sampling method for controllable generation proposed in this paper. This system mainly consists of two parts: a harmonic rhythm model provides coarse-grained chord onset information, while a chord model generates specific pitches for chords based on the given melody and the corresponding harmonic rhythm sequence previously generated. To evaluate the performance of AutoHarmonizer, we use nine metrics to compare the chord progressions from humans, the system proposed in this paper and the baseline. Experimental results show that AutoHarmonizer not only generates harmonic rhythms comparable to the human level, but generates chords with overall better quality than baseline at different settings. In addition, we use AutoHarmonizer to harmonize the Session Dataset (which were originally chordless), and ended with 40,925 traditional Irish folk songs with harmonies, named the Session Lead Sheet Dataset, which is the largest lead sheet dataset to date.      
### 30.Developing and Validating Semi-Markov Occupancy Generative Models: A Technical Report  [ :arrow_down: ](https://arxiv.org/pdf/2112.11111.pdf)
>  This report documents recent technical work on developing and validating stochastic occupancy models in commercial buildings, performed by the Pacific Northwest National Laboratory (PNNL) as part of the Sensor Impact Evaluation and Verification project under the U.S. Department of Energy (DOE) Building Technologies Office (BTO). In this report, we present our work on developing and validating inhomogeneous semi-Markov chain models for generating sequences of zone-level occupancy presence and occupancy counts in a commercial building. Real datasets are used to learn and validate the generative occupancy models. Relevant metrics such as normalized Jensen-Shannon distance (NJSD) are used to demonstrate the ability of the models to express realistic occupancy behavioral patterns.      
### 31.Aerial Base Station Positioning and Power Control for Securing Communications: A Deep Q-Network Approach  [ :arrow_down: ](https://arxiv.org/pdf/2112.11090.pdf)
>  The unmanned aerial vehicle (UAV) is one of the technological breakthroughs that supports a variety of services, including communications. UAV will play a critical role in enhancing the physical layer security of wireless networks. This paper defines the problem of eavesdropping on the link between the ground user and the UAV, which serves as an aerial base station (ABS). The reinforcement learning algorithms Q-learning and deep Q-network (DQN) are proposed for optimizing the position of the ABS and the transmission power to enhance the data rate of the ground user. This increases the secrecy capacity without the system knowing the location of the eavesdropper. Simulation results show fast convergence and the highest secrecy capacity of the proposed DQN compared to Q-learning and baseline approaches.      
### 32.Online programming system for robotic fillet welding in Industry 4.0  [ :arrow_down: ](https://arxiv.org/pdf/2112.11061.pdf)
>  Fillet welding is one of the most widespread types of welding in the industry, which is still carried out manually or automated by contact. This paper aims to describe an online programming system for noncontact fillet welding robots with U and L shaped structures, which responds to the needs of the Fourth Industrial Revolution. In this paper, the authors propose an online robot programming methodology that eliminates unnecessary steps traditionally performed in robotic welding, so that the operator only performs three steps to complete the welding task. First, choose the piece to weld. Then, enter the welding parameters. Finally, it sends the automatically generated program to the robot. The system finally managed to perform the fillet welding task with the proposed method in a more efficient preparation time than the compared methods. For this, a reduced number of components was used compared to other systems, such as, a structured light 3D camera, two computers and a concentrator, in addition to the six axis industrial robotic arm. The operating complexity of the system has been reduced as much as possible. To the best of the authors knowledge, there is no scientific or commercial evidence of an online robot programming system capable of performing a fillet welding process, simplifying the process so that it is completely transparent for the operator and framed in the Industry 4.0 paradigm. Its commercial potential lies mainly in its simple and low cost implementation in a flexible system capable of adapting to any industrial fillet welding job and to any support that can accommodate it.      
### 33.Local Strong Convexity of Source Localization and Error Bound for Target Tracking under Time-of-Arrival Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2112.11045.pdf)
>  In this paper, we consider a time-varying optimization approach to the problem of tracking a moving target using noisy time-of-arrival (TOA) measurements. Specifically, we formulate the problem as that of sequential TOA-based source localization and apply online gradient descent (OGD) to it to generate the position estimates of the target. To analyze the tracking performance of OGD, we first revisit the classic least-squares formulation of the (static) TOA-based source localization problem and elucidate its estimation and geometric properties. In particular, under standard assumptions on the TOA measurement model, we establish a bound on the distance between an optimal solution to the least-squares formulation and the true target position. Using this bound, we show that the loss function in the formulation, albeit non-convex in general, is locally strongly convex at its global minima. To the best of our knowledge, these results are new and can be of independent interest. By combining them with existing techniques from online strongly convex optimization, we then establish the first non-trivial bound on the cumulative target tracking error of OGD. Our numerical results corroborate the theoretical findings and show that OGD can effectively track the target at different noise levels.      
### 34.Point spread function estimation for blind image deblurring problems based on framelet transform  [ :arrow_down: ](https://arxiv.org/pdf/2112.11004.pdf)
>  One of the most important issues in the image processing is the approximation of the image that has been lost due to the blurring process. These types of matters are divided into non-blind and blind problems. The second type of problem is more complex in terms of calculations than the first problems due to the unknown of original image and point spread function estimation. In the present paper, an algorithm based on coarse-to-fine iterative by $l_0-\alpha l_1$ regularization and framelet transform is introduced to approximate the spread function estimation. Framelet transfer improves the restored kernel due to the decomposition of the kernel to different frequencies. Also in the proposed model fraction gradient operator is used instead of ordinary gradient operator. The proposed method is investigated on different kinds of images such as text, face, natural. The output of the proposed method reflects the effectiveness of the proposed algorithm in restoring the images from blind problems.      
### 35.X-ray dark-field and phase retrieval without optics, via the Fokker-Planck equation  [ :arrow_down: ](https://arxiv.org/pdf/2112.10999.pdf)
>  Emerging methods of x-ray imaging that capture phase and dark-field effects are equipping medicine with complementary sensitivity to conventional radiography. These methods are being applied over a wide range of scales, from virtual histology to clinical chest imaging, and typically require the introduction of optics such as gratings. Here, we consider extracting x-ray phase and dark-field signals from bright-field images collected using nothing more than a coherent x-ray source and detector. Our approach is based on the Fokker--Planck equation for paraxial imaging, which is the diffusive generalization of the transport-of-intensity equation. Specifically, we utilize the Fokker--Planck equation in the context of propagation-based phase-contrast imaging, where we show that two intensity images are sufficient for successful retrieval of the projected thickness and dark-field signals associated with the sample. We show the results of our algorithm using both a simulated dataset and an experimental dataset. These demonstrate that the x-ray dark-field signal can be extracted from propagation-based images, and that x-ray phase can be retrieved with better spatial resolution when dark-field effects are taken into account. We anticipate the proposed algorithm will be of benefit in biomedical imaging, industrial settings, and other non-invasive imaging applications.      
### 36.Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement  [ :arrow_down: ](https://arxiv.org/pdf/2112.10991.pdf)
>  End-to-end speech-to-text translation~(E2E-ST) is becoming increasingly popular due to the potential of its less error propagation, lower latency, and fewer parameters. Given the triplet training corpus $\langle speech, transcription, translation\rangle$, the conventional high-quality E2E-ST system leverages the $\langle speech, transcription\rangle$ pair to pre-train the model and then utilizes the $\langle speech, translation\rangle$ pair to optimize it further. However, this process only involves two-tuple data at each stage, and this loose coupling fails to fully exploit the association between triplet data. In this paper, we attempt to model the joint probability of transcription and translation based on the speech input to directly leverage such triplet data. Based on that, we propose a novel regularization method for model training to improve the agreement of dual-path decomposition within triplet data, which should be equal in theory. To achieve this goal, we introduce two Kullback-Leibler divergence regularization terms into the model training objective to reduce the mismatch between output probabilities of dual-path. Then the well-trained model can be naturally transformed as the E2E-ST models by the pre-defined early stop tag. Experiments on the MuST-C benchmark demonstrate that our proposed approach significantly outperforms state-of-the-art E2E-ST baselines on all 8 language pairs, while achieving better performance in the automatic speech recognition task. Our code is open-sourced at <a class="link-external link-https" href="https://github.com/duyichao/E2E-ST-TDA" rel="external noopener nofollow">this https URL</a>.      
### 37.Joint Learning of Linear Time-Invariant Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.10955.pdf)
>  Learning the parameters of a linear time-invariant dynamical system (LTIDS) is a problem of current interest. In many applications, one is interested in jointly learning the parameters of multiple related LTIDS, which remains unexplored to date. To that end, we develop a joint estimator for learning the transition matrices of LTIDS that share common basis matrices. Further, we establish finite-time error bounds that depend on the underlying sample size, dimension, number of tasks, and spectral properties of the transition matrices. The results are obtained under mild regularity assumptions and showcase the gains from pooling information across LTIDS, in comparison to learning each system separately. We also study the impact of misspecifying the joint structure of the transition matrices and show that the established results are robust in the presence of moderate misspecifications.      
### 38.Subject-Independent Drowsiness Recognition from Single-Channel EEG with an Interpretable CNN-LSTM model  [ :arrow_down: ](https://arxiv.org/pdf/2112.10894.pdf)
>  For EEG-based drowsiness recognition, it is desirable to use subject-independent recognition since conducting calibration on each subject is time-consuming. In this paper, we propose a novel Convolutional Neural Network (CNN)-Long Short-Term Memory (LSTM) model for subject-independent drowsiness recognition from single-channel EEG signals. Different from existing deep learning models that are mostly treated as black-box classifiers, the proposed model can explain its decisions for each input sample by revealing which parts of the sample contain important features identified by the model for classification. This is achieved by a visualization technique by taking advantage of the hidden states output by the LSTM layer. Results show that the model achieves an average accuracy of 72.97% on 11 subjects for leave-one-out subject-independent drowsiness recognition on a public dataset, which is higher than the conventional baseline methods of 55.42%-69.27%, and state-of-the-art deep learning methods. Visualization results show that the model has discovered meaningful patterns of EEG signals related to different mental states across different subjects.      
