# ArXiv eess --Mon, 13 Dec 2021
### 1.Deep Learning based Framework for Automatic Diagnosis of Glaucoma based on analysis of Focal Notching in the Optic Nerve Head  [ :arrow_down: ](https://arxiv.org/pdf/2112.05748.pdf)
>  Automatic evaluation of the retinal fundus image is emerging as one of the most important tools for early detection and treatment of progressive eye diseases like Glaucoma. Glaucoma results to a progressive degeneration of vision and is characterized by the deformation of the shape of optic cup and the degeneration of the blood vessels resulting in the formation of a notch along the neuroretinal rim. In this paper, we propose a deep learning-based pipeline for automatic segmentation of optic disc (OD) and optic cup (OC) regions from Digital Fundus Images (DFIs), thereby extracting distinct features necessary for prediction of Glaucoma. This methodology has utilized focal notch analysis of neuroretinal rim along with cup-to-disc ratio values as classifying parameters to enhance the accuracy of Computer-aided design (CAD) systems in analyzing glaucoma. Support Vector-based Machine Learning algorithm is used for classification, which classifies DFIs as Glaucomatous or Normal based on the extracted features. The proposed pipeline was evaluated on the freely available DRISHTI-GS dataset with a resultant accuracy of 93.33% for detecting Glaucoma from DFIs.      
### 2.A Simple and Efficient Sampling-based Algorithm for General Reachability Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2112.05745.pdf)
>  In this work, we analyze an efficient sampling-based algorithm for general-purpose reachability analysis, which remains a notoriously challenging problem with applications ranging from neural network verification to safety analysis of dynamical systems. By sampling inputs, evaluating their images in the true reachable set, and taking their $\epsilon$-padded convex hull as a set estimator, this algorithm applies to general problem settings and is simple to implement. Our main contribution is the derivation of asymptotic and finite-sample accuracy guarantees using random set theory. This analysis informs algorithmic design to obtain an $\epsilon$-close reachable set approximation with high probability, provides insights into which reachability problems are most challenging, and motivates safety-critical applications of the technique. On a neural network verification task, we show that this approach is more accurate and significantly faster than prior work. Informed by our analysis, we also design a robust model predictive controller that we demonstrate in hardware experiments.      
### 3.Learning-based personal speech enhancement for teleconferencing by exploiting spatial-spectral features  [ :arrow_down: ](https://arxiv.org/pdf/2112.05686.pdf)
>  Teleconferencing is becoming essential during the COVID-19 pandemic. However, in real-world applications, speech quality can deteriorate due to, for example, background interference, noise, or reverberation. To solve this problem, target speech extraction from the mixture signals can be performed with the aid of the user's vocal features. Various features are accounted for in this study's proposed system, including speaker embeddings derived from user enrollment and a novel long-short-term spatial coherence (LSTSC) feature to the target speaker activity. As a learning-based approach, a target speech sifting network was employed to extract the relevant features. The network trained with LSTSC in the proposed approach is robust to microphone array geometries and the number of microphones. Furthermore, the proposed enhancement system was compared with a baseline system with speaker embeddings and interchannel phase difference. The results demonstrated the superior performance of the proposed system over the baseline in enhancement performance and robustness.      
### 4.An Improved Fractional-Order Active Disturbance Rejection Control: Performance Analysis and Experiment Verification  [ :arrow_down: ](https://arxiv.org/pdf/2112.05553.pdf)
>  This paper presents an improved active disturbance rejection control scheme (IFO-ADRC) with an improved fractional-order extended state observer (IFO-ESO). The structural information of the system is utilized in IFO-ESO rather than buried as in the typical fractional-order extended state observer (FO-ESO) and help significantly improve the performance of IFO-ESO and closed-loop system. Compared with the integer-order active disturbance rejection controller (IO-ADRC), the auxiliary tracking controller of IFO-ADRC has a simpler form and fewer parameters need to be tuned. Frequency-domain analysis shows that IFO-ESO has better performance over the larger frequency band than FO-ESO, and time-domain simulation shows that IFO-ADRC has better transient performance and is more robust against the parameter variations than traditional fractional-order active disturbance rejection controller (FO-ADRC) and IO-ADRC. The IFO-ADRC is applied to permanent magnet synchronous motor (PMSM) servo control system and demonstrates its capability in the real-world application.      
### 5.DeepRLS: A Recurrent Network Architecture with Least Squares Implicit Layers for Non-blind Image Deconvolution  [ :arrow_down: ](https://arxiv.org/pdf/2112.05505.pdf)
>  In this work, we study the problem of non-blind image deconvolution and propose a novel recurrent network architecture that leads to very competitive restoration results of high image quality. Motivated by the computational efficiency and robustness of existing large scale linear solvers, we manage to express the solution to this problem as the solution of a series of adaptive non-negative least-squares problems. This gives rise to our proposed Recurrent Least Squares Deconvolution Network (RLSDN) architecture, which consists of an implicit layer that imposes a linear constraint between its input and output. By design, our network manages to serve two important purposes simultaneously. The first is that it implicitly models an effective image prior that can adequately characterize the set of natural images, while the second is that it recovers the corresponding maximum a posteriori (MAP) estimate. Experiments on publicly available datasets, comparing recent state-of-the-art methods, show that our proposed RLSDN approach achieves the best reported performance both for grayscale and color images for all tested scenarios. Furthermore, we introduce a novel training strategy that can be adopted by any network architecture that involves the solution of linear systems as part of its pipeline. Our strategy eliminates completely the need to unroll the iterations required by the linear solver and, thus, it reduces significantly the memory footprint during training. Consequently, this enables the training of deeper network architectures which can further improve the reconstruction results.      
### 6.An Adaptive Observer for Uncertain Linear Time-Varying Systems with Unknown Additive Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2112.05497.pdf)
>  In this paper we are interested in the problem of adaptive state observation of linear time-varying (LTV) systems where the system and the input matrices depend on unknown time-varying parameters. It is assumed that these parameters satisfy some known LTV dynamics, but with unknown initial conditions. Moreover, the state equation is perturbed by an additive signal generated from an exosystem with uncertain constant parameters. Our main contribution is to propose a globally convergent state observer that requires only a weak excitation assumption on the system.      
### 7.Off-Grid Direction-of-Arrival Estimation Using Second-Order Taylor Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2112.05487.pdf)
>  The problem of off-grid direction-of-arrival (DOA) estimation is investigated in this paper. We develop a grid-based method to jointly estimate the closest spatial frequency (the sine of DOA) grids, and the gaps between the estimated grids and the corresponding frequencies. By using a second-order Taylor approximation, the data model under the framework of joint-sparse representation is formulated. We point out an important property of the signals of interest in the model, namely the proportionality relationship, which is empirically demonstrated to be useful in the sense that it increases the probability of the mixing matrix satisfying the block restricted isometry property. Simulation examples demonstrate the effectiveness and superiority of the proposed method against several state-of-the-art grid-based approaches.      
### 8.Latency-Aware Multi-antenna SWIPT System with Battery-Constrained Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2112.05483.pdf)
>  Power splitting (PS) based simultaneous wireless information and power transfer (SWIPT) is considered in a multi-user multiple-input-single-output broadcast scenario. Specifically, we focus on jointly configuring the transmit beamforming vectors and receive PS ratios to minimize the total transmit energy of the base station under the user-specific latency and energy harvesting (EH) requirements. The battery depletion phenomenon is avoided by preemptively incorporating information regarding the receivers' battery state and EH fluctuations into the resource allocation design. The resulting time-average sum-power minimization problem is temporally correlated, non-convex (including mutually coupled latency-battery queue dynamics), and in general intractable. We use the Lyapunov optimization framework and derive a dynamic control algorithm to transform the original problem into a sequence of deterministic and independent subproblems, which are then solved via two alternative approaches: i) semidefinite relaxation combined with fractional programming, and ii) successive convex approximation. Furthermore, we design a low-complexity closed-form iterative algorithm exploiting the Karush-Kuhn-Tucker optimality conditions for a specific scenario with delay bounded batteryless receivers. Numerical results provide insights on the robustness of the proposed design to realize an energy-efficient SWIPT system while ensuring latency and EH requirements in a time dynamic mobile access network.      
### 9.A recurrent neural network approach for remaining useful life prediction utilizing a novel trend features construction method  [ :arrow_down: ](https://arxiv.org/pdf/2112.05372.pdf)
>  Data-driven methods for remaining useful life (RUL) prediction normally learn features from a fixed window size of a priori of degradation, which may lead to less accurate prediction results on different datasets because of the variance of local features. This paper proposes a method for RUL prediction which depends on a trend feature representing the overall time sequence of degradation. Complete ensemble empirical mode decomposition, followed by a reconstruction procedure, is created to build the trend features. The probability distribution of sensors' measurement learned by conditional neural processes is used to evaluate the trend features. With the best trend feature, a data-driven model using long short-term memory is developed to predict the RUL. To prove the effectiveness of the proposed method, experiments on a benchmark C-MAPSS dataset are carried out and compared with other state-of-the-art methods. Comparison results show that the proposed method achieves the smallest root mean square values in prediction of all RUL.      
### 10.Mean-Square Stabilizability of Low-Order Systems Against Correlated Stochastic Multiplicative Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2112.05363.pdf)
>  In this paper, we first study the robust stability problem for discrete-time linear time-invariant systems under stochastic multiplicative uncertainties. Those uncertainties could be are susceptible to describing transmission errors, packet drops, random delays, and fading phenomena in networked control systems. In its full generality, we assume that the multiplicative uncertainties are diagonally structured, and are allowed to be spatially correlated across different patterns, which differs from previously related work significantly. We derive a necessary and sufficient condition for robust stability in the mean-square sense against such uncertainties. Based on the obtained stability condition, we further investigate the mean-square stabilizability and consensusability problems through two case studies of first-order single- and two-agent systems. The necessary and sufficient conditions to guarantee stabilizability and consensusability are derived, which rely on the unstable system dynamics and the stochastic uncertainty variances.      
### 11.A New Approach to Image Compression in Industrial Internet of Things  [ :arrow_down: ](https://arxiv.org/pdf/2112.05361.pdf)
>  Applying image sensors in automation of Industrial Internet of Things (IIoT) technology is on the rise, day by day. In such companies, a large number of high volume images are transmitted at any moment; therefore, a significant challenge is reducing the amount of transmitted information and consequently bandwidth without reducing the quality of images. Image compression in sensors, in this regard, will save bandwidth and speed up data transmitting. There are several pieces of research in image compression for sensor networks, but, according to the nature of image transfer in IIoT, there is no study in this particular field. In this paper, it is for the first time that a new reusable technique to improve productivity in image compression is introduced and applied. To do this, a new adaptive lossy compression technique to compact sensor-generated images in IIoT by using K- Means++ and Intelligent Embedded Coding (IEC) as our novel approach, is presented. The new method is based on the colour of pixels so that pixels with the same or nearly the same colours are clustered around a centroid and finally, the colour of the pixels will be encoded. The experiments are based on a reputable image dataset from a real smart greenhouse; i.e. KOMATSUNA. The evaluation results reveal that, with the same compression rate, our approach compresses images with higher quality in comparison with other methods such as K-means, fuzzy C-means and fuzzy C-means++ clustering.      
### 12.Open-Access Data and Toolbox for Tracking COVID-19 Impact on Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.05320.pdf)
>  Intervention policies against COVID-19 have caused large-scale disruptions globally, and led to a series of pattern changes in the power system operation. Analyzing these pandemic-induced patterns is imperative to identify the potential risks and impacts of this extreme event. With this purpose, we developed an open-access data hub (COVID-EMDA+), an open-source toolbox (CoVEMDA), and a few evaluation methods to explore what the U.S. power systems are experiencing during COVID-19. These resources could be broadly used for research, policy making, or educational purposes. Technically, our data hub harmonizes a variety of raw data such as generation mix, demand profiles, electricity price, weather observations, mobility, confirmed cases and deaths. Several support methods and metrics are then implemented in our toolbox, including baseline estimation, regression analysis, and scientific visualization. Based on these, we conduct three empirical studies on the U.S. power systems and markets to introduce some new solutions and unexpected findings. This conveys a more complete picture of the pandemic's impacts, and also opens up several attractive topics for future work. Python, Matlab source codes, and user manuals are all publicly shared on a Github repository.      
### 13.Surrogate-based cross-correlation for particle image velocimetry  [ :arrow_down: ](https://arxiv.org/pdf/2112.05303.pdf)
>  This paper presents a novel surrogate-based cross-correlation (SBCC) framework to improve the correlation performance between two image signals. The basic idea behind the SBCC is that an optimized surrogate filter/image, supplanting one original image, will produce a more robust and more accurate correlation signal. The cross-correlation estimation of the SBCC is formularized with an objective function composed of surrogate loss and correlation consistency loss. The closed-form solution provides an efficient estimation. To our surprise, the SBCC framework could provide an alternative view to explain a set of generalized cross-correlation (GCC) methods and comprehend the meaning of parameters. With the help of our SBCC framework, we further propose four new specific cross-correlation methods, and provide some suggestions for improving existing GCC methods. A noticeable fact is that the SBCC could enhance the correlation robustness by incorporating other negative context images. Considering the sub-pixel accuracy and robustness requirement of particle image velocimetry (PIV), the contribution of each term in the objective function is investigated with particles' images. Compared with the state-of-the-art baseline methods, the SBCC methods exhibit improved performance (accuracy and robustness) on the synthetic dataset and several challenging real experimental PIV cases.      
### 14.On the Dilution of Precision for Time Difference of Arrival with Station Deployment  [ :arrow_down: ](https://arxiv.org/pdf/2112.05296.pdf)
>  The paper aims to reveal the relationship between the performance of moving object tracking algorithms and the tracking anchors (station) deployment. The Dilution of Precision (DoP) for Time difference of arrival (TDoA) technique with respect to anchor deployment is studied. Linear estimator and non-linear estimator are used for TDoA algorithms. The research findings are: for the linear estimator, the DoP attain a lower value when other anchors are scattered around a central anchor; for the nonlinear estimator, the DoP is optimal when the anchors are scattered around the target tag. Experiments on both of the algorithms are conducted, targeting the location precision related to the anchors' deployment, with practical situations for tracking moving objects integrated with a Kalman Filter (KF) in an Ultra-Wide Band (UWB) based real-time localization system. The work provides a guideline for deploying anchors in UWB-based tracking systems.      
### 15.System-Level Analysis of Full-Duplex Self-Backhauled Millimeter Wave Networks  [ :arrow_down: ](https://arxiv.org/pdf/2112.05263.pdf)
>  Integrated access and backhaul (IAB) facilitates cost-effective deployment of millimeter wave(mmWave) cellular networks through multihop self-backhauling. Full-duplex (FD) technology, particularly for mmWave systems, is a potential means to overcome latency and throughput challenges faced by IAB networks. We derive practical and tractable throughput and latency constraints using queueing theory and formulate a network utility maximization problem to evaluate both FD-IAB and half-duplex(HD)-IAB networks. We use this to characterize the network-level improvements seen when upgrading from conventional HD IAB nodes to FD ones by deriving closed-form expressions for (i) latency gain of FD-IAB over HD-IAB and (ii) the maximum number of hops that a HD- and FD-IAB network can support while satisfying latency and throughput targets. Extensive simulations illustrate that FD-IAB can facilitate reduced latency, higher throughput, deeper networks, and fairer service. Compared to HD-IAB,FD-IAB can improve throughput by 8x and reduce latency by 4x for a fourth-hop user. In fact, upgrading IAB nodes with FD capability can allow the network to support latency and throughput targets that its HD counterpart fundamentally cannot meet. The gains are more profound for users further from the donor and can be achieved even when residual self-interference is significantly above the noise floor.      
### 16.High Voltage Shore Connection Systems: Grounding Resistance Selection and Short Circuit Currents Evaluation  [ :arrow_down: ](https://arxiv.org/pdf/2112.05222.pdf)
>  Cold ironing represents an effective solution to remove air polluting emissions from ports. The high voltage shore connection system is the key enabling facility that allows to provide power from the shore side electrical system to the ship. The design of the shore connection needs a comprehensive assessment of the fault currents in different operating scenarios. International standards require the neutral point of the shore connection transformer be equipped with a neutral grounding resistor. Its value has to be defined to guarantee safety and protection of equipment and personnel in case of single phase-to-ground faults. Moreover, three-phase short circuits need to be considered to size equipment and protection devices. A crucial role is played by the frequency converter control system, required to adapt the mains frequency to the frequency of the ship. In this work, a complete electro-magnetic dynamic model of the high voltage shore connection and of the on-board power system has been developed, including frequency converter, shore-side transformer, connection MV cables and power system of the ship, to analyze in detail the behavior of the system in case of single phase-to-ground fault and three-phase short circuit, taking into account relevant standards and best practices.      
### 17.MantissaCam: Learning Snapshot High-dynamic-range Imaging with Perceptually-based In-pixel Irradiance Encoding  [ :arrow_down: ](https://arxiv.org/pdf/2112.05221.pdf)
>  The ability to image high-dynamic-range (HDR) scenes is crucial in many computer vision applications. The dynamic range of conventional sensors, however, is fundamentally limited by their well capacity, resulting in saturation of bright scene parts. To overcome this limitation, emerging sensors offer in-pixel processing capabilities to encode the incident irradiance. Among the most promising encoding schemes is modulo wrapping, which results in a computational photography problem where the HDR scene is computed by an irradiance unwrapping algorithm from the wrapped low-dynamic-range (LDR) sensor image. Here, we design a neural network--based algorithm that outperforms previous irradiance unwrapping methods and, more importantly, we design a perceptually inspired "mantissa" encoding scheme that more efficiently wraps an HDR scene into an LDR sensor. Combined with our reconstruction framework, MantissaCam achieves state-of-the-art results among modulo-type snapshot HDR imaging approaches. We demonstrate the efficacy of our method in simulation and show preliminary results of a prototype MantissaCam implemented with a programmable sensor.      
### 18.Hidden Path Selection Network for Semantic Segmentation of Remote Sensing Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.05220.pdf)
>  Targeting at depicting land covers with pixel-wise semantic categories, semantic segmentation in remote sensing images needs to portray diverse distributions over vast geographical locations, which is difficult to be achieved by the homogeneous pixel-wise forward paths in the architectures of existing deep models. Although several algorithms have been designed to select pixel-wise adaptive forward paths for natural image analysis, it still lacks theoretical supports on how to obtain optimal selections. In this paper, we provide mathematical analyses in terms of the parameter optimization, which guides us to design a method called Hidden Path Selection Network (HPS-Net). With the help of hidden variables derived from an extra mini-branch, HPS-Net is able to tackle the inherent problem about inaccessible global optimums by adjusting the direct relationships between feature maps and pixel-wise path selections in existing algorithms, which we call hidden path selection. For the better training and evaluation, we further refine and expand the 5-class Gaofen Image Dataset (GID-5) to a new one with 15 land-cover categories, i.e., GID-15. The experimental results on both GID-5 and GID-15 demonstrate that the proposed modules can stably improve the performance of different deep structures, which validates the proposed mathematical analyses.      
### 19.Report-Guided Automatic Lesion Annotation for Deep Learning-Based Prostate Cancer Detection in bpMRI  [ :arrow_down: ](https://arxiv.org/pdf/2112.05151.pdf)
>  Deep learning-based diagnostic performance increases with more annotated data, but manual annotation is a bottleneck in most fields. Experts evaluate diagnostic images during clinical routine, and write their findings in reports. Automatic annotation based on clinical reports could overcome the manual labelling bottleneck. We hypothesise that dense annotations for detection tasks can be generated using model predictions, guided by sparse information from these reports. To demonstrate efficacy, we generated clinically significant prostate cancer (csPCa) annotations, guided by the number of clinically significant findings in the radiology reports. We included 7,756 prostate MRI examinations, of which 3,050 were manually annotated and 4,706 were automatically annotated. We evaluated the automatic annotation quality on the manually annotated subset: our score extraction correctly identified the number of csPCa lesions for $99.3\%$ of the reports and our csPCa segmentation model correctly localised $83.8 \pm 1.1\%$ of the lesions. We evaluated prostate cancer detection performance on 300 exams from an external centre with histopathology-confirmed ground truth. Augmenting the training set with automatically labelled exams improved patient-based diagnostic area under the receiver operating characteristic curve from $88.1\pm 1.1\%$ to $89.8\pm 1.0\%$ ($P = 1.2 \cdot 10^{-4}$) and improved lesion-based sensitivity at one false positive per case from $79.2 \pm 2.8\%$ to $85.4 \pm 1.9\%$ ($P&lt;10^{-4}$), with $mean \pm std.$ over 15 independent runs. This improved performance demonstrates the feasibility of our report-guided automatic annotations. Source code is made publicly available at <a class="link-external link-https" href="https://github.com/DIAGNijmegen/Report-Guided-Annotation" rel="external noopener nofollow">this https URL</a>. Best csPCa detection algorithm is made available at <a class="link-external link-https" href="https://grand-challenge.org/algorithms/bpmri-cspca-detection-report-guided-annotations/" rel="external noopener nofollow">this https URL</a>.      
### 20.Deep Recurrent Neural Network with Multi-scale Bi-directional Propagation for Video Deblurring  [ :arrow_down: ](https://arxiv.org/pdf/2112.05150.pdf)
>  The success of the state-of-the-art video deblurring methods stems mainly from implicit or explicit estimation of alignment among the adjacent frames for latent video restoration. However, due to the influence of the blur effect, estimating the alignment information from the blurry adjacent frames is not a trivial task. Inaccurate estimations will interfere the following frame restoration. Instead of estimating alignment information, we propose a simple and effective deep Recurrent Neural Network with Multi-scale Bi-directional Propagation (RNN-MBP) to effectively propagate and gather the information from unaligned neighboring frames for better video deblurring. Specifically, we build a Multi-scale Bi-directional Propagation~(MBP) module with two U-Net RNN cells which can directly exploit the inter-frame information from unaligned neighboring hidden states by integrating them in different scales. Moreover, to better evaluate the proposed algorithm and existing state-of-the-art methods on real-world blurry scenes, we also create a Real-World Blurry Video Dataset (RBVD) by a well-designed Digital Video Acquisition System (DVAS) and use it as the training and evaluation dataset. Extensive experimental results demonstrate that the proposed RBVD dataset effectively improves the performance of existing algorithms on real-world blurry videos, and the proposed algorithm performs favorably against the state-of-the-art methods on three typical benchmarks. The code is available at <a class="link-external link-https" href="https://github.com/XJTU-CVLAB-LOWLEVEL/RNN-MBP" rel="external noopener nofollow">this https URL</a>.      
### 21.DiffuseMorph: Unsupervised Deformable Image Registration Along Continuous Trajectory Using Diffusion Models  [ :arrow_down: ](https://arxiv.org/pdf/2112.05149.pdf)
>  Deformable image registration is one of the fundamental tasks for medical imaging and computer vision. Classical registration algorithms usually rely on iterative optimization approaches to provide accurate deformation, which requires high computational cost. Although many deep-learning-based methods have been developed to carry out fast image registration, it is still challenging to estimate the deformation field with less topological folding problem. Furthermore, these approaches only enable registration to a single fixed image, and it is not possible to obtain continuously varying registration results between the moving and fixed images. To address this, here we present a novel approach of diffusion model-based probabilistic image registration, called DiffuseMorph. Specifically, our model learns the score function of the deformation between moving and fixed images. Similar to the existing diffusion models, DiffuseMorph not only provides synthetic deformed images through a reverse diffusion process, but also enables various levels of deformation of the moving image along with the latent space. Experimental results on 2D face expression image and 3D brain image registration tasks demonstrate that our method can provide flexible and accurate deformation with a capability of topology preservation.      
### 22.Learning Deep Context-Sensitive Decomposition for Low-Light Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2112.05147.pdf)
>  Enhancing the quality of low-light images plays a very important role in many image processing and multimedia applications. In recent years, a variety of deep learning techniques have been developed to address this challenging task. A typical framework is to simultaneously estimate the illumination and reflectance, but they disregard the scene-level contextual information encapsulated in feature spaces, causing many unfavorable outcomes, e.g., details loss, color unsaturation, artifacts, and so on. To address these issues, we develop a new context-sensitive decomposition network architecture to exploit the scene-level contextual dependencies on spatial scales. More concretely, we build a two-stream estimation mechanism including reflectance and illumination estimation network. We design a novel context-sensitive decomposition connection to bridge the two-stream mechanism by incorporating the physical principle. The spatially-varying illumination guidance is further constructed for achieving the edge-aware smoothness property of the illumination component. According to different training patterns, we construct CSDNet (paired supervision) and CSDGAN (unpaired supervision) to fully evaluate our designed architecture. We test our method on seven testing benchmarks to conduct plenty of analytical and evaluated experiments. Thanks to our designed context-sensitive decomposition connection, we successfully realized excellent enhanced results, which fully indicates our superiority against existing state-of-the-art approaches. Finally, considering the practical needs for high-efficiency, we develop a lightweight CSDNet (named LiteCSDNet) by reducing the number of channels. Further, by sharing an encoder for these two components, we obtain a more lightweight version (SLiteCSDNet for short). SLiteCSDNet just contains 0.0301M parameters but achieves the almost same performance as CSDNet.      
### 23.Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction  [ :arrow_down: ](https://arxiv.org/pdf/2112.05146.pdf)
>  Diffusion models have recently attained significant interest within the community owing to their strong performance as generative models. Furthermore, its application to inverse problems have demonstrated state-of-the-art performance. Unfortunately, diffusion models have a critical downside - they are inherently slow to sample from, needing few thousand steps of iteration to generate images from pure Gaussian noise. In this work, we show that starting from Gaussian noise is unnecessary. Instead, starting from a single forward diffusion with better initialization significantly reduces the number of sampling steps in the reverse conditional diffusion. This phenomenon is formally explained by the contraction theory of the stochastic difference equations like our conditional diffusion strategy - the alternating applications of reverse diffusion followed by a non-expansive data consistency step. The new sampling strategy, dubbed Come-Closer-Diffuse-Faster (CCDF), also reveals a new insight on how the existing feed-forward neural network approaches for inverse problems can be synergistically combined with the diffusion models. Experimental results with super-resolution, image inpainting, and compressed sensing MRI demonstrate that our method can achieve state-of-the-art reconstruction performance at significantly reduced sampling steps.      
### 24.unrolling palm for sparse semi-blind source separation  [ :arrow_down: ](https://arxiv.org/pdf/2112.05694.pdf)
>  Sparse Blind Source Separation (BSS) has become a well established tool for a wide range of applications - for instance, in astrophysics and remote sensing. Classical sparse BSS methods, such as the Proximal Alternating Linearized Minimization (PALM) algorithm, nevertheless often suffer from a difficult hyperparameter choice, which undermines their results. To bypass this pitfall, we propose in this work to build on the thriving field of algorithm unfolding/unrolling. Unrolling PALM enables to leverage the data-driven knowledge stemming from realistic simulations or ground-truth data by learning both PALM hyperparameters and variables. In contrast to most existing unrolled algorithms, which assume a fixed known dictionary during the training and testing phases, this article further emphasizes on the ability to deal with variable mixing matrices (a.k.a. dictionaries). The proposed Learned PALM (LPALM) algorithm thus enables to perform semi-blind source separation, which is key to increase the generalization of the learnt model in real-world applications. We illustrate the relevance of LPALM in astrophysical multispectral imaging: the algorithm not only needs up to $10^4-10^5$ times fewer iterations than PALM, but also improves the separation quality, while avoiding the cumbersome hyperparameter and initialization choice of PALM. We further show that LPALM outperforms other unrolled source separation methods in the semi-blind setting.      
### 25.Optimal transport and control of active drops  [ :arrow_down: ](https://arxiv.org/pdf/2112.05676.pdf)
>  Understanding the complex patterns in space-time exhibited by active systems has been the subject of much interest in recent times. Complementing this forward problem is the inverse problem of controlling active matter. Here we use optimal control theory to pose the problem of transporting a slender drop of an active fluid and determine the dynamical profile of the active stresses to move it with minimal viscous dissipation. By parametrizing the position and size of the drop using a low-order description based on lubrication theory, we uncover a natural ''gather-move-spread'' strategy that leads to an optimal bound on the maximum achievable displacement of the drop relative to its size. In the continuum setting, the competition between passive surface tension, and active controls generates richer behaviour with futile oscillations and complex drop morphologies that trade internal dissipation against the transport cost to select optimal strategies. Our work combines active hydrodynamics and optimal control in a tractable and interpretable framework, and begins to pave the way for the spatiotemporal manipulation of active matter.      
### 26.An Ensemble 1D-CNN-LSTM-GRU Model with Data Augmentation for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.05666.pdf)
>  In this paper, we propose an ensemble of deep neural networks along with data augmentation (DA) learned using effective speech-based features to recognize emotions from speech. Our ensemble model is built on three deep neural network-based models. These neural networks are built using the basic local feature acquiring blocks (LFAB) which are consecutive layers of dilated 1D Convolutional Neural networks followed by the max pooling and batch normalization layers. To acquire the long-term dependencies in speech signals further two variants are proposed by adding Gated Recurrent Unit (GRU) and Long Short Term Memory (LSTM) layers respectively. All three network models have consecutive fully connected layers before the final softmax layer for classification. The ensemble model uses a weighted average to provide the final classification. We have utilized five standard benchmark datasets: TESS, EMO-DB, RAVDESS, SAVEE, and CREMA-D for evaluation. We have performed DA by injecting Additive White Gaussian Noise, pitch shifting, and stretching the signal level to generalize the models, and thus increasing the accuracy of the models and reducing the overfitting as well. We handcrafted five categories of features: Mel-frequency cepstral coefficients, Log Mel-Scaled Spectrogram, Zero-Crossing Rate, Chromagram, and statistical Root Mean Square Energy value from each audio sample. These features are used as the input to the LFAB blocks that further extract the hidden local features which are then fed to either fully connected layers or to LSTM or GRU based on the model type to acquire the additional long-term contextual representations. LFAB followed by GRU or LSTM results in better performance compared to the baseline model. The ensemble model achieves the state-of-the-art weighted average accuracy in all the datasets.      
### 27.Deep Odometry Systems on Edge with EKF-LoRa Backend for Real-Time Positioning in Adverse Environment  [ :arrow_down: ](https://arxiv.org/pdf/2112.05665.pdf)
>  Ubiquitous positioning for pedestrian in adverse environment has served a long standing challenge. Despite dramatic progress made by Deep Learning, multi-sensor deep odometry systems yet pose a high computational cost and suffer from cumulative drifting errors over time. Thanks to the increasing computational power of edge devices, we propose a novel ubiquitous positioning solution by integrating state-of-the-art deep odometry models on edge with an EKF (Extended Kalman Filter)-LoRa backend. We carefully compare and select three sensor modalities, i.e., an Inertial Measurement Unit (IMU), a millimetre-wave (mmWave) radar, and a thermal infrared camera, and realise their deep odometry inference engines which runs in real-time. A pipeline of deploying deep odometry considering accuracy, complexity, and edge platform is proposed. We design a LoRa link for positional data backhaul and projecting aggregated positions of deep odometry into the global frame. We find that a simple EKF based fusion module is sufficient for generic positioning calibration with over 34% accuracy gains against any standalone deep odometry system. Extensive tests in different environments validate the efficiency and efficacy of our proposed positioning system.      
### 28.On the Relationships between Transform-Learning NMF and Joint-Diagonalization  [ :arrow_down: ](https://arxiv.org/pdf/2112.05664.pdf)
>  Non-negative matrix factorization with transform learning (TL-NMF) is a recent idea that aims at learning data representations suited to NMF. In this work, we relate TL-NMF to the classical matrix joint-diagonalization (JD) problem. We show that, when the number of data realizations is sufficiently large, TL-NMF can be replaced by a two-step approach -- termed as JD+NMF -- that estimates the transform through JD, prior to NMF computation. In contrast, we found that when the number of data realizations is limited, not only is JD+NMF no longer equivalent to TL-NMF, but the inherent low-rank constraint of TL-NMF turns out to be an essential ingredient to learn meaningful transforms for NMF.      
### 29.DPU: DAG Processing Unit for Irregular Graphs with Precision-Scalable Posit Arithmetic in 28nm  [ :arrow_down: ](https://arxiv.org/pdf/2112.05660.pdf)
>  Computation in several real-world applications like probabilistic machine learning, sparse linear algebra, and robotic navigation, can be modeled as irregular directed acyclic graphs (DAGs). The irregular data dependencies in DAGs pose challenges to parallel execution on general-purpose CPUs and GPUs, resulting in severe under-utilization of the hardware. This paper proposes DPU, a specialized processor designed for the efficient execution of irregular DAGs. The DPU is equipped with parallel compute units that execute different subgraphs of a DAG independently. The compute units can synchronize within a cycle using a hardware-supported synchronization primitive, and communicate via an efficient interconnect to a global banked scratchpad. Furthermore, a precision-scalable posit arithmetic unit is developed to enable application-dependent precision. The DPU is taped-out in 28nm CMOS, achieving a speedup of 5.1$\times$ and 20.6$\times$ over state-of-the-art CPU and GPU implementations on DAGs of sparse linear algebra and probabilistic machine learning workloads. This performance is achieved while operating at a power budget of 0.23W, as opposed to 55W and 98W of the CPU and GPU, resulting in a peak efficiency of 538 GOPS/W with DPU, which is 1350$\times$ and 9000$\times$ higher than the CPU and GPU, respectively. Thus, with specialized architecture, DPU enables low-power execution of irregular DAG workloads.      
### 30.Graph-structured tensor optimization for nonlinear density control and mean field games  [ :arrow_down: ](https://arxiv.org/pdf/2112.05645.pdf)
>  In this work we develop a numerical method for solving a type of convex graph-structured tensor optimization problems. This type of problems, which can be seen as a generalization of multi-marginal optimal transport problems with graph-structured costs, appear in many applications. In particular, we show that it can be used to model and solve nonlinear density control problems, including convex dynamic network flow problems and multi-species potential mean field games. The method is based on coordinate ascent in a Lagrangian dual, and under mild assumptions we prove that the algorithm converges globally. Moreover, under a set of stricter assumptions, the algorithm converges R-linearly. To perform the coordinate ascent steps one has to compute projections of the tensor, and doing so by brute force is in general not computationally feasible. Nevertheless, for certain graph structures we derive efficient methods for computing these projections. In particular, these graph structures are the ones that occur in convex dynamic network flow problems and multi-species potential mean field games. We also illustrate the methodology on numerical examples from these problem classes.      
### 31.Decentralized Spectrum Access System: Vision, Challenges, and a Blockchain Solution  [ :arrow_down: ](https://arxiv.org/pdf/2112.05612.pdf)
>  Spectrum access system (SAS) is widely considered the de facto solution to coordinating dynamic spectrum sharing (DSS) and protecting incumbent users. The current SAS paradigm prescribed by the FCC for the CBRS band and standardized by the WInnForum follows a centralized service model in that a spectrum user subscribes to a SAS server for spectrum allocation service. This model, however, neither tolerates SAS server failures (crash or Byzantine) nor resists dishonest SAS administrators, leading to serious concerns on SAS system reliability and trustworthiness. This is especially concerning for the evolving DSS landscape where an increasing number of SAS service providers and heterogeneous user requirements are coming up. To address these challenges, we propose a novel blockchain-based decentralized SAS architecture called BD-SAS that provides SAS services securely and efficiently, without relying on the trust of each individual SAS server for the overall system trustworthiness. In BD-SAS, a global blockchain (G-Chain) is used for spectrum regulatory compliance while smart contract-enabled local blockchains (L-Chains) are instantiated in individual spectrum zones for automating spectrum access assignment per user request. We hope our vision of a decentralized SAS, the BD-SAS architecture, and discussion on future challenges can open up a new direction towards reliable spectrum management in a decentralized manner.      
### 32.Marvin: Innovative Omni-Directional Robotic Assistant for Domestic Environments  [ :arrow_down: ](https://arxiv.org/pdf/2112.05597.pdf)
>  Technology is progressively reshaping the domestic environment as we know it, enhancing home security and the overall ambient quality through smart connected devices. However, demographic shift and pandemics recently demonstrate to cause isolation of elderly people in their houses, generating the need for a reliable assistive figure. Robotic assistants are the new frontier of innovation for domestic welfare. Elderly monitoring is only one of the possible service applications an intelligent robotic platform can handle for collective wellbeing. In this paper, we present Marvin, a novel assistive robot we developed with a modular layer-based architecture, merging a flexible mechanical design with state-of-the-art Artificial Intelligence for perception and vocal control. With respect to previous works on robotic assistants, we propose an omnidirectional platform provided with four mecanum wheels, which enable autonomous navigation in conjunction with efficient obstacle avoidance in cluttered environments. Moreover, we design a controllable positioning device to extend the visual range of sensors and to improve the access to the user interface for telepresence and connectivity. Lightweight deep learning solutions for visual perception, person pose classification and vocal command completely run on the embedded hardware of the robot, avoiding privacy issues arising from private data collection on cloud services.      
### 33.A Review of Indoor Millimeter Wave Device-based Localization and Device-free Sensing Technologies  [ :arrow_down: ](https://arxiv.org/pdf/2112.05593.pdf)
>  The commercial availability of low-cost millimeter wave (mmWave) communication and radar devices is starting to improve the penetration of such technologies in consumer markets, paving the way for large-scale and dense deployments in fifth-generation (5G)-and-beyond as well as 6G networks. At the same time, pervasive mmWave access will enable device localization and device-free sensing with unprecedented accuracy, especially with respect to sub-6 GHz commercial-grade devices. This paper surveys the state of the art in device-based localization and device-free sensing using mmWave communication and radar devices, with a focus on indoor deployments. We first overview key concepts about mmWave signal propagation and system design. Then, we provide a detailed account of approaches and algorithms for localization and sensing enabled by mmWaves. We consider several dimensions in our analysis, including the main objectives, techniques, and performance of each work, whether each research reached some degree of implementation, and which hardware platforms were used for this purpose. We conclude by discussing that better algorithms for consumer-grade devices, data fusion methods for dense deployments, as well as an educated application of machine learning methods are promising, relevant and timely research directions.      
### 34.A Device and Method to Identify Hip, Knee and Ankle Joint Impedance During Walking  [ :arrow_down: ](https://arxiv.org/pdf/2112.05564.pdf)
>  Knowledge on joint impedance during walking in various conditions is relevant for clinical decision making and the development of robotic gait trainers, leg prostheses, leg orthotics, and wearable exoskeletons. Whereas ankle impedance during walking has been experimentally assessed, knee and hip joint impedance during walking have not been identified yet. Here we developed and evaluated a lower limb perturbator to identify hip, knee and ankle joint impedance during treadmill walking. The lower limb perturbator (LOPER) consists of an actuator connected to the thigh via rods. The LOPER allows to apply force perturbations to a free-hanging leg, while standing on the contralateral leg, with a bandwidth of up to 39Hz. While walking in minimal impedance mode, the interaction forces between LOPER and the thigh were low (&lt;5N) and the effect on the walking pattern was smaller than the within-subject variability during normal walking. Using a non-linear multibody dynamical model of swing leg dynamics, the hip, knee and ankle joint impedance were estimated at three time points during the swing phase for nine subjects walking at a speed of 0.5 m/s. The identified model was well able to predict the experimental responses, since the mean variance accounted for was 99%, 96%, and 77%, for the hip, knee and ankle respectively. The averaged across subjects stiffness varied between the three time point within 34-66 Nm/rad, 0-3.5 Nm/rad, and 2.5-24 Nm/rad for the hip, knee and ankle joint respectively. The damping varied between 1.9-4.6 Nms/rad, 0.02-0.14 Nms/rad, and 0.2-2.4 Nms/rad for hip, knee, and ankle respectively. The developed LOPER has a negligible effect on the unperturbed walking pattern and allows to identify hip, knee and ankle joint impedance during the swing phase.      
### 35.Collaborative Learning over Wireless Networks: An Introductory Overview  [ :arrow_down: ](https://arxiv.org/pdf/2112.05559.pdf)
>  In this chapter, we will mainly focus on collaborative training across wireless devices. Training a ML model is equivalent to solving an optimization problem, and many distributed optimization algorithms have been developed over the last decades. These distributed ML algorithms provide data locality; that is, a joint model can be trained collaboratively while the data available at each participating device remains local. This addresses, to some extend, the privacy concern. They also provide computational scalability as they allow exploiting computational resources distributed across many edge devices. However, in practice, this does not directly lead to a linear gain in the overall learning speed with the number of devices. This is partly due to the communication bottleneck limiting the overall computation speed. Additionally, wireless devices are highly heterogeneous in their computational capabilities, and both their computation speed and communication rate can be highly time-varying due to physical factors. Therefore, distributed learning algorithms, particularly those to be implemented at the wireless network edge, must be carefully designed taking into account the impact of time-varying communication network as well as the heterogeneous and stochastic computation capabilities of devices.      
### 36.Shennong: a Python toolbox for audio speech features extraction  [ :arrow_down: ](https://arxiv.org/pdf/2112.05555.pdf)
>  We introduce Shennong, a Python toolbox and command-line utility for speech features extraction. It implements a wide range of well-established state of art algorithms including spectro-temporal filters such as Mel-Frequency Cepstral Filterbanks or Predictive Linear Filters, pre-trained neural networks, pitch estimators as well as speaker normalization methods and post-processing algorithms. Shennong is an open source, easy-to-use, reliable and extensible framework. The use of Python makes the integration to others speech modeling and machine learning tools easy. It aims to replace or complement several heterogeneous software, such as Kaldi or Praat. After describing the Shennong software architecture, its core components and implemented algorithms, this paper illustrates its use on three applications: a comparison of speech features performances on a phones discrimination task, an analysis of a Vocal Tract Length Normalization model as a function of the speech duration used for training and a comparison of pitch estimation algorithms under various noise conditions.      
### 37.Music demixing with the sliCQ transform  [ :arrow_down: ](https://arxiv.org/pdf/2112.05509.pdf)
>  Music source separation is the task of extracting an estimate of one or more isolated sources or instruments (for example, drums or vocals) from musical audio. The task of music demixing or unmixing considers the case where the musical audio is separated into an estimate of all of its constituent sources that can be summed back to the original mixture. The Music Demixing Challenge was created to inspire new demixing research. Open-Unmix (UMX), and the improved variant CrossNet-Open-Unmix (X-UMX), were included in the challenge as the baselines. Both models use the Short-Time Fourier Transform (STFT) as the representation of music signals. The time-frequency uncertainty principle states that the STFT of a signal cannot have maximal resolution in both time and frequency. The tradeoff in time-frequency resolution can significantly affect music demixing results. Our proposed adaptation of UMX replaced the STFT with the sliCQT, a time-frequency transform with varying time-frequency resolution. Unfortunately, our model xumx-sliCQ achieved lower demixing scores than UMX.      
### 38.Network Compression via Central Filter  [ :arrow_down: ](https://arxiv.org/pdf/2112.05493.pdf)
>  Neural network pruning has remarkable performance for reducing the complexity of deep network models. Recent network pruning methods usually focused on removing unimportant or redundant filters in the network. In this paper, by exploring the similarities between feature maps, we propose a novel filter pruning method, Central Filter (CF), which suggests that a filter is approximately equal to a set of other filters after appropriate adjustments. Our method is based on the discovery that the average similarity between feature maps changes very little, regardless of the number of input images. Based on this finding, we establish similarity graphs on feature maps and calculate the closeness centrality of each node to select the Central Filter. Moreover, we design a method to directly adjust weights in the next layer corresponding to the Central Filter, effectively minimizing the error caused by pruning. Through experiments on various benchmark networks and datasets, CF yields state-of-the-art performance. For example, with ResNet-56, CF reduces approximately 39.7% of FLOPs by removing 47.1% of the parameters, with even 0.33% accuracy improvement on CIFAR-10. With GoogLeNet, CF reduces approximately 63.2% of FLOPs by removing 55.6% of the parameters, with only a small loss of 0.35% in top-1 accuracy on CIFAR-10. With ResNet-50, CF reduces approximately 47.9% of FLOPs by removing 36.9% of the parameters, with only a small loss of 1.07% in top-1 accuracy on ImageNet. The codes can be available at <a class="link-external link-https" href="https://github.com/8ubpshLR23/Central-Filter" rel="external noopener nofollow">this https URL</a>.      
### 39.Camera Condition Monitoring and Readjustment by means of Noise and Blur  [ :arrow_down: ](https://arxiv.org/pdf/2112.05456.pdf)
>  Autonomous vehicles and robots require increasingly more robustness and reliability to meet the demands of modern tasks. These requirements specially apply to cameras because they are the predominant sensors to acquire information about the environment and support actions. A camera must maintain proper functionality and take automatic countermeasures if necessary. However, there is little work that examines the practical use of a general condition monitoring approach for cameras and designs countermeasures in the context of an envisaged high-level application. We propose a generic and interpretable self-health-maintenance framework for cameras based on data- and physically-grounded models. To this end, we determine two reliable, real-time capable estimators for typical image effects of a camera in poor condition (defocus blur, motion blur, different noise phenomena and most common combinations) by comparing traditional and retrained machine learning-based approaches in extensive experiments. Furthermore, we demonstrate how one can adjust the camera parameters (e.g., exposure time and ISO gain) to achieve optimal whole-system capability based on experimental (non-linear and non-monotonic) input-output performance curves, using object detection, motion blur and sensor noise as examples. Our framework not only provides a practical ready-to-use solution to evaluate and maintain the health of cameras, but can also serve as a basis for extensions to tackle more sophisticated problems that combine additional data sources (e.g., sensor or environment parameters) empirically in order to attain fully reliable and robust machines.      
### 40.Dandelion-Picking Legged Robot  [ :arrow_down: ](https://arxiv.org/pdf/2112.05383.pdf)
>  Agriculture is currently undergoing a robotics revolution, but robots using wheeled or treads suffer from known disadvantages: they are unable to move over rubble and steep or loose ground, and they trample continuous strips of land thereby reducing the viable crop area. Legged robots offer an alternative, but existing commercial legged robots are complex, expensive, and hard to maintain. We propose the use of multilegged robots using low-degree-of-freedom (low-DoF) legs and demonstrate our approach with a lawn pest control task: picking dandelions using our inexpensive and easy to fabricate BigANT robot. For this task we added an RGB-D camera to the robot. We also rigidly attached a flower picking appendage to the robot chassis. Thanks to the versatility of legs, the robot could be programmed to perform a ``swooping'' motion that allowed this 0-DoF appendage to pluck the flowers. Our results suggest that robots with six or more low-DoF legs may hit a sweet-spot for legged robots designed for agricultural applications by providing enough mobility, stability, and low complexity.      
### 41.Dynamic hardware system for cascade SVM classification of melanoma  [ :arrow_down: ](https://arxiv.org/pdf/2112.05322.pdf)
>  Melanoma is the most dangerous form of skin cancer, which is responsible for the majority of skin cancer-related deaths. Early diagnosis of melanoma can significantly reduce mortality rates and treatment costs. Therefore, skin cancer specialists are using image-based diagnostic tools for detecting melanoma earlier. We aim to develop a handheld device featured with low cost and high performance to enhance early detection of melanoma at the primary healthcare. But, developing this device is very challenging due to the complicated computations required by the embedded diagnosis system. Thus, we aim to exploit the recent hardware technology in reconfigurable computing to achieve a high-performance embedded system at low cost. Support vector machine (SVM) is a common classifier that shows high accuracy for classifying melanoma within the diagnosis system and is considered as the most compute-intensive task in the system. In this paper, we propose a dynamic hardware system for implementing a cascade SVM classifier on FPGA for early melanoma detection. A multi-core architecture is proposed to implement a two-stage cascade classifier using two classifiers with accuracies of 98% and 73%. The hardware implementation results were optimized by using the dynamic partial reconfiguration technology, where very low resource utilization of 1% slices and power consumption of 1.5 W were achieved. Consequently, the implemented dynamic hardware system meets vital embedded system constraints of high performance and low cost, resource utilization, and power consumption, while achieving efficient classification with high accuracy.      
### 42.Robustness Certificates for Implicit Neural Networks: A Mixed Monotone Contractive Approach  [ :arrow_down: ](https://arxiv.org/pdf/2112.05310.pdf)
>  Implicit neural networks are a general class of learning models that replace the layers in traditional feedforward models with implicit algebraic equations. Compared to traditional learning models, implicit networks offer competitive performance and reduced memory consumption. However, they can remain brittle with respect to input adversarial perturbations. <br>This paper proposes a theoretical and computational framework for robustness verification of implicit neural networks; our framework blends together mixed monotone systems theory and contraction theory. First, given an implicit neural network, we introduce a related embedded network and show that, given an $\ell_\infty$-norm box constraint on the input, the embedded network provides an $\ell_\infty$-norm box overapproximation for the output of the given network. Second, using $\ell_{\infty}$-matrix measures, we propose sufficient conditions for well-posedness of both the original and embedded system and design an iterative algorithm to compute the $\ell_{\infty}$-norm box robustness margins for reachability and classification problems. Third, of independent value, we propose a novel relative classifier variable that leads to tighter bounds on the certified adversarial robustness in classification problems. Finally, we perform numerical simulations on a Non-Euclidean Monotone Operator Network (NEMON) trained on the MNIST dataset. In these simulations, we compare the accuracy and run time of our mixed monotone contractive approach with the existing robustness verification approaches in the literature for estimating the certified adversarial robustness.      
### 43.Long-Range Thermal 3D Perception in Low Contrast Environments  [ :arrow_down: ](https://arxiv.org/pdf/2112.05280.pdf)
>  This report discusses the results of SBIR Phase I effort to prove the feasibility of dramatic improvement of the microbolometer-based Long Wave Infrared (LWIR) detectors sensitivity, especially for the 3D measurements. The resulting low SWaP-C thermal depth-sensing system will enable the situational awareness of Autonomous Air Vehicles for Advanced Air Mobility (AAM). It will provide robust 3D information of the surrounding environment, including low-contrast static and moving objects, at far distances in degraded visual conditions and GPS-denied areas. Our multi-sensor 3D perception enabled by COTS uncooled thermal sensors mitigates major weakness of LWIR sensors - low contrast by increasing the system sensitivity over an order of magnitude. <br>There were no available thermal image sets suitable for evaluating this technology, making datasets acquisition our first goal. We discuss the design and construction of the prototype system with sixteen 640pix x 512pix LWIR detectors, camera calibration to subpixel resolution, capture, and process synchronized image. The results show the 3.84x contrast increase for intrascene-only data and an additional 5.5x - with the interscene accumulation, reaching system noise-equivalent temperature difference (NETD) of 1.9 mK with the 40 mK sensors.      
### 44.Label-free virtual HER2 immunohistochemical staining of breast tissue using deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.05240.pdf)
>  The immunohistochemical (IHC) staining of the human epidermal growth factor receptor 2 (HER2) biomarker is widely practiced in breast tissue analysis, preclinical studies and diagnostic decisions, guiding cancer treatment and investigation of pathogenesis. HER2 staining demands laborious tissue treatment and chemical processing performed by a histotechnologist, which typically takes one day to prepare in a laboratory, increasing analysis time and associated costs. Here, we describe a deep learning-based virtual HER2 IHC staining method using a conditional generative adversarial network that is trained to rapidly transform autofluorescence microscopic images of unlabeled/label-free breast tissue sections into bright-field equivalent microscopic images, matching the standard HER2 IHC staining that is chemically performed on the same tissue sections. The efficacy of this virtual HER2 staining framework was demonstrated by quantitative analysis, in which three board-certified breast pathologists blindly graded the HER2 scores of virtually stained and immunohistochemically stained HER2 whole slide images (WSIs) to reveal that the HER2 scores determined by inspecting virtual IHC images are as accurate as their immunohistochemically stained counterparts. A second quantitative blinded study performed by the same diagnosticians further revealed that the virtually stained HER2 images exhibit a comparable staining quality in the level of nuclear detail, membrane clearness, and absence of staining artifacts with respect to their immunohistochemically stained counterparts. This virtual HER2 staining framework bypasses the costly, laborious, and time-consuming IHC staining procedures in laboratory, and can be extended to other types of biomarkers to accelerate the IHC tissue staining used in life sciences and biomedical workflow.      
### 45.Reinforcement Learning with Almost Sure Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2112.05198.pdf)
>  In this work we address the problem of finding feasible policies for Constrained Markov Decision Processes under probability one constraints. We argue that stationary policies are not sufficient for solving this problem, and that a rich class of policies can be found by endowing the controller with a scalar quantity, so called budget, that tracks how close the agent is to violating the constraint. We show that the minimal budget required to act safely can be obtained as the smallest fixed point of a Bellman-like operator, for which we analyze its convergence properties. We also show how to learn this quantity when the true kernel of the Markov decision process is not known, while providing sample-complexity bounds. The utility of knowing this minimal budget relies in that it can aid in the search of optimal or near-optimal policies by shrinking down the region of the state space the agent must navigate. Simulations illustrate the different nature of probability one constraints against the typically used constraints in expectation.      
### 46.Classification of Anuran Frog Species Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.05148.pdf)
>  Acoustic classification of frogs has gotten a lot of attention recently due to its potential applicability in ecological investigations. Numerous studies have been presented for identifying frog species, although the majority of recorded species are thought to be monotypic. The purpose of this study is to demonstrate a method for classifying various frog species using an audio recording. To be more exact, continuous frog recordings are cut into audio snippets first (10 seconds). Then, for each ten-second recording, several time-frequency representations are constructed. Following that, rather than using manually created features, Machine Learning methods are employed to classify the frog species. Data reduction techniques; Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are used to extract the most important features before classification. Finally, to validate our classification accuracy, cross validation and prediction accuracy are used. Experimental results show that PCA extracted features that achieved better classification accuracy both with cross validation and prediction accuracy.      
### 47.Edge-aware Guidance Fusion Network for RGB Thermal Scene Parsing  [ :arrow_down: ](https://arxiv.org/pdf/2112.05144.pdf)
>  RGB thermal scene parsing has recently attracted increasing research interest in the field of computer vision. However, most existing methods fail to perform good boundary extraction for prediction maps and cannot fully use high level features. In addition, these methods simply fuse the features from RGB and thermal modalities but are unable to obtain comprehensive fused features. To address these problems, we propose an edge-aware guidance fusion network (EGFNet) for RGB thermal scene parsing. First, we introduce a prior edge map generated using the RGB and thermal images to capture detailed information in the prediction map and then embed the prior edge information in the feature maps. To effectively fuse the RGB and thermal information, we propose a multimodal fusion module that guarantees adequate cross-modal fusion. Considering the importance of high level semantic information, we propose a global information module and a semantic information module to extract rich semantic information from the high-level features. For decoding, we use simple elementwise addition for cascaded feature fusion. Finally, to improve the parsing accuracy, we apply multitask deep supervision to the semantic and boundary maps. Extensive experiments were performed on benchmark datasets to demonstrate the effectiveness of the proposed EGFNet and its superior performance compared with state of the art methods. The code and results can be found at <a class="link-external link-https" href="https://github.com/ShaohuaDong2021/EGFNet" rel="external noopener nofollow">this https URL</a>.      
