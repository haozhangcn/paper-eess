# ArXiv eess --Wed, 1 Dec 2021
### 1.Radio-Frequency Multi-Mode OAM Detection Based on UCA Samples Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.15638.pdf)
>  Orbital angular momentum (OAM) at radio-frequency provides a novel approach of multiplexing a set of orthogonal modes on the same frequency channel to achieve high spectral efficiencies. However, classical phase gradient-based OAM mode detection methods require perfect alignment of transmit and receive antennas, which greatly challenges the practical application of OAM communications. In this paper, we first show the effect of non-parallel misalignment on the OAM phase structure, and then propose the OAM mode detection method based on uniform circular array (UCA) samples learning for the more general alignment or non-parallel misalignment case. Specifically, we applied three classifiers: K-nearest neighbor (KNN), support vector machine (SVM), and back-propagation neural network (BPNN) to both single-mode and multi-mode OAM detection. The simulation results validate that the proposed learning-based OAM mode detection methods are robust to misalignment errors and especially BPNN classifier has the best generalization performance.      
### 2.Generating gapless land surface temperature with a high spatio-temporal resolution by fusing multi-source satellite-observed and model-simulated data  [ :arrow_down: ](https://arxiv.org/pdf/2111.15636.pdf)
>  Land surface temperature (LST) is a key parameter when monitoring land surface processes. However, cloud contamination and the tradeoff between the spatial and temporal resolutions greatly impede the access to high-quality thermal infrared (TIR) remote sensing data. Despite the massive efforts made to solve these dilemmas, it is still difficult to generate LST estimates with concurrent spatial completeness and a high spatio-temporal resolution. Land surface models (LSMs) can be used to simulate gapless LST with a high temporal resolution, but this usually comes with a low spatial resolution. In this paper, we present an integrated temperature fusion framework for satellite-observed and LSM-simulated LST data to map gapless LST at a 60-m spatial resolution and half-hourly temporal resolution. The global linear model (GloLM) model and the diurnal land surface temperature cycle (DTC) model are respectively performed as preprocessing steps for sensor and temporal normalization between the different LST data. The Landsat LST, Moderate Resolution Imaging Spectroradiometer (MODIS) LST, and Community Land Model Version 5.0 (CLM 5.0)-simulated LST are then fused using a filter-based spatio-temporal integrated fusion model. Evaluations were implemented in an urban-dominated region (the city of Wuhan in China) and a natural-dominated region (the Heihe River Basin in China), in terms of accuracy, spatial variability, and diurnal temporal dynamics. Results indicate that the fused LST is highly consistent with actual Landsat LST data (in situ LST measurements), in terms of a Pearson correlation coefficient of 0.94 (0.97-0.99), a mean absolute error of 0.71-0.98 K (0.82-3.17 K), and a root-mean-square error of 0.97-1.26 K (1.09-3.97 K).      
### 3.A Nonlinear Autoregressive Neural Network for Interference Prediction and Resource Allocation in URLLC Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2111.15630.pdf)
>  Ultra reliable low latency communications (URLLC) is a new service class introduced in 5G which is characterized by strict reliability $(1-10^{-5})$ and low latency requirements (1 ms). To meet these requisites, several strategies like overprovisioning of resources and channel-predictive algorithms have been developed. This paper describes the application of a Nonlinear Autoregressive Neural Network (NARNN) as a novel approach to forecast interference levels in a wireless system for the purpose of efficient resource allocation. Accurate interference forecasts also grant the possibility of meeting specific outage probability requirements in URLLC scenarios. Performance of this proposal is evaluated upon the basis of NARNN predictions accuracy and system resource usage. Our proposed approach achieved a promising mean absolute percentage error of 7.8 % on interference predictions and also reduced the resource usage in up to 15 % when compared to a recently proposed interference prediction algorithm.      
### 4.GEOSCAN: Global Earth Observation using Swarm of Coordinated Autonomous Nanosats  [ :arrow_down: ](https://arxiv.org/pdf/2111.15627.pdf)
>  The climate crisis we are facing calls for significant improvements in our understanding of natural phenomena, with clouds being identified as a dominant source of uncertainty. To this end, the emerging field of 3D computed cloud tomography (CCT) aims to more precisely characterize clouds by utilizing multi-dimensional imaging to reconstruct their outer and inner structure. In this paper, we propose a future Earth observation mission concept, driven by the needs of CCT, that operates constellation of NanoSats to provide multi-angular, spectrally-resolved, spatial and temporal scientific measurements of natural atmospheric phenomena. Our proposed mission, GEOSCAN, will on-board active steering capability to rapidly reconfigure networked swarm of autonomous Nanosats to track evolving phenomena of interest, on-demand, in real-time. We present the structure of the GEOSCAN constellation and discuss details of the mission concept from both science and engineering perspectives. On the science side, we outline the types of remote Earth observation measurements that GEOSCAN enables beyond the state-of-the-art, and how such measurements translate to improvements in CCT that can lead to reduction in uncertainty of the global climate models (GCMs). From the engineering side, we investigate feasibility of the concept starting from hardware components of the NanoSat that form the basis of the constellation. In particular, we focus on the active steering capability of the GEOSCAN with algorithmic approaches that enable coordination from new software. We identify technology gaps that need to be bridged and discuss other aspects of the mission that require in-depth analysis to further mature the concept.      
### 5.Variational Autoencoders for Studying the Manifold of Precoding Matrices with High Spectral Efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2111.15626.pdf)
>  In multiple-input multiple-output (MIMO) wireless communications systems, neural networks have been employed for channel decoding, detection, channel estimation, and resource management. In this paper, we look at how to use a variational autoencoder to find a precoding matrix with a high Spectral Efficiency (SE). To identify efficient precoding matrices, an optimization approach is used. Our objective is to create a less time-consuming algorithm with minimum quality degradation. To build precoding matrices, we employed two forms of variational autoencoders: conventional variational autoencoders (VAE) and conditional variational autoencoders (CVAE). Both methods may be used to study a wide range of optimal precoding matrices. To the best of our knowledge, the development of precoding matrices for the spectral efficiency objective function (SE) utilising VAE and CVAE methods is being published for the first time.      
### 6.Mean Square Performance of a family of Adaptive Algorithms for colored noise  [ :arrow_down: ](https://arxiv.org/pdf/2111.15625.pdf)
>  In real-time applications the characteristics and properties of a signal vary inconsistently. So, to maintain the integrity of such signals there is a need for effective adaptive filters. The conventional Least Mean Squared(LMS) algorithm is widely used because of its computational simplicity and ease of implementation. But, its convergence speed rapidly reduces when colored noise is present in the signal. Affine projection(AP) algorithms are used to speed up the convergence but have high computational costs. In this paper, the mean square performance of LMS and AP algorithms is analyzed when subject to white noise and colored noise.      
### 7.Scalable Machine Learning Architecture for Neonatal Seizure Detection on Ultra-Edge Devices  [ :arrow_down: ](https://arxiv.org/pdf/2111.15569.pdf)
>  Neonatal seizures are a commonly encountered neurological condition. They are the first clinical signs of a serious neurological disorder. Thus, rapid recognition and treatment are necessary to prevent serious fatalities. The use of electroencephalography (EEG) in the field of neurology allows precise diagnosis of several medical conditions. However, interpreting EEG signals needs the attention of highly specialized staff since the infant brain is developmentally immature during the neonatal period. Detecting seizures on time could potentially prevent the negative effects on the neurocognitive development of the infants. In recent years, neonatal seizure detection using machine learning algorithms have been gaining traction. Since there is a need for the classification of bio-signals to be computationally inexpensive in the case of seizure detection, this research presents a machine learning (ML) based architecture that operates with comparable predictive performance as previous models but with minimum level configuration. The proposed classifier was trained and tested on a public dataset of NICU seizures recorded at the Helsinki University Hospital. Our architecture achieved a best sensitivity of 87%, which is 6% more than that of the standard ML model chosen in this study. The model size of the ML classifier is optimized to just 4.84 KB with minimum prediction time of 182.61 milliseconds, thus enabling it to be deployed on wearable ultra-edge devices for quick and accurate response and obviating the need for cloud-based and other such exhaustive computational methods.      
### 8.ViSTRA3: Video Coding with Deep Parameter Adaptation and Post Processing  [ :arrow_down: ](https://arxiv.org/pdf/2111.15536.pdf)
>  This paper presents a deep learning-based video compression framework (ViSTRA3). The proposed framework intelligently adapts video format parameters of the input video before encoding, subsequently employing a CNN at the decoder to restore their original format and enhance reconstruction quality. ViSTRA3 has been integrated with the H.266/VVC Test Model VTM 14.0, and evaluated under the Joint Video Exploration Team Common Test Conditions. BjÃ¸negaard Delta (BD) measurement results show that the proposed framework consistently outperforms the original VVC VTM, with average BD-rate savings of 1.8% and 3.7% based on the assessment of PSNR and VMAF.      
### 9.Gram Barcodes for Histopathology Tissue Texture Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2111.15519.pdf)
>  Recent advances in digital pathology have led to the need for Histopathology Image Retrieval (HIR) systems that search through databases of biopsy images to find similar cases to a given query image. These HIR systems allow pathologists to effortlessly and efficiently access thousands of previously diagnosed cases in order to exploit the knowledge in the corresponding pathology reports. Since HIR systems may have to deal with millions of gigapixel images, the extraction of compact and expressive image features must be available to allow for efficient and accurate retrieval. In this paper, we propose the application of Gram barcodes as image features for HIR systems. Unlike most feature generation schemes, Gram barcodes are based on high-order statistics that describe tissue texture by summarizing the correlations between different feature maps in layers of convolutional neural networks. We run HIR experiments on three public datasets using a pre-trained VGG19 network for Gram barcode generation and showcase highly competitive results.      
### 10.Assessment of Data Consistency through Cascades of Independently Recurrent Inference Machines for fast and robust accelerated MRI reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2111.15498.pdf)
>  Interpretability and robustness are imperative for integrating Machine Learning methods for accelerated Magnetic Resonance Imaging (MRI) reconstruction in clinical applications. Doing so would allow fast high-quality imaging of anatomy and pathology. Data Consistency (DC) is crucial for generalization in multi-modal data and robustness in detecting pathology. This work proposes the Cascades of Independently Recurrent Inference Machines (CIRIM) to assess DC through unrolled optimization, implicitly by gradient descent and explicitly by a designed term. We perform extensive comparison of the CIRIM to other unrolled optimization methods, being the End-to-End Variational Network (E2EVN) and the RIM, and to the UNet and Compressed Sensing (CS). Evaluation is done in two stages. Firstly, learning on multiple trained MRI modalities is assessed, i.e., brain data with ${T_1}$-weighting and FLAIR contrast, and ${T_2}$-weighted knee data. Secondly, robustness is tested on reconstructing pathology through white matter lesions in 3D FLAIR MRI data of relapsing remitting Multiple Sclerosis (MS) patients. Results show that the CIRIM performs best when implicitly enforcing DC, while the E2EVN requires explicitly formulated DC. The CIRIM shows the highest lesion contrast resolution in reconstructing the clinical MS data. Performance improves by approximately 11% compared to CS, while the reconstruction time is twenty times reduced.      
### 11.Fully Automatic Deep Learning Framework for Pancreatic Ductal Adenocarcinoma Detection on Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2111.15409.pdf)
>  Early detection improves prognosis in pancreatic ductal adenocarcinoma (PDAC) but is challenging as lesions are often small and poorly defined on contrast-enhanced computed tomography scans (CE-CT). Deep learning can facilitate PDAC diagnosis, however current models still fail to identify small (&lt;2cm) lesions. In this study, state-of-the-art deep learning models were used to develop an automatic framework for PDAC detection, focusing on small lesions. Additionally, the impact of integrating surrounding anatomy was investigated. CE-CT scans from a cohort of 119 pathology-proven PDAC patients and a cohort of 123 patients without PDAC were used to train a nnUnet for automatic lesion detection and segmentation (\textit{nnUnet\_T}). Two additional nnUnets were trained to investigate the impact of anatomy integration: (1) segmenting the pancreas and tumor (\textit{nnUnet\_TP}), (2) segmenting the pancreas, tumor, and multiple surrounding anatomical structures (\textit{nnUnet\_MS}). An external, publicly available test set was used to compare the performance of the three networks. The \textit{nnUnet\_MS} achieved the best performance, with an area under the receiver operating characteristic curve of 0.91 for the whole test set and 0.88 for tumors &lt;2cm, showing that state-of-the-art deep learning can detect small PDAC and benefits from anatomy information.      
### 12.Monotone one-port circuits  [ :arrow_down: ](https://arxiv.org/pdf/2111.15407.pdf)
>  Maximal monotonicity is explored as a generalization of the linear theory of passivity, aiming at an algorithmic input/output analysis of physical models. The theory is developed for maximal monotone one-port circuits, formed by the series and parallel interconnection of basic elements. An algorithmic method is presented for solving the periodic output of a periodically driven circuit using a maximal monotone splitting algorithm, which allows computation to be separated for each circuit component. A new splitting algorithm is presented, which applies to any monotone circuit defined as a port interconnection of monotone elements.      
### 13.Two-dimensional flow field measurement of sediment-laden flow based on ultrasound image velocimetry  [ :arrow_down: ](https://arxiv.org/pdf/2111.15395.pdf)
>  This paper proposes a novel particle image velocimetry (PIV) technique to generate an instantaneous two-dimensional velocity field for sediment-laden fluid based on the optical flow algorithm of ultrasound imaging. In this paper, an ultrasonic PIV (UPIV) system is constructed by integrating a medical ultrasound instrument and an ultrasonic particle image velocimetry algorithm. The medical ultrasound instrument with a phased sensor array is used to acquire acoustic echo signals and generate two-dimensional underwater ultrasound images. Based on the optical flow field, the instantaneous velocity of the particles in water corresponding to the pixels in the ultrasonic particle images is derived from the grayscale change between adjacent images under the L-K local constraint, and finally, the two-dimensional flow field is obtained. Through multiple sets of experiments, the proposed algorithm is verified. The experimental results are compared with those of the conventional cross-correlation algorithms. The results show that the L-K optical flow method can not only obtain the underwater velocity field accurately but also has the advantages of good smoothness and extensive suitability, especially for the flow field measurement in sediment-laden fluid than conventional algorithms.      
### 14.Transient Stability of Low-Inertia Power Systems with Inverter-Based Generation  [ :arrow_down: ](https://arxiv.org/pdf/2111.15380.pdf)
>  This paper studies the transient stability of low-inertia power systems with inverter-based generation (IBG) and proposes a sufficient stability criterion. In low-inertia grids, interactions are induced between electromagnetic dynamics of the IBG and electromechanical dynamics of the synchronous generator (SG) under fault. For this, a hybrid IBG-SG system is established and a delta-power-frequency model is developed. Based on the model, new mechanisms different from conventional power systems are discovered from the energy perspective. Firstly, two types of loss of synchronization (LOS) are identified, depending on the relative power imbalance due to the mismatch between the inertia of IBG and SG under fault. Secondly, the relative angle and frequency will jump at the moment of a fault, thus affecting the energy of the system. Thirdly, the cosine damping coefficient results in a positive energy dissipation, therefore contributing to the stabilization of the system. A unified criterion for identifying both two types of LOS is proposed employing the energy function method. The criterion is proved to be a sufficient stability condition in addressing the effects of the jumps and the cosine damping. Simulation results are provided to verify the new mechanisms and effectiveness of the criterion.      
### 15.Passivity of Electrical Transmission Networks modelled using Rectangular and Polar D-Q variables  [ :arrow_down: ](https://arxiv.org/pdf/2111.15377.pdf)
>  The increasing penetration of converter-interfaced distributed energy resources has brought out the need to develop decentralized criteria that would ensure the small-signal stability of the inter-connected system. Passivity of the D-Q admittance or impedance is a promising candidate for such an approach. It is facilitated by the inherent passivity of the D-Q impedance of an electrical network. However, the passivity conditions are generally restrictive and cannot be complied with in the low frequency range by the D-Q admittance of devices that follow typical power control strategies. However, this does not imply that the system is unstable. Therefore, alternative formulations that use polar variables (magnitude/phase angle of voltages and real/reactive power injection instead of the D-Q components of voltages and currents) are investigated. Passivity properties of the electrical network using these different formulations are brought out in this paper through analytical results and illustrative examples.      
### 16.Gaussian Mechanisms Against Statistical Inference: Synthesis Tools  [ :arrow_down: ](https://arxiv.org/pdf/2111.15307.pdf)
>  In this manuscript, we provide a set of tools (in terms of semidefinite programs) to synthesize Gaussian mechanisms to maximize privacy of databases. Information about the database is disclosed through queries requested by (potentially) adversarial users. We aim to keep part of the database private (private sensitive information); however, disclosed data could be used to estimate private information. To avoid an accurate estimation by the adversaries, we pass the requested data through distorting (privacy-preserving) mechanisms before transmission and send the distorted data to the user. These mechanisms consist of a coordinate transformation and an additive dependent Gaussian vector. We formulate the synthesis of distorting mechanisms in terms of semidefinite programs in which we seek to minimize the mutual information (our privacy metric) between private data and the disclosed distorted data given a desired distortion level -- how different actual and distorted data are allowed to be.      
### 17.Double Fuzzy Probabilistic Interval Linguistic Term Set and a Dynamic Fuzzy Decision Making Model based on Markov Process with tts Application in Multiple Criteria Group Decision Making  [ :arrow_down: ](https://arxiv.org/pdf/2111.15255.pdf)
>  The probabilistic linguistic term has been proposed to deal with probability distributions in provided linguistic evaluations. However, because it has some fundamental defects, it is often difficult for decision-makers to get reasonable information of linguistic evaluations for group decision making. In addition, weight information plays a significant role in dynamic information fusion and decision making process. However, there are few research methods to determine the dynamic attribute weight with time. In this paper, I propose the concept of double fuzzy probability interval linguistic term set (DFPILTS). Firstly, fuzzy semantic integration, DFPILTS definition, its preference relationship, some basic algorithms and aggregation operators are defined. Then, a fuzzy linguistic Markov matrix with its network is developed. Then, a weight determination method based on distance measure and information entropy to reducing the inconsistency of DFPILPR and obtain collective priority vector based on group consensus is developed. Finally, an aggregation-based approach is developed, and an optimal investment case from a financial risk is used to illustrate the application of DFPILTS and decision method in multi-criteria decision making.      
### 18.Contrastive Learning for Local and Global Learning MRI Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2111.15200.pdf)
>  Magnetic Resonance Imaging (MRI) is an important medical imaging modality, while it requires a long acquisition time. To reduce the acquisition time, various methods have been proposed. However, these methods failed to reconstruct images with a clear structure for two main reasons. Firstly, similar patches widely exist in MR images, while most previous deep learning-based methods ignore this property and only adopt CNN to learn local information. Secondly, the existing methods only use clear images to constrain the upper bound of the solution space, while the lower bound is not constrained, so that a better parameter of the network cannot be obtained. To address these problems, we propose a Contrastive Learning for Local and Global Learning MRI Reconstruction Network (CLGNet). Specifically, according to the Fourier theory, each value in the Fourier domain is calculated from all the values in Spatial domain. Therefore, we propose a Spatial and Fourier Layer (SFL) to simultaneously learn the local and global information in Spatial and Fourier domains. Moreover, compared with self-attention and transformer, the SFL has a stronger learning ability and can achieve better performance in less time. Based on the SFL, we design a Spatial and Fourier Residual block as the main component of our model. Meanwhile, to constrain the lower bound and upper bound of the solution space, we introduce contrastive learning, which can pull the result closer to the clear image and push the result further away from the undersampled image. Extensive experimental results on different datasets and acceleration rates demonstrate that the proposed CLGNet achieves new state-of-the-art results.      
### 19.Wideband Beamforming with Rainbow Beam Training using Reconfigurable True-Time-Delay Arrays for Millimeter-Wave Wireless  [ :arrow_down: ](https://arxiv.org/pdf/2111.15191.pdf)
>  The decadal research in integrated true-time-delay arrays have seen organic growth enabling realization of wideband beamformers for large arrays with wide aperture widths. This article introduces highly reconfigurable delay elements implementable at analog or digital baseband that enables multiple SSP functions including wideband beamforming, wideband interference cancellation, and fast beam training. Details of the beam-training algorithm, system design considerations, system architecture and circuits with large delay range-to-resolution ratios are presented leveraging integrated delay compensation techniques. The article lays out the framework for true-time-delay based arrays in next-generation network infrastructure supporting 3D beam training in planar arrays, low latency massive multiple access, and emerging wireless communications standards.      
### 20.SAVER: Safe Learning-Based Controller for Real-Time Voltage Regulation  [ :arrow_down: ](https://arxiv.org/pdf/2111.15152.pdf)
>  Fast and safe voltage regulation algorithms can serve as fundamental schemes for achieving a high level of renewable penetration in the modern distribution power grids. Faced with uncertain or even unknown distribution grid models and fast-changing power injections, model-free deep reinforcement learning (DRL) algorithms have been proposed to find the reactive power injections for inverters while optimizing the voltage profiles. However, such data-driven controllers can not guarantee satisfaction of the hard operational constraints, such as maintaining voltage profiles within a certain range of the nominal value. To this end, we propose SAVER: SAfe VoltagE Regulator, which is composed of an RL learner and a specifically designed, computational efficient safety projection layer. SAVER provides a plug-and-play interface for a set of DRL algorithms that guarantees the system voltages to be within safe bounds. Numerical simulations on real-world data validate the performance of the proposed algorithm.      
### 21.State-of-Charge Aware EV Charging  [ :arrow_down: ](https://arxiv.org/pdf/2111.15147.pdf)
>  Recent proliferation in electric vehicles (EVs) are posing profound impacts over the operation of electrical grids. In particular, due to the physical constraints on charging stations' capacity and uncertainty in charging demand, it becomes an emerging challenge to design high performance scheduling algorithms to better serve charging sessions. In this paper, we design a predictive charging controller by actively incorporating each EV's state-of-charge (SOC) information, which has strong effects on the utilization of dispatchable power during peak hours. Simulation results on both synthetic and real-world EV session and charging demand data demonstrate the proposed algorithm's benefits on maximizing charging throughput and achieving higher rate of feasible charging sessions while satisfying battery and station physical constraints at the same time.      
### 22.Manifold Optimization Methods for Hybrid beamforming in mmWave Dual-Function Radar-Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2111.15102.pdf)
>  As a cost-effective alternative, hybrid analog and digital beamforming architecture is a promising scheme for millimeter wave (mmWave) system. This paper considers two hybrid beamforming architectures, i.e. the partially-connected and fully-connected structures, for mmWave dual-function radar communication (DFRC) system, where the transmitter communicates with the downlink users and detects radar targets simultaneously. The optimization problems are formulated by minimizing a weighted summation of radar and communication performance, subject to constant modulus and power constraints. To tackle the non-convexities caused by the two resultant problems, effective Riemannian optimization algorithms are proposed. Specifically, for the fully-connected structure, a manifold algorithm based on the alternating direction method of multipliers (ADMM) is developed. While for the partially-connected structure, a low-complexity Riemannian product manifold trust region (RPM-TR) algorithm is proposed to approach the near-optional solution. Numerical simulations are provided to demonstrate the effectiveness of the proposed methods.      
### 23.Network Traffic Shaping for Enhancing Privacy in IoT Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.14992.pdf)
>  Motivated by privacy issues caused by inference attacks on user activities in the packet sizes and timing information of Internet of Things (IoT) network traffic, we establish a rigorous event-level differential privacy (DP) model on infinite packet streams. We propose a memoryless traffic shaping mechanism satisfying a first-come-first-served queuing discipline that outputs traffic dependent on the input using a DP mechanism. We show that in special cases the proposed mechanism recovers existing shapers which standardize the output independently from the input. To find the optimal shapers for given levels of privacy and transmission efficiency, we formulate the constrained problem of minimizing the expected delay per packet and propose using the expected queue size across time as a proxy. We further show that the constrained minimization is a convex program. We demonstrate the effect of shapers on both synthetic data and packet traces from actual IoT devices. The experimental results reveal inherent privacy-overhead tradeoffs: more shaping overhead provides better privacy protection. Under the same privacy level, there naturally exists a tradeoff between dummy traffic and delay. When dealing with heavier or less bursty input traffic, all shapers become more overhead-efficient. We also show that increased traffic from a larger number of IoT devices makes guaranteeing event-level privacy easier. The DP shaper offers tunable privacy that is invariant with the change in the input traffic distribution and has an advantage in handling burstiness over traffic-independent shapers. This approach well accommodates heterogeneous network conditions and enables users to adapt to their privacy/overhead demands.      
### 24.A Natural Language Processing and Deep Learning based Model for Automated Vehicle Diagnostics using Free-Text Customer Service Reports  [ :arrow_down: ](https://arxiv.org/pdf/2111.14977.pdf)
>  Initial fault detection and diagnostics are imperative measures to improve the efficiency, safety, and stability of vehicle operation. In recent years, numerous studies have investigated data-driven approaches to improve the vehicle diagnostics process using available vehicle data. Moreover, data-driven methods are employed to enhance customer-service agent interactions. In this study, we demonstrate a machine learning pipeline to improve automated vehicle diagnostics. First, Natural Language Processing (NLP) is used to automate the extraction of crucial information from free-text failure reports (generated during customers' calls to the service department). Then, deep learning algorithms are employed to validate service requests and filter vague or misleading claims. Ultimately, different classification algorithms are implemented to classify service requests so that valid service requests can be directed to the relevant service department. The proposed model- Bidirectional Long Short Term Memory (BiLSTM) along with Convolution Neural Network (CNN)- shows more than 18\% accuracy improvement in validating service requests compared to technicians' capabilities. In addition, using domain-based NLP techniques at preprocessing and feature extraction stages along with CNN-BiLSTM based request validation enhanced the accuracy ($&gt;25\%$), sensitivity ($&gt;39\%$), specificity ($&gt;11\%$), and precision ($&gt;11\%$) of Gradient Tree Boosting (GTB) service classification model. The Receiver Operating Characteristic Area Under the Curve (ROC-AUC) reached 0.82.      
### 25.Improving the Segmentation of Pediatric Low-Grade Gliomas through Multitask Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.14959.pdf)
>  Brain tumor segmentation is a critical task for tumor volumetric analyses and AI algorithms. However, it is a time-consuming process and requires neuroradiology expertise. While there has been extensive research focused on optimizing brain tumor segmentation in the adult population, studies on AI guided pediatric tumor segmentation are scarce. Furthermore, MRI signal characteristics of pediatric and adult brain tumors differ, necessitating the development of segmentation algorithms specifically designed for pediatric brain tumors. We developed a segmentation model trained on magnetic resonance imaging (MRI) of pediatric patients with low-grade gliomas (pLGGs) from The Hospital for Sick Children (Toronto, Ontario, Canada). The proposed model utilizes deep Multitask Learning (dMTL) by adding tumor's genetic alteration classifier as an auxiliary task to the main network, ultimately improving the accuracy of the segmentation results.      
### 26.Localized Perturbations For Weakly-Supervised Segmentation of Glioma Brain Tumours  [ :arrow_down: ](https://arxiv.org/pdf/2111.14953.pdf)
>  Deep convolutional neural networks (CNNs) have become an essential tool in the medical imaging-based computer-aided diagnostic pipeline. However, training accurate and reliable CNNs requires large fine-grain annotated datasets. To alleviate this, weakly-supervised methods can be used to obtain local information from global labels. This work proposes the use of localized perturbations as a weakly-supervised solution to extract segmentation masks of brain tumours from a pretrained 3D classification model. Furthermore, we propose a novel optimal perturbation method that exploits 3D superpixels to find the most relevant area for a given classification using a U-net architecture. Our method achieved a Dice similarity coefficient (DSC) of 0.44 when compared with expert annotations. When compared against Grad-CAM, our method outperformed both in visualization and localization ability of the tumour region, with Grad-CAM only achieving 0.11 average DSC.      
### 27.Forecasting battery capacity and power degradation with multi-task learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.14937.pdf)
>  Lithium-ion batteries degrade due to usage and exposure to environmental conditions, which affects their capability to store energy and supply power. Accurately predicting the capacity and power fade of lithium-ion battery cells is challenging due to intrinsic manufacturing variances and coupled nonlinear ageing mechanisms. In this paper, we propose a data-driven prognostics framework to predict both capacity and power fade simultaneously with multi-task learning. The model is able to predict the degradation trajectory of both capacity and internal resistance together with knee-points and end-of-life points accurately with as little as 100 cycles. The validation shows an average percentage error of 2.37% and 1.24% for the prediction of capacity fade and resistance rise, respectively. The model's ability to accurately predict the degradation, facing capacity and resistance estimation errors, further demonstrates the model's robustness and generalizability. Compared with single-task learning models for forecasting capacity and power degradation, the model shows a significant prediction accuracy improvement and computational cost reduction. This work presents the highlights of multi-task learning in the degradation prognostics for lithium-ion batteries.      
### 28.High-Speed Light Focusing through Scattering Medium by Cooperatively Accelerated Genetic Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2111.14916.pdf)
>  We develop an accelerated Genetic Algorithm (GA) system constructed by the cooperation of field-programmable gate array (FPGA) and optimized parameters of the GA. We found the enhanced decay of mutation rate makes convergence of the GA much faster, enabling the parameter-induced acceleration of the GA. Furthermore, the accelerated configuration of the GA is programmed in FPGA to boost processing speed at the hardware level without external computation devices. This system has ability to focus light through scattering medium within 4 seconds with robust noise resistance and stable repetition performance, which could be further reduced to millisecond level with advanced board configuration. This study solves the long-term limitation of the GA, it promotes the applications of the GA in dynamic scattering mediums, with the capability to tackle wavefront shaping in biological material.      
### 29.Do We Still Need Automatic Speech Recognition for Spoken Language Understanding?  [ :arrow_down: ](https://arxiv.org/pdf/2111.14842.pdf)
>  Spoken language understanding (SLU) tasks are usually solved by first transcribing an utterance with automatic speech recognition (ASR) and then feeding the output to a text-based model. Recent advances in self-supervised representation learning for speech data have focused on improving the ASR component. We investigate whether representation learning for speech has matured enough to replace ASR in SLU. We compare learned speech features from wav2vec 2.0, state-of-the-art ASR transcripts, and the ground truth text as input for a novel speech-based named entity recognition task, a cardiac arrest detection task on real-world emergency calls and two existing SLU benchmarks. We show that learned speech features are superior to ASR transcripts on three classification tasks. For machine translation, ASR transcripts are still the better choice. We highlight the intrinsic robustness of wav2vec 2.0 representations to out-of-vocabulary words as key to better performance.      
### 30.HOTTBOX: Higher Order Tensor ToolBOX  [ :arrow_down: ](https://arxiv.org/pdf/2111.15662.pdf)
>  HOTTBOX is a Python library for exploratory analysis and visualisation of multi-dimensional arrays of data, also known as tensors. The library includes methods ranging from standard multi-way operations and data manipulation through to multi-linear algebra based tensor decompositions. HOTTBOX also comprises sophisticated algorithms for generalised multi-linear classification and data fusion, such as Support Tensor Machine (STM) and Tensor Ensemble Learning (TEL). For user convenience, HOTTBOX offers a unifying API which establishes a self-sufficient ecosystem for various forms of efficient representation of multi-way data and the corresponding decomposition and association algorithms. Particular emphasis is placed on scalability and interactive visualisation, to support multidisciplinary data analysis communities working on big data and tensors. HOTTBOX also provides means for integration with other popular data science libraries for visualisation and data manipulation. The source code, examples and documentation ca be found at <a class="link-external link-https" href="https://github.com/hottbox/hottbox" rel="external noopener nofollow">this https URL</a>.      
### 31.Wound Healing Modeling Using Partial Differential Equation and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.15632.pdf)
>  The process of wound healing has been an active area of research around the world. The problem is the wounds of different patients heal differently. For example, patients with a background of diabetes may have difficulties in healing [1]. By clearly understanding this process, we can determine the type and quantity of medicine to give to patients with varying types of wounds. In this research, we use a variation of the Alternating Direction Implicit method to solve a partial differential equation that models part of the wound healing process. Wound images are used as the dataset that we analyze. To segment the image's wound, we implement deep learning-based models. We show that the combination of a variant of the Alternating Direction Implicit method and Deep Learning provides a reasonably accurate model for the process of wound healing. To the best of our knowledge, this is the first attempt to combine both numerical PDE and deep learning techniques in an automated system to capture the long-term behavior of wound healing.      
### 32.Distributed Computation of A Posteriori Bit Likelihood Ratios in Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2111.15568.pdf)
>  This paper presents a novel strategy to decentralize the soft detection procedure in an uplink cell-free massive multiple-input-multiple-output network. We propose efficient approaches to compute the a posteriori probability-per-bit, exactly or approximately when having a sequential fronthaul. More precisely, each access point (AP) in the network computes partial sufficient statistics locally, fuses it with received partial statistics from another AP, and then forwards the result to the next AP. Once the sufficient statistics reach the central processing unit, it performs the soft demodulation by computing the log-likelihood ratio (LLR) per bit, and then a channel decoding algorithm (e.g., a Turbo decoder) is utilized to decode the bits. We derive the distributed computation of LLR analytically.      
### 33.Spatio-Temporal Multi-Flow Network for Video Frame Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2111.15483.pdf)
>  Video frame interpolation (VFI) is currently a very active research topic, with applications spanning computer vision, post production and video encoding. VFI can be extremely challenging, particularly in sequences containing large motions, occlusions or dynamic textures, where existing approaches fail to offer perceptually robust interpolation performance. In this context, we present a novel deep learning based VFI method, ST-MFNet, based on a Spatio-Temporal Multi-Flow architecture. ST-MFNet employs a new multi-scale multi-flow predictor to estimate many-to-one intermediate flows, which are combined with conventional one-to-one optical flows to capture both large and complex motions. In order to enhance interpolation performance for various textures, a 3D CNN is also employed to model the content dynamics over an extended temporal window. Moreover, ST-MFNet has been trained within an ST-GAN framework, which was originally developed for texture synthesis, with the aim of further improving perceptual interpolation quality. Our approach has been comprehensively evaluated -- compared with fourteen state-of-the-art VFI algorithms -- clearly demonstrating that ST-MFNet consistently outperforms these benchmarks on varied and representative test datasets, with significant gains up to 1.09dB in PSNR for cases including large motions and dynamic textures. Project page: <a class="link-external link-https" href="https://danielism97.github.io/ST-MFNet" rel="external noopener nofollow">this https URL</a>.      
### 34.Energy-Efficient Design for a NOMA assisted STAR-RIS Network with Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.15464.pdf)
>  Simultaneous transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs) has been considered as a promising auxiliary device to enhance the performance of the wireless network, where users located at the different sides of the surfaces can be simultaneously served by the transmitting and reflecting signals. In this paper, the energy efficiency (EE) maximization problem for a non-orthogonal multiple access (NOMA) assisted STAR-RIS downlink network is investigated. Due to the fractional form of the EE, it is challenging to solve the EE maximization problem by the traditional convex optimization solutions. In this work, a deep deterministic policy gradient (DDPG)-based algorithm is proposed to maximize the EE by jointly optimizing the transmission beamforming vectors at the base station and the coefficients matrices at the STAR-RIS. Simulation results demonstrate that the proposed algorithm can effectively maximize the system EE considering the time-varying channels.      
### 35.FMD-cGAN: Fast Motion Deblurring using Conditional Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.15438.pdf)
>  In this paper, we present a Fast Motion Deblurring-Conditional Generative Adversarial Network (FMD-cGAN) that helps in blind motion deblurring of a single image. FMD-cGAN delivers impressive structural similarity and visual appearance after deblurring an image. Like other deep neural network architectures, GANs also suffer from large model size (parameters) and computations. It is not easy to deploy the model on resource constraint devices such as mobile and robotics. With the help of MobileNet based architecture that consists of depthwise separable convolution, we reduce the model size and inference time, without losing the quality of the images. More specifically, we reduce the model size by 3-60x compare to the nearest competitor. The resulting compressed Deblurring cGAN faster than its closest competitors and even qualitative and quantitative results outperform various recently proposed state-of-the-art blind motion deblurring models. We can also use our model for real-time image deblurring tasks. The current experiment on the standard datasets shows the effectiveness of the proposed method.      
### 36.Clustering-Based Activity Detection Algorithms for Grant-Free Random Access in Cell-Free Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2111.15378.pdf)
>  Future wireless networks need to support massive machine type communication (mMTC) where a massive number of devices accesses the network and massive MIMO is a promising enabling technology. Massive access schemes have been studied for co-located massive MIMO arrays. In this paper, we investigate the activity detection in grant-free random access for mMTC in cell-free massive MIMO networks using distributed arrays. Each active device transmits a non-orthogonal pilot sequence to the access points (APs) and the APs send the received signals to a central processing unit (CPU) for joint activity detection. The maximum likelihood device activity detection problem is formulated and algorithms for activity detection in cell-free massive MIMO are provided to solve it. The simulation results show that the macro-diversity gain provided by the cell-free architecture improves the activity detection performance compared to co-located architecture when the coverage area is large.      
### 37.Reconstruction Student with Attention for Student-Teacher Pyramid Matching  [ :arrow_down: ](https://arxiv.org/pdf/2111.15376.pdf)
>  Anomaly detection and localization are important problems in computer vision. Recently, Convolutional Neural Network (CNN) has been used for visual inspection. In particular, the scarcity of anomalous samples increases the difficulty of this task, and unsupervised leaning based methods are attracting attention. We focus on Student-Teacher Feature Pyramid Matching (STPM) which can be trained from only normal images with small number of epochs. Here we proposed a powerful method which compensates for the shortcomings of STPM. Proposed method consists of two students and two teachers that a pair of student-teacher network is the same as STPM. The other student-teacher network has a role to reconstruct the features of normal products. By reconstructing the features of normal products from an abnormal image, it is possible to detect abnormalities with higher accuracy by taking the difference between them. The new student-teacher network uses attention modules and different teacher network from the original STPM. Attention mechanism acts to successfully reconstruct the normal regions in an input image. Different teacher network prevents looking at the same regions as the original STPM. Six anomaly maps obtained from the two student-teacher networks are used to calculate the final anomaly map. Student-teacher network for reconstructing features improved AUC scores for pixel level and image level in comparison with the original STPM.      
### 38.RadioWeaves for Extreme Spatial Multiplexing in Indoor Environments  [ :arrow_down: ](https://arxiv.org/pdf/2111.15339.pdf)
>  With the advances in virtual and augmented reality, gaming applications, and entertainment, certain indoor scenarios will require vastly higher capacity than what can be delivered by 5G. In this paper, we focus on massive MIMO for indoor environments. We provide a case study of the distributed deployment of the antenna elements over the walls of a room and not restricting the antenna separation to be half the wavelength. This is a new paradigm of massive MIMO antenna deployment, introduced under the name RadioWeaves. We investigate different antenna deployment scenarios in line of sight communication. We observe that the RadioWeaves deployment can spatially separate users much better than a conventional co-located deployment, which outweighs the losses caused by grating lobes and thus saves a lot on transmit power. Through simulations, we show that the RadioWeaves technology can provide high rates to multiple users by spending very little power at the transmitter compared to a co-located deployment.      
### 39.Optical Wireless Sytems for Spine and Leaf Data Center Downlinks  [ :arrow_down: ](https://arxiv.org/pdf/2111.15301.pdf)
>  The continually growing demands for traffic as a result of advanced technologies in 5G and 6G systems offering services with intensive demands such as IoT and virtual reality applications has resulted in significant performance expectations of data center networks (DCNs). More specifically, DCNs are expected to meet high bandwidth connectivity, high throughput, low latency, and high scalability requirements. However, the current wired DCN architectures introduce large cabling requirements and limit the ability to reconfigure data centres as they expand. To that end, wireless technologies such as Optical Wireless Communication (OWC) have been proposed as a viable and cost-effective solution to meet the aforementioned requirements. This paper proposes the use of Infrared (IR) OWC systems that employ Wavelength Division Multiplexing (WDM) to enhance the DCN communication in the downlink direction; i.e. from Access Points (APs) in the ceiling, connected to spine switches, to receivers attached to the top of the racks representing leaf switches. The proposed systems utilize Angle Diversity Transmitters (ADTs) mounted on the room ceiling to facilitate inter-rack communication. Two different optical receiver types are considered, namely Angle Diversity Receivers (ADRs) and Wide Field-of-View Receivers (WFOVR). The simulation (i.e. channel modeling) results show that our proposed data center links achieve good data rates in the data centre up to 15 Gbps.      
### 40.SP-SEDT: Self-supervised Pre-training for Sound Event Detection Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2111.15222.pdf)
>  Recently, an event-based end-to-end model (SEDT) has been proposed for sound event detection (SED) and achieves competitive performance. However, compared with the frame-based model, it requires more training data with temporal annotations to improve the localization ability. Synthetic data is an alternative, but it suffers from a great domain gap with real recordings. Inspired by the great success of UP-DETR in object detection, we propose to self-supervisedly pre-train SEDT (SP-SEDT) by detecting random patches (only cropped along the time axis). Experiments on the DCASE2019 task4 dataset show the proposed SP-SEDT can outperform fine-tuned frame-based model. The ablation study is also conducted to investigate the impact of different loss functions and patch size.      
### 41.HyperPCA: a Powerful Tool to Extract Elemental Maps from Noisy Data Obtained in LIBS Mapping of Materials  [ :arrow_down: ](https://arxiv.org/pdf/2111.15187.pdf)
>  Laser-induced breakdown spectroscopy is a preferred technique for fast and direct multi-elemental mapping of samples under ambient pressure, without any limitation on the targeted element. However, LIBS mapping data have two peculiarities: an intrinsically low signal-to-noise ratio due to single-shot measurements, and a high dimensionality due to the high number of spectra acquired for imaging. This is all the truer as lateral resolution gets higher: in this case, the ablation spot diameter is reduced, as well as the ablated mass and the emission signal, while the number of spectra for a given surface increases. Therefore, efficient extraction of physico-chemical information from a noisy and large dataset is a major issue. Multivariate approaches were introduced by several authors as a means to cope with such data, particularly Principal Component Analysis. Yet, PCA is known to present theoretical constraints for the consistent reconstruction of the dataset, and has therefore limitations to efficient interpretation of LIBS mapping data. In this paper, we introduce HyperPCA, a new analysis tool for hyperspectral images based on a sparse representation of the data using Discrete Wavelet Transform and kernel-based sparse PCA to reduce the impact of noise on the data and to consistently reconstruct the spectroscopic signal, with a particular emphasis on LIBS data. The method is first illustrated using simulated LIBS mapping datasets to emphasize its performances with highly noisy and/or highly interfered spectra. Comparisons to standard PCA and to traditional univariate data analyses are provided. Finally, it is used to process real data in two cases that clearly illustrate the potential of the proposed algorithm. We show that the method presents advantages both in quantity and quality of the information recovered, thus improving the physico-chemical characterisation of analysed surfaces.      
### 42.CycleTransGAN-EVC: A CycleGAN-based Emotional Voice Conversion Model with Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2111.15159.pdf)
>  In this study, we explore the transformer's ability to capture intra-relations among frames by augmenting the receptive field of models. Concretely, we propose a CycleGAN-based model with the transformer and investigate its ability in the emotional voice conversion task. In the training procedure, we adopt curriculum learning to gradually increase the frame length so that the model can see from the short segment till the entire speech. The proposed method was evaluated on the Japanese emotional speech dataset and compared to several baselines (ACVAE, CycleGAN) with objective and subjective evaluations. The results show that our proposed model is able to convert emotion with higher strength and quality.      
### 43.Automated Speech Scoring System Under The Lens: Evaluating and interpreting the linguistic cues for language proficiency  [ :arrow_down: ](https://arxiv.org/pdf/2111.15156.pdf)
>  English proficiency assessments have become a necessary metric for filtering and selecting prospective candidates for both academia and industry. With the rise in demand for such assessments, it has become increasingly necessary to have the automated human-interpretable results to prevent inconsistencies and ensure meaningful feedback to the second language learners. Feature-based classical approaches have been more interpretable in understanding what the scoring model learns. Therefore, in this work, we utilize classical machine learning models to formulate a speech scoring task as both a classification and a regression problem, followed by a thorough study to interpret and study the relation between the linguistic cues and the English proficiency level of the speaker. First, we extract linguist features under five categories (fluency, pronunciation, content, grammar and vocabulary, and acoustic) and train models to grade responses. In comparison, we find that the regression-based models perform equivalent to or better than the classification approach. Second, we perform ablation studies to understand the impact of each of the feature and feature categories on the performance of proficiency grading. Further, to understand individual feature contributions, we present the importance of top features on the best performing algorithm for the grading task. Third, we make use of Partial Dependence Plots and Shapley values to explore feature importance and conclude that the best performing trained model learns the underlying rubrics used for grading the dataset used in this study.      
### 44.Reconfigurable Intelligent Surface Optimization for Uplink Sparse Code Multiple Access  [ :arrow_down: ](https://arxiv.org/pdf/2111.15110.pdf)
>  The reconfigurable intelligent surface (RIS)-assisted sparse code multiple access (RIS-SCMA) is an attractive scheme for future wireless networks. In this letter, for the first time, the RIS phase shifts of the uplink RIS-SCMA system are optimized based on the alternate optimization (AO) technique to improve the received signal-to-noise ratio (SNR) for a discrete set of RIS phase shifts. The system model of the uplink RIS-SCMA is formulated to utilize the AO algorithm. For further reduction in the computational complexity, a low-complexity AO (LC-AO) algorithm is proposed. The complexity analysis of the two proposed algorithms is performed. Monte Carlo simulations and complexity analysis show that the proposed algorithms significantly improve the received SNR compared to the non-optimized RIS-SCMA scenario. The LC-AO provides the same received SNR as the AO algorithm, with a significant reduction in complexity. Moreover, the deployment of RISs for the uplink RIS-SCMA is investigated.      
### 45.Communication-Efficient Federated Learning via Quantized Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2111.15071.pdf)
>  In this paper, we present a communication-efficient federated learning framework inspired by quantized compressed sensing. The presented framework consists of gradient compression for wireless devices and gradient reconstruction for a parameter server (PS). Our strategy for gradient compression is to sequentially perform block sparsification, dimensional reduction, and quantization. Thanks to gradient sparsification and quantization, our strategy can achieve a higher compression ratio than one-bit gradient compression. For accurate aggregation of the local gradients from the compressed signals at the PS, we put forth an approximate minimum mean square error (MMSE) approach for gradient reconstruction using the expectation-maximization generalized-approximate-message-passing (EM-GAMP) algorithm. Assuming Bernoulli Gaussian-mixture prior, this algorithm iteratively updates the posterior mean and variance of local gradients from the compressed signals. We also present a low-complexity approach for the gradient reconstruction. In this approach, we use the Bussgang theorem to aggregate local gradients from the compressed signals, then compute an approximate MMSE estimate of the aggregated gradient using the EM-GAMP algorithm. We also provide a convergence rate analysis of the presented framework. Using the MNIST dataset, we demonstrate that the presented framework achieves almost identical performance with the case that performs no compression, while significantly reducing communication overhead for federated learning.      
### 46.Online Robust Control of Linear Dynamical Systems with Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2111.15063.pdf)
>  We address the online robust control problem of a linear dynamical system with adversarial cost functions and adversarial disturbances. The goal is to find an online control policy that minimizes the disturbance gain, defined as the ratio of the cumulative cost and the cumulative energy in the disturbances. This problem is similar to the well-studied $\mathcal{H}_{\infty}$ problem in the robust control literature. However, unlike the standard $\mathcal{H}_{\infty}$ problem, where the cost functions are quadratic and fully known, we consider a more challenging online control setting where the cost functions are general and unknown a priori. We first propose an online robust control algorithm for the setting where the algorithm has access to an $N$-length preview of the future cost functions and future disturbances. We show that, under standard system assumptions, with $N$ greater than a threshold, the proposed algorithm can achieve a disturbance gain $(2+\rho(N)) \overline{\gamma}^2$, where $\overline{\gamma}^2$ is the best (minimum) possible disturbance gain for an oracle policy with full knowledge of the cost functions and disturbances, with $\rho(N) = O(1/N)$. We then propose an online robust control algorithm for a more challenging setting where only the preview of the cost functions is available. We show that under similar assumptions, with $N$ greater than the same threshold, the proposed algorithm achieves a disturbance gain of $6\overline{\gamma}^2$ with respect to the maximum cumulative energy in the disturbances.      
### 47.Second-order Approximation of Minimum Discrimination Information in Independent Component Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2111.15060.pdf)
>  Independent Component Analysis (ICA) is intended to recover the mutually independent sources from their linear mixtures, and F astICA is one of the most successful ICA algorithms. Although it seems reasonable to improve the performance of F astICA by introducing more nonlinear functions to the negentropy estimation, the original fixed-point method (approximate Newton method) in F astICA degenerates under this circumstance. To alleviate this problem, we propose a novel method based on the second-order approximation of minimum discrimination information (MDI). The joint maximization in our method is consisted of minimizing single weighted least squares and seeking unmixing matrix by the fixed-point method. Experimental results validate its efficiency compared with other popular ICA algorithms.      
### 48.Multi-period facility location and capacity planning under $\infty$-Wasserstein joint chance constraints in humanitarian logistics  [ :arrow_down: ](https://arxiv.org/pdf/2111.15057.pdf)
>  The key of the post-disaster humanitarian logistics (PD-HL) is to build a good facility location and capacity planning (FLCP) model for delivering relief supplies to affected areas in time. To fully exploit the historical PD data, this paper adopts the data-driven distributionally robust (DR) approach and proposes a novel multi-period FLCP model under the $\infty$-Wasserstein joint chance constraints (MFLCP-W). Specifically, we sequentially decide locations from a candidate set to build facilities with supply capacities, which are expanded if more economical, and use a finite number of historical demand samples in chance constraints to ensure a high probability of on-time delivery. To solve the MFLCP-W model, we equivalently reformulate it as a mixed integer second-order cone program and then solve it by designing an effective outer approximation algorithm with two tailored valid cuts. Finally, a case study under hurricane threats shows that MFLCP-W outperforms its counterparts in the terms of the cost and service quality, and that our algorithm converges significantly faster than the commercial solver CPLEX 12.8 with a better optimality gap.      
### 49.A Secure Key Sharing Algorithm Exploiting Phase Reciprocity in Wireless Channels  [ :arrow_down: ](https://arxiv.org/pdf/2111.15046.pdf)
>  This article presents a secure key exchange algorithm that exploits reciprocity in wireless channels to share a secret key between two nodes $A$ and $B$. Reciprocity implies that the channel phases in the links $A\rightarrow B$ and $B\rightarrow A$ are the same. A number of such reciprocal phase values are measured at nodes $A$ and $B$, called shared phase values hereafter. Each shared phase value is used to mask points of a Phase Shift Keying (PSK) constellation. Masking is achieved by rotating each PSK constellation with a shared phase value. Rotation of constellation is equivalent to adding phases modulo-$2\pi$, and as the channel phase is uniformly distributed in $[0,2\pi)$, the result of summation conveys zero information about summands. To enlarge the key size over a static or slow fading channel, the Radio Frequency (RF) propagation path is perturbed to create several independent realizations of multi-path fading, each used to share a new phase value. To eavesdrop a phase value shared in this manner, the Eavesdropper (Eve) will always face an under-determined system of linear equations which will not reveal any useful information about its actual solution value. This property is used to establish a secure key between two legitimate users.      
### 50.Online Learning for Receding Horizon Control with Provable Regret Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2111.15041.pdf)
>  We address the problem of learning to control an unknown linear dynamical system with time varying cost functions through the framework of online Receding Horizon Control (RHC). We consider the setting where the control algorithm does not know the true system model and has only access to a fixed-length (that does not grow with the control horizon) preview of the future cost functions. We characterize the performance of an algorithm using the metric of dynamic regret, which is defined as the difference between the cumulative cost incurred by the algorithm and that of the best sequence of actions in hindsight. We propose two different online RHC algorithms to address this problem, namely Certainty Equivalence RHC (CE-RHC) algorithm and Optimistic RHC (O-RHC) algorithm. We show that under the standard stability assumption for the model estimate, the CE-RHC algorithm achieves $\mathcal{O}(T^{2/3})$ dynamic regret. We then extend this result to the setting where the stability assumption hold only for the true system model by proposing the O-RHC algorithm. We show that O-RHC algorithm achieves $\mathcal{O}(T^{2/3})$ dynamic regret but with some additional computation.      
### 51.Hyperspectral Image Segmentation based on Graph Processing over Multilayer Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.15018.pdf)
>  Hyperspectral imaging is an important sensing technology with broad applications and impact in areas including environmental science, weather, and geo/space exploration. One important task of hyperspectral image (HSI) processing is the extraction of spectral-spatial features. Leveraging on the recent-developed graph signal processing over multilayer networks (M-GSP), this work proposes several approaches to HSI segmentation based on M-GSP feature extraction. To capture joint spectral-spatial information, we first customize a tensor-based multilayer network (MLN) model for HSI, and define a MLN singular space for feature extraction. We then develop an unsupervised HSI segmentation method by utilizing MLN spectral clustering. Regrouping HSI pixels via MLN-based clustering, we further propose a semi-supervised HSI classification based on multi-resolution fusions of superpixels. Our experimental results demonstrate the strength of M-GSP in HSI processing and spectral-spatial information extraction.      
### 52.Joint Modeling of Code-Switched and Monolingual ASR via Conditional Factorization  [ :arrow_down: ](https://arxiv.org/pdf/2111.15016.pdf)
>  Conversational bilingual speech encompasses three types of utterances: two purely monolingual types and one intra-sententially code-switched type. In this work, we propose a general framework to jointly model the likelihoods of the monolingual and code-switch sub-tasks that comprise bilingual speech recognition. By defining the monolingual sub-tasks with label-to-frame synchronization, our joint modeling framework can be conditionally factorized such that the final bilingual output, which may or may not be code-switched, is obtained given only monolingual information. We show that this conditionally factorized joint framework can be modeled by an end-to-end differentiable neural network. We demonstrate the efficacy of our proposed model on bilingual Mandarin-English speech recognition across both monolingual and code-switched corpora.      
### 53.Real-Time CRLB based Antenna Selection in Planar Antenna Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2111.15008.pdf)
>  Estimation of User Terminals' (UTs') Angle of Arrival (AoA) plays a significant role in the next generation of wireless systems. Due to high demands, energy efficiency concerns, and scarcity of available resources, it is pivotal how these resources are used. Installed antennas and their corresponding hardware at the Base Station (BS) are of these resources. In this paper, we address the problem of antenna selection to minimize Cramer-Rao Lower Bound (CRLB) of a planar antenna array when fewer antennas than total available antennas have to be used for a UT. First, the optimal antenna selection strategy to minimize the expected CRLB in a planar antenna array is proposed. Then, using this strategy as a preliminary step, we present a two-step antenna selection method whose goal is to minimize the instantaneous CRLB. Minimizing instantaneous CRLB through antenna selection is a combinatorial optimization problem for which we utilize a greedy algorithm. The optimal start point of the greedy algorithm is presented alongside some methods to reduce the computational complexity of the selection procedure. Numerical results confirm the accuracy of the proposed solutions and highlight the benefits of using antenna selection in the localization phase in a wireless system.      
### 54.Validating CircaCP: a Generic Sleep-Wake Cycle Detection Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2111.14960.pdf)
>  Sleep-wake cycle detection is a key step when extrapolating sleep patterns from actigraphy data. Numerous supervised detection algorithms have been developed with parameters estimated from and optimized for a particular dataset, yet their generalizability from sensor to sensor or study to study is unknown. In this paper, we propose and validate an unsupervised algorithm -- CircaCP -- to detect sleep-wake cycles from minute-by-minute actigraphy data. It first uses a robust cosinor model to estimate circadian rhythm, then searches for a single change point (CP) within each cycle. We used CircaCP to estimate sleep/wake onset times (S/WOTs) from 2125 indviduals' data in the MESA Sleep study and compared the estimated S/WOTs against self-reported S/WOT event markers. Lastly, we quantified the biases between estimated and self-reported S/WOTs, as well as variation in S/WOTs contributed by the two methods, using linear mixed-effects models and variance component analysis. <br>On average, SOTs estimated by CircaCP were five minutes behind those reported by event markers, and WOTs estimated by CircaCP were less than one minute behind those reported by markers. These differences accounted for less than 0.2% variability in SOTs and in WOTs, taking into account other sources of between-subject variations. By focusing on the commonality in human circadian rhythms captured by actigraphy, our algorithm transferred seamlessly from hip-worn ActiGraph data collected from children in our previous study to wrist-worn Actiwatch data collected from adults. The large between- and within-subject variability highlights the need for estimating individual-level S/WOTs when conducting actigraphy research. The generalizability of our algorithm also suggests that it could be widely applied to actigraphy data collected by other wearable sensors.      
### 55.Privacy-Preserving Serverless Edge Learning with Decentralized Small Data  [ :arrow_down: ](https://arxiv.org/pdf/2111.14955.pdf)
>  In the last decade, data-driven algorithms outperformed traditional optimization-based algorithms in many research areas, such as computer vision, natural language processing, etc. However, extensive data usages bring a new challenge or even threat to deep learning algorithms, i.e., privacy-preserving. Distributed training strategies have recently become a promising approach to ensure data privacy when training deep models. This paper extends conventional serverless platforms with serverless edge learning architectures and provides an efficient distributed training framework from the networking perspective. This framework dynamically orchestrates available resources among heterogeneous physical units to efficiently fulfill deep learning objectives. The design jointly considers learning task requests and underlying infrastructure heterogeneity, including last-mile transmissions, computation abilities of mobile devices, edge and cloud computing centers, and devices battery status. Furthermore, to significantly reduce distributed training overheads, small-scale data training is proposed by integrating with a general, simple data classifier. This low-load enhancement can seamlessly work with various distributed deep models to improve communications and computation efficiencies during the training phase. Finally, open challenges and future research directions encourage the research community to develop efficient distributed deep learning techniques.      
### 56.Expressive Communication: A Common Framework for Evaluating Developments in Generative Models and Steering Interfaces  [ :arrow_down: ](https://arxiv.org/pdf/2111.14951.pdf)
>  There is an increasing interest from ML and HCI communities in empowering creators with better generative models and more intuitive interfaces with which to control them. In music, ML researchers have focused on training models capable of generating pieces with increasing long-range structure and musical coherence, while HCI researchers have separately focused on designing steering interfaces that support user control and ownership. In this study, we investigate through a common framework how developments in both models and user interfaces are important for empowering co-creation where the goal is to create music that communicates particular imagery or ideas (e.g., as is common for other purposeful tasks in music creation like establishing mood or creating accompanying music for another media). Our study is distinguished in that it measures communication through both composer's self-reported experiences, and how listeners evaluate this communication through the music. In an evaluation study with 26 composers creating 100+ pieces of music and listeners providing 1000+ head-to-head comparisons, we find that more expressive models and more steerable interfaces are important and complementary ways to make a difference in composers communicating through music and supporting their creative empowerment.      
### 57.Catch Me If You Hear Me: Audio-Visual Navigation in Complex Unmapped Environments with Moving Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2111.14843.pdf)
>  Audio-visual navigation combines sight and hearing to navigate to a sound-emitting source in an unmapped environment. While recent approaches have demonstrated the benefits of audio input to detect and find the goal, they focus on clean and static sound sources and struggle to generalize to unheard sounds. In this work, we propose the novel dynamic audio-visual navigation benchmark which requires to catch a moving sound source in an environment with noisy and distracting sounds. We introduce a reinforcement learning approach that learns a robust navigation policy for these complex settings. To achieve this, we propose an architecture that fuses audio-visual information in the spatial feature space to learn correlations of geometric information inherent in both local maps and audio signals. We demonstrate that our approach consistently outperforms the current state-of-the-art by a large margin across all tasks of moving sounds, unheard sounds, and noisy environments, on two challenging 3D scanned real-world environments, namely Matterport3D and Replica. The benchmark is available at <a class="link-external link-http" href="http://dav-nav.cs.uni-freiburg.de" rel="external noopener nofollow">this http URL</a>.      
### 58.MIST-net: Multi-domain Integrative Swin Transformer network for Sparse-View CT Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2111.14831.pdf)
>  The deep learning-based tomographic image reconstruction have been attracting much attention among these years. The sparse-view data reconstruction is one of typical underdetermined inverse problems, how to reconstruct high-quality CT images from dozens of projections is still a challenge in practice. To address this challenge, in this article we proposed a Multi-domain Integrative Swin Transformer network (MIST-net). First, the proposed MIST-net incorporated lavish domain features from data, residual-data, image, and residual-image using flexible network architectures. Here, the residual-data and residual-image domains network components can be considered as the data consistency module to eliminate interpolation errors in both residual data and image domains, and then further retain image details. Second, to detect the image features and further protect image edge, the trainable Sobel Filter was incorporated into the network to improve the encode-decode ability. Third, with the classical Swin transformer, we further designed the high-quality reconstruction transformer (i.e., Recformer) to improve the reconstruction performance. The Recformer inherited the power of Swin transformer to capture the global and local features of the reconstructed image. The experiments on the numerical datasets with 48 views demonstrated our proposed MIST-net provided higher reconstructed image quality with small feature recovery and edge protection than other competitors including the advanced unrolled networks. The quantitative results show that our MIST-net also obtained the best performance. The trained network was transferred to the real cardiac CT dataset with 48 views, the reconstruction results further validated the advantages of our MIST-net and further demonstrated the good robustness of our MIST in clinical applications.      
