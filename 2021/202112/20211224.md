# ArXiv eess --Fri, 24 Dec 2021
### 1.AI-based Reconstruction for Fast MRI -- A Systematic Review and Meta-analysis  [ :arrow_down: ](https://arxiv.org/pdf/2112.12744.pdf)
>  Compressed sensing (CS) has been playing a key role in accelerating the magnetic resonance imaging (MRI) acquisition process. With the resurgence of artificial intelligence, deep neural networks and CS algorithms are being integrated to redefine the state of the art of fast MRI. The past several years have witnessed substantial growth in the complexity, diversity, and performance of deep learning-based CS techniques that are dedicated to fast MRI. In this meta-analysis, we systematically review the deep learning-based CS techniques for fast MRI, describe key model designs, highlight breakthroughs, and discuss promising directions. We have also introduced a comprehensive analysis framework and a classification system to assess the pivotal role of deep learning in CS-based acceleration for MRI.      
### 2.Multi-speaker Multi-style Text-to-speech Synthesis With Single-speaker Single-style Training Data Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2112.12743.pdf)
>  In the existing cross-speaker style transfer task, a source speaker with multi-style recordings is necessary to provide the style for a target speaker. However, it is hard for one speaker to express all expected styles. In this paper, a more general task, which is to produce expressive speech by combining any styles and timbres from a multi-speaker corpus in which each speaker has a unique style, is proposed. To realize this task, a novel method is proposed. This method is a Tacotron2-based framework but with a fine-grained text-based prosody predicting module and a speaker identity controller. Experiments demonstrate that the proposed method can successfully express a style of one speaker with the timber of another speaker bypassing the dependency on a single speaker's multi-style corpus. Moreover, the explicit prosody features used in the prosody predicting module can increase the diversity of synthetic speech by adjusting the value of prosody features.      
### 3.Data-driven Safety Verification of Stochastic Systems via Barrier Certificates  [ :arrow_down: ](https://arxiv.org/pdf/2112.12709.pdf)
>  In this paper, we propose a data-driven approach to formally verify the safety of (potentially) unknown discrete-time continuous-space stochastic systems. The proposed framework is based on a notion of barrier certificates together with data collected from trajectories of unknown systems. We first reformulate the barrier-based safety verification as a robust convex problem (RCP). Solving the acquired RCP is hard in general because not only the state of the system lives in a continuous set, but also and more problematic, the unknown model appears in one of the constraints of RCP. Instead, we leverage a finite number of data, and accordingly, the RCP is casted as a scenario convex problem (SCP). We then relate the optimizer of the SCP to that of the RCP, and consequently, we provide a safety guarantee over the unknown stochastic system with a priori guaranteed confidence. We apply our approach to an unknown room temperature system by collecting sampled data from trajectories of the system and verify formally that temperature of the room lies in a comfort zone for a finite time horizon with a desired confidence.      
### 4.Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data  [ :arrow_down: ](https://arxiv.org/pdf/2112.12665.pdf)
>  Computer-assisted quantitative analysis on Giga-pixel pathology images has provided a new avenue in precision medicine. The innovations have been largely focused on cancer pathology (i.e., tumor segmentation and characterization). In non-cancer pathology, the learning algorithms can be asked to examine more comprehensive tissue types simultaneously, as a multi-label setting. The prior arts typically needed to train multiple segmentation networks in order to match the domain-specific knowledge for heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal tubular, distal tubular, peritubular capillaries, and arteries). In this paper, we propose a dynamic single segmentation network (Omni-Seg) that learns to segment multiple tissue types using partially labeled images (i.e., only one tissue type is labeled for each training image) for renal pathology. By learning from ~150,000 patch-wise pathological images from six tissue types, the proposed Omni-Seg network achieved superior segmentation accuracy and less resource consumption when compared to the previous the multiple-network and multi-head design. In the testing stage, the proposed method obtains "completely labeled" tissue segmentation results using only "partially labeled" training images. The source code is available at <a class="link-external link-https" href="https://github.com/ddrrnn123/Omni-Seg" rel="external noopener nofollow">this https URL</a>.      
### 5.Data-driven design of safe control for polynomial systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.12664.pdf)
>  We consider the problem of designing an invariant set using only a finite set of input-state data collected from an unknown polynomial system in continuous time. We consider noisy data, i.e., corrupted by an unknown-but-bounded disturbance. We derive a data-dependent sum-of-squares program that enforces invariance of a set and also optimizes the size of the invariant set while keeping it within a set of user-defined safety constraints; the solution of this program directly provides a polynomial invariant set and a state-feedback controller. We numerically test the design on a system of two platooning cars.      
### 6.InDuDoNet+: A Model-Driven Interpretable Dual Domain Network for Metal Artifact Reduction in CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2112.12660.pdf)
>  During the computed tomography (CT) imaging process, metallic implants within patients always cause harmful artifacts, which adversely degrade the visual quality of reconstructed CT images and negatively affect the subsequent clinical diagnosis. For the metal artifact reduction (MAR) task, current deep learning based methods have achieved promising performance. However, most of them share two main common limitations: 1) the CT physical imaging geometry constraint is not comprehensively incorporated into deep network structures; 2) the entire framework has weak interpretability for the specific MAR task; hence, the role of every network module is difficult to be evaluated. To alleviate these issues, in the paper, we construct a novel interpretable dual domain network, termed InDuDoNet+, into which CT imaging process is finely embedded. Concretely, we derive a joint spatial and Radon domain reconstruction model and propose an optimization algorithm with only simple operators for solving it. By unfolding the iterative steps involved in the proposed algorithm into the corresponding network modules, we easily build the InDuDoNet+ with clear interpretability. Furthermore, we analyze the CT values among different tissues, and merge the prior observations into a prior network for our InDuDoNet+, which significantly improve its generalization performance. Comprehensive experiments on synthesized data and clinical data substantiate the superiority of the proposed methods as well as the superior generalization performance beyond the current state-of-the-art (SOTA) MAR methods. Code is available at \url{<a class="link-external link-https" href="https://github.com/hongwang01/InDuDoNet_plus" rel="external noopener nofollow">this https URL</a>}.      
### 7.Spectral and spatial power evolution design with machine learning-enabled Raman amplification  [ :arrow_down: ](https://arxiv.org/pdf/2112.12637.pdf)
>  We present a machine learning (ML) framework for designing desired signal power profiles over the spectral and spatial domains in the fiber span. The proposed framework adjusts the Raman pump power values to obtain the desired two-dimensional (2D) profiles using a convolutional neural network (CNN) followed by the differential evolution (DE) technique. The CNN learns the mapping between the 2D profiles and their corresponding pump power values using a data-set generated by exciting the amplification setup. Nonetheless, its performance is not accurate for designing 2D profiles of practical interest, such as a 2D flat or a 2D symmetric (with respect to the middle point in distance). To adjust the pump power values more accurately, the DE fine-tunes the power values initialized by the CNN to design the proposed 2D profile with a lower cost value. In the fine-tuning process, the DE employs the direct amplification model which consists of 8 bidirectional propagating pumps, including 2 second-order and 6 first order, in an 80 km fiber span. We evaluate the framework to design broadband 2D flat and symmetric power profiles, as two goals for wavelength division multiplexing (WDM) system performing over the whole C-band. Results indicate the framework's ability to achieve maximum power excursion of 2.81 dB for a 2D flat, and maximum asymmetry of 14% for a 2D symmetric profile.      
### 8.Deep Filtering with DNN, CNN and RNN  [ :arrow_down: ](https://arxiv.org/pdf/2112.12616.pdf)
>  This paper is about a deep learning approach for linear and nonlinear filtering. The idea is to train a neural network with Monte Carlo samples generated from a nominal dynamic model. Then the network weights are applied to Monte Carlo samples from an actual dynamic model. A main focus of this paper is on the deep filters with three major neural network architectures (DNN, CNN, RNN). Our deep filter compares favorably to the traditional Kalman filter in linear cases and outperform the extended Kalman filter in nonlinear cases. Then a switching model with jumps is studied to show the adaptiveness and power of our deep filtering. Among the three major NNs, the CNN outperform the others on average. while the RNN does not seem to be suitable for the filtering problem. One advantage of the deep filter is its robustness when the nominal model and actual model differ. The other advantage of deep filtering is real data can be used directly to train the deep neutral network. Therefore, model calibration can be by-passed all together.      
### 9.The two-dimensional OLCT of angularly periodic functions in polar coordinates  [ :arrow_down: ](https://arxiv.org/pdf/2112.12615.pdf)
>  The two-dimensional (2D) offset linear canonical transform (OLCT) in polar coordinates plays an important role in many fields of optics and signal processing. This paper studies the 2D OLCT in polar coordinates. Firstly, we extend the 2D OLCT to the polar coordinate system, and obtain the offset linear canonical Hankel transform (OLCHT) formula. Secondly, through the angular periodic function with a period of 2{\pi}, the relationship between the 2D OLCT and the OLCHT is revealed. Finally, the spatial shift and convolution theorems for the 2D OLCT are proposed by using this relationship.      
### 10.Adaptive Beamwidth Configuration for Millimeter Wave V2X Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2112.12614.pdf)
>  Millimeter wave (mmWave) technologies will support the high bandwidth and data rate requirements of V2X services demanded by connected and automated vehicles (CAVs). MmWave V2X technologies will leverage directional antennas that challenge the management of the communications in dynamic scenarios including the identification of available links, beams alignment, and scheduling. Previous studies have shown that these challenges can be reduced when mmWave communications are supported by side information like the one transmitted in sub-6GHz V2X technologies. In this context, this paper proposes a beamwidth-aware mmWave scheduling scheme for V2V communications supported by sub-6GHz V2X technologies. The proposal enables mmWave transmitters to schedule a mmWave transmission to several neighboring vehicles at the same time by adapting the beamwidth configuration. In addition, the proposal derives the minimum beamwidth that mmWave transmitters should use to contact their neighboring vehicles in a limited number of scheduling intervals. The obtained results demonstrate that the proposal helps increasing the amount of mmWave data that can be transmitted to neighboring vehicles.      
### 11.Predição da Idade Cerebral a partir de Imagens de Ressonância Magnética utilizando Redes Neurais Convolucionais  [ :arrow_down: ](https://arxiv.org/pdf/2112.12609.pdf)
>  In this work, deep learning techniques for brain age prediction from magnetic resonance images are investigated, aiming to assist in the identification of biomarkers of the natural aging process. The identification of biomarkers is useful for detecting an early-stage neurodegenerative process, as well as for predicting age-related or non-age-related cognitive decline. Two techniques are implemented and compared in this work: a 3D Convolutional Neural Network applied to the volumetric image and a 2D Convolutional Neural Network applied to slices from the axial plane, with subsequent fusion of individual predictions. The best result was obtained by the 2D model, which achieved a mean absolute error of 3.83 years. <br>-- <br>Neste trabalho são investigadas técnicas de aprendizado profundo para a predição da idade cerebral a partir de imagens de ressonância magnética, visando auxiliar na identificação de biomarcadores do processo natural de envelhecimento. A identificação de biomarcadores é útil para a detecção de um processo neurodegenerativo em estágio inicial, além de possibilitar prever um declínio cognitivo relacionado ou não à idade. Duas técnicas são implementadas e comparadas neste trabalho: uma Rede Neural Convolucional 3D aplicada na imagem volumétrica e uma Rede Neural Convolucional 2D aplicada a fatias do plano axial, com posterior fusão das predições individuais. O melhor resultado foi obtido pelo modelo 2D, que alcançou um erro médio absoluto de 3.83 anos.      
### 12.Are E2E ASR models ready for an industrial usage?  [ :arrow_down: ](https://arxiv.org/pdf/2112.12572.pdf)
>  The Automated Speech Recognition (ASR) community experiences a major turning point with the rise of the fully-neural (End-to-End, E2E) approaches. At the same time, the conventional hybrid model remains the standard choice for the practical usage of ASR. According to previous studies, the adoption of E2E ASR in real-world applications was hindered by two main limitations: their ability to generalize on unseen domains and their high operational cost. In this paper, we investigate both above-mentioned drawbacks by performing a comprehensive multi-domain benchmark of several contemporary E2E models and a hybrid baseline. Our experiments demonstrate that E2E models are viable alternatives for the hybrid approach, and even outperform the baseline both in accuracy and in operational efficiency. As a result, our study shows that the generalization and complexity issues are no longer the major obstacle for industrial integration, and draws the community's attention to other potential limitations of the E2E approaches in some specific use-cases.      
### 13.On the relationship between calibrated predictors and unbiased volume estimation  [ :arrow_down: ](https://arxiv.org/pdf/2112.12560.pdf)
>  Machine learning driven medical image segmentation has become standard in medical image analysis. However, deep learning models are prone to overconfident predictions. This has led to a renewed focus on calibrated predictions in the medical imaging and broader machine learning communities. Calibrated predictions are estimates of the probability of a label that correspond to the true expected value of the label conditioned on the confidence. Such calibrated predictions have utility in a range of medical imaging applications, including surgical planning under uncertainty and active learning systems. At the same time it is often an accurate volume measurement that is of real importance for many medical applications. This work investigates the relationship between model calibration and volume estimation. We demonstrate both mathematically and empirically that if the predictor is calibrated per image, we can obtain the correct volume by taking an expectation of the probability scores per pixel/voxel of the image. Furthermore, we show that convex combinations of calibrated classifiers preserve volume estimation, but do not preserve calibration. Therefore, we conclude that having a calibrated predictor is a sufficient, but not necessary condition for obtaining an unbiased estimate of the volume. We validate our theoretical findings empirically on a collection of 18 different (calibrated) training strategies on the tasks of glioma volume estimation on BraTS 2018, and ischemic stroke lesion volume estimation on ISLES 2018 datasets.      
### 14.Long-Term Optimal Delivery Planning for Replacing the Liquefied Petroleum Gas Cylinder  [ :arrow_down: ](https://arxiv.org/pdf/2112.12530.pdf)
>  This study proposes a method for efficient delivery of liquefied petroleum gas cylinders based on demand forecasts of gas usage. To maintain a liquefied petroleum gas service, gas providers visit each customer to check the gas meter and replace the gas cylinder depending on the remaining amount of gas. These visits can be categorized into three patterns: non-replacement visit, replacement before the customer runs out of gas, and replacement after the customer runs out of gas. The last pattern is a crucial problem in sustaining a liquefied petroleum gas service, and it must be reduced. By contrast, frequent non-replacement visits are required to prevent the customer from running out of gas, but it requires considerable effort. One of the most severe difficulties of this problem is acquiring the gas usages of each customer only by visiting. However, with the recent spread of smart sensors, the daily gas consumption of each house can be monitored without having to visit customers. Our main idea is to categorize all customers into three groups: high-risk, moderate-risk, and low-risk by focusing on an urgent need for cylinder replacement based on the demand forecast. Based on this idea, we construct an algorithm to maximize the delivery for moderate-risk customers while ensuring delivery to high-risk customers. Long-term optimal delivery planning is realized by achieving workload balancing per day. The verification experiment in Chiba prefecture in Japan showed the effectiveness of our algorithm in reducing the number of out-of-gas cylinders. Moreover, the proposed model is a new generic framework for building an optimal vehicle routing plan, consisting of a complementary algorithm, demand forecast, delivery list optimization, and delivery route optimization for realizing a long-term optimal delivery plan by setting the priority.      
### 15.An Asymptotically Optimal Approximation of the Conditional Mean Channel Estimator based on Gaussian Mixture Models  [ :arrow_down: ](https://arxiv.org/pdf/2112.12499.pdf)
>  This paper investigates a channel estimator based on Gaussian mixture models (GMMs) in the context of linear inverse problems with additive Gaussian noise. We fit a GMM to given channel samples to obtain an analytic probability density function (PDF) which approximates the true channel PDF. Then, a conditional mean estimator (CME) corresponding to this approximating PDF is computed in closed form and used as an approximation of the optimal CME based on the true channel PDF. This optimal CME cannot be calculated analytically because the true channel PDF is generally unknown. We present mild conditions which allow us to prove the convergence of the GMM-based CME to the optimal CME as the number of GMM components is increased. Additionally, we investigate the estimator's computational complexity and present simplifications based on common model-based insights. Further, we study the estimator's behavior in numerical experiments including multiple-input multiple-output (MIMO) and wideband systems.      
### 16.Complexity-Oriented Per-shot Video Coding Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2112.12424.pdf)
>  Current per-shot encoding schemes aim to improve the compression efficiency by shot-level optimization. It splits a source video sequence into shots and imposes optimal sets of encoding parameters to each shot. Per-shot encoding achieved approximately 20% bitrate savings over baseline fixed QP encoding at the expense of pre-processing complexity. However, the adjustable parameter space of the current per-shot encoding schemes only has spatial resolution and QP/CRF, resulting in a lack of encoding flexibility. In this paper, we extend the per-shot encoding framework in the complexity dimension. We believe that per-shot encoding with flexible complexity will help in deploying user-generated content. We propose a rate-distortion-complexity optimization process for encoders and a methodology to determine the coding parameters under the constraints of complexities and bitrate ladders. Experimental results show that our proposed method achieves complexity constraints ranging from 100% to 3% in a dense form compared to the slowest per-shot anchor. With similar complexities of the per-shot scheme fixed in specific presets, our proposed method achieves BDrate gain up to -19.17%.      
### 17.Deep Proximal Learning for High-Resolution Plane Wave Compounding  [ :arrow_down: ](https://arxiv.org/pdf/2112.12410.pdf)
>  Plane Wave imaging enables many applications that require high frame rates, including localisation microscopy, shear wave elastography, and ultra-sensitive Doppler. To alleviate the degradation of image quality with respect to conventional focused acquisition, typically, multiple acquisitions from distinctly steered plane waves are coherently (i.e. after time-of-flight correction) compounded into a single image. This poses a trade-off between image quality and achievable frame-rate. To that end, we propose a new deep learning approach, derived by formulating plane wave compounding as a linear inverse problem, that attains high resolution, high-contrast images from just 3 plane wave transmissions. Our solution unfolds the iterations of a proximal gradient descent algorithm as a deep network, thereby directly exploiting the physics-based generative acquisition model into the neural network design. We train our network in a greedy manner, i.e. layer-by-layer, using a combination of pixel, temporal, and distribution (adversarial) losses to achieve both perceptual fidelity and data consistency. Through the strong model-based inductive bias, the proposed architecture outperforms several standard benchmark architectures in terms of image quality, with a low computational and memory footprint.      
### 18.Directional Analytic Discrete Cosine Frames  [ :arrow_down: ](https://arxiv.org/pdf/2112.12407.pdf)
>  Block frames called directional analytic discrete cosine frames (DADCFs) are proposed for sparse image representation. In contrast to conventional overlapped frames, the proposed DADCFs require a reduced amount of 1) computational complexity, 2) memory usage, and 3) global memory access. These characteristics are strongly required for current high-resolution image processing. Specifically, we propose two DADCFs based on discrete cosine transform (DCT) and discrete sine transform (DST). The first DADCF is constructed from parallel separable transforms of DCT and DST, where the DST is permuted by row. The second DADCF is also designed based on DCT and DST, while the DST is customized to have no DC leakage property which is a desirable property for image processing. Both DADCFs have rich directional selectivity with slightly different characteristics each other and they can be implemented as non-overlapping block-based transforms, especially they form Parseval block frames with low transform redundancy. We perform experiments to evaluate our DADCFs and compare them with conventional directional block transforms in image recovery.      
### 19.KFWC: A Knowledge-Driven Deep Learning Model for Fine-grained Classification of Wet-AMD  [ :arrow_down: ](https://arxiv.org/pdf/2112.12386.pdf)
>  Automated diagnosis using deep neural networks can help ophthalmologists detect the blinding eye disease wet Age-related Macular Degeneration (AMD). Wet-AMD has two similar subtypes, Neovascular AMD and Polypoidal Choroidal Vessels (PCV). However, due to the difficulty in data collection and the similarity between images, most studies have only achieved the coarse-grained classification of wet-AMD rather than a finer-grained one of wet-AMD subtypes. To solve this issue, in this paper we propose a Knowledge-driven Fine-grained Wet-AMD Classification Model (KFWC), to classify fine-grained diseases with insufficient data. With the introduction of a priori knowledge of 10 lesion signs of input images into the KFWC, we aim to accelerate the KFWC by means of multi-label classification pre-training, to locate the decisive image features in the fine-grained disease classification task and therefore achieve better classification. Simultaneously, the KFWC can also provide good interpretability and effectively alleviate the pressure of data collection and annotation in the field of fine-grained disease classification for wet-AMD. The experiments demonstrate the effectiveness of the KFWC which reaches 99.71% in AU-ROC scores, and its considerable improvements over the data-driven w/o Knowledge and ophthalmologists, with the rates of 6.69% over the strongest baseline and 4.14% over ophthalmologists.      
### 20.Temporal Analysis of Functional Brain Connectivity for EEG-based Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.12380.pdf)
>  EEG signals in emotion recognition absorb special attention owing to their high temporal resolution and their information about what happens in the brain. Different regions of brain work together to process information and meanwhile the activity of brain changes during the time. Therefore, the investigation of the connection between different brain areas and their temporal patterns plays an important role in neuroscience. In this study, we investigate the emotion classification performance using functional connectivity features in different frequency bands and compare them with the classification performance using differential entropy feature, which has been previously used for this task. Moreover, we investigate the effect of using different time periods on the classification performance. Our results on publicly available SEED dataset show that as time goes on, emotions become more stable and the classification accuracy increases. Among different time periods, we achieve the highest classification accuracy using the time period of 140s-end. In this time period, the accuracy is improved by 4 to 6% compared to using the entire signal. The mean accuracy of about 88% is obtained using any of the Pearson correlation coefficient, coherence, and phase locking value features and SVM. Therefore, functional connectivity features lead to better classification accuracy than DE features (with the mean accuracy of 84.89%) using the proposed framework. Finally, in a relatively fair comparison, we show that using the best time interval and SVM, we achieve better accuracy than using Recurrent Neural Networks which need large amount of data and have high computational cost.      
### 21.Shaped Four-Dimensional Modulation Formats for Optical Fiber Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2112.12377.pdf)
>  We review the design of multidimensional modulations by maximizing generalized mutual information and compare the maximum transmission reach of recently introduced 4D formats. A model-based optimization for nonlinear-tolerant 4D modulations is also discussed.      
### 22.Globally convergent visual-feature range estimation with biased inertial measurements  [ :arrow_down: ](https://arxiv.org/pdf/2112.12325.pdf)
>  The design of a globally convergent position observer for feature points from visual information is a challenging problem, especially for the case with only inertial measurements and without assumptions of uniform observability, which remained open for a long time. We give a solution to the problem in this paper assuming that only the bearing of a feature point, and biased linear acceleration and rotational velocity of a robot -- all in the body-fixed frame -- are available. Further, in contrast to existing related results, we do not need the value of the gravitational constant either. The proposed approach builds upon the parameter estimation-based observer recently developed in (Ortega et al., Syst. Control. Lett., vol.85, 2015) and its extension to matrix Lie groups in our previous work. Conditions on the robot trajectory under which the observer converges are given, and these are strictly weaker than the standard persistency of excitation and uniform complete observability conditions. Finally, we apply the proposed design to the visual inertial navigation problem. Simulation results are also presented to illustrate our observer design.      
### 23.Nonnegative OPLS for Supervised Design of Filter Banks: Application to Image and Audio Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2112.12280.pdf)
>  Audio or visual data analysis tasks usually have to deal with high-dimensional and nonnegative signals. However, most data analysis methods suffer from overfitting and numerical problems when data have more than a few dimensions needing a dimensionality reduction preprocessing. Moreover, interpretability about how and why filters work for audio or visual applications is a desired property, especially when energy or spectral signals are involved. In these cases, due to the nature of these signals, the nonnegativity of the filter weights is a desired property to better understand its working. Because of these two necessities, we propose different methods to reduce the dimensionality of data while the nonnegativity and interpretability of the solution are assured. In particular, we propose a generalized methodology to design filter banks in a supervised way for applications dealing with nonnegative data, and we explore different ways of solving the proposed objective function consisting of a nonnegative version of the orthonormalized partial least-squares method. We analyze the discriminative power of the features obtained with the proposed methods for two different and widely studied applications: texture and music genre classification. Furthermore, we compare the filter banks achieved by our methods with other state-of-the-art methods specifically designed for feature extraction.      
### 24.Entropy-Regularized Partially Observed Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2112.12255.pdf)
>  We investigate partially observed Markov decision processes (POMDPs) with cost functions regularized by entropy terms describing state, observation, and control uncertainty. Standard POMDP techniques are shown to offer bounded-error solutions to these entropy-regularized POMDPs, with exact solutions when the regularization involves the joint entropy of the state, observation, and control trajectories. Our joint-entropy result is particularly surprising since it constitutes a novel, tractable formulation of active state estimation.      
### 25.Combinations of Adaptive Filters  [ :arrow_down: ](https://arxiv.org/pdf/2112.12245.pdf)
>  Adaptive filters are at the core of many signal processing applications, ranging from acoustic noise supression to echo cancelation, array beamforming, channel equalization, to more recent sensor network applications in surveillance, target localization, and tracking. A trending approach in this direction is to recur to in-network distributed processing in which individual nodes implement adaptation rules and diffuse their estimation to the network. <br>When the a priori knowledge about the filtering scenario is limited or imprecise, selecting the most adequate filter structure and adjusting its parameters becomes a challenging task, and erroneous choices can lead to inadequate performance. To address this difficulty, one useful approach is to rely on combinations of adaptive structures. <br>The combination of adaptive filters exploits to some extent the same divide and conquer principle that has also been successfully exploited by the machine-learning community (e.g., in bagging or boosting). In particular, the problem of combining the outputs of several learning algorithms (mixture of experts) has been studied in the computational learning field under a different perspective: rather than studying the expected performance of the mixture, deterministic bounds are derived that apply to individual sequences and, therefore, reflect worst-case scenarios. These bounds require assumptions different from the ones typically used in adaptive filtering, which is the emphasis of this overview article. We review the key ideas and principles behind these combination schemes, with emphasis on design rules. We also illustrate their performance with a variety of examples.      
### 26.A review of data-driven short-term voltage stability assessment of power systems: Concept, principle, and challenges  [ :arrow_down: ](https://arxiv.org/pdf/2112.12240.pdf)
>  With the rapid growth of power market reform and power demand, the power transmission capacity of a power grid is approaching its limit, and the secure and stable operation of power systems becomes increasingly important. In particular, in modern power grids, the proportion of dynamic loads with fast recovery characteristics such as air conditioners, refrigerators, and industrial motors is increasing. As well as the increasing proportion of different forms of renewable energy in power systems. Therefore, short-term voltage stability (STVS) of power systems cannot be ignored. This article comprehensively sorts out the STVS problems of power systems from the perspective of data-driven methods and discusses existing challenges.      
### 27.Data-driven Distributed and Localized Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2112.12229.pdf)
>  Motivated by large-scale but computationally constrained settings, e.g., the Internet of Things, we present a novel data-driven distributed control algorithm that is synthesized directly from trajectory data. Our method, data-driven Distributed and Localized Model Predictive Control (D$^3$LMPC), builds upon the data-driven System Level Synthesis (SLS) framework, which allows one to parameterize \emph{closed-loop} system responses directly from collected open-loop trajectories. The resulting model-predictive controller can be implemented with distributed computation and only local information sharing. By imposing locality constraints on the system response, we show that the amount of data needed for our synthesis problem is independent of the size of the global system. Moreover, we show that our algorithm enjoys theoretical guarantees for recursive feasibility and asymptotic stability. Finally, we also demonstrate the optimality and scalability of our algorithm in a simulation experiment.      
### 28.Person Detection in Collaborative Group Learning Environments Using Multiple Representations  [ :arrow_down: ](https://arxiv.org/pdf/2112.12217.pdf)
>  We introduce the problem of detecting a group of students from classroom videos. The problem requires the detection of students from different angles and the separation of the group from other groups in long videos (one to one and a half hours). We use multiple image representations to solve the problem. We use FM components to separate each group from background groups, AM-FM components for detecting the back-of-the-head, and YOLO for face detection. We use classroom videos from four different groups to validate our approach. Our use of multiple representations is shown to be significantly more accurate than the use of YOLO alone.      
### 29.Electromagnetic neural source imaging under sparsity constraints with SURE-based hyperparameter tuning  [ :arrow_down: ](https://arxiv.org/pdf/2112.12178.pdf)
>  Estimators based on non-convex sparsity-promoting penalties were shown to yield state-of-the-art solutions to the magneto-/electroencephalography (M/EEG) brain source localization problem. In this paper we tackle the model selection problem of these estimators: we propose to use a proxy of the Stein's Unbiased Risk Estimator (SURE) to automatically select their regularization parameters. The effectiveness of the method is demonstrated on realistic simulations and $30$ subjects from the Cam-CAN dataset. To our knowledge, this is the first time that sparsity promoting estimators are automatically calibrated at such a scale. Results show that the proposed SURE approach outperforms cross-validation strategies and state-of-the-art Bayesian statistics methods both computationally and statistically.      
### 30.Capacity Bounds under Imperfect Polarization Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2112.12661.pdf)
>  In optical fiber communication, due to the random variation of the environment, the state of polarization (SOP) fluctuates randomly with time leading to distortion and performance degradation. The memory-less SOP fluctuations can be regarded as a two-by-two random unitary matrix. In this paper, for what we believe to be the first time, the capacity of the polarization drift channel under an average power constraint with imperfect channel knowledge is characterized. An achievable information rate (AIR) is derived when imperfect channel knowledge is available and is shown to be highly dependent on the channel estimation technique. It is also shown that a tighter lower bound can be achieved when a unitary estimation of the channel is available. However, the conventional estimation algorithms do not guarantee a unitary channel estimation. Therefore, by considering the unitary constraint of the channel, a data-aided channel estimator based on the Kabsch algorithm is proposed, and its performance is numerically evaluated in terms of AIR. Monte Carlo simulations show that Kabsch outperforms the least-square error algorithm. In particular, with complex, Gaussian inputs and eight pilot symbols per block, Kabsch improves the AIR by 0:2 to 0:35 bits/symbol throughout the range of studied signal-to-noise ratios.      
### 31.Artifacts in optical projection tomography due to refractive index mismatch: model and correction  [ :arrow_down: ](https://arxiv.org/pdf/2112.12602.pdf)
>  Optical Projection Tomography (OPT) is a powerful tool for 3D imaging of mesoscopic samples, thus of great importance to image whole organs for the study of various disease models in life sciences. OPT is able to achieve resolution at a few tens of microns over a large sample volume of several cubic centimeters. However, the reconstructed OPT images often suffer from artifacts caused by different kinds of physical miscalibration. This work focuses on the refractive index (RI) mismatch between the rotating object and the surrounding medium. We derive a 3D cone beam forward model to approximate the effect of RI mismatch and implement a fast and efficient reconstruction method to correct the induced seagull-shaped artifacts on experimental images of fluorescent beads.      
### 32.INTRPRT: A Systematic Review of and Guidelines for Designing and Validating Transparent AI in Medical Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2112.12596.pdf)
>  Transparency in Machine Learning (ML), attempts to reveal the working mechanisms of complex models. Transparent ML promises to advance human factors engineering goals of human-centered AI in the target users. From a human-centered design perspective, transparency is not a property of the ML model but an affordance, i.e. a relationship between algorithm and user; as a result, iterative prototyping and evaluation with users is critical to attaining adequate solutions that afford transparency. However, following human-centered design principles in healthcare and medical image analysis is challenging due to the limited availability of and access to end users. To investigate the state of transparent ML in medical image analysis, we conducted a systematic review of the literature. Our review reveals multiple severe shortcomings in the design and validation of transparent ML for medical image analysis applications. We find that most studies to date approach transparency as a property of the model itself, similar to task performance, without considering end users during neither development nor evaluation. Additionally, the lack of user research, and the sporadic validation of transparency claims put contemporary research on transparent ML for medical image analysis at risk of being incomprehensible to users, and thus, clinically irrelevant. To alleviate these shortcomings in forthcoming research while acknowledging the challenges of human-centered design in healthcare, we introduce the INTRPRT guideline, a systematic design directive for transparent ML systems in medical image analysis. The INTRPRT guideline suggests formative user research as the first step of transparent model design to understand user needs and domain requirements. Following this process produces evidence to support design choices, and ultimately, increases the likelihood that the algorithms afford transparency.      
### 33.Attention Based Communication and Control for Multi-UAV Path Planning  [ :arrow_down: ](https://arxiv.org/pdf/2112.12584.pdf)
>  Inspired by the multi-head attention (MHA) mechanism in natural language processing, this letter proposes an iterative single-head attention (ISHA) mechanism for multi-UAV path planning. The ISHA mechanism is run by a communication helper collecting the state embeddings of UAVs and distributing an attention score vector to each UAV. The attention scores computed by ISHA identify how many interactions with other UAVs should be considered in each UAV's control decision-making. Simulation results corroborate that the ISHA-based communication and control framework achieves faster travel with lower inter-UAV collision risks than an MHA-aided baseline, particularly under limited communication resources.      
### 34.Data Augmentation based Consistency Contrastive Pre-training for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2112.12522.pdf)
>  Self-supervised acoustic pre-training has achieved amazing results on the automatic speech recognition (ASR) task. Most of the successful acoustic pre-training methods use contrastive learning to learn the acoustic representations by distinguish the representations from different time steps, ignoring the speaker and environment robustness. As a result, the pre-trained model could show poor performance when meeting out-of-domain data during fine-tuning. In this letter, we design a novel consistency contrastive learning (CCL) method by utilizing data augmentation for acoustic pre-training. Different kinds of augmentation are applied on the original audios and then the augmented audios are fed into an encoder. The encoder should not only contrast the representations within one audio but also maximize the measurement of the representations across different augmented audios. By this way, the pre-trained model can learn a text-related representation method which is more robust with the change of the speaker or the environment.Experiments show that by applying the CCL method on the Wav2Vec2.0, better results can be realized both on the in-domain data and the out-of-domain data. Especially for noisy out-of-domain data, more than 15% relative improvement can be obtained.      
### 35.S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation  [ :arrow_down: ](https://arxiv.org/pdf/2112.12389.pdf)
>  Emotion recognition in conversation (ERC) has attracted much attention in recent years for its necessity in widespread applications. Existing ERC methods mostly model the self and inter-speaker context separately, posing a major issue for lacking enough interaction between them. In this paper, we propose a novel Speaker and Position-Aware Graph neural network model for ERC (S+PAGE), which contains three stages to combine the benefits of both Transformer and relational graph convolution network (R-GCN) for better contextual modeling. Firstly, a two-stream conversational Transformer is presented to extract the coarse self and inter-speaker contextual features for each utterance. Then, a speaker and position-aware conversation graph is constructed, and we propose an enhanced R-GCN model, called PAG, to refine the coarse features guided by a relative positional encoding. Finally, both of the features from the former two stages are input into a conditional random field layer to model the emotion transfer.      
### 36.Graph attentive feature aggregation for text-independent speaker verification  [ :arrow_down: ](https://arxiv.org/pdf/2112.12343.pdf)
>  The objective of this paper is to combine multiple frame-level features into a single utterance-level representation considering pairwise relationship. For this purpose, we propose a novel graph attentive feature aggregation module by interpreting each frame-level feature as a node of a graph. The inter-relationship between all possible pairs of features, typically exploited indirectly, can be directly modeled using a graph. The module comprises a graph attention layer and a graph pooling layer followed by a readout operation. The graph attention layer first models the non-Euclidean data manifold between different nodes. Then, the graph pooling layer discards less informative nodes considering the significance of the nodes. Finally, the readout operation combines the remaining nodes into a single representation. We employ two recent systems, SE-ResNet and RawNet2, with different input features and architectures and demonstrate that the proposed feature aggregation module consistently shows a relative improvement over 10%, compared to the baseline.      
### 37.On the Detection of Markov Decision Processes  [ :arrow_down: ](https://arxiv.org/pdf/2112.12338.pdf)
>  We study the detection problem for a finite set of Markov decision processes (MDPs) where the MDPs have the same state and action spaces but possibly different probabilistic transition functions. Any one of these MDPs could be the model for some underlying controlled stochastic process, but it is unknown a priori which MDP is the ground truth. We investigate whether it is possible to asymptotically detect the ground truth MDP model perfectly based on a single observed history (state-action sequence). Since the generation of histories depends on the policy adopted to control the MDPs, we discuss the existence and synthesis of policies that allow for perfect detection. We start with the case of two MDPs and establish a necessary and sufficient condition for the existence of policies that lead to perfect detection. Based on this condition, we then develop an algorithm that efficiently (in time polynomial in the size of the MDPs) determines the existence of policies and synthesizes one when they exist. We further extend the results to the more general case where there are more than two MDPs in the candidate set, and we develop a policy synthesis algorithm based on the breadth-first search and recursion. We demonstrate the effectiveness of our algorithms through numerical examples.      
### 38.Analysis of ECG data to detect Atrial Fibrillation  [ :arrow_down: ](https://arxiv.org/pdf/2112.12298.pdf)
>  Atrial fibrillation(termed as AF/Afib henceforth) is a discrete and often rapid heart rhythm that can lead to clots near the heart. We can detect Afib by ECG signal by the absence of p and inconsistent intervals between R waves as shown in fig(1). Existing methods revolve around CNN that are used to detect afib but most of them work with 12 point lead ECG data where in our case the health gauge watch deals with single-point ECG data. Twelve-point lead ECG data is more accurate than a single point. Furthermore, the health gauge watch data is much noisier. Implementing a model to detect Afib for the watch is a test of how the CNN is changed/modified to work with real life data      
### 39.Sub-Chain Beam for mmWave Devices: A Trade-off between Power Saving and Beam Correspondence  [ :arrow_down: ](https://arxiv.org/pdf/2112.12296.pdf)
>  Beam correspondence, or downlink-uplink (DL-UL) beam reciprocity, refers to the assumption that the best beams in the DL are also the best beams in the UL. This is an important assumption that allows the existing beam management framework in 5G to rely heavily on DL beam sweeping and avoid UL beam sweeping: UL beams are inferred from the measurements of the DL reference signals. Beam correspondence holds when the radio configurations are symmetric in the DL and UL. However, as mmWave technology matures, the DL and the UL face different constraints often breaking the beam correspondence. For example, power constraints may require a UE to activate only a portion of its antenna array for UL transmission, while still activating the full array for DL reception. Meanwhile, if the UL beam with sub-array, named as sub-chain beam in this paper, has a similar radiation pattern as the DL beam, the beam correspondence can still hold. This paper proposes methods for sub-chain beam codebook design to achieve a trade-off between the power saving and beam correspondence.      
### 40.Safety and Liveness Guarantees through Reach-Avoid Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2112.12288.pdf)
>  Reach-avoid optimal control problems, in which the system must reach certain goal conditions while staying clear of unacceptable failure modes, are central to safety and liveness assurance for autonomous robotic systems, but their exact solutions are intractable for complex dynamics and environments. Recent successes in reinforcement learning methods to approximately solve optimal control problems with performance objectives make their application to certification problems attractive; however, the Lagrange-type objective used in reinforcement learning is not suitable to encode temporal logic requirements. Recent work has shown promise in extending the reinforcement learning machinery to safety-type problems, whose objective is not a sum, but a minimum (or maximum) over time. In this work, we generalize the reinforcement learning formulation to handle all optimal control problems in the reach-avoid category. We derive a time-discounted reach-avoid Bellman backup with contraction mapping properties and prove that the resulting reach-avoid Q-learning algorithm converges under analogous conditions to the traditional Lagrange-type problem, yielding an arbitrarily tight conservative approximation to the reach-avoid set. We further demonstrate the use of this formulation with deep reinforcement learning methods, retaining zero-violation guarantees by treating the approximate solutions as untrusted oracles in a model-predictive supervisory control framework. We evaluate our proposed framework on a range of nonlinear systems, validating the results against analytic and numerical solutions, and through Monte Carlo simulation in previously intractable problems. Our results open the door to a range of learning-based methods for safe-and-live autonomous behavior, with applications across robotics and automation. See <a class="link-external link-https" href="https://github.com/SafeRoboticsLab/safety_rl" rel="external noopener nofollow">this https URL</a> for code and supplementary material.      
### 41.A Survey on Perceptually Optimized Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2112.12284.pdf)
>  Videos are developing in the trends of Ultra High Definition (UHD), High Frame Rate (HFR), High Dynamic Range (HDR), Wide Color Gammut (WCG) and high fidelity, which provide users with more realistic visual experiences. However, the amount of video data increases exponentially and requires high efficiency video compression for storage and network transmission. Perceptually optimized video coding aims to exploit visual redundancies in videos so as to maximize compression efficiency. In this paper, we present a systematic survey on the recent advances and challenges on perceptually optimized video coding. Firstly, we present problem formulation and framework of perceptually optimized video coding, which includes visual perception modelling, visual quality assessment and perception guided coding optimization. Secondly, the recent advances on visual factors, key computational visual models and quality assessment models are presented. Thirdly, we do systematic review on perceptual video coding optimizations from four key aspects, which includes perceptually optimized bit allocation, rate-distortion optimization, transform and quantization, filtering and enhancement. In each part, problem formulation, working flow, recent advances, advantages and challenges are presented. Fourthly, perceptual coding performance of latest coding standards and tools are experimentally analyzed. Finally, challenging issues and future opportunities on perceptual video coding are identified.      
### 42.Perceptual Evaluation of 360 Audiovisual Quality and Machine Learning Predictions  [ :arrow_down: ](https://arxiv.org/pdf/2112.12273.pdf)
>  In an earlier study, we gathered perceptual evaluations of the audio, video, and audiovisual quality for 360 audiovisual content. This paper investigates perceived audiovisual quality prediction based on objective quality metrics and subjective scores of 360 video and spatial audio content. Thirteen objective video quality metrics and three objective audio quality metrics were evaluated for five stimuli for each coding parameter. Four regression-based machine learning models were trained and tested here, i.e., multiple linear regression, decision tree, random forest, and support vector machine. Each model was constructed using a combination of audio and video quality metrics and two cross-validation methods (k-Fold and Leave-One-Out) were investigated and produced 312 predictive models. The results indicate that the model based on the evaluation of VMAF and AMBIQUAL is better than other combinations of audio-video quality metric. In this study, support vector machine provides higher performance using k-Fold (PCC = 0.909, SROCC = 0.914, and RMSE = 0.416). These results can provide insights for the design of multimedia quality metrics and the development of predictive models for audiovisual omnidirectional media.      
### 43.Real-Time Multi-Convex Model Predictive Control for Occlusion Free Target Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2112.12177.pdf)
>  This paper proposes a Model Predictive Control (MPC) algorithm for target tracking amongst static and dynamic obstacles. Our main contribution lies in improving the computational tractability and reliability of the underlying non-convex trajectory optimization. The result is an MPC algorithm that runs real-time on laptops and embedded hardware devices such as Jetson TX2. Our approach relies on novel reformulations for the tracking, collision, and occlusion constraints that induce a multi-convex structure in the resulting trajectory optimization. We exploit these mathematical structures using the split Bregman Iteration technique, eventually reducing our MPC to a series of convex Quadratic Programs solvable in a few milliseconds. The fast re-planning of our MPC allows for occlusion and collision-free tracking in complex environments even while considering a simple constant-velocity prediction for the target trajectory and dynamic obstacles. We perform extensive bench-marking in a realistic physics engine and show that our MPC outperforms the state-of-the-art algorithms in visibility, smoothness, and computation-time metrics.      
### 44.Signaling Design for MIMO-NOMA with Different Security Requirements  [ :arrow_down: ](https://arxiv.org/pdf/2112.12166.pdf)
>  Signaling design for secure transmission in two-user multiple-input multiple-output (MIMO) non-orthogonal multiple access (NOMA) networks is investigated in this paper. The base station broadcasts multicast data to all users and also integrates additional services, unicast data targeted to certain users, and confidential data protected against eavesdroppers. We categorize the above MIMO-NOMA with different security requirements into several communication scenarios. The associated problem in each scenario is nonconvex. We propose a unified approach, called the power splitting scheme, for optimizing the rate equations corresponding to the scenarios. The proposed method decomposes the optimization of the secure MIMO-NOMA channel into a set of simpler problems, including multicast, point-to-point, and wiretap MIMO problems, corresponding to the three basic messages: multicast, private/unicast, and confidential messages. We then leverage existing solutions to design signaling for the above problems such that the messages are transmitted with high security and reliability. Numerical results illustrate the efficacy of the proposed covariance matrix design in secure MIMO-NOMA transmission. The proposed method also outperforms existing solutions, when applicable. <br>In the case of no multicast messages, we also reformulate the nonconvex problem into weighted sum rate (WSR) maximization problems by applying the block successive maximization method and generalizing the zero duality gap. The two methods have their advantages and limitations. Power splitting is a general tool that can be applied to the MIMO-NOMA with any combination of the three messages (multicast, private, and confidential) whereas WSR maximization shows greater potential for secure MIMO-NOMA communication without multicasting. In such cases, WSR maximization provides a slightly better rate than the power splitting method.      
