# ArXiv eess --Wed, 25 Aug 2021
### 1.A Techno-Economic Cost Framework for Satellite Networks Applied to Low Earth Orbit Constellations: Assessing Starlink, OneWeb and Kuiper  [ :arrow_down: ](https://arxiv.org/pdf/2108.10834.pdf)
>  Delivering broadband connectivity to unconnected areas is extremely challenging. The emergence of Low Earth Orbit (LEO) satellite systems has been seen as a potential solution for connecting remote areas. Despite the hype around these new technologies, we still lack an open-source modeling framework for assessing the techno-economics of satellite broadband connectivity which is therefore the purpose of this paper. Firstly, a generalizable techno-economic model is presented to assess the engineering-economics of satellite constellations. Secondly, the approach is applied to assess the three main competing LEO constellations which include Starlink, OneWeb and Kuiper. This involves simulating the impact on coverage, capacity and cost as both the number of satellites and quantity of subscribers increases. Finally, a global assessment is undertaken visualizing the potential capacity and cost per user via different subscriber scenarios. The results demonstrate how limited the capacity will be once resources are spread across users in each satellite coverage area. For example, if there is 1 user per 10 km^2 we estimate a mean per user capacity of 24.94 Mbps, 1.01 Mbps and 10.30 Mbps for Starlink, OneWeb and Kuiper respectively in the busiest hour of the day. But if the subscriber density increases to 1 user per km^2, then the mean per user capacity drops significantly to 2.49 Mbps, 0.10 Mbps and 1.02 Mbps for Starlink, OneWeb and Kuiper respectively. LEO broadband will be an essential part of the connectivity toolkit, but the results reveal that these mega-constellations will most likely have to operate below 0.1 users per km^2 to provide a service that outcompetes other broadband connectivity options. The open-source codebase which the paper contributes is provided with the hope that other engineers will access, use, and further develop the satellite assessment capability.      
### 2.Economic and technical study for the construction of a 1 MW grid-connected solar power plant in southern Iran  [ :arrow_down: ](https://arxiv.org/pdf/2108.10815.pdf)
>  Renewable energy such as solar and wind energy can solve the major problems of humanity such as electricity and fresh water. The renewable energy sources are promising to take a significant share in the energy sector as a viable option for integration with conventional fossil fuel power plants. In this paper, the production of 1 MW of electricity for several households in the city of Sirjan in southern Iran has been studied. The actual data required by the model including solar irradiation, air temperature, load profile, cost of energy for Sirjan, Iran have been utilized in the proposed model. Considering the Iranian market, the fixed and current costs of building this power plant have been studied. Then, using Hummer software, the amount of electricity production per month has been studied. Results showed that Scaled data were used for calculations in HOMER. It had a scaled annual average of 1127 kWh/day and the peak load was 0.467 kW. The maximum electricity energy is obtained in July.      
### 3.Nonhomogeneous Stochastic Geometry Analysis of Massive LEO Communication Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2108.10785.pdf)
>  Providing truly ubiquitous connectivity requires development of low Earth orbit (LEO) satellite Internet, whose theoretical study is lagging behind network-specific simulations. In this paper, we derive analytical expressions for the downlink coverage probability and average data rate of a massive inclined LEO constellation in terms of total interference power's Laplace transform in the presence of fading and shadowing, ergo presenting a stochastic geometry based analysis. We assume the desired link to experience Nakagami m fading, which serves to represent different fading scenarios by varying integer m, while the interfering channels can follow any fading model without an effect on analytical tractability. To take into account the inherent nonuniform distribution of satellites across different latitudes, we model the LEO network as a nonhomogeneous Poisson point process with its intensity being a function of satellites' actual distribution in terms of constellation size, the altitude of the constellation, and the inclination of orbital planes. From the numerical results, we observe optimum points for both the constellation altitude and the number of orthogonal frequency channels; interestingly, the optimum user's latitude is greater than the inclination angle due to the presence of fewer interfering satellites. Overall, the presented study facilitates general stochastic evaluation and planning of satellite Internet constellations without specific orbital simulations or tracking data on satellites' exact positions in space.      
### 4.DU-GAN: Generative Adversarial Networks with Dual-Domain U-Net Based Discriminators for Low-Dose CT Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2108.10772.pdf)
>  LDCT has drawn major attention in the medical imaging field due to the potential health risks of CT-associated X-ray radiation to patients. Reducing the radiation dose, however, decreases the quality of the reconstructed images, which consequently compromises the diagnostic performance. Various deep learning techniques have been introduced to improve the image quality of LDCT images through denoising. GANs-based denoising methods usually leverage an additional classification network, i.e. discriminator, to learn the most discriminate difference between the denoised and normal-dose images and, hence, regularize the denoising model accordingly; it often focuses either on the global structure or local details. To better regularize the LDCT denoising model, this paper proposes a novel method, termed DU-GAN, which leverages U-Net based discriminators in the GANs framework to learn both global and local difference between the denoised and normal-dose images in both image and gradient domains. The merit of such a U-Net based discriminator is that it can not only provide the per-pixel feedback to the denoising network through the outputs of the U-Net but also focus on the global structure in a semantic level through the middle layer of the U-Net. In addition to the adversarial training in the image domain, we also apply another U-Net based discriminator in the image gradient domain to alleviate the artifacts caused by photon starvation and enhance the edge of the denoised CT images. Furthermore, the CutMix technique enables the per-pixel outputs of the U-Net based discriminator to provide radiologists with a confidence map to visualize the uncertainty of the denoised results, facilitating the LDCT-based screening and diagnosis. Extensive experiments on the simulated and real-world datasets demonstrate superior performance over recently published methods both qualitatively and quantitatively.      
### 5.Data-driven predictive control with reduced computational effort and improved performance using segmented trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2108.10753.pdf)
>  A class of data-driven control methods has recently emerged based on Willems' fundamental lemma. Such methods can ease the modelling burden in control design but can be sensitive to disturbances acting on the system under control. In this paper, we extend these methods to incorporate segmented prediction trajectories. The proposed segmentation enables longer prediction horizons to be used in the presence of unmeasured disturbance. Furthermore, a computation time reduction can be achieved through segmentation by exploiting the problem structure, with computation time scaling linearly with increasing horizon length. The performance characteristics are illustrated in a set-point tracking case study in which the segmented formulation enables more consistent performance over a wide range of prediction horizons. The computation time for the segmented formulation is approximately half that of an unsegmented formulation for a horizon of 100 samples. The method is then applied to a building energy management problem, using a detailed simulation environment, in which we seek to minimise the discomfort and energy of a 6-room apartment. With the segmented formulation, a 72% reduction in discomfort and 5% financial cost reduction is achieved, compared to an unsegmented formulation using a one-day-ahead prediction horizon.      
### 6.Generalizing RNN-Transducer to Out-Domain Audio via Sparse Self-Attention Layers  [ :arrow_down: ](https://arxiv.org/pdf/2108.10752.pdf)
>  Recurrent neural network transducers (RNN-T) are a promising end-to-end speech recognition framework that transduces input acoustic frames into a character sequence. The state-of-the-art encoder network for RNN-T is the Conformer, which can effectively model the local-global context information via its convolution and self-attention layers. Although Conformer RNN-T has shown outstanding performance (measured by word error rate (WER) in general), most studies have been verified in the setting where the train and test data are drawn from the same domain. The domain mismatch problem for Conformer RNN-T has not been intensively investigated yet, which is an important issue for the product-level speech recognition system. In this study, we identified that fully connected self-attention layers in the Conformer caused high deletion errors, specifically in the long-form out-domain utterances. To address this problem, we introduce sparse self-attention layers for Conformer-based encoder networks, which can exploit local and generalized global information by pruning most of the in-domain fitted global connections. Further, we propose a state reset method for the generalization of the prediction network to cope with long-form utterances. Applying proposed methods to an out-domain test, we obtained 24.6\% and 6.5\% relative character error rate (CER) reduction compared to the fully connected and local self-attention layer-based Conformers, respectively.      
### 7.Principle-driven Fiber Transmission Model based on PINN Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2108.10734.pdf)
>  In this paper, a novel principle-driven fiber transmission model based on physical induced neural network (PINN) is proposed. Unlike data-driven models which regard fiber transmission problem as data regression tasks, this model views it as an equation solving problem. Instead of adopting input signals and output signals which are calculated by SSFM algorithm in advance before training, this principle-driven PINN based fiber model adopts frames of time and distance as its inputs and the corresponding real and imaginary parts of NLSE solutions as its outputs. By taking into account of pulses and signals before transmission as initial conditions and fiber physical principles as NLSE in the design of loss functions, this model will progressively learn the transmission rules. Therefore, it can be effectively trained without the data labels, referred as the pre-calculated signals after transmission in data-driven models. Due to this advantage, SSFM algorithm is no longer needed before the training of principle-driven fiber model which can save considerable time consumption. Through numerical demonstration, the results show that this principle-driven PINN based fiber model can handle the prediction tasks of pulse evolution, signal transmission and fiber birefringence for different transmission parameters of fiber telecommunications.      
### 8.Curricular SincNet: Towards Robust Deep Speaker Recognition by Emphasizing Hard Samples in Latent Space  [ :arrow_down: ](https://arxiv.org/pdf/2108.10714.pdf)
>  Deep learning models have become an increasingly preferred option for biometric recognition systems, such as speaker recognition. SincNet, a deep neural network architecture, gained popularity in speaker recognition tasks due to its parameterized sinc functions that allow it to work directly on the speech signal. The original SincNet architecture uses the softmax loss, which may not be the most suitable choice for recognition-based tasks. Such loss functions do not impose inter-class margins nor differentiate between easy and hard training samples. Curriculum learning, particularly those leveraging angular margin-based losses, has proven very successful in other biometric applications such as face recognition. The advantage of such a curriculum learning-based techniques is that it will impose inter-class margins as well as taking to account easy and hard samples. In this paper, we propose Curricular SincNet (CL-SincNet), an improved SincNet model where we use a curricular loss function to train the SincNet architecture. The proposed model is evaluated on multiple datasets using intra-dataset and inter-dataset evaluation protocols. In both settings, the model performs competitively with other previously published work. In the case of inter-dataset testing, it achieves the best overall results with a reduction of 4\% error rate compare to SincNet and other published work.      
### 9.Time Dependence in Kalman Filter Tuning  [ :arrow_down: ](https://arxiv.org/pdf/2108.10712.pdf)
>  In this paper, we propose an approach to address the problems with ambiguity in tuning the process and observation noises for a discrete-time linear Kalman filter. Conventional approaches to tuning (e.g. using normalized estimation error squared and covariance minimization) compute empirical measures of filter performance and the parameter are selected manually or selected using some kind of optimization algorithm to maximize these measures of performance. However, there are two challenges with this approach. First, in theory, many of these measures do not guarantee a unique solution due to observability issues. Second, in practice, empirically computed statistical quantities can be very noisy due to a finite number of samples. We propose a method to overcome these limitations. Our method has two main parts to it. The first is to ensure that the tuning problem has a single unique solution. We achieve this by simultaneously tuning the filter over multiple different prediction intervals. Although this yields a unique solution, practical issues (such as sampling noise) mean that it cannot be directly applied. Therefore, we use Bayesian Optimization. This technique handles noisy data and the local minima that it introduces.      
### 10.On a Low-Frequency and Contrast Stabilized Full-Wave Volume Integral Equation Solver for Lossy Media  [ :arrow_down: ](https://arxiv.org/pdf/2108.10690.pdf)
>  In this paper we present a new regularized electric flux volume integral equation (D-VIE) for modeling high-contrast conductive dielectric objects in a broad frequency range. This new formulation is particularly suitable for modeling biological tissues at low frequencies, as it is required by brain epileptogenic area imaging, but also at higher ones, as it is required by several applications including, but not limited to, transcranial magnetic and deep brain stimulation (TMS and DBS, respectively). When modeling inhomogeneous objects with high complex permittivities at low frequencies, the traditional D-VIE is ill-conditioned and suffers from numerical instabilities that result in slower convergence and in less accurate solutions. In this work we address these shortcomings by leveraging a new set of volume quasi-Helmholtz projectors. Their scaling by the material permittivity matrix allows for the re-balancing of the equation when applied to inhomogeneous scatterers and thereby makes the proposed method accurate and stable even for high complex permittivity objects until arbitrarily low frequencies. Numerical results, canonical and realistic, corroborate the theory and confirm the stability and the accuracy of this new method both in the quasi-static regime and at higher frequencies.      
### 11.Scorpiano -- A System for Automatic Music Transcription for Monophonic Piano Music  [ :arrow_down: ](https://arxiv.org/pdf/2108.10689.pdf)
>  Music transcription is the process of transcribing music audio into music notation. It is a field in which the machines still cannot beat human performance. The main motivation for automatic music transcription is to make it possible for anyone playing a musical instrument, to be able to generate the music notes for a piece of music quickly and accurately. It does not matter if the person is a beginner and simply struggles to find the music score by searching, or an expert who heard a live jazz improvisation and would like to reproduce it without losing time doing manual transcription. We propose Scorpiano -- a system that can automatically generate a music score for simple monophonic piano melody tracks using digital signal processing. The system integrates multiple digital audio processing methods: notes onset detection, tempo estimation, beat detection, pitch detection and finally generation of the music score. The system has proven to give good results for simple piano melodies, comparable to commercially available neural network based systems.      
### 12.Investigation of lightweight acoustic curtains for mid-to-high frequency noise insulations  [ :arrow_down: ](https://arxiv.org/pdf/2108.10683.pdf)
>  The continuous surge of environmental noise levels has become a vital challenge for humanity. Earlier studies have reported that prolonged exposure to loud noise may cause auditory and non-auditory disorders. Therefore, there is a growing demand for suitable noise barriers. Herein, we have investigated several commercially available curtain fabrics' acoustic performance, potentially used for sound insulation purposes. Thorough experimental investigations have been performed on PVC coated polyester fabrics' acoustical performances and 100 % pure PVC sheets. The PVC-coated polyester fabric exhibited better sound insulation properties, particularly in the mid-to-high frequency range (600-1600 Hz) with a transmission loss of about 11 to 22 dB, while insertion loss of &gt; 10 dB has been achieved. Also, the acoustic performance of multi-layer curtains has been investigated. These multi-layer curtains have shown superior acoustic properties to that of single-layer acoustic curtains.      
### 13.Graph Laplacian Diffusion Localization of Connected and Automated Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2108.10678.pdf)
>  In this paper, we design distributed multi-modal localization approaches for Connected and Automated vehicles. We utilize information diffusion on graphs formed by moving vehicles, based on Adapt-then-Combine strategies combined with the Least-Mean-Squares and the Conjugate Gradient algorithms. We treat the vehicular network as an undirected graph, where vehicles communicate with each other by means of Vehicle-to- Vehicle communication protocols. Connected vehicles perform cooperative fusion of different measurement modalities, including location and range measurements, in order to estimate both their positions and the positions of all other networked vehicles, by interacting only with their local neighborhood. The trajectories of vehicles were generated either by a well-known kinematic model, or by using the CARLA autonomous driving simulator. The various proposed distributed and diffusion localization schemes significantly reduce the GPS error and do not only converge to the global solution, but they even outperformed it. Extensive simulation studies highlight the benefits of the various approaches, outperforming the accuracy of the state of the art approaches. The impact of the network connections and the network latency are also investigated.      
### 14.High Precision Indoor Localization with Dummy Antennas -- An Experimental Study  [ :arrow_down: ](https://arxiv.org/pdf/2108.10646.pdf)
>  With the rising demand for indoor localization, high precision technique-based fingerprints became increasingly important nowadays. The newest advanced localization system makes effort to improve localization accuracy in the time or frequency domain, for example, the UWB localization technique can achieve centimeter-level accuracy but have a high cost. Therefore, we present a spatial domain extension-based scheme with low cost and verify the effectiveness of antennas extension in localization accuracy. In this paper, we achieve sub-meter level localization accuracy using a single AP by extending three radio links of the modified laptops to more antennas. Moreover, the experimental results show that the localization performance is superior as the number of antennas increases with the help of spatial domain extension and angular domain assisted.      
### 15.Image-free single-pixel segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2108.10617.pdf)
>  The existing segmentation techniques require high-fidelity images as input to perform semantic segmentation. Since the segmentation results contain most of edge information that is much less than the acquired images, the throughput gap leads to both hardware and software waste. In this letter, we report an image-free single-pixel segmentation technique. The technique combines structured illumination and single-pixel detection together, to efficiently samples and multiplexes scene's segmentation information into compressed one-dimensional measurements. The illumination patterns are optimized together with the subsequent reconstruction neural network, which directly infers segmentation maps from the single-pixel measurements. The end-to-end encoding-and-decoding learning framework enables optimized illumination with corresponding network, which provides both high acquisition and segmentation efficiency. Both simulation and experimental results validate that accurate segmentation can be achieved using two-order-of-magnitude less input data. When the sampling ratio is 1%, the Dice coefficient reaches above 80% and the pixel accuracy reaches above 96%. We envision that this image-free segmentation technique can be widely applied in various resource-limited platforms such as UAV and unmanned vehicle that require real-time sensing.      
### 16.On the Sum of Extended $η$-$μ$ Variates with MRC Applications  [ :arrow_down: ](https://arxiv.org/pdf/2108.10610.pdf)
>  In this paper, the sum of L independent but not necessarily identically distributed (i.n.i.d.) extended $\eta$-$\mu$ variates is considered. In particular, novel expressions for the probability density function and cumulative distribution function are derived in closed-forms. The derived expressions are represented in two different forms, i.e., in terms of confluent multivariate hypergeometric function and general Fox's H-function. Subsequently, closed-form expressions for the outage probability and average symbol error rate are derived. Our analytical results are validated by some numerical and Monte-Carlo simulation results.      
### 17.Lossy Medical Image Compression using Residual Learning-based Dual Autoencoder Model  [ :arrow_down: ](https://arxiv.org/pdf/2108.10579.pdf)
>  In this work, we propose a two-stage autoencoder based compressor-decompressor framework for compressing malaria RBC cell image patches. We know that the medical images used for disease diagnosis are around multiple gigabytes size, which is quite huge. The proposed residual-based dual autoencoder network is trained to extract the unique features which are then used to reconstruct the original image through the decompressor module. The two latent space representations (first for the original image and second for the residual image) are used to rebuild the final original image. Color-SSIM has been exclusively used to check the quality of the chrominance part of the cell images after decompression. The empirical results indicate that the proposed work outperformed other neural network related compression technique for medical images by approximately 35%, 10% and 5% in PSNR, Color SSIM and MS-SSIM respectively. The algorithm exhibits a significant improvement in bit savings of 76%, 78%, 75% &amp; 74% over JPEG-LS, JP2K-LM, CALIC and recent neural network approach respectively, making it a good compression-decompression technique.      
### 18.Communication Modes with Large Intelligent Surfaces in the Near Field  [ :arrow_down: ](https://arxiv.org/pdf/2108.10569.pdf)
>  This paper proposes a practical method for the definition of multiple communication modes when antennas operate in the near-field region, by realizing ad-hoc beams exploiting the focusing capability of large antennas. The beamspace modeling proposed to define the communication modes is then exploited to derive expressions for the number of communication modes (i.e., degrees of freedom) in a generic setup, beyond the traditional paraxial approximation, together with closed-form definitions for the basis set at the transmitting and receiving antennas for several cases of interest, such as for the communication between a large antenna and a small antenna. Numerical results indicate that quasi-optimal communication can be obtained starting from focusing functions. This translates into the possibility of a significant enhancement of the channel capacity even in line-of-sight channel condition without the need of resorting to optimal but complex phase/amplitude antenna profiles as well as intensive numerical simulations. Traditional results valid under paraxial approximation are revised in light of the proposed modeling, showing that similar conclusions can be obtained from different perspectives.      
### 19.MIMO OFDM Dual-Function Radar-Communication Under Error Rate and Beampattern Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2108.10555.pdf)
>  In this work we consider a multiple-input multiple-output (MIMO) dual-function radar-communication (DFRC) system that employs an orthogonal frequency division multiplexing (OFDM) and a differential phase shift keying (DPSK) modulation, and study the design of the radiated waveforms and of the receive filters employed by the radar and the users. The approach is communication-centric, in the sense that a radar-oriented objective is optimized under constraints on the average transmit power, the power leakage towards specific directions, and the error rate of each user, thus safeguarding the communication quality of service (QoS). We adopt a unified design approach allowing a broad family of radar objectives, including both estimation- and detection-oriented merit functions. We devise a suboptimal solution based on alternating optimization of the involved variables, a convex restriction of the feasible search set, and minorization-maximization, offering a single algorithm for all of the radar merit functions in the considered family. Finally, the performance is inspected through numerical examples.      
### 20.Lossless Image Compression Using a Multi-Scale Progressive Statistical Model  [ :arrow_down: ](https://arxiv.org/pdf/2108.10551.pdf)
>  Lossless image compression is an important technique for image storage and transmission when information loss is not allowed. With the fast development of deep learning techniques, deep neural networks have been used in this field to achieve a higher compression rate. Methods based on pixel-wise autoregressive statistical models have shown good performance. However, the sequential processing way prevents these methods to be used in practice. Recently, multi-scale autoregressive models have been proposed to address this limitation. Multi-scale approaches can use parallel computing systems efficiently and build practical systems. Nevertheless, these approaches sacrifice compression performance in exchange for speed. In this paper, we propose a multi-scale progressive statistical model that takes advantage of the pixel-wise approach and the multi-scale approach. We developed a flexible mechanism where the processing order of the pixels can be adjusted easily. Our proposed method outperforms the state-of-the-art lossless image compression methods on two large benchmark datasets by a significant margin without degrading the inference speed dramatically.      
### 21.A generative adversarial approach to facilitate archival-quality histopathologic diagnoses from frozen tissue sections  [ :arrow_down: ](https://arxiv.org/pdf/2108.10550.pdf)
>  In clinical diagnostics and research involving histopathology, formalin fixed paraffin embedded (FFPE) tissue is almost universally favored for its superb image quality. However, tissue processing time (more than 24 hours) can slow decision-making. In contrast, fresh frozen (FF) processing (less than 1 hour) can yield rapid information but diagnostic accuracy is suboptimal due to lack of clearing, morphologic deformation and more frequent artifacts. Here, we bridge this gap using artificial intelligence. We synthesize FFPE-like images ,virtual FFPE, from FF images using a generative adversarial network (GAN) from 98 paired kidney samples derived from 40 patients. Five board-certified pathologists evaluated the results in a blinded test. Image quality of the virtual FFPE data was assessed to be high and showed a close resemblance to real FFPE images. Clinical assessments of disease on the virtual FFPE images showed a higher inter-observer agreement compared to FF images. The nearly instantaneously generated virtual FFPE images can not only reduce time to information but can facilitate more precise diagnosis from routine FF images without extraneous costs and effort.      
### 22.Design and Performance Evaluation of Joint Sensing and Communication Integrated System for 5G MmWave Enabled CAVs  [ :arrow_down: ](https://arxiv.org/pdf/2108.10535.pdf)
>  The safety of connected automated vehicles (CAVs) relies on the reliable and efficient raw data sharing from multiple types of sensors. The 5G millimeter wave (mmWave) communication technology can enhance the environment sensing ability of different isolated vehicles. In this paper, a joint sensing and communication integrated system (JSCIS) is designed to support the dynamic frame structure configuration for sensing and communication dual functions based on the 5G New Radio protocol in the mmWave frequency band, which can solve the low latency and high data rate problems of raw sensing data sharing among CAVs. To evaluate the timeliness of raw sensing data transmission, the best time duration allocation ratio of sensing and communication dual functions for one vehicle is achieved by modeling the M/M/1 queuing problem using the age of information (AoI) in this paper. Furthermore, the resource allocation optimization problem among multiple CAVs is formulated as a non-cooperative game using the radar mutual information as a key indicator. And the feasibility and existence of pure strategy Nash equilibrium (NE) are proved theoretically, and a centralized time resource allocation (CTRA) algorithm is proposed to achieve the best feasible pure strategy NE. Finally, both simulation and hardware testbed are designed, and the results show that the proposed CTRA algorithm can improve the radar total mutual information by 26%, and the feasibility of the proposed JSCIS is achieved with an acceptable radar ranging accuracy within 0.25 m, as well as a stable data rate of 2.8 Gbps using the 28 GHz mmWave frequency band.      
### 23.Control Barrier Functions With Unmodeled Dynamics Using Integral Quadratic Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2108.10491.pdf)
>  This paper presents a control design method that achieves safety for systems with unmodeled dynamics at the plant input. The proposed method combines control barrier functions (CBFs) and integral quadratic constraints (IQCs). Simplified, low-order models are often used in the design of the controller. Parasitic, unmodeled dynamics (e.g. actuator dynamics, time delays, etc) can lead to safety violations. The proposed method bounds the input-output behavior of these unmodeled dynamics in the time-domain using an alpha-IQC. The alpha-IQC is then incorporated into the CBF constraint to ensure safety. The approach is demonstrated with a simple example.      
### 24.All You Need is Color: Image based Spatial Gene Expression Prediction using Neural Stain Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.10446.pdf)
>  "Is it possible to predict expression levels of different genes at a given spatial location in the routine histology image of a tumor section by modeling its stain absorption characteristics?" In this work, we propose a "stain-aware" machine learning approach for prediction of spatial transcriptomic gene expression profiles using digital pathology image of a routine Hematoxylin &amp; Eosin (H&amp;E) histology section. Unlike recent deep learning methods which are used for gene expression prediction, our proposed approach termed Neural Stain Learning (NSL) explicitly models the association of stain absorption characteristics of the tissue with gene expression patterns in spatial transcriptomics by learning a problem-specific stain deconvolution matrix in an end-to-end manner. The proposed method with only 11 trainable weight parameters outperforms both classical regression models with cellular composition and morphological features as well as deep learning methods. We have found that the gene expression predictions from the proposed approach show higher correlations with true expression values obtained through sequencing for a larger set of genes in comparison to other approaches.      
### 25.Resource Allocation in Heterogeneously-Distributed Joint Radar-Communications under Asynchronous Bayesian Tracking Framework  [ :arrow_down: ](https://arxiv.org/pdf/2108.10432.pdf)
>  Optimal allocation of shared resources is key to deliver the promise of jointly operating radar and communications systems. In this paper, unlike prior works which examine synergistic access to resources in colocated joint radar-communications or among identical systems, we investigate this problem for a distributed system comprising heterogeneous radars and multi-tier communications. In particular, we focus on resource allocation in the context of multi-target tracking (MTT) while maintaining stable communication connections. By simultaneously allocating the available power, dwell time and shared bandwidth, we improve the MTT performance under a Bayesian tracking framework and guarantee the communications throughput. Our alternating allocation of heterogeneous resources (ANCHOR) approach solves the resulting nonconvex problem based on the alternating optimization method that monotonically improves the Bayesian Cramér-Rao bound. Numerical experiments demonstrate that ANCHOR significant improves the tracking error over two baseline allocations and stability under different target scenarios and radar-communications network distributions.      
### 26.Multi-Criteria Radio Spectrum Sharing With Subspace-Based Pareto Tracing  [ :arrow_down: ](https://arxiv.org/pdf/2108.10414.pdf)
>  Radio spectrum is a high-demand finite resource. To meet growing demands of data throughput for forthcoming and deployed wireless networks, new wireless technologies must operate in shared spectrum over unlicensed bands (coexist). As an example application, we consider a model of Long-Term Evolution (LTE) License-Assisted Access (LAA) with incumbent IEEE 802.11 wireless-fidelity (Wi-Fi) systems in a coexistence scenario. This scenario considers multiple LAA and Wi-Fi links sharing an unlicensed band; however, a multitude of other scenarios could be applicable to our general approach. We aim to improve coexistence by maximizing the key performance indicators (KPIs) of two networks simultaneously via dimension reduction and multi-criteria optimization. These KPIs are network throughputs as a function of medium access control (MAC) protocols and physical layer parameters. We perform an exploratory analysis of coexistence behavior by approximating active subspaces to identify low-dimensional structure in the optimization criteria, i.e., few linear combinations of parameters for simultaneously maximizing LAA and Wi-Fi throughputs. We take advantage of an aggregate low-dimensional subspace parametrized by approximate active subspaces of both throughputs to regularize a multi-criteria optimization. Additionally, a choice of two-dimensional subspace enables visualizations augmenting interpretability and explainability of the results. The visualizations and approximations suggest a predominantly convex set of KPIs over active coordinates leading to a regularized manifold of approximately Pareto optimal solutions. Subsequent geodesics lead to continuous traces through parameter space constituting non-dominated solutions in contrast to random grid search.      
### 27.A Sensitivity Matrix Approach Using Two-Stage Optimization for Voltage Regulation of LV Networks with High PV Penetration  [ :arrow_down: ](https://arxiv.org/pdf/2108.10387.pdf)
>  The occurrence of voltage violations are a major deterrent for absorbing more roof-top solar power to smart Low Voltage Distribution Grids (LVDG). Recent studies have focused on decentralized control methods to solve this problem due to the high computational time in performing load flows in centralized control techniques. To address this issue a novel sensitivity matrix is developed to estimate voltages of the network by replacing load flow simulations. In this paper, a Centralized Active, Reactive Power Management System (CARPMS) is proposed to optimally utilize the reactive power capability of smart photo-voltaic inverters with minimal active power curtailment to mitigate the voltage violation problem. The developed sensitivity matrix is able to reduce the time consumed by 48% compared to load flow simulations, enabling near real-time control optimization. Given the large solution space of power systems, a novel two-stage optimization is proposed, where the solution space is narrowed down by a Feasible Region Search (FRS) step, followed by Particle Swarm Optimization (PSO). The performance of the proposed methodology is analyzed in comparison to the load flow method to demonstrate the accuracy and the capability of the optimization algorithm to mitigate voltage violations in near real-time. The deviation of mean voltages of the proposed methodology from load flow method was; 6.5*10^-3 p.u for reactive power control using Q-injection, 1.02*10^-2 p.u for reactive power control using Q-absorption, and 0 p.u for active power curtailment case.      
### 28.Learning Sparse Analytic Filters for Piano Transcription  [ :arrow_down: ](https://arxiv.org/pdf/2108.10382.pdf)
>  In recent years, filterbank learning has become an increasingly popular strategy for various audio-related machine learning tasks. This is partly due to its ability to discover task-specific audio characteristics which can be leveraged in downstream processing. It is also a natural extension of the nearly ubiquitous deep learning methods employed to tackle a diverse array of audio applications. In this work, several variations of a frontend filterbank learning module are investigated for piano transcription, a challenging low-level music information retrieval task. We build upon a standard piano transcription model, modifying only the feature extraction stage. The filterbank module is designed such that its complex filters are unconstrained 1D convolutional kernels with long receptive fields. Additional variations employ the Hilbert transform to render the filters intrinsically analytic and apply variational dropout to promote filterbank sparsity. Transcription results are compared across all experiments, and we offer visualization and analysis of the filterbanks.      
### 29.On Adaptive Transmission for Distributed Detection in Energy Harvesting Wireless Sensor Networks with Limited Fusion Center Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2108.10358.pdf)
>  We consider a wireless sensor network, consisting of N heterogeneous sensors and a fusion center (FC), tasked with solving a binary distributed detection problem. Sensors communicate directly with the FC over orthogonal fading channels. Each sensor can harvest randomly arriving energy and store it in a battery. Also, it knows its quantized channel state information (CSI), acquired via a limited feedback channel from the FC. We propose a transmit power control strategy such that the J-divergence based detection metric is maximized, subject to an average transmit power per sensor constraint. The proposed strategy is parametrized in terms of the channel gain quantization thresholds and the scale factors corresponding to the quantization intervals, to strike a balance between the rates of energy harvesting and energy consumption for data transmission. This strategy allows each sensor to adapt its transmit power based on its battery state and its qunatized CSI. Finding the optimal strategy requires solving a non-convex optimization problem that is not differentiable with respect to the optimization variables.We propose near-optimal strategy based on hybrid search methods that have a low-computational complexity.      
### 30.End-to-End Open Vocabulary Keyword Search  [ :arrow_down: ](https://arxiv.org/pdf/2108.10357.pdf)
>  Recently, neural approaches to spoken content retrieval have become popular. However, they tend to be restricted in their vocabulary or in their ability to deal with imbalanced test settings. These restrictions limit their applicability in keyword search, where the set of queries is not known beforehand, and where the system should return not just whether an utterance contains a query but the exact location of any such occurrences. In this work, we propose a model directly optimized for keyword search. The model takes a query and an utterance as input and returns a sequence of probabilities for each frame of the utterance of the query having occurred in that frame. Experiments show that the proposed model not only outperforms similar end-to-end models on a task where the ratio of positive and negative trials is artificially balanced, but it is also able to deal with the far more challenging task of keyword search with its inherent imbalance. Furthermore, using our system to rescore the outputs an LVCSR-based keyword search system leads to significant improvements on the latter.      
### 31.Model-Free Learning of Optimal Deterministic Resource Allocations in Wireless Systems via Action-Space Exploration  [ :arrow_down: ](https://arxiv.org/pdf/2108.10352.pdf)
>  Wireless systems resource allocation refers to perpetual and challenging nonconvex constrained optimization tasks, which are especially timely in modern communications and networking setups involving multiple users with heterogeneous objectives and imprecise or even unknown models and/or channel statistics. In this paper, we propose a technically grounded and scalable primal-dual deterministic policy gradient method for efficiently learning optimal parameterized resource allocation policies. Our method not only efficiently exploits gradient availability of popular universal policy representations, such as deep neural networks, but is also truly model-free, as it relies on consistent zeroth-order gradient approximations of the associated random network services constructed via low-dimensional perturbations in action space, thus fully bypassing any dependence on critics. Both theory and numerical simulations confirm the efficacy and applicability of the proposed approach, as well as its superiority over the current state of the art in terms of both achieving near-optimal performance and scalability.      
### 32.Risk-based Classical Failure Mode and Effect Analysis (FMEA) of Microgrid Cyber-physical Energy Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.10349.pdf)
>  Modern microgrids are networked systems comprising physical and cyber components for networking, computation, and monitoring. These cyber components make microgrids more reliable but increase the system complexity. Therefore, risk assessment methods are an imperative technology for cyber-physical systems to ensure safe and secure operations. The authors propose a reliability approach that utilizes the failure modes of power and cyber network key components to perform the risk analysis in microgrids. In this work, the authors have used the Failure Mode and Effect Analysis (FMEA) approach for risk assessment of microgrid systems and determine the influence of various failure modes on their performance. FMEA is one of the most effective methods to assess the risk involving the cyber components. It is the method of examining components, modules, and subsystems to determine failure modes in a system and their causes and consequences. A novel approach is proposed to calculate risk priority numbers based on factors like severity, occurrence, and detection. A risk matrix is calculated from the proposed FMEA worksheet, which acts as a graphical representation for evaluating components risk based on risk priority number or criticality metrics.      
### 33.Reducing Exposure Bias in Training Recurrent Neural Network Transducers  [ :arrow_down: ](https://arxiv.org/pdf/2108.10803.pdf)
>  When recurrent neural network transducers (RNNTs) are trained using the typical maximum likelihood criterion, the prediction network is trained only on ground truth label sequences. This leads to a mismatch during inference, known as exposure bias, when the model must deal with label sequences containing errors. In this paper we investigate approaches to reducing exposure bias in training to improve the generalization of RNNT models for automatic speech recognition (ASR). A label-preserving input perturbation to the prediction network is introduced. The input token sequences are perturbed using SwitchOut and scheduled sampling based on an additional token language model. Experiments conducted on the 300-hour Switchboard dataset demonstrate their effectiveness. By reducing the exposure bias, we show that we can further improve the accuracy of a high-performance RNNT ASR model and obtain state-of-the-art results on the 300-hour Switchboard dataset.      
### 34.Simplified Doppler frequency shift measurement enabled by Serrodyne optical frequency translation  [ :arrow_down: ](https://arxiv.org/pdf/2108.10760.pdf)
>  A simplified Doppler frequency shift measurement approach based on Serrodyne optical frequency translation is reported. A sawtooth wave with an appropriate amplitude is sent to one phase modulation arm of a Mach-Zehnder modulator in conjunction with the transmitted signal to implement the Serrodyne optical frequency transition, as well as the optical phase modulation of the transmitted signal on the frequency-shifted optical carrier. The echo signal is applied to the other phase modulation arm of the Mach-Zehnder modulator. The optical signals from the two arms are combined in the Mach-Zehnder modulator, whose lower optical sidebands are selected by an optical bandpass filter and then detected in a photodetector. By simply measuring the frequency of the output low-frequency signal, the value and direction of DFS can be determined simultaneously. An experiment is performed. DFS from -100 to 100 kHz is measured for microwave signals from 6 to 17 GHz with a measurement error of less than 0.03 Hz and a measurement stability of 0.015 Hz in 30 minutes when a 500-kHz sawtooth wave is used as the reference.      
### 35.Federated Learning for UAV Swarms Under Class Imbalance and Power Consumption Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2108.10748.pdf)
>  The usage of unmanned aerial vehicles (UAVs) in civil and military applications continues to increase due to the numerous advantages that they provide over conventional approaches. Despite the abundance of such advantages, it is imperative to investigate the performance of UAV utilization while considering their design limitations. This paper investigates the deployment of UAV swarms when each UAV carries a machine learning classification task. To avoid data exchange with ground-based processing nodes, a federated learning approach is adopted between a UAV leader and the swarm members to improve the local learning model while avoiding excessive air-to-ground and ground-to-air communications. Moreover, the proposed deployment framework considers the stringent energy constraints of UAVs and the problem of class imbalance, where we show that considering these design parameters significantly improves the performances of the UAV swarm in terms of classification accuracy, energy consumption and availability of UAVs when compared with several baseline algorithms.      
### 36.Sonic: A Sampling-based Online Controller for Streaming Applications  [ :arrow_down: ](https://arxiv.org/pdf/2108.10701.pdf)
>  Many applications in important problem domains such as machine learning and computer vision are streaming applications that take a sequence of inputs over time. It is challenging to find knob settings that optimize the run-time performance of such applications because the optimal knob settings are usually functions of inputs, computing platforms, time as well as user's requirements, which can be very diverse. <br>Most prior works address this problem by offline profiling followed by training models for control. However, profiling-based approaches incur large overhead before execution; it is also difficult to redeploy them in other run-time configurations. <br>In this paper, we propose Sonic, a sampling-based online controller for long-running streaming applications that does not require profiling ahead of time. Within each phase of a streaming application's execution, Sonic utilizes the beginning portion to sample the knob space strategically and aims to pick the optimal knob setting for the rest of the phase, given a user-specified constrained optimization problem. A hybrid approach of machine learning regressions and Bayesian optimization are used for better overall sampling choices. <br>Sonic is implemented independent of application, device, input, performance objective and constraints. We evaluate Sonic on traditional parallel benchmarks as well as on deep learning inference benchmarks across multiple platforms. Our experiments show that when using Sonic to control knob settings, application run-time performance is only 5.3% less than if optimal knob settings were used, demonstrating that Sonic is able to find near-optimal knob settings under diverse run-time configurations without prior knowledge quickly.      
### 37.Secrecy Rate Maximization for Intelligent Reflecting Surface Assisted MIMOME Wiretap Channels  [ :arrow_down: ](https://arxiv.org/pdf/2108.10688.pdf)
>  Intelligent reflecting surface (IRS) has gained tremendous attention recently as a disruptive technology for beyond 5G networks. In this paper, we consider the problem of secrecy rate maximization for an IRS-assisted Gaussian multiple-input multiple-output multi-antenna-eavesdropper (MIMOME) wiretap channel (WTC). In this context, we aim to jointly optimize the input covariance matrix and the IRS phase shifts to maximize the achievable secrecy rate of the considered system. To solve the formulated problem which is non-convex, we propose an iterative method based on the block successive maximization (BSM), where each iteration is done in closed form. More specifically, we maximize a lower bound on the achievable secrecy rate to update the input covariance matrix for fixed phase shifts, and then maximize the (exact) achievable secrecy rate to update phase shifts for a given input covariance.We consider the total free space path loss (FSPL) in this system to emphasize the first-order measure of the applicability of the IRS in the considered communication system. We present a convergence proof and the associated complexity analysis of the proposed algorithm. Numerical results are provided to demonstrate the superiority of the proposed method compared to a known solution, and also to show the effect of different parameters of interest on the achievable secrecy rate of the IRS-assisted MIMOME WTC.      
### 38.Finite-dimensional observer-based boundary stabilization of reaction-diffusion equations with a either Dirichlet or Neumann boundary measurement  [ :arrow_down: ](https://arxiv.org/pdf/2108.10664.pdf)
>  This paper investigates the output feedback boundary control of reaction-diffusion equations with either distributed or boundary measurement by means of a finite-dimensional observer. A constructive method dealing with the design of finite-dimensional observers for the feedback stabilization of reaction-diffusion equations was reported in a recent paper in the case where either the control or the observation operator is bounded and also satisfies certain regularity assumptions. In this paper, we go beyond by demonstrating that a finite-dimensional state-feedback combined with a finite-dimensional observer can always be successfully designed in order to achieve the Dirichlet boundary stabilization of reaction-diffusion PDEs with a either Dirichlet or Neumann boundary measurement.      
### 39.Integral action for setpoint regulation control of a reaction-diffusion equation in the presence of a state delay  [ :arrow_down: ](https://arxiv.org/pdf/2108.10658.pdf)
>  This paper is concerned with the regulation control of a one-dimensional reaction-diffusion equation in the presence of a state-delay in the reaction term. The objective is to achieve the PI regulation of the right Dirichlet trace with a command selected as the left Dirichlet trace. The control design strategy consists of the design of a PI controller on a finite dimensional truncated model obtained by spectral reduction. By an adequate selection of the number of modes of the original infinite-dimensional system, we show that the proposed control design procedure achieves both the exponential stabilization of the original infinite-dimensional system as well as the setpoint regulation of the right Dirichlet trace.      
### 40.A Random Geometric Model of Blockages in Vehicular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.10632.pdf)
>  This paper presents a novel spatially consistent approach for modeling line-of-sight (LOS) paths in vehicular networks. We use stochastic geometry to model transmitters, obstacles, and receivers located in three parallel lines, respectively. Their geometric interactions are leveraged to characterize the existence of LOS paths. Specifically, the proposed approach focuses on the role of obstacles in blocking one or more LOS paths, which has been overlooked in most statistical models for blockage. Under the proposed framework, we derive the probability that a typical vehicle is in LOS with respect to transmitters with received signal-to-noise ratios greater than a threshold. The proposed framework and LOS coverage analysis are instrumental to the analysis of LOS-critical applications such as positioning or mmWave communications in vehicular networks.      
### 41.Spectral Efficiency Analysis of Cell-free Distributed Massive MIMO Systems with Imperfect Covariance Matrix  [ :arrow_down: ](https://arxiv.org/pdf/2108.10574.pdf)
>  In this paper, the impacts of imperfect channel covariance matrix on the spectral efficiency (SE) of cell-free distributed massive multiple-input multiple-output (MIMO) systems are analyzed. We propose to estimate the channel covariance matrix by alternately using the assigned pilots and their phase-shifted pilots in different coherent blocks, which improves the accuracy of channel estimation with imperfect covariance matrix and reduces pilot overhead. Under this scheme, the closed-form expressions of SE with maximum ratio combination (MRC) and zero-forcing (ZF) receivers are derived, which enables us to select key parameters for better system performance. Simulation results verify the effectiveness of the proposed channel estimation method and the accuracy of the derived closed-form expressions. When more coherent blocks are used to estimate the covariance matrix, we can get better system performance. Moreover, some insightful conclusions are arrived at from the SE comparisons between different receiving schemes (ZF and MRC) and different pilot allocation schemes (orthogonal pilot and pilot reuse).      
### 42.Optimal UAV Hitching on Ground Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2108.10572.pdf)
>  Due to its mobility and agility, unmanned aerial vehicle (UAV) has emerged as a promising technology for various tasks, such as sensing, inspection and delivery. However, a typical UAV has limited energy storage and cannot fly a long distance without being recharged. This motivates several existing proposals to use trucks and other ground vehicles to offer riding to help UAVs save energy and expand the operation radius. We present the first theoretical study regarding how UAVs should optimally hitch on ground vehicles, considering vehicles' different travelling patterns and supporting capabilities. For a single UAV, we derive closed-form optimal vehicle selection and hitching strategy. When vehicles only support hitching, a UAV would prefer the vehicle that can carry it closest to its final destination. When vehicles can offer hitching plus charging, the UAV may hitch on a vehicle that carries it farther away from its destination and hitch a longer distance. The UAV may also prefer to hitch on a slower vehicle for the benefit of battery recharging. For multiple UAVs in need of hitching, we develop the max-saving algorithm (MSA) to optimally match UAV-vehicle collaboration. We prove that the MSA globally optimizes the total hitching benefits for the UAVs.      
### 43.Creating Electronic Oscillator-based Ising Machines without External Injection Locking  [ :arrow_down: ](https://arxiv.org/pdf/2108.10499.pdf)
>  Coupled electronic oscillators have recently been explored as a compact, integrated circuit- and room temperature operation- compatible hardware platform to design Ising machines. However, such implementations presently require the injection of an externally generated second-harmonic signal to impose the phase bipartition among the oscillators. In this work, we experimentally demonstrate a new electronic autaptic oscillator (EAO) that uses engineered feedback to eliminate the need for the generation and injection of the external second harmonic signal to minimize the Ising Hamiltonian. The feedback in the EAO is engineered to effectively generate the second harmonic signal internally. Using this oscillator design, we show experimentally, that a system of capacitively coupled EAOs exhibits the desired bipartition in the oscillator phases, and subsequently, demonstrate its application in solving the computationally hard Maximum Cut (MaxCut) problem. Our work not only establishes a new oscillator design aligned to the needs of the oscillator Ising machine but also advances the efforts to creating application specific analog computing platforms.      
### 44.Differential Music: Automated Music Generation Using LSTM Networks with Representation Based on Melodic and Harmonic Intervals  [ :arrow_down: ](https://arxiv.org/pdf/2108.10449.pdf)
>  This paper presents a generative AI model for automated music composition with LSTM networks that takes a novel approach at encoding musical information which is based on movement in music rather than absolute pitch. Melodies are encoded as a series of intervals rather than a series of pitches, and chords are encoded as the set of intervals that each chord note makes with the melody at each timestep. Experimental results show promise as they sound musical and tonal. There are also weaknesses to this method, mainly excessive modulations in the compositions, but that is expected from the nature of the encoding. This issue is discussed later in the paper and is a potential topic for future work.      
### 45.Fast Robust Tensor Principal Component Analysis via Fiber CUR Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2108.10448.pdf)
>  We study the problem of tensor robust principal component analysis (TRPCA), which aims to separate an underlying low-multilinear-rank tensor and a sparse outlier tensor from their sum. In this work, we propose a fast non-convex algorithm, coined Robust Tensor CUR (RTCUR), for large-scale TRPCA problems. RTCUR considers a framework of alternating projections and utilizes the recently developed tensor Fiber CUR decomposition to dramatically lower the computational complexity. The performance advantage of RTCUR is empirically verified against the state-of-the-arts on the synthetic datasets and is further demonstrated on the real-world application such as color video background subtraction.      
### 46.One TTS Alignment To Rule Them All  [ :arrow_down: ](https://arxiv.org/pdf/2108.10447.pdf)
>  Speech-to-text alignment is a critical component of neural textto-speech (TTS) models. Autoregressive TTS models typically use an attention mechanism to learn these alignments on-line. However, these alignments tend to be brittle and often fail to generalize to long utterances and out-of-domain text, leading to missing or repeating words. Most non-autoregressive endto-end TTS models rely on durations extracted from external sources. In this paper we leverage the alignment mechanism proposed in RAD-TTS as a generic alignment learning framework, easily applicable to a variety of neural TTS models. The framework combines forward-sum algorithm, the Viterbi algorithm, and a simple and efficient static prior. In our experiments, the alignment learning framework improves all tested TTS architectures, both autoregressive (Flowtron, Tacotron 2) and non-autoregressive (FastPitch, FastSpeech 2, RAD-TTS). Specifically, it improves alignment convergence speed of existing attention-based mechanisms, simplifies the training pipeline, and makes the models more robust to errors on long utterances. Most importantly, the framework improves the perceived speech synthesis quality, as judged by human evaluators.      
### 47.CoverTheFace: face covering monitoring and demonstrating using deep learning and statistical shape analysis  [ :arrow_down: ](https://arxiv.org/pdf/2108.10430.pdf)
>  Wearing a mask is a strong protection against the COVID-19 pandemic, even though the vaccine has been successfully developed and is widely available. However, many people wear them incorrectly. This observation prompts us to devise an automated approach to monitor the condition of people wearing masks. Unlike previous studies, our work goes beyond mask detection; it focuses on generating a personalized demonstration on proper mask-wearing, which helps people use masks better through visual demonstration rather than text explanation. The pipeline starts from the detection of face covering. For images where faces are improperly covered, our mask overlay module incorporates statistical shape analysis (SSA) and dense landmark alignment to approximate the geometry of a face and generates corresponding face-covering examples. Our results show that the proposed system successfully identifies images with faces covered properly. Our ablation study on mask overlay suggests that the SSA model helps to address variations in face shapes, orientations, and scales. The final face-covering examples, especially half profile face images, surpass previous arts by a noticeable margin.      
### 48.Predicting Vehicles' Longitudinal Trajectories and Lane Changes on Highway On-Ramps  [ :arrow_down: ](https://arxiv.org/pdf/2108.10397.pdf)
>  Vehicles on highway on-ramps are one of the leading contributors to congestion. In this paper, we propose a prediction framework that predicts the longitudinal trajectories and lane changes (LCs) of vehicles on highway on-ramps and tapers. Specifically, our framework adopts a combination of prediction models that inputs a 4 seconds duration of a trajectory to output a forecast of the longitudinal trajectories and LCs up to 15 seconds ahead. Training and Validation based on next generation simulation (NGSIM) data show that the prediction power of the developed model and its accuracy outperforms a traditional long-short term memory (LSTM) model. Ultimately, the work presented here can alleviate the congestion experienced on on-ramps, improve safety, and guide effective traffic control strategies.      
### 49.A generalized stacked reinforcement learning method for sampled systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.10392.pdf)
>  A common setting of reinforcement learning (RL) is a Markov decision process (MDP) in which the environment is a stochastic discrete-time dynamical system. Whereas MDPs are suitable in such applications as video-games or puzzles, physical systems are time-continuous. Continuous methods of RL are known, but they have their limitations, such as, e.g., collapse of Q-learning. A general variant of RL is of digital format, where updates of the value and policy are performed at discrete moments in time. The agent-environment loop then amounts to a sampled system, whereby sample-and-hold is a specific case. In this paper, we propose and benchmark two RL methods suitable for sampled systems. Specifically, we hybridize model-predictive control (MPC) with critics learning the Q- and value function. Optimality is analyzed and performance comparison is done in an experimental case study with a mobile robot.      
### 50.Reachability of weakly nonlinear systems using Carleman linearization  [ :arrow_down: ](https://arxiv.org/pdf/2108.10390.pdf)
>  In this article we introduce a solution method for a special class of nonlinear initial-value problems using set-based propagation techniques. The novelty of the approach is that we employ a particular embedding (Carleman linearization) to leverage recent advances of high-dimensional reachability solvers for linear ordinary differential equations based on the support function. Using a global error bound for the Carleman linearization abstraction, we are able to describe the full set of behaviors of the system for sets of initial conditions and in dense time.      
### 51.edge-SR: Super-Resolution For The Masses  [ :arrow_down: ](https://arxiv.org/pdf/2108.10335.pdf)
>  Classic image scaling (e.g. bicubic) can be seen as one convolutional layer and a single upscaling filter. Its implementation is ubiquitous in all display devices and image processing software. In the last decade deep learning systems have been introduced for the task of image super-resolution (SR), using several convolutional layers and numerous filters. These methods have taken over the benchmarks of image quality for upscaling tasks. Would it be possible to replace classic upscalers with deep learning architectures on edge devices such as display panels, tablets, laptop computers, etc.? On one hand, the current trend in Edge-AI chips shows a promising future in this direction, with rapid development of hardware that can run deep-learning tasks efficiently. On the other hand, in image SR only few architectures have pushed the limit to extreme small sizes that can actually run on edge devices at real-time. We explore possible solutions to this problem with the aim to fill the gap between classic upscalers and small deep learning configurations. As a transition from classic to deep-learning upscaling we propose edge-SR (eSR), a set of one-layer architectures that use interpretable mechanisms to upscale images. Certainly, a one-layer architecture cannot reach the quality of deep learning systems. Nevertheless, we find that for high speed requirements, eSR becomes better at trading-off image quality and runtime performance. Filling the gap between classic and deep-learning architectures for image upscaling is critical for massive adoption of this technology. It is equally important to have an interpretable system that can reveal the inner strategies to solve this problem and guide us to future improvements and better understanding of larger networks.      
