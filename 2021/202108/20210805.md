# ArXiv eess --Thu, 5 Aug 2021
### 1.Channel Rank Improvement in Urban Drone Corridors Using Passive Intelligent Reflectors  [ :arrow_down: ](https://arxiv.org/pdf/2108.02179.pdf)
>  Multiple-input multiple-output (MIMO) techniques can help in scaling the achievable air-to-ground (A2G) channel capacity while communicating with drones. However, spatial multiplexing with drones suffers from rank deficient channels due to the unobstructed line-of-sight (LoS), especially in millimeter-wave (mmWave) frequencies that use narrow beams. One possible solution is utilizing low-cost and low-complexity metamaterial-based intelligent reflecting surfaces (IRS) to enrich the multipath environment, taking into account that the drones are restricted to fly only within well-defined drone corridors. A hurdle with this solution is placing the IRSs optimally. In this study, we propose an approach for IRS placement with a goal to improve the spatial multiplexing gains, and hence to maximize the average channel capacity in a predefined drone corridor. Our results at 6 GHz, 28 GHz and 60 GHz show that the proposed approach increases the average rates for all frequency bands for a given drone corridor, when compared with the environment where there are no IRSs present, and IRS-aided channels perform close to each other at sub-6 and mmWave bands.      
### 2.MRI to PET Cross-Modality Translation using Globally and Locally Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2108.02160.pdf)
>  Medical imaging datasets are inherently high dimensional with large variability and low sample sizes that limit the effectiveness of deep learning algorithms. Recently, generative adversarial networks (GANs) with the ability to synthesize realist images have shown great potential as an alternative to standard data augmentation techniques. Our work focuses on cross-modality synthesis of fluorodeoxyglucose~(FDG) Positron Emission Tomography~(PET) scans from structural Magnetic Resonance~(MR) images using generative models to facilitate multi-modal diagnosis of Alzheimer's disease (AD). Specifically, we propose a novel end-to-end, globally and locally aware image-to-image translation GAN (GLA-GAN) with a multi-path architecture that enforces both global structural integrity and fidelity to local details. We further supplement the standard adversarial loss with voxel-level intensity, multi-scale structural similarity (MS-SSIM) and region-of-interest (ROI) based loss components that reduce reconstruction error, enforce structural consistency at different scales and perceive variation in regional sensitivity to AD respectively. Experimental results demonstrate that our GLA-GAN not only generates synthesized FDG-PET scans with enhanced image quality but also superior clinical utility in improving AD diagnosis compared to state-of-the-art models. Finally, we attempt to interpret some of the internal units of the GAN that are closely related to this specific cross-modality generation task.      
### 3.Physics-based Noise Modeling for Extreme Low-light Photography  [ :arrow_down: ](https://arxiv.org/pdf/2108.02158.pdf)
>  Enhancing the visibility in extreme low-light environments is a challenging task. Under nearly lightless condition, existing image denoising methods could easily break down due to significantly low SNR. In this paper, we systematically study the noise statistics in the imaging pipeline of CMOS photosensors, and formulate a comprehensive noise model that can accurately characterize the real noise structures. Our novel model considers the noise sources caused by digital camera electronics which are largely overlooked by existing methods yet have significant influence on raw measurement in the dark. It provides a way to decouple the intricate noise structure into different statistical distributions with physical interpretations. Moreover, our noise model can be used to synthesize realistic training data for learning-based low-light denoising algorithms. In this regard, although promising results have been shown recently with deep convolutional neural networks, the success heavily depends on abundant noisy clean image pairs for training, which are tremendously difficult to obtain in practice. Generalizing their trained models to images from new devices is also problematic. Extensive experiments on multiple low-light denoising datasets -- including a newly collected one in this work covering various devices -- show that a deep neural network trained with our proposed noise formation model can reach surprisingly-high accuracy. The results are on par with or sometimes even outperform training with paired real data, opening a new door to real-world extreme low-light photography.      
### 4.Uplink Energy Efficiency of Cell-Free Massive MIMO With Transmit Power Control in Measured Propagation Channels  [ :arrow_down: ](https://arxiv.org/pdf/2108.02130.pdf)
>  Cell-free massive MIMO (CF-mMIMO) is expected to provide reliable wireless services for a large number of user equipments (UEs) using access points (APs) distributed across a wide area. When the UEs are battery-powered, uplink energy efficiency (EE) becomes an important performance metric for CF-mMIMO systems. Therefore, if the "target" spectral efficiency (SE) is met, it is important to optimize the uplink EE when setting the transmit powers of the UEs. Also, such transmit power control (TPC) method must be tested on channel data from real-world measurements to prove its effectiveness. In this paper, we compare three different TPC algorithms using zero-forcing reception by applying them to 3.5 GHz channel measurement data featuring ~30,000 possible AP locations and 8 UE locations in a 200mx200m area. We show that the max-min EE algorithm is highly effective in improving the uplink EE at a target SE, especially if the number of single-antenna APs is large, circuit power consumption is low, and the maximum allowed transmit power of the UEs is high.      
### 5.Low-complexity Scaling Methods for DCT-II Approximations  [ :arrow_down: ](https://arxiv.org/pdf/2108.02119.pdf)
>  This paper introduces a collection of scaling methods for generating $2N$-point DCT-II approximations based on $N$-point low-complexity transformations. Such scaling is based on the Hou recursive matrix factorization of the exact $2N$-point DCT-II matrix. Encompassing the widely employed Jridi-Alfalou-Meher scaling method, the proposed techniques are shown to produce DCT-II approximations that outperform the transforms resulting from the JAM scaling method according to total error energy and mean squared error. Orthogonality conditions are derived and an extensive error analysis based on statistical simulation demonstrates the good performance of the introduced scaling methods. A hardware implementation is also provided demonstrating the competitiveness of the proposed methods when compared to the JAM scaling method.      
### 6.Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction  [ :arrow_down: ](https://arxiv.org/pdf/2108.02110.pdf)
>  A number of deep learning based algorithms have been proposed to recover high-quality videos from low-quality compressed ones. Among them, some restore the missing details of each frame via exploring the spatiotemporal information of neighboring frames. However, these methods usually suffer from a narrow temporal scope, thus may miss some useful details from some frames outside the neighboring ones. In this paper, to boost artifact removal, on the one hand, we propose a Recursive Fusion (RF) module to model the temporal dependency within a long temporal range. Specifically, RF utilizes both the current reference frames and the preceding hidden state to conduct better spatiotemporal compensation. On the other hand, we design an efficient and effective Deformable Spatiotemporal Attention (DSTA) module such that the model can pay more effort on restoring the artifact-rich areas like the boundary area of a moving object. Extensive experiments show that our method outperforms the existing ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual effect. Code is available at <a class="link-external link-https" href="https://github.com/zhaominyiz/RFDA-PyTorch" rel="external noopener nofollow">this https URL</a>.      
### 7.Auto-encoder based Model for High-dimensional Imbalanced Industrial Data  [ :arrow_down: ](https://arxiv.org/pdf/2108.02083.pdf)
>  With the proliferation of IoT devices, the distributed control systems are now capturing and processing more sensors at higher frequency than ever before. These new data, due to their volume and novelty, cannot be effectively consumed without the help of data-driven techniques. Deep learning is emerging as a promising technique to analyze these data, particularly in soft sensor modeling. The strong representational capabilities of complex data and the flexibility it offers from an architectural perspective make it a topic of active applied research in industrial settings. However, the successful applications of deep learning in soft sensing are still not widely integrated in factory control systems, because most of the research on soft sensing do not have access to large scale industrial data which are varied, noisy and incomplete. The results published in most research papers are therefore not easily reproduced when applied to the variety of data in industrial settings. Here we provide manufacturing data sets that are much larger and more complex than public open soft sensor data. Moreover, the data sets are from Seagate factories on active service with only necessary anonymization, so that they reflect the complex and noisy nature of real-world data. We introduce a variance weighted multi-headed auto-encoder classification model that fits well into the high-dimensional and highly imbalanced data. Besides the use of weighting or sampling methods to handle the highly imbalanced data, the model also simultaneously predicts multiple outputs by exploiting output-supervised representation learning and multi-task weighting.      
### 8.Random Convolution Kernels with Multi-Scale Decomposition for Preterm EEG Inter-burst Detection  [ :arrow_down: ](https://arxiv.org/pdf/2108.02039.pdf)
>  Linear classifiers with random convolution kernels are computationally efficient methods that need no design or domain knowledge. Unlike deep neural networks, there is no need to hand-craft a network architecture; the kernels are randomly generated and only the linear classifier needs training. A recently proposed method, RandOm Convolutional KErnel Transforms (ROCKETs), has shown high accuracy across a range of time-series data sets. Here we propose a multi-scale version of this method, using both high- and low-frequency components. We apply our methods to inter-burst detection in a cohort of preterm EEG recorded from 36 neonates &lt;30 weeks gestational age. Two features from the convolution of 10,000 random kernels are combined using ridge regression. The proposed multi-scale ROCKET method out-performs the method without scale: median (interquartile range, IQR) Matthews correlation coefficient (MCC) of 0.859 (0.815 to 0.874) for multi-scale versus 0.841 (0.807 to 0.865) without scale, p&lt;0.001. The proposed method lags behind an existing feature-based machine learning method developed with deep domain knowledge, but is fast to train and can quickly set an initial baseline threshold of performance for generic and biomedical time-series classification.      
### 9.OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations  [ :arrow_down: ](https://arxiv.org/pdf/2108.02016.pdf)
>  FDG PET/CT imaging is a resource intensive examination critical for managing malignant disease and is particularly important for longitudinal assessment during therapy. Approaches to automate longtudinal analysis present many challenges including lack of available longitudinal datasets, managing complex large multimodal imaging examinations, and need for detailed annotations for traditional supervised machine learning. In this work we develop OncoNet, novel machine learning algorithm that assesses treatment response from a 1,954 pairs of sequential FDG PET/CT exams through weak supervision using the standard uptake values (SUVmax) in associated radiology reports. OncoNet demonstrates an AUROC of 0.86 and 0.84 on internal and external institution test sets respectively for determination of change between scans while also showing strong agreement to clinical scoring systems with a kappa score of 0.8. We also curated a dataset of 1,954 paired FDG PET/CT exams designed for response assessment for the broader machine learning in healthcare research community. Automated assessment of radiographic response from FDG PET/CT with OncoNet could provide clinicians with a valuable tool to rapidly and consistently interpret change over time in longitudinal multi-modal imaging exams.      
### 10.High-performance Passive Eigen-model-based Detectors of Single Emitter Using Massive MIMO Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2108.02011.pdf)
>  For a passive direction of arrival (DoA) measurement system using massive multiple input multiple output (MIMO), it is mandatory to infer whether the emitter exists or not before performing DOA estimation operation. Inspired by the detection idea from radio detection and ranging (radar), three high-performance detectors are proposed to infer the existence of single passive emitter from the eigen-space of sample covariance matrix of receive signal vector. The test statistic (TS) of the first method is defined as the ratio of maximum eigen-value (Max-EV) to minimum eigen-value (R-MaxEV-MinEV) while that of the second one is defined as the ratio of Max-EV to noise variance (R-MaxEV-NV). The TS of the third method is the mean of maximum eigen-value (EV) and minimum EV(M-MaxEV-MinEV). Their closed-form expressions are presented and the corresponding detection performance is given. Simulation results show that the proposed M-MaxEV-MinEV and R-MaxEV-NV methods can approximately achieve the same detection performance that is better than the traditional generalized likelihood ratio test method with false alarm probability being less than 0.3.      
### 11.Estimation of road traffic state at a multi-lanes controlled junction  [ :arrow_down: ](https://arxiv.org/pdf/2108.02007.pdf)
>  We present in this paper a method for the estimation of traffic state at road junctions controlled with traffic lights. We assume mixed traffic where a proportion of vehicles are equipped with communication resources. The estimation of road traffic state uses information given by communicating vehicles. The method we propose is built upon a previously published method which was applied to estimate the traffic in the case where roads are composed of two lanes. In this paper, we consider the case where roads are composed of three lanes and we show that this solution can address the general case, where roads are composed of any number of lanes. We assume the geometry of the road junction is known, as well as its connections between incoming and outgoing lanes and roads. Using the location data provided by the communicating vehicles, first, we estimate some primary parameters including the penetration ratio of the probe vehicles, as well as the arrival rates of vehicles (equipped and non-equipped) per lane by introducing the assignment onto the lanes. Second, we give estimations of the queue length of the 3-lanes road, without and with the additional information provided by the location of the communicating vehicles in the queue. We illustrate and discuss the proposed model with numerical simulations.      
### 12.Robust direct acoustic impedance control using two microphones for mixed feedforward-feedback controller  [ :arrow_down: ](https://arxiv.org/pdf/2108.02003.pdf)
>  This paper presents an impedance control architecture for an electroacoustic absorber combining both a feedforward and feedback microphone-based system on a current driven loudspeaker. Feedforward systems enable good performance for direct impedance control. However, inaccuracies in the required actuator model can lead to a loss of passivity, which can cause unstable behaviors. The feedback contribution allows the absorber to better handle model errors and still achieve an accurate impedance. Numerical and experimental studies were conducted to compare this new architecture against a state-of-the-art feedforward control method.      
### 13.Online unsupervised Learning for domain shift in COVID-19 CT scan datasets  [ :arrow_down: ](https://arxiv.org/pdf/2108.02002.pdf)
>  Neural networks often require large amounts of expert annotated data to train. When changes are made in the process of medical imaging, trained networks may not perform as well, and obtaining large amounts of expert annotations for each change in the imaging process can be time consuming and expensive. Online unsupervised learning is a method that has been proposed to deal with situations where there is a domain shift in incoming data, and a lack of annotations. The aim of this study is to see whether online unsupervised learning can help COVID-19 CT scan classification models adjust to slight domain shifts, when there are no annotations available for the new data. A total of six experiments are performed using three test datasets with differing amounts of domain shift. These experiments compare the performance of the online unsupervised learning strategy to a baseline, as well as comparing how the strategy performs on different domain shifts. Code for online unsupervised learning can be found at this link: <a class="link-external link-https" href="https://github.com/Mewtwo/online-unsupervised-learning" rel="external noopener nofollow">this https URL</a>      
### 14.Adversarial Energy Disaggregation for Non-intrusive Load Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2108.01998.pdf)
>  Energy disaggregation, also known as non-intrusive load monitoring (NILM), challenges the problem of separating the whole-home electricity usage into appliance-specific individual consumptions, which is a typical application of data analysis. {NILM aims to help households understand how the energy is used and consequently tell them how to effectively manage the energy, thus allowing energy efficiency which is considered as one of the twin pillars of sustainable energy policy (i.e., energy efficiency and renewable energy).} Although NILM is unidentifiable, it is widely believed that the NILM problem can be addressed by data science. Most of the existing approaches address the energy disaggregation problem by conventional techniques such as sparse coding, non-negative matrix factorization, and hidden Markov model. Recent advances reveal that deep neural networks (DNNs) can get favorable performance for NILM since DNNs can inherently learn the discriminative signatures of the different appliances. In this paper, we propose a novel method named adversarial energy disaggregation (AED) based on DNNs. We introduce the idea of adversarial learning into NILM, which is new for the energy disaggregation task. Our method trains a generator and multiple discriminators via an adversarial fashion. The proposed method not only learns shard representations for different appliances, but captures the specific multimode structures of each appliance. Extensive experiments on real-world datasets verify that our method can achieve new state-of-the-art performance.      
### 15.DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2108.01997.pdf)
>  Early detection of the coronavirus disease 2019 (COVID-19) helps to treat patients timely and increase the cure rate, thus further suppressing the spread of the disease. In this study, we propose a novel deep learning based detection and similar case recommendation network to help control the epidemic. Our proposed network contains two stages: the first one is a lung region segmentation step and is used to exclude irrelevant factors, and the second is a detection and recommendation stage. Under this framework, in the second stage, we develop a dual-children network (DuCN) based on a pre-trained ResNet-18 to simultaneously realize the disease diagnosis and similar case recommendation. Besides, we employ triplet loss and intrapulmonary distance maps to assist the detection, which helps incorporate tiny differences between two images and is conducive to improving the diagnostic accuracy. For each confirmed COVID-19 case, we give similar cases to provide radiologists with diagnosis and treatment references. We conduct experiments on a large publicly available dataset (CC-CCII) and compare the proposed model with state-of-the-art COVID-19 detection methods. The results show that our proposed model achieves a promising clinical performance.      
### 16.Robustness of convolutional neural networks to physiological ECG noise  [ :arrow_down: ](https://arxiv.org/pdf/2108.01995.pdf)
>  The electrocardiogram (ECG) is one of the most widespread diagnostic tools in healthcare and supports the diagnosis of cardiovascular disorders. Deep learning methods are a successful and popular technique to detect indications of disorders from an ECG signal. However, there are open questions around the robustness of these methods to various factors, including physiological ECG noise. In this study we generate clean and noisy versions of an ECG dataset before applying Symmetric Projection Attractor Reconstruction (SPAR) and scalogram image transformations. A pretrained convolutional neural network is trained using transfer learning to classify these image transforms. For the clean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were 0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the noisy ECG datasets. Notably, when the network trained on clean data was used to classify the noisy datasets, performance decreases of up to 0.18 in F1 scores were seen. However, when the network trained on the noisy data was used to classify the clean dataset, the performance decrease was less than 0.05. We conclude that physiological ECG noise impacts classification using deep learning methods and careful consideration should be given to the inclusion of noisy ECG signals in the training data when developing supervised networks for ECG classification.      
### 17.Lung Sound Classification Using Co-tuning and Stochastic Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2108.01991.pdf)
>  In this paper, we use pre-trained ResNet models as backbone architectures for classification of adventitious lung sounds and respiratory diseases. The knowledge of the pre-trained model is transferred by using vanilla fine-tuning, co-tuning, stochastic normalization and the combination of the co-tuning and stochastic normalization techniques. Furthermore, data augmentation in both time domain and time-frequency domain is used to account for the class imbalance of the ICBHI and our multi-channel lung sound dataset. Additionally, we apply spectrum correction to consider the variations of the recording device properties on the ICBHI dataset. Empirically, our proposed systems mostly outperform all state-of-the-art lung sound classification systems for the adventitious lung sounds and respiratory diseases of both datasets.      
### 18.Automatic hemisphere segmentation in rodent MRI with lesions  [ :arrow_down: ](https://arxiv.org/pdf/2108.01941.pdf)
>  We present MedicDeepLabv3+, a convolutional neural network that is the first completely automatic method to segment brain hemispheres in magnetic resonance (MR) images of rodents with lesions. MedicDeepLabv3+ improves the state-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial attention layers and additional skip connections that, as we show in our experiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR image preprocessing, such as bias-field correction or registration to a template, produces segmentations in less than a second, and its GPU memory requirements can be adjusted based on the available resources. Using a large dataset of 723 MR rat brain images, we evaluated our MedicDeepLabv3+, two state-of-the-art convolutional neural networks (DeepLabv3+, UNet) and three approaches that were specifically designed for skull-stripping rodent MR images (Demon, RATS and RBET). In our experiments, MedicDeepLabv3+ outperformed the other methods, yielding an average Dice coefficient of 0.952 and 0.944 in the brain and contralateral hemisphere regions. Additionally, we show that despite limiting the GPU memory and the training data to only three images, our MedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our method, publicly available at <a class="link-external link-https" href="https://github.com/jmlipman/MedicDeepLabv3Plus" rel="external noopener nofollow">this https URL</a>, yielded excellent results in multiple scenarios, demonstrating its capability to reduce human workload in rodent neuroimaging studies.      
### 19.WIDEFT: A Corpus of Radio Frequency Signals for Wireless Device Fingerprint Research  [ :arrow_down: ](https://arxiv.org/pdf/2108.01878.pdf)
>  Wireless network security may be improved by identifying networked devices via traits that are tied to hardware differences, typically related to unique variations introduced in the manufacturing process. One way these variations manifest is through unique transient events when a radio transmitter is activated or deactivated. Features extracted from these signal bursts have in some cases, shown to provide a unique "fingerprint" for a wireless device. However, only recently have researchers made such data available for research and comparison. Herein, we describe a publicly-available corpus of radio frequency signals that can be used for wireless device fingerprint research. The WIDEFT corpus contains signal bursts from 138 unique devices (100 bursts per device), including Bluetooth- and WiFi-enabled devices, from 79 unique models. Additionally, to demonstrate the utility of the WIDEFT corpus, we provide four baseline evaluations using a minimal subset of previously-proposed features and a simple ensemble classifier.      
### 20.Improving Freeway Merging Efficiency via Flow-Level Coordination of Connected and Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2108.01875.pdf)
>  Freeway on-ramps are typical bottlenecks in the freeway network due to the frequent disturbances caused by their associated merging, weaving, and lane-changing behaviors. With real-time communication and precise motion control, Connected and Autonomous Vehicles (CAVs) provide an opportunity to substantially enhance the traffic operational performance of on-ramp bottlenecks. In this paper, we propose an upper-level control strategy to coordinate the two traffic streams at on-ramp merging through proactive gap creation and platoon formation. The coordination consists of three components: (1) mainline vehicles proactively decelerate to create large merging gaps; (2) ramp vehicles form platoons before entering the main road; (3) the gaps created on the main road and the platoons formed on the ramp are coordinated with each other in terms of size, speed, and arrival time. The coordination is formulated as a constrained optimization problem, incorporating both macroscopic and microscopic traffic flow models, for flow-level efficiency gains. The model uses traffic state parameters as inputs and determines the optimal coordination plan adaptive to real-time traffic conditions. The benefits of the proposed coordination are demonstrated through an illustrative case study. Results show that the coordination is compatible with real-world implementation and can substantially improve the overall efficiency of on-ramp merging, especially under high traffic volume conditions, where recurrent traffic congestion is prevented, and merging throughput increased.      
### 21.1.71 Tb/s Single-Channel and 56.51 Tb/s DWDM Transmission over 96.5 km Field-Deployed SSMF  [ :arrow_down: ](https://arxiv.org/pdf/2108.01873.pdf)
>  We report an industry leading optical dense wavelength division multiplexing (DWDM) field trial with line rates per channel exceeding 1.66 Tb/s using 130 GBaud dual-polarization probabilistic constellation shaping 256-ary quadrature amplitude modulation (DP-PCS256QAM) in a high capacity data center interconnect (DCI) scenario. This research trial was performed on 96.5 km of field-deployed standard single mode G.652 fiber infrastructure of Deutsche Telekom in Germany employing Erbium-doped fiber amplifier (EDFA)-only amplification. A total of 34 channels were transmitted with 150 GHz spacing for a total fiber capacity of 56.51 Tb/s and a spectral efficiency higher than 11bit/s/Hz. In the single-channel transmission scenario 1.71 Tb/s was achieved over the same link. In addition, we successfully demonstrate record net bitrates of 1.88 Tb/s in back-to-back (B2B) using 130 GBaud DP-PCS400QAM.      
### 22.Feedback Free Distributed Transmit Beamforming using Guided Directionality  [ :arrow_down: ](https://arxiv.org/pdf/2108.01837.pdf)
>  Distributed transmit beamforming enables cooperative radios to act as one virtual antenna array, extending their communications' range beyond the capabilities of a single radio. Most existing distributed beamforming approaches rely on the destination radio sending feedback to adjust the transmitters' signals for coherent combining. However, relying on the destination radio's feedback limits the communications range to that of a single radio. Existing feedback free approaches rely on phase synchronization and knowing the node locations with sub-wavelength accuracy, which becomes impractical for radios mounted on high-mobility platforms like UAVs. In this work, we propose and demonstrate a feedback free distributed beamforming approach that leverages the radio's mobility and coarse location information in a dominant line-of-sight channel. In the proposed approach, one radio acts as a guide and moves to point the beam of the remaining radios towards the destination. We specify the radios' position requirements and verify their relation to the combined signal at the destination using simulations. A proof of concept demo was implemented using software defined radios, showing up to 9dB SNR improvement in the beamforming direction just by relying on the coarse placement of four radios.      
### 23.Blind and neural network-guided convolutional beamformer for joint denoising, dereverberation, and source separation  [ :arrow_down: ](https://arxiv.org/pdf/2108.01836.pdf)
>  This paper proposes an approach for optimizing a Convolutional BeamFormer (CBF) that can jointly perform denoising (DN), dereverberation (DR), and source separation (SS). First, we develop a blind CBF optimization algorithm that requires no prior information on the sources or the room acoustics, by extending a conventional joint DR and SS method. For making the optimization computationally tractable, we incorporate two techniques into the approach: the Source-Wise Factorization (SW-Fact) of a CBF and the Independent Vector Extraction (IVE). To further improve the performance, we develop a method that integrates a neural network(NN) based source power spectra estimation with CBF optimization by an inverse-Gamma prior. Experiments using noisy reverberant mixtures reveal that our proposed method with both blind and NN-guided scenarios greatly outperforms the conventional state-of-the-art NN-supported mask-based CBF in terms of the improvement in automatic speech recognition and signal distortion reduction performance.      
### 24.Unsupervised Domain Adaptation for Retinal Vessel Segmentation with Adversarial Learning and Transfer Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2108.01821.pdf)
>  Retinal vessel segmentation plays a key role in computer-aided screening, diagnosis, and treatment of various cardiovascular and ophthalmic diseases. Recently, deep learning-based retinal vessel segmentation algorithms have achieved remarkable performance. However, due to the domain shift problem, the performance of these algorithms often degrades when they are applied to new data that is different from the training data. Manually labeling new data for each test domain is often a time-consuming and laborious task. In this work, we explore unsupervised domain adaptation in retinal vessel segmentation by using entropy-based adversarial learning and transfer normalization layer to train a segmentation network, which generalizes well across domains and requires no annotation of the target domain. Specifically, first, an entropy-based adversarial learning strategy is developed to reduce the distribution discrepancy between the source and target domains while also achieving the objective of entropy minimization on the target domain. In addition, a new transfer normalization layer is proposed to further boost the transferability of the deep network. It normalizes the features of each domain separately to compensate for the domain distribution gap. Besides, it also adaptively selects those feature channels that are more transferable between domains, thus further enhancing the generalization performance of the network. We conducted extensive experiments on three regular fundus image datasets and an ultra-widefield fundus image dataset, and the results show that our approach yields significant performance gains compared to other state-of-the-art methods.      
### 25.On Exponential Utility and Conditional Value-at-Risk as Risk-Averse Performance Criteria  [ :arrow_down: ](https://arxiv.org/pdf/2108.01771.pdf)
>  The standard approach to risk-averse control is to use the Exponential Utility (EU) functional, which has been studied for several decades. Like other risk-averse utility functionals, EU encodes risk aversion through an increasing convex mapping $\varphi$ of objective costs to subjective costs. An objective cost is a realization $y$ of a random variable $Y$. In contrast, a subjective cost is a realization $\varphi(y)$ of a random variable $\varphi(Y)$ that has been transformed to measure preferences about the outcomes. For EU, the transformation is $\varphi(y) = \exp(\frac{-\theta}{2}y)$, and under certain conditions, the quantity $\varphi^{-1}(E(\varphi(Y)))$ can be approximated by a linear combination of the mean and variance of $Y$. More recently, there has been growing interest in risk-averse control using the Conditional Value-at-Risk (CVaR) functional. In contrast to the EU functional, the CVaR of a random variable $Y$ concerns a fraction of its possible realizations. If $Y$ is a continuous random variable with finite $E(|Y|)$, then the CVaR of $Y$ at level $\alpha$ is the expectation of $Y$ in the $\alpha \cdot 100 \%$ worst cases. Here, we study the applications of risk-averse functionals to controller synthesis and safety analysis through the development of numerical examples, with emphasis on EU and CVaR. Our contribution is to examine the decision-theoretic, mathematical, and computational trade-offs that arise when using EU and CVaR for optimal control and safety analysis. We are hopeful that this work will advance the interpretability and elucidate the potential benefits of risk-averse control technology.      
### 26.Ellipsotopes: Combining Ellipsoids and Zonotopes for Reachability Analysis and Fault Detection  [ :arrow_down: ](https://arxiv.org/pdf/2108.01750.pdf)
>  Ellipsoids are a common representation for reachability analysis because they are closed under affine maps and allow conservative approximation of Minkowski sums; this enables one to incorporate uncertainty and linearization error in a dynamical system by exapnding the size of the reachable set. Zonotopes, a type of symmetric, convex polytope, are similarly frequently used due to efficient numerical implementation of affine maps and exact Minkowski sums. Both of these representations also enable efficient, convex collision detection for fault detection or formal verification tasks, wherein one checks if the reachable set of a system collides (i.e., intersects) with an unsafe set. However, both representations often result in conservative representations for reachable sets of arbitrary systems, and neither is closed under intersection. Recently, constrained zonotopes and constrained polynomial zonotopes have been shown to overcome some of these conservatism challenges, and are closed under intersection. However, constrained zonotopes can not represent shapes with smooth boundaries such as ellipsoids, and constrained polynomial zonotopes can require solving a non-convex program for collision checking (i.e., fault detection). This paper introduces ellipsotopes, a set representation that is closed under affine maps, Minkowski sums, and intersections. Ellipsotopes combine the advantages of ellipsoids and zonotopes, and enable convex collision checking at the expense of more conservative reachable sets than constrained polynomial zonotopes. The utility of this representation is demonstrated on several examples.      
### 27.TreCap: A wearable device to measure and assess tremor data of visually guided hand movements in real time  [ :arrow_down: ](https://arxiv.org/pdf/2108.01736.pdf)
>  The assessment and treatment of motor symptoms such as tremor in Parkinson's disease depends exclusively on the physician's visual observation of standardised movements (i.e. motor tasks). Wearable sensors such as accelerometers are able to detect some manifestations of these pathological signs in movement disorders. Sensor data from motor tasks, however, must be processed sequentially with annotated data from clinical experts. Hence, we designed TreCap, a custom-built wearable device with new software to capture and evaluate motor symptoms such as tremor in real time. Inertial sensor data is systematically processed, stored and tailored to each motor task by this software, including annotated data from clinical rating scores and deep brain stimulation parameters. For prototype testing, the wearable device was validated in a pilot study on subjects with physiological hand tremor. The processed data sets are suitable for machine learning to classify motor tasks. Results on healthy subjects demonstrate an accuracy of 95% with support vector machine algorithms. TreCap software is expandable and allows full access to the configuration of all sensors via Bluetooth. Finally, the functions of the entire device provide a platform to be apt for future clinical trials.      
### 28.Unrolled Wirtinger Flow with Deep Priors for Phaseless Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2108.01735.pdf)
>  We introduce a deep learning (DL) based network for imaging from measurement intensities. The network architecture uses a recurrent structure that unrolls the Wirtinger Flow (WF) algorithm with a deep prior which enables performing the algorithm updates in a lower dimensional encoded image space. We use a separate deep network (DN), referred to as the encoding network, for transforming the spectral initialization used in the WF algorithm to an appropriate initial value for the encoded domain. The unrolling scheme that models a fixed number of iterations of the underlying algorithm into a recurrent neural network (RNN) enable us to simultaneously learn the parameters of the prior network, the encoding network and the RNN during training. We establish sufficient conditions on the network to guarantee exact recovery under deterministic forward models and demonstrate the relation between the Lipschitz constants of the trained prior and encoding networks to the convergence rate. We show the practical applicability of our method on synthetic aperture imaging using high fidelity simulation data from the PCSWAT software. Our numerical study shows that the deep prior facilitates improvements in sample complexity.      
### 29.Bifocal Neural ASR: Exploiting Keyword Spotting for Inference Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2108.01704.pdf)
>  We present Bifocal RNN-T, a new variant of the Recurrent Neural Network Transducer (RNN-T) architecture designed for improved inference time latency on speech recognition tasks. The architecture enables a dynamic pivot for its runtime compute pathway, namely taking advantage of keyword spotting to select which component of the network to execute for a given audio frame. To accomplish this, we leverage a recurrent cell we call the Bifocal LSTM (BFLSTM), which we detail in the paper. The architecture is compatible with other optimization strategies such as quantization, sparsification, and applying time-reduction layers, making it especially applicable for deployed, real-time speech recognition settings. We present the architecture and report comparative experimental results on voice-assistant speech recognition tasks. Specifically, we show our proposed Bifocal RNN-T can improve inference cost by 29.1% with matching word error rates and only a minor increase in memory size.      
### 30.Classification of Electrical Impedance Tomography Data Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.01668.pdf)
>  Patients suffering from pulmonary diseases typically exhibit pathological lung ventilation in terms of homogeneity. Electrical Impedance Tomography (EIT) is a non-invasive imaging method that allows to analyze and quantify the distribution of ventilation in the lungs. In this article, we present a new approach to promote the use of EIT data and the implementation of new clinical applications for differential diagnosis, with the development of several machine learning models to discriminate between EIT data from healthy and non-healthy subjects. EIT data from 16 subjects were acquired: 5 healthy and 11 non-healthy subjects (with multiple pulmonary conditions). Preliminary results have shown accuracy percentages of 66\% in challenging evaluation scenarios. The results suggest that the pairing of EIT feature engineering methods with machine learning methods could be further explored and applied in the diagnostic and monitoring of patients suffering from lung diseases. Also, we introduce the use of a new feature in the context of EIT data analysis (Impedance Curve Correlation).      
### 31.Optimization of retina-like illumination patterns in ghost imaging  [ :arrow_down: ](https://arxiv.org/pdf/2108.01667.pdf)
>  Ghost imaging (GI) reconstructs images using a single-pixel or bucket detector, which has the advantages of scattering robustness, wide spectrum and beyond-visual-field imaging. However, this technique needs large amount of measurements to obtain a sharp image. There have been a lot of methods proposed to overcome this disadvantage. Retina-like patterns, as one of the compressive sensing approaches, enhance the imaging quality of region of interest (ROI) while not increase measurements. The design of the retina-like patterns determines the performance of the ROI in the reconstructed image. Unlike the conventional method to fill in ROI with random patterns, we propose to optimize retina-like patterns by filling in the ROI with the patterns containing the sparsity prior of objects. This proposed method is verified by simulations and experiments compared with conventional GI, retina-like GI and GI using patterns optimized by principal component analysis. The method using optimized retina-like patterns obtain the best imaging quality in ROI than other methods. Meanwhile, the good generalization ability of the optimized retina-like pattern is also verified. While designing the size and position of the ROI of retina-like pattern, the feature information of the target can be obtained to optimize the pattern of ROI. This proposed method paves the way for realizing high-quality GI.      
### 32.Complementary Fourier single-pixel imaging  [ :arrow_down: ](https://arxiv.org/pdf/2108.01666.pdf)
>  Single-pixel imaging, with the advantages of a wide spectrum, beyond-visual-field imaging, and robustness to light scattering, has attracted increasing attention in recent years. Fourier single-pixel imaging (FSI) can reconstruct sharp images under sub-Nyquist sampling. However, the conventional FSI has difficulty with balancing the imaging quality and efficiency. To overcome this issue, we proposed a novel approach called complementary Fourier single-pixel imaging (CFSI) to reduce measurements while retaining its robustness. The complementary nature of Fourier patterns based on a four-step phase-shift algorithm is combined with the complementary nature of a digital micromirror device. CFSI only requires two phase-shifted patterns to obtain one Fourier spectral value. Four light intensity values are obtained by load the two patterns, and the spectral value is calculated through differential measurement, which has good robustness to noise. The proposed method is verified by simulations and experiments compared with FSI based on two-, three-, and four-step phase shift algorithms. CFSI performed better than the other methods under the condition that the best imaging quality of CFSI is not reached. The reported technique provides an alternative approach to realize real-time and high-quality imaging.      
### 33.Efficient Neural Network Approximation of Robust PCA for Automated Analysis of Calcium Imaging Data  [ :arrow_down: ](https://arxiv.org/pdf/2108.01665.pdf)
>  Calcium imaging is an essential tool to study the activity of neuronal populations. However, the high level of background fluorescence in images hinders the accurate identification of neurons and the extraction of neuronal activities. While robust principal component analysis (RPCA) is a promising method that can decompose the foreground and background in such images, its computational complexity and memory requirement are prohibitively high to process large-scale calcium imaging data. Here, we propose BEAR, a simple bilinear neural network for the efficient approximation of RPCA which achieves an order of magnitude speed improvement with GPU acceleration compared to the conventional RPCA algorithms. In addition, we show that BEAR can perform foreground-background separation of calcium imaging data as large as tens of gigabytes. We also demonstrate that two BEARs can be cascaded to perform simultaneous RPCA and non-negative matrix factorization for the automated extraction of spatial and temporal footprints from calcium imaging data. The source code used in the paper is available at <a class="link-external link-https" href="https://github.com/NICALab/BEAR" rel="external noopener nofollow">this https URL</a>.      
### 34.Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.02148.pdf)
>  Due to the mass advancement in ubiquitous technologies nowadays, new pervasive methods have come into the practice to provide new innovative features and stimulate the research on new human-computer interactions. This paper presents a hand gesture recognition method that utilizes the smartphone's built-in speakers and microphones. The proposed system emits an ultrasonic sonar-based signal (inaudible sound) from the smartphone's stereo speakers, which is then received by the smartphone's microphone and processed via a Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data augmentation techniques are proposed to improve the detection accuracy and three dual-channel input fusion methods are compared. The first method merges the dual-channel audio as a single input spectrogram image. The second method adopts early fusion by concatenating the dual-channel spectrograms. The third method adopts late fusion by having two convectional input branches processing each of the dual-channel spectrograms and then the outputs are merged by the last layers. Our experimental results demonstrate a promising detection accuracy for the six gestures presented in our publicly available dataset with an accuracy of 93.58\% as a baseline.      
### 35.Dyn-ASR: Compact, Multilingual Speech Recognition via Spoken Language and Accent Identification  [ :arrow_down: ](https://arxiv.org/pdf/2108.02034.pdf)
>  Running automatic speech recognition (ASR) on edge devices is non-trivial due to resource constraints, especially in scenarios that require supporting multiple languages. We propose a new approach to enable multilingual speech recognition on edge devices. This approach uses both language identification and accent identification to select one of multiple monolingual ASR models on-the-fly, each fine-tuned for a particular accent. Initial results for both recognition performance and resource usage are promising with our approach using less than 1/12th of the memory consumed by other solutions.      
### 36.A purely data-driven framework for prediction, optimization, and control of networked processes: application to networked SIS epidemic model  [ :arrow_down: ](https://arxiv.org/pdf/2108.02005.pdf)
>  Networks are landmarks of many complex phenomena where interweaving interactions between different agents transform simple local rule-sets into nonlinear emergent behaviors. While some recent studies unveil associations between the network structure and the underlying dynamical process, identifying stochastic nonlinear dynamical processes continues to be an outstanding problem. Here we develop a simple data-driven framework based on operator-theoretic techniques to identify and control stochastic nonlinear dynamics taking place over large-scale networks. The proposed approach requires no prior knowledge of the network structure and identifies the underlying dynamics solely using a collection of two-step snapshots of the states. This data-driven system identification is achieved by using the Koopman operator to find a low dimensional representation of the dynamical patterns that evolve linearly. Further, we use the global linear Koopman model to solve critical control problems by applying to model predictive control (MPC)--typically, a challenging proposition when applied to large networks. We show that our proposed approach tackles this by converting the original nonlinear programming into a more tractable optimization problem that is both convex and with far fewer variables.      
### 37.Do What You Know: Coupling Knowledge with Action in Discrete-Event Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.02000.pdf)
>  An epistemic model for decentralized discrete-event systems with non-binary control is presented. This framework combines existing work on conditional control decisions with existing work on formal reasoning about knowledge in discrete-event systems. The novelty in the model presented is that the necessary and sufficient conditions for problem solvability encapsulate the actions that supervisors must take. This direct coupling between knowledge and action -- in a formalism that mimics natural language -- makes it easier, when the problem conditions fail, to determine how the problem requirements should be revised.      
### 38.Graph Attention Network For Microwave Imaging of Brain Anomaly  [ :arrow_down: ](https://arxiv.org/pdf/2108.01965.pdf)
>  So far, numerous learned models have been pressed to use in microwave imaging problems. These models however, are oblivious to the imaging geometry. It has always been hard to bake the physical setup of the imaging array into the structure of the network, resulting in a data-intensive models that are not practical. This work put forward a graph formulation of the microwave imaging array. The architectures proposed is made cognizant of the physical setup, allowing it to incorporate the symmetries, resulting in a less data requirements. Graph convolution and attention mechanism is deployed to handle the cases of fully-connected graphs corresponding to multi-static arrays. The graph-treatment of the problem is evaluated on experimental setup in context of brain anomaly localization with microwave imaging.      
### 39.A universal detector of CNN-generated images using properties of checkerboard artifacts in the frequency domain  [ :arrow_down: ](https://arxiv.org/pdf/2108.01892.pdf)
>  We propose a novel universal detector for detecting images generated by using CNNs. In this paper, properties of checkerboard artifacts in CNN-generated images are considered, and the spectrum of images is enhanced in accordance with the properties. Next, a classifier is trained by using the enhanced spectrums to judge a query image to be a CNN-generated ones or not. In addition, an ensemble of the proposed detector with emphasized spectrums and a conventional detector is proposed to improve the performance of these methods. In an experiment, the proposed ensemble is demonstrated to outperform a state-of-the-art method under some conditions.      
### 40.Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations  [ :arrow_down: ](https://arxiv.org/pdf/2108.01846.pdf)
>  Training-time safety violations have been a major concern when we deploy reinforcement learning algorithms in the real world. This paper explores the possibility of safe RL algorithms with zero training-time safety violations in the challenging setting where we are only given a safe but trivial-reward initial policy without any prior knowledge of the dynamics model and additional offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe RL (CRABS), which iteratively learns barrier certificates, dynamics models, and policies. The barrier certificates, learned via adversarial training, ensure the policy's safety assuming calibrated learned dynamics model. We also add a regularization term to encourage larger certified regions to enable better exploration. Empirical simulations show that zero safety violations are already challenging for a suite of simple environments with only 2-4 dimensional state space, especially if high-reward policies have to visit regions near the safety boundary. Prior methods require hundreds of violations to achieve decent rewards on these tasks, whereas our proposed algorithms incur zero violations.      
### 41.Information Sieve: Content Leakage Reduction in End-to-End Prosody For Expressive Speech Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2108.01831.pdf)
>  Expressive neural text-to-speech (TTS) systems incorporate a style encoder to learn a latent embedding as the style information. However, this embedding process may encode redundant textual information. This phenomenon is called content leakage. Researchers have attempted to resolve this problem by adding an ASR or other auxiliary supervision loss functions. In this study, we propose an unsupervised method called the "information sieve" to reduce the effect of content leakage in prosody transfer. The rationale of this approach is that the style encoder can be forced to focus on style information rather than on textual information contained in the reference speech by a well-designed downsample-upsample filter, i.e., the extracted style embeddings can be downsampled at a certain interval and then upsampled by duplication. Furthermore, we used instance normalization in convolution layers to help the system learn a better latent style space. Objective metrics such as the significantly lower word error rate (WER) demonstrate the effectiveness of this model in mitigating content leakage. Listening tests indicate that the model retains its prosody transferability compared with the baseline models such as the original GST-Tacotron and ASR-guided Tacotron.      
### 42.Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation  [ :arrow_down: ](https://arxiv.org/pdf/2108.01812.pdf)
>  Fine-tuning pretrained language models (LMs) is a popular approach to automatic speech recognition (ASR) error detection during post-processing. While error detection systems often take advantage of statistical language archetypes captured by LMs, at times the pretrained knowledge can hinder error detection performance. For instance, presence of speech disfluencies might confuse the post-processing system into tagging disfluent but accurate transcriptions as ASR errors. Such confusion occurs because both error detection and disfluency detection tasks attempt to identify tokens at statistically unlikely positions. This paper proposes a scheme to improve existing LM-based ASR error detection systems, both in terms of detection scores and resilience to such distracting auxiliary tasks. Our approach adopts the popular mixup method in text feature space and can be utilized with any black-box ASR output. To demonstrate the effectiveness of our method, we conduct post-processing experiments with both traditional and end-to-end ASR systems (both for English and Korean languages) with 5 different speech corpora. We find that our method improves both ASR error detection F 1 scores and reduces the number of correctly transcribed disfluencies wrongly detected as ASR errors. Finally, we suggest methods to utilize resulting LMs directly in semi-supervised ASR training.      
### 43.Deformation Recovery Control and Post-Impact Trajectory Replanning for Collision-Resilient Mobile Robots  [ :arrow_down: ](https://arxiv.org/pdf/2108.01802.pdf)
>  The paper focuses on collision-inclusive motion planning for impact-resilient mobile robots. We propose a new deformation recovery and replanning strategy to handle collisions that may occur at run-time. Contrary to collision avoidance methods that generate trajectories only in conservative local space or require collision checking that has high computational cost, our method directly generates (local) trajectories with imposing only waypoint constraints. If a collision occurs, our method then estimates the post-impact state and computes from there an intermediate waypoint to recover from the collision. To achieve so, we develop two novel components: 1) a deformation recovery controller that optimizes the robot's states during post-impact recovery phase, and 2) a post-impact trajectory replanner that adjusts the next waypoint with the information from the collision for the robot to pass through and generates a polynomial-based minimum effort trajectory. The proposed strategy is evaluated experimentally with an omni-directional impact-resilient wheeled robot. The robot is designed in house, and it can perceive collisions with the aid of Hall effect sensors embodied between the robot's main chassis and a surrounding deflection ring-like structure.      
### 44.Nonconvex Factorization and Manifold Formulations are Almost Equivalent in Low-rank Matrix Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2108.01772.pdf)
>  In this paper, we consider the geometric landscape connection of the widely studied manifold and factorization formulations in low-rank positive semidefinite (PSD) and general matrix optimization. We establish an equivalence on the set of first-order stationary points (FOSPs) and second-order stationary points (SOSPs) between the manifold and the factorization formulations. We further give a sandwich inequality on the spectrum of Riemannian and Euclidean Hessians at FOSPs, which can be used to transfer more geometric properties from one formulation to another. Similarities and differences on the landscape connection under the PSD case and the general case are discussed. To the best of our knowledge, this is the first geometric landscape connection between the manifold and the factorization formulations for handling rank constraints. In the general low-rank matrix optimization, the landscape connection of two factorization formulations (unregularized and regularized ones) is also provided. By applying these geometric landscape connections, we are able to solve unanswered questions in literature and establish stronger results in the applications on geometric analysis of phase retrieval, well-conditioned low-rank matrix optimization, and the role of regularization in factorization arising from machine learning and signal processing.      
### 45.An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2108.01769.pdf)
>  Previous work has shown that neural architectures are able to perform optical music recognition (OMR) on monophonic and homophonic music with high accuracy. However, piano and orchestral scores frequently exhibit polyphonic passages, which add a second dimension to the task. Monophonic and homophonic music can be described as homorhythmic, or having a single musical rhythm. Polyphonic music, on the other hand, can be seen as having multiple rhythmic sequences, or voices, concurrently. We first introduce a workflow for creating large-scale polyphonic datasets suitable for end-to-end recognition from sheet music publicly available on the MuseScore forum. We then propose two novel formulations for end-to-end polyphonic OMR -- one treating the problem as a type of multi-task binary classification, and the other treating it as multi-sequence detection. Building upon the encoder-decoder architecture and an image encoder proposed in past work on end-to-end OMR, we propose two novel decoder models -- FlagDecoder and RNNDecoder -- that correspond to the two formulations. Finally, we compare the empirical performance of these end-to-end approaches to polyphonic OMR and observe a new state-of-the-art performance with our multi-sequence detection decoder, RNNDecoder.      
### 46.Finite Horizon Privacy of Stochastic Dynamical Systems: A Synthesis Framework for Dependent Gaussian Mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2108.01755.pdf)
>  We address the problem of synthesizing distorting mechanisms that maximize privacy of stochastic dynamical systems. Information about the system state is obtained through sensor measurements. This data is transmitted to a remote station through an unsecured/public communication network. We aim to keep part of the system state private (a private output); however, because the network is unsecured, adversaries might access sensor data and input signals, which can be used to estimate private outputs. To prevent an accurate estimation, we pass sensor data and input signals through a distorting (privacy-preserving) mechanism before transmission, and send the distorted data to the trusted user. These mechanisms consist of a coordinate transformation and additive dependent Gaussian vectors. We formulate the synthesis of the distorting mechanisms as a convex program, where we minimize the mutual information (our privacy metric) between an arbitrarily large sequence of private outputs and the disclosed distorted data for desired distortion levels -- how different actual and distorted data are allowed to be.      
### 47.Terahertz Wireless Transmissions with Maximal Ratio Combining over Fluctuating Two-Ray Fading  [ :arrow_down: ](https://arxiv.org/pdf/2108.01698.pdf)
>  Mitigating channel fading and transceiver impairments are desirable for high-speed terahertz (THz) wireless links. This paper analyzes the performance of a multi-antenna THz wireless system by considering the combined effect of pointing errors and fluctuating two-ray (FTR) fading model. We provide a statistical characterization of the maximal ratio combining (MRC) receiver over independent and nonidentical (i.ni.d.) channel conditions in terms of multi-variate Fox's H by deriving density and distribution functions of the signal-to-noise ratio (SNR) of a single-link THz link using incomplete Gamma function. We develop exact analytical expressions of outage probability, average bit-error-rate (BER), and ergodic capacity for both single-antenna and MRC receivers. We also present the diversity order of the system by deriving asymptotic expressions for outage probability and average BER at high SNR to obtain insights into the system performance. We validate our derived analytical expressions with Monte-Carlo simulations and demonstrate the effect of various system and channel parameters on the performance of single and multi-antenna THz wireless communications.      
