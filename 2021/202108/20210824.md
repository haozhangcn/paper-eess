# ArXiv eess --Tue, 24 Aug 2021
### 1.Suitability of FPS and DPS in NOMA for Real-Time and Non-Real Time Applications  [ :arrow_down: ](https://arxiv.org/pdf/2108.10269.pdf)
>  Non-Orthogonal Multiple Access (NOMA) is a popular solution for supporting a high number of users and along with significant bandwidth in 5G cellular communication. By using a technique called cooperative relaying, the same data is sent to all the users, and one user can relay data to the other. In order to provide enough power for the users, energy harvesting techniques have been introduced with Simultaneous Wireless Information and Power Transfer (SWIPT) coming to prominence in recent times. In this paper, analysis has been made comparing two different power allocation schemes in NOMA, Fixed Power allocation Scheme (FPS) and Dynamic Power allocation Scheme (DPS). The comparisons were made in terms of their performance and characteristics while undergoing SWIPT. It has been found that by using DPS, an almost 25% increase in peak spectral efficiency can be obtained compared to FPS. However, DPS suffers from a higher outage probability as the increase of power causes the signal bandwidth to drop below the target rate a significant number of times. Based on the detailed results, conclusions were drawn as to which power allocation coefficient scheme would be used in real-time and non-real time communication standards, respectively. The results suggest that for real-time communication, FPS is more suitable while for non-real-time communication, DPS appears to work better than FPS.      
### 2.SwinIR: Image Restoration Using Swin Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2108.10257.pdf)
>  Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by $\textbf{up to 0.14$\sim$0.45dB}$, while the total number of parameters can be reduced by $\textbf{up to 67%}$.      
### 3.Smoother Entropy for Active State Trajectory Estimation and Obfuscation in POMDPs  [ :arrow_down: ](https://arxiv.org/pdf/2108.10227.pdf)
>  We study the problem of controlling a partially observed Markov decision process (POMDP) to either aid or hinder the estimation of its state trajectory by optimising the conditional entropy of the state trajectory given measurements and controls, a quantity we dub the smoother entropy. Our consideration of the smoother entropy contrasts with previous active state estimation and obfuscation approaches that instead resort to measures of marginal (or instantaneous) state uncertainty due to tractability concerns. By establishing novel expressions of the smoother entropy in terms of the usual POMDP belief state, we show that our active estimation and obfuscation problems can be reformulated as Markov decision processes (MDPs) that are fully observed in the belief state. Surprisingly, we identify belief-state MDP reformulations of both active estimation and obfuscation with concave cost and cost-to-go functions, which enables the use of standard POMDP techniques to construct tractable bounded-error (approximate) solutions. We show in simulations that optimisation of the smoother entropy leads to superior trajectory estimation and obfuscation compared to alternative approaches.      
### 4.ECG-Based Heart Arrhythmia Diagnosis Through Attentional Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.10226.pdf)
>  Electrocardiography (ECG) signal is a highly applied measurement for individual heart condition, and much effort have been endeavored towards automatic heart arrhythmia diagnosis based on machine learning. However, traditional machine learning models require large investment of time and effort for raw data preprocessing and feature extraction, as well as challenged by poor classification performance. Here, we propose a novel deep learning model, named Attention-Based Convolutional Neural Networks (ABCNN) that taking advantage of CNN and multi-head attention, to directly work on the raw ECG signals and automatically extract the informative dependencies for accurate arrhythmia detection. To evaluate the proposed approach, we conduct extensive experiments over a benchmark ECG dataset. Our main task is to find the arrhythmia from normal heartbeats and, at the meantime, accurately recognize the heart diseases from five arrhythmia types. We also provide convergence analysis of ABCNN and intuitively show the meaningfulness of extracted representation through visualization. The experimental results show that the proposed ABCNN outperforms the widely used baselines, which puts one step closer to intelligent heart disease diagnosis system.      
### 5.IQ Photonic Receiver for Coherent Imaging with a Scalable Aperture  [ :arrow_down: ](https://arxiv.org/pdf/2108.10225.pdf)
>  Silicon photonics (SiP) integrated coherent image sensors offer higher sensitivity and improved range-resolution-product compared to direct detection image sensors such as CCD and CMOS devices. Previous generation of SiP coherent imagers suffer from relative optical phase fluctuations between the signal and reference paths, which results in random phase and amplitude fluctuations in the output signal. This limitation negatively impacts the SNR and signal acquisition times. Here we present a coherent imager system that suppresses the optical carrier signal and removes non-idealities from the relative optical path using a photonic in-phase (I) and quadrature (Q) receiver via a $90^\circ$ hybrid detector. Furthermore, we incorporate row-column read-out and row-column addressing schemes to address the electro-optical interconnect density challenge. Our novel row-column read-out architecture for the sensor array requires only $2N$ interconnects for $N^2$ sensors. An $8\times8$ IQ sensor array is presented as a proof-of-concept demonstration with $1.2\times 10^{-5}$ resolution over range accuracy. Free-space FMCW ranging with 250um resolution at 1m distance has been demonstrated using this sensor array.      
### 6.Achieving Full Grating-Lobe-Free Field-of-View with Low-Complexity Co-prime Photonic Beamforming Transceivers  [ :arrow_down: ](https://arxiv.org/pdf/2108.10223.pdf)
>  Integrated photonic active beamforming can significantly reduce the size and cost of coherent imagers for LiDAR and medical imaging applications. In current architectures, the complexity of photonic and electronic circuitry linearly increases with the desired imaging resolution. We propose a novel photonic transceiver architecture based on co-prime sampling techniques that breaks this trade-off and achieves the full (radiating-element-limited) field-of-view (FOV) for a 2D aperture with a single-frequency laser. Using only order-of-N radiating elements, this architecture achieves beamwidth and side-lobe level (SLL) performance equivalent to a transceiver with order-of-N-squared elements with half-wavelength spacing. Furthermore, we incorporate a pulse amplitude modulation (PAM) row-column drive methodology to reduce the number of required electrical drivers for this architecture from order of N to order of square root of N. A silicon photonics implementation of this architecture using two 64-element apertures, one for transmitting and one for receiving, requires only 34 PAM electrical drivers and achieves a transceiver SLL of -11.3dB with 1026 total resolvable spots, and 0.6 degree beamwidth within a 23x16.3 degree FOV.      
### 7.Cooperative Localization Utilizing Reinforcement Learning for 5G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.10222.pdf)
>  The demand for accurate localization has risen in recent years to enable the emerging of autonomous vehicles. To have these vehicles in the traffic ecosystem of smart cities, the need for an accurate positioning system is emphasized. To realize accurate positioning, collaborative localization plays an important role. This type of localization computes range measurements between vehicles and improves the accuracy of position by correcting the possibly faulty values of one of them by using the more accurate values of the other. 5G signals with the technology of Millimeter Wave (mmWave) support precise range measurements and 5G networks provide Device to Device (D2D) communication which improves collaborative localization. The aim of this paper is to provide an accurate collaborative positioning for autonomous vehicles, which is less prone to errors utilizing reinforcement learning technique for selecting the most accurate and suitable range measurement technique for the 5G signal.      
### 8.AI and conventional methods for UCT projection data estimation  [ :arrow_down: ](https://arxiv.org/pdf/2108.10220.pdf)
>  A 2D Compact ultrasound computerized tomography (UCT) system is developed. Fully automatic post processing tools involving signal and image processing are developed as well. Square of the amplitude values are used in transmission mode with natural 1.5 MHz frequency and rise time 10.4 ns and fall time 8.4 ns and duty cycle of 4.32%. Highest peak to corresponding trough values are considered as transmitting wave between transducers in direct line talk. Sensitivity analysis of methods to extract peak to corresponding trough per transducer are discussed in this paper. Total five methods are tested. These methods are taken from broad categories: (a) Conventional and (b) Artificial Intelligence (AI) based methods. Conventional methods, namely: (a) simple gradient based peak detection, (b) Fourier based, (c) wavelet transform are compared with AI based methods: (a) support vector machine (SVM), (b) artificial neural network (ANN). Classification step was performed as well to discard the signal which does not has contribution of transmission wave. It is found that conventional methods have better performance. Reconstruction error, accuracy, F-Score, recall, precision, specificity and MCC for 40 x 40 data 1600 data files are measured. Each data file contains 50,002 data point. Ten such data files are used for training the Neural Network. Each data file has 7/8 wave packets and each packet corresponds to one transmission amplitude data. Reconstruction error is found to be minimum for ANN method. Other performance indices show that FFT method is processing the UCT signal with best recovery.      
### 9.Study of Proximal Normalized Subband Adaptive Algorithm for Acoustic Echo Cancellation  [ :arrow_down: ](https://arxiv.org/pdf/2108.10219.pdf)
>  In this paper, we propose a novel normalized subband adaptive filter algorithm suited for sparse scenarios, which combines the proportionate and sparsity-aware mechanisms. The proposed algorithm is derived based on the proximal forward-backward splitting and the soft-thresholding methods. We analyze the mean and mean square behaviors of the algorithm, which is supported by simulations. In addition, an adaptive approach for the choice of the thresholding parameter in the proximal step is also proposed based on the minimization of the mean square deviation. Simulations in the contexts of system identification and acoustic echo cancellation verify the superiority of the proposed algorithm over its counterparts.      
### 10.SALIENCE: An Unsupervised User Adaptation Model for Multiple Wearable Sensors Based Human Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2108.10213.pdf)
>  Unsupervised user adaptation aligns the feature distributions of the data from training users and the new user, so a well-trained wearable human activity recognition (WHAR) model can be well adapted to the new user. With the development of wearable sensors, multiple wearable sensors based WHAR is gaining more and more attention. In order to address the challenge that the transferabilities of different sensors are different, we propose SALIENCE (unsupervised user adaptation model for multiple wearable sensors based human activity recognition) model. It aligns the data of each sensor separately to achieve local alignment, while uniformly aligning the data of all sensors to ensure global alignment. In addition, an attention mechanism is proposed to focus the activity classifier of SALIENCE on the sensors with strong feature discrimination and well distribution alignment. Experiments are conducted on two public WHAR datasets, and the experimental results show that our model can yield a competitive performance.      
### 11.Ultralow complexity long short-term memory network for fiber nonlinearity mitigation in coherent optical communication systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.10212.pdf)
>  Fiber Kerr nonlinearity is a fundamental limitation to the achievable capacity of long-distance optical fiber communication. Digital back-propagation (DBP) is a primary methodology to mitigate both linear and nonlinear impairments by solving the inverse-propagating nonlinear Schrödinger equation (NLSE), which requires detailed link information. Recently, the paradigms based on neural network (NN) were proposed to mitigate nonlinear transmission impairments in optical communication systems. However, almost all neural network-based equalization schemes yield high computation complexity, which prevents the practical implementation in commercial transmission systems. In this paper, we propose a center-oriented long short-term memory network (Co-LSTM) incorporating a simplified mode with a recycling mechanism in the equalization operation, which can mitigate fiber nonlinearity in coherent optical communication systems with ultralow complexity. To validate the proposed methodology, we carry out an experiment of ten-channel wavelength division multiplexing (WDM) transmission with 64 Gbaud polarization-division-multiplexed 16-ary quadrature amplitude modulation (16-QAM) signals. Co-LSTM and DBP achieve a comparable performance of nonlinear mitigation. However, the complexity of Co-LSTM with a simplified mode is almost independent of the transmission distance, which is much lower than that of the DBP. The proposed Co-LSTM methodology presents an attractive approach for low complexity nonlinearity mitigation with neural networks.      
### 12.Pediatric Automatic Sleep Staging: A comparative study of state-of-the-art deep learning methods  [ :arrow_down: ](https://arxiv.org/pdf/2108.10211.pdf)
>  Despite the tremendous progress recently made towards automatic sleep staging in adults, it is currently known if the most advanced algorithms generalize to the pediatric population, which displays distinctive characteristics in overnight polysomnography (PSG). To answer the question, in this work, we conduct a large-scale comparative study on the state-of-the-art deep learning methods for pediatric automatic sleep staging. A selection of six different deep neural networks with diverging features are adopted to evaluate a sample of more than 1,200 children across a wide spectrum of obstructive sleep apnea (OSA) severity. Our experimental results show that the performance of automated pediatric sleep staging when evaluated on new subjects is equivalent to the expert-level one reported on adults, reaching an overall accuracy of 87.0%, a Cohen's kappa of 0.829, and a macro F1-score of 83.5% in case of single-channel EEG. The performance is further improved when dual-channel EEG$\cdot$EOG are used, reaching an accuracy of 88.2%, a Cohen's kappa of 0.844, and a macro F1-score of 85.1%. The results also show that the studied algorithms are robust to concept drift when the training and test data were recorded 7-months apart. Detailed analyses further demonstrate "almost perfect" agreement between the automatic scorers to one another and their similar behavioral patterns on the staging errors.      
### 13.Anomaly Detection Based on Generalized Gaussian Distribution approach for Ultra-Wideband (UWB) Indoor Positioning System  [ :arrow_down: ](https://arxiv.org/pdf/2108.10210.pdf)
>  With the rapid development of the Internet of Things (IoT), Indoor Positioning System (IPS) has attracted significant interest in academic research. Ultra-Wideband (UWB) is an emerging technology that can be employed for IPS as it offers centimetre-level accuracy. However, the UWB system still faces several technical challenges in practice, one of which is Non-Line-of-Sight (NLoS) signal propagation. Several machine learning approaches have been applied for the NLoS component identification. However, when the data contains a very small amount of NLoS components it becomes very difficult for existing algorithms to classify them. This paper focuses on employing an anomaly detection approach based on Gaussian Distribution (GD) and Generalized Gaussian Distribution (GGD) algorithms to detect and identify the NLoS components. The simulation results indicate that the proposed approach can provide a robust NLoS component identification which improves the NLoS signal classification accuracy which results in significant improvement in the UWB positioning system.      
### 14.Noise2Fast: Fast Self-Supervised Single Image Blind Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2108.10209.pdf)
>  In the last several years deep learning based approaches have come to dominate many areas of computer vision, and image denoising is no exception. Neural networks can learn by example to map noisy images to clean images. However, access to noisy/clean or even noisy/noisy image pairs isn't always readily available in the desired domain. Recent approaches have allowed for the denoising of single noisy images without access to any training data aside from that very image. But since they require both training and inference to be carried out on each individual input image, these methods require significant computation time. As such, they are difficult to integrate into automated microscopy pipelines where denoising large datasets is essential but needs to be carried out in a timely manner. Here we present Noise2Fast, a fast single image blind denoiser. Our method is tailored for speed by training on a four-image dataset produced using a unique form of downsampling we refer to as "checkerboard downsampling". Noise2Fast is faster than all tested approaches and is more accurate than all except Self2Self, which takes well over 100 times longer to denoise an image. This allows for a combination of speed and flexibility that was not previously attainable using any other method.      
### 15.A Simple Probabilistic Model With Extended Kalman Filter To Predict Multi-leak In Pipelines  [ :arrow_down: ](https://arxiv.org/pdf/2108.10206.pdf)
>  Pipelines for water supply are susceptible to burst-leakage due to fluid pressures of various nature. High pressure heads resulting in circumferential and (or) axial stresses larger than the material yield stress could cause pipe failure. Of equal concern is the local boiling or cavitation effect in regions of fluid pressure dropping below its vapor pressure, which in turn develop air bubbles that get transported through the pipeline, bursting later at remote locations. We initially developed a simple probabilistic model based on Method of Characteristics (MOC) to simulate burst leakage in pipelines, and compared with a pure deterministic hydraulic model. We had not considered cavitation effects for simplicity. The results indicated that the simple probabilistic model was only marginally different in its prediction of the transients on comparison with the latter. In order to determine the position and amount of leakage in the distribution system, the detection method based on simulating hydraulic transients was further evaluated using Extended Kalman Filter (EKF). We found that this non-linear filtering approach on the fluid transient model considerably reduced the number of input parameters required, and it was able to predict leakage rate and burst positions even in a highly noisy environment.      
### 16.Power transformer faults diagnosis using undestructive methods (Roger and IEC) and artificial neural network for dissolved gas analysis applied on the functional transformer in the Algerian north-eastern: a comparative study  [ :arrow_down: ](https://arxiv.org/pdf/2108.10205.pdf)
>  Nowadays, power transformer aging and failures are viewed with great attention in power transmission industry. Dissolved gas analysis (DGA) is classified among the biggest widely used methods used within the context of asset management policy to detect the incipient faults in their earlier stage in power transformers. Up to now, several procedures have been employed for the lecture of DGA results. Among these useful means, we find Key Gases, Rogers Ratios, IEC Ratios, the historical technique less used today Doernenburg Ratios, the two types of Duval Pentagons methods, several versions of the Duval Triangles method and Logarithmic Nomograph. Problem. DGA data extracted from different units in service served to verify the ability and reliability of these methods in assessing the state of health of the power transformer. Aim. An improving the quality of diagnostics of electrical power transformer by artificial neural network tools based on two conventional methods in the case of a functional power transformer at Sétif province in East North of Algeria. Methodology. Design an inelegant tool for power transformer diagnosis using neural networks based on traditional methods IEC and Rogers, which allows to early detection faults, to increase the reliability, of the entire electrical energy system from transport to consumers and improve a continuity and quality of service. Results. The solution of the problem was carried out by using feed-forward back-propagation neural networks implemented in MATLAB-Simulink environment. Four real power transformers working under different environment and climate conditions such as: desert, humid, cold were taken into account. The practical results of the diagnosis of these power transformers by the DGA are presented. Practical value.....      
### 17.Two-Way Reflecting Communication with UAV-Borne Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2108.10180.pdf)
>  This paper investigates the unmanned aerial vehicle (UAV)-aided two-way reflecting (TWR) communication system under the probabilistic line-of-sight (LoS) channel model, where a UAV equipped with an RIS is deployed to assist two ground nodes in their information exchange. An optimization problem with the objective of maximizing the expected achievable rate is formulated to design the communication scheduling, the RIS's phase, and the UAV trajectory. To solve such a non-convex problem, we propose an efficient iterative algorithm to obtain suboptimal its solution. Simulation results show that our proposed design provides new insights into the elevation angle-distance trade-off for the TWR communication system, and improves the rate by $28\%$ compared to the scheme under the conventional LoS channel model.      
### 18.Inverse Aerodynamic Design of Gas Turbine Blades using Probabilistic Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.10163.pdf)
>  One of the critical components in Industrial Gas Turbines (IGT) is the turbine blade. Design of turbine blades needs to consider multiple aspects like aerodynamic efficiency, durability, safety and manufacturing, which make the design process sequential and iterative.The sequential nature of these iterations forces a long design cycle time, ranging from several months to years. Due to the reactionary nature of these iterations, little effort has been made to accumulate data in a manner that allows for deep exploration and understanding of the total design space. This is exemplified in the process of designing the individual components of the IGT resulting in a potential unrealized efficiency. To overcome the aforementioned challenges, we demonstrate a probabilistic inverse design machine learning framework (PMI), to carry out an explicit inverse design. PMI calculates the design explicitly without excessive costly iteration and overcomes the challenges associated with ill-posed inverse problems. In this work, the framework will be demonstrated on inverse aerodynamic design of three-dimensional turbine blades.      
### 19.Emotion Recognition from Multiple Modalities: Fundamentals and Methodologies  [ :arrow_down: ](https://arxiv.org/pdf/2108.10152.pdf)
>  Humans are emotional creatures. Multiple modalities are often involved when we express emotions, whether we do so explicitly (e.g., facial expression, speech) or implicitly (e.g., text, image). Enabling machines to have emotional intelligence, i.e., recognizing, interpreting, processing, and simulating emotions, is becoming increasingly important. In this tutorial, we discuss several key aspects of multi-modal emotion recognition (MER). We begin with a brief introduction on widely used emotion representation models and affective modalities. We then summarize existing emotion annotation strategies and corresponding computational tasks, followed by the description of main challenges in MER. Furthermore, we present some representative approaches on representation learning of each affective modality, feature fusion of different affective modalities, classifier optimization for MER, and domain adaptation for MER. Finally, we outline several real-world applications and discuss some future directions.      
### 20.Finding essential parts of the brain in rs-fMRI can improve diagnosing ADHD by Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.10137.pdf)
>  Attention Deficit\Hyperactivity Disorder(ADHD) is considered a very common psychiatric disorder, but it is difficult to establish an accurate diagnostic method for ADHD. Recently, with the development of computing resources and machine learning methods, studies have been conducted to classify ADHD using resting-state functional magnetic resonance(rsfMRI) imaging data. However, most of them utilized all areas of the brain for training the models. In this study, as a different way from this approach, we conducted a study to classify ADHD by selecting areas that are essential for using a deep learning model. For the experiment, rsfMRI data provided by ADHD 200 global competition was used. To obtain an integrated result from the multiple sites, each region of the brain was evaluated with Leave one site out cross-validation. As a result, when we only used 15 important region of interest(ROIs) for training, an accuracy of 70.6% was obtained, significantly exceeding the existing results of 68.6% from all ROIs. In addition, to explore the new structure based on SCCNN-RNN, we performed the same experiment with three modified models: (1) Separate Channel CNN RNN with Attention (ASCRNN), (2) Separate Channel dilate CNN RNN with Attention (ASDRNN), (3) Separate Channel CNN slicing RNN with Attention (ASSRNN). As a result, the ASSRNN model provided a high accuracy of 70.46% when training with only 13 important region of interest (ROI). These results show that finding and using the crucial parts of the brain in diagnosing ADHD by Deep Learning can get better results than using all areas.      
### 21.Circuit design and integration feasibility of a high-resolution broadband on-chip spectral monitor  [ :arrow_down: ](https://arxiv.org/pdf/2108.10121.pdf)
>  Up-to-date network telemetry is the key enabler for resource optimization by a variety of means including capacity scaling, fault recovery, network reconfiguration. Reliable optical performance monitoring in general and specifically the monitoring of the spectral profile of WDM signals in fixed- and flex-grid architecture across the entire C-band remains challenging. This article describes a spectrometer circuit architecture along with an original data processing algorithm that combined can measure the spectrum quantitatively across the entire C-band aiming at 1 GHz resolution bandwidth. The circuit is composed of a scanning ring resonator followed by a parallel arrangement of AWGs with interlaced channel spectra. The comb of ring resonances provides the high resolution and the algorithm creates a virtual tuneable AWG that isolates individual resonances of the comb within the flat pass-band of its synthesized channels. The parallel arrangement of AWGs may be replaced by a time multiplexed multi-input port AWG. The feasibility of a ring resonator functioning over whole C-band is experimentally validated. Full tuning of the comb of resonances over a free spectral range is achieved with a high-resolution bandwidth of 1.30 GHz. Due to its maturity and low loss, CMOS compatible silicon nitride is chosen for integration. Additionally, the whole system demonstration is presented using industry standard simulation tool. The architecture is robust to fabrication process variations owing to its data processing approach.      
### 22.Performance Analysis of Mixed RF/FSO Relaying under HPA Nonlinearity and IQ Imbalance  [ :arrow_down: ](https://arxiv.org/pdf/2108.10119.pdf)
>  In this paper, we present the performance analysis of asymmetric dual-hop RF/FSO system with multiple relays. The RF channels follow the correlated Rayleigh fading while the optical links are subject to the Gamma-Gamma fading. To select the candidate relay to forward the communication, we assume Partial Relay Selection (PRS) with outdated Channel State Information (CSI). Unlike the vast majority of work in this area, we introduce the impairments to the relays and the destination. We will propose three impairment models called Soft Envelope Limiter (SEL), Traveling Wave Tube Amplifier (TWTA) and IQ Imbalance in order to compare the resilience of our system with the RF one against the hardware impairments. Closed-from of the outage probability (OP) is derived in terms of Meijer's G function as well as the upper bound of the ergodic capacity (EC). The Bit Error Rate (BER) and the exact EC are evaluated numerically. Finally, analytical and numerical results are presented and validated by Monte Carlo simulation.      
### 23.Development of A Fully Data-Driven Artificial Intelligence and Deep Learning for URLLC Application in 6G Wireless Systems: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2108.10076.pdf)
>  The full future of the sixth generation will develop a fully data-driven that provide terabit rate per second, and adopt an average of 1000+ massive number of connections per person in 10 years 2030 virtually instantaneously. Data-driven for ultra-reliable and low latency communication is a new service paradigm provided by a new application of future sixth-generation wireless communication and network architecture, involving 100+ Gbps data rates with one millisecond latency. The key constraint is the amount of computing power available to spread massive data and well-designed artificial neural networks. Artificial Intelligence provides a new technique to design wireless networks by apply learning, predicting, and make decisions to manage the stream of big data training individuals, which provides more the capacity to transform that expert learning to develop the performance of wireless networks. We study the developing technologies that will be the driving force are artificial intelligence, communication systems to guarantee low latency. This paper aims to discuss the efficiency of the developing network and alleviate the great challenge for application scenarios and study Holographic radio, enhanced wireless channel coding, enormous Internet of Things integration, and haptic communication for virtual and augmented reality provide new services on the 6G network. Furthermore, improving a multi-level architecture for ultra-reliable and low latency in deep Learning allows for data-driven AI and 6G networks for device intelligence, as well as allowing innovations based on effective learning capabilities. These difficulties must be solved in order to meet the needs of future smart networks. Furthermore, this research categorizes various unexplored research gaps between machine learning and the sixth generation.      
### 24.Automatic Modulation Classification Using Involution Enabled Residual Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.10001.pdf)
>  Automatic modulation classification (AMC) is of crucial importance for realizing wireless intelligence communications. Many deep learning based models especially convolution neural networks (CNNs) have been proposed for AMC. However, the computation cost is very high, which makes them inappropriate for beyond the fifth generation wireless communication networks that have stringent requirements on the classification accuracy and computing time. In order to tackle those challenges, a novel involution enabled AMC scheme is proposed by using the bottleneck structure of the residual networks. Involution is utilized instead of convolution to enhance the discrimination capability and expressiveness of the model by incorporating a self-attention mechanism. Simulation results demonstrate that our proposed scheme achieves superior classification performance and faster convergence speed comparing with other benchmark schemes.      
### 25.Learned Image Coding for Machines: A Content-Adaptive Approach  [ :arrow_down: ](https://arxiv.org/pdf/2108.09992.pdf)
>  Today, according to the Cisco Annual Internet Report (2018-2023), the fastest-growing category of Internet traffic is machine-to-machine communication. In particular, machine-to-machine communication of images and videos represents a new challenge and opens up new perspectives in the context of data compression. One possible solution approach consists of adapting current human-targeted image and video coding standards to the use case of machine consumption. Another approach consists of developing completely new compression paradigms and architectures for machine-to-machine communications. In this paper, we focus on image compression and present an inference-time content-adaptive finetuning scheme that optimizes the latent representation of an end-to-end learned image codec, aimed at improving the compression efficiency for machine-consumption. The conducted experiments show that our online finetuning brings an average bitrate saving (BD-rate) of -3.66% with respect to our pretrained image codec. In particular, at low bitrate points, our proposed method results in a significant bitrate saving of -9.85%. Overall, our pretrained-and-then-finetuned system achieves -30.54% BD-rate over the state-of-the-art image/video codec Versatile Video Coding (VVC).      
### 26.Efficient Medical Image Segmentation Based on Knowledge Distillation  [ :arrow_down: ](https://arxiv.org/pdf/2108.09987.pdf)
>  Recent advances have been made in applying convolutional neural networks to achieve more precise prediction results for medical image segmentation problems. However, the success of existing methods has highly relied on huge computational complexity and massive storage, which is impractical in the real-world scenario. To deal with this problem, we propose an efficient architecture by distilling knowledge from well-trained medical image segmentation networks to train another lightweight network. This architecture empowers the lightweight network to get a significant improvement on segmentation capability while retaining its runtime efficiency. We further devise a novel distillation module tailored for medical image segmentation to transfer semantic region information from teacher to student network. It forces the student network to mimic the extent of difference of representations calculated from different tissue regions. This module avoids the ambiguous boundary problem encountered when dealing with medical imaging but instead encodes the internal information of each semantic region for transferring. Benefited from our module, the lightweight network could receive an improvement of up to 32.6% in our experiment while maintaining its portability in the inference phase. The entire structure has been verified on two widely accepted public CT datasets LiTS17 and KiTS19. We demonstrate that a lightweight network distilled by our method has non-negligible value in the scenario which requires relatively high operating speed and low storage usage.      
### 27.Non-Minimal Systems with Switching Topology: Dynamics and Controls  [ :arrow_down: ](https://arxiv.org/pdf/2108.09903.pdf)
>  This paper presents a non-minimal order dynamics model for many analysis, simulation, and control problems of constrained mechanical systems with switching topology by making use of linear projection operator. The distinct features of this model describing dynamics of the dependent coordinates are: i) The mass matrix $\bar{M}(q)$ is always positive definite even at singular configurations; ii) matrix $\dot{\bar M} - 2 \bar{C}$ is skew symmetric, where all nonlinear terms are lumped into vector $\bar{C}(q, \dot q) \dot q$ after elimination of constraint forces. Eigenvalue analysis shows that the condition number of the constraint mass matrix can be minimized upon adequate selection of a scalar parameter called ``virtual mass'' thereby reducing the sensitivity to round-off errors in numerical computation. It follows by derivation of two oblique projection matrices for computation of constraint forces and actuation forces. It is shown that projection-based model allows feedback control of dependent coordinates which, unlike reduced-order dependent coordinates, uniquely define spatial configuration of constrained systems.      
### 28.Composite Adaptive Control for Anti-Unwinding Attitude Maneuvers: An Exponential Stability Result Without Persistent Excitation  [ :arrow_down: ](https://arxiv.org/pdf/2108.09901.pdf)
>  This paper provides an exponential stability result for the adaptive anti-unwinding attitude tracking control problem of a rigid body with uncertain but constant inertia parameters, without requiring the satisfaction of persistent excitation (PE) condition. Specifically, a composite immersion and invariance (I&amp;I) adaptive controller is derived by integrating a prediction-error-driven learning law into the dynamically scaled I&amp;I adaptive control framework, wherein we modify the scaling factor so that the algorithm design does not involve any dynamic gains. To avoid the unwinding problem, a barrier function is introduced as the attitude error function, along with the tactful establishment of two crucial algebra properties for exponential stability analysis. The regressor filtering method is adopted in combination with the dynamic regressor extension and mixing (DREM) procedure to acquire the prediction error using only easily obtainable signals. In particular, aiding by a constructive liner time-varying filter, the scalar regressor of DREM is extended to generate a new exciting counterpart. In this way, the derived controller is shown to permit closed-loop exponential stability without PE, in the sense that both output-tracking and parameter estimation errors exponentially converge to zero. Further, the composite learning law is augmented with a power term to achieve synchronized finite/fixed-time parameter convergence. Numerical simulations are performed to verify the theoretical findings.      
### 29.An Adversarial Learning Based Approach for Unknown View Tomographic Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2108.09873.pdf)
>  The goal of 2D tomographic reconstruction is to recover an image given its projection lines from various views. It is often presumed that projection angles associated with the projection lines are known in advance. Under certain situations, however, these angles are known only approximately or are completely unknown. It becomes more challenging to reconstruct the image from a collection of random projection lines. We propose an adversarial learning based approach to recover the image and the projection angle distribution by matching the empirical distribution of the measurements with the generated data. Fitting the distributions is achieved through solving a min-max game between a generator and a critic based on Wasserstein generative adversarial network structure. To accommodate the update of the projection angle distribution through gradient back propagation, we approximate the loss using the Gumbel-Softmax reparameterization of samples from discrete distributions. Our theoretical analysis verifies the unique recovery of the image and the projection distribution up to a rotation and reflection upon convergence. Our extensive numerical experiments showcase the potential of our method to accurately recover the image and the projection angle distribution under noise contamination.      
### 30.Sparse regularization with a non-convex penalty for SAR imaging and autofocusing  [ :arrow_down: ](https://arxiv.org/pdf/2108.09855.pdf)
>  In this paper, SAR image reconstruction with joint phase error estimation (autofocusing) is formulated as an inverse problem. An optimization model utilising a sparsity-enforcing Cauchy regularizer is proposed, and an alternating minimization framework is used to solve it, in which the desired image and the phase errors are optimized alternatively. For the image reconstruction sub-problem (f-sub-problem), two methods are presented capable of handling the problem's complex nature, and we thus present two variants of our SAR image autofocusing algorithm. Firstly, we design a complex version of the forward-backward splitting algorithm (CFBA) to solve the f-sub-problem iteratively. For the second variant, the Wirtinger alternating minimization autofocusing (WAMA) method is presented, in which techniques of Wirtinger calculus are utilized to minimize the complex-valued cost function in the f-sub-problem in a direct fashion. For both methods, the phase error estimation sub-problem is solved by simply expanding and observing its cost function. Moreover, the convergence of both algorithms is discussed in detail. By conducting experiments on both simulated scenes and real SAR images, the proposed method is demonstrated to give impressive autofocusing results compared to other state of the art methods.      
### 31.Electroencephalogram Signal Processing with Independent Component Analysis and Cognitive Stress Classification using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.09817.pdf)
>  Electroencephalogram (EEG) is the recording which is the result due to the activity of bio-electrical signals that is acquired from electrodes placed on the scalp. In Electroencephalogram signal(EEG) recordings, the signals obtained are contaminated predominantly by the Electrooculogram(EOG) signal. Since this artifact has higher magnitude compared to EEG signals, these noise signals have to be removed in order to have a better understanding regarding the functioning of a human brain for applications such as medical diagnosis. This paper proposes an idea of using Independent Component Analysis(ICA) along with cross-correlation to de-noise EEG signal. This is done by selecting the component based on the cross-correlation coefficient with a threshold value and reducing its effect instead of zeroing it out completely, thus reducing the information loss. The results of the recorded data show that this algorithm can eliminate the EOG signal artifact with little loss in EEG data. The denoising is verified by an increase in SNR value and the decrease in cross-correlation coefficient value. The denoised signals are used to train an Artificial Neural Network(ANN) which would examine the features of the input EEG signal and predict the stress levels of the individual.      
### 32.A Transformer Architecture for Stress Detection from ECG  [ :arrow_down: ](https://arxiv.org/pdf/2108.09737.pdf)
>  Electrocardiogram (ECG) has been widely used for emotion recognition. This paper presents a deep neural network based on convolutional layers and a transformer mechanism to detect stress using ECG signals. We perform leave-one-subject-out experiments on two publicly available datasets, WESAD and SWELL-KW, to evaluate our method. Our experiments show that the proposed model achieves strong results, comparable or better than the state-of-the-art models for ECG-based stress detection on these two datasets. Moreover, our method is end-to-end, does not require handcrafted features, and can learn robust representations with only a few convolutional blocks and the transformer component.      
### 33.QoS-Oriented Sensing-Communication-Control Co-Design for UAV-Enabled Positioning  [ :arrow_down: ](https://arxiv.org/pdf/2108.09725.pdf)
>  Unmanned aerial vehicle (UAV)-enabled positioning that uses UAVs as aerial anchor nodes is a promising solution for providing positioning services in harsh environments. In previous research, the state sensing and control of UAVs were either ignored or simply set to be performed continuously, resulting in system instability or waste of wireless resources. Therefore, in this article, we propose a quality-of-service (QoS)-oriented UAV-enabled positioning system based on the concept of sensing-communication-control (SCC) co-design. We first establish the mathematical models of UAV state sensing and control. Then, the influence of sensing scheduling and transmission failure on UAV stability, as well as the performance of positioning services in the presence of UAV control error, are analyzed. Based on these models and analysis results, we further study the problem of minimizing the amount of data transmitted by optimizing the sensing scheduling and blocklength allocation under the condition of satisfying each user's demand. Finally, an efficient scheme is developed to solve this mixed-integer nonlinear problem. Numerical results show that the proposed system could work efficiently and meet users' requirements. In addition, compared with two benchmark schemes, our scheme reduces the failure rate or resource consumption of positioning services by more than 76.2% or 82.7%.      
### 34.FEDI: Few-shot learning based on Earth Mover's Distance algorithm combined with deep residual network to identify diabetic retinopathy  [ :arrow_down: ](https://arxiv.org/pdf/2108.09711.pdf)
>  Diabetic retinopathy(DR) is the main cause of blindness in diabetic patients. However, DR can easily delay the occurrence of blindness through the diagnosis of the fundus. In view of the reality, it is difficult to collect a large amount of diabetic retina data in clinical practice. This paper proposes a few-shot learning model of a deep residual network based on Earth Mover's Distance algorithm to assist in diagnosing DR. We build training and validation classification tasks for few-shot learning based on 39 categories of 1000 sample data, train deep residual networks, and obtain experience maximization pre-training models. Based on the weights of the pre-trained model, the Earth Mover's Distance algorithm calculates the distance between the images, obtains the similarity between the images, and changes the model's parameters to improve the accuracy of the training model. Finally, the experimental construction of the small sample classification task of the test set to optimize the model further, and finally, an accuracy of 93.5667% on the 3way10shot task of the diabetic retina test set. For the experimental code and results, please refer to: <a class="link-external link-https" href="https://github.com/panliangrui/few-shot-learning-funds" rel="external noopener nofollow">this https URL</a>.      
### 35.Pairwise Node Localization From Differences in Their UWB Channels to Observer Nodes  [ :arrow_down: ](https://arxiv.org/pdf/2108.09703.pdf)
>  We consider the problem of localization and distance estimation between a pair of wireless nodes in a multipath propagation environment, but not the usual way of processing a channel measurement between them. We propose a novel paradigm which compares the two nodes' ultra-wideband (UWB) channels to other nodes, called observers. The key principle is that the channel impulse responses (CIRs) are similar at small inter-node distance $d$ and differ increasingly with increasing $d$. We present distance estimators which utilize the rich location information contained in the delay differences of extracted multipath components (MPCs). Likewise, we present estimators for the relative position vector which process both MPC delays and MPC directions. We do so for various important cases: with and without clock synchronization, delay measurement errors, and knowledge of the MPC association between the two CIRs. The estimators exhibit great technological advantages: they do not require precise time-synchronization, line-of-sight conditions, or knowledge about the observer locations or the environment. We study the estimation accuracy with a numerical evaluation based on random sampling and, additionally, with an experimental evaluation based on measurements in an indoor environment. The proposal shows the potential for great accuracy in theory and practice. Integrating the paradigm into existing localization algorithms and systems could enable low-cost localization of wireless users or networks in dynamic multipath settings.      
### 36.Using Large Pre-Trained Models with Cross-Modal Attention for Multi-Modal Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2108.09669.pdf)
>  Recently, self-supervised pre-training has shown significant improvements in many areas of machine learning, including speech and NLP. We propose using large self-supervised pre-trained models for both audio and text modality with cross-modality attention for multimodal emotion recognition. We use Wav2Vec2.0 [1] as an audio encoder base for robust speech features extraction and the BERT model [2] as a text encoder base for better contextual representation of text. These high capacity models trained on large amounts of unlabeled data contain rich feature representations and improve the downstream task's performance. We use the cross-modal attention [3] mechanism to learn alignment between audio and text representations from self-supervised models. Cross-modal attention also helps in extracting interactive information between audio and text features. We obtain utterance-level feature representation from frame-level features using statistics pooling for both audio and text modality and combine them using the early fusion technique. Our experiments show that the proposed approach obtains a 1.88% absolute improvement in accuracy compared to the previous state-of-the-art method [3] on the IEMOCAP dataset [35]. We also conduct unimodal experiments for both audio and text modalities and compare them with previous best methods.      
### 37.Deep survival analysis with longitudinal X-rays for COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2108.09641.pdf)
>  Time-to-event analysis is an important statistical tool for allocating clinical resources such as ICU beds. However, classical techniques like the Cox model cannot directly incorporate images due to their high dimensionality. We propose a deep learning approach that naturally incorporates multiple, time-dependent imaging studies as well as non-imaging data into time-to-event analysis. Our techniques are benchmarked on a clinical dataset of 1,894 COVID-19 patients, and show that image sequences significantly improve predictions. For example, classical time-to-event methods produce a concordance error of around 30-40% for predicting hospital admission, while our error is 25% without images and 20% with multiple X-rays included. Ablation studies suggest that our models are not learning spurious features such as scanner artifacts. While our focus and evaluation is on COVID-19, the methods we develop are broadly applicable.      
### 38.Energy Efficient Power Allocation in Massive MIMO Systems with Mismatch Channel Estimation Error  [ :arrow_down: ](https://arxiv.org/pdf/2108.09626.pdf)
>  In this paper, we investigate the energy efficient power allocation for the downlink in the massive multiple-input multiple-output (MIMO) systems under zero-forcing (ZF) receiver. The radio frequencies (RF) that have a significant effect on system performance has not been considered in most of previous researches. Increasing of random changes in RF creates a mismatch channel. We must consider the effects of the RF circuit for the performance evaluation of the massive MIMO systems. We also consider the rate of system in the presence of estimation error, similar to real world, with the quality of service (QoS) constraint and the transfer capacity of users. In the scenario of this paper, users are divided into two groups. The first group are users who have stronger channel conditions or in other words are located in the center of the cell, and the second group belongs to users who have weak channel conditions and are located at the edge of the cell. By using Karush-Kuhn-Tucker (KKT) conditions, we obtain the optimal power of users. The results of implementation and simulations are given to confirm the efficiency of the proposed algorithm.      
### 39.Multimodal Breast Lesion Classification Using Cross-Attention Deep Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.09591.pdf)
>  Accurate breast lesion risk estimation can significantly reduce unnecessary biopsies and help doctors decide optimal treatment plans. Most existing computer-aided systems rely solely on mammogram features to classify breast lesions. While this approach is convenient, it does not fully exploit useful information in clinical reports to achieve the optimal performance. Would clinical features significantly improve breast lesion classification compared to using mammograms alone? How to handle missing clinical information caused by variation in medical practice? What is the best way to combine mammograms and clinical features? There is a compelling need for a systematic study to address these fundamental questions. This paper investigates several multimodal deep networks based on feature concatenation, cross-attention, and co-attention to combine mammograms and categorical clinical variables. We show that the proposed architectures significantly increase the lesion classification performance (average area under ROC curves from 0.89 to 0.94). We also evaluate the model when clinical variables are missing.      
### 40.Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform  [ :arrow_down: ](https://arxiv.org/pdf/2108.09551.pdf)
>  We propose a versatile deep image compression network based on Spatial Feature Transform (SFT <a class="link-https" data-arxiv-id="1804.02815" href="https://arxiv.org/abs/1804.02815">arXiv:1804.02815</a>), which takes a source image and a corresponding quality map as inputs and produce a compressed image with variable rates. Our model covers a wide range of compression rates using a single model, which is controlled by arbitrary pixel-wise quality maps. In addition, the proposed framework allows us to perform task-aware image compressions for various tasks, e.g., classification, by efficiently estimating optimized quality maps specific to target tasks for our encoding network. This is even possible with a pretrained network without learning separate models for individual tasks. Our algorithm achieves outstanding rate-distortion trade-off compared to the approaches based on multiple models that are optimized separately for several different target rates. At the same level of compression, the proposed approach successfully improves performance on image classification and text region quality preservation via task-aware quality map estimation without additional model training. The code is available at the project website: <a class="link-external link-https" href="https://github.com/micmic123/QmapCompression" rel="external noopener nofollow">this https URL</a>      
### 41.Systematic Clinical Evaluation of A Deep Learning Method for Medical Image Segmentation: Radiosurgery Application  [ :arrow_down: ](https://arxiv.org/pdf/2108.09535.pdf)
>  We systematically evaluate a Deep Learning (DL) method in a 3D medical image segmentation task. Our segmentation method is integrated into the radiosurgery treatment process and directly impacts the clinical workflow. With our method, we address the relative drawbacks of manual segmentation: high inter-rater contouring variability and high time consumption of the contouring process. The main extension over the existing evaluations is the careful and detailed analysis that could be further generalized on other medical image segmentation tasks. Firstly, we analyze the changes in the inter-rater detection agreement. We show that the segmentation model reduces the ratio of detection disagreements from 0.162 to 0.085 (p &lt; 0.05). Secondly, we show that the model improves the inter-rater contouring agreement from 0.845 to 0.871 surface Dice Score (p &lt; 0.05). Thirdly, we show that the model accelerates the delineation process in between 1.6 and 2.0 times (p &lt; 0.05). Finally, we design the setup of the clinical experiment to either exclude or estimate the evaluation biases, thus preserve the significance of the results. Besides the clinical evaluation, we also summarize the intuitions and practical ideas for building an efficient DL-based model for 3D medical image segmentation.      
### 42.Development of an R-Mode Simulator Using MF DGNSS Signals  [ :arrow_down: ](https://arxiv.org/pdf/2108.09496.pdf)
>  With the development of positioning, navigation, and timing (PNT) information-based industries, PNT information is becoming increasingly important. Therefore, various navigation studies have been actively conducted to back up global positioning system (GPS) in scenarios in which it is disabled. Ranging using signals of opportunity (SoOP) has the advantage of infrastructure already being in place. Among them, the ranging mode (R-Mode) is a technology that uses available SoOPs such as a medium frequency (MF) differential global navigation satellite System (DGNSS) signal that has recently been recognized for its potential for navigation and is currently under research. In this study, we developed a signal simulator that considers the characteristics of MF DGNSS signals and skywaves used in R-Mode.      
### 43.GPS Multipath Detection Based on Carrier-to-Noise-Density Ratio Measurements from a Dual-Polarized Antenna  [ :arrow_down: ](https://arxiv.org/pdf/2108.09493.pdf)
>  In this study, the global positioning system (GPS) multipath detection was performed based on the carrier-to-noise-density ratio, C/N0, measured through a dual-polarized antenna. As the right hand circular polarization (RHCP) antenna is sensitive to the signals directly received from the GPS, and the left hand circular polarization (LHCP) antenna is sensitive to the singly reflected signals, the C/N0 difference between the RHCP and LHCP measurements is used for multipath detection. Once we collected the GPS signals in a low multipath location, we calculated the C/N0 difference to obtain a threshold value that can be used to detect the multipath GPS signal received from another location. The results were validated through a ray-tracing simulation.      
### 44.L3C-Stereo: Lossless Compression for Stereo Images  [ :arrow_down: ](https://arxiv.org/pdf/2108.09422.pdf)
>  A large number of autonomous driving tasks need high-definition stereo images, which requires a large amount of storage space. Efficiently executing lossless compression has become a practical problem. Commonly, it is hard to make accurate probability estimates for each pixel. To tackle this, we propose L3C-Stereo, a multi-scale lossless compression model consisting of two main modules: the warping module and the probability estimation module. The warping module takes advantage of two view feature maps from the same domain to generate a disparity map, which is used to reconstruct the right view so as to improve the confidence of the probability estimate of the right view. The probability estimation module provides pixel-wise logistic mixture distributions for adaptive arithmetic coding. In the experiments, our method outperforms the hand-crafted compression methods and the learning-based method on all three datasets used. Then, we show that a better maximum disparity can lead to a better compression effect. Furthermore, thanks to a compression property of our model, it naturally generates a disparity map of an acceptable quality for the subsequent stereo tasks.      
### 45.Observability is Sufficient for the Design of Globally Exponentially Convergent State Observers for State-affine Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.09406.pdf)
>  In this paper we are interested in the problem of state observation of state-affine nonlinear systems. Our main contribution is to propose a globally exponentially convergent observer that requires only the necessary assumption of observability of the system. To the best of the authors' knowledge this is the first time such a result is reported in the literature.      
### 46.Observer Design for Nonlinear Systems with Equivariance  [ :arrow_down: ](https://arxiv.org/pdf/2108.09387.pdf)
>  Equivariance is a common and natural property of many nonlinear control systems, especially those associated with models of mechatronic and navigation systems. Such systems admit a symmetry, associated with the equivariance, that provides structure enabling the design of robust and high performance observers. A key insight is to pose the observer state to lie in the symmetry group rather than on the system state space. This allows one to define a globally defined intrinsic equivariant error but poses a challenge in defining internal dynamics for the observer. By choosing an equivariant lift of the system dynamics for the observer internal model we show that the error dynamics have a particularly nice form. Applying the methodology of Extended Kalman Filtering (EKF) to the equivariant error state yields the Equivariant Filter (EqF). The geometry of the state-space manifold appears naturally as a curvature modification to the classical EKF Riccati equation. The equivariant filter exploits the symmetry and respects the geometry of an equivariant system model and yields high performance robust filters for a wide range of mechatronic and navigation systems.      
### 47.Fourier Neural Operator Networks: A Fast and General Solver for the Photoacoustic Wave Equation  [ :arrow_down: ](https://arxiv.org/pdf/2108.09374.pdf)
>  Simulation tools for photoacoustic wave propagation have played a key role in advancing photoacoustic imaging by providing quantitative and qualitative insights into parameters affecting image quality. Classical methods for numerically solving the photoacoustic wave equation relies on a fine discretization of space and can become computationally expensive for large computational grids. In this work, we apply Fourier Neural Operator (FNO) networks as a fast data-driven deep learning method for solving the 2D photoacoustic wave equation in a homogeneous medium. Comparisons between the FNO network and pseudo-spectral time domain approach demonstrated that the FNO network generated comparable simulations with small errors and was several orders of magnitude faster. Moreover, the FNO network was generalizable and can generate simulations not observed in the training data.      
### 48.Operating Dynamic Reserve Dimensioning Using Probabilistic Forecasts  [ :arrow_down: ](https://arxiv.org/pdf/2108.09362.pdf)
>  The rapid integration of variable energy sources (VRES) into power grids increases variability and uncertainty of the net demand, making the power system operation challenging. Operating reserve is used by system operators to manage and hedge against such variability and uncertainty. Traditionally, reserve requirements are determined by rules-of-thumb (static reserve requirements, e.g., NERC Reliability Standards), and more recently, dynamic reserve requirements from tools and methods which are in the adoption process (e.g., DynADOR, DRD, and RESERVE, among others). While these methods/tools significantly improve the static rule-of-thumb approaches, they rely exclusively on deterministic data (i.e., best guess only). Consequently, these methods disregard the probabilistic uncertainty thresholds associated with specific days and their weather conditions (i.e., best guess plus probabilistic uncertainty). <br>This work presents practical approaches to determine the operating reserve requirements leveraging the wealth information from probabilistic forecasts. Proposed approaches are validated and tested using actual data from the CAISO system. Results show the benefits in terms of risk reduction of considering the probabilistic forecast information into the dimensioning process of operating reserve requirements.      
### 49.Temporally Nonstationary Component Analysis; Application to Noninvasive Fetal Electrocardiogram Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2108.09353.pdf)
>  Objective: Mixtures of temporally nonstationary signals are very common in biomedical applications. The nonstationarity of the source signals can be used as a discriminative property for signal separation. Herein, a semi-blind source separation algorithm is proposed for the extraction of temporally nonstationary components from linear multichannel mixtures of signals and noises. Methods: A hypothesis test is proposed for the detection and fusion of temporally nonstationary events, by using ad hoc indexes for monitoring the first and second order statistics of the innovation process. As proof of concept, the general framework is customized and tested over noninvasive fetal cardiac recordings acquired from the maternal abdomen, over publicly available datasets, using two types of nonstationarity detectors: 1) a local power variations detector, and 2) a model-deviations detector using the innovation process properties of an extended Kalman filter. Results: The performance of the proposed method is assessed in presence of white and colored noise, in different signal-to-noise ratios. Conclusion and Significance: The proposed scheme is general and it can be used for the extraction of nonstationary events and sample deviations from a presumed model in multivariate data, which is a recurrent problem in many machine learning applications.      
### 50.General Theory of Music by Icosahedron 2: Analysis of musical pieces by the exceptional musical icosahedra  [ :arrow_down: ](https://arxiv.org/pdf/2108.10294.pdf)
>  We propose a new approach to analyses of musical pieces by using the exceptional musical icosahedra where all the major/minor triads are represented by golden triangles and golden gnomons. First, we introduce a concept of the golden neighborhood that characterizes golden triangles/gnomons that neighbor a given golden triangle or gnomon. Then, we investigate a relation between the exceptional musical icosahedra and the neo-Riemannian theory, and find that the golden neighborhoods and the icosahedron symmetry relate any major/minor triad with any major/minor triad. Second, we show how the exceptional musical icosahedra are applied to analyzing harmonies constructed by four or more tones. We introduce two concepts, golden decomposition and golden singular. The golden decomposition is a decomposition of a given harmony into some harmonies constructing the given harmony and represented the golden figure (a golden triangle, a golden gnomon, or a golden rectangle). A harmony is golden singular if and only if the harmony does not have golden decompositions. We show results of the golden analysis (analysis by the golden decomposition) of the tertian seventh chords and the mystic chords. While the dominant seventh chord is golden singular in the type 1[star] and the type 4[star] exceptional musical icosahedron, the half-diminished seventh chord is golden singular in the type 2 [star] and the type 3[star] exceptional musical icosahedron. Last, we apply the golden analysis to the famous prelude in C major by Johan Sebastian Bach (BWV 846). We found 7 combinations of the golden figures on the type 2 [star] or the type 3 [star] exceptional musical icosahedron dually represent all the measures of the BWV 846.      
### 51.Exclusive Group Lasso for Structured Variable Selection  [ :arrow_down: ](https://arxiv.org/pdf/2108.10284.pdf)
>  A structured variable selection problem is considered in which the covariates, divided into predefined groups, activate according to sparse patterns with few nonzero entries per group. Capitalizing on the concept of atomic norm, a composite norm can be properly designed to promote such exclusive group sparsity patterns. The resulting norm lends itself to efficient and flexible regularized optimization algorithms for support recovery, like the proximal algorithm. Moreover, an active set algorithm is proposed that builds the solution by successively including structure atoms into the estimated support. It is also shown that such an algorithm can be tailored to match more rigid structures than plain exclusive group sparsity. Asymptotic consistency analysis (with both the number of parameters as well as the number of groups growing with the observation size) establishes the effectiveness of the proposed solution in terms of signed support recovery under conventional assumptions. Finally, a set of numerical simulations further corroborates the results.      
### 52.ChiNet: Deep Recurrent Convolutional Learning for Multimodal Spacecraft Pose Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2108.10282.pdf)
>  This paper presents an innovative deep learning pipeline which estimates the relative pose of a spacecraft by incorporating the temporal information from a rendezvous sequence. It leverages the performance of long short-term memory (LSTM) units in modelling sequences of data for the processing of features extracted by a convolutional neural network (CNN) backbone. Three distinct training strategies, which follow a coarse-to-fine funnelled approach, are combined to facilitate feature learning and improve end-to-end pose estimation by regression. The capability of CNNs to autonomously ascertain feature representations from images is exploited to fuse thermal infrared data with red-green-blue (RGB) inputs, thus mitigating the effects of artefacts from imaging space objects in the visible wavelength. Each contribution of the proposed framework, dubbed ChiNet, is demonstrated on a synthetic dataset, and the complete pipeline is validated on experimental data.      
### 53.Proportional-Integral Projected Gradient Method for Conic Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2108.10260.pdf)
>  Conic optimization is the minimization of a differentiable convex objective function subject to conic constraints. We propose a novel primal-dual first-order method for conic optimization, named proportional-integral projected gradient method (PIPG). PIPG ensures that both the primal-dual gap and the constraint violation converge to zero at the rate of \(O(1/k)\), where \(k\) is the number of iterations. If the objective function is strongly convex, PIPG improves the convergence rate of the primal-dual gap to \(O(1/k^2)\). Further, unlike any existing first-order methods, PIPG also improves the convergence rate of the constraint violation to \(O(1/k^3)\). We demonstrate the application of PIPG in constrained optimal control problems.      
### 54.Automatic Speech Recognition using limited vocabulary: A survey  [ :arrow_down: ](https://arxiv.org/pdf/2108.10254.pdf)
>  Automatic Speech Recognition (ASR) is an active field of research due to its huge number of applications and the proliferation of interfaces or computing devices that can support speech processing. But the bulk of applications is based on well-resourced languages that overshadow under-resourced ones. Yet ASR represents an undeniable mean to promote such languages, especially when design human-to-human or human-to-machine systems involving illiterate people. An approach to design an ASR system targeting under-resourced languages is to start with a limited vocabulary. ASR using a limited vocabulary is a subset of the speech recognition problem that focuses on the recognition of a small number of words or sentences. This paper aims to provide a comprehensive view of mechanisms behind ASR systems as well as techniques, tools, projects, recent contributions, and possibly future directions in ASR using a limited vocabulary. This work consequently provides a way to go when designing ASR system using limited vocabulary. Although an emphasis is put on limited vocabulary, most of the tools and techniques reported in this survey applied to ASR systems in general.      
### 55.Adaptable GAN Encoders for Image Reconstruction via Multi-type Latent Vectors with Two-scale Attentions  [ :arrow_down: ](https://arxiv.org/pdf/2108.10201.pdf)
>  Although current deep generative adversarial networks (GANs) could synthesize high-quality (HQ) images, discovering novel GAN encoders for image reconstruction is still favorable. When embedding images to latent space, existing GAN encoders work well for aligned images (such as the human face), but they do not adapt to more generalized GANs. To our knowledge, current state-of-the-art GAN encoders do not have a proper encoder to reconstruct high-fidelity images from most misaligned HQ synthesized images on different GANs. Their performances are limited, especially on non-aligned and real images. We propose a novel method (named MTV-TSA) to handle such problems. Creating multi-type latent vectors (MTV) from latent space and two-scale attentions (TSA) from images allows designing a set of encoders that can be adaptable to a variety of pre-trained GANs. We generalize two sets of loss functions to optimize the encoders. The designed encoders could make GANs reconstruct higher fidelity images from most synthesized HQ images. In addition, the proposed method can reconstruct real images well and process them based on learned attribute directions. The designed encoders have unified convolutional blocks and could match well in current GAN architectures (such as PGGAN, StyleGANs, and BigGAN) by fine-tuning the corresponding normalization layers and the last block. Such well-designed encoders can also be trained to converge more quickly.      
### 56.Spatio-Temporal Split Learning for Privacy-Preserving Medical Platforms: Case Studies with COVID-19 CT, X-Ray, and Cholesterol Data  [ :arrow_down: ](https://arxiv.org/pdf/2108.10147.pdf)
>  Machine learning requires a large volume of sample data, especially when it is used in high-accuracy medical applications. However, patient records are one of the most sensitive private information that is not usually shared among institutes. This paper presents spatio-temporal split learning, a distributed deep neural network framework, which is a turning point in allowing collaboration among privacy-sensitive organizations. Our spatio-temporal split learning presents how distributed machine learning can be efficiently conducted with minimal privacy concerns. The proposed split learning consists of a number of clients and a centralized server. Each client has only has one hidden layer, which acts as the privacy-preserving layer, and the centralized server comprises the other hidden layers and the output layer. Since the centralized server does not need to access the training data and trains the deep neural network with parameters received from the privacy-preserving layer, privacy of original data is guaranteed. We have coined the term, spatio-temporal split learning, as multiple clients are spatially distributed to cover diverse datasets from different participants, and we can temporally split the learning process, detaching the privacy preserving layer from the rest of the learning process to minimize privacy breaches. This paper shows how we can analyze the medical data whilst ensuring privacy using our proposed multi-site spatio-temporal split learning algorithm on Coronavirus Disease-19 (COVID-19) chest Computed Tomography (CT) scans, MUsculoskeletal RAdiographs (MURA) X-ray images, and cholesterol levels.      
### 57.Ghost Panorama  [ :arrow_down: ](https://arxiv.org/pdf/2108.10122.pdf)
>  Computational ghost imaging or single-pixel imaging enables the image formation of an unknown scene using a lens-free photodetector. In this Letter, we present a computational panoramic ghost imaging system that can achieve the full-color panorama using a single-pixel photodetector, where a convex mirror performs the optical transformation of the engineered Hadamard-based circular illumination pattern from unidirectionally to omnidirectionally. To our best knowledge, it is the first time to propose the concept of ghost panorama and realize preliminary experimentations. It is foreseeable that ghost panorama will have more advantages in imaging and detection in many extreme conditions (e.g., scattering/turbulence, cryogenic temperatures, and unconventional spectra), as well as broad application prospects in the positioning of fast-moving targets and situation awareness for autonomous vehicles.      
### 58.Tracked 3D Ultrasound and Deep Neural Network-based Thyroid Segmentation reduce Interobserver Variability in Thyroid Volumetry  [ :arrow_down: ](https://arxiv.org/pdf/2108.10118.pdf)
>  Background: Thyroid volumetry is crucial in diagnosis, treatment and monitoring of thyroid diseases. However, conventional thyroid volumetry with 2D ultrasound is highly operator-dependent. This study compares 2D ultrasound and tracked 3D ultrasound with an automatic thyroid segmentation based on a deep neural network regarding inter- and intraobserver variability, time and accuracy. Volume reference was MRI. Methods: 28 healthy volunteers were scanned with 2D and 3D ultrasound as well as by MRI. Three physicians (MD 1, 2, 3) with different levels of experience (6, 4 and 1 a) performed three 2D ultrasound and three tracked 3D ultrasound scans on each volunteer. In the 2D scans the thyroid lobe volumes were calculated with the ellipsoid formula. A convolutional deep neural network (CNN) segmented the 3D thyroid lobes automatically. On MRI (T1 VIBE sequence) the thyroid was manually segmented by an experienced medical doctor. Results: The CNN was trained to obtain a dice score of 0.94. The interobserver variability comparing two MDs showed mean differences for 2D and 3D respectively of 0.58 ml to 0.52 ml (MD1 vs. 2), -1.33 ml to -0.17 ml (MD1 vs. 3) and -1.89 ml to -0.70 ml (MD2 vs. 3). Paired samples t-tests showed significant differences in two comparisons for 2D and none for 3D. Intraobsever variability was similar for 2D and 3D ultrasound. Comparison of ultrasound volumes and MRI volumes by paired samples t-tests showed a significant difference for the 2D volumetry of all MDs, and no significant difference for 3D ultrasound. Acquisition time was significantly shorter for 3D ultrasound. Conclusion: Tracked 3D ultrasound combined with a CNN segmentation significantly reduces interobserver variability in thyroid volumetry and increases the accuracy of the measurements with shorter acquisition times.      
### 59.Deep neural networks approach to microbial colony detection -- a comparative analysis  [ :arrow_down: ](https://arxiv.org/pdf/2108.10103.pdf)
>  Counting microbial colonies is a fundamental task in microbiology and has many applications in numerous industry branches. Despite this, current studies towards automatic microbial counting using artificial intelligence are hardly comparable due to the lack of unified methodology and the availability of large datasets. The recently introduced AGAR dataset is the answer to the second need, but the research carried out is still not exhaustive. To tackle this problem, we compared the performance of three well-known deep learning approaches for object detection on the AGAR dataset, namely two-stage, one-stage and transformer based neural networks. The achieved results may serve as a benchmark for future experiments.      
### 60.On the Acceleration of Deep Neural Network Inference using Quantized Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2108.10101.pdf)
>  Accelerating deep neural network (DNN) inference on resource-limited devices is one of the most important barriers to ensuring a wider and more inclusive adoption. To alleviate this, DNN binary quantization for faster convolution and memory savings is one of the most promising strategies despite its serious drop in accuracy. The present paper therefore proposes a novel binary quantization function based on quantized compressed sensing (QCS). Theoretical arguments conjecture that our proposal preserves the practical benefits of standard methods, while reducing the quantization error and the resulting drop in accuracy.      
### 61.CS-Based CSIT Estimation for Downlink Pilot Decontamination in Multi-Cell FDD Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2108.10090.pdf)
>  Efficient channel state information at transmitter (CSIT) for frequency division duplex (FDD) massive MIMO can facilitate its backward compatibility with existing FDD cellular networks. To date, several CSIT estimation schemes have been proposed for FDD single-cell massive MIMO systems, but they fail to consider inter-cell-interference (ICI) and suffer from downlink pilot contamination in multi-cell scenario. To solve this problem, this paper proposes a compressive sensing (CS)-based CSIT estimation scheme to combat ICI in FDD multi-cell massive MIMO systems. Specifically, angle-domain massive MIMO channels exhibit the common sparsity over different subcarriers, and such sparsity is partially shared by adjacent users. By exploiting these sparsity properties, we design the pilot signal and the associated channel estimation algorithm under the framework of CS theory, where the channels associated with multiple adjacent BSs can be reliably estimated with low training overhead for downlink pilot decontamination. Simulation results verify the good downlink pilot decontamination performance of the proposed solution compared to its conventional counterparts in multi-cell FDD massive MIMO.      
### 62.On Kurtosis-limited Enumerative Sphere Shaping for Reach Increase in Single-span Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.10080.pdf)
>  The effect of decreasing the kurtosis of channel inputs is investigated for the first time with an algorithmic shaping implementation. No significant gains in decoding performance are observed for multi-span systems, while an increase in reach is obtained for single-span transmission.      
### 63.System Matrix based Reconstruction for Pulsed Sequences in Magnetic Particle Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2108.10073.pdf)
>  Improving resolution and sensitivity will widen possible medical applications of magnetic particle imaging in its clinical application. Pulsed excitation promises such benefits, at the cost of more complex hardware solutions and restrictions on drive field amplitude and frequency. In this work, a sequence is proposed, that combines high drive-field amplitudes and high frequency rectangular excitation. State of the art systems utilize a sinusoidal excitation to drive superparamagnetic nanoparticles into the non-linear part of their magnetization curve, which creates a spectrum with a clear separation of direct feed-through and higher harmonics caused by the particles response. One challenge for rectangular excitation is the discrimination of particle and excitation signals, both broad-band. Another is the drive-field sequence itself, as particles that are not placed at the same spatial position, may react simultaneously and are not separable by their signals phase or signal shape. This loss of information in spatial encoding is overcome in this work by utilizing a superposition of shifting fields and drive-field rotations. The software framework developed for this work processes measured data from an Arbitrary Waveform Magnetic Particle Spectrometer, which is calibrated to guarantee device independence. Multiple sequence types and waveforms are compared, based on frequency space image reconstruction from emulated signals, that are derived from these measured particle responses. A resolution of 1.0 mT (0.8 mm for a gradient of (-1.25,-1.25,2.5) T/m) in x- and y-direction was achieved and a superior sensitivity was detected on the basis of reference phantoms for the proposed sequence in this work.      
### 64.Image coding for machines: an end-to-end learned approach  [ :arrow_down: ](https://arxiv.org/pdf/2108.09993.pdf)
>  Over recent years, deep learning-based computer vision systems have been applied to images at an ever-increasing pace, oftentimes representing the only type of consumption for those images. Given the dramatic explosion in the number of images generated per day, a question arises: how much better would an image codec targeting machine-consumption perform against state-of-the-art codecs targeting human-consumption? In this paper, we propose an image codec for machines which is neural network (NN) based and end-to-end learned. In particular, we propose a set of training strategies that address the delicate problem of balancing competing loss functions, such as computer vision task losses, image distortion losses, and rate loss. Our experimental results show that our NN-based codec outperforms the state-of-the-art Versa-tile Video Coding (VVC) standard on the object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast thanks to its compact size. To the best of our knowledge, this is the first end-to-end learned machine-targeted image codec.      
### 65.Subject Envelope based Multitype Reconstruction Algorithm of Speech Samples of Parkinson's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2108.09922.pdf)
>  The risk of Parkinson's disease (PD) is extremely serious, and PD speech recognition is an effective method of diagnosis nowadays. However, due to the influence of the disease stage, corpus, and other factors on data collection, the ability of every samples within one subject to reflect the status of PD vary. No samples are useless totally, and not samples are 100% perfect. This characteristic means that it is not suitable just to remove some samples or keep some samples. It is necessary to consider the sample transformation for obtaining high quality new samples. Unfortunately, existing PD speech recognition methods focus mainly on feature learning and classifier design rather than sample learning, and few methods consider the sample transformation. To solve the problem above, a PD speech sample transformation algorithm based on multitype reconstruction operators is proposed in this paper. The algorithm is divided into four major steps. Three types of reconstruction operators are designed in the algorithm: types A, B and C. Concerning the type A operator, the original dataset is directly reconstructed by designing a linear transformation to obtain the first dataset. The type B operator is designed for clustering and linear transformation of the dataset to obtain the second new dataset. The third operator, namely, the type C operator, reconstructs the dataset by clustering and convolution to obtain the third dataset. Finally, the base classifier is trained based on the three new datasets, and then the classification results are fused by decision weighting. In the experimental section, two representative PD speech datasets are used for verification. The results show that the proposed algorithm is effective. Compared with other algorithms, the proposed algorithm achieves apparent improvements in terms of classification accuracy.      
### 66.Mathematical Analysis of Modified BEM-FEM Coupling Approach for 3D Electromagnetic Levitation Problem  [ :arrow_down: ](https://arxiv.org/pdf/2108.09632.pdf)
>  In electromagnetic analysis, the finite element and boundary element methods jointly known as 'FEM-BEM coupling' is applied for numerically solving levitation problem based on eddy current. The main focus behind this coupled analysis method is to determine the dynamic characteristic of the levitating body in the presence of a magnetic field. An innovative 3D structure is developed that couples Lagrangian description and BEM-FEM coupling method for this purpose. The coupling methodology is based on the boundary conditions on the common boundaries between FEM and BEM sub-domains. Subsequent coding has been developed to simulate the problem in the MATLAB environment. An example similar to TEAM (Testing Electromagnetic Analysis Methods) workshop problem 28 has been used to study the efficiency of code for computationally inexpensive analysis.      
### 67.Event-Triggered Control for Weight-Unbalanced Directed Networks  [ :arrow_down: ](https://arxiv.org/pdf/2108.09609.pdf)
>  We develop an event-triggered control strategy for a weighted-unbalanced directed homogeneous robot network to reach a dynamic consensus in this work. We present some guarantees for synchronizing a robot network when all robots have access to the reference and when a limited number of robots have access. The proposed event-triggered control can reduce and avoid the periodic updating of the signals. Unlike some current control methods, we prove stability by making use of a logarithmic norm, which extends the possibilities of the control law to be applied to a wide range of directed graphs, in contrast to other works where the event-triggered control can be only implemented over strongly connected and weight-balanced digraphs. We test the performance of our algorithm by carrying out experiments both in simulation and in a real team of robots.      
### 68.A generalized forecasting solution to enable future insights of COVID-19 at sub-national level resolutions  [ :arrow_down: ](https://arxiv.org/pdf/2108.09556.pdf)
>  COVID-19 continues to cause a significant impact on public health. To minimize this impact, policy makers undertake containment measures that however, when carried out disproportionately to the actual threat, as a result if errorneous threat assessment, cause undesirable long-term socio-economic complications. In addition, macro-level or national level decision making fails to consider the localized sensitivities in small regions. Hence, the need arises for region-wise threat assessments that provide insights on the behaviour of COVID-19 through time, enabled through accurate forecasts. In this study, a forecasting solution is proposed, to predict daily new cases of COVID-19 in regions small enough where containment measures could be locally implemented, by targeting three main shortcomings that exist in literature; the unreliability of existing data caused by inconsistent testing patterns in smaller regions, weak deploy-ability of forecasting models towards predicting cases in previously unseen regions, and model training biases caused by the imbalanced nature of data in COVID-19 epi-curves. Hence, the contributions of this study are three-fold; an optimized smoothing technique to smoothen less deterministic epi-curves based on epidemiological dynamics of that region, a Long-Short-Term-Memory (LSTM) based forecasting model trained using data from select regions to create a representative and diverse training set that maximizes deploy-ability in regions with lack of historical data, and an adaptive loss function whilst training to mitigate the data imbalances seen in epi-curves. The proposed smoothing technique, the generalized training strategy and the adaptive loss function largely increased the overall accuracy of the forecast, which enables efficient containment measures at a more localized micro-level.      
### 69.Covariance Steering for Nonlinear Control-affine Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.09530.pdf)
>  We consider the covariance steering problem for nonlinear control-affine systems. Our objective is to find an optimal control strategy to steer the state of a system from an initial distribution to a target one whose mean and covariance are given. Due to the nonlinearity, the existing techniques for linear covariance steering problems are not directly applicable. By leveraging the celebrated Girsanov theorem, we formulate the problem into an optimization over the space path distributions. We then adopt a generalized proximal gradient algorithm to solve this optimization, where each update requires solving a linear covariance steering problem. Our algorithm is guaranteed to converge to a local optimal solution with a sublinear rate. In addition, each iteration of the algorithm can be achieved in closed form, and thus the computational complexity of it is insensitive to the resolution of time-discretization.      
### 70.MITI Minimum Information guidelines for highly multiplexed tissue images  [ :arrow_down: ](https://arxiv.org/pdf/2108.09499.pdf)
>  The imminent release of atlases combining highly multiplexed tissue imaging with single cell sequencing and other omics data from human tissues and tumors creates an urgent need for data and metadata standards compliant with emerging and traditional approaches to histology. We describe the development of a Minimum Information about highly multiplexed Tissue Imaging (MITI) standard that draws on best practices from genomics and microscopy of cultured cells and model organisms.      
### 71.Active User Detection and Channel Estimation for Spatial-based Random Access in Crowded Massive MIMO Systems via Blind Super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2108.09498.pdf)
>  This work presents a novel framework for random access in crowded scenarios of multiple-input multiple-output(MIMO) systems. A multi-antenna base station (BS) and multiple single-antenna users are considered in these systems. A huge portion of the system resources is dedicated as orthogonal pilots for accurate channel estimation which imposes a huge training overhead. This overhead can be highly mitigated by exploiting intrinsic angular domain sparsity of massive MIMO channels and the sporadic traffic of users, i.e., few number of users are active to sent or receive data in each coherence interval. In fact, the angles of arrivals (AoAs) coming from active users are continuous parameters and can take any arbitrary values. Besides, the AoAs corresponding to each active user are alongside each other forming a specific cluster. This work revolves around exploiting these features. Specifically, a blind clustering-based algorithm is proposed that not only recovers the transmitted data by users in grant free random access and primary pilots in random access blocks of coherent transmission, but also provides accurate channel estimation. Our approach is based on transforming the unknown variables into a higher dimensional space with matrix variables. An off-grid atomic norm minimization is then proposed to obtain the unknown matrix from only a few observed arrays at the BS. Then, a clustering-based approach is employed to identify which AoAs correspond to which active users. After identifying active users and their AoAs, an alternating-based approach is performed to obtain the channels and data or primary pilots of active users. Simulation results demonstrate the effectiveness of our approach in AoA detection as well as data recovery.      
### 72.An Attention-Aided Deep Learning Framework for Massive MIMO Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2108.09430.pdf)
>  Channel estimation is one of the key issues in practical massive multiple-input multiple-output (MIMO) systems. Compared with conventional estimation algorithms, deep learning (DL) based ones have exhibited great potential in terms of performance and complexity. In this paper, an attention mechanism, exploiting the channel distribution characteristics, is proposed to improve the estimation accuracy of highly separable channels with narrow angular spread by realizing the "divide-and-conquer" policy. Specifically, we introduce a novel attention-aided DL channel estimation framework for conventional massive MIMO systems and devise an embedding method to effectively integrate the attention mechanism into the fully connected neural network for the hybrid analog-digital (HAD) architecture. Simulation results show that in both scenarios, the channel estimation performance is significantly improved with the aid of attention at the cost of small complexity overhead. Furthermore, strong robustness under different system and channel parameters can be achieved by the proposed approach, which further strengthens its practical value. We also investigate the distributions of learned attention maps to reveal the role of attention, which endows the proposed approach with a certain degree of interpretability.      
### 73.Adaptive unsupervised learning with enhanced feature representation for intra-tumor partitioning and survival prediction for glioblastoma  [ :arrow_down: ](https://arxiv.org/pdf/2108.09423.pdf)
>  Glioblastoma is profoundly heterogeneous in regional microstructure and vasculature. Characterizing the spatial heterogeneity of glioblastoma could lead to more precise treatment. With unsupervised learning techniques, glioblastoma MRI-derived radiomic features have been widely utilized for tumor sub-region segmentation and survival prediction. However, the reliability of algorithm outcomes is often challenged by both ambiguous intermediate process and instability introduced by the randomness of clustering algorithms, especially for data from heterogeneous patients. <br>In this paper, we propose an adaptive unsupervised learning approach for efficient MRI intra-tumor partitioning and glioblastoma survival prediction. A novel and problem-specific Feature-enhanced Auto-Encoder (FAE) is developed to enhance the representation of pairwise clinical modalities and therefore improve clustering stability of unsupervised learning algorithms such as K-means. Moreover, the entire process is modelled by the Bayesian optimization (BO) technique with a custom loss function that the hyper-parameters can be adaptively optimized in a reasonably few steps. The results demonstrate that the proposed approach can produce robust and clinically relevant MRI sub-regions and statistically significant survival predictions.      
### 74.Reconfigurable Intelligent Surface Aided Communications: Asymptotic Analysis under Imperfect CSI  [ :arrow_down: ](https://arxiv.org/pdf/2108.09388.pdf)
>  This work studies the asymptotic sum-rate performance of a multi-user reconfigurable intelligent surface (RIS) assisted-multiple-input single-output (MISO) downlink system under imperfect CSI and Rayleigh and Rician fading. We first extend the existing least squares (LS) ON/OFF channel estimation protocol to a multi-user system, where we derive minimum mean squared error (MMSE) estimates of all RIS-assisted channels over multiple sub-phases. We also consider a low-complexity direct estimation (DE) scheme, where the BS obtains the MMSE estimate of the overall channel in a single sub-phase. Under both protocols, the BS implements maximum ratio transmission (MRT) precoding while the RIS phases are studied in the large system limit, where we derive deterministic equivalents of the signal- to-interference-plus-noise ratio (SINR) and the sum-rate. The derived asymptotic expressions reveal that under Rayleigh fading, the RIS phase-shift values do not play a significant role in improving the sum-rate but the RIS still provides an array gain. However, under Rician fading, we show that RIS provides both array and reflect beamforming gains. A projected gradient ascent-based algorithm is used to optimize the phase-shifts under both ON/OFF and DE protocol. Simulation results show that the DE of the overall channel yields better downlink performance when considering large systems.      
### 75.Design of Novel 3T Ternary DRAM with Single Word-Line using CNTFET  [ :arrow_down: ](https://arxiv.org/pdf/2108.09342.pdf)
>  Ternary logic system is the most promising and pursued alternate to the prevailing binary logic systems due to the energy efficiency of circuits following reduced circuit complexity and chip area. In this paper, we have proposed a ternary 3-Transistor Dynamic Random-Access Memory (3T-DRAM) cell using a single word-line for both read and write operation. For simulation of the circuit, we have used Carbon-Nano-Tube Field Effect Transistor (CNTFET). Here, we have analyzed the operation of the circuit considering different process variations and showed the results for write delay, read sensing time, and consumed current. Along with the basic DRAM design, we have proposed a ternary sense circuitry for the proper read operation of the proposed DRAM. The simulation and analysis are executed using the H-SPICE tool with Stanford University CNTFET model.      
### 76.Remarks on input to state stability of perturbed gradient flows, motivated by model-free feedback control learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.02632.pdf)
>  Recent work on data-driven control and reinforcement learning has renewed interest in a relative old field in control theory: model-free optimal control approaches which work directly with a cost function and do not rely upon perfect knowledge of a system model. Instead, an "oracle" returns an estimate of the cost associated to, for example, a proposed linear feedback law to solve a linear-quadratic regulator problem. This estimate, and an estimate of the gradient of the cost, might be obtained by performing experiments on the physical system being controlled. This motivates in turn the analysis of steepest descent algorithms and their associated gradient differential equations. This note studies the effect of errors in the estimation of the gradient, framed in the language of input to state stability, where the input represents a perturbation from the true gradient. Since one needs to study systems evolving on proper open subsets of Euclidean space, a self-contained review of input to state stability definitions and theorems for systems that evolve on such sets is included. The results are then applied to the study of noisy gradient systems, as well as the associated steepest descent algorithms.      
