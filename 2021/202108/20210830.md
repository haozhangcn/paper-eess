# ArXiv eess --Mon, 30 Aug 2021
### 1.A Novel Hierarchical Light Field Coding Scheme Based on Hybrid Stacked Multiplicative Layers and Fourier Disparity Layers for Glasses-Free 3D Displays  [ :arrow_down: ](https://arxiv.org/pdf/2108.12399.pdf)
>  This paper presents a novel hierarchical coding scheme for light fields based on transmittance patterns of low-rank multiplicative layers and Fourier disparity layers. The proposed scheme identifies multiplicative layers of light field view subsets optimized using a convolutional neural network for different scanning orders. Our approach exploits the hidden low-rank structure in the multiplicative layers obtained from the subsets of different scanning patterns. The spatial redundancies in the multiplicative layers can be efficiently removed by performing low-rank approximation at different ranks on the Krylov subspace. The intra-view and inter-view redundancies between approximated layers are further removed by HEVC encoding. Next, a Fourier disparity layer representation is constructed from the first subset of the approximated light field based on the chosen hierarchical order. Subsequent view subsets are synthesized by modeling the Fourier disparity layers that iteratively refine the representation with improved accuracy. The critical advantage of the proposed hybrid layered representation and coding scheme is that it utilizes not just spatial and temporal redundancies in light fields but efficiently exploits intrinsic similarities among neighboring sub-aperture images in both horizontal and vertical directions as specified by different predication orders. In addition, the scheme is flexible to realize a range of multiple bitrates at the decoder within a single integrated system. The compression performance of the proposed scheme is analyzed on real light fields. We achieved substantial bitrate savings and maintained good light field reconstruction quality.      
### 2.A Guide to Reproducible Research in Signal Processing and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.12383.pdf)
>  Reproducibility is a growing problem that has been extensively studied among computational researchers and within the signal processing and machine learning research community. However, with the changing landscape of signal processing and machine learning research come new obstacles and unseen challenges in creating reproducible experiments. Due to these new challenges most experiments have become difficult, if not impossible, to be reproduced by an independent researcher. In 2016 a survey conducted by the journal Nature found that 50% of researchers were unable to reproduce their own experiments. While the issue of reproducibility has been discussed in the literature and specifically within the signal processing community, it is still unclear to most researchers what are the best practices to ensure reproducibility without impinging on their primary responsibility of conducting research. We feel that although researchers understand the importance of making experiments reproducible, the lack of a clear set of standards and tools makes it difficult to incorporate good reproducibility practices in most labs. It is in this regard that we aim to present signal processing researchers with a set of practical tools and strategies that can help mitigate many of the obstacles to producing reproducible computational experiments.      
### 3.Artificial Neural Networks Based Analysis of BLDC Motor Speed Control  [ :arrow_down: ](https://arxiv.org/pdf/2108.12320.pdf)
>  Artificial Neural Network (ANN) is a simple network that has an input, an output, and numerous hidden layers with a set of nodes. Implementation of ANN algorithms in electrical, and electronics engineering always satisfies with the expected results as ANN handles binary data more accurately. Brushless Direct Current motor (BLDC motor) uses electronic closed-loop controllers to switch DC current to the motor windings and produces the magnetic fields. The BLDC motor finds various applications owing to its high speed, low maintenance and adequate torque capability. They are highly preferred than the other motors because of their better performance and it is easy to control their speed by Power Converters. This article presents a method of speed control of BLDC motors where speed is controlled by changing the DC input voltage of the bridge converter that feeds the motor winding. The control is done by using a PI based speed controller. The motor is modeled in the MATLAB/Simulink and the speed control is obtained with a PI controller. EMF signals, rotor speed, electromagnetic torque, Hall Effect signals, PWM and EMF signals simulations are then obtained. This acquired data is then fed into binary artificial neural networks and as a result, the ANN model predicts the corresponding parameters close to the simulation results. Both the mathematical based simulation and data based prediction gives satisfactory results      
### 4.Multiple Hypothesis Testing Framework for Spatial Signals  [ :arrow_down: ](https://arxiv.org/pdf/2108.12314.pdf)
>  The problem of identifying regions of spatially interesting, different or adversarial behavior is inherent to many practical applications involving distributed multisensor systems. In this work, we develop a general framework stemming from multiple hypothesis testing to identify such regions. A discrete spatial grid is assumed for the monitored environment. The spatial grid points associated with different hypotheses are identified while controlling the false discovery rate at a pre-specified level. Measurements are acquired using a large-scale sensor network. We propose a novel, data-driven method to estimate local false discovery rates based on the spectral method of moments. Our method is agnostic to specific spatial propagation models of the underlying physical phenomenon. It relies on a broadly applicable density model for local summary statistics. In between sensors, locations are assigned to regions associated with different hypotheses based on interpolated local false discovery rates. The benefits of our method are illustrated by applications to spatially propagating radio waves.      
### 5.CoCo DistillNet: a Cross-layer Correlation Distillation Network for Pathological Gastric Cancer Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2108.12173.pdf)
>  In recent years, deep convolutional neural networks have made significant advances in pathology image segmentation. However, pathology image segmentation encounters with a dilemma in which the higher-performance networks generally require more computational resources and storage. This phenomenon limits the employment of high-accuracy networks in real scenes due to the inherent high-resolution of pathological images. To tackle this problem, we propose CoCo DistillNet, a novel Cross-layer Correlation (CoCo) knowledge distillation network for pathological gastric cancer segmentation. Knowledge distillation, a general technique which aims at improving the performance of a compact network through knowledge transfer from a cumbersome network. Concretely, our CoCo DistillNet models the correlations of channel-mixed spatial similarity between different layers and then transfers this knowledge from a pre-trained cumbersome teacher network to a non-trained compact student network. In addition, we also utilize the adversarial learning strategy to further prompt the distilling procedure which is called Adversarial Distillation (AD). Furthermore, to stabilize our training procedure, we make the use of the unsupervised Paraphraser Module (PM) to boost the knowledge paraphrase in the teacher network. As a result, extensive experiments conducted on the Gastric Cancer Segmentation Dataset demonstrate the prominent ability of CoCo DistillNet which achieves state-of-the-art performance.      
### 6.Towards model predictive control of supercritical CO2 cycles  [ :arrow_down: ](https://arxiv.org/pdf/2108.12127.pdf)
>  Control of non-condensing non-ideal-gas power cycles is challenging because their output power dynamics depend on complex system interactions, non-ideal-gas effects complicate turbomachinery behavior, and state constraints must be respected. This article presents a control methodology for these systems, comprising a control modeling approach and model predictive control (MPC) strategy. This methodology is demonstrated on the high-pressure side of a simple supercritical CO2 cycle power block, composed of a variable-speed compressor, heat exchanger, and fixed-speed turbine. The control model is developed by applying timescale-separation arguments to a high-fidelity simulation model and locally linearizing non-ideal-gas turbomachinery performance maps. MPC is implemented by linearizing the control model online at each sampling instant. Closed-loop simulations with a full-order gas-dynamics truth model demonstrate the effectiveness of the proposed control methodology. In response to load changes, the controller maintains high turbine inlet temperatures while achieving net power output ramp rates in excess of 100% of nameplate output per minute. The controller often acts at the intersection of motor torque, compressor surge, and turbine inlet temperature constraints, and performs well from 35 to 105% of nameplate capacity with no parameter scheduling. Good performance and fast update rates are obtained via online linearization. The results demonstrate the suitability of MPC for the supercritical CO2 cycle, and the proposed methodology is extensible to more complex cycle variants such as the recuperated and recompression cycle.      
### 7.A Numerical Verification Framework for Differential Privacy in Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2108.12094.pdf)
>  This work proposes a numerical method to verify differential privacy in estimation with performance guarantees. To achieve differential privacy, a mechanism (estimation) is turned into a stochastic mapping; which makes it hard to distinguish outputs produced by close inputs. While obtaining theoretical conditions that guarantee privacy may be possible, verifying these in practice can be hard. To address this problem, we propose a data-driven, test framework for continuous-range mechanisms that i) finds a highly-likely, compact event set, as well as a partition of this event, and ii) evaluates differential privacy wrt this partition. This results into a type of approximate differential privacy with high confidence, which we are able to quantify precisely. This approach is then used to evaluate the differential-privacy properties of the recently proposed $W_2$ Moving Horizon Estimator. We confirm its properties, while comparing its performance with alternative approaches in simulation.      
### 8.A Three-terminal Non-Volatile Ferroelectric Switch with an Insulator-Metal Transition Channel  [ :arrow_down: ](https://arxiv.org/pdf/2108.12091.pdf)
>  Ferroelectrics offer a promising materials platform to realize energy-efficient non-volatile memory technology with the FeFET-based implementations being one of the most area-efficient ferroelectric memory architectures. However, the FeFET operation entails a fundamental trade-off between the read and the program operations. To overcome this trade-off, we propose in this work, a novel device, Mott-FeFET, that aims to replace the Silicon channel of the FeFET with VO2- a material that exhibits an electrically driven insulator-metal phase transition. The Mott-FeFET design, which demonstrates a (ferroelectric) polarization-dependent threshold voltage, enables the read current distinguishability (i.e., the ratio of current sensed when the Mott-FeFET is in state 1 and 0, respectively) to be independent of the program voltage. This enables the device to be programmed at low voltages without affecting the ability to sense/read the state of the device. Our work provides a pathway to realize low-voltage and energy-efficient non-volatile memory solutions.      
### 9.Deep Denoising Method for Side Scan Sonar Images without High-quality Reference Data  [ :arrow_down: ](https://arxiv.org/pdf/2108.12083.pdf)
>  Subsea images measured by the side scan sonars (SSSs) are necessary visual data in the process of deep-sea exploration by using the autonomous underwater vehicles (AUVs). They could vividly reflect the topography of the seabed, but usually accompanied by complex and severe noise. This paper proposes a deep denoising method for SSS images without high-quality reference data, which uses one single noise SSS image to perform self-supervised denoising. Compared with the classical artificially designed filters, the deep denoising method shows obvious advantages. The denoising experiments are performed on the real seabed SSS images, and the results demonstrate that our proposed method could effectively reduce the noise on the SSS image while minimizing the image quality and detail loss.      
### 10.Deep Reinforcement Learning for Dynamic Band Switch in Cellular-Connected UAV  [ :arrow_down: ](https://arxiv.org/pdf/2108.12054.pdf)
>  The choice of the transmitting frequency to provide cellular-connected Unmanned Aerial Vehicle (UAV) reliable connectivity and mobility support introduce several challenges. Conventional sub-6 GHz networks are optimized for ground Users (UEs). Operating at the millimeter Wave (mmWave) band would provide high-capacity but highly intermittent links. To reach the destination while minimizing a weighted function of traveling time and number of radio failures, we propose in this paper a UAV joint trajectory and band switch approach. By leveraging Double Deep Q-Learning we develop two different approaches to learn a trajectory besides managing the band switch. A first blind approach switches the band along the trajectory anytime the UAV-UE throughput is below a predefined threshold. In addition, we propose a smart approach for simultaneous learning-based path planning of UAV and band switch. The two approaches are compared with an optimal band switch strategy in terms of radio failure and band switches for different thresholds. Results reveal that the smart approach is able in a high threshold regime to reduce the number of radio failures and band switches while reaching the desired destination.      
### 11.Ultrafast Focus Detection for Automated Microscopy  [ :arrow_down: ](https://arxiv.org/pdf/2108.12050.pdf)
>  Recent advances in scientific instruments have resulted in dramatic increase in the volumes and velocities of data being generated in every-day laboratories. Scanning electron microscopy is one such example where technological advancements are now overwhelming scientists with critical data for montaging, alignment, and image segmentation -- key practices for many scientific domains, including, for example, neuroscience, where they are used to derive the anatomical relationships of the brain. These instruments now necessitate equally advanced computing resources and techniques to realize their full potential. Here we present a fast out-of-focus detection algorithm for electron microscopy images collected serially and demonstrate that it can be used to provide near-real time quality control for neurology research. Our technique, Multi-scale Histologic Feature Detection, adapts classical computer vision techniques and is based on detecting various fine-grained histologic features. We further exploit the inherent parallelism in the technique by employing GPGPU primitives in order to accelerate characterization. Tests are performed that demonstrate near-real-time detection of out-of-focus conditions. We deploy these capabilities as a funcX function and show that it can be applied as data are collected using an automated pipeline . We discuss extensions that enable scaling out to support multi-beam microscopes and integration with existing focus systems for purposes of implementing auto-focus.      
### 12.Evaluating Transformer based Semantic Segmentation Networks for Pathological Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2108.11993.pdf)
>  Histopathology has played an essential role in cancer diagnosis. With the rapid advances in convolutional neural networks (CNN). Various CNN-based automated pathological image segmentation approaches have been developed in computer-assisted pathological image analysis. In the past few years, Transformer neural networks (Transformer) have shown the unique merit of capturing the global long distance dependencies across the entire image as a new deep learning paradigm. Such merit is appealing for exploring spatially heterogeneous pathological images. However, there have been very few, if any, studies that have systematically evaluated the current Transformer based approaches in pathological image segmentation. To assess the performance of Transformer segmentation models on whole slide images (WSI), we quantitatively evaluated six prevalent transformer-based models on tumor segmentation, using the widely used PAIP liver histopathological dataset. For a more comprehensive analysis, we also compare the transformer-based models with six major traditional CNN-based models. The results show that the Transformer-based models exhibit a general superior performance over the CNN-based models. In particular, Segmenter, Swin-Transformer and TransUNet, all transformer-based, came out as the best performers among the twelve evaluated models.      
### 13.Anomaly Detection in Medical Imaging -- A Mini Review  [ :arrow_down: ](https://arxiv.org/pdf/2108.11986.pdf)
>  The increasing digitization of medical imaging enables machine learning based improvements in detecting, visualizing and segmenting lesions, easing the workload for medical experts. However, supervised machine learning requires reliable labelled data, which is is often difficult or impossible to collect or at least time consuming and thereby costly. Therefore methods requiring only partly labeled data (semi-supervised) or no labeling at all (unsupervised methods) have been applied more regularly. Anomaly detection is one possible methodology that is able to leverage semi-supervised and unsupervised methods to handle medical imaging tasks like classification and segmentation. This paper uses a semi-exhaustive literature review of relevant anomaly detection papers in medical imaging to cluster into applications, highlight important results, establish lessons learned and give further advice on how to approach anomaly detection in medical imaging. The qualitative analysis is based on google scholar and 4 different search terms, resulting in 120 different analysed papers. The main results showed that the current research is mostly motivated by reducing the need for labelled data. Also, the successful and substantial amount of research in the brain MRI domain shows the potential for applications in further domains like OCT and chest X-ray.      
### 14.SVM Classifier on Chip for Melanoma Detection  [ :arrow_down: ](https://arxiv.org/pdf/2108.11957.pdf)
>  Support Vector Machine (SVM) is a common classifier used for efficient classification with high accuracy. SVM shows high accuracy for classifying melanoma (skin cancer) clinical images within computer-aided diagnosis systems used by skin cancer specialists to detect melanoma early and save lives. We aim to develop a medical low-cost handheld device that runs a real-time embedded SVM- based diagnosis system for use in primary care for early detection of melanoma. In this paper, an optimized SVM classifier is implemented onto a recent FPGA platform using the latest design methodology to be embedded into the proposed device for realizing online efficient melanoma detection on a single system on chip/device. The hardware implementation results demonstrate a high classification accuracy of 97.9% and a significant acceleration factor of 26 from equivalent software implementation on an embedded processor, with 34% of resources utilization and 2 watts for power consumption. Consequently, the implemented system meets crucial embedded systems constraints of high performance and low cost, resources utilization and power consumption, while achieving high classification accuracy.      
### 15.Cascading Neural Network Methodology for Artificial Intelligence-Assisted Radiographic Detection and Classification of Lead-Less Implanted Electronic Devices within the Chest  [ :arrow_down: ](https://arxiv.org/pdf/2108.11954.pdf)
>  Background &amp; Purpose: Chest X-Ray (CXR) use in pre-MRI safety screening for Lead-Less Implanted Electronic Devices (LLIEDs), easily overlooked or misidentified on a frontal view (often only acquired), is common. Although most LLIED types are "MRI conditional": 1. Some are stringently conditional; 2. Different conditional types have specific patient- or device- management requirements; and 3. Particular types are "MRI unsafe". This work focused on developing CXR interpretation-assisting Artificial Intelligence (AI) methodology with: 1. 100% detection for LLIED presence/location; and 2. High classification in LLIED typing. Materials &amp; Methods: Data-mining (03/1993-02/2021) produced an AI Model Development Population (1,100 patients/4,871 images) creating 4,924 LLIED Region-Of-Interests (ROIs) (with image-quality grading) used in Training, Validation, and Testing. For developing the cascading neural network (detection via Faster R-CNN and classification via Inception V3), "ground-truth" CXR annotation (ROI labeling per LLIED), as well as inference display (as Generated Bounding Boxes (GBBs)), relied on a GPU-based graphical user interface. Results: To achieve 100% LLIED detection, probability threshold reduction to 0.00002 was required by Model 1, resulting in increasing GBBs per LLIED-related ROI. Targeting LLIED-type classification following detection of all LLIEDs, Model 2 multi-classified to reach high-performance while decreasing falsely positive GBBs. Despite 24% suboptimal ROI image quality, classification was correct in 98.9% and AUCs for the 9 LLIED-types were 1.00 for 8 and 0.92 for 1. For all misclassification cases: 1. None involved stringently conditional or unsafe LLIEDs; and 2. Most were attributable to suboptimal images. Conclusion: This project successfully developed a LLIED-related AI methodology supporting: 1. 100% detection; and 2. Typically 100% type classification.      
### 16.Group Testing with Non-identical Infection Probabilities  [ :arrow_down: ](https://arxiv.org/pdf/2108.12418.pdf)
>  We consider a zero-error probabilistic group testing problem where individuals are defective independently but not with identical probabilities. We propose a greedy set formation method to build sets of individuals to be tested together. We develop an adaptive group testing algorithm that uses the proposed set formation method recursively. We prove novel upper bounds on the number of tests for the proposed algorithm. Via numerical results, we show that our algorithm outperforms the state of the art, and performs close to the entropy lower bound.      
### 17.Bayesian Sparse Blind Deconvolution Using MCMC Methods Based on Normal-Inverse-Gamma Prior  [ :arrow_down: ](https://arxiv.org/pdf/2108.12398.pdf)
>  Bayesian estimation methods for sparse blind deconvolution problems conventionally employ Bernoulli-Gaussian (BG) prior for modeling sparse sequences and utilize Markov Chain Monte Carlo (MCMC) methods for the estimation of unknowns. However, the discrete nature of the BG model creates computational bottlenecks, preventing efficient exploration of the probability space even with the recently proposed enhanced sampler schemes. To address this issue, we propose an alternative MCMC method by modeling the sparse sequences using the Normal-Inverse-Gamma (NIG) prior. We derive effective Gibbs samplers for this prior and illustrate that the computational burden associated with the BG model can be eliminated by transferring the problem into a completely continuous-valued framework. In addition to sparsity, we also incorporate time and frequency domain constraints on the convolving sequences. We demonstrate the effectiveness of the proposed methods via extensive simulations and characterize computational gains relative to the existing methods that utilize BG modeling.      
### 18.FAST-PCA: A Fast and Exact Algorithm for Distributed Principal Component Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2108.12373.pdf)
>  Principal Component Analysis (PCA) is a fundamental data preprocessing tool in the world of machine learning. While PCA is often reduced to dimension reduction, the purpose of PCA is actually two-fold: dimension reduction and feature learning. Furthermore, the enormity of the dimensions and sample size in the modern day datasets have rendered the centralized PCA solutions unusable. In that vein, this paper reconsiders the problem of PCA when data samples are distributed across nodes in an arbitrarily connected network. While a few solutions for distributed PCA exist those either overlook the feature learning part of the purpose, have communication overhead making them inefficient and/or lack exact convergence guarantees. To combat these aforementioned issues, this paper proposes a distributed PCA algorithm called FAST-PCA (Fast and exAct diSTributed PCA). The proposed algorithm is efficient in terms of communication and can be proved to converge linearly and exactly to the principal components that lead to dimension reduction as well as uncorrelated features. Our claims are further supported by experimental results.      
### 19.Distributed Control and Optimization of DC Microgrids: A Port-Hamiltonian Approach  [ :arrow_down: ](https://arxiv.org/pdf/2108.12341.pdf)
>  This article proposes a distributed secondary control scheme that drives a dc microgrid to an equilibrium point where the generators share optimal currents, and their voltages have a weighted average of nominal value. The scheme does not rely on the electric system topology nor its specifications; it guarantees plug-and-play design and functionality of the generators. First, the incremental model of the microgrid system with constant impedance, current, and power devices is shown to admit a port-Hamiltonian (pH) representation, and its passive output is determined. The economic dispatch problem is then solved by the Lagrange multipliers method; the Karush-Kuhn-Tucker conditions and weighted average formation of voltages are then formulated as the control objectives. We propose a control scheme that is based on the Control by Interconnection design philosophy, where the consensus-based controller is viewed as a virtual pH system to be interconnected with the physical one. We prove the regional asymptotic stability of the closed-loop system using Lyapunov and LaSalle theorems. Equilibrium analysis is also conducted based on the concepts of graph theory and economic dispatch. Finally, the effectiveness of the presented scheme for different case studies is validated with a test microgrid system, simulated in both MATLAB/Simulink and OPAL-RT environments.      
### 20.Music Composition with Deep Learning: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2108.12290.pdf)
>  Generating a complex work of art such as a musical composition requires exhibiting true creativity that depends on a variety of factors that are related to the hierarchy of musical language. Music generation have been faced with Algorithmic methods and recently, with Deep Learning models that are being used in other fields such as Computer Vision. In this paper we want to put into context the existing relationships between AI-based music composition models and human musical composition and creativity processes. We give an overview of the recent Deep Learning models for music composition and we compare these models to the music composition process from a theoretical point of view. We have tried to answer some of the most relevant open questions for this task by analyzing the ability of current Deep Learning models to generate music with creativity or the similarity between AI and human composition processes, among others.      
### 21.In vivo functional and structural retina imaging using multimodal photoacoustic remote sensing microscopy and optical coherence tomography  [ :arrow_down: ](https://arxiv.org/pdf/2108.12279.pdf)
>  We have developed a multimodal photoacoustic remote sensing (PARS) microscope combined with swept source optical coherence tomography for in vivo, non-contact retinal imaging. Building on the proven strength of multiwavelength PARS imaging, the system is applied for estimating retinal oxygen saturation in the rat retina. The capability of the technology is demonstrated by imaging both microanatomy and the microvasculature of the retina in vivo. To our knowledge this is the first time a non-contact photoacoustic imaging technique is employed for in vivo oxygen saturation measurement in the retina.      
### 22.A System-on-Chip for Closed-loop Optogenetic Sleep Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2108.12261.pdf)
>  Stimulation of target neuronal populations using optogenetic techniques during specific sleep stages has begun to elucidate the mechanisms and effects of sleep. To conduct closed-loop optogenetic sleep studies in untethered animals, we designed a fully integrated, low-power system-on-chip (SoC) for real-time sleep stage classification and stage-specific optical stimulation. The SoC consists of a 4-channel analog front-end for recording polysomnography signals, a mixed-signal machine-learning (ML) core, and a 16-channel optical stimulation back-end. A novel ML algorithm and innovative circuit design techniques improved the online classification performance while minimizing power consumption. The SoC was designed and simulated in 180 nm CMOS technology. In an evaluation using an expert labeled sleep database with 20 subjects, the SoC achieves a high sensitivity of 0.806 and a specificity of 0.947 in discriminating 5 sleep stages. Overall power consumption in continuous operation is 97 uW.      
### 23.Injecting Text in Self-Supervised Speech Pretraining  [ :arrow_down: ](https://arxiv.org/pdf/2108.12226.pdf)
>  Self-supervised pretraining for Automated Speech Recognition (ASR) has shown varied degrees of success. In this paper, we propose to jointly learn representations during pretraining from two different modalities: speech and text. The proposed method, tts4pretrain complements the power of contrastive learning in self-supervision with linguistic/lexical representations derived from synthesized speech, effectively learning from untranscribed speech and unspoken text. Lexical learning in the speech encoder is enforced through an additional sequence loss term that is coupled with contrastive loss during pretraining. We demonstrate that this novel pretraining method yields Word Error Rate (WER) reductions of 10% relative on the well-benchmarked, Librispeech task over a state-of-the-art baseline pretrained with wav2vec2.0 only. The proposed method also serves as an effective strategy to compensate for the lack of transcribed speech, effectively matching the performance of 5000 hours of transcribed speech with just 100 hours of transcribed speech on the AMI meeting transcription task. Finally, we demonstrate WER reductions of up to 15% on an in-house Voice Search task over traditional pretraining. Incorporating text into encoder pretraining is complimentary to rescoring with a larger or in-domain language model, resulting in additional 6% relative reduction in WER.      
### 24.Fast Rule-Based Clutter Detection in Automotive Radar Data  [ :arrow_down: ](https://arxiv.org/pdf/2108.12224.pdf)
>  Automotive radar sensors output a lot of unwanted clutter or ghost detections, whose position and velocity do not correspond to any real object in the sensor's field of view. This poses a substantial challenge for environment perception methods like object detection or tracking. Especially problematic are clutter detections that occur in groups or at similar locations in multiple consecutive measurements. In this paper, a new algorithm for identifying such erroneous detections is presented. It is mainly based on the modeling of specific commonly occurring wave propagation paths that lead to clutter. In particular, the three effects explicitly covered are reflections at the underbody of a car or truck, signals traveling back and forth between the vehicle on which the sensor is mounted and another object, and multipath propagation via specular reflection. The latter often occurs near guardrails, concrete walls or similar reflective surfaces. Each of these effects is described both theoretically and regarding a method for identifying the corresponding clutter detections. Identification is done by analyzing detections generated from a single sensor measurement only. The final algorithm is evaluated on recordings of real extra-urban traffic. For labeling, a semi-automatic process is employed. The results are promising, both in terms of performance and regarding the very low execution time. Typically, a large part of clutter is found, while only a small ratio of detections corresponding to real objects are falsely classified by the algorithm.      
### 25.Deep Reinforcement Learning for Wireless Resource Allocation Using Buffer State Information  [ :arrow_down: ](https://arxiv.org/pdf/2108.12198.pdf)
>  As the number of user equipments (UEs) with various data rate and latency requirements increases in wireless networks, the resource allocation problem for orthogonal frequency-division multiple access (OFDMA) becomes challenging. In particular, varying requirements lead to a non-convex optimization problem when maximizing the systems data rate while preserving fairness between UEs. In this paper, we solve the non-convex optimization problem using deep reinforcement learning (DRL). We outline, train and evaluate a DRL agent, which performs the task of media access control scheduling for a downlink OFDMA scenario. To kickstart training of our agent, we introduce mimicking learning. For improvement of scheduling performance, full buffer state information at the base station (e.g. packet age, packet size) is taken into account. Techniques like input feature compression, packet shuffling and age capping further improve the performance of the agent. We train and evaluate our agents using Nokia's wireless suite and evaluate against different benchmark agents. We show that our agents clearly outperform the benchmark agents.      
### 26.Grammar Based Identification Of Speaker Role For Improving ATCO And Pilot ASR  [ :arrow_down: ](https://arxiv.org/pdf/2108.12175.pdf)
>  Assistant Based Speech Recognition (ABSR) for air traffic control is generally trained by pooling both Air Traffic Controller (ATCO) and pilot data. In practice, this is motivated by the fact that the proportion of pilot data is lesser compared to ATCO while their standard language of communication is similar. However, due to data imbalance of ATCO and pilot and their varying acoustic conditions, the ASR performance is usually significantly better for ATCOs than pilots. In this paper, we propose to (1) split the ATCO and pilot data using an automatic approach exploiting ASR transcripts, and (2) consider ATCO and pilot ASR as two separate tasks for Acoustic Model (AM) training. For speaker role classification of ATCO and pilot data, a hypothesized ASR transcript is generated with a seed model, subsequently used to classify the speaker role based on the knowledge extracted from grammar defined by International Civil Aviation Organization (ICAO). This approach provides an average speaker role identification accuracy of 83% for ATCO and pilot. Finally, we show that training AMs separately for each task, or using a multitask approach is well suited for this data compared to AM trained by pooling all data.      
### 27.Modal Strong Structural Controllability for Networks with Dynamical Nodes  [ :arrow_down: ](https://arxiv.org/pdf/2108.12171.pdf)
>  In this article, a new notion of modal strong structural controllability is introduced and examined for a family of LTI networks. These networks include structured LTI subsystems, whose system matrices have the same zero/nonzero/arbitrary pattern. An eigenvalue associated with a system matrix is controllable if it can be directly influenced by the control inputs. We consider an arbitrary set \Delta, and we refer to a network as modal strongly structurally controllable with respect to \Delta if, for all systems in a specific family of LTI networks, every \lambda\in\Delta is a controllable eigenvalue. For this family of LTI networks, not only is the zero/nonzero/arbitrary pattern of system matrices available, but also for a given \Delta, there might be extra information about the intersection of the spectrum associated with some subsystems and \Delta. Given a set \Delta, we first define a \Delta-network graph, and by introducing a coloring process of this graph, we establish a correspondence between the set of control subsystems and the so-called zero forcing sets. We also demonstrate how with \Delta={0} or \Delta=C\{0}, existing results on strong structural controllability can be derived through our approach. Compared to relevant literature, a more restricted family of LTI networks is considered in this work, and then, the derived condition is less conservative.      
### 28.Improving callsign recognition with air-surveillance data in air-traffic communication  [ :arrow_down: ](https://arxiv.org/pdf/2108.12156.pdf)
>  Automatic Speech Recognition (ASR) can be used as the assistance of speech communication between pilots and air-traffic controllers. Its application can significantly reduce the complexity of the task and increase the reliability of transmitted information. Evidently, high accuracy predictions are needed to minimize the risk of errors. Especially, high accuracy is required in recognition of key information, such as commands and callsigns, used to navigate pilots. Our results prove that the surveillance data containing callsigns can help to considerably improve the recognition of a callsign in an utterance when the weights of probable callsign n-grams are reduced per utterance. In this paper, we investigate two approaches: (1) G-boosting, when callsigns weights are adjusted at language model level (G) and followed by the dynamic decoder with an on-the-fly composition, and (2) lattice rescoring when callsign information is introduced on top of lattices generated using a conventional decoder. Boosting callsign n-grams with the combination of two methods allowed us to gain 28.4% of absolute improvement in callsign recognition accuracy and up to 74.2% of relative improvement in WER of callsign recognition.      
### 29.Separable Temporal Convolution plus Temporally Pooled Attention for Lightweight High-performance Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2108.12146.pdf)
>  Keyword spotting (KWS) on mobile devices generally requires a small memory footprint. However, most current models still maintain a large number of parameters in order to ensure good performance. In this paper, we propose a temporally pooled attention module which can capture global features better than the AveragePool. Besides, we design a separable temporal convolution network which leverages depthwise separable and temporal convolution to reduce the number of parameter and calculations. Finally, taking advantage of separable temporal convolution and temporally pooled attention, a efficient neural network (ST-AttNet) is designed for KWS system. We evaluate the models on the publicly available Google speech commands data sets V1. The number of parameters of proposed model (48K) is 1/6 of state-of-the-art TC-ResNet14-1.5 model (305K). The proposed model achieves a 96.6% accuracy, which is comparable to the TC-ResNet14-1.5 model (96.6%).      
### 30.Task-aware Warping Factors in Mask-based Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2108.12128.pdf)
>  This paper proposes the use of two task-aware warping factors in mask-based speech enhancement (SE). One controls the balance between speech-maintenance and noise-removal in training phases, while the other controls SE power applied to specific downstream tasks in testing phases. Our intention is to alleviate the problem that SE systems trained to improve speech quality often fail to improve other downstream tasks, such as automatic speaker verification (ASV) and automatic speech recognition (ASR), because they do not share the same objects. It is easy to apply the proposed dual-warping factors approach to any mask-based SE method, and it allows a single SE system to handle multiple tasks without task-dependent training. The effectiveness of our proposed approach has been confirmed on the SITW dataset for ASV evaluation and the LibriSpeech dataset for ASR and speech quality evaluations of 0-20dB. We show that different warping values are necessary for a single SE to achieve optimal performance w.r.t. the three tasks. With the use of task-dependent warping factors, speech quality was improved by an 84.7% PESQ increase, ASV had a 22.4% EER reduction, and ASR had a 52.2% WER reduction, on 0dB speech. The effectiveness of the task-dependent warping factors were also cross-validated on VoxCeleb-1 test set for ASV and LibriSpeech dev-clean set for ASV and quality evaluations. The proposed method is highly effective and easy to apply in practice.      
### 31.Full Attention Bidirectional Deep Learning Structure for Single Channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2108.12105.pdf)
>  As the cornerstone of other important technologies, such as speech recognition and speech synthesis, speech enhancement is a critical area in audio signal processing. In this paper, a new deep learning structure for speech enhancement is demonstrated. The model introduces a "full" attention mechanism to a bidirectional sequence-to-sequence method to make use of latent information after each focal frame. This is an extension of the previous attention-based RNN method. The proposed bidirectional attention-based architecture achieves better performance in terms of speech quality (PESQ), compared with OM-LSA, CNN-LSTM, T-GSA and the unidirectional attention-based LSTM baseline.      
### 32.Stationary Multi-source AI-powered Real-time Tomography (SMART) for Dynamic Cardiac Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2108.12076.pdf)
>  A first stationary multi-source computed tomography (CT) system is prototyped for preclinical imaging to achieve real-time temporal resolution for dynamic cardiac imaging. This unique is featured by 29 source-detector pairs fixed on a circular track for each detector to collect x-ray signals only from the opposite x-ray source. The new system architecture potentially leads to a major improvement in temporal resolution. To demonstrate the feasibility of this Stationary Multi-source AI-based Real-time Tomography (SMART) system, we develop a novel reconstruction scheme integrating both sparsified image prior (SIP) and deep image prior (DIP), which is referred to as the SIP-DIP network. Then, the SIP-DIP network for cardiac imaging is evaluated on preclinical cardiac datasets of alive rats. The reconstructed image volumes demonstrate the feasibility of the SMART system and the SIP-DIP network and the merits over other reconstruction methods.      
### 33.4-bit Quantization of LSTM-based Speech Recognition Models  [ :arrow_down: ](https://arxiv.org/pdf/2108.12074.pdf)
>  We investigate the impact of aggressive low-precision representations of weights and activations in two families of large LSTM-based architectures for Automatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden Markov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers (RNN-Ts). Using a 4-bit integer representation, a naïve quantization approach applied to the LSTM portion of these models results in significant Word Error Rate (WER) degradation. On the other hand, we show that minimal accuracy loss is achievable with an appropriate choice of quantizers and initializations. In particular, we customize quantization schemes depending on the local properties of the network, improving recognition performance while limiting computational time. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH) test sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or 2000 hours of SWB data achieves $&lt;$0.5% and $&lt;$1% average WER degradation, respectively. On the more challenging RNN-T models, our quantization strategy limits degradation in 4-bit inference to 1.3%.      
### 34.Robust Motion Planning in the Presence of Estimation Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2108.11983.pdf)
>  Motion planning is a fundamental problem and focuses on finding control inputs that enable a robot to reach a goal region while safely avoiding obstacles. However, in many situations, the state of the system may not be known but only estimated using, for instance, a Kalman filter. This results in a novel motion planning problem where safety must be ensured in the presence of state estimation uncertainty. Previous approaches to this problem are either conservative or integrate state estimates optimistically which leads to non-robust solutions. Optimistic solutions require frequent replanning to not endanger the safety of the system. We propose a new formulation to this problem with the aim to be robust to state estimation errors while not being overly conservative. In particular, we formulate a stochastic optimal control problem that contains robustified risk-aware safety constraints by incorporating robustness margins to account for state estimation errors. We propose a novel sampling-based approach that builds trees exploring the reachable space of Gaussian distributions that capture uncertainty both in state estimation and in future measurements. We provide robustness guarantees and show, both in theory and simulations, that the induced robustness margins constitute a trade-off between conservatism and robustness for planning under estimation uncertainty that allows to control the frequency of replanning.      
### 35.Classification of Emotions and Evaluation of Customer Satisfaction from Speech in Real World Acoustic Environments  [ :arrow_down: ](https://arxiv.org/pdf/2108.11981.pdf)
>  This paper focuses on finding suitable features to robustly recognize emotions and evaluate customer satisfaction from speech in real acoustic scenarios. The classification of emotions is based on standard and well-known corpora and the evaluation of customer satisfaction is based on recordings of real opinions given by customers about the received service during phone calls with call-center agents. The feature sets considered in this study include two speaker models, namely x-vectors and i-vectors, and also the well known feature set introduced in the Interspeech 2010 Paralinguistics Challenge (I2010PC). Additionally, we introduce the use of phonation, articulation and prosody features extracted with the DisVoice framework as alternative feature sets to robustly model emotions and customer satisfaction from speech. The results indicate that the I2010PC feature set is the best approach to classify emotions in the standard databases typically used in the literature. When considering the recordings collected in the call-center, without any control over the acoustic conditions, the best results are obtained with our articulation features. The I2010PC feature set includes 1584 measures while the articulation approach only includes 488 measures. We think that the proposed approach is more suitable for real-world applications where the acoustic conditions are not controlled and also it is potentially more convenient for industrial applications.      
### 36.Finite-time System Identification and Adaptive Control in Autoregressive Exogenous Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.11959.pdf)
>  Autoregressive exogenous (ARX) systems are the general class of input-output dynamical systems used for modeling stochastic linear dynamical systems (LDS) including partially observable LDS such as LQG systems. In this work, we study the problem of system identification and adaptive control of unknown ARX systems. We provide finite-time learning guarantees for the ARX systems under both open-loop and closed-loop data collection. Using these guarantees, we design adaptive control algorithms for unknown ARX systems with arbitrary strongly convex or convex quadratic regulating costs. Under strongly convex cost functions, we design an adaptive control algorithm based on online gradient descent to design and update the controllers that are constructed via a convex controller reparametrization. We show that our algorithm has $\tilde{\mathcal{O}}(\sqrt{T})$ regret via explore and commit approach and if the model estimates are updated in epochs using closed-loop data collection, it attains the optimal regret of $\text{polylog}(T)$ after $T$ time-steps of interaction. For the case of convex quadratic cost functions, we propose an adaptive control algorithm that deploys the optimism in the face of uncertainty principle to design the controller. In this setting, we show that the explore and commit approach has a regret upper bound of $\tilde{\mathcal{O}}(T^{2/3})$, and the adaptive control with continuous model estimate updates attains $\tilde{\mathcal{O}}(\sqrt{T})$ regret after $T$ time-steps.      
### 37.Design, Characterization, and Control of a Size Adaptable In-pipe Robot for Water Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2012.15236.pdf)
>  Leak detection and water quality monitoring are requirements and challenging tasks in Water Distribution Systems (WDS). In-line robots are designed for this aim. In our previous work, we designed an in-pipe robot [1]. In this research, we present the design of the central processor, characterize and control the robot based on the condition of operation in a highly pressurized environment of pipelines with the presence of high-speed flow. To this aim, an extreme operation condition is simulated with computational fluid dynamics (CFD) and the spring mechanism is characterized to ensure sufficient stabilizing force during operation based on the extreme operating condition. Also, an end-to-end method is suggested for power considerations for our robot that calculates minimum battery capacity and operation duration in the extreme operating condition. Finally, we design a novel LQR-PID based controller based on the system auxiliary matrices that retain the robot stability inside the pipeline against disturbances and uncertainties during operation. The ADAMS-MATLAB co-simulation of the robot-controller shows the rotational velocity with -4 degree/sec and +3 degree/sec margin around x, y, and z axes while the system tracks different desired velocities in pipelines (i.e. 0.12m/s, 0.17m/s, and 0.35m/s). Also, experimental results for four iterations in a 14-inch diameter PVC pipe show that the controller brings initial values of stabilizing states to zero and oscillate around it with a margin of 2 degrees and the system tracks desired velocities of 0.1m/s, 0.2m/s, 0.3m/s, and 0.35m/s in which makes the robot dexterous in uncertain and highly disturbed the environment of pipelines during operation.      
