# ArXiv eess --Mon, 16 Aug 2021
### 1.NMPC-Based Cooperative Strategy For A Target Pair To Lure Two Attackers Into Collision  [ :arrow_down: ](https://arxiv.org/pdf/2108.06276.pdf)
>  This paper presents a cooperative target defense strategy using nonlinear model-predictive control (NMPC) framework for a two--targets two--attackers (2T2A) game. The 2T2A game consists of two attackers and two targets. Each attacker needs to capture a designated target individually. However, the two targets cooperate to lure the attackers into a collision. We assume that the cooperative target pair do not have perfect knowledge of the attacker states, and hence they estimate the attacker states using an extended Kalman filter (EKF). The NMPC scheme computes closed- loop optimal control commands for the targets while respecting imposed state and control constraints. Theoretical analysis is carried out to determine regions that will lead to the targets' survival, given the initial positions of the attacker and target agents. Numerical simulations are carried out to evaluate the performance of the proposed NMPC- based strategy for different scenarios.      
### 2.Enhancing audio quality for expressive Neural Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2108.06270.pdf)
>  Artificial speech synthesis has made a great leap in terms of naturalness as recent Text-to-Speech (TTS) systems are capable of producing speech with similar quality to human recordings. However, not all speaking styles are easy to model: highly expressive voices are still challenging even to recent TTS architectures since there seems to be a trade-off between expressiveness in a generated audio and its signal quality. In this paper, we present a set of techniques that can be leveraged to enhance the signal quality of a highly-expressive voice without the use of additional data. The proposed techniques include: tuning the autoregressive loop's granularity during training; using Generative Adversarial Networks in acoustic modelling; and the use of Variational Auto-Encoders in both the acoustic model and the neural vocoder. We show that, when combined, these techniques greatly closed the gap in perceived naturalness between the baseline system and recordings by 39% in terms of MUSHRA scores for an expressive celebrity voice.      
### 3.Trajectory Planning Under Environmental Uncertainty With Finite-Sample Safety Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2108.06250.pdf)
>  We tackle the problem of trajectory planning in an environment comprised of a set of obstacles with uncertain time-varying locations. The uncertainties are modeled using widely accepted Gaussian distributions, resulting in a chance-constrained program. Contrary to previous approaches however, we do not assume perfect knowledge of the moments of the distribution, and instead estimate them through finite samples available from either sensors or past data. We derive tight concentration bounds on the error of these estimates to sufficiently tighten the chance-constraint program. As such, we provide provable guarantees on satisfaction of the chance-constraints corresponding to the nominal yet unknown moments. We illustrate our results with two autonomous vehicle trajectory planning case studies.      
### 4.Simultaneously Transmitting and Reflecting (STAR) Intelligent Omni-Surfaces, Their Modeling and Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2108.06233.pdf)
>  Given the rapid development of advanced electromagnetic manipulation technologies, researchers have turned their attention to the investigation of smart surfaces for enhancing the radio coverage. Simultaneously transmitting and reflecting intelligent omni-surfaces (STAR-IOSs) constitute one of the most promising categories. Although previous research contributions have demonstrated the benefits of STAR-IOSs in terms of its wireless communication performance gains, several important issues remain unresolved, including both their practical hardware implementations and their accurate physical models. In this paper, we address these issues by discussing four practical hardware implementations of STAR-IOSs, as well as three hardware modeling techniques and five channel modeling methods. We clarify the taxonomy of the smart surface technologies in support of further investigating the family of STAR-IOSs.      
### 5.Nonlinear modal testing of damped structures: Velocity feedback vs. phase resonance  [ :arrow_down: ](https://arxiv.org/pdf/2108.06189.pdf)
>  In recent years, a new method for experimental nonlinear modal analysis has been developed, which is based on the extended periodic motion concept. The method is well suited to experimentally obtain amplitude-dependent modal properties (modal frequency, damping ratio and deflection shape) for strongly nonlinear systems. To isolate a nonlinear mode, the negative viscous damping term of the extended periodic motion concept is approximated by ensuring phase resonance between excitation and response. In this work, an alternative approach to isolate a nonlinear mode is developed and analyzed: velocity feedback. The accuracy of the extracted modal properties and robustness of velocity feedback is first assessed by means of simulated experiments. The two approaches phase resonance and velocity feedback are then compared in terms of accuracy and experimental implementation effort. To this end, both approaches are applied to an experimental specimen, which is a cantilevered beam influenced by a strong dry friction nonlinearity. In this work, the discussion is limited to single-point excitation. It is shown that a robust implementation of velocity feedback requires the measurement of several response signals, distributed over the structure. An advantage of velocity feedback is that no controller is needed. The accuracy of the modal properties can, however, suffer from imperfections of the excitation mechanism such as a phase lag due to exciter-structure interactions or gyroscopic forces due to single-point excitation.      
### 6.Feature learning for efficient ASR-free keyword spotting in low-resource languages  [ :arrow_down: ](https://arxiv.org/pdf/2108.06174.pdf)
>  We consider feature learning for efficient keyword spotting that can be applied in severely under-resourced settings. The objective is to support humanitarian relief programmes by the United Nations in parts of Africa in which almost no language resources are available. For rapid development in such languages, we rely on a small, easily-compiled set of isolated keywords. These keyword templates are applied to a large corpus of in-domain but untranscribed speech using dynamic time warping (DTW). The resulting DTW alignment scores are used to train a convolutional neural network (CNN) which is orders of magnitude more computationally efficient and suitable for real-time application. We optimise this neural network keyword spotter by identifying robust acoustic features in this almost zero-resource setting. First, we incorporate information from well-resourced but unrelated languages using a multilingual bottleneck feature (BNF) extractor. Next, we consider features extracted from an autoencoder (AE) trained on in-domain but untranscribed data. Finally, we consider correspondence autoencoder (CAE) features which are fine-tuned on the small set of in-domain labelled data. Experiments in South African English and Luganda, a low-resource language, show that BNF and CAE features achieve a 5% relative performance improvement over baseline MFCCs. However, using BNFs as input to the CAE results in a more than 27% relative improvement over MFCCs in ROC area-under-the-curve (AUC) and more than twice as many top-10 retrievals. We show that, using these features, the CNN-DTW keyword spotter performs almost as well as the DTW keyword spotter while outperforming a baseline CNN trained only on the keyword templates. The CNN-DTW keyword spotter using BNF-derived CAE features represents an efficient approach with competitive performance suited to rapid deployment in a severely under-resourced scenario.      
### 7.Multilingual training set selection for ASR in under-resourced Malian languages  [ :arrow_down: ](https://arxiv.org/pdf/2108.06164.pdf)
>  We present first speech recognition systems for the two severely under-resourced Malian languages Bambara and Maasina Fulfulde. These systems will be used by the United Nations as part of a monitoring system to inform and support humanitarian programmes in rural Africa. We have compiled datasets in Bambara and Maasina Fulfulde, but since these are very small, we take advantage of six similarly under-resourced datasets in other languages for multilingual training. We focus specifically on the best composition of the multilingual pool of speech data for multilingual training. We find that, although maximising the training pool by including all six additional languages provides improved speech recognition in both target languages, substantially better performance can be achieved by a more judicious choice. Our experiments show that the addition of just one language provides best performance. For Bambara, this additional language is Maasina Fulfulde, and its introduction leads to a relative word error rate reduction of 6.7%, as opposed to a 2.4% relative reduction achieved when pooling all six additional languages. For the case of Maasina Fulfulde, best performance was achieved when adding only Luganda, leading to a relative word error rate improvement of 9.4% as opposed to a 3.9% relative improvement when pooling all six languages. We conclude that careful selection of the out-of-language data is worthwhile for multilingual training even in highly under-resourced settings, and that the general assumption that more data is better does not always hold.      
### 8.Harmonic Retrieval of CFO and Frame Misalignment for OFDM-based Inter-Satellite Links  [ :arrow_down: ](https://arxiv.org/pdf/2108.06074.pdf)
>  As dense low Earth orbit (LEO) constellations are being planned, the need for accurate synchronization schemes in high-speed environments remains a challenging problem to tackle. To further improve synchronization accuracy in channeling environments, which can also be applied in the LEO networks, we present a new method for estimating the carrier frequency offset (CFO) and frame misalignment in orthogonal frequency division multiplexing (OFDM) based inter-satellite links. The proposed method requires the transmission of pilot symbols to exploit 2-D estimation of signal parameters via rotational invariance techniques (ESPRIT) and estimate the CFO and the frame misalignment. The Cramer-Rao lower bounds (CRLB) of the joint estimation of the CFO and frame misalignment are also derived. Numerical results show that the difference between the proposed method and the state-of-art method is less than 5dB at its worse.      
### 9.Worst-Case Services and State-Based Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2108.06062.pdf)
>  In this paper, we shed new light on a classical scheduling problem: given a slot-timed, constant-capacity server, what short-run scheduling decisions must be made to provide long-run service guarantees to competing flows of unit-sized tasks? We model the flows' long-run guarantees as worst-case services that map each arrival vector recording a flow's cumulative task arrivals to a worst-case acceptable departure vector lower-bounding its cumulative task departures. We show that these services are states that can be updated as tasks arrive and depart, introduce state-based scheduling, and find the schedulability condition that must be preserved to maintain all flows' long-run guarantees. We then use this condition to identify, in each slot, all short-run scheduling decisions that preserve schedulability. To illustrate how scheduling complexity can be reduced, we additionally show that special schedules can be efficiently identified by maximizing the server's capacity slack, and that special services can be efficiently specified and updated using properties of the min-plus algebra.      
### 10.Infrared Small Target Detection Using Multi-patch Attention Network  [ :arrow_down: ](https://arxiv.org/pdf/2108.06054.pdf)
>  Infrared small target detection plays an important role in the infrared search and tracking applications. In recent years, deep learning techniques were introduced to this task and achieved noteworthy effects. Following general object segmentation methods, existing deep learning methods usually processed the image from the global view. However, the imaging locality of small targets and extreme class-imbalance between the target and background pixels were not well-considered by these deep learning methods, which causes the low-efficiency on training and high-dependence on numerous data. A multi-patch attention network (MANet) is proposed in this paper to detect small targets by jointly considering the global and local properties of infrared small target images. From the global view, a supervised attention module trained by the small target spread map is proposed to suppress most background pixels irrelevant with small target features. From the local view, local patches are split from global features and share the same convolution weights with each other in a patch net. By synthesizing the global and local properties, the data-driven framework proposed in this paper has fused multi-scale features for small target detection. Extensive synthetic and real data experiments show that the proposed method achieves the state-of-the-art performance compared with existing both conventional and deep learning methods.      
### 11.Multimodal Unrolled Robust PCA for Background Foreground Separation  [ :arrow_down: ](https://arxiv.org/pdf/2108.06031.pdf)
>  Background foreground separation (BFS) is a popular computer vision problem where dynamic foreground objects are separated from the static background of a scene. Typically, this is performed using consumer cameras because of their low cost, human interpretability, and high resolution. Yet, cameras and the BFS algorithms that process their data have common failure modes due to lighting changes, highly reflective surfaces, and occlusion. One solution is to incorporate an additional sensor modality that provides robustness to such failure modes. In this paper, we explore the ability of a cost-effective radar system to augment the popular Robust PCA technique for BFS. We apply the emerging technique of algorithm unrolling to yield real-time computation, feedforward inference, and strong generalization in comparison with traditional deep learning methods. We benchmark on the RaDICaL dataset to demonstrate both quantitative improvements of incorporating radar data and qualitative improvements that confirm robustness to common failure modes of image-based methods.      
### 12.Enhanced Accuracy Simulator for a Future Korean Nationwide eLoran System  [ :arrow_down: ](https://arxiv.org/pdf/2108.06008.pdf)
>  The Global Positioning System (GPS) has become the most widely used positioning, navigation, and timing system. However, the vulnerability of GPS to radio frequency interference has attracted significant attention. After experiencing several incidents of intentional high-power GPS jamming trials by North Korea, South Korea decided to deploy the enhanced long-range navigation (eLoran) system, which is a high-power terrestrial radio-navigation system that can complement GPS. As the first phase of the South Korean eLoran program, an eLoran testbed system was recently developed and declared operational on June 1, 2021. Once its operational performance is determined to be satisfactory, South Korea plans to move to the second phase of the program, which is a nationwide eLoran system. For the optimal deployment of additional eLoran transmitters in a nationwide system, it is necessary to properly simulate the expected positioning accuracy of the said future system. In this study, we propose enhanced eLoran accuracy simulation methods based on a land cover map and transmitter jitter estimation. Using actual measurements over the country, the simulation accuracy of the proposed methods was confirmed to be approximately 10%-91% better than that of the existing Loran (i.e., Loran-C and eLoran) positioning accuracy simulators depending on the test locations.      
### 13.Joint Spatio-Temporal Discretisation of Nonlinear Active Cochlear Models  [ :arrow_down: ](https://arxiv.org/pdf/2108.05993.pdf)
>  Biologically inspired auditory models play an important role in developing effective audio representations that can be tightly integrated into speech and audio processing systems. Current computational models of the cochlea are typically expressed in terms of systems of differential equations and do not directly lend themselves for use in computational speech processing systems. Specifically, these models are spatially discrete and temporally continuous. This paper presents a jointly discretised (spatially and temporally discrete) model of the cochlea which allows for processing at fixed time intervals suited to discrete time speech and audio processing systems. The proposed model takes into account the active feedback mechanism in the cochlea, a core characteristic lacking in traditional speech processing front-ends, which endows it with significant dynamic range compression capability. This model is derived by jointly discretising an established semi-discretised (spatially discrete and temporally continuous) cochlear model in a state space form. We then demonstrate that the proposed jointly discretised implementation matches the semi-discrete model in terms of its characteristics and finally present stability analyses of the proposed model.      
### 14.Synthesis of Static Test Environments for Observing Sequence-like Behaviors in Autonomous Systems  [ :arrow_down: ](https://arxiv.org/pdf/2108.05911.pdf)
>  In this paper, we investigate formal test-case generation for high-level mission objectives, specifically reachability, of autonomous systems. We use Kripke structures to represent the high-level decision-making of the agent under test and the abstraction of the test environment. First, we define the notion of a test specification, focusing on a fragment of linear temporal logic represented by sequence temporal logic formulas. Second, we formulate the problem of test graph synthesis to find a test configuration for which the agent must satisfy the test specification to satisfy its mission objectives. We an algorithm, based on network flows, for synthesizing a test graph by restricting transitions, represented by edge deletions, on the original graph induced by the Kripke structures. The algorithm synthesizes the test graph iteratively using an integer linear program. We prove completeness for our algorithm, and we show that the edge deletions in each iteration maintain feasibility of the integer linear program in the subsequent iteration. We formalize the notion of a minimally constrained test graph in terms of maximum flow, and prove the synthesized test graph to be minimally constrained. We demonstrate our algorithm on a simple graph and on gridworlds.      
### 15.Hierarchical Power Flow Control in Smart Grids: Enhancing Rotor Angle and Frequency Stability with Demand-Side Flexibility  [ :arrow_down: ](https://arxiv.org/pdf/2108.05898.pdf)
>  Large-scale integration of renewables in power systems gives rise to new challenges for keeping synchronization and frequency stability in volatile and uncertain power flow states. To ensure the safety of operation, the system must maintain adequate disturbance rejection capability at the time scales of both rotor angle and system frequency dynamics. This calls for flexibility to be exploited on both the generation and demand sides, compensating volatility and ensuring stability at the two separate time scales. This article proposes a hierarchical power flow control architecture that involves both transmission and distribution networks as well as individual buildings to enhance both small-signal rotor angle stability and frequency stability of the transmission network. The proposed architecture consists of a transmission-level optimizer enhancing system damping ratios, a distribution-level controller following transmission commands and providing frequency support, and a building-level scheduler accounting for quality of service and following the distribution-level targets. We validate the feasibility and performance of the whole control architecture through real-time hardware-in-loop tests involving real-world transmission and distribution network models along with real devices at the Stone Edge Farm Microgrid.      
### 16.Downlink Resource Allocation in Multiuser Cell-free MIMO Networks with User-centric Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2108.06316.pdf)
>  In this paper, we optimize user scheduling, power allocation and beamforming in distributed multiple-input multiple-output (MIMO) networks implementing user-centric clustering. We study both the coherent and non-coherent transmission modes, formulating a weighted sum rate maximization problem for each; finding the optimal solution to these problems is known to be NP-hard. We use tools from fractional programming, block coordinate descent, and compressive sensing to construct an algorithm that optimizes the beamforming weights and user scheduling and converges in a smooth non-decreasing pattern. Channel state information (CSI) being crucial for optimization, we highlight the importance of employing a low-overhead pilot assignment policy for scheduling problems. In this regard, we use a variant of hierarchical agglomerative clustering, which provides a suboptimal, but feasible, pilot assignment scheme; for our cell-free case, we formulate an area-based pilot reuse factor. Our results show that our scheme provides large gains in the long-term network sum spectral efficiency compared to benchmark schemes such as zero-forcing and conjugate beamforming (with round-robin scheduling) respectively. Furthermore, the results show the superiority of coherent transmission compared to the non-coherent mode under ideal and imperfect CSI for the area-based pilot-reuse factors we consider.      
### 17.Towards artificially intelligent recycling Improving image processing for waste classification  [ :arrow_down: ](https://arxiv.org/pdf/2108.06274.pdf)
>  The ever-increasing amount of global refuse is overwhelming the waste and recycling management industries. The need for smart systems for environmental monitoring and the enhancement of recycling processes is thus greater than ever. Amongst these efforts lies IBM's Wastenet project which aims to improve recycling by using artificial intelligence for waste classification. The work reported in this paper builds on this project through the use of transfer learning and data augmentation techniques to ameliorate classification accuracy. Starting with a convolutional neural network (CNN), a systematic approach is followed for selecting appropriate splitting ratios and for tuning multiple training parameters including learning rate schedulers, layers freezing, batch sizes and loss functions, in the context of the given scenario which requires classification of waste into different recycling types. Results are compared and contrasted using 10-fold cross validation and demonstrate that the model developed achieves a 91.21% test accuracy. Subsequently, a range of data augmentation techniques are then incorporated into this work including flipping, rotation, shearing, zooming, and brightness control. Results show that these augmentation techniques further improve the test accuracy of the final model to 95.40%. Unlike other work reported in the field, this paper provides full details regarding the training of the model. Furthermore, the code for this work has been made open-source and we have demonstrated that the model can perform successful real-time classification of recycling waste items using a standard computer webcam.      
### 18.Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.06266.pdf)
>  The last half-decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. Our review includes: learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximity to humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches.      
### 19.Hardware-Aware Beamspace Precoding for All-Digital mmWave Massive MU-MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2108.06229.pdf)
>  Massive multi-user multiple-input multiple-output (MU-MIMO) wireless systems operating at millimeter-wave (mmWave) frequencies enable simultaneous wideband data transmission to a large number of users. In order to reduce the complexity of MU precoding in all-digital basestation architectures, we propose a two-stage precoding architecture that first performs precoding using a sparse matrix in the beamspace domain, followed by an inverse fast Fourier transform that converts the result to the antenna domain. The sparse precoding matrix requires a small number of multipliers and enables regular hardware architectures, which allows the design of hardware-efficient all-digital precoders. Simulation results demonstrate that our methods approach the error-rate of conventional Wiener filter precoding with more than 2x reduced complexity.      
### 20.W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training  [ :arrow_down: ](https://arxiv.org/pdf/2108.06209.pdf)
>  Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\% to~10\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\% relatively.      
### 21.Gaze-Contingent Retinal Speckle Suppression for Perceptually-Matched Foveated Holographic Displays  [ :arrow_down: ](https://arxiv.org/pdf/2108.06192.pdf)
>  Computer-generated holographic (CGH) displays show great potential and are emerging as the next-generation displays for augmented and virtual reality, and automotive heads-up displays. One of the critical problems harming the wide adoption of such displays is the presence of speckle noise inherent to holography, that compromises its quality by introducing perceptible artifacts. Although speckle noise suppression has been an active research area, the previous works have not considered the perceptual characteristics of the Human Visual System (HVS), which receives the final displayed imagery. However, it is well studied that the sensitivity of the HVS is not uniform across the visual field, which has led to gaze-contingent rendering schemes for maximizing the perceptual quality in various computer-generated imagery. Inspired by this, we present the first method that reduces the "perceived speckle noise" by integrating foveal and peripheral vision characteristics of the HVS, along with the retinal point spread function, into the phase hologram computation. Specifically, we introduce the anatomical and statistical retinal receptor distribution into our computational hologram optimization, which places a higher priority on reducing the perceived foveal speckle noise while being adaptable to any individual's optical aberration on the retina. Our method demonstrates superior perceptual quality on our emulated holographic display. Our evaluations with objective measurements and subjective studies demonstrate a significant reduction of the human perceived noise.      
### 22.A VCSEL Array Transmission System with Novel Beam Activation Mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2108.06086.pdf)
>  Optical wireless communication (OWC) is considered to be a promising technology which will alleviate traffic burden caused by the increasing number of mobile devices. In this study, a novel vertical-cavity surface-emitting laser (VCSEL) array is proposed for indoor OWC systems. To activate the best beam for a mobile user, two beam activation methods are proposed for the system. The method based on a corner-cube retroreflector (CCR) provides very low latency and allows real-time activation for high-speed users. The other method uses the omnidirectional transmitter (ODTx). The ODTx can serve the purpose of uplink transmission and beam activation simultaneously. Moreover, systems with ODTx are very robust to the random orientation of a user equipment (UE). System level analyses are carried out for the proposed VCSEL array system. For a single user scenario, the probability density function (PDF) of the signal-to-noise ratio (SNR) for the central beam of the VCSEL array system can be approximated as a uniform distribution. In addition, the average data rate of the central beam and its upper bound are given analytically and verified by Monte-Carlo simulations. For a multi-user scenario, an analytical upper bound for the average data rate is given. The effects of the cell size and the full width at half maximum (FWHM) angle on the system performance are studied. The results show that the system with a FWHM angle of $4^\circ$ outperforms the others.      
### 23.Machine Learning Based Parameter Estimation of Gaussian Quantum States  [ :arrow_down: ](https://arxiv.org/pdf/2108.06061.pdf)
>  We propose a machine learning framework for parameter estimation of single mode Gaussian quantum states. Under a Bayesian framework, our approach estimates parameters of suitable prior distributions from measured data. For phase-space displacement and squeezing parameter estimation, this is achieved by introducing Expectation-Maximization (EM) based algorithms, while for phase parameter estimation an empirical Bayes method is applied. The estimated prior distribution parameters along with the observed data are used for finding the optimal Bayesian estimate of the unknown displacement, squeezing and phase parameters. Our simulation results show that the proposed algorithms have estimation performance that is very close to that of Genie Aided Bayesian estimators, that assume perfect knowledge of the prior parameters. Our proposed methods can be utilized by experimentalists to find the optimum Bayesian estimate of parameters of Gaussian quantum states by using only the observed measurements without requiring any knowledge about the prior distribution parameters.      
### 24.Interference Mitigation using Optimized Angle Diversity Receiver in LiFi Cellular network  [ :arrow_down: ](https://arxiv.org/pdf/2108.06025.pdf)
>  Light-fidelity (LiFi) is an emerging technology for high-speed short-range mobile communications. Inter-cell interference (ICI) is an important issue that limits the system performance in an optical attocell network. Angle diversity receivers (ADRs) have been proposed to mitigate ICI. In this paper, the structure of pyramid receivers (PRs) and truncated pyramid receivers (TPRs) are studied. The coverage problems of PRs and TPRs are defined and investigated, and the lower bound of field of view (FOV) for each PD is given analytically. The impact of random device orientation and diffuse link signal propagation are taken into consideration. The performances of PRs and TPRs are compared and then optimized ADR structures are proposed. The performance comparison between the select best combining (SBC) and maximum ratio combining (MRC) is given under different noise levels. It is shown that SBC will outperform MRC in an interference limited system, otherwise, MRC is a preferred scheme. In addition, the double source system, where each LiFi AP consists of two sources transmitting the same information signals but with opposite polarity, is proved to outperform the single source (SS) system under certain conditions.      
### 25.Screenline-based Two-step Calibration and its application to an agent-based urban freight simulator  [ :arrow_down: ](https://arxiv.org/pdf/2108.05995.pdf)
>  Calibration is an essential process to make an agent-based simulator operational. Especially, the calibration for freight demand is challenging due to the model complexity and the shortage of available freight demand data compared with passenger data. This paper proposes a novel calibration method that relies solely on screenline counts, named Screenline-based Two-step Calibration (SLTC). SLTC consists of two parts: (1) tour-based demand adjustment and (2) model parameter updates. The former generates screenline-based tours by cloning/removing instances of the simulated goods vehicle tours, aiming to minimize the gaps between the observed and the simulated screenline counts. The latter updates the parameters of the commodity flow model which generates inputs to simulate goods vehicle tours. To demonstrate the practicality of the proposed method, we apply it to an agent-based urban freight simulator, SimMobility Freight. The result shows that SLTC allows the simulator to replicate the observed screenline counts with reasonable computational cost for calibration.      
### 26.A Systematic Benchmarking Analysis of Transfer Learning for Medical Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2108.05930.pdf)
>  Transfer learning from supervised ImageNet models has been frequently used in medical image analysis. Yet, no large-scale evaluation has been conducted to benchmark the efficacy of newly-developed pre-training techniques for medical image analysis, leaving several important questions unanswered. As the first step in this direction, we conduct a systematic study on the transferability of models pre-trained on iNat2021, the most recent large-scale fine-grained dataset, and 14 top self-supervised ImageNet models on 7 diverse medical tasks in comparison with the supervised ImageNet model. Furthermore, we present a practical approach to bridge the domain gap between natural and medical images by continually (pre-)training supervised ImageNet models on medical images. Our comprehensive evaluation yields new insights: (1) pre-trained models on fine-grained data yield distinctive local representations that are more suitable for medical segmentation tasks, (2) self-supervised ImageNet models learn holistic features more effectively than supervised ImageNet models, and (3) continual pre-training can bridge the domain gap between natural and medical images. We hope that this large-scale open evaluation of transfer learning can direct the future research of deep learning for medical imaging. As open science, all codes and pre-trained models are available on our GitHub page <a class="link-external link-https" href="https://github.com/JLiangLab/BenchmarkTransferLearning" rel="external noopener nofollow">this https URL</a>.      
### 27.Parameter Tuning of Time-Frequency Masking Algorithms for Reverberant Artifact Removal within the Cochlear Implant Stimulus  [ :arrow_down: ](https://arxiv.org/pdf/2108.05929.pdf)
>  Cochlear implant users struggle to understand speech in reverberant environments. To restore speech perception, artifacts dominated by reverberant reflections can be removed from the cochlear implant stimulus. Artifacts can be identified and removed by applying a matrix of gain values, a technique referred to as time-frequency masking. Gain values are determined by an oracle algorithm that uses knowledge of the undistorted signal to minimize retention of the signal components dominated by reverberant reflections. In practice, gain values are estimated from the distorted signal, with the oracle algorithm providing the estimation objective. Different oracle techniques exist for determining gain values, and each technique must be parameterized to set the amount of signal retention. This work assesses which oracle masking strategies and parameterizations lead to the best improvements in speech intelligibility for cochlear implant users in reverberant conditions using online speech intelligibility testing of normal-hearing individuals with vocoding.      
### 28.Improving Music Performance Assessment with Contrastive Learning  [ :arrow_down: ](https://arxiv.org/pdf/2108.01711.pdf)
>  Several automatic approaches for objective music performance assessment (MPA) have been proposed in the past, however, existing systems are not yet capable of reliably predicting ratings with the same accuracy as professional judges. This study investigates contrastive learning as a potential method to improve existing MPA systems. Contrastive learning is a widely used technique in representation learning to learn a structured latent space capable of separately clustering multiple classes. It has been shown to produce state of the art results for image-based classification problems. We introduce a weighted contrastive loss suitable for regression tasks applied to a convolutional neural network and show that contrastive loss results in performance gains in regression tasks for MPA. Our results show that contrastive-based methods are able to match and exceed SoTA performance for MPA regression tasks by creating better class clusters within the latent space of the neural networks.      
