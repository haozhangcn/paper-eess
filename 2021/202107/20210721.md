# ArXiv eess --Wed, 21 Jul 2021
### 1.A Novel Approach to Vehicle Pose Estimation using Automotive Radar  [ :arrow_down: ](https://arxiv.org/pdf/2107.09607.pdf)
>  This paper presents a set of novel scan-matching techniques for vehicle pose estimation using automotive radar measurements. The proposed approach modifies the Normal Distributions Transform (NDT) -- a state-of-the-art scan-matching SLAM technique, widely used in lidar-based localization -- to account for particular aspects of radar environment perception. First, the polar NDT (PNDT) is introduced by solving the NDT problem in the polar coordinate system, natural for radar measurements. A better agreement between the measurement uncertainties and their representation in the scan-matching algorithm is achieved. Second, the extension of PNDT to take into account the Doppler measurements -- Doppler polar NDT (DPNDT) -- is proposed. Third, the SNR of detected targets is added to the optimization procedure to minimize the impact of RCS fluctuation. The improvement over the conventional NDT is demonstrated in numerical simulations and real data processing, showing the ability to decrease the localization error by a factor of 3 to 5, depending on the scenario, with a negligible increase in computational complexity. Finally, a DPNDT extension with the capability to compensate angular bias in array beam-forming is presented. Simulation results and real data processing show the possibility to correct it with the accuracy of $0.1^\circ$ almost in real-time.      
### 2.Medical Imaging with Deep Learning for COVID- 19 Diagnosis: A Comprehensive Review  [ :arrow_down: ](https://arxiv.org/pdf/2107.09602.pdf)
>  The outbreak of novel coronavirus disease (COVID- 19) has claimed millions of lives and has affected all aspects of human life. This paper focuses on the application of deep learning (DL) models to medical imaging and drug discovery for managing COVID-19 disease. In this article, we detail various medical imaging-based studies such as X-rays and computed tomography (CT) images along with DL methods for classifying COVID-19 affected versus pneumonia. The applications of DL techniques to medical images are further described in terms of image localization, segmentation, registration, and classification leading to COVID-19 detection. The reviews of recent papers indicate that the highest classification accuracy of 99.80% is obtained when InstaCovNet-19 DL method is applied to an X-ray dataset of 361 COVID-19 patients, 362 pneumonia patients and 365 normal people. Furthermore, it can be seen that the best classification accuracy of 99.054% can be achieved when EDL_COVID DL method is applied to a CT image dataset of 7500 samples where COVID-19 patients, lung tumor patients and normal people are equal in number. Moreover, we illustrate the potential DL techniques in drug or vaccine discovery in combating the coronavirus. Finally, we address a number of problems, concerns and future research directions relevant to DL applications for COVID-19.      
### 3.Accelerating Edge Intelligence via Integrated Sensing and Communication  [ :arrow_down: ](https://arxiv.org/pdf/2107.09574.pdf)
>  Realizing edge intelligence consists of sensing, communication, training, and inference stages. Conventionally, the sensing and communication stages are executed sequentially, which results in excessive amount of dataset generation and uploading time. This paper proposes to accelerate edge intelligence via integrated sensing and communication (ISAC). As such, the sensing and communication stages are merged so as to make the best use of the wireless signals for the dual purpose of dataset generation and uploading. However, ISAC also introduces additional interference between sensing and communication functionalities. To address this challenge, this paper proposes a classification error minimization formulation to design the ISAC beamforming and time allocation. Globally optimal solution is derived via the rank-1 guaranteed semidefinite relaxation, and performance analysis is performed to quantify the ISAC gain. Simulation results are provided to verify the effectiveness of the proposed ISAC scheme. Interestingly, it is found that when the sensing time dominates the communication time, ISAC is always beneficial. However, when the communication time dominates, the edge intelligence with ISAC scheme may not be better than that with the conventional scheme, since ISAC introduces harmful interference between the sensing and communication signals.      
### 4.Explicit Calibration of mmWave Phased Arrays with Phase Dependent Errors  [ :arrow_down: ](https://arxiv.org/pdf/2107.09561.pdf)
>  We consider an error model for phased array with gain errors and phase errors, with errors dependent on the phase applied and the antenna index. Under this model, we propose an algorithm for measuring the errors by selectively turning on the antennas at specific phases and measuring the transmitted power. In our algorithm, the antennas are turned on individually and then pairwise for the measurements, and rotation of the phased array is not required. We give numerical results to measure the accuracy of the algorithm as a function of the signal-to-noise ratio in the measurement setup. We also compare the performance of our algorithm with the traditional rotating electric vector (REV) method and observe the superiority of our algorithm. Simulations also demonstrate an improvement in the coverage on comparing the cumulative distribution function (CDF) of equivalent isotropically radiated power (EIRP) before and after calibration.      
### 5.SynthSeg: Domain Randomisation for Segmentation of Brain MRI Scans of any Contrast and Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2107.09559.pdf)
>  Despite advances in data augmentation and transfer learning, convolutional neural networks (CNNs) have difficulties generalising to unseen target domains. When applied to segmentation of brain MRI scans, CNNs are highly sensitive to changes in resolution and contrast: even within the same MR modality, decreases in performance can be observed across datasets. We introduce SynthSeg, the first segmentation CNN agnostic to brain MRI scans of any contrast and resolution. SynthSeg is trained with synthetic data sampled from a generative model inspired by Bayesian segmentation. Crucially, we adopt a \textit{domain randomisation} strategy where we fully randomise the generation parameters to maximise the variability of the training data. Consequently, SynthSeg can segment preprocessed and unpreprocessed real scans of any target domain, without retraining or fine-tuning. Because SynthSeg only requires segmentations to be trained (no images), it can learn from label maps obtained automatically from existing datasets of different populations (e.g., with atrophy and lesions), thus achieving robustness to a wide range of morphological variability. We demonstrate SynthSeg on 5,500 scans of 6 modalities and 10 resolutions, where it exhibits unparalleled generalisation compared to supervised CNNs, test time adaptation, and Bayesian segmentation. The code and trained model are available at <a class="link-external link-https" href="https://github.com/BBillot/SynthSeg" rel="external noopener nofollow">this https URL</a>.      
### 6.A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2107.09543.pdf)
>  Despite technological and medical advances, the detection, interpretation, and treatment of cancer based on imaging data continue to pose significant challenges. These include high inter-observer variability, difficulty of small-sized lesion detection, nodule interpretation and malignancy determination, inter- and intra-tumour heterogeneity, class imbalance, segmentation inaccuracies, and treatment effect uncertainty. The recent advancements in Generative Adversarial Networks (GANs) in computer vision as well as in medical imaging may provide a basis for enhanced capabilities in cancer detection and analysis. In this review, we assess the potential of GANs to address a number of key challenges of cancer imaging, including data scarcity and imbalance, domain and dataset shifts, data access and privacy, data annotation and quantification, as well as cancer detection, tumour profiling and treatment planning. We provide a critical appraisal of the existing literature of GANs applied to cancer imagery, together with suggestions on future research directions to address these challenges. We analyse and discuss 163 papers that apply adversarial training techniques in the context of cancer imaging and elaborate their methodologies, advantages and limitations. With this work, we strive to bridge the gap between the needs of the clinical cancer imaging community and the current and prospective research on GANs in the artificial intelligence community.      
### 7.ProfileSR-GAN: A GAN based Super-Resolution Method for Generating High-Resolution Load Profiles  [ :arrow_down: ](https://arxiv.org/pdf/2107.09523.pdf)
>  It is a common practice for utilities to down-sample smart meter measurements from high resolution (e.g. 1-min or 1-sec) to low resolution (e.g. 15-, 30- or 60-min) to lower the data transmission and storage cost. However, down-sampling can remove high-frequency components from time-series load profiles, making them unsuitable for in-depth studies such as quasi-static power flow analysis or non-intrusive load monitoring (NILM). Thus, in this paper, we propose ProfileSR-GAN: a Generative Adversarial Network (GAN) based load profile super-resolution (LPSR) framework for restoring high-frequency components lost through the smoothing effect of the down-sampling process. The LPSR problem is formulated as a Maximum-a-Prior problem. When training the ProfileSR-GAN generator network, to make the generated profiles more realistic, we introduce two new shape-related losses in addition to conventionally used content loss: adversarial loss and feature-matching loss. Moreover, a new set of shape-based evaluation metrics are proposed to evaluate the realisticness of the generated profiles. Simulation results show that ProfileSR-GAN outperforms Mean-Square Loss based methods in all shape-based metrics. The successful application in NILM further demonstrates that ProfileSR-GAN is effective in recovering high-resolution realistic waveforms.      
### 8.On the Design of Complex EM Devices and Systems through the System-by-Design Paradigm -- A Framework for Dealing with the Computational Complexity  [ :arrow_down: ](https://arxiv.org/pdf/2107.09521.pdf)
>  The System-by-Design (SbD) is an emerging engineering framework for the optimization-driven design of complex electromagnetic (EM) devices and systems. More specifically, the computational complexity of the design problem at hand is addressed by means of a suitable selection and integration of functional blocks comprising problem-dependent and computationally-efficient modeling and analysis tools as well as reliable prediction and optimization strategies. Thanks to the suitable re-formulation of the problem at hand as an optimization one, the profitable minimum-size coding of the degrees-of-freedom (DoFs), the "smart" replacement of expensive full-wave (FW) simulators with proper surrogate models (SMs), which yield fast yet accurate predictions starting from minimum size/reduced CPU-costs training sets, a favorable "environment" for an optimal exploitation of the features of global optimization tools in sampling wide/complex/nonlinear solution spaces is built. This research summary is then aimed at (i) providing a comprehensive description of the SbD framework and of its pillar concepts and strategies, (ii) giving useful guidelines for its successful customization and application to different EM design problems characterized by different levels of computational complexity, (iii) envisaging future trends and advances in this fascinating and high-interest (because of its relevant and topical industrial and commercial implications) topic. Representative benchmarks concerned with the synthesis of complex EM systems are presented to highlight advantages and potentialities as well as current limitations of the SbD paradigm.      
### 9.Probability density evolution filter  [ :arrow_down: ](https://arxiv.org/pdf/2107.09514.pdf)
>  Based on probability density evolution method (PDEM) and Bayes law, a new filter strategy is proposed, in which the prior probability of system state of interest is predicted by solving the general density evolution equation (GDEE), the posterior probability of system state is then updated in terms of Bayes formula. Furthermore, a Chebyshev polynomial-based collocation method is employed to obtain numerical solutions of the prior probability. An illustrative example is finally presented to validate the probability density evolution filter (PDEF) in comparison to particle filter (PF) and UKF. Overall, PDEF exhibits accuracy close to PF without any resampling algorithm.      
### 10.Channel Performance Estimations with Extended Channel Probing  [ :arrow_down: ](https://arxiv.org/pdf/2107.09513.pdf)
>  We test the concept of extended channel probing in an Optical Spectrum as a Service scenario in coherent optimized flex-grid long-haul and 10Gbit/s OOK optimized 100-GHz fixed-grid dispersion-managed legacy DWDM networks. An estimation accuracy better than +/- 0.1dB in GSNR implementation margin is obtained for both networks by using flexible coherent transceivers on lightpaths up to 822km on regional haul legacy network and up to 5738km on a long-haul network. We also explain, how to detect the operation regime of the channel by analyzing the received GSNR results from constant power spectral density (PSD) probing mode and constant signal power probing mode      
### 11.An Information Theory Approach to Physical Domain Discovery  [ :arrow_down: ](https://arxiv.org/pdf/2107.09511.pdf)
>  The project of physics discovery is often equivalent to finding the most concise description of a physical system. The description with optimum predictive capability for a dataset generated by a physical system is one that minimizes both predictive error on the dataset and the complexity of the description. The discovery of the governing physics of a system can therefore be viewed as a mathematical optimization problem. We outline here a method to optimize the description of arbitrarily complex physical systems by minimizing the entropy of the description of the system. The Recursive Domain Partitioning (RDP) procedure finds the optimum partitioning of each physical domain into subdomains, and the optimum predictive function within each subdomain. Penalty functions are introduced to limit the complexity of the predictive function within each domain. Examples are shown in 1D and 2D. In 1D, the technique effectively discovers the elastic and plastic regions within a stress-strain curve generated by simulations of amorphous carbon material, while in 2D the technique discovers the free-flow region and the inertially-obstructed flow region in the simulation of fluid flow across a plate.      
### 12.Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild  [ :arrow_down: ](https://arxiv.org/pdf/2107.09510.pdf)
>  Multimodal wearable physiological data in daily life settings have been used to estimate self-reported stress labels.However, missing data modalities in data collection make it challenging to leverage all the collected samples. Besides, heterogeneous sensor data and labels among individuals add challenges in building robust stress detection models. In this paper, we proposed a modality fusion network (MFN) to train models and infer self-reported binary stress labels under both complete and incomplete modality condition. In addition, we applied a personalized attention (PA) strategy to leverage personalized representation along with the generalized one-size-fits-all model. We evaluated our methods on a multimodal wearable sensor dataset (N=41) including galvanic skin response (GSR) and electrocardiogram (ECG). Compared to the baseline method using the samples with complete modalities, the performance of the MFN improved by 1.6\% in f1-scores. On the other hand, the proposed PA strategy showed a 2.3\% higher stress detection f1-score and approximately up to 70\% reduction in personalized model parameter size (9.1 MB) compared to the previous state-of-the-art transfer learning strategy (29.3 MB).      
### 13.Wearable Health Monitoring System for Older Adults in a Smart Home Environment  [ :arrow_down: ](https://arxiv.org/pdf/2107.09509.pdf)
>  The advent of IoT has enabled the design of connected and integrated smart health monitoring systems. These smart health monitoring systems could be realized in a smart home context to render long-term care to the elderly population. In this paper, we present the design of a wearable health monitoring system suitable for older adults in a smart home context. The proposed system offers solutions to monitor the stress, blood pressure, and location of an individual within a smart home environment. The stress detection model proposed in this work uses Electrodermal Activity (EDA), Photoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart wristband for detecting physiological stress. The stress detection model is trained and tested using stress labels obtained from salivary cortisol which is a clinically established biomarker for physiological stress. A voice-based prototype is also implemented and the feasibility of the proposed system for integration in a smart home environment is analyzed by simulating a data acquisition and streaming scenario. We have also proposed a blood pressure estimation model using PPG signal and advanced regression techniques for integration with the stress detection model in the wearable health monitoring system. Finally, the design of a voice-assisted indoor location system is proposed for integration with the proposed system within a smart home environment. The proposed wearable health monitoring system is an important direction to realize a smart home environment with extensive diagnostic capabilities so that such a system could be useful for rendering long-term and personalized care to the aging population in the comfort of their home.      
### 14.EEG-based Cross-Subject Driver Drowsiness Recognition with Interpretable CNN  [ :arrow_down: ](https://arxiv.org/pdf/2107.09507.pdf)
>  In the context of electroencephalogram (EEG)-based driver drowsiness recognition, it is still a challenging task to design a calibration-free system, since there exists a significant variability of EEG signals among different subjects and recording sessions. As deep learning has received much research attention in recent years, many efforts have been made to use deep learning methods for EEG signal recognition. However, existing works mostly treat deep learning models as blackbox classifiers, while what have been learned by the models and to which extent they are affected by the noise from EEG data are still underexplored. In this paper, we develop a novel convolutional neural network that can explain its decision by highlighting the local areas of the input sample that contain important information for the classification. The network has a compact structure for ease of interpretation and takes advantage of separable convolutions to process the EEG signals in a spatial-temporal sequence. Results show that the model achieves an average accuracy of 78.35% on 11 subjects for leave-one-out cross-subject drowsiness recognition, which is higher than the conventional baseline methods of 53.4%-72.68% and state-of-art deep learning methods of 63.90%-65.61%. Visualization results show that the model has learned to recognize biologically explainable features from EEG signals, e.g., Alpha spindles, as strong indicators of drowsiness across different subjects. In addition, we also explore reasons behind some wrongly classified samples and how the model is affected by artifacts and noise in the data. Our work illustrates a promising direction on using interpretable deep learning models to discover meaning patterns related to different mental states from complex EEG signals.      
### 15.Supply Chain Digital Twin Framework Design: An Approach of Supply Chain Operations Reference Model and System of Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.09485.pdf)
>  Digital twin technology has been regarded as a beneficial approach in supply chain development. Different from traditional digital twin (temporal dynamic), supply chain digital twin is a spatio-temporal dynamic system. This paper explains what is 'twined' in supply chain digital twin and how to 'twin' them to handle the spatio-temporal dynamic issue. A supply chain digital twin framework is developed based on the theories of system of systems and supply chain operations reference model. This framework is universal and can be applied in various types of supply chain systems. We firstly decompose the supply chain system into unified standard blocks preparing for the adoption of digital twin. Next, the idea of supply chain operations reference model is adopted to digitise basic supply chain activities within each block and explain how to use existing information system. Then, individual sub-digital twin is established for each member in supply chain system. After that, we apply the concept of system of systems to integrate and coordinate sub-digital twin into supply chain digital twin from the views of supply chain business integration and information system integration. At last, one simple supply chain system is applied to illustrate the application of the proposed model.      
### 16.Automated Segmentation and Volume Measurement of Intracranial Carotid Artery Calcification on Non-Contrast CT  [ :arrow_down: ](https://arxiv.org/pdf/2107.09442.pdf)
>  Purpose: To evaluate a fully-automated deep-learning-based method for assessment of intracranial carotid artery calcification (ICAC). Methods: Two observers manually delineated ICAC in non-contrast CT scans of 2,319 participants (mean age 69 (SD 7) years; 1154 women) of the Rotterdam Study, prospectively collected between 2003 and 2006. These data were used to retrospectively develop and validate a deep-learning-based method for automated ICAC delineation and volume measurement. To evaluate the method, we compared manual and automatic assessment (computed using ten-fold cross-validation) with respect to 1) the agreement with an independent observer's assessment (available in a random subset of 47 scans); 2) the accuracy in delineating ICAC as judged via blinded visual comparison by an expert; 3) the association with first stroke incidence from the scan date until 2012. All method performance metrics were computed using 10-fold cross-validation. Results: The automated delineation of ICAC reached sensitivity of 83.8% and positive predictive value (PPV) of 88%. The intraclass correlation between automatic and manual ICAC volume measures was 0.98 (95% CI: 0.97, 0.98; computed in the entire dataset). Measured between the assessments of independent observers, sensitivity was 73.9%, PPV was 89.5%, and intraclass correlation was 0.91 (95% CI: 0.84, 0.95; computed in the 47-scan subset). In the blinded visual comparisons, automatic delineations were more accurate than manual ones (p-value = 0.01). The association of ICAC volume with incident stroke was similarly strong for both automated (hazard ratio, 1.38 (95% CI: 1.12, 1.75) and manually measured volumes (hazard ratio, 1.48 (95% CI: 1.20, 1.87)). Conclusions: The developed model was capable of automated segmentation and volume quantification of ICAC with accuracy comparable to human experts.      
### 17.Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models  [ :arrow_down: ](https://arxiv.org/pdf/2107.09428.pdf)
>  Non-autoregressive (NAR) modeling has gained more and more attention in speech processing. With recent state-of-the-art attention-based automatic speech recognition (ASR) structure, NAR can realize promising real-time factor (RTF) improvement with only small degradation of accuracy compared to the autoregressive (AR) models. However, the recognition inference needs to wait for the completion of a full speech utterance, which limits their applications on low latency scenarios. To address this issue, we propose a novel end-to-end streaming NAR speech recognition system by combining blockwise-attention and connectionist temporal classification with mask-predict (Mask-CTC) NAR. During inference, the input audio is separated into small blocks and then processed in a blockwise streaming way. To address the insertion and deletion error at the edge of the output of each block, we apply an overlapping decoding strategy with a dynamic mapping trick that can produce more coherent sentences. Experimental results show that the proposed method improves online ASR recognition in low latency conditions compared to vanilla Mask-CTC. Moreover, it can achieve a much faster inference speed compared to the AR attention-based models. All of our codes will be publicly available at <a class="link-external link-https" href="https://github.com/espnet/espnet" rel="external noopener nofollow">this https URL</a>.      
### 18.DeepSMILE: Self-supervised heterogeneity-aware multiple instance learning for DNA damage response defect classification directly from H&amp;E whole-slide images  [ :arrow_down: ](https://arxiv.org/pdf/2107.09405.pdf)
>  We propose a Deep learning-based weak label learning method for analysing whole slide images (WSIs) of Hematoxylin and Eosin (H&amp;E) stained tumorcells not requiring pixel-level or tile-level annotations using Self-supervised pre-training and heterogeneity-aware deep Multiple Instance LEarning (DeepSMILE). We apply DeepSMILE to the task of Homologous recombination deficiency (HRD) and microsatellite instability (MSI) prediction. We utilize contrastive self-supervised learning to pre-train a feature extractor on histopathology tiles of cancer tissue. Additionally, we use variability-aware deep multiple instance learning to learn the tile feature aggregation function while modeling tumor heterogeneity. Compared to state-of-the-art genomic label classification methods, DeepSMILE improves classification performance for HRD from $70.43\pm4.10\%$ to $83.79\pm1.25\%$ AUC and MSI from $78.56\pm6.24\%$ to $90.32\pm3.58\%$ AUC in a multi-center breast and colorectal cancer dataset, respectively. These improvements suggest we can improve genomic label classification performance without collecting larger datasets. In the future, this may reduce the need for expensive genome sequencing techniques, provide personalized therapy recommendations based on widely available WSIs of cancer tissue, and improve patient care with quicker treatment decisions - also in medical centers without access to genome sequencing resources.      
### 19.SVSNet: An End-to-end Speaker Voice Similarity Assessment Model  [ :arrow_down: ](https://arxiv.org/pdf/2107.09392.pdf)
>  Neural evaluation metrics derived for numerous speech generation tasks have recently attracted great attention. In this paper, we propose SVSNet, the first end-to-end neural network model to assess the speaker voice similarity between natural speech and synthesized speech. Unlike most neural evaluation metrics that use hand-crafted features, SVSNet directly takes the raw waveform as input to more completely utilize speech information for prediction. SVSNet consists of encoder, co-attention, distance calculation, and prediction modules and is trained in an end-to-end manner. The experimental results on the Voice Conversion Challenge 2018 and 2020 (VCC2018 and VCC2020) datasets show that SVSNet notably outperforms well-known baseline systems in the assessment of speaker similarity at the utterance and system levels.      
### 20.Protecting Semantic Segmentation Models by Using Block-wise Image Encryption with Secret Key from Unauthorized Access  [ :arrow_down: ](https://arxiv.org/pdf/2107.09362.pdf)
>  Since production-level trained deep neural networks (DNNs) are of a great business value, protecting such DNN models against copyright infringement and unauthorized access is in a rising demand. However, conventional model protection methods focused only the image classification task, and these protection methods were never applied to semantic segmentation although it has an increasing number of applications. In this paper, we propose to protect semantic segmentation models from unauthorized access by utilizing block-wise transformation with a secret key for the first time. Protected models are trained by using transformed images. Experiment results show that the proposed protection method allows rightful users with the correct key to access the model to full capacity and deteriorate the performance for unauthorized users. However, protected models slightly drop the segmentation performance compared to non-protected models.      
### 21.Neural-Network-based NLOS Identification in Angular Domain at 60-GHz  [ :arrow_down: ](https://arxiv.org/pdf/2107.09343.pdf)
>  This paper introduces an identification method that determines whether a millimeter-wave wireless transmission using directional antennas is being established over a line-of-sight (LOS) or a non-line-of-sight (NLOS) cluster for indoor localization applications. The proposed technique utilizes the channel power angular spectrum that is readily available from a beam training process. In particular, the behavior of five different channel metrics, namely the spatial-domain, time-domain, and frequency-domain channel kurtosis, the mean excess delay, and the RMS delay spread, is analyzed using maximum likelihood ratio and artificial neural network. A noticeable difference between LOS and NLOS clusters is observed and assessed for identification. Hypothesis testing shows errors as low as 0.01-0.02 in simulation and 0.04-0.07 in measurements at 60 GHz.      
### 22.Collaborative rover-copter path planning and exploration with temporal logic specifications based on Bayesian update under uncertain environments  [ :arrow_down: ](https://arxiv.org/pdf/2107.09303.pdf)
>  This paper investigates a collaborative rover-copter path planning and exploration with temporal logic specifications under uncertain environments. The objective of the rover is to complete a mission expressed by a syntactically co-safe linear temporal logic (scLTL) formula, while the objective of the copter is to actively explore the environment and reduce its uncertainties, aiming at assisting the rover and enhancing the efficiency of the mission completion. To formalize our approach, we first capture the environmental uncertainties by environmental beliefs of the atomic propositions, under an assumption that it is unknown which properties (or, atomic propositions) are satisfied in each area of the environment. The environmental beliefs of the atomic propositions are updated according to the Bayes rule based on the Bernoulli-type sensor measurements provided by both the rover and the copter. Then, the optimal policy for the rover is synthesized by maximizing a belief of the satisfaction of the scLTL formula through an implementation of an automata-based model checking. An exploration policy for the copter is then synthesized by employing the notion of an entropy that is evaluated based on the environmental beliefs of the atomic propositions, and a path that the rover intends to follow according to the optimal policy. As such, the copter can actively explore regions whose uncertainties are high and that are relevant to the mission completion. Finally, some numerical examples illustrate the effectiveness of the proposed approach.      
### 23.High-Power and High-Capacity Mobile Optical SWIPT  [ :arrow_down: ](https://arxiv.org/pdf/2107.09299.pdf)
>  The increasing demands of power supply and data rate for mobile devices promote the research of simultaneous information and power transfer (SWIPT). Optical SWIPT, as known as simultaneous light information and power transfer (SLIPT), can provide high-capacity communication and high-power charging. However, light emitting diodes (LEDs)-based SLIPT technologies have low efficiency due to energy dissipation over the air. Laser-based SLIPT technologies face the challenge in mobility, as it needs accurate positioning, fast beam steering, and real-time tracking. In this paper, we propose a mobile SLIPT scheme based on spatially separated laser resonator (SSLR) and intra-cavity second harmonic generation (SHG). The power and data are transferred via separated frequencies, while they share the same self-aligned resonant beam path, without the needs of receiver positioning and beam steering. We establish the analysis model of the resonant beam power and its second harmonic power. We also evaluate the system performance on deliverable power and channel capacity. Numerical results show that the proposed system can achieve watt-level battery charging power and above 20-bit/s/Hz communication capacity over 8-m distance, which satisfies the requirements of most indoor mobile devices.      
### 24.Cluster Consensus on Matrix-weighted Switching Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.09292.pdf)
>  This paper examines the cluster consensus problem of multi-agent systems on matrix-weighted switching networks. Necessary and/or sufficient conditions under which cluster consensus can be achieved are obtained and quantitative characterization of the steady-state of the cluster consensus are provided as well. Specifically, if the underlying network switches amongst finite number of networks, a necessary condition for cluster consensus of multi-agent system on switching matrix-weighted networks is firstly presented, it is shown that the steady-state of the system lies in the intersection of the null space of matrix-valued Laplacians corresponding to all switching networks. Second, if the underlying network switches amongst infinite number of networks, the matrix-weighted integral network is employed to provide sufficient conditions for cluster consensus and the quantitative characterization of the corresponding steady-state of the multi-agent system, using null space analysis of matrix-valued Laplacian related of integral network associated with the switching networks. In particular, conditions for the bipartite consensus under the matrix-weighted switching networks are examined. Simulation results are finally provided to demonstrate the theoretical analysis.      
### 25.Application of Terminal Region Enlargement Approach for Discrete Time Quasi Infinite Horizon NMPC  [ :arrow_down: ](https://arxiv.org/pdf/2107.09267.pdf)
>  Ensuring nominal asymptotic stability of the Non-linear Model Predictive Control (NMPC) controller is not trivial. Stabilizing ingredients such as terminal penalty term and terminal region are crucial in establishing the asymptotic stability. Approaches available in the literature provide limited degrees of freedom for the characterization of the terminal region for the discrete time Quasi Infinite Horizon NMPC (QIH-NMPC) formulation. Current work presents alternate approaches namely arbitrary controller based approach and LQR based approach, which provide large degrees of freedom for enlarging the terminal region. Both the approaches are scalable to system of any state and input dimension. Approach from the literature provides a scalar whereas proposed approaches provide a linear controller and two additive matrices as tuning parameters for shaping of the terminal region. Proposed approaches involve solving modified Lyapunov equations to compute terminal penalty term, followed by explicit characterization of the terminal region. Efficacy of the proposed approaches is demonstrated using benchmark two state system. Terminal region obtained using the arbitrary controller based approach and LQR based approach are approximately 10.4723 and 9.5055 times larger by area measure when compared to the largest terminal region obtained using the approach from the literature.      
### 26.Location-aware Channel Estimation for RIS-aided mmWave MIMO Systems via Atomic Norm Minimization  [ :arrow_down: ](https://arxiv.org/pdf/2107.09222.pdf)
>  In this paper, we propose a location-aware channel estimation based on the atomic norm minimization (ANM) for the reconfigurable intelligent surface (RIS)-aided millimeter-wave multiple-input-multiple-output (MIMO) systems. The beam training overhead at the base station (BS) is reduced by the direct beam steering towards the RIS with the location of the BS and the RIS. The RIS beamwidth adaptation is proposed to reduce the beam training overhead at the RIS, and also it enables accurate channel estimation by ensuring the user equipment receives all the multipath components from the RIS. After the beam training, the cascaded effective channel of the RIS-aided MIMO systems is estimated by ANM. Depending on whether the beam training overhead at the BS or at the RIS is reduced or not, the channel is represented as a linear combination of either 1D atoms, 2D atoms, or 3D atoms, and the ANM is applied to estimate the channel. Simulation results show that the proposed location-aware channel estimation via 2D ANM and 3D ANM achieves superior estimation accuracy to benchmarks.      
### 27.Atomic Norm Minimization-based Low-Overhead Channel Estimation for RIS-aided MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.09216.pdf)
>  Large beam training overhead has been considered as one of main issues in the channel estimation for reconfigurable intelligent surface (RIS)-aided systems. In this paper, we propose an atomic norm minimization (ANM)-based low-overhead channel estimation for RIS-aided multiple-input-multiple-output (MIMO) systems. When the number of beam training is reduced, some multipath signals may not be received during beam training, and this causes channel estimation failure. To solve this issue, the width of beams created by RIS is widened to capture all multipath signals. Pilot signals received during beam training are compiled into one matrix to define the atomic norm of the channel for RIS-aided MIMO systems. Simulation results show that the proposed algorithm outperforms other channel estimation algorithms.      
### 28.OSLO: On-the-Sphere Learning for Omnidirectional images and its application to 360-degree image compression  [ :arrow_down: ](https://arxiv.org/pdf/2107.09179.pdf)
>  State-of-the-art 2D image compression schemes rely on the power of convolutional neural networks (CNNs). Although CNNs offer promising perspectives for 2D image compression, extending such models to omnidirectional images is not straightforward. First, omnidirectional images have specific spatial and statistical properties that can not be fully captured by current CNN models. Second, basic mathematical operations composing a CNN architecture, e.g., translation and sampling, are not well-defined on the sphere. In this paper, we study the learning of representation models for omnidirectional images and propose to use the properties of HEALPix uniform sampling of the sphere to redefine the mathematical tools used in deep learning models for omnidirectional images. In particular, we: i) propose the definition of a new convolution operation on the sphere that keeps the high expressiveness and the low complexity of a classical 2D convolution; ii) adapt standard CNN techniques such as stride, iterative aggregation, and pixel shuffling to the spherical domain; and then iii) apply our new framework to the task of omnidirectional image compression. Our experiments show that our proposed on-the-sphere solution leads to a better compression gain that can save 13.7% of the bit rate compared to similar learned models applied to equirectangular images. Also, compared to learning models based on graph convolutional networks, our solution supports more expressive filters that can preserve high frequencies and provide a better perceptual quality of the compressed images. Such results demonstrate the efficiency of the proposed framework, which opens new research venues for other omnidirectional vision tasks to be effectively implemented on the sphere manifold.      
### 29.Improving Reverberant Speech Separation with Multi-stage Training and Curriculum Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.09177.pdf)
>  We present a novel approach that improves the performance of reverberant speech separation. Our approach is based on an accurate geometric acoustic simulator (GAS) which generates realistic room impulse responses (RIRs) by modeling both specular and diffuse reflections. We also propose three training methods - pre-training, multi-stage training and curriculum learning that significantly improve separation quality in the presence of reverberation. We also demonstrate that mixing the synthetic RIRs with a small number of real RIRs during training enhances separation performance. We evaluate our approach on reverberant mixtures generated from real, recorded data (in several different room configurations) from the VOiCES dataset. Our novel approach (curriculum learning+pre-training+multi-stage training) results in a significant relative improvement over prior techniques based on image source method (ISM).      
### 30.PhD Thesis on Code Modulated Interferometric Imaging System using Phased Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2107.09138.pdf)
>  This work presents techniques which can allow low-cost phased-array receivers to be reconfigured as interferometric imagers and thereby reducing cost. Since traditional phased arrays power combine incoming signals prior to digitization, orthogonal code-modulation is applied to each incoming signal using phase shifters within each front-end. These code-modulated signals can then be combined and processed coherently through a shared hardware path. Visibility functions can be recovered through squaring and code-demultiplexing operations. The proposed system modulates incoming signals but demodulates desired correlations. Firstly, we present the operation of the system, a validation of its operation using behavioral models of a traditional phased array and a benchmarking of the code-modulated interferometer against traditional interferometer using simulation results and sensitivity analysis. Secondly, we present a simple CMI system operating in the license-free 60-GHz band using a four-element phased-array receiver. The four-element phased array is thinned to obtain a 13-pixel image and the system is demonstrated through a point-source detected at different locations. Finally, the operation and capabilities of code-modulated interferometry (CMI) are demonstrated at 10-GHz using commercially-available phased arrays. A 33-pixel, eight-element prototype is created using two commercially-available ADAR1000 phased-array receivers from Analog Devices Inc. The chips are connected at board level to a patch antenna array. The 33-pixel camera is demonstrated in hardware for point-source detection. Further to demonstrate the scalability of the concept, a 16-element, 169-pixels CMI imaging system is presented at 10-GHz using the four of the same commercially-available phased arrays from ADI. Two active point sources are imaged simultaneously to present the resolution of the system.      
### 31.Quality and Complexity Assessment of Learning-Based Image Compression Solutions  [ :arrow_down: ](https://arxiv.org/pdf/2107.09136.pdf)
>  This work presents an analysis of state-of-the-art learning-based image compression techniques. We compare 8 models available in the Tensorflow Compression package in terms of visual quality metrics and processing time, using the KODAK data set. The results are compared with the Better Portable Graphics (BPG) and the JPEG2000 codecs. Results show that JPEG2000 has the lowest execution times compared with the fastest learning-based model, with a speedup of 1.46x in compression and 30x in decompression. However, the learning-based models achieved improvements over JPEG2000 in terms of quality, specially for lower bitrates. Our findings also show that BPG is more efficient in terms of PSNR, but the learning models are better for other quality metrics, and sometimes even faster. The results indicate that learning-based techniques are promising solutions towards a future mainstream compression method.      
### 32.Convolutional module for heart localization and segmentation in MRI  [ :arrow_down: ](https://arxiv.org/pdf/2107.09134.pdf)
>  Magnetic resonance imaging (MRI) is a widely known medical imaging technique used to assess the heart function. Deep learning (DL) models perform several tasks in cardiac MRI (CMR) images with good efficacy, such as segmentation, estimation, and detection of diseases. Many DL models based on convolutional neural networks (CNN) were improved by detecting regions-of-interest (ROI) either automatically or by hand. In this paper we describe Visual-Motion-Focus (VMF), a module that detects the heart motion in the 4D MRI sequence, and highlights ROIs by focusing a Radial Basis Function (RBF) on the estimated motion field. We experimented and evaluated VMF on three CMR datasets, observing that the proposed ROIs cover 99.7% of data labels (Recall score), improved the CNN segmentation (mean Dice score) by 1.7 (p &lt; .001) after the ROI extraction, and improved the overall training speed by 2.5 times (+150%).      
### 33.Enhanced Transmission and Distribution NetworkCoordination to Host More Electric Vehicles and PV  [ :arrow_down: ](https://arxiv.org/pdf/2107.09132.pdf)
>  Distributed energy resources (DERs) installed at electric distribution networks create different opportunities and challenges for the distribution system operator (DSO). By increasing the penetration level of DERs, the impacts of these technologies are also sensed by the transmission system operator (TSO). The focus of this paper is on investigating and managing the impacts of the Electric Vehicle (EV) charging and solar Photo-Voltaic (PV) generation considering TSO-DSO interaction constraints. These constraints include the driving point impedance that reflects the impact of the DER-rich distribution system on the upstream HV network and voltage profile control through the on-load tap changing (OLTC) transformer's operation. The aim is to optimally utilize the existing assets and flexibilities to have more EVs and PVs at the distribution level without violating constraints at both transmission and distribution levels. The proposed model could readily be used by policymakers to design and implement the proper time-of-use tariffs for improved handling of the increasing EVs and PVs penetrations in emerging distribution networks. The proposed method is implemented on the IEEE 69-bus standard MV distribution network to demonstrate the applicability.      
### 34.Two-stage Planning for Electricity-Gas Coupled Integrated Energy System with CCUS Considering Carbon Tax and Price Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2107.09127.pdf)
>  In this article, we propose two-stage planning models for Electricity-Gas Coupled Integrated Energy System (EGC-IES), in which traditional thermal power plants (TTPPs) are considered to be retrofitted into carbon capture power plants (CCPPs), with power to gas (PtG) coupling CCPPs to gas system. The sizing and siting of carbon capture, utilisation and storage (CCUS)/PtG facilities, as well as the operation cost of TTPPs/CCPPs/gas sources/PtG, are all considered in the proposed model, including penalty on carbon emissions and revenue of CCUS. With changing policy on climate change and carbon emission regulation, the uncertainties of carbon price and carbon tax are also analysed and considered in the proposed planning model. The stochastic planning, and robust planning methods are introduced to verify mutually through economic and carbon indices. The proposed methods' effectiveness in reducing carbon emissions, increasing profit of CCUS from EGC-IES are demonstrated through various cases and discussions.      
### 35.Confidence Aware Neural Networks for Skin Cancer Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.09118.pdf)
>  Deep learning (DL) models have received particular attention in medical imaging due to their promising pattern recognition capabilities. However, Deep Neural Networks (DNNs) require a huge amount of data, and because of the lack of sufficient data in this field, transfer learning can be a great solution. DNNs used for disease diagnosis meticulously concentrate on improving the accuracy of predictions without providing a figure about their confidence of predictions. Knowing how much a DNN model is confident in a computer-aided diagnosis model is necessary for gaining clinicians' confidence and trust in DL-based solutions. To address this issue, this work presents three different methods for quantifying uncertainties for skin cancer detection from images. It also comprehensively evaluates and compares performance of these DNNs using novel uncertainty-related metrics. The obtained results reveal that the predictive uncertainty estimation methods are capable of flagging risky and erroneous predictions with a high uncertainty estimate. We also demonstrate that ensemble approaches are more reliable in capturing uncertainties through inference.      
### 36.LAPNet: Non-rigid Registration derived in k-space for Magnetic Resonance Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2107.09060.pdf)
>  Physiological motion, such as cardiac and respiratory motion, during Magnetic Resonance (MR) image acquisition can cause image artifacts. Motion correction techniques have been proposed to compensate for these types of motion during thoracic scans, relying on accurate motion estimation from undersampled motion-resolved reconstruction. A particular interest and challenge lie in the derivation of reliable non-rigid motion fields from the undersampled motion-resolved data. Motion estimation is usually formulated in image space via diffusion, parametric-spline, or optical flow methods. However, image-based registration can be impaired by remaining aliasing artifacts due to the undersampled motion-resolved reconstruction. In this work, we describe a formalism to perform non-rigid registration directly in the sampled Fourier space, i.e. k-space. We propose a deep-learning based approach to perform fast and accurate non-rigid registration from the undersampled k-space data. The basic working principle originates from the Local All-Pass (LAP) technique, a recently introduced optical flow-based registration. The proposed LAPNet is compared against traditional and deep learning image-based registrations and tested on fully-sampled and highly-accelerated (with two undersampling strategies) 3D respiratory motion-resolved MR images in a cohort of 40 patients with suspected liver or lung metastases and 25 healthy subjects. The proposed LAPNet provided consistent and superior performance to image-based approaches throughout different sampling trajectories and acceleration factors.      
### 37.Proximal Policy Optimization for Tracking Control Exploiting Future Reference Information  [ :arrow_down: ](https://arxiv.org/pdf/2107.09647.pdf)
>  In recent years, reinforcement learning (RL) has gained increasing attention in control engineering. Especially, policy gradient methods are widely used. In this work, we improve the tracking performance of proximal policy optimization (PPO) for arbitrary reference signals by incorporating information about future reference values. Two variants of extending the argument of the actor and the critic taking future reference values into account are presented. In the first variant, global future reference values are added to the argument. For the second variant, a novel kind of residual space with future reference values applicable to model-free reinforcement learning is introduced. Our approach is evaluated against a PI controller on a simple drive train model. We expect our method to generalize to arbitrary references better than previous approaches, pointing towards the applicability of RL to control real systems.      
### 38.Linear Invariants for Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.09642.pdf)
>  A central question in verification is characterizing when a system has invariants of a certain form, and then synthesizing them. We say a system has a $k$ linear invariant, $k$-LI in short, if it has a conjunction of $k$ linear (non-strict) inequalities -- equivalently, an intersection of $k$ (closed) half spaces -- as an invariant. We present a sufficient condition -- solely in terms of eigenvalues of the $A$-matrix -- for an $n$-dimensional linear dynamical system to have a $k$-LI. Our proof of sufficiency is constructive, and we get a procedure that computes a $k$-LI if the condition holds. We also present a necessary condition, together with many example linear systems where either the sufficient condition, or the necessary is tight, and which show that the gap between the conditions is not easy to overcome. In practice, the gap implies that using our procedure, we synthesize $k$-LI for a larger value of $k$ than what might be necessary. Our result enables analysis of continuous and hybrid systems with linear dynamics in their modes solely using reasoning in the theory of linear arithmetic (polygons), without needing reasoning over nonlinear arithmetic (ellipsoids).      
### 39.Co-optimization of Energy and Reserve with Incentives to Wind Generation: Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2107.09636.pdf)
>  This case study presents an analysis and quantification of the impact of the lack of co-optimization of energy and reserve in the presence of high penetration of wind energy. The methodology is developed in a companion paper, Part I. Two models, with and without co-optimization are confronted. The modeling of reserve and the incentive to renewable as well as the calibration of the model are inspired by the Spanish market. A sensitivity analysis is performed on configurations that differ by generation capacity, ramping capability, and market parameters (available wind, Feed in Premium to wind, generators risk aversion, and reserve requirement). The models and the case study are purely illustrative but the methodology is general.      
### 40.Rethinking the Tradeoff in Integrated Sensing and Communication: Recognition Accuracy versus Communication Rate  [ :arrow_down: ](https://arxiv.org/pdf/2107.09621.pdf)
>  Integrated sensing and communication (ISAC) is a promising technology to improve the band-utilization efficiency via spectrum sharing or hardware sharing between radar and communication systems. Since a common radio resource budget is shared by both functionalities, there exists a tradeoff between the sensing and communication performance. However, this tradeoff curve is currently unknown in ISAC systems with human motion recognition tasks based on deep learning. To fill this gap, this paper formulates and solves a multi-objective optimization problem which simultaneously maximizes the recognition accuracy and the communication data rate. The key ingredient of this new formulation is a nonlinear recognition accuracy model with respect to the wireless resources, where the model is derived from power function regression of the system performance of the deep spectrogram network. To avoid cost-expensive data collection procedures, a primitive-based autoregressive hybrid (PBAH) channel model is developed, which facilitates efficient training and testing dataset generation for human motion recognition in a virtual environment. Extensive results demonstrate that the proposed wireless recognition accuracy and PBAH channel models match the actual experimental data very well. Moreover, it is found that the accuracy-rate region consists of a communication saturation zone, a sensing saturation zone, and a communication-sensing adversarial zone, of which the third zone achieves the desirable balanced performance for ISAC systems.      
### 41.Parametric Scattering Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.09539.pdf)
>  The wavelet scattering transform creates geometric invariants and deformation stability from an initial structured signal. In multiple signal domains it has been shown to yield more discriminative representations compared to other non-learned representations, and to outperform learned representations in certain tasks, particularly on limited labeled data and highly structured signals. The wavelet filters used in the scattering transform are typically selected to create a tight frame via a parameterized mother wavelet. Focusing on Morlet wavelets, we propose to instead adapt the scales, orientations, and slants of the filters to produce problem-specific parametrizations of the scattering transform. We show that our learned versions of the scattering transform yield significant performance gains over the standard scattering transform in the small sample classification settings, and our empirical results suggest that tight frames may not always be necessary for scattering transforms to extract effective representations.      
### 42.Canonical Polyadic Decomposition and Deep Learning for Machine Fault Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.09519.pdf)
>  Acoustic monitoring for machine fault detection is a recent and expanding research path that has already provided promising results for industries. However, it is impossible to collect enough data to learn all types of faults from a machine. Thus, new algorithms, trained using data from healthy conditions only, were developed to perform unsupervised anomaly detection. A key issue in the development of these algorithms is the noise in the signals, as it impacts the anomaly detection performance. In this work, we propose a powerful data-driven and quasi non-parametric denoising strategy for spectral data based on a tensor decomposition: the Non-negative Canonical Polyadic (CP) decomposition. This method is particularly adapted for machine emitting stationary sound. We demonstrate in a case study, the Malfunctioning Industrial Machine Investigation and Inspection (MIMII) baseline, how the use of our denoising strategy leads to a sensible improvement of the unsupervised anomaly detection. Such approaches are capable to make sound-based monitoring of industrial processes more reliable.      
### 43.Relay-Assisted Cooperative Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.09518.pdf)
>  Federated learning (FL) has recently emerged as a promising technology to enable artificial intelligence (AI) at the network edge, where distributed mobile devices collaboratively train a shared AI model under the coordination of an edge server. To significantly improve the communication efficiency of FL, over-the-air computation allows a large number of mobile devices to concurrently upload their local models by exploiting the superposition property of wireless multi-access channels. Due to wireless channel fading, the model aggregation error at the edge server is dominated by the weakest channel among all devices, causing severe straggler issues. In this paper, we propose a relay-assisted cooperative FL scheme to effectively address the straggler issue. In particular, we deploy multiple half-duplex relays to cooperatively assist the devices in uploading the local model updates to the edge server. The nature of the over-the-air computation poses system objectives and constraints that are distinct from those in traditional relay communication systems. Moreover, the strong coupling between the design variables renders the optimization of such a system challenging. To tackle the issue, we propose an alternating-optimization-based algorithm to optimize the transceiver and relay operation with low complexity. Then, we analyze the model aggregation error in a single-relay case and show that our relay-assisted scheme achieves a smaller error than the one without relays provided that the relay transmit power and the relay channel gains are sufficiently large. The analysis provides critical insights on relay deployment in the implementation of cooperative FL. Extensive numerical results show that our design achieves faster convergence compared with state-of-the-art schemes.      
### 44.On Prosody Modeling for ASR+TTS based Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2107.09477.pdf)
>  In voice conversion (VC), an approach showing promising results in the latest voice conversion challenge (VCC) 2020 is to first use an automatic speech recognition (ASR) model to transcribe the source speech into the underlying linguistic contents; these are then used as input by a text-to-speech (TTS) system to generate the converted speech. Such a paradigm, referred to as ASR+TTS, overlooks the modeling of prosody, which plays an important role in speech naturalness and conversion similarity. Although some researchers have considered transferring prosodic clues from the source speech, there arises a speaker mismatch during training and conversion. To address this issue, in this work, we propose to directly predict prosody from the linguistic representation in a target-speaker-dependent manner, referred to as target text prediction (TTP). We evaluate both methods on the VCC2020 benchmark and consider different linguistic representations. The results demonstrate the effectiveness of TTP in both objective and subjective evaluations.      
### 45.DNN is not all you need: Parallelizing Non-Neural ML Algorithms on Ultra-Low-Power IoT Processors  [ :arrow_down: ](https://arxiv.org/pdf/2107.09448.pdf)
>  Machine Learning (ML) functions are becoming ubiquitous in latency- and privacy-sensitive IoT applications, prompting for a shift toward near-sensor processing at the extreme edge and the consequent increasing adoption of Parallel Ultra-Low Power (PULP) IoT processors. These compute- and memory-constrained parallel architectures need to run efficiently a wide range of algorithms, including key Non-Neural ML kernels that compete favorably with Deep Neural Networks (DNNs) in terms of accuracy under severe resource constraints. In this paper, we focus on enabling efficient parallel execution of Non-Neural ML algorithms on two RISCV-based PULP platforms, namely GAP8, a commercial chip, and PULP-OPEN, a research platform running on an FPGA emulator. We optimized the parallel algorithms through a fine-grained analysis and intensive optimization to maximize the speedup, considering two alternative Floating-Point (FP) emulation libraries on GAP8 and the native FPU support on PULP-OPEN. Experimental results show that a target-optimized emulation library can lead to an average 1.61x runtime improvement compared to a standard emulation library, while the native FPU support reaches up to 32.09x. In terms of parallel speedup, our design improves the sequential execution by 7.04x on average on the targeted octa-core platforms. Lastly, we present a comparison with the ARM Cortex-M4 microcontroller (MCU), a widely adopted commercial solution for edge deployments, which is 12.87$x slower than PULP-OPEN.      
### 46.Maximizing the Set Cardinality of Users Scheduled for Ultra-dense uRLLC Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.09404.pdf)
>  Ultra-reliability and low latency communication plays an important role in the fifth and sixth generation communication systems. Among the different research issues, scheduling as many users as possible to serve on the limited time-frequency resource is a crucial topic, with requirement of the maximum allowable transmission power and the minimum rate requirement of each user.We address it by proposing a mixed integer programming model, with objective function is maximizing the set cardinality of users instead of maximizing the system sum rate. Mathematical transformations and successive convex approximation are combined to solve the problem. Numerical results show that the proposed method achieves a considerable performance compared with exhaustive search method, but with lower computational complexity.      
### 47.Assessment of Self-Attention on Learned Features For Sound Event Localization and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.09388.pdf)
>  Joint sound event localization and detection (SELD) is an emerging audio signal processing task adding spatial dimensions to acoustic scene analysis and sound event detection. A popular approach to modeling SELD jointly is using convolutional recurrent neural network (CRNN) models, where CNNs learn high-level features from multi-channel audio input and the RNNs learn temporal relationships from these high-level features. However, RNNs have some drawbacks, such as a limited capability to model long temporal dependencies and slow training and inference times due to their sequential processing nature. Recently, a few SELD studies used multi-head self-attention (MHSA), among other innovations in their models. MHSA and the related transformer networks have shown state-of-the-art performance in various domains. While they can model long temporal dependencies, they can also be parallelized efficiently. In this paper, we study in detail the effect of MHSA on the SELD task. Specifically, we examined the effects of replacing the RNN blocks with self-attention layers. We studied the influence of stacking multiple self-attention blocks, using multiple attention heads in each self-attention block, and the effect of position embeddings and layer normalization. Evaluation on the DCASE 2021 SELD (task 3) development data set shows a significant improvement in all employed metrics compared to the baseline CRNN accompanying the task.      
### 48.A DNN-based OTFS Transceiver with Delay-Doppler Channel Training and IQI Compensation  [ :arrow_down: ](https://arxiv.org/pdf/2107.09376.pdf)
>  In this paper, we present a deep neural network (DNN) based transceiver architecture for delay-Doppler (DD) channel training and detection of orthogonal time frequency space (OTFS) modulation signals along with IQ imbalance (IQI) compensation. The proposed transceiver learns the DD channel over a spatial coherence interval and detects the information symbols using a single DNN trained for this purpose at the receiver. The proposed transceiver also learns the IQ imbalances present in the transmitter and receiver and effectively compensates them. The transmit IQI compensation is realized using a single DNN at the transmitter which learns and provides a compensating modulation alphabet (to pre-rotate the modulation symbols before sending through the transmitter) without explicitly estimating the transmit gain and phase imbalances. The receive IQI imbalance compensation is realized using two DNNs at the receiver, one DNN for explicit estimation of receive gain and phase imbalances and another DNN for compensation. Simulation results show that the proposed DNN-based architecture provides very good performance, making it as a promising approach for the design of practical OTFS transceivers.      
### 49.A Real-time Speaker Diarization System Based on Spatial Spectrum  [ :arrow_down: ](https://arxiv.org/pdf/2107.09321.pdf)
>  In this paper we describe a speaker diarization system that enables localization and identification of all speakers present in a conversation or meeting. We propose a novel systematic approach to tackle several long-standing challenges in speaker diarization tasks: (1) to segment and separate overlapping speech from two speakers; (2) to estimate the number of speakers when participants may enter or leave the conversation at any time; (3) to provide accurate speaker identification on short text-independent utterances; (4) to track down speakers movement during the conversation; (5) to detect speaker change incidence real-time. First, a differential directional microphone array-based approach is exploited to capture the target speakers' voice in far-field adverse environment. Second, an online speaker-location joint clustering approach is proposed to keep track of speaker location. Third, an instant speaker number detector is developed to trigger the mechanism that separates overlapped speech. The results suggest that our system effectively incorporates spatial information and achieves significant gains.      
### 50.PERSA+: A Deep Learning Front-End for Context-Agnostic Audio Classification  [ :arrow_down: ](https://arxiv.org/pdf/2107.09311.pdf)
>  Deep learning has been applied to diverse audio semantics tasks, enabling the construction of models that learn hierarchical levels of features from high-dimensional raw data, delivering state-of-the-art performance. But do these algorithms perform similarly in real-world conditions, or just at the benchmark, where their high learning capability assures the complete memorization of the employed datasets? This work presents a deep learning front-end, aiming at discarding detrimental information before entering the modeling stage, bringing the learning process closer to the point, anticipating the development of robust and context-agnostic classification algorithms.      
### 51.Joint Echo Cancellation and Noise Suppression based on Cascaded Magnitude and Complex Mask Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.09298.pdf)
>  Acoustic echo and background noise can seriously degrade the intelligibility of speech. In practice, echo and noise suppression are usually treated as two separated tasks and can be removed with various digital signal processing (DSP) and deep learning techniques. In this paper, we propose a new cascaded model, magnitude and complex temporal convolutional neural network (MC-TCN), to jointly perform acoustic echo cancellation and noise suppression with the help of adaptive filters. The MC-TCN cascades two separation cores, which are used to extract robust magnitude spectra feature and to enhance magnitude and phase simultaneously. Experimental results reveal that the proposed method can achieve superior performance by removing both echo and noise in real-time. In terms of DECMOS, the subjective test shows our method achieves a mean score of 4.41 and outperforms the INTERSPEECH2021 AEC-Challenge baseline by 0.54.      
### 52.Robust Deep Learning Frameworks for Acoustic Scene and Respiratory Sound Classification  [ :arrow_down: ](https://arxiv.org/pdf/2107.09268.pdf)
>  This thesis focuses on dealing with the task of acoustic scene classification (ASC), and then applied the techniques developed for ASC to a real-life application of detecting respiratory disease. To deal with ASC challenges, this thesis addresses three main factors that directly affect the performance of an ASC system. Firstly, this thesis explores input features by making use of multiple spectrograms (log-mel, Gamma, and CQT) for low-level feature extraction to tackle the issue of insufficiently discriminative or descriptive input features. Next, a novel Encoder network architecture is introduced. The Encoder firstly transforms each low-level spectrogram into high-level intermediate features, or embeddings, and thus combines these high-level features to form a very distinct composite feature. The composite or combined feature is then explored in terms of classification performance, with different Decoders such as Random Forest (RF), Multilayer Perception (MLP), and Mixture of Experts (MoE). By using this Encoder-Decoder framework, it helps to reduce the computation cost of the reference process in ASC systems which make use of multiple spectrogram inputs. Since the proposed techniques applied for general ASC tasks were shown to be highly effective, this inspired an application to a specific real-life application. This was namely the 2017 Internal Conference on Biomedical Health Informatics (ICBHI) respiratory sound dataset. Building upon the proposed ASC framework, the ICBHI tasks were tackled with a deep learning framework, and the resulting system shown to be capable at detecting respiratory anomaly cycles and diseases.      
### 53.Attitude and In-orbit Residual Magnetic Moment Estimation of Small Satellites Using only Magnetometer  [ :arrow_down: ](https://arxiv.org/pdf/2107.09257.pdf)
>  Attitude estimation or determination is a fundamental task for satellites to remain effectively operational. This task is furthermore complicated on small satellites by the limited space and computational power available on-board. This, coupled with a usually low budget, restricts small satellites from using high precision sensors for its especially important task of attitude estimation. On top of this, small satellites, on account of their size and weight, are comparatively more sensitive to environmental or orbital disturbances as compared to their larger counterparts. Magnetic disturbance forms the major contributor to orbital disturbances on small satellites in Lower Earth Orbits (LEO). This magnetic disturbance depends on the Residual Magnetic Moment (RMM) of the satellite itself, which for higher accuracy should be determined in real-time. This paper presents a method for in-orbit estimation of the satellite magnetic dipole using a Random Walk Model in order to circumnavigate the inaccuracy arising due to unknown orbital magnetic disturbances. It is also ensured that the dipole as well as attitude estimation of the satellite is done using only a magnetometer as the sensor.      
### 54.Constellation Design of Remote Sensing Small Satellite for Infrastructure Monitoring in India  [ :arrow_down: ](https://arxiv.org/pdf/2107.09253.pdf)
>  A constellation of remote sensing small satellite system has been developed for infrastructure monitoring in India by using SAR Payload. The LEO constellation of the small satellites is designed in a way, which can cover the entire footprint of India. Since India lies a little above the equatorial region, the orbital parameters are adjusted in a way that inclination of 36 degrees and RAAN varies from 70-130 degrees at a height of 600 km has been considered. A total number of 4 orbital planes are designed in which each orbital plane consisting 3 small satellites with 120-degrees true anomaly separation. Each satellite is capable of taking multiple look images with the minimum resolution of 1 meter per pixel and swath width of 10 km approx. The multiple look images captured by the SAR payload help in continuous infrastructure monitoring of our interested footprint area in India. Each small satellite is equipped with a communication payload that uses X-band and VHF antenna, whereas the TT&amp;C will use a high data-rate S-band transmitter. The paper presents only a coverage metrics analysis method of our designed constellation for our India footprint by considering the important metrics like revisit time, response time, and coverage efficiency. The result shows that the average revisits time for our constellation ranges from about 15- 35 min which is less than an hour and the average response time for this iteratively designed constellation ranges from about 25-120 min along with hundred percent coverage efficiency most of the time. Finally, it was concluded that each satellite has 70kg of total mass and costs around $ 0.75M to develop.      
### 55.S2Looking: A Satellite Side-Looking Dataset for Building Change Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.09244.pdf)
>  Collecting large-scale annotated satellite imagery datasets is essential for deep-learning-based global building change surveillance. In particular, the scroll imaging mode of optical satellites enables larger observation ranges and shorter revisit periods, facilitating efficient global surveillance. However, the images in recent satellite change detection datasets are mainly captured at near-nadir viewing angles. In this paper, we introduce S2Looking, a building change detection dataset that contains large-scale side-looking satellite images captured at varying off-nadir angles. Our S2Looking dataset consists of 5000 registered bitemporal image pairs (size of 1024*1024, 0.5 ~ 0.8 m/pixel) of rural areas throughout the world and more than 65,920 annotated change instances. We provide two label maps to separately indicate the newly built and demolished building regions for each sample in the dataset. We establish a benchmark task based on this dataset, i.e., identifying the pixel-level building changes in the bi-temporal images. We test several state-of-the-art methods on both the S2Looking dataset and the (near-nadir) LEVIR-CD+ dataset. The experimental results show that recent change detection methods exhibit much poorer performance on the S2Looking than on LEVIR-CD+. The proposed S2Looking dataset presents three main challenges: 1) large viewing angle changes, 2) large illumination variances and 3) various complex scene characteristics encountered in rural areas. Our proposed dataset may promote the development of algorithms for satellite image change detection and registration under conditions of large off-nadir angles. The dataset is available at <a class="link-external link-https" href="https://github.com/AnonymousForACMMM/" rel="external noopener nofollow">this https URL</a>.      
### 56.Music Tempo Estimation via Neural Networks -- A Comparative Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2107.09208.pdf)
>  This paper presents a comparative analysis on two artificial neural networks (with different architectures) for the task of tempo estimation. For this purpose, it also proposes the modeling, training and evaluation of a B-RNN (Bidirectional Recurrent Neural Network) model capable of estimating tempo in bpm (beats per minutes) of musical pieces, without using external auxiliary modules. An extensive database (12,550 pieces in total) was curated to conduct a quantitative and qualitative analysis over the experiment. Percussion-only tracks were also included in the dataset. The performance of the B-RNN is compared to that of state-of-the-art models. For further comparison, a state-of-the-art CNN was also retrained with the same datasets used for the B-RNN training. Evaluation results for each model and datasets are presented and discussed, as well as observations and ideas for future research. Tempo estimation was more accurate for the percussion only dataset, suggesting that the estimation can be more accurate for percussion-only tracks, although further experiments (with more of such datasets) should be made to gather stronger evidence.      
### 57.A Comparison of Supervised and Unsupervised Deep Learning Methods for Anomaly Detection in Images  [ :arrow_down: ](https://arxiv.org/pdf/2107.09204.pdf)
>  Anomaly detection in images plays a significant role for many applications across all industries, such as disease diagnosis in healthcare or quality assurance in manufacturing. Manual inspection of images, when extended over a monotonously repetitive period of time is very time consuming and can lead to anomalies being overlooked.Artificial neural networks have proven themselves very successful on simple, repetitive tasks, in some cases even outperforming humans. Therefore, in this paper we investigate different methods of deep learning, including supervised and unsupervised learning, for anomaly detection applied to a quality assurance use case. We utilize the MVTec anomaly dataset and develop three different models, a CNN for supervised anomaly detection, KD-CAE for autoencoder anomaly detection, NI-CAE for noise induced anomaly detection and a DCGAN for generating reconstructed images. By experiments, we found that KD-CAE performs better on the anomaly datasets compared to CNN and NI-CAE, with NI-CAE performing the best on the Transistor dataset. We also implemented a DCGAN for the creation of new training data but due to computational limitation and lack of extrapolating the mechanics of AnoGAN, we restricted ourselves just to the generation of GAN based images. We conclude that unsupervised methods are more powerful for anomaly detection in images, especially in a setting where only a small amount of anomalous data is available, or the data is unlabeled.      
### 58.Wide and Deep Graph Neural Network with Distributed Online Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.09203.pdf)
>  Graph neural networks (GNNs) are naturally distributed architectures for learning representations from network data. This renders them suitable candidates for decentralized tasks. In these scenarios, the underlying graph often changes with time due to link failures or topology variations, creating a mismatch between the graphs on which GNNs were trained and the ones on which they are tested. Online learning can be leveraged to retrain GNNs at testing time to overcome this issue. However, most online algorithms are centralized and usually offer guarantees only on convex problems, which GNNs rarely lead to. This paper develops the Wide and Deep GNN (WD-GNN), a novel architecture that can be updated with distributed online learning mechanisms. The WD-GNN consists of two components: the wide part is a linear graph filter and the deep part is a nonlinear GNN. At training time, the joint wide and deep architecture learns nonlinear representations from data. At testing time, the wide, linear part is retrained, while the deep, nonlinear one remains fixed. This often leads to a convex formulation. We further propose a distributed online learning algorithm that can be implemented in a decentralized setting. We also show the stability of the WD-GNN to changes of the underlying graph and analyze the convergence of the proposed online learning procedure. Experiments on movie recommendation, source localization and robot swarm control corroborate theoretical findings and show the potential of the WD-GNN for distributed online learning.      
### 59.Compressing Multisets with Large Alphabets  [ :arrow_down: ](https://arxiv.org/pdf/2107.09202.pdf)
>  Current methods that optimally compress multisets are not suitable for high-dimensional symbols, as their compute time scales linearly with alphabet size. Compressing a multiset as an ordered sequence with off-the-shelf codecs is computationally more efficient, but has a sub-optimal compression rate, as bits are wasted encoding the order between symbols. We present a method that can recover those bits, assuming symbols are i.i.d., at the cost of an additional $\mathcal{O}(|\mathcal{M}|\log M)$ in average time complexity, where $|\mathcal{M}|$ and $M$ are the total and unique number of symbols in the multiset. Our method is compatible with any prefix-free code. Experiments show that, when paired with efficient coders, our method can efficiently compress high-dimensional sources such as multisets of images and collections of JSON files.      
### 60.Compute RAMs: Adaptable Compute and Storage Blocks for DL-Optimized FPGAs  [ :arrow_down: ](https://arxiv.org/pdf/2107.09178.pdf)
>  The configurable building blocks of current FPGAs -- Logic blocks (LBs), Digital Signal Processing (DSP) slices, and Block RAMs (BRAMs) -- make them efficient hardware accelerators for the rapid-changing world of Deep Learning (DL). Communication between these blocks happens through an interconnect fabric consisting of switching elements spread throughout the FPGA. In this paper, a new block, Compute RAM, is proposed. Compute RAMs provide highly-parallel processing-in-memory (PIM) by combining computation and storage capabilities in one block. Compute RAMs can be integrated in the FPGA fabric just like the existing FPGA blocks and provide two modes of operation (storage or compute) that can be dynamically chosen. They reduce power consumption by reducing data movement, provide adaptable precision support, and increase the effective on-chip memory bandwidth. Compute RAMs also help increase the compute density of FPGAs. In our evaluation of addition, multiplication and dot-product operations across multiple data precisions (int4, int8 and bfloat16), we observe an average savings of 80% in energy consumption, and an improvement in execution time ranging from 20% to 80%. Adding Compute RAMs can benefit non-DL applications as well, and make FPGAs more efficient, flexible, and performant accelerators.      
### 61.Exploring the Non-Overlapping Visibility Regions in XL-MIMO Random Access Protocol  [ :arrow_down: ](https://arxiv.org/pdf/2107.09169.pdf)
>  The recent extra-large scale massive multiple-input multiple-output (XL-MIMO) systems are seen as a promising technology for providing very high data rates in increased user-density scenarios. Spatial non-stationarities and visibility regions (VRs) appear across the XL-MIMO array since its large dimension is of the same order as the distances to the user-equipments (UEs). Due to the increased density of UEs in typical applications of XL-MIMO systems and the scarcity of pilots, the design of random access (RA) protocols and scheduling algorithms become challenging. In this paper, we propose a joint RA and scheduling protocol, namely non-overlapping VR XL- MIMO (NOVR-XL) RA protocol, which takes advantage of the different VRs of the UEs for improving RA performance, besides seeking UEs with non-overlapping VRs to be scheduled in the same payload data pilot resource. Our results reveal that the proposed scheme achieves significant gains in terms of sum rate compared with traditional RA schemes, as well as reducing access latency and improving connectivity performance as a whole.      
### 62.User Association in Dense mmWave Networks as Restless Bandits  [ :arrow_down: ](https://arxiv.org/pdf/2107.09153.pdf)
>  We study the problem of user association, i.e., determining which base station (BS) a user should associate with, in a dense millimeter wave (mmWave) network. In our system model, in each time slot, a user arrives with some probability in a region with a relatively small geographical area served by a dense mmWave network. Our goal is to devise an association policy under which, in each time slot in which a user arrives, it is assigned to exactly one BS so as to minimize the weighted average amount of time that users spend in the system. The above problem is a restless multi-armed bandit problem and is provably hard to solve. We prove that the problem is Whittle indexable, and based on this result, propose an association policy under which an arriving user is associated with the BS having the smallest Whittle index. Using simulations, we show that our proposed policy outperforms several user association policies proposed in prior work.      
### 63.Sequence-to-Sequence Piano Transcription with Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2107.09142.pdf)
>  Automatic Music Transcription has seen significant progress in recent years by training custom deep neural networks on large datasets. However, these models have required extensive domain-specific design of network architectures, input/output representations, and complex decoding schemes. In this work, we show that equivalent performance can be achieved using a generic encoder-decoder Transformer with standard decoding methods. We demonstrate that the model can learn to translate spectrogram inputs directly to MIDI-like output events for several transcription tasks. This sequence-to-sequence approach simplifies transcription by jointly modeling audio features and language-like output dependencies, thus removing the need for task-specific architectures. These results point toward possibilities for creating new Music Information Retrieval models by focusing on dataset creation and labeling rather than custom model design.      
### 64.Learning a Sensor-invariant Embedding of Satellite Data: A Case Study for Lake Ice Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2107.09092.pdf)
>  Fusing satellite imagery acquired with different sensors has been a long-standing challenge of Earth observation, particularly across different modalities such as optical and Synthetic Aperture Radar (SAR) images. Here, we explore the joint analysis of imagery from different sensors in the light of representation learning: we propose to learn a joint, sensor-invariant embedding (feature representation) within a deep neural network. Our application problem is the monitoring of lake ice on Alpine lakes. To reach the temporal resolution requirement of the Swiss Global Climate Observing System (GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra MODIS and Suomi-NPP VIIRS. The large gaps between the optical and SAR domains and between the sensor resolutions make this a challenging instance of the sensor fusion problem. Our approach can be classified as a feature-level fusion that is learnt in a data-driven manner. The proposed network architecture has separate encoding branches for each image sensor, which feed into a single latent embedding. I.e., a common feature representation shared by all inputs, such that subsequent processing steps deliver comparable output irrespective of which sort of input image was used. By fusing satellite data, we map lake ice at a temporal resolution of &lt;1.5 days. The network produces spatially explicit lake ice maps with pixel-wise accuracies &gt;91.3% (respectively, mIoU scores &gt;60.7%) and generalises well across different lakes and winters. Moreover, it sets a new state-of-the-art for determining the important ice-on and ice-off dates for the target lakes, in many cases meeting the GCOS requirement.      
### 65.Energy Disaggregation using Variational Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2103.12177.pdf)
>  Non-intrusive load monitoring (NILM) is a technique that uses a single sensor to measure the total power consumption of a building. Using an energy disaggregation method, the consumption of individual appliances can be estimated from the aggregate measurement. Recent disaggregation algorithms have significantly improved the performance of NILM systems. However, the generalization capability of these methods to different houses as well as the disaggregation of multi-state appliances are still major challenges. In this paper we address these issues and propose an energy disaggregation approach based on the variational autoencoders framework. The probabilistic encoder makes this approach an efficient model for encoding information relevant to the reconstruction of the target appliance consumption. In particular, the proposed model accurately generates more complex load profiles, thus improving the power signal reconstruction of multi-state appliances. Moreover, its regularized latent space improves the generalization capabilities of the model across different houses. The proposed model is compared to state-of-the-art NILM approaches on the UK-DALE and REFIT datasets, and yields competitive results. The mean absolute error reduces by 18% on average across all appliances compared to the state-of-the-art. The F1-Score increases by more than 11%, showing improvements for the detection of the target appliance in the aggregate measurement.      
### 66.Pantomorphic Perspective for Immersive Imagery  [ :arrow_down: ](https://arxiv.org/pdf/2102.12682.pdf)
>  A wide choice of cinematic lenses enables motion-picture creators to adapt image visual-appearance to their creative vision. Such choice does not exist in the realm of real-time computer graphics, where only one type of perspective projection is widely used. This work provides a perspective imaging model that in an artistically convincing manner resembles anamorphic photography lens variety and more. It presents an asymmetrical-anamorphic azimuthal projection map with natural vignetting and realistic chromatic aberration. The mathematical model for this projection has been chosen such that its parameters reflect the psycho-physiological aspects of visual perception. That enables its use in artistic and professional environments, where specific aspects of the photographed space are to be presented.      
### 67.Control of continuous-mode single-photon states: a review  [ :arrow_down: ](https://arxiv.org/pdf/1902.10961.pdf)
>  In this survey, we first introduce quantum fields and open quantum systems, then we present continuous-mode single-photon states and discuss discrete measurements of a single-photon field. After that, we introduce linear quantum systems and show how a linear quantum system responds to a single-photon input. Then we investigate how a coherent feedback network can be used to manipulate the temporal pulse shape of a single-photon state. Afterwards, we present single-photon filter and master equations. Finally, we discuss the generation of Schrdinger cat states by means of photon addition and subtraction      
