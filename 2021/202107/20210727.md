# ArXiv eess --Tue, 27 Jul 2021
### 1.Structure-Preserving Multi-Domain Stain Color Augmentation using Style-Transfer with Disentangled Representations  [ :arrow_down: ](https://arxiv.org/pdf/2107.12357.pdf)
>  In digital pathology, different staining procedures and scanners cause substantial color variations in whole-slide images (WSIs), especially across different laboratories. These color shifts result in a poor generalization of deep learning-based methods from the training domain to external pathology data. To increase test performance, stain normalization techniques are used to reduce the variance between training and test domain. Alternatively, color augmentation can be applied during training leading to a more robust model without the extra step of color normalization at test time. We propose a novel color augmentation technique, HistAuGAN, that can simulate a wide variety of realistic histology stain colors, thus making neural networks stain-invariant when applied during training. Based on a generative adversarial network (GAN) for image-to-image translation, our model disentangles the content of the image, i.e., the morphological tissue structure, from the stain color attributes. It can be trained on multiple domains and, therefore, learns to cover different stain colors as well as other domain-specific variations introduced in the slide preparation and imaging process. We demonstrate that HistAuGAN outperforms conventional color augmentation techniques on a classification task on the publicly available dataset Camelyon17 and show that it is able to mitigate present batch effects.      
### 2.MAG-Net: Mutli-task attention guided network for brain tumor segmentation and classification  [ :arrow_down: ](https://arxiv.org/pdf/2107.12321.pdf)
>  Brain tumor is the most common and deadliest disease that can be found in all age groups. Generally, MRI modality is adopted for identifying and diagnosing tumors by the radiologists. The correct identification of tumor regions and its type can aid to diagnose tumors with the followup treatment plans. However, for any radiologist analysing such scans is a complex and time-consuming task. Motivated by the deep learning based computer-aided-diagnosis systems, this paper proposes multi-task attention guided encoder-decoder network (MAG-Net) to classify and segment the brain tumor regions using MRI images. The MAG-Net is trained and evaluated on the Figshare dataset that includes coronal, axial, and sagittal views with 3 types of tumors meningioma, glioma, and pituitary tumor. With exhaustive experimental trials the model achieved promising results as compared to existing state-of-the-art models, while having least number of training parameters among other state-of-the-art models.      
### 3.End-to-End Deep Learning of Long-Haul Coherent Optical Fiber Communications via Regular Perturbation Model  [ :arrow_down: ](https://arxiv.org/pdf/2107.12320.pdf)
>  We present a novel end-to-end autoencoder-based learning for coherent optical communications using a "parallelizable" perturbative channel model. We jointly optimized constellation shaping and nonlinear pre-emphasis achieving mutual information gain of 0.18 bits/sym./pol. simulating 64 GBd dual-polarization single-channel transmission over 30x80 km G.652 SMF link with EDFAs.      
### 4.B-line Detection in Lung Ultrasound Videos: Cartesian vs Polar Representation  [ :arrow_down: ](https://arxiv.org/pdf/2107.12291.pdf)
>  Lung ultrasound (LUS) imaging is becoming popular in the intensive care units (ICU) for assessing lung abnormalities such as the appearance of B-line artefacts as a result of severe dengue. These artefacts appear in the LUS images and disappear quickly, making their manual detection very challenging. They also extend radially following the propagation of the sound waves. As a result, we hypothesize that a polar representation may be more adequate for automatic image analysis of these images. This paper presents an attention-based Convolutional+LSTM model to automatically detect B-lines in LUS videos, comparing performance when image data is taken in Cartesian and polar representations. Results indicate that the proposed framework with polar representation achieves competitive performance compared to the Cartesian representation for B-line classification and that attention mechanism can provide better localization.      
### 5.Deep Transfer Clustering of Radio Signals  [ :arrow_down: ](https://arxiv.org/pdf/2107.12237.pdf)
>  Modulation recognition is an important task in radio signal processing. Most of the current researches focus on supervised learning. However, in many real scenarios, it is difficult and cost to obtain the labels of signals. In this letter, we turn to the more challenging problem: can we cluster the modulation types just based on a large number of unlabeled radio signals? If this problem can be solved, we then can also recognize modulation types by manually labeling a very small number of samples. To answer this problem, we propose a deep transfer clustering (DTC) model. DTC naturally integrates feature learning and deep clustering, and further adopts a transfer learning mechanism to improve the feature extraction ability of an embedded convolutional neural network (CNN) model. The experiments validate that our DTC significantly outperforms a number of baselines, achieving the state-of-the-art performance in clustering radio signals for modulation recognition.      
### 6.Raw Differentiable Architecture Search for Speech Deepfake and Spoofing Detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.12212.pdf)
>  End-to-end approaches to anti-spoofing, especially those which operate directly upon the raw signal, are starting to be competitive with their more traditional counterparts. Until recently, all such approaches consider only the learning of network parameters; the network architecture is still hand crafted. This too, however, can also be learned. Described in this paper is our attempt to learn automatically the network architecture of a speech deepfake and spoofing detection solution, while jointly optimising other network components and parameters, such as the first convolutional layer which operates on raw signal inputs. The resulting raw differentiable architecture search system delivers a tandem detection cost function score of 0.0517 for the ASVspoof 2019 logical access database, a result which is among the best single-system results reported to date.      
### 7.Early Diagnosis of Lung Cancer Using Computer Aided Detection via Lung Segmentation Approach  [ :arrow_down: ](https://arxiv.org/pdf/2107.12205.pdf)
>  Lung cancer begins in the lungs and leading to the reason of cancer demise amid population in the creation. According to the American Cancer Society, which estimates about 27% of the deaths because of cancer. In the early phase of its evolution, lung cancer does not cause any symptoms usually. Many of the patients have been diagnosed in a developed phase where symptoms become more prominent, that results in poor curative treatment and high mortality rate. Computer Aided Detection systems are used to achieve greater accuracies for the lung cancer diagnosis. In this research exertion, we proposed a novel methodology for lung Segmentation on the basis of Fuzzy C-Means Clustering, Adaptive Thresholding, and Segmentation of Active Contour Model. The experimental results are analysed and presented.      
### 8.Utilizing synchronization to partition power networks into microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2107.12165.pdf)
>  The problem of partitioning a power grid into a set of microgrids, or islands, is of interest for both the design of future smart grids, and as a last resort to restore power dispatchment in sections of a grid affected by an extreme failure. In the literature this problem is usually solved by turning it into a combinatorial optimization problem, often solved through generic heruristic methods such as Genetic Algorithms or Tabu Search. In this paper, we take a different route and obtain the grid partition by exploiting the synchronization dynamics of a cyberlayer of Kuramoto oscillators, each parameterized as a rough approximation of the dynamics of the grid's node it corresponds to. We present first a centralised algorithm and then a decentralised strategy. In the former, nodes are aggregated based on their internode synchronization times while in the latter they exploit synchronization of the oscillators in the cyber layer to selforganise into islands. Our preliminary results show that the heuristic synchronization based algorithms do converge towards partitions that are comparable to those obtained via other more cumbersome and computationally expensive optimization-based methods.      
### 9.Crowdsourcing strong labels for sound event detection  [ :arrow_down: ](https://arxiv.org/pdf/2107.12089.pdf)
>  Strong labels are a necessity for evaluation of sound event detection methods, but often scarcely available due to the high resources required by the annotation task. We present a method for estimating strong labels using crowdsourced weak labels, through a process that divides the annotation task into simple unit tasks. Based on estimations of annotators' competence, aggregation and processing of the weak labels results in a set of objective strong labels. The experiment uses synthetic audio in order to verify the quality of the resulting annotations through comparison with ground truth. The proposed method produces labels with high precision, though not all event instances are recalled. Detection metrics comparing the produced annotations with the ground truth show 80% F-score in 1 s segments, and up to 89.5% intersection-based F1-score calculated according to the polyphonic sound detection score metrics.      
### 10.Robust Regularized Locality Preserving Indexing for Fiedler Vector Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.12070.pdf)
>  The Fiedler vector of a connected graph is the eigenvector associated with the algebraic connectivity of the graph Laplacian and it provides substantial information to learn the latent structure of a graph. In real-world applications, however, the data may be subject to heavy-tailed noise and outliers which results in deteriorations in the structure of the Fiedler vector estimate. We design a Robust Regularized Locality Preserving Indexing (RRLPI) method for Fiedler vector estimation that aims to approximate the nonlinear manifold structure of the Laplace Beltrami operator while minimizing the negative impact of outliers. First, an analysis of the effects of two fundamental outlier types on the eigen-decomposition for block affinity matrices which are essential in cluster analysis is conducted. Then, an error model is formulated and a robust Fiedler vector estimation algorithm is developed. An unsupervised penalty parameter selection algorithm is proposed that leverages the geometric structure of the projection space to perform robust regularized Fiedler estimation. The performance of RRLPI is benchmarked against existing competitors in terms of detection probability, partitioning quality, image segmentation capability, robustness and computation time using a large variety of synthetic and real data experiments.      
### 11.Adaptation of Tacotron2-based Text-To-Speech for Articulatory-to-Acoustic Mapping using Ultrasound Tongue Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2107.12051.pdf)
>  For articulatory-to-acoustic mapping, typically only limited parallel training data is available, making it impossible to apply fully end-to-end solutions like Tacotron2. In this paper, we experimented with transfer learning and adaptation of a Tacotron2 text-to-speech model to improve the final synthesis quality of ultrasound-based articulatory-to-acoustic mapping with a limited database. We use a multi-speaker pre-trained Tacotron2 TTS model and a pre-trained WaveGlow neural vocoder. The articulatory-to-acoustic conversion contains three steps: 1) from a sequence of ultrasound tongue image recordings, a 3D convolutional neural network predicts the inputs of the pre-trained Tacotron2 model, 2) the Tacotron2 model converts this intermediate representation to an 80-dimensional mel-spectrogram, and 3) the WaveGlow model is applied for final inference. This generated speech contains the timing of the original articulatory data from the ultrasound recording, but the F0 contour and the spectral information is predicted by the Tacotron2 model. The F0 values are independent of the original ultrasound images, but represent the target speaker, as they are inferred from the pre-trained Tacotron2 model. In our experiments, we demonstrated that the synthesized speech quality is more natural with the proposed solutions than with our earlier model.      
### 12.Towards Generative Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2107.12038.pdf)
>  We present a neural video compression method based on generative adversarial networks (GANs) that outperforms previous neural video compression methods and is comparable to HEVC in a user study. We propose a technique to mitigate temporal error accumulation caused by recursive frame compression that uses randomized shifting and un-shifting, motivated by a spectral analysis. We present in detail the network design choices, their relative importance, and elaborate on the challenges of evaluating video compression methods in user studies.      
### 13.Distributed Neighbor Selection in Multi-agent Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.12022.pdf)
>  Achieving consensus via nearest neighbor rules is an important prerequisite for multi-agent networks to accomplish collective tasks. A common assumption in consensus setup is that each agent interacts with all its neighbors during the process. This paper examines whether network functionality and performance can be maintained-and even enhanced-when agents interact only with a subset of their respective (available) neighbors. As shown in the paper, the answer to this inquiry is affirmative. In this direction, we show that by using the monotonicity property of the Laplacian eigenvectors, a neighbor selection rule with guaranteed performance enhancements, can be realized for consensus-type networks. For the purpose of distributed implementation, a quantitative connection between Laplacian eigenvectors and the "relative rate of change" in the state between neighboring agents is further established; this connection facilitates a distributed algorithm for each agent to identify "favorable" neighbors to interact with. Multi-agent networks with and without external influence are examined, as well as extensions to signed networks. This paper underscores the utility of Laplacian eigenvectors in the context of distributed neighbor selection, providing novel insights into distributed data-driven control of multi-agent systems.      
### 14.UR Channel-Robust Synthetic Speech Detection System for ASVspoof 2021  [ :arrow_down: ](https://arxiv.org/pdf/2107.12018.pdf)
>  In this paper, we present UR-AIR system submission to the logical access (LA) and the speech deepfake (DF) tracks of the ASVspoof 2021 Challenge. The LA and DF tasks focus on synthetic speech detection (SSD), i.e. detecting text-to-speech and voice conversion as spoofing attacks. Different from previous ASVspoof challenges, the LA task this year presents codec and transmission channel variability, while the new task DF presents general audio compression. Built upon our previous research work on improving the robustness of the SSD systems to channel effects, we propose a channel-robust synthetic speech detection system for the challenge. To mitigate the channel variability issue, we use an acoustic simulator to apply transmission codec, compression codec, and convolutional impulse responses to the original datasets. For the neural network backbone, we propose to use Emphasized Channel Attention, Propagation and Aggregation Time Delay Neural Networks (ECAPA-TDNN) as our primary model. We also incorporate one-class learning with channel-robust training strategies to further learn a channel-invariant speech representation. Our submission achieved EER 20.33% in the DF task; EER 5.46% and min-tDCF 0.3094 in the LA task.      
### 15.Weakly Supervised Attention Model for RV StrainClassification from volumetric CTPA Scans  [ :arrow_down: ](https://arxiv.org/pdf/2107.12009.pdf)
>  Pulmonary embolus (PE) refers to obstruction of pulmonary arteries by blood clots. PE accounts for approximately 100,000 deaths per year in the United States alone. The clinical presentation of PE is often nonspecific, making the diagnosis challenging. Thus, rapid and accurate risk stratification is of paramount importance. High-risk PE is caused by right ventricular (RV) dysfunction from acute pressure overload, which in return can help identify which patients require more aggressive therapy. Reconstructed four-chamber views of the heart on chest CT can detect right ventricular enlargement. CT pulmonary angiography (CTPA) is the golden standard in the diagnostic workup of suspected PE. Therefore, it can link between diagnosis and risk stratification strategies. We developed a weakly supervised deep learning algorithm, with an emphasis on a novel attention mechanism, to automatically classify RV strain on CTPA. Our method is a 3D DenseNet model with integrated 3D residual attention blocks. We evaluated our model on a dataset of CTPAs of emergency department (ED) PE patients. This model achieved an area under the receiver operating characteristic curve (AUC) of 0.88 for classifying RV strain. The model showed a sensitivity of 87% and specificity of 83.7%. Our solution outperforms state-of-the-art 3D CNN networks. The proposed design allows for a fully automated network that can be trained easily in an end-to-end manner without requiring computationally intensive and time-consuming preprocessing or strenuous labeling of the data.We infer that unmarked CTPAs can be used for effective RV strain classification. This could be used as a second reader, alerting for high-risk PE patients. To the best of our knowledge, there are no previous deep learning-based studies that attempted to solve this problem.      
### 16.Inplace Gated Convolutional Recurrent Neural Network For Dual-channel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2107.11968.pdf)
>  For dual-channel speech enhancement, it is a promising idea to design an end-to-end model based on the traditional array signal processing guideline and the manifold space of multi-channel signals. We found that the idea above can be effectively implemented by the classical convolutional recurrent neural networks (CRN) architecture. We propose a very compact in place gated convolutional recurrent neural network (inplace GCRN) for end-to-end multi-channel speech enhancement, which utilizes inplace-convolution for frequency pattern extraction and reconstruction. The inplace characteristics efficiently preserve spatial cues in each frequency bin for channel-wise long short-term memory neural networks (LSTM) tracing the spatial source. In addition, we come up with a new spectrum recovery method by predict amplitude mask, mapping, and phase, which effectively improves the speech quality.      
### 17.Superconducting Quantum Amplifier-Integrator in Ultra-High Speed Continuous-time Delta-Sigma Converter  [ :arrow_down: ](https://arxiv.org/pdf/2107.11964.pdf)
>  Present semiconductor research is increasingly focusing on either higher speeds or higher linearity or both. Applications range from consumer, industrial, healthcare and military. Typically such circuits are fabricated in today's low-voltage CMOS processes using silicon and in few cases BJT-CMOS combined like Gallium-Arsenide or Indium-Phosphide. These technology nodes face a plethora of problems like reduction of dynamic range of the circuit due to mismatch, distortion, noise, thermal and electromigration issues due to excessive currents, etc. Compounding these problems is the issue with lower achievable gain from an amplifier which often gets limited due to lower supply voltages in such technology nodes. Slowly circuit techniques like chopping, cascoding, cascading and calibration are nearing their limits. In this paper we present a radically different approach to our regular analog design building blocks using macroscopic quantum effects which have hitherto not found favour with the design community. We will solely focus on the effect of superconductivity and adopting its macroscopic phenomena to amplifiers, integrators and comparators. Using staggered superconductors we can achieve a gain which depends only on physical quantum constants and remains invariant under process, temperature, supply, interference, etc. This robustness of gain in an amplifier goes a long way in attaining higher linearity. The comparator can resolve a minimum of 2.07fT magnetic flux but when embedded inside a Delta-Sigma loop can typically attain 100 times smaller resolution pushing the boundaries of sensing.      
### 18.Deep Learning for Estimation and Pilot Signal Design in Few-Bit Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.11958.pdf)
>  Estimation in few-bit MIMO systems is challenging, since the received signals are nonlinearly distorted by the low-resolution ADCs. In this paper, we propose a deep learning framework for channel estimation, data detection, and pilot signal design to address the nonlinearity in such systems. The proposed channel estimation and data detection networks are model-driven and have special structures that take advantage of the domain knowledge in the few-bit quantization process. While the first data detection network, namely B-DetNet, is based on a linearized model obtained from the Bussgang decomposition, the channel estimation network and the second data detection network, namely FBM-CENet and FBM-DetNet respectively, rely on the original quantized system model. To develop FBM-CENet and FBM-DetNet, the maximum-likelihood channel estimation and data detection problems are reformulated to overcome the vanishing gradient issue. An important feature of the proposed FBM-CENet structure is that the pilot matrix is integrated into its weight matrices of the channel estimator. Thus, training the proposed FBM-CENet enables a joint optimization of both the channel estimator at the base station and the pilot signal transmitted from the users. Simulation results show significant performance gain in estimation accuracy by the proposed deep learning framework.      
### 19.A Unified Hyper-GAN Model for Unpaired Multi-contrast MR Image Translation  [ :arrow_down: ](https://arxiv.org/pdf/2107.11945.pdf)
>  Cross-contrast image translation is an important task for completing missing contrasts in clinical diagnosis. However, most existing methods learn separate translator for each pair of contrasts, which is inefficient due to many possible contrast pairs in real scenarios. In this work, we propose a unified Hyper-GAN model for effectively and efficiently translating between different contrast pairs. Hyper-GAN consists of a pair of hyper-encoder and hyper-decoder to first map from the source contrast to a common feature space, and then further map to the target contrast image. To facilitate the translation between different contrast pairs, contrast-modulators are designed to tune the hyper-encoder and hyper-decoder adaptive to different contrasts. We also design a common space loss to enforce that multi-contrast images of a subject share a common feature space, implicitly modeling the shared underlying anatomical structures. Experiments on two datasets of IXI and BraTS 2019 show that our Hyper-GAN achieves state-of-the-art results in both accuracy and efficiency, e.g., improving more than 1.47 and 1.09 dB in PSNR on two datasets with less than half the amount of parameters.      
### 20.Computation of Reachable Sets Based on Hamilton-Jacobi-Bellman Equation with Running Cost Function  [ :arrow_down: ](https://arxiv.org/pdf/2107.11941.pdf)
>  A novel method for computing reachable sets is proposed in this paper. In the proposed method, a Hamilton-Jacobi-Bellman equation with running cost functionis numerically solved and the reachable sets of different time horizons are characterized by a family of non-zero level sets of the solution of the Hamilton-Jacobi-Bellman equation. In addition to the classical reachable set, by setting different running cost functions and terminal conditionsof the Hamilton-Jacobi-Bellman equation, the proposed method allows to compute more generalized reachable sets, which are referred to as cost-limited reachable sets. In order to overcome the difficulty of solving the Hamilton-Jacobi-Bellman equation caused by the discontinuity of the solution, a method based on recursion and grid interpolation is employed. <br>At the end of this paper, some examples are taken to illustrate the validity and generality of the proposed method.      
### 21.Phase Spectrometry For High Precision mm-Wave DoA Estimation In 5G Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.11891.pdf)
>  In this paper, we introduce a direction of arrival (DoA) estimation method based on a technique named phase spectrometry (PS) that is mainly suitable for mm-Wave and Tera-hertz applications as an alternative for DoA estimation using antenna arrays. PS is a conventional technique in optics to measure phase difference between two waves at different frequencies of the spectrum. Here we adapt PS for the same purpose in the radio frequency band. We show that we can emulate a large array exploiting only two antennas. To this end, we measure phase difference between the two antennas for different frequencies using PS. Consequently, we demonstrate that we can radically reduce the complexity of the receiver required for DoA estimation employing PS. We consider two different schemes for implementation of PS: via a long wave-guide and frequency code-book. We show that using a frequency code-book, higher processing gain can be achieved. Moreover, we introduce three PS architectures: for device to device DoA estimation, for base-station in uplink scenario and an ultra-fast DoA estimation technique mainly for radar and aerial and satellite communications. Simulation and analytical results show that, PS is capable of detecting and discriminating between multiple incoming signals with different DoAs. Moreover, our results also show that, the angular resolution of PS depends on the distance between the two antennas and the band-width of the frequency code-book. Finally, the performance of PS is compared with a uniform linear array (ULA) and it is shown that PS can perform the same, with a much less complex receiver, and without the prerequisite of spatial search for DoA estimation.      
### 22.Lung Cancer Risk Estimation with Incomplete Data: A Joint Missing Imputation Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2107.11882.pdf)
>  Data from multi-modality provide complementary information in clinical prediction, but missing data in clinical cohorts limits the number of subjects in multi-modal learning context. Multi-modal missing imputation is challenging with existing methods when 1) the missing data span across heterogeneous modalities (e.g., image vs. non-image); or 2) one modality is largely missing. In this paper, we address imputation of missing data by modeling the joint distribution of multi-modal data. Motivated by partial bidirectional generative adversarial net (PBiGAN), we propose a new Conditional PBiGAN (C-PBiGAN) method that imputes one modality combining the conditional knowledge from another modality. Specifically, C-PBiGAN introduces a conditional latent space in a missing imputation framework that jointly encodes the available multi-modal data, along with a class regularization loss on imputed data to recover discriminative information. To our knowledge, it is the first generative adversarial model that addresses multi-modal missing imputation by modeling the joint distribution of image and non-image data. We validate our model with both the national lung screening trial (NLST) dataset and an external clinical validation cohort. The proposed C-PBiGAN achieves significant improvements in lung cancer risk estimation compared with representative imputation methods (e.g., AUC values increase in both NLST (+2.9\%) and in-house dataset (+4.3\%) compared with PBiGAN, p$&lt;$0.05).      
### 23.A Study on Speech Enhancement Based on Diffusion Probabilistic Model  [ :arrow_down: ](https://arxiv.org/pdf/2107.11876.pdf)
>  Diffusion probabilistic models have demonstrated an outstanding capability to model natural images and raw audio waveforms through a paired diffusion and reverse processes. The unique property of the reverse process (namely, eliminating non-target signals from the Gaussian noise and noisy signals) could be utilized to restore clean signals. Based on this property, we propose a diffusion probabilistic model-based speech enhancement (DiffuSE) model that aims to recover clean speech signals from noisy signals. The fundamental architecture of the proposed DiffuSE model is similar to that of DiffWave--a high-quality audio waveform generation model that has a relatively low computational cost and footprint. To attain better enhancement performance, we designed an advanced reverse process, termed the supportive reverse process, which adds noisy speech in each time-step to the predicted speech. The experimental results show that DiffuSE yields performance that is comparable to related audio generative models on the standardized Voice Bank corpus SE task. Moreover, relative to the generally suggested full sampling schedule, the proposed supportive reverse process especially improved the fast sampling, taking few steps to yield better enhancement results over the conventional full step inference process.      
### 24.Deep Learning Explicit Differentiable Predictive Control Laws for Buildings  [ :arrow_down: ](https://arxiv.org/pdf/2107.11843.pdf)
>  We present a differentiable predictive control (DPC) methodology for learning constrained control laws for unknown nonlinear systems. DPC poses an approximate solution to multiparametric programming problems emerging from explicit nonlinear model predictive control (MPC). Contrary to approximate MPC, DPC does not require supervision by an expert controller. Instead, a system dynamics model is learned from the observed system's dynamics, and the neural control law is optimized offline by leveraging the differentiable closed-loop system model. The combination of a differentiable closed-loop system and penalty methods for constraint handling of system outputs and inputs allows us to optimize the control law's parameters directly by backpropagating economic MPC loss through the learned system model. The control performance of the proposed DPC method is demonstrated in simulation using learned model of multi-zone building thermal dynamics.      
### 25.Distributed and Autonomous Aerial Data Collection in Smart City Surveillance Applications  [ :arrow_down: ](https://arxiv.org/pdf/2107.11790.pdf)
>  The massive growth of Smart City and Internet of Things applications enables safety and security. The data those are produced from surveillance cameras in aerial devices such as unmanned aerial networks (UAVs) are needed to be transferred to ground stations for secure data analysis. When the scale of network is relatively large compare to the wireless communication coverage of device, it is not always available to transmit the data to the ground stations, thus distributed and autonomous algorithms are essentially desired. Based on the needs, we propose a novel algorithm that is for collecting surveillance data under the consideration of mobility and flexibility of UAV networks. Due to the battery limitation in UAVs, we selectively collect data from the UAVs by setting rules under the consideration of distance and similarity. As a sequence, the UAV devices have to compete for a chance to get data processing. For this purpose, this paper designs a Myerson auction-based deep learning algorithm to leverage the UAV's revenue compare to traditional second-price auction while preserving truthfulness. Based on simulation results, we verify that our proposed algorithm achieves desired performance improvements.      
### 26.Deep Learning-based Frozen Section to FFPE Translation  [ :arrow_down: ](https://arxiv.org/pdf/2107.11786.pdf)
>  Frozen sectioning (FS) is the preparation method of choice for microscopic evaluation of tissues during surgical operations. The high speed of procedure allows pathologists to rapidly assess the key microscopic features, such as tumor margins and malignant status to guide surgical decision-making and minimise disruptions to the course of the operation. However, FS is prone to introducing many misleading artificial structures (histological artefacts), such as nuclear ice crystals, compression, and cutting artefacts, hindering timely and accurate diagnostic judgement of the pathologist. On the other hand, the gold standard tissue preparation technique of formalin-fixation and paraffin-embedding (FFPE) provides significantly superior image quality, but is a very time-consuming process (12-48 hours), making it unsuitable for intra-operative use. In this paper, we propose an artificial intelligence (AI) method that improves FS image quality by computationally transforming frozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style images in minutes. AI-FFPE rectifies FS artefacts with the guidance of an attention-mechanism that puts a particular emphasis on artefacts while utilising a self-regularization mechanism established between FS input image and synthesized FFPE-style image that preserves clinically relevant features. As a result, AI-FFPE method successfully generates FFPE-style images without significantly extending tissue processing time and consequently improves diagnostic accuracy.      
### 27.Dispatch of Virtual Inertia and Damping: Numerical Method with SDP and ADMM  [ :arrow_down: ](https://arxiv.org/pdf/2107.11764.pdf)
>  Power grids are evolving toward 100% renewable energy interfaced by inverters. Virtual inertia and damping provided by inverters are essential to synchronism and frequency stability of future power grids. This paper numerically addresses the problem of dispatch of virtual inertia and damping (DID) among inverters in the transmission network. The DID problem is first formulated as a nonlinear program (NLP) by the Radua collocation method which is flexible to handle various types of disturbances and bounds constraints. Since the NLP of DID is highly non-convex, semi-definite programming (SDP) relaxation for the NLP is further derived to tackle the non-convexity, followed by its sparsity being exploited hierarchically based on chordality of graphs to seek enhancement of computational efficiency. Considering high dimension and inexactness of the SDP relaxation, a feasibility-embedded distributed approach is finally proposed under the framework of alternating direction method of multipliers (ADMM), which achieves parallel computing and solution feasibility regarding the original NLP. Numerical simulations carried out for five test power systems demonstrate the proposed method and necessity of DID.      
### 28.Bipartite Consensus in the Presence of Denial of Service Adversary  [ :arrow_down: ](https://arxiv.org/pdf/2107.11729.pdf)
>  Attacks on a set of agents with cooperative and antagonistic interactions attempting to achieve linear bipartite consensus are considered here. In bipartite consensus, two clusters are formed, agents in each cluster converging to a final state which is negative of the other cluster's final state. The adversary seeks to slow down the bipartite consensus by a Denial of Service (DoS) type attack where the attacker has the capability to break a specific number of links at each time instant. The problem is formulated as an optimal control problem and the optimal strategy for the adversary is determined.      
### 29.A 51.3 TOPS/W, 134.4 GOPS In-memory Binary Image Filtering in 65nm CMOS  [ :arrow_down: ](https://arxiv.org/pdf/2107.11723.pdf)
>  Neuromorphic vision sensors (NVS) can enable energy savings due to their event-driven that exploits the temporal redundancy in video streams from a stationary camera. However, noise-driven events lead to the false triggering of the object recognition processor. Image denoise operations require memoryintensive processing leading to a bottleneck in energy and latency. In this paper, we present in-memory filtering (IMF), a 6TSRAM in-memory computing based image denoising for eventbased binary image (EBBI) frame from an NVS. We propose a non-overlap median filter (NOMF) for image denoising. An inmemory computing framework enables hardware implementation of NOMF leveraging the inherent read disturb phenomenon of 6T-SRAM. To demonstrate the energy-saving and effectiveness of the algorithm, we fabricated the proposed architecture in a 65nm CMOS process. As compared to fully digital implementation, IMF enables &gt; 70x energy savings and a &gt; 3x improvement of processing time when tested with the video recordings from a DAVIS sensor and achieves a peak throughput of 134.4 GOPS. Furthermore, the peak energy efficiencies of the NOMF is 51.3 TOPS/W, comparable with state of the art inmemory processors. We also show that the accuracy of the images obtained by NOMF provide comparable accuracy in tracking and classification applications when compared with images obtained by conventional median filtering.      
### 30.Deep-learning-driven Reliable Single-pixel Imaging with Uncertainty Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2107.11678.pdf)
>  Single-pixel imaging (SPI) has the advantages of high-speed acquisition over a broad wavelength range and system compactness, which are difficult to achieve by conventional imaging sensors. However, a common challenge is low image quality arising from undersampling. Deep learning (DL) is an emerging and powerful tool in computational imaging for many applications and researchers have applied DL in SPI to achieve higher image quality than conventional reconstruction approaches. One outstanding challenge, however, is that the accuracy of DL predictions in SPI cannot be assessed in practical applications where the ground truths are unknown. Here, we propose the use of the Bayesian convolutional neural network (BCNN) to approximate the uncertainty (coming from finite training data and network model) of the DL predictions in SPI. Each pixel in the predicted result from BCNN represents the parameter of a probability distribution rather than the image intensity value. Then, the uncertainty can be approximated with BCNN by minimizing a negative log-likelihood loss function in the training stage and Monte Carlo dropout in the prediction stage. The results show that the BCNN can reliably approximate the uncertainty of the DL predictions in SPI with varying compression ratios and noise levels. The predicted uncertainty from BCNN in SPI reveals that most of the reconstruction errors in deep-learning-based SPI come from the edges of the image features. The results show that the proposed BCNN can provide a reliable tool to approximate the uncertainty of DL predictions in SPI and can be widely used in many applications of SPI.      
### 31.Synthesis-guided Adversarial Scenario Generation for Gray-box Feedback Control Systems with Sensing Imperfections  [ :arrow_down: ](https://arxiv.org/pdf/2107.11667.pdf)
>  In this paper, we study feedback dynamical systems with memoryless controllers under imperfect information. We develop an algorithm that searches for "adversarial scenarios", which can be thought of as the strategy for the adversary representing the noise and disturbances, that lead to safety violations. The main challenge is to analyze the closed-loop system's vulnerabilities with a potentially complex or even unknown controller in the loop. As opposed to commonly adopted approaches that treat the system under test as a black-box, we propose a synthesis-guided approach, which leverages the knowledge of a plant model at hand. This hence leads to a way to deal with gray-box systems (i.e., with known plant and unknown controller). Our approach reveals the role of the imperfect information in the violation. Examples show that our approach can find non-trivial scenarios that are difficult to expose by random simulations. This approach is further extended to incorporate model mismatch and to falsify vision-in-the-loop systems against finite-time reach-avoid specifications.      
### 32.Utilizing the Structure of the Curvelet Transform with Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2107.11664.pdf)
>  The discrete curvelet transform decomposes an image into a set of fundamental components that are distinguished by direction and size as well as a low-frequency representation. The curvelet representation is approximately sparse; thus, it is a useful sparsifying transformation to be used with compressed sensing. Although the curvelet transform of a natural image is sparse, the low-frequency portion is not. This manuscript presents a method to modify the sparsifying transformation to take advantage of this fact. Instead of relying on sparsity for this low-frequency estimate, the Nyquist-Shannon theorem specifies a square region to be collected centered on the $0$ frequency. A Basis Pursuit Denoising problem is solved to determine the missing details after modifying the sparisfying transformation to take advantage of the known fully sampled region. Finally, by taking advantage of this structure with a redundant dictionary comprised of both the wavelet and curvelet transforms, additional gains in quality are achieved.      
### 33.Lightweight Hardware Design of the Inverse Transform Module for 4K ASIC VVC Decoders  [ :arrow_down: ](https://arxiv.org/pdf/2107.11659.pdf)
>  Versatile Video Coding (VVC) is the next generation video coding standard finalized in July 2020. VVC introduces new coding tools enhancing the coding efficiency compared to its predecessor High Efficiency Video Coding (HEVC). These new tools have a significant impact on the VVC software decoder complexity estimated to 2 times HEVC decoder complexity. In particular, the transform module includes in VVC separable and non-separable transforms named Multiple Transform Selection (MTS) and Low Frequency Non-Separable Transform (LFNST) tools, respectively. In this paper, we present an area-efficient hardware architecture of the inverse transform module for a VVC decoder. The proposed design uses a total of 64 regular multipliers in a pipelined architecture targeting ASIC platforms. It consists in a multi-standard architecture that supports the transform modules of recent MPEG standards including AVC, HEVC and VVC. The architecture leverages all primary and secondary transforms optimisations including butterfly decomposition, coefficients zeroing and the inherent linear relationship between the transforms. The synthesized results show that the proposed method sustains a constant throughput of 1 sample per cycle and a constant latency for all block sizes. The proposed hardware inverse transform module operates at 600 MHz frequency enabling to decode in real-time 4K video at 30 frames per second in 4:2:2 chroma sub-sampling format. The proposed module is integrated in an ASIC UHD decoder targeting energy-aware decoding of VVC videos on consumer devices.      
### 34.Accelerated MRI Reconstruction with Separable and Enhanced Low-Rank Hankel Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2107.11650.pdf)
>  The combination of the sparse sampling and the low-rank structured matrix reconstruction has shown promising performance, enabling a significant reduction of the magnetic resonance imaging data acquisition time. However, the low-rank structured approaches demand considerable memory consumption and are time-consuming due to a noticeable number of matrix operations performed on the huge-size block Hankel-like matrix. In this work, we proposed a novel framework to utilize the low-rank property but meanwhile to achieve faster reconstructions and promising results. The framework allows us to enforce the low-rankness of Hankel matrices constructing from 1D vectors instead of 2D matrices from 1D vectors and thus avoid the construction of huge block Hankel matrix for 2D k-space matrices. Moreover, under this framework, we can easily incorporate other information, such as the smooth phase of the image and the low-rankness in the parameter dimension, to further improve the image quality. We built and validated two models for parallel and parameter magnetic resonance imaging experiments, respectively. Our retrospective in-vivo results indicate that the proposed approaches enable faster reconstructions than the state-of-the-art approaches, e.g., about 8x faster than STDLRSPIRiT, and faithful removal of undersampling artifacts.      
### 35.Dual-Attention Enhanced BDense-UNet for Liver Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2107.11645.pdf)
>  In this work, we propose a new segmentation network by integrating DenseUNet and bidirectional LSTM together with attention mechanism, termed as DA-BDense-UNet. DenseUNet allows learning enough diverse features and enhancing the representative power of networks by regulating the information flow. Bidirectional LSTM is responsible to explore the relationships between the encoded features and the up-sampled features in the encoding and decoding paths. Meanwhile, we introduce attention gates (AG) into DenseUNet to diminish responses of unrelated background regions and magnify responses of salient regions progressively. Besides, the attention in bidirectional LSTM takes into account the contribution differences of the encoded features and the up-sampled features in segmentation improvement, which can in turn adjust proper weights for these two kinds of features. We conduct experiments on liver CT image data sets collected from multiple hospitals by comparing them with state-of-the-art segmentation models. Experimental results indicate that our proposed method DA-BDense-UNet has achieved comparative performance in terms of dice coefficient, which demonstrates its effectiveness.      
### 36.Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform  [ :arrow_down: ](https://arxiv.org/pdf/2107.11627.pdf)
>  Fast and accurate simulation of imaging through atmospheric turbulence is essential for developing turbulence mitigation algorithms. Recognizing the limitations of previous approaches, we introduce a new concept known as the phase-to-space (P2S) transform to significantly speed up the simulation. P2S is build upon three ideas: (1) reformulating the spatially varying convolution as a set of invariant convolutions with basis functions, (2) learning the basis function via the known turbulence statistics models, (3) implementing the P2S transform via a light-weight network that directly convert the phase representation to spatial representation. The new simulator offers 300x -- 1000x speed up compared to the mainstream split-step simulators while preserving the essential turbulence statistics.      
### 37.Reflection and Relay Dual-Functional RIS Assisted MU-MISO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.11579.pdf)
>  Reconfigurable intelligent surfaces (RISs) have been deemed as one of potential components of future wireless communication systems because they can adaptively manipulate the wireless propagation environment with low-cost passive devices. However, due to double fading effect, the passive RIS can offer sufficient signal strength only when receivers are nearby and located at the same side as the incident signals. Moreover, RIS cannot provide service coverage for the users at the back side of it. In this paper we introduce a novel reflection and relay dual-functional RIS architecture, which can simultaneously realize passive reflection and active relay functionalities to enhance the coverage. The problem of joint transmit beamforming and dual-functional RIS design is investigated to maximize the achievable sum-rate of a multiuser multiple-input single-output (MU-MISO) system. Based on fractional programming (FP) theory and majorization-minimization (MM) technique, we propose an efficient iterative transmit beamforming and RIS design algorithm. Simulation results demonstrate the superiority of the introduced dual-functional RIS architecture and the effectiveness of the proposed algorithm.      
### 38.Reconstructing Images of Two Adjacent Objects through Scattering Medium Using Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2107.11574.pdf)
>  Reconstruction of image by using convolutional neural networks (CNNs) has been vigorously studied in the last decade. Until now, there have being developed several techniques for imaging of a single object through scattering medium by using neural networks, however how to reconstruct images of more than one object simultaneously seems hard to realize. In this paper, we demonstrate an approach by using generative adversarial network (GAN) to reconstruct images of two adjacent objects through scattering media. We construct an imaging system for imaging of two adjacent objects behind the scattering media. In general, as the light field of two adjacent object images pass through the scattering slab, a speckle pattern is obtained. The designed adversarial network, which is called as YGAN, is employed to reconstruct the images simultaneously. It is shown that based on the trained YGAN, we can reconstruct images of two adjacent objects from one speckle pattern with high fidelity. In addition, we study the influence of the object image types, and the distance between the two adjacent objects on the fidelity of the reconstructed images. Moreover even if another scattering medium is inserted between the two objects, we can also reconstruct the images of two objects from a speckle with high quality. The technique presented in this work can be used for applications in areas of medical image analysis, such as medical image classification, segmentation, and studies of multi-object scattering imaging etc.      
### 39.Multiradar Data Fusion for Respiratory Measurement of Multiple People  [ :arrow_down: ](https://arxiv.org/pdf/2107.11525.pdf)
>  This study proposes a data fusion method for multiradar systems to enable measurement of the respiration of multiple people located at arbitrary positions. Using the proposed method, the individual respiration rates of multiple people can be measured, even when echoes from some of these people cannot be received by one of the radar systems because of shadowing. In addition, the proposed method does not require information about the positions and orientations of the radar systems used because the method can estimate the layout of these radar systems by identifying multiple human targets that can be measured from different angles using multiple radar systems. When a single target person can be measured using multiple radar systems simultaneously, the proposed method selects an accurate signal from among the multiple signals based on the spectral characteristics. To verify the effectiveness of the proposed method, we performed experiments based on two scenarios with different layouts that involved seven participants and two radar systems. Through these experiments, the proposed method was demonstrated to be capable of measuring the respiration of all seven people by overcoming the shadowing issue. In the two scenarios, the average errors of the proposed method in estimating the respiration rates were 0.33 and 1.24 respirations per minute (rpm), respectively, thus demonstrating accurate and simultaneous respiratory measurements of multiple people using the multiradar system.      
### 40.Primary-Auxiliary Model Scheduling Based Estimation of the Vertical Wheel Force in a Full Vehicle System  [ :arrow_down: ](https://arxiv.org/pdf/2107.11511.pdf)
>  In this work, we study estimation problems in nonlinear mechanical systems subject to non-stationary and unknown excitation, which are common and critical problems in design and health management of mechanical systems. <br>A primary-auxiliary model scheduling procedure based on time-domain transmissibilities is proposed and performed under switching linear dynamics: In addition to constructing a primary transmissibility family from the pseudo-inputs to the output during the offline stage, an auxiliary transmissibility family is constructed by further decomposing the pseudo-input vector into two parts. The auxiliary family enables to determine the unknown working condition at which the system is currently running at, and then an appropriate transmissibility from the primary transmissibility family for estimating the unknown output can be selected during the online estimation stage. As a result, the proposed approach offers a generalizable and explainable solution to the signal estimation problems in nonlinear mechanical systems in the context of switching linear dynamics with unknown inputs. <br>A real-world application to the estimation of the vertical wheel force in a full vehicle system are, respectively, conducted to demonstrate the effectiveness of the proposed method. During the vehicle design phase, the vertical wheel force is the most important one among Wheel Center Loads (WCLs), and it is often measured directly with expensive, intrusive, and hard-to-install measurement devices during full vehicle testing campaigns. Meanwhile, the estimation problem of the vertical wheel force has not been solved well and is still of great interest. The experimental results show good performances of the proposed method in the sense of estimation accuracy for estimating the vertical wheel force.      
### 41.Use of speaker recognition approaches for learning timbre representations of musical instrument sounds from raw waveforms  [ :arrow_down: ](https://arxiv.org/pdf/2107.11506.pdf)
>  Timbre representations of musical instruments, essential for diverse applications such as musical audio synthesis and separation, might be learned as bottleneck features from an instrumental recognition model. Given the similarities between speaker recognition and musical instrument recognition, in this paper, we investigate how to adapt successful speaker recognition algorithms to musical instrument recognition to learn meaningful instrumental timbre representations. To address the mismatch between musical audio and models devised for speech, we introduce a group of trainable filters to generate proper acoustic features from input raw waveforms, making it easier for a model to be optimized in an input-agnostic and end-to-end manner. Through experiments on both the NSynth and RWC databases in both musical instrument closed-set identification and open-set verification scenarios, the modified speaker recognition model was capable of generating discriminative embeddings for instrument and instrument-family identities. We further conducted extensive experiments to characterize the encoded information in learned timbre embeddings.      
### 42.Low Complexity Hybrid Precoding Designs for Multiuser mmWave/THz Ultra Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.11499.pdf)
>  Millimeter-wave and terahertz technologies have been attracting attention from the wireless research community since they can offer large underutilized bandwidths which can enable the support of ultra-high-speed connections in future wireless communication systems. While the high signal attenuation occurring at these frequencies requires the adoption of very large (or the so-called ultra-massive) antenna arrays, in order to accomplish low complexity and low power consumption, hybrid analog/digital designs must be adopted. In this paper we present a hybrid design algorithm suitable for both mmWave and THz multiuser multiple-input multiple-output (MIMO) systems, which comprises separate computation steps for the digital precoder, analog precoder and multiuser interference mitigation. The design can also incorporate different analog architectures such as phase shifters, switches and inverters, antenna selection and so on. Furthermore, it is also applicable for different structures namely, fully connected, arrays of subarrays (AoSA) and dynamic arrays of subarrays (DAoSA), making it suitable for the support of ultra-massive MIMO (UM-MIMO) in severely hardware constrained THz systems. We will show that, by using the proposed approach, it is possible to achieve good trade-offs between spectral efficiency and simplified implementation, even as the number of users and data streams increases.)      
### 43.A Novel Mathematical Model for Infrastructure Planning of Dynamic Wireless Power Transfer Systems for Electric Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2107.11428.pdf)
>  About 26% of total U.S. energy consumption is used in the transportation sector. Conventional vehicles use fuels such as gasoline, emit harmful gases, and have adverse effects on the environment. Electric vehicles (EVs) provide an alternative solution that decreases the dependency on traditional fuels such as gasoline and reduces hazardous gas emissions. EVs can drive longer distances by employing dynamic wireless power transfer systems (DWPT) without increasing their battery size or having stopovers. Additionally, developing a decision system that avoids an excessive load on the power grid is essential. These decision systems are particularly beneficial for autonomous driving for personal and public transportation. This study briefly reviews the available literature in dynamic wireless power transfer systems and proposes a novel system-level mathematical decision model to find the optimal profile for wireless charging infrastructures. We analyze the role of renewable energy integration on DWPT systems and identify the framework, benefits, and challenges in implementing DWPT for EVs. The mathematical model is mixed-integer, multi-period, and linear, minimizing the total system cost while satisfying the systems requirements. The proposed model and the case study analysis in this research determine the near-optimal plan for DWPT infrastructure allocations and pave the road toward a more detailed power grid and renewable energy integration. Our result indicates that renewable energies can significantly decrease the DWPT total system cost, infrastructure requirements and increase the EVs' reliability. Keywords: Dynamic wireless power transfer, Renewable energy integration, Electric vehicles, Power grid planning, Wireless charging allocation, Infrastructure planning, Mixed-integer optimization.      
### 44.Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations  [ :arrow_down: ](https://arxiv.org/pdf/2107.12346.pdf)
>  Voice conversion (VC) consists of digitally altering the voice of an individual to manipulate part of its content, primarily its identity, while maintaining the rest unchanged. Research in neural VC has accomplished considerable breakthroughs with the capacity to falsify a voice identity using a small amount of data with a highly realistic rendering. This paper goes beyond voice identity and presents a neural architecture that allows the manipulation of voice attributes (e.g., gender and age). Leveraging the latest advances on adversarial learning of structured speech representation, a novel structured neural network is proposed in which multiple auto-encoders are used to encode speech as a set of idealistically independent linguistic and extra-linguistic representations, which are learned adversariarly and can be manipulated during VC. Moreover, the proposed architecture is time-synchronized so that the original voice timing is preserved during conversion which allows lip-sync applications. Applied to voice gender conversion on the real-world VCTK dataset, our proposed architecture can learn successfully gender-independent representation and convert the voice gender with a very high efficiency and naturalness.      
### 45.Giga-voxel multidimensional fluorescence imaging combining single-pixel detection and data fusion  [ :arrow_down: ](https://arxiv.org/pdf/2107.12337.pdf)
>  Time-resolved fluorescence imaging is a key tool in biomedical applications, as it allows to non-invasively obtain functional and structural information. However, the big amount of collected data introduces challenges in both acquisition speed and processing needs. Here, we introduce a novel technique that allows to reconstruct a Giga-voxel 4D hypercube in a fast manner while only measuring 0.03 % of the information. The system combines two single-pixel cameras and a conventional 2D array detector working in parallel. Data fusion techniques are introduced to combine the individual 2D and 3D projections acquired by each sensor in the final high-resolution 4D hypercube, which can be used to identify different fluorophore species by their spectral and temporal signatures.      
### 46.Embedding Signals on Knowledge Graphs with Unbalanced Diffusion Earth Mover's Distance  [ :arrow_down: ](https://arxiv.org/pdf/2107.12334.pdf)
>  In modern relational machine learning it is common to encounter large graphs that arise via interactions or similarities between observations in many domains. Further, in many cases the target entities for analysis are actually signals on such graphs. We propose to compare and organize such datasets of graph signals by using an earth mover's distance (EMD) with a geodesic cost over the underlying graph. Typically, EMD is computed by optimizing over the cost of transporting one probability distribution to another over an underlying metric space. However, this is inefficient when computing the EMD between many signals. Here, we propose an unbalanced graph earth mover's distance that efficiently embeds the unbalanced EMD on an underlying graph into an $L^1$ space, whose metric we call unbalanced diffusion earth mover's distance (UDEMD). This leads us to an efficient nearest neighbors kernel over many signals defined on a large graph. Next, we show how this gives distances between graph signals that are robust to noise. Finally, we apply this to organizing patients based on clinical notes who are modelled as signals on the SNOMED-CT medical knowledge graph, embedding lymphoblast cells modeled as signals on a gene graph, and organizing genes modeled as signals over a large peripheral blood mononuclear (PBMC) cell graph. In each case, we show that UDEMD-based embeddings find accurate distances that are highly efficient compared to other methods.      
### 47.Uplink Data Detection Analysis of 1-Bit Quantized Massive MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2107.12331.pdf)
>  This paper presents an analytical framework for the data detection in massive multiple-input multiple-output uplink systems with 1-bit analog-to-digital converters (ADCs). Considering the single-user case, we provide closed-form expressions of the expected value and the variance of the estimated symbols when maximum ratio combining is adopted at the base station (BS) along with their asymptotic behavior at high signal-to-noise ratio (SNR). These results are exploited to enhance the performance of maximum likelihood detection by taking into account the dispersion of the estimated symbols about their expected values. The symbol error rate with 1-bit ADCs is evaluated with respect to the number of BS antennas, the SNR, and the pilot length used for the channel estimation. The proposed analysis highlights a fundamental SNR trade-off, according to which operating at the right SNR considerably improves the data detection accuracy.      
### 48.Effective Capacity Analysis of HARQ-enabled D2D Communication in Multi-Tier Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.12217.pdf)
>  This work does the statistical quality-of-service (QoS) analysis of a block-fading device-to-device (D2D) link in a multi-tier cellular network that consists of a macro-BS (BSMC) and a micro-BS (BSmC) which both operate in full-duplex (FD) mode. For the D2D link under consideration, we first formulate the mode selection problem-whereby D2D pair could either communicate directly, or, through the BSmC, or, through the BSMC-as a ternary hypothesis testing problem. Next, to compute the effective capacity (EC) for the given D2D link, we assume that the channel state information (CSI) is not available at the transmit D2D node, and hence, it transmits at a fixed rate r with a fixed power. This allows us to model the D2D link as a Markov system with six-states. We consider both overlay and underlay modes for the D2D link. Moreover, to improve the throughput of the D2D link, we assume that the D2D pair utilizes two special automatic repeat request (ARQ) schemes, i.e., Hybrid-ARQ (HARQ) and truncated HARQ. Furthermore, we consider two distinct queue models at the transmit D2D node, based upon how it responds to the decoding failure at the receive D2D node. Eventually, we provide closed-form expressions for the EC for both HARQ-enabled D2D link and truncated HARQ-enabled D2D link, under both queue models. Noting that the EC looks like a quasi-concave function of r, we further maximize the EC by searching for an optimal rate via the gradient-descent method. Simulation results provide us the following insights: i) EC decreases with an increase in the QoS exponent, ii) EC of the D2D link improves when HARQ is employed, iii) EC increases with an increase in the quality of self-interference cancellation techniques used at BSmC and BSMC in FD mode.      
### 49.The Role of Functional Programming in Management and Orchestration of Virtualized Network Resources Part I. System structure for Complex Systems and Design Principles  [ :arrow_down: ](https://arxiv.org/pdf/2107.12136.pdf)
>  This is part I of the follow-up lecture notes of the lectures given by the authors at the Three \CO" (Composability, Comprehensibility, Correctness) Winter School held in Koice, Slovakia, in January 2018, and Summer School held in Budapest, Hungary, in June 2019. In this part we explain the role of functional programming paradigm in the management of complex software systems, and how the functional programming concepts play important role in the designing such systems. Key prerequisite for implementing functional programming concepts is properly designed system structure following well defined design principles and rules. That is the main goal of this lecture to introduce students with proper system modeling. Furthermore, we also explain how new emerging technologies are designed in such a way that they enforce the development of systems that comply to the design rules inspired by the functional programming. This is extremely important in view of the current network evolution and virtualization concepts, which will require many functional programming concepts in the network services and functions, as will be discussed in part II of these lecture notes. These notes provide an introduction to the subject, with the goal of explaining the problems and the principles, methods and techniques used for their solution. The worked examples and exercises serve students as the teaching material, from which they can learn how to use design principles to model effective system structures. Here we focus on students understanding of importance of effective system structures for coordination of development and management processes that are driven by business goals and further evolution.      
### 50.Development of a 3D Digital Twin of the Swalmen Tunnel in the Rijkswaterstaat Project  [ :arrow_down: ](https://arxiv.org/pdf/2107.12108.pdf)
>  In an ongoing project, a cooperation between the TU/e and the Dutch Department of Waterways and Public Works (Rijkswaterstaat in Dutch, abbreviated to RWS) is established. The project focuses on investigating applicability of synthesis-based engineering in the design of supervisory controllers for bridges, waterways and tunnels. Supervisory controllers ensure correct cooperation between components in a system. The design process of these controllers partly relies on simulation with models of the plant (the physical system). A possible addition to this design process is digital twin technology. A digital twin is a virtual copy of a system that is generally much more realistic than the 2D simulation models that are currently used for supervisory controller validation. In this report, the development of a digital twin of the Swalmen tunnel that is suitable for supervisory control validation is described. The Swalmen tunnel is a highway tunnel in Limburg, the Netherlands. This case study is relevant, because the Swalmen tunnel will be renovated in 2023 and 2028. These renovation projects include updating controlled subsystems in the tunnel, such as boom barriers and traffic lights, and updating the supervisory controller of the tunnel. The digital twin might be useful to aid the supervisory controller design process in these renovation projects.      
### 51.Position-agnostic Algebraic Estimation of 6G V2X MIMO Channels via Unsupervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.12098.pdf)
>  MIMO systems in the context of 6G Vehicle-to-Everything (V2X) will require an accurate channel knowledge to enable efficient communication. Standard channel estimation techniques, such as Unconstrained Maximum Likelihood (U-ML), are extremely noisy in massive MIMO settings, while structured approaches, e.g., compressed sensing, are suited to low-mobility scenarios and are sensitive to hardware impairments. We propose a novel Multi-Vehicular algebraic channel estimation method for 6G V2X based on unsupervised learning which exploits recurrent vehicle passages in typical urban settings. Multiple training sequences are clustered via K-medoids algorithm based on their \textit{algebraic similarity} to retrieve the MIMO channel eigenmodes, which can be used to improve the channel estimates. Numerical results show remarkable benefits of the proposed method in terms of Mean Squared Error (MSE) compared to standard U-ML solution (15 dB less).      
### 52.6DCNN with roto-translational convolution filters for volumetric data processing  [ :arrow_down: ](https://arxiv.org/pdf/2107.12078.pdf)
>  In this work, we introduce 6D Convolutional Neural Network (6DCNN) designed to tackle the problem of detecting relative positions and orientations of local patterns when processing three-dimensional volumetric data. 6DCNN also includes SE(3)-equivariant message-passing and nonlinear activation operations constructed in the Fourier space. Working in the Fourier space allows significantly reducing the computational complexity of our operations. We demonstrate the properties of the 6D convolution and its efficiency in the recognition of spatial patterns. We also assess the 6DCNN model on several datasets from the recent CASP protein structure prediction challenges. Here, 6DCNN improves over the baseline architecture and also outperforms the state of the art.      
### 53.Virtual Drive-Tests: A Case for Predicting QoE in Adaptive Video Streaming  [ :arrow_down: ](https://arxiv.org/pdf/2107.12068.pdf)
>  Intelligent and autonomous troubleshooting is a crucial enabler for the current 5G and future 6G networks. In this work, we develop a flexible architecture for detecting anomalies in adaptive video streaming comprising three main components: i) A pattern recognizer that learns a typical pattern for video quality from the client-side application traces of a specific reference video, ii) A predictor for mapping Radio Frequency (RF) performance indicators collected on the network-side using user-based traces to a video quality measure, iii) An anomaly detector for comparing the predicted video quality pattern with the typical pattern to identify anomalies. We use real network traces (i.e., on-device measurements) collected in different geographical locations and at various times of day to train our machine learning models. We perform extensive numerical analysis to demonstrate key parameters impacting correct video quality prediction and anomaly detection. In particular, we have shown that the video playback time is the most crucial parameter determining the video quality since buffering continues during the playback and resulting in better video quality further into the playback. However, we also reveal that RF performance indicators characterizing the quality of the cellular connectivity are required to correctly predict QoE in anomalous cases. Then, we have exhibited that the mean maximum F1-score of our method is 77%, verifying the efficacy of our models. Our architecture is flexible and autonomous, so one can apply it to -- and operate with -- other user applications as long as the relevant user-based traces are available.      
### 54.Provably Accelerated Decentralized Gradient Method Over Unbalanced Directed Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2107.12065.pdf)
>  In this work, we consider the decentralized optimization problem in which a network of $n$ agents, each possessing a smooth and convex objective function, wish to collaboratively minimize the average of all the objective functions through peer-to-peer communication in a directed graph. To solve the problem, we propose two accelerated Push-DIGing methods termed APD and APD-SC for minimizing non-strongly convex objective functions and strongly convex ones, respectively. We show that APD and APD-SC respectively converge at the rates $O\left(\frac{1}{k^2}\right)$ and $O\left(\left(1 - C\sqrt{\frac{\mu}{L}}\right)^k\right)$ up to constant factors depending only on the mixing matrix. To the best of our knowledge, APD and APD-SC are the first decentralized methods to achieve provable acceleration over unbalanced directed graphs. Numerical experiments demonstrate the effectiveness of both methods.      
### 55.SVEva Fair: A Framework for Evaluating Fairness in Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2107.12049.pdf)
>  Despite the success of deep neural networks (DNNs) in enabling on-device voice assistants, increasing evidence of bias and discrimination in machine learning is raising the urgency of investigating the fairness of these systems. Speaker verification is a form of biometric identification that gives access to voice assistants. Due to a lack of fairness metrics and evaluation frameworks that are appropriate for testing the fairness of speaker verification components, little is known about how model performance varies across subgroups, and what factors influence performance variation. To tackle this emerging challenge, we design and develop SVEva Fair, an accessible, actionable and model-agnostic framework for evaluating the fairness of speaker verification components. The framework provides evaluation measures and visualisations to interrogate model performance across speaker subgroups and compare fairness between models. We demonstrate SVEva Fair in a case study with end-to-end DNNs trained on the VoxCeleb datasets to reveal potential bias in existing embedded speech recognition systems based on the demographic attributes of speakers. Our evaluation shows that publicly accessible benchmark models are not fair and consistently produce worse predictions for some nationalities, and for female speakers of most nationalities. To pave the way for fair and reliable embedded speaker verification, SVEva Fair has been implemented as an open-source python library and can be integrated into the embedded ML development pipeline to facilitate developers and researchers in troubleshooting unreliable speaker verification performance, and selecting high impact approaches for mitigating fairness challenges      
### 56.Workpiece Image-based Tool Wear Classification in Blanking Processes Using Deep Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2107.12034.pdf)
>  Blanking processes belong to the most widely used manufacturing techniques due to their economic efficiency. Their economic viability depends to a large extent on the resulting product quality and the associated customer satisfaction as well as on possible downtimes. In particular, the occurrence of increased tool wear reduces the product quality and leads to downtimes, which is why considerable research has been carried out in recent years with regard to wear detection. While processes have widely been monitored based on force and acceleration signals, a new approach is pursued in this paper. Blanked workpieces manufactured by punches with 16 different wear states are photographed and then used as inputs for Deep Convolutional Neural Networks to classify wear states. The results show that wear states can be predicted with surprisingly high accuracy, opening up new possibilities and research opportunities for tool wear monitoring of blanking processes.      
### 57.Joint Direction and Proximity Classification of Overlapping Sound Events from Binaural Audio  [ :arrow_down: ](https://arxiv.org/pdf/2107.12033.pdf)
>  Sound source proximity and distance estimation are of great interest in many practical applications, since they provide significant information for acoustic scene analysis. As both tasks share complementary qualities, ensuring efficient interaction between these two is crucial for a complete picture of an aural environment. In this paper, we aim to investigate several ways of performing joint proximity and direction estimation from binaural recordings, both defined as coarse classification problems based on Deep Neural Networks (DNNs). Considering the limitations of binaural audio, we propose two methods of splitting the sphere into angular areas in order to obtain a set of directional classes. For each method we study different model types to acquire information about the direction-of-arrival (DoA). Finally, we propose various ways of combining the proximity and direction estimation problems into a joint task providing temporal information about the onsets and offsets of the appearing sources. Experiments are performed for a synthetic reverberant binaural dataset consisting of up to two overlapping sound events.      
### 58.Facetron: Multi-speaker Face-to-Speech Model based on Cross-modal Latent Representations  [ :arrow_down: ](https://arxiv.org/pdf/2107.12003.pdf)
>  In this paper, we propose an effective method to synthesize speaker-specific speech waveforms by conditioning on videos of an individual's face. Using a generative adversarial network (GAN) with linguistic and speaker characteristic features as auxiliary conditions, our method directly converts face images into speech waveforms under an end-to-end training framework. The linguistic features are extracted from lip movements using a lip-reading model, and the speaker characteristic features are predicted from face images using cross-modal learning with a pre-trained acoustic model. Since these two features are uncorrelated and controlled independently, we can flexibly synthesize speech waveforms whose speaker characteristics vary depending on the input face images. Therefore, our method can be regarded as a multi-speaker face-to-speech waveform model. We show the superiority of our proposed model over conventional methods in terms of both objective and subjective evaluation results. Specifically, we evaluate the performances of the linguistic feature and the speaker characteristic generation modules by measuring the accuracy of automatic speech recognition and automatic speaker/gender recognition tasks, respectively. We also evaluate the naturalness of the synthesized speech waveforms using a mean opinion score (MOS) test.      
### 59.Adding air attenuation to simulated room impulse responses: A modal approach  [ :arrow_down: ](https://arxiv.org/pdf/2107.11871.pdf)
>  Air absorption is an important effect to consider when simulating room acoustics as it leads to significant attenuation in high frequencies. In this study, an offline method for adding air absorption to simulated room impulse responses is devised. The proposed method is based on a modal scheme for a system of one-dimensional dissipative wave equations, which can be used to post-process a room impulse response simulated without air absorption, thereby incorporating missing frequency-dependent distance-based air attenuation. Numerical examples are presented to evaluate the proposed method, along with comparisons to existing filter-based methods.      
### 60.Wavelet Selection and Employment for Side-Channel Disassembly  [ :arrow_down: ](https://arxiv.org/pdf/2107.11870.pdf)
>  Side-channel analysis, originally used in cryptanalysis is growing in use cases, both offensive and defensive. Wavelet analysis is a commonly employed time-frequency analysis technique used across disciplines, with a variety of purposes, and has shown increasing prevalence within side-channel literature. This paper explores wavelet selection and analysis parameters for use in side-channel analysis, particularly power side-channel-based instruction disassembly and classification. Experiments are conducted on an ATmega328P microcontroller and a subset of the AVR instruction set. Classification performance is evaluated with a time-series convolutional neural network (CNN) at clock-cycle fidelity. This work demonstrates that wavelet selection and employment parameters have meaningful impact on analysis outcomes. Practitioners should make informed decisions and consider optimizing these factors similarly to machine learning architecture and hyperparameters. We conclude that the gaus1 wavelet with scales 1-21 and grayscale colormap provided the best balance of classification performance, time, and memory efficiency in our application.      
### 61.Reconfigurable Intelligent Surface Phase Hopping for Ultra-Reliable Communications  [ :arrow_down: ](https://arxiv.org/pdf/2107.11852.pdf)
>  We introduce a phase hopping scheme for reconfigurable intelligent surfaces (RISs) in which the phases of the individual RIS elements are randomly varied with each transmitted symbol. This effectively converts slow fading into fast fading. We show how this can be leveraged to significantly improve the outage performance and even achieve an outage probability of zero at a positive data rate without channel state information (CSI) at the transmitter and RIS. Furthermore, the same result can be accomplished even if only two possible phase values are available. Since we do not require perfect CSI at the transmitter or RIS, the proposed scheme has no additional communication overhead for adjusting the phases. This enables robust ultra-reliable communications with a reduced effort for channel estimation.      
### 62.Cough Detection from Acoustic signals for patient monitoring system  [ :arrow_down: ](https://arxiv.org/pdf/2107.11835.pdf)
>  Cough is one of the most common symptoms in all respiratory diseases. In cases like Chronic Obstructive Pulmonary Disease, Asthma, acute and chronic Bronchitis and the recent pandemic Covid-19, the early identification of cough is important to provide healthcare professionals with useful clinical information such as frequency, severity, and nature of cough to enable better diagnosis. This paper presents and demonstrates best feature selection using MFCC which can help to determine cough events, eventually helping a neural network to learn and improve accuracy of cough detection. The paper proposes to achieve performance of 97.77% Sensitivity (SE), 98.75% Specificity (SP) and 98.17% F1-score with a very light binary classification network of size close to 16K parameters, enabling fitment into smart IoT devices.      
### 63.A Survey of Monte Carlo Methods for Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2107.11820.pdf)
>  Statistical signal processing applications usually require the estimation of some parameters of interest given a set of observed data. These estimates are typically obtained either by solving a multi-variate optimization problem, as in the maximum likelihood (ML) or maximum a posteriori (MAP) estimators, or by performing a multi-dimensional integration, as in the minimum mean squared error (MMSE) estimators. Unfortunately, analytical expressions for these estimators cannot be found in most real-world applications, and the Monte Carlo (MC) methodology is one feasible approach. MC methods proceed by drawing random samples, either from the desired distribution or from a simpler one, and using them to compute consistent estimators. The most important families of MC algorithms are Markov chain MC (MCMC) and importance sampling (IS). On the one hand, MCMC methods draw samples from a proposal density, building then an ergodic Markov chain whose stationary distribution is the desired distribution by accepting or rejecting those candidate samples as the new state of the chain. On the other hand, IS techniques draw samples from a simple proposal density, and then assign them suitable weights that measure their quality in some appropriate way. In this paper, we perform a thorough review of MC methods for the estimation of static parameters in signal processing applications. A historical note on the development of MC schemes is also provided, followed by the basic MC method and a brief description of the rejection sampling (RS) algorithm, as well as three sections describing many of the most relevant MCMC and IS algorithms, and their combined use.      
### 64.Denoising and Segmentation of Epigraphical Scripts  [ :arrow_down: ](https://arxiv.org/pdf/2107.11801.pdf)
>  This paper is a presentation of a new method for denoising images using Haralick features and further segmenting the characters using artificial neural networks. The image is divided into kernels, each of which is converted to a GLCM (Gray Level Co-Occurrence Matrix) on which a Haralick Feature generation function is called, the result of which is an array with fourteen elements corresponding to fourteen features The Haralick values and the corresponding noise/text classification form a dictionary, which is then used to de-noise the image through kernel comparison. Segmentation is the process of extracting characters from a document and can be used when letters are separated by white space, which is an explicit boundary marker. Segmentation is the first step in many Natural Language Processing problems. This paper explores the process of segmentation using Neural Networks. While there have been numerous methods to segment characters of a document, this paper is only concerned with the accuracy of doing so using neural networks. It is imperative that the characters be segmented correctly, for failing to do so will lead to incorrect recognition by Natural language processing tools. Artificial Neural Networks was used to attain accuracy of upto 89%. This method is suitable for languages where the characters are delimited by white space. However, this method will fail to provide acceptable results when the language heavily uses connected letters. An example would be the Devanagari script, which is predominantly used in northern India.      
### 65.Multi-Rate Nyquist-SCM for C-Band 100Gbit/s Signal over 50km Dispersion-Uncompensated Link  [ :arrow_down: ](https://arxiv.org/pdf/2107.11792.pdf)
>  In this paper, to the best of our knowledge, we propose the first multi-rate Nyquist-subcarriers modulation (SCM) for C-band 100Gbit/s signal transmission over 50km dispersion-uncompensated link. Chromatic dispersion (CD) introduces severe spectral nulls on optical double-sideband signal, which greatly degrades the performance of intensity-modulation and direct-detection systems. In the previous works, high-complexity digital signal processing (DSP) is required to resist the CD-caused spectral nulls. Based on the characteristics of dispersive channel, Nyquist-SCM with multi-rate subcarriers is proposed to keep away from the CD-caused spectral nulls flexibly. Signal on each subcarrier can be individually recovered by a DSP with an acceptable complexity, including the feed-forward equalizer with no more than 31 taps, a two-tap post filter, and maximum likelihood sequence estimation with one memory length. Combining with entropy loading based on probabilistic constellation shaping to maximize the capacity-reach, the C-band 100Gbit/s multi-rate Nyquist-SCM signal over 50km dispersion-uncompensated link can achieve 7% hard-decision forward error correction limit and average normalized generalized mutual information of 0.967. In conclusion, the multi-rate Nyquist-SCM shows great potentials in solving the CD-caused spectral distortions.      
### 66.On the Scheduling and Power Control for Uplink Cellular-Connected UAV Communications  [ :arrow_down: ](https://arxiv.org/pdf/2107.11738.pdf)
>  Cellular connected unmanned aerial vehicle (UAV) has been identified as a promising paradigm and attracted a surge of research interest recently. Although the nearly line-of-sight (LoS) channels are favorable to receive higher powers, UAV can in turn cause severe interference to each other and to any other users in the same frequency band. In this contribution, we focus on the uplink communications of cellular-connected UAV. To cope with the severe interference among UAV-UEs, several different scheduling and power control algorithms are proposed to optimize the spectrum efficiency (SE) based on the geometrical programming (GP) principle together with the successive convex approximation (SCA) technique. The proposed schemes include maximizing the sum SE of UAVs, maximizing the minimum SE of UAVs, etc., applied in the frequency domain and/or the time domain. Moreover, the quality of service (QoS) constraint and the uplink single-carrier (SC) constraint are also considered. The performances of these power and resource allocation algorithms are evaluated via extensive simulations in both full buffer transmission mode and bursty traffic mode. Numerical results show that the proposed algorithms can effectively enhance the uplink SEs of cellular-connected UAVs.      
### 67.One-Leg Stance of Humanoid Robot using Active Balance Control  [ :arrow_down: ](https://arxiv.org/pdf/2107.11703.pdf)
>  The task of self-balancing is one of the most important tasks when developing humanoid robots. This paper proposes a novel external balance mechanism for humanoid robot to maintain sideway balance. First, a dynamic model of the humanoid robot with balance mechanism and its simplified model are introduced. Secondly, a backstepping-based control method is utilized to split the system into two sub-systems. Then, a minimum observer-based controller is used to control the first sub-system. Since the second sub-system has unknown parameters, a model reference adaptive controller (MRAC) is used to control it. The proposed design divides the walking and balancing into two separated tasks, allowing the walking control can be executed independently of the balancing control. Furthermore, the use of the balance mechanism ensures the humanoid robot's hip movement does not exceed the threshold of a human when walking. Thus, making the overall pose of the humanoid robot looks more natural. An experiment is carried out on a commercial humanoid robot known as UXA-90 to evaluate the effectiveness of the proposed method.      
### 68.Inference of collective Gaussian hidden Markov models  [ :arrow_down: ](https://arxiv.org/pdf/2107.11662.pdf)
>  We consider inference problems for a class of continuous state collective hidden Markov models, where the data is recorded in aggregate (collective) form generated by a large population of individuals following the same dynamics. We propose an aggregate inference algorithm called collective Gaussian forward-backward algorithm, extending recently proposed Sinkhorn belief propagation algorithm to models characterized by Gaussian densities. Our algorithm enjoys convergence guarantee. In addition, it reduces to the standard Kalman filter when the observations are generated by a single individual. The efficacy of the proposed algorithm is demonstrated through multiple experiments.      
### 69.Deep Machine Learning Based Egyptian Vehicle License Plate Recognition Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.11640.pdf)
>  Automated Vehicle License Plate (VLP) detection and recognition have ended up being a significant research issue as of late. VLP localization and recognition are some of the most essential techniques for managing traffic using digital techniques. In this paper, four smart systems are developed to recognize Egyptian vehicles license plates. Two systems are based on character recognition, which are (System1, Characters Recognition with Classical Machine Learning) and (System2, Characters Recognition with Deep Machine Learning). The other two systems are based on the whole plate recognition which are (System3, Whole License Plate Recognition with Classical Machine Learning) and (System4, Whole License Plate Recognition with Deep Machine Learning). We use object detection algorithms, and machine learning based object recognition algorithms. The performance of the developed systems has been tested on real images, and the experimental results demonstrate that the best detection accuracy rate for VLP is provided by using the deep learning method. Where the VLP detection accuracy rate is better than the classical system by 32%. However, the best detection accuracy rate for Vehicle License Plate Arabic Character (VLPAC) is provided by using the classical method. Where VLPAC detection accuracy rate is better than the deep learning-based system by 6%. Also, the results show that deep learning is better than the classical technique used in VLP recognition processes. Where the recognition accuracy rate is better than the classical system by 8%. Finally, the paper output recommends a robust VLP recognition system based on both statistical and deep machine learning.      
### 70.Differentiable Allophone Graphs for Language-Universal Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2107.11628.pdf)
>  Building language-universal speech recognition systems entails producing phonological units of spoken sound that can be shared across languages. While speech annotations at the language-specific phoneme or surface levels are readily available, annotations at a universal phone level are relatively rare and difficult to produce. In this work, we present a general framework to derive phone-level supervision from only phonemic transcriptions and phone-to-phoneme mappings with learnable weights represented using weighted finite-state transducers, which we call differentiable allophone graphs. By training multilingually, we build a universal phone-based speech recognition model with interpretable probabilistic phone-to-phoneme mappings for each language. These phone-based systems with learned allophone graphs can be used by linguists to document new languages, build phone-based lexicons that capture rich pronunciation variations, and re-evaluate the allophone mappings of seen language. We demonstrate the aforementioned benefits of our proposed framework with a system trained on 7 diverse languages.      
### 71.Wind Farm Layout Optimization with Cooperative Control Considerations  [ :arrow_down: ](https://arxiv.org/pdf/2107.11620.pdf)
>  The wake effect is one of the leading causes of energy losses in offshore wind farms (WFs). Both turbine placement and cooperative control can influence the wake interactions inside the WF and thus the overall WF power production. Traditionally, greedy control strategy is assumed in the layout design phase. To exploit the potential synergy between the WF layout and control so that a system-level optimal layout can be obtained with the greatest energy yields, the layout optimization should be performed with cooperative control considerations. For this purpose, a novel two-stage WF layout optimization model is developed in this paper. Cooperative WF control of both turbine yaw and axis-induction are considered. However, the integration of WF control makes the layout optimization much more complicated and results in a large-scale nonconvex problem, hindering the application of current layout optimization methods. To increase the computational efficiency, we leverage the hierarchy and decomposability of the joint optimization problem and design a decomposition-based hybrid method (DBHM). Case studies are carried out on different WFs. It is shown that WF layouts with higher energy yields can be obtained by the proposed joint optimization compared to traditional separate layout optimization. Moreover, the computational advantages of the proposed DBHM on the considered joint layout optimization problem are also demonstrated.      
### 72.LAConv: Local Adaptive Convolution for Image Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2107.11617.pdf)
>  The convolution operation is a powerful tool for feature extraction and plays a prominent role in the field of computer vision. However, when targeting the pixel-wise tasks like image fusion, it would not fully perceive the particularity of each pixel in the image if the uniform convolution kernel is used on different patches. In this paper, we propose a local adaptive convolution (LAConv), which is dynamically adjusted to different spatial locations. LAConv enables the network to pay attention to every specific local area in the learning process. Besides, the dynamic bias (DYB) is introduced to provide more possibilities for the depiction of features and make the network more flexible. We further design a residual structure network equipped with the proposed LAConv and DYB modules, and apply it to two image fusion tasks. Experiments for pansharpening and hyperspectral image super-resolution (HISR) demonstrate the superiority of our method over other state-of-the-art methods. It is worth mentioning that LAConv can also be competent for other super-resolution tasks with less computation effort.      
### 73.Channel Estimation for IRS-Assisted Millimeter-Wave MIMO Systems: Sparsity-Inspired Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2107.11605.pdf)
>  Due to their ability to create favorable line-of-sight (LoS) propagation environments, intelligent reflecting surfaces (IRSs) are regarded as promising enablers for future millimeter-wave (mm-wave) wireless communication. In this paper, we investigate channel estimation for IRS-assisted mm-wave multiple-input multiple-output (MIMO) {\color{black}wireles}s systems. By leveraging the sparsity of mm-wave channels in the angular domain, we formulate the channel estimation problem as an $\ell_1$-norm regularized optimization problem with fixed-rank constraints. To tackle the non-convexity of the formulated problem, an efficient algorithm is proposed by capitalizing on alternating minimization and manifold optimization (MO), which yields a locally optimal solution. To further reduce the computational complexity of the estimation algorithm, we propose a compressive sensing- (CS-) based channel estimation approach. In particular, a three-stage estimation protocol is put forward where the subproblem in each stage can be solved via low-complexity CS methods. Furthermore, based on the acquired channel state information (CSI) of the cascaded channel, we design a passive beamforming algorithm for maximization of the spectral efficiency. Simulation results reveal that the proposed MO-based estimation (MO-EST) and beamforming algorithms significantly outperform two benchmark schemes while the CS-based estimation (CS-EST) algorithm strikes a balance between performance and complexity. In addition, we demonstrate the robustness of the MO-EST algorithm with respect to imperfect knowledge of the sparsity level of the channels, which is crucial for practical implementations.      
### 74.Accelerating Federated Edge Learning via Optimized Probabilistic Device Scheduling  [ :arrow_down: ](https://arxiv.org/pdf/2107.11588.pdf)
>  The popular federated edge learning (FEEL) framework allows privacy-preserving collaborative model training via frequent learning-updates exchange between edge devices and server. Due to the constrained bandwidth, only a subset of devices can upload their updates at each communication round. This has led to an active research area in FEEL studying the optimal device scheduling policy for minimizing communication time. However, owing to the difficulty in quantifying the exact communication time, prior work in this area can only tackle the problem partially by considering either the communication rounds or per-round latency, while the total communication time is determined by both metrics. To close this gap, we make the first attempt in this paper to formulate and solve the communication time minimization problem. We first derive a tight bound to approximate the communication time through cross-disciplinary effort involving both learning theory for convergence analysis and communication theory for per-round latency analysis. Building on the analytical result, an optimized probabilistic scheduling policy is derived in closed-form by solving the approximate communication time minimization problem. It is found that the optimized policy gradually turns its priority from suppressing the remaining communication rounds to reducing per-round latency as the training process evolves. The effectiveness of the proposed scheme is demonstrated via a use case on collaborative 3D objective detection in autonomous driving.      
### 75.Two Headed Dragons: Multimodal Fusion and Cross Modal Transactions  [ :arrow_down: ](https://arxiv.org/pdf/2107.11585.pdf)
>  As the field of remote sensing is evolving, we witness the accumulation of information from several modalities, such as multispectral (MS), hyperspectral (HSI), LiDAR etc. Each of these modalities possess its own distinct characteristics and when combined synergistically, perform very well in the recognition and classification tasks. However, fusing multiple modalities in remote sensing is cumbersome due to highly disparate domains. Furthermore, the existing methods do not facilitate cross-modal interactions. To this end, we propose a novel transformer based fusion method for HSI and LiDAR modalities. The model is composed of stacked auto encoders that harness the cross key-value pairs for HSI and LiDAR, thus establishing a communication between the two modalities, while simultaneously using the CNNs to extract the spectral and spatial information from HSI and LiDAR. We test our model on Houston (Data Fusion Contest - 2013) and MUUFL Gulfport datasets and achieve competitive results.      
### 76.Dynamic Portal Occlusion for Precomputed Interactive Sound Propagation  [ :arrow_down: ](https://arxiv.org/pdf/2107.11548.pdf)
>  An immersive audio-visual experience in games and virtual reality requires fast calculation of diffraction-based acoustic effects. To maintain plausibility, the effects must retain spatial smoothness on source and listener motion within geometrically complex scenes. Precomputed wave-based techniques can render such results at low runtime CPU cost, but remain limited to static scenes. Modeling the occlusion effect of dynamic portals such as doors present an unresolved challenge to maintain audio-visual consistency. We present a fast solution implementable as a drop-in extension to existing precomputed systems. Key is a novel portal-search method that leverages precomputed propagation delay and direction data to find portals intervening the diffracted shortest path connecting dynamic source and listener at runtime. The method scales linearly with number of portals in worst case, far cheaper than explicit global path search that scales with scene area. We discuss culling techniques to accelerate further. The search algorithm is combined with geometric-acoustic approximations to model the additional direct and indirect energy loss from intervening portals depending on their dynamic closure state. We demonstrate plausible audio-visual animations within our system integrated with Unreal Engine 4 (TM) and Wwise (TM).      
### 77.Using a Cross-Task Grid of Linear Probes to Interpret CNN Model Predictions On Retinal Images  [ :arrow_down: ](https://arxiv.org/pdf/2107.11468.pdf)
>  We analyze a dataset of retinal images using linear probes: linear regression models trained on some "target" task, using embeddings from a deep convolutional (CNN) model trained on some "source" task as input. We use this method across all possible pairings of 93 tasks in the UK Biobank dataset of retinal images, leading to ~164k different models. We analyze the performance of these linear probes by source and target task and by layer depth. We observe that representations from the middle layers of the network are more generalizable. We find that some target tasks are easily predicted irrespective of the source task, and that some other target tasks are more accurately predicted from correlated source tasks than from embeddings trained on the same task.      
### 78.Automatic Detection Of Noise Events at Shooting Range Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.11453.pdf)
>  Outdoor shooting ranges are subject to noise regulations from local and national authorities. Restrictions found in these regulations may include limits on times of activities, the overall number of noise events, as well as limits on number of events depending on the class of noise or activity. A noise monitoring system may be used to track overall sound levels, but rarely provide the ability to detect activity or count the number of events, required to compare directly with such regulations. This work investigates the feasibility and performance of an automatic detection system to count noise events. An empirical evaluation was done by collecting data at a newly constructed shooting range and training facility. The data includes tests of multiple weapon configurations from small firearms to high caliber rifles and explosives, at multiple source positions, and collected on multiple different days. Several alternative machine learning models are tested, using as inputs time-series of standard acoustic indicators such as A-weighted sound levels and 1/3 octave spectrogram, and classifiers such as Logistic Regression and Convolutional Neural Networks. Performance for the various alternatives are reported in terms of the False Positive Rate and False Negative Rate. The detection performance was found to be satisfactory for use in automatic logging of time-periods with training activity.      
### 79.Explicit Solutions and Stability Properties of Homogeneous Polynomial Dynamical Systems via Tensor Orthogonal Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2107.11438.pdf)
>  In this paper, we investigate the explicit solutions and stability properties of certain continuous-time homogeneous polynomial dynamical systems via tensor algebra. In particular, if a system of homogeneous polynomial differential equations can be represented by an orthogonal decomposable tensor, we can write down its explicit solution in a simple fashion by exploiting tensor Z-eigenvalues and Z-eigenvectors. In addition, according to the form of the explicit solution, we explore the stability properties of the homogeneous polynomial dynamical system. We discover that the Z-eigenvalues from the orthogonal decomposition of the corresponding dynamic tensor can offer necessary and sufficient stability conditions, similar to these from linear systems theory. Finally, we demonstrate our results with numerical examples.      
### 80.MARS: Middleware for Adaptive Reflective Computer Systems  [ :arrow_down: ](https://arxiv.org/pdf/2107.11417.pdf)
>  Self-adaptive approaches for runtime resource management of manycore computing platforms often require a runtime model of the system that represents the software organization or the architecture of the target platform. The increasing heterogeneity in a platform's resource types and the interactions between resources pose challenges for coordinated model-based decision making in the face of dynamic workloads. Self-awareness properties address these challenges for emerging heterogeneous manycore processing (HMP) platforms through reflective resource managers. However, with HMP computing platform architectures evolving rapidly, porting the self-aware decision logic across different hardware platforms is challenging, requiring resource managers to update their models and platform-specific interfaces. We propose MARS (Middleware for Adaptive and Reflective Systems), a cross-layer and multi-platform framework that allows users to easily create resource managers by composing system models and resource management policies in a flexible and coordinated manner. MARS consists of a generic user-level sensing/actuation interface that allows for portable policy design, and a reflective system model used to coordinate multiple policies. We demonstrate MARS' interaction across multiple layers of the system stack through a dynamic voltage and frequency scaling (DVFS) policy example which can run on any Linux-based HMP computing platform.      
### 81.Device Scheduling and Update Aggregation Policies for Asynchronous Federated Learning  [ :arrow_down: ](https://arxiv.org/pdf/2107.11415.pdf)
>  Federated Learning (FL) is a newly emerged decentralized machine learning (ML) framework that combines on-device local training with server-based model synchronization to train a centralized ML model over distributed nodes. In this paper, we propose an asynchronous FL framework with periodic aggregation to eliminate the straggler issue in FL systems. For the proposed model, we investigate several device scheduling and update aggregation policies and compare their performances when the devices have heterogeneous computation capabilities and training data distributions. From the simulation results, we conclude that the scheduling and aggregation design for asynchronous FL can be rather different from the synchronous case. For example, a norm-based significance-aware scheduling policy might not be efficient in an asynchronous FL setting, and an appropriate "age-aware" weighting design for the model aggregation can greatly improve the learning performance of such systems.      
### 82.Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2107.11412.pdf)
>  The recent developments in technology have re-warded us with amazing audio synthesis models like TACOTRON and WAVENETS. On the other side, it poses greater threats such as speech clones and deep fakes, that may go undetected. To tackle these alarming situations, there is an urgent need to propose models that can help discriminate a synthesized speech from an actual human speech and also identify the source of such a synthesis. Here, we propose a model based on Convolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network (BiRNN) that helps to achieve both the aforementioned objectives. The temporal dependencies present in AI synthesized speech are exploited using Bidirectional RNN and CNN. The model outperforms the state-of-the-art approaches by classifying the AI synthesized audio from real human speech with an error rate of 1.9% and detecting the underlying architecture with an accuracy of 97%.      
