# ArXiv eess --Wed, 10 Nov 2021
### 1.Resistance Distance and Control Performance for Google bittide Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2111.05296.pdf)
>  We discuss control of bittide distributed systems, which are designed to provide logical synchronization between networked machines by observing data flow rates between adjacent systems at the physical network layer and controlling local reference clock frequencies. We analyze the performance of approximate proportional-integral control of the synchronization mechanism and develop a simple continuous-time model to show the resulting dynamics are stable for any positive choice of gains. We then construct explicit formulae to show that closed-loop performance measured using the L2 norm is a product of two terms, one depending only on resistance distances in the graph, and the other depending only on controller gains.      
### 2.MIMO for MATLAB: A Toolbox for Simulating MIMO Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.05273.pdf)
>  We present MIMO FOR MATLAB (MFM), a toolbox for MATLAB that aims to simplify the simulation of multiple-input multiple-output (MIMO) communication systems research while facilitating reproducibility, consistency, and community-driven customization. MFM offers users an object-oriented solution for simulating a variety of MIMO systems including sub-6 GHz, massive MIMO, millimeter wave, and terahertz communication. Out-of-the-box, MFM supplies users with widely used channel and path loss models from academic literature and wireless standards; if a particular channel or path loss model is not provided by MFM, users can create custom models by following a few simple rules. The complexity and overhead associated with simulating networks of multiple devices can be significantly reduced with MFM versus raw MATLAB code, especially when users want to investigate various channel models, path loss models, precoding/combining schemes, or other system-level parameters. MFM's heavy-lifting to automatically collect and distribute channel state information, aggregate interference, and report performance metrics relieves users of otherwise tedious tasks and instills confidence and consistency in the results of simulation. The use-cases of MFM vary widely from networks of hundreds of devices; to simple point-to-point communication; to serving as a channel generator; to radar, sonar, and underwater acoustic communication.      
### 3.Cooperative Multiterminal Radar and Communication: A New Paradigm for 6G Mobile Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.05260.pdf)
>  The impending spectrum congestion imposed by the emergence of new bandwidth-thirsty applications may be mitigated by the integration of radar and classic communications functionalities in a common system. Furthermore, the merger of a sensing component into wireless communication networks has raised interest in recent years and it may become a compelling design objective for 6G. This article presents the evolution of the hitherto separate radar and communication systems towards their amalgam known as a joint radar and communication (RADCOM) system. Explicitly, we propose to integrate a radio sensing component into 6G. We consider an ultra-dense network (UDN) scenario relying on an active multistatic radar configuration and on cooperation between the access points across the entire coverage area. The technological trends required to reach a feasible integration, the applications anticipated and the open research challenges are identified, with an emphasis on high-accuracy network synchronization. The successful integration of these technologies would facilitate centimeter-level resolution, hence supporting compelling high-resolution applications for next-generation networks, such as robotic cars and industrial assembly lines.      
### 4.Leveraging blur information for plenoptic camera calibration  [ :arrow_down: ](https://arxiv.org/pdf/2111.05226.pdf)
>  This paper presents a novel calibration algorithm for plenoptic cameras, especially the multi-focus configuration, where several types of micro-lenses are used, using raw images only. Current calibration methods rely on simplified projection models, use features from reconstructed images, or require separated calibrations for each type of micro-lens. In the multi-focus configuration, the same part of a scene will demonstrate different amounts of blur according to the micro-lens focal length. Usually, only micro-images with the smallest amount of blur are used. In order to exploit all available data, we propose to explicitly model the defocus blur in a new camera model with the help of our newly introduced Blur Aware Plenoptic (BAP) feature. First, it is used in a pre-calibration step that retrieves initial camera parameters, and second, to express a new cost function to be minimized in our single optimization process. Third, it is exploited to calibrate the relative blur between micro-images. It links the geometric blur, i.e., the blur circle, to the physical blur, i.e., the point spread function. Finally, we use the resulting blur profile to characterize the camera's depth of field. Quantitative evaluations in controlled environment on real-world data demonstrate the effectiveness of our calibrations.      
### 5.Resilient expansion planning of virtual energy plant with an integrated energy system -- reliability criteria of lines and towers  [ :arrow_down: ](https://arxiv.org/pdf/2111.05212.pdf)
>  Virtual power plants, while being virtual, rely on a physical network for operations. The portfolio of the virtual power plants is flexible in facilitating a wide range of resources including the local heat pumps. The power transmission network has the responsibility to ensure the security of supply, reliability of operation, planning, and expansion. The power transmission network and apparatus including lines and towers are also aging with time. Furthermore, the transmission network covers a large geographical area which is expensive to maintain. The objective of this paper is to investigate the effect of power network conditions on power network expansion planning. The condition of the power network is determined by the maintenance cost of lines and a health index and a risk factor associated with the tower. The investigation begins with answering how the inclusion of heat pump impacts the decisions on network interventions. Thereafter, the condition network is factored into the decision-making by better understanding the impact of the network condition on the overall expansion planning. Furthermore, many sensitivity analyses are conducted to evaluate the trade-offs between decision variables such as cost of heat pump, coefficient of performance of heat pump, risk factors, and line and tower costs.      
### 6.Deep Learning Adapted Acceleration for Limited-view Photoacoustic Computed Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2111.05194.pdf)
>  Photoacoustic imaging (PAI) is a non-invasive imaging modality that detects the ultrasound signal generated from tissue with light excitation. Photoacoustic computed tomography (PACT) uses unfocused large-area light to illuminate the target with ultrasound transducer array for PA signal detection. Limited-view issue could cause a low-quality image in PACT due to the limitation of geometric condition. The model-based method is used to resolve this problem, which contains different regularization. To adapt fast and high-quality reconstruction of limited-view PA data, in this paper, a model-based method that combines the mathematical variational model with deep learning is proposed to speed up and regularize the unrolled procedure of reconstruction. A deep neural network is designed to adapt the step of the gradient updated term of data consistency in the gradient descent procedure, which can obtain a high-quality PA image only with a few iterations. Note that all parameters and priors are automatically learned during the offline training stage. In experiments, we show that this method outperforms the other methods with half-view (180 degrees) simulation and real data. The comparison of different model-based methods show that our proposed scheme has superior performances (over 0.05 for SSIM) with same iteration (3 times) steps. Furthermore, an unseen data is used to validate the generalization of different methods. Finally, we find that our method obtains superior results (0.94 value of SSIM for in vivo) with a high robustness and accelerated reconstruction.      
### 7.Approaching the Limit of Image Rescaling via Flow Guidance  [ :arrow_down: ](https://arxiv.org/pdf/2111.05133.pdf)
>  Image downscaling and upscaling are two basic rescaling operations. Once the image is downscaled, it is difficult to be reconstructed via upscaling due to the loss of information. To make these two processes more compatible and improve the reconstruction performance, some efforts model them as a joint encoding-decoding task, with the constraint that the downscaled (i.e. encoded) low-resolution (LR) image must preserve the original visual appearance. To implement this constraint, most methods guide the downscaling module by supervising it with the bicubically downscaled LR version of the original high-resolution (HR) image. However, this bicubic LR guidance may be suboptimal for the subsequent upscaling (i.e. decoding) and restrict the final reconstruction performance. In this paper, instead of directly applying the LR guidance, we propose an additional invertible flow guidance module (FGM), which can transform the downscaled representation to the visually plausible image during downscaling and transform it back during upscaling. Benefiting from the invertibility of FGM, the downscaled representation could get rid of the LR guidance and would not disturb the downscaling-upscaling process. It allows us to remove the restrictions on the downscaling module and optimize the downscaling and upscaling modules in an end-to-end manner. In this way, these two modules could cooperate to maximize the HR reconstruction performance. Extensive experiments demonstrate that the proposed method can achieve state-of-the-art (SotA) performance on both downscaled and reconstructed images.      
### 8.Segmentation of Multiple Myeloma Plasma Cells in Microscopy Images with Noisy Labels  [ :arrow_down: ](https://arxiv.org/pdf/2111.05125.pdf)
>  A key component towards an improved and fast cancer diagnosis is the development of computer-assisted tools. In this article, we present the solution that won the SegPC-2021 competition for the segmentation of multiple myeloma plasma cells in microscopy images. The labels used in the competition dataset were generated semi-automatically and presented noise. To deal with it, a heavy image augmentation procedure was carried out and predictions from several models were combined using a custom ensemble strategy. State-of-the-art feature extractors and instance segmentation architectures were used, resulting in a mean Intersection-over-Union of 0.9389 on the SegPC-2021 final test set.      
### 9.A Deep Learning Technique using Low Sampling rate for residential Non Intrusive Load Monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2111.05120.pdf)
>  Individual device loads and energy consumption feedback is one of the important approaches for pursuing users to save energy in residences. This can help in identifying faulty devices and wasted energy by devices when left On unused. The main challenge is to identity and estimate the energy consumption of individual devices without intrusive sensors on each device. Non-intrusive load monitoring (NILM) or energy disaggregation, is a blind source separation problem which requires a system to estimate the electricity usage of individual appliances from the aggregated household energy consumption. In this paper, we propose a novel deep neural network-based approach for performing load disaggregation on low frequency power data obtained from residential households. We combine a series of one-dimensional Convolutional Neural Networks and Long Short Term Memory (1D CNN-LSTM) to extract features that can identify active appliances and retrieve their power consumption given the aggregated household power value. We used CNNs to extract features from main readings in a given time frame and then used those features to classify if a given appliance is active at that time period or not. Following that, the extracted features are used to model a generation problem using LSTM. We train the LSTM to generate the disaggregated energy consumption of a particular appliance. Our neural network is capable of generating detailed feedback of demand-side, providing vital insights to the end-user about their electricity consumption. The algorithm was designed for low power offline devices such as ESP32. Empirical calculations show that our model outperforms the state-of-the-art on the Reference Energy Disaggregation Dataset (REDD).      
### 10.AI-Based Radio Resource Management and Trajectory Design in CoMP UAV VLC Networks: Constant Velocity Vs. Constant Acceleration  [ :arrow_down: ](https://arxiv.org/pdf/2111.05116.pdf)
>  In this paper, we consider UAVs equipped with a VLC access point and coordinated multipoint (CoMP) capability that allows users to connect to more than one UAV. UAVs can move in 3-dimensional (3D) at a constant acceleration in each master timescale, where a central server is responsible for synchronization and cooperation among UAVs. We define the data rate for each user type, CoMP and non-CoMP. The constant speed in UAVs' motion is not practical, and the effect of acceleration on the movement of UAVs is necessary to be considered. Unlike most existing works, we see the effect of variable speed on kinetic and allocation formulas. For the proposed system model, we define timescales for two different slots in which resources are allocated. In the master timescale, the acceleration of each UAV is specified, and in each short timescale, radio resources are allocated. The initial velocity in each small time slot is obtained from the previous time slot's velocity. Our goal is to formulate a multiobjective optimization problem where the total data rate is maximized and the total communication power consumption is minimized simultaneously. To deal this multiobjective optimization, we first apply the weighted method and then apply multi-agent deep deterministic policy gradient (MADDPG) which is a multi-agent method based on deep deterministic policy gradient (DDPG) that ensures more stable and faster convergence. We improve this solution method by adding two critic networks as well as allocating the two step acceleration. Simulation results indicate that the constant acceleration motion of UAVs gives about 8\% better results than conventional motion systems in terms of performance. Furthermore, CoMP supports the system to achieve an average of about 12\% higher rates comparing with non-CoMP system.      
### 11.EEGEyeNet: a Simultaneous Electroencephalography and Eye-tracking Dataset and Benchmark for Eye Movement Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2111.05100.pdf)
>  We present a new dataset and benchmark with the goal of advancing research in the intersection of brain activities and eye movements. Our dataset, EEGEyeNet, consists of simultaneous Electroencephalography (EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms. Using this dataset, we also propose a benchmark to evaluate gaze prediction from EEG measurements. The benchmark consists of three tasks with an increasing level of difficulty: left-right, angle-amplitude and absolute position. We run extensive experiments on this benchmark in order to provide solid baselines, both based on classical machine learning models and on large neural networks. We release our complete code and data and provide a simple and easy-to-use interface to evaluate new methods.      
### 12.Monitoring Atmospheric Pollutants From Ground-based Observations  [ :arrow_down: ](https://arxiv.org/pdf/2111.05094.pdf)
>  Remote sensing analysts continuously monitor the amount of pollutants in the atmosphere. They are usually performed via satellite images. However, these images suffer from low temporal and low spatial resolution. Therefore, observations recorded from the ground offer us a fantastic alternative. There are low-cost sensors that continuously record the PM$_{2.5}$ and PM$_{10}$ concentration levels in the atmosphere. In this position paper, we provide an overview of the state-of-the-art techniques for pollutant forecasting. We establish the interdependence of meteorological parameters on atmospheric pollutants. Our case study in this paper is based on the island of Australia.      
### 13.Classification of sums of complex exponentials  [ :arrow_down: ](https://arxiv.org/pdf/2111.05081.pdf)
>  Numerous signals in relevant signal processing applications can be modeled as a sum of complex exponentials. Each exponential term entails a particular property of the modeled physical system, and it is possible to define families of signals that are associated with the complex exponentials. In this paper, we formulate a classification problem for this guiding principle and we propose a data processing strategy. In particular, we exploit the information obtained from the analytical model by combining it with data-driven learning techniques. As a result, we obtain a classification strategy that is robust under modeling uncertainties and experimental perturbations. To assess the performance of the new scheme, we test it with experimental data obtained from the scattering response of targets illuminated with an impulse radio ultra-wideband radar.      
### 14.MAC-ReconNet: A Multiple Acquisition Context based Convolutional Neural Network for MR Image Reconstruction using Dynamic Weight Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2111.05055.pdf)
>  Convolutional Neural network-based MR reconstruction methods have shown to provide fast and high quality reconstructions. A primary drawback with a CNN-based model is that it lacks flexibility and can effectively operate only for a specific acquisition context limiting practical applicability. By acquisition context, we mean a specific combination of three input settings considered namely, the anatomy under study, undersampling mask pattern and acceleration factor for undersampling. The model could be trained jointly on images combining multiple contexts. However the model does not meet the performance of context specific models nor extensible to contexts unseen at train time. This necessitates a modification to the existing architecture in generating context specific weights so as to incorporate flexibility to multiple contexts. We propose a multiple acquisition context based network, called MAC-ReconNet for MRI reconstruction, flexible to multiple acquisition contexts and generalizable to unseen contexts for applicability in real scenarios. The proposed network has an MRI reconstruction module and a dynamic weight prediction (DWP) module. The DWP module takes the corresponding acquisition context information as input and learns the context-specific weights of the reconstruction module which changes dynamically with context at run time. We show that the proposed approach can handle multiple contexts based on cardiac and brain datasets, Gaussian and Cartesian undersampling patterns and five acceleration factors. The proposed network outperforms the naive jointly trained model and gives competitive results with the context-specific models both quantitatively and qualitatively. We also demonstrate the generalizability of our model by testing on contexts unseen at train time.      
### 15.GDCA: GAN-based single image super resolution with Dual discriminators and Channel Attention  [ :arrow_down: ](https://arxiv.org/pdf/2111.05014.pdf)
>  Single Image Super-Resolution (SISR) is a very active research field. This paper addresses SISR by using a GAN-based approach with dual discriminators and incorporating it with an attention mechanism. The experimental results show that GDCA can generate sharper and high pleasing images compare to other conventional methods.      
### 16.Model Predictive Control for a Medium-head Hydropower Plant Hybridized with Battery Energy Storage to Reduce Penstock Fatigue  [ :arrow_down: ](https://arxiv.org/pdf/2111.05004.pdf)
>  A hybrid hydropower power plant is a conventional HydroPower Plant (HPP) augmented with a Battery Energy Storage System (BESS) to decrease the wear and tear of sensitive mechanical components and improve the reliability and regulation performance of the overall plant. A central task of controlling hybrid power plants is determining how the total power set-point should be split between the BESS and the hybridized unit (power set-point splitting) as a function of the operational objectives. This paper describes a Model Predictive Control (MPC) framework for hybrid medium- and high-head plants to determine the power set-point of the hydropower unit and the BESS. The splitting policy relies on an explicit formulation of the mechanical loads incurred by the HPP's penstock, which can be damaged due to fatigue when providing regulation services to the grid. By filtering out from the HPP's power set-point the components conducive to excess penstock fatigue and properly controlling the BESS, the proposed MPC is able to maintain the same level of regulation performance while significantly decreasing damages to the hydraulic conduits. A proof-of-concept by simulations is provided considering a 230 MW medium-head hydropower plant.      
### 17.MPC-Based Real-Time Charging Coordination for Electric Vehicle Aggregator to Provide Regulation Service in a Market Environment  [ :arrow_down: ](https://arxiv.org/pdf/2111.04991.pdf)
>  The optimal operation problem of electric vehicle aggregator (EVA) is considered. An EVA can participate in energy and regulation markets with its current and upcoming EVs, thus reducing its total cost of purchasing energy to fulfill EVs' charging requirements. An MPC based optimization model is developed to consider future arrival of EVs as well as energy and regulation prices. The index of CVaR is used to model risk-averseness of an EVA. Simulations on the 1000-EV test system validate the effectiveness of our work in achieving a lucrative revenue while satisfying the charging requests from EV owners.      
### 18.Bilinear pooling and metric learning network for early Alzheimer's disease identification with FDG-PET images  [ :arrow_down: ](https://arxiv.org/pdf/2111.04985.pdf)
>  FDG-PET reveals altered brain metabolism in individuals with mild cognitive impairment (MCI) and Alzheimer's disease (AD). Some biomarkers derived from FDG-PET by computer-aided-diagnosis (CAD) technologies have been proved that they can accurately diagnosis normal control (NC), MCI, and AD. However, the studies of identification of early MCI (EMCI) and late MCI (LMCI) with FDG-PET images are still insufficient. Compared with studies based on fMRI and DTI images, the researches of the inter-region representation features in FDG-PET images are insufficient. Moreover, considering the variability in different individuals, some hard samples which are very similar with both two classes limit the classification performance. To tackle these problems, in this paper, we propose a novel bilinear pooling and metric learning network (BMNet), which can extract the inter-region representation features and distinguish hard samples by constructing embedding space. To validate the proposed method, we collect 998 FDG-PET images from ADNI. Following the common preprocessing steps, 90 features are extracted from each FDG-PET image according to the automatic anatomical landmark (AAL) template and then sent into the proposed network. Extensive 5-fold cross-validation experiments are performed for multiple two-class classifications. Experiments show that most metrics are improved after adding the bilinear pooling module and metric losses to the Baseline model respectively. Specifically, in the classification task between EMCI and LMCI, the specificity improves 6.38% after adding the triple metric loss, and the negative predictive value (NPV) improves 3.45% after using the bilinear pooling module.      
### 19.Data-Based Moving Horizon Estimation for Linear Discrete-Time Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.04979.pdf)
>  This paper introduces a data-based moving horizon estimation (MHE) scheme for linear time-invariant discrete-time systems. The scheme solely relies on collected data without employing any system identification step. It is formulated for a robust case in which the online output measurements are corrupted by some non-vanishing measurement noise. Robust global exponential stability of the data-based MHE is proven under standard assumptions. A simulation example illustrates the behavior of the data-based MHE scheme.      
### 20.Optical Adaptive LMS Equalizer with an Opto-electronic Feedback Loop  [ :arrow_down: ](https://arxiv.org/pdf/2111.04969.pdf)
>  We propose a photonic adaptive Least-Mean-Squares equalizer with an opto-electronic feedback loop to determine the updating of an optical Finite-Impulse-Response filter's weights, enabling dispersion compensation introduced by 30-km fiber based on our photonic integrated chip.      
### 21.Aggregated Feasible Region of Heterogeneous Demand-Side Flexible Resources---Part I: Theoretical Derivation of the Exact Model  [ :arrow_down: ](https://arxiv.org/pdf/2111.04963.pdf)
>  In the first part of the two-part series, the model to describe the exact aggregated feasible region (AFR) of multiple types of demand-side resources is derived. Based on a discrete-time unified individual model of heterogeneous resources, the calculation of AFR is, in fact, a feasible region projection problem. Therefore, the Fourier-Motzkin Elimination (FME) method is used for derivation. By analyzing the redundancy of all possible constraints in the FME process, the mathematical expression and calculation method for the exact AFR is proposed. The number of constraints is linear with the number of resources and is exponential with the number of time intervals, respectively. The computational complexity has been dramatically simplified compared with the original FME. However, the number of constraints in the model is still exponential and cannot be simplified anymore. Hence, In Part II of this paper, several approximation methods are proposed and analyzed in detail.      
### 22.A Dynamic Watermarking Algorithm for Finite Markov Decision Problems  [ :arrow_down: ](https://arxiv.org/pdf/2111.04952.pdf)
>  Dynamic watermarking, as an active intrusion detection technique, can potentially detect replay attacks, spoofing attacks, and deception attacks in the feedback channel for control systems. In this paper, we develop a novel dynamic watermarking algorithm for finite-state finite-action Markov decision processes and present upper bounds on the mean time between false alarms, and the mean delay between the time an attack occurs and when it is detected. We further compute the sensitivity of the performance of the control system as a function of the watermark. We demonstrate the effectiveness of the proposed dynamic watermarking algorithm by detecting a spoofing attack in a sensor network system.      
### 23.Real-time Instance Segmentation of Surgical Instruments using Attention and Multi-scale Feature Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2111.04911.pdf)
>  Precise instrument segmentation aid surgeons to navigate the body more easily and increase patient safety. While accurate tracking of surgical instruments in real-time plays a crucial role in minimally invasive computer-assisted surgeries, it is a challenging task to achieve, mainly due to 1) complex surgical environment, and 2) model design with both optimal accuracy and speed. Deep learning gives us the opportunity to learn complex environment from large surgery scene environments and placements of these instruments in real world scenarios. The Robust Medical Instrument Segmentation 2019 challenge (ROBUST-MIS) provides more than 10,000 frames with surgical tools in different clinical settings. In this paper, we use a light-weight single stage instance segmentation model complemented with a convolutional block attention module for achieving both faster and accurate inference. We further improve accuracy through data augmentation and optimal anchor localisation strategies. To our knowledge, this is the first work that explicitly focuses on both real-time performance and improved accuracy. Our approach out-performed top team performances in the ROBUST-MIS challenge with over 44% improvement on both area-based metric MI_DSC and distance-based metric MI_NSD. We also demonstrate real-time performance (&gt; 60 frames-per-second) with different but competitive variants of our final approach.      
### 24.Joint AEC AND Beamforming with Double-Talk Detection using RNN-Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2111.04904.pdf)
>  Acoustic echo cancellation (AEC) is a technique used in full-duplex communication systems to eliminate acoustic feedback of far-end speech. However, their performance degrades in naturalistic environments due to nonlinear distortions introduced by the speaker, as well as background noise, reverberation, and double-talk scenarios. To address nonlinear distortions and co-existing background noise, several deep neural network (DNN)-based joint AEC and denoising systems were developed. These systems are based on either purely "black-box" neural networks or "hybrid" systems that combine traditional AEC algorithms with neural networks. We propose an all-deep-learning framework that combines multi-channel AEC and our recently proposed self-attentive recurrent neural network (RNN) beamformer. We propose an all-deep-learning framework that combines multi-channel AEC and our recently proposed self-attentive recurrent neural network (RNN) beamformer. Furthermore, we propose a double-talk detection transformer (DTDT) module based on the multi-head attention transformer structure that computes attention over time by leveraging frame-wise double-talk predictions. Experiments show that our proposed method outperforms other approaches in terms of improving speech quality and speech recognition rate of an ASR system.      
### 25.Mitigating domain shift in AI-based tuberculosis screening with unsupervised domain adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2111.04893.pdf)
>  We demonstrate that Domain Invariant Feature Learning (DIFL) can improve the out-of-domain generalizability of a deep learning Tuberculosis screening algorithm. It is well known that state of the art deep learning algorithms often have difficulty generalizing to unseen data distributions due to "domain shift". In the context of medical imaging, this could lead to unintended biases such as the inability to generalize from one patient population to another. We analyze the performance of a ResNet-50 classifier for the purposes of Tuberculosis screening using the four most popular public datasets with geographically diverse sources of imagery. We show that without domain adaptation, ResNet-50 has difficulty in generalizing between imaging distributions from a number of public Tuberculosis screening datasets with imagery from geographically distributed regions. However, with the incorporation of DIFL, the out-of-domain performance is greatly enhanced. Analysis criteria includes a comparison of accuracy, sensitivity, specificity and AUC over both the baseline, as well as the DIFL enhanced algorithms. We conclude that DIFL improves generalizability of Tuberculosis screening while maintaining acceptable accuracy over the source domain imagery when applied across a variety of public datasets.      
### 26.Universal Lesion Detection in CT Scans using Neural Network Ensembles  [ :arrow_down: ](https://arxiv.org/pdf/2111.04886.pdf)
>  In clinical practice, radiologists are reliant on the lesion size when distinguishing metastatic from non-metastatic lesions. A prerequisite for lesion sizing is their detection, as it promotes the downstream assessment of tumor spread. However, lesions vary in their size and appearance in CT scans, and radiologists often miss small lesions during a busy clinical day. To overcome these challenges, we propose the use of state-of-the-art detection neural networks to flag suspicious lesions present in the NIH DeepLesion dataset for sizing. Additionally, we incorporate a bounding box fusion technique to minimize false positives (FP) and improve detection accuracy. Finally, to resemble clinical usage, we constructed an ensemble of the best detection models to localize lesions for sizing with a precision of 65.17% and sensitivity of 91.67% at 4 FP per image. Our results improve upon or maintain the performance of current state-of-the-art methods for lesion detection in challenging CT scans.      
### 27.Lymph Node Detection in T2 MRI with Transformers  [ :arrow_down: ](https://arxiv.org/pdf/2111.04885.pdf)
>  Identification of lymph nodes (LN) in T2 Magnetic Resonance Imaging (MRI) is an important step performed by radiologists during the assessment of lymphoproliferative diseases. The size of the nodes play a crucial role in their staging, and radiologists sometimes use an additional contrast sequence such as diffusion weighted imaging (DWI) for confirmation. However, lymph nodes have diverse appearances in T2 MRI scans, making it tough to stage for metastasis. Furthermore, radiologists often miss smaller metastatic lymph nodes over the course of a busy day. To deal with these issues, we propose to use the DEtection TRansformer (DETR) network to localize suspicious metastatic lymph nodes for staging in challenging T2 MRI scans acquired by different scanners and exam protocols. False positives (FP) were reduced through a bounding box fusion technique, and a precision of 65.41\% and sensitivity of 91.66\% at 4 FP per image was achieved. To the best of our knowledge, our results improve upon the current state-of-the-art for lymph node detection in T2 MRI scans.      
### 28.Parameter Conditions to Prevent Voltage Oscillations Caused by LTC-Inverter Hunting on Power Distribution Grids  [ :arrow_down: ](https://arxiv.org/pdf/2111.04815.pdf)
>  As more distributed energy resources (DERs) are connected to the power grid, it becomes increasingly important to ensure safe and effective coordination between legacy voltage regulation devices and inverter-based DERs. In this work, we show how a distribution circuit model, composed of two LTCs and two inverter devices, can create voltage oscillations even with reasonable choices of control parameters. By modeling the four-device circuit as a switched affine hybrid system, we analyze the system's oscillatory behavior, both during normal operation and after a cyber-physical attack. Through the analysis we determine the specific region of the voltage state space where oscillations are possible and derive conditions on the control parameters to guarantee against the oscillations. Finally, we project the derived parameter conditions onto 2D spaces, and describe the application of our problem formulation to grids with many devices.      
### 29.Unsupervised Approaches for Out-Of-Distribution Dermoscopic Lesion Detection  [ :arrow_down: ](https://arxiv.org/pdf/2111.04807.pdf)
>  There are limited works showing the efficacy of unsupervised Out-of-Distribution (OOD) methods on complex medical data. Here, we present preliminary findings of our unsupervised OOD detection algorithm, SimCLR-LOF, as well as a recent state of the art approach (SSD), applied on medical images. SimCLR-LOF learns semantically meaningful features using SimCLR and uses LOF for scoring if a test sample is OOD. We evaluated on the multi-source International Skin Imaging Collaboration (ISIC) 2019 dataset, and show results that are competitive with SSD as well as with recent supervised approaches applied on the same data.      
### 30.Data-Driven Prediction with Stochastic Data: Confidence Regions and Minimum Mean-Squared Error Estimates  [ :arrow_down: ](https://arxiv.org/pdf/2111.04789.pdf)
>  Recently, direct data-driven prediction has found important applications for controlling unknown systems, particularly in predictive control. Such an approach provides exact prediction using behavioral system theory when noise-free data are available. For stochastic data, although approximate predictors exist based on different statistical criteria, they fail to provide statistical guarantees of prediction accuracy. In this paper, confidence regions are provided for these stochastic predictors based on the prediction error distribution. Leveraging this, an optimal predictor which achieves minimum mean-squared prediction error is also proposed to enhance prediction accuracy. These results depend on some true model parameters, but they can also be replaced with an approximate data-driven formulation in practice. Numerical results show that the derived confidence region is valid and smaller prediction errors are observed for the proposed minimum mean-squared error estimate, even with the approximate data-driven formulation.      
### 31.DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet  [ :arrow_down: ](https://arxiv.org/pdf/2111.04739.pdf)
>  Accurate retinal vessel segmentation is an important task for many computer-aided diagnosis systems. Yet, it is still a challenging problem due to the complex vessel structures of an eye. Numerous vessel segmentation methods have been proposed recently, however more research is needed to deal with poor segmentation of thin and tiny vessels. To address this, we propose a new deep learning pipeline combining the efficiency of residual dense net blocks and, residual squeeze and excitation blocks. We validate experimentally our approach on three datasets and show that our pipeline outperforms current state of the art techniques on the sensitivity metric relevant to assess capture of small vessels.      
### 32.Synthetic magnetic resonance images for domain adaptation: Application to fetal brain tissue segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2111.04737.pdf)
>  The quantitative assessment of the developing human brain in utero is crucial to fully understand neurodevelopment. Thus, automated multi-tissue fetal brain segmentation algorithms are being developed, which in turn require annotated data to be trained. However, the available annotated fetal brain datasets are limited in number and heterogeneity, hampering domain adaptation strategies for robust segmentation. In this context, we use FaBiAN, a Fetal Brain magnetic resonance Acquisition Numerical phantom, to simulate various realistic magnetic resonance images of the fetal brain along with its class labels. We demonstrate that these multiple synthetic annotated data, generated at no cost and further reconstructed using the target super-resolution technique, can be successfully used for domain adaptation of a deep learning method that segments seven brain tissues. Overall, the accuracy of the segmentation is significantly enhanced, especially in the cortical gray matter, the white matter, the cerebellum, the deep gray matter and the brain stem.      
### 33.Multi-Modality Cardiac Image Analysis with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.04736.pdf)
>  Accurate cardiac computing, analysis and modeling from multi-modality images are important for the diagnosis and treatment of cardiac disease. Late gadolinium enhancement magnetic resonance imaging (LGE MRI) is a promising technique to visualize and quantify myocardial infarction (MI) and atrial scars. Automating quantification of MI and atrial scars can be challenging due to the low image quality and complex enhancement patterns of LGE MRI. Moreover, compared with the other sequences LGE MRIs with gold standard labels are particularly limited, which represents another obstacle for developing novel algorithms for automatic segmentation and quantification of LGE MRIs. This chapter aims to summarize the state-of-the-art and our recent advanced contributions on deep learning based multi-modality cardiac image analysis. Firstly, we introduce two benchmark works for multi-sequence cardiac MRI based myocardial and pathology segmentation. Secondly, two novel frameworks for left atrial scar segmentation and quantification from LGE MRI were presented. Thirdly, we present three unsupervised domain adaptation techniques for cross-modality cardiac image segmentation.      
### 34.Mixed Transformer U-Net For Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2111.04734.pdf)
>  Though U-Net has achieved tremendous success in medical image segmentation tasks, it lacks the ability to explicitly model long-range dependencies. Therefore, Vision Transformers have emerged as alternative segmentation structures recently, for their innate ability of capturing long-range correlations through Self-Attention (SA). However, Transformers usually rely on large-scale pre-training and have high computational complexity. Furthermore, SA can only model self-affinities within a single sample, ignoring the potential correlations of the overall dataset. To address these problems, we propose a novel Transformer module named Mixed Transformer Module (MTM) for simultaneous inter- and intra- affinities learning. MTM first calculates self-affinities efficiently through our well-designed Local-Global Gaussian-Weighted Self-Attention (LGG-SA). Then, it mines inter-connections between data samples through External Attention (EA). By using MTM, we construct a U-shaped model named Mixed Transformer U-Net (MT-UNet) for accurate medical image segmentation. We test our method on two different public datasets, and the experimental results show that the proposed method achieves better performance over other state-of-the-art methods. The code is available at: <a class="link-external link-https" href="https://github.com/Dootmaan/MT-UNet" rel="external noopener nofollow">this https URL</a>.      
### 35.Real-time landmark detection for precise endoscopic submucosal dissection via shape-aware relation network  [ :arrow_down: ](https://arxiv.org/pdf/2111.04733.pdf)
>  We propose a novel shape-aware relation network for accurate and real-time landmark detection in endoscopic submucosal dissection (ESD) surgery. This task is of great clinical significance but extremely challenging due to bleeding, lighting reflection, and motion blur in the complicated surgical environment. Compared with existing solutions, which either neglect geometric relationships among targeting objects or capture the relationships by using complicated aggregation schemes, the proposed network is capable of achieving satisfactory accuracy while maintaining real-time performance by taking full advantage of the spatial relations among landmarks. We first devise an algorithm to automatically generate relation keypoint heatmaps, which are able to intuitively represent the prior knowledge of spatial relations among landmarks without using any extra manual annotation efforts. We then develop two complementary regularization schemes to progressively incorporate the prior knowledge into the training process. While one scheme introduces pixel-level regularization by multi-task learning, the other integrates global-level regularization by harnessing a newly designed grouped consistency evaluator, which adds relation constraints to the proposed network in an adversarial manner. Both schemes are beneficial to the model in training, and can be readily unloaded in inference to achieve real-time detection. We establish a large in-house dataset of ESD surgery for esophageal cancer to validate the effectiveness of our proposed method. Extensive experimental results demonstrate that our approach outperforms state-of-the-art methods in terms of accuracy and efficiency, achieving better detection results faster. Promising results on two downstream applications further corroborate the great potential of our method in ESD clinical practice.      
### 36.Emotional Prosody Control for Speech Generation  [ :arrow_down: ](https://arxiv.org/pdf/2111.04730.pdf)
>  Machine-generated speech is characterized by its limited or unnatural emotional variation. Current text to speech systems generates speech with either a flat emotion, emotion selected from a predefined set, average variation learned from prosody sequences in training data or transferred from a source style. We propose a text to speech(TTS) system, where a user can choose the emotion of generated speech from a continuous and meaningful emotion space (Arousal-Valence space). The proposed TTS system can generate speech from the text in any speaker's style, with fine control of emotion. We show that the system works on emotion unseen during training and can scale to previously unseen speakers given his/her speech sample. Our work expands the horizon of the state-of-the-art FastSpeech2 backbone to a multi-speaker setting and gives it much-coveted continuous (and interpretable) affective control, without any observable degradation in the quality of the synthesized speech.      
### 37.On Large Ground Station Antennas as Potential Radar Targets for Biomass  [ :arrow_down: ](https://arxiv.org/pdf/2111.05316.pdf)
>  Radio-telescopes or ground-station antennas can, if pointed, act as a radar target with high radar cross-section (RCS). Space-based Synthetic Aperture Radar (SAR) data confirmed it at 5:3GHz for a modified ground-station antenna. Operational ground-station antennas cannot be modified. The latter antennas might operate at frequencies well above the radar band. The radar signal could be scattered with high RCS from such an antenna, with less influence due to a load (receiver). The antenna geometry should be precisely known to derive its RCS. BIOMASS SAR operates near 435 MHz (P-band). Results are given, also for BIOMASS antenna itself. Large antennas in an array as in Westerbork are of potential interest, located on an East-West line, nearly perpendicular to ascending and descending polar orbits. Related material is discussed.      
### 38.Stain-free Detection of Embryo Polarization using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.05315.pdf)
>  Polarization of the mammalian embryo at the right developmental time is critical for its development to term and would be valuable in assessing the potential of human embryos. However, tracking polarization requires invasive fluorescence staining, impermissible in the in vitro fertilization clinic. Here, we report the use of artificial intelligence to detect polarization from unstained time-lapse movies of mouse embryos. We assembled a dataset of bright-field movie frames from 8-cell-stage embryos, side-by-side with corresponding images of fluorescent markers of cell polarization. We then used an ensemble learning model to detect whether any bright-field frame showed an embryo before or after onset of polarization. Our resulting model has an accuracy of 85% for detecting polarization, significantly outperforming human volunteers trained on the same data (61% accuracy). We discovered that our self-learning model focuses upon the angle between cells as one known cue for compaction, which precedes polarization, but it outperforms the use of this cue alone. By compressing three-dimensional time-lapsed image data into two-dimensions, we are able to reduce data to an easily manageable size for deep learning processing. In conclusion, we describe a method for detecting a key developmental feature of embryo development that avoids clinically impermissible fluorescence staining.      
### 39.Unsupervised Spiking Instance Segmentation on Event Data using STDP  [ :arrow_down: ](https://arxiv.org/pdf/2111.05283.pdf)
>  Spiking Neural Networks (SNN) and the field of Neuromorphic Engineering has brought about a paradigm shift in how to approach Machine Learning (ML) and Computer Vision (CV) problem. This paradigm shift comes from the adaption of event-based sensing and processing. An event-based vision sensor allows for sparse and asynchronous events to be produced that are dynamically related to the scene. Allowing not only the spatial information but a high-fidelity of temporal information to be captured. Meanwhile avoiding the extra overhead and redundancy of conventional high frame rate approaches. However, with this change in paradigm, many techniques from traditional CV and ML are not applicable to these event-based spatial-temporal visual streams. As such a limited number of recognition, detection and segmentation approaches exist. In this paper, we present a novel approach that can perform instance segmentation using just the weights of a Spike Time Dependent Plasticity trained Spiking Convolutional Neural Network that was trained for object recognition. This exploits the spatial and temporal aspects of the network's internal feature representations adding this new discriminative capability. We highlight the new capability by successfully transforming a single class unsupervised network for face detection into a multi-person face recognition and instance segmentation network.      
### 40.Cross Attentional Audio-Visual Fusion for Dimensional Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2111.05222.pdf)
>  Multimodal analysis has recently drawn much interest in affective computing, since it can improve the overall accuracy of emotion recognition over isolated uni-modal approaches. The most effective techniques for multimodal emotion recognition efficiently leverage diverse and complimentary sources of information, such as facial, vocal, and physiological modalities, to provide comprehensive feature representations. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos, where complex spatiotemporal relationships may be captured. Most of the existing fusion techniques rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complimentary nature of audio-visual (A-V) modalities. We introduce a cross-attentional fusion approach to extract the salient features across A-V modalities, allowing for accurate prediction of continuous values of valence and arousal. Our new cross-attentional A-V fusion model efficiently leverages the inter-modal relationships. In particular, it computes cross-attention weights to focus on the more contributive features across individual modalities, and thereby combine contributive feature representations, which are then fed to fully connected layers for the prediction of valence and arousal. The effectiveness of the proposed approach is validated experimentally on videos from the RECOLA and Fatigue (private) data-sets. Results indicate that our cross-attentional A-V fusion model is a cost-effective approach that outperforms state-of-the-art fusion approaches. Code is available: \url{<a class="link-external link-https" href="https://github.com/praveena2j/Cross-Attentional-AV-Fusion" rel="external noopener nofollow">this https URL</a>}      
### 41.Robot control for simultaneous impact tasks via QP based reference spreading  [ :arrow_down: ](https://arxiv.org/pdf/2111.05211.pdf)
>  With the aim of further enabling the exploitation of impacts in robotic manipulation, a control framework is presented that directly tackles the challenges posed by tracking control of robotic manipulators that are tasked to perform nominally simultaneous impacts associated to multiple contact points. To this end, we extend the framework of reference spreading, which uses an extended ante- and post-impact reference coherent with a rigid impact map, determined under the assumption of an inelastic simultaneous impact. In practice, the robot will not reside exactly on the reference at the impact moment; as a result a sequence of impacts at the different contact points will typically occur. Our new approach extends reference spreading in this context via the introduction of an additional intermediate control mode. In this mode, a torque command is still based on the ante-impact reference with the goal of reaching the target contact state, but velocity feedback is disabled as this can be potentially harmful due to rapid velocity changes. With an eye towards real implementation, the approach is formulated using a QP control framework and is validated using numerical simulations both on a rigid robot model and on a realistic robot model with flexible joints.      
### 42.Footstep Adjustment for Biped Push Recovery on Slippery Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2111.05203.pdf)
>  Despite extensive studies on motion stabilization of bipeds, they still suffer from the lack of disturbance coping capability on slippery surfaces. In this paper, a novel controller for stabilizing a bipedal motion in its sagittal plane is developed with regard to the surface friction limitations. By taking into account the physical limitation of the surface in the stabilization trend, a more advanced level of reliability is achieved that provides higher functionalities such as push recovery on low-friction surfaces and prevents the stabilizer from overreacting. The discrete event-based strategy consists of modifying the step length and time period at the beginning of each footstep in order to reestablish stability necessary conditions while taking into account the surface friction limitation as a constraint to prevent slippage. Adjusting footsteps to prevent slippage in confronting external disturbances is perceived as a novel strategy for keeping stability, quite similar to human reaction. The developed methodology consists of rough closed-form solutions utilizing elementary math operations for obtaining the control inputs, allowing to reach a balance between convergence and computational cost, which is quite suitable for real-time operations even with modest computational hardware. Several numerical simulations, including push recovery and switching between different gates on low-friction surfaces, are performed to demonstrate the effectiveness of the proposed controller. In correlation with human-gait experience, the results also reveal some physical aspects favoring stability and the fact of switching between gaits to reduce the risk of falling in confronting different conditions.      
### 43.Does Thermal data make the detection systems more reliable?  [ :arrow_down: ](https://arxiv.org/pdf/2111.05191.pdf)
>  Deep learning-based detection networks have made remarkable progress in autonomous driving systems (ADS). ADS should have reliable performance across a variety of ambient lighting and adverse weather conditions. However, luminance degradation and visual obstructions (such as glare, fog) result in poor quality images by the visual camera which leads to performance decline. To overcome these challenges, we explore the idea of leveraging a different data modality that is disparate yet complementary to the visual data. We propose a comprehensive detection system based on a multimodal-collaborative framework that learns from both RGB (from visual cameras) and thermal (from Infrared cameras) data. This framework trains two networks collaboratively and provides flexibility in learning optimal features of its own modality while also incorporating the complementary knowledge of the other. Our extensive empirical results show that while the improvement in accuracy is nominal, the value lies in challenging and extremely difficult edge cases which is crucial in safety-critical applications such as AD. We provide a holistic view of both merits and limitations of using a thermal imaging system in detection.      
### 44.CAESynth: Real-Time Timbre Interpolation and Pitch Control with Conditional Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2111.05174.pdf)
>  In this paper, we present a novel audio synthesizer, CAESynth, based on a conditional autoencoder. CAESynth synthesizes timbre in real-time by interpolating the reference sounds in their shared latent feature space, while controlling a pitch independently. We show that training a conditional autoencoder based on accuracy in timbre classification together with adversarial regularization of pitch content allows timbre distribution in latent space to be more effective and stable for timbre interpolation and pitch conditioning. The proposed method is applicable not only to creation of musical cues but also to exploration of audio affordance in mixed reality based on novel timbre mixtures with environmental sounds. We demonstrate by experiments that CAESynth achieves smooth and high-fidelity audio synthesis in real-time through timbre interpolation and independent yet accurate pitch control for musical cues as well as for audio affordance with environmental sound. A Python implementation along with some generated samples are shared online.      
### 45.Losses, Dissonances, and Distortions  [ :arrow_down: ](https://arxiv.org/pdf/2111.05128.pdf)
>  In this paper I present a study in using the losses and gradients obtained during the training of a simple function approximator as a mechanism for creating musical dissonance and visual distortion in a solo piano performance setting. These dissonances and distortions become part of an artistic performance not just by affecting the visualizations, but also by affecting the artistic musical performance. The system is designed such that the performer can in turn affect the training process itself, thereby creating a closed feedback loop between two processes: the training of a machine learning model and the performance of an improvised piano piece.      
### 46.Membership Inference Attacks Against Self-supervised Speech Models  [ :arrow_down: ](https://arxiv.org/pdf/2111.05113.pdf)
>  Recently, adapting the idea of self-supervised learning (SSL) on continuous speech has started gaining attention. SSL models pre-trained on a huge amount of unlabeled audio can generate general-purpose representations that benefit a wide variety of speech processing tasks. Despite their ubiquitous deployment, however, the potential privacy risks of these models have not been well investigated. In this paper, we present the first privacy analysis on several SSL speech models using Membership Inference Attacks (MIA) under black-box access. The experiment results show that these pre-trained models are vulnerable to MIA and prone to membership information leakage with high adversarial advantage scores in both utterance-level and speaker-level. Furthermore, we also conduct several ablation studies to understand the factors that contribute to the success of MIA.      
### 47.Speaker Generation  [ :arrow_down: ](https://arxiv.org/pdf/2111.05095.pdf)
>  This work explores the task of synthesizing speech in nonexistent human-sounding voices. We call this task "speaker generation", and present TacoSpawn, a system that performs competitively at this task. TacoSpawn is a recurrent attention-based text-to-speech model that learns a distribution over a speaker embedding space, which enables sampling of novel and diverse speakers. Our method is easy to implement, and does not require transfer learning from speaker ID systems. We present objective and subjective metrics for evaluating performance on this task, and demonstrate that our proposed objective metrics correlate with human perception of speaker similarity. Audio samples are available on our demo page.      
### 48.MMD-ReID: A Simple but Effective Solution for Visible-Thermal Person ReID  [ :arrow_down: ](https://arxiv.org/pdf/2111.05059.pdf)
>  Learning modality invariant features is central to the problem of Visible-Thermal cross-modal Person Reidentification (VT-ReID), where query and gallery images come from different modalities. Existing works implicitly align the modalities in pixel and feature spaces by either using adversarial learning or carefully designing feature extraction modules that heavily rely on domain knowledge. We propose a simple but effective framework, MMD-ReID, that reduces the modality gap by an explicit discrepancy reduction constraint. MMD-ReID takes inspiration from Maximum Mean Discrepancy (MMD), a widely used statistical tool for hypothesis testing that determines the distance between two distributions. MMD-ReID uses a novel margin-based formulation to match class-conditional feature distributions of visible and thermal samples to minimize intra-class distances while maintaining feature discriminability. MMD-ReID is a simple framework in terms of architecture and loss formulation. We conduct extensive experiments to demonstrate both qualitatively and quantitatively the effectiveness of MMD-ReID in aligning the marginal and class conditional distributions, thus learning both modality-independent and identity-consistent features. The proposed framework significantly outperforms the state-of-the-art methods on SYSU-MM01 and RegDB datasets. Code will be released at <a class="link-external link-https" href="https://github.com/vcl-iisc/MMD-ReID" rel="external noopener nofollow">this https URL</a>      
### 49.RAVE: A variational autoencoder for fast and high-quality neural audio synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2111.05011.pdf)
>  Deep generative models applied to audio have improved by a large margin the state-of-the-art in many speech and music related tasks. However, as raw waveform modelling remains an inherently difficult task, audio generative models are either computationally intensive, rely on low sampling rates, are complicated to control or restrict the nature of possible signals. Among those models, Variational AutoEncoders (VAE) give control over the generation by exposing latent variables, although they usually suffer from low synthesis quality. In this paper, we introduce a Realtime Audio Variational autoEncoder (RAVE) allowing both fast and high-quality audio waveform synthesis. We introduce a novel two-stage training procedure, namely representation learning and adversarial fine-tuning. We show that using a post-training analysis of the latent space allows a direct control between the reconstruction fidelity and the representation compactness. By leveraging a multi-band decomposition of the raw waveform, we show that our model is the first able to generate 48kHz audio signals, while simultaneously running 20 times faster than real-time on a standard laptop CPU. We evaluate synthesis quality using both quantitative and qualitative subjective experiments and show the superiority of our approach compared to existing models. Finally, we present applications of our model for timbre transfer and signal compression. All of our source code and audio examples are publicly available.      
### 50.Ultra-Low Power Keyword Spotting at the Edge  [ :arrow_down: ](https://arxiv.org/pdf/2111.04988.pdf)
>  Keyword spotting (KWS) has become an indispensable part of many intelligent devices surrounding us, as audio is one of the most efficient ways of interacting with these devices. The accuracy and performance of KWS solutions have been the main focus of the researchers, and thanks to deep learning, substantial progress has been made in this domain. However, as the use of KWS spreads into IoT devices, energy efficiency becomes a very critical requirement besides the performance. We believe KWS solutions that would seek power optimization both in the hardware and the neural network (NN) model architecture are advantageous over many solutions in the literature where mostly the architecture side of the problem is considered. In this work, we designed an optimized KWS CNN model by considering end-to-end energy efficiency for the deployment at MAX78000, an ultra-low-power CNN accelerator. With the combined hardware and model optimization approach, we achieve 96.3\% accuracy for 12 classes while only consuming 251 uJ per inference. We compare our results with other small-footprint neural network-based KWS solutions in the literature. Additionally, we share the energy consumption of our model in power-optimized ARM Cortex-M4F to depict the effectiveness of the chosen hardware for the sake of clarity.      
### 51.Evaluation Of Orthogonal Chirp Division Multiplexing For Automotive Integrated Sensing And Communications  [ :arrow_down: ](https://arxiv.org/pdf/2111.04975.pdf)
>  We consider a bistatic vehicular integrated sensing and communications (ISAC) system that employs the recently proposed orthogonal chirp division multiplexing (OCDM) multicarrier waveform. As a stand-alone communications waveform, OCDM has been shown to be robust against the interference in time-frequency selective channels. In a bistatic ISAC, we exploit this property to develop efficient receive processing algorithms that achieve high target resolution as well as high communications rate. We derive statistical bounds for our proposed Sequential symbol decoding and radar parameter estimation (SUNDAE) algorithm and compare its competitive performance with other multicarrier waveforms through numerical experiments.      
### 52.Time-Varying Channel Prediction for RIS-Assisted MU-MISO Networks via Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.04971.pdf)
>  To mitigate the effects of shadow fading and obstacle blocking, reconfigurable intelligent surface (RIS) has become a promising technology to improve the signal transmission quality of wireless communications by controlling the reconfigurable passive elements with less hardware cost and lower power consumption. However, accurate, low-latency and low-pilot-overhead channel state information (CSI) acquisition remains a considerable challenge in RIS-assisted systems due to the large number of RIS passive elements. In this paper, we propose a three-stage joint channel decomposition and prediction framework to require CSI. The proposed framework exploits the two-timescale property that the base station (BS)-RIS channel is quasi-static and the RIS-user equipment (UE) channel is fast time-varying. Specifically, in the first stage, we use the full-duplex technique to estimate the channel between a BS's specific antenna and the RIS, addressing the critical scaling ambiguity problem in the channel decomposition. We then design a novel deep neural network, namely, the sparse-connected long short-term memory (SCLSTM), and propose a SCLSTM-based algorithm in the second and third stages, respectively. The algorithm can simultaneously decompose the BS-RIS channel and RIS-UE channel from the cascaded channel and capture the temporal relationship of the RIS-UE channel for prediction. Simulation results show that our proposed framework has lower pilot overhead than the traditional channel estimation algorithms, and the proposed SCLSTM-based algorithm can also achieve more accurate CSI acquisition robustly and effectively.      
### 53.Graph-Based Depth Denoising &amp; Dequantization for Point Cloud Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2111.04946.pdf)
>  A 3D point cloud is typically constructed from depth measurements acquired by sensors at one or more viewpoints. The measurements suffer from both quantization and noise corruption. To improve quality, previous works denoise a point cloud \textit{a posteriori} after projecting the imperfect depth data onto 3D space. Instead, we enhance depth measurements directly on the sensed images \textit{a priori}, before synthesizing a 3D point cloud. By enhancing near the physical sensing process, we tailor our optimization to our depth formation model before subsequent processing steps that obscure measurement errors. Specifically, we model depth formation as a combined process of signal-dependent noise addition and non-uniform log-based quantization. The designed model is validated (with parameters fitted) using collected empirical data from an actual depth sensor. To enhance each pixel row in a depth image, we first encode intra-view similarities between available row pixels as edge weights via feature graph learning. We next establish inter-view similarities with another rectified depth image via viewpoint mapping and sparse linear interpolation. This leads to a maximum a posteriori (MAP) graph filtering objective that is convex and differentiable. We optimize the objective efficiently using accelerated gradient descent (AGD), where the optimal step size is approximated via Gershgorin circle theorem (GCT). Experiments show that our method significantly outperformed recent point cloud denoising schemes and state-of-the-art image denoising schemes, in two established point cloud quality metrics.      
### 54.Cascaded Multilingual Audio-Visual Learning from Videos  [ :arrow_down: ](https://arxiv.org/pdf/2111.04823.pdf)
>  In this paper, we explore self-supervised audio-visual models that learn from instructional videos. Prior work has shown that these models can relate spoken words and sounds to visual content after training on a large-scale dataset of videos, but they were only trained and evaluated on videos in English. To learn multilingual audio-visual representations, we propose a cascaded approach that leverages a model trained on English videos and applies it to audio-visual data in other languages, such as Japanese videos. With our cascaded approach, we show an improvement in retrieval performance of nearly 10x compared to training on the Japanese videos solely. We also apply the model trained on English videos to Japanese and Hindi spoken captions of images, achieving state-of-the-art performance.      
### 55.Privacy Guarantees for Cloud-based State Estimation using Partially Homomorphic Encryption  [ :arrow_down: ](https://arxiv.org/pdf/2111.04818.pdf)
>  The privacy aspect of state estimation algorithms has been drawing high research attention due to the necessity for a trustworthy private environment in cyber-physical systems. These systems usually engage cloud-computing platforms to aggregate essential information from spatially distributed nodes and produce desired estimates. The exchange of sensitive data among semi-honest parties raises privacy concerns, especially when there are coalitions between parties. We propose two privacy-preserving protocols using Kalman filter and partially homomorphic encryption of the measurements and estimates while exposing the covariances and other model parameters. We prove that the proposed protocols achieve satisfying computational privacy guarantees against various coalitions based on formal cryptographic definitions of indistinguishability. We evaluated the proposed protocols to demonstrate their efficiency using data from a real testbed.      
### 56.A Multirate Variational Approach to Nonlinear MPC  [ :arrow_down: ](https://arxiv.org/pdf/2111.04811.pdf)
>  A nonlinear model predictive control (NMPC) approach is proposed based on a variational representation of the system model and the receding horizon optimal control problem. The proposed tube-based convex MPC approach provides improvements in model accuracy and computational efficiency, and allows for alternative means of computing linearization error bounds. To this end we investigate the use of single rate and multirate system representations derived from a discrete variational principle to obtain structure-preserving time-stepping schemes. We show empirically that the desirable conservation properties of the discrete time model are inherited by the optimal control problem. Model linearization is achieved either by direct Jacobian Linearization or by quadratic and linear Taylor series approximations of the Lagrangian and generalized forces respectively. These two linearization schemes are proved to be equivalent for a specific choice of approximation points. Using the multirate variational formulation we derive a novel multirate NMPC approach, and show that it can provide large computational savings for systems with dynamics or control inputs evolving on different time scales.      
### 57.Capacity and Performance Analysis of RIS-Assisted Communication Over Rician Fading Channels  [ :arrow_down: ](https://arxiv.org/pdf/2111.04783.pdf)
>  This paper investigates two performance metrics, namely ergodic capacity and symbol error rate, of mmWave communication system assisted by a reconfigurable intelligent surface (RIS). We assume independent and identically distributed (i.i.d.) Rician fadings between user-RIS-Access Point (AP), with RIS surface consisting of passive reflecting elements. First, we derive a new unified closed-form formula for the average symbol error probability of generalised M-QAM/M-PSK signalling over this mmWave link. We then obtain new closed-form expressions for the ergodic capacity with and without channel state information (CSI) at the AP.      
### 58.BRACS: A Dataset for BReAst Carcinoma Subtyping in H&amp;E Histology Images  [ :arrow_down: ](https://arxiv.org/pdf/2111.04740.pdf)
>  Breast cancer is the most commonly diagnosed cancer and registers the highest number of deaths for women with cancer. Recent advancements in diagnostic activities combined with large-scale screening policies have significantly lowered the mortality rates for breast cancer patients. However, the manual inspection of tissue slides by the pathologists is cumbersome, time-consuming, and is subject to significant inter- and intra-observer variability. Recently, the advent of whole-slide scanning systems have empowered the rapid digitization of pathology slides, and enabled to develop digital workflows. These advances further enable to leverage Artificial Intelligence (AI) to assist, automate, and augment pathological diagnosis. But the AI techniques, especially Deep Learning (DL), require a large amount of high-quality annotated data to learn from. Constructing such task-specific datasets poses several challenges, such as, data-acquisition level constrains, time-consuming and expensive annotations, and anonymization of private information. In this paper, we introduce the BReAst Carcinoma Subtyping (BRACS) dataset, a large cohort of annotated Hematoxylin &amp; Eosin (H&amp;E)-stained images to facilitate the characterization of breast lesions. BRACS contains 547 Whole-Slide Images (WSIs), and 4539 Regions of Interest (ROIs) extracted from the WSIs. Each WSI, and respective ROIs, are annotated by the consensus of three board-certified pathologists into different lesion categories. Specifically, BRACS includes three lesion types, i.e., benign, malignant and atypical, which are further subtyped into seven categories. It is, to the best of our knowledge, the largest annotated dataset for breast cancer subtyping both at WSI- and ROI-level. Further, by including the understudied atypical lesions, BRACS offers an unique opportunity for leveraging AI to better understand their characteristics.      
### 59.HEROHE Challenge: assessing HER2 status in breast cancer without immunohistochemistry or in situ hybridization  [ :arrow_down: ](https://arxiv.org/pdf/2111.04738.pdf)
>  Breast cancer is the most common malignancy in women, being responsible for more than half a million deaths every year. As such, early and accurate diagnosis is of paramount importance. Human expertise is required to diagnose and correctly classify breast cancer and define appropriate therapy, which depends on the evaluation of the expression of different biomarkers such as the transmembrane protein receptor HER2. This evaluation requires several steps, including special techniques such as immunohistochemistry or in situ hybridization to assess HER2 status. With the goal of reducing the number of steps and human bias in diagnosis, the HEROHE Challenge was organized, as a parallel event of the 16th European Congress on Digital Pathology, aiming to automate the assessment of the HER2 status based only on hematoxylin and eosin stained tissue sample of invasive breast cancer. Methods to assess HER2 status were presented by 21 teams worldwide and the results achieved by some of the proposed methods open potential perspectives to advance the state-of-the-art.      
