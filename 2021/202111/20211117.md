# ArXiv eess --Wed, 17 Nov 2021
### 1.A Latent Encoder Coupled Generative Adversarial Network (LE-GAN) for Efficient Hyperspectral Image Super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2111.08685.pdf)
>  Realistic hyperspectral image (HSI) super-resolution (SR) techniques aim to generate a high-resolution (HR) HSI with higher spectral and spatial fidelity from its low-resolution (LR) counterpart. The generative adversarial network (GAN) has proven to be an effective deep learning framework for image super-resolution. However, the optimisation process of existing GAN-based models frequently suffers from the problem of mode collapse, leading to the limited capacity of spectral-spatial invariant reconstruction. This may cause the spectral-spatial distortion on the generated HSI, especially with a large upscaling factor. To alleviate the problem of mode collapse, this work has proposed a novel GAN model coupled with a latent encoder (LE-GAN), which can map the generated spectral-spatial features from the image space to the latent space and produce a coupling component to regularise the generated samples. Essentially, we treat an HSI as a high-dimensional manifold embedded in a latent space. Thus, the optimisation of GAN models is converted to the problem of learning the distributions of high-resolution HSI samples in the latent space, making the distributions of the generated super-resolution HSIs closer to those of their original high-resolution counterparts. We have conducted experimental evaluations on the model performance of super-resolution and its capability in alleviating mode collapse. The proposed approach has been tested and validated based on two real HSI datasets with different sensors (i.e. AVIRIS and UHD-185) for various upscaling factors and added noise levels, and compared with the state-of-the-art super-resolution models (i.e. HyCoNet, LTTR, BAGAN, SR- GAN, WGAN).      
### 2.Unsupervised Speech Enhancement with speech recognition embedding and disentanglement losses  [ :arrow_down: ](https://arxiv.org/pdf/2111.08678.pdf)
>  Speech enhancement has recently achieved great success with various deep learning methods. However, most conventional speech enhancement systems are trained with supervised methods that impose two significant challenges. First, a majority of training datasets for speech enhancement systems are synthetic. When mixing clean speech and noisy corpora to create the synthetic datasets, domain mismatches occur between synthetic and real-world recordings of noisy speech or audio. Second, there is a trade-off between increasing speech enhancement performance and degrading speech recognition (ASR) performance. Thus, we propose an unsupervised loss function to tackle those two problems. Our function is developed by extending the MixIT loss function with speech recognition embedding and disentanglement loss. Our results show that the proposed function effectively improves the speech enhancement performance compared to a baseline trained in a supervised way on the noisy VoxCeleb dataset. While fully unsupervised training is unable to exceed the corresponding baseline, with joint super- and unsupervised training, the system is able to achieve similar speech quality and better ASR performance than the best supervised baseline.      
### 3.A Dynamic Codebook Design for Analog Beamforming in MIMO LEO Satellite Communications  [ :arrow_down: ](https://arxiv.org/pdf/2111.08655.pdf)
>  Beamforming gain is a key ingredient in the performance of LEO satellite communication systems to be integrated into cellular networks. However, beam codebooks previously designed in the context of MIMO communication for terrestrial networks, do not provide the appropriate performance in terms of inter-beam interference and gain stability as the satellite moves. In this paper, we propose a dynamic codebook that provides a stable gain during the period of time that the satellite covers a given cell, while avoiding link retraining and extra calculation as the satellite moves. In addition, the proposed codebook provides a higher signal-to-interference-plus-noise (SINR) ratio than those DFT codebooks commonly used in cellular systems.      
### 4.Single-channel speech separation using Soft-minimum Permutation Invariant Training  [ :arrow_down: ](https://arxiv.org/pdf/2111.08635.pdf)
>  The goal of speech separation is to extract multiple speech sources from a single microphone recording. Recently, with the advancement of deep learning and availability of large datasets, speech separation has been formulated as a supervised learning problem. These approaches aim to learn discriminative patterns of speech, speakers, and background noise using a supervised learning algorithm, typically a deep neural network. A long-lasting problem in supervised speech separation is finding the correct label for each separated speech signal, referred to as label permutation ambiguity. Permutation ambiguity refers to the problem of determining the output-label assignment between the separated sources and the available single-speaker speech labels. Finding the best output-label assignment is required for calculation of separation error, which is later used for updating parameters of the model. Recently, Permutation Invariant Training (PIT) has been shown to be a promising solution in handling the label ambiguity problem. However, the overconfident choice of the output-label assignment by PIT results in a sub-optimal trained model. In this work, we propose a probabilistic optimization framework to address the inefficiency of PIT in finding the best output-label assignment. Our proposed method entitled trainable Soft-minimum PIT is then employed on the same Long-Short Term Memory (LSTM) architecture used in Permutation Invariant Training (PIT) speech separation method. The results of our experiments show that the proposed method outperforms conventional PIT speech separation significantly (p-value $ &lt; 0.01$) by +1dB in Signal to Distortion Ratio (SDR) and +1.5dB in Signal to Interference Ratio (SIR).      
### 5.Array Placement in Distributed Massive MIMO for Power Saving considering Environment Information  [ :arrow_down: ](https://arxiv.org/pdf/2111.08619.pdf)
>  Distributed massive MIMO (D-mMIMO) has been considered for future networks as it holds the potential to offer superior capacity while enabling energy savings in the network. A D-mMIMO system has multiple arrays. Optimizing the locations of the arrays is essential for the energy efficiency of the system. In existing works, array placement has been optimized mostly based on common channel models, which rely on a given statistical distribution and Euclidean distance between user locations and arrays. These models are justified if applied to sufficiently large cells, where the statistical description of the channel is expected to fit its empirical condition. However, with the advent of small cells, this is no longer the case. The channel propagation condition becomes highly environment-specific. This paper investigates array placement optimization with different ways of modeling the propagation conditions taking the environment information (e.g., buildings) into account. We capture the environment information via a graph. Two shortest path-based propagation models are introduced based on the graph. We validate the performance of the models with a ray-tracing simulator considering different signal coverage levels. The simulation results demonstrate that higher energy efficiency is achieved by using the array placements found via the proposed models compared to a Euclidean distance-based propagation model in small cells. The average power saving at 96% signal coverage, for example, reaches more than 5 dB.      
### 6.Advancement of Deep Learning in Pneumonia and Covid-19 Classification and Localization: A Qualitative and Quantitative Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2111.08606.pdf)
>  Around 450 million people are affected by pneumonia every year which results in 2.5 million deaths. Covid-19 has also affected 181 million people which has lead to 3.92 million casualties. The chances of death in both of these diseases can be significantly reduced if they are diagnosed early. However, the current methods of diagnosing pneumonia (complaints + chest X-ray) and covid-19 (RT-PCR) require the presence of expert radiologists and time, respectively. With the help of Deep Learning models, pneumonia and covid-19 can be detected instantly from Chest X-rays or CT scans. This way, the process of diagnosing Pneumonia/Covid-19 can be made more efficient and widespread. In this paper, we aim to elicit, explain, and evaluate, qualitatively and quantitatively, major advancements in deep learning methods aimed at detecting or localizing community-acquired pneumonia (CAP), viral pneumonia, and covid-19 from images of chest X-rays and CT scans. Being a systematic review, the focus of this paper lies in explaining deep learning model architectures which have either been modified or created from scratch for the task at hand wiwth focus on generalizability. For each model, this paper answers the question of why the model is designed the way it is, the challenges that a particular model overcomes, and the tradeoffs that come with modifying a model to the required specifications. A quantitative analysis of all models described in the paper is also provided to quantify the effectiveness of different models with a similar goal. Some tradeoffs cannot be quantified, and hence they are mentioned explicitly in the qualitative analysis, which is done throughout the paper. By compiling and analyzing a large quantum of research details in one place with all the datasets, model architectures, and results, we aim to provide a one-stop solution to beginners and current researchers interested in this field.      
### 7.A layer-stress learning framework universally augments deep neural network tasks  [ :arrow_down: ](https://arxiv.org/pdf/2111.08597.pdf)
>  Deep neural networks (DNN) such as Multi-Layer Perception (MLP) and Convolutional Neural Networks (CNN) represent one of the most established deep learning algorithms. Given the tremendous effects of the number of hidden layers on network architecture and performance, it is very important to choose the number of hidden layers but still a serious challenge. More importantly, the current network architectures can only process the information from the last layer of the feature extractor, which greatly limited us to further improve its performance. Here we presented a layer-stress deep learning framework (x-NN) which implemented automatic and wise depth decision on shallow or deep feature map in a deep network through firstly designing enough number of layers and then trading off them by Multi-Head Attention Block. The x-NN can make use of features from various depth layers through attention allocation and then help to make final decision as well. As a result, x-NN showed outstanding prediction ability in the Alzheimer's Disease Classification Technique Challenge PRCV 2021, in which it won the top laurel and outperformed all other AI models. Moreover, the performance of x-NN was verified by one more AD neuroimaging dataset and other AI tasks.      
### 8.Weighted Histogram Equalization Using Entropy of Probability Density Function  [ :arrow_down: ](https://arxiv.org/pdf/2111.08578.pdf)
>  Low-contrast image enhancement is essential for high-quality image display and other visual applications. However, it is a challenging task as the enhancement is expected to increase the visibility of an image while maintaining its naturalness. In this paper, the weighted histogram equalization using the entropy of the probability density function is proposed. The computation of the local mapping functions utilizes the relationship between non-height bin and height bin distributions. Finally, the complete tone mapping function is produced by concatenating local mapping functions. Computer simulation results on the CSIQ dataset demonstrate that the proposed method produces images with higher visibility and visual quality, which outperforms traditional and recently proposed contrast enhancement algorithms methods in qualitative and quantitative metrics.      
### 9.Sensitivity to User Mischaracterizations in Electric Vehicle Charging  [ :arrow_down: ](https://arxiv.org/pdf/2111.08542.pdf)
>  In this paper, we consider electric vehicle charging facilities that offer various levels of service for varying prices such that rational users choose a level of service that minimizes the total cost to themselves including an opportunity cost that incorporates users' value of time. In this setting, we study the sensitivity of the expected occupancy at the facility to mischaracterizations of user profiles and uncharacterized heterogeneity. For user profile mischaracterizations, we first provide a fundamental upper bound for the difference between the expected occupancy under any two different distributions on a user's impatience (i.e., value of time) that only depends on the minimum and maximum charging rate offered by the charging facility. Next, we consider the case when a user's impatience is a discrete random variable and study the sensitivity of the expected occupancy to the probability masses and attained values of the random variable. We show that the expected occupancy varies linearly with respect to the probability masses and is piecewise constant with respect to the attained values. Furthermore, we study the effects on the expected occupancy from the occurrence of heterogeneous user populations. In particular, we quantify the effect on the expected occupancy from the existence of sub-populations that may only select a subset of the offered service levels. Lastly, we quantify the variability of early departures on the expected occupancy. These results demonstrate how the facility operator might design prices such that the expected occupancy does not vary much under small changes in the distribution of a user's impatience, variable and limited user service needs, or uncharacterized early departure, quantities which are generally difficult to characterize accurately from data. We further demonstrate our results via examples.      
### 10.Radar Aided mmWave Vehicle-to-InfrastructureLink Configuration Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.08527.pdf)
>  The high overhead of the beam training process is the main challenge when establishing mmWave communication links, especially for vehicle-to-everything (V2X) scenarios where the channels are highly dynamic. In this paper, we obtain prior information to speed up the beam training process by implementing two deep neural networks (DNNs) that realize radar-to-communication (R2C) channel information translation in a vehicle-to-infrastructure (V2I) system. Specifically, the first DNN is built to extract the information from the radar azimuth power spectrum (APS) to reconstruct the communication APS, while the second DNN exploits the information extracted from the spatial covariance of the radar channel to realize R2C covariance prediction. The achieved data rate and the similarity between the estimated and the true communication APS are used to evaluate the prediction performance. The covariance estimation method generally provides higher similarity, as the APS predictions cannot always capture the mismatch between the radar and communication APS. Compared to the beam training method which exploits directly the radar APS without an attempt to translate it to the communication channel, our proposed deep learning (DL) aided methods remarkably reduce the beam training overhead, resulting in a 13.3% and 21.9% rate increase when using the communication APS prediction and covariance prediction, respectively.      
### 11.Binary classification of spoken words with passive elastic metastructures  [ :arrow_down: ](https://arxiv.org/pdf/2111.08503.pdf)
>  Many electronic devices spend most of their time waiting for a wake-up event: pacemakers waiting for an anomalous heartbeat, security systems on alert to detect an intruder, smartphones listening for the user to say a wake-up phrase. These devices continuously convert physical signals into electrical currents that are then analyzed on a digital computer -- leading to power consumption even when no event is taking place. Solving this problem requires the ability to passively distinguish relevant from irrelevant events (e.g. tell a wake-up phrase from a regular conversation). Here, we experimentally demonstrate an elastic metastructure, consisting of a network of coupled silicon resonators, that passively discriminates between pairs of spoken words -- solving the wake-up problem for scenarios where only two classes of events are possible. This passive speech recognition is demonstrated on a dataset from speakers with significant gender and accent diversity. The geometry of the metastructure is determined during the design process, in which the network of resonators ('mechanical neurones') learns to selectively respond to spoken words. Training is facilitated by a machine learning model that reduces the number of computationally expensive three-dimensional elastic wave simulations. By embedding event detection in the structural dynamics, mechanical neural networks thus enable novel classes of always-on smart devices with no standby power consumption.      
### 12.Human-error-potential Estimation based on Wearable Biometric Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2111.08502.pdf)
>  This study tackles on a new problem of estimating human-error potential on a shop floor on the basis of wearable sensors. Unlike existing studies that utilize biometric sensing technology to estimate people's internal state such as fatigue and mental stress, we attempt to estimate the human-error potential in a situation where a target person does not stay calm, which is much more difficult as sensor noise significantly increases. We propose a novel formulation, in which the human-error-potential estimation problem is reduced to a classification problem, and introduce a new method that can be used for solving the classification problem even with noisy sensing data. The key ideas are to model the process of calculating biometric indices probabilistically so that the prior knowledge on the biometric indices can be integrated, and to utilize the features that represent the movement of target persons in combination with biometric features. The experimental analysis showed that our method effectively estimates the human-error potential.      
### 13.Distributed Optimal Output Consensus of Uncertain Nonlinear Multi-Agent Systems over Unbalanced Directed Networks via Output Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2111.08482.pdf)
>  In this note, a novel observer-based output feedback control approach is proposed to address the distributed optimal output consensus problem of uncertain nonlinear multi-agent systems in the normal form over unbalanced directed graphs. The main challenges of the concerned problem lie in unbalanced directed graphs and nonlinearities of multi-agent systems with their agent states not available for feedback control. Based on a two-layer controller structure, a distributed optimal coordinator is first designed to convert the considered problem into a reference-tracking problem. Then a decentralized output feedback controller is developed to stabilize the resulting augmented system. A high-gain observer is exploited in controller design to estimate the agent states in the presence of uncertainties and disturbances so that the proposed controller relies only on agent outputs. The semi-global convergence of the agent outputs toward the optimal solution that minimizes the sum of all local cost functions is proved under standard assumptions. A key feature of the obtained results is that the nonlinear agents under consideration are only required to be locally Lipschitz and possess globally asymptotically stable and locally exponentially stable zero dynamics.      
### 14.PySINDy: A comprehensive Python package for robust sparse system identification  [ :arrow_down: ](https://arxiv.org/pdf/2111.08481.pdf)
>  Automated data-driven modeling, the process of directly discovering the governing equations of a system from data, is increasingly being used across the scientific community. PySINDy is a Python package that provides tools for applying the sparse identification of nonlinear dynamics (SINDy) approach to data-driven model discovery. In this major update to PySINDy, we implement several advanced features that enable the discovery of more general differential equations from noisy and limited data. The library of candidate terms is extended for the identification of actuated systems, partial differential equations (PDEs), and implicit differential equations. Robust formulations, including the integral form of SINDy and ensembling techniques, are also implemented to improve performance for real-world data. Finally, we provide a range of new optimization algorithms, including several sparse regression techniques and algorithms to enforce and promote inequality constraints and stability. Together, these updates enable entirely new SINDy model discovery capabilities that have not been reported in the literature, such as constrained PDE identification and ensembling with different sparse regression optimizers.      
### 15.A Shallow U-Net Architecture for Reliably Predicting Blood Pressure (BP) from Photoplethysmogram (PPG) and Electrocardiogram (ECG) Signals  [ :arrow_down: ](https://arxiv.org/pdf/2111.08480.pdf)
>  Cardiovascular diseases are the most common causes of death around the world. To detect and treat heart-related diseases, continuous Blood Pressure (BP) monitoring along with many other parameters are required. Several invasive and non-invasive methods have been developed for this purpose. Most existing methods used in the hospitals for continuous monitoring of BP are invasive. On the contrary, cuff-based BP monitoring methods, which can predict Systolic Blood Pressure (SBP) and Diastolic Blood Pressure (DBP), cannot be used for continuous monitoring. Several studies attempted to predict BP from non-invasively collectible signals such as Photoplethysmogram (PPG) and Electrocardiogram (ECG), which can be used for continuous monitoring. In this study, we explored the applicability of autoencoders in predicting BP from PPG and ECG signals. The investigation was carried out on 12,000 instances of 942 patients of the MIMIC-II dataset and it was found that a very shallow, one-dimensional autoencoder can extract the relevant features to predict the SBP and DBP with the state-of-the-art performance on a very large dataset. Independent test set from a portion of the MIMIC-II dataset provides an MAE of 2.333 and 0.713 for SBP and DBP, respectively. On an external dataset of forty subjects, the model trained on the MIMIC-II dataset, provides an MAE of 2.728 and 1.166 for SBP and DBP, respectively. For both the cases, the results met British Hypertension Society (BHS) Grade A and surpassed the studies from the current literature.      
### 16.An Energy Consumption Model for Electrical Vehicle Networks via Extended Federated-learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.08472.pdf)
>  Electrical vehicle (EV) raises to promote an eco-sustainable society. Nevertheless, the ``range anxiety'' of EV hinders its wider acceptance among customers. This paper proposes a novel solution to range anxiety based on a federated-learning model, which is capable of estimating battery consumption and providing energy-efficient route planning for vehicle networks. Specifically, the new approach extends the federated-learning structure with two components: anomaly detection and sharing policy. The first component identifies preventing factors in model learning, while the second component offers guidelines for information sharing amongst vehicle networks when the sharing is necessary to preserve learning efficiency. The two components collaborate to enhance learning robustness against data heterogeneities in networks. Numerical experiments are conducted, and the results show that compared with considered solutions, the proposed approach could provide higher accuracy of battery-consumption estimation for vehicles under heterogeneous data distributions, without increasing the time complexity or transmitting raw data among vehicle networks.      
### 17.A Novel TSK Fuzzy System Incorporating Multi-view Collaborative Transfer Learning for Personalized Epileptic EEG Detection  [ :arrow_down: ](https://arxiv.org/pdf/2111.08457.pdf)
>  In clinical practice, electroencephalography (EEG) plays an important role in the diagnosis of epilepsy. EEG-based computer-aided diagnosis of epilepsy can greatly improve the ac-curacy of epilepsy detection while reducing the workload of physicians. However, there are many challenges in practical applications for personalized epileptic EEG detection (i.e., training of detection model for a specific person), including the difficulty in extracting effective features from one single view, the undesirable but common scenario of lacking sufficient training data in practice, and the no guarantee of identically distributed training and test data. To solve these problems, we propose a TSK fuzzy system-based epilepsy detection algorithm that integrates multi-view collaborative transfer learning. To address the challenge due to the limitation of single-view features, multi-view learning ensures the diversity of features by extracting them from different views. The lack of training data for building a personalized detection model is tackled by leveraging the knowledge from the source domain (reference scene) to enhance the performance of the target domain (current scene of interest), where mismatch of data distributions between the two domains is resolved with adaption technique based on maximum mean discrepancy. Notably, the transfer learning and multi-view feature extraction are performed at the same time. Furthermore, the fuzzy rules of the TSK fuzzy system equip the model with strong fuzzy logic inference capability. Hence, the proposed method has the potential to detect epileptic EEG signals effectively, which is demonstrated with the positive results from a large number of experiments on the CHB-MIT dataset.      
### 18.Development of a miniaturized laser-communication terminal for small satellites  [ :arrow_down: ](https://arxiv.org/pdf/2111.08454.pdf)
>  Free-space optical communication is becoming a mature technology that has been demonstrated in space a number of times in the last few years. The Japanese National Institute of Information and Communications Technology (NICT) has carried out some of the most-significant in-orbit demonstrations over the last three decades. However, this technology has not reached a wide commercial adoption yet. For this reason, NICT is currently working towards the development of a miniaturized laser-communication terminal that can be installed in very-small satellites, while also compatible with a variety of other different platforms, meeting a wide span of bandwidth requirements. The strategy adopted in this design has been to create a versatile lasercom terminal that can operate in multiple scenarios and platforms without the need of extensive customization. This manuscript describes the current efforts in NICT towards the development of this terminal, and it shows the prototype that has been already developed for the preliminary tests, which are described as well. These tests will include the performance verification using drones first with the goal of installing the prototype on High-Altitude Platform Systems (HAPS) to carry out communication links between HAPS and ground, and later with the Geostationary (GEO) orbit, covering this way a wide range of operating conditions. For these tests, in the former case the counter terminal is a simple transmitter in the case of the drone, and a transportable ground station in the case of the HAPS; and in the latter case the counter terminal is the GEO satellite ETS-IX, foreseen to be launched by NICT in 2023.      
### 19.Automatic Sleep Staging: Recent Development, Challenges, and Future Directions  [ :arrow_down: ](https://arxiv.org/pdf/2111.08446.pdf)
>  Modern deep learning holds a great potential to transform clinical practice on human sleep. Teaching a machine to carry out routine tasks would be a tremendous reduction in workload for clinicians. Sleep staging, a fundamental step in sleep practice, is a suitable task for this and will be the focus in this article. Recently, automatic sleep staging systems have been trained to mimic manual scoring, leading to similar performance to human sleep experts, at least on scoring of healthy subjects. Despite tremendous progress, we have not seen automatic sleep scoring adopted widely in clinical environments. This review aims to give a shared view of the authors on the most recent state-of-the-art development in automatic sleep staging, the challenges that still need to be addressed, and the future directions for automatic sleep scoring to achieve clinical value.      
### 20.Conjugate gradient MIMO iterative learning control using data-driven stochastic gradients  [ :arrow_down: ](https://arxiv.org/pdf/2111.08445.pdf)
>  Data-driven iterative learning control can achieve high performance for systems performing repeating tasks without the need for modeling. The aim of this paper is to develop a fast data-driven method for iterative learning control that is suitable for massive MIMO systems through the use of efficient unbiased gradient estimates. A stochastic conjugate gradient descent algorithm is developed that uses dedicated experiments to determine the conjugate search direction and optimal step size at each iteration. The approach is illustrated on a multivariable example, and it is shown that the method is superior to both the earlier stochastic gradient descent and deterministic conjugate gradient descent methods.      
### 21.Use of machine learning in geriatric clinical care for chronic diseases: a systematic literature review  [ :arrow_down: ](https://arxiv.org/pdf/2111.08441.pdf)
>  Objectives-Geriatric clinical care is a multidisciplinary assessment designed to evaluate older patients (age 65 years and above) functional ability, physical health, and cognitive wellbeing. The majority of these patients suffer from multiple chronic conditions and require special attention. Recently, hospitals utilize various artificial intelligence (AI) systems to improve care for elderly patients. The purpose of this systematic literature review is to understand the current use of AI systems, particularly machine learning (ML), in geriatric clinical care for chronic diseases. Materials and Methods-We restricted our search to eight databases, namely PubMed, WorldCat, MEDLINE, ProQuest, ScienceDirect, SpringerLink, Wiley, and ERIC, to analyze research articles published in English between January 2010 and June 2019. We focused on studies that used ML algorithms in the care of geriatrics patients with chronic conditions. Results-We identified 35 eligible studies and classified in three groups-psychological disorder (n=22), eye diseases (n=6), and others (n=7). This review identified the lack of standardized ML evaluation metrics and the need for data governance specific to health care applications. Conclusion- More studies and ML standardization tailored to health care applications are required to confirm whether ML could aid in improving geriatric clinical care.      
### 22.Comparison between Time Shifting Deviation and Cross-correlation Methods  [ :arrow_down: ](https://arxiv.org/pdf/2111.08428.pdf)
>  Time delay estimation (TDE) is an important step to identify and locate vibration source. The TDE result can be obtained by cross-correlation method through seeking the maximum correlation peak of two signals. However, the cross-correlation method will induce random error when dealing with the nonstationary signal. We propose a novel time shifting deviation (TSDEV) method to solve this problem, which has been proved to achieve ultrahigh precision localization result in the fiber vibration monitoring system. This paper compares TSDEV method with cross-correlation in detail by simulating TDE process in different conditions, such as signals with arbitrary intercepted length, nonstationary drift and correlated noise. Besides, experimental demonstration has been carried out on 60 km fiber to localize a wide band vibration signal. The typical localization error is 2 m with standard deviation of 21.4 m using TSDEV method. It stands in clear contrast to the result of cross-correlation method, whose localization error is 70 m and the standard deviation is 208.4 m. Compared with cross-correlation method, TSDEV has the same resistance to white noise, but has fewer boundary conditions and better suppression on linear drift or common noise, which leads to more precise TDE results.      
### 23.Blind Channel Estimation for MIMO Systems via Variational Inference  [ :arrow_down: ](https://arxiv.org/pdf/2111.08391.pdf)
>  In this paper, we investigate the blind channel estimation problem for MIMO systems under Rayleigh fading channel. Conventional MIMO communication techniques require transmitting a considerable amount of training symbols as pilots in each data block to obtain the channel state information (CSI) such that the transmitted signals can be successfully recovered. However, the pilot overhead and contamination become a bottleneck for the practical application of MIMO systems with the increase of the number of antennas. To overcome this obstacle, we propose a blind channel estimation framework, where we introduce an auxiliary posterior distribution of CSI and the transmitted signals given the received signals to derive a lower bound to the intractable likelihood function of the received signal. Meanwhile, we generate this auxiliary distribution by a neural network based variational inference framework, which is trained by maximizing the lower bound. The optimal auxiliary distribution which approaches real prior distribution is then leveraged to obtain the maximum a posterior (MAP) estimation of channel matrix and transmitted data. The simulation results demonstrate that the performance of the proposed blind channel estimation method closely approaches that of the conventional pilot-aided methods in terms of the channel estimation error and symbol error rate (SER) of the detected signals even without the help of pilots.      
### 24.S-DCCRN: Super Wide Band DCCRN with learnable complex feature for speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2111.08387.pdf)
>  In speech enhancement, complex neural network has shown promising performance due to their effectiveness in processing complex-valued spectrum. Most of the recent speech enhancement approaches mainly focus on wide-band signal with a sampling rate of 16K Hz. However, research on super wide band (e.g., 32K Hz) or even full-band (48K) denoising is still lacked due to the difficulty of modeling more frequency bands and particularly high frequency components. In this paper, we extend our previous deep complex convolution recurrent neural network (DCCRN) substantially to a super wide band version -- S-DCCRN, to perform speech denoising on speech of 32K Hz sampling rate. We first employ a cascaded sub-band and full-band processing module, which consists of two small-footprint DCCRNs -- one operates on sub-band signal and one operates on full-band signal, aiming at benefiting from both local and global frequency information. Moreover, instead of simply adopting the STFT feature as input, we use a complex feature encoder trained in an end-to-end manner to refine the information of different frequency bands. We also use a complex feature decoder to revert the feature to time-frequency domain. Finally, a learnable spectrum compression method is adopted to adjust the energy of different frequency bands, which is beneficial for neural network learning. The proposed model, S-DCCRN, has surpassed PercepNet as well as several competitive models and achieves state-of-the-art performance in terms of speech quality and intelligibility. Ablation studies also demonstrate the effectiveness of different contributions.      
### 25.Minimax Robust Landmine Detection Using Forward-Looking Ground-Penetrating Radar  [ :arrow_down: ](https://arxiv.org/pdf/2111.08379.pdf)
>  We propose a robust likelihood-ratio test (LRT) to detect landmines and unexploded ordnance using a forward-looking ground-penetrating radar. Instead of modeling the distributions of the target and clutter returns with parametric families, we construct a band of feasible probability densities under each hypothesis. The LRT is then devised based on the least favorable densities within the bands. This detector is designed to maximize the worst-case performance over all feasible density pairs and, hence, does not require strong assumptions about the clutter and noise distributions. The proposed technique is evaluated using electromagnetic field simulation data of shallow-buried targets. We show that, compared to detectors based on parametric models, robust detectors can lead to significantly reduced false alarm rates, particularly in cases where there is a mismatch between the assumed model and the true distributions.      
### 26.Batch Model Predictive Control for Selective Laser Melting  [ :arrow_down: ](https://arxiv.org/pdf/2111.08363.pdf)
>  Selective laser melting is a promising additive manufacturing technology enabling the fabrication of highly customizable products. A major challenge in selective laser melting is ensuring the quality of produced parts, which is influenced greatly by the thermal history of printed layers. We propose a Batch-Model Predictive Control technique based on the combination of model predictive control and iterative learning control. This approach succeeds in rejecting both repetitive and non-repetitive disturbances and thus achieves improved tracking performance and process quality. In a simulation study, the selective laser melting dynamics is approximated with a reduced-order control-oriented linear model to ensure reasonable computational complexity. The proposed approach provides convergence to the desired temperature field profile despite model uncertainty and disturbances.      
### 27.Image-specific Convolutional Kernel Modulation for Single Image Super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2111.08362.pdf)
>  Recently, deep-learning-based super-resolution methods have achieved excellent performances, but mainly focus on training a single generalized deep network by feeding numerous samples. Yet intuitively, each image has its representation, and is expected to acquire an adaptive model. For this issue, we propose a novel image-specific convolutional kernel modulation (IKM) by exploiting the global contextual information of image or feature to generate an attention weight for adaptively modulating the convolutional kernels, which outperforms the vanilla convolution and several existing attention mechanisms while embedding into the state-of-the-art architectures without any additional parameters. Particularly, to optimize our IKM in mini-batch training, we introduce an image-specific optimization (IsO) algorithm, which is more effective than the conventional mini-batch SGD optimization. Furthermore, we investigate the effect of IKM on the state-of-the-art architectures and exploit a new backbone with U-style residual learning and hourglass dense block learning, terms U-Hourglass Dense Network (U-HDN), which is an appropriate architecture to utmost improve the effectiveness of IKM theoretically and experimentally. Extensive experiments on single image super-resolution show that the proposed methods achieve superior performances over state-of-the-art methods. Code is available at <a class="link-external link-http" href="http://github.com/YuanfeiHuang/IKM" rel="external noopener nofollow">this http URL</a>.      
### 28.A Performance Bound for Model Based Online Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.08319.pdf)
>  Model based reinforcement learning (RL) refers to an approximate optimal control design for infinite-horizon (IH) problems that aims at approximating the optimal IH controller and associated cost parametrically. In online RL, the training process of the respective approximators is performed along the de facto system trajectory (potentially in addition to offline data). While there exist stability results for online RL, the IH controller performance has been addressed only fragmentary, rarely considering the parametric and error-prone nature of the approximation explicitly even in the model based case. To assess the performance for such a case, this work utilizes a model predictive control framework to mimic an online RL controller. More precisely, the optimization based controller is associated with an online adapted approximate cost which serves as a terminal cost function. The results include a stability and performance estimate statement for the control and training scheme and demonstrate the dependence of the controller's performance bound on the error resulting from parameterized cost approximation.      
### 29.End-to-end Learning of a Constellation Shape Robust to Channel Condition Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2111.08302.pdf)
>  Vendor interoperability is one of the desired future characteristics of optical networks. This means that the transmission system needs to support a variety of hardware with different components, leading to system uncertainties throughout the network. For example, uncertainties in signal-to-noise ratio and laser linewidth can negatively affect the quality of transmission within an optical network due to e.g. mis-parametrization of the transceiver signal processing parameters. In this paper, we propose to geometrically optimize a constellation shape that is robust to uncertainties in the channel conditions by utilizing end-to-end learning. In the optimization step, the channel model includes additive noise and residual phase noise. In the testing step, the channel model consists of laser phase noise, additive noise and blind phase search as the carrier phase recovery algorithm. Two noise models are considered for the additive noise: white Gaussian noise and nonlinear interference noise model for fiber communication. The latter models the behavior of an optical fiber channel more accurately because it considers the nonlinear effects of the optical fiber. For this model, the uncertainty in the signal-to-noise ratio can be divided between amplifier noise figures and launch power variations. For both noise models, our results indicate that the learned constellations are more robust to uncertainties in channel conditions compared to a standard constellation scheme such as quadrature amplitude modulation and more importantly standard geometric constellation shaping techniques.      
### 30.Image Denoising in FPGA using Generic Risk Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2111.08297.pdf)
>  The generic risk estimator addresses the problem of denoising images corrupted by additive white noise without placing any restriction on the statistical distribution of the noise. In this paper, we discuss an efficient FPGA implementation of this algorithm. We use the undecimated Haar wavelet transform with shrinkage parameters for each sub-band as the denoising function. The computational complexity and memory requirement of the algorithm is first analyzed. To optimize the performance, a combination of convolution and recursion is employed to realize Haar filter bank and gradient descent algorithm is used to find the shrinkage parameters. A fully pipelined and parallel architecture is developed to achieve high throughput. The proposed design achieves an execution time of 3.5ms for an image of size 512x512. We also show that the recursive implementation of Haar wavelet is more expensive than the direct implementation in terms of hardware utilization.      
### 31.Online Meta Adaptation for Variable-Rate Learned Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2111.08256.pdf)
>  This work addresses two major issues of end-to-end learned image compression (LIC) based on deep neural networks: variable-rate learning where separate networks are required to generate compressed images with varying qualities, and the train-test mismatch between differentiable approximate quantization and true hard quantization. We introduce an online meta-learning (OML) setting for LIC, which combines ideas from meta learning and online learning in the conditional variational auto-encoder (CVAE) framework. By treating the conditional variables as meta parameters and treating the generated conditional features as meta priors, the desired reconstruction can be controlled by the meta parameters to accommodate compression with variable qualities. The online learning framework is used to update the meta parameters so that the conditional reconstruction is adaptively tuned for the current image. Through the OML mechanism, the meta parameters can be effectively updated through SGD. The conditional reconstruction is directly based on the quantized latent representation in the decoder network, and therefore helps to bridge the gap between the training estimation and true quantized latent distribution. Experiments demonstrate that our OML approach can be flexibly applied to different state-of-the-art LIC methods to achieve additional performance improvements with little computation and transmission overhead.      
### 32.Blockage Prediction Using Wireless Signatures: Deep Learning Enables Real-World Demonstration  [ :arrow_down: ](https://arxiv.org/pdf/2111.08242.pdf)
>  Overcoming the link blockage challenges is essential for enhancing the reliability and latency of millimeter wave (mmWave) and sub-terahertz (sub-THz) communication networks. Previous approaches relied mainly on either (i) multiple-connectivity, which under-utilizes the network resources, or on (ii) the use of out-of-band and non-RF sensors to predict link blockages, which is associated with increased cost and system complexity. In this paper, we propose a novel solution that relies only on in-band mmWave wireless measurements to proactively predict future dynamic line-of-sight (LOS) link blockages. The proposed solution utilizes deep neural networks and special patterns of received signal power, that we call pre-blockage wireless signatures to infer future blockages. Specifically, the developed machine learning models attempt to predict: (i) If a future blockage will occur? (ii) When will this blockage happen? (iii) What is the type of the blockage? And (iv) what is the direction of the moving blockage? To evaluate our proposed approach, we build a large-scale real-world dataset comprising nearly $0.5$ million data points (mmWave measurements) for both indoor and outdoor blockage scenarios. The results, using this dataset, show that the proposed approach can successfully predict the occurrence of future dynamic blockages with more than 85\% accuracy. Further, for the outdoor scenario with highly-mobile vehicular blockages, the proposed model can predict the exact time of the future blockage with less than $80$ms error for blockages happening within the future $500$ms. These results, among others, highlight the promising gains of the proposed proactive blockage prediction solution which could potentially enhance the reliability and latency of future wireless networks.      
### 33.Toward UL-DL Rate Balancing: Joint Resource Allocation and Hybrid-Mode Multiple Access for UAV-BS Assisted Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.08233.pdf)
>  In this paper, we investigate unmanned aerial vehicle (UAV) assisted communication systems that require quasi-balanced data rates in uplink (UL) and downlink (DL), as well as users' heterogeneous traffic. To the best of our knowledge, this is the first work to explicitly investigate joint UL-DL optimization for UAV assisted systems under heterogeneous requirements. A hybrid-mode multiple access (HMMA) scheme is proposed toward heterogeneous traffic, where non-orthogonal multiple access (NOMA) targets high average data rate, while orthogonal multiple access (OMA) aims to meet users' instantaneous rate demands by compensating for their rates. HMMA enables a higher degree of freedom in multiple access and achieves a superior minimum average rate among users than the UAV assisted NOMA or OMA schemes. Under HMMA, a joint UL-DL resource allocation algorithm is proposed with a closed-form optimal solution for UL/DL power allocation to achieve quasi-balanced average rates for UL and DL. Furthermore, considering the error propagation in successive interference cancellation (SIC) of NOMA, an enhanced-HMMA scheme is proposed, which demonstrates high robustness against SIC error and a higher minimum average rate than the HMMA scheme.      
### 34.Attention-based Multi-hypothesis Fusion for Speech Summarization  [ :arrow_down: ](https://arxiv.org/pdf/2111.08201.pdf)
>  Speech summarization, which generates a text summary from speech, can be achieved by combining automatic speech recognition (ASR) and text summarization (TS). With this cascade approach, we can exploit state-of-the-art models and large training datasets for both subtasks, i.e., Transformer for ASR and Bidirectional Encoder Representations from Transformers (BERT) for TS. However, ASR errors directly affect the quality of the output summary in the cascade approach. We propose a cascade speech summarization model that is robust to ASR errors and that exploits multiple hypotheses generated by ASR to attenuate the effect of ASR errors on the summary. We investigate several schemes to combine ASR hypotheses. First, we propose using the sum of sub-word embedding vectors weighted by their posterior values provided by an ASR system as an input to a BERT-based TS system. Then, we introduce a more general scheme that uses an attention-based fusion module added to a pre-trained BERT module to align and combine several ASR hypotheses. Finally, we perform speech summarization experiments on the How2 dataset and a newly assembled TED-based dataset that we will release with this paper. These experiments show that retraining the BERT-based TS system with these schemes can improve summarization performance and that the attention-based fusion module is particularly effective.      
### 35.SALSA-Lite: A Fast and Effective Feature for Polyphonic Sound Event Localization and Detection with Microphone Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2111.08192.pdf)
>  Polyphonic sound event localization and detection (SELD) has many practical applications in acoustic sensing and monitoring. However, the development of real-time SELD has been limited by the demanding computational requirement of most recent SELD systems. In this work, we introduce SALSA-Lite, a fast and effective feature for polyphonic SELD using microphone array inputs. SALSA-Lite is a lightweight variation of a previously proposed SALSA feature for polyphonic SELD. SALSA, which stands for Spatial Cue-Augmented Log-Spectrogram, consists of multichannel log-spectrograms stacked channelwise with the normalized principal eigenvectors of the spectrotemporally corresponding spatial covariance matrices. In contrast to SALSA, which uses eigenvector-based spatial features, SALSA-Lite uses normalized inter-channel phase differences as spatial features, allowing a 30-fold speedup compared to the original SALSA feature. Experimental results on the TAU-NIGENS Spatial Sound Events 2021 dataset showed that the SALSA-Lite feature achieved competitive performance compared to the full SALSA feature, and significantly outperformed the traditional feature set of multichannel log-mel spectrograms with generalized cross-correlation spectra. Specifically, using SALSA-Lite features increased localization-dependent F1 score and class-dependent localization recall by 15% and 5%, respectively, compared to using multichannel log-mel spectrograms with generalized cross-correlation spectra.      
### 36.Graph neural network-based fault diagnosis: a review  [ :arrow_down: ](https://arxiv.org/pdf/2111.08185.pdf)
>  Graph neural network (GNN)-based fault diagnosis (FD) has received increasing attention in recent years, due to the fact that data coming from several application domains can be advantageously represented as graphs. Indeed, this particular representation form has led to superior performance compared to traditional FD approaches. In this review, an easy introduction to GNN, potential applications to the field of fault diagnosis, and future perspectives are given. First, the paper reviews neural network-based FD methods by focusing on their data representations, namely, time-series, images, and graphs. Second, basic principles and principal architectures of GNN are introduced, with attention to graph convolutional networks, graph attention networks, graph sample and aggregate, graph auto-encoder, and spatial-temporal graph convolutional networks. Third, the most relevant fault diagnosis methods based on GNN are validated through the detailed experiments, and conclusions are made that the GNN-based methods can achieve good fault diagnosis performance. Finally, discussions and future challenges are provided.      
### 37.Deep Diffusion Models for Robust Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2111.08177.pdf)
>  Channel estimation is a critical task in digital communications that greatly impacts end-to-end system performance. In this work, we introduce a novel approach for multiple-input multiple-output (MIMO) channel estimation using deep diffusion models. Our method uses a deep neural network that is trained to estimate the gradient of the log-likelihood of wireless channels at any point in high-dimensional space, and leverages this model to solve channel estimation via posterior sampling. We train a deep diffusion model on channel realizations from the CDL-D model for two antenna spacings and show that the approach leads to competitive in- and out-of-distribution performance when compared to generative adversarial network (GAN) and compressed sensing (CS) methods. When tested on CDL-C channels which are never seen during training or fine-tuned on, our approach leads to end-to-end coded performance gains of up to $3$ dB compared to CS methods and losses of only $0.5$ dB compared to ideal channel knowledge. To encourage open and reproducible research, our source code is available at <a class="link-external link-https" href="https://github.com/utcsilab/diffusion-channels" rel="external noopener nofollow">this https URL</a> .      
### 38.An Unsupervised Deep Unfolding Framework for robust Symbol Level Precoding  [ :arrow_down: ](https://arxiv.org/pdf/2111.08129.pdf)
>  Symbol Level Precoding (SLP) has attracted significant research interest due to its ability to exploit interference for energy-efficient transmission. This paper proposes an unsupervised deep-neural network (DNN) based SLP framework. Instead of naively training a DNN architecture for SLP without considering the specifics of the optimization objective of the SLP domain, our proposal unfolds a power minimization SLP formulation based on the interior point method (IPM) proximal `log' barrier function. Furthermore, we extend our proposal to a robust precoding design under channel state information (CSI) uncertainty. The results show that our proposed learning framework provides near-optimal performance while reducing the computational cost from O(n7.5) to O(n3) for the symmetrical system case where n = number of transmit antennas = number of users. This significant complexity reduction is also reflected in a proportional decrease in the proposed approach's execution time compared to the SLP optimization-based solution.      
### 39.Speech Prediction using an Adaptive Recurrent Neural Network with Application to Packet Loss Concealment  [ :arrow_down: ](https://arxiv.org/pdf/2111.08116.pdf)
>  This paper proposes a novel approach for speech signal prediction based on a recurrent neural network (RNN). Unlike existing RNN-based predictors, which operate on parametric features and are trained offline on a large collection of such features, the proposed predictor operates directly on speech samples and is trained online on the recent past of the speech signal. Optionally, the network can be pre-trained offline to speed-up convergence at start-up. The proposed predictor is a single end-to-end network that captures all sorts of dependencies between samples, and therefore has the potential to outperform classical linear/non-linear and short-term/long-term speech predictor structures. We apply it to the packet loss concealment (PLC) problem and show that it outperforms the standard ITU G.711 Appendix I PLC technique.      
### 40.Biologically inspired speech emotion recognition  [ :arrow_down: ](https://arxiv.org/pdf/2111.08112.pdf)
>  Conventional feature-based classification methods do not apply well to automatic recognition of speech emotions, mostly because the precise set of spectral and prosodic features that is required to identify the emotional state of a speaker has not been determined yet. This paper presents a method that operates directly on the speech signal, thus avoiding the problematic step of feature extraction. Furthermore, this method combines the strengths of the classical source-filter model of human speech production with those of the recently introduced liquid state machine (LSM), a biologically-inspired spiking neural network (SNN). The source and vocal tract components of the speech signal are first separated and converted into perceptually relevant spectral representations. These representations are then processed separately by two reservoirs of neurons. The output of each reservoir is reduced in dimensionality and fed to a final classifier. This method is shown to provide very good classification performance on the Berlin Database of Emotional Speech (Emo-DB). This seems a very promising framework for solving efficiently many other problems in speech processing.      
### 41.Learning-Based Symbol Level Precoding: A Memory-Efficient Unsupervised Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2111.08110.pdf)
>  Symbol level precoding (SLP) has been proven to be an effective means of managing the interference in a multiuser downlink transmission and also enhancing the received signal power. This paper proposes an unsupervised learning based SLP that applies to quantized deep neural networks (DNNs). Rather than simply training a DNN in a supervised mode, our proposal unfolds a power minimization SLP formulation in an imperfect channel scenario using the interior point method (IPM) proximal `log' barrier function. We use binary and ternary quantizations to compress the DNN's weight values. The results show significant memory savings for our proposals compared to the existing full-precision SLP-DNet with significant model compression of ~21x and ~13x for both binary DNN-based SLP (RSLP-BDNet) and ternary DNN-based SLP (RSLP-TDNets), respectively.      
### 42.Advantage of Machine Learning over Maximum Likelihood in Limited-Angle Low-Photon X-Ray Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2111.08011.pdf)
>  Limited-angle X-ray tomography reconstruction is an ill-conditioned inverse problem in general. Especially when the projection angles are limited and the measurements are taken in a photon-limited condition, reconstructions from classical algorithms such as filtered backprojection may lose fidelity and acquire artifacts due to the missing-cone problem. To obtain satisfactory reconstruction results, prior assumptions, such as total variation minimization and nonlocal image similarity, are usually incorporated within the reconstruction algorithm. In this work, we introduce deep neural networks to determine and apply a prior distribution in the reconstruction process. Our neural networks learn the prior directly from synthetic training samples. The neural nets thus obtain a prior distribution that is specific to the class of objects we are interested in reconstructing. In particular, we used deep generative models with 3D convolutional layers and 3D attention layers which are trained on 3D synthetic integrated circuit (IC) data from a model dubbed CircuitFaker. We demonstrate that, when the projection angles and photon budgets are limited, the priors from our deep generative models can dramatically improve the IC reconstruction quality on synthetic data compared with maximum likelihood estimation. Training the deep generative models with synthetic IC data from CircuitFaker illustrates the capabilities of the learned prior from machine learning. We expect that if the process were reproduced with experimental data, the advantage of the machine learning would persist. The advantages of machine learning in limited angle X-ray tomography may further enable applications in low-photon nanoscale imaging.      
### 43.Disparities in Dermatology AI: Assessments Using Diverse Clinical Images  [ :arrow_down: ](https://arxiv.org/pdf/2111.08006.pdf)
>  More than 3 billion people lack access to care for skin disease. AI diagnostic tools may aid in early skin cancer detection; however most models have not been assessed on images of diverse skin tones or uncommon diseases. To address this, we curated the Diverse Dermatology Images (DDI) dataset - the first publicly available, pathologically confirmed images featuring diverse skin tones. We show that state-of-the-art dermatology AI models perform substantially worse on DDI, with ROC-AUC dropping 29-40 percent compared to the models' original results. We find that dark skin tones and uncommon diseases, which are well represented in the DDI dataset, lead to performance drop-offs. Additionally, we show that state-of-the-art robust training methods cannot correct for these biases without diverse training data. Our findings identify important weaknesses and biases in dermatology AI that need to be addressed to ensure reliable application to diverse patients and across all disease.      
### 44.Solving Inverse Problems in Medical Imaging with Score-Based Generative Models  [ :arrow_down: ](https://arxiv.org/pdf/2111.08005.pdf)
>  Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.      
### 45.Continuous-Aperture MIMO for Electromagnetic Information Theory  [ :arrow_down: ](https://arxiv.org/pdf/2111.08630.pdf)
>  In recent years, the concept of continuous-aperture MIMO (CAP-MIMO) is reinvestigated to achieve improved communication performance with limited antenna apertures. Unlike the classical MIMO composed of discrete antennas, CAP-MIMO has a continuous antenna surface, which is expected to generate any current distribution (i.e., pattern) and induce controllable spatial electromagnetic waves. In this way, the information can be modulated on the electromagnetic waves, which makes it promising to approach the ultimate capacity of finite apertures. The pattern design is the key factor to determine the system performance of CAP-MIMO, but it has not been well studied in the literature. In this paper, we propose the pattern-division multiplexing to design the patterns for CAP-MIMO. Specifically, we first derive the system model of a typical CAP-MIMO system, which allows us to formulate the capacity maximization problem. Then we propose a general pattern-division multiplexing technique to transform the design of continuous pattern functions to the design of their projection lengths on finite orthogonal bases, which is able to overcome the design challenge of continuous functions. Based on this technique, we further propose an alternating optimization based pattern design scheme to solve the formulated capacity maximization problem. Simulation results show that, the capacity achieved by the proposed scheme is about 260% higher than that achieved by the benchmark scheme, which demonstrates the effectiveness of the proposed pattern-division multiplexing for CAP-MIMO.      
### 46.Communication by means of Modulated Johnson Noise  [ :arrow_down: ](https://arxiv.org/pdf/2111.08629.pdf)
>  We present the design of a new passive communication method that does not rely on ambient or generated RF sources. Instead, we exploit the Johnson (thermal) noise generated by a resistor to transmit information bits wirelessly. By switching the load connected to an antenna between a resistor and open circuit, we can achieve data rates of up to 26bps and distances of up to 7.3 meters. This communication method is orders of magnitude less power consuming than conventional communication schemes and presents the opportunity to enable wireless communication in areas with a complete lack of connectivity.      
### 47.Multi-Centroid Hyperdimensional Computing Approach for Epileptic Seizure Detection  [ :arrow_down: ](https://arxiv.org/pdf/2111.08463.pdf)
>  Long-term monitoring of patients with epilepsy presents a challenging problem from the engineering perspective of real-time detection and wearable devices design. It requires new solutions that allow continuous unobstructed monitoring and reliable detection and prediction of seizures. A high variability in the electroencephalogram (EEG) patterns exists among people, brain states, and time instances during seizures, but also during non-seizure periods. This makes epileptic seizure detection very challenging, especially if data is grouped under only seizure and non-seizure labels. <br>Hyperdimensional (HD) computing, a novel machine learning approach, comes in as a promising tool. However, it has certain limitations when the data shows a high intra-class variability. Therefore, in this work, we propose a novel semi-supervised learning approach based on a multi-centroid HD computing. The multi-centroid approach allows to have several prototype vectors representing seizure and non-seizure states, which leads to significantly improved performance when compared to a simple 2-class HD model. <br>Further, real-life data imbalance poses an additional challenge and the performance reported on balanced subsets of data is likely to be overestimated. Thus, we test our multi-centroid approach with three different dataset balancing scenarios, showing that performance improvement is higher for the less balanced dataset. More specifically, up to 14% improvement is achieved on an unbalanced test set with 10 times more non-seizure than seizure data. At the same time, the total number of sub-classes is not significantly increased compared to the balanced dataset. Thus, the proposed multi-centroid approach can be an important element in achieving a high performance of epilepsy detection with real-life data balance or during online learning, where seizures are infrequent.      
### 48.Code-free development and deployment of deep segmentation models for digital pathology  [ :arrow_down: ](https://arxiv.org/pdf/2111.08430.pdf)
>  Application of deep learning on histopathological whole slide images (WSIs) holds promise of improving diagnostic efficiency and reproducibility but is largely dependent on the ability to write computer code or purchase commercial solutions. We present a code-free pipeline utilizing free-to-use, open-source software (QuPath, DeepMIB, and FastPathology) for creating and deploying deep learning-based segmentation models for computational pathology. We demonstrate the pipeline on a use case of separating epithelium from stroma in colonic mucosa. A dataset of 251 annotated WSIs, comprising 140 hematoxylin-eosin (HE)-stained and 111 CD3 immunostained colon biopsy WSIs, were developed through active learning using the pipeline. On a hold-out test set of 36 HE and 21 CD3-stained WSIs a mean intersection over union score of 96.6% and 95.3% was achieved on epithelium segmentation. We demonstrate pathologist-level segmentation accuracy and clinical acceptable runtime performance and show that pathologists without programming experience can create near state-of-the-art segmentation solutions for histopathological WSIs using only free-to-use software. The study further demonstrates the strength of open-source solutions in its ability to create generalizable, open pipelines, of which trained models and predictions can seamlessly be exported in open formats and thereby used in external solutions. All scripts, trained models, a video tutorial, and the full dataset of 251 WSIs with ~31k epithelium annotations are made openly available at <a class="link-external link-https" href="https://github.com/andreped/NoCodeSeg" rel="external noopener nofollow">this https URL</a> to accelerate research in the field.      
### 49.Integrated Semantic and Phonetic Post-correction for Chinese Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2111.08400.pdf)
>  Due to the recent advances of natural language processing, several works have applied the pre-trained masked language model (MLM) of BERT to the post-correction of speech recognition. However, existing pre-trained models only consider the semantic correction while the phonetic features of words is neglected. The semantic-only post-correction will consequently decrease the performance since homophonic errors are fairly common in Chinese ASR. In this paper, we proposed a novel approach to collectively exploit the contextualized representation and the phonetic information between the error and its replacing candidates to alleviate the error rate of Chinese ASR. Our experiment results on real world speech recognition datasets showed that our proposed method has evidently lower CER than the baseline model, which utilized a pre-trained BERT MLM as the corrector.      
### 50.Analysis of Model-Free Reinforcement Learning Control Schemes on self-balancing Wheeled Extendible System  [ :arrow_down: ](https://arxiv.org/pdf/2111.08389.pdf)
>  Traditional linear control strategies have been extensively researched and utilized in many robotic and industrial applications and yet they dont respond to total dynamics of the systems To avoid tedious calculations for nonlinear control schemes like H infinity control and Predictive Control application of Reinforcement Learning can provide alternative solutions This article presents the implementation of RL control with Deep Deterministic Policy Gradient and Proximal Policy Optimization on a mobile selfbalancing Extendible Wheeled Inverted Pendulum EWIP system Such RL models make the task of finding satisfactory control scheme easier and respond to the dynamics effectively while self-tuning the parameters to provide better control In this article two RLbased controllers are pitted against an MPC controller to evaluate the performance on the basis of state variables of the EWIP system while following a specific desired trajectory      
### 51.Video Background Music Generation with Controllable Music Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2111.08380.pdf)
>  In this work, we address the task of video background music generation. Some previous works achieve effective music generation but are unable to generate melodious music tailored to a particular video, and none of them considers the video-music rhythmic consistency. To generate the background music that matches the given video, we first establish the rhythmic relations between video and background music. In particular, we connect timing, motion speed, and motion saliency from video with beat, simu-note density, and simu-note strength from music, respectively. We then propose CMT, a Controllable Music Transformer that enables local control of the aforementioned rhythmic features and global control of the music genre and instruments. Objective and subjective evaluations show that the generated background music has achieved satisfactory compatibility with the input videos, and at the same time, impressive music quality. Code and models are available at <a class="link-external link-https" href="https://github.com/wzk1015/video-bgm-generation" rel="external noopener nofollow">this https URL</a>.      
### 52.Hybrid Reflection Modulation  [ :arrow_down: ](https://arxiv.org/pdf/2111.08355.pdf)
>  Reconfigurable intelligent surface (RIS)-empowered communication has emerged as a novel concept for customizing future wireless environments in a cost- and energy-efficient way. However, due to double path loss, existing fully passive RIS systems that purely reflect the incident signals into preferred directions attain an unsatisfactory performance improvement over the traditional wireless networks in certain conditions. To overcome this bottleneck, we propose a novel transmission scheme, named hybrid reflection modulation (HRM), exploiting both active and passive reflecting elements at the RIS and their combinations, which enables to convey information without using any radio frequency (RF) chains. In the HRM scheme, the active reflecting elements using additional power amplifiers are able to amplify and reflect the incoming signal, while the remaining passive elements can simply reflect the signals with appropriate phase shifts. Based on this novel transmission model, we obtain an upper bound for the average bit error probability (ABEP), and derive achievable rate of the system using an information theoretic approach. Moreover, comprehensive computer simulations are performed to prove the superiority of the proposed HRM scheme over existing fully passive, fully active and reflection modulation (RM) systems.      
### 53.SEnSeI: A Deep Learning Module for Creating Sensor Independent Cloud Masks  [ :arrow_down: ](https://arxiv.org/pdf/2111.08349.pdf)
>  We introduce a novel neural network architecture -- Spectral ENcoder for SEnsor Independence (SEnSeI) -- by which several multispectral instruments, each with different combinations of spectral bands, can be used to train a generalised deep learning model. We focus on the problem of cloud masking, using several pre-existing datasets, and a new, freely available dataset for Sentinel-2. Our model is shown to achieve state-of-the-art performance on the satellites it was trained on (Sentinel-2 and Landsat 8), and is able to extrapolate to sensors it has not seen during training such as Landsat 7, PerSat-1, and Sentinel-3 SLSTR. Model performance is shown to improve when multiple satellites are used in training, approaching or surpassing the performance of specialised, single-sensor models. This work is motivated by the fact that the remote sensing community has access to data taken with a hugely variety of sensors. This has inevitably led to labelling efforts being undertaken separately for different sensors, which limits the performance of deep learning models, given their need for huge training sets to perform optimally. Sensor independence can enable deep learning models to utilise multiple datasets for training simultaneously, boosting performance and making them much more widely applicable. This may lead to deep learning approaches being used more frequently for on-board applications and in ground segment data processing, which generally require models to be ready at launch or soon afterwards.      
### 54.Pansharpening by convolutional neural networks in the full resolution framework  [ :arrow_down: ](https://arxiv.org/pdf/2111.08334.pdf)
>  In recent years, there has been a growing interest on deep learning-based pansharpening. Research has mainly focused on architectures. However, lacking a ground truth, model training is also a major issue. A popular approach is to train networks in a reduced resolution domain, using the original data as ground truths. The trained networks are then used on full resolution data, relying on an implicit scale invariance hypothesis. Results are generally good at reduced resolution, but more questionable at full resolution. Here, we propose a full-resolution training framework for deep learning-based pansharpening. Training takes place in the high resolution domain, relying only on the original data, with no loss of information. To ensure spectral and spatial fidelity, suitable losses are defined, which force the pansharpened output to be consistent with the available panchromatic and multispectral input. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1 images show that methods trained with the proposed framework guarantee an excellent performance in terms of both full-resolution numerical indexes and visual quality. The framework is fully general, and can be used to train and fine-tune any deep learning-based pansharpening network.      
### 55.Detecting acoustic reflectors using a robot's ego-noise  [ :arrow_down: ](https://arxiv.org/pdf/2111.08327.pdf)
>  In this paper, we propose a method to estimate the proximity of an acoustic reflector, e.g., a wall, using ego-noise, i.e., the noise produced by the moving parts of a listening robot. This is achieved by estimating the times of arrival of acoustic echoes reflected from the surface. Simulated experiments show that the proposed nonintrusive approach is capable of accurately estimating the distance of a reflector up to 1 meter and outperforms a previously proposed intrusive approach under loud ego-noise conditions. The proposed method is helped by a probabilistic echo detector that estimates whether or not an acoustic reflector is within a short range of the robotic platform. This preliminary investigation paves the way towards a new kind of collision avoidance system that would purely rely on audio sensors rather than conventional proximity sensors.      
### 56.A Markov Chain Approach for Myopic Multi-hop Relaying: Outage and Diversity Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2111.08296.pdf)
>  In this paper, a cooperative protocol is investigated for a multi-hop network consisting of relays with buffers of finite size, which may operate in different communication modes. The protocol is based on the myopic decode-and-forward strategy, where each node of the network cooperates with a limited number of neighboring nodes for the transmission of the signals. Each relay stores in its buffer the messages that were successfully decoded, in order to forward them through the appropriate channel links, based on its supported communication modes. A complete theoretical framework is investigated that models the evolution of the buffers and the transitions at the operations of each relay as a state Markov chain (MC). We analyze the performance of the proposed protocol in terms of outage probability and derive an expression for the achieved diversity-multiplexing tradeoff, by using the state transition matrix and the related steady state of the MC. Our results show that the proposed protocol outperforms the conventional multi-hop relaying scheme and the system's outage probability as well as the achieved diversity order depend on the degree of cooperation among neighboring nodes and the communication model that is considered for every relay of the network.      
### 57.Switching Recurrent Kalman Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.08291.pdf)
>  Forecasting driving behavior or other sensor measurements is an essential component of autonomous driving systems. Often real-world multivariate time series data is hard to model because the underlying dynamics are nonlinear and the observations are noisy. In addition, driving data can often be multimodal in distribution, meaning that there are distinct predictions that are likely, but averaging can hurt model performance. To address this, we propose the Switching Recurrent Kalman Network (SRKN) for efficient inference and prediction on nonlinear and multi-modal time-series data. The model switches among several Kalman filters that model different aspects of the dynamics in a factorized latent state. We empirically test the resulting scalable and interpretable deep state-space model on toy data sets and real driving data from taxis in Porto. In all cases, the model can capture the multimodal nature of the dynamics in the data.      
### 58.An Exploratory Study on Perceptual Spaces of the Singing Voice  [ :arrow_down: ](https://arxiv.org/pdf/2111.08196.pdf)
>  Sixty participants provided dissimilarity ratings between various singing techniques. Multidimensional scaling, class averaging and clustering techniques were used to analyse timbral spaces and how they change between different singers, genders and registers. Clustering analysis showed that ground-truth similarity and silhouette scores that were not significantly different between gender or register conditions, while similarity scores were positively correlated with participants' instrumental abilities and task comprehension. Participant feedback showed how a revised study design might mitigate noise in our data, leading to more detailed statistical results. Timbre maps and class distance analysis showed us which singing techniques remained similar to one another across gender and register conditions. This research provides insight into how the timbre space of singing changes under different conditions, highlights the subjectivity of perception between participants, and provides generalised timbre maps for regularisation in machine learning.      
### 59.MoRe-Fi: Motion-robust and Fine-grained Respiration Monitoring via Deep-Learning UWB Radar  [ :arrow_down: ](https://arxiv.org/pdf/2111.08195.pdf)
>  Crucial for healthcare and biomedical applications, respiration monitoring often employs wearable sensors in practice, causing inconvenience due to their direct contact with human bodies. Therefore, researchers have been constantly searching for contact-free alternatives. Nonetheless, existing contact-free designs mostly require human subjects to remain static, largely confining their adoptions in everyday environments where body movements are inevitable. Fortunately, radio-frequency (RF) enabled contact-free sensing, though suffering motion interference inseparable by conventional filtering, may offer a potential to distill respiratory waveform with the help of deep learning. To realize this potential, we introduce MoRe-Fi to conduct fine-grained respiration monitoring under body movements. MoRe-Fi leverages an IR-UWB radar to achieve contact-free sensing, and it fully exploits the complex radar signal for data augmentation. The core of MoRe-Fi is a novel variational encoder-decoder network; it aims to single out the respiratory waveforms that are modulated by body movements in a non-linear manner. Our experiments with 12 subjects and 66-hour data demonstrate that MoRe-Fi accurately recovers respiratory waveform despite the interference caused by body movements. We also discuss potential applications of MoRe-Fi for pulmonary disease diagnoses.      
### 60.CCA-MDD: A Coupled Cross-Attention based Framework for Streaming Mispronunciation detection and diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2111.08191.pdf)
>  End-to-end models are becoming popular approaches for mispronunciation detection and diagnosis (MDD). A streaming MDD framework which is demanded by many practical applications still remains a challenge. This paper proposes a streaming end-to-end MDD framework called CCA-MDD. CCA-MDD supports online processing and is able to run strictly in real-time. The encoder of CCA-MDD consists of a conv-Transformer network based streaming acoustic encoder and an improved cross-attention named coupled cross-attention (CCA). The coupled cross-attention integrates encoded acoustic features with pre-encoded linguistic features. An ensemble of decoders trained from multi-task learning is applied for final MDD decision. Experiments on publicly available corpora demonstrate that CCA-MDD achieves comparable performance to published offline end-to-end MDD models.      
### 61.RapidRead: Global Deployment of State-of-the-art Radiology AI for a Large Veterinary Teleradiology Practice  [ :arrow_down: ](https://arxiv.org/pdf/2111.08165.pdf)
>  This work describes the development and real-world deployment of a deep learning-based AI system for evaluating canine and feline radiographs across a broad range of findings and abnormalities. We describe a new semi-supervised learning approach that combines NLP-derived labels with self-supervised training leveraging more than 2.5 million x-ray images. Finally we describe the clinical deployment of the model including system architecture, real-time performance evaluation and data drift detection.      
### 62.Sparse Graph Learning Under Laplacian-Related Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2111.08161.pdf)
>  We consider the problem of learning a sparse undirected graph underlying a given set of multivariate data. We focus on graph Laplacian-related constraints on the sparse precision matrix that encodes conditional dependence between the random variables associated with the graph nodes. Under these constraints the off-diagonal elements of the precision matrix are non-positive (total positivity), and the precision matrix may not be full-rank. We investigate modifications to widely used penalized log-likelihood approaches to enforce total positivity but not the Laplacian structure. The graph Laplacian can then be extracted from the off-diagonal precision matrix. An alternating direction method of multipliers (ADMM) algorithm is presented and analyzed for constrained optimization under Laplacian-related constraints and lasso as well as adaptive lasso penalties. Numerical results based on synthetic data show that the proposed constrained adaptive lasso approach significantly outperforms existing Laplacian-based approaches. We also evaluate our approach on real financial data.      
### 63.On the utility of power spectral techniques with feature selection techniques for effective mental task classification in noninvasive BCI  [ :arrow_down: ](https://arxiv.org/pdf/2111.08154.pdf)
>  In this paper classification of mental task-root Brain-Computer Interfaces (BCI) is being investigated, as those are a dominant area of investigations in BCI and are of utmost interest as these systems can be augmented life of people having severe disabilities. The BCI model's performance is primarily dependent on the size of the feature vector, which is obtained through multiple channels. In the case of mental task classification, the availability of training samples to features are minimal. Very often, feature selection is used to increase the ratio for the mental task classification by getting rid of irrelevant and superfluous features. This paper proposes an approach to select relevant and non-redundant spectral features for the mental task classification. This can be done by using four very known multivariate feature selection methods viz, Bhattacharya's Distance, Ratio of Scatter Matrices, Linear Regression and Minimum Redundancy &amp; Maximum Relevance. This work also deals with a comparative analysis of multivariate and univariate feature selection for mental task classification. After applying the above-stated method, the findings demonstrate substantial improvements in the performance of the learning model for mental task classification. Moreover, the efficacy of the proposed approach is endorsed by carrying out a robust ranking algorithm and Friedman's statistical test for finding the best combinations and comparing different combinations of power spectral density and feature selection methods.      
### 64.Joint Unsupervised and Supervised Training for Multilingual ASR  [ :arrow_down: ](https://arxiv.org/pdf/2111.08137.pdf)
>  Self-supervised training has shown promising gains in pretraining models and facilitating the downstream finetuning for speech recognition, like multilingual ASR. Most existing methods adopt a 2-stage scheme where the self-supervised loss is optimized in the first pretraining stage, and the standard supervised finetuning resumes in the second stage. In this paper, we propose an end-to-end (E2E) Joint Unsupervised and Supervised Training (JUST) method to combine the supervised RNN-T loss and the self-supervised contrastive and masked language modeling (MLM) losses. We validate its performance on the public dataset Multilingual LibriSpeech (MLS), which includes 8 languages and is extremely imbalanced. On MLS, we explore (1) JUST trained from scratch, and (2) JUST finetuned from a pretrained checkpoint. Experiments show that JUST can consistently outperform other existing state-of-the-art methods, and beat the monolingual baseline by a significant margin, demonstrating JUST's capability of handling low-resource languages in multilingual ASR. Our average WER of all languages outperforms average monolingual baseline by 33.3%, and the state-of-the-art 2-stage XLSR by 32%. On low-resource languages like Polish, our WER is less than half of the monolingual baseline and even beats the supervised transfer learning method which uses external supervision.      
### 65.Joint State and Input Estimation of Agent Based on Recursive Kalman Filter Given Prior Knowledge  [ :arrow_down: ](https://arxiv.org/pdf/2111.08091.pdf)
>  Modern autonomous systems are purposed for many challenging scenarios, where agents will face unexpected events and complicated tasks. The presence of disturbance noise with control command and unknown inputs can negatively impact robot performance. Previous research of joint input and state estimation separately study the continuous and discrete cases without any prior information. This paper combines the continuous space and discrete space estimation into a unified theory based on the Expectation-Maximum (EM) algorithm. By introducing prior knowledge of events as the constraint, inequality optimization problems are formulated to determine a gain matrix or dynamic weights to realize an optimal input estimation with lower variance and more accurate decision-making. Finally, statistical results from experiments show that our algorithm owns 81\% improvement of the variance than KF and 47\% improvement than RKF in continuous space; a remarkable improvement of right decision-making probability of our input estimator in discrete space, identification ability is also analyzed by experiments.      
### 66.Learning Robust Scheduling with Search and Attention  [ :arrow_down: ](https://arxiv.org/pdf/2111.08073.pdf)
>  Allocating physical layer resources to users based on channel quality, buffer size, requirements and constraints represents one of the central optimization problems in the management of radio resources. The solution space grows combinatorially with the cardinality of each dimension making it hard to find optimal solutions using an exhaustive search or even classical optimization algorithms given the stringent time requirements. This problem is even more pronounced in MU-MIMO scheduling where the scheduler can assign multiple users to the same time-frequency physical resources. Traditional approaches thus resort to designing heuristics that trade optimality in favor of feasibility of execution. In this work we treat the MU-MIMO scheduling problem as a tree-structured combinatorial problem and, borrowing from the recent successes of AlphaGo Zero, we investigate the feasibility of searching for the best performing solutions using a combination of Monte Carlo Tree Search and Reinforcement Learning. To cater to the nature of the problem at hand, like the lack of an intrinsic ordering of the users as well as the importance of dependencies between combinations of users, we make fundamental modifications to the neural network architecture by introducing the self-attention mechanism. We then demonstrate that the resulting approach is not only feasible but vastly outperforms state-of-the-art heuristic-based scheduling approaches in the presence of measurement uncertainties and finite buffers.      
### 67.Two-dimensional Deep Regression for Early Yield Prediction of Winter Wheat  [ :arrow_down: ](https://arxiv.org/pdf/2111.08069.pdf)
>  Crop yield prediction is one of the tasks of Precision Agriculture that can be automated based on multi-source periodic observations of the fields. We tackle the yield prediction problem using a Convolutional Neural Network (CNN) trained on data that combines radar satellite imagery and on-ground information. We present a CNN architecture called Hyper3DNetReg that takes in a multi-channel input image and outputs a two-dimensional raster, where each pixel represents the predicted yield value of the corresponding input pixel. We utilize radar data acquired from the Sentinel-1 satellites, while the on-ground data correspond to a set of six raster features: nitrogen rate applied, precipitation, slope, elevation, topographic position index (TPI), and aspect. We use data collected during the early stage of the winter wheat growing season (March) to predict yield values during the harvest season (August). We present experiments over four fields of winter wheat and show that our proposed methodology yields better results than five compared methods, including multiple linear regression, an ensemble of feedforward networks using AdaBoost, a stacked autoencoder, and two other CNN architectures.      
### 68.ModelLight: Model-Based Meta-Reinforcement Learning for Traffic Signal Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.08067.pdf)
>  Traffic signal control is of critical importance for the effective use of transportation infrastructures. The rapid increase of vehicle traffic and changes in traffic patterns make traffic signal control more and more challenging. Reinforcement Learning (RL)-based algorithms have demonstrated their potential in dealing with traffic signal control. However, most existing solutions require a large amount of training data, which is unacceptable for many real-world scenarios. This paper proposes a novel model-based meta-reinforcement learning framework (ModelLight) for traffic signal control. Within ModelLight, an ensemble of models for road intersections and the optimization-based meta-learning method are used to improve the data efficiency of an RL-based traffic light control method. Experiments on real-world datasets demonstrate that ModelLight can outperform state-of-the-art traffic light control algorithms while substantially reducing the number of required interactions with the real-world environment.      
### 69.Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention  [ :arrow_down: ](https://arxiv.org/pdf/2111.08046.pdf)
>  Binaural audio gives the listener an immersive experience and can enhance augmented and virtual reality. However, recording binaural audio requires specialized setup with a dummy human head having microphones in left and right ears. Such a recording setup is difficult to build and setup, therefore mono audio has become the preferred choice in common devices. To obtain the same impact as binaural audio, recent efforts have been directed towards lifting mono audio to binaural audio conditioned on the visual input from the scene. Such approaches have not used an important cue for the task: the distance of different sound producing objects from the microphones. In this work, we argue that depth map of the scene can act as a proxy for inducing distance information of different objects in the scene, for the task of audio binauralization. We propose a novel encoder-decoder architecture with a hierarchical attention mechanism to encode image, depth and audio feature jointly. We design the network on top of state-of-the-art transformer networks for image and depth representation. We show empirically that the proposed method outperforms state-of-the-art methods comfortably for two challenging public datasets FAIR-Play and MUSIC-Stereo. We also demonstrate with qualitative results that the method is able to focus on the right information required for the task. The project details are available at \url{<a class="link-external link-https" href="https://krantiparida.github.io/projects/bmonobinaural.html" rel="external noopener nofollow">this https URL</a>}      
