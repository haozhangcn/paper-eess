# ArXiv eess --Mon, 1 Nov 2021
### 1.Angle Diversity Trasmitter For High Speed Data Center Uplink Communications  [ :arrow_down: ](https://arxiv.org/pdf/2110.15841.pdf)
>  This paper proposes an uplink optical wireless communication (OWC) link design that can be used by data centers to support communication in spine and leaf architectures between the top of rack leaf switches and large spine switches whose access points are mounted in the ceiling. The use of optical wireless links reduces cabling and allows easy reconfigurability for example when data centres expand. We consider three racks in a data center where each rack contains an Angle Diversity Transmitter (ADT) positioned on the top of the rack to realize the uplink function of a top-of-the-rack (ToR) or a leaf switch. Four receivers are considered to be installed on the ceiling where each is connected to a spine switch. Two types of optical receivers are studied which are a Wide Field-of-View Receiver (WFOVR) and an Angle Diversity Receiver (ADR). The performance of the proposed system is evaluated when the links run at data rates higher than 19 Gbps. The results indicate that the proposed approach achieves excellent performance using simple On-Off Keying (OOK)      
### 2.Characterising Linear Spatio-Temporal Dynamical Systems in the Frequency Domain  [ :arrow_down: ](https://arxiv.org/pdf/2110.15830.pdf)
>  A new concept, called the spatio-temporal transfer function (STTF), is introduced to characterise a class of linear time-invariant (LTI) spatio-temporal dynamical systems. The spatio-temporal transfer function is a natural extension of the ordinary transfer function for classical linear time-invariant control systems. As in the case of the classical transfer function, the spatio-temporal transfer function can be used to characterise, in the frequency domain, the inherent dynamics of linear time-invariant spatio-temporal systems. The introduction of the spatio-temporal transfer function should also facilitate the analysis of the dynamical stability of discrete-time spatio-temporal systems.      
### 3.Singularity and Similarity Detection from Signals Using Wavelet Transform  [ :arrow_down: ](https://arxiv.org/pdf/2110.15825.pdf)
>  The wavelet transform and related techniques are used to analyze singular and fractal signals. The normalized wavelet scalogram is introduced to detect singularities including jumps, cusps and other sharply changing points. The wavelet auto-covariance is applied to estimate the self-similarity exponent for statistical self-affine signals.      
### 4.C-MADA: Unsupervised Cross-Modality Adversarial Domain Adaptation framework for medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2110.15823.pdf)
>  Deep learning models have obtained state-of-the-art results for medical image analysis. However, when these models are tested on an unseen domain there is a significant performance degradation. In this work, we present an unsupervised Cross-Modality Adversarial Domain Adaptation (C-MADA) framework for medical image segmentation. C-MADA implements an image- and feature-level adaptation method in a sequential manner. First, images from the source domain are translated to the target domain through an un-paired image-to-image adversarial translation with cycle-consistency loss. Then, a U-Net network is trained with the mapped source domain images and target domain images in an adversarial manner to learn domain-invariant feature representations. Furthermore, to improve the networks segmentation performance, information about the shape, texture, and con-tour of the predicted segmentation is included during the adversarial train-ing. C-MADA is tested on the task of brain MRI segmentation, obtaining competitive results.      
### 5.CVAD: A generic medical anomaly detector based on Cascade VAE  [ :arrow_down: ](https://arxiv.org/pdf/2110.15811.pdf)
>  Detecting out-of-distribution (OOD) samples in medical imaging plays an important role for downstream medical diagnosis. However, existing OOD detectors are demonstrated on natural images composed of inter-classes and have difficulty generalizing to medical images. The key issue is the granularity of OOD data in the medical domain, where intra-class OOD samples are predominant. We focus on the generalizability of OOD detection for medical images and propose a self-supervised Cascade Variational autoencoder-based Anomaly Detector (CVAD). We use a variational autoencoders' cascade architecture, which combines latent representation at multiple scales, before being fed to a discriminator to distinguish the OOD data from the in-distribution (ID) data. Finally, both the reconstruction error and the OOD probability predicted by the binary discriminator are used to determine the anomalies. We compare the performance with the state-of-the-art deep learning models to demonstrate our model's efficacy on various open-access medical imaging datasets for both intra- and inter-class OOD. Further extensive results on datasets including common natural datasets show our model's effectiveness and generalizability. The code is available at <a class="link-external link-https" href="https://github.com/XiaoyuanGuo/CVAD" rel="external noopener nofollow">this https URL</a>.      
### 6.An Effective Image Restorer: Denoising and Luminance Adjustment for Low-photon-count Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2110.15715.pdf)
>  Imaging under photon-scarce situations introduces challenges to many applications as the captured images are with low signal-to-noise ratio and poor luminance. In this paper, we investigate the raw image restoration under low-photon-count conditions by simulating the imaging of quanta image sensor (QIS). We develop a lightweight framework, which consists of a multi-level pyramid denoising network (MPDNet) and a luminance adjustment (LA) module to achieve separate denoising and luminance enhancement. The main component of our framework is the multi-skip attention residual block (MARB), which integrates multi-scale feature fusion and attention mechanism for better feature representation. Our MPDNet adopts the idea of Laplacian pyramid to learn the small-scale noise map and larger-scale high-frequency details at different levels, and feature extractions are conducted on the multi-scale input images to encode richer contextual information. Our LA module enhances the luminance of the denoised image by estimating its illumination, which can better avoid color distortion. Extensive experimental results have demonstrated that our image restorer can achieve superior performance on the degraded images with various photon levels by suppressing noise and recovering luminance and color effectively.      
### 7.Fusing ASR Outputs in Joint Training for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2110.15684.pdf)
>  Alongside acoustic information, linguistic features based on speech transcripts have been proven useful in Speech Emotion Recognition (SER). However, due to the scarcity of emotion labelled data and the difficulty of recognizing emotional speech, it is hard to obtain reliable linguistic features and models in this research area. In this paper, we propose to fuse Automatic Speech Recognition (ASR) outputs into the pipeline for joint training SER. The relationship between ASR and SER is understudied, and it is unclear what and how ASR features benefit SER. By examining various ASR outputs and fusion methods, our experiments show that in joint ASR-SER training, incorporating both ASR hidden and text output using a hierarchical co-attention fusion approach improves the SER performance the most. On the IEMOCAP corpus, our approach achieves 63.4% weighted accuracy, which is close to the baseline results achieved by combining ground-truth transcripts. In addition, we also present novel word error rate analysis on IEMOCAP and layer-difference analysis of the Wav2vec 2.0 model to better understand the relationship between ASR and SER.      
### 8.3D-OOCS: Learning Prostate Segmentation with Inductive Bias  [ :arrow_down: ](https://arxiv.org/pdf/2110.15664.pdf)
>  Despite the great success of convolutional neural networks (CNN) in 3D medical image segmentation tasks, the methods currently in use are still not robust enough to the different protocols utilized by different scanners, and to the variety of image properties or artefacts they produce. To this end, we introduce OOCS-enhanced networks, a novel architecture inspired by the innate nature of visual processing in the vertebrates. With different 3D U-Net variants as the base, we add two 3D residual components to the second encoder blocks: on and off center-surround (OOCS). They generalise the ganglion pathways in the retina to a 3D setting. The use of 2D-OOCS in any standard CNN network complements the feedforward framework with sharp edge-detection inductive biases. The use of 3D-OOCS also helps 3D U-Nets to scrutinise and delineate anatomical structures present in 3D images with increased accuracy.We compared the state-of-the-art 3D U-Nets with their 3D-OOCS extensions and showed the superior accuracy and robustness of the latter in automatic prostate segmentation from 3D Magnetic Resonance Images (MRIs). For a fair comparison, we trained and tested all the investigated 3D U-Nets with the same pipeline, including automatic hyperparameter optimisation and data augmentation.      
### 9.Explicit Port-Hamiltonian FEM-Models for Linear Mechanical Systems with Non-Uniform Boundary Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2110.15608.pdf)
>  In this contribution we present how to obtain explicit state space models in port-Hamiltonian form when a mixed finite element method is applied to a linear mechanical system with non-uniform boundary conditions. The key is to express the variational problem based on the principle of virtual power, with both the Dirichlet (velocity) and Neumann (stress) boundary conditions imposed in a weak sense. As a consequence, the formal skew-adjointness of the system operator becomes directly visible after integration by parts, and, after compatible FE discretization, the boundary degrees of freedom of both causalities appear as explicit inputs in the resulting state space model. The rationale behind our formulation is illustrated using a lumped parameter example, and numerical experiments on a one-dimensional rod show the properties of the approach in practice.      
### 10.Whole Brain Segmentation with Full Volume Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2110.15601.pdf)
>  Whole brain segmentation is an important neuroimaging task that segments the whole brain volume into anatomically labeled regions-of-interest. Convolutional neural networks have demonstrated good performance in this task. Existing solutions, usually segment the brain image by classifying the voxels, or labeling the slices or the sub-volumes separately. Their representation learning is based on parts of the whole volume whereas their labeling result is produced by aggregation of partial segmentation. Learning and inference with incomplete information could lead to sub-optimal final segmentation result. To address these issues, we propose to adopt a full volume framework, which feeds the full volume brain image into the segmentation network and directly outputs the segmentation result for the whole brain volume. The framework makes use of complete information in each volume and can be implemented easily. An effective instance in this framework is given subsequently. We adopt the $3$D high-resolution network (HRNet) for learning spatially fine-grained representations and the mixed precision training scheme for memory-efficient training. Extensive experiment results on a publicly available $3$D MRI brain dataset show that our proposed model advances the state-of-the-art methods in terms of segmentation performance. Source code is publicly available at <a class="link-external link-https" href="https://github.com/microsoft/VoxHRNet" rel="external noopener nofollow">this https URL</a>.      
### 11.Data Driven based Dynamic Correction Prediction Model for NOx Emission of Coal Fired Boiler  [ :arrow_down: ](https://arxiv.org/pdf/2110.15600.pdf)
>  The real-time prediction of NOx emissions is of great significance for pollutant emission control and unit operation of coal-fired power plants. Aiming at dealing with the large time delay and strong nonlinear characteristics of the combustion process, a dynamic correction prediction model considering the time delay is proposed. First, the maximum information coefficient (MIC) is used to calculate the delay time between related parameters and NOx emissions, and the modeling data set is reconstructed; then, an adaptive feature selection algorithm based on Lasso and ReliefF is constructed to filter out the high correlation with NOx emissions. Parameters; Finally, an extreme learning machine (ELM) model combined with error correction was established to achieve the purpose of dynamically predicting the concentration of nitrogen oxides. Experimental results based on actual data show that the same variable has different delay times under load conditions such as rising, falling, and steady; and there are differences in model characteristic variables under different load conditions; dynamic error correction strategies effectively improve modeling accuracy; proposed The prediction error of the algorithm under different working conditions is less than 2%, which can accurately predict the NOx concentration at the combustion outlet, and provide guidance for NOx emission monitoring and combustion process optimization.      
### 12.Nash equilibrium of multi-agent graphical game with a privacy information encrypted learning algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2110.15588.pdf)
>  This paper studies the global Nash equilibrium problem of leader-follower multi-agent dynamics, which yields consensus with a privacy information encrypted learning algorithm. With the secure hierarchical structure, the relationship between the secure consensus problem and global Nash equilibrium is discussed under potential packet loss attacks, and the necessary and sufficient condition for the existence of global Nash equilibrium is provided regarding the soft-constrained graphical game. To achieve the optimal policies, the convergence of decentralized learning algorithm is guaranteed with an iteratively updated pair of decoupled gains. By using the developed quantization scheme and additive-multiplicative property, the encryption-decryption is successfully embedded in the data transmission and computation to overcome the potential privacy violation in unreliable networks. A simulation example is provided to verify the effectiveness of the designed algorithm.      
### 13.SA-SDR: A novel loss function for separation of meeting style data  [ :arrow_down: ](https://arxiv.org/pdf/2110.15581.pdf)
>  Many state-of-the-art neural network-based source separation systems use the averaged Signal-to-Distortion Ratio (SDR) as a training objective function. The basic SDR is, however, undefined if the network reconstructs the reference signal perfectly or if the reference signal contains silence, e.g., when a two-output separator processes a single-speaker recording. Many modifications to the plain SDR have been proposed that trade-off between making the loss more robust and distorting its value. We propose to switch from a mean over the SDRs of each individual output channel to a global SDR over all output channels at the same time, which we call source-aggregated SDR (SA-SDR). This makes the loss robust against silence and perfect reconstruction as long as at least one reference signal is not silent. We experimentally show that our proposed SA-SDR is more stable and preferable over other well-known modifications when processing meeting-style data that typically contains many silent or single-speaker regions.      
### 14.Unsupervised PET Reconstruction from a Bayesian Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2110.15568.pdf)
>  Positron emission tomography (PET) reconstruction has become an ill-posed inverse problem due to low-count projection data, and a robust algorithm is urgently required to improve imaging quality. Recently, the deep image prior (DIP) has drawn much attention and has been successfully applied in several image restoration tasks, such as denoising and inpainting, since it does not need any labels (reference image). However, overfitting is a vital defect of this framework. Hence, many methods have been proposed to mitigate this problem, and DeepRED is a typical representation that combines DIP and regularization by denoising (RED). In this article, we leverage DeepRED from a Bayesian perspective to reconstruct PET images from a single corrupted sinogram without any supervised or auxiliary information. In contrast to the conventional denoisers customarily used in RED, a DnCNN-like denoiser, which can add an adaptive constraint to DIP and facilitate the computation of derivation, is employed. Moreover, to further enhance the regularization, Gaussian noise is injected into the gradient updates, deriving a Markov chain Monte Carlo (MCMC) sampler. Experimental studies on brain and whole-body datasets demonstrate that our proposed method can achieve better performance in terms of qualitative and quantitative results compared to several classic and state-of-the-art methods.      
### 15.Low-Complexity Geometrical Shaping for 4D Modulation Formats via Amplitude Coding  [ :arrow_down: ](https://arxiv.org/pdf/2110.15560.pdf)
>  Signal shaping is vital to approach Shannon's capacity, yet it is challenging to implement at very high speeds. For example, probabilistic shaping often requires arithmetic coding to realize the target distribution. Geometric shaping requires look-up tables to store the constellation points. In this paper, we propose a four-dimensional amplitude coding (4D-AC) geometrical shaper architecture. The proposed architecture can generate in real time geometrically shaped 4D formats via simple logic circuit operations and two conventional quadrature amplitude modulation (QAM) modulators. This paper describes the 4D-AC used in generating approximated versions of two recently proposed 4D orthant symmetric modulation formats with spectral efficiencies of 6 bit/4D-sym and 7 bit/4D-sym, respectively. Numerical results show losses below 0.05 dB when compared against the baseline formats.      
### 16.AI-Powered Semantic Segmentation and Fluid Volume Calculation of Lung CT images in Covid-19 Patients  [ :arrow_down: ](https://arxiv.org/pdf/2110.15558.pdf)
>  COVID-19 pandemic is a deadly disease spreading very fast. People with the confronted immune system are susceptible to many health conditions. A highly significant condition is pneumonia, which is found to be the cause of death in the majority of patients. The main purpose of this study is to find the volume of GGO and consolidation of a covid-19 patient so that the physicians can prioritize the patients. Here we used transfer learning techniques for segmentation of lung CTs with the latest libraries and techniques which reduces training time and increases the accuracy of the AI Model. This system is trained with DeepLabV3+ network architecture and model Resnet50 with Imagenet weights. We used different augmentation techniques like Gaussian Noise, Horizontal shift, color variation, etc to get to the result. Intersection over Union(IoU) is used as the performance metrics. The IoU of lung masks is predicted as 99.78% and that of infected masks is as 89.01%. Our work effectively measures the volume of infected region by calculating the volume of infected and lung mask region of the patients.      
### 17.Newtonian Mechanics Based Transient Stability PART VI: Machine Transformation  [ :arrow_down: ](https://arxiv.org/pdf/2110.15491.pdf)
>  This paper focuses on the transformations from the individual machine to the equivalent machine through the "correction" perspective of the inner-group machine. The machines are first classified as the real machine with equation of motion and the pseudo machine without equation of motion. Then, it is clarified that both individual machine and equivalent machine are real machines, while the superimposed machine is a pseudo machine. Based on the classifications of the machines, two types of machine transformations are provided. The two types of machine transformations are based on the "energy correction" and "trajectory correction" of the inner-group machine, respectively. For the energy correction case, it is clarified that the trajectory transformation completely fails, while the energy transformation mathematically holds yet it is physically meaningless. For the trajectory correction case, it is clarified that both energy transformation and trajectory transformation are established. The reason is that each trajectory-correction based individual machine has the same equation of motion, i.e., the motion of the equivalent Machine-CR. Simulation results show that the machine transformation from the individual machine to the equivalent machine can be realized only through trajectory correction.      
### 18.Physics-Driven Learning of Wasserstein GAN for Density Reconstruction in Dynamic Tomography  [ :arrow_down: ](https://arxiv.org/pdf/2110.15424.pdf)
>  Object density reconstruction from projections containing scattered radiation and noise is of critical importance in many applications. Existing scatter correction and density reconstruction methods may not provide the high accuracy needed in many applications and can break down in the presence of unmodeled or anomalous scatter and other experimental artifacts. Incorporating machine-learned models could prove beneficial for accurate density reconstruction particularly in dynamic imaging, where the time-evolution of the density fields could be captured by partial differential equations or by learning from hydrodynamics simulations. In this work, we demonstrate the ability of learned deep neural networks to perform artifact removal in noisy density reconstructions, where the noise is imperfectly characterized. We use a Wasserstein generative adversarial network (WGAN), where the generator serves as a denoiser that removes artifacts in densities obtained from traditional reconstruction algorithms. We train the networks from large density time-series datasets, with noise simulated according to parametric random distributions that may mimic noise in experiments. The WGAN is trained with noisy density frames as generator inputs, to match the generator outputs to the distribution of clean densities (time-series) from simulations. A supervised loss is also included in the training, which leads to improved density restoration performance. In addition, we employ physics-based constraints such as mass conservation during network training and application to further enable highly accurate density reconstructions. Our preliminary numerical results show that the models trained in our frameworks can remove significant portions of unknown noise in density time-series data.      
### 19.Adaptive Coalition Formation-Based Coordinated Voltage Regulation in Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.15390.pdf)
>  High penetrations of photovoltaic (PV) systems can cause severe voltage quality problems in distribution networks. This paper proposes a distributed control strategy based on the dynamic formation of coalitions to coordinate a large number of PV inverters for voltage regulation. In this strategy, a rule-based coalition formation scheme deals with the zonal voltage difference caused by the uneven integration of PV capacity. Under this scheme, PV inverters form into separate voltage regulation coalitions autonomously according to local, neighbor as well as coalition voltage magnitude and regulation capacity information. To coordinate control within each coalition, we develop a feedback-based leader-follower consensus algorithm which eliminates the voltage violations caused by the fast fluctuations of load and PV generation. This algorithm allocates the required reactive power contribution among the PV inverters according to their maximum available capacity to promote an effective and fair use of the overall voltage regulation capacity. Case studies based on realistic distribution networks and field-recorded data validate the effectiveness of the proposed control strategy. Moreover, comparison with a centralized network decomposition-based scheme shows the flexibility of coalition formation in organizing the distributed PV inverters. The robustness and generalizability of the proposed strategy are also demonstrated.      
### 20.Data-driven Residual Generation for Early Fault Detection with Limited Data  [ :arrow_down: ](https://arxiv.org/pdf/2110.15385.pdf)
>  Traditionally, fault detection and isolation community has used system dynamic equations to generate diagnosers and to analyze detectability and isolability of the dynamic systems. Model-based fault detection and isolation methods use system model to generate a set of residuals as the bases for fault detection and isolation. However, in many complex systems it is not feasible to develop highly accurate models for the systems and to keep the models updated during the system lifetime. Recently, data-driven solutions have received an immense attention in the industries systems for several practical reasons. First, these methods do not require the initial investment and expertise for developing accurate models. Moreover, it is possible to automatically update and retrain the diagnosers as the system or the environment change over time. Finally, unlike the model-based methods it is straight forward to combine time series measurements such as pressure and voltage with other sources of information such as system operating hours to achieve a higher accuracy. In this paper, we extend the traditional model-based fault detection and isolation concepts such as residuals, and detectable and isolable faults to the data-driven domain. We then propose an algorithm to automatically generate residuals from the normal operating data. We present the performance of our proposed approach through a comparative case study.      
### 21.Dual-band Harmonic and Subharmonic Frequency Generation Circuitry for Joint Communication and Localization Applications Under Severe Multipath Environment  [ :arrow_down: ](https://arxiv.org/pdf/2110.15363.pdf)
>  The next generation of ultra-dense connected and automated wireless sensor networks (WSN) requires proximity intelligence for many of its applications, especially for identification and localization. This work presents the first bidirectional circuitry for Internet of Things (IoT) transponder that reciprocally generates harmonics and subharmonics, dual-band frequencies. A multi-band or wideband localization system is essential for future intelligent WSN to mitigate the influence of multipath signals for indoor dense environment. The proposed frequency generation circuitry is based on the novel nonlinear ring resonator (NRR) operating based on standing wave resonation. The proposed NRR generates two sustainable oscillation frequencies based on the periodicity of the nonlinear circuit in the ring configuration. Due to the symmetry and reciprocity of the ring layout, the two bidirectional ports can excite the circuit at the two opposite nodes while maintaining the required boundary conditions for oscillation. The sustainable resonance conditions occur by creating zero, short impedance, or pole, infinite impedance, at subharmonic and harmonic excitation ports. The NRR circuit consumes zero DC power and covers two communication frequency plans interchangeably, which makes it a premier technique compared to the conventional ultra-wideband (UWB) localization system and conventional single-band nonlinear passive circuitry. The latter is narrowband due to the tunning limitation of the nonlinear varactor while the former is power-hungry approach with complex hardware requirements.      
### 22.On the Performance of Multihop THz Wireless System Over Mixed Channel Fading with Shadowing and Antenna Misalignment  [ :arrow_down: ](https://arxiv.org/pdf/2110.15952.pdf)
>  The existing relay-assisted terahertz (THz) wireless system is limited to dual-hop transmission with pointing errors and short-term fading without considering the shadowing effect. This paper analyzes the performance of a multihop-assisted backhaul communication mixed with an access link under the shadowed fading with antenna misalignment errors. We derive statistical results of the signal-to-noise ratio (SNR) of the multihop link by considering independent but not identically distributed (i.ni.d) $\alpha$-$\mu$ fading channel with pointing errors employing channel-assisted (CA) and fixed-gain (FG) amplify-and-forward (AF) relaying for each hop. We analyze the outage probability, average BER, and ergodic capacity performance of the mixed system considering the generalized-$K$ shadowed fading model with AF and decode-and-forward (DF) protocols employed for the access link. We derive exact expressions of the performance metrics for the CA-multihop system with the DF relaying for the last hop and upper bound of the performance for the FG-multihop system using FG and DF relaying at the last relay. We also develop asymptotic analysis in the high SNR to derive the diversity order of the system and use computer simulations to provide design and deployment aspects of multiple relays in the backhaul link to extend the communication range for THz wireless transmissions.      
### 23.Personalized breath based biometric authentication with wearable multimodality  [ :arrow_down: ](https://arxiv.org/pdf/2110.15941.pdf)
>  Breath with nose sound features has been shown as a potential biometric in personal identification and verification. In this paper, we show that information that comes from other modalities captured by motion sensors on the chest in addition to audio features could further improve the performance. Our work is composed of three main contributions: hardware creation, dataset publication, and proposed multimodal models. To be more specific, we design new hardware which consists of an acoustic sensor to collect audio features from the nose, as well as an accelerometer and gyroscope to collect movement on the chest as a result of an individual's breathing. Using this hardware, we publish a collected dataset from a number of sessions from different volunteers, each session includes three common gestures: normal, deep, and strong breathing. Finally, we experiment with two multimodal models based on Convolutional Long Short Term Memory (CNN-LSTM) and Temporal Convolutional Networks (TCN) architectures. The results demonstrate the suitability of our new hardware for both verification and identification tasks.      
### 24.Sequential Detection of a Temporary Change in Multivariate Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2110.15935.pdf)
>  In this work, we aim to provide a new and efficient recursive detection method for temporarily monitored signals. Motivated by the case of the propagation of an event through a field of sensors, we postulated that the change in the statistical properties in the monitored signals can only be temporary. Unfortunately, to our best knowledge, existing recursive and simple detection techniques such as the ones based on the cumulative sum (CUSUM) do not consider the temporary aspect of the change in a multivariate time series. In this paper, we propose a novel simple and efficient sequential detection algorithm, named Temporary-Event-CUSUM (TE-CUSUM). Combined with a new adaptive way to aggregate local CUSUM variables from each data stream, we empirically show that the TE-CUSUM has a very good detection rate in the case of an event passing through a field of sensors in a very noisy environment.      
### 25.Joint Channel Estimation and Data Detection in Cell-Free Massive MU-MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.15928.pdf)
>  We propose a joint channel estimation and data detection (JED) algorithm for densely-populated cell-free massive multiuser (MU) multiple-input multiple-output (MIMO) systems, which reduces the channel training overhead caused by the presence of hundreds of simultaneously transmitting user equipments (UEs). Our algorithm iteratively solves a relaxed version of a maximum a-posteriori JED problem and simultaneously exploits the sparsity of cell-free massive MU-MIMO channels as well as the boundedness of QAM constellations. In order to improve the performance and convergence of the algorithm, we propose methods that permute the access point and UE indices to form so-called virtual cells, which leads to better initial solutions. We assess the performance of our algorithm in terms of root-mean-squared-symbol error, bit error rate, and mutual information, and we demonstrate that JED significantly reduces the pilot overhead compared to orthogonal training, which enables reliable communication with short packets to a large number of UEs.      
### 26.Delayed Propagation Transformer: A Universal Computation Engine towards Practical Control in Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.15926.pdf)
>  Multi-agent control is a central theme in the Cyber-Physical Systems (CPS). However, current control methods either receive non-Markovian states due to insufficient sensing and decentralized design, or suffer from poor convergence. This paper presents the Delayed Propagation Transformer (DePT), a new transformer-based model that specializes in the global modeling of CPS while taking into account the immutable constraints from the physical world. DePT induces a cone-shaped spatial-temporal attention prior, which injects the information propagation and aggregation principles and enables a global view. With physical constraint inductive bias baked into its design, our DePT is ready to plug and play for a broad class of multi-agent systems. The experimental results on one of the most challenging CPS -- network-scale traffic signal control system in the open world -- show that our model outperformed the state-of-the-art expert methods on synthetic and real-world datasets. Our codes are released at: <a class="link-external link-https" href="https://github.com/VITA-Group/DePT" rel="external noopener nofollow">this https URL</a>.      
### 27.Performance Analysis of Dual-Hop THz Wireless Transmission for Backhaul Applications  [ :arrow_down: ](https://arxiv.org/pdf/2110.15919.pdf)
>  THz transmissions suffer from pointing errors due to antenna misalignment and incur higher path loss because of molecular absorption at such a high frequency. In this paper, we employ an amplify-and-forward (AF) dual-hop relay to mitigate the effect of pointing errors and extend the range of a wireless backhaul network. We provide statistical analysis on the performance of the considered system by deriving analytical expressions for the outage probability, average bit-error-rate (BER), average signal-to-noise ratio (SNR), and a lower bound on the ergodic capacity over independent and identical (i.i.d) $\alpha$-$\mu$ fading model and statistical pointing errors. Using computer simulations, we validate the derived analysis of the relay-assisted system. We demonstrate the effect of the system parameters on outage probability and average BER with the help of diversity order. We show that data rates up to several \mbox{Gbps} can be achieved using THz transmissions, which is desirable for next-generation wireless systems, especially for backhaul applications.      
### 28.Physics-informed linear regression is a competitive approach compared to Machine Learning methods in building MPC  [ :arrow_down: ](https://arxiv.org/pdf/2110.15911.pdf)
>  Because physics-based building models are difficult to obtain as each building is individual, there is an increasing interest in generating models suitable for building MPC directly from measurement data. Machine learning methods have been widely applied to this problem and validated mostly in simulation; there are, however, few studies on a direct comparison of different models or validation in real buildings to be found in the literature. Methods that are indeed validated in application often lead to computationally complex non-convex optimization problems. Here we compare physics-informed Autoregressive-Moving-Average with Exogenous Inputs (ARMAX) models to Machine Learning models based on Random Forests and Input Convex Neural Networks and the resulting convex MPC schemes in experiments on a practical building application with the goal of minimizing energy consumption while maintaining occupant comfort, and in a numerical case study. We demonstrate that Predictive Control in general leads to savings between 26% and 49% of heating and cooling energy, compared to the building's baseline hysteresis controller. Moreover, we show that all model types lead to satisfactory control performance in terms of constraint satisfaction and energy reduction. However, we also see that the physics-informed ARMAX models have a lower computational burden, and a superior sample efficiency compared to the Machine Learning based models. Moreover, even if abundant training data is available, the ARMAX models have a significantly lower prediction error than the Machine Learning models, which indicates that the encoded physics-based prior of the former cannot independently be found by the latter.      
### 29.Contrastive prediction strategies for unsupervised segmentation and categorization of phonemes and words  [ :arrow_down: ](https://arxiv.org/pdf/2110.15909.pdf)
>  We investigate the performance on phoneme categorization and phoneme and word segmentation of several self-supervised learning (SSL) methods based on Contrastive Predictive Coding (CPC). Our experiments show that with the existing algorithms there is a trade off between categorization and segmentation performance. We investigate the source of this conflict and conclude that the use of context building networks, albeit necessary for superior performance on categorization tasks, harms segmentation performance by causing a temporal shift on the learned representations. Aiming to bridge this gap, we take inspiration from the leading approach on segmentation, which simultaneously models the speech signal at the frame and phoneme level, and incorporate multi-level modelling into Aligned CPC (ACPC), a variation of CPC which exhibits the best performance on categorization tasks. Our multi-level ACPC (mACPC) improves in all categorization metrics and achieves state-of-the-art performance in word segmentation.      
### 30.Upper and Lower Bounds for End-to-End Risks in Stochastic Robot Navigation  [ :arrow_down: ](https://arxiv.org/pdf/2110.15879.pdf)
>  We present novel upper and lower bounds to estimate the collision probability of motion plans for autonomous agents with discrete-time linear Gaussian dynamics. Motion plans generated by planning algorithms cannot be perfectly executed by autonomous agents in reality due to the inherent uncertainties in the real world. Estimating collision probability is crucial to characterize the safety of trajectories and plan risk optimal trajectories. Our approach is an application of standard results in probability theory including the inequalities of Hunter, Kounias, Frechet, and Dawson. Using a ground robot navigation example, we numerically demonstrate that our method is considerably faster than the naive Monte Carlo sampling method and the proposed bounds are significantly less conservative than Boole's bound commonly used in the literature.      
### 31.Combining Unsupervised and Text Augmented Semi-Supervised Learning for Low Resourced Autoregressive Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2110.15836.pdf)
>  Recent advances in unsupervised representation learning have demonstrated the impact of pretraining on large amounts of read speech. We adapt these techniques for domain adaptation in low-resource -- both in terms of data and compute -- conversational and broadcast domains. Moving beyond CTC, we pretrain state-of-the-art Conformer models in an unsupervised manner. While the unsupervised approach outperforms traditional semi-supervised training, the techniques are complementary. Combining the techniques is a 5% absolute improvement in WER, averaged over all conditions, compared to semi-supervised training alone. Additional text data is incorporated through external language models. By using CTC-based decoding, we are better able to take advantage of the additional text data. When used as a transcription model, it allows the Conformer model to better incorporate the knowledge from the language model through semi-supervised training than shallow fusion. Final performance is an additional 2% better absolute when using CTC-based decoding for semi-supervised training compared to shallow fusion.      
### 32.Sliding window strategy for convolutional spike sorting with Lasso : Algorithm, theoretical guarantees and complexity  [ :arrow_down: ](https://arxiv.org/pdf/2110.15813.pdf)
>  We present a fast algorithm for the resolution of the Lasso for convolutional models in high dimension, with a particular focus on the problem of spike sorting in neuroscience. Making use of biological properties related to neurons, we explain how the particular structure of the problem allows several optimizations, leading to an algorithm with a temporal complexity which grows linearly with respect to the size of the recorded signal and can be performed online. Moreover the spatial separability of the initial problem allows to break it into subproblems, further reducing the complexity and making possible its application on the latest recording devices which comprise a large number of sensors. We provide several mathematical results: the size and numerical complexity of the subproblems can be estimated mathematically by using percolation theory. We also show under reasonable assumptions that the Lasso estimator retrieves the true support with large probability. Finally the theoretical time complexity of the algorithm is given. Numerical simulations are also provided in order to illustrate the efficiency of our approach.      
### 33.VRAIN-UPV MLLP's system for the Blizzard Challenge 2021  [ :arrow_down: ](https://arxiv.org/pdf/2110.15792.pdf)
>  This paper presents the VRAIN-UPV MLLP's speech synthesis system for the SH1 task of the Blizzard Challenge 2021. The SH1 task consisted in building a Spanish text-to-speech system trained on (but not limited to) the corpus released by the Blizzard Challenge 2021 organization. It included 5 hours of studio-quality recordings from a native Spanish female speaker. In our case, this dataset was solely used to build a two-stage neural text-to-speech pipeline composed of a non-autoregressive acoustic model with explicit duration modeling and a HiFi-GAN neural vocoder. Our team is identified as J in the evaluation results. Our system obtained very good results in the subjective evaluation tests. Only one system among other 11 participants achieved better naturalness than ours. Concretely, it achieved a naturalness MOS of 3.61 compared to 4.21 for real samples.      
### 34.LSTM-RPA: A Simple but Effective Long Sequence Prediction Algorithm for Music Popularity Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2110.15790.pdf)
>  The big data about music history contains information about time and users' behavior. Researchers could predict the trend of popular songs accurately by analyzing this data. The traditional trend prediction models can better predict the short trend than the long trend. In this paper, we proposed the improved LSTM Rolling Prediction Algorithm (LSTM-RPA), which combines LSTM historical input with current prediction results as model input for next time prediction. Meanwhile, this algorithm converts the long trend prediction task into multiple short trend prediction tasks. The evaluation results show that the LSTM-RPA model increased F score by 13.03%, 16.74%, 11.91%, 18.52%, compared with LSTM, BiLSTM, GRU and RNN. And our method outperforms tradi-tional sequence models, which are ARIMA and SMA, by 10.67% and 3.43% improvement in F score.Code: <a class="link-external link-https" href="https://github.com/maliaosaide/lstm-rpa" rel="external noopener nofollow">this https URL</a>      
### 35.Intelligent Reflecting Surface-Aided Wideband THz Communications: Modeling and Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2110.15768.pdf)
>  In this paper, we study the performance of wideband terahertz (THz) communications assisted by an intelligent reflecting surface (IRS). Specifically, we first introduce a generalized channel model that is suitable for electrically large THz IRSs operating in the near-field. Unlike prior works, our channel model takes into account the spherical wavefront of the emitted electromagnetic waves and the spatial-wideband effect. We next show that conventional frequency-flat beamfocusing significantly reduces the power gain due to beam squint, and hence is highly suboptimal. More importantly, we analytically characterize this reduction when the spacing between adjacent reflecting elements is negligible, i.e., holographic reflecting surfaces. Numerical results corroborate our analysis and provide important insights into the design of future IRS-aided THz systems.      
### 36.CORAA: a large corpus of spontaneous and prepared speech manually validated for speech recognition in Brazilian Portuguese  [ :arrow_down: ](https://arxiv.org/pdf/2110.15731.pdf)
>  Automatic Speech recognition (ASR) is a complex and challenging task. In recent years, there have been significant advances in the area. In particular, for the Brazilian Portuguese (BP) language, there were about 376 hours public available for ASR task until the second half of 2020. With the release of new datasets in early 2021, this number increased to 574 hours. The existing resources, however, are composed of audios containing only read and prepared speech. There is a lack of datasets including spontaneous speech, which are essential in different ASR applications. This paper presents CORAA (Corpus of Annotated Audios) v1. with 291 hours, a publicly available dataset for ASR in BP containing validated pairs (audio-transcription). CORAA also contains European Portuguese audios (4.69 hours). We also present two public ASR models based on Wav2Vec 2.0 XLSR-53 and fine-tuned over CORAA. Our best model achieved a Word Error Rate of 27.35% on CORAA test set and 16.01% on Common Voice test set. When measuring the Character Error Rate, we obtained 14.26% and 5.45% for CORAA and Common Voice, respectively. CORAA corpora were assembled to both improve ASR models in BP with phenomena from spontaneous speech and motivate young researchers to start their studies on ASR for Portuguese. All the corpora are publicly available at <a class="link-external link-https" href="https://github.com/nilc-nlp/CORAA" rel="external noopener nofollow">this https URL</a> under the CC BY-NC-ND 4.0 license.      
### 37.Decision Attentive Regularization to Improve Simultaneous Speech Translation Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.15729.pdf)
>  Simultaneous Speech-to-text Translation (SimulST) systems translate source speech in tandem with the speaker using partial input. Recent works have tried to leverage the text translation task to improve the performance of Speech Translation (ST) in the offline domain. Motivated by these improvements, we propose to add Decision Attentive Regularization (DAR) to Monotonic Multihead Attention (MMA) based SimulST systems. DAR improves the read/write decisions for speech using the Simultaneous text Translation (SimulMT) task. We also extend several techniques from the offline domain to the SimulST task. Our proposed system achieves significant performance improvements for the MuST-C English-German (EnDe) SimulST task, where we provide an average BLUE score improvement of around 4.57 points or 34.17% across different latencies. Further, the latency-quality tradeoffs establish that the proposed model achieves better results compared to the baseline.      
### 38.Influence of ASR and Language Model on Alzheimer's Disease Detection  [ :arrow_down: ](https://arxiv.org/pdf/2110.15704.pdf)
>  Alzheimer's Disease is the most common form of dementia. Automatic detection from speech could help to identify symptoms at early stages, so that preventive actions can be carried out. This research is a contribution to the ADReSSo Challenge, we analyze the usage of a SotA ASR system to transcribe participant's spoken descriptions from a picture. We analyse the loss of performance regarding the use of human transcriptions (measured using transcriptions from the 2020 ADReSS Challenge). Furthermore, we study the influence of a language model -- which tends to correct non-standard sequences of words -- with the lack of language model to decode the hypothesis from the ASR. This aims at studying the language bias and get more meaningful transcriptions based only on the acoustic information from patients. The proposed system combines acoustic -- based on prosody and voice quality -- and lexical features based on the first occurrence of the most common words. The reported results show the effect of using automatic transcripts with or without language model. The best fully automatic system achieves up to 76.06 % of accuracy (without language model), significantly higher, 3 % above, than a system employing word transcriptions decoded using general purpose language models.      
### 39.Improved FRQI on superconducting processors and its restrictions in the NISQ era  [ :arrow_down: ](https://arxiv.org/pdf/2110.15672.pdf)
>  In image processing, the amount of data to be processed grows rapidly, in particular when imaging methods yield images of more than two dimensions or time series of images. Thus, efficient processing is a challenge, as data sizes may push even supercomputers to their limits. Quantum image processing promises to encode images with logarithmically less qubits than classical pixels in the image. In theory, this is a huge progress, but so far not many experiments have been conducted in practice, in particular on real backends. Often, the precise conversion of classical data to quantum states, the exact implementation, and the interpretation of the measurements in the classical context are challenging. We investigate these practical questions in this paper. In particular, we study the feasibility of the Flexible Representation of Quantum Images (FRQI). Furthermore, we check experimentally what is the limit in the current noisy intermediate-scale quantum era, i.e. up to which image size an image can be encoded, both on simulators and on real backends. Finally, we propose a method for simplifying the circuits needed for the FRQI. With our alteration, the number of gates needed, especially of the error-prone controlled-NOT gates, can be reduced. As a consequence, the size of manageable images increases.      
### 40.Frame-Capture-Based CSI Recomposition Pertaining to Firmware-Agnostic WiFi Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2110.15660.pdf)
>  With regard to the implementation of WiFi sensing agnostic according to the availability of channel state information (CSI), we investigate the possibility of estimating a CSI matrix based on its compressed version, which is known as beamforming feedback matrix (BFM). Being different from the CSI matrix that is processed and discarded in physical layer components, the BFM can be captured using a medium-access-layer frame-capturing technique because this is exchanged among an access point (AP) and stations (STAs) over the air. This indicates that WiFi sensing that leverages the BFM matrix is more practical to implement using the pre-installed APs. However, the ability of BFM-based sensing has been evaluated in a few tasks, and more general insights into its performance should be provided. To fill this gap, we propose a CSI estimation method based on BFM, approximating the estimation function with a machine learning model. In addition, to improve the estimation accuracy, we leverage the inter-subcarrier dependency using the BFMs at multiple subcarriers in orthogonal frequency division multiplexing transmissions. Our simulation evaluation reveals that the estimated CSI matches the ground-truth amplitude. Moreover, compared to CSI estimation at each individual subcarrier, the effect of the BFMs at multiple subcarriers on the CSI estimation accuracy is validated.      
### 41.Towards automatic detection and classification of orca (Orcinus orca) calls using cross-correlation methods  [ :arrow_down: ](https://arxiv.org/pdf/2110.15593.pdf)
>  Orca (Orcinus orca) is known for complex vocalisation. Their social structure consists of pods and clans sharing unique dialects due to geographic isolation. Sound type repertoires are fundamental for monitoring orca populations and are typically created visually and aurally. An orca pod occurring in the Ligurian Sea (Pelagos Sanctuary) in December 2019 provided a unique occasion for long-term recordings. The numerous data collected with the bottom recorder were analysed with a traditional human-driven inspection to create a repertoire of this pod and to compare it to catalogues from different orca populations (Icelandic and Antarctic) investigating its origins. Automatic signal detection and cross-correlation methods (R package warbleR) were used for the first time in orca studies. We found the Pearson cross-correlation method to be efficient for most pairwise calculations (&gt; 85%) but with false positives. One sound type from our repertoire presented a high positive match (range 0.62-0.67) with one from the Icelandic catalogue, which was confirmed visually and aurally. Our first attempt to automatically classify orca sound types presented limitations due to background noise and sound complexity of orca communication. We show cross-correlation methods can be a powerful tool for sound type classification in combination with conventional methods.      
### 42.Exposing Deepfake with Pixel-wise AR and PPG Correlation from Faint Signals  [ :arrow_down: ](https://arxiv.org/pdf/2110.15561.pdf)
>  Deepfake poses a serious threat to the reliability of judicial evidence and intellectual property protection. In spite of an urgent need for Deepfake identification, existing pixel-level detection methods are increasingly unable to resist the growing realism of fake videos and lack generalization. In this paper, we propose a scheme to expose Deepfake through faint signals hidden in face videos. This scheme extracts two types of minute information hidden between face pixels-photoplethysmography (PPG) features and auto-regressive (AR) features, which are used as the basis for forensics in the temporal and spatial domains, respectively. According to the principle of PPG, tracking the absorption of light by blood cells allows remote estimation of the temporal domains heart rate (HR) of face video, and irregular HR fluctuations can be seen as traces of tampering. On the other hand, AR coefficients are able to reflect the inter-pixel correlation, and can also reflect the traces of smoothing caused by up-sampling in the process of generating fake faces. Furthermore, the scheme combines asymmetric convolution block (ACBlock)-based improved densely connected networks (DenseNets) to achieve face video authenticity forensics. Its asymmetric convolutional structure enhances the robustness of network to the input feature image upside-down and left-right flipping, so that the sequence of feature stitching does not affect detection results. Simulation results show that our proposed scheme provides more accurate authenticity detection results on multiple deep forgery datasets and has better generalization compared to the benchmark strategy.      
### 43.Comment on "Failure of the simultaneous block diagonalization technique applied to complete and cluster synchronization of random networks"  [ :arrow_down: ](https://arxiv.org/pdf/2110.15493.pdf)
>  In their recent preprint [<a class="link-https" data-arxiv-id="2108.07893v1" href="https://arxiv.org/abs/2108.07893v1">arXiv:2108.07893v1</a>], S. Panahi, N. Amaya, I. Klickstein, G. Novello, and F. Sorrentino tested the simultaneous block diagonalization (SBD) technique on synchronization in random networks and found the dimensionality reduction to be limited. Based on this observation, they claimed the SBD technique to be a failure in generic situations. Here, we show that this is not a failure of the SBD technique. Rather, it is caused by inappropriate choices of network models. SBD provides a unified framework to analyze the stability of synchronization patterns that are not encumbered by symmetry considerations, and it always finds the optimal reduction for any given synchronization pattern and network structure [SIAM Rev. 62, 817-836 (2020)]. The networks considered by Panahi et al. are poor benchmarks for the performance of the SBD technique, as these systems are often intrinsically irreducible, regardless of the method used. Thus, although the results in Panahi et al. are technically valid, their interpretations are misleading and akin to claiming a community detection algorithm to be a failure because it does not find any meaningful communities in Erdős-Rényi networks.      
### 44.Improving Noise Robustness of Contrastive Speech Representation Learning with Speech Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2110.15430.pdf)
>  Noise robustness is essential for deploying automatic speech recognition (ASR) systems in real-world environments. One way to reduce the effect of noise interference is to employ a preprocessing module that conducts speech enhancement, and then feed the enhanced speech to an ASR backend. In this work, instead of suppressing background noise with a conventional cascaded pipeline, we employ a noise-robust representation learned by a refined self-supervised framework for noisy speech recognition. We propose to combine a reconstruction module with contrastive learning and perform multi-task continual pre-training on noisy data. The reconstruction module is used for auxiliary learning to improve the noise robustness of the learned representation and thus is not required during inference. Experiments demonstrate the effectiveness of our proposed method. Our model substantially reduces the word error rate (WER) for the synthesized noisy LibriSpeech test sets, and yields around 4.1/7.5% WER reduction on noisy clean/other test sets compared to data augmentation. For the real-world noisy speech from the CHiME-4 challenge (1-channel track), we have obtained the state of the art ASR performance without any denoising front-end. Moreover, we achieve comparable performance to the best supervised approach reported with only 16% of labeled data.      
