# ArXiv eess --Thu, 18 Nov 2021
### 1.Impact of subsidy on profitability of residential photovoltaics, battery and inverter in Flanders  [ :arrow_down: ](https://arxiv.org/pdf/2111.09286.pdf)
>  The sustained future growth of renewable energy in the distribution network is governed by its financial viability. The recent subsidies in Flanders for photovoltaic (PV) and battery are used to calculate the payback period on prosumer investment in residential PV, battery and inverter installations. Realistic scenario of PV, battery and inverter are created based on the market catalogue of one manufacturer per product. Using these scenarios, one-year simulations for a residential prosumer are conducted in a rolling horizon. Luminus, an energy service provider in Limburg, time-of-use and feed-in-tariff contract rates are utilized. The numerical evaluation indicates that the policies prefer new PV installations. The subsidy comparison for 2021 to 2024 is also conducted. Oversizing PV compared to inverter size increases profitability. Batteries performing only arbitrage may not attract much investment, as the payback period is high. This shows the need to maximize the value addition of prosumer investment on residential batteries by participating in multiple goals.      
### 2.Optimization of Grant-Free NOMA with Multiple Configured-Grants for mURLLC  [ :arrow_down: ](https://arxiv.org/pdf/2111.09284.pdf)
>  Massive Ultra-Reliable and Low-Latency Communications (mURLLC), which integrates URLLC with massive access, is emerging as a new and important service class in the next generation (6G) for time-sensitive traffics and has recently received tremendous research attention. However, realizing efficient, delay-bounded, and reliable communications for a massive number of user equipments (UEs) in mURLLC, is extremely challenging as it needs to simultaneously take into account the latency, reliability, and massive access requirements. To support these requirements, the third generation partnership project (3GPP) has introduced enhanced grant-free (GF) transmission in the uplink (UL), with multiple active configured-grants (CGs) for URLLC UEs. With multiple CGs (MCG) for UL, UE can choose any of these grants as soon as the data arrives. In addition, non-orthogonal multiple access (NOMA) has been proposed to synergize with GF transmission to mitigate the serious transmission delay and network congestion problems. In this paper, we develop a novel learning framework for MCG-GF-NOMA systems with bursty traffic. We first design the MCG-GF-NOMA model by characterizing each CG using the parameters: the number of contention-transmission units (CTUs), the starting slot of each CG within a subframe, and the number of repetitions of each CG. Based on the model, the latency and reliability performances are characterized. We then formulate the MCG-GF-NOMA resources configuration problem taking into account three constraints. Finally, we propose a Cooperative Multi-Agent based Double Deep Q-Network (CMA-DDQN) algorithm to allocate the channel resources among MCGs so as to maximize the number of successful transmissions under the latency constraint. Our results show that the MCG-GF-NOMA framework can simultaneously improve the low latency and high reliability performances in massive URLLC.      
### 3.Segmentation of Lung Tumor from CT Images using Deep Supervision  [ :arrow_down: ](https://arxiv.org/pdf/2111.09262.pdf)
>  Lung cancer is a leading cause of death in most countries of the world. Since prompt diagnosis of tumors can allow oncologists to discern their nature, type and the mode of treatment, tumor detection and segmentation from CT Scan images is a crucial field of study worldwide. This paper approaches lung tumor segmentation by applying two-dimensional discrete wavelet transform (DWT) on the LOTUS dataset for more meticulous texture analysis whilst integrating information from neighboring CT slices before feeding them to a Deeply Supervised MultiResUNet model. Variations in learning rates, decay and optimization algorithms while training the network have led to different dice co-efficients, the detailed statistics of which have been included in this paper. We also discuss the challenges in this dataset and how we opted to overcome them. In essence, this study aims to maximize the success rate of predicting tumor regions from two dimensional CT Scan slices by experimenting with a number of adequate networks, resulting in a dice co-efficient of 0.8472.      
### 4.Convex Optimization for Fuel Cell Hybrid Trains: Speed, Energy Management System, and Battery Thermals  [ :arrow_down: ](https://arxiv.org/pdf/2111.09238.pdf)
>  We optimize the operation of a fuel cell hybrid train using convex optimization. The main objective is to minimize hydrogen fuel consumption for a target journey time while considering battery thermal constraints. The state trajectories: train speed, energy management system, and battery temperature, are all optimized concurrently within a single optimization problem. A novel thermal model is proposed in order to include battery temperature yet maintain formulation convexity. Simulations show fuel savings and better thermal management when temperature is optimized concurrently with the other states rather than sequentially -- separately afterwards. The fuel reduction is caused by reduced cooling effort which is motivated by the formulation's awareness of active cooling energy consumption. The benefit is more pronounced for warmer ambient temperatures that require more cooling.      
### 5.Single-pass Object-adaptive Data Undersampling and Reconstruction for MRI  [ :arrow_down: ](https://arxiv.org/pdf/2111.09212.pdf)
>  There is much recent interest in techniques to accelerate the data acquisition process in MRI by acquiring limited measurements. Often sophisticated reconstruction algorithms are deployed to maintain high image quality in such settings. In this work, we propose a data-driven sampler using a convolutional neural network, MNet, to provide object-specific sampling patterns adaptive to each scanned object. The network observes very limited low-frequency k-space data for each object and rapidly predicts the desired undersampling pattern in one go that achieves high image reconstruction quality. <br>We propose an accompanying alternating-type training framework with a mask-backward procedure that efficiently generates training labels for the sampler network and jointly trains an image reconstruction network. Experimental results on the fastMRI knee dataset demonstrate the ability of the proposed learned undersampling network to generate object-specific masks at fourfold and eightfold acceleration that achieve superior image reconstruction performance than several existing schemes. The source code for the proposed joint sampling and reconstruction learning framework is available at <a class="link-external link-https" href="https://github.com/zhishenhuang/mri" rel="external noopener nofollow">this https URL</a>.      
### 6.End-to-end optimized image compression with competition of prior distributions  [ :arrow_down: ](https://arxiv.org/pdf/2111.09172.pdf)
>  Convolutional autoencoders are now at the forefront of image compression research. To improve their entropy coding, encoder output is typically analyzed with a second autoencoder to generate per-variable parametrized prior probability distributions. We instead propose a compression scheme that uses a single convolutional autoencoder and multiple learned prior distributions working as a competition of experts. Trained prior distributions are stored in a static table of cumulative distribution functions. During inference, this table is used by an entropy coder as a look-up-table to determine the best prior for each spatial location. Our method offers rate-distortion performance comparable to that obtained with a predicted parametrized prior with only a fraction of its entropy coding and decoding complexity.      
### 7.Fast and Light-Weight Network for Single Frame Structured Illumination Microscopy Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2111.09103.pdf)
>  Structured illumination microscopy (SIM) is an important super-resolution based microscopy technique that breaks the diffraction limit and enhances optical microscopy systems. With the development of biology and medical engineering, there is a high demand for real-time and robust SIM imaging under extreme low light and short exposure environments. Existing SIM techniques typically require multiple structured illumination frames to produce a high-resolution image. In this paper, we propose a single-frame structured illumination microscopy (SF-SIM) based on deep learning. Our SF-SIM only needs one shot of a structured illumination frame and generates similar results compared with the traditional SIM systems that typically require 15 shots. In our SF-SIM, we propose a noise estimator which can effectively suppress the noise in the image and enable our method to work under the low light and short exposure environment, without the need for stacking multiple frames for non-local denoising. We also design a bandpass attention module that makes our deep network more sensitive to the change of frequency and enhances the imaging quality. Our proposed SF-SIM is almost 14 times faster than traditional SIM methods when achieving similar results. Therefore, our method is significantly valuable for the development of microbiology and medicine.      
### 8.Implementation of Noise-Shaped Signaling System through Software-Defined Radio  [ :arrow_down: ](https://arxiv.org/pdf/2111.09051.pdf)
>  As developments of electromagnetic weapons, Electronic Warfare (EW) has been rising as the future form of war. Especially in wireless communications, the high security defense systems, such as Low Probability of Detection (LPD), Low Probability of Interception (LPI), or Low Prob-ability of Exploitation (LPE) communication algorithms, are studied to prevent the military force loss. One of the LPD, LPI, and LPE communication algorithm, physical-layer security, has been discussed and studied. We propose a noise signaling system, a type of physical-layer secu-rity, which modifies conventionally modulated I/Q data into a noise-like shape. For presenting the possibility of realistic implementation, we use Software-Defined Radio (SDR). Since there are certain limitations of hardware, we present the limitations, requirements, and preferences of practical implementation of noise signaling system, and the proposed system is ring-shaped signaling. We present the ring-shaped signaling system algorithm, SDR implementation meth-odology, and performance evaluations of the system by the metrics of Bit Error Rate (BER) and Probability of Modulation Identification (PMI), which we obtain by Convolutional Neural Net-work (CNN) algorithm. We conclude that the ring-shaped signaling system can perform a high LPI/LPE communication function due to the eavesdropper cannot obtain the correct used modu-lation scheme information, and the performance can vary by the configurations of the I/Q data modifying factors.      
### 9.Edge Computing and Communication for Energy-Efficient Earth Surveillance with LEO Satellites  [ :arrow_down: ](https://arxiv.org/pdf/2111.09045.pdf)
>  Modern satellites deployed in low Earth orbit (LEO) accommodate processing payloads that can be exploited for edge computing. Furthermore, by implementing inter-satellite links, the LEO satellites in a constellation can route the data end-to-end (E2E). These capabilities can be exploited to greatly improve the current store-and-forward approaches in Earth surveillance systems. However, they give rise to an NP-hard problem of joint communication and edge computing resource management (RM). In this paper, we propose an algorithm that allows the satellites to select between computing the tasks at the edge or at a cloud server and to allocate an adequate power for communication. The overall objective is to minimize the energy consumption at the satellites while fulfilling specific service E2E latency constraints for the computing tasks. Experimental results show that our algorithm achieves energy savings of up to 18% when compared to the selected benchmarks with either 1) fixed edge computing decisions or 2) maximum power allocation.      
### 10.Image Super-Resolution Using T-Tetromino Pixels  [ :arrow_down: ](https://arxiv.org/pdf/2111.09013.pdf)
>  For modern high-resolution imaging sensors, pixel binning is performed in low-lighting conditions and in case high frame rates are required. To recover the original spatial resolution, single-image super-resolution techniques can be applied for upscaling. To achieve a higher image quality after upscaling, we propose a novel binning concept using tetromino-shaped pixels. In doing so, we investigate the reconstruction quality using tetromino pixels for the first time in literature. Instead of using different types of tetrominoes as proposed in the literature for a sensor layout, we show that using a small repeating cell consisting of only four T-tetrominoes is sufficient. For reconstruction, we use a locally fully connected reconstruction (LFCR) network as well as two classical reconstruction methods from the field of compressed sensing. Using the LFCR network in combination with the proposed tetromino layout, we achieve superior image quality in terms of PSNR, SSIM, and visually compared to conventional single-image super-resolution using the very deep super-resolution (VDSR) network. For the PSNR, a gain of up to +1.92 dB is achieved.      
### 11.A Generalized Proportionate-Type Normalized Subband Adaptive Filter  [ :arrow_down: ](https://arxiv.org/pdf/2111.08952.pdf)
>  We show that a new design criterion, i.e., the least squares on subband errors regularized by a weighted norm, can be used to generalize the proportionate-type normalized subband adaptive filtering (PtNSAF) framework. The new criterion directly penalizes subband errors and includes a sparsity penalty term which is minimized using the damped regularized Newton's method. The impact of the proposed generalized PtNSAF (GPtNSAF) is studied for the system identification problem via computer simulations. Specifically, we study the effects of using different numbers of subbands and various sparsity penalty terms for quasi-sparse, sparse, and dispersive systems. The results show that the benefit of increasing the number of subbands is larger than promoting sparsity of the estimated filter coefficients when the target system is quasi-sparse or dispersive. On the other hand, for sparse target systems, promoting sparsity becomes more important. More importantly, the two aspects provide complementary and additive benefits to the GPtNSAF for speeding up convergence.      
### 12.Survey on Symbiotic Radio: A Paradigm Shift in Spectrum Sharing and Coexistence  [ :arrow_down: ](https://arxiv.org/pdf/2111.08948.pdf)
>  Sixth-generation (6G) of mobile communication aims to connect this world digitally through green communication networks that provide secure, ubiquitous, and unlimited connectivity in an attempt to improve the overall quality of life.The driving force behind the development of these networks is the rapid evolution of Internet-of-Things (IoT), which has stimulated the proliferation of wireless applications in health,education, agriculture, utilities, etc. However, these applications are accompanied by the deployment of a massive number of IoT devices that require a significant radio spectrum for wireless connectivity. IoT devices usually have low data rate requirements and limited power provision but desirably a long life.Recently, the development of passive radio systems has opened new paradigms of spectrum sharing and coexistence. These systems utilize the radio resources and infrastructure of the active radio systems to perform their functionalities. By enabling the dependent coexistence, a new technology named symbiotic radio (SRad) enables the symbiotic relationships between the different radio systems ranging from mutual benefits or competition in terms of sharing the resources, in particular for IoT devices. This survey first provides the motivation for dependent coexistence and background of spectrum sharing through coexistence along with existing literature. Then, it describes the active and passive radio systems definition and a brief overview. Afterward, the history of symbiosis and the role of SRad technology in spectrum sharing and coexistence are defined while focusing on symbiotic communication. Lastly, we discuss research challenges, future directions, and applications scenarios.      
### 13.Distributed Dual Gradient Tracking for Economic Dispatch in Power Systems with Noisy Information  [ :arrow_down: ](https://arxiv.org/pdf/2111.08935.pdf)
>  Distributed algorithms can be efficiently used for solving economic dispatch problem (EDP) in power systems. To implement a distributed algorithm, a communication network is required, making the algorithm vulnerable to noise which may cause detrimental decisions or even instability. In this paper, we propose an agent-based method which enables a fully distributed solution of the EDP in power systems with noisy information exchange. Through the novel design of the gradient tracking update and introducing suppression parameters, the proposed algorithm can effectively alleviate the impact of noise and it is shown to be more robust than the existing distributed algorithms. The convergence of the algorithm is also established under standard assumptions. Moreover, a strategy are presented to accelerate our proposed algorithm. Finally, the algorithm is tested on several IEEE bus systems to demonstrate its effectiveness and scalability.      
### 14.Reconstruction-Computation-Quantization (RCQ): A Paradigm for Low Bit Width LDPC Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2111.08920.pdf)
>  This paper uses the reconstruction-computation-quantization (RCQ) paradigm to decode low-density parity-check (LDPC) codes. RCQ facilitates dynamic non-uniform quantization to achieve good frame error rate (FER) performance with very low message precision. For message-passing according to a flooding schedule, the RCQ parameters are designed by discrete density evolution (DDE). Simulation results on an IEEE 802.11 LDPC code show that for 4-bit messages, a flooding MinSum RCQ decoder outperforms table-lookup approaches such as information bottleneck (IB) or Min-IB decoding, with significantly fewer parameters to be stored. <br>Additionally, this paper introduces layer-specific RCQ (LS-RCQ), an extension of RCQ decoding for layered architectures. LS-RCQ uses layer-specific message representations to achieve the best possible FER performance. For LS-RCQ, this paper proposes using layered DDE featuring hierarchical dynamic quantization (HDQ) to design LS-RCQ parameters efficiently. <br>Finally, this paper studies field-programmable gate array (FPGA) implementations of RCQ decoders. Simulation results for a (9472, 8192) quasi-cyclic (QC) LDPC code show that a layered MinSum RCQ decoder with 3-bit messages achieves more than a $10\%$ reduction in LUTs and routed nets and more than a $6\%$ decrease in register usage while maintaining comparable decoding performance, compared to a 5-bit offset MinSum decoder.      
### 15.Adversarial Tradeoffs in Linear Inverse Problems and Robust StateEstimation  [ :arrow_down: ](https://arxiv.org/pdf/2111.08864.pdf)
>  Adversarially robust training has been shown to reduce the susceptibility of learned models to targeted input data perturbations. However, it has also been observed that such adversarially robust models suffer a degradation in accuracy when applied to unperturbed data sets, leading to a robustness-accuracy tradeoff. In this paper, we provide sharp and interpretable characterizations of such robustness-accuracy tradeoffs for linear inverse problems. In particular, we provide an algorithm to find the optimal adversarial perturbation given data, and develop tight upper and lower bounds on the adversarial loss in terms of the standard (non-adversarial) loss and the spectral properties of the resulting estimator. Further, motivated by the use of adversarial training in reinforcement learning, we define and analyze the \emph{adversarially robust Kalman Filtering problem.} We apply a refined version of our general theory to this problem, and provide the first characterization of robustness-accuracy tradeoffs in a setting where the data is generated by a dynamical system. In doing so, we show a natural connection between a filter's robustness to adversarial perturbation and underlying control theoretic properties of the system being observed, namely the spectral properties of its observability gramian.      
### 16.NNSynth: Neural Network Guided Abstraction-Based Controller Synthesis for Stochastic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.08853.pdf)
>  In the last decade, abstraction-based controller synthesis techniques have gained considerable attention due to their ability to synthesize correct-by-design feedback controllers from high-level specifications. Nevertheless, a significant drawback of these techniques is the need to explore large spaces of quantized state and input spaces to find a controller that satisfies the specification. On the contrary, recent advances in machine learning, in particular imitation learning and reinforcement learning, paved the way to several techniques to design controllers (or policies) for highly nonlinear systems with large state and input spaces, albeit their lack of rigorous correctness guarantees. This motivates the question of how to use machine learning techniques to guide the synthesis of abstraction-based controllers. In this paper, we introduce NNSynth, a novel tool for abstraction-based controller synthesis. Unique to NNSynth is the use of machine learning techniques to guide the search over the space of controllers to find candidate Neural Network (NN)-based controllers. Next, these NNs are "projected" and the control actions that are close to the NN outputs are used to construct a "local" abstraction for the system. An abstraction-based controller is then synthesized from such a "local" abstract model. If a controller that satisfies the specifications is not found, then the best found controller is "lifted" to a neural network controller for additional training. Our experiments show that this neural network-guided synthesis leads to more than 50x or even 100x speedup in high dimensional systems compared to the state-of-the-art.      
### 17.One hundred percent renewable energy generation in 2030 with the lowest cost commercially available power plants  [ :arrow_down: ](https://arxiv.org/pdf/2111.08829.pdf)
>  We hypothesize that the present expansion of energy generation by variable renewable energy (VRE) power plants, such as wind and photovoltaic power plants, leads to a 100% renewable energy supply in 2030 because of its inherent exponential growth function. This behavior is related to the exponential cost reduction of its generated energy and the nearly unconstrained available potential of its natural resources. The cost reduction results from the continuous improvements in development, research, manufacturing, and installation, also showing a growth of its installation power per power plant or aero generator. We prove that if the historic exponential growth is followed in the future, it is possible to decarbonize the world's electric energy systems' power supply in 2026. Furthermore, the global demand on primary energy can be supplied in 2030, which leads to the total suppression of CO2 emissions related to the energy need of humanity. Because of the related cost reduction, energy costs are not anymore relevant. Our extrapolation is based on the continuation of the historic growth functions of the globally installed PV and the wind power plants, and we also discuss the conditions necessary to enable a transition to such a 100% renewable energy production. Considering a non-constrained growth of VRE power plants' installation power, decarbonization related to energy generation and use can be accomplished in a much shorter time frame as previously scheduled. As a result, climate change mitigation, energy cost reduction, and high employment are attained much earlier than previously planned.      
### 18.A TCN-based Spatial-Temporal PV Forecasting Framework with Automated Detector Network Selection  [ :arrow_down: ](https://arxiv.org/pdf/2111.08809.pdf)
>  This paper proposes a two-stage PV forecasting framework for MW-level PV farms based on Temporal Convolutional Network (TCN). In the day-ahead stage, inverter-level physics-based model is built to convert Numerical Weather Prediction (NWP) to hourly power forecasts. TCN works as the NWP blender to merge different NWP sources to improve the forecasting accuracy. In the real-time stage, TCN can leverage the spatial-temporal correlations between the target site and its neighbors to achieve intra-hour power forecasts. A scenario-based correlation analysis method is proposed to automatically identify the most contributive neighbors. Simulation results based on 95 PV farms in North Carolina demonstrate the accuracy and efficiency of the proposed method.      
### 19.Adaptive Transmit Waveform Design  [ :arrow_down: ](https://arxiv.org/pdf/2111.08746.pdf)
>  Recent research efforts in the Anti-Submarine Warfare (ASW) community have focused on developing sonar systems that adapt to their acoustic environment, referred to as "cognitive" sonars. Cognitive active sonar systems utilize principles of the perception action cycle of cognition to leverage information gathered from earlier sensing interactions with the underwater acoustic environment. This in turn informs the selection of system parameters to optimize target detection, classification, localization, and tracking performance in that acoustic environment. Of the many system parameters such a cognitive sonar system could potentially adapt, the acoustic signal transmitted into the medium, also known as the transmit waveform, has a profound impact on system performance. Many of the physical characteristics of the acoustic environment are contained in the return echo signal that is composed of amplitude scaled (target strength), time-delayed (target range) and Doppler shifted (target range-rate) echoes of the transmit waveform. This paper briefly describes a spectrally compact adaptive FM waveform model using Multi-Tone Sinusoidal Frequency Modulation (MTSFM). The MTSFM waveform's frequency and phase modulation functions are composed of a finite set of weighted sinusoidal harmonics. The weights for each harmonic are utilized as a discrete set of design coefficients. Adjusting these coefficients results in constant amplitude, spectrally compact FM waveforms with unique characteristics. The adaptability of the MTSFM combined with its transmitter friendly properties make it an attractive waveform type for a variety of active sonar applications and may provide a cognitive sonar system the ability to generate a complementary set of finely tuned waveforms for the novel scenarios and environments that it may encounter.      
### 20.Formal Synthesis of Controllers for Uncertain Linear Systems against $Ï‰$-Regular Properties: A Set-based Approach  [ :arrow_down: ](https://arxiv.org/pdf/2111.08734.pdf)
>  In this paper, we present how to synthesize controllers to enforce $\omega$-regular properties over linear control systems affected by bounded disturbances. In particular, these controllers are synthesized based on so-called hybrid controlled invariant (HCI) sets. To compute these sets, we first construct a product system using the linear control system and the deterministic Rabin automata (DRA) modeling the negation of the desired property. Then, we compute the maximal HCI set over the state set of the product system by leveraging a set-based approach. To ensure termination of the computation of the HCI sets within a finite number of iterations, we also propose two iterative schemes to compute approximations of the maximal HCI set. Finally, we show the effectiveness of our approach on two case studies.      
### 21.Automatic Semantic Segmentation of the Lumbar Spine. Clinical Applicability in a Multi-parametric and Multi-centre MRI study  [ :arrow_down: ](https://arxiv.org/pdf/2111.08712.pdf)
>  One of the major difficulties in medical image segmentation is the high variability of these images, which is caused by their origin (multi-centre), the acquisition protocols (multi-parametric), as well as the variability of human anatomy, the severity of the illness, the effect of age and gender, among others. The problem addressed in this work is the automatic semantic segmentation of lumbar spine Magnetic Resonance images using convolutional neural networks. The purpose is to assign a classes label to each pixel of an image. Classes were defined by radiologists and correspond to different structural elements like vertebrae, intervertebral discs, nerves, blood vessels, and other tissues. The proposed network topologies are variants of the U-Net architecture. Several complementary blocks were used to define the variants: Three types of convolutional blocks, spatial attention models, deep supervision and multilevel feature extractor. This document describes the topologies and analyses the results of the neural network designs that obtained the most accurate segmentations. Several of the proposed designs outperform the standard U-Net used as baseline, especially when used in ensembles where the output of multiple neural networks is combined according to different strategies.      
### 22.Two-step adversarial debiasing with partial learning -- medical image case-studies  [ :arrow_down: ](https://arxiv.org/pdf/2111.08711.pdf)
>  The use of artificial intelligence (AI) in healthcare has become a very active research area in the last few years. While significant progress has been made in image classification tasks, only a few AI methods are actually being deployed in hospitals. A major hurdle in actively using clinical AI models currently is the trustworthiness of these models. More often than not, these complex models are black boxes in which promising results are generated. However, when scrutinized, these models begin to reveal implicit biases during the decision making, such as detecting race and having bias towards ethnic groups and subpopulations. In our ongoing study, we develop a two-step adversarial debiasing approach with partial learning that can reduce the racial disparity while preserving the performance of the targeted task. The methodology has been evaluated on two independent medical image case-studies - chest X-ray and mammograms, and showed promises in bias reduction while preserving the targeted performance.      
### 23.CNN Filter Learning from Drawn Markers for the Detection of Suggestive Signs of COVID-19 in CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2111.08710.pdf)
>  Early detection of COVID-19 is vital to control its spread. Deep learning methods have been presented to detect suggestive signs of COVID-19 from chest CT images. However, due to the novelty of the disease, annotated volumetric data are scarce. Here we propose a method that does not require either large annotated datasets or backpropagation to estimate the filters of a convolutional neural network (CNN). For a few CT images, the user draws markers at representative normal and abnormal regions. The method generates a feature extractor composed of a sequence of convolutional layers, whose kernels are specialized in enhancing regions similar to the marked ones, and the decision layer of our CNN is a support vector machine. As we have no control over the CT image acquisition, we also propose an intensity standardization approach. Our method can achieve mean accuracy and kappa values of $0.97$ and $0.93$, respectively, on a dataset with 117 CT images extracted from different sites, surpassing its counterpart in all scenarios.      
### 24.Exploring dual-attention mechanism with multi-scale feature extraction scheme for skin lesion segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2111.08708.pdf)
>  Automatic segmentation of skin lesions from dermoscopic images is a challenging task due to the irregular lesion boundaries, poor contrast between the lesion and the background, and the presence of artifacts. In this work, a new convolutional neural network-based approach is proposed for skin lesion segmentation. In this work, a novel multi-scale feature extraction module is proposed for extracting more discriminative features for dealing with the challenges related to complex skin lesions; this module is embedded in the UNet, replacing the convolutional layers in the standard architecture. Further in this work, two different attention mechanisms refine the feature extracted by the encoder and the post-upsampled features. This work was evaluated using the two publicly available datasets, including ISBI2017 and ISIC2018 datasets. The proposed method reported an accuracy, recall, and JSI of 97.5%, 94.29%, 91.16% on the ISBI2017 dataset and 95.92%, 95.37%, 91.52% on the ISIC2018 dataset. It outperformed the existing methods and the top-ranked models in the respective competitions.      
### 25.A Hierarchical Multi-Task Approach to Gastrointestinal Image Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2111.08707.pdf)
>  A large number of different lesions and pathologies can affect the human digestive system, resulting in life-threatening situations. Early detection plays a relevant role in the successful treatment and the increase of current survival rates to, e.g., colorectal cancer. The standard procedure enabling detection, endoscopic video analysis, generates large quantities of visual data that need to be carefully analyzed by an specialist. Due to the wide range of color, shape, and general visual appearance of pathologies, as well as highly varying image quality, such process is greatly dependent on the human operator experience and skill. In this work, we detail our solution to the task of multi-category classification of images from the gastrointestinal (GI) human tract within the 2020 Endotect Challenge. Our approach is based on a Convolutional Neural Network minimizing a hierarchical error function that takes into account not only the finding category, but also its location within the GI tract (lower/upper tract), and the type of finding (pathological finding/therapeutic intervention/anatomical landmark/mucosal views' quality). We also describe in this paper our solution for the challenge task of polyp segmentation in colonoscopies, which was addressed with a pretrained double encoder-decoder network. Our internal cross-validation results show an average performance of 91.25 Mathews Correlation Coefficient (MCC) and 91.82 Micro-F1 score for the classification task, and a 92.30 F1 score for the polyp segmentation task. The organization provided feedback on the performance in a hidden test set for both tasks, which resulted in 85.61 MCC and 86.96 F1 score for classification, and 91.97 F1 score for polyp segmentation. At the time of writing no public ranking for this challenge had been released.      
### 26.Automated Atlas-based Segmentation of Single Coronal Mouse Brain Slices using Linear 2D-2D Registration  [ :arrow_down: ](https://arxiv.org/pdf/2111.08705.pdf)
>  A significant challenge for brain histological data analysis is to precisely identify anatomical regions in order to perform accurate local quantifications and evaluate therapeutic solutions. Usually, this task is performed manually, becoming therefore tedious and subjective. Another option is to use automatic or semi-automatic methods, among which segmentation using digital atlases co-registration. However, most available atlases are 3D, whereas digitized histological data are 2D. Methods to perform such 2D-3D segmentation from an atlas are required. This paper proposes a strategy to automatically and accurately segment single 2D coronal slices within a 3D volume of atlas, using linear registration. We validated its robustness and performance using an exploratory approach at whole-brain scale.      
### 27.Interpretability Aware Model Training to Improve Robustness against Out-of-Distribution Magnetic Resonance Images in Alzheimer's Disease Classification  [ :arrow_down: ](https://arxiv.org/pdf/2111.08701.pdf)
>  Owing to its pristine soft-tissue contrast and high resolution, structural magnetic resonance imaging (MRI) is widely applied in neurology, making it a valuable data source for image-based machine learning (ML) and deep learning applications. The physical nature of MRI acquisition and reconstruction, however, causes variations in image intensity, resolution, and signal-to-noise ratio. Since ML models are sensitive to such variations, performance on out-of-distribution data, which is inherent to the setting of a deployed healthcare ML application, typically drops below acceptable levels. We propose an interpretability aware adversarial training regime to improve robustness against out-of-distribution samples originating from different MRI hardware. The approach is applied to 1.5T and 3T MRIs obtained from the Alzheimer's Disease Neuroimaging Initiative database. We present preliminary results showing promising performance on out-of-distribution samples.      
### 28.XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale  [ :arrow_down: ](https://arxiv.org/pdf/2111.09296.pdf)
>  This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.      
### 29.Fast BATLLNN: Fast Box Analysis of Two-Level Lattice Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.09293.pdf)
>  In this paper, we present the tool Fast Box Analysis of Two-Level Lattice Neural Networks (Fast BATLLNN) as a fast verifier of box-like output constraints for Two-Level Lattice (TLL) Neural Networks (NNs). In particular, Fast BATLLNN can verify whether the output of a given TLL NN always lies within a specified hyper-rectangle whenever its input constrained to a specified convex polytope (not necessarily a hyper-rectangle). Fast BATLLNN uses the unique semantics of the TLL architecture and the decoupled nature of box-like output constraints to dramatically improve verification performance relative to known polynomial-time verification algorithms for TLLs with generic polytopic output constraints. In this paper, we evaluate the performance and scalability of Fast BATLLNN, both in its own right and compared to state-of-the-art NN verifiers applied to TLL NNs. Fast BATLLNN compares very favorably to even the fastest NN verifiers, completing our synthetic TLL test bench more than 400x faster than its nearest competitor.      
### 30.A fast solver for the pseudo-two-dimensional model of lithium-ion batteries  [ :arrow_down: ](https://arxiv.org/pdf/2111.09251.pdf)
>  The pseudo-two-dimensional (P2D) model is a complex mathematical model that can capture the electrochemical processes in Li-ion batteries. However, the model also brings a heavy computational burden. Many simplifications to the model have been introduced in the literature to reduce the complexity. We present a method for fast computation of the P2D model which can be used when simplifications are not accurate enough. By rearranging the calculations, we reduce the complexity of the linear algebra problem. We also employ automatic differentiation, using an open source package JAX for robustness, while also allowing easy implementation of changes to coefficient expressions. The method alleviates the computational bottleneck in P2D models without compromising accuracy.      
### 31.Secure Federated Learning for Residential Short Term Load Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2111.09248.pdf)
>  The inclusion of intermittent and renewable energy sources has increased the importance of demand forecasting in power systems. Smart meters can play a critical role in demand forecasting due to the measurement granularity they provide. Consumers' privacy concerns, reluctance of utilities and vendors to share data with competitors or third parties, and regulatory constraints are some constraints smart meter forecasting faces. This paper examines a collaborative machine learning method for short-term demand forecasting using smart meter data as a solution to the previous constraints. Privacy preserving techniques and federated learning enable to ensure consumers' confidentiality concerning both, their data, the models generated using it (Differential Privacy), and the communication mean (Secure Aggregation). The methods evaluated take into account several scenarios that explore how traditional centralized approaches could be projected in the direction of a decentralized, collaborative and private system. The results obtained over the evaluations provided almost perfect privacy budgets (1.39,$10e^{-5}$) and (2.01,$10e^{-5}$) with a negligible performance compromise.      
### 32.Optimal-Horizon Model-Predictive Control with Differential Dynamic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2111.09207.pdf)
>  We present an algorithm, based on the Differential Dynamic Programming framework, to handle trajectory optimization problems in which the horizon is determined online rather than fixed a priori. This algorithm exhibits exact one-step convergence for linear, quadratic, time-invariant problems and is fast enough for real-time nonlinear model-predictive control. We show derivations for the nonlinear algorithm in the discrete-time case, and apply this algorithm to a variety of nonlinear problems. Finally, we show the efficacy of the optimal-horizon model-predictive control scheme compared to a standard MPC controller, on an obstacle-avoidance problem with planar robots.      
### 33.Execution Order Matters in Greedy Algorithms with Limited Information  [ :arrow_down: ](https://arxiv.org/pdf/2111.09154.pdf)
>  In this work, we study the multi-agent decision problem where agents try to coordinate to optimize a given system-level objective. While solving for the global optimal is intractable in many cases, the greedy algorithm is a well-studied and efficient way to provide good approximate solutions - notably for submodular optimization problems. Executing the greedy algorithm requires the agents to be ordered and execute a local optimization based on the solutions of the previous agents. However, in limited information settings, passing the solution from the previous agents may be nontrivial, as some agents may not be able to directly communicate with each other. Thus the communication time required to execute the greedy algorithm is closely tied to the order that the agents are given. In this work, we characterize interplay between the communication complexity and agent orderings by showing that the complexity using the best ordering is O(n) and increases considerably to O(n^2) when using the worst ordering. Motivated by this, we also propose an algorithm that can find an ordering and execute the greedy algorithm quickly, in a distributed fashion. We also show that such an execution of the greedy algorithm is advantageous over current methods for distributed submodular maximization.      
### 34.Rapping-Singing Voice Synthesis based on Phoneme-level Prosody Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.09146.pdf)
>  In this paper, a text-to-rapping/singing system is introduced, which can be adapted to any speaker's voice. It utilizes a Tacotron-based multispeaker acoustic model trained on read-only speech data and which provides prosody control at the phoneme level. Dataset augmentation and additional prosody manipulation based on traditional DSP algorithms are also investigated. The neural TTS model is fine-tuned to an unseen speaker's limited recordings, allowing rapping/singing synthesis with the target's speaker voice. The detailed pipeline of the system is described, which includes the extraction of the target pitch and duration values from an a capella song and their conversion into target speaker's valid range of notes before synthesis. An additional stage of prosodic manipulation of the output via WSOLA is also investigated for better matching the target duration values. The synthesized utterances can be mixed with an instrumental accompaniment track to produce a complete song. The proposed system is evaluated via subjective listening tests as well as in comparison to an available alternate system which also aims to produce synthetic singing voice from read-only training data. Results show that the proposed approach can produce high quality rapping/singing voice with increased naturalness.      
### 35.The Neural Correlates of Image Texture in the Human Vision Using Magnetoencephalography  [ :arrow_down: ](https://arxiv.org/pdf/2111.09118.pdf)
>  Undoubtedly, textural property of an image is one of the most important features in object recognition task in both human and computer vision applications. Here, we investigated the neural signatures of four well-known statistical texture features including contrast, homogeneity, energy, and correlation computed from the gray level co-occurrence matrix (GLCM) of the images viewed by the participants in the process of magnetoencephalography (MEG) data collection. To trace these features in the human visual system, we used multivariate pattern analysis (MVPA) and trained a linear support vector machine (SVM) classifier on every timepoint of MEG data representing the brain activity and compared it with the textural descriptors of images using the Spearman correlation. The result of this study demonstrates that hierarchical structure in the processing of these four texture descriptors in the human brain with the order of contrast, homogeneity, energy, and correlation. Additionally, we found that energy, which carries broad texture property of the images, shows a more sustained statistically meaningful correlation with the brain activity in the course of time.      
### 36.Trajectory Prediction &amp; Path Planning for an Object Intercepting UAV with a Mounted Depth Camera  [ :arrow_down: ](https://arxiv.org/pdf/2111.09083.pdf)
>  A novel control &amp; software architecture using ROS C++ is introduced for object interception by a UAV with a mounted depth camera and no external aid. Existing work in trajectory prediction focused on the use of off-board tools like motion capture rooms to intercept thrown objects. The present study designs the UAV architecture to be completely on-board capable of object interception with the use of a depth camera and point cloud processing. The architecture uses an iterative trajectory prediction algorithm for non-propelled objects like a ping-pong ball. A variety of path planning approaches to object interception and their corresponding scenarios are discussed, evaluated &amp; simulated in Gazebo. The successful simulations exemplify the potential of using the proposed architecture for the on-board autonomy of UAVs intercepting objects.      
### 37.Cross-lingual Low Resource Speaker Adaptation Using Phonological Features  [ :arrow_down: ](https://arxiv.org/pdf/2111.09075.pdf)
>  The idea of using phonological features instead of phonemes as input to sequence-to-sequence TTS has been recently proposed for zero-shot multilingual speech synthesis. This approach is useful for code-switching, as it facilitates the seamless uttering of foreign text embedded in a stream of native text. In our work, we train a language-agnostic multispeaker model conditioned on a set of phonologically derived features common across different languages, with the goal of achieving cross-lingual speaker adaptation. We first experiment with the effect of language phonological similarity on cross-lingual TTS of several source-target language combinations. Subsequently, we fine-tune the model with very limited data of a new speaker's voice in either a seen or an unseen language, and achieve synthetic speech of equal quality, while preserving the target speaker's identity. With as few as 32 and 8 utterances of target speaker data, we obtain high speaker similarity scores and naturalness comparable to the corresponding literature. In the extreme case of only 2 available adaptation utterances, we find that our model behaves as a few-shot learner, as the performance is similar in both the seen and unseen adaptation language scenarios.      
### 38.High Quality Streaming Speech Synthesis with Low, Sentence-Length-Independent Latency  [ :arrow_down: ](https://arxiv.org/pdf/2111.09052.pdf)
>  This paper presents an end-to-end text-to-speech system with low latency on a CPU, suitable for real-time applications. The system is composed of an autoregressive attention-based sequence-to-sequence acoustic model and the LPCNet vocoder for waveform generation. An acoustic model architecture that adopts modules from both the Tacotron 1 and 2 models is proposed, while stability is ensured by using a recently proposed purely location-based attention mechanism, suitable for arbitrary sentence length generation. During inference, the decoder is unrolled and acoustic feature generation is performed in a streaming manner, allowing for a nearly constant latency which is independent from the sentence length. Experimental results show that the acoustic model can produce feature sequences with minimal latency about 31 times faster than real-time on a computer CPU and 6.5 times on a mobile CPU, enabling it to meet the conditions required for real-time applications on both devices. The full end-to-end system can generate almost natural quality speech, which is verified by listening tests.      
### 39.Subject Enveloped Deep Sample Fuzzy Ensemble Learning Algorithm of Parkinson's Speech Data  [ :arrow_down: ](https://arxiv.org/pdf/2111.09014.pdf)
>  Parkinson disease (PD)'s speech recognition is an effective way for its diagnosis, which has become a hot and difficult research area in recent years. As we know, there are large corpuses (segments) within one subject. However, too large segments will increase the complexity of the classification model. Besides, the clinicians interested in finding diagnostic speech markers that reflect the pathology of the whole subject. Since the optimal relevant features of each speech sample segment are different, it is difficult to find the uniform diagnostic speech markers. Therefore, it is necessary to reconstruct the existing large segments within one subject into few segments even one segment within one subject, which can facilitate the extraction of relevant speech features to characterize diagnostic markers for the whole subject. To address this problem, an enveloped deep speech sample learning algorithm for Parkinson's subjects based on multilayer fuzzy c-mean (MlFCM) clustering and interlayer consistency preservation is proposed in this paper. The algorithm can be used to achieve intra-subject sample reconstruction for Parkinson's disease (PD) to obtain a small number of high-quality prototype sample segments. At the end of the paper, several representative PD speech datasets are selected and compared with the state-of-the-art related methods, respectively. The experimental results show that the proposed algorithm is effective signifcantly.      
### 40.LVAC: Learned Volumetric Attribute Compression for Point Clouds using Coordinate Based Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.08988.pdf)
>  We consider the attributes of a point cloud as samples of a vector-valued volumetric function at discrete positions. To compress the attributes given the positions, we compress the parameters of the volumetric function. We model the volumetric function by tiling space into blocks, and representing the function over each block by shifts of a coordinate-based, or implicit, neural network. Inputs to the network include both spatial coordinates and a latent vector per block. We represent the latent vectors using coefficients of the region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based point cloud codec G-PCC. The coefficients, which are highly compressible, are rate-distortion optimized by back-propagation through a rate-distortion Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT by 2--4 dB. This is the first work to compress volumetric functions represented by local coordinate-based neural networks. As such, we expect it to be applicable beyond point clouds, for example to compression of high-resolution neural radiance fields.      
### 41.Generating Unrestricted 3D Adversarial Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2111.08973.pdf)
>  Utilizing 3D point cloud data has become an urgent need for the deployment of artificial intelligence in many areas like facial recognition and self-driving. However, deep learning for 3D point clouds is still vulnerable to adversarial attacks, e.g., iterative attacks, point transformation attacks, and generative attacks. These attacks need to restrict perturbations of adversarial examples within a strict bound, leading to the unrealistic adversarial 3D point clouds. In this paper, we propose an Adversarial Graph-Convolutional Generative Adversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D point clouds from scratch. Specifically, we use a graph convolutional generator and a discriminator with an auxiliary classifier to generate realistic point clouds, which learn the latent distribution from the real 3D data. The unrestricted adversarial attack loss is incorporated in the special adversarial training of GAN, which enables the generator to generate the adversarial examples to spoof the target network. Compared with the existing state-of-art attack methods, the experiment results demonstrate the effectiveness of our unrestricted adversarial attack methods with a higher attack success rate and visual quality. Additionally, the proposed AdvGCGAN can achieve better performance against defense models and better transferability than existing attack methods with strong camouflage.      
### 42.Edge Computing in IoT: A 6G Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2111.08943.pdf)
>  Edge computing is one of the key driving forces to enable Beyond 5G (B5G) and 6G networks. Due to the unprecedented increase in traffic volumes and computation demands of future networks, Multi-access Edge Computing (MEC) is considered as a promising solution to provide cloud-computing capabilities within the radio access network (RAN) closer to the end users. There has been a huge amount of research on MEC and its potential applications; however, very little has been said about the key factors of MEC deployment to meet the diverse demands of future applications. In this article, we present key considerations for edge deployments in B5G/6G networks including edge architecture, server location and capacity, user density, security etc. We further provide state-of-the-art edge-centric services in future B5G/6G networks. Lastly, we present some interesting insights and open research problems in edge computing for 6G networks.      
### 43.Local Texture Estimator for Implicit Representation Function  [ :arrow_down: ](https://arxiv.org/pdf/2111.08918.pdf)
>  Recent works with an implicit neural function shed light on representing images in arbitrary resolution. However, a standalone multi-layer perceptron (MLP) shows limited performance in learning high-frequency components. In this paper, we propose a Local Texture Estimator (LTE), a dominant-frequency estimator for natural images, enabling an implicit function to capture fine details while reconstructing images in a continuous manner. When jointly trained with a deep super-resolution (SR) architecture, LTE is capable of characterizing image textures in 2D Fourier space. We show that an LTE-based neural function outperforms existing deep SR methods within an arbitrary-scale for all datasets and all scale factors. Furthermore, we demonstrate that our implementation takes the shortest running time compared to previous works. Source code will be open.      
### 44.Information Fusion in Attention Networks Using Adaptive and Multi-level Factorized Bilinear Pooling for Audio-visual Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2111.08910.pdf)
>  Multimodal emotion recognition is a challenging task in emotion computing as it is quite difficult to extract discriminative features to identify the subtle differences in human emotions with abstract concept and multiple expressions. Moreover, how to fully utilize both audio and visual information is still an open problem. In this paper, we propose a novel multimodal fusion attention network for audio-visual emotion recognition based on adaptive and multi-level factorized bilinear pooling (FBP). First, for the audio stream, a fully convolutional network (FCN) equipped with 1-D attention mechanism and local response normalization is designed for speech emotion recognition. Next, a global FBP (G-FBP) approach is presented to perform audio-visual information fusion by integrating selfattention based video stream with the proposed audio stream. To improve G-FBP, an adaptive strategy (AG-FBP) to dynamically calculate the fusion weight of two modalities is devised based on the emotion-related representation vectors from the attention mechanism of respective modalities. Finally, to fully utilize the local emotion information, adaptive and multi-level FBP (AMFBP) is introduced by combining both global-trunk and intratrunk data in one recording on top of AG-FBP. Tested on the IEMOCAP corpus for speech emotion recognition with only audio stream, the new FCN method outperforms the state-ofthe-art results with an accuracy of 71.40%. Moreover, validated on the AFEW database of EmotiW2019 sub-challenge and the IEMOCAP corpus for audio-visual emotion recognition, the proposed AM-FBP approach achieves the best accuracy of 63.09% and 75.49% respectively on the test set.      
### 45.A Continuous-Time Optimal Control Approach to Congestion Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.08908.pdf)
>  Traffic congestion has become a nightmare to modern life in metropolitan cities. On average, a driver spending X hours a year stuck in traffic is one of most common sentences we often read regarding traffic congestion. Our aim in this article is to provide a method to control this seemingly ever-growing problem of traffic congestion. We model traffic dynamics using a continuous-time mass-flow conservation law, and apply optimal control techniques to control traffic congestion. First, we apply the mass-flow conservation law to specify traffic feasibility and present continuous-time dynamics for modeling traffic as a network problem by defining a network of interconnected roads (NOIR). The traffic congestion control is formulated as a boundary control problem and we use the concept of statetransition matrix to help with the optimization of boundary flow by solving a constrained optimal control problem using quadratic programming. Finally, we show that the proposed algorithm is successful by simulating on a NOIR.      
### 46.SEIHAI: A Sample-efficient Hierarchical AI for the MineRL Competition  [ :arrow_down: ](https://arxiv.org/pdf/2111.08857.pdf)
>  The MineRL competition is designed for the development of reinforcement learning and imitation learning algorithms that can efficiently leverage human demonstrations to drastically reduce the number of environment interactions needed to solve the complex \emph{ObtainDiamond} task with sparse rewards. To address the challenge, in this paper, we present \textbf{SEIHAI}, a \textbf{S}ample-\textbf{e}ff\textbf{i}cient \textbf{H}ierarchical \textbf{AI}, that fully takes advantage of the human demonstrations and the task structure. Specifically, we split the task into several sequentially dependent subtasks, and train a suitable agent for each subtask using reinforcement learning and imitation learning. We further design a scheduler to select different agents for different subtasks automatically. SEIHAI takes the first place in the preliminary and final of the NeurIPS-2020 MineRL competition.      
### 47.Privacy Guarantees of BLE Contact Tracing: A Case Study on COVIDWISE  [ :arrow_down: ](https://arxiv.org/pdf/2111.08842.pdf)
>  Google and Apple jointly introduced a digital contact tracing technology and an API called "exposure notification," to help health organizations and governments with contact tracing. The technology and its interplay with security and privacy constraints require investigation. In this study, we examine and analyze the security, privacy, and reliability of the technology with actual and typical scenarios (and expected typical adversary in mind), and quite realistic use cases. We do it in the context of Virginia's COVIDWISE app. This experimental analysis validates the properties of the system under the above conditions, a result that seems crucial for the peace of mind of the exposure notification technology adopting authorities, and may also help with the system's transparency and overall user trust.      
### 48.Zero-shot Singing Technique Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2111.08839.pdf)
>  In this paper we propose modifications to the neural network framework, AutoVC for the task of singing technique conversion. This includes utilising a pretrained singing technique encoder which extracts technique information, upon which a decoder is conditioned during training. By swapping out a source singer's technique information for that of the target's during conversion, the input spectrogram is reconstructed with the target's technique. We document the beneficial effects of omitting the latent loss, the importance of sequential training, and our process for fine-tuning the bottleneck. We also conducted a listening study where participants rate the specificity of technique-converted voices as well as their naturalness. From this we are able to conclude how effective the technique conversions are and how different conditions affect them, while assessing the model's ability to reconstruct its input data.      
### 49.Federated Learning for Smart Healthcare: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2111.08834.pdf)
>  Recent advances in communication technologies and Internet-of-Medical-Things have transformed smart healthcare enabled by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may be infeasible in realistic healthcare scenarios due to the high scalability of modern healthcare networks and growing data privacy concerns. Federated Learning (FL), as an emerging distributed collaborative AI paradigm, is particularly attractive for smart healthcare, by coordinating multiple clients (e.g., hospitals) to perform AI training without sharing raw data. Accordingly, we provide a comprehensive survey on the use of FL in smart healthcare. First, we present the recent advances in FL, the motivations, and the requirements of using FL in smart healthcare. The recent FL designs for smart healthcare are then discussed, ranging from resource-aware FL, secure and privacy-aware FL to incentive FL and personalized FL. Subsequently, we provide a state-of-the-art review on the emerging applications of FL in key healthcare domains, including health data management, remote health monitoring, medical imaging, and COVID-19 detection. Several recent FL-based smart healthcare projects are analyzed, and the key lessons learned from the survey are also highlighted. Finally, we discuss interesting research challenges and possible directions for future FL research in smart healthcare.      
### 50.A Projection Operator-based Newton Method for the Trajectory Optimization of Closed Quantum Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.08795.pdf)
>  Quantum optimal control is an important technology that enables fast state preparation and gate design. In the absence of an analytic solution, most quantum optimal control methods rely on an iterative scheme to update the solution estimate. At present, the convergence rate of existing solvers is at most superlinear. This paper develops a new general purpose solver for quantum optimal control based on the PRojection Operator Newton method for Trajectory Optimization, or PRONTO. Specifically, the proposed approach uses a projection operator to incorporate the SchrÃ¶dinger equation directly into the cost function, which is then minimized using a quasi-Newton method. At each iteration, the descent direction is obtained by computing the analytic solution to a Linear-Quadratic trajectory optimization problem. The resulting method guarantees monotonic convergence at every iteration and quadratic convergence in proximity of the solution. To highlight the potential of PRONTO, we present an numerical example that employs it to solve the optimal state-to-state mapping problem for a qubit and compares its performance to a state-of-the-art quadratic optimal control method.      
### 51.Stronger Generalization Guarantees for Robot Learning by Combining Generative Models and Real-World Data  [ :arrow_down: ](https://arxiv.org/pdf/2111.08761.pdf)
>  We are motivated by the problem of learning policies for robotic systems with rich sensory inputs (e.g., vision) in a manner that allows us to guarantee generalization to environments unseen during training. We provide a framework for providing such generalization guarantees by leveraging a finite dataset of real-world environments in combination with a (potentially inaccurate) generative model of environments. The key idea behind our approach is to utilize the generative model in order to implicitly specify a prior over policies. This prior is updated using the real-world dataset of environments by minimizing an upper bound on the expected cost across novel environments derived via Probably Approximately Correct (PAC)-Bayes generalization theory. We demonstrate our approach on two simulated systems with nonlinear/hybrid dynamics and rich sensing modalities: (i) quadrotor navigation with an onboard vision sensor, and (ii) grasping objects using a depth sensor. Comparisons with prior work demonstrate the ability of our approach to obtain stronger generalization guarantees by utilizing generative models. We also present hardware experiments for validating our bounds for the grasping task.      
### 52.Achieving Short-Blocklength RCU bound via CRC List Decoding of TCM with Probabilistic Shaping  [ :arrow_down: ](https://arxiv.org/pdf/2111.08756.pdf)
>  This paper applies probabilistic amplitude shaping (PAS) to a cyclic redundancy check (CRC) aided trellis coded modulation (TCM) to achieve the short-blocklength random coding union (RCU) bound. In the transmitter, the equally likely message bits are first encoded by distribution matcher to generate amplitude symbols with the desired distribution. The binary representations of the distribution matcher outputs are then encoded by a CRC. Finally, the CRC-encoded bits are encoded and modulated by Ungerboeck's TCM scheme, which consists of a $\frac{k_0}{k_0+1}$ systematic tail-biting convolutional code and a mapping function that maps coded bits to channel signals with capacity-achieving distribution. This paper proves that, for the proposed transmitter, the CRC bits have uniform distribution and that the channel signals have symmetric distribution. In the receiver, the serial list Viterbi decoding (S-LVD) is used to estimate the information bits. Simulation results show that, for the proposed CRC-TCM-PAS system with 87 input bits and 65-67 8-AM coded output symbols, the decoding performance under additive white Gaussian noise channel achieves the RCU bound with properly designed CRC and convolutional codes.      
### 53.Learning Provably Robust Motion Planners Using Funnel Libraries  [ :arrow_down: ](https://arxiv.org/pdf/2111.08733.pdf)
>  This paper presents an approach for learning motion planners that are accompanied with probabilistic guarantees of success on new environments that hold uniformly for any disturbance to the robot's dynamics within an admissible set. We achieve this by bringing together tools from generalization theory and robust control. First, we curate a library of motion primitives where the robustness of each primitive is characterized by an over-approximation of the forward reachable set, i.e., a "funnel". Then, we optimize probably approximately correct (PAC)-Bayes generalization bounds for training our planner to compose these primitives such that the entire funnels respect the problem specification. We demonstrate the ability of our approach to provide strong guarantees on two simulated examples: (i) navigation of an autonomous vehicle under external disturbances on a five-lane highway with multiple vehicles, and (ii) navigation of a drone across an obstacle field in the presence of wind disturbances.      
