# ArXiv eess --Fri, 12 Nov 2021
### 1.Robust Moving Target Defence Against False Data Injection Attacks in Power Grids  [ :arrow_down: ](https://arxiv.org/pdf/2111.06346.pdf)
>  Recently, moving target defence (MTD) has been proposed to thwart the false data injection (FDI) attacks in power system state estimation by proactively triggering the distributed flexible AC transmission system (D-FACTS) devices. One of the key challenges for MTD in power grid is to design its real-time implementation with performance guarantees against unknown attacks. To tackle this, a novel robust MTD strategy is proposed to guarantee the worst-case detection rate against all unknown attacks in noisy environment. We first theoretically prove that, for any given MTD strategy, the minimal principal angle between subspaces corresponds to the worst-case performance against all potential attacks. Based on this, robust MTD algorithms are then formulated for the systems with both complete and incomplete configurations. In addition, this paper proposes the concept of robust hidden MTD under noisy environment, which is shown to alleviate the contradiction between the effectiveness and the hiddenness of MTD. Extensive simulations using standard IEEE benchmarks demonstrate the improved average and worst-case performances of MTD by using the proposed algorithms.      
### 2.Joint Radar-Communications Processing from a Dual-Blind Deconvolution Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2111.06304.pdf)
>  We consider a general spectral coexistence scenario, wherein the channels and transmit signals of both radar and communications systems are unknown at the receiver. In this \textit{dual-blind deconvolution} (DBD) problem, a common receiver admits the multi-carrier wireless communications signal that is overlaid with the radar signal reflected-off multiple targets. When the radar receiver is not collocated with the transmitter, such as in passive or multistatic radars, the transmitted signal is also unknown apart from the target parameters. Similarly, apart from the transmitted messages, the communications channel may also be unknown in dynamic environments such as vehicular networks. As a result, the estimation of unknown target and communications parameters in a DBD scenario is highly challenging. In this work, we exploit the sparsity of the channel to solve DBD by casting it as an atomic norm minimization problem. Our theoretical analyses and numerical experiments demonstrate perfect recovery of continuous-valued range-time and Doppler velocities of multiple targets as well as delay-Doppler communications channel parameters using uniformly-spaced time samples in the dual-blind receiver.      
### 3.Related Work on Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2111.06291.pdf)
>  Due to the existence of quality degradations introduced in various stages of visual signal acquisition, compression, transmission and display, image quality assessment (IQA) plays a vital role in image-based applications. According to whether the reference image is complete and available, image quality evaluation can be divided into three categories: Full-Reference(FR), Reduced- Reference(RR), and Non- Reference(NR). This article will review the state-of-the-art image quality assessment algorithms.      
### 4.Detecting COVID-19 from Chest Computed Tomography Scans using AI-Driven Android Application  [ :arrow_down: ](https://arxiv.org/pdf/2111.06254.pdf)
>  The COVID-19 (coronavirus disease 2019) pandemic affected more than 186 million people with over 4 million deaths worldwide by June 2021. The magnitude of which has strained global healthcare systems. Chest Computed Tomography (CT) scans have a potential role in the diagnosis and prognostication of COVID-19. Designing a diagnostic system which is cost-efficient and convenient to operate on resource-constrained devices like mobile phones would enhance the clinical usage of chest CT scans and provide swift, mobile, and accessible diagnostic capabilities. This work proposes developing a novel Android application that detects COVID-19 infection from chest CT scans using a highly efficient and accurate deep learning algorithm. It further creates an attention heatmap, augmented on the segmented lung parenchyma region in the CT scans through an algorithm developed as a part of this work, which shows the regions of infection in the lungs. We propose a selection approach combined with multi-threading for a faster generation of heatmaps on Android Device, which reduces the processing time by about 93%. The neural network trained to detect COVID-19 in this work is tested with F1 score and accuracy, both of 99.58% and sensitivity of 99.69%, which is better than most of the results in the domain of COVID diagnosis from CT scans. This work will be beneficial in high volume practices and help doctors triage patients in the early diagnosis of the COVID-19 quickly and efficiently.      
### 5.Model-Based Reinforcement Learning for Stochastic Hybrid Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.06211.pdf)
>  Optimal control of general nonlinear systems is a central challenge in automation. Data-driven approaches to control, enabled by powerful function approximators, have recently had great success in tackling challenging robotic applications. However, such methods often obscure the structure of dynamics and control behind black-box over-parameterized representations, thus limiting our ability to understand the closed-loop behavior. This paper adopts a hybrid-system view of nonlinear modeling and control that lends an explicit hierarchical structure to the problem and breaks down complex dynamics into simpler localized units. Therefore, we consider a sequence modeling paradigm that captures the temporal structure of the data and derive an expecation-maximization (EM) algorithm that automatically decomposes nonlinear dynamics into stochastic piecewise affine dynamical systems with nonlinear boundaries. Furthermore, we show that these time-series models naturally admit a closed-loop extension that we use to extract locally linear or polynomial feedback controllers from nonlinear experts via imitation learning. Finally, we introduce a novel hybrid realtive entropy policy search (Hb-REPS) technique that incorporates the hierarchical nature of hybrid systems and optimizes a set of time-invariant local feedback controllers derived from a locally polynomial approximation of a global value function.      
### 6.User Activity Detection for Irregular Repetition Slotted Aloha based MMTC  [ :arrow_down: ](https://arxiv.org/pdf/2111.06140.pdf)
>  Irregular repetition slotted aloha (IRSA) is a grant-free random access protocol for massive machine-type communications, where a large number of users sporadically send their data packets to a base station (BS). IRSA is a completely distributed multiple access protocol: in any given frame, a small subset of the users, i.e., the active users, transmit replicas of their packet in randomly selected resource elements (REs). The first step in the decoding process at the BS is to detect which users are active in each frame. To this end, a novel Bayesian user activity detection (UAD) algorithm is developed, which exploits both the sparsity in user activity as well as the underlying structure of IRSA-based transmissions. Next, the Cramer-Rao bound (CRB) on the mean squared error in channel estimation is derived. It is empirically shown that the channel estimates obtained as a by-product of the proposed UAD algorithm achieves the CRB. Then, the signal to interference plus noise ratio achieved by the users is analyzed, accounting for UAD, channel estimation errors, and pilot contamination. The impact of these non-idealities on the throughput of IRSA is illustrated via Monte Carlo simulations. For example, in a system with 1500 users and 10% of the users being active per frame, a pilot length of as low as 20 symbols is sufficient for accurate user activity detection. In contrast, using classical compressed sensing approaches for UAD would require a pilot length of about 346 symbols. The results also reveal crucial insights into dependence of UAD errors and throughput on parameters such as the length of the pilot sequence, the number of antennas at the BS, the number of users, and the signal to noise ratio.      
### 7.System Parameter Exploration of Ship Maneuvering Model for Automatic Docking / Berthing using CMA-ES  [ :arrow_down: ](https://arxiv.org/pdf/2111.06124.pdf)
>  Accurate maneuvering estimation is essential to establish autonomous berthing control. The system-based mathematical model is widely used to estimate the ship's maneuver. Commonly, the system parameters of the mathematical model are obtained by the captive model test (CMT), which is time-consuming to construct an accurate model suitable for complex berthing maneuvers. System identification (SI) is an alternative to constructing the mathematical model. However, SI on the mathematical model of ship's maneuver has been only conducted on much simpler maneuver: turning and zig-zag. Therefore, this study investigates the SI on a mathematical model capable of berthing maneuver. The main contributions of this study are as follows: (i) construct the system-based mathematical model on berthing by optimizing system parameters with a reduced amount of model tests than the CMT-based scheme; (ii) Find the favorable choice of objective function and type of training data for optimization. Global optimization scheme CMA-ES explored the system parameters of the MMG model from the free-running model's trajectories. The berthing simulation with the parameters obtained by the proposed method showed better agreement with the free-running model test than parameters obtained by the CMT. Furthermore, the proposed method required fewer data amounts than a CMT-based scheme.      
### 8.On Neural Network Identification for Low-Speed Ship Maneuvering Model  [ :arrow_down: ](https://arxiv.org/pdf/2111.06120.pdf)
>  Several studies on ship maneuvering models have been conducted using captive model tests or computational fluid dynamics (CFD) and physical models, such as the maneuvering modeling group (MMG) model. A new system identification method for generating a low-speed maneuvering model using recurrent neural networks (RNNs) and free running model tests is proposed in this study. We especially focus on a low-speed maneuver such as the final phase in berthing to achieve automatic berthing control. Accurate dynamic modeling with minimum modeling error is highly desired to establish a model-based control system. We propose a new loss function that reduces the effect of the noise included in the training data. Besides, we revealed the following facts - an RNN that ignores the memory before a certain time improved the prediction accuracy compared with the "standard" RNN, and the random maneuver test was effective in obtaining an accurate berthing maneuver model. In addition, several low-speed free running model tests were performed for the scale model of the M.V. Esso Osaka. As a result, this paper showed that the proposed method using a neural network model could accurately represent low-speed maneuvering motions.      
### 9.Environmental variation compensated damage classification and localization in ultrasonic guided wave SHM using self-learnt features and Gaussian mixture models  [ :arrow_down: ](https://arxiv.org/pdf/2111.06092.pdf)
>  Conventional damage localization algorithms used in ultrasonic guided wave-based structural health monitoring (GW-SHM) rely on physics-defined features of GW signals. In addition to requiring domain knowledge of the interaction of various GW modes with various types of damages, they also suffer from errors due to variations in environmental and operating conditions (EOCs) in practical use cases. While several machine learning tools have been reported for EOC compensation, they need to be custom-designed for each combination of damage and structure due to their dependence on physics-defined feature extraction. In this work, we propose a CNN-based automated feature extraction framework coupled with Gaussian mixture model (GMM) based EOC compensation and damage classification and localization method. Features learnt by the CNNs are used for damage classification and localization of damage by modeling the probability distribution of the features using GMMs. The Kullback-Leibler (KL) divergence of these GMMs with respect to corresponding baseline GMMs are used as signal difference coefficients (SDCs) to compute damage indices (DIs) along various GW sensor paths, and thus for damage localization. The efficacy of the proposed method is demonstrated using FE generated GW-data for an aluminum plate with a network of six lead zirconate titanate (PZT) sensors, for three different types of damages (rivet hole, added mass, notch) at various temperatures, with added white noise and pink noise to incorporate errors due to EOCs. We also present experimental validation of the method through characterization of notch damage in an aluminum panel under varying and non-uniform temperature profiles, using a portable custom-designed field programmable gate array (FPGA) based signal transduction and data acquisition system.      
### 10.On the Problem of Reformulating Systems with Uncertain Dynamics as a Stochastic Differential Equation  [ :arrow_down: ](https://arxiv.org/pdf/2111.06084.pdf)
>  We identify an issue in recent approaches to learning-based control that reformulate systems with uncertain dynamics using a stochastic differential equation. Specifically, we discuss the approximation that replaces a model with fixed but uncertain parameters (a source of epistemic uncertainty) with a model subject to external disturbances modeled as a Brownian motion (corresponding to aleatoric uncertainty).      
### 11.CodEx: A Modular Framework for Joint Temporal De-blurring and Tomographic Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2111.06069.pdf)
>  In many computed tomography (CT) imaging applications, it is important to rapidly collect data from an object that is moving or changing with time. Tomographic acquisition is generally assumed to be step-and-shoot, where the object is rotated to each desired angle, and a view is taken. However, step-and-shoot acquisition is slow and can waste photons, so in practice fly-scanning is done where the object is continuously rotated while collecting data. However, this can result in motion-blurred views and consequently reconstructions with severe motion artifacts. <br>In this paper, we introduce CodEx, a modular framework for joint de-blurring and tomographic reconstruction that can effectively invert the motion blur introduced in fly-scanning. The method is a synergistic combination of a novel acquisition method with a novel non-convex Bayesian reconstruction algorithm. CodEx works by encoding the acquisition with a known binary code that the reconstruction algorithm then inverts. Using a well chosen binary code to encode the measurements can improve the accuracy of the inversion process. The CodEx reconstruction method uses the alternating direction method of multipliers (ADMM) to split the inverse problem into iterative deblurring and reconstruction sub-problems, making reconstruction practical to implement. We present reconstruction results on both simulated and experimental data to demonstrate the effectiveness of our method.      
### 12.Socially-Aware Evaluation Framework for Transportation  [ :arrow_down: ](https://arxiv.org/pdf/2111.06059.pdf)
>  Technological advancements are rapidly changing traffic management in cities. Massive adoption of mobile devices and cloud-based applications have created new mechanisms for urban traffic control and management. Specifically, navigation applications have impacted cities in multiple ways by rerouting traffic on their streets. As different routing strategies distribute traffic differently across the city network, understanding these differences across multiple dimensions is highly relevant for policymakers. In this paper, we develop a holistic framework of indicators, called Socially-Aware Evaluation Framework for Transportation (SAEF), that will assist in understanding how traffic routing and the resultant traffic dynamics impact city metrics, with the intent of avoiding unintended consequences and adhering to city objectives. SAEF is a holistic decision framework formed as an assembled set of city performance indicators grounded in the literature. The selected indicators can be evaluated for cities of various sizes and at the urban scale. The SAEF framework is presented for four Bay Area cities, for which we compare three different routing strategies. Our intent with this work is to provide an evaluation framework that enables reflection on the consequence of policies, traffic management strategies and network changes. With an ability to model out proposed traffic management strategies, the policymaker can consider the trade-offs and potential unintended consequences.      
### 13.Processing of large sets of stochastic signals: filtering based on piecewise interpolation technique  [ :arrow_down: ](https://arxiv.org/pdf/2111.06041.pdf)
>  Suppose $K_{_Y}$ and $K_{_X}$ are large sets of observed and reference signals, respectively, each containing $N$ signals. Is it possible to construct a filter $F$ that requires a priori information only on few signals, $p\ll N$, from $K_{_X}$ but performs better than the known filters based on a priori information on every reference signal from $K_{_X}$? It is shown that the positive answer is achievable under quite unrestrictive assumptions. The device behind the proposed method is based on a special extension of the piecewise linear interpolation technique to the case of random signal sets. The proposed technique provides a single filter to process any signal from the arbitrarily large signal set. The filter is determined in terms of pseudo-inverse matrices so that it always exists.      
### 14.Uformer: A Unet based dilated complex &amp; real dual-path conformer network for simultaneous speech enhancement and dereverberation  [ :arrow_down: ](https://arxiv.org/pdf/2111.06015.pdf)
>  Complex spectrum and magnitude are considered as two major features of speech enhancement and dereverberation. Traditional approaches always treat these two features separately, ignoring their underlying relationship. In this paper, we proposem Uformer, a Unet based dilated complex &amp; real dual-path conformer network in both complex and magnitude domain for simultaneous speech enhancement and dereverberation. We exploit time attention (TA) and dilated convolution (DC) to leverage local and global contextual information and frequency attention (FA) to model dimensional information. These three sub-modules contained in the proposed dilated complex &amp; real dual-path conformer module effectively improve the speech enhancement and dereverberation performance. Furthermore, hybrid encoder and decoder are adopted to simultaneously model the complex spectrum and magnitude and promote the information interaction between two domains. Encoder decoder attention is also applied to enhance the interaction between encoder and decoder. Our experimental results outperform all SOTA time and complex domain models objectively and subjectively. Specifically, Uformer reaches 3.6032 DNSMOS on the blind test set of Interspeech 2021 DNS Challenge, which outperforms all top-performed models. We also carry out ablation experiments to tease apart all proposed sub-modules that are most important.      
### 15.Trustworthy Medical Segmentation with Uncertainty Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2111.05978.pdf)
>  Deep Learning (DL) holds great promise in reshaping the healthcare systems given its precision, efficiency, and objectivity. However, the brittleness of DL models to noisy and out-of-distribution inputs is ailing their deployment in the clinic. Most systems produce point estimates without further information about model uncertainty or confidence. This paper introduces a new Bayesian deep learning framework for uncertainty quantification in segmentation neural networks, specifically encoder-decoder architectures. The proposed framework uses the first-order Taylor series approximation to propagate and learn the first two moments (mean and covariance) of the distribution of the model parameters given the training data by maximizing the evidence lower bound. The output consists of two maps: the segmented image and the uncertainty map of the segmentation. The uncertainty in the segmentation decisions is captured by the covariance matrix of the predictive distribution. We evaluate the proposed framework on medical image segmentation data from Magnetic Resonances Imaging and Computed Tomography scans. Our experiments on multiple benchmark datasets demonstrate that the proposed framework is more robust to noise and adversarial attacks as compared to state-of-the-art segmentation models. Moreover, the uncertainty map of the proposed framework associates low confidence (or equivalently high uncertainty) to patches in the test input images that are corrupted with noise, artifacts or adversarial attacks. Thus, the model can self-assess its segmentation decisions when it makes an erroneous prediction or misses part of the segmentation structures, e.g., tumor, by presenting higher values in the uncertainty map.      
### 16.Advancing Brain Metastases Detection in T1-Weighted Contrast-Enhanced 3D MRI using Noisy Student-based Training  [ :arrow_down: ](https://arxiv.org/pdf/2111.05959.pdf)
>  The detection of brain metastases (BM) in their early stages could have a positive impact on the outcome of cancer patients. We previously developed a framework for detecting small BM (with diameters of less than 15mm) in T1-weighted Contrast-Enhanced 3D Magnetic Resonance images (T1c) to assist medical experts in this time-sensitive and high-stakes task. The framework utilizes a dedicated convolutional neural network (CNN) trained using labeled T1c data, where the ground truth BM segmentations were provided by a radiologist. This study aims to advance the framework with a noisy student-based self-training strategy to make use of a large corpus of unlabeled T1c data (i.e., data without BM segmentations or detections). Accordingly, the work (1) describes the student and teacher CNN architectures, (2) presents data and model noising mechanisms, and (3) introduces a novel pseudo-labeling strategy factoring in the learned BM detection sensitivity of the framework. Finally, it describes a semi-supervised learning strategy utilizing these components. We performed the validation using 217 labeled and 1247 unlabeled T1c exams via 2-fold cross-validation. The framework utilizing only the labeled exams produced 9.23 false positives for 90% BM detection sensitivity; whereas, the framework using the introduced learning strategy led to ~9% reduction in false detections (i.e., 8.44) for the same sensitivity level. Furthermore, while experiments utilizing 75% and 50% of the labeled datasets resulted in algorithm performance degradation (12.19 and 13.89 false positives respectively), the impact was less pronounced with the noisy student-based training strategy (10.79 and 12.37 false positives respectively).      
### 17.Kalman Filtering with Adversarial Corruptions  [ :arrow_down: ](https://arxiv.org/pdf/2111.06395.pdf)
>  Here we revisit the classic problem of linear quadratic estimation, i.e. estimating the trajectory of a linear dynamical system from noisy measurements. The celebrated Kalman filter gives an optimal estimator when the measurement noise is Gaussian, but is widely known to break down when one deviates from this assumption, e.g. when the noise is heavy-tailed. Many ad hoc heuristics have been employed in practice for dealing with outliers. In a pioneering work, Schick and Mitter gave provable guarantees when the measurement noise is a known infinitesimal perturbation of a Gaussian and raised the important question of whether one can get similar guarantees for large and unknown perturbations. <br>In this work we give a truly robust filter: we give the first strong provable guarantees for linear quadratic estimation when even a constant fraction of measurements have been adversarially corrupted. This framework can model heavy-tailed and even non-stationary noise processes. Our algorithm robustifies the Kalman filter in the sense that it competes with the optimal algorithm that knows the locations of the corruptions. Our work is in a challenging Bayesian setting where the number of measurements scales with the complexity of what we need to estimate. Moreover, in linear dynamical systems past information decays over time. We develop a suite of new techniques to robustly extract information across different time steps and over varying time scales.      
### 18.Full-Body Visual Self-Modeling of Robot Morphologies  [ :arrow_down: ](https://arxiv.org/pdf/2111.06389.pdf)
>  Internal computational models of physical bodies are fundamental to the ability of robots and animals alike to plan and control their actions. These "self-models" allow robots to consider outcomes of multiple possible future actions, without trying them out in physical reality. Recent progress in fully data-driven self-modeling has enabled machines to learn their own forward kinematics directly from task-agnostic interaction data. However, forward-kinema\-tics models can only predict limited aspects of the morphology, such as the position of end effectors or velocity of joints and masses. A key challenge is to model the entire morphology and kinematics, without prior knowledge of what aspects of the morphology will be relevant to future tasks. Here, we propose that instead of directly modeling forward-kinematics, a more useful form of self-modeling is one that could answer space occupancy queries, conditioned on the robot's state. Such query-driven self models are continuous in the spatial domain, memory efficient, fully differentiable and kinematic aware. In physical experiments, we demonstrate how a visual self-model is accurate to about one percent of the workspace, enabling the robot to perform various motion planning and control tasks. Visual self-modeling can also allow the robot to detect, localize and recover from real-world damage, leading to improved machine resiliency. Our project website is at: <a class="link-external link-https" href="https://robot-morphology.cs.columbia.edu/" rel="external noopener nofollow">this https URL</a>      
### 19.Flattening the Duck Curve: A Case for Distributed Decision Making  [ :arrow_down: ](https://arxiv.org/pdf/2111.06361.pdf)
>  The large penetration of renewable energy installments has resulted in rapidly changing net loads. The resulting ramping requirements of bulk system resources is an operational challenge. To address this, we propose a distributed optimization framework within which distributed energy resources located in the distribution grid are coordinated to provide support to the bulk system. We model the power flow of the multi-phase unbalanced distribution grid using a Current Injection (CI) approach, which leverages McCormick Envelope based convex relaxation to render a linear model. We then solve this CI-OPF with an accelerated Proximal Atomic Coordination (PAC) which employs Nesterov type acceleration, termed NST-PAC. We evaluate our distributed approach against a local approach, on a case study of San Francisco, California, using a modified IEEE-34 node network and under a high penetration of solar PV, flexible loads, and battery units. Our distributed approach reduced the ramping requirements of bulk system generators by up to 23%.      
### 20.Performance of Queueing Models for MISO Content-Centric Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.06352.pdf)
>  MISO networks have garnered attention in wireless content-centric networks due to the additional degrees of freedoms they provide. Several beamforming techniques such as NOMA, OMA, SDMA and Rate splitting have been proposed for such networks. These techniques utilise the redundancy in the content requests across users and leverage the spatial multicast and multiplexing gains of multi-antenna transmit beamforming to improve the content delivery rate. However, queueing delays and user traffic dynamics which significantly affect the performance of these schemes, have generally been ignored. We study queueing delays in the downlink for several scheduling and beamforming schemes in content-centric networks, with one base-station possessing multiple transmit antennas. These schemes are studied along with a recently proposed Simple Multicast Queue, to improve the delay performance of the network. This work is particularly relevant for content delivery in 5G and eMBB networks.      
### 21.Towards an Efficient Voice Identification Using Wav2Vec2.0 and HuBERT Based on the Quran Reciters Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2111.06331.pdf)
>  Current authentication and trusted systems depend on classical and biometric methods to recognize or authorize users. Such methods include audio speech recognitions, eye, and finger signatures. Recent tools utilize deep learning and transformers to achieve better results. In this paper, we develop a deep learning constructed model for Arabic speakers identification by using Wav2Vec2.0 and HuBERT audio representation learning tools. The end-to-end Wav2Vec2.0 paradigm acquires contextualized speech representations learnings by randomly masking a set of feature vectors, and then applies a transformer neural network. We employ an MLP classifier that is able to differentiate between invariant labeled classes. We show several experimental results that safeguard the high accuracy of the proposed model. The experiments ensure that an arbitrary wave signal for a certain speaker can be identified with 98% and 97.1% accuracies in the cases of Wav2Vec2.0 and HuBERT, respectively.      
### 22.Unsupervised Noise Adaptive Speech Enhancement by Discriminator-Constrained Optimal Transport  [ :arrow_down: ](https://arxiv.org/pdf/2111.06316.pdf)
>  This paper presents a novel discriminator-constrained optimal transport network (DOTN) that performs unsupervised domain adaptation for speech enhancement (SE), which is an essential regression task in speech processing. The DOTN aims to estimate clean references of noisy speech in a target domain, by exploiting the knowledge available from the source domain. The domain shift between training and testing data has been reported to be an obstacle to learning problems in diverse fields. Although rich literature exists on unsupervised domain adaptation for classification, the methods proposed, especially in regressions, remain scarce and often depend on additional information regarding the input data. The proposed DOTN approach tactically fuses the optimal transport (OT) theory from mathematical analysis with generative adversarial frameworks, to help evaluate continuous labels in the target domain. The experimental results on two SE tasks demonstrate that by extending the classical OT formulation, our proposed DOTN outperforms previous adversarial domain adaptation frameworks in a purely unsupervised manner.      
### 23.Self-Normalized Importance Sampling for Neural Language Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2111.06310.pdf)
>  To mitigate the problem of having to traverse over the full vocabulary in the softmax normalization of a neural language model, sampling-based training criteria are proposed and investigated in the context of large vocabulary word-based neural language models. These training criteria typically enjoy the benefit of faster training and testing, at a cost of slightly degraded performance in terms of perplexity and almost no visible drop in word error rate. While noise contrastive estimation is one of the most popular choices, recently we show that other sampling-based criteria can also perform well, as long as an extra correction step is done, where the intended class posterior probability is recovered from the raw model outputs. In this work, we propose self-normalized importance sampling. Compared to our previous work, the criteria considered in this work are self-normalized and there is no need to further conduct a correction step. Compared to noise contrastive estimation, our method is directly comparable in terms of complexity in application. Through self-normalized language model training as well as lattice rescoring experiments, we show that our proposed self-normalized importance sampling is competitive in both research-oriented and production-oriented automatic speech recognition tasks.      
### 24.Branch and Bound in Mixed Integer Linear Programming Problems: A Survey of Techniques and Trends  [ :arrow_down: ](https://arxiv.org/pdf/2111.06257.pdf)
>  In this paper, we surveyed the existing literature studying different approaches and algorithms for the four critical components in the general branch and bound (B&amp;B) algorithm, namely, branching variable selection, node selection, node pruning, and cutting-plane selection. However, the complexity of the B&amp;B algorithm always grows exponentially with respect to the increase of the decision variable dimensions. In order to improve the speed of B&amp;B algorithms, learning techniques have been introduced in this algorithm recently. We further surveyed how machine learning can be used to improve the four critical components in B&amp;B algorithms. In general, a supervised learning method helps to generate a policy that mimics an expert but significantly improves the speed. An unsupervised learning method helps choose different methods based on the features. In addition, models trained with reinforcement learning can beat the expert policy, given enough training and a supervised initialization. Detailed comparisons between different algorithms have been summarized in our survey. Finally, we discussed some future research directions to accelerate and improve the algorithms further in the literature.      
### 25.On Novel Peer Review System for Academic Journals: Experimental Study Based on Social Computing  [ :arrow_down: ](https://arxiv.org/pdf/2111.06099.pdf)
>  For improving the performance and effectiveness of peer review, a novel review system is proposed, based on analysis of peer review process for academic journals under a parallel model built via Monte Carlo method. The model can simulate the review, application and acceptance activities of the review systems, in a distributed manner. According to simulation experiments on two distinct review systems respectively, significant advantages manifest for the novel one.      
### 26.Music Score Expansion with Variable-Length Infilling  [ :arrow_down: ](https://arxiv.org/pdf/2111.06046.pdf)
>  In this paper, we investigate using the variable-length infilling (VLI) model, which is originally proposed to infill missing segments, to "prolong" existing musical segments at musical boundaries. Specifically, as a case study, we expand 20 musical segments from 12 bars to 16 bars, and examine the degree to which the VLI model preserves musical boundaries in the expanded results using a few objective metrics, including the Register Histogram Similarity we newly propose. The results show that the VLI model has the potential to address the expansion task.      
### 27.Hybrid Saturation Restoration for LDR Images of HDR Scenes  [ :arrow_down: ](https://arxiv.org/pdf/2111.06038.pdf)
>  There are shadow and highlight regions in a low dynamic range (LDR) image which is captured from a high dynamic range (HDR) scene. It is an ill-posed problem to restore the saturated regions of the LDR image. In this paper, the saturated regions of the LDR image are restored by fusing model-based and data-driven approaches. With such a neural augmentation, two synthetic LDR images are first generated from the underlying LDR image via the model-based approach. One is brighter than the input image to restore the shadow regions and the other is darker than the input image to restore the high-light regions. Both synthetic images are then refined via a novel exposedness aware saturation restoration network (EASRN). Finally, the two synthetic images and the input image are combined together via an HDR synthesis algorithm or a multi-scale exposure fusion algorithm. The proposed algorithm can be embedded in any smart phones or digital cameras to produce an information-enriched LDR image.      
### 28.FINO: Flow-based Joint Image and Noise Model  [ :arrow_down: ](https://arxiv.org/pdf/2111.06031.pdf)
>  One of the fundamental challenges in image restoration is denoising, where the objective is to estimate the clean image from its noisy measurements. To tackle such an ill-posed inverse problem, the existing denoising approaches generally focus on exploiting effective natural image priors. The utilization and analysis of the noise model are often ignored, although the noise model can provide complementary information to the denoising algorithms. In this paper, we propose a novel Flow-based joint Image and NOise model (FINO) that distinctly decouples the image and noise in the latent space and losslessly reconstructs them via a series of invertible transformations. We further present a variable swapping strategy to align structural information in images and a noise correlation matrix to constrain the noise based on spatially minimized correlation information. Experimental results demonstrate FINO's capacity to remove both synthetic additive white Gaussian noise (AWGN) and real noise. Furthermore, the generalization of FINO to the removal of spatially variant noise and noise with inaccurate estimation surpasses that of the popular and state-of-the-art methods by large margins.      
### 29.Low Complexity Channel Estimation for OTFS Modulation with Fractional Delay and Doppler  [ :arrow_down: ](https://arxiv.org/pdf/2111.06009.pdf)
>  We consider the problem of accurate channel estimation for OTFS based systems with few transmit/receive antennas, where additional sparsity due to large number of antennas is not a possibility. For such systems the sparsity of the effective delay-Doppler (DD) domain channel is adversely affected in the presence of channel path delay and Doppler shifts which are non-integer multiples of the delay and Doppler domain resolution. The sparsity is also adversely affected when practical transmit and receive pulses are used. In this paper we propose a Modified Maximum Likelihood Channel Estimation (M-MLE) method for OTFS based systems which exploits the fine delay and Doppler domain resolution of the OTFS modulated signal to decouple the joint estimation of the channel parameters (i.e., channel gain, delay and Doppler shift) of all channel paths into separate estimation of the channel parameters for each path. We further observe that with fine delay and Doppler domain resolution, the received DD domain signal along a particular channel path can be written as a product of a delay domain term and a Doppler domain term where the delay domain term is primarily dependent on the delay of this path and the Doppler domain term is primarily dependent on the Doppler shift of this path. This allows us to propose another method termed as the two-step method (TSE), where the joint two-dimensional estimation of the delay and Doppler shift of a particular path in the M-MLE method is further decoupled into two separate one-dimensional estimation for the delay and for the Doppler shift of that path. Simulations reveal that the proposed methods (M-MLE and TSE) achieve better channel estimation accuracy at lower complexity when compared to other known methods for accurate OTFS channel estimation.      
### 30.PowerGridworld: A Framework for Multi-Agent Reinforcement Learning in Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.05969.pdf)
>  We present the PowerGridworld software package to provide users with a lightweight, modular, and customizable framework for creating power-systems-focused, multi-agent Gym environments that readily integrate with existing training frameworks for reinforcement learning (RL). Although many frameworks exist for training multi-agent RL (MARL) policies, none can rapidly prototype and develop the environments themselves, especially in the context of heterogeneous (composite, multi-device) power systems where power flow solutions are required to define grid-level variables and costs. PowerGridworld is an open-source software package that helps to fill this gap. To highlight PowerGridworld's key features, we present two case studies and demonstrate learning MARL policies using both OpenAI's multi-agent deep deterministic policy gradient (MADDPG) and RLLib's proximal policy optimization (PPO) algorithms. In both cases, at least some subset of agents incorporates elements of the power flow solution at each time step as part of their reward (negative cost) structures.      
### 31.Scaling ASR Improves Zero and Few Shot Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.05948.pdf)
>  With 4.5 million hours of English speech from 10 different sources across 120 countries and models of up to 10 billion parameters, we explore the frontiers of scale for automatic speech recognition. We propose data selection techniques to efficiently scale training data to find the most valuable samples in massive datasets. To efficiently scale model sizes, we leverage various optimizations such as sparse transducer loss and model sharding. By training 1-10B parameter universal English ASR models, we push the limits of speech recognition performance across many domains. Furthermore, our models learn powerful speech representations with zero and few-shot capabilities on novel domains and styles of speech, exceeding previous results across multiple in-house and public benchmarks. For speakers with disorders due to brain damage, our best zero-shot and few-shot models achieve 22% and 60% relative improvement on the AphasiaBank test set, respectively, while realizing the best performance on public social media videos. Furthermore, the same universal model reaches equivalent performance with 500x less in-domain data on the SPGISpeech financial-domain dataset.      
### 32.Privacy signalling games with binary alphabets  [ :arrow_down: ](https://arxiv.org/pdf/2111.05947.pdf)
>  In this paper, we consider a privacy signaling game problem for binary alphabets where a transmitter has a pair of messages one of which is a casual message that needs to be conveyed whereas the other message contains sensitive data and needs to be protected. The receiver wishes to estimate both messages with the aim of acquiring as much information as possible. For this setup, we study the interactions between the transmitter and the receiver with non-aligned information theoretic objectives (modeled by mutual information and hamming distance) due to the privacy concerns of the transmitter. We derive conditions under which Nash and/or Stackelberg equilibria exist and identify the optimal responses of the encoder and decoders strategies for each type of game. One particularly surprising result is that when both type of equilibria exist, they admit the same encoding and decoding strategies. We corroborate our analysis with simulation studies.      
### 33.A soft thumb-sized vision-based sensor with accurate all-round force perception  [ :arrow_down: ](https://arxiv.org/pdf/2111.05934.pdf)
>  Vision-based haptic sensors have emerged as a promising approach to robotic touch due to affordable high-resolution cameras and successful computer-vision techniques. However, their physical design and the information they provide do not yet meet the requirements of real applications. We present a robust, soft, low-cost, vision-based, thumb-sized 3D haptic sensor named Insight: it continually provides a directional force-distribution map over its entire conical sensing surface. Constructed around an internal monocular camera, the sensor has only a single layer of elastomer over-molded on a stiff frame to guarantee sensitivity, robustness, and soft contact. Furthermore, Insight is the first system to combine photometric stereo and structured light using a collimator to detect the 3D deformation of its easily replaceable flexible outer shell. The force information is inferred by a deep neural network that maps images to the spatial distribution of 3D contact force (normal and shear). Insight has an overall spatial resolution of 0.4 mm, force magnitude accuracy around 0.03 N, and force direction accuracy around 5 degrees over a range of 0.03--2 N for numerous distinct contacts with varying contact area. The presented hardware and software design concepts can be transferred to a wide variety of robot parts.      
### 34.A Generic Deep Learning Based Cough Analysis System from Clinically Validated Samples for Point-of-Need Covid-19 Test and Severity Levels  [ :arrow_down: ](https://arxiv.org/pdf/2111.05895.pdf)
>  We seek to evaluate the detection performance of a rapid primary screening tool of Covid-19 solely based on the cough sound from 8,380 clinically validated samples with laboratory molecular-test (2,339 Covid-19 positives and 6,041 Covid-19 negatives). Samples were clinically labeled according to the results and severity based on quantitative RT-PCR (qRT-PCR) analysis, cycle threshold, and lymphocytes count from the patients. Our proposed generic method is an algorithm based on Empirical Mode Decomposition (EMD) with subsequent classification based on a tensor of audio features and a deep artificial neural network classifier with convolutional layers called DeepCough'. Two different versions of DeepCough based on the number of tensor dimensions, i.e. DeepCough2D and DeepCough3D, have been investigated. These methods have been deployed in a multi-platform proof-of-concept Web App CoughDetect to administer this test anonymously. Covid-19 recognition results rates achieved a promising AUC (Area Under Curve) of 98.800.83%, sensitivity of 96.431.85%, and specificity of 96.201.74%, and 81.08%5.05% AUC for the recognition of three severity levels. Our proposed web tool and underpinning algorithm for the robust, fast, point-of-need identification of Covid-19 facilitates the rapid detection of the infection. We believe that it has the potential to significantly hamper the Covid-19 pandemic across the world.      
### 35.A Portable and Passive Gravity Compensation Arm Support for Drone Teleoperation  [ :arrow_down: ](https://arxiv.org/pdf/2111.05891.pdf)
>  Gesture-based interfaces are often used to achieve a more natural and intuitive teleoperation of robots. Yet, sometimes, gesture control requires postures or movements that cause significant fatigue to the user. In a previous user study, we demonstrated that naÃ¯ve users can control a fixed-wing drone with torso movements while their arms are spread out. However, this posture induced significant arm fatigue. In this work, we present a passive arm support that compensates the arm weight with a mean torque error smaller than 0.005 N/kg for more than 97% of the range of motion used by subjects to fly, therefore reducing muscular fatigue in the shoulder of on average 58%. In addition, this arm support is designed to fit users from the body dimension of the 1st percentile female to the 99th percentile male. The performance analysis of the arm support is described with a mechanical model and its implementation is validated with both a mechanical characterization and a user study, which measures the flight performance, the shoulder muscle activity and the user acceptance.      
### 36.Multimodal End-to-End Group Emotion Recognition using Cross-Modal Attention  [ :arrow_down: ](https://arxiv.org/pdf/2111.05890.pdf)
>  Classifying group-level emotions is a challenging task due to complexity of video, in which not only visual, but also audio information should be taken into consideration. Existing works on multimodal emotion recognition are using bulky approach, where pretrained neural networks are used as a feature extractors and then extracted features are being fused. However, this approach does not consider attributes of multimodal data and feature extractors cannot be fine-tuned for specific task which can be disadvantageous for overall model accuracy. To this end, our impact is twofold: (i) we train model end-to-end, which allows early layers of neural network to be adapted with taking into account later, fusion layers, of two modalities; (ii) all layers of our model was fine-tuned for downstream task of emotion recognition, so there were no need to train neural networks from scratch. Our model achieves best validation accuracy of 60.37% which is approximately 8.5% higher, than VGAF dataset baseline and is competitive with existing works, audio and video modalities.      
### 37.A Histopathology Study Comparing Contrastive Semi-Supervised and Fully Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.05882.pdf)
>  Data labeling is often the most challenging task when developing computational pathology models. Pathologist participation is necessary to generate accurate labels, and the limitations on pathologist time and demand for large, labeled datasets has led to research in areas including weakly supervised learning using patient-level labels, machine assisted annotation and active learning. In this paper we explore self-supervised learning to reduce labeling burdens in computational pathology. We explore this in the context of classification of breast cancer tissue using the Barlow Twins approach, and we compare self-supervision with alternatives like pre-trained networks in low-data scenarios. For the task explored in this paper, we find that ImageNet pre-trained networks largely outperform the self-supervised representations obtained using Barlow Twins.      
