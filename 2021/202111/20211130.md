# ArXiv eess --Tue, 30 Nov 2021
### 1.Radar Aided Proactive Blockage Prediction in Real-World Millimeter Wave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.14805.pdf)
>  Millimeter wave (mmWave) and sub-terahertz communication systems rely mainly on line-of-sight (LOS) links between the transmitters and receivers. The sensitivity of these high-frequency LOS links to blockages, however, challenges the reliability and latency requirements of these communication networks. In this paper, we propose to utilize radar sensors to provide sensing information about the surrounding environment and moving objects, and leverage this information to proactively predict future link blockages before they happen. This is motivated by the low cost of the radar sensors, their ability to efficiently capture important features such as the range, angle, velocity of the moving scatterers (candidate blockages), and their capability to capture radar frames at relatively high speed. We formulate the radar-aided proactive blockage prediction problem and develop two solutions for this problem based on classical radar object tracking and deep neural networks. The two solutions are designed to leverage domain knowledge and the understanding of the blockage prediction problem. To accurately evaluate the proposed solutions, we build a large-scale real-world dataset, based on the DeepSense framework, gathering co-existing radar and mmWave communication measurements of more than $10$ thousand data points and various blockage objects (vehicles, bikes, humans, etc.). The evaluation results, based on this dataset, show that the proposed approaches can predict future blockages $1$ second before they happen with more than $90\%$ $F_1$ score (and more than $90\%$ accuracy). These results, among others, highlight a promising solution for blockage prediction and reliability enhancement in future wireless mmWave and terahertz communication systems.      
### 2.Unsupervised cross domain learning with applications to 7 layer segmentation of OCTs  [ :arrow_down: ](https://arxiv.org/pdf/2111.14804.pdf)
>  Unsupervised cross domain adaptation for OCT 7 layer segmentation and other medical applications where labeled training data is only available in a source domain and unavailable in the target domain. Our proposed method helps generalize of deep learning to many areas in the medical field where labeled training data are expensive and time consuming to acquire or where target domains are too novel to have had labelling.      
### 3.Computer Vision Aided Beam Tracking in A Real-World Millimeter Wave Deployment  [ :arrow_down: ](https://arxiv.org/pdf/2111.14803.pdf)
>  Millimeter-wave (mmWave) and terahertz (THz) communications require beamforming to acquire adequate receive signal-to-noise ratio (SNR). To find the optimal beam, current beam management solutions perform beam training over a large number of beams in pre-defined codebooks. The beam training overhead increases the access latency and can become infeasible for high-mobility applications. To reduce or even eliminate this beam training overhead, we propose to utilize the visual data, captured for example by cameras at the base stations, to guide the beam tracking/refining process. We propose a machine learning (ML) framework, based on an encoder-decoder architecture, that can predict the future beams using the previously obtained visual sensing information. Our proposed approach is evaluated on a large-scale real-world dataset, where it achieves an accuracy of $64.47\%$ (and a normalized receive power of $97.66\%$) in predicting the future beam. This is achieved while requiring less than $1\%$ of the beam training overhead of a corresponding baseline solution that uses a sequence of previous beams to predict the future one. This high performance and low overhead obtained on the real-world dataset demonstrate the potential of the proposed vision-aided beam tracking approach in real-world applications.      
### 4.Harmonic Retrieval with $L_1$-Tucker Tensor Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2111.14780.pdf)
>  Harmonic retrieval (HR) has a wide range of applications in the scenes where signals are modelled as a summation of sinusoids. Past works have developed a number of approaches to recover the original signals. Most of them rely on classical singular value decomposition, which are vulnerable to unexpected outliers. In this paper, we present new decomposition algorithms of third-order complex-valued tensors with $L_1$-principle component analysis ($L_1$-PCA) of complex data and apply them to a novel random access HR model in presence of outliers. We also develop a novel subcarrier recovery method for the proposed model. Simulations are designed to compare our proposed method with some existing tensor-based algorithms for HR. The results demonstrate the outlier-insensitivity of the proposed method.      
### 5.Model-Based End-to-End Learning for WDM Systems With Transceiver Hardware Impairments  [ :arrow_down: ](https://arxiv.org/pdf/2111.14515.pdf)
>  We propose an AE-based transceiver for a WDM system impaired by hardware imperfections. We design our AE following the architecture of conventional communication systems. This enables to initialize the AE-based transceiver to have similar performance to its conventional counterpart prior to training and improves the training convergence rate. We first train the AE in a single-channel system, and show that it achieves performance improvements by putting energy outside the desired bandwidth, and therefore cannot be used for a WDM system. We then train the AE in a WDM setup. Simulation results show that the proposed AE significantly outperforms the conventional approach. More specifically, it increases the spectral efficiency of the considered system by reducing the guard band by 37\% and 50\% for a root-raised-cosine filter-based matched filter with 10\% and 1\% roll-off, respectively. An ablation study indicates that the performance gain can be ascribed to the optimization of the symbol mapper, the pulse-shaping filter, and the symbol demapper. Finally, we use reinforcement learning to learn the pulse-shaping filter under the assumption that the channel model is unknown. Simulation results show that the reinforcement-learning-based algorithm achieves similar performance to the standard supervised end-to-end learning approach assuming perfect channel knowledge.      
### 6.Deep Video Coding with Dual-Path Generative Adversarial Network  [ :arrow_down: ](https://arxiv.org/pdf/2111.14474.pdf)
>  The deep-learning-based video coding has attracted substantial attention for its great potential to squeeze out the spatial-temporal redundancies of video sequences. This paper proposes an efficient codec namely dual-path generative adversarial network-based video codec (DGVC). First, we propose a dual-path enhancement with generative adversarial network (DPEG) to reconstruct the compressed video details. The DPEG consists of an $\alpha$-path of auto-encoder and convolutional long short-term memory (ConvLSTM), which facilitates the structure feature reconstruction with a large receptive field and multi-frame references, and a $\beta$-path of residual attention blocks, which facilitates the reconstruction of local texture features. Both paths are fused and co-trained by a generative-adversarial process. Second, we reuse the DPEG network in both motion compensation and quality enhancement modules, which are further combined with motion estimation and entropy coding modules in our DGVC framework. Third, we employ a joint training of deep video compression and enhancement to further improve the rate-distortion (RD) performance. Compared with x265 LDP very fast mode, our DGVC reduces the average bit-per-pixel (bpp) by 39.39%/54.92% at the same PSNR/MS-SSIM, which outperforms the state-of-the art deep video codecs by a considerable margin.      
### 7.Stability Analysis of a Feedback-linearization-based Controller with Saturation: A Tilt Vehicle with the Penguin-inspired Gait Plan  [ :arrow_down: ](https://arxiv.org/pdf/2111.14456.pdf)
>  Saturations in control signal can challenge the stability proof of a feedback-linearization-based controller, even leading the system unstable [1]. Thus, several approaches are established to avoid reaching the saturation bound [2,3]. Meanwhile, to help design the controller for a quad-tilt-rotor, [1] modeled a tilt vehicle with implementing the feedback-linearization-based controllers. In this article, we provide a gait plan for this tilt vehicle and control it utilizing the feedback linearization. Since saturations exist in the control signals, we study the stability based on Lyapunov theory.      
### 8.Unknown Input Observer Design for Linear Time-Invariant Systems -- A Unifying Framework  [ :arrow_down: ](https://arxiv.org/pdf/2111.14404.pdf)
>  This paper presents a new observer design approach for linear time invariant multivariable systems subject to unknown inputs. The design is based on a transformation to the so-called special coordinate basis. This form reveals important system properties like invertability or the finite and infinite zero structure. Depending on the system's strong observability properties, the special coordinate basis allows for a straightforward unknown input observer design utilizing linear or nonlinear observers design techniques. The chosen observer design technique does not only depend on the system properties, but also on the desired convergence behavior of the observer. Hence, the proposed design procedure can be seen as a unifying framework for unknown input observer design.      
### 9.Enhanced Transfer Learning Through Medical Imaging and Patient Demographic Data Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2111.14388.pdf)
>  In this work we examine the performance enhancement in classification of medical imaging data when image features are combined with associated non-image data. We compare the performance of eight state-of-the-art deep neural networks in classification tasks when using only image features, compared to when these are combined with patient metadata. We utilise transfer learning with networks pretrained on ImageNet used directly as feature extractors and fine tuned on the target domain. Our experiments show that performance can be significantly enhanced with the inclusion of metadata and use interpretability methods to identify which features lead to these enhancements. Furthermore, our results indicate that the performance enhancement for natural medical imaging (e.g. optical images) benefit most from direct use of pre-trained models, whereas non natural images (e.g. representations of non imaging data) benefit most from fine tuning pre-trained networks. These enhancements come at a negligible additional cost in computation time, and therefore is a practical method for other applications.      
### 10.Unsupervised Image Denoising with Frequency Domain Knowledge  [ :arrow_down: ](https://arxiv.org/pdf/2111.14362.pdf)
>  Supervised learning-based methods yield robust denoising results, yet they are inherently limited by the need for large-scale clean/noisy paired datasets. The use of unsupervised denoisers, on the other hand, necessitates a more detailed understanding of the underlying image statistics. In particular, it is well known that apparent differences between clean and noisy images are most prominent on high-frequency bands, justifying the use of low-pass filters as part of conventional image preprocessing steps. However, most learning-based denoising methods utilize only one-sided information from the spatial domain without considering frequency domain information. To address this limitation, in this study we propose a frequency-sensitive unsupervised denoising method. To this end, a generative adversarial network (GAN) is used as a base structure. Subsequently, we include spectral discriminator and frequency reconstruction loss to transfer frequency knowledge into the generator. Results using natural and synthetic datasets indicate that our unsupervised learning method augmented with frequency information achieves state-of-the-art denoising performance, suggesting that frequency domain information could be a viable factor in improving the overall performance of unsupervised learning-based methods.      
### 11.Physics-informed Evolutionary Strategy based Control for Mitigating Delayed Voltage Recovery  [ :arrow_down: ](https://arxiv.org/pdf/2111.14352.pdf)
>  In this work we propose a novel data-driven, real-time power system voltage control method based on the physics-informed guided meta evolutionary strategy (ES). The main objective is to quickly provide an adaptive control strategy to mitigate the fault-induced delayed voltage recovery (FIDVR) problem. Reinforcement learning methods have been developed for the same or similar challenging control problems, but they suffer from training inefficiency and lack of robustness for "corner or unseen" scenarios. On the other hand, extensive physical knowledge has been developed in power systems but little has been leveraged in learning-based approaches. To address these challenges, we introduce the trainable action mask technique for flexibly embedding physical knowledge into RL models to rule out unnecessary or unfavorable actions, and achieve notable improvements in sample efficiency, control performance and robustness. Furthermore, our method leverages past learning experience to derive surrogate gradient to guide and accelerate the exploration process in training. Case studies on the IEEE 300-bus system and comparisons with other state-of-the-art benchmark methods demonstrate effectiveness and advantages of our method.      
### 12.SwiftSRGAN -- Rethinking Super-Resolution for Efficient and Real-time Inference  [ :arrow_down: ](https://arxiv.org/pdf/2111.14320.pdf)
>  In recent years, there have been several advancements in the task of image super-resolution using the state of the art Deep Learning-based architectures. Many super-resolution-based techniques previously published, require high-end and top-of-the-line Graphics Processing Unit (GPUs) to perform image super-resolution. With the increasing advancements in Deep Learning approaches, neural networks have become more and more compute hungry. We took a step back and, focused on creating a real-time efficient solution. We present an architecture that is faster and smaller in terms of its memory footprint. The proposed architecture uses Depth-wise Separable Convolutions to extract features and, it performs on-par with other super-resolution GANs (Generative Adversarial Networks) while maintaining real-time inference and a low memory footprint. A real-time super-resolution enables streaming high resolution media content even under poor bandwidth conditions. While maintaining an efficient trade-off between the accuracy and latency, we are able to produce a comparable performance model which is one-eighth (1/8) the size of super-resolution GANs and computes 74 times faster than super-resolution GANs.      
### 13.Passive Indoor Localization with WiFi Fingerprints  [ :arrow_down: ](https://arxiv.org/pdf/2111.14281.pdf)
>  This paper proposes passive WiFi indoor localization. Instead of using WiFi signals received by mobile devices as fingerprints, we use signals received by routers to locate the mobile carrier. Consequently, software installation on the mobile device is not required. To resolve the data insufficiency problem, flow control signals such as request to send (RTS) and clear to send (CTS) are utilized. In our model, received signal strength indicator (RSSI) and channel state information (CSI) are used as fingerprints for several algorithms, including deterministic, probabilistic and neural networks localization algorithms. We further investigated localization algorithms performance through extensive on-site experiments with various models of phones at hundreds of testing locations. We demonstrate that our passive scheme achieves an average localization error of 0.8 m when the phone is actively transmitting data frames and 1.5 m when it is not transmitting data frames.      
### 14.Radio Frequency Fingerprint Identification for Security in Low-Cost IoT Devices  [ :arrow_down: ](https://arxiv.org/pdf/2111.14275.pdf)
>  Radio frequency fingerprint identification (RFFI) can uniquely classify wireless devices by analyzing the received signal distortions caused by the intrinsic hardware impairments. The state-of-the-art deep learning techniques such as convolutional neural network (CNN) have been adopted to classify IoT devices with high accuracy. However, deep learning-based RFFI requires input data of a fixed size. In addition, many IoT devices work in low signal-to-noise ratio (SNR) scenarios but the low SNR RFFI is rarely investigated. In this paper, the state-of-the-art transformer model is used as the classifier, which can process signals of variable length. Data augmentation is adopted to improve low SNR RFFI performance. A multi-packet inference approach is further proposed to improve the classification accuracy in low SNR scenarios. We take LoRa as a case study and evaluate the system by classifying 10 commercial-off-the-shelf LoRa devices in various SNR conditions. The online augmentation can boost the low SNR RFFI performance by up to 50% and multi-packet inference can further increase it by over 20%.      
### 15.3D High-Quality Magnetic Resonance Image Restoration in Clinics Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.14259.pdf)
>  Shortening acquisition time and reducing the motion-artifact are two of the most essential concerns in magnetic resonance imaging. As a promising solution, deep learning-based high quality MR image restoration has been investigated to generate higher resolution and motion artifact-free MR images from lower resolution images acquired with shortened acquisition time, without costing additional acquisition time or modifying the pulse sequences. However, numerous problems still exist to prevent deep learning approaches from becoming practical in the clinic environment. Specifically, most of the prior works focus solely on the network model but ignore the impact of various downsampling strategies on the acquisition time. Besides, the long inference time and high GPU consumption are also the bottle neck to deploy most of the prior works in clinics. Furthermore, prior studies employ random movement in retrospective motion artifact generation, resulting in uncontrollable severity of motion artifact. More importantly, doctors are unsure whether the generated MR images are trustworthy, making diagnosis difficult. To overcome all these problems, we employed a unified 2D deep learning neural network for both 3D MRI super resolution and motion artifact reduction, demonstrating such a framework can achieve better performance in 3D MRI restoration task compared to other states of the art methods and remains the GPU consumption and inference time significantly low, thus easier to deploy. We also analyzed several downsampling strategies based on the acceleration factor, including multiple combinations of in-plane and through-plane downsampling, and developed a controllable and quantifiable motion artifact generation method. At last, the pixel-wise uncertainty was calculated and used to estimate the accuracy of generated image, providing additional information for reliable diagnosis.      
### 16.Projected Multi-Agent Consensus Equilibrium for Ptychographic Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2111.14240.pdf)
>  Ptychography is a computational imaging technique using multiple, overlapping, coherently illuminated snapshots to achieve nanometer resolution by solving a nonlinear phase-field recovery problem. Ptychography is vital for imaging of manufactured nanomaterials, but existing algorithms have computational shortcomings that limit large-scale application. In this paper, we present the Projected Multi-Agent Consensus Equilibrium (PMACE) approach for solving the ptychography inversion problem. This approach extends earlier work on MACE, which formulates an inversion problem as an equilibrium among multiple agents, each acting independently to update a full reconstruction. In PMACE, each agent acts on a portion (projection) corresponding to one of the snapshots, and these updates to projections are then combined to give an update to the full reconstruction. The resulting algorithm is easily parallelized, with convergence properties inherited from convergence results associated with MACE. We apply our method on simulated data and demonstrate that it outperforms competing algorithms in both reconstruction quality and convergence speed.      
### 17.Low-complexity Rounded KLT Approximation for Image Compression  [ :arrow_down: ](https://arxiv.org/pdf/2111.14239.pdf)
>  The Karhunen-Loève transform (KLT) is often used for data decorrelation and dimensionality reduction. Because its computation depends on the matrix of covariances of the input signal, the use of the KLT in real-time applications is severely constrained by the difficulty in developing fast algorithms to implement it. In this context, this paper proposes a new class of low-complexity transforms that are obtained through the application of the round function to the elements of the KLT matrix. The proposed transforms are evaluated considering figures of merit that measure the coding power and distance of the proposed approximations to the exact KLT and are also explored in image compression experiments. Fast algorithms are introduced for the proposed approximate transforms. It was shown that the proposed transforms perform well in image compression and require a low implementation cost.      
### 18.Data-independent Low-complexity KLT Approximations for Image and Video Coding  [ :arrow_down: ](https://arxiv.org/pdf/2111.14237.pdf)
>  The Karhunen-Loève transform (KLT) is often used for data decorrelation and dimensionality reduction. The KLT is able to optimally retain the signal energy in only few transform components, being mathematically suitable for image and video compression. However, in practice, because of its high computational cost and dependence on the input signal, its application in real-time scenarios is precluded. This work proposes low-computational cost approximations for the KLT. We focus on the blocklengths $N \in \{4, 8, 16, 32 \}$ because they are widely employed in image and video coding standards such as JPEG and high efficiency video coding (HEVC). Extensive computational experiments demonstrate the suitability of the proposed low-complexity transforms for image and video compression.      
### 19.Transfer Learning with Jukebox for Music Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2111.14200.pdf)
>  In this work, we demonstrate how to adapt a publicly available pre-trained Jukebox model for the problem of audio source separation from a single mixed audio channel. Our neural network architecture for transfer learning is fast to train and results demonstrate comparable performance to other state-of-the-art approaches. We provide an open-source code implementation of our architecture (<a class="link-external link-https" href="https://rebrand.ly/transfer-jukebox-github" rel="external noopener nofollow">this https URL</a>).      
### 20.Generalized Fully Coherent Closed-form Receiver Design for Joint Radar and Communication System  [ :arrow_down: ](https://arxiv.org/pdf/2111.14158.pdf)
>  In conventional radar, the transmission of the same waveform is repeated after a predefined interval of time called pulse-repetition-interval (PRI). This technique helps to estimate the range and Doppler shift of targets and suppress clutter. In dual-function radar communication (DFRC), different waveforms are transmitted after each PRI. Thus, each waveform yields different range-side-lobe (RSL) levels at the receiver's output. As a consequence, Doppler shift estimation and clutter suppression become challenging tasks. A state-of-the-art (SOTA) method claims that if the number of waveforms is more than two, it is impossible to achieve fully coherent RSL levels with both waveforms. Therefore, this algorithm uses iterative methods to achieve as much as possible coherency and minimize the RSL levels. In contrast to that SOTA method, we proposed two novel closed-form receivers for the DFRC that yield a fully coherent response for several waveforms and suppress the RSL levels. Experimental results demonstrate that the proposed receivers achieve full coherency and the RSL levels are significantly lower than the conventional method.      
### 21.Waveform Optimization for Wireless Power Transfer with Power Amplifier and Energy Harvester Non-Linearities  [ :arrow_down: ](https://arxiv.org/pdf/2111.14156.pdf)
>  Waveform optimization has recently been shown to be a key technique to boost the efficiency and range of far-field wireless power transfer (WPT). Current research has optimized transmit waveform adaptive to channel state information (CSI) and accounting for energy harvester (EH)'s non-linearity but under the assumption of linear high power amplifiers (HPA) at the transmitter. This paper proposes a channel-adaptive waveform design strategy that optimizes the transmitter's input waveform considering both HPA and EH non-linearities. Simulations demonstrate that HPA's non-linearity degrades the energy harvesting efficiency of WPT significantly, while the performance loss can be compensated by using the proposed optimal input waveform.      
### 22.Optimization of RIS Configurations for Multiple-RIS-Aided mmWave Positioning Systems based on CRLB Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2111.14023.pdf)
>  Reconfigurable intelligent surface (RIS) is a promising technology for future millimeter-wave (mmWave) communication systems. However, its potential benefits of adopting RIS for high-precision positioning in mmWave systems are still less understood. In this paper, we study a multiple-RIS-aided mmWave positioning system and derive the Cram$\rm{\acute{e}}$r-Rao error bound. Based on the derived bound, we optimize the phase shift of the RISs by the particle swarm optimization (PSO) algorithm. Numerical results have demonstrated the advantages of using multiple RISs in enhancing the positioning accuracy in mmWave systems.      
### 23.Learning A 3D-CNN and Transformer Prior for Hyperspectral Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2111.13923.pdf)
>  To solve the ill-posed problem of hyperspectral image super-resolution (HSISR), an usually method is to use the prior information of the hyperspectral images (HSIs) as a regularization term to constrain the objective function. Model-based methods using hand-crafted priors cannot fully characterize the properties of HSIs. Learning-based methods usually use a convolutional neural network (CNN) to learn the implicit priors of HSIs. However, the learning ability of CNN is limited, it only considers the spatial characteristics of the HSIs and ignores the spectral characteristics, and convolution is not effective for long-range dependency modeling. There is still a lot of room for improvement. In this paper, we propose a novel HSISR method that uses Transformer instead of CNN to learn the prior of HSIs. Specifically, we first use the proximal gradient algorithm to solve the HSISR model, and then use an unfolding network to simulate the iterative solution processes. The self-attention layer of Transformer makes it have the ability of spatial global interaction. In addition, we add 3D-CNN behind the Transformer layers to better explore the spatio-spectral correlation of HSIs. Both quantitative and visual results on two widely used HSI datasets and the real-world dataset demonstrate that the proposed method achieves a considerable gain compared to all the mainstream algorithms including the most competitive conventional methods and the recently proposed deep learning-based methods.      
### 24.AdaDM: Enabling Normalization for Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2111.13905.pdf)
>  Normalization like Batch Normalization (BN) is a milestone technique to normalize the distributions of intermediate layers in deep learning, enabling faster training and better generalization accuracy. However, in fidelity image Super-Resolution (SR), it is believed that normalization layers get rid of range flexibility by normalizing the features and they are simply removed from modern SR networks. In this paper, we study this phenomenon quantitatively and qualitatively. We found that the standard deviation of the residual feature shrinks a lot after normalization layers, which causes the performance degradation in SR networks. Standard deviation reflects the amount of variation of pixel values. When the variation becomes smaller, the edges will become less discriminative for the network to resolve. To address this problem, we propose an Adaptive Deviation Modulator (AdaDM), in which a modulation factor is adaptively predicted to amplify the pixel deviation. For better generalization performance, we apply BN in state-of-the-art SR networks with the proposed AdaDM. Meanwhile, the deviation amplification strategy in AdaDM makes the edge information in the feature more distinguishable. As a consequence, SR networks with BN and our AdaDM can get substantial performance improvements on benchmark datasets. Extensive experiments have been conducted to show the effectiveness of our method.      
### 25.Resource Allocation in Laser-based Optical Wireless Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.13899.pdf)
>  Optical wireless communication provides data transmission at high speeds which can satisfy the increasing demands <br>for connecting a massive number of devices to the Internet. In this paper, vertical-cavity surface-emitting(VCSEL) lasers are used as transmitters due to their high modulation speed and energy efficiency. However, a high number of VCSEL lasers is required to ensure coverage where each laser source illuminates a confined area. Therefore, multiple users are classified into different sets according to their connectivity. Given this point, a transmission scheme that uses blind interference alignment (BIA) is implemented to manage the interference in the laser-based network. In addition, an optimization problem is formulated to maximize the utility sum rate taking into consideration the classification of the users. To solve this problem, a decentralized algorithm is proposed where the main problem is divided into sub-problems, each can be solved independently avoiding complexity. The results demonstrate the optimality of the decentralized algorithm where a sub-optimal solution is provided. Finally, it is shown that BIA can provide high performance in laser-based networks compared with zero forcing (ZF) transmit precoding scheme.      
### 26.Artificial Neural Network for Resource Allocation in Laser-based Optical wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.13898.pdf)
>  Optical wireless communication offers unprecedented communication speeds that can support the massive use of the Internet on a daily basis. In indoor environments, optical wireless networks are usually multi-user multiple-input multiple-output (MU-MIMO) systems, where a high number of optical access points (APs) is required to ensure coverage. In this work, a laser-based optical wireless network is considered for serving multiple users. Moreover, blind inference alignment (BIA) is implemented to achieve a high degree of freedom (DoF) without the need for channel state information (CSI) at transmitters, which is difficult to provide in such wireless networks. Then, an objective function is defined to allocate the resources of the network taking into consideration the requirements of users and the available resources. This optimization problem can be solved through exhaustive search or distributed algorithms. However, a practical algorithm that provides immediate solutions in real time scenarios is required. In this context, an artificial neural network (ANN) model is derived in order to obtain a sub-optimal solution with low computational time. The implementation of the ANN model involves three important steps, dataset generation, offline training, and real time application. The results show that the trained ANN model provides a significant solution close to the optimal one.      
### 27.Analogue Radio over Fiber aided Multi-service Communications for High Speed Trains  [ :arrow_down: ](https://arxiv.org/pdf/2111.13852.pdf)
>  High speed trains (HST) have gradually become an essential means of transportation, where given our digital world, it is expected that passengers will be connected all the time. More specifically, the on-board passengers require fast mobile connections, which cannot be provided by the currently implemented cellular networks. Hence, in this article, we propose an analogue radio over fiber (A-RoF) aided multi-service network architecture for high-speed trains, in order to enhance the quality of service as well as reduce the cost of the radio access network (RAN). The proposed design can simultaneously support sub- 6GHz as well as milimeter wave (mmWave) communications using the same architecture. Explicitly, we design a photonics aided beamforming technique in order to eliminate the bulky high-speed electronic phase-shifters and the hostile broadband mmWave mixers while providing a low-cost RAN solution. Finally, a beamforming range of 180 is demonstrated with a high resolution using our proposed system.      
### 28.Analogue Radio Over Fiber for Next-Generation RAN: Challenges and Opportunities  [ :arrow_down: ](https://arxiv.org/pdf/2111.13851.pdf)
>  The radio access network (RAN) connects the users to the core networks, where typically digitised radio over fiber (D-RoF) links are employed. The data rate of the RAN is limited by the hardware constraints of the D-RoF-based backhaul and fronthaul. In order to break this bottleneck, the potential of the analogue radio over fiber (A-RoF) based RAN techniques are critically appraised for employment in the next-generation systems, where increased-rate massive multiple-input-multiple-output (massive-MIMO) and millimeter wave (mmWave) techniques will be implemented. We demonstrate that huge bandwidth and power-consumption cost benefits may accrue upon using A-RoF for next-generation RANs. We provide an overview of the recent A-RoF research and a performance comparison of A-RoF and D-RoF, concluding with further insights on the future potential of A-RoF.      
### 29.Robust Finite-Time Adaptation Law for Safety-Critical Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.13849.pdf)
>  The process of parameter adaptation and identification relies on the excitation of the measurable signals. To ensure at least interval excitation, designed control or noise input is often introduced. The main concern in this paper is the safety of adaptation process, i.e. the empirically chosen noise input may lead system state into unsafe region. First, a safe adaptation framework is presented to achieve both finite-time parameter identification and guaranteed safety. The element-wise parameter estimation is obtained using finite-time dynamic regressor extension and mixing (FT-DREM), by which a worst-case estimation error can also be computed. In this situation, a point-wise safe strategy based on adaptive control barrier function (aCBF) is used to filter the designed control or noise input. Some criteria are proposed to reduce the conservativeness of aCBF method. The robustness of the proposed algorithms is analyzed and a robust algorithm for safe adaptation is also presented. In the end, a simulation experiment of adaptive cruise control with estimation for slope resistance is studied to illustrate the effectiveness of the proposed methods.      
### 30.Optimal Tracking Control for Unknown Linear Systems with Finite-Time Parameter Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2111.13848.pdf)
>  The optimal control input for linear systems can be solved from algebraic Riccati equation (ARE), from which it remains questionable to get the form of the exact solution. In engineering, the acceptable numerical solutions of ARE can be found by iteration or optimization. Recently, the gradient descent based numerical solutions has been proven effective to approximate the optimal ones. This paper introduces this method to tracking problem for heterogeneous linear systems. Differently, the parameters in the dynamics of the linear systems are all assumed to be unknown, which is intractable since the gradient as well as the allowable initialization needs the prior knowledge of system dynamics. To solve this problem, the method named dynamic regressor extension and mix (DREM) is improved to estimate the parameter matrices in finite time. Besides, a discounted factor is introduced to ensure the existence of optimal solutions for heterogeneous systems. Two simulation experiments are given to illustrate the effectiveness.      
### 31.Resource Allocation for IRS-Enabled Secure Multiuser Multi-Carrier Downlink URLLC Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.13847.pdf)
>  Secure ultra-reliable low-latency communication (URLLC) has been recently investigated with the fundamental limits of finite block length (FBL) regime in mind. Analysis has revealed that when eavesdroppers outnumber BS antennas or enjoy a more favorable channel condition compared to the legitimate users, base station (BS) transmit power should increase exorbitantly to meet quality of service (QoS) constraints. Channel-induced impairments such as shadowing and/or blockage pose a similar challenge. These practical considerations can drastically limit secure URLLC performance in FBL regime. Deployment of an intelligent reflecting surface (IRS) can endow such systems with much-needed resiliency and robustness to satisfy stringent latency, availability, and reliability requirements. We address this problem and propose a joint design of IRS platform and secure URLLC network. We minimize the total BS transmit power by simultaneously designing the beamformers and artificial noise at the BS and phase-shifts at the IRS, while guaranteeing the required number of securely transmitted bits with the desired packet error probability, information leakage, and maximum affordable delay. The proposed optimization problem is non-convex and we apply block coordinate descent and successive convex approximation to iteratively solve a series of convex sub-problems instead. The proposed algorithm converges to a sub-optimal solution in a few iterations and attains substantial power saving and robustness compared to baseline schemes.      
### 32.Online Speaker Diarization with Graph-based Label Generation  [ :arrow_down: ](https://arxiv.org/pdf/2111.13803.pdf)
>  This paper introduces an online speaker diarization system that can handle long-time audio with low latency. First, a new variant of agglomerative hierarchy clustering is built to cluster the speakers in an online fashion. Then, a speaker embedding graph is proposed. We use this graph to exploit a graph-based reclustering method to further improve the performance. Finally, a label matching algorithm is introduced to generate consistent speaker labels, and we evaluate our system on both DIHARD3 and VoxConverse datasets, which contain long audios with various kinds of scenarios. The experimental results show that our online diarization system outperforms the baseline offline system and has comparable performance to our offline system.      
### 33.Towards Terabit LiFi Networking  [ :arrow_down: ](https://arxiv.org/pdf/2111.13784.pdf)
>  Light Fidelity (Li-Fi) is a networked version of optical wireless communication (OWC), which is a strong candidate to fulfill the unprecedented increase in user-traffic expected in the near future. In OWC, a high number of optical access points (APs) is usually deployed on the ceiling of an indoor environment to serve multiple users with different demands. Despite the high data rates of OWC networks, due to the use of the optical band for data transmission, they cannot replace current radio frequency (RF) wireless networks where OWC has several issues including the small converge area of an optical AP, the lack of uplink transmission and high blockage probabilities. However, OWC has the potential to support the requirements in the next generation (6G) of wireless communications. In this context, heterogeneous optical/RF networks can be considered to overcome the limitations of OWC and RF systems, while providing a high quality of service in terms of achievable data rates and coverage. In this work, infrared lasers, vertical-cavity surface-emitting(VCSEL) lasers, are used as the key elements of optical APs for serving multiple users. Then, transmission schemes such as zero forcing (ZF) and blind interference alignment (BIA) are introduced to manage multi-user interference and maximize the sum rate of users. Moreover, a WiFi system is considered to provide uplink transmission and serve users that experience a low signal to noise ratio (SNR) from the optical system. To use the resources of the heterogeneous optical/RF network efficiently, we derive a utility-based objective function that aims to maximize the overall sum rate of the network.      
### 34.Machine Learning for Real-Time, Automatic, and Early Diagnosis of Parkinson's Disease by Extracting Signs of Micrographia from Handwriting Images  [ :arrow_down: ](https://arxiv.org/pdf/2111.14781.pdf)
>  Parkinson's disease (PD) is debilitating, progressive, and clinically marked by motor symptoms. As the second most common neurodegenerative disease in the world, it affects over 10 million lives globally. Existing diagnoses methods have limitations, such as the expense of visiting doctors and the challenge of automated early detection, considering that behavioral differences in patients and healthy individuals are often indistinguishable in the early stages. However, micrographia, a handwriting disorder that leads to abnormally small handwriting, tremors, dystonia, and slow movement in the hands and fingers, is commonly observed in the early stages of PD. In this work, we apply machine learning techniques to extract signs of micrographia from drawing samples gathered from two open-source datasets and achieve a predictive accuracy of 94%. This work also sets the foundations for a publicly available and user-friendly web portal that anyone with access to a pen, printer, and phone can use for early PD detection.      
### 35.Mesarovician Abstract Learning Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.14766.pdf)
>  The solution methods used to realize artificial general intelligence (AGI) may not contain the formalism needed to adequately model and characterize AGI. In particular, current approaches to learning hold notions of problem domain and problem task as fundamental precepts, but it is hardly apparent that an AGI encountered in the wild will be discernable into a set of domain-task pairings. Nor is it apparent that the outcomes of AGI in a system can be well expressed in terms of domain and task, or as consequences thereof. Thus, there is both a practical and theoretical use for meta-theories of learning which do not express themselves explicitly in terms of solution methods. General systems theory offers such a meta-theory. Herein, Mesarovician abstract systems theory is used as a super-structure for learning. Abstract learning systems are formulated. Subsequent elaboration stratifies the assumptions of learning systems into a hierarchy and considers the hierarchy such stratification projects onto learning theory. The presented Mesarovician abstract learning systems theory calls back to the founding motivations of artificial intelligence research by focusing on the thinking participants directly, in this case, learning systems, in contrast to the contemporary focus on the problems thinking participants solve.      
### 36.Optimal and $H_\infty$ Control of Stochastic Reaction Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.14754.pdf)
>  Stochastic reaction networks is a powerful class of models for the representation a wide variety of population models including biochemistry. The control of such networks has been recently considered due to their important implications for the control of biological systems. Their optimal control, however, has been relatively few studied until now. The continuous-time finite-horizon optimal control problem is formulated first and explicitly solved in the case of unimolecular reaction networks. The problems of the optimal sampled-data control, the continuous $H_\infty$ control, and the sampled-data $H_\infty$ control of such networks are then addressed next.      
### 37.ESPnet-SLU: Advancing Spoken Language Understanding through ESPnet  [ :arrow_down: ](https://arxiv.org/pdf/2111.14706.pdf)
>  As Automatic Speech Processing (ASR) systems are getting better, there is an increasing interest of using the ASR output to do downstream Natural Language Processing (NLP) tasks. However, there are few open source toolkits that can be used to generate reproducible results on different Spoken Language Understanding (SLU) benchmarks. Hence, there is a need to build an open source standard that can be used to have a faster start into SLU research. We present ESPnet-SLU, which is designed for quick development of spoken language understanding in a single framework. ESPnet-SLU is a project inside end-to-end speech processing toolkit, ESPnet, which is a widely used open-source standard for various speech processing tasks like ASR, Text to Speech (TTS) and Speech Translation (ST). We enhance the toolkit to provide implementations for various SLU benchmarks that enable researchers to seamlessly mix-and-match different ASR and NLU models. We also provide pretrained models with intensively tuned hyper-parameters that can match or even outperform the current state-of-the-art performances. The toolkit is publicly available at <a class="link-external link-https" href="https://github.com/espnet/espnet" rel="external noopener nofollow">this https URL</a>.      
### 38.Graph Embedding via High Dimensional Model Representation for Hyperspectral Images  [ :arrow_down: ](https://arxiv.org/pdf/2111.14680.pdf)
>  Learning the manifold structure of remote sensing images is of paramount relevance for modeling and understanding processes, as well as to encapsulate the high dimensionality in a reduced set of informative features for subsequent classification, regression, or unmixing. Manifold learning methods have shown excellent performance to deal with hyperspectral image (HSI) analysis but, unless specifically designed, they cannot provide an explicit embedding map readily applicable to out-of-sample data. A common assumption to deal with the problem is that the transformation between the high-dimensional input space and the (typically low) latent space is linear. This is a particularly strong assumption, especially when dealing with hyperspectral images due to the well-known nonlinear nature of the data. To address this problem, a manifold learning method based on High Dimensional Model Representation (HDMR) is proposed, which enables to present a nonlinear embedding function to project out-of-sample samples into the latent space. The proposed method is compared to manifold learning methods along with its linear counterparts and achieves promising performance in terms of classification accuracy of a representative set of hyperspectral images.      
### 39.MedRDF: A Robust and Retrain-Less Diagnostic Framework for Medical Pretrained Models Against Adversarial Attack  [ :arrow_down: ](https://arxiv.org/pdf/2111.14564.pdf)
>  Deep neural networks are discovered to be non-robust when attacked by imperceptible adversarial examples, which is dangerous for it applied into medical diagnostic system that requires high reliability. However, the defense methods that have good effect in natural images may not be suitable for medical diagnostic tasks. The preprocessing methods (e.g., random resizing, compression) may lead to the loss of the small lesions feature in the medical image. Retraining the network on the augmented data set is also not practical for medical models that have already been deployed online. Accordingly, it is necessary to design an easy-to-deploy and effective defense framework for medical diagnostic tasks. In this paper, we propose a Robust and Retrain-Less Diagnostic Framework for Medical pretrained models against adversarial attack (i.e., MedRDF). It acts on the inference time of the pertained medical model. Specifically, for each test image, MedRDF firstly creates a large number of noisy copies of it, and obtains the output labels of these copies from the pretrained medical diagnostic model. Then, based on the labels of these copies, MedRDF outputs the final robust diagnostic result by majority voting. In addition to the diagnostic result, MedRDF produces the Robust Metric (RM) as the confidence of the result. Therefore, it is convenient and reliable to utilize MedRDF to convert pre-trained non-robust diagnostic models into robust ones. The experimental results on COVID-19 and DermaMNIST datasets verify the effectiveness of our MedRDF in improving the robustness of medical diagnostic models.      
### 40.BP-Net: Efficient Deep Learning for Continuous Arterial Blood Pressure Estimation using Photoplethysmogram  [ :arrow_down: ](https://arxiv.org/pdf/2111.14558.pdf)
>  Blood pressure (BP) is one of the most influential bio-markers for cardiovascular diseases and stroke; therefore, it needs to be regularly monitored to diagnose and prevent any advent of medical complications. Current cuffless approaches to continuous BP monitoring, though non-invasive and unobtrusive, involve explicit feature engineering surrounding fingertip Photoplethysmogram (PPG) signals. To circumvent this, we present an end-to-end deep learning solution, BP-Net, that uses PPG waveform to estimate Systolic BP (SBP), Mean Average Pressure (MAP), and Diastolic BP (DBP) through intermediate continuous Arterial BP (ABP) waveform. Under the terms of the British Hypertension Society (BHS) standard, BP-Net achieves Grade A for DBP and MAP estimation and Grade B for SBP estimation. BP-Net also satisfies Advancement of Medical Instrumentation (AAMI) criteria for DBP and MAP estimation and achieves Mean Absolute Error (MAE) of 5.16 mmHg and 2.89 mmHg for SBP and DBP, respectively. Further, we establish the ubiquitous potential of our approach by deploying BP-Net on a Raspberry Pi 4 device and achieve 4.25 ms inference time for our model to translate the PPG waveform to ABP waveform.      
### 41.Out-of-Band Distortion in Massive MIMO: What to expect under realistic conditions?  [ :arrow_down: ](https://arxiv.org/pdf/2111.14548.pdf)
>  Massive Multiple Input Multiple Output (MIMO) offers superior capacity for future networks. In the quest for energy efficient implementation of these large array-based trans-mission systems, the power consumption of the Power Amplifiers(PAs) is a main bottleneck. This paper investigates whether it is possible to operate the PAs in their efficient nonlinear region, as the Out-of-Band (OOB) distortion may not get the same array gain as the in-band (IB) signals. We present a framework to simulate the effects under realistic conditions, leveraging on an accurate Ray-Tracing Simulator (RTS). The results show that the often assumed i.i.d. Rayleigh fading channel model results in too optimistic predictions, also in Non Line of Sight (NLoS) multi-path scenarios, regarding the spatial distribution of OOB emissions. We further comment on the consequences in view of current regulatory constraints.      
### 42.Just Least Squares: Binary Compressive Sampling with Low Generative Intrinsic Dimension  [ :arrow_down: ](https://arxiv.org/pdf/2111.14486.pdf)
>  In this paper, we consider recovering $n$ dimensional signals from $m$ binary measurements corrupted by noises and sign flips under the assumption that the target signals have low generative intrinsic dimension, i.e., the target signals can be approximately generated via an $L$-Lipschitz generator $G: \mathbb{R}^k\rightarrow\mathbb{R}^{n}, k\ll n$. Although the binary measurements model is highly nonlinear, we propose a least square decoder and prove that, up to a constant $c$, with high probability, the least square decoder achieves a sharp estimation error $\mathcal{O} (\sqrt{\frac{k\log (Ln)}{m}})$ as long as $m\geq \mathcal{O}( k\log (Ln))$. Extensive numerical simulations and comparisons with state-of-the-art methods demonstrated the least square decoder is robust to noise and sign flips, as indicated by our theory. By constructing a ReLU network with properly chosen depth and width, we verify the (approximately) deep generative prior, which is of independent interest.      
### 43.Mixed Precision DNN Qunatization for Overlapped Speech Separation and Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2111.14479.pdf)
>  Recognition of overlapped speech has been a highly challenging task to date. State-of-the-art multi-channel speech separation system are becoming increasingly complex and expensive for practical applications. To this end, low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. However, current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different model components to quantization errors. In this paper, novel mixed precision DNN quantization methods are proposed by applying locally variable bit-widths to individual TCN components of a TF masking based multi-channel speech separation system. The optimal local precision settings are automatically learned using three techniques. The first two approaches utilize quantization sensitivity metrics based on either the mean square error (MSE) loss function curvature, or the KL-divergence measured between full precision and quantized separation models. The third approach is based on mixed precision neural architecture search. Experiments conducted on the LRS3-TED corpus simulated overlapped speech data suggest that the proposed mixed precision quantization techniques consistently outperform the uniform precision baseline speech separation systems of comparable bit-widths in terms of SI-SNR and PESQ scores as well as word error rate (WER) reductions up to 2.88% absolute (8% relative).      
### 44.Decoupled Low-light Image Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2111.14458.pdf)
>  The visual quality of photographs taken under imperfect lightness conditions can be degenerated by multiple factors, e.g., low lightness, imaging noise, color distortion and so on. Current low-light image enhancement models focus on the improvement of low lightness only, or simply deal with all the degeneration factors as a whole, therefore leading to a sub-optimal performance. In this paper, we propose to decouple the enhancement model into two sequential stages. The first stage focuses on improving the scene visibility based on a pixel-wise non-linear mapping. The second stage focuses on improving the appearance fidelity by suppressing the rest degeneration factors. The decoupled model facilitates the enhancement in two aspects. On the one hand, the whole low-light enhancement can be divided into two easier subtasks. The first one only aims to enhance the visibility. It also helps to bridge the large intensity gap between the low-light and normal-light images. In this way, the second subtask can be shaped as the local appearance adjustment. On the other hand, since the parameter matrix learned from the first stage is aware of the lightness distribution and the scene structure, it can be incorporated into the second stage as the complementary information. In the experiments, our model demonstrates the state-of-the-art performance in both qualitative and quantitative comparisons, compared with other low-light image enhancement models. In addition, the ablation studies also validate the effectiveness of our model in multiple aspects, such as model structure and loss function. The trained model is available at <a class="link-external link-https" href="https://github.com/hanxuhfut/Decoupled-Low-light-Image-Enhancement" rel="external noopener nofollow">this https URL</a>.      
### 45.AVA-AVD: Audio-visual Speaker Diarization in the Wild  [ :arrow_down: ](https://arxiv.org/pdf/2111.14448.pdf)
>  Audio-visual speaker diarization aims at detecting ``who spoken when`` using both auditory and visual signals. Existing audio-visual diarization datasets are mainly focused on indoor environments like meeting rooms or news studios, which are quite different from in-the-wild videos in many scenarios such as movies, documentaries, and audience sitcoms. To create a testbed that can effectively compare diarization methods on videos in the wild, we annotate the speaker diarization labels on the AVA movie dataset and create a new benchmark called AVA-AVD. This benchmark is challenging due to the diverse scenes, complicated acoustic conditions, and completely off-screen speakers. Yet, how to deal with off-screen and on-screen speakers together still remains a critical challenge. To overcome it, we propose a novel Audio-Visual Relation Network (AVR-Net) which introduces an effective modality mask to capture discriminative information based on visibility. Experiments have shown that our method not only can outperform state-of-the-art methods but also is more robust as varying the ratio of off-screen speakers. Ablation studies demonstrate the advantages of the proposed AVR-Net and especially the modality mask on diarization. Our data and code will be made publicly available.      
### 46.Responding to Challenge Call of Machine Learning Model Development in Diagnosing Respiratory Disease Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2111.14354.pdf)
>  In this study, a machine learning model was developed for automatically detecting respiratory system sounds such as sneezing and coughing in disease diagnosis. The automatic model and approach development of breath sounds, which carry valuable information, results in early diagnosis and treatment. A successful machine learning model was developed in this study, which was a strong response to the challenge called the "Pfizer digital medicine challenge" on the "OSFHOME" open access platform. "Environmental sound classification" called ESC-50 and AudioSet sound files were used to prepare the dataset. In this dataset, which consisted of three parts, features that effectively showed coughing and sneezing sound analysis were extracted from training, testing and validating samples. Based on the Mel frequency cepstral coefficients (MFCC) feature extraction method, mathematical and statistical features were prepared. Three different classification techniques were considered to perform successful respiratory sound classification in the dataset containing more than 3800 different sounds. Support vector machine (SVM) with radial basis function (RBF) kernels, ensemble aggregation and decision tree classification methods were used as classification techniques. In an attempt to classify coughing and sneezing sounds from other sounds, SVM with RBF kernels was achieved with 83% success.      
### 47.Pessimistic Model Selection for Offline Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.14346.pdf)
>  Deep Reinforcement Learning (DRL) has demonstrated great potentials in solving sequential decision making problems in many applications. Despite its promising performance, practical gaps exist when deploying DRL in real-world scenarios. One main barrier is the over-fitting issue that leads to poor generalizability of the policy learned by DRL. In particular, for offline DRL with observational data, model selection is a challenging task as there is no ground truth available for performance demonstration, in contrast with the online setting with simulated environments. In this work, we propose a pessimistic model selection (PMS) approach for offline DRL with a theoretical guarantee, which features a provably effective framework for finding the best policy among a set of candidate models. Two refined approaches are also proposed to address the potential bias of DRL model in identifying the optimal policy. Numerical studies demonstrated the superior performance of our approach over existing methods.      
### 48.Data Augmentation For Medical MR Image Using Generative Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.14297.pdf)
>  Computer-assisted diagnosis (CAD) based on deep learning has become a crucial diagnostic technology in the medical industry, effectively improving diagnosis accuracy. However, the scarcity of brain tumor Magnetic Resonance (MR) image datasets causes the low performance of deep learning algorithms. The distribution of transformed images generated by traditional data augmentation (DA) intrinsically resembles the original ones, resulting in a limited performance in terms of generalization ability. This work improves Progressive Growing of GANs with a structural similarity loss function (PGGAN-SSIM) to solve image blurriness problems and model collapse. We also explore other GAN-based data augmentation to demonstrate the effectiveness of the proposed model. Our results show that PGGAN-SSIM successfully generates 256x256 realistic brain tumor MR images which fill the real image distribution uncovered by the original dataset. Furthermore, PGGAN-SSIM exceeds other GAN-based methods, achieving promising performance improvement in Frechet Inception Distance (FID) and Multi-scale Structural Similarity (MS-SSIM).      
### 49.Optimal Multi-Robot Motion Planning via Parabolic Relaxation  [ :arrow_down: ](https://arxiv.org/pdf/2111.14268.pdf)
>  Multi-robot systems offer enhanced capability over their monolithic counterparts, but they come at a cost of increased complexity in coordination. To reduce complexity and to make the problem tractable, multi-robot motion planning (MRMP) methods in the literature adopt de-coupled approaches that sacrifice either optimality or dynamic feasibility. In this paper, we present a convexification method, namely "parabolic relaxation", to generate optimal and dynamically feasible trajectories for MRMP in the coupled joint-space of all robots. We leverage upon the proposed relaxation to tackle the problem complexity and to attain computational tractability for planning over one hundred robots in extremely clustered environments. We take a multi-stage optimization approach that consists of i) mathematically formulating MRMP as a non-convex optimization, ii) lifting the problem into a higher dimensional space, iii) convexifying the problem through the proposed computationally efficient parabolic relaxation, and iv) penalizing with iterative search to ensure feasibility and recovery of feasible and near-optimal solutions to the original problem. Our numerical experiments demonstrate that the proposed approach is capable of generating optimal and dynamically feasible trajectories for challenging motion planning problems with higher success rate than the state-of-the-art, yet remain computationally tractable for over one hundred robots in a highly dense environment.      
### 50.On the Robustness and Generalization of Deep Learning Driven Full Waveform Inversion  [ :arrow_down: ](https://arxiv.org/pdf/2111.14220.pdf)
>  The data-driven approach has been demonstrated as a promising technique to solve complicated scientific problems. Full Waveform Inversion (FWI) is commonly epitomized as an image-to-image translation task, which motivates the use of deep neural networks as an end-to-end solution. Despite being trained with synthetic data, the deep learning-driven FWI is expected to perform well when evaluated with sufficient real-world data. In this paper, we study such properties by asking: how robust are these deep neural networks and how do they generalize? For robustness, we prove the upper bounds of the deviation between the predictions from clean and noisy data. Moreover, we demonstrate an interplay between the noise level and the additional gain of loss. For generalization, we prove a norm-based generalization error upper bound via a stability-generalization framework. Experimental results on seismic FWI datasets corroborate with the theoretical results, shedding light on a better understanding of utilizing Deep Learning for complicated scientific applications.      
### 51.How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey  [ :arrow_down: ](https://arxiv.org/pdf/2111.14203.pdf)
>  Deepfake is content or material that is synthetically generated or manipulated using artificial intelligence (AI) methods, to be passed off as real and can include audio, video, image, and text synthesis. This survey has been conducted with a different perspective compared to existing survey papers, that mostly focus on just video and image deepfakes. This survey not only evaluates generation and detection methods in the different deepfake categories, but mainly focuses on audio deepfakes that are overlooked in most of the existing surveys. This paper critically analyzes and provides a unique source of audio deepfake research, mostly ranging from 2016 to 2020. To the best of our knowledge, this is the first survey focusing on audio deepfakes in English. This survey provides readers with a summary of 1) different deepfake categories 2) how they could be created and detected 3) the most recent trends in this domain and shortcomings in detection methods 4) audio deepfakes, how they are created and detected in more detail which is the main focus of this paper. We found that Generative Adversarial Networks(GAN), Convolutional Neural Networks (CNN), and Deep Neural Networks (DNN) are common ways of creating and detecting deepfakes. In our evaluation of over 140 methods we found that the majority of the focus is on video deepfakes and in particular in the generation of video deepfakes. We found that for text deepfakes there are more generation methods but very few robust methods for detection, including fake news detection, which has become a controversial area of research because of the potential of heavy overlaps with human generation of fake content. This paper is an abbreviated version of the full survey and reveals a clear need to research audio deepfakes and particularly detection of audio deepfakes.      
### 52.On data-driven control: informativity of noisy input-output data with cross-covariance bounds  [ :arrow_down: ](https://arxiv.org/pdf/2111.14193.pdf)
>  In this paper we develop new data informativity based controller synthesis methods that extend existing frameworks in two relevant directions: a more general noise characterization in terms of cross-covariance bounds and informativity conditions for control based on input-output data. Previous works have derived necessary and sufficient informativity conditions for noisy input-state data with quadratic noise bounds via an S-procedure. We develop sufficient conditions for informativity of input-output data for stability, $\mathcal{H}_\infty$ and $\mathcal{H}_2$ control, based on this S-procedure, which are also necessary for input-state data. Simulation experiments illustrate that cross-covariance bounds can be less conservative for informativity, compared to norm bounds typically employed in the literature.      
### 53.UAV-based Crowd Surveillance in Post COVID-19 Era  [ :arrow_down: ](https://arxiv.org/pdf/2111.14176.pdf)
>  To cope with the current pandemic situation and reinstate pseudo-normal daily life, several measures have been deployed and maintained, such as mask wearing, social distancing, hands sanitizing, etc. Since outdoor cultural events, concerts, and picnics, are gradually allowed, a close monitoring of the crowd activity is needed to avoid undesired contact and disease transmission. In this context, intelligent unmanned aerial vehicles (UAVs) can be occasionally deployed to ensure the surveillance of these activities, that health restriction measures are applied, and to trigger alerts when the latter are not respected. Consequently, we propose in this paper a complete UAV framework for intelligent monitoring of post COVID-19 outdoor activities. Specifically, we propose a three steps approach. In the first step, captured images by a UAV are analyzed using machine learning to detect and locate individuals. The second step consists of a novel coordinates mapping approach to evaluate distances among individuals, then cluster them, while the third step provides an energy-efficient and/or reliable UAV trajectory to inspect clusters for restrictions violation such as mask wearing. Obtained results provide the following insights: 1) Efficient detection of individuals depends on the angle from which the image was captured, 2) coordinates mapping is very sensitive to the estimation error in individuals' bounding boxes, and 3) UAV trajectory design algorithm 2-Opt is recommended for practical real-time deployments due to its low-complexity and near-optimal performance.      
### 54.Learning Physical Concepts in Cyber-Physical Systems: A Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2111.14151.pdf)
>  Machine Learning (ML) has achieved great successes in recent decades, both in research and in practice. In Cyber-Physical Systems (CPS), ML can for example be used to optimize systems, to detect anomalies or to identify root causes of system failures. However, existing algorithms suffer from two major drawbacks: (i) They are hard to interpret by human experts. (ii) Transferring results from one systems to another (similar) system is often a challenge. Concept learning, or Representation Learning (RepL), is a solution to both of these drawbacks; mimicking the human solution approach to explain-ability and transfer-ability: By learning general concepts such as physical quantities or system states, the model becomes interpretable by humans. Furthermore concepts on this abstract level can normally be applied to a wide range of different systems. Modern ML methods are already widely used in CPS, but concept learning and transfer learning are hardly used so far. In this paper, we provide an overview of the current state of research regarding methods for learning physical concepts in time series data, which is the primary form of sensor data of CPS. We also analyze the most important methods from the current state of the art using the example of a three-tank system. Based on these concrete implementations1, we discuss the advantages and disadvantages of the methods and show for which purpose and under which conditions they can be used.      
### 55.Throughput Maximization for IRS-Aided MIMO FD-WPCN with Non-Linear EH Model  [ :arrow_down: ](https://arxiv.org/pdf/2111.14132.pdf)
>  This paper studies an intelligent reflecting surface (IRS)-aided multiple-input-multiple-output (MIMO) full-duplex (FD) wireless-powered communication network (WPCN), where a hybrid access point (HAP) operating in FD broadcasts energy signals to multiple devices for their energy harvesting (EH) in the downlink (DL) and meanwhile receives information signals from devices in the uplink (UL) with the help of an IRS. Taking into account the practical finite self-interference (SI) and the non-linear EH model, we formulate the weighted sum throughput maximization optimization problem by jointly optimizing DL/UL time allocation, precoding matrices at devices, transmit covariance matrices at the HAP, and phase shifts at the IRS. Since the resulting optimization problem is non-convex, there are no standard methods to solve it optimally in general. To tackle this challenge, we first propose an element-wise (EW) based algorithm, where each IRS phase shift is alternately optimized in an iterative manner. To reduce the computational complexity, a minimum mean-square error (MMSE) based algorithm is proposed, where we transform the original problem into an equivalent form based on the MMSE method, which facilities the design of an efficient iterative algorithm. In particular, the IRS phase shift optimization problem is recast as an second-order cone program (SOCP), where all the IRS phase shifts are simultaneously optimized. For comparison, we also study two suboptimal IRS beamforming configurations in simulations, namely partially dynamic IRS beamforming (PDBF) and static IRS beamforming (SBF), which strike a balance between the system performance and practical complexity.      
### 56.Rate-Splitting Multiple Access for Satellite-Terrestrial Integrated Networks:Benefits of Coordination and Cooperation  [ :arrow_down: ](https://arxiv.org/pdf/2111.14074.pdf)
>  This work studies the joint beamforming design problem of achieving max-min rate fairness in a satellite-terrestrial integrated network (STIN) where the satellite provides wide coverage to multibeam multicast satellite users (SUs), and the terrestrial base station (BS) serves multiple cellular users (CUs) in a densely populated area. Both the satellite and BS operate in the same frequency band. Since rate-splitting multiple access (RSMA) has recently emerged as a promising strategy for non-orthogonal transmission and robust interference management in multi-antenna wireless networks, we present two RSMA-based STIN schemes, namely the coordinated scheme relying on channel state information (CSI) sharing and the cooperative scheme relying on CSI and data sharing. Our objective is to maximize the minimum fairness rate amongst all SUs and CUs subject to transmit power constraints at the satellite and the BS. A joint beamforming algorithm is proposed to reformulate the original problem into an approximately equivalent convex one which can be iteratively solved. Moreover, an expectation-based robust joint beamforming algorithm is proposed against the practical environment when satellite channel phase uncertainties are considered. Simulation results demonstrate the effectiveness and robustness of our proposed RSMA schemes for STIN, and exhibit significant performance gains compared with various traditional transmission strategies.      
### 57.Joint Sensing and Communication for Situational Awareness in Wireless THz Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.14044.pdf)
>  Next-generation wireless systems are rapidly evolving from communication-only systems to multi-modal systems with integrated sensing and communications. In this paper a novel joint sensing and communication framework is proposed for enabling wireless extended reality (XR) at terahertz (THz) bands. To gather rich sensing information and a higher line-of-sight (LoS) availability, THz-operated reconfigurable intelligent surfaces (RISs) acting as base stations are deployed. The sensing parameters are extracted by leveraging THz's quasi-opticality and opportunistically utilizing uplink communication waveforms. This enables the use of the same waveform, spectrum, and hardware for both sensing and communication purposes. The environmental sensing parameters are then derived by exploiting the sparsity of THz channels via tensor decomposition. Hence, a high-resolution indoor mapping is derived so as to characterize the spatial availability of communications and the mobility of users. Simulation results show that in the proposed framework, the resolution and data rate of the overall system are positively correlated, thus allowing a joint optimization between these metrics with no tradeoffs. Results also show that the proposed framework improves the system reliability in static and mobile systems. In particular, the highest reliability gains of 10% in reliability are achieved in a walking speed mobile environment compared to communication only systems with beam tracking.      
### 58.Cell-Free Massive MIMO Detection: A Distributed Expectation Propagation Approach  [ :arrow_down: ](https://arxiv.org/pdf/2111.14022.pdf)
>  Cell-free massive MIMO is one of the core technologies for future wireless networks. It is expected to bring enormous benefits, including ultra-high reliability, data throughput, energy efficiency, and uniform coverage. As a radically distributed system, the performance of cell-free massive MIMO critically relies on efficient distributed processing algorithms. In this paper, we propose a distributed expectation propagation (EP) detector for cell-free massive MIMO, which consists of two modules: a nonlinear module at the central processing unit (CPU) and a linear module at each access point (AP). The turbo principle in iterative channel decoding is utilized to compute and pass the extrinsic information between the two modules. An analytical framework is provided to characterize the asymptotic performance of the proposed EP detector with a large number of antennas. Furthermore, a distributed joint channel estimation and data detection (JCD) algorithm is developed to handle the practical setting with imperfect channel state information (CSI). Simulation results will show that the proposed method outperforms existing detectors for cell-free massive MIMO systems in terms of the bit-error rate and demonstrate that the developed theoretical analysis accurately predicts system performance. Finally, it is shown that with imperfect CSI, the proposed JCD algorithm improves the system performance significantly and enables non-orthogonal pilots to reduce the pilot overhead.      
### 59.Multi-modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics  [ :arrow_down: ](https://arxiv.org/pdf/2111.13987.pdf)
>  The availability of multi-modality datasets provides a unique opportunity to characterize the same object of interest using multiple viewpoints more comprehensively. In this work, we investigate the use of canonical correlation analysis (CCA) and penalized variants of CCA (pCCA) for the fusion of two modalities. We study a simple graphical model for the generation of two-modality data. We analytically show that, with known model parameters, posterior mean estimators that jointly use both modalities outperform arbitrary linear mixing of single modality posterior estimators in latent variable prediction. Penalized extensions of CCA (pCCA) that incorporate domain knowledge can discover correlations with high-dimensional, low-sample data, whereas traditional CCA is inapplicable. To facilitate the generation of multi-dimensional embeddings with pCCA, we propose two matrix deflation schemes that enforce desirable properties exhibited by CCA. We propose a two-stage prediction pipeline using pCCA embeddings generated with deflation for latent variable prediction by combining all the above. On simulated data, our proposed model drastically reduces the mean-squared error in latent variable prediction. When applied to publicly available histopathology data and RNA-sequencing data from The Cancer Genome Atlas (TCGA) breast cancer patients, our model can outperform principal components analysis (PCA) embeddings of the same dimension in survival prediction.      
### 60.NCVX: A User-Friendly and Scalable Package for Nonconvex Optimization in Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.13984.pdf)
>  Optimizing nonconvex (NCVX) problems, especially those nonsmooth (NSMT) and constrained (CSTR), is an essential part of machine learning and deep learning. But it is hard to reliably solve this type of problems without optimization expertise. Existing general-purpose NCVX optimization packages are powerful, but typically cannot handle nonsmoothness. GRANSO is among the first packages targeting NCVX, NSMT, CSTR problems. However, it has several limitations such as the lack of auto-differentiation and GPU acceleration, which preclude the potential broad deployment by non-experts. To lower the technical barrier for the machine learning community, we revamp GRANSO into a user-friendly and scalable python package named NCVX, featuring auto-differentiation, GPU acceleration, tensor input, scalable QP solver, and zero dependency on proprietary packages. As a highlight, NCVX can solve general CSTR deep learning problems, the first of its kind. NCVX is available at <a class="link-external link-https" href="https://ncvx.org" rel="external noopener nofollow">this https URL</a>, with detailed documentation and numerous examples from machine learning and other fields.      
### 61.Reinforcement Learning-based Switching Controller for a Milliscale Robot in a Constrained Environment  [ :arrow_down: ](https://arxiv.org/pdf/2111.13969.pdf)
>  This work presents a reinforcement learning-based switching control mechanism to autonomously move a ferromagnetic object (representing a milliscale robot) around obstacles within a constrained environment in the presence of disturbances. This mechanism can be used to navigate objects (e.g., capsule endoscopy, swarms of drug particles) through complex environments when active control is a necessity but where direct manipulation can be hazardous. The proposed control scheme consists of a switching control architecture implemented by two sub-controllers. The first sub-controller is designed to employs the robot's inverse kinematic solutions to do an environment search of the to-be-carried ferromagnetic particle while being robust to disturbances. The second sub-controller uses a customized rainbow algorithm to control a robotic arm, i.e., the UR5 robot, to carry a ferromagnetic particle to a desired position through a constrained environment. For the customized Rainbow algorithm, Quantile Huber loss from the Implicit Quantile Networks (IQN) algorithm and ResNet are employed. The proposed controller is first trained and tested in a real-time physics simulation engine (PyBullet). Afterward, the trained controller is transferred to a UR5 robot to remotely transport a ferromagnetic particle in a real-world scenario to demonstrate the applicability of the proposed approach. The experimental results show an average success rate of 98.86\% calculated over 30 episodes for randomly generated trajectories.      
### 62.Energy-Efficient Autonomous Driving Using Cognitive Driver Behavioral Models and Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.13966.pdf)
>  Autonomous driving technologies are expected to not only improve mobility and road safety but also bring energy efficiency benefits. In the foreseeable future, autonomous vehicles (AVs) will operate on roads shared with human-driven vehicles. To maintain safety and liveness while simultaneously minimizing energy consumption, the AV planning and decision-making process should account for interactions between the autonomous ego vehicle and surrounding human-driven vehicles. In this chapter, we describe a framework for developing energy-efficient autonomous driving policies on shared roads by exploiting human-driver behavior modeling based on cognitive hierarchy theory and reinforcement learning.      
### 63.A Recommender System-Inspired Cloud Data Filling Scheme for Satellite-based Coastal Observation  [ :arrow_down: ](https://arxiv.org/pdf/2111.13955.pdf)
>  Filling missing data in cloud-covered areas of satellite imaging is an important task to improve data quantity and quality for enhanced earth observation. Traditional cloud filling studies focused on continuous numerical data such as temperature and cyanobacterial concentration in the open ocean. Cloud data filling issues in coastal imaging is far less studied because of the complex landscape. Inspired by the success of data imputation methods in recommender systems that are designed for online shopping, the present study explored their application to satellite cloud data filling tasks. A numerical experiment was designed and conducted for a LandSat dataset with a range of synthetic cloud covers to examine the performance of different data filling schemes. The recommender system-inspired matrix factorization algorithm called Funk-SVD showed superior performance in computational accuracy and efficiency for the task of recovering landscape types in a complex coastal area than the traditional data filling scheme of DINEOF (Data Interpolating Empirical Orthogonal Functions) and the deep learning method of Datawig. The new method achieved the best filling accuracy and reached a speed comparable to DINEOF and much faster than deep learning. A theoretical framework was created to analyze the error propagation in DINEOF and found the algorithm needs to be modified to converge to the ground truth. The present study showed that Funk-SVD has great potential to enhance cloud data filling performance and connects the fields of recommender systems and cloud filling to promote the improvement and sharing of useful algorithms.      
### 64.Resource-Aware Asynchronous Online Federated Learning for Nonlinear Regression  [ :arrow_down: ](https://arxiv.org/pdf/2111.13931.pdf)
>  Many assumptions in the federated learning literature present a best-case scenario that can not be satisfied in most real-world applications. An asynchronous setting reflects the realistic environment in which federated learning methods must be able to operate reliably. Besides varying amounts of non-IID data at participants, the asynchronous setting models heterogeneous client participation due to available computational power and battery constraints and also accounts for delayed communications between clients and the server. To reduce the communication overhead associated with asynchronous online federated learning (ASO-Fed), we use the principles of partial-sharing-based communication. In this manner, we reduce the communication load of the participants and, therefore, render participation in the learning task more accessible. We prove the convergence of the proposed ASO-Fed and provide simulations to analyze its behavior further. The simulations reveal that, in the asynchronous setting, it is possible to achieve the same convergence as the federated stochastic gradient (Online-FedSGD) while reducing the communication tenfold.      
### 65.Sparse Subspace Clustering Friendly Deep Dictionary Learning for Hyperspectral Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2111.13920.pdf)
>  Subspace clustering techniques have shown promise in hyperspectral image segmentation. The fundamental assumption in subspace clustering is that the samples belonging to different clusters/segments lie in separable subspaces. What if this condition does not hold? We surmise that even if the condition does not hold in the original space, the data may be nonlinearly transformed to a space where it will be separable into subspaces. In this work, we propose a transformation based on the tenets of deep dictionary learning (DDL). In particular, we incorporate the sparse subspace clustering (SSC) loss in the DDL formulation. Here DDL nonlinearly transforms the data such that the transformed representation (of the data) is separable into subspaces. We show that the proposed formulation improves over the state-of-the-art deep learning techniques in hyperspectral image clustering.      
### 66.A Quantum-like Model for Predicting Human Decisions in the Entangled Social Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.13902.pdf)
>  Human-centered systems of systems such as social networks, Internet of Things, or healthcare systems are growingly becoming major facets of modern life. Realistic models of human behavior in such systems play a significant role in their accurate modeling and prediction. Yet, human behavior under uncertainty often violates the predictions by the conventional probabilistic models. Recently, quantum-like decision theories have shown a considerable potential to explain the contradictions in human behavior by applying quantum probability. But providing a quantum-like decision theory that could predict, rather than describe the current, state of human behavior is still one of the unsolved challenges. The main novelty of our approach is introducing an entangled Bayesian network inspired by the entanglement concept in quantum information theory, in which each human is a part of the entire society. Accordingly, society's effect on the dynamic evolution of the decision-making process, which is less often considered in decision theories, is modeled by the entanglement measures. The proposed predictive entangled quantum-like Bayesian network (PEQBN) is evaluated on 22 experimental tasks. Results confirm that PEQBN provides more realistic predictions of human decisions under uncertainty, when compared with classical Bayesian networks and three recent quantum-like approaches.      
### 67.Automated Antenna Testing Using Encoder-Decoder-based Anomaly Detection  [ :arrow_down: ](https://arxiv.org/pdf/2111.13884.pdf)
>  We propose a new method for testing antenna arrays that records the radiating electromagnetic (EM) field using an absorbing material and evaluating the resulting thermal image series through an AI using a conditional encoder-decoder model. Given the power and phase of the signals fed into each array element, we are able to reconstruct normal sequences through our trained model and compare it to the real sequences observed by a thermal camera. These thermograms only contain low-level patterns such as blobs of various shapes. A contour-based anomaly detector can then map the reconstruction error matrix to an anomaly score to identify faulty antenna arrays and increase the classification F-measure (F-M) by up to 46%. We show our approach on the time series thermograms collected by our antenna testing system. Conventionally, a variational autoencoder (VAE) learning observation noise may yield better results than a VAE with a constant noise assumption. However, we demonstrate that this is not the case for anomaly detection on such low-level patterns for two reasons. First, the baseline metric reconstruction probability, which incorporates the learned observation noise, fails to differentiate anomalous patterns. Second, the area under the receiver operating characteristic (ROC) curve of a VAE with a lower observation noise assumption achieves 11.83% higher than that of a VAE with learned noise.      
### 68.Temporal Context Mining for Learned Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2111.13850.pdf)
>  We address end-to-end learned video compression with a special focus on better learning and utilizing temporal contexts. For temporal context mining, we propose to store not only the previously reconstructed frames, but also the propagated features into the generalized decoded picture buffer. From the stored propagated features, we propose to learn multi-scale temporal contexts, and re-fill the learned temporal contexts into the modules of our compression scheme, including the contextual encoder-decoder, the frame generator, and the temporal context encoder. Our scheme discards the parallelization-unfriendly auto-regressive entropy model to pursue a more practical decoding time. We compare our scheme with x264 and x265 (representing industrial software for H.264 and H.265, respectively) as well as the official reference software for H.264, H.265, and H.266 (JM, HM, and VTM, respectively). When intra period is 32 and oriented to PSNR, our scheme outperforms H.265--HM by 14.4% bit rate saving; when oriented to MS-SSIM, our scheme outperforms H.266--VTM by 21.1% bit rate saving.      
### 69.A Topological Approach for Computing Supremal Sublanguages for Some Language Equations in Supervisory Control Theory  [ :arrow_down: ](https://arxiv.org/pdf/2111.13840.pdf)
>  In this paper, we shall present a topological approach for the computation of some supremal sublanguages, often specified by language equations, which arise from the study of the supervisory control theory. The basic idea is to identify the solutions of the language equations as open sets for some (semi)-topologies. Then, the supremal sublanguages naturally correspond to the supremal open subsets, i.e., the interiors. This provides an elementary and uniform approach for computing various supremal sublanguages encountered in the supervisory control theory and is closely related to a theory of approximation, known as the rough set theory, in artificial intelligence.      
### 70.Achieving an Accurate Random Process Model for PV Power using Cheap Data: Leveraging the SDE and Public Weather Reports  [ :arrow_down: ](https://arxiv.org/pdf/2111.13812.pdf)
>  The stochastic differential equation (SDE)-based random process models of volatile renewable energy sources (RESs) jointly capture the evolving probability distribution and temporal correlation in continuous time. It has enabled recent studies to remarkably improve the performance of power system dynamic uncertainty quantification and optimization. However, considering the non-homogeneous random process nature of PV, there still remains a challenging question: how can a realistic and accurate SDE model for PV power be obtained that reflects its weather-dependent uncertainty in online operation, especially when high-resolution numerical weather prediction (NWP) is unavailable for many distributed plants? To fill this gap, this article finds that an accurate SDE model for PV power can be constructed by only using the cheap data from low-resolution public weather reports. Specifically, an hourly parameterized Jacobi diffusion process is constructed to recreate the temporal patterns of PV volatility during a day. Its parameters are mapped from the public weather report using an ensemble of extreme learning machines (ELMs) to reflect the varying weather conditions. The SDE model jointly captures intraday and intrahour volatility. Statistical examination based on real-world data collected in Macau shows the proposed approach outperforms a selection of state-of-the-art deep learning-based time-series forecast methods.      
### 71.Dynamic Analysis of Nonlinear Civil Engineering Structures using Artificial Neural Network with Adaptive Training  [ :arrow_down: ](https://arxiv.org/pdf/2111.13759.pdf)
>  Dynamic analysis of structures subjected to earthquake excitation is a time-consuming process, particularly in the case of extremely small time step required, or in the presence of high geometric and material nonlinearity. Performing parametric studies in such cases is even more tedious. The advancement of computer graphics hardware in recent years enables efficient training of artificial neural networks that are well-known to be capable of learning highly nonlinear mappings. In this study, artificial neural networks are developed with adaptive training algorithms, which enables automatic nodes generation and layers addition. The hyperbolic tangent function is selected as the activation function. Stochastic Gradient Descent and Back Propagation algorithms are adopted to train the networks. The neural networks initiate with a small number of hidden layers and nodes. During training, the performance of the network is continuously tracked, and new nodes or layers are added to the hidden layers if the neural network reaches its capacity. At the end of the training process, the network with appropriate architecture is automatically formed. The performance of the networks has been validated for inelastic shear frames, as well as rocking structures, of which both are first built in finite element program for dynamic analysis to generate training data. Results have shown the developed networks can successfully predict the time-history response of the shear frame and the rock structure subjected to real ground motion records. The efficiency of the proposed neural networks is also examined, which shows the computational time can be reduced by 43% by the neural networks method than FE models. This indicates the trained networks can be utilized to generate rocking spectrums of structures more efficiently which demands a large number of time-history analyses.      
### 72.Toward Next Generation Open Radio Access Network--What O-RAN Can and Cannot Do!  [ :arrow_down: ](https://arxiv.org/pdf/2111.13754.pdf)
>  The open radio access network (O-RAN) describes an industry-driven open architecture and interfaces for building next generation RANs with artificial intelligence (AI) based control. This paper dissects the O-RAN framework and provides a unique assessment of what it enables and where its current shortcomings are. This was motivated by a user survey that we conducted with responses from representative wireless researchers and developers from industry, academia, and government. The survey shows that the physical and medium access control layers are of most relevance to researchers and that nearly 80% believe that the O-RAN architecture will drive future cellular network innovations and deployments. The major contributions of O-RAN are the open interfaces among RAN modules and the specifications of different control loops that facilitate data driven network configuration and operation using AI. These AI controllers are encapsulated as xApps that can be introduced to control communications, networking, or service functions. The important O-RAN limitations relate to security, scope of AI control, and latency. We conclude this paper by indicating how specific limitations can be overcome in future O-RAN architecture revisions.      
### 73.Speaker Embedding-aware Neural Diarization for Flexible Number of Speakers with Textual Information  [ :arrow_down: ](https://arxiv.org/pdf/2111.13694.pdf)
>  Overlapping speech diarization is always treated as a multi-label classification problem. In this paper, we reformulate this task as a single-label prediction problem by encoding the multi-speaker labels with power set. Specifically, we propose the speaker embedding-aware neural diarization (SEND) method, which predicts the power set encoded labels according to the similarities between speech features and given speaker embeddings. Our method is further extended and integrated with downstream tasks by utilizing the textual information, which has not been well studied in previous literature. The experimental results show that our method achieves lower diarization error rate than the target-speaker voice activity detection. When textual information is involved, the diarization errors can be further reduced. For the real meeting scenario, our method can achieve 34.11% relative improvement compared with the Bayesian hidden Markov model based clustering algorithm.      
