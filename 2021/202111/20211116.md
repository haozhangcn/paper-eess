# ArXiv eess --Tue, 16 Nov 2021
### 1.Short-Term Power Prediction for Renewable Energy Using Hybrid Graph Convolutional Network and Long Short-Term Memory Approach  [ :arrow_down: ](https://arxiv.org/pdf/2111.07958.pdf)
>  Accurate short-term solar and wind power predictions play an important role in the planning and operation of power systems. However, the short-term power prediction of renewable energy has always been considered a complex regression problem, owing to the fluctuation and intermittence of output powers and the law of dynamic change with time due to local weather conditions, i.e. spatio-temporal correlation. To capture the spatio-temporal features simultaneously, this paper proposes a new graph neural network-based short-term power forecasting approach, which combines the graph convolutional network (GCN) and long short-term memory (LSTM). Specifically, the GCN is employed to learn complex spatial correlations between adjacent renewable energies, and the LSTM is used to learn dynamic changes of power curves. The simulation results show that the proposed hybrid approach can model the spatio-temporal correlation of renewable energies, and its performance outperforms popular baselines on real-world datasets.      
### 2.Transformer for Polyp Detection  [ :arrow_down: ](https://arxiv.org/pdf/2111.07918.pdf)
>  In recent years, as the Transformer has performed increasingly well on NLP tasks, many researchers have ported the Transformer structure to vision tasks ,bridging the gap between NLP and CV tasks. In this work, we evaluate some deep learning network for the detection track. Because the ground truth is mask, so we can try both the current detection and segmentation method. We select the DETR as our baseline through experiment. Besides, we modify the train strategy to fit the dataset.      
### 3.Experimental Investigation on the Friction-induced Vibration with Periodic Characteristics in a Running-in Process under Lubrication  [ :arrow_down: ](https://arxiv.org/pdf/2111.07914.pdf)
>  This paper investigated the friction-induced vibration (FIV) behavior under the running-in process with oil lubrication. The FIV signal with periodic characteristics under lubrication was identified with the help of the squeal signal induced in an oil-free wear experiment and then extracted by the harmonic wavelet packet transform (HWPT). The variation of the FIV signal from running-in wear stage to steady wear stage was studied by its root mean square (RMS) values. The result indicates that the time-frequency characteristics of the FIV signals evolve with the wear process and can reflect the wear stages of the friction pairs. The RMS evolvement of the FIV signal is in the same trend to the composite surface roughness and demonstrates that the friction pair goes through the running-in wear stage and the steady wear stage. Therefore, the FIV signal with periodic characteristics can describe the evolvement of the running-in process and distinguish the running-in wear stage and the stable wear stage of the friction pair.      
### 4.On Sparse High-Dimensional Graphical Model Learning For Dependent Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2111.07897.pdf)
>  We consider the problem of inferring the conditional independence graph (CIG) of a sparse, high-dimensional stationary multivariate Gaussian time series. A sparse-group lasso-based frequency-domain formulation of the problem based on frequency-domain sufficient statistic for the observed time series is presented. We investigate an alternating direction method of multipliers (ADMM) approach for optimization of the sparse-group lasso penalized log-likelihood. We provide sufficient conditions for convergence in the Frobenius norm of the inverse PSD estimators to the true value, jointly across all frequencies, where the number of frequencies are allowed to increase with sample size. This results also yields a rate of convergence. We also empirically investigate selection of the tuning parameters based on Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.      
### 5.Performance bounds of adaptive MPC with bounded parameter uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2111.07896.pdf)
>  Model predictive control is a particular control technique that minimizes a so-called stage cost and is capable of handling state and input constraints. A model of the system plays a key role in the behavior of the closed-loop system. If the model is uncertain, robust or adaptive methods can be used to guarantee stability. It can be argued that the precision of the system model affects the controller performance, as measured by a sum of the stage costs. If the model is adapted online, a performance bound is difficult to obtain and thus the impact of model adaptation is mostly unknown. This work provides a such (worst-case) performance bound for a linear adaptive predictive control scheme with specific model parameter estimation. The proposed bound is expressed in terms of quantities such as the initial system parameter error and the constraint set, among others and can be calculated a priori. The results are discussed in a numerical example.      
### 6.Data privacy protection in microscopic image analysis for material data mining  [ :arrow_down: ](https://arxiv.org/pdf/2111.07892.pdf)
>  Recent progress in material data mining has been driven by high-capacity models trained on large datasets. However, collecting experimental data has been extremely costly owing to the amount of human effort and expertise required. Therefore, material researchers are often reluctant to easily disclose their private data, which leads to the problem of data island, and it is difficult to collect a large amount of data to train high-quality models. In this study, a material microstructure image feature extraction algorithm FedTransfer based on data privacy protection is proposed. The core contributions are as follows: 1) the federated learning algorithm is introduced into the polycrystalline microstructure image segmentation task to make full use of different user data to carry out machine learning, break the data island and improve the model generalization ability under the condition of ensuring the privacy and security of user data; 2) A data sharing strategy based on style transfer is proposed. By sharing style information of images that is not urgent for user confidentiality, it can reduce the performance penalty caused by the distribution difference of data among different users.      
### 7.Transfer Learning Capabilities of Untrained Neural Networks for MIMO CSI Recreation  [ :arrow_down: ](https://arxiv.org/pdf/2111.07858.pdf)
>  Machine learning (ML) applications for wireless communications have gained momentum on the standardization discussions for 5G advanced and beyond. One of the biggest challenges for real world ML deployment is the need for labeled signals and big measurement campaigns. To overcome those problems, we propose the use of untrained neural networks (UNNs) for MIMO channel recreation/estimation and low overhead reporting. The UNNs learn the propagation environment by fitting a few channel measurements and we exploit their learned prior to provide higher channel estimation gains. Moreover, we present a UNN for simultaneous channel recreation for multiple users, or multiple user equipment (UE) positions, in which we have a trade-off between the estimated channel gain and the number of parameters. Our results show that transfer learning techniques are effective in accessing the learned prior on the environment structure as they provide higher channel gain for neighbouring users. Moreover, we indicate how the under-parameterization of UNNs can further enable low-overhead channel state information (CSI) reporting.      
### 8.Machine Learning for CSI Recreation Based on Prior Knowledge  [ :arrow_down: ](https://arxiv.org/pdf/2111.07854.pdf)
>  Knowledge of channel state information (CSI) is fundamental to many functionalities within the mobile wireless communications systems. With the advance of machine learning (ML) and digital maps, i.e., digital twins, we have a big opportunity to learn the propagation environment and design novel methods to derive and report CSI. In this work, we propose to combine untrained neural networks (UNNs) and conditional generative adversarial networks (cGANs) for MIMO channel recreation based on prior knowledge. The UNNs learn the prior-CSI for some locations which are used to build the input to a cGAN. Based on the prior-CSIs, their locations and the location of the desired channel, the cGAN is trained to output the channel expected at the desired location. This combined approach can be used for low overhead CSI reporting as, after training, we only need to report the desired location. Our results show that our method is successful in modelling the wireless channel and robust to location quantization errors in line of sight conditions.      
### 9.Spatial-Interference Aware Cooperative Resource Allocation for 5G NR Sidelink Communications  [ :arrow_down: ](https://arxiv.org/pdf/2111.07814.pdf)
>  Distributed resource allocation (RA) schemes have been introduced in cellular vehicle-to-everything (C-V2X) standard for vehicle-to-vehicle (V2V) sidelink (SL) communications to share the limited spectrum (sub-6GHz) efficiently. However, the recent progress in connected and automated vehicles and mobility services requires a huge amount of available spectrum resources. Therefore, millimeter-wave and sub-THz frequencies are being considered as they offer a large free bandwidth. However, they require beamforming techniques to compensate for the higher path loss attenuation. The current fifth-generation (5G) RA standard for SL communication is inherited from the previous C-V2X standard, which is not suited for beam-based communication since it does not explore the spatial dimension. In this context, we propose a novel RA scheme that addresses the directional component by adding this third spatial dimension to the bandwidth part structure and promotes cooperation between vehicles in resource selection, namely cooperative three-dimensional RA. Numerical results show an average of 10% improvement in packet delivery ratio, an average 50% decrease in collision probability, and a 30% better channel busy ratio compared to the current standard, thus, confirming the validity of the proposed method.      
### 10.LoS-Map Construction for Proactive Relay of Opportunity Selection in 6G V2X Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.07804.pdf)
>  The evolution of connected and automated vehicles (CAVs) technology is boosting the development of innovative solutions for the sixth generation (6G) of Vehicular-to-Everything (V2X) networks. Lower frequency networks provide control of millimeter waves (mmWs) or sub-THz beam-based 6G communications. In CAVs, the mmW/Sub-THz guarantees a huge amount of bandwidth (&gt;1GHz) and a high data rate (&gt; 10 Gbit/s), enhancing the safety of CAVs applications. However, high-frequency is impaired by severe path-loss, and line of sight (LoS) propagation can be easily blocked. Static and dynamic blocking (e.g., by non-connected vehicles) heavily affects V2X links, and thus, in a multi-vehicular case, the knowledge of LoS (or visibility) mapping is mandatory for stable connections and proactive beam pointing that might involve relays whenever necessary. <br>In this paper, we design a criterion for dynamic LoS-map estimation, and we propose a novel framework for relay of opportunity selection to enable high-quality and stable V2X links. Relay selection is based on cooperative sensing to cope with LoS blockage conditions. LoS-map is dynamically estimated on top of the static map of the environment by merging the perceptive sensors' data to achieve cooperative awareness of the surrounding scenario. Multiple relay selection architectures are based on centralized and decentralized strategies. 3GPP standard-compliant simulation is the framework methodology adopted herein to reproduce real-world urban vehicular environments and vehicles' mobility patterns.      
### 11.Joint Far- and Near-End Speech Intelligibility Enhancement based on the Approximated Speech Intelligibility Index  [ :arrow_down: ](https://arxiv.org/pdf/2111.07759.pdf)
>  This paper considers speech enhancement of signals picked up in one noisy environment which must be presented to a listener in another noisy environment. Recently, it has been shown that an optimal solution to this problem requires the consideration of the noise sources in both environments jointly. However, the existing optimal mutual information based method requires a complicated system model that includes natural speech variations, and relies on approximations and assumptions of the underlying signal distributions. In this paper, we propose to use a simpler signal model and optimize speech intelligibility based on the Approximated Speech Intelligibility Index (ASII). We derive a closed-form solution to the joint far- and near-end speech enhancement problem that is independent of the marginal distribution of signal coefficients, and that achieves similar performance to existing work. In addition, we do not need to model or optimize for natural speech variations.      
### 12.Investigating self-supervised front ends for speech spoofing countermeasures  [ :arrow_down: ](https://arxiv.org/pdf/2111.07725.pdf)
>  Self-supervised speech model is a rapid progressing research topic, and many pre-trained models have been released and used in various down-stream tasks. For speech anti-spoofing, most countermeasures (CMs) are using signal processing algorithms to extract acoustic features for classification. In this study, we use pre-trained self-supervised speech models as the front end of spoofing CMs. We investigated different back end architectures to be combined with the self-supervised front end, the effectiveness of fine tuning the front end, and the performance of using different pre-trained self-supervised models. Our experiments found that, when a good pre-trained front end was fine tuned with either a shallow or a deep neural-network-based back end on the ASVspoof 2019 logical access (LA) training set, the resulting CM not only achieved a low EER score on the 2019 LA test set but also significantly outperformed the baseline on ASVspoof 2015, 2021 LA, and 2021 deepfake test sets.      
### 13.Energy-optimal Design and Control of Electric Powertrains under Motor Thermal Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2111.07711.pdf)
>  This paper presents a modeling and optimization framework to minimize the energy consumption of a fully electric powertrain by optimizing its design and control strategies whilst explicitly accounting for the thermal behavior of the Electric Motor (EM). Specifically, we first derive convex models of the powertrain components, including the battery, the EM, the transmission and a Lumped Parameter Thermal Network (LPTN) capturing the thermal dynamics of the EM. Second, we frame the optimal control problem in time domain, and devise a two-step algorithm to accelerate convergence and efficiently solve the resulting convex problem via nonlinear programming. Subsequently, we present a case study for a compact family car, optimize its transmission design and operation jointly with the regenerative braking and EM cooling control strategies for different motors and transmission technologies. We validate our proposed models using the high-fidelity simulation software Motor-CAD, showing that the LPTN quite accurately captures the thermal dynamics of the EM, and that the permanent magnets' temperature is the limiting factor during extended driving. Furthermore, our results reveal that powertrains equipped with a continuously variable transmission (CVT) result into a lower energy consumption than with a fixed-gear transmission (FGT), as a CVT can lower the EM losses, resulting in lower EM temperatures. Finally, our results emphasize the significance of considering the thermal behavior when designing an EM and the potential offered by CVTs in terms of downsizing.      
### 14.Reachability analysis of neural networks using mixed monotonicity  [ :arrow_down: ](https://arxiv.org/pdf/2111.07683.pdf)
>  This paper presents a new reachability analysis tool to compute an interval over-approximation of the output set of a feedforward neural network under given input uncertainty. The proposed approach adapts to neural networks an existing mixed-monotonicity method for the reachability analysis of dynamical systems and applies it to all possible partial networks within the given neural network. This ensures that the intersection of the obtained results is the tightest interval over-approximation of the output of each layer that can be obtained using mixed-monotonicity. Unlike other tools in the literature that focus on small classes of piecewise-affine or monotone activation functions, the main strength of our approach is its generality in the sense that it can handle neural networks with any Lipschitz-continuous activation function. In addition, the simplicity of the proposed framework allows users to very easily add unimplemented activation functions, by simply providing the function, its derivative and the global extrema and corresponding arguments of the derivative. Our algorithm is tested and compared to five other interval-based tools on 1000 randomly generated neural networks for four activation functions (ReLU, TanH, ELU, SiLU). We show that our tool always outperforms the Interval Bound Propagation method and that we obtain tighter output bounds than ReluVal, Neurify, VeriNet and CROWN (when they are applicable) in 15 to 60 percent of cases.      
### 15.Reliability Assessment of Distribution Systems including Microgrids  [ :arrow_down: ](https://arxiv.org/pdf/2111.07674.pdf)
>  The electrical distribution system is moving toward a more decentralized, complex, and dynamic system. The system is experiencing a higher penetration of distributed energy resources, flexible resources, and active end-users, leading to an active distribution system. If these components are used actively, they can have a positive effect on the distribution system's reliability. This paper aims to investigate how a microgrid with renewable energy sources and energy storage might influence the reliability of electricity supply in a radially operated distribution system. The reliability is investigated from both the distribution system perspective and the microgrid perspective with the application of different scenarios. A reliability assessment method for modern distribution systems based on Monte Carlo simulations is presented. The method includes load flow calculations to capture the behavior of the system and system components. The model is tested on the IEEE 33-bus network. The result is confirmed through statistical testing showing the statistical significance in providing support from the microgrid on the distribution system's reliability.      
### 16.Improving needle visibility in LED-based photoacoustic imaging using deep learning with semi-synthetic datasets  [ :arrow_down: ](https://arxiv.org/pdf/2111.07673.pdf)
>  Photoacoustic imaging has shown great potential for guiding minimally invasive procedures by accurate identification of critical tissue targets and invasive medical devices (such as metallic needles). Recently, the use of light-emitting diodes (LEDs) as the excitation light sources further accelerates its clinical translation owing to its high affordability and portability. However, needle visibility in LED-based photoacoustic imaging is compromised primarily due to its low optical fluence. In this work, we propose a deep learning framework based on a modified U-Net to improve the visibility of clinical metallic needles with a LED-based photoacoustic and ultrasound imaging system. This framework included the generation of semi-synthetic training datasets combining both simulated data to represent features from the needles and in vivo measurements for tissue background. Evaluation of the trained neural network was performed with needle insertions into a blood-vessel-mimicking phantom, pork joint tissue ex vivo, and measurements on human volunteers. This deep learning-based framework substantially improved the needle visibility in photoacoustic imaging in vivo compared to conventional reconstructions by suppressing background noise and image artefacts, achieving ~1.9 times higher signal-to-noise ratios and an increase of the intersection over union (IoU) from 16.4% to 61.9%. Thus, the proposed framework could be helpful for reducing complications during percutaneous needle insertions by accurate identification of clinical needles in photoacoustic imaging.      
### 17.Instant magnetic tissue field and susceptibility mapping from MR raw phase using Laplacian enabled deep neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07665.pdf)
>  Quantitative susceptibility mapping (QSM) is a valuable MRI post-processing technique that quantifies the magnetic susceptibility of body tissue from phase data. However, the traditional QSM reconstruction pipeline involves multiple non-trivial steps, including phase unwrapping, background field removal, and dipole inversion. These intermediate steps not only increase the reconstruction time but amplify noise and errors. This study develops a large-stencil Laplacian preprocessed deep learning-based neural network for near instant quantitative field and susceptibility mapping (i.e., iQFM and iQSM) from raw MR phase data. The proposed iQFM and iQSM methods were compared with established reconstruction pipelines on simulated and in vivo datasets. In addition, experiments on patients with intracranial hemorrhage and multiple sclerosis were also performed to test the generalization of the novel neural networks. The proposed iQFM and iQSM methods yielded comparable results to multi-step methods in healthy subjects while dramatically improving reconstruction accuracies on intracranial hemorrhages with large susceptibilities. The reconstruction time was also substantially shortened from minutes using multi-step methods to only 30 milliseconds using the trained iQFM and iQSM neural networks.      
### 18.Pseudo-domains in imaging data improve prediction of future disease status in multi-center studies  [ :arrow_down: ](https://arxiv.org/pdf/2111.07634.pdf)
>  In multi-center randomized clinical trials imaging data can be diverse due to acquisition technology or scanning protocols. Models predicting future outcome of patients are impaired by this data heterogeneity. Here, we propose a prediction method that can cope with a high number of different scanning sites and a low number of samples per site. We cluster sites into pseudo-domains based on visual appearance of scans, and train pseudo-domain specific models. Results show that they improve the prediction accuracy for steatosis after 48 weeks from imaging data acquired at an initial visit and 12-weeks follow-up in liver disease      
### 19.Ultra-Low-Power IoT Communications: A novel address decoding approach for wake-up receivers  [ :arrow_down: ](https://arxiv.org/pdf/2111.07607.pdf)
>  Providing energy-efficient Internet of Things (IoT) connectivity has attracted significant attention in fifth-generation (5G) wireless networks and beyond. A potential solution for realizing a long-lasting network of IoT devices is to equip each IoT device with a wake-up receiver (WuR) to have always-accessible devices instead of always-on devices. WuRs typically comprise a radio frequency demodulator, sequence decoder, and digital address decoder and are provided with a unique authentication address in the network. Although the literature on efficient demodulators is mature, it lacks research on fast, low-power, and reliable address decoders. As this module continuously monitors the received ambient energy for potential paging of the device, its contribution to WuR's power consumption is crucial. Motivated by this need, a low-power, reliable address decoder is developed in this paper. We further investigate the integration of WuR in low-power uplink/downlink communications and, using system-level energy analysis; we characterize operation regions in which WuR can contribute significantly to energy saving. The device-level energy analysis confirms the superior performance of our decoder. The results show that the proposed decoder significantly outperforms the state-of-the-art with a power consumption of 60 nW, at cost of compromising a negligible increase in decoding delay.      
### 20.Block-Sparse Recovery Network for Two-Dimensional Harmonic Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2111.07589.pdf)
>  As a typical signal processing problem, multidimensional harmonic retrieval (MHR) has been adapted to a wide range of applications in signal processing. Block-sparse signals, whose nonzero entries appearing in clusters, have received much attention recently. An unfolded network, named Ada-BlockLISTA, was proposed to recover a block-sparse signal at a small computational cost, which learns an individual weight matrix for each block. However, as the number of network parameters is increasingly associated with the number of blocks, the demand for parameter reduction becomes very significant, especially for large-scale MHR. Based on the dictionary characteristics in two-dimensional (2D) harmonic retrieve problems, we introduce a weight coupling structure to shrink Ada-BlockLISTA, which significantly reduces the number of weights without performance degradation. In simulations, our proposed block-sparse reconstruction network, named AdaBLISTA-CP, shows excellent recovery performance and convergence speed in 2D harmonic retrieval problems.      
### 21.Monaural source separation: From anechoic to reverberant environments  [ :arrow_down: ](https://arxiv.org/pdf/2111.07578.pdf)
>  Impressive progress in neural network-based single-channel speech source separation has been made in recent years. But those improvements have been mostly reported on anechoic data, a situation that is hardly met in practice. Taking the SepFormer as a starting point, which achieves state-of-the-art performance on anechoic mixtures, we gradually modify it to optimize its performance on reverberant mixtures. Although this leads to a word error rate improvement by 8 percentage points compared to the standard SepFormer implementation, the system ends up with only marginally better performance than our improved PIT-BLSTM separation system, that is optimized with rather straightforward means. This is surprising and at the same time sobering, challenging the practical usefulness of many improvements reported in recent years for monaural source separation on nonreverberant data.      
### 22.Vision-Position Multi-Modal Beam Prediction Using Real Millimeter Wave Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2111.07574.pdf)
>  Enabling highly-mobile millimeter wave (mmWave) and terahertz (THz) wireless communication applications requires overcoming the critical challenges associated with the large antenna arrays deployed at these systems. In particular, adjusting the narrow beams of these antenna arrays typically incurs high beam training overhead that scales with the number of antennas. To address these challenges, this paper proposes a multi-modal machine learning based approach that leverages positional and visual (camera) data collected from the wireless communication environment for fast beam prediction. The developed framework has been tested on a real-world vehicular dataset comprising practical GPS, camera, and mmWave beam training data. The results show the proposed approach achieves more than $\approx$ 75\% top-1 beam prediction accuracy and close to 100\% top-3 beam prediction accuracy in realistic communication scenarios.      
### 23.Dynamic Placement of Rapidly Deployable Mobile Sensor Robots Using Machine Learning and Expected Value of Information  [ :arrow_down: ](https://arxiv.org/pdf/2111.07552.pdf)
>  Although the Industrial Internet of Things has increased the number of sensors permanently installed in industrial plants, there will be gaps in coverage due to broken sensors or sparse density in very large plants, such as in the petrochemical industry. Modern emergency response operations are beginning to use Small Unmanned Aerial Systems (sUAS) that have the ability to drop sensor robots to precise locations. sUAS can provide longer-term persistent monitoring that aerial drones are unable to provide. Despite the relatively low cost of these assets, the choice of which robotic sensing systems to deploy to which part of an industrial process in a complex plant environment during emergency response remains challenging. <br>This paper describes a framework for optimizing the deployment of emergency sensors as a preliminary step towards realizing the responsiveness of robots in disaster circumstances. AI techniques (Long short-term memory, 1-dimensional convolutional neural network, logistic regression, and random forest) identify regions where sensors would be most valued without requiring humans to enter the potentially dangerous area. In the case study described, the cost function for optimization considers costs of false-positive and false-negative errors. Decisions on mitigation include implementing repairs or shutting down the plant. The Expected Value of Information (EVI) is used to identify the most valuable type and location of physical sensors to be deployed to increase the decision-analytic value of a sensor network. This method is applied to a case study using the Tennessee Eastman process data set of a chemical plant, and we discuss implications of our findings for operation, distribution, and decision-making of sensors in plant emergency and resilience scenarios.      
### 24.T-AutoML: Automated Machine Learning for Lesion Segmentation using Transformers in 3D Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2111.07535.pdf)
>  Lesion segmentation in medical imaging has been an important topic in clinical research. Researchers have proposed various detection and segmentation algorithms to address this task. Recently, deep learning-based approaches have significantly improved the performance over conventional methods. However, most state-of-the-art deep learning methods require the manual design of multiple network components and training strategies. In this paper, we propose a new automated machine learning algorithm, T-AutoML, which not only searches for the best neural architecture, but also finds the best combination of hyper-parameters and data augmentation strategies simultaneously. The proposed method utilizes the modern transformer model, which is introduced to adapt to the dynamic length of the search space embedding and can significantly improve the ability of the search. We validate T-AutoML on several large-scale public lesion segmentation data-sets and achieve state-of-the-art performance.      
### 25.Facilitating Satellite-Airborne-Balloon-Terrestrial Integration for Dynamic and Infrastructure-less Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07506.pdf)
>  This magazine investigates the potential enhancement of the data throughput of ground users by integrating ground base stations (GBS) with air stations, such as balloon, airborne, and satellite. The objective is to establish dynamic bi-directional wireless services (i.e., uplink and downlink) for ground users in congested and remote areas. The proposed integration involves satellite, high-altitude platforms (HAPs), and tethered balloons (TBs) in the exosphere, stratosphere, and troposphere, respectively, for better altitude reuse coupled with emerging optical or other high-frequency directional transceivers. This will lead to a significant enhancement in scarce spectrum aggregate efficiency. However, the air stations deployment and resource managements in this integrated system faces difficulties. This article tackles resource management challenges by (i) providing wireless services to ground users in remote areas and connecting them with metropolitan and rural areas and (ii) employing HAPs equipped with free-space-optical communication modules as back-hauling backbones. Finally, we illustrate some numerical results to show the benefit of our proposed integrated system.      
### 26.Exploring latent networks in resting-state fMRI using voxel-to-voxel causal modeling feature selection  [ :arrow_down: ](https://arxiv.org/pdf/2111.07488.pdf)
>  Functional networks characterize the coordinated neural activity observed by functional neuroimaging. The prevalence of different networks during resting state periods provide useful features for predicting the trajectory of neurodegenerative diseases. Techniques for network estimation rely on statistical correlation or dependence between voxels. Due to the large number of voxels, rather than consider the voxel-to-voxel correlations between all voxels, a small set of seed voxels are chosen. Consequently, the network identification may depend on the selected seeds. As an alternative, we propose to fit first-order linear models with sparse priors on the coefficients to model activity across the entire set of cortical grey matter voxels as a linear combination of a smaller subset of voxels. We propose a two-stage algorithm for voxel subset selection that uses different sparsity-inducing regularization approaches to identify subject-specific causally predictive voxels. To reveal the functional networks among these voxels, we then apply independent component analysis (ICA) to model these voxels' signals as a mixture of latent sources each defining a functional network. Based on the inter-subject similarity of the sources' spatial patterns we identify independent sources that are well-matched across subjects but fail to match the independent sources from a group-based ICA. These are resting state networks, common across subjects that group ICA does not reveal. These complementary networks could help to better identify neurodegeneration, a task left for future work.      
### 27.Extension of Chance-Constrained System Identification of Nonlinear Discrete Systems with Safety and Stability Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2111.07466.pdf)
>  This paper presents a discrete-time nonlinear system identification method while satisfying the stability and safety properties of the system with high probability. An Extreme Learning Machine (ELM) is used with a Gaussian assumption on the function reconstruction error. A quadratically constrained quadratic program (QCQP) is developed with probabilistic safety and stability constraints that are only required to be satisfied at sampled points inside the invariant region. The proposed method is validated using two simulation examples: a two degrees-of-freedom (DoF) robot manipulator with constraints on joint angles whose trajectories are guaranteed to remain inside a safe set and on motion trajectories data of a hand-drawn shape.      
### 28.Beamspace Multidimensional ESPRIT Approaches for Simultaneous Localization and Communications  [ :arrow_down: ](https://arxiv.org/pdf/2111.07450.pdf)
>  Modern wireless communication systems operating at high carrier frequencies are characterized by a high dimensionality of the underlying parameter space (including channel gains, angles, delays, and possibly Doppler shifts). Estimating these parameters is valuable for communication purposes, but also for localization and sensing, making channel estimation a critical component in any joint communication and localization or sensing application. The high dimensionality make it difficult to use search-based methods such as maximum likelihood. Search-free methods such as ESPRIT provide an attractive alternative, but require a complex decomposition step in both the tensor and matrix version of ESPRIT. To mitigate this, we propose, develop, and analyze a reduced complexity beamspace ESPRIT method. Complexity is reduced both by beampace processing as well as low-complex implementation of the singular value decomposition. A novel perturbation analysis provides important insights for both channel estimation and localization performance. The proposed method is compared to the tensor ESPRIT method, in terms of channel estimation, communication, localization, and sensing performance, further validating the perturbation analysis.      
### 29.Unified stability criteria for perturbed LTV systems with unstable instantaneous dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2111.07443.pdf)
>  In this work the stability of perturbed linear time-varying systems is studied. The main features of the problem are threefold. Firstly, the time-varying dynamics is not required to be continuous but allowed to have jumps. Also the system matrix is not assumed to be always Hurwitz. In addition, there is nonlinear time-varying perturbation which may be persistent. We first propose several mild regularity assumptions, under which the total variations of the system matrix and its abscissa are well-defined over arbitrary time interval. We then state our main result of the work, which requires the combined assessment of the total variation of the system matrix, the measure when the system is not sufficiently "stable" and the estimate of the perturbation to be upper bounded by a function affine in time. When this condition is met, we prove that the neighborhood of the origin, whose size depends on the magnitude of the perturbation, is uniformly globally exponentially stable for the system. We make several remarks, connecting our results with the known stability theory from continuous linear time-varying systems and switched systems. Finally, a numerical example is included to further illustrate the application of the main result.      
### 30.Jumping Fluid Models and Delay Stability of Max-Weight Dynamics under Heavy-Tailed Traffic  [ :arrow_down: ](https://arxiv.org/pdf/2111.07420.pdf)
>  We say that a random variable is $light$-$tailed$ if moments of order $2+\epsilon$ are finite for some $\epsilon&gt;0$; otherwise, we say that it is $heavy$-$tailed$. We study queueing networks that operate under the Max-Weight scheduling policy, for the case where some queues receive heavy-tailed traffic, while some other queues receive light-tailed traffic. We say that a queue is $delay$ $stable$ if its expected size (and hence its expected delay) is uniformly bounded over time. It is well-known that a queue with heavy-tailed arrivals is always delay unstable. Queues with light-tailed arrivals are often delay stable, as long as their arrival rate does not exceed the service capacity, but can also become delay unstable because of resource-sharing with other delay unstable queues. <br>Within this context, and for any given "tail exponents" of the input traffic, we develop tight (necessary and sufficient) conditions under which a queue with light-tailed arrivals is robustly delay stable, in terms of $jumping$ $fluid$ (JF) models-an extension of traditional fluid models that allows for jumps along coordinates associated with heavy-tailed flows. In particular, we show that a queue is robustly delay stable if and only if all JF trajectories in a certain class, and whose number of jumps is limited by a certain "budget," keep the queue of interest at zero. This result elucidates the precise mechanism that leads to delay instability, through a coordination of multiple abnormally large arrivals at possibly different times and queues. Moreover, it shows that traditional fluid models are intrinsically unable to provide tight conditions for delay stability and settles an earlier open question on the sufficiency of a particular fluid-based criterion. We also explore the power of Lyapunov functions in the study of robust delay stability.      
### 31.Eco-Coasting Strategies Using Road Grade Preview: Evaluation and Online Implementation Based on Mixed Integer Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.07377.pdf)
>  In this paper, two different coasting strategies are proposed: one leverages fuel cut-off and another uses engine start/stop. Engine drag torque and energy-cost used for engine restart are considered in the modeling to give a fair evaluation. Then, the performance of these two coasting methods is evaluated with dynamic programming (DP) under various driving scenarios with different slope profiles. Offline simulation shows that the engine start/stop method outperforms the fuel cut-off method in terms of fuel consumption and travel time by getting rid of the engine drag torque. Furthermore, on-line performance of these two coasting methods is evaluated using Mixed Integer Model Predictive Control (MIMPC). A novel operational constraint on the minimum off steps is added in the MIMPC formulation to avoid frequent switch of the integer variables representing the fuel cut-off and the engine start/stop mechanism. Simulation results show that, for both fuel cut-off and engine start/stop coasting methods, the MPC improves fuel consumption to a level comparable to DP without sacrificing the travel time.      
### 32.Estimation of Acetabular Version from Anteroposterior Pelvic Radiograph Employing Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07369.pdf)
>  Background and Objective: The Acetabular version, an essential factor in total hip arthroplasty, is measured by CT scan as the gold standard. The dose of radiation and expensiveness of CT make anterior-posterior pelvic radiograph an appropriate alternative procedure. In this study, we applied a deep learning approach on anteroposterior pelvic X-rays to measure anatomical version, eliminating the necessity of using Computed tomography scan. Methods: The right and left acetabular version angles of the hips of 300 patients are computed using their CT images. The proposed deep learning model, Attention on Pretrained-VGG16 for Bone Age, is applied to the AP images of the included population. The age and gender of these people are added as two other inputs to the last fully connected layer of attention mechanism. As the output, the angles of both hips are predicted. Results: The angles of hips computed on CT increase as people get older with the mean values of 16.54 and 16.11 (right and left angles) for men and 20.61 and 19.55 for women in our dataset. The predicted errors in the estimation of right and left angles using the proposed method of deep learning are in the accurate region of error (&lt;=3 degrees) which shows the ability of the proposed method in measuring anatomical version based on AP images. Conclusion: The suggested algorithm, applying pre-trained vgg16 on the AP images of the pelvis of patients followed by an attention model considering age and gender of patients, can assess version accurately using only AP radiographs while obviating the need for CT scan. The applied technique of estimation of anatomical acetabular version based on AP pelvic images using DL approaches, to the best of authors' knowledge, has not been published yet.      
### 33.Fracture Detection in Wrist X-ray Images Using Deep Learning-Based Object Detection Models  [ :arrow_down: ](https://arxiv.org/pdf/2111.07355.pdf)
>  Wrist fractures are common cases in hospitals, particularly in emergency services. Physicians need images from various medical devices, and patients medical history and physical examination to diagnose these fractures correctly and apply proper treatment. This study aims to perform fracture detection using deep learning on wrist Xray images to assist physicians not specialized in the field, working in emergency services in particular, in diagnosis of fractures. For this purpose, 20 different detection procedures were performed using deep learning based object detection models on dataset of wrist Xray images obtained from Gazi University Hospital. DCN, Dynamic R_CNN, Faster R_CNN, FSAF, Libra R_CNN, PAA, RetinaNet, RegNet and SABL deep learning based object detection models with various backbones were used herein. To further improve detection procedures in the study, 5 different ensemble models were developed, which were later used to reform an ensemble model to develop a detection model unique to our study, titled wrist fracture detection combo (WFD_C). Based on detection of 26 different fractures in total, the highest result of detection was 0.8639 average precision (AP50) in WFD_C model developed. This study is supported by Huawei Turkey R&amp;D Center within the scope of the ongoing cooperation project coded 071813 among Gazi University, Huawei and Medskor.      
### 34.Relative Distributed Formation and Obstacle Avoidance with Multi-agent Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07334.pdf)
>  Multi-agent formation as well as obstacle avoidance is one of the most actively studied topics in the field of multi-agent systems. Although some classic controllers like model predictive control (MPC) and fuzzy control achieve a certain measure of success, most of them require precise global information which is not accessible in harsh environments. On the other hand, some reinforcement learning (RL) based approaches adopt the leader-follower structure to organize different agents' behaviors, which sacrifices the collaboration between agents thus suffering from bottlenecks in maneuverability and robustness. In this paper, we propose a distributed formation and obstacle avoidance method based on multi-agent reinforcement learning (MARL). Agents in our system only utilize local and relative information to make decisions and control themselves distributively. Agent in the multi-agent system will reorganize themselves into a new topology quickly in case that any of them is disconnected. Our method achieves better performance regarding formation error, formation convergence rate and on-par success rate of obstacle avoidance compared with baselines (both classic control methods and another RL-based method). The feasibility of our method is verified by both simulation and hardware implementation with Ackermann-steering vehicles.      
### 35.Optimizing the Age of Information in RIS-aided SWIPT Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07318.pdf)
>  In this letter, a reconfigurable intelligent surface (RIS)-assisted simultaneous wireless information and power transfer (SWIPT) network is investigated. To quantify the freshness of the data packets at the information receiver, the concept of age of information (AoI) is considered. The AoI is decided as the time elapsed since the generation of the last successfully delivered signal containing status update information about the system. To minimize the total AoI for the information users while ensuring that the power transferred to energy harvesting users is greater than the demanded value, we formulate a scheduling scheme at the base station (BS), and a joint transmit beamforming and phase shift optimization at the BS and RIS, respectively. Specifically, the alternative optimization (AO) algorithm is proposed for handling the coupled variables of the joint active and passive beamforming design, and the successive convex approximation (SCA) algorithm is utilized for tackling the non-convexity of the formulated problems. The improvement in terms of AoI provided by the proposed algorithm is quantified by the numerical simulation results.      
### 36.Blocking Probability in Obstructed Tunnels with Reconfigurable Intelligent Surface  [ :arrow_down: ](https://arxiv.org/pdf/2111.07297.pdf)
>  In the obstructed tunnels, the signal transmission will suffer the risk of ray-path blocking caused by the obstacles owing to the Snell's law. In this letter, the reconfigurable intelligent surface (RIS) that can reflect the electromagnetic waves to any specific directions is introduced to mitigate the signal blocking. The closed-form expressions for blocking probability (BP) for one reflection with single RIS and multiple RISs under various scenarios are derived. Compared with the case without RIS, significant reduction of BP can be found with proper configuration of the RIS. Moreover, the impact of the location of RIS, the height of the transmitter, and the location of the receiver, on the BP is investigated. Finally, the case of multiple obstacles with different distributions is discussed to further verify the effectiveness of RIS on reduction of BP.      
### 37.Integrating Counter-UAS Systems into the UTM System for Reliable Decision Making  [ :arrow_down: ](https://arxiv.org/pdf/2111.07291.pdf)
>  Despite significant progress, the deployment of UAV technology in commercial and civil applications is still lagging. This is essentially due to the risks associated with drone flights and the lack of coordinated technologies that would mitigate these risks. While Unmanned Aircraft System Traffic Management systems (UTM) are being developed worldwide to enable safe operation, the counter-drone technology operates on an all-enemy basis and regards any sighted drone as a threat. This situation is essentially caused by the lack of information exchange between stakeholders. Without the exchange of relevant information, a counter-drone system can misclassify drones and initiate erroneous interdiction procedures. This paper proposes a system that integrates counter-drone technology into the UTM system for information exchange and coordination using a set of clarification protocols towards accountable response to sighted drones. The system functionality and performance were evaluated by simulation.      
### 38.Deep Joint Demosaicing and High Dynamic Range Imaging within a Single Shot  [ :arrow_down: ](https://arxiv.org/pdf/2111.07281.pdf)
>  Spatially varying exposure (SVE) is a promising choice for high-dynamic-range (HDR) imaging (HDRI). The SVE-based HDRI, which is called single-shot HDRI, is an efficient solution to avoid ghosting artifacts. However, it is very challenging to restore a full-resolution HDR image from a real-world image with SVE because: a) only one-third of pixels with varying exposures are captured by camera in a Bayer pattern, b) some of the captured pixels are over- and under-exposed. For the former challenge, a spatially varying convolution (SVC) is designed to process the Bayer images carried with varying exposures. For the latter one, an exposure-guidance method is proposed against the interference from over- and under-exposed pixels. Finally, a joint demosaicing and HDRI deep learning framework is formalized to include the two novel components and to realize an end-to-end single-shot HDRI. Experiments indicate that the proposed end-to-end framework avoids the problem of cumulative errors and surpasses the related state-of-the-art methods.      
### 39.Moment Transform-Based Compressive Sensing in Image Processing  [ :arrow_down: ](https://arxiv.org/pdf/2111.07254.pdf)
>  Over the last decades, images have become an important source of information in many domains, thus their high quality has become necessary to acquire better information. One of the important issues that arise is image denoising, which means recovering a signal from inaccurately and/or partially measured samples. This interpretation is highly correlated to the compressive sensing theory, which is a revolutionary technology and implies that if a signal is sparse then the original signal can be obtained from a few measured values, which are much less, than the ones suggested by other used theories like Shannon's sampling theories. A strong factor in Compressive Sensing (CS) theory to achieve the sparsest solution and the noise removal from the corrupted image is the selection of the basis dictionary. In this paper, Discrete Cosine Transform (DCT) and moment transform (Tchebichef, Krawtchouk) are compared in order to achieve image denoising of Gaussian additive white noise based on compressive sensing and sparse approximation theory. The experimental results revealed that the basis dictionaries constructed by the moment transform perform competitively to the traditional DCT. The latter transform shows a higher PSNR of 30.82 dB and the same 0.91 SSIM value as the Tchebichef transform. Moreover, from the sparsity point of view, Krawtchouk moments provide approximately 20-30% more sparse results than DCT.      
### 40.Simultaneous estimation of parameters and the state of an optical parametric oscillator system  [ :arrow_down: ](https://arxiv.org/pdf/2111.07249.pdf)
>  In this paper, we consider the filtering problem of an optical parametric oscillator (OPO). The OPO pump power may fluctuate due to environmental disturbances, resulting in uncertainty in the system modeling. Thus, both the state and the unknown parameter may need to be estimated simultaneously. We formulate this problem using a state-space representation of the OPO dynamics. Under the assumption of Gaussianity and proper constraints, the dual Kalman filter method and the joint extended Kalman filter method are employed to simultaneously estimate the system state and the pump power. Numerical examples demonstrate the effectiveness of the proposed algorithms.      
### 41.Doors in the Sky: Detection, Localization and Classification of Aerial Vehicles using Laser Mesh  [ :arrow_down: ](https://arxiv.org/pdf/2111.07231.pdf)
>  The stealth technology and unmanned aerial vehicles (UAVs) are expected to dominate current and future aerial warfare. The radar systems at their maximum operating ranges, however, are not always able to detect stealth and small UAVs mainly due to their small radar cross-sections and/or low altitudes. In this paper, a novel technique as an alternative to radar technology is proposed. The proposed approach is based on creating a mesh structure of laser beams initiated from aerial platforms towards the ground. The laser mesh acts as a virtual net in the sky. Any aerial vehicle disrupting the path of the laser beams are detected and subsequently localized and tracked. As an additional feature, steering of the beams can be used for increased coverage and improved localization and classification performance. A database of different types of aerial vehicles is created artificially based on Gaussian distributions. The database is used to develop several machine learning (ML) models using different algorithms to classify a target. Overall, we demonstrated through simulations that our proposed model achieves simultaneous detection, classification, localization, and tracking of a target.      
### 42.SDnDTI: Self-supervised deep learning-based denoising for diffusion tensor MRI  [ :arrow_down: ](https://arxiv.org/pdf/2111.07220.pdf)
>  The noise in diffusion-weighted images (DWIs) decreases the accuracy and precision of diffusion tensor magnetic resonance imaging (DTI) derived microstructural parameters and leads to prolonged acquisition time for achieving improved signal-to-noise ratio (SNR). Deep learning-based image denoising using convolutional neural networks (CNNs) has superior performance but often requires additional high-SNR data for supervising the training of CNNs, which reduces the practical feasibility. We develop a self-supervised deep learning-based method entitled "SDnDTI" for denoising DTI data, which does not require additional high-SNR data for training. Specifically, SDnDTI divides multi-directional DTI data into many subsets, each consisting of six DWI volumes along optimally chosen diffusion-encoding directions that are robust to noise for the tensor fitting, and then synthesizes DWI volumes along all acquired directions from the diffusion tensors fitted using each subset of the data as the input data of CNNs. On the other hand, SDnDTI synthesizes DWI volumes along acquired diffusion-encoding directions with higher SNR from the diffusion tensors fitted using all acquired data as the training target. SDnDTI removes noise from each subset of synthesized DWI volumes using a deep 3-dimensional CNN to match the quality of the cleaner target DWI volumes and achieves even higher SNR by averaging all subsets of denoised data. The denoising efficacy of SDnDTI is demonstrated on two datasets provided by the Human Connectome Project (HCP) and the Lifespan HCP in Aging. The SDnDTI results preserve image sharpness and textural details and substantially improve upon those from the raw data. The results of SDnDTI are comparable to those from supervised learning-based denoising and outperform those from state-of-the-art conventional denoising algorithms including BM4D, AONLM and MPPCA.      
### 43.Meta-Voice: Fast few-shot style transfer for expressive voice cloning using meta learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07218.pdf)
>  The task of few-shot style transfer for voice cloning in text-to-speech (TTS) synthesis aims at transferring speaking styles of an arbitrary source speaker to a target speaker's voice using very limited amount of neutral data. This is a very challenging task since the learning algorithm needs to deal with few-shot voice cloning and speaker-prosody disentanglement at the same time. Accelerating the adaptation process for a new target speaker is of importance in real-world applications, but even more challenging. In this paper, we approach to the hard fast few-shot style transfer for voice cloning task using meta learning. We investigate the model-agnostic meta-learning (MAML) algorithm and meta-transfer a pre-trained multi-speaker and multi-prosody base TTS model to be highly sensitive for adaptation with few samples. Domain adversarial training mechanism and orthogonal constraint are adopted to disentangle speaker and prosody representations for effective cross-speaker style transfer. Experimental results show that the proposed approach is able to conduct fast voice cloning using only 5 samples (around 12 second speech data) from a target speaker, with only 100 adaptation steps. Audio samples are available online.      
### 44.Reliably-stabilizing piecewise-affine neural network controllers  [ :arrow_down: ](https://arxiv.org/pdf/2111.07183.pdf)
>  A common problem affecting neural network (NN) approximations of model predictive control (MPC) policies is the lack of analytical tools to assess the stability of the closed-loop system under the action of the NN-based controller. We present a general procedure to quantify the performance of such a controller, or to design minimum complexity NNs with rectified linear units (ReLUs) that preserve the desirable properties of a given MPC scheme. By quantifying the approximation error between NN-based and MPC-based state-to-input mappings, we first establish suitable conditions involving two key quantities, the worst-case error and the Lipschitz constant, guaranteeing the stability of the closed-loop system. We then develop an offline, mixed-integer optimization-based method to compute those quantities exactly. Together these techniques provide conditions sufficient to certify the stability and performance of a ReLU-based approximation of an MPC control law.      
### 45.Deep Reinforcement Learning with Shallow Controllers: An Experimental Application to PID Tuning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07171.pdf)
>  Deep reinforcement learning (RL) is an optimization-driven framework for producing control strategies for general dynamical systems without explicit reliance on process models. Good results have been reported in simulation. Here we demonstrate the challenges in implementing a state of the art deep RL algorithm on a real physical system. Aspects include the interplay between software and existing hardware; experiment design and sample efficiency; training subject to input constraints; and interpretability of the algorithm and control law. At the core of our approach is the use of a PID controller as the trainable RL policy. In addition to its simplicity, this approach has several appealing features: No additional hardware needs to be added to the control system, since a PID controller can easily be implemented through a standard programmable logic controller; the control law can easily be initialized in a "safe'' region of the parameter space; and the final product -- a well-tuned PID controller -- has a form that practitioners can reason about and deploy with confidence.      
### 46.The Pseudo Projection Operator: Applications of Deep Learning to Projection Based Filtering in Non-Trivial Frequency Regimes  [ :arrow_down: ](https://arxiv.org/pdf/2111.07140.pdf)
>  Traditional frequency based projection filters, or projection operators (PO), separate signal and noise through a series of transformations which remove frequencies where noise is present. However, this technique relies on a priori knowledge of what frequencies contain signal and noise and that these frequencies do not overlap, which is difficult to achieve in practice. To address these issues, we introduce a PO-neural network hybrid model, the Pseudo Projection Operator (PPO), which leverages a neural network to perform frequency selection. We compare the filtering capabilities of a PPO, PO, and denoising autoencoder (DAE) on the University of Rochester Multi-Modal Music Performance Dataset with a variety of added noise types. In the majority of experiments, the PPO outperforms both the PO and DAE. Based upon these results, we suggest future application of the PPO to filtering problems in the physical and biological sciences.      
### 47.A strong baseline for image and video quality assessment  [ :arrow_down: ](https://arxiv.org/pdf/2111.07104.pdf)
>  In this work, we present a simple yet effective unified model for perceptual quality assessment of image and video. In contrast to existing models which usually consist of complex network architecture, or rely on the concatenation of multiple branches of features, our model achieves a comparable performance by applying only one global feature derived from a backbone network (i.e. resnet18 in the presented work). Combined with some training tricks, the proposed model surpasses the current baselines of SOTA models on public and private datasets. Based on the architecture proposed, we release the models well trained for three common real-world scenarios: UGC videos in the wild, PGC videos with compression, Game videos with compression. These three pre-trained models can be directly applied for quality assessment, or be further fine-tuned for more customized usages. All the code, SDK, and the pre-trained weights of the proposed models are publicly available at <a class="link-external link-https" href="https://github.com/Tencent/CenseoQoE" rel="external noopener nofollow">this https URL</a>.      
### 48.Optimal Planning of Single-Port and Multi-Port Charging Stations for Electric Vehicles in Medium Voltage Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07100.pdf)
>  This paper describes a method based on mixed-integer linear programming to cost-optimally locate and size chargers for electric vehicles (EVs) in distribution grids as a function of the driving demand. The problem accounts for the notion of single-port chargers (SPCs), where a charger can interface one EV maximum, and multi-port chargers (MPCs), where the same charger can interface multiple EVs. The advantage of MPCs is twofold. First, multiple ports allow arbitraging the charging among multiple vehicles without requiring the drivers to plug and unplug EVs. Second, the charger's power electronics is not sized for the total number of charging ports, enabling cost savings when the grid constraints are bottleneck of the problem. The proposed method can account for different charger typologies, such as slow and fast chargers, and model the drivers' flexibility of plugging and unplugging their EVs. Simulation results from a synthetic case study show that implementing MPCs is beneficial over both SPCs and drivers' flexibility in terms of total investments required for the charging infrastructure.      
### 49.Geometric PID Controller for Stabilization of Nonholonomic Mechanical Systems on Lie Groups  [ :arrow_down: ](https://arxiv.org/pdf/2111.07061.pdf)
>  The PID controller is an elegant and versatile controller for set point tracking in double integrator systems of which mechanical systems evolving on Euclidean space constitute a large class. But since mechanical systems are typically constrained interconnections of rigid bodies whose configuration space is $SE(3)$, which is not even topologically Euclidean, a geometric PID controller has been developed for mechanical systems evolving on Lie groups. In this work, we extend the framework to such systems which have nonholonomic constraints. It encompasses many practically applicable mechanical systems encountered in robotics as robots are constrained interconnections of rigid bodies where the constraints could either be holonomic or nonholonomic.      
### 50.Selection of the Speed Command Distance for Improved Performance of a Rule-Based VSL and Lane Change Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.07056.pdf)
>  Variable Speed Limit (VSL) control has been one of the most popular approaches with the potential of smoothing traffic flow, maximizing throughput at bottlenecks and improving mobility and safety. Despite the substantial research efforts in the field of VSL control, few studies have looked into the effect of the VSL zone distance from the point of an accident or a bottleneck. In this paper, we show that this distance has a significant impact on the effectiveness and performance of VSL control. We propose a rule-based VSL strategy that matches the outflow of VSL zone with the bottleneck capacity based on a multi-section cell transmission model (CTM). Then, we consider the distance of the upstream VSL zone as a control variable and perform a comprehensive analysis of its impact on the performance of the closed-loop traffic control system based on the CTM. We develop a lower bound that the VSL zone distance needs to satisfy in order to guarantee convergence to a desired flow and density, verified analytically and demonstrated using microscopic traffic simulations.      
### 51.Improving the Otsu Thresholding Method of Global Binarization Using Ring Theory for Ultrasonographies of Congestive Heart Failure  [ :arrow_down: ](https://arxiv.org/pdf/2111.07031.pdf)
>  Ring Theory states that a ring is an algebraic structure where two binary operations can be performed among the elements addition and multiplication. Binarization is a method of image processing where values within pixels are reduced to a scale from zero to one, with zero representing the most absence of light and one representing the most presence of light. Currently, sonograms are implemented in scanning for congestive heart failure. However, the renowned Playboy Bunny symbol representing the ailment becomes increasingly difficult to isolate due to surrounding organs and lower quality image productions. This paper examines the Otsu thresholding method and incorporates new elements to account for different image features meant to better isolate congestive heart failure indicators in ultrasound images.      
### 52.Closed-form Two-way TOA Localization and Synchronization for User Devices with Motion and Clock Drift  [ :arrow_down: ](https://arxiv.org/pdf/2111.07019.pdf)
>  A two-way time-of-arrival (TOA system is composed of anchor nodes (ANs and user devices (UDs . Two-way TOA measurements between AN-UD pairs are obtained via round-trip communications to achieve localization and synchronization (LAS for a UD. Existing LAS method for a moving UD with clock drift adopts an iterative algorithm, which requires accurate initialization and has high computational complexity. In this paper, we propose a new closed-form two-way TOA LAS approach, namely CFTWLAS, which does not require initialization, has low complexity and empirically achieves optimal LAS accuracy. We first linearize the LAS problem by squaring and differencing the two-way TOA equations. We employ two auxiliary variables to simplify the problem to finding the analytical solution of quadratic equations. Due to the measurement noise, we can only obtain a raw LAS estimation from the solution of the auxiliary variables. Then, a weighted least squares step is applied to further refine the raw estimation. We analyze the theoretical error of the new CFTWLAS and show that it empirically reaches the Cramer-Rao lower bound (CRLB with sufficient ANs under the condition with proper geometry and small noise. Numerical results in a 3D scenario verify the theoretical analysis that the estimation accuracy of the new CFTWLAS method reaches CRLB in the presented experiments when the number of the ANs is large, the geometry is appropriate, and the noise is small. Unlike the iterative method whose complexity increases with the iteration count, the new CFTWLAS has constant low complexity.      
### 53.Delay-Oriented Distributed Scheduling Using Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07017.pdf)
>  In wireless multi-hop networks, delay is an important metric for many applications. However, the max-weight scheduling algorithms in the literature typically focus on instantaneous optimality, in which the schedule is selected by solving a maximum weighted independent set (MWIS) problem on the interference graph at each time slot. These myopic policies perform poorly in delay-oriented scheduling, in which the dependency between the current backlogs of the network and the schedule of the previous time slot needs to be considered. To address this issue, we propose a delay-oriented distributed scheduler based on graph convolutional networks (GCNs). In a nutshell, a trainable GCN module generates node embeddings that capture the network topology as well as multi-step lookahead backlogs, before calling a distributed greedy MWIS solver. In small- to medium-sized wireless networks with heterogeneous transmit power, where a few central links have many interfering neighbors, our proposed distributed scheduler can outperform the myopic schedulers based on greedy and instantaneously optimal MWIS solvers, with good generalizability across graph models and minimal increase in communication complexity.      
### 54.Fault Diagnosis of Nonlinear Systems Using a Hybrid-Degree Dual Cubature-based Estimation Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2111.07004.pdf)
>  In this paper, a novel hybrid-degree dual estimation approach based on cubature rules and cubature-based nonlinear filters is proposed for fault diagnosis of nonlinear systems through simultaneous state and time-varying parameter estimation. Our proposed dual nonlinear filtering scheme is developed based on case-dependent cubature rules that are motivated by the following observations and facts, namely (i) dynamic characteristics of nonlinear system states and parameters generally are distinct and posses different degrees of complexities, and (ii) performance of cubature rules depend on the system dynamics and vary due to handling of high-dimensional integrations approximations. For improving the robustness capability of our proposed methodologies modified cubature point propagation method is incorporated. The performance of our proposed dual estimation strategy is demonstrated and evaluated by application to a nonlinear gas turbine engine for addressing the component fault diagnosis problem within an integrated fault detection, isolation and identification framework. Robustness analysis is implemented to verify the capability of our proposed approaches to deal with parametric uncertainties and unmodeled dynamics. Extensive simulation case studies and discussions with respect to component fouling, erosion or abrupt faults are provided to substantiate and justify the superiority of our proposed fault diagnosis methodology when compared to other well-known alternative diagnostic techniques such as the Unscented Kalman Filters (UKF) and Particle Filters (PF) that are commonly available in the literature.      
### 55.Control Barrier Function Augmentation in Sampling-based Control Algorithm for Sample Efficiency  [ :arrow_down: ](https://arxiv.org/pdf/2111.06974.pdf)
>  For a nonlinear stochastic path planning problem, sampling-based algorithms generate thousands of random sample trajectories to find the optimal path while guaranteeing safety by Lagrangian penalty methods. However, the sampling-based algorithm can perform poorly in obstacle-rich environments because most samples might violate safety constraints, invalidating the corresponding samples. To improve the sample efficiency of sampling-based algorithms in cluttered environments, we propose an algorithm based on model predictive path integral control and control barrier functions. The proposed algorithm needs fewer samples and time-steps and has a better performance in cluttered environments compared to the original model predictive path integral control algorithm.      
### 56.Adaptive and quasi-sliding control of shimmy in landing gears  [ :arrow_down: ](https://arxiv.org/pdf/2111.06972.pdf)
>  Shimmy is a dangerous phenomenon that occurs when aircraft's nose landing gears oscillate in a rapid and uncontrollable fashion. In this paper, we propose the use of two nonlinear control approaches (zero average control and model reference adaptive control based on minimal control synthesis) as simple yet effective strategies to suppress undesired oscillations, even in the presence of uncertainties and partial state measurements. Numerical results are presented to validate the proposed control approaches.      
### 57.Synchronization of networks of piecewise-smooth systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.06964.pdf)
>  We study convergence in networks of piecewise-smooth (PWS) systems that commonly arise in applications to model dynamical systems whose evolution is affected by macroscopic events such as switches and impacts. Existing approaches were typically oriented toward guaranteeing global bounded synchronizability, local stability of the synchronization manifold, or achieving synchronization by exerting a control action on each node. Here we start by generalizing existing results on QUAD systems to the case of PWS systems, accounting for a large variety of nonlinear coupling laws. Then, we propose that a discontinuous coupling can be used to guarantee global synchronizability of a network of N PWS agents under mild assumptions on the individual dynamics. We provide extensive numerical simulations to gain insights on larger networks.      
### 58.Convolutional Nets Versus Vision Transformers for Diabetic Foot Ulcer Classification  [ :arrow_down: ](https://arxiv.org/pdf/2111.06894.pdf)
>  This paper compares well-established Convolutional Neural Networks (CNNs) to recently introduced Vision Transformers for the task of Diabetic Foot Ulcer Classification, in the context of the DFUC 2021 Grand-Challenge, in which this work attained the first position. Comprehensive experiments demonstrate that modern CNNs are still capable of outperforming Transformers in a low-data regime, likely owing to their ability for better exploiting spatial correlations. In addition, we empirically demonstrate that the recent Sharpness-Aware Minimization (SAM) optimization algorithm considerably improves the generalization capability of both kinds of models. Our results demonstrate that for this task, the combination of CNNs and the SAM optimization process results in superior performance than any other of the considered approaches.      
### 59.Impact of loss functions on the performance of a deep neural network designed to restore low-dose digital mammography  [ :arrow_down: ](https://arxiv.org/pdf/2111.06890.pdf)
>  Digital mammography is still the most common imaging tool for breast cancer screening. Although the benefits of using digital mammography for cancer screening outweigh the risks associated with the x-ray exposure, the radiation dose must be kept as low as possible while maintaining the diagnostic utility of the generated images, thus minimizing patient risks. Many studies investigated the feasibility of dose reduction by restoring low-dose images using deep neural networks. In these cases, choosing the appropriate training database and loss function is crucial and impacts the quality of the results. In this work, a modification of the ResNet architecture, with hierarchical skip connections, is proposed to restore low-dose digital mammography. We compared the restored images to the standard full-dose images. Moreover, we evaluated the performance of several loss functions for this task. For training purposes, we extracted 256,000 image patches from a dataset of 400 images of retrospective clinical mammography exams, where different dose levels were simulated to generate low and standard-dose pairs. To validate the network in a real scenario, a physical anthropomorphic breast phantom was used to acquire real low-dose and standard full-dose images in a commercially avaliable mammography system, which were then processed through our trained model. An analytical restoration model for low-dose digital mammography, previously presented, was used as a benchmark in this work. Objective assessment was performed through the signal-to-noise ratio (SNR) and mean normalized squared error (MNSE), decomposed into residual noise and bias. Results showed that the perceptual loss function (PL4) is able to achieve virtually the same noise levels of a full-dose acquisition, while resulting in smaller signal bias compared to other loss functions.      
### 60.Nonprehensile Riemannian Motion Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2111.07986.pdf)
>  Nonprehensile manipulation involves long horizon underactuated object interactions and physical contact with different objects that can inherently introduce a high degree of uncertainty. In this work, we introduce a novel Real-to-Sim reward analysis technique, called Riemannian Motion Predictive Control (RMPC), to reliably imagine and predict the outcome of taking possible actions for a real robotic platform. Our proposed RMPC benefits from Riemannian motion policy and second order dynamic model to compute the acceleration command and control the robot at every location on the surface. Our approach creates a 3D object-level recomposed model of the real scene where we can simulate the effect of different trajectories. We produce a closed-loop controller to reactively push objects in a continuous action space. We evaluate the performance of our RMPC approach by conducting experiments on a real robot platform as well as simulation and compare against several baselines. We observe that RMPC is robust in cluttered as well as occluded environments and outperforms the baselines.      
### 61.Metric-based multimodal meta-learning for human movement identification via footstep recognition  [ :arrow_down: ](https://arxiv.org/pdf/2111.07979.pdf)
>  We describe a novel metric-based learning approach that introduces a multimodal framework and uses deep audio and geophone encoders in siamese configuration to design an adaptable and lightweight supervised model. This framework eliminates the need for expensive data labeling procedures and learns general-purpose representations from low multisensory data obtained from omnipresent sensing systems. These sensing systems provide numerous applications and various use cases in activity recognition tasks. Here, we intend to explore the human footstep movements from indoor environments and analyze representations from a small self-collected dataset of acoustic and vibration-based sensors. The core idea is to learn plausible similarities between two sensory traits and combining representations from audio and geophone signals. We present a generalized framework to learn embeddings from temporal and spatial features extracted from audio and geophone signals. We then extract the representations in a shared space to maximize the learning of a compatibility function between acoustic and geophone features. This, in turn, can be used effectively to carry out a classification task from the learned model, as demonstrated by assigning high similarity to the pairs with a human footstep movement and lower similarity to pairs containing no footstep movement. Performance analyses show that our proposed multimodal framework achieves a 19.99\% accuracy increase (in absolute terms) and avoided overfitting on the evaluation set when the training samples were increased from 200 pairs to just 500 pairs while satisfactorily learning the audio and geophone representations. Our results employ a metric-based contrastive learning approach for multi-sensor data to mitigate the impact of data scarcity and perform human movement identification with limited data size.      
### 62.Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2111.07910.pdf)
>  Hyperspectral image (HSI) reconstruction aims to recover the 3D spatial-spectral signal from a 2D measurement in the coded aperture snapshot spectral imaging (CASSI) system. The HSI representations are highly similar and correlated across the spectral dimension. Modeling the inter-spectra interactions is beneficial for HSI reconstruction. However, existing CNN-based methods show limitations in capturing spectral-wise similarity and long-range dependencies. Besides, the HSI information is modulated by a coded aperture (physical mask) in CASSI. Nonetheless, current algorithms have not fully explored the guidance effect of the mask for HSI restoration. In this paper, we propose a novel framework, Mask-guided Spectral-wise Transformer (MST), for HSI reconstruction. Specifically, we present a Spectral-wise Multi-head Self-Attention (S-MSA) that treats each spectral feature as a token and calculates self-attention along the spectral dimension. In addition, we customize a Mask-guided Mechanism (MM) that directs S-MSA to pay attention to spatial regions with high-fidelity spectral representations. Extensive experiments show that our MST significantly outperforms state-of-the-art (SOTA) methods on simulation and real HSI datasets while requiring dramatically cheaper computational and memory costs.      
### 63.Design of a SOIMUMPs Inertial Sensor and readout Charge Amplifier  [ :arrow_down: ](https://arxiv.org/pdf/2111.07880.pdf)
>  This paper presents the design and post-layout characteristics of a differential capacitance based inertial accelerometer This includes a MEMS based mechanical sensing element and a CMOS charge amplifier, which is the first stage of a readout circuit. The mechanical sensor is designed according to the SOIMUMPs fabrication process technology, and the readout circuit targeted AMS 0.35um technology. Post layout simulations indicated a +/-5G dynamic range, a maximum bandwidth of 1.58 kHz, non-linearity of 0.077% and a resolution of 10.5 uG/Hz^0.5. The readout circuit charge amplifier is fully differential and incorporated in a switched capacitor (SC) topology with CDS.      
### 64.Colored Noise Mechanism for Differentially Private Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2111.07850.pdf)
>  The goal of this paper is to propose and analyze a differentially private randomized mechanism for the $K$-means query. The goal is to ensure that the information received about the cluster-centroids is differentially private. The method consists in adding Gaussian noise with an optimum covariance. The main result of the paper is the analytical solution for the optimum covariance as a function of the database. Comparisons with the state of the art prove the efficacy of our approach.      
### 65.A teacher-student framework for online correctional learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07818.pdf)
>  A classical learning setting is one in which a student collects data, or observations, about a system, and estimates a certain quantity of interest about it. Correctional learning is a type of cooperative teacher-student framework where a teacher, who has knowledge about the system, has the possibility to observe and alter (correct) the observations received by the student in order to improve its estimation. In this paper, we show that the variance of the estimate of the student is reduced with the help of the teacher. We further formulate the online problem - where the teacher has to decide at each time instant whether or not to change the observations - as a Markov decision process, from which the optimal policy is derived using dynamic programming. We validate the framework in numerical experiments, and compare the optimal online policy with the one from the batch setting.      
### 66.A Finite-Sampling, Operational Domain Specific, and Provably Unbiased Connected and Automated Vehicle Safety Metric  [ :arrow_down: ](https://arxiv.org/pdf/2111.07769.pdf)
>  A connected and automated vehicle safety metric determines the performance of a subject vehicle (SV) by analyzing the data involving the interactions among the SV and other dynamic road users and environmental features. When the data set contains only a finite set of samples collected from the naturalistic mixed-traffic driving environment, a metric is expected to generalize the safety assessment outcome from the observed finite samples to the unobserved cases by specifying in what domain the SV is expected to be safe and how safe the SV is, statistically, in that domain. However, to the best of our knowledge, none of the existing safety metrics are able to justify the above properties with an operational domain specific, guaranteed complete, and provably unbiased safety evaluation outcome. In this paper, we propose a novel safety metric that involves the $\alpha$-shape and the $\epsilon$-almost robustly forward invariant set to characterize the SV's almost safe operable domain and the probability for the SV to remain inside the safe domain indefinitely, respectively. The empirical performance of the proposed method is demonstrated in several different operational design domains through a series of cases covering a variety of fidelity levels (real-world and simulators), driving environments (highway, urban, and intersections), road users (car, truck, and pedestrian), and SV driving behaviors (human driver and self driving algorithms).      
### 67.Fast Computation of Hahn Polynomials for High Order Moments  [ :arrow_down: ](https://arxiv.org/pdf/2111.07749.pdf)
>  Discrete Hahn polynomials (DHPs) and their moments are considered to be one of the efficient orthogonal moments and they are applied in various scientific areas such as image processing and feature extraction. Commonly, DHPs are used as object representation; however, they suffer from the problem of numerical instability when the moment order becomes large. In this paper, an efficient method for computation of Hahn orthogonal basis is proposed and applied to high orders. This paper developed a new mathematical model for computing the initial value of the DHP and for different values of DHP parameters ($\alpha$ and $\beta$). In addition, the proposed method is composed of two recurrence algorithms with an adaptive threshold to stabilize the generation of the DHP coefficients. It is compared with state-of-the-art algorithms in terms of computational cost and the maximum size that can be correctly generated. The experimental results show that the proposed algorithm performs better in both parameters for wide ranges of parameter values of ($\alpha$ and $\beta$) and polynomial sizes.      
### 68.Symbolic Music Loop Generation with VQ-VAE  [ :arrow_down: ](https://arxiv.org/pdf/2111.07657.pdf)
>  Music is a repetition of patterns and rhythms. It can be composed by repeating a certain number of bars in a structured way. In this paper, the objective is to generate a loop of 8 bars that can be used as a building block of music. Even considering musical diversity, we assume that music patterns familiar to humans can be defined in a finite set. With explicit rules to extract loops from music, we found that discrete representations are sufficient to model symbolic music sequences. Among VAE family, musical properties from VQ-VAE are better observed rather than other models. Further, to emphasize musical structure, we have manipulated discrete latent features to be repetitive so that the properties are more strengthened. Quantitative and qualitative experiments are extensively conducted to verify our assumptions.      
### 69.Multimodal Generalized Zero Shot Learning for Gleason Grading using Self-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07646.pdf)
>  Gleason grading from histopathology images is essential for accurate prostate cancer (PCa) diagnosis. Since such images are obtained after invasive tissue resection quick diagnosis is challenging under the existing paradigm. We propose a method to predict Gleason grades from magnetic resonance (MR) images which are non-interventional and easily acquired. We solve the problem in a generalized zero-shot learning (GZSL) setting since we may not access training images of every disease grade. Synthetic MRI feature vectors of unseen grades (classes) are generated by exploiting Gleason grades' ordered nature through a conditional variational autoencoder (CVAE) incorporating self-supervised learning. Corresponding histopathology features are generated using cycle GANs, and combined with MR features to predict Gleason grades of test images. Experimental results show our method outperforms competing feature generating approaches for GZSL, and comes close to performance of fully supervised methods.      
### 70.Drone delivery: Reliable Cellular UAV Communication Using Multi-Operator Diversity  [ :arrow_down: ](https://arxiv.org/pdf/2111.07637.pdf)
>  The market size of Unmanned Aerial Vehicles (UAVs, a.k.a drones) can reach up to 10\% of the global market value. In particular, drone delivery is one of the most attractive applications. The growing number of drones requires appropriate traffic management systems that will rely on cellular networks. However, it has been shown in the literature that these networks cannot provide reliable communication due to low coverage probability and frequent handovers. This article presents a potential solution targeting these problems while requiring no modifications of the existing infrastructure. Namely, equipping the UAV with multiple cellular modems to connect to different providers' networks introduces network diversity resulting in 98\% coverage probability at the flight altitude of 100 meters. In contrast, one network ensures only 80\% coverage. At the same time, the size of the outage zones becomes up to ten times smaller and the frequency of harmful handovers is reduced to zero. The results are obtained with a physical-layer simulator utilizing a real urban 3D environment, cellular network parameters (e.g., site locations, antenna orientation and gains), and specific aerial channel models.      
### 71.On the validation of pansharpening methods  [ :arrow_down: ](https://arxiv.org/pdf/2111.07625.pdf)
>  Validation of the quality of pansharpening methods is a difficult task because the reference is not directly available. In the meantime, two main approaches have been established: validation in reduced resolution and original resolution. In the former approach it is still not clear how the data are to be processed to a lower resolution. Other open issues are related to the question which resolution and measures should be used. In the latter approach the main problem is how the appropriate measure should be selected. In the most comparison studies the results of both approaches do not correspond, that means in each case other methods are selected as the best ones. Thus, the developers of the new pansharpening methods still stand in the front of dilemma: how to perform a correct or appropriate comparison/evaluation/validation. It should be noted, that the third approach is possible, that is to perform the comparison of methods in a particular application with the usage of their ground truth. But this is not always possible, because usually developers are not working with applications. Moreover, it can be an additional computational load for a researcher in a particular application. In this paper some of the questions/problems raised above are approached/discussed. The following component substitution (CS) and high pass filtering (HPF) pansharpening methods with additive and multiplicative models and their enhancements such as haze correction, histogram matching, usage of spectral response functions (SRF), modulation transfer function (MTF) based lowpass filtering are investigated on remote sensing data of WorldView-2 and WorldView-4 sensors.      
### 72.Discriminative Mutual Information Estimation for the Design of Channel Capacity Driven Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2111.07606.pdf)
>  The development of optimal and efficient machine learning-based communication systems is likely to be a key enabler of beyond 5G communication technologies. In this direction, physical layer design has been recently reformulated under a deep learning framework where the autoencoder paradigm foresees the full communication system as an end-to-end coding-decoding problem. Given the loss function, the autoencoder jointly learns the coding and decoding optimal blocks under a certain channel model. Because performance in communications typically refers to achievable rates and channel capacity, the mutual information between channel input and output can be included in the end-to-end training process, thus, its estimation becomes essential. In this paper, we present a set of novel discriminative mutual information estimators and we discuss how to exploit them to design capacity-approaching codes and ultimately estimate the channel capacity.      
### 73.Improving Prosody for Unseen Texts in Speech Synthesis by Utilizing Linguistic Information and Noisy Data  [ :arrow_down: ](https://arxiv.org/pdf/2111.07549.pdf)
>  Recent advancements in end-to-end speech synthesis have made it possible to generate highly natural speech. However, training these models typically requires a large amount of high-fidelity speech data, and for unseen texts, the prosody of synthesized speech is relatively unnatural. To address these issues, we propose to combine a fine-tuned BERT-based front-end with a pre-trained FastSpeech2-based acoustic model to improve prosody modeling. The pre-trained BERT is fine-tuned on the polyphone disambiguation task, the joint Chinese word segmentation (CWS) and part-of-speech (POS) tagging task, and the prosody structure prediction (PSP) task in a multi-task learning framework. FastSpeech 2 is pre-trained on large-scale external data that are noisy but easier to obtain. Experimental results show that both the fine-tuned BERT model and the pre-trained FastSpeech 2 can improve prosody, especially for those structurally complex sentences.      
### 74.Time-Frequency Attention for Monaural Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2111.07518.pdf)
>  Most studies on speech enhancement generally don't consider the energy distribution of speech in time-frequency (T-F) representation, which is important for accurate prediction of mask or spectra. In this paper, we present a simple yet effective T-F attention (TFA) module, where a 2-D attention map is produced to provide differentiated weights to the spectral components of T-F representation. To validate the effectiveness of our proposed TFA module, we use the residual temporal convolution network (ResTCN) as the backbone network and conduct extensive experiments on two commonly used training targets. Our experiments demonstrate that applying our TFA module significantly improves the performance in terms of five objective evaluation metrics with negligible parameter overhead. The evaluation results show that the proposed ResTCN with the TFA module (ResTCN+TFA) consistently outperforms other baselines by a large margin.      
### 75.Convergence and Equilibria Analysis of a Networked Bivirus Epidemic Model  [ :arrow_down: ](https://arxiv.org/pdf/2111.07507.pdf)
>  This paper studies a networked bivirus model, in which two competing viruses spread across a network of interconnected populations; each node represents a population with a large number of individuals. The viruses may spread through possibly different network structures, and an individual cannot be simultaneously infected with both viruses. Focusing on convergence and equilibria analysis, a number of new results are provided. First, we show that for networks with generic system parameters, there exist a finite number of equilibria. Exploiting monotone systems theory, we further prove that for bivirus networks with generic system parameters, then convergence to an equilibrium occurs for all initial conditions, except possibly for a set of measure zero. Given the network structure of one virus, a method is presented to construct an infinite family of network structures for the other virus that results in an infinite number of equilibria in which both viruses coexist. Necessary and sufficient conditions are derived for the local stability/instability of boundary equilibria, in which one virus is present and the other is extinct. A sufficient condition for a boundary equilibrium to be almost globally stable is presented. Then, we show how to use monotone systems theory to generate conclusions on the ordering of stable and unstable equilibria, and in some instances identify the number of equilibria via rapid simulation testing. Last, we provide an analytical method for computing equilibria in networks with only two nodes, and show that it is possible for a bivirus network to have an unstable coexistence equilibrium and two locally stable boundary equilibria.      
### 76.Power Allocation for Wireless Federated Learning using Graph Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07480.pdf)
>  We propose a data-driven approach for power allocation in the context of federated learning (FL) over interference-limited wireless networks. The power policy is designed to maximize the transmitted information during the FL process under communication constraints, with the ultimate objective of improving the accuracy and efficiency of the global FL model being trained. The proposed power allocation policy is parameterized using a graph convolutional network and the associated constrained optimization problem is solved through a primal-dual algorithm. Numerical experiments show that the proposed method outperforms three baseline methods in both transmission success rate and FL global performance.      
### 77.Towards Interpretability of Speech Pause in Dementia Detection using Adversarial Learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.07454.pdf)
>  Speech pause is an effective biomarker in dementia detection. Recent deep learning models have exploited speech pauses to achieve highly accurate dementia detection, but have not exploited the interpretability of speech pauses, i.e., what and how positions and lengths of speech pauses affect the result of dementia detection. In this paper, we will study the positions and lengths of dementia-sensitive pauses using adversarial learning approaches. Specifically, we first utilize an adversarial attack approach by adding the perturbation to the speech pauses of the testing samples, aiming to reduce the confidence levels of the detection model. Then, we apply an adversarial training approach to evaluate the impact of the perturbation in training samples on the detection model. We examine the interpretability from the perspectives of model accuracy, pause context, and pause length. We found that some pauses are more sensitive to dementia than other pauses from the model's perspective, e.g., speech pauses near to the verb "is". Increasing lengths of sensitive pauses or adding sensitive pauses leads the model inference to Alzheimer's Disease, while decreasing the lengths of sensitive pauses or deleting sensitive pauses leads to non-AD.      
### 78.A Comparative Study of Fingerprint Image-Quality Estimation Methods  [ :arrow_down: ](https://arxiv.org/pdf/2111.07432.pdf)
>  One of the open issues in fingerprint verification is the lack of robustness against image-quality degradation. Poor-quality images result in spurious and missing features, thus degrading the performance of the overall system. Therefore, it is important for a fingerprint recognition system to estimate the quality and validity of the captured fingerprint images. In this work, we review existing approaches for fingerprint image-quality estimation, including the rationale behind the published measures and visual examples showing their behavior under different quality conditions. We have also tested a selection of fingerprint image-quality estimation algorithms. For the experiments, we employ the BioSec multimodal baseline corpus, which includes 19200 fingerprint images from 200 individuals acquired in two sessions with three different sensors. The behavior of the selected quality measures is compared, showing high correlation between them in most cases. The effect of low-quality samples in the verification performance is also studied for a widely available minutiae-based fingerprint matching system.      
### 79.Read-and-Run Constrained Coding for Modern Flash Devices  [ :arrow_down: ](https://arxiv.org/pdf/2111.07415.pdf)
>  The pivotal storage density win achieved by solid-state devices over magnetic devices in 2015 is a result of multiple innovations in physics, architecture, and signal processing. One of the most important innovations in that regard is enabling the storage of more than one bit per cell in the Flash device, i.e., having more than two charge levels per cell. Constrained coding is used in Flash devices to increase reliability via mitigating inter-cell interference that stems from charge propagation among cells. Recently, capacity-achieving constrained codes were introduced to serve that purpose in modern Flash devices, which have more than two levels per cell. While these codes result in minimal redundancy via exploiting the underlying physics, they result in non-negligible complexity increase and access speed limitation since pages cannot be read separately. In this paper, we suggest new constrained coding schemes that have low-complexity and preserve the desirable high access speed in modern Flash devices. The idea is to eliminate error-prone patterns by coding data only on the left-most page while leaving data on all the remaining pages uncoded. Our coding schemes work for any number of levels per cell, offer systematic encoding and decoding, and are capacity-approaching. Since the proposed schemes enable the separation of pages, we refer to them as read-and-run (RR) constrained coding schemes as opposed to schemes adopting read-and-wait for other pages. We analyze the new RR coding schemes and discuss their impact on the probability of occurrence of different charge levels. We also demonstrate the performance improvement achieved via RR coding on a practical triple-level cell Flash device.      
### 80.Textless Speech Emotion Conversion using Decomposed and Discrete Representations  [ :arrow_down: ](https://arxiv.org/pdf/2111.07402.pdf)
>  Speech emotion conversion is the task of modifying the perceived emotion of a speech utterance while preserving the lexical content and speaker identity. In this study, we cast the problem of emotion conversion as a spoken language translation task. We decompose speech into discrete and disentangled learned representations, consisting of content units, F0, speaker, and emotion. First, we modify the speech content by translating the content units to a target emotion, and then predict the prosodic features based on these units. Finally, the speech waveform is generated by feeding the predicted representations into a neural vocoder. Such a paradigm allows us to go beyond spectral and parametric changes of the signal, and model non-verbal vocalizations, such as laughter insertion, yawning removal, etc. We demonstrate objectively and subjectively that the proposed method is superior to the baselines in terms of perceived emotion and audio quality. We rigorously evaluate all components of such a complex system and conclude with an extensive model analysis and ablation study to better emphasize the architectural choices, strengths and weaknesses of the proposed method. Samples and code will be publicly available under the following link: <a class="link-external link-https" href="https://speechbot.github.io/emotion" rel="external noopener nofollow">this https URL</a>.      
### 81.Neural Capacity Estimators: How Reliable Are They?  [ :arrow_down: ](https://arxiv.org/pdf/2111.07401.pdf)
>  Recently, several methods have been proposed for estimating the mutual information from sample data using deep neural networks and without the knowing closed form distribution of the data. This class of estimators is referred to as neural mutual information estimators. Although very promising, such techniques have yet to be rigorously bench-marked so as to establish their efficacy, ease of implementation, and stability for capacity estimation which is joint maximization frame-work. In this paper, we compare the different techniques proposed in the literature for estimating capacity and provide a practitioner perspective on their effectiveness. In particular, we study the performance of mutual information neural estimator (MINE), smoothed mutual information lower-bound estimator (SMILE), and directed information neural estimator (DINE) and provide insights on InfoNCE. We evaluated these algorithms in terms of their ability to learn the input distributions that are capacity approaching for the AWGN channel, the optical intensity channel, and peak power-constrained AWGN channel. For both scenarios, we provide insightful comments on various aspects of the training process, such as stability, sensitivity to initialization.      
### 82.Interpretable ECG classification via a query-based latent space traversal (qLST)  [ :arrow_down: ](https://arxiv.org/pdf/2111.07386.pdf)
>  Electrocardiography (ECG) is an effective and non-invasive diagnostic tool that measures the electrical activity of the heart. Interpretation of ECG signals to detect various abnormalities is a challenging task that requires expertise. Recently, the use of deep neural networks for ECG classification to aid medical practitioners has become popular, but their black box nature hampers clinical implementation. Several saliency-based interpretability techniques have been proposed, but they only indicate the location of important features and not the actual features. We present a novel interpretability technique called qLST, a query-based latent space traversal technique that is able to provide explanations for any ECG classification model. With qLST, we train a neural network that learns to traverse in the latent space of a variational autoencoder trained on a large university hospital dataset with over 800,000 ECGs annotated for 28 diseases. We demonstrate through experiments that we can explain different black box classifiers by generating ECGs through these traversals.      
### 83.On Optimizing Rate Splitting in Laser-based Optical Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2111.07341.pdf)
>  Optical wireless communication (OWC) is a promising technology that has the potential to provide Tb/s aggregate rates. In this paper, interference management is studied in a Laser-based optical wireless network where vertical-cavity surface-emitting (VCSEL) lasers are used for data transmission. In particular, rate splitting (RS) and hierarchical rate splitting (HRS) are proposed to align multi-user interference, while maximizing the multiplexing gain of the network. Basically, RS serves multiple users simultaneously by splitting a message of a user into common and private messages, each message with a certain level of power, while on the other side users decode their messages following a specific methodology. The performance of the conventional RS scheme is limited in high density wireless networks. Therefore, the HRS scheme is developed aiming to achieve high rates where users are divided into multiple groups, and a new message called outer common message is used for managing inter-group interference. We formulate an optimization problem that addresses power allocation among the messages of the HRS scheme to further enhance the performance of the network. The results show that the proposed approach provides high achievable rates compared with the conventional RS and HRS schemes in different scenarios.      
### 84.Speech Emotion Recognition System by Quaternion Nonlinear Echo State Network  [ :arrow_down: ](https://arxiv.org/pdf/2111.07234.pdf)
>  The echo state network (ESN) is a powerful and efficient tool for displaying dynamic data. However, many existing ESNs have limitations for properly modeling high-dimensional data. The most important limitation of these networks is the high memory consumption due to their reservoir structure, which has prevented the increase of reservoir units and the maximum use of special capabilities of this type of network. One way to solve this problem is to use quaternion algebra. Because quaternions have four different dimensions, high-dimensional data are easily represented and, using Hamilton multiplication, with fewer parameters than real numbers, make external relations between the multidimensional features easier. In addition to the memory problem in the ESN network, the linear output of the ESN network poses an indescribable limit to its processing capacity, as it cannot effectively utilize higher-order statistics of features provided by the nonlinear dynamics of reservoir neurons. In this research, a new structure based on ESN is presented, in which quaternion algebra is used to compress the network data with the simple split function, and the output linear combiner is replaced by a multidimensional bilinear filter. This filter will be used for nonlinear calculations of the output layer of the ESN. In addition, the two-dimensional principal component analysis technique is used to reduce the number of data transferred to the bilinear filter. In this study, the coefficients and the weights of the quaternion nonlinear ESN (QNESN) are optimized using the genetic algorithm. In order to prove the effectiveness of the proposed model compared to the previous methods, experiments for speech emotion recognition have been performed on EMODB, SAVEE, and IEMOCAP speech emotional datasets. Comparisons show that the proposed QNESN network performs better than the ESN and most currently SER systems.      
### 85.Developing a Novel Approach for Periapical Dental Radiographs Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2111.07156.pdf)
>  Image processing techniques has been widely used in dental researches such as human identification and forensic dentistry, teeth numbering, dental carries detection and periodontal disease analysis. One of the most challenging parts in dental imaging is teeth segmentation and how to separate them from each other. In this paper, an automated method for teeth segmentation of Periapical dental x-ray images which contain at least one root-canalled tooth is proposed. The result of this approach can be used as an initial step in bone lesion detection. The proposed algorithm is made of two stages. The first stage is pre-processing. The second and main part of this algorithm calculated rotation degree and uses the integral projection method for tooth isolation. Experimental results show that this algorithm is robust and achieves high accuracy.      
### 86.MC-CIM: Compute-in-Memory with Monte-Carlo Dropouts for Bayesian Edge Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2111.07125.pdf)
>  We propose MC-CIM, a compute-in-memory (CIM) framework for robust, yet low power, Bayesian edge intelligence. Deep neural networks (DNN) with deterministic weights cannot express their prediction uncertainties, thereby pose critical risks for applications where the consequences of mispredictions are fatal such as surgical robotics. To address this limitation, Bayesian inference of a DNN has gained attention. Using Bayesian inference, not only the prediction itself, but the prediction confidence can also be extracted for planning risk-aware actions. However, Bayesian inference of a DNN is computationally expensive, ill-suited for real-time and/or edge deployment. An approximation to Bayesian DNN using Monte Carlo Dropout (MC-Dropout) has shown high robustness along with low computational complexity. Enhancing the computational efficiency of the method, we discuss a novel CIM module that can perform in-memory probabilistic dropout in addition to in-memory weight-input scalar product to support the method. We also propose a compute-reuse reformulation of MC-Dropout where each successive instance can utilize the product-sum computations from the previous iteration. Even more, we discuss how the random instances can be optimally ordered to minimize the overall MC-Dropout workload by exploiting combinatorial optimization methods. Application of the proposed CIM-based MC-Dropout execution is discussed for MNIST character recognition and visual odometry (VO) of autonomous drones. The framework reliably gives prediction confidence amidst non-idealities imposed by MC-CIM to a good extent. Proposed MC-CIM with 16x31 SRAM array, 0.85 V supply, 16nm low-standby power (LSTP) technology consumes 27.8 pJ for 30 MC-Dropout instances of probabilistic inference in its most optimal computing and peripheral configuration, saving 43% energy compared to typical execution.      
### 87.5 Gbps Optical Wireless Communication using Commercial SPAD Array Receivers  [ :arrow_down: ](https://arxiv.org/pdf/2111.07123.pdf)
>  Photon counting detectors such as single-photon avalanche diode (SPAD) arrays can be utilized to improve the sensitivity of optical wireless communication (OWC) systems. However, the achievable data rate of SPAD-based OWC systems is strongly limited by the nonlinearity induced by SPAD dead time. In this work, the performances of SPAD receivers for two different modulation schemes, namely, on-off keying (OOK) and orthogonal frequency division multiplexing (OFDM), are compared demonstrating contrasting optimal regimes of operation. We employ nonlinear equalization and peak-to-average power ratio optimization by adjusting the OFDM clipping level to achieve record experimental data rates of up to 5 Gbps. In particular, the experimental results demonstrate the achievable data rates of 3.22 Gbps and 5 Gbps when OOK and OFDM are employed, respectively. It is also illustrated that to achieve the best data rate performance over a wide range of received power, adaptive switching between OOK and OFDM may be utilized.      
### 88.Direct Noisy Speech Modeling for Noisy-to-Noisy Voice Conversion  [ :arrow_down: ](https://arxiv.org/pdf/2111.07116.pdf)
>  Beyond the conventional voice conversion (VC) where the speaker information is converted without altering the linguistic content, the background sounds are informative and need to be retained in some real-world scenarios, such as VC in movie/video and VC in music where the voice is entangled with background sounds. As a new VC framework, we have developed a noisy-to-noisy (N2N) VC framework to convert the speaker's identity while preserving the background sounds. Although our framework consisting of a denoising module and a VC module well handles the background sounds, the VC module is sensitive to the distortion caused by the denoising module. To address this distortion issue, in this paper we propose the improved VC module to directly model the noisy speech waveform while controlling the background sounds. The experimental results have demonstrated that our improved framework significantly outperforms the previous one and achieves an acceptable score in terms of naturalness, while reaching comparable similarity performance to the upper bound of our framework.      
### 89.Posetal Games: Efficiency, Existence, and Refinement of Equilibria in Games with Prioritized Metrics  [ :arrow_down: ](https://arxiv.org/pdf/2111.07099.pdf)
>  Modern applications require robots to comply with multiple, often conflicting rules and to interact with the other agents. We present Posetal Games as a class of games in which each player expresses a preference over the outcomes via a partially ordered set of metrics. This allows one to combine hierarchical priorities of each player with the interactive nature of the environment. By contextualizing standard game theoretical notions, we provide two sufficient conditions on the preference of the players to prove existence of pure Nash Equilibria in finite action sets. Moreover, we define formal operations on the preference structures and link them to a refinement of the game solutions, showing how the set of equilibria can be systematically shrunk. The presented results are showcased in a driving game where autonomous vehicles select from a finite set of trajectories. The results demonstrate the interpretability of results in terms of minimum-rank-violation for each player.      
### 90.Speech Emotion Recognition Using Deep Sparse Auto-Encoder Extreme Learning Machine with a New Weighting Scheme and Spectro-Temporal Features Along with Classical Feature Selection and A New Quantum-Inspired Dimension Reduction Method  [ :arrow_down: ](https://arxiv.org/pdf/2111.07094.pdf)
>  Affective computing is very important in the relationship between man and machine. In this paper, a system for speech emotion recognition (SER) based on speech signal is proposed, which uses new techniques in different stages of processing. The system consists of three stages: feature extraction, feature selection, and finally feature classification. In the first stage, a complex set of long-term statistics features is extracted from both the speech signal and the glottal-waveform signal using a combination of new and diverse features such as prosodic, spectral, and spectro-temporal features. One of the challenges of the SER systems is to distinguish correlated emotions. These features are good discriminators for speech emotions and increase the SER's ability to recognize similar and different emotions. This feature vector with a large number of dimensions naturally has redundancy. In the second stage, using classical feature selection techniques as well as a new quantum-inspired technique to reduce the feature vector dimensionality, the number of feature vector dimensions is reduced. In the third stage, the optimized feature vector is classified by a weighted deep sparse extreme learning machine (ELM) classifier. The classifier performs classification in three steps: sparse random feature learning, orthogonal random projection using the singular value decomposition (SVD) technique, and discriminative classification in the last step using the generalized Tikhonov regularization technique. Also, many existing emotional datasets suffer from the problem of data imbalanced distribution, which in turn increases the classification error and decreases system performance. In this paper, a new weighting method has also been proposed to deal with class imbalance, which is more efficient than existing weighting methods. The proposed method is evaluated on three standard emotional databases.      
### 91.Evaluating Contrastive Learning on Wearable Timeseries for Downstream Clinical Outcomes  [ :arrow_down: ](https://arxiv.org/pdf/2111.07089.pdf)
>  Vast quantities of person-generated health data (wearables) are collected but the process of annotating to feed to machine learning models is impractical. This paper discusses ways in which self-supervised approaches that use contrastive losses, such as SimCLR and BYOL, previously applied to the vision domain, can be applied to high-dimensional health signals for downstream classification tasks of various diseases spanning sleep, heart, and metabolic conditions. To this end, we adapt the data augmentation step and the overall architecture to suit the temporal nature of the data (wearable traces) and evaluate on 5 downstream tasks by comparing other state-of-the-art methods including supervised learning and an adversarial unsupervised representation learning method. We show that SimCLR outperforms the adversarial method and a fully-supervised method in the majority of the downstream evaluation tasks, and that all self-supervised methods outperform the fully-supervised methods. This work provides a comprehensive benchmark for contrastive methods applied to the wearable time-series domain, showing the promise of task-agnostic representations for downstream clinical outcomes.      
### 92.Hyperspectral Mixed Noise Removal via Subspace Representation and Weighted Low-rank Tensor Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2111.07044.pdf)
>  Recently, the low-rank property of different components extracted from the image has been considered in man hyperspectral image denoising methods. However, these methods usually unfold the 3D tensor to 2D matrix or 1D vector to exploit the prior information, such as nonlocal spatial self-similarity (NSS) and global spectral correlation (GSC), which break the intrinsic structure correlation of hyperspectral image (HSI) and thus lead to poor restoration quality. In addition, most of them suffer from heavy computational burden issues due to the involvement of singular value decomposition operation on matrix and tensor in the original high-dimensionality space of HSI. We employ subspace representation and the weighted low-rank tensor regularization (SWLRTR) into the model to remove the mixed noise in the hyperspectral image. Specifically, to employ the GSC among spectral bands, the noisy HSI is projected into a low-dimensional subspace which simplified calculation. After that, a weighted low-rank tensor regularization term is introduced to characterize the priors in the reduced image subspace. Moreover, we design an algorithm based on alternating minimization to solve the nonconvex problem. Experiments on simulated and real datasets demonstrate that the SWLRTR method performs better than other hyperspectral denoising methods quantitatively and visually.      
### 93.Agile Satellite Planning for Multi-Payload Observations for Earth Science  [ :arrow_down: ](https://arxiv.org/pdf/2111.07042.pdf)
>  We present planning challenges, methods and preliminary results for a new model-based paradigm for earth observing systems in adaptive remote sensing. Our heuristically guided constraint optimization planner produces coordinated plans for multiple satellites, each with multiple instruments (payloads). The satellites are agile, meaning they can quickly maneuver to change viewing angles in response to rapidly changing phenomena. The planner operates in a closed-loop context, updating the plan as it receives regular sensor data and updated predictions. We describe the planner's search space and search procedure, and present preliminary experiment results. Contributions include initial identification of the planner's search space, constraints, heuristics, and performance metrics applied to a soil moisture monitoring scenario using spaceborne radars.      
### 94.Identification and Adaptive Control of Markov Jump Systems: Sample Complexity and Regret Bounds  [ :arrow_down: ](https://arxiv.org/pdf/2111.07018.pdf)
>  Learning how to effectively control unknown dynamical systems is crucial for intelligent autonomous systems. This task becomes a significant challenge when the underlying dynamics are changing with time. Motivated by this challenge, this paper considers the problem of controlling an unknown Markov jump linear system (MJS) to optimize a quadratic objective. By taking a model-based perspective, we consider identification-based adaptive control for MJSs. We first provide a system identification algorithm for MJS to learn the dynamics in each mode as well as the Markov transition matrix, underlying the evolution of the mode switches, from a single trajectory of the system states, inputs, and modes. Through mixing-time arguments, sample complexity of this algorithm is shown to be $\mathcal{O}(1/\sqrt{T})$. We then propose an adaptive control scheme that performs system identification together with certainty equivalent control to adapt the controllers in an episodic fashion. Combining our sample complexity results with recent perturbation results for certainty equivalent control, we prove that when the episode lengths are appropriately chosen, the proposed adaptive control scheme achieves $\mathcal{O}(\sqrt{T})$ regret, which can be improved to $\mathcal{O}(polylog(T))$ with partial knowledge of the system. Our proof strategy introduces innovations to handle Markovian jumps and a weaker notion of stability common in MJSs. Our analysis provides insights into system theoretic quantities that affect learning accuracy and control performance. Numerical simulations are presented to further reinforce these insights.      
### 95.Leveraging Unsupervised Image Registration for Discovery of Landmark Shape Descriptor  [ :arrow_down: ](https://arxiv.org/pdf/2111.07009.pdf)
>  In current biological and medical research, statistical shape modeling (SSM) provides an essential framework for the characterization of anatomy/morphology. Such analysis is often driven by the identification of a relatively small number of geometrically consistent features found across the samples of a population. These features can subsequently provide information about the population shape variation. Dense correspondence models can provide ease of computation and yield an interpretable low-dimensional shape descriptor when followed by dimensionality reduction. However, automatic methods for obtaining such correspondences usually require image segmentation followed by significant preprocessing, which is taxing in terms of both computation as well as human resources. In many cases, the segmentation and subsequent processing require manual guidance and anatomy specific domain expertise. This paper proposes a self-supervised deep learning approach for discovering landmarks from images that can directly be used as a shape descriptor for subsequent analysis. We use landmark-driven image registration as the primary task to force the neural network to discover landmarks that register the images well. We also propose a regularization term that allows for robust optimization of the neural network and ensures that the landmarks uniformly span the image domain. The proposed method circumvents segmentation and preprocessing and directly produces a usable shape descriptor using just 2D or 3D images. In addition, we also propose two variants on the training loss function that allows for prior shape information to be integrated into the model. We apply this framework on several 2D and 3D datasets to obtain their shape descriptors, and analyze their utility for various applications.      
### 96.Adversarially Robust Learning for Security-Constrained Optimal Power Flow  [ :arrow_down: ](https://arxiv.org/pdf/2111.06961.pdf)
>  In recent years, the ML community has seen surges of interest in both adversarially robust learning and implicit layers, but connections between these two areas have seldom been explored. In this work, we combine innovations from these areas to tackle the problem of N-k security-constrained optimal power flow (SCOPF). N-k SCOPF is a core problem for the operation of electrical grids, and aims to schedule power generation in a manner that is robust to potentially k simultaneous equipment outages. Inspired by methods in adversarially robust training, we frame N-k SCOPF as a minimax optimization problem - viewing power generation settings as adjustable parameters and equipment outages as (adversarial) attacks - and solve this problem via gradient-based techniques. The loss function of this minimax problem involves resolving implicit equations representing grid physics and operational decisions, which we differentiate through via the implicit function theorem. We demonstrate the efficacy of our framework in solving N-3 SCOPF, which has traditionally been considered as prohibitively expensive to solve given that the problem size depends combinatorially on the number of potential outages.      
### 97.Outage Analysis over Correlated Fisher-Snedecor F Fading Multi-User Channels  [ :arrow_down: ](https://arxiv.org/pdf/2111.06921.pdf)
>  In this paper, we investigate the impact of correlated fading on the performance of wireless multiple access channels (MAC) in the presence and absence of side information (SI) at transmitters, where the fading coefficients are modeled according to the Fisher-Snedecor F distribution. Specifically, we represent two scenarios: (i) clear MAC (i.e, without SI at transmitters), (ii) doubly dirty MAC (i.e., with the non-causally known SI at transmitters). For both system models, we derive the closed-form expressions for the outage probability in independent fading conditions. Besides, exploiting copula theory, we obtain exact analytical expressions for the outage probability under the positive dependence fading conditions in both considered models. Finally, the efficiency of the analytical results is illustrated numerically.      
### 98.Neural optimal feedback control with local learning rules  [ :arrow_down: ](https://arxiv.org/pdf/2111.06920.pdf)
>  A major problem in motor control is understanding how the brain plans and executes proper movements in the face of delayed and noisy stimuli. A prominent framework for addressing such control problems is Optimal Feedback Control (OFC). OFC generates control actions that optimize behaviorally relevant criteria by integrating noisy sensory stimuli and the predictions of an internal model using the Kalman filter or its extensions. However, a satisfactory neural model of Kalman filtering and control is lacking because existing proposals have the following limitations: not considering the delay of sensory feedback, training in alternating phases, and requiring knowledge of the noise covariance matrices, as well as that of systems dynamics. Moreover, the majority of these studies considered Kalman filtering in isolation, and not jointly with control. To address these shortcomings, we introduce a novel online algorithm which combines adaptive Kalman filtering with a model free control approach (i.e., policy gradient algorithm). We implement this algorithm in a biologically plausible neural network with local synaptic plasticity rules. This network performs system identification and Kalman filtering, without the need for multiple phases with distinct update rules or the knowledge of the noise covariances. It can perform state estimation with delayed sensory feedback, with the help of an internal model. It learns the control policy without requiring any knowledge of the dynamics, thus avoiding the need for weight transport. In this way, our implementation of OFC solves the credit assignment problem needed to produce the appropriate sensory-motor control in the presence of stimulus delay.      
### 99.Symbolic Regression Methods for Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/1903.09688.pdf)
>  Reinforcement learning algorithms can solve dynamic decision-making and optimal control problems. With continuous-valued state and input variables, reinforcement learning algorithms must rely on function approximators to represent the value function and policy mappings. Commonly used numerical approximators, such as neural networks or basis function expansions, have two main drawbacks: they are black-box models offering little insight into the mappings learned, and they require extensive trial and error tuning of their hyper-parameters. In this paper, we propose a new approach to constructing smooth value functions in the form of analytic expressions by using symbolic regression. We introduce three off-line methods for finding value functions based on a state-transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. The methods are illustrated on four nonlinear control problems: velocity control under friction, one-link and two-link pendulum swing-up, and magnetic manipulation. The results show that the value functions yield well-performing policies and are compact, mathematically tractable, and easy to plug into other algorithms. This makes them potentially suitable for further analysis of the closed-loop system. A comparison with an alternative approach using neural networks shows that our method outperforms the neural network-based one.      
