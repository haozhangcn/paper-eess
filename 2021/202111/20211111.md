# ArXiv eess --Thu, 11 Nov 2021
### 1.Early Myocardial Infarction Detection over Multi-view Echocardiography  [ :arrow_down: ](https://arxiv.org/pdf/2111.05790.pdf)
>  Myocardial infarction (MI) is the leading cause of mortality in the world that occurs due to a blockage of the coronary arteries feeding the myocardium. An early diagnosis of MI and its localization can mitigate the extent of myocardial damage by facilitating early therapeutic interventions. Following the blockage of a coronary artery, the regional wall motion abnormality (RWMA) of the ischemic myocardial segments is the earliest change to set in. Echocardiography is the fundamental tool to assess any RWMA. Assessing the motion of the left ventricle (LV) wall only from a single echocardiography view may lead to missing the diagnosis of MI as the RWMA may not be visible on that specific view. Therefore, in this study, we propose to fuse apical 4-chamber (A4C) and apical 2-chamber (A2C) views in which a total of 11 myocardial segments can be analyzed for MI detection. The proposed method first estimates the motion of the LV wall by Active Polynomials (APs), which extract and track the endocardial boundary to compute myocardial segment displacements. The features are extracted from the A4C and A2C view displacements, which are fused and fed into the classifiers to detect MI. The main contributions of this study are 1) creation of a new benchmark dataset by including both A4C and A2C views in a total of 260 echocardiography recordings, which is publicly shared with the research community, 2) improving the performance of the prior work of threshold-based APs by a Machine Learning based approach, and 3) a pioneer MI detection approach via multi-view echocardiography by fusing the information of A4C and A2C views. Experimental results show that the proposed method achieves 90.91% sensitivity and 86.36% precision for MI detection over multi-view echocardiography.      
### 2.Evaluation of Deep Learning Topcoders Method for Neuron Individualization in Histological Macaque Brain Section  [ :arrow_down: ](https://arxiv.org/pdf/2111.05789.pdf)
>  Cell individualization has a vital role in digital pathology image analysis. Deep Learning is considered as an efficient tool for instance segmentation tasks, including cell individualization. However, the precision of the Deep Learning model relies on massive unbiased dataset and manual pixel-level annotations, which is labor intensive. Moreover, most applications of Deep Learning have been developed for processing oncological data. To overcome these challenges, i) we established a pipeline to synthesize pixel-level labels with only point annotations provided; ii) we tested an ensemble Deep Learning algorithm to perform cell individualization on neurological data. Results suggest that the proposed method successfully segments neuronal cells in both object-level and pixel-level, with an average detection accuracy of 0.93.      
### 3.Maximum-distance Race Strategies for a Fully Electric Endurance Race Car  [ :arrow_down: ](https://arxiv.org/pdf/2111.05784.pdf)
>  This paper presents a bi-level optimization framework to compute the maximum-distance stint and charging strategies for a fully electric endurance race car. Thereby, the lower level computes the minimum-stint-time Powertrain Operation (PO) for a given battery energy budget and stint length, whilst the upper level leverages that information to jointly optimize the stint length, charge time and number of pit stops, in order to maximize the driven distance in the course of a fixed-time endurance race. Specifically, we first extend a convex lap time optimization framework to capture multiple laps and force-based electric motor models, and use it to create a map linking the charge time and stint length to the achievable stint time. Second, we leverage the map to frame the maximum-race-distance problem as a mixed-integer second order conic program that can be efficiently solved to the global optimum with off-the-shelf optimization algorithms. Finally, we showcase our framework on a 6 h race around the Zandvoort circuit. Our results show that a flat-out strategy can be extremely detrimental, and that, compared to when the stints are optimized for a fixed number of pit stops, jointly optimizing the stints and number of pit stops can increase the driven distance of several laps.      
### 4.OSSEM: one-shot speaker adaptive speech enhancement using meta learning  [ :arrow_down: ](https://arxiv.org/pdf/2111.05703.pdf)
>  Although deep learning (DL) has achieved notable progress in speech enhancement (SE), further research is still required for a DL-based SE system to adapt effectively and efficiently to particular speakers. In this study, we propose a novel meta-learning-based speaker-adaptive SE approach (called OSSEM) that aims to achieve SE model adaptation in a one-shot manner. OSSEM consists of a modified transformer SE network and a speaker-specific masking (SSM) network. In practice, the SSM network takes an enrolled speaker embedding extracted using ECAPA-TDNN to adjust the input noisy feature through masking. To evaluate OSSEM, we designed a modified Voice Bank-DEMAND dataset, in which one utterance from the testing set was used for model adaptation, and the remaining utterances were used for testing the performance. Moreover, we set restrictions allowing the enhancement process to be conducted in real time, and thus designed OSSEM to be a causal SE system. Experimental results first show that OSSEM can effectively adapt a pretrained SE model to a particular speaker with only one utterance, thus yielding improved SE results. Meanwhile, OSSEM exhibits a competitive performance compared to state-of-the-art causal SE systems.      
### 5.HASA-net: A non-intrusive hearing-aid speech assessment network  [ :arrow_down: ](https://arxiv.org/pdf/2111.05691.pdf)
>  Without the need of a clean reference, non-intrusive speech assessment methods have caught great attention for objective evaluations. Recently, deep neural network (DNN) models have been applied to build non-intrusive speech assessment approaches and confirmed to provide promising performance. However, most DNN-based approaches are designed for normal-hearing listeners without considering hearing-loss factors. In this study, we propose a DNN-based hearing aid speech assessment network (HASA-Net), formed by a bidirectional long short-term memory (BLSTM) model, to predict speech quality and intelligibility scores simultaneously according to input speech signals and specified hearing-loss patterns. To the best of our knowledge, HASA-Net is the first work to incorporate quality and intelligibility assessments utilizing a unified DNN-based non-intrusive model for hearing aids. Experimental results show that the predicted speech quality and intelligibility scores of HASA-Net are highly correlated to two well-known intrusive hearing-aid evaluation metrics, hearing aid speech quality index (HASQI) and hearing aid speech perception index (HASPI), respectively.      
### 6.Robust channel-wise illumination estimation  [ :arrow_down: ](https://arxiv.org/pdf/2111.05681.pdf)
>  Recently, Convolutional Neural Networks (CNNs) have been widely used to solve the illuminant estimation problem and have often led to state-of-the-art results. Standard approaches operate directly on the input image. In this paper, we argue that this problem can be decomposed into three channel-wise independent and symmetric sub-problems and propose a novel CNN-based illumination estimation approach based on this decomposition. The proposed method substantially reduces the number of parameters needed to solve the task while achieving competitive experimental results compared to state-of-the-art methods. Furthermore, the practical application of illumination estimation techniques typically requires identifying the extreme error cases. This can be achieved using an uncertainty estimation technique. In this work, we propose a novel color constancy uncertainty estimation approach that augments the trained model with an auxiliary branch which learns to predict the error based on the feature representation. Intuitively, the model learns which feature combinations are robust and are thus likely to yield low errors and which combinations result in erroneous estimates. We test this approach on the proposed method and show that it can indeed be used to avoid several extreme error cases and, thus, improves the practicality of the proposed technique.      
### 7.Explanatory Analysis and Rectification of the Pitfalls in COVID-19 Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2111.05679.pdf)
>  Since the onset of the COVID-19 pandemic in 2020, millions of people have succumbed to this deadly virus. Many attempts have been made to devise an automated method of testing that could detect the virus. Various researchers around the globe have proposed deep learning based methodologies to detect the COVID-19 using Chest X-Rays. However, questions have been raised on the presence of bias in the publicly available Chest X-Ray datasets which have been used by the majority of the researchers. In this paper, we propose a 2 staged methodology to address this topical issue. Two experiments have been conducted as a part of stage 1 of the methodology to exhibit the presence of bias in the datasets. Subsequently, an image segmentation, super-resolution and CNN based pipeline along with different image augmentation techniques have been proposed in stage 2 of the methodology to reduce the effect of bias. InceptionResNetV2 trained on Chest X-Ray images that were augmented with Histogram Equalization followed by Gamma Correction when passed through the pipeline proposed in stage 2, yielded a top accuracy of 90.47% for 3-class (Normal, Pneumonia, and COVID-19) classification task.      
### 8.Spectrum Allocation with Adaptive Sub-band Bandwidth for Terahertz Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2111.05629.pdf)
>  We study spectrum allocation for terahertz (THz) band communication (THzCom) systems, while considering the frequency and distance-dependent nature of THz channels. Different from existing studies, we explore multi-band-based spectrum allocation with adaptive sub-band bandwidth (ASB) by allowing the spectrum of interest to be divided into sub-bands with unequal bandwidths. Also, we investigate the impact of sub-band assignment on multi-connectivity (MC) enabled THzCom systems, where users associate and communicate with multiple access points simultaneously. We formulate resource allocation problems, with the primary focus on spectrum allocation, to determine sub-band assignment, sub-band bandwidth, and optimal transmit power. Thereafter, we propose reasonable approximations and transformations, and develop iterative algorithms based on the successive convex approximation technique to analytically solve the formulated problems. Aided by numerical results, we show that by enabling and optimizing ASB, significantly higher throughput can be achieved as compared to adopting equal sub-band bandwidth, and this throughput gain is most profound when the power budget constraint is more stringent. We also show that our sub-band assignment strategy in MC-enabled THzCom systems outperforms the state-of-the-art sub-band assignment strategies and the performance gain is most profound when the spectrum with the lowest average molecular absorption coefficient is selected during spectrum allocation.      
### 9.An efficient solver for designing optimal sampling schemes  [ :arrow_down: ](https://arxiv.org/pdf/2111.05579.pdf)
>  In this short paper, we describe an efficient numerical solver for the optimal sampling problem considered in "Designing Sampling Schemes for Multi-Dimensional Data". An implementation may be found on <a class="link-external link-https" href="https://www.maths.lu.se/staff/andreas-jakobsson/publications/" rel="external noopener nofollow">this https URL</a>.      
### 10.Can phase change materials in building insulation improve self-consumption of residential rooftop solar? An Australian case study  [ :arrow_down: ](https://arxiv.org/pdf/2111.05524.pdf)
>  This work investigates the extent to which phase change material (PCM) in the building's envelope can be used as an alternative to battery storage systems to increase self-consumption of rooftop solar photovoltaic (PV) generation. In particular, we explore the electricity cost-savings and increase in PV self-consumption that can be achieved by using PCMs and the operation of the heating, ventilation, and air conditioning (HVAC) system optimised by a home energy management system (HEMS). In more detail, we consider a HEMS with an HVAC system, rooftop PV, and a PCM layer integrated into the building envelope. The objective of the HEMS optimisation is to minimise electricity costs while maximising PV self-consumption and maintaining the indoor building temperature in a preferred comfort range. Solving this problem is challenging due to PCM's nonlinear characteristics, and using methods that can deal with the resulting non-convexity of the optimisation problem, like dynamic programming is computationally expensive. Therefore, we use multi-timescale approximate dynamic programming (MADP) that we developed in our earlier work to explore a number of Australian PCM scenarios. Specifically, we analyse a large number of residential buildings across five Australian capital cities. We find that using PCM can reduce annual electricity costs by between 10.6% in Brisbane and 19% in Adelaide. However, somewhat surprisingly, using PCM reduces PV self-consumption by between 1.5% in Brisbane and 2.7% in Perth.      
### 11.System Level Synthesis-based Robust Model Predictive Control through Convex Inner Approximation  [ :arrow_down: ](https://arxiv.org/pdf/2111.05509.pdf)
>  We propose a robust model predictive control (MPC) method for discrete-time linear time-invariant systems with norm-bounded additive disturbances and model uncertainty. In our method, at each time step we solve a finite time robust optimal control problem (OCP) which jointly searches over robust linear state feedback controllers and bounds the deviation of the system states from the nominal predicted trajectory. By leveraging the System Level Synthesis (SLS) framework, the proposed robust OCP is formulated as a convex quadratic program in the space of closed-loop system responses. When an adaptive horizon strategy is used, we prove the recursive feasibility of the proposed MPC controller and input-to-state stability of the origin for the closed-loop system. We demonstrate through numerical examples that the proposed method considerably reduces conservatism when compared with existing SLS-based and tube-based robust MPC methods, while also enjoying low computational complexity.      
### 12.Preserving Dense Features for Ki67 Nuclei Detection  [ :arrow_down: ](https://arxiv.org/pdf/2111.05482.pdf)
>  Nuclei detection is a key task in Ki67 proliferation index estimation in breast cancer images. Deep learning algorithms have shown strong potential in nuclei detection tasks. However, they face challenges when applied to pathology images with dense medium and overlapping nuclei since fine details are often diluted or completely lost by early maxpooling layers. This paper introduces an optimized UV-Net architecture, specifically developed to recover nuclear details with high-resolution through feature preservation for Ki67 proliferation index computation. UV-Net achieves an average F1-score of 0.83 on held-out test patch data, while other architectures obtain 0.74-0.79. On tissue microarrays (unseen) test data obtained from multiple centers, UV-Net's accuracy exceeds other architectures by a wide margin, including 9-42\% on Ontario Veterinary College, 7-35\% on Protein Atlas and 0.3-3\% on University Health Network.      
### 13.ASL Trigger Recognition in Mixed Activity/Signing Sequences for RF Sensor-Based User Interfaces  [ :arrow_down: ](https://arxiv.org/pdf/2111.05480.pdf)
>  The past decade has seen great advancements in speech recognition for control of interactive devices, personal assistants, and computer interfaces. However, Deaf and hard-ofhearing (HoH) individuals, whose primary mode of communication is sign language, cannot use voice-controlled interfaces. Although there has been significant work in video-based sign language recognition, video is not effective in the dark and has raised privacy concerns in the Deaf community when used in the context of human ambient intelligence. RF sensors have been recently proposed as a new modality that can be effective under the circumstances where video is not. This paper considers the problem of recognizing a trigger sign (wake word) in the context of daily living, where gross motor activities are interwoven with signing sequences. The proposed approach exploits multiple RF data domain representations (time-frequency, range-Doppler, and range-angle) for sequential classification of mixed motion data streams. The recognition accuracy of signs with varying kinematic properties is compared and used to make recommendations on appropriate trigger sign selection for RFsensor based user interfaces. The proposed approach achieves a trigger sign detection rate of 98.9% and a classification accuracy of 92% for 15 ASL words and 3 gross motor activities.      
### 14.Perceptual Quality Assessment of Colored 3D Point Clouds  [ :arrow_down: ](https://arxiv.org/pdf/2111.05474.pdf)
>  The real-world applications of 3D point clouds have been growing rapidly in recent years, but not much effective work has been dedicated to perceptual quality assessment of colored 3D point clouds. In this work, we first build a large 3D point cloud database for subjective and objective quality assessment of point clouds. We construct 20 high quality, realistic, and omni-directional point clouds of diverse contents. We then apply downsampling, Gaussian noise, and three types of compression algorithms to create 740 distorted point clouds. We carry out a subjective experiment to evaluate the quality of distorted point clouds. Our statistical analysis finds that existing objective point cloud quality assessment (PCQA) models only achieve limited success in predicting subjective quality ratings. We propose a novel objective PCQA model based on the principle of information content weighted structural similarity. Our experimental results show that the proposed model well correlates with subjective opinions and significantly outperforms the existing PCQA models. The database has been made publicly available to facilitate reproducible research at <a class="link-external link-https" href="https://github.com/qdushl/Waterloo-Point-Cloud-Database" rel="external noopener nofollow">this https URL</a>.      
### 15.Inequitable Access to EV Charging Infrastructure  [ :arrow_down: ](https://arxiv.org/pdf/2111.05437.pdf)
>  Access to and affordability of electric vehicle (EV) charging infrastructure are the two prominent barriers for EV adoption. While major efforts are underway in the United States to roll-out public EV charging infrastructure, persistent social disparities in EV adoption call for interventions. In this paper, we analyze the existing EV charging infrastructure across New York City (NYC) to identify such socio-demographic and transportation features that correlate with the current distribution of EV charging stations. Our results demonstrate that population density is not correlated with the density of EV chargers, hindering New York's EV adoption and decarbonization goals. On the contrary, the distribution of EV charging stations is heavily skewed against low-income, Black-identifying, and disinvested neighborhoods in NYC, however, positively correlated to presence of highways in a zip code. The results underscore the need for policy frameworks that incorporate equity and justice in the roll-out of EV charging infrastructure.      
### 16.Differential Modulation in Massive MIMO With Low-Resolution ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2111.05419.pdf)
>  In this paper, we present a differential modulation and detection scheme for use in the uplink of a system with a large number of antennas at the base station, each equipped with low-resolution analog-to-digital converters (ADCs). We derive an expression for the maximum likelihood (ML) detector of a differentially encoded phase information symbol received by a base station operating in the low-resolution ADC regime. We also present an equal performing reduced complexity receiver for detecting the phase information. To increase the supported data rate, we also present a maximum likelihood expression to detect differential amplitude phase shift keying symbols with low-resolution ADCs. We note that the derived detectors are unable to detect the amplitude information. To overcome this limitation, we use the Bussgang Theorem and the Central Limit Theorem (CLT) to develop two detectors capable of detecting the amplitude information. We numerically show that while the first amplitude detector requires multiple quantization bits for acceptable performance, similar performance can be achieved using one-bit ADCs by grouping the receive antennas and employing variable quantization levels (VQL) across distinct antenna groups. We validate the performance of the proposed detectors through simulations and show a comparison with corresponding coherent detectors. Finally, we present a complexity analysis of the proposed low-resolution differential detectors      
### 17.Robust deep learning-based semantic organ segmentation in hyperspectral images  [ :arrow_down: ](https://arxiv.org/pdf/2111.05408.pdf)
>  Semantic image segmentation is an important prerequisite for context-awareness and autonomous robotics in surgery. The state of the art has focused on conventional RGB video data acquired during minimally invasive surgery, but full-scene semantic segmentation based on spectral imaging data and obtained during open surgery has received almost no attention to date. To address this gap in the literature, we are investigating the following research questions based on hyperspectral imaging (HSI) data of pigs acquired in an open surgery setting: (1) What is an adequate representation of HSI data for neural network-based fully automated organ segmentation, especially with respect to the spatial granularity of the data (pixels vs. superpixels vs. patches vs. full images)? (2) Is there a benefit of using HSI data compared to other modalities, namely RGB data and processed HSI data (e.g. tissue parameters like oxygenation), when performing semantic organ segmentation? According to a comprehensive validation study based on 506 HSI images from 20 pigs, annotated with a total of 19 classes, deep learning-based segmentation performance increases - consistently across modalities - with the spatial context of the input data. Unprocessed HSI data offers an advantage over RGB data or processed data from the camera provider, with the advantage increasing with decreasing size of the input to the neural network. Maximum performance (HSI applied to whole images) yielded a mean dice similarity coefficient (DSC) of 0.89 (standard deviation (SD) 0.04), which is in the range of the inter-rater variability (DSC of 0.89 (SD 0.07)). We conclude that HSI could become a powerful image modality for fully-automatic surgical scene understanding with many advantages over traditional imaging, including the ability to recover additional functional tissue information.      
### 18.Fitting ellipses to noisy measurements  [ :arrow_down: ](https://arxiv.org/pdf/2111.05359.pdf)
>  This work deals with fitting of ellipses to noisy measurements. The literature knows many different approaches for this. The main representatives are presented and discussed in this paper. Furthermore, the case is considered when outliers are present in the measurement data. Robust methods which are less sensitive to outliers are suitable for this case. All discussed methods are compared by a simulation. The code for the simulation is available for free use on <a class="link-external link-http" href="http://github.com/sebdi/ellipse-fitting" rel="external noopener nofollow">this http URL</a>.      
### 19.Structure from Silence: Learning Scene Structure from Ambient Sound  [ :arrow_down: ](https://arxiv.org/pdf/2111.05846.pdf)
>  From whirling ceiling fans to ticking clocks, the sounds that we hear subtly vary as we move through a scene. We ask whether these ambient sounds convey information about 3D scene structure and, if so, whether they provide a useful learning signal for multimodal models. To study this, we collect a dataset of paired audio and RGB-D recordings from a variety of quiet indoor scenes. We then train models that estimate the distance to nearby walls, given only audio as input. We also use these recordings to learn multimodal representations through self-supervision, by training a network to associate images with their corresponding sounds. These results suggest that ambient sound conveys a surprising amount of information about scene structure, and that it is a useful signal for learning multimodal features.      
### 20.A Probabilistic Domain-knowledge Framework for Nosocomial Infection Risk Estimation of Communicable Viral Diseases in Healthcare Personnel: A Case Study for COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2111.05761.pdf)
>  Hospital-acquired infections of communicable viral diseases (CVDs) are posing a tremendous challenge to healthcare workers globally. Healthcare personnel (HCP) is facing a consistent risk of hospital-acquired infections, and subsequently higher rates of morbidity and mortality. We proposed a domain knowledge-driven infection risk model to quantify the individual HCP and the population-level healthcare facility risks. For individual-level risk estimation, a time-variant infection risk model is proposed to capture the transmission dynamics of CVDs. At the population-level, the infection risk is estimated using a Bayesian network model constructed from three feature sets including individual-level factors, engineering control factors, and administrative control factors. The sensitivity analyses indicated that the uncertainty in the individual infection risk can be attributed to two variables: the number of close contacts and the viral transmission probability. The model validation was implemented in the transmission probability model, individual level risk model, and population-level risk model using a Coronavirus disease case study. Regarding the first, multivariate logistic regression was applied for a cross-sectional data in the UK with an AIC value of 7317.70 and a 10-fold cross validation accuracy of 78.23%. For the second model, we collected laboratory-confirmed COVID-19 cases of HCP in different occupations. The occupation-specific risk evaluation suggested the highest-risk occupations were registered nurses, medical assistants, and respiratory therapists, with estimated risks of 0.0189, 0.0188, and 0.0176, respectively. To validate the population-level risk model, the infection risk in Texas and California was estimated. The proposed model will significantly influence the PPE allocation and safety plans for HCP      
### 21.Generating a robustly stabilizable class of nonlinear systems for the converse optimality problem  [ :arrow_down: ](https://arxiv.org/pdf/2111.05730.pdf)
>  Converse optimality theory addresses an optimal control problem conversely where the system is unknown and the value function is chosen. Previous work treated this problem both in continuous and discrete time and non-extensively considered disturbances. In this paper, the converse optimality theory is extended to the class of affine systems with disturbances in continuous time while considering norm constraints on both control inputs and disturbances. The admissibility theorem and the design of the internal dynamics model are generalized in this context. A robust stabilizability condition is added for the initial converse optimality probelm using inverse optimality tool: the robust control Lyapunov function. A design for nonlinear class of systems that are both robustly stabilizable and globally asymptotically stable in open loop is obtained. A case study illustrates the presented theory.      
### 22.Tracking multiple spawning targets using Poisson multi-Bernoulli mixtures on sets of tree trajectories  [ :arrow_down: ](https://arxiv.org/pdf/2111.05620.pdf)
>  This paper proposes a Poisson multi-Bernoulli mixture (PMBM) filter on the space of sets of tree trajectories for multiple target tracking with spawning targets. A tree trajectory contains all trajectory information of a target and its descendants, which appear due to the spawning process. Each tree contains a set of branches, where each branch has trajectory information of a target or one of the descendants and its genealogy. For the standard dynamic and measurement models with multi-Bernoulli spawning, the posterior is a PMBM density, with each Bernoulli having information on a potential tree trajectory. To enable a computationally efficient implementation, we derive an approximate PMBM filter in which each Bernoulli tree trajectory has multi-Bernoulli branches obtained by minimising the Kullback-Leibler divergence. The resulting filter improves tracking performance of state-of-the-art algorithms in a simulated scenario.      
### 23.Improving the Chamberlin Digital State Variable Filter  [ :arrow_down: ](https://arxiv.org/pdf/2111.05592.pdf)
>  The state variable filter configuration is a classic analogue design which has been employed in many electronic music applications. A digital implementation of this filter was put forward by Chamberlin, which has been deployed in both software and hardware forms. While this has proven to be a straightforward and successful digital filter design, it suffers from some issues, which have already been identified in the literature. From a modified Chamberlin block diagram, we derive the transfer functions describing its three basic responses, highpass, bandpass, and lowpass. An analysis of these leads to the development of an improvement, which attempts to better shape the filter spectrum. From these new transfer functions, a set of filter equations is developed. Finally, the approach is compared to an alternative time-domain based re-organisation of update equations, which is shown to deliver a similar result.      
### 24.Learning Graphs from Smooth and Graph-Stationary Signals with Hidden Variables  [ :arrow_down: ](https://arxiv.org/pdf/2111.05588.pdf)
>  Network-topology inference from (vertex) signal observations is a prominent problem across data-science and engineering disciplines. Most existing schemes assume that observations from all nodes are available, but in many practical environments, only a subset of nodes is accessible. A natural (and sometimes effective) approach is to disregard the role of unobserved nodes, but this ignores latent network effects, deteriorating the quality of the estimated graph. Differently, this paper investigates the problem of inferring the topology of a network from nodal observations while taking into account the presence of hidden (latent) variables. Our schemes assume the number of observed nodes is considerably larger than the number of hidden variables and build on recent graph signal processing models to relate the signals and the underlying graph. Specifically, we go beyond classical correlation and partial correlation approaches and assume that the signals are smooth and/or stationary in the sought graph. The assumptions are codified into different constrained optimization problems, with the presence of hidden variables being explicitly taken into account. Since the resulting problems are ill-conditioned and non-convex, the block matrix structure of the proposed formulations is leveraged and suitable convex-regularized relaxations are presented. Numerical experiments over synthetic and real-world datasets showcase the performance of the developed methods and compare them with existing alternatives.      
### 25.ResNEsts and DenseNEsts: Block-based DNN Models with Improved Representation Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2111.05496.pdf)
>  Models recently used in the literature proving residual networks (ResNets) are better than linear predictors are actually different from standard ResNets that have been widely used in computer vision. In addition to the assumptions such as scalar-valued output or single residual block, these models have no nonlinearities at the final residual representation that feeds into the final affine layer. To codify such a difference in nonlinearities and reveal a linear estimation property, we define ResNEsts, i.e., Residual Nonlinear Estimators, by simply dropping nonlinearities at the last residual representation from standard ResNets. We show that wide ResNEsts with bottleneck blocks can always guarantee a very desirable training property that standard ResNets aim to achieve, i.e., adding more blocks does not decrease performance given the same set of basis elements. To prove that, we first recognize ResNEsts are basis function models that are limited by a coupling problem in basis learning and linear prediction. Then, to decouple prediction weights from basis learning, we construct a special architecture termed augmented ResNEst (A-ResNEst) that always guarantees no worse performance with the addition of a block. As a result, such an A-ResNEst establishes empirical risk lower bounds for a ResNEst using corresponding bases. Our results demonstrate ResNEsts indeed have a problem of diminishing feature reuse; however, it can be avoided by sufficiently expanding or widening the input space, leading to the above-mentioned desirable property. Inspired by the DenseNets that have been shown to outperform ResNets, we also propose a corresponding new model called Densely connected Nonlinear Estimator (DenseNEst). We show that any DenseNEst can be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts, DenseNEsts exhibit the desirable property without any special architectural re-design.      
