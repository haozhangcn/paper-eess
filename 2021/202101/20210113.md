# ArXiv eess --Wed, 13 Jan 2021
### 1.LMI-Based Control Design for Output Regulation Problem of Linear Systems in the Presence of Limited Measurement Streams  [ :arrow_down: ](https://arxiv.org/pdf/2101.04662.pdf)
>  This paper deals with the output regulation problem of a linear time-invariant system in the presence of sporadically available measurement streams. A regulator with a continuous intersample injection term is proposed, where the intersample injection is provided by a linear dynamical system and the state of which is reset with the arrival of every new measurement updates. The resulting system is augmented with a timer triggering an instantaneous update of the new measurement and the overall system is then analyzed in a hybrid system framework. With the Lyapunov based stability analysis, we offer sufficient conditions to ensure the objectives of the output regulation problem are achieved under intermittency of the measurement streams. Then, from the solution to linear matrix inequalities, a numerically tractable regulator design procedure is presented. Finally, with the help of an illustrative example, the effectiveness of the theoretical results are validated.      
### 2.Anisotropic field-of-views in radial imaging  [ :arrow_down: ](https://arxiv.org/pdf/2101.04660.pdf)
>  Radial imaging techniques, such as projection-reconstruction (PR), are used in magnetic resonance imaging (MRI) for dynamic imaging, angiography, and short-imaging. They are robust to flow and motion, have diffuse aliasing patterns, and support short readouts and echo times. One drawback is that standard implementations do not support anisotropic field-of-view (FOV) shapes, which are used to match the imaging parameters to the object or region-of-interest. A set of fast, simple algorithms for 2-D and 3-D PR, and 3-D cones acquisitions are introduced that match the sampling density in frequency space to the desired FOV shape. Tailoring the acquisitions allows for reduction of aliasing artifacts in undersampled applications or scan time reductions without introducing aliasing in fully-sampled applications. It also makes possible new radial imaging applications that were previously unsuitable, such as imaging elongated regions or thin slabs. 2-D PR longitudinal leg images and thin-slab, single breath-hold 3-D PR abdomen images, both with isotropic resolution, demonstrate these new possibilities. No scan time to volume efficiency is lost by using anisotropic FOVs. The acquisition trajectories can be computed on a scan by scan basis.      
### 3.Deep Gaussian Denoiser Epistemic Uncertainty and Decoupled Dual-Attention Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2101.04631.pdf)
>  Following the performance breakthrough of denoising networks, improvements have come chiefly through novel architecture designs and increased depth. While novel denoising networks were designed for real images coming from different distributions, or for specific applications, comparatively small improvement was achieved on Gaussian denoising. The denoising solutions suffer from epistemic uncertainty that can limit further advancements. This uncertainty is traditionally mitigated through different ensemble approaches. However, such ensembles are prohibitively costly with deep networks, which are already large in size. <br>Our work focuses on pushing the performance limits of state-of-the-art methods on Gaussian denoising. We propose a model-agnostic approach for reducing epistemic uncertainty while using only a single pretrained network. We achieve this by tapping into the epistemic uncertainty through augmented and frequency-manipulated images to obtain denoised images with varying error. We propose an ensemble method with two decoupled attention paths, over the pixel domain and over that of our different manipulations, to learn the final fusion. Our results significantly improve over the state-of-the-art baselines and across varying noise levels.      
### 4.Quickest Detection and Forecast of Pandemic Outbreaks: Analysis of COVID-19 Waves  [ :arrow_down: ](https://arxiv.org/pdf/2101.04620.pdf)
>  The COVID-19 pandemic has, worldwide and up to December 2020, caused over 1.7 million deaths, and put the world's most advanced healthcare systems under heavy stress. In many countries, drastic restriction measures adopted by political authorities, such as national lockdowns, have not prevented the outbreak of new pandemic's waves. In this article, we propose an integrated detection-estimation-forecasting framework that, using publicly available data published by the national authorities, is designed to: (i) learn relevant features of the epidemic (e.g., the infection rate); (ii) detect as quickly as possible the onset (or the termination) of an exponential growth of the contagion; and (iii) reliably forecast the epidemic evolution. The proposed solution is validated by analyzing the COVID-19 second and third waves in the USA.      
### 5.Implementation of OpenAirInterface-based real-world channel measurement for evaluating wireless transmission algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2101.04608.pdf)
>  The fourth-generation Wireless Technology (4G) has been adopted by all major operators in the world and has already ruled the cellular landscape for around a decade. A lot of researches and new technologies are being considered as potential elements contributing to the next generation wireless communication (5G). The lack of realistic and flexible experimentation platforms for collecting real communication data has limited and slowed the landing of new approaches. Software Defined Radio (SDR) can provide flexible, upgradable, and long lifetime radio equipment for the wireless communications infrastructure, which can also provide more flexible and possibly cheaper multi-standard-terminals for end users. By altering the open-source code individually, we can freely perform the real value measurement. This paper provides a real Long Term Evolution (LTE) channel measurement method based on the OpenAirInterface (OAI) for the evaluation of the channel prediction algorithm. Firstly, the experimentation platform will be established by using OAI, Universal Software Radio Peripheral (USRP), and commercial User Equipment (UE). Then, some source codes of OAI are analyzed and changed, so that the real-time over-the-air channel measurement can be achieved. The results from the measurement are then trained and tested on the channel prediction algorithm. The results of the test illustrate that the implemented channel measurement method can meet the need for algorithms' verification and can be further extended for more development of algorithms.      
### 6.Non-Orthogonal Multiple Access and Network Slicing: Scalable Coexistence of eMBB and URLLC  [ :arrow_down: ](https://arxiv.org/pdf/2101.04605.pdf)
>  The 5G systems will feature three generic services: enhanced Mobile BroadBand (eMBB), massive Machine-Type Communications (mMTC) and Ultra-Reliable and Low-Latency Communications (URLLC). The diverse requirements of these services in terms of data-rates, number of connected devices, latency and reliability can lead to a sub-optimal use of the 5G network, thus network slicing is proposed as a solution that creates customized slices of the network specifically designed to meet the requirements of each service. Under the network slicing, the radio resources can be shared in orthogonal and non-orthogonal schemes. Motivated by Industrial Internet of Things (IIoT) scenarios where a large number of sensors may require connectivity with stringent requirements of latency and reliability, we propose the use of Non-Orthogonal Multiple Access (NOMA) to improve the number of URLLC users that are connected in the uplink to the same base station (BS), for both orthogonal and non-orthogonal network slicing with eMBB users. The multiple URLLC users transmit simultaneously and across multiple frequency channels. We set the reliability requirements for the two services and analyze their pair of sum rates. We show that, even with overlapping transmissions from multiple eMBB and URLLC users, the use of NOMA techniques allows us to guarantee the reliability requirements for both services.      
### 7.Quantify Change of Inertia and Its Distribution in High Renewable Power Grids Using PMU  [ :arrow_down: ](https://arxiv.org/pdf/2101.04593.pdf)
>  This paper proposed an approach to identify the change of inertia distribution in high renewable power systems. Using the footprints of electromechanical wave propagation at the distribution level, this approach provides a new and non-invasive way to aware the system inertia distribution for primary frequency response. Actual measurements and high renewable dynamic models validated effectiveness of the approach.      
### 8.Fast Randomized-MUSIC for mm-Wave Massive MIMO Radars  [ :arrow_down: ](https://arxiv.org/pdf/2101.04570.pdf)
>  Subspace methods are essential to high-resolution environment sensing in the emerging unmanned systems, if further combined with the millimeter-wave (mm-Wave) massive multi-input multi-output (MIMO) technique. The estimation of signal/noise subspace, as one critical step, is yet computationally complex and presents a particular challenge when developing high-resolution yet low-complexity automotive radars. In this work, we develop a fast randomized-MUSIC (R-MUSIC) algorithm, which exploits the random matrix sketching to estimate the signal subspace via approximated computation. Our new approach substantially reduces the time complexity in acquiring a high-quality signal subspace. Moreover, the accuracy of R-MUSIC suffers no degradation unlike others low-complexity counterparts, i.e. the high-resolution angle of arrival (AoA) estimation is attained. Numerical simulations are provided to validate the performance of our R-MUSIC method. As shown, it resolves the long-standing contradiction in complexity and accuracy of MIMO radar signal processing, which hence have great potentials in real-time super-resolution automotive sensing.      
### 9.Polarized hyperspectral imaging with single fiber bundle via incoherent light transmission matrix approach  [ :arrow_down: ](https://arxiv.org/pdf/2101.04538.pdf)
>  The scattering of multispectral incoherent light is a common and unfavorable signal scrambling in natural scenes. However, the blurred light spot due to scattering still holds lots of information remaining to be explored. Former methods failed to recover the polarized hyperspectral information from scattered incoherent light or relied on additional dispersion elements. Here we put forward the transmission matrix (TM) approach for extended objects under incoherent illumination by speculating the unknown TM through experimentally calibrated or digitally emulated ways. Employing a fiber bundle as a powerful imaging and dispersion element, we recover the spatial information in 252 polarized-spectral channels from a single speckle, thus achieving single-shot, high-resolution, broadband hyperspectral imaging for two polarization states with the cheap, compact, fiber-bundle-only system. Based on the scattering principle itself, our method not only greatly improves the robustness of the TM approach to retrieve the input spectral information, but also reveals the feasibility to explore the polarized spatio-spectral information from blurry speckles only with the help of simple optical setups.      
### 10.Transient Theoretical Analysis of Diffusion RLS Algorithm for Cyclostationary Colored Inputs  [ :arrow_down: ](https://arxiv.org/pdf/2101.04502.pdf)
>  Convergence of the diffusion RLS (DRLS) algorithm to steady-state has been extensively studied in the literature, whereas no analysis of its transient convergence behavior has been reported yet. In this letter, we conduct a theoretical analysis of the transient behavior of the DRLS algorithm for cyclostationary colored inputs, in the mean and mean-square error sense. The resulting analytical models allows us to thoroughly investigate the convergence behavior of the algorithm over adaptive networks in such complex scenarios. Simulation results support the accuracy and correctness of the theoretical findings.      
### 11.Latency Minimization in Intelligent Reflecting Surface Assisted D2D Offloading Systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.04444.pdf)
>  In this letter, we investigate an intelligent reflecting surface (IRS) aided device-to-device (D2D) offloading system, where an IRS is employed to assist in computation offloading from a group of users with intensive tasks to another group of idle users. We propose a new two-timescale joint passive beamforming and resource allocation algorithm based on stochastic successive convex approximation to minimize the system latency while cutting down the heavy overhead in exchange of channel state information (CSI). Specifically, the high-dimensional passive beamforming vector at the IRS is updated in a frame-based manner based on the channel statistics, where each frame consists of a number of time slots, while the offloading ratio and user matching strategy are optimized relied on the low-dimensional real-time effective channel coefficients in each time slot. The convergence property and the computational complexity of the proposed algorithm are also examined. Simulation results show that our proposed algorithm significantly outperforms the conventional benchmarks.      
### 12.Automated feature selection for data-driven models of rapid battery capacity fade and end of life  [ :arrow_down: ](https://arxiv.org/pdf/2101.04440.pdf)
>  Lithium-ion cells may experience rapid degradation in later life, especially with more extreme usage protocols. The onset of rapid degradation is called the 'knee point', and forecasting it is important for the safe and economically viable use for batteries. We propose a data-driven method that uses automated feature selection to produce inputs for a Gaussian process regression model that estimates changes in battery health, from which the entire capacity fade trajectory, knee point and end of life may be predicted. The feature selection procedure flexibly adapts to varying inputs and prioritises those that impact degradation. For the datasets considered, it was found that calendar time and time spent in specific voltage regions had a strong impact on degradation rate. The approach produced median root mean square errors on capacity estimates under 1%, and also produced median knee point and end of life prediction errors of 2.6% and 1.3% respectively.      
### 13.Ancillary services in Great Britain during the COVID-19 lockdown: a glimpse of the carbon-free future  [ :arrow_down: ](https://arxiv.org/pdf/2101.04387.pdf)
>  The COVID-19 pandemic led to partial or total lockdowns in several countries during the first half of 2020, which in turn caused a depressed electricity demand. In Great Britain (GB), this low demand combined with large renewable output at times, created conditions that were not expected until renewable capacity increases to meet emissions targets in coming years. The GB system experienced periods of very high instantaneous penetration of non-synchronous renewables, compromising system stability due to the lack of inertia in the grid. In this paper, a detailed analysis of the consequences of the lockdown on the GB electricity system is provided, focusing on the ancillary services procured to guarantee stability. Ancillary-services costs increased by Â£200m in the months of May to July 2020 compared to the same period in 2019 (a threefold increase), highlighting the importance of ancillary services in low-carbon systems. Furthermore, a frequency-secured scheduling model is used in the present paper to showcase the future trends that GB is expected to experience, as penetration of renewables increases on the road to net-zero emissions by 2050. Several sensitivities are considered, demonstrating that the share of total operating costs represented by ancillary services could reach 35%.      
### 14.Using uncertainty estimation to reduce false positives in liver lesion detection  [ :arrow_down: ](https://arxiv.org/pdf/2101.04386.pdf)
>  Despite the successes of deep learning techniques at detecting objects in medical images, false positive detections occur which may hinder an accurate diagnosis. We propose a technique to reduce false positive detections made by a neural network using an SVM classifier trained with features derived from the uncertainty map of the neural network prediction. We demonstrate the effectiveness of this method for the detection of liver lesions on a dataset of abdominal MR images. We find that the use of a dropout rate of 0.5 produces the least number of false positives in the neural network predictions and the trained classifier filters out approximately 90% of these false positives detections in the test-set.      
### 15.Exponential convergence of distributed optimization for heterogeneous linear multi-agent systems  [ :arrow_down: ](https://arxiv.org/pdf/2101.04353.pdf)
>  In this work we study a distributed optimal output consensus problem for heterogeneous linear multi-agent systems where the agents aim to reach consensus with the purpose of minimizing the sum of private convex costs. Based on output feedback, a fully distributed control law is proposed by using the proportional-integral (PI) control technique. For strongly convex cost functions with Lipschitz gradients, the designed controller can achieve convergence exponentially in an undirected and connected network. Furthermore, to remove the requirement of continuous communications, the proposed control law is then extended to periodic and event-triggered communication schemes, which also achieve convergence exponentially. Two simulation examples are given to verify the proposed control algorithms.      
### 16.From Control to Mathematics-Part I: Controllability-Based Design for Iterative Methods in Solving Linear Equations  [ :arrow_down: ](https://arxiv.org/pdf/2101.04345.pdf)
>  In the interaction between control and mathematics, mathematical tools are fundamental for all the control methods, but it is unclear how control impacts mathematics. This is the first part of our paper that attempts to give an answer with focus on solving linear algebraic equations (LAEs) from the perspective of systems and control, where it mainly introduces the controllability-based design results. By proposing an iterative method that integrates a learning control mechanism, a class of tracking problems for iterative learning control (ILC) is explored for the problem solving of LAEs. A trackability property of ILC is newly developed, by which analysis and synthesis results are established to disclose the equivalence between the solvability of LAEs and the controllability of discrete control systems. Hence, LAEs can be solved by equivalently achieving the perfect tracking tasks of resulting ILC systems via the classic state feedback-based design and analysis methods. It is shown that the solutions for any solvable LAE can all be calculated with different selections of the initial input. Moreover, the presented ILC method is applicable to determining all the least squares solutions of any unsolvable LAE. In particular, a deadbeat design is incorporated to ILC such that the solving of LAEs can be completed within finite iteration steps. The trackability property is also generalized to conventional two-dimensional ILC systems, which creates feedback-based methods, instead of the common used contraction mapping-based methods, for the design and convergence analysis of ILC.      
### 17.Non-Bayesian Parametric Missing-Mass Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2101.04329.pdf)
>  We consider the classical problem of missing-mass estimation, which deals with estimating the total probability of unseen elements in a sample. The missing-mass estimation problem has various applications in machine learning, statistics, language processing, ecology, sensor networks, and others. The naive, constrained maximum likelihood (CML) estimator is inappropriate for this problem since it tends to overestimate the probability of the observed elements. Similarly, the conventional constrained Cramer-Rao bound (CCRB), which is a lower bound on the mean-squared-error (MSE) of unbiased estimators, does not provide a relevant bound on the performance for this problem. In this paper, we introduce a frequentist, non-Bayesian parametric model of the problem of missing-mass estimation. We introduce the concept of missing-mass unbiasedness by using the Lehmann unbiasedness definition. We derive a non-Bayesian CCRB-type lower bound on the missing-mass MSE (mmMSE), named the missing-mass CCRB (mmCCRB), based on the missing-mass unbiasedness. The missing-mass unbiasedness and the proposed mmCCRB can be used to evaluate the performance of existing estimators. Based on the new mmCCRB, we propose a new method to improve existing estimators by an iterative missing-mass Fisher scoring method. Finally, we demonstrate via numerical simulations that the proposed mmCCRB is a valid and informative lower bound on the mmMSE of state-of-the-art estimators for this problem: the CML, the Good-Turing, and Laplace estimators. We also show that the performance of the Laplace estimator is improved by using the new Fisher-scoring method.      
### 18.Neural Network-based Virtual Microphone Estimator  [ :arrow_down: ](https://arxiv.org/pdf/2101.04315.pdf)
>  Developing microphone array technologies for a small number of microphones is important due to the constraints of many devices. One direction to address this situation consists of virtually augmenting the number of microphone signals, e.g., based on several physical model assumptions. However, such assumptions are not necessarily met in realistic conditions. In this paper, as an alternative approach, we propose a neural network-based virtual microphone estimator (NN-VME). The NN-VME estimates virtual microphone signals directly in the time domain, by utilizing the precise estimation capability of the recent time-domain neural networks. We adopt a fully supervised learning framework that uses actual observations at the locations of the virtual microphones at training time. Consequently, the NN-VME can be trained using only multi-channel observations and thus directly on real recordings, avoiding the need for unrealistic physical model-based assumptions. Experiments on the CHiME-4 corpus show that the proposed NN-VME achieves high virtual microphone estimation performance even for real recordings and that a beamformer augmented with the NN-VME improves both the speech enhancement and recognition performance.      
### 19.Two beams are better than one: Enabling reliable and high throughput mmWave links  [ :arrow_down: ](https://arxiv.org/pdf/2101.04249.pdf)
>  Millimeter-wave communication with high throughput and high reliability is poised to be a gamechanger for V2X and VR applications. However, mmWave links are notorious for low reliability since they suffer from frequent outages due to blockage and user mobility. Traditional mmWave systems are hardly reliable for two reasons. First, they create a highly directional link that acts as a single point of failure and cannot be sustained for high user mobility. Second, they follow a `reactive' approach, which reacts after the link has already suffered an outage. We build mmReliable, a reliable mmWave system that implements smart analog beamforming and user tracking to handle environmental vulnerabilities. It creates custom beam patterns with multiple lobes and optimizes their angle, phase, and amplitude to maximize the signal strength at the receiver. Such phase-coherent multi-beam patterns allow the signal to travel along multiple paths and add up constructively at the receiver to improve throughput. Of course, multi-beam links are resilient to occasional blockages of few beams in multi-beam compared to a single-beam system. With user mobility, mmReliable proactively tracks the motion in the background by leveraging continuous channel estimates without affecting the data rates. We implement mmReliable on a 28 GHz testbed with 400 MHz bandwidth and a 64 element phased-array supporting 5G NR waveforms. Rigorous indoor and outdoor experiments demonstrate that mmReliable achieves close to 100% reliability providing 1.5 times better throughput than traditional single-beam systems.      
### 20.Smartajweed Automatic Recognition of Arabic Quranic Recitation Rules  [ :arrow_down: ](https://arxiv.org/pdf/2101.04200.pdf)
>  Tajweed is a set of rules to read the Quran in a correct Pronunciation of the letters with all its Qualities, while Reciting the Quran. which means you have to give every letter in the Quran its due of characteristics and apply it to this particular letter in this specific situation while reading, which may differ in other times. These characteristics include melodic rules, like where to stop and for how long, when to merge two letters in pronunciation or when to stretch some, or even when to put more strength on some letters over other. Most of the papers focus mainly on the main recitation rules and the pronunciation but not (Ahkam AL Tajweed) which give different rhythm and different melody to the pronunciation with every different rule of (Tajweed). Which is also considered very important and essential in Reading the Quran as it can give different meanings to the words. In this paper we discuss in detail full system for automatic recognition of Quran Recitation Rules (Tajweed) by using support vector machine and threshold scoring system      
### 21.Resolution-Based Distillation for Efficient Histology Image Classification  [ :arrow_down: ](https://arxiv.org/pdf/2101.04170.pdf)
>  Developing deep learning models to analyze histology images has been computationally challenging, as the massive size of the images causes excessive strain on all parts of the computing pipeline. This paper proposes a novel deep learning-based methodology for improving the computational efficiency of histology image classification. The proposed approach is robust when used with images that have reduced input resolution and can be trained effectively with limited labeled data. Pre-trained on the original high-resolution (HR) images, our method uses knowledge distillation (KD) to transfer learned knowledge from a teacher model to a student model trained on the same images at a much lower resolution. To address the lack of large-scale labeled histology image datasets, we perform KD in a self-supervised manner. We evaluate our approach on two histology image datasets associated with celiac disease (CD) and lung adenocarcinoma (LUAD). Our results show that a combination of KD and self-supervision allows the student model to approach, and in some cases, surpass the classification accuracy of the teacher, while being much more efficient. Additionally, we observe an increase in student classification performance as the size of the unlabeled dataset increases, indicating that there is potential to scale further. For the CD data, our model outperforms the HR teacher model, while needing 4 times fewer computations. For the LUAD data, our student model results at 1.25x magnification are within 3% of the teacher model at 10x magnification, with a 64 times computational cost reduction. Moreover, our CD outcomes benefit from performance scaling with the use of more unlabeled data. For 0.625x magnification, using unlabeled data improves accuracy by 4% over the baseline. Thus, our method can improve the feasibility of deep learning solutions for digital pathology with standard computational hardware.      
### 22.PolyAR: A Highly Parallelizable Solver For Polynomial Inequality Constraints Using Convex Abstraction Refinement  [ :arrow_down: ](https://arxiv.org/pdf/2101.04655.pdf)
>  Numerical tools for constraints solving are a cornerstone to control verification problems. This is evident by the plethora of research that uses tools like linear and convex programming for the design of control systems. Nevertheless, the capability of linear and convex programming is limited and is not adequate to reason about general nonlinear polynomials constraints that arise naturally in the design of nonlinear systems. This limitation calls for new solvers that are capable of utilizing the power of linear and convex programming to reason about general multivariate polynomials. In this paper, we propose PolyAR, a highly parallelizable solver for polynomial inequality constraints. PolyAR provides several key contributions. First, it uses convex relaxations of the problem to accelerate the process of finding a solution to the set of the non-convex multivariate polynomials. Second, it utilizes an iterative convex abstraction refinement process which aims to prune the search space and identify regions for which the convex relaxation fails to solve the problem. Third, it allows for a highly parallelizable usage of off-the-shelf solvers to analyze the regions in which the convex relaxation failed to provide solutions. We compared the scalability of PolyAR against Z3 8.9 and Yices 2.6 on control designing problems. Finally, we demonstrate the performance of PolyAR on designing switching signals for continuous-time linear switching systems.      
### 23.An Early-Stopping Mechanism for DSCF Decoding of Polar Codes  [ :arrow_down: ](https://arxiv.org/pdf/2101.04586.pdf)
>  Polar codes can be decoded with the low-complexity successive-cancellation flip (SCF) algorithm. To improve error-correction performance, the dynamic successive-cancellation flip (DSCF) variant was proposed, where the resulting error-correction performance is similar to that of the successive-cancellation list algorithm with low to moderate list sizes. Regardless of the variant, the SCF algorithm exhibits a variable execution time with a high (worst-case) latency. In this work, we propose an early-stopping metric used to detect codewords that are likely undecodable such that the decoder can be stopped at earlier stages for those codewords. We then propose a modified version of the DSCF algorithm that integrates our early-stopping metric that exploits the specific properties of DSCF. Compared to the original DSCF algorithm, in the region of interest for wireless communications, simulation results show that our proposed modifications can lead to reductions of 22% to the average execution time and of 45% to the execution-time variance at the cost of a minor error-correction loss of approximately 0.05 dB.      
### 24.Spatial Photonic Reservoir Computing based on Non-Linear Phase-to-Amplitude Conversion in Micro-Ring Resonators  [ :arrow_down: ](https://arxiv.org/pdf/2101.04487.pdf)
>  We present a photonic reservoir computing, relying on a non-linear phase-to-amplitude mapping process, able to classify in real-time multi-Gbaud time traces subject to transmission effects. This approach delivers an all-optical, low-power neuromorphic dispersion compensator.      
### 25.Sound Event Detection with Binary Neural Networks on Tightly Power-Constrained IoT Devices  [ :arrow_down: ](https://arxiv.org/pdf/2101.04446.pdf)
>  Sound event detection (SED) is a hot topic in consumer and smart city applications. Existing approaches based on Deep Neural Networks are very effective, but highly demanding in terms of memory, power, and throughput when targeting ultra-low power always-on devices. <br>Latency, availability, cost, and privacy requirements are pushing recent IoT systems to process the data on the node, close to the sensor, with a very limited energy supply, and tight constraints on the memory size and processing capabilities precluding to run state-of-the-art DNNs. <br>In this paper, we explore the combination of extreme quantization to a small-footprint binary neural network (BNN) with the highly energy-efficient, RISC-V-based (8+1)-core GAP8 microcontroller. Starting from an existing CNN for SED whose footprint (815 kB) exceeds the 512 kB of memory available on our platform, we retrain the network using binary filters and activations to match these memory constraints. (Fully) binary neural networks come with a natural drop in accuracy of 12-18% on the challenging ImageNet object recognition challenge compared to their equivalent full-precision baselines. This BNN reaches a 77.9% accuracy, just 7% lower than the full-precision version, with 58 kB (7.2 times less) for the weights and 262 kB (2.4 times less) memory in total. With our BNN implementation, we reach a peak throughput of 4.6 GMAC/s and 1.5 GMAC/s over the full network, including preprocessing with Mel bins, which corresponds to an efficiency of 67.1 GMAC/s/W and 31.3 GMAC/s/W, respectively. Compared to the performance of an ARM Cortex-M4 implementation, our system has a 10.3 times faster execution time and a 51.1 times higher energy-efficiency.      
### 26.Joint Demosaicking and Denoising in the Wild: The Case of Training Under Ground Truth Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2101.04442.pdf)
>  Image demosaicking and denoising are the two key fundamental steps in digital camera pipelines, aiming to reconstruct clean color images from noisy luminance readings. In this paper, we propose and study Wild-JDD, a novel learning framework for joint demosaicking and denoising in the wild. In contrast to previous works which generally assume the ground truth of training data is a perfect reflection of the reality, we consider here the more common imperfect case of ground truth uncertainty in the wild. We first illustrate its manifestation as various kinds of artifacts including zipper effect, color moire and residual noise. Then we formulate a two-stage data degradation process to capture such ground truth uncertainty, where a conjugate prior distribution is imposed upon a base distribution. After that, we derive an evidence lower bound (ELBO) loss to train a neural network that approximates the parameters of the conjugate prior distribution conditioned on the degraded input. Finally, to further enhance the performance for out-of-distribution input, we design a simple but effective fine-tuning strategy by taking the input as a weakly informative prior. Taking into account ground truth uncertainty, Wild-JDD enjoys good interpretability during optimization. Extensive experiments validate that it outperforms state-of-the-art schemes on joint demosaicking and denoising tasks on both synthetic and realistic raw datasets.      
### 27.Ergodic Exploration using Tensor Train: Applications in Insertion Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2101.04428.pdf)
>  By generating control policies that create natural search behaviors in autonomous systems, ergodic control provides a principled solution to address tasks that require exploration. A large class of ergodic control algorithms relies on spectral analysis, which suffers from the curse of dimensionality, both in storage and computation. This drawback has prohibited the application of ergodic control in robot manipulation since it often requires exploration in state space with more than 2 dimensions. Indeed, the original ergodic control formulation will typically not allow exploratory behaviors to be generated for a complete 6D end-effector pose. In this paper, we propose a solution for ergodic exploration based on the spectral analysis in multidimensional spaces using low-rank tensor approximation techniques. We rely on tensor train decomposition, a recent approach from multilinear algebra for low-rank approximation and efficient computation of multidimensional arrays. The proposed solution is efficient both computationally and storage-wise, hence making it suitable for its online implementation in robotic systems. The approach is applied to a peg-in-hole insertion task using a 7-axis Franka Emika Panda robot, where ergodic exploration allows the task to be achieved without requiring the use of force/torque sensors.      
### 28.Predictive-sensitivity: Beyond Singular Perturbation for Control Design on Multiple Time Scales  [ :arrow_down: ](https://arxiv.org/pdf/2101.04367.pdf)
>  A classical approach to design controllers for interconnected systems is to assume that the different subsystems operate at different time scales, then design simpler controllers within each time scale, and finally certify stability of the interconnected system via singular perturbation analysis. In this work, we propose an alternative approach that also allows to separately design the controllers of the individual subsystems. Instead of requiring a sufficiently large time-scale separation, our approach consists of adding a feed-forward term to modify the dynamics of faster systems in order to anticipate the dynamics of slower ones. We present several examples in bilevel optimization and cascade control design, where our approach improves the performance of currently available methods.      
### 29.Event-Driven Source Traffic Prediction in Machine-Type Communications Using LSTM Networks  [ :arrow_down: ](https://arxiv.org/pdf/2101.04365.pdf)
>  Source traffic prediction is one of the main challenges of enabling predictive resource allocation in machine type communications (MTC). In this paper, a Long Short-Term Memory (LSTM) based deep learning approach is proposed for event-driven source traffic prediction. The source traffic prediction problem can be formulated as a sequence generation task where the main focus is predicting the transmission states of machine-type devices (MTDs) based on their past transmission data. This is done by restructuring the transmission data in a way that the LSTM network can identify the causal relationship between the devices. Knowledge of such a causal relationship can enable event-driven traffic prediction. The performance of the proposed approach is studied using data regarding events from MTDs with different ranges of entropy. Our model outperforms existing baseline solutions in saving resources and accuracy with a margin of around 9%. Reduction in Random Access (RA) requests by our model is also analyzed to demonstrate the low amount of signaling required as a result of our proposed LSTM based source traffic prediction approach.      
### 30.On the Differential Private Data Market: Endogenous Evolution, Dynamic Pricing, and Incentive Compatibility  [ :arrow_down: ](https://arxiv.org/pdf/2101.04357.pdf)
>  Privacy is essential in data trading markets. This work uses a mechanism design approach to study the data buyer's optimal data market model with differential privacy. Motivated by the discovery of individuals' dual motives for privacy protection, we consider that each data owner privately possesses an intrinsic motive and an instrumental motive. We study optimal market design in a dynamic environment by determining the privacy assignment rule that specifies the privacy protection at each data usage and the payment rules to compensate for the privacy loss when the owners' instrumental motive is endogenously dynamic due to the buyer's dynamic activities. Due to the privacy-utility tradeoff of differential privacy, privacy loss is inevitable when data is traded with privacy protection. To mitigate the risk of uncertainties, we allow the owners to leave the market using optimal stopping time if the accumulated privacy loss is beyond their privacy budgets that depend on their intrinsic motives. In order to influence the data owners' stopping decisions, the data buyer uses a stopping payment rule that is independent of the data owners' preferences and specifies a monetary transfer to a data owner only at the period when he decides to stop at the end of that period. We introduce the notion of dynamic incentive compatibility to capture the joint deviations from optimal stopping and truthful reporting. Under a monotonicity assumption about the dynamics, the optimal stopping rule can be formulated as a threshold-based rule. A design principle is provided by a sufficient condition of dynamic incentive compatibility. We relax the buyer's optimal market design by characterizing the monetary transfer rules in terms of privacy assignment rules and the threshold functions. To address the analytical intractability, we provide a sufficient condition for a relaxed dynamic incentive-compatible model.      
### 31.Automated Detection of Patellofemoral Osteoarthritis from Knee Lateral View Radiographs Using Deep Learning: Data from the Multicenter Osteoarthritis Study (MOST)  [ :arrow_down: ](https://arxiv.org/pdf/2101.04350.pdf)
>  Objective: To assess the ability of imaging-based deep learning to predict radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view radiographs. <br>Design: Knee lateral view radiographs were extracted from The Multicenter Osteoarthritis Study (MOST) (n = 18,436 knees). Patellar region-of-interest (ROI) was first automatically detected, and subsequently, end-to-end deep convolutional neural networks (CNNs) were trained and validated to detect the status of patellofemoral OA. Patellar ROI was detected using deep-learning-based object detection method. Manual PFOA status assessment provided in the MOST dataset was used as a classification outcome for the CNNs. Performance of prediction models was assessed using the area under the receiver operating characteristic curve (ROC AUC) and the average precision (AP) obtained from the precision-recall (PR) curve in the stratified 5-fold cross validation setting. <br>Results: Of the 18,436 knees, 3,425 (19%) had PFOA. AUC and AP for the reference model including age, sex, body mass index (BMI), the total Western Ontario and McMaster Universities Arthritis Index (WOMAC) score, and tibiofemoral Kellgren-Lawrence (KL) grade to predict PFOA were 0.806 and 0.478, respectively. The CNN model that used only image data significantly improved the prediction of PFOA status (ROC AUC= 0.958, AP= 0.862). <br>Conclusion: We present the first machine learning based automatic PFOA detection method. Furthermore, our deep learning based model trained on patella region from knee lateral view radiographs performs better at predicting PFOA than models based on patient characteristics and clinical assessments.      
### 32.Blind Modulation Classification via Combined Machine Learning and Signal Feature Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2101.04337.pdf)
>  In this study, an algorithm to blind and automatic modulation classification has been proposed. It well benefits combined machine leaning and signal feature extraction to recognize diverse range of modulation in low signal power to noise ratio (SNR). The presented algorithm contains four. First, it advantages spectrum analyzing to branching modulated signal based on regular and irregular spectrum character. Seconds, a nonlinear soft margin support vector (NS SVM) problem is applied to received signal, and its symbols are classified to correct and incorrect (support vectors) symbols. The NS SVM employment leads to discounting in physical layer noise effect on modulated signal. After that, a k-center clustering can find center of each class. finally, in correlation function estimation of scatter diagram is correlated with pre-saved ideal scatter diagram of modulations. The correlation outcome is classification result. For more evaluation, success rate, performance, and complexity in compare to many published methods are provided. The simulation prove that the proposed algorithm can classified the modulated signal in less SNR. For example, it can recognize 4-QAM in SNR=-4.2 dB, and 4-FSK in SNR=2.1 dB with %99 success rate. Moreover, due to using of kernel function in dual problem of NS SVM and feature base function, the proposed algorithm has low complexity and simple implementation in practical issues.      
### 33.Regret Analysis of Distributed Gaussian Process Estimation and Coverage  [ :arrow_down: ](https://arxiv.org/pdf/2101.04306.pdf)
>  We study the problem of distributed multi-robot coverage over an unknown, nonuniform sensory field. Modeling the sensory field as a realization of a Gaussian Process and using Bayesian techniques, we devise a policy which aims to balance the tradeoff between learning the sensory function and covering the environment. We propose an adaptive coverage algorithm called Deterministic Sequencing of Learning and Coverage (DSLC) that schedules learning and coverage epochs such that its emphasis gradually shifts from exploration to exploitation while never fully ceasing to learn. Using a novel definition of coverage regret which characterizes overall coverage performance of a multi-robot team over a time horizon $T$, we analyze DSLC to provide an upper bound on expected cumulative coverage regret. Finally, we illustrate the empirical performance of the algorithm through simulations of the coverage task over an unknown distribution of wildfires.      
### 34.A Robotic System for Implant Modification in Single-stage Cranioplasty  [ :arrow_down: ](https://arxiv.org/pdf/2101.04303.pdf)
>  Craniomaxillofacial reconstruction with patient-specific customized craniofacial implants (CCIs) is most commonly performed for large-sized skeletal defects. Because the exact size of skull resection may not be known prior to the surgery, in the single-stage cranioplasty, a large CCI is prefabricated and resized intraoperatively with a manual-cutting process provided by a surgeon. The manual resizing, however, may be inaccurate and significantly add to the operating time. This paper introduces a fast and non-contact approach for intraoperatively determining the exact contour of the skull resection and automatically resizing the implant to fit the resection area. Our approach includes four steps: First, a patient's defect information is acquired by a 3D scanner. Second, the scanned defect is aligned to the CCI by registering the scanned defect to the reconstructed CT model. Third, a cutting toolpath is generated from the contour of the scanned defect. Lastly, the large CCI is resized by a cutting robot to fit the resection area according to the given toolpath. To evaluate the resizing performance of our method, six different resection shapes were used in the cutting experiments. We compared the performance of our method to the performances of surgeon's manual resizing and an existing technique which collects the defect contour with an optical tracking system and projects the contour on the CCI to guide the manual modification. The results show that our proposed method improves the resizing accuracy by 56% compared to the surgeon's manual modification and 42% compared to the projection method.      
### 35.Pneumonia Detection on Chest X-ray using Radiomic Features and Contrastive Learning  [ :arrow_down: ](https://arxiv.org/pdf/2101.04269.pdf)
>  Chest X-ray becomes one of the most common medical diagnoses due to its noninvasiveness. The number of chest X-ray images has skyrocketed, but reading chest X-rays still have been manually performed by radiologists, which creates huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology that can extract a large number of quantitative features from medical images, demonstrates its potential to facilitate medical imaging diagnosis before the deep learning era. With the rise of deep learning, the explainability of deep neural networks on chest X-ray diagnosis remains opaque. In this study, we proposed a novel framework that leverages radiomics features and contrastive learning to detect pneumonia in chest X-ray. Experiments on the RSNA Pneumonia Detection Challenge dataset show that our model achieves superior results to several state-of-the-art models (&gt; 10% in F1-score) and increases the model's interpretability.      
### 36.Explaining the Black-box Smoothly- A Counterfactual Approach  [ :arrow_down: ](https://arxiv.org/pdf/2101.04230.pdf)
>  We propose a BlackBox \emph{Counterfactual Explainer} that is explicitly developed for medical imaging applications. Classical approaches (e.g. saliency maps) assessing feature importance do not explain \emph{how} and \emph{why} variations in a particular anatomical region is relevant to the outcome, which is crucial for transparent decision making in healthcare application. Our framework explains the outcome by gradually \emph{exaggerating} the semantic effect of the given outcome label. Given a query input to a classifier, Generative Adversarial Networks produce a progressive set of perturbations to the query image that gradually changes the posterior probability from its original class to its negation. We design the loss function to ensure that essential and potentially relevant details, such as support devices, are preserved in the counterfactually generated images. We provide an extensive evaluation of different classification tasks on the chest X-Ray images. Our experiments show that a counterfactually generated visual explanation is consistent with the disease's clinical relevant measurements, both quantitatively and qualitatively.      
### 37.HePPCAT: Probabilistic PCA for Data with Heteroscedastic Noise  [ :arrow_down: ](https://arxiv.org/pdf/2101.03468.pdf)
>  Principal component analysis (PCA) is a classical and ubiquitous method for reducing data dimensionality, but it is suboptimal for heterogeneous data that are increasingly common in modern applications. PCA treats all samples uniformly so degrades when the noise is heteroscedastic across samples, as occurs, e.g., when samples come from sources of heterogeneous quality. This paper develops a probabilistic PCA variant that estimates and accounts for this heterogeneity by incorporating it in the statistical model. Unlike in the homoscedastic setting, the resulting nonconvex optimization problem is not seemingly solved by singular value decomposition. This paper develops a heteroscedastic probabilistic PCA technique (HePPCAT) that uses efficient alternating maximization algorithms to jointly estimate both the underlying factors and the unknown noise variances. Simulation experiments illustrate the comparative speed of the algorithms, the benefit of accounting for heteroscedasticity, and the seemingly favorable optimization landscape of this problem.      
