# ArXiv eess --Mon, 4 Oct 2021
### 1.RLO-MPC: Robust Learning-Based Output Feedback MPC for Improving the Performance of Uncertain Systems in Iterative Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2110.00542.pdf)
>  In this work we address the problem of performing a repetitive task when we have uncertain observations and dynamics. We formulate this problem as an iterative infinite horizon optimal control problem with output feedback. Previously, this problem was solved for linear time-invariant (LTI) system for the case when noisy full-state measurements are available using a robust iterative learning control framework, which we refer to as robust learning-based model predictive control (RL-MPC). However, this work does not apply to the case when only noisy observations of part of the state are available. This limits the applicability of current approaches in practice: First, in practical applications we typically do not have access to the full state. Second, uncertainties in the observations, when not accounted for, can lead to instability and constraint violations. To overcome these limitations, we propose a combination of RL-MPC with robust output feedback model predictive control, named robust learning-based output feedback model predictive control (RLO-MPC). We show recursive feasibility and stability, and prove theoretical guarantees on the performance over iterations. We validate the proposed approach with a numerical example in simulation and a quadrotor stabilization task in experiments.      
### 2.A survey on active noise control techniques -- Part I: Linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.00531.pdf)
>  Active noise control (ANC) is an effective way for reducing the noise level in electroacoustic or electromechanical systems. Since its first introduction in 1936, this approach has been greatly developed. This paper focuses on discussing the development of ANC techniques over the past decade. Linear ANC algorithms, including the celebrated filtered-x least-mean-square (FxLMS)-based algorithms and distributed ANC algorithms, are investigated and evaluated. Nonlinear ANC (NLANC) techniques, such as functional link artificial neural network (FLANN)-based algorithms, are pursued in Part II. Furthermore, some novel methods and applications of ANC emerging in the past decade are summarized. Finally, future research challenges regarding the ANC technique are discussed.      
### 3.A Wideband Signal Recognition Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2110.00518.pdf)
>  Signal recognition is a spectrum sensing problem that jointly requires detection, localization in time and frequency, and classification. This is a step beyond most spectrum sensing work which involves signal detection to estimate "present" or "not present" detections for either a single channel or fixed sized channels or classification which assumes a signal is present. We define the signal recognition task, present the metrics of precision and recall to the RF domain, and review recent machine-learning based approaches to this problem. We introduce a new dataset that is useful for training neural networks to perform these tasks and show a training framework to train wideband signal recognizers.      
### 4.Optic Disc Segmentation using Disk-Centered Patch Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2110.00512.pdf)
>  The optic disc is a crucial diagnostic feature in the eye since changes to its physiognomy is correlated with the severity of various ocular and cardiovascular diseases. While identifying the bulk of the optic disc in a color fundus image is straightforward, accurately segmenting its boundary at the pixel level is very challenging. In this work, we propose disc-centered patch augmentation (DCPA) -- a simple, yet novel training scheme for deep neural networks -- to address this problem. DCPA achieves state-of-the-art results on full-size images even when using small neural networks, specifically a U-Net with only 7 million parameters as opposed to the original 31 million. In DCPA, we restrict the training data to patches that fully contain the optic nerve. In addition, we also train the network using dynamic cost functions to increase its robustness. We tested DCPA-trained networks on five retinal datasets: DRISTI, DRIONS-DB, DRIVE, AV-WIDE, and CHASE-DB. The first two had available optic disc ground truth, and we manually estimated the ground truth for the latter three. Our approach achieved state-of-the-art F1 and IOU results on four datasets (95 % F1, 91 % IOU on DRISTI; 92 % F1, 84 % IOU on DRIVE; 83 % F1, 71 % IOU on AV-WIDE; 83 % F1, 71 % IOU on CHASEDB) and competitive results on the fifth (95 % F1, 91 % IOU on DRIONS-DB), confirming its generality. Our open-source code and ground-truth annotations are available at: <a class="link-external link-https" href="https://github.com/saeidmotevali/fundusdisk" rel="external noopener nofollow">this https URL</a>      
### 5.Preconditioned Plug-and-Play ADMM with Locally Adjustable Denoiser for Image Restoration  [ :arrow_down: ](https://arxiv.org/pdf/2110.00493.pdf)
>  Plug-and-Play optimization recently emerged as a powerful technique for solving inverse problems by plugging a denoiser into a classical optimization algorithm. The denoiser accounts for the regularization and therefore implicitly determines the prior knowledge on the data, hence replacing typical handcrafted priors. In this paper, we extend the concept of plug-and-play optimization to use denoisers that can be parameterized for non-constant noise variance. In that aim, we introduce a preconditioning of the ADMM algorithm, which mathematically justifies the use of such an adjustable denoiser. We additionally propose a procedure for training a convolutional neural network for high quality non-blind image denoising that also allows for pixel-wise control of the noise standard deviation. We show that our pixel-wise adjustable denoiser, along with a suitable preconditioning strategy, can further improve the plug-and-play ADMM approach for several applications, including image completion, interpolation, demosaicing and Poisson denoising.      
### 6.A Graph-theoretic Algorithm for Small Bowel Path Tracking in CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2110.00466.pdf)
>  We present a novel graph-theoretic method for small bowel path tracking. It is formulated as finding the minimum cost path between given start and end nodes on a graph that is constructed based on the bowel wall detection. We observed that a trivial solution with many short-cuts is easily made even with the wall detection, where the tracked path penetrates indistinct walls around the contact between different parts of the small bowel. Thus, we propose to include must-pass nodes in finding the path to better cover the entire course of the small bowel. The proposed method does not entail training with ground-truth paths while the previous methods do. We acquired ground-truth paths that are all connected from start to end of the small bowel for 10 abdominal CT scans, which enables the evaluation of the path tracking for the entire course of the small bowel. The proposed method showed clear improvements in terms of several metrics compared to the baseline method. The maximum length of the path that is tracked without an error per scan, by the proposed method, is above 800mm on average.      
### 7.Comparing Run Time Assurance Approaches for Safe Spacecraft Docking  [ :arrow_down: ](https://arxiv.org/pdf/2110.00447.pdf)
>  Run Time Assurance (RTA) systems are online safety verification techniques that filter the output of a primary controller to assure safety. RTA approaches are used in safety-critical control to intervene when a performance-driven primary controller would cause the system to violate safety constraints. This paper presents four categories of RTA approaches based on their membership to explicit or implicit monitoring and switching or optimization interventions. To validate the feasibility of each approach and compare computation time, four RTAs are defined for a three-dimensional spacecraft docking example with safety constraints on velocity.      
### 8.A Bayesian approach to location estimation of mobile devices from mobile network operator data  [ :arrow_down: ](https://arxiv.org/pdf/2110.00439.pdf)
>  Mobile network operator (MNO) data are a rich data source for official statistics, such as present population, mobility, migration, and tourism. Estimating the geographic location of mobile devices is an essential step for statistical inference. Most studies use the Voronoi tessellation for this, which is based on the assumption that mobile devices are always connected to the nearest radio cell. This paper uses a modular Bayesian approach, allowing for different modules of prior knowledge about where devices are expected to be, and different modules for the likelihood of connection given a geographic location. We discuss and compare the use of several prior modules, including one that is based on land use. We show that the Voronoi tessellation can be used as a likelihood module. Alternatively, we propose a signal strength model using radio cell properties such as antenna height, propagation direction, and power. Using Bayes' rule, we derive a posterior probability distribution that is an estimate for the geographic location, which can be used for further statistical inference. We describe the method and provide illustrations of a fictional example that resembles a real-world situation. The method has been implemented in the R packages mobloc and mobvis, which are briefly described.      
### 9.Learning of Inter-Label Geometric Relationships Using Self-Supervised Learning: Application To Gleason Grade Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2110.00404.pdf)
>  Segmentation of Prostate Cancer (PCa) tissues from Gleason graded histopathology images is vital for accurate diagnosis. Although deep learning (DL) based segmentation methods achieve state-of-the-art accuracy, they rely on large datasets with manual annotations. We propose a method to synthesize for PCa histopathology images by learning the geometrical relationship between different disease labels using self-supervised learning. We use a weakly supervised segmentation approach that uses Gleason score to segment the diseased regions and the resulting segmentation map is used to train a Shape Restoration Network (ShaRe-Net) to predict missing mask segments in a self-supervised manner. Using DenseUNet as the backbone generator architecture we incorporate latent variable sampling to inject diversity in the image generation process and thus improve robustness. Experiments on multiple histopathology datasets demonstrate the superiority of our method over competing image synthesis methods for segmentation tasks. Ablation studies show the benefits of integrating geometry and diversity in generating high-quality images, and our self-supervised approach with limited class-labeled data achieves similar performance as fully supervised learning.      
### 10.Finite Time Exact Quantized Average Consensus with Limited Resources and Transmission Stopping for Energy-Aware Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.00359.pdf)
>  Composed of spatially distributed sensors and actuators that communicate through wireless networks, networked control systems are emerging as a fundamental infrastructure technology in 5G and IoT technologies, including diverse applications, such as autonomous vehicles, UAVs, and various sensing devices. In order to increase flexibility and reduce deployment and maintenance costs, many such applications consider battery-powered or energy-harvesting networks, which bring additional limitations on the energy consumption of the wireless network. Specifically, the operation of battery-powered or energy-harvesting wireless communication networks needs to guarantee (i) efficient communication between nodes and (ii) preservation of available energy. Motivated by these novel requirements, in this paper, we present and analyze a novel distributed average consensus algorithm, which (i) operates exclusively on quantized values (in order to guarantee efficient communication and data storage), and (ii) relies on event-driven updates (in order to reduce energy consumption, communication bandwidth, network congestion, and/or processor usage). We characterize the properties of the proposed algorithm and show that its execution, on any time-invariant and strongly connected digraph, will allow all nodes to reach, in finite time, a common consensus value that is equal to the exact average (represented as the ratio of two quantized values). Furthermore, we show that our algorithm allows each node to cease transmissions once the exact average of the initial quantized values has been reached (in order to preserve its battery energy). Then, we present upper bounds on (i) the number of transmissions and computations each node has to perform during the execution of the algorithm, and (ii) the memory and energy requirements of each node in order for the algorithm to be executed.      
### 11.LiDAR Aided Human Blockage Prediction for 6G  [ :arrow_down: ](https://arxiv.org/pdf/2110.00349.pdf)
>  Leveraging higher frequencies up to THz band paves the way towards a faster network in the next generation of wireless communications. However, such shorter wavelengths are susceptible to higher scattering and path loss forcing the link to depend predominantly on the line-of-sight (LOS) path. Dynamic movement of humans has been identified as a major source of blockages to such LOS links. In this work, we aim to overcome this challenge by predicting human blockages to the LOS link enabling the transmitter to anticipate the blockage and act intelligently. We propose an end-to-end system of infrastructure-mounted LiDAR sensors to capture the dynamics of the communication environment visually, process the data with deep learning and ray casting techniques to predict future blockages. Experiments indicate that the system achieves an accuracy of 87% predicting the upcoming blockages while maintaining a precision of 78% and a recall of 79% for a window of 300 ms.      
### 12.DCT based Fusion of Variable Exposure Images for HDRI  [ :arrow_down: ](https://arxiv.org/pdf/2110.00312.pdf)
>  Combining images with different exposure settings are of prime importance in the field of computational photography. Both transform domain approach and filtering based approaches are possible for fusing multiple exposure images, to obtain the well-exposed image. We propose a Discrete Cosine Transform (DCT-based) approach for fusing multiple exposure images. The input image stack is processed in the transform domain by an averaging operation and the inverse transform is performed on the averaged image obtained to generate the fusion of multiple exposure image. The experimental observation leads us to the conjecture that the obtained DCT coefficients are indicators of parameters to measure well-exposedness, contrast and saturation as specified in the traditional exposure fusion based approach and the averaging performed indicates equal weights assigned to the DCT coefficients in this non-parametric and non pyramidal approach to fuse the multiple exposure stack.      
### 13.Incremental Dissipativity based Control of Discrete-Time Nonlinear Systems via the LPV Framework  [ :arrow_down: ](https://arxiv.org/pdf/2110.00290.pdf)
>  Unlike for Linear Time-Invariant (LTI) systems, for nonlinear systems, there exists no general framework for systematic convex controller design which incorporates performance shaping. The Linear Parameter-Varying (LPV) framework sought to bridge this gap by extending convex LTI synthesis results such that they could be applied to nonlinear systems. However, recent literature has shown that naive application of the LPV framework can fail to guarantee the desired asymptotic stability guarantees for nonlinear systems. Incremental dissipativity theory has been successfully used in the literature to overcome these issues for Continuous-Time (CT) systems. However, so far no solution has been proposed for output-feedback based incremental control for the Discrete-Time (DT) case. Using recent results on convex analysis of incremental dissipativity for DT nonlinear systems, in this paper, we propose a convex output-feedback controller synthesis method to ensure closed-loop incremental dissipativity of DT nonlinear systems via the LPV framework. The proposed method is applied on a simulation example, demonstrating improved stability and performance properties compared to a standard LPV controller design.      
### 14.SALSA: Spatial Cue-Augmented Log-Spectrogram Features for Polyphonic Sound Event Localization and Detection  [ :arrow_down: ](https://arxiv.org/pdf/2110.00275.pdf)
>  Sound event localization and detection (SELD) consists of two subtasks, which are sound event detection and direction-of-arrival estimation. While sound event detection mainly relies on time-frequency patterns to distinguish different sound classes, direction-of-arrival estimation uses amplitude and/or phase differences between microphones to estimate source directions. As a result, it is often difficult to jointly optimize these two subtasks. We propose a novel feature called Spatial cue-Augmented Log-SpectrogrAm (SALSA) with exact time-frequency mapping between the signal power and the source directional cues, which is crucial for resolving overlapping sound sources. The SALSA feature consists of multichannel log-spectrograms stacked along with the normalized principal eigenvector of the spatial covariance matrix at each corresponding time-frequency bin. Depending on the microphone array format, the principal eigenvector can be normalized differently to extract amplitude and/or phase differences between the microphones. As a result, SALSA features are applicable for different microphone array formats such as first-order ambisonics (FOA) and multichannel microphone array (MIC). Experimental results on the TAU-NIGENS Spatial Sound Events 2021 dataset with directional interferences showed that SALSA features outperformed other state-of-the-art features. Specifically, the use of SALSA features in the FOA format increased the F1 score and localization recall by 6% each, compared to the multichannel log-mel spectrograms with intensity vectors. For the MIC format, using SALSA features increased F1 score and localization recall by 16% and 7%, respectively, compared to using multichannel log-mel spectrograms with generalized cross-correlation spectra. Our ensemble model trained on SALSA features ranked second in the team category of the SELD task in the 2021 DCASE Challenge.      
### 15.Learn to Communicate with Neural Calibration: Scalability and Generalization  [ :arrow_down: ](https://arxiv.org/pdf/2110.00272.pdf)
>  The conventional design of wireless communication systems typically relies on established mathematical models that capture the characteristics of different communication modules. Unfortunately, such design cannot be easily and directly applied to future wireless networks, which will be characterized by large-scale ultra-dense networks whose design complexity scales exponentially with the network size. Furthermore, such networks will vary dynamically in a significant way, which makes it intractable to develop comprehensive analytical models. Recently, deep learning-based approaches have emerged as potential alternatives for designing complex and dynamic wireless systems. However, existing learning-based methods have limited capabilities to scale with the problem size and to generalize with varying network settings. In this paper, we propose a scalable and generalizable neural calibration framework for future wireless system design, where a neural network is adopted to calibrate the input of conventional model-based algorithms. Specifically, the backbone of a traditional time-efficient algorithm is integrated with deep neural networks to achieve a high computational efficiency, while enjoying enhanced performance. The permutation equivariance property, carried out by the topological structure of wireless systems, is furthermore utilized to develop a generalizable neural network architecture. The proposed neural calibration framework is applied to solve challenging resource management problems in massive multiple-input multiple-output (MIMO) systems. Simulation results will show that the proposed neural calibration approach enjoys significantly improved scalability and generalization compared with the existing learning-based methods.      
### 16.Safety aware model-based reinforcement learning for optimal control of a class of output-feedback nonlinear systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.00271.pdf)
>  The ability to learn and execute optimal control policies safely is critical to realization of complex autonomy, especially where task restarts are not available and/or the systems are safety-critical. Safety requirements are often expressed in terms of state and/or control constraints. Methods such as barrier transformation and control barrier functions have been successfully used, in conjunction with model-based reinforcement learning, for safe learning in systems under state constraints, to learn the optimal control policy. However, existing barrier-based safe learning methods rely on full state feedback. In this paper, an output-feedback safe model-based reinforcement learning technique is developed that utilizes a novel dynamic state estimator to implement simultaneous learning and control for a class of safety-critical systems with partially observable state.      
### 17.New Approaches for Verification of Delay Coobservability of Discrete Event Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.00265.pdf)
>  In the context of decentralized networked supervisory control of discrete event systems, the local supervisors need to observe event occurrences to make correct control decisions. Delay coobservability describes whether or not the local supervisors can make sufficient observations under observation delays. We study the verification of delay coobservability in this paper. The existing approach constructs a set of verifiers to track all the indistinguishable string collections. To improve the existing approach, this paper presents two different approaches. The first approach is an extension of the existing approach, and only uses one of the verifiers constructed before. The second approach is entirely new compared with the first approach and the existing approach. In our second approach, the specification language is partitioned into a finite number of sets such that strings in different sets have different lengths. For each of the sets, a verifier is constructed to check if there exists a string in the set causing a violation of delay coobservability. Both of these two approaches are shown to be more efficient than the existing approach. Finally, a practical example is provided to illustrate how to verify delay coobservability using the results derived in this paper.      
### 18.Neural Networks Compensation of Systems with Multi-segment Piecewise Linear Nonlinearities  [ :arrow_down: ](https://arxiv.org/pdf/2110.00219.pdf)
>  A neural networks (NN) compensator is designed for systems with multi-segment piecewise-linear nonlinearities. The compensator uses the back stepping technique with NN for inverting the multi-segment piecewise-linear nonlinearities in the feedforward path. This scheme provides a general procedure for determining the dynamic pre-inversion of an invertible dynamic system using NN. A tuning algorithm is presented for the NN compensator which yields a stable closed-loop system. In the case of nonlinear stability proofs, the tracking error is small. It is noted that PI controller without NN compensation requires much higher gain to achieve same performance. It is also difficult to ensure the stability of such highly nonlinear systems using only PI controllers. Using NN compensation, stability of the system is proven, and tracking errors can be arbitrarily kept small by increasing the gain. The NN weight errors are basically bounded in terms of input weight and hidden weight. Simulation results show the effectiveness of the piecewise linear NN compensator in the system. This scheme is applicable to xy table-like servo system and shows neural network stability proofs. In addition, the NN piecewise linear nonlinearity compensation can be further and applied to backlash, hysteresis, and another actuator nonlinear compensation.      
### 19.In-Circuit Differential-Mode Impedance Extraction at the AC Input of a Motor Drive System  [ :arrow_down: ](https://arxiv.org/pdf/2110.00208.pdf)
>  The in-circuit differential-mode (DM) impedance at the AC input of a motor drive system (MDS) serves as a key parameter to evaluate and estimate the DM electromagnetic interference (EMI) noise caused by the switching of power semiconductor devices in the MDS. This paper discusses a single-probe setup (SPS) with frequency-domain measurement to extract the in-circuit DM impedance of an MDS under its different operating modes. The advantages of the SPS are its non-contact measurement and simple structure.      
### 20.Contraction-Based Methods for Stable Identification and Robust Machine Learning: a Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2110.00207.pdf)
>  This tutorial paper provides an introduction to recently developed tools for machine learning, especially learning dynamical systems (system identification), with stability and robustness constraints. The main ideas are drawn from contraction analysis and robust control, but adapted to problems in which large-scale models can be learnt with behavioural guarantees. We illustrate the methods with applications in robust image recognition and system identification.      
### 21.Error-free approximation of explicit linear MPC through lattice piecewise affine expression  [ :arrow_down: ](https://arxiv.org/pdf/2110.00201.pdf)
>  In this paper, the disjunctive and conjunctive lattice piecewise affine (PWA) approximations of explicit linear model predictive control (MPC) are proposed. The training data is generated uniformly in the domain of interest, consisting of the state samples and corresponding affine control laws, based on which the lattice PWA approximations are constructed. Resampling of data is also proposed to guarantee that the lattice PWA approximations are identical to the explicit MPC control law in unique order (UO) regions containing the sample points as interior points. Besides, under mild assumptions, the equivalence of the 2 lattice PWA approximations guarantees the approximations are error-free in the domain of interest. The algorithms for deriving statistical error-free approximation to the explicit linear MPC is proposed and the complexity of the whole procedure is analyzed, which is polynomial with respect to the number of samples. The performance of the proposed approximation strategy is tested through 2 simulation examples, and the result shows that with a moderate number of sample points, we can construct lattice PWA approximations that are equivalent to optimal control law of the explicit linear MPC.      
### 22.Improving Load Forecast in Energy Markets During COVID-19  [ :arrow_down: ](https://arxiv.org/pdf/2110.00181.pdf)
>  The abrupt outbreak of the COVID-19 pandemic was the most significant event in 2020, which had profound and lasting impacts across the world. Studies on energy markets observed a decline in energy demand and changes in energy consumption behaviors during COVID-19. However, as an essential part of system operation, how the load forecasting performs amid COVID-19 is not well understood. This paper aims to bridge the research gap by systematically evaluating models and features that can be used to improve the load forecasting performance amid COVID-19. Using real-world data from the New York Independent System Operator, our analysis employs three deep learning models and adopts both novel COVID-related features as well as classical weather-related features. We also propose simulating the stay-at-home situation with pre-stay-at-home weekend data and demonstrate its effectiveness in improving load forecasting accuracy during COVID-19.      
### 23.On the design of fixed-gain tracking filters by pole placement: Or an introduction to applied signals-and-systems theory for engineers  [ :arrow_down: ](https://arxiv.org/pdf/2110.00153.pdf)
>  The Kalman filter computes the optimal variable-gain using prior knowledge of the initial state and random (process and measurement) noise distributions, which are assumed to be Gaussian with known variance. However, when these distributions are unknown, the Kalman filter is not necessarily optimal and other simpler state-estimators, such as fixed-gain ({\alpha}, {\alpha}-\b{eta} or {\alpha}-\b{eta}-{\gamma} etc.) filters may be sufficient. When such filters are used as low-complexity state-estimators in embedded tracking systems, the fixed gain parameters are usually set equal to the steady-state gains of the corresponding Kalman filter. An alternative procedure, that does not rely prior distributions, based on Luenberger observers, is presented here. It is suggested that the arbitrary placement of closed-loop state-observer poles is a simple and intuitive way of tuning the transient and steady-state response of a fixed-gain tracking filter when prior distributions are unknown. All poles are placed inside the unit circle on the positive real axis of the complex z-plane at p for a well damped response and a configurable bandwidth. Transient bias errors, e.g. due to target manoeuvres or process modelling errors, decrease as p=0 is approached for a wider bandwidth. Steady-state random errors, e.g. due to sensor noise, decrease as p=1 is approached for a narrower bandwidth. Thus the p parameter (with 0&lt;p&lt;1) may be interpreted as a dimensionless smoothing factor. This tutorial-style report examines state-observer design by pole placement, which is a standard procedure for feedback controls but unusual for tracking filters, due to the success and popularity of the Kalman filter. As Bayesian trackers are designed via statistical modelling, not by pole-zero placement in the complex plane, the underlying principles of linear time-invariant signals and systems are also reviewed.      
### 24.Development of the algorithm for differentiating bone metastases and trauma of the ribs in bone scintigraphy and demonstration of visual evidence of the algorithm -- Using only anterior bone scan view of thorax  [ :arrow_down: ](https://arxiv.org/pdf/2110.00130.pdf)
>  Background: Although there are many studies on the application of artificial intelligence (AI) models to medical imaging, there is no report of an AI model that determines the accumulation of ribs in bone metastases and trauma only using the anterior image of thorax of bone scintigraphy. In recent years, a method for visualizing diagnostic grounds called Gradient-weighted Class Activation Mapping (Grad-CAM) has been proposed in the area of diagnostic images using Deep Convolutional Neural Network (DCNN). As far as we have investigated, there are no reports of visualization of the diagnostic basis in bone scintigraphy. Our aim is to visualize the area of interest of DCNN, in addition to developing an algorithm to classify and diagnose whether RI accumulation on the ribs is bone metastasis or trauma using only anterior bone scan view of thorax. Material and Methods: For this retrospective study, we used 838 patients who underwent bone scintigraphy to search for bone metastases at our institution. A frontal chest image of bone scintigraphy was used to create the algorithm. We used 437 cases with bone metastases on the ribs and 401 cases with abnormal RI accumulation due to trauma. Result: AI model was able to detect bone metastasis lesion with a sensitivity of 90.00% and accuracy of 86.5%. And it was possible to visualize the part that the AI model focused on with Grad-CAM.      
### 25.DeepMCAT: Large-Scale Deep Clustering for Medical Image Categorization  [ :arrow_down: ](https://arxiv.org/pdf/2110.00109.pdf)
>  In recent years, the research landscape of machine learning in medical imaging has changed drastically from supervised to semi-, weakly- or unsupervised methods. This is mainly due to the fact that ground-truth labels are time-consuming and expensive to obtain manually. Generating labels from patient metadata might be feasible but it suffers from user-originated errors which introduce biases. In this work, we propose an unsupervised approach for automatically clustering and categorizing large-scale medical image datasets, with a focus on cardiac MR images, and without using any labels. We investigated the end-to-end training using both class-balanced and imbalanced large-scale datasets. Our method was able to create clusters with high purity and achieved over 0.99 cluster purity on these datasets. The results demonstrate the potential of the proposed method for categorizing unstructured large medical databases, such as organizing clinical PACS systems in hospitals.      
### 26.Information Design for a Non-atomic Service Scheduling Game  [ :arrow_down: ](https://arxiv.org/pdf/2110.00090.pdf)
>  We study an information design problem for a non-atomic service scheduling game. The service starts at a random time and there is a continuum of agent population who have a prior belief about the service start time but do not observe the actual realization of it. The agents want to make decisions of when to join the queue in order to avoid long waits in the queue or not to arrive earlier than the service has started. There is a planner who knows when the service starts and makes suggestions to the agents about when to join the queue through an obedient direct signaling strategy, in order to minimize the average social cost. We characterize the full information and the no information equilibria and we show in what conditions it is optimal for the planner to reveal the full information to the agents. Further, by imposing appropriate assumptions on the model, we formulate the information design problem as a generalized problem of moments (GPM) and use computational tools developed for such problems to solve the problem numerically.      
### 27.Noise2Recon: A Semi-Supervised Framework for Joint MRI Reconstruction and Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2110.00075.pdf)
>  Deep learning (DL) has shown promise for faster, high quality accelerated MRI reconstruction. However, standard supervised DL methods depend on extensive amounts of fully-sampled ground-truth data and are sensitive to out-of-distribution (OOD) shifts, in particular for low signal-to-noise ratio (SNR) acquisitions. To alleviate this challenge, we propose a semi-supervised, consistency-based framework (termed Noise2Recon) for joint MR reconstruction and denoising. Our method enables the usage of a limited number of fully-sampled and a large number of undersampled-only scans. We compare our method to augmentation-based supervised techniques and fine-tuned denoisers. Results demonstrate that even with minimal ground-truth data, Noise2Recon (1) achieves high performance on in-distribution (low-noise) scans and (2) improves generalizability to OOD, noisy scans.      
### 28.Interleaved One-shot Semi-Persistent Scheduling for BSM Transmissions in C-V2X Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.00056.pdf)
>  Cellular vehicle-to-everything (C-V2X) networks are regarded as one of the main pillars to enable efficient and sustainable Intelligent Transportation Systems (ITS) safety applications and services. Such services rely on the concept of exchanging periodic status updates (i.e., basic safety messages (BSMs)) between nearby vehicular users (VUEs). Hence, it is essential to ensure small inter-packet gaps (IPGs) between successive BSMs from nearby VUEs. Large IPGs, due to successive packet losses, can result in stale information at a VUE. In this paper, we study the tail behavior of the IPG and the information age (IA) distributions using C-V2X transmission mode 4 (a decentralized resource allocation method based on semi-persistent scheduling (SPS)). Specifically, we investigate improvements and trade-offs introduced by the SAE-specified concept of one-shot transmissions. We use a high-fidelity system-level simulator that closely follows the SPS process of C-V2X transmission mode 4 to evaluate the performance of the interleaved one-shot SPS transmissions. Our numerical results show that the tails of the IA and IPG complementary cumulative distribution functions (CCDFs) are significantly improved when one-shot transmissions are enabled in various simulation scenarios.      
### 29.Trajectory Planning with Deep Reinforcement Learning in High-Level Action Spaces  [ :arrow_down: ](https://arxiv.org/pdf/2110.00044.pdf)
>  This paper presents a technique for trajectory planning based on continuously parameterized high-level actions (motion primitives) of variable duration. This technique leverages deep reinforcement learning (Deep RL) to formulate a policy which is suitable for real-time implementation. There is no separation of motion primitive generation and trajectory planning: each individual short-horizon motion is formed during the Deep RL training to achieve the full-horizon objective. Effectiveness of the technique is demonstrated numerically on a well-studied trajectory generation problem and a planning problem on a known obstacle-rich map. This paper also develops a new loss function term for policy-gradient-based Deep RL, which is analogous to an anti-windup mechanism in feedback control. We demonstrate the inclusion of this new term in the underlying optimization increases the average policy return in our numerical example.      
### 30.Learning Multi-Site Harmonization of Magnetic Resonance Images Without Traveling Human Phantoms  [ :arrow_down: ](https://arxiv.org/pdf/2110.00041.pdf)
>  Harmonization improves data consistency and is central to effective integration of diverse imaging data acquired across multiple sites. Recent deep learning techniques for harmonization are predominantly supervised in nature and hence require imaging data of the same human subjects to be acquired at multiple sites. Data collection as such requires the human subjects to travel across sites and is hence challenging, costly, and impractical, more so when sufficient sample size is needed for reliable network training. Here we show how harmonization can be achieved with a deep neural network that does not rely on traveling human phantom data. Our method disentangles site-specific appearance information and site-invariant anatomical information from images acquired at multiple sites and then employs the disentangled information to generate the image of each subject for any target site. We demonstrate with more than 6,000 multi-site T1- and T2-weighted images that our method is remarkably effective in generating images with realistic site-specific appearances without altering anatomical details. Our method allows retrospective harmonization of data in a wide range of existing modern large-scale imaging studies, conducted via different scanners and protocols, without additional data collection.      
### 31.Leveraging Low-Distortion Target Estimates for Improved Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2110.00570.pdf)
>  A promising approach for multi-microphone speech separation involves two deep neural networks (DNN), where the predicted target speech from the first DNN is used to compute signal statistics for time-invariant minimum variance distortionless response (MVDR) beamforming, and the MVDR result is then used as extra features for the second DNN to predict target speech. Previous studies suggested that the MVDR result can provide complementary information for the second DNN to better predict target speech. However, on fixed-geometry arrays, both DNNs can take in, for example, the real and imaginary (RI) components of the multi-channel mixture as features to leverage the spatial and spectral information for enhancement. It is not explained clearly why the linear MVDR result can be complementary and why it is still needed, considering that the DNNs and the beamformer use the same input, and the DNNs perform non-linear filtering and could render the linear filtering of MVDR unnecessary. Similarly, in monaural cases, one can replace the MVDR beamformer with a monaural weighted prediction error (WPE) filter. Although the linear WPE filter and the DNNs use the same mixture RI components as input, the WPE result is found to significantly improve the second DNN. This study provides a novel explanation from the perspective of the low-distortion nature of such algorithms, and finds that they can consistently improve phase estimation. Equipped with this understanding, we investigate several low-distortion target estimation algorithms including several beamformers, WPE, forward convolutive prediction, and their combinations, and use their results as extra features to train the second network to achieve better enhancement. Evaluation results on single- and multi-microphone speech dereverberation and enhancement tasks indicate the effectiveness of the proposed approach, and the validity of the proposed view.      
### 32.Weight Vector Tuning and Asymptotic Analysis of Binary Linear Classifiers  [ :arrow_down: ](https://arxiv.org/pdf/2110.00567.pdf)
>  Unlike its intercept, a linear classifier's weight vector cannot be tuned by a simple grid search. Hence, this paper proposes weight vector tuning of a generic binary linear classifier through the parameterization of a decomposition of the discriminant by a scalar which controls the trade-off between conflicting informative and noisy terms. By varying this parameter, the original weight vector is modified in a meaningful way. Applying this method to a number of linear classifiers under a variety of data dimensionality and sample size settings reveals that the classification performance loss due to non-optimal native hyperparameters can be compensated for by weight vector tuning. This yields computational savings as the proposed tuning method reduces to tuning a scalar compared to tuning the native hyperparameter, which may involve repeated weight vector generation along with its burden of optimization, dimensionality reduction, etc., depending on the classifier. It is also found that weight vector tuning significantly improves the performance of Linear Discriminant Analysis (LDA) under high estimation noise. Proceeding from this second finding, an asymptotic study of the misclassification probability of the parameterized LDA classifier in the growth regime where the data dimensionality and sample size are comparable is conducted. Using random matrix theory, the misclassification probability is shown to converge to a quantity that is a function of the true statistics of the data. Additionally, an estimator of the misclassification probability is derived. Finally, computationally efficient tuning of the parameter using this estimator is demonstrated on real data.      
### 33.Design of multiplicative watermarking against covert attacks  [ :arrow_down: ](https://arxiv.org/pdf/2110.00555.pdf)
>  This paper addresses the design of an active cyberattack detection architecture based on multiplicative watermarking, allowing for detection of covert attacks. We propose an optimal design problem, relying on the so-called output-to-output l2-gain, which characterizes the maximum gain between the residual output of a detection scheme and some performance output. Although optimal, this control problem is non-convex. Hence, we propose an algorithm to design the watermarking filters by solving the problem suboptimally via LMIs. We show that, against covert attacks, the output-to-output l2-gain is unbounded without watermarking, and we provide a sufficient condition for boundedness in the presence of watermarks.      
### 34.Channel Estimation with Reconfigurable Intelligent Surfaces -- A General Framework  [ :arrow_down: ](https://arxiv.org/pdf/2110.00553.pdf)
>  Optimally extracting the advantages available from reconfigurable intelligent surfaces (RISs) in wireless communications systems requires estimation of the channels to and from the RIS. The process of determining these channels is complicated by the fact that the RIS is typically composed of passive elements without any data processing capabilities, and thus the channels must be estimated indirectly by a non-colocated device, typically a controlling base station. In this article, we examine channel estimation for RIS-based systems from a fundamental viewpoint. We study various possible channel models and the identifiability of the models as a function of the available pilot data and behavior of the RIS during training. In particular, we consider situations with and without line-of-sight propagation, single- and multiple-antenna configurations for the users and base station, correlated and sparse channel models, single-carrier and wideband OFDM scenarios, availability of direct links between the users and base station, exploitation of prior information, as well as a number of other special cases. We further conduct numerical comparisons of achievable performance for various channel models using the relevant Cramer-Rao bounds.      
### 35.An Ensemble-based Multi-Criteria Decision Making Method for COVID-19 Cough Classification  [ :arrow_down: ](https://arxiv.org/pdf/2110.00508.pdf)
>  The objectives of this research are analysing the performance of the state-of-the-art machine learning techniques for classifying COVID-19 from cough sound and identifying the model(s) that consistently perform well across different cough datasets. Different performance evaluation metrics (such as precision, sensitivity, specificity, AUC, accuracy, etc.) make it difficult to select the best performance model. To address this issue, in this paper, we propose an ensemble-based multi-criteria decision making (MCDM) method for selecting top performance machine learning technique(s) for COVID-19 cough classification. We use four cough datasets, namely Cambridge, Coswara, Virufy, and NoCoCoDa to verify the proposed method. At first, our proposed method uses the audio features of cough samples and then applies machine learning (ML) techniques to classify them as COVID-19 or non-COVID-19. Then, we consider a multi-criteria decision-making (MCDM) method that combines ensemble technologies (i.e., soft and hard) to select the best model. In MCDM, we use the technique for order preference by similarity to ideal solution (TOPSIS) for ranking purposes, while entropy is applied to calculate evaluation criteria weights. In addition, we apply the feature reduction process through recursive feature elimination with cross-validation under different estimators. The results of our empirical evaluations show that the proposed method outperforms the state-of-the-art models.      
### 36.Robustly Removing Deep Sea Lighting Effects for Visual Mapping of Abyssal Plains  [ :arrow_down: ](https://arxiv.org/pdf/2110.00480.pdf)
>  The majority of Earth's surface lies deep in the oceans, where no surface light reaches. Robots diving down to great depths must bring light sources that create moving illumination patterns in the darkness, such that the same 3D point appears with different color in each image. On top, scattering and attenuation of light in the water makes images appear foggy and typically blueish, the degradation depending on each pixel's distance to its observed seafloor patch, on the local composition of the water and the relative poses and cones of the light sources. Consequently, visual mapping, including image matching and surface albedo estimation, severely suffers from the effects that co-moving light sources produce, and larger mosaic maps from photos are often dominated by lighting effects that obscure the actual seafloor structure. In this contribution a practical approach to estimating and compensating these lighting effects on predominantly homogeneous, flat seafloor regions, as can be found in the Abyssal plains of our oceans, is presented. The method is essentially parameter-free and intended as a preprocessing step to facilitate visual mapping, but already produces convincing lighting artefact compensation up to a global white balance factor. It does not require to be trained beforehand on huge sets of annotated images, which are not available for the deep sea. Rather, we motivate our work by physical models of light propagation, perform robust statistics-based estimates of additive and multiplicative nuisances that avoid explicit parameters for light, camera, water or scene, discuss the breakdown point of the algorithms and show results on imagery captured by robots in several kilometer water depth.      
### 37.Survey and synthesis of state of the art in driver monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2110.00472.pdf)
>  Road-vehicle accidents are mostly due to human errors, and many such accidents could be avoided by continuously monitoring the driver. Driver monitoring (DM) is a topic of growing interest in the automotive industry, and it will remain relevant for all vehicles that are not fully autonomous, and thus for decades for the average vehicle owner. The present paper focuses on the first step of DM, which consists in characterizing the state of the driver. Since DM will be increasingly linked to driving automation (DA), this paper presents a clear view of the role of DM at each of the six SAE levels of DA. This paper surveys the state of the art of DM, and then synthesizes it, providing a unique, structured, polychotomous view of the many characterization techniques of DM. Informed by the survey, the paper characterizes the driver state along the five main dimensions--called here "(sub)states"--of drowsiness, mental workload, distraction, emotions, and under the influence. The polychotomous view of DM is presented through a pair of interlocked tables that relate these states to their indicators (e.g., the eye-blink rate) and the sensors that can access each of these indicators (e.g., a camera). The tables factor in not only the effects linked directly to the driver, but also those linked to the (driven) vehicle and the (driving) environment. They show, at a glance, to concerned researchers, equipment providers, and vehicle manufacturers (1) most of the options they have to implement various forms of advanced DM systems, and (2) fruitful areas for further research and innovation.      
### 38.Guiding Evolutionary Strategies by Differentiable Robot Simulators  [ :arrow_down: ](https://arxiv.org/pdf/2110.00438.pdf)
>  In recent years, Evolutionary Strategies were actively explored in robotic tasks for policy search as they provide a simpler alternative to reinforcement learning algorithms. However, this class of algorithms is often claimed to be extremely sample-inefficient. On the other hand, there is a growing interest in Differentiable Robot Simulators (DRS) as they potentially can find successful policies with only a handful of trajectories. But the resulting gradient is not always useful for the first-order optimization. In this work, we demonstrate how DRS gradient can be used in conjunction with Evolutionary Strategies. Preliminary results suggest that this combination can reduce sample complexity of Evolutionary Strategies by 3x-5x times in both simulation and the real world.      
### 39.Predicting Flat-Fading Channels via Meta-Learned Closed-Form Linear Filters and Equilibrium Propagation  [ :arrow_down: ](https://arxiv.org/pdf/2110.00414.pdf)
>  Predicting fading channels is a classical problem with a vast array of applications, including as an enabler of artificial intelligence (AI)-based proactive resource allocation for cellular networks. Under the assumption that the fading channel follows a stationary complex Gaussian process, as for Rayleigh and Rician fading models, the optimal predictor is linear, and it can be directly computed from the Doppler spectrum via standard linear minimum mean squared error (LMMSE) estimation. However, in practice, the Doppler spectrum is unknown, and the predictor has only access to a limited time series of estimated channels. This paper proposes to leverage meta-learning in order to mitigate the requirements in terms of training data for channel fading prediction. Specifically, it first develops an offline low-complexity solution based on linear filtering via a meta-trained quadratic regularization. Then, an online method is proposed based on gradient descent and equilibrium propagation (EP). Numerical results demonstrate the advantages of the proposed approach, showing its capacity to approach the genie-aided LMMSE solution with a small number of training data points.      
### 40.Leveraging power grid topology in machine learning assisted optimal power flow  [ :arrow_down: ](https://arxiv.org/pdf/2110.00306.pdf)
>  Machine learning assisted optimal power flow (OPF) aims to reduce the computational complexity of these non-linear and non-convex constrained optimisation problems by consigning expensive (online) optimisation to offline training. The majority of work in this area typically employs fully-connected neural networks (FCNN). However, recently convolutional (CNN) and graph (GNN) neural networks have been also investigated, in effort to exploit topological information within the power grid. Although promising results have been obtained, there lacks a systematic comparison between these architectures throughout literature. Accordingly, we assess the performance of a variety of FCNN, CNN and GNN models for two fundamental approaches to machine learning assisted OPF: regression (predicting optimal generator set-points) and classification (predicting the active set of constraints). For several synthetic grids with interconnected utilities, we show that locality properties between feature and target variables are scarce, hence find limited merit of harnessing topological information in NN models for this set of problems.      
### 41.Cyber-physical risk modeling with imperfect cyber-attackers  [ :arrow_down: ](https://arxiv.org/pdf/2110.00301.pdf)
>  We model the risk posed by a malicious cyber-attacker seeking to induce grid insecurity by means of a load redistribution attack, while explicitly acknowledging that such an actor would plausibly base its decision strategy on imperfect information. More specifically, we introduce a novel formulation for the cyber-attacker's decision-making problem and analyze the distribution of decisions taken with randomly inaccurate data on the grid branch admittances or capacities, and the distribution of their respective impact. Our findings indicate that inaccurate admittance values most often lead to suboptimal cyber-attacks that still compromise the grid security, while inaccurate capacity values result in notably less effective attacks. We also find common attacked cyber-assets and common affected physical-assets between all (random) imperfect cyber-attacks, which could be exploited in a preventive and/or corrective sense for effective cyber-physical risk management.      
### 42.Machine learning aided noise filtration and signalc lassification for CREDO experiment  [ :arrow_down: ](https://arxiv.org/pdf/2110.00297.pdf)
>  The wealth of smartphone data collected by the Cosmic Ray Extremely Distributed Observatory(CREDO) greatly surpasses the capabilities of manual analysis. So, efficient means of rejectingthe non-cosmic-ray noise and identification of signals attributable to extensive air showers arenecessary. To address these problems we discuss a Convolutional Neural Network-based method ofartefact rejection and complementary method of particle identification based on common statisticalclassifiers as well as their ensemble extensions. These approaches are based on supervised learning,so we need to provide a representative subset of the CREDO dataset for training and validation.According to this approach over 2300 images were chosen and manually labeled by 5 judges.The images were split into spot, track, worm (collectively named signals) and artefact classes.Then the preprocessing consisting of luminance summation of RGB channels (grayscaling) andbackground removal by adaptive thresholding was performed. For purposes of artefact rejectionthe binary CNN-based classifier was proposed which is able to distinguish between artefacts andsignals. The classifier was fed with input data in the form of Daubechies wavelet transformedimages. In the case of cosmic ray signal classification, the well-known feature-based classifierswere considered. As feature descriptors, we used Zernike moments with additional feature relatedto total image luminance. For the problem of artefact rejection, we obtained an accuracy of 99%. For the 4-class signal classification, the best performing classifiers achieved a recognition rate of 88%.      
### 43.pyFFS: A Python Library for Fast Fourier Series Computation  [ :arrow_down: ](https://arxiv.org/pdf/2110.00262.pdf)
>  Fourier transforms are an often necessary component in many computational tasks, and can be computed efficiently through the fast Fourier transform (FFT) algorithm. However, many applications involve an underlying continuous signal, and a more natural choice would be to work with e.g. the Fourier series (FS) coefficients in order to avoid the additional overhead of translating between the analog and discrete domains. Unfortunately, there exists very little literature and tools for the manipulation of FS coefficients from discrete samples. This paper introduces a Python library called pyFFS for efficient FS coefficient computation, convolution, and interpolation. While the libraries SciPy and NumPy provide efficient functionality for discrete Fourier transform coefficients via the FFT algorithm, pyFFS addresses the computation of FS coefficients through what we call the fast Fourier series (FFS). Moreover, pyFFS includes an FS interpolation method based on the chirp Z-transform that can make it more than an order of magnitude faster than the SciPy equivalent when one wishes to perform interpolation. GPU support through the CuPy library allows for further acceleration, e.g. an order of magnitude faster for computing the 2-D FS coefficients of 1000 x 1000 samples and nearly two orders of magnitude faster for 2-D interpolation. As an application, we discuss the use of pyFFS in Fourier optics. pyFFS is available as an open source package at <a class="link-external link-https" href="https://github.com/imagingofthings/pyFFS" rel="external noopener nofollow">this https URL</a>, with documentation at <a class="link-external link-https" href="https://pyffs.readthedocs.io" rel="external noopener nofollow">this https URL</a>.      
### 44.Real-Time Risk-Bounded Tube-Based Trajectory Safety Verification  [ :arrow_down: ](https://arxiv.org/pdf/2110.00233.pdf)
>  In this paper, we address the real-time risk-bounded safety verification problem of continuous-time state trajectories of autonomous systems in the presence of uncertain time-varying nonlinear safety constraints. Risk is defined as the probability of not satisfying the uncertain safety constraints. Existing approaches to address the safety verification problems under uncertainties either are limited to particular classes of uncertainties and safety constraints, e.g., Gaussian uncertainties and linear constraints, or rely on sampling based methods. In this paper, we provide a fast convex algorithm to efficiently evaluate the probabilistic nonlinear safety constraints in the presence of arbitrary probability distributions and long planning horizons in real-time, without the need for uncertainty samples and time discretization. The provided approach verifies the safety of the given state trajectory and its neighborhood (tube) to account for the execution uncertainties and risk. In the provided approach, we first use the moments of the probability distributions of the uncertainties to transform the probabilistic safety constraints into a set of deterministic safety constraints. We then use convex methods based on sum-of-squares polynomials to verify the obtained deterministic safety constraints over the entire planning time horizon without time discretization. To illustrate the performance of the proposed method, we apply the provided method to the safety verification problem of self-driving vehicles and autonomous aerial vehicles.      
### 45.DNN-Opt: An RL Inspired Optimization for Analog Circuit Sizing using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.00211.pdf)
>  Analog circuit sizing takes a significant amount of manual effort in a typical design cycle. With rapidly developing technology and tight schedules, bringing automated solutions for sizing has attracted great attention. This paper presents DNN-Opt, a Reinforcement Learning (RL) inspired Deep Neural Network (DNN) based black-box optimization framework for analog circuit sizing. The key contributions of this paper are a novel sample-efficient two-stage deep learning optimization framework leveraging RL actor-critic algorithms, and a recipe to extend it on large industrial circuits using critical device identification. Our method shows 5--30x sample efficiency compared to other black-box optimization methods both on small building blocks and on large industrial circuits with better performance metrics. To the best of our knowledge, this is the first application of DNN-based circuit sizing on industrial scale circuits.      
### 46.Q-Net: A Quantitative Susceptibility Mapping-based Deep Neural Network for Differential Diagnosis of Brain Iron Deposition in Hemochromatosis  [ :arrow_down: ](https://arxiv.org/pdf/2110.00203.pdf)
>  Brain iron deposition, in particular deep gray matter nuclei, increases with advancing age. Hereditary Hemochromatosis (HH) is the most common inherited disorder of systemic iron excess in Europeans and recent studies claimed high brain iron accumulation in patient with Hemochromatosis. In this study, we focus on Artificial Intelligence (AI)-based differential diagnosis of brain iron deposition in HH via Quantitative Susceptibility Mapping (QSM), which is an established Magnetic Resonance Imaging (MRI) technique to study the distribution of iron in the brain. Our main objective is investigating potentials of AI-driven frameworks to accurately and efficiently differentiate individuals with Hemochromatosis from those of the healthy control group. More specifically, we developed the Q-Net framework, which is a data-driven model that processes information on iron deposition in the brain obtained from multi-echo gradient echo imaging data and anatomical information on T1-Weighted images of the brain. We illustrate that the Q-Net framework can assist in differentiating between someone with HH and Healthy control (HC) of the same age, something that is not possible by just visualizing images. The study is performed based on a unique dataset that was collected from 52 subjects with HH and 47 HC. The Q-Net provides a differential diagnosis accuracy of 83.16% and 80.37% in the scan-level and image-level classification, respectively.      
### 47.What is Semantic Communication? A View on Conveying Meaning in the Era of Machine Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2110.00196.pdf)
>  In 1940s, Claude Shannon developed the information theory focusing on quantifying the maximum data rate that can be supported by a communication channel. Guided by this, the main theme of wireless system design up until 5G was the data rate maximization. In his theory, the semantic aspect and meaning of messages were treated as largely irrelevant to communication. The classic theory started to reveal its limitations in the modern era of machine intelligence, consisting of the synergy between IoT and AI. By broadening the scope of the classic framework, in this article we present a view of semantic communication (SemCom) and conveying meaning through the communication systems. We address three communication modalities, human-to-human (H2H), human-to-machine (H2M), and machine-to-machine (M2M) communications. The latter two, the main theme of the article, represent the paradigm shift in communication and computing. H2M SemCom refers to semantic techniques for conveying meanings understandable by both humans and machines so that they can interact. M2M SemCom refers to effectiveness techniques for efficiently connecting machines such that they can effectively execute a specific computation task in a wireless network. The first part of the article introduces SemCom principles including encoding, system architecture, and layer-coupling and end-to-end design approaches. The second part focuses on specific techniques for application areas of H2M (human and AI symbiosis, recommendation, etc.) and M2M SemCom (distributed learning, split inference, etc.) Finally, we discuss the knowledge graphs approach for designing SemCom systems. We believe that this comprehensive introduction will provide a useful guide into the emerging area of SemCom that is expected to play an important role in 6G featuring connected intelligence and integrated sensing, computing, communication, and control.      
### 48.Batch Belief Trees for Motion Planning Under Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2110.00173.pdf)
>  In this work, we develop the Batch Belief Trees (BBT) algorithm for motion planning under motion and sensing uncertainties. The algorithm interleaves between batch sampling, building a graph of nominal trajectories in the state space, and searching over the graph to find belief space motion plans. By searching over the graph, BBT finds sophisticated plans that will visit (and revisit) information-rich regions to reduce uncertainty. One of the key benefits of this algorithm is the modified interplay between exploration and exploitation. Instead of an exhaustive search (exploitation) after one exploration step, the proposed algorithm uses batch samples to explore the state space and also does not require exhaustive search before the next iteration of batch sampling, which adds flexibility. The algorithm finds motion plans that converge to the optimal one as more samples are added to the graph. We test BBT in different planning environments. Our numerical investigation confirms that BBT finds non-trivial motion plans and is faster compared with previous similar methods.      
### 49.Incremental Layer-wise Self-Supervised Learning for Efficient Speech Domain Adaptation On Device  [ :arrow_down: ](https://arxiv.org/pdf/2110.00155.pdf)
>  Streaming end-to-end speech recognition models have been widely applied to mobile devices and show significant improvement in efficiency. These models are typically trained on the server using transcribed speech data. However, the server data distribution can be very different from the data distribution on user devices, which could affect the model performance. There are two main challenges for on device training, limited reliable labels and limited training memory. While self-supervised learning algorithms can mitigate the mismatch between domains using unlabeled data, they are not applicable on mobile devices directly because of the memory constraint. In this paper, we propose an incremental layer-wise self-supervised learning algorithm for efficient speech domain adaptation on mobile devices, in which only one layer is updated at a time. Extensive experimental results demonstrate that the proposed algorithm obtains a Word Error Rate (WER) on the target domain $24.2\%$ better than supervised baseline and costs $89.7\%$ less training memory than the end-to-end self-supervised learning algorithm.      
### 50.Reconfigurable Intelligent Surfaces Based on Single, Group, and Fully Connected Discrete-Value Impedance Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.00077.pdf)
>  Reconfigurable Intelligent Surfaces (RISs) allow to control the propagation environment in wireless networks by properly tuning multiple reflecting elements. Traditionally, RISs have been realized through single connected reconfigurable impedance networks, in which each RIS element is independently controlled by an impedance connected to ground. In a recent work, this architecture has been extended by realizing more efficient RISs with group and fully connected reconfigurable impedance networks. However, impedance networks tunable with arbitrary precision are hard to realize in practice. In this paper, we propose a practical RIS design strategy based on reconfigurable impedance networks with discrete values. Besides, we address the problem of how to group the RIS elements in group connected architectures. We optimize single, group, and fully connected architectures considering finite-resolution elements, and we compare them in terms of received signal power. Through Monte Carlo simulations, supported by theoretical justifications, we show that only a few resolution bits per reconfigurable impedance are sufficient to achieve the performance upper bound. In particular, while four resolution bits are needed to reach the upper bound in single connected architectures, only a single resolution bit is sufficient in fully connected ones, simplifying significantly the future development of these promising RIS architectures.      
### 51.Quantitative Jones matrix imaging using vectorial Fourier ptychography  [ :arrow_down: ](https://arxiv.org/pdf/2110.00076.pdf)
>  This paper presents a microscopic imaging technique that uses variable-angle illumination to recover the complex polarimetric properties of a specimen at high resolution and over a large field-of-view. The approach extends Fourier ptychography, which is a synthetic aperture-based imaging approach to improve resolution with phaseless measurements, to additionally account for the vectorial nature of light. After images are acquired using a standard microscope outfitted with an LED illumination array and two polarizers, our vectorial Fourier Ptychography (vFP) algorithm solves for the complex 2x2 Jones matrix of the anisotropic specimen of interest at each resolved spatial location. We introduce a new sequential Gauss-Newton-based solver that additionally jointly estimates and removes polarization-dependent imaging system aberrations. We demonstrate effective vFP performance by generating large-area (29 mm$^2$), high-resolution (1.24 $\mu$m full-pitch) reconstructions of sample absorption, phase, orientation, diattenuation, and retardance for a variety of calibration samples and biological specimens.      
### 52.Velocity-aware Antenna Selection in Predictor Antenna Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.00064.pdf)
>  Moving relay (MR), which is a candidate solution for supporting in-vehicle users, has been investigated in different studies. Due to the mobile nature of the MR, acquiring channel state information at the transmitter side (CSIT) is challenging because of the fast-changing environment around the vehicle. On top of an MR, one can use predictor antenna (PA), i.e., an additional antenna in front of the receive antenna (RA), to obtain CSIT, and recent works have investigated the benefits of such a set up. PA-aided CSIT acquisition normally works with the help of different content information such as the location and the velocity of the MR. In this paper, we study the effect of velocity awareness on the PA system, and develop adaptive antenna selection schemes in PA-assisted MRs. Results show that, compared to no-CSIT schemes, a velocity-aware antenna selection-based PA system can improve the end-to-end throughput by an order of magnitude.      
### 53.Simulation-based multi-criteria comparison of mono-articular and bi-articular exoskeletons during walking with and without load  [ :arrow_down: ](https://arxiv.org/pdf/2110.00062.pdf)
>  Developing exoskeletons that can reduce the metabolic cost of assisted subjects is challenging since a systematic design approach is required to capture the effects of device dynamics and the assistance torques on human performance. Design studies that rely on musculoskeletal models hold high promise in providing effective design guidelines, as the effect of various devices and different assistance torque profiles on metabolic cost can be studied systematically. In this paper, we present a simulation-based multi-criteria design approach to systematically study the effect of different device kinematics and corresponding optimal assistive torque profiles under actuator saturation on the metabolic cost, muscle activation, and joint reaction forces of subjects walking under different loading conditions. For the multi-criteria comparison of exoskeletons, we introduce a Pareto optimization approach to simultaneously optimize the exoskeleton power consumption and the human metabolic rate reduction during walking, under different loading conditions. We further superpose the effects of device inertia and electrical regeneration on the metabolic rate and power consumption, respectively. Our results explain the effects of heavy loads on the optimal assistance profiles of the exoskeletons and provide guidelines on choosing optimal device configurations under actuator torque limitations, device inertia, and regeneration effects. The multi-criteria comparison of devices indicates that despite the similar assistance levels of both devices, mono-articular exoskeletons show better performance on reducing the peak reaction forces, while the power consumption of bi-articular devices is less sensitive to the loading. Furthermore, for the bi-articular exoskeletons, the device inertia has lower detrimental effects on the metabolic cost of subjects and does not affect the Pareto-optimality of solutions.      
### 54.SpliceOut: A Simple and Efficient Audio Augmentation Method  [ :arrow_down: ](https://arxiv.org/pdf/2110.00046.pdf)
>  Time masking has become a de facto augmentation technique for speech and audio tasks, including automatic speech recognition (ASR) and audio classification, most notably as a part of SpecAugment. In this work, we propose SpliceOut, a simple modification to time masking which makes it computationally more efficient. SpliceOut performs comparably to (and sometimes outperforms) SpecAugment on a wide variety of speech and audio tasks, including ASR for seven different languages using varying amounts of training data, as well as on speech translation, sound and music classification, thus establishing itself as a broadly applicable audio augmentation method. SpliceOut also provides additional gains when used in conjunction with other augmentation techniques. Apart from the fully-supervised setting, we also demonstrate that SpliceOut can complement unsupervised representation learning with performance gains in the semi-supervised and self-supervised settings.      
