# ArXiv eess --Wed, 27 Oct 2021
### 1.Real-time division-of-focal-plane polarization imaging system with progressive networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.13823.pdf)
>  Division-of-focal-plane (DoFP) polarization imaging technical recently has been applied in many fields. However, the images captured by such sensors cannot be used directly because they suffer from instantaneous field-of-view errors and low resolution problem. This paper builds a fast DoFP demosaicing system with proposed progressive polarization demosaicing convolutional neural network (PPDN), which is specifically designed for edge-side GPU devices like Navidia Jetson TX2. The proposed network consists of two parts: reconstruction stage and refining stage. The former recovers four polarization channels from a single DoFP image. The latter fine-tune the four channels to obtain more accurate polarization information. PPDN can be implemented in another version: PPDN-L (large), for the platforms of high computing resources. Experiments show that PPDN can compete with the best existing methods with fewer parameters and faster inference speed and meet the real-time demands of imaging system.      
### 2.Real time Simulation of Gird-connected Photovoltaic Multilevel Inverter using Hybrid GA/PSO Optimization Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2110.13817.pdf)
>  This paper presents a new real-time intelligent optimization algorithm to minimize the voltage harmonics of a multilevel photovoltaic inverter. Hybrid Genetic algorithm /Particle swarm optimization algorithm is employed in a real-time simulation to identify the best fire angels of the multilevel inverter to eliminate any destructive effect, such as dc voltage variations and changes in line and dc-link resistors. The dual objective function of harmonic minimization and voltage regulation is considered in this real-time simulation. This approach can be applied to any multilevel inverter with various numbers of levels. The validity of the proposed algorithm is proven by real-time simulation of seven and an eleven-level inverter.      
### 3.Distributed Transmit Beamforming: Design and Demonstration from the Lab to UAVs  [ :arrow_down: ](https://arxiv.org/pdf/2110.13804.pdf)
>  Cooperating radios can extend their communication range by adjusting their signals to ensure coherent combining at a destination radio. This technique is called distributed transmit beamforming. Beamforming (BF) relies on the BF radios having frequency synchronized carriers and phases adjusted for coherent combining. Both requirements are typically met by exchanging preambles with the destination. However, since BF aims to increase the communication range, the individually transmitted preambles are typically at low SNR and their lengths are constrained by the channel coherence time. These noisy preambles lead to errors in frequency and phase estimation, which result in randomly changing BF gains. To build reliable distributed BF systems, the impact of estimation errors on the BF gains need to be considered in the design. In this work, assuming a destination-led BF protocol and Kalman filter for frequency tracking, we optimize the number of BF radios and the preamble lengths to achieve reliable BF gain. To do that, we characterize the relations between the BF gains distribution, the channel coherence time, and design parameters like the SNR, preamble lengths, and the number of radios. The proposed relations are verified using simulations and via experiments using software-defined radios in a lab and on UAVs.      
### 4.Deep DIC: Deep Learning-Based Digital Image Correlation for End-to-End Displacement and Strain Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2110.13720.pdf)
>  Digital image correlation (DIC) has become an industry standard to retrieve accurate displacement and strain measurement in tensile testing and other material characterization. Though traditional DIC offers a high precision estimation of deformation for general tensile testing cases, the prediction becomes unstable at large deformation or when the speckle patterns start to tear. In addition, traditional DIC requires a long computation time and often produces a low spatial resolution output affected by filtering and speckle pattern quality. To address these challenges, we propose a new deep learning-based DIC approach -- Deep DIC, in which two convolutional neural networks, DisplacementNet and StrainNet, are designed to work together for end-to-end prediction of displacements and strains. DisplacementNet predicts the displacement field and adaptively tracks the change of a region of interest. StrainNet predicts the strain field directly from the image input without relying on the displacement prediction, which significantly improves the strain prediction accuracy. A new dataset generation method is proposed to synthesize a realistic and comprehensive dataset including artificial speckle patterns, randomly generated displacement and strain fields, and deformed images based on the given deformation. Proposed Deep DIC is trained purely on a synthetic dataset, but designed to perform both on simulated and experimental data. Its performance is systematically evaluated and compared with commercial DIC software. Deep DIC gives highly consistent and comparable predictions of displacement and strain with those obtained from commercial DIC software, while it outperforms commercial software with very robust strain prediction even with large and localized deformation and varied pattern qualities.      
### 5.A Radar Signal Deinterleaving Method Based on Semantic Segmentation Thought with Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2110.13706.pdf)
>  Radar signal deinterleaving is an important content of electronic reconnaissance. In this paper, a new radar signal deinterleaving method based on semantic segmentation thought is proposed. We select representative sequence modeling neural network architectures, and input the difference of time of arrival (DTOA) of pulse stream to them. According to the semantics contained in different categories of radar signals, each pulse in the pulse stream is marked according to the category of semantics contained, and radar signals are deinterleaved. Compared with the traditional einterleaving method, this method can adapt to any pulse repetition interval (PRI) modulation mode and does not require PRI periodicity. Compared with other deinterleaving methods using neural network, this method does not need to digitize the data and train a network for each type of target. This method also eliminates the need to iterate the input and output of data. The proposed method has high robustness under the condition of pulse loss and noise pulses. The research also shows that recurrent neural network (RNN) still has more advantages than convolutional neural network (CNN) in this sequence modeling problem.      
### 6.A Closer Look at Reference Learning for Fourier Phase Retrieval  [ :arrow_down: ](https://arxiv.org/pdf/2110.13688.pdf)
>  Reconstructing images from their Fourier magnitude measurements is a problem that often arises in different research areas. This process is also referred to as phase retrieval. In this work, we consider a modified version of the phase retrieval problem, which allows for a reference image to be added onto the image before the Fourier magnitudes are measured. We analyze an unrolled Gerchberg-Saxton (GS) algorithm that can be used to learn a good reference image from a dataset. Furthermore, we take a closer look at the learned reference images and propose a simple and efficient heuristic to construct reference images that, in some cases, yields reconstructions of comparable quality as approaches that learn references. Our code is available at <a class="link-external link-https" href="https://github.com/tuelwer/reference-learning" rel="external noopener nofollow">this https URL</a>.      
### 7.Malicious Mode Attack on EV Coordinated Charging Load and MIADRC Defense Strategy  [ :arrow_down: ](https://arxiv.org/pdf/2110.13681.pdf)
>  The Internet of Things (IoT) provides a salient communication environment to facilitate the coordinated charging of electric vehicle (EV) load. However, as IoT is connected with the public network, the coordinated charging system is in a low-level cyber security and greatly vulnerable to malicious attacks. This paper investigates the malicious mode attack (MMA), which is a new cyberattack pattern that simultaneously attacks massive EV charging piles to generate continuous sinusoidal power disturbance with the same frequency as the poorly-damped wide-area electromechanical mode. Thereby, high amplitude forced oscillations could be stimulated by MMA, which seriously threats the power system stability. First, the potential threat of MMA is clarified by investigating the vulnerability of the IoT-based coordinated charging load control system, and an MMA process like Mirai is pointed out as an example. And then, based on the attack process, an MMA model is established for impact analysis where expressions of the mean and stochastic responses of the MMA forced oscillation are derived to discover main impact factors. Further, to mitigate the impact of MMA, a defense strategy based on multi-index information active disturbance rejection control is proposed to improve the stability and anti-disturbance ability of the power system, which considers the impact factors of both mode damping and disturbance compensation. Simulations are conducted to verify the existence and characteristics of MMA threats, and the efficiency of the proposed defense strategy is also validated.      
### 8.W-Net: A Two-Stage Convolutional Network for Nucleus Detection in Histopathology Image  [ :arrow_down: ](https://arxiv.org/pdf/2110.13670.pdf)
>  Pathological diagnosis is the gold standard for cancer diagnosis, but it is labor-intensive, in which tasks such as cell detection, classification, and counting are particularly prominent. A common solution for automating these tasks is using nucleus segmentation technology. However, it is hard to train a robust nucleus segmentation model, due to several challenging problems, the nucleus adhesion, stacking, and excessive fusion with the background. Recently, some researchers proposed a series of automatic nucleus segmentation methods based on point annotation, which can significant improve the model performance. Nevertheless, the point annotation needs to be marked by experienced pathologists. In order to take advantage of segmentation methods based on point annotation, further alleviate the manual workload, and make cancer diagnosis more efficient and accurate, it is necessary to develop an automatic nucleus detection algorithm, which can automatically and efficiently locate the position of the nucleus in the pathological image and extract valuable information for pathologists. In this paper, we propose a W-shaped network for automatic nucleus detection. Different from the traditional U-Net based method, mapping the original pathology image to the target mask directly, our proposed method split the detection task into two sub-tasks. The first sub-task maps the original pathology image to the binary mask, then the binary mask is mapped to the density mask in the second sub-task. After the task is split, the task's difficulty is significantly reduced, and the network's overall performance is improved.      
### 9.A Non-linear Differentiable Model for Stormwater-based Irrigation of a Green Roof in Toronto  [ :arrow_down: ](https://arxiv.org/pdf/2110.13669.pdf)
>  Green infrastructure has potential to alleviate the environmental impact of rapidly growing cities. This potential has inspired laws in Toronto that require the inclusion of rooftops with large vegetation beds, called green roofs, into sufficiently sized construction projects. We study the problem of reusing stormwater to irrigate a green roof in Toronto, where potable water is the current irrigation source. The vision is that widespread reuse of stormwater runoff for irrigation of green roofs and other purposes can reduce sewer overflow volumes without over-building (with the added benefit of conserving potable water). Towards this vision, our goal is to develop and evaluate two pump controllers for transporting stormwater to the green roof of interest in simulation. A key contribution is our development of a site-specific non-linear model for stormwater flow using smoothing techniques that permits linearization and a standard model predictive controller (MPC). We compare the efficacy of the MPC, which anticipates the weather, and an on/off controller, which is reactive rather than anticipative, for the site in simulation. With further study, we are hopeful that this research will advance control systems technology to improve the performance of green and stormwater infrastructure in growing urban areas.      
### 10.Decentralized Thermal Control of Buildings  [ :arrow_down: ](https://arxiv.org/pdf/2110.13654.pdf)
>  Energy requirements for heating and cooling of buildings constitute a major fraction of end use energy consumed. Therefore, it is important to provide the occupant comfort requirements in buildings in an energy efficient manner. However, buildings are large scale complex systems, susceptible to sensor, actuator or communication network failures in their thermal control infrastructure, that can affect their performance in terms of occupant comfort and energy efficiency. The degree of decentralization in the control architecture determines a fundamental tradeoff between performance and robustness. This thesis studies the problem of thermal control of buildings from the perspective of partitioning them into clusters for decentralized control, to balance underlying performance and robustness requirements. Measures of deviation in performance and robustness between centralized and decentralized architectures in the Model Predictive Control framework are derived. Appropriate clustering algorithms are then proposed to determine decentralized control architectures which provide a satisfactory trade-off between the underlying performance and robustness objectives. Two different partitioning methodologies the CLF-MCS method and the OLF-FPM method are developed and compared. The problem of decentralized control design based on the architectures obtained using these methodologies is also considered. It entails the use of decentralized extended state observers to address the issue of unavailability of unknown states and disturbances in the system. The potential use of the proposed control architecture selection and decentralized control design methodologies is demonstrated in simulation on a real world multi-zone building.      
### 11.Learning Speaker Representation with Semi-supervised Learning approach for Speaker Profiling  [ :arrow_down: ](https://arxiv.org/pdf/2110.13653.pdf)
>  Speaker profiling, which aims to estimate speaker characteristics such as age and height, has a wide range of applications inforensics, recommendation systems, etc. In this work, we propose a semisupervised learning approach to mitigate the issue of low training data for speaker profiling. This is done by utilizing external corpus with speaker information to train a better representation which can help to improve the speaker profiling systems. Specifically, besides the standard supervised learning path, the proposed framework has two more paths: (1) an unsupervised speaker representation learning path that helps to capture the speaker information; (2) a consistency training path that helps to improve the robustness of the system by enforcing it to produce similar predictions for utterances of the same speaker.The proposed approach is evaluated on the TIMIT and NISP datasets for age, height, and gender estimation, while the Librispeech is used as the unsupervised external corpus. Trained both on single-task and multi-task settings, our approach was able to achieve state-of-the-art results on age estimation on the TIMIT Test dataset with Root Mean Square Error(RMSE) of6.8 and 7.4 years and Mean Absolute Error(MAE) of 4.8 and5.0 years for male and female speakers respectively.      
### 12.A Precision Diagnostic Framework of Renal Cell Carcinoma on Whole-Slide Images using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.13652.pdf)
>  Diagnostic pathology, which is the basis and gold standard of cancer diagnosis, provides essential information on the prognosis of the disease and vital evidence for clinical treatment. Tumor region detection, subtype and grade classification are the fundamental diagnostic indicators for renal cell carcinoma (RCC) in whole-slide images (WSIs). However, pathological diagnosis is subjective, differences in observation and diagnosis between pathologists is common in hospitals with inadequate diagnostic capacity. The main challenge for developing deep learning based RCC diagnostic system is the lack of large-scale datasets with precise annotations. In this work, we proposed a deep learning-based framework for analyzing histopathological images of patients with renal cell carcinoma, which has the potential to achieve pathologist-level accuracy in diagnosis. A deep convolutional neural network (InceptionV3) was trained on the high-quality annotated dataset of The Cancer Genome Atlas (TCGA) whole-slide histopathological image for accurate tumor area detection, classification of RCC subtypes, and ISUP grades classification of clear cell carcinoma subtypes. These results suggest that our framework can help pathologists in the detection of cancer region and classification of subtypes and grades, which could be applied to any cancer type, providing auxiliary diagnosis and promoting clinical consensus.      
### 13.Distributional Robustness Regularized Scenario Optimization with Application to Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2110.13588.pdf)
>  We provide a functional view of distributional robustness motivated by robust statistics and functional analysis. This results in two practical computational approaches for approximate distributionally robust nonlinear optimization based on gradient norms and reproducing kernel Hilbert spaces. Our method can be applied to the settings of statistical learning with small sample size and test distribution shift. As a case study, we robustify scenario-based stochastic model predictive control with general nonlinear constraints. In particular, we demonstrate constraint satisfaction with only a small number of scenarios under distribution shift.      
### 14.Towards Audio Domain Adaptation for Acoustic Scene Classification using Disentanglement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.13586.pdf)
>  The deployment of machine listening algorithms in real-life applications is often impeded by a domain shift caused for instance by different microphone characteristics. In this paper, we propose a novel domain adaptation strategy based on disentanglement learning. The goal is to disentangle task-specific and domain-specific characteristics in the analyzed audio recordings. In particular, we combine two strategies: First, we apply different binary masks to internal embedding representations and, second, we suggest a novel combination of categorical cross-entropy and variance-based losses. Our results confirm the disentanglement of both tasks on an embedding level but show only minor improvement in the acoustic scene classification performance, when training data from both domains can be used. As a second finding, we can confirm the effectiveness of a state-of-the-art unsupervised domain adaptation strategy, which performs across-domain adaptation on a feature-level instead.      
### 15.Multi-scale Deterministic Optimisation of Blended Composite Structures: Case Study of a Box-Wing  [ :arrow_down: ](https://arxiv.org/pdf/2110.13463.pdf)
>  This work presents a multi-scale design methodology for the deterministic optimisation of thin-walled composite structures integrating a global-local approach for the assessment of the buckling strength and a dedicated strategy to recover blended stacking sequences. The methodology is based on the multi-scale two-level optimisation strategy for anisotropic materials and structures. In the first step, focused on the macroscopic scale, several design requirements are included in the problem formulation: lightness, feasibility, manufacturing, blending, buckling failure, static failure and stiffness. The second step, which focuses on the laminate mesoscopic scale, deals with the recovery of blended stacking sequences, for the structure at hand, matching the optimal geometric and elastic properties determined in the first step. As a case study, the unconventional PrandtlPlane box-wing system is used to show the effectiveness of the proposed design methodology.      
### 16.Modeling of Frequency Security Constraints and Quantification of Frequency Control Reserve Requirements for Unit Commitment  [ :arrow_down: ](https://arxiv.org/pdf/2110.13448.pdf)
>  The high penetration of converter-based renewable energy sources has brought challenges to the power system frequency control. It is essential to consider the frequency security constraints and frequency control reserve requirements in unit commitment (UC). Considering that the risk of frequency insecurity varies under the changeable operational condition, we propose to optimize the PFC droop gains and reserve capacities in the UC model to provide diverse control efforts in different risk levels adaptively. Copula theory is used to establish the joint distribution model among frequency control performance, secondary frequency control (SFC) reserve capacities, and power fluctuations. Then the distributionally robust optimization technique is utilized in the SFC reserve requirement determination to handle the possible error in the probability model. The UC simulation is conducted on IEEE 118-bus system to test the proposed optimal PFC droop gain strategy and SFC reserve requirement quantification method. Simulation results show that the proposed optimal PFC droop gain strategy is better than the traditional fixed PFC droop gain setting on economic efficiency and operational flexibility. Besides, the SFC reserve requirement calculated by the proposed method is more appropriate than the actual SFC reserve capacity in the historical operation.      
### 17.An Analysis of LOS Coverage in Vehicular Networks with Roadside Units and Relays  [ :arrow_down: ](https://arxiv.org/pdf/2110.13436.pdf)
>  This paper analyzes the use of vehicular relays as a means to extend the Line-of-Sight (LOS) coverage from roadside units(RSUs) toward users on the streets in mmWave or visible light communications. In this paper, we consider the scenario where RSUs select vehicles within their LOS coverage as relays. As a result, the LOS coverage of those RSUs is extended by the LOS coverage newly provided by the vehicular relays. To account for the spatial relationship between vehicles and RSUs, we use Cox point processes. We assume that the LOS distances from RSUs or relays are independent and exponentially distributed. To address the spatial interactions between RSU LOS coverage and relay LOS coverage, we use the notion of mean area fraction to evaluate the LOS coverage.      
### 18.Enhanced ELM Based Channel Estimation for RIS-Assisted OFDM systems with Insufficient CP and Imperfect Hardware  [ :arrow_down: ](https://arxiv.org/pdf/2110.13433.pdf)
>  Reconfigurable intelligent surface (RIS)-assisted orthogonal frequency division multiplexing (OFDM) systems have aroused extensive research interests due to the controllable communication environment and the performance of combating multi-path interference. However, as the premise of RIS-assisted OFDM systems, the accuracy of channel estimation is severely degraded by the increased possibility of insufficient cyclic prefix (CP) produced by extra cascaded channels of RIS and the nonlinear distortion lead by imperfect hardware. To address these issues, an enhanced extreme learning machine (ELM)- based channel estimation (eELM-CE) is proposed in this letter to facilitate accurate channel estimation. Based on the model-driven mode, least square (LS) estimation is employed to highlight the initial linear features for channel estimation. Then, according to the obtained initial features, an enhanced ELM network is constructed to refine the channel estimation. In particular, we start from the perspective of guiding it to recognize the feature, and normalize the data after the network activation function to enhance the ability of identifying non-linear factors. Experiment results show that, compared with existing methods, the proposed method achieves a much lower normalized mean square error (NMSE) given insufficient CP and imperfect hardware. In addition, the simulation results indicate that the proposed method possesses robustness against the parameter variations.      
### 19.Deep Learning-based Segmentation of Cerebral Aneurysms in 3D TOF-MRA using Coarse-to-Fine Framework  [ :arrow_down: ](https://arxiv.org/pdf/2110.13432.pdf)
>  BACKGROUND AND PURPOSE: Cerebral aneurysm is one of the most common cerebrovascular diseases, and SAH caused by its rupture has a very high mortality and disability rate. Existing automatic segmentation methods based on DLMs with TOF-MRA modality could not segment edge voxels very well, so that our goal is to realize more accurate segmentation of cerebral aneurysms in 3D TOF-MRA with the help of DLMs. MATERIALS AND METHODS: In this research, we proposed an automatic segmentation framework of cerebral aneurysm in 3D TOF-MRA. The framework was composed of two segmentation networks ranging from coarse to fine. The coarse segmentation network, namely DeepMedic, completed the coarse segmentation of cerebral aneurysms, and the processed results were fed into the fine segmentation network, namely dual-channel SE_3D U-Net trained with weighted loss function, for fine segmentation. Images from ADAM2020 (n=113) were used for training and validation and images from another center (n=45) were used for testing. The segmentation metrics we used include DSC, HD, and VS. RESULTS: The trained cerebral aneurysm segmentation model achieved DSC of 0.75, HD of 1.52, and VS of 0.91 on validation cohort. On the totally independent test cohort, our method achieved the highest DSC of 0.12, the lowest HD of 11.61, and the highest VS of 0.16 in comparison with state-of-the-art segmentation networks. CONCLUSIONS: The coarse-to-fine framework, which composed of DeepMedic and dual-channel SE_3D U-Net can segment cerebral aneurysms in 3D TOF-MRA with a superior accuracy.      
### 20.Meter-Range Wireless Motor Drive for Pipeline Transportation  [ :arrow_down: ](https://arxiv.org/pdf/2110.13431.pdf)
>  This paper proposes and implements a meter-range wireless motor drive (WMD) system for promising applications of underground pipeline transportations or in-pipe robots. To power a pipeline network beneath the earth, both the power grid and the control system are usually required to be deployed deep underground, thus increasing the construction cost, maintenance difficulty and system complexity. The proposed system newly develops a hybrid repeater to enable the desired meter-range wireless power and drive transfer, which can offer a fault-tolerant network with a robust structure for the underground sensor-free WMD while maintaining a high transmission efficiency. Hence, this wireless pipeline network can reduce the maintenance requirement and regulate the flow rate effectively. A full-scale prototype has been built for practical verification, and the system efficiency can reach 88.8% at a long transfer distance of 150 cm. Theoretical analysis, software simulation and hardware experimentation are given to verify the feasibility of proposed meter-range WMD for underground pipeline transportations.      
### 21.Image Magnification Network for Vessel Segmentation in OCTA Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.13428.pdf)
>  Optical coherence tomography angiography (OCTA) is a novel non-invasive imaging modality that allows micron-level resolution to visualize the retinal microvasculature. The retinal vessel segmentation in OCTA images is still an open problem, and especially the thin and dense structure of the capillary plexus is an important challenge of this problem. In this work, we propose a novel image magnification network (IMN) for vessel segmentation in OCTA images. Contrary to the U-Net structure with a down-sampling encoder and up-sampling decoder, the proposed IMN adopts the design of up-sampling encoding and then down-sampling decoding. This design is to capture more image details and reduce the omission of thin-and-small structures. The experimental results on three open OCTA datasets show that the proposed IMN with an average dice score of 90.2% achieves the best performance in vessel segmentation of OCTA images. Besides, we also demonstrate the superior performance of IMN in cross-field image vessel segmentation and vessel skeleton extraction.      
### 22.An Automatic Detection Method Of Cerebral Aneurysms In Time-Of-Flight Magnetic Resonance Angiography Images Based On Attention 3D U-Net  [ :arrow_down: ](https://arxiv.org/pdf/2110.13367.pdf)
>  Background:Subarachnoid hemorrhage caused by ruptured cerebral aneurysm often leads to fatal consequences.However,if the aneurysm can be found and treated during asymptomatic periods,the probability of rupture can be greatly <a class="link-external link-http" href="http://reduced.At" rel="external noopener nofollow">this http URL</a> present,time-of-flight magnetic resonance angiography is one of the most commonly used non-invasive screening techniques for cerebral aneurysm,and the application of deep learning technology in aneurysm detection can effectively improve the screening effect of aneurysm.Existing studies have found that three-dimensional features play an important role in aneurysm detection,but they require a large amount of training data and have problems such as a high false positive rate. Methods:This paper proposed a novel method for aneurysm detection.First,a fully automatic cerebral artery segmentation algorithm without training data was used to extract the volume of interest,and then the 3D U-Net was improved by the 3D SENet module to establish an aneurysm detection model.Eventually a set of fully automated,end-to-end aneurysm detection methods have been formed. Results:A total of 231 magnetic resonance angiography image data were used in this study,among which 132 were training sets,34 were internal test sets and 65 were external test sets.The presented method obtained 97.89% sensitivity in the five-fold cross-validation and obtained 91.0% sensitivity with 2.48 false positives/case in the detection of the external test sets. Conclusions:Compared with the results of our previous studies and other studies,the method in this paper achieves a very competitive sensitivity with less training data and maintains a low false positive <a class="link-external link-http" href="http://rate.As" rel="external noopener nofollow">this http URL</a> the only method currently using 3D U-Net for aneurysm detection,it proves the feasibility and superior performance of this network in aneurysm detection,and also explores the potential of the channel attention mechanism in this task.      
### 23.Newtonian Mechanics Based Transient Stability PART IV: Equivalent Machine  [ :arrow_down: ](https://arxiv.org/pdf/2110.13366.pdf)
>  This paper analyzes the mechanisms of the equivalent machine and also its advantages in TSA. Based on the two group separations, an equivalent machine is modeled through the equivalence of the motions of all machines inside each group. This "motion equivalence" fully ensures the modeling of the two-machine system and the corresponding Newtonian energy conversion. Against this background, the original system becomes the equivalent system. It is clarified that the equivalent machine strictly follows the machine paradigms. These strict followings bring the two advantages in the equivalent-machine based TSA: (i) the stability of the equivalent machine is characterized precisely, and (ii) the equivalent-machine trajectory variance is depicted clearly. The two advantages are fully reflected in the precise definitions of the equivalent-machine based transient stability concepts. In particular, the equivalent machine swing is clearly depicted through the EDSP or EDLP of the machine, and the critical stability of the equivalent system is strictly defined as the critical stability of the equivalent machine. Simulation results show that the effectiveness of the equivalent-machine in TSA.      
### 24.Event-triggered Consensus of Matrix-weighted Networks Subject to Actuator Saturation  [ :arrow_down: ](https://arxiv.org/pdf/2110.13356.pdf)
>  The ubiquitous interdependencies among higher-dimensional states of neighboring agents can be characterized by matrix-weighted networks. This paper examines event-triggered global consensus of matrix-weighted networks subject to actuator saturation. Specifically, a distributed dynamic event-triggered coordination strategy, whose design involves sampled state of agents, saturation constraint and auxiliary systems, is proposed for this category of generalized network to guarantee its global consensus. Under the proposed event-triggered coordination strategy, sufficient conditions are derived to guarantee the leaderless and leader-follower global consensus of the multi-agent systems on matrix-weighted networks, respectively. The Zeno phenomenon can be excluded for both cases under the proposed coordination strategy. It turns out that the spectral properties of matrix-valued weights are crucial in event-triggered mechanism design for matrix-weighted networks with actuator saturation constraint. Finally, simulations are provided to demonstrate the effectiveness of proposed event-triggered coordination strategy. This work provides a more general design framework compared with existing results that are only applicable to scalar-weighted networks.      
### 25.High-Order Signed Distance Transform of Sampled Signals  [ :arrow_down: ](https://arxiv.org/pdf/2110.13354.pdf)
>  Signed distance transforms of sampled signals can be constructed better than the traditional exact signed distance transform. Such a transform is termed the high-order signed distance transform and is defined as satisfying three conditions: the Eikonal equation, recovery by a Heaviside function, and has an order of accuracy greater than unity away from the medial axis. Such a transform is an improvement to the classic notion of an exact signed distance transform because it does not exhibit artifacts of quantization. A large constant, linear time complexity high-order signed distance transform for arbitrary dimensionality sampled signals is developed based on the high order fast sweeping method. The transform is initialized with an exact signed distance transform and quantization corrected through an upwind solver for the boundary value Eikonal equation. The proposed method cannot attain arbitrary order of accuracy and is limited by the initialization method and non-uniqueness of the problem. However, meshed surfaces are visually smoother and do not exhibit artifacts of quantization in local mean and Gaussian curvature.      
### 26.Cell Zooming with Masked Data for Off-Grid Small Cell Networks: Distributed Optimization Approach  [ :arrow_down: ](https://arxiv.org/pdf/2110.13349.pdf)
>  Cell zooming has been becoming an essential enabler for off-grid small cell networks. Traditional models often utilize the numbers of active users in order to determine cell zooming strategies. However, such confidential measurement data must be concealed from others. We therefore propose a novel cell zooming method with masking noise. The proposed algorithm is designed based on distributed optimization, in which each SBS locally solves a divided optimization problem and learns how much a global constraint is satisfied or violated for temporal solutions. The important feature of this distributed control method is robustness against masking noise. We analyze the trade-off between confidentiality and optimization accuracy, using the notion of differential privacy. Numerical simulations show that the proposed distributed control method outperforms a standard centralized control method in the presence of masking noise.      
### 27.Gaussian Mixture Model Based Distributionally Robust Optimal Power Flow With CVaR Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2110.13336.pdf)
>  This paper proposes a distributionally robust optimal power flow (OPF) model for transmission grids with wind power generation. The model uses the conditional value-at-risk (CVaR) constraints to control the reserve and branch flow limit violations caused by wind power forecast errors. Meanwhile, the Gaussian mixture model (GMM) is integrated into the CVaR constraints to guard against the non-Gaussian forecast error distributions. Unlike the previous studies considering the GMM with fixed parameters, this paper allows the GMM parameters to be variable within some credible regions and develops a data-driven GMM-based ambiguity set to achieve the distributional robustness. Also, rather than using the traditional sample-based approximation of CVaR with high computational burden, this paper designs a scalable cutting-plane algorithm to handle the distributionally robust CVaR constraints. Case studies on the IEEE 2736-bus system show the effectiveness and scalability of the proposed OPF model.      
### 28.IRS-Aided Radar: Enhanced Target Parameter Estimation via Intelligent Reflecting Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2110.13251.pdf)
>  The intelligent reflecting surface (IRS) technology has recently attracted a lot of interest in wireless communications research. An IRS consists of passive reflective elements capable of tuning the phase, amplitude, frequency and polarization of the impinging waveforms. Given such desirable properties, the wireless channel characteristics can be controlled and optimized for specific signal design and processing needs -- thus promising significant potential in radar applications. In this paper, we establish the theoretical foundations for introducing IRS into a radar system and study the potential to improve target parameter estimation. More specifically, we will investigate the deployment of IRS in cases where the line-of-sight (LOS) link is weak or blocked by obstructions. We demonstrate that the IRS can provide a virtual or non-line-of-sight (NLOS) link between the radar and target leading to an enhanced radar performance. The effectiveness of such an IRS-provided virtual link in estimating the moving target parameters is illustrated under both optimized and non-optimized IRS scenarios. Numerical simulations indicate that the IRS can enhance the target parameter estimation when the LOS link is weaker than $\sim 10^{-1}$ in relative strength in comparison with the NLOS link.      
### 29.Adaptive maximum power point tracking using neural networks for a photovoltaic systems according grid  [ :arrow_down: ](https://arxiv.org/pdf/2110.13246.pdf)
>  Introduction. This article deals with the optimization of the energy conversion of a grid-connected photovoltaic system. The novelty is to develop an intelligent maximum power point tracking technique using artificial neural network algorithms. Purpose. Intelligent maximum power point tracking technique is developed in order to improve the photovoltaic system performances under the variations of the temperature and irradiation. Methods. This work is to calculate and follow the maximum power point for a photovoltaic system operating according to the artificial intelligence mechanism is and the latter is used an adaptive modified perturbation and observation maximum power point tracking algorithm based on function sign to generate an specify duty cycle applied to DC-DC converter, where we use the feed forward artificial neural network type trained by Levenberg-Marquardt backpropagation. Results. The photovoltaic system that we chose to simulate and apply this intelligent technique on it is a stand-alone photovoltaic system. According to the results obtained from simulation of the photovoltaic system using adaptive modified perturbation and observation artificial neural network the efficiency and the quality of the production of energy from photovoltaic is increased. Practical value. The proposed algorithm is validated by a dSPACE DS1104 for different operating conditions. All practice results confirm the effectiveness of our proposed algorithm.      
### 30.Homography-based Visual Servoing with Remote Center of Motion for Semi-autonomous Robotic Endoscope Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2110.13245.pdf)
>  The dominant visual servoing approaches in Minimally Invasive Surgery (MIS) follow single points or adapt the endoscope's field of view based on the surgical tools' distance. These methods rely on point positions with respect to the camera frame to infer a control policy. Deviating from the dominant methods, we formulate a robotic controller that allows for image-based visual servoing that requires neither explicit tool and camera positions nor any explicit image depth information. The proposed method relies on homography-based image registration, which changes the automation paradigm from point-centric towards surgical-scene-centric approach. It simultaneously respects a programmable Remote Center of Motion (RCM). Our approach allows a surgeon to build a graph of desired views, from which, once built, views can be manually selected and automatically servoed to irrespective of robot-patient frame transformation changes. We evaluate our method on an abdominal phantom and provide an open source ROS Moveit integration for use with any serial manipulator.      
### 31.RBSRICNN: Raw Burst Super-Resolution through Iterative Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2110.13217.pdf)
>  Modern digital cameras and smartphones mostly rely on image signal processing (ISP) pipelines to produce realistic colored RGB images. However, compared to DSLR cameras, low-quality images are usually obtained in many portable mobile devices with compact camera sensors due to their physical limitations. The low-quality images have multiple degradations i.e., sub-pixel shift due to camera motion, mosaick patterns due to camera color filter array, low-resolution due to smaller camera sensors, and the rest information are corrupted by the noise. Such degradations limit the performance of current Single Image Super-resolution (SISR) methods in recovering high-resolution (HR) image details from a single low-resolution (LR) image. In this work, we propose a Raw Burst Super-Resolution Iterative Convolutional Neural Network (RBSRICNN) that follows the burst photography pipeline as a whole by a forward (physical) model. The proposed Burst SR scheme solves the problem with classical image regularization, convex optimization, and deep learning techniques, compared to existing black-box data-driven methods. The proposed network produces the final output by an iterative refinement of the intermediate SR estimates. We demonstrate the effectiveness of our proposed approach in quantitative and qualitative experiments that generalize robustly to real LR burst inputs with onl synthetic burst data available for training.      
### 32.Light-Field Microscopy for optical imaging of neuronal activity: when model-based methods meet data-driven approaches  [ :arrow_down: ](https://arxiv.org/pdf/2110.13142.pdf)
>  Understanding how networks of neurons process information is one of the key challenges in modern neuroscience. A necessary step to achieve this goal is to be able to observe the dynamics of large populations of neurons over a large area of the brain. Light-field microscopy (LFM), a type of scanless microscope, is a particularly attractive candidate for high-speed three-dimensional (3D) imaging. It captures volumetric information in a single snapshot, allowing volumetric imaging at video frame-rates. Specific features of imaging neuronal activity using LFM call for the development of novel machine learning approaches that fully exploit priors embedded in physics and optics models. Signal processing theory and wave-optics theory could play a key role in filling this gap, and contribute to novel computational methods with enhanced interpretability and generalization by integrating model-driven and data-driven approaches. This paper is devoted to a comprehensive survey to state-of-the-art of computational methods for LFM, with a focus on model-based and data-driven approaches.      
### 33.NeRV: Neural Representations for Videos  [ :arrow_down: ](https://arxiv.org/pdf/2110.13903.pdf)
>  We propose a novel neural representation for videos (NeRV) which encodes videos in neural networks. Unlike conventional representations that treat videos as frame sequences, we represent videos as neural networks taking frame index as input. Given a frame index, NeRV outputs the corresponding RGB image. Video encoding in NeRV is simply fitting a neural network to video frames and decoding process is a simple feedforward operation. As an image-wise implicit representation, NeRV output the whole image and shows great efficiency compared to pixel-wise implicit representation, improving the encoding speed by 25x to 70x, the decoding speed by 38x to 132x, while achieving better video quality. With such a representation, we can treat videos as neural networks, simplifying several video-related tasks. For example, conventional video compression methods are restricted by a long and complex pipeline, specifically designed for the task. In contrast, with NeRV, we can use any neural network compression method as a proxy for video compression, and achieve comparable performance to traditional frame-based video compression approaches (H.264, HEVC \etc). Besides compression, we demonstrate the generalization of NeRV for video denoising. The source code and pre-trained model can be found at <a class="link-external link-https" href="https://github.com/haochen-rye/NeRV.git" rel="external noopener nofollow">this https URL</a>.      
### 34.WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing  [ :arrow_down: ](https://arxiv.org/pdf/2110.13900.pdf)
>  Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. In this paper, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM is built based on the HuBERT framework, with an emphasis on both spoken content modeling and speaker identity preservation. We first equip the Transformer structure with gated relative position bias to improve its capability on recognition tasks. For better speaker discrimination, we propose an utterance mixing training strategy, where additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pretrained models are available at <a class="link-external link-https" href="https://aka.ms/wavlm" rel="external noopener nofollow">this https URL</a>.      
### 35.Overcoming Pedestrian Blockage in mm-Wave Bands using Ground Reflections  [ :arrow_down: ](https://arxiv.org/pdf/2110.13884.pdf)
>  mm-Wave communication employs directional beams to overcome high path loss. High data rate communication is typically along line-of-sight (LoS). In outdoor environments, such communication is susceptible to temporary blockage by pedestrians interposed between the transmitter and receiver. It results in outages in which the user is lost, and has to be reacquired as a new user, severely disrupting interactive and high throughput applications. It has been presumed that the solution is to have a densely deployed set of base stations that will allow the mobile to perform a handover to a different non-blocked base station every time a current base station is blocked. This is however a very costly solution for outdoor environments. Through extensive experiments we show that it is possible to exploit a strong ground reflection with a received signal strength (RSS) about 4dB less than the LoS path in outdoor built environments with concrete or gravel surfaces, for beams that are narrow in azimuth but wide in zenith. While such reflected paths cannot support the high data rates of LoS paths, they can support control channel communication, and, importantly, sustain time synchronization between the mobile and the base station. This allows a mobile to quickly recover to the LoS path upon the cessation of the temporary blockage, which typically lasts a few hundred milliseconds. We present a simple in-band protocol that quickly discovers ground reflected radiation and uses it to recover the LoS link when the temporary blockage disappears.      
### 36.Assessing Evaluation Metrics for Speech-to-Speech Translation  [ :arrow_down: ](https://arxiv.org/pdf/2110.13877.pdf)
>  Speech-to-speech translation combines machine translation with speech synthesis, introducing evaluation challenges not present in either task alone. How to automatically evaluate speech-to-speech translation is an open question which has not previously been explored. Translating to speech rather than to text is often motivated by unwritten languages or languages without standardized orthographies. However, we show that the previously used automatic metric for this task is best equipped for standardized high-resource languages only. In this work, we first evaluate current metrics for speech-to-speech translation, and second assess how translation to dialectal variants rather than to standardized languages impacts various evaluation methods.      
### 37.Synchronous-Clock Range-Angle Relative Acoustic Navigation: A Unified Approach to Multi-AUV Localization, Command, Control and Coordination  [ :arrow_down: ](https://arxiv.org/pdf/2110.13825.pdf)
>  This paper presents a scalable acoustic navigation approach for the unified command, control and coordination of multiple autonomous underwater vehicles (AUVs). Existing multi-AUV operations typically achieve coordination manually, by programming individual vehicles on the surface via radio communications, which becomes impractical with large vehicle numbers; or they require bi-directional inter-vehicle acoustic communications to achieve limited coordination when submerged, with limited scalability due to the physical properties of the acoustic channel. Our approach utilizes a single, periodically-broadcasting beacon acting as a navigation reference for the group of AUVs, each of which carries a chip-scale atomic clock (CSAC) and fixed ultra-short baseline (USBL) array of acoustic receivers. One-way travel-time (OWTT) from synchronized clocks and time-delays between signals received by each array element allows any number of vehicles within receive distance to determine range, angle, and thus determine their relative position to the beacon. The operator can command different vehicle behaviors by selecting between broadcast signals from a predetermined set, while coordination between AUVs is achieved without inter-vehicle communication, by defining individual vehicle behaviors within the context of the group. Vehicle behaviors are designed within a beacon-centric moving frame of reference, allowing the operator to control the absolute position of the AUV group by re-positioning the navigation beacon to survey the area of interest. Multiple deployments with a fleet of three miniature, low-cost SandShark AUVs performing closed-loop acoustic navigation in real-time provide experimental results validated against a secondary long-baseline (LBL) positioning system, demonstrating the capabilities and robustness of our approach with real-world data.      
### 38.CloudFindr: A Deep Learning Cloud Artifact Masker for Satellite DEM Data  [ :arrow_down: ](https://arxiv.org/pdf/2110.13819.pdf)
>  Artifact removal is an integral component of cinematic scientific visualization, and is especially challenging with big datasets in which artifacts are difficult to define. In this paper, we describe a method for creating cloud artifact masks which can be used to remove artifacts from satellite imagery using a combination of traditional image processing together with deep learning based on U-Net. Compared to previous methods, our approach does not require multi-channel spectral imagery but performs successfully on single-channel Digital Elevation Models (DEMs). DEMs are a representation of the topography of the Earth and have a variety applications including planetary science, geology, flood modeling, and city planning.      
### 39.DPCOVID: Privacy-Preserving Federated Covid-19 Detection  [ :arrow_down: ](https://arxiv.org/pdf/2110.13760.pdf)
>  Coronavirus (COVID-19) has shown an unprecedented global crisis by the detrimental effect on the global economy and health. The number of COVID-19 cases has been rapidly increasing, and there is no sign of stopping. It leads to a severe shortage of test kits and accurate detection models. A recent study demonstrated that the chest X-ray radiography outperformed laboratory testing in COVID-19 detection. Therefore, using chest X-ray radiography analysis can help to screen suspected COVID-19 cases at an early stage. Moreover, the patient data is sensitive, and it must be protected to avoid revealing through model updates and reconstruction from the malicious attacker. In this paper, we present a privacy-preserving Federated Learning system for COVID-19 detection based on chest X-ray images. First, a Federated Learning system is constructed from chest X-ray images. The main idea is to build a decentralized model across multiple hospitals without sharing data among hospitals. Second, we first show that the accuracy of Federated Learning for COVID-19 identification reduces significantly for Non-IID data. We then propose a strategy to improve model's accuracy on Non-IID COVID-19 data by increasing the total number of clients, parallelism (client fraction), and computation per client. Finally, we apply a Differential Privacy Stochastic Gradient Descent (DP-SGD) to enhance the preserving of patient data privacy for our Federated Learning model. A strategy is also proposed to keep the robustness of Federated Learning to ensure the security and accuracy of the model.      
### 40.Learning to Pre-process Laser Induced Breakdown Spectroscopy Signals Without Clean Data  [ :arrow_down: ](https://arxiv.org/pdf/2110.13748.pdf)
>  This work tests whether deep neural networks can clean laser induced breakdown spectroscopy (LIBS) signals by using only uncleaned raw measurements. Our view of this problem considers a disentanglement of the effects of the target of interest from those of the nuisance factors (with non-zero mean) by leveraging the vast amounts of redundancies in LIBS data and our proposed learning formulation. This later aims at promoting consistency between repeated measurement views of a target while simultaneously removing consistencies with all other LIBS measurements taken throughout the history of the instrument. Evaluations on real data from the ChemCam instrument onboard the Martian Curiosity rover show a superior performance in cleaning LIBS signals compared to the standard approaches being used by the ChemCam team.      
### 41.Contrastive Neural Processes for Self-Supervised Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.13623.pdf)
>  Recent contrastive methods show significant improvement in self-supervised learning in several domains. In particular, contrastive methods are most effective where data augmentation can be easily constructed e.g. in computer vision. However, they are less successful in domains without established data transformations such as time series data. In this paper, we propose a novel self-supervised learning framework that combines contrastive learning with neural processes. It relies on recent advances in neural processes to perform time series forecasting. This allows to generate augmented versions of data by employing a set of various sampling functions and, hence, avoid manually designed augmentations. We extend conventional neural processes and propose a new contrastive loss to learn times series representations in a self-supervised setup. Therefore, unlike previous self-supervised methods, our augmentation pipeline is task-agnostic, enabling our method to perform well across various applications. In particular, a ResNet with a linear classifier trained using our approach is able to outperform state-of-the-art techniques across industrial, medical and audio datasets improving accuracy over 10% in ECG periodic data. We further demonstrate that our self-supervised representations are more efficient in the latent space, improving multiple clustering indexes and that fine-tuning our method on 10% of labels achieves results competitive to fully-supervised learning.      
### 42.AQP: An Open Modular Python Platform for Objective Speech and Audio Quality Metrics  [ :arrow_down: ](https://arxiv.org/pdf/2110.13589.pdf)
>  Audio quality assessment has been widely researched in the signal processing area. Full-reference objective metrics (e.g., POLQA, ViSQOL) have been developed to estimate the audio quality relying only on human rating experiments. To evaluate the audio quality of novel audio processing techniques, researchers constantly need to compare objective quality metrics. Testing different implementations of the same metric and evaluating new datasets are fundamental and ongoing iterative activities. In this paper, we present AQP - an open-source, node-based, light-weight Python pipeline for audio quality assessment. AQP allows researchers to test and compare objective quality metrics helping to improve robustness, reproducibility and development speed. We introduce the platform, explain the motivations, and illustrate with examples how, using AQP, objective quality metrics can be (i) compared and benchmarked; (ii) prototyped and adapted in a modular fashion; (iii) visualised and checked for errors. The code has been shared on GitHub to encourage adoption and contributions from the community.      
### 43.Concepts for Automated Machine Learning in Smart Grid Applications  [ :arrow_down: ](https://arxiv.org/pdf/2110.13585.pdf)
>  Undoubtedly, the increase of available data and competitive machine learning algorithms has boosted the popularity of data-driven modeling in energy systems. Applications are forecasts for renewable energy generation and energy consumption. Forecasts are elementary for sector coupling, where energy-consuming sectors are interconnected with the power-generating sector to address electricity storage challenges by adding flexibility to the power system. However, the large-scale application of machine learning methods in energy systems is impaired by the need for expert knowledge, which covers machine learning expertise and a profound understanding of the application's process. The process knowledge is required for the problem formalization, as well as the model validation and application. The machine learning skills include the processing steps of i) data pre-processing, ii) feature engineering, extraction, and selection, iii) algorithm selection, iv) hyperparameter optimization, and possibly v) post-processing of the model's output. Tailoring a model for a particular application requires selecting the data, designing various candidate models and organizing the data flow between the processing steps, selecting the most suitable model, and monitoring the model during operation - an iterative and time-consuming procedure. Automated design and operation of machine learning aim to reduce the human effort to address the increasing demand for data-driven models. We define five levels of automation for forecasting in alignment with the SAE standard for autonomous vehicles, where manual design and application reflect Automation level 0.      
### 44.Coherent False Seizure Prediction in Epilepsy, Coincidence or Providence?  [ :arrow_down: ](https://arxiv.org/pdf/2110.13550.pdf)
>  Seizure forecasting using machine learning is possible, but the performance is far from ideal, as indicated by many false predictions and low specificity. Here, we examine false and missing alarms of two algorithms on long-term datasets to show that the limitations are less related to classifiers or features, but rather to intrinsic changes in the data. We evaluated two algorithms on three datasets by computing the correlation of false predictions and estimating the information transfer between both classification methods. For 9 out of 12 individuals both methods showed a performance better than chance. For all individuals we observed a positive correlation in predictions. For individuals with strong correlation in false predictions we were able to boost the performance of one method by excluding test samples based on the results of the second method. Substantially different algorithms exhibit a highly consistent performance and a strong coherency in false and missing alarms. Hence, changing the underlying hypothesis of a preictal state of fixed time length prior to each seizure to a proictal state is more helpful than further optimizing classifiers. The outcome is significant for the evaluation of seizure prediction algorithms on continuous data.      
### 45.Software Implementation of the Krylov Methods Based Reconstruction for the 3D Cone Beam CT Operator  [ :arrow_down: ](https://arxiv.org/pdf/2110.13526.pdf)
>  Krylov subspace methods are considered a standard tool to solve large systems of linear algebraic equations in many scientific disciplines such as image restoration or solving partial differential equations in mechanics of continuum. In the context of computer tomography however, the mostly used algebraic reconstruction techniques are based on classical iterative schemes. In this work we present software package that implements fully 3D cone beam projection operator and uses Krylov subspace methods, namely CGLS and LSQR to solve related tomographic reconstruction problems. It also implements basic preconditioning strategies. On the example of the cone beam CT reconstruction of 3D Shepp-Logan phantom we show that the speed of convergence of the CGLS clearly outperforms PSIRT algorithm. Therefore Krylov subspace methods present an interesting option for the reconstruction of large 3D cone beam CT problems.      
### 46.Tensor Network Kalman Filtering for Large-Scale LS-SVMs  [ :arrow_down: ](https://arxiv.org/pdf/2110.13501.pdf)
>  Least squares support vector machines are a commonly used supervised learning method for nonlinear regression and classification. They can be implemented in either their primal or dual form. The latter requires solving a linear system, which can be advantageous as an explicit mapping of the data to a possibly infinite-dimensional feature space is avoided. However, for large-scale applications, current low-rank approximation methods can perform inadequately. For example, current methods are probabilistic due to their sampling procedures, and/or suffer from a poor trade-off between the ranks and approximation power. In this paper, a recursive Bayesian filtering framework based on tensor networks and the Kalman filter is presented to alleviate the demanding memory and computational complexities associated with solving large-scale dual problems. The proposed method is iterative, does not require explicit storage of the kernel matrix, and allows the formulation of early stopping conditions. Additionally, the framework yields confidence estimates of obtained models, unlike alternative methods. The performance is tested on two regression and three classification experiments, and compared to the Nyström and fixed size LS-SVM methods. Results show that our method can achieve high performance and is particularly useful when alternative methods are computationally infeasible due to a slowly decaying kernel matrix spectrum.      
### 47.TUNet: A Block-online Bandwidth Extension Model based on Transformers and Self-supervised Pretraining  [ :arrow_down: ](https://arxiv.org/pdf/2110.13492.pdf)
>  We introduce a block-online variant of the temporal feature-wise linear modulation (TFiLM) model to achieve bandwidth extension. The proposed architecture simplifies the UNet backbone of the TFiLM to reduce inference time and employs an efficient transformer at the bottleneck to alleviate performance degradation. We also utilize self-supervised pretraining and data augmentation to enhance the quality of bandwidth extended signals and reduce the sensitivity with respect to downsampling methods. Experiment results on the VCTK dataset show that the proposed method outperforms several recent baselines in terms of spectral distance and source-to-distortion ratio. Pretraining and filter augmentation also help stabilize and enhance the overall performance.      
### 48.Enhanced User Grouping and Pairing Schemes for CoMP NOMA based Cellular Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.13468.pdf)
>  Non-orthogonal multiple access (NOMA) has been identified as one of the promising technologies to enhance the spectral efficiency and throughput for the 5G and beyond cellular networks. Alternatively, coordinated multi-point (CoMP) improves the cell edge users coverage. Thus, CoMP and NOMA can be used together to improve the overall coverage and throughput of the cell edge users. However, user grouping and pairing for CoMP-NOMA based cellular networks has not been suitably addressed in the existing literature. Motivated by this, we propose two user grouping and pairing schemes for a CoMP-NOMA based system. Both the schemes are compared in terms of overall throughput and coverage. Numerical results are presented for various densities of users, base stations, and CoMP thresholds. Moreover, the results are compared with the purely OMA-based benchmark system, NOMA only, and CoMP only systems. We show through simulation results that the proposed schemes offer a trade-off between throughput and coverage as compared to a purely NOMA or CoMP based system.      
### 49.CS-Rep: Making Speaker Verification Networks Embracing Re-parameterization  [ :arrow_down: ](https://arxiv.org/pdf/2110.13465.pdf)
>  Automatic speaker verification (ASV) systems, which determine whether two speeches are from the same speaker, mainly focus on verification accuracy while ignoring inference speed. However, in real applications, both inference speed and verification accuracy are essential. This study proposes cross-sequential re-parameterization (CS-Rep), a novel topology re-parameterization strategy for multi-type networks, to increase the inference speed and verification accuracy of models. CS-Rep solves the problem that existing re-parameterization methods are unsuitable for typical ASV backbones. When a model applies CS-Rep, the training-period network utilizes a multi-branch topology to capture speaker information, whereas the inference-period model converts to a time-delay neural network (TDNN)-like plain backbone with stacked TDNN layers to achieve the fast inference speed. Based on CS-Rep, an improved TDNN with friendly test and deployment called Rep-TDNN is proposed. Compared with the state-of-the-art model ECAPA-TDNN, which is highly recognized in the industry, Rep-TDNN increases the actual inference speed by about 50% and reduces the EER by 10%. The code will be released.      
### 50.Distributed Multi-Agent Deep Reinforcement Learning Framework for Whole-building HVAC Control  [ :arrow_down: ](https://arxiv.org/pdf/2110.13450.pdf)
>  It is estimated that about 40%-50% of total electricity consumption in commercial buildings can be attributed to Heating, Ventilation, and Air Conditioning (HVAC) systems. Minimizing the energy cost while considering the thermal comfort of the occupants is very challenging due to unknown and complex relationships between various HVAC controls and thermal dynamics inside a building. To this end, we present a multi-agent, distributed deep reinforcement learning (DRL) framework based on Energy Plus simulation environment for optimizing HVAC in commercial buildings. This framework learns the complex thermal dynamics in the building and takes advantage of the differential effect of cooling and heating systems in the building to reduce energy costs, while maintaining the thermal comfort of the occupants. With adaptive penalty, the RL algorithm can be prioritized for energy savings or maintaining thermal comfort. Using DRL, we achieve more than 75\% savings in energy consumption. The distributed DRL framework can be scaled to multiple GPUs and CPUs of heterogeneous types.      
### 51.Novel Binary Addition Tree Algorithm (BAT) for Calculating the Direct Lower-Bound of the Highly Reliable Binary-State Network Reliability  [ :arrow_down: ](https://arxiv.org/pdf/2110.13390.pdf)
>  Real-world applications such as the internet of things, wireless sensor networks, smart grids, transportation networks, communication networks, social networks, and computer grid systems are typically modeled as network structures. Network reliability represents the success probability of a network and it is an effective and popular metric for evaluating the performance of all types of networks. Binary-state networks composed of binary-state (e.g., working or failed) components (arcs and/or nodes) are some of the most popular network structures. The scale of networks has grown dramatically in recent years. For example, social networks have more than a billion users. Additionally, the reliability of components has increased as a result of both mature and emergent technology. For highly reliable networks, it is more practical to calculate approximated reliability, rather than exact reliability, which is an NP-hard problem. Therefore, we propose a novel direct reliability lower bound based on the binary addition tree algorithm to calculate approximate reliability. The efficiency and effectiveness of the proposed reliability bound are analyzed based on time complexity and validated through numerical experiments.      
### 52.Deep Learning Tools for Audacity: Helping Researchers Expand the Artist's Toolkit  [ :arrow_down: ](https://arxiv.org/pdf/2110.13323.pdf)
>  We present a software framework that integrates neural networks into the popular open-source audio editing software, Audacity, with a minimal amount of developer effort. In this paper, we showcase some example use cases for both end-users and neural network developers. We hope that this work fosters a new level of interactivity between deep learning practitioners and end-users.      
### 53.Self-aware Social Learning over Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2110.13292.pdf)
>  In this paper we study the problem of social learning under multiple true hypotheses and self-interested agents which exchange information over a graph. In this setup, each agent receives data that might be generated from a different hypothesis (or state) than the data other agents receive. In contrast to the related literature in social learning, which focuses on showing that the network achieves consensus, here we study the case where every agent is self-interested and wants to find the hypothesis that generates its own observations. However, agents do not know which ones of their peers wants to find the same state with them and as a result they do not know which agents they should cooperate with. To this end, we propose a scheme with adaptive combination weights and study the consistency of the agents' learning process. The scheme allows each agent to identify and collaborate with neighbors that observe the same hypothesis, while excluding others, thus resulting in improved performance compared to both non-cooperative learning and cooperative social learning solutions. We analyze the asymptotic behavior of agents' beliefs under the proposed social learning algorithm and provide sufficient conditions that enable all agents to correctly identify their true hypotheses. The theoretical analysis is corroborated by numerical simulations.      
### 54.Controlling Smart Propagation Environments: Long-Term versus Short-Term Phase Shift Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2110.13288.pdf)
>  Reconfigurable intelligent surfaces (RISs) have recently gained significant interest as an emerging technology for future wireless networks. This paper studies an RIS-assisted propagation environment, where a single-antenna source transmits data to a single-antenna destination in the presence of a weak direct link. We analyze and compare RIS designs based on long-term and short-term channel statistics in terms of coverage probability and ergodic rate. For the considered optimization designs, closed-form expressions for the coverage probability and ergodic rate are derived. We use numerical simulations to analyze and compare against analytic results in finite samples. Also, we show that the considered optimal phase shift designs outperform several heuristic benchmarks.      
### 55.Image Quality Assessment using Contrastive Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.13266.pdf)
>  We consider the problem of obtaining image quality representations in a self-supervised manner. We use prediction of distortion type and degree as an auxiliary task to learn features from an unlabeled image dataset containing a mixture of synthetic and realistic distortions. We then train a deep Convolutional Neural Network (CNN) using a contrastive pairwise objective to solve the auxiliary problem. We refer to the proposed training framework and resulting deep IQA model as the CONTRastive Image QUality Evaluator (CONTRIQUE). During evaluation, the CNN weights are frozen and a linear regressor maps the learned representations to quality scores in a No-Reference (NR) setting. We show through extensive experiments that CONTRIQUE achieves competitive performance when compared to state-of-the-art NR image quality models, even without any additional fine-tuning of the CNN backbone. The learned representations are highly robust and generalize well across images afflicted by either synthetic or authentic distortions. Our results suggest that powerful quality representations with perceptual relevance can be obtained without requiring large labeled subjective image quality datasets. The implementations used in this paper are available at \url{<a class="link-external link-https" href="https://github.com/pavancm/CONTRIQUE" rel="external noopener nofollow">this https URL</a>}.      
### 56.Beyond $L_p$ clipping: Equalization-based Psychoacoustic Attacks against ASRs  [ :arrow_down: ](https://arxiv.org/pdf/2110.13250.pdf)
>  Automatic Speech Recognition (ASR) systems convert speech into text and can be placed into two broad categories: traditional and fully end-to-end. Both types have been shown to be vulnerable to adversarial audio examples that sound benign to the human ear but force the ASR to produce malicious transcriptions. Of these attacks, only the "psychoacoustic" attacks can create examples with relatively imperceptible perturbations, as they leverage the knowledge of the human auditory system. Unfortunately, existing psychoacoustic attacks can only be applied against traditional models, and are obsolete against the newer, fully end-to-end ASRs. In this paper, we propose an equalization-based psychoacoustic attack that can exploit both traditional and fully end-to-end ASRs. We successfully demonstrate our attack against real-world ASRs that include DeepSpeech and Wav2Letter. Moreover, we employ a user study to verify that our method creates low audible distortion. Specifically, 80 of the 100 participants voted in favor of all our attack audio samples as less noisier than the existing state-of-the-art attack. Through this, we demonstrate both types of existing ASR pipelines can be exploited with minimum degradation to attack audio quality.      
### 57.Variational framework for partially-measured physical system control: examples of vision neuroscience and optical random media  [ :arrow_down: ](https://arxiv.org/pdf/2110.13228.pdf)
>  To characterize a physical system to behave as desired, either its underlying governing rules must be known a priori or the system itself be accurately measured. The complexity of full measurements of the system scales with its size. When exposed to real-world conditions, such as perturbations or time-varying settings, the system calibrated for a fixed working condition might require non-trivial re-calibration, a process that could be prohibitively expensive, inefficient and impractical for real-world use cases. In this work, we propose a learning procedure to obtain a desired target output from a physical system. We use Variational Auto-Encoders (VAE) to provide a generative model of the system function and use this model to obtain the required input of the system that produces the target output. We showcase the applicability of our method for two datasets in optical physics and neuroscience.      
### 58.Support Recovery Guarantees for Periodic Signals with Nested Periodic Dictionaries  [ :arrow_down: ](https://arxiv.org/pdf/2110.13200.pdf)
>  Periodic signals composed of periodic mixtures admit sparse representations in nested periodic dictionaries (NPDs). Therefore, their underlying hidden periods can be estimated by recovering the exact support of said representations. In this paper, support recovery guarantees of such signals are derived both in noise-free and noisy settings. While exact recovery conditions have long been studied in the theory of compressive sensing, existing conditions fall short of yielding meaningful achievability regions in the context of periodic signals with sparse representations in NPDs, in part since existing bounds do not capture structures intrinsic to these dictionaries. We leverage known properties of NPDs to derive several conditions for exact sparse recovery of periodic mixtures in the noise-free setting. These conditions rest on newly introduced notions of nested periodic coherence and restricted coherence, which can be efficiently computed and verified. In the presence of noise, we obtain improved conditions for recovering the exact support set of the sparse representation of the periodic mixture via orthogonal matching pursuit based on the introduced notions of coherence. The theoretical findings are corroborated using numerical experiments for different families of NPDs. Our results show significant improvement over generic recovery bounds as the conditions hold over a larger range of sparsity levels.      
### 59.Spectral unmixing of Raman microscopic images of single human cells using Independent Component Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2110.13189.pdf)
>  Application of independent component analysis (ICA) as an unmixing and image clustering technique for high spatial resolution Raman maps is reported. A hyperspectral map of a fixed human cell was collected by a Raman micro spectrometer in a raster pattern on a 0.5um grid. Unlike previously used unsupervised machine learning techniques such as principal component analysis, ICA is based on non-Gaussianity and statistical independence of data which is the case for mixture Raman spectra. Hence, ICA is a great candidate for assembling pseudo-colour maps from the spectral hypercube of Raman spectra. Our experimental results revealed that ICA is capable of reconstructing false colour maps of Raman hyperspectral data of human cells, showing the nuclear region constituents as well as subcellular organelle in the cytoplasm and distribution of mitochondria in the perinuclear region. Minimum preprocessing requirements and label-free nature of the ICA method make it a great unmixed method for extraction of endmembers in Raman hyperspectral maps of living cells.      
### 60.As if by magic: self-supervised training of deep despeckling networks with MERLIN  [ :arrow_down: ](https://arxiv.org/pdf/2110.13148.pdf)
>  Speckle fluctuations seriously limit the interpretability of synthetic aperture radar (SAR) images. Speckle reduction has thus been the subject of numerous works spanning at least four decades. Techniques based on deep neural networks have recently achieved a new level of performance in terms of SAR image restoration quality. Beyond the design of suitable network architectures or the selection of adequate loss functions, the construction of training sets is of uttermost importance. So far, most approaches have considered a supervised training strategy: the networks are trained to produce outputs as close as possible to speckle-free reference images. Speckle-free images are generally not available, which requires resorting to natural or optical images or the selection of stable areas in long time series to circumvent the lack of ground truth. Self-supervision, on the other hand, avoids the use of speckle-free images. We introduce a self-supervised strategy based on the separation of the real and imaginary parts of single-look complex SAR images, called MERLIN (coMplex sElf-supeRvised despeckLINg), and show that it offers a straightforward way to train all kinds of deep despeckling networks. Networks trained with MERLIN take into account the spatial correlations due to the SAR transfer function specific to a given sensor and imaging mode. By requiring only a single image, and possibly exploiting large archives, MERLIN opens the door to hassle-free as well as large-scale training of despeckling networks. The code of the trained models is made freely available at <a class="link-external link-https" href="https://gitlab.telecom-paris.fr/RING/MERLIN" rel="external noopener nofollow">this https URL</a>.      
