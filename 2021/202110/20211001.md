# ArXiv eess --Fri, 1 Oct 2021
### 1.Bend-Net: Bending Loss Regularized Multitask Learning Network for Nuclei Segmentation in Histopathology Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.15283.pdf)
>  Separating overlapped nuclei is a major challenge in histopathology image analysis. Recently published approaches have achieved promising overall performance on nuclei segmentation; however, their performance on separating overlapped nuclei is quite limited. To address the issue, we propose a novel multitask learning network with a bending loss regularizer to separate overlapped nuclei accurately. The newly proposed multitask learning architecture enhances the generalization by learning shared representation from three tasks: instance segmentation, nuclei distance map prediction, and overlapped nuclei distance map prediction. The proposed bending loss defines high penalties to concave contour points with large curvatures, and applies small penalties to convex contour points with small curvatures. Minimizing the bending loss avoids generating contours that encompass multiple nuclei. In addition, two new quantitative metrics, Aggregated Jaccard Index of overlapped nuclei (AJIO) and Accuracy of overlapped nuclei (ACCO), are designed for the evaluation of overlapped nuclei segmentation. We validate the proposed approach on the CoNSeP and MoNuSegv1 datasets using seven quantitative metrics: Aggregate Jaccard Index, Dice, Segmentation Quality, Recognition Quality, Panoptic Quality, AJIO, and ACCO. Extensive experiments demonstrate that the proposed Bend-Net outperforms eight state-of-the-art approaches.      
### 2.PortaSpeech: Portable and High-Quality Generative Text-to-Speech  [ :arrow_down: ](https://arxiv.org/pdf/2109.15166.pdf)
>  Non-autoregressive text-to-speech (NAR-TTS) models such as FastSpeech 2 and Glow-TTS can synthesize high-quality speech from the given text in parallel. After analyzing two kinds of generative NAR-TTS models (VAE and normalizing flow), we find that: VAE is good at capturing the long-range semantics features (e.g., prosody) even with small model size but suffers from blurry and unnatural results; and normalizing flow is good at reconstructing the frequency bin-wise details but performs poorly when the number of model parameters is limited. Inspired by these observations, to generate diverse speech with natural details and rich prosody using a lightweight architecture, we propose PortaSpeech, a portable and high-quality generative text-to-speech model. Specifically, 1) to model both the prosody and mel-spectrogram details accurately, we adopt a lightweight VAE with an enhanced prior followed by a flow-based post-net with strong conditional inputs as the main architecture. 2) To further compress the model size and memory footprint, we introduce the grouped parameter sharing mechanism to the affine coupling layers in the post-net. 3) To improve the expressiveness of synthesized speech and reduce the dependency on accurate fine-grained alignment between text and speech, we propose a linguistic encoder with mixture alignment combining hard inter-word alignment and soft intra-word alignment, which explicitly extracts word-level semantic information. Experimental results show that PortaSpeech outperforms other TTS models in both voice quality and prosody modeling in terms of subjective and objective evaluation metrics, and shows only a slight performance degradation when reducing the model parameters to 6.7M (about 4x model size and 3x runtime memory compression ratio compared with FastSpeech 2). Our extensive ablation studies demonstrate that each design in PortaSpeech is effective.      
### 3.Convolution-Free Waveform Transformers for Multi-Lead ECG Classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.15129.pdf)
>  We present our entry to the 2021 PhysioNet/CinC challenge - a waveform transformer model to detect cardiac abnormalities from ECG recordings. We compare the performance of the waveform transformer model on different ECG-lead subsets using approximately 88,000 ECG recordings from six datasets. In the official rankings, team prna ranked between 9 and 15 on 12, 6, 4, 3 and 2-lead sets respectively. Our waveform transformer model achieved an average challenge metric of 0.47 on the held-out test set across all ECG-lead subsets. Our combined performance across all leads placed us at rank 11 out of 39 officially ranking teams.      
### 4.Real-Time Multi-Level Neonatal Heart and Lung Sound Quality Assessment for Telehealth Applications  [ :arrow_down: ](https://arxiv.org/pdf/2109.15127.pdf)
>  Digital stethoscopes in combination with telehealth allow chest sounds to be easily collected and transmitted for remote monitoring and diagnosis. Chest sounds contain important information about a newborn's cardio-respiratory health. However, low-quality recordings complicate the remote monitoring and diagnosis. In this study, a new method is proposed to objectively and automatically assess heart and lung signal quality on a 5-level scale in real-time and to assess the effect of signal quality on vital sign estimation. For the evaluation, a total of 207 10s long chest sounds were taken from 119 preterm and full-term babies. Thirty of the recordings from ten subjects were obtained with synchronous vital signs from the Neonatal Intensive Care Unit (NICU) based on electrocardiogram recordings. As reference, seven annotators independently assessed the signal quality. For automatic quality classification, 400 features were extracted from the chest sounds. After feature selection using minimum redundancy and maximum relevancy algorithm, class balancing, and hyper-parameter optimization, a variety of multi-class and ordinal classification and regression algorithms were trained. Then, heart rate and breathing rate were automatically estimated from the chest sounds using adapted pre-existing methods. The results of subject-wise leave-one-out cross-validation show that the best-performing models had a mean squared error (MSE) of 0.49 and 0.61, and balanced accuracy of 57% and 51% for heart and lung qualities, respectively. The best-performing models for real-time analysis (&lt;200ms) had MSE of 0.459 and 0.67, and balanced accuracy of 57% and 46%, respectively. Our experimental results underscore that increasing the signal quality leads to a reduction in vital sign error, with only high-quality recordings having a mean absolute error of less than 5 beats per minute, as required for clinical usage.      
### 5.A Frequency-Domain Approach to Nonlinear Negative Imaginary Systems Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.15126.pdf)
>  In this study, we extend the theory of negative imaginary (NI) systems to a nonlinear framework using a frequency-domain approach. The extended notion is completely characterized via a finite-frequency integration over a "kernel function" on energy-bounded input and output signal pairs. The notion is closely related to and carefully contrasted with the well-studied extension of negative imaginariness -- the theory of counterclockwise dynamics. A condition for feedback stability of the proposed nonlinear NI systems is then developed based on the technique of integral quadratic constraints. Examples and simulations on feedback interconnections of typical nonlinear systems are provided to demonstrate the effectiveness.      
### 6.Visualisation to Explain Personal Health Trends in Smart Homes  [ :arrow_down: ](https://arxiv.org/pdf/2109.15125.pdf)
>  An ambient sensor network is installed in Smart Homes to identify low-level events taking place by residents, which are then analysed to generate a profile of activities of daily living. These profiles are compared to both the resident's typical profile and to known "risky" profiles to support recommendation of evidence-based interventions. Maintaining trust presents an XAI challenge because the recommendations are not easily interpretable. Trust in the system can be improved by making the decision-making process more transparent. We propose a visualisation workflow which presents the data in clear, colour-coded graphs.      
### 7.Learning generalized Nash equilibria in monotone games: A hybrid adaptive extremum seeking control approach  [ :arrow_down: ](https://arxiv.org/pdf/2109.15113.pdf)
>  In this paper, we solve the problem of learning a generalized Nash equilibrium (GNE) in merely monotone games. First, we propose a novel continuous semi-decentralized solution algorithm without projections that uses first-order information to compute a GNE with a central coordinator. As the second main contribution, we design a gain adaptation scheme for the previous algorithm in order to alleviate the problem of improper scaling of the cost functions versus the constraints. Third, we propose a data-driven variant of the former algorithm, where each agent estimates their individual pseudogradient via zeroth-order information, namely, measurements of their individual cost function values. Finally, we apply our method to a perturbation amplitude optimization problem in oil extraction engineering.      
### 8.Federated Learning in ASR: Not as Easy as You Think  [ :arrow_down: ](https://arxiv.org/pdf/2109.15108.pdf)
>  With the growing availability of smart devices and cloud services, personal speech assistance systems are increasingly used on a daily basis. Most devices redirect the voice recordings to a central server, which uses them for upgrading the recognizer model. This leads to major privacy concerns, since private data could be misused by the server or third parties. Federated learning is a decentralized optimization strategy that has been proposed to address such concerns. Utilizing this approach, private data is used for on-device training. Afterwards, updated model parameters are sent to the server to improve the global model, which is redistributed to the clients. In this work, we implement federated learning for speech recognition in a hybrid and an end-to-end model. We discuss the outcomes of these systems, which both show great similarities and only small improvements, pointing to a need for a deeper understanding of federated learning for speech recognition.      
### 9.Deep Homography Estimation in Dynamic Surgical Scenes for Laparoscopic Camera Motion Extraction  [ :arrow_down: ](https://arxiv.org/pdf/2109.15098.pdf)
>  Current laparoscopic camera motion automation relies on rule-based approaches or only focuses on surgical tools. Imitation Learning (IL) methods could alleviate these shortcomings, but have so far been applied to oversimplified setups. Instead of extracting actions from oversimplified setups, in this work we introduce a method that allows to extract a laparoscope holder's actions from videos of laparoscopic interventions. We synthetically add camera motion to a newly acquired dataset of camera motion free da Vinci surgery image sequences through the introduction of a novel homography generation algorithm. The synthetic camera motion serves as a supervisory signal for camera motion estimation that is invariant to object and tool motion. We perform an extensive evaluation of state-of-the-art (SOTA) Deep Neural Networks (DNNs) across multiple compute regimes, finding our method transfers from our camera motion free da Vinci surgery dataset to videos of laparoscopic interventions, outperforming classical homography estimation approaches in both, precision by 41%, and runtime on a CPU by 43%.      
### 10.A novel class of fixed-time consensus protocols for multi-agent systems with simple dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2109.15094.pdf)
>  This paper investigates the fixed-time consensus problem for a class of multi-agent systems with simple dynamics. Unlike the traditional way to realize fixed-time convergence, a novel strategy using the property of periodic functions is proposed to achieve fixed-time convergence. On this basis, novel protocols for achieving fixed-time consensus and fixed-time average consensus are then given, where the upper bound of the consensus time is independent of initial conditions. Moreover, the result of fixed-time average consensus is extended to a more general case, where the weights of different states can be allocated in advance. Finally, the fixed-time consensus in the presence of disturbances is derived with the help of sliding mode control, where a fixed-time sliding manifold and fixed-time reaching law are designed. All the conclusions are demonstrated by dedicated simulation examples.      
### 11.Robust Multi-Domain Mitosis Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.15092.pdf)
>  Domain variability is a common bottle neck in developing generalisable algorithms for various medical applications. Motivated by the observation that the domain variability of the medical images is to some extent compact, we propose to learn a target representative feature space through unpaired image to image translation (CycleGAN). We comprehensively evaluate the performanceand usefulness by utilising the transformation to mitosis detection with candidate proposal and classification. This work presents a simple yet effective multi-step mitotic figure detection algorithm developed as a baseline for the MIDOG challenge. On the preliminary test set, the algorithm scoresan F1 score of 0.52.      
### 12.Subcarrier Number and Indices-Based KeyGeneration for Future Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.15091.pdf)
>  Physical layer key generation from the wireless channel is an emerging area of interest to provide confidentiality and authentication. One of the main challenges in this domain is to increase the length of the secret key while maintaining its randomness and uniformity. In this work, new dimensions for wireless channel-based key generation are proposed for orthogonal frequency division multiplexing (OFDM) systems. The novel perspective of the proposed work lies in the generation of key bits not only from the magnitudes of OFDM subchannels as it has conventionally been done but also from the number and positions of those subchannels whose channel gains are above the mean of the respective subblock. The effectiveness of the proposed algorithms is evaluated in terms of key generation rate and key mismatch rate. Additionally, a statistical test suite offered by the National Institute of Standards and Technology is used to evaluate the randomness of the generated key bits. It is shown in the simulation results that the involvement of the proposed dimensions can double the key generation rate compared to conventional algorithms.      
### 13.Workflow Augmentation of Video Data for Event Recognition with Time-Sensitive Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.15063.pdf)
>  Supervised training of neural networks requires large, diverse and well annotated data sets. In the medical field, this is often difficult to achieve due to constraints in time, expert knowledge and prevalence of an event. Artificial data augmentation can help to prevent overfitting and improve the detection of rare events as well as overall performance. However, most augmentation techniques use purely spatial transformations, which are not sufficient for video data with temporal correlations. In this paper, we present a novel methodology for workflow augmentation and demonstrate its benefit for event recognition in cataract surgery. The proposed approach increases the frequency of event alternation by creating artificial videos. The original video is split into event segments and a workflow graph is extracted from the original annotations. Finally, the segments are assembled into new videos based on the workflow graph. Compared to the original videos, the frequency of event alternation in the augmented cataract surgery videos increased by 26%. Further, a 3% higher classification accuracy and a 7.8% higher precision was achieved compared to a state-of-the-art approach. Our approach is particularly helpful to increase the occurrence of rare but important events and can be applied to a large variety of use cases.      
### 14.Deep Contextual Video Compression  [ :arrow_down: ](https://arxiv.org/pdf/2109.15047.pdf)
>  Most of the existing neural video compression methods adopt the predictive coding framework, which first generates the predicted frame and then encodes its residue with the current frame. However, as for compression ratio, predictive coding is only a sub-optimal solution as it uses simple subtraction operation to remove the redundancy across frames. In this paper, we propose a deep contextual video compression framework to enable a paradigm shift from predictive coding to conditional coding. In particular, we try to answer the following questions: how to define, use, and learn condition under a deep video compression framework. To tap the potential of conditional coding, we propose using feature domain context as condition. This enables us to leverage the high dimension context to carry rich information to both the encoder and the decoder, which helps reconstruct the high-frequency contents for higher video quality. Our framework is also extensible, in which the condition can be flexibly designed. Experiments show that our method can significantly outperform the previous state-of-the-art (SOTA) deep video compression methods. When compared with x265 using veryslow preset, we can achieve 26.0% bitrate saving for 1080P standard test videos.      
### 15.DOA Estimation in Nonuniform Sensor Noise  [ :arrow_down: ](https://arxiv.org/pdf/2109.15043.pdf)
>  The problem of direction-of-arrival (DOA) estimation in the presence of nonuniform sensor noise is considered and a novel algorithm is developed. The algorithm consists of three phases. First, the diagonal nonuniform sensor noise covariance matrix is estimated using an iterative procedure that requires only few iterations to obtain an accurate estimate. The asymptotic variance of one iteration is derived for the proposed noise covariance estimator. Second, a forward-only rooting-based DOA estimator as well as its forward-backward averaging extension are developed for DOA estimation. The DOA estimators take advantage of using second-order statistics of signal subspace perturbation in constructing a weight matrix of a properly designed generalized least squares minimization problem. Despite the fact that these DOA estimators are iterative, only a few iterations are sufficient to reach accurate results. The asymptotic performance of these DOA estimators is also investigated. Third, a newly designed DOA selection strategy with reasonable computational cost is developed to select L actual sources out of 2L candidates generated at the second phase. Numerical simulations are conducted in order to establish the considerable superiority of the proposed algorithm compared to the the existing state-of-the-art methods in challenging scenarios in both cases of uniform and nonuniform sensor noise.      
### 16.An optimal reeling control strategy for pumping airborne wind energy systems without wind speed feedback  [ :arrow_down: ](https://arxiv.org/pdf/2109.15032.pdf)
>  Pumping airborne wind energy (AWE) systems employ a kite to convert wind energy into electricity, through a cyclic reeling motion of the tether. The problem of computing the optimal reeling speed for the sake of maximizing the average cycle power is considered. The difficulty stems from two aspects: 1) the uncertain, time- (and space-) varying nature of wind speed, which can not be measured accurately, and 2) the need to consider, in the same optimization problem, the different operational phases of the power cycle. A new, model-based approach that solves this problem is proposed. In the design phase, a model of the AWE system is employed to collect data pertaining to the cycle power obtained with various reel-in/reel-out speed pairs, assuming known wind speed. Then, a nonlinear map, identified from these data, is used as cost function in an optimization program that computes the best reel-in and -out speed pairs for each wind speed. Finally, the optimization results are exploited to infer the link between optimal reeling speed and tether force, which are both measured with high accuracy. Such a link is used to design a feedback controller that computes the reeling speed based on the measured tether force, in order to converge on the optimal force-speed manifold. Simulation results with a realistic model illustrate the effectiveness of the approach.      
### 17.An investigation of pre-upsampling generative modelling and Generative Adversarial Networks in audio super resolution  [ :arrow_down: ](https://arxiv.org/pdf/2109.14994.pdf)
>  There have been several successful deep learning models that perform audio super-resolution. Many of these approaches involve using preprocessed feature extraction which requires a lot of domain-specific signal processing knowledge to implement. Convolutional Neural Networks (CNNs) improved upon this framework by automatically learning filters. An example of a convolutional approach is AudioUNet, which takes inspiration from novel methods of upsampling images. Our paper compares the pre-upsampling AudioUNet to a new generative model that upsamples the signal before using deep learning to transform it into a more believable signal. Based on the EDSR network for image super-resolution, the newly proposed model outperforms UNet with a 20% increase in log spectral distance and a mean opinion score of 4.06 compared to 3.82 for the two times upsampling case. AudioEDSR also has 87% fewer parameters than AudioUNet. How incorporating AudioUNet into a Wasserstein GAN (with gradient penalty) (WGAN-GP) structure can affect training is also explored. Finally the effects artifacting has on the current state of the art is analysed and solutions to this problem are proposed. The methods used in this paper have broad applications to telephony, audio recognition and audio generation tasks.      
### 18.Comparative Validation of Machine Learning Algorithms for Surgical Workflow and Skill Analysis with the HeiChole Benchmark  [ :arrow_down: ](https://arxiv.org/pdf/2109.14956.pdf)
>  PURPOSE: Surgical workflow and skill analysis are key technologies for the next generation of cognitive surgical assistance systems. These systems could increase the safety of the operation through context-sensitive warnings and semi-autonomous robotic assistance or improve training of surgeons via data-driven feedback. In surgical workflow analysis up to 91% average precision has been reported for phase recognition on an open data single-center dataset. In this work we investigated the generalizability of phase recognition algorithms in a multi-center setting including more difficult recognition tasks such as surgical action and surgical skill. METHODS: To achieve this goal, a dataset with 33 laparoscopic cholecystectomy videos from three surgical centers with a total operation time of 22 hours was created. Labels included annotation of seven surgical phases with 250 phase transitions, 5514 occurences of four surgical actions, 6980 occurences of 21 surgical instruments from seven instrument categories and 495 skill classifications in five skill dimensions. The dataset was used in the 2019 Endoscopic Vision challenge, sub-challenge for surgical workflow and skill analysis. Here, 12 teams submitted their machine learning algorithms for recognition of phase, action, instrument and/or skill assessment. RESULTS: F1-scores were achieved for phase recognition between 23.9% and 67.7% (n=9 teams), for instrument presence detection between 38.5% and 63.8% (n=8 teams), but for action recognition only between 21.8% and 23.3% (n=5 teams). The average absolute error for skill assessment was 0.78 (n=1 team). CONCLUSION: Surgical workflow and skill analysis are promising technologies to support the surgical team, but are not solved yet, as shown by our comparison of algorithms. This novel benchmark can be used for comparable evaluation and validation of future work.      
### 19.Neural Networks-based Equalizers for Coherent Optical Transmission: Caveats and Pitfalls  [ :arrow_down: ](https://arxiv.org/pdf/2109.14942.pdf)
>  This paper performs a detailed multi-faceted analysis of the key challenges and common design caveats related to the development of efficient neural networks (NN) nonlinear channel equalizers in coherent optical communication systems. Our study aims to guide researchers and engineers working in this field. We start by clarifying the metrics used to evaluate the equalizers' performance, relating them to the loss functions employed in the training of the NN equalizers. The relationships between the channel propagation model's accuracy and the performance of the equalizers are addressed and quantified. Next, we assess the impact of the order of the pseudo-random bit sequence used to generate the -- numerical and experimental -- data as well as of the DAC memory limitations on the operation of the NN equalizers both during training and validation phases. Finally, we examine the critical issues of overfitting limitations, a difference between using classification instead of regression, and the batch size-related peculiarities. We conclude by providing analytical expressions for the equalizers' complexity evaluation in the digital signal processing (DSP) terms.      
### 20.A Deep Learning Localization Method for Measuring Abdominal Muscle Dimensions in Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.14919.pdf)
>  Health professionals extensively use Two- Dimensional (2D) Ultrasound (US) videos and images to visualize and measure internal organs for various purposes including evaluation of muscle architectural changes. US images can be used to measure abdominal muscles dimensions for the diagnosis and creation of customized treatment plans for patients with Low Back Pain (LBP), however, they are difficult to interpret. Due to high variability, skilled professionals with specialized training are required to take measurements to avoid low intra-observer reliability. This variability stems from the challenging nature of accurately finding the correct spatial location of measurement endpoints in abdominal US images. In this paper, we use a Deep Learning (DL) approach to automate the measurement of the abdominal muscle thickness in 2D US images. By treating the problem as a localization task, we develop a modified Fully Convolutional Network (FCN) architecture to generate blobs of coordinate locations of measurement endpoints, similar to what a human operator does. We demonstrate that using the TrA400 US image dataset, our network achieves a Mean Absolute Error (MAE) of 0.3125 on the test set, which almost matches the performance of skilled ultrasound technicians. Our approach can facilitate next steps for automating the process of measurements in 2D US images, while reducing inter-observer as well as intra-observer variability for more effective clinical outcomes.      
### 21.Sensing Integrated DFT-Spread OFDM Waveform and Deep Learning-powered Receiver Design for Terahertz Integrated Sensing and Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.14918.pdf)
>  Terahertz (THz) communications are envisioned as a key technology of next-generation wireless systems due to its ultra-broad bandwidth. One step forward, THz integrated sensing and communication (ISAC) system can realize both unprecedented data rates and millimeter-level accurate sensing. However, THz ISAC meets stringent challenges on waveform and receiver design, to fully exploit the peculiarities of THz channel and transceivers. In this work, a sensing integrated discrete Fourier transform spread orthogonal frequency division multiplexing (SI-DFT-s-OFDM) system is proposed for THz ISAC, which can provide lower peak-to-average power ratio than OFDM and is adaptive to flexible delay spread of the THz channel. Without compromising communication capabilities, the proposed SI-DFT-s-OFDM realizes millimeter-level range estimation and decimeter-per-second-level velocity estimation accuracy. In addition, the bit error rate (BER) performance is improved by 5 dB gain at the $10^{-3}$ BER level compared with OFDM. At the receiver, a two-level multi-task neural network based ISAC detector is developed to jointly recover transmitted data and estimate target range and velocity, while mitigating the imperfections and non-linearities of THz systems. Extensive simulation results demonstrate that the deep learning method can realize mutually enhanced performance for communication and sensing, and is robust against white noise, Doppler effects, multi-path fading and phase noise.      
### 22.Stability Constrained Reinforcement Learning for Real-Time Voltage Control  [ :arrow_down: ](https://arxiv.org/pdf/2109.14854.pdf)
>  Deep reinforcement learning (RL) has been recognized as a promising tool to address the challenges in real-time control of power systems. However, its deployment in real-world power systems has been hindered by a lack of formal stability and safety guarantees. In this paper, we propose a stability constrained reinforcement learning method for real-time voltage control in distribution grids and we prove that the proposed approach provides a formal voltage stability guarantee. The key idea underlying our approach is an explicitly constructed Lyapunov function that certifies stability. We demonstrate the effectiveness of the approach in case studies, where the proposed method can reduce the transient control cost by more than 30\% and shorten the response time by a third compared to a widely used linear policy, while always achieving voltage stability. In contrast, standard RL methods often fail to achieve voltage stability.      
### 23.End-to-End Image Compression with Probabilistic Decoding  [ :arrow_down: ](https://arxiv.org/pdf/2109.14837.pdf)
>  Lossy image compression is a many-to-one process, thus one bitstream corresponds to multiple possible original images, especially at low bit rates. However, this nature was seldom considered in previous studies on image compression, which usually chose one possible image as reconstruction, e.g. the one with the maximal a posteriori probability. We propose a learned image compression framework to natively support probabilistic decoding. The compressed bitstream is decoded into a series of parameters that instantiate a pre-chosen distribution; then the distribution is used by the decoder to sample and reconstruct images. The decoder may adopt different sampling strategies and produce diverse reconstructions, among which some have higher signal fidelity and some others have better visual quality. The proposed framework is dependent on a revertible neural network-based transform to convert pixels into coefficients that obey the pre-chosen distribution as much as possible. Our code and models will be made publicly available.      
### 24.USEV: Universal Speaker Extraction with Visual Cue  [ :arrow_down: ](https://arxiv.org/pdf/2109.14831.pdf)
>  A speaker extraction algorithm seeks to extract the target speaker's voice from a multi-talker speech mixture. An auxiliary reference, such as a video recording or a pre-recorded speech, is usually used as a cue to form a top-down auditory attention. The prior studies are focused mostly on speaker extraction from a multi-talker speech mixture with highly overlapping speakers. However, a multi-talker speech mixture is often sparsely overlapped, furthermore, the target speaker could even be absent sometimes. In this paper, we propose a universal speaker extraction network that works for all multi-talker scenarios, where the target speaker can be either absent or present. When the target speaker is present, the network performs over a wide range of target-interference speaker overlapping ratios, from 0% to 100%. The speech in such universal multi-talker scenarios is generally described as sparsely overlapped speech. We advocate that a visual cue, i.e. lips movement, is more informative to serve as the auxiliary reference than an audio cue, i.e. pre-recorded speech. In addition, we propose a scenario-aware differentiated loss function for network training. The experimental results show that our proposed network outperforms various competitive baselines in disentangling sparsely overlapped speech in terms of signal fidelity and perceptual evaluations.      
### 25.Spark in the Dark: Evaluating Encoder-Decoder Pairs for COVID-19 CT's Semantic Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.14818.pdf)
>  With the COVID-19 global pandemic, computerassisted diagnoses of medical images have gained a lot of attention, and robust methods of Semantic Segmentation of Computed Tomography (CT) turned highly desirable. Semantic Segmentation of CT is one of many research fields of automatic detection of Covid-19 and was widely explored since the Covid19 outbreak. In the robotic field, Semantic Segmentation of organs and CTs are widely used in robots developed for surgery tasks. As new methods and new datasets are proposed quickly, it becomes apparent the necessity of providing an extensive evaluation of those methods. To provide a standardized comparison of different architectures across multiple recently proposed datasets, we propose in this paper an extensive benchmark of multiple encoders and decoders with a total of 120 architectures evaluated in five datasets, with each dataset being validated through a five-fold cross-validation strategy, totaling 3.000 experiments. To the best of our knowledge, this is the largest evaluation in number of encoders, decoders, and datasets proposed in the field of Covid-19 CT segmentation.      
### 26.Unsupervised Landmark Detection Based Spatiotemporal Motion Estimation for 4D Dynamic Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.14805.pdf)
>  Motion estimation is a fundamental step in dynamic medical image processing for the assessment of target organ anatomy and function. However, existing image-based motion estimation methods, which optimize the motion field by evaluating the local image similarity, are prone to produce implausible estimation, especially in the presence of large motion. In this study, we provide a novel motion estimation framework of Dense-Sparse-Dense (DSD), which comprises two stages. In the first stage, we process the raw dense image to extract sparse landmarks to represent the target organ anatomical topology and discard the redundant information that is unnecessary for motion estimation. For this purpose, we introduce an unsupervised 3D landmark detection network to extract spatially sparse but representative landmarks for the target organ motion estimation. In the second stage, we derive the sparse motion displacement from the extracted sparse landmarks of two images of different time points. Then, we present a motion reconstruction network to construct the motion field by projecting the sparse landmarks displacement back into the dense image domain. Furthermore, we employ the estimated motion field from our two-stage DSD framework as initialization and boost the motion estimation quality in light-weight yet effective iterative optimization. We evaluate our method on two dynamic medical imaging tasks to model cardiac motion and lung respiratory motion, respectively. Our method has produced superior motion estimation accuracy compared to existing comparative methods. Besides, the extensive experimental results demonstrate that our solution can extract well representative anatomical landmarks without any requirement of manual annotation. Our code is publicly available online.      
### 27.Automated airway segmentation by learning graphical structure  [ :arrow_down: ](https://arxiv.org/pdf/2109.14792.pdf)
>  In this research project, we put forward an advanced method for airway segmentation based on the existent convolutional neural network (CNN) and graph neural network (GNN). The method is originated from the vessel segmentation, but we ameliorate it and enable the novel model to perform better for datasets from computed tomography (CT) scans. Current methods for airway segmentation are considering the regular grid only. No matter what the detailed model is, including the 3-dimensional CNN or 2-dimensional CNN in three directions, the overall graph structures are not taken into consideration. In our model, with the neighbourhoods of airway taken into account, the graph structure is incorporated and the segmentation of airways are improved compared with the traditional CNN methods. We perform experiments on the chest CT scans, where the ground truth segmentation labels are produced manually. The proposed model shows that compared with the CNN-only method, the combination of CNN and GNN has a better performance in that the bronchi in the chest CT scans can be detected in most cases. In addition, the model we propose has a wide extension since the architecture is also utilitarian in fulfilling similar aims in other datasets. Hence, the state-of-the-art model is of great significance and highly applicable in our daily lives. <br>Keywords: Airway segmentation, Convolutional neural network, Graph neural network      
### 28.Time Coordination of Multiple UAVs over Switching Communication Networks with Digraph Topologies  [ :arrow_down: ](https://arxiv.org/pdf/2109.14779.pdf)
>  This paper presents a time-coordination algorithm for multiple UAVs executing cooperative missions. Unlike previous algorithms, it does not rely on the assumption that the communication between UAVs is bidirectional. Thus, the topology of the inter-UAV information flow can be characterized by digraphs. To achieve coordination with weak connectivity, we design a switching law that orchestrates switching between jointly connected digraph topologies. In accordance with the law, the UAVs with a transmitter switch the topology of their coordination information flow. A Lyapunov analysis shows that a decentralized coordination controller steers coordination errors to a neighborhood of zero. Simulation results illustrate that the algorithm attains coordination objectives with significantly reduced inter-UAV communication compared to previous work.      
### 29.A Prior Knowledge Based Tumor and Tumoral Subregion Segmentation Tool for Pediatric Brain Tumors  [ :arrow_down: ](https://arxiv.org/pdf/2109.14775.pdf)
>  In the past few years, deep learning (DL) models have drawn great attention and shown superior performance on brain tumor and subregion segmentation tasks. However, the success is limited to segmentation of adult gliomas, where sufficient data have been collected, manually labeled, and published for training DL models. It is still challenging to segment pediatric tumors, because the appearances are different from adult gliomas. Hence, directly applying a pretained DL model on pediatric data usually generates unacceptable results. Because pediatric data is very limited, both labeled and unlabeled, we present a brain tumor segmentation model that is based on knowledge rather than learning from data. We also provide segmentation of more subregions for super heterogeneous tumor like atypical teratoid rhabdoid tumor (ATRT). Our proposed approach showed superior performance on both whole tumor and subregion segmentation tasks to DL based models on our pediatric data when training data is not available for transfer learning.      
### 30.Chest X-Rays Image Classification from beta-Variational Autoencoders Latent Features  [ :arrow_down: ](https://arxiv.org/pdf/2109.14760.pdf)
>  Chest X-Ray (CXR) is one of the most common diagnostic techniques used in everyday clinical practice all around the world. We hereby present a work which intends to investigate and analyse the use of Deep Learning (DL) techniques to extract information from such images and allow to classify them, trying to keep our methodology as general as possible and possibly also usable in a real world scenario without much effort, in the future. To move in this direction, we trained several beta-Variational Autoencoder (beta-VAE) models on the CheXpert dataset, one of the largest publicly available collection of labeled CXR images; from these models, latent features have been extracted and used to train other Machine Learning models, able to classify the original images from the features extracted by the beta-VAE. Lastly, tree-based models have been combined together in ensemblings to improve the results without the necessity of further training or models engineering. Expecting some drop in pure performance with the respect to state of the art classification specific models, we obtained encouraging results, which show the viability of our approach and the usability of the high level features extracted by the autoencoders for classification tasks.      
### 31.MetaHistoSeg: A Python Framework for Meta Learning in Histopathology Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.14754.pdf)
>  Few-shot learning is a standard practice in most deep learning based histopathology image segmentation, given the relatively low number of digitized slides that are generally available. While many models have been developed for domain specific histopathology image segmentation, cross-domain generalization remains a key challenge for properly validating models. Here, tooling and datasets to benchmark model performance across histopathological domains are lacking. To address this limitation, we introduce MetaHistoSeg - a Python framework that implements unique scenarios in both meta learning and instance based transfer learning. Designed for easy extension to customized datasets and task sampling schemes, the framework empowers researchers with the ability of rapid model design and experimentation. We also curate a histopathology meta dataset - a benchmark dataset for training and validating models on out-of-distribution performance across a range of cancer types. In experiments we showcase the usage of MetaHistoSeg with the meta dataset and find that both meta-learning and instance based transfer learning deliver comparable results on average, but in some cases tasks can greatly benefit from one over the other.      
### 32.A Convex Method of Generalized State Estimation using Circuit-theoretic Node-breaker Model  [ :arrow_down: ](https://arxiv.org/pdf/2109.14742.pdf)
>  An accurate and up-to-date grid topology is critical for situational awareness. However, it is non-trivial to obtain due to inaccurate switch status data caused by physical damage, communication error, or cyber-attack. This paper formulates a circuit-theoretic node-breaker (NB) model to create a generalized state estimation (GSE) method that is scalable and easily solvable for a practical grid with RTU and PMU measurements. We demonstrate that all switching devices (with discrete status) and meters (with continuous measurements) can be replaced with linear circuit models without relaxation so that the entire grid is mapped to an expanded linear circuit. Using this grid model, the state estimation is formulated as a Linear Programming (LP) problem whose solution includes a sparse vector of noise terms, which localizes suspicious wrong status and bad data separately. The proposed method provides the benefits of convexity and a reliable state estimation with intrinsic robustness against wrong switch status and bad measurement data.      
### 33.Machine Learning Assisted Phase-less Millimeter-Wave Beam Alignment in Multipath Channels  [ :arrow_down: ](https://arxiv.org/pdf/2109.14689.pdf)
>  Communication systems at millimeter-wave (mmW) and sub-terahertz frequencies are of increasing interest for future high-data rate networks. One critical challenge faced by phased array systems at these high frequencies is the efficiency of the initial beam alignment, typically using only phase-less power measurements due to high frequency oscillator phase noise. Traditional methods for beam alignment require exhaustive sweeps of all possible beam directions, thus scale communications overhead linearly with antenna array size. For better scaling with the large arrays required at high mmW bands, compressive sensing methods have been proposed as their overhead scales logarithmically with the array size. However, algorithms utilizing machine learning have shown more efficient and more accurate alignment when using real hardware due to array impairments. Additionally, few existing phase-less beam alignment algorithms have been tested over varied secondary path strength in multipath channels. In this work, we introduce a novel, machine learning based algorithm for beam alignment in multipath environments using only phase-less received power measurements. We consider the impacts of phased array sounding beam design and machine learning architectures on beam alignment performance and validate our findings experimentally using 60 GHz radios with 36-element phased arrays. Using experimental data in multipath channels, our proposed algorithm demonstrates an 88\% reduction in beam alignment overhead compared to an exhaustive search and at least a 62\% reduction in overhead compared to existing compressive methods.      
### 34.Automatic Estimation of Ulcerative Colitis Severity from Endoscopy Videos using Ordinal Multi-Instance Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.14685.pdf)
>  Ulcerative colitis (UC) is a chronic inflammatory bowel disease characterized by relapsing inflammation of the large intestine. The severity of UC is often represented by the Mayo Endoscopic Subscore (MES) which quantifies mucosal disease activity from endoscopy videos. In clinical trials, an endoscopy video is assigned an MES based upon the most severe disease activity observed in the video. For this reason, severe inflammation spread throughout the colon will receive the same MES as an otherwise healthy colon with severe inflammation restricted to a small, localized segment. Therefore, the extent of disease activity throughout the large intestine, and overall response to treatment, may not be completely captured by the MES. In this work, we aim to automatically estimate UC severity for each frame in an endoscopy video to provide a higher resolution assessment of disease activity throughout the colon. Because annotating severity at the frame-level is expensive, labor-intensive, and highly subjective, we propose a novel weakly supervised, ordinal classification method to estimate frame severity from video MES labels alone. Using clinical trial data, we first achieved 0.92 and 0.90 AUC for predicting mucosal healing and remission of UC, respectively. Then, for severity estimation, we demonstrate that our models achieve substantial Cohen's Kappa agreement with ground truth MES labels, comparable to the inter-rater agreement of expert clinicians. These findings indicate that our framework could serve as a foundation for novel clinical endpoints, based on a more localized scoring system, to better evaluate UC drug efficacy in clinical trials.      
### 35.Joint Information and Mechanism Design for Queues with Heterogeneous Users  [ :arrow_down: ](https://arxiv.org/pdf/2109.14673.pdf)
>  We consider a queue with an unobservable backlog by the incoming users. There is an information designer that observes the queue backlog and makes recommendations to the users arriving at the queue whether to join or not to join the queue. The arriving users have payoff relevant private types. The users, upon arrival, send a message, that is supposed to be their type, to the information designer if they are willing to hear a recommendation. The information designer then creates a recommendation for that specific type of user. The users have to pay a tax in exchange for the information they receive. In this setting, the information designer has two types of commitments. The first commitment is the recommendation policy and the second commitment is the tax function. We combine mechanism design and information design to study a queuing system with heterogeneous users. In this setting, the information designer is a sender of the information in the information design aspect and a receiver in the mechanism design aspect of the model. We formulate an optimization problem that characterizes the solution of the joint design problem. We characterize the tax functions and provide structural results for the recommendation policy of the information designer.      
### 36.Non-Hermitian physics and engineering in silicon photonics  [ :arrow_down: ](https://arxiv.org/pdf/2109.15262.pdf)
>  Silicon photonics has been studied as an integratable optical platform where numerous applicable devices and systems are created based on modern physics and state-of-the-art nanotechnologies. The implementation of quantum mechanics has been the driving force of the most intriguing design of photonic structures, since the optical systems are found of great capability and potential in realizing the analogues of quantum concepts and phenomena. Non-Hermitian physics, which breaks the conventional scope of quantum mechanics based on Hermitian Hamiltonian, has been widely explored in the platform of silicon photonics, with promising design of optical refractive index, modal coupling and gain-loss distribution. As we will discuss in this chapter, the unconventional properties of exceptional points and parity-time symmetry realized in silicon photonics have created new opportunities for ultrasensitive sensors, laser engineering, control of light propagation, topological mode conversion, etc. The marriage between the quantum non-Hermiticity and classical silicon platforms not only spurs numerous studies on the fundamental physics, but also enriches the potential functionalities of the integrated photonic systems.      
### 37.Assessing Algorithmic Biases for Musical Version Identification  [ :arrow_down: ](https://arxiv.org/pdf/2109.15188.pdf)
>  Version identification (VI) systems now offer accurate and scalable solutions for detecting different renditions of a musical composition, allowing the use of these systems in industrial applications and throughout the wider music ecosystem. Such use can have an important impact on various stakeholders regarding recognition and financial benefits, including how royalties are circulated for digital rights management. In this work, we take a step toward acknowledging this impact and consider VI systems as socio-technical systems rather than isolated technologies. We propose a framework for quantifying performance disparities across 5 systems and 6 relevant side attributes: gender, popularity, country, language, year, and prevalence. We also consider 3 main stakeholders for this particular information retrieval use case: the performing artists of query tracks, those of reference (original) tracks, and the composers. By categorizing the recordings in our dataset using such attributes and stakeholders, we analyze whether the considered VI systems show any implicit biases. We find signs of disparities in identification performance for most of the groups we include in our analyses. Moreover, we also find that learning- and rule-based systems behave differently for some attributes, which suggests an additional dimension to consider along with accuracy and scalability when evaluating VI systems. Lastly, we share our dataset with attribute annotations to encourage VI researchers to take these aspects into account while building new systems.      
### 38.Fly Out The Window: Exploiting Discrete-Time Flatness for Fast Vision-Based Multirotor Flight  [ :arrow_down: ](https://arxiv.org/pdf/2109.15174.pdf)
>  Current control design for fast vision-based flight tends to rely on high-rate, high-dimensional and perfect state estimation. This is challenging in real-world environments due to imperfect sensing and state estimation drift and noise. In this letter, we present an alternative control design that bypasses the need for a state estimate by exploiting discrete-time flatness. To the best of our knowledge, this is the first work to demonstrate that discrete-time flatness holds for the Euler discretization of multirotor dynamics. This allows us to design a controller using only a window of input and output information. We highlight in simulation how exploiting this property in control design can provide robustness to noisy output measurements (where estimating higher-order derivatives and the full state can be challenging). Fast vision-based navigation requires high performance flight despite possibly noisy high-rate real-time position estimation. In outdoor experiments, we show the application of discrete-time flatness to vision-based flight at speeds up to 10 m/s and how it can outperform controllers that hinge on accurate state estimation.      
### 39.High-Availability Clusters A Taxonomy, Review, and Future Directions  [ :arrow_down: ](https://arxiv.org/pdf/2109.15139.pdf)
>  The delivery of key services in domains ranging from finance and manufacturing to healthcare and transportation is underpinned by a rapidly growing number of mission-critical enterprise applications. Ensuring the continuity of these complex applications requires the use of software-managed infrastructures called high-availability clusters (HACs). HACs employ sophisticated techniques to monitor the health of key enterprise application layers and of the resources they use, and to seamlessly restart or relocate application components after failures. In this paper, we first describe the manifold uses of HACs to protect essential layers of a critical application and present the architecture of high availability clusters. We then propose a taxonomy that covers all key aspects of HACs -- deployment patterns, application areas, types of cluster, topology, cluster management, failure detection and recovery, consistency and integrity, and data synchronisation; and we use this taxonomy to provide a comprehensive survey of the end-to-end software solutions available for the HAC deployment of enterprise applications. Finally, we discuss the limitations and challenges of existing HAC solutions, and we identify opportunities for future research in the area.      
### 40.Finite-time and Fixed-time Convergence in Continuous-time Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2109.15064.pdf)
>  It is known that the gradient method can be viewed as a dynamic system where various iterative schemes can be designed as a part of the closed loop system with desirable properties. In this paper, the finite-time and fixed-time convergence in continuous-time optimization are mainly considered. By the advantage of sliding mode control, a finite-time gradient method is proposed, whose convergence time is dependent on initial conditions. To make the convergence time robust to initial conditions, two different designs of fixed-time gradient methods are then provided. One is designed using the property of sine function, whose convergence time is dependent on the frequency of a sine function. The other one is designed using the property of Mittag-Leffler function, whose convergence time is determined by the first positive zero of a Mittag-Leffler function. All the results are extended to more general cases and finally demonstrated by some dedicated simulation examples.      
### 41.Fine-tuning wav2vec2 for speaker recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.15053.pdf)
>  This paper explores applying the wav2vec2 framework to speaker recognition instead of speech recognition. We study the effectiveness of the pre-trained weights on the speaker recognition task, and how to pool the wav2vec2 output sequence into a fixed-length speaker embedding. To adapt the framework to speaker recognition, we propose a single-utterance classification variant with CE or AAM softmax loss, and an utterance-pair classification variant with BCE loss. Our best performing variant, w2v2-aam, achieves a 1.88% EER on the extended voxceleb1 test set compared to 1.69% EER with an ECAPA-TDNN baseline. Code is available at <a class="link-external link-https" href="https://github.com/nikvaessen/w2v2-speaker" rel="external noopener nofollow">this https URL</a>.      
### 42.Automated Workers Ergonomic Risk Assessment in Manual Material Handling using sEMG Wearable Sensors and Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.15036.pdf)
>  Manual material handling tasks have the potential to be highly unsafe from an ergonomic viewpoint. Safety inspections to monitor body postures can help mitigate ergonomic risks of material handling. However, the real effect of awkward muscle movements, strains, and excessive forces that may result in an injury may not be identified by external cues. This paper evaluates the ability of surface electromyogram (EMG)-based systems together with machine learning algorithms to automatically detect body movements that may harm muscles in material handling. The analysis utilized a lifting equation developed by the U.S. National Institute for Occupational Safety and Health (NIOSH). This equation determines a Recommended Weight Limit, which suggests the maximum acceptable weight that a healthy worker can lift and carry as well as a Lifting Index value to assess the risk extent. Four different machine learning models, namely Decision Tree, Support Vector Machine, K-Nearest Neighbor, and Random Forest are developed to classify the risk assessments calculated based on the NIOSH lifting equation. The sensitivity of the models to various parameters is also evaluated to find the best performance using each algorithm. Results indicate that Decision Tree models have the potential to predict the risk level with close to 99.35% accuracy.      
### 43.Can 5G NR-Light Operate at Millimeter Waves? Design Guidelines for Mid-Market IoT Use Cases  [ :arrow_down: ](https://arxiv.org/pdf/2109.15017.pdf)
>  5th generation (5G) systems have been designed with three main objectives in mind: increasing throughput, reducing latency, and enabling reliable communications. To meet these (often conflicting) constraints, in 2019 the 3GPP released a set of specifications for 5G NR, one of the main innovations being the support for communications in the millimeter wave (mmWave) bands. However, how to implement lower complexity, energy efficient, mid-market Internet of Things (IoT) applications is still an on-going investigation, currently led by the 3GPP which is extending the NR standard with NR-Light specifications to support devices with reduced capabilities (REDCAP). In this paper we investigate the feasibility of operating such devices at mmWaves, in view of the requirements and expectations for NR- Light applications in terms of cost and complexity, throughput, and latency. Contributions of this paper are threefold. First, we il- lustrate the potential of mmWave communication for mid-market IoT use cases. Then, we highlight and motivate the design of an NR-Light candidate interface derived from NR by a selection of features. Finally, we demonstrate the technical soundness of this interface in an industrial IoT setup via simulations.      
### 44.Xenakis: Experimenting with Data, Cities, and Sounds  [ :arrow_down: ](https://arxiv.org/pdf/2109.14992.pdf)
>  In this work, we report on the results and lessons learned from different disciplines while researching the loosely-defined problem of hearing a city. We present Xenakis, a tool for the musification of urban data, which is able to capture some features of a city's topology through the distribution of street orientations, and turn it into a (very) small piece of music, a loop, which can be used as building block for compositions. Besides providing complementary visual and auditory channels to interface with this data, we also allow the piping of \textit{midi} signals to other applications. This concept was developed by visualization researchers collaborating with musicians using design study methodologies in an open-ended way. Our results include musical tracks, and we take advantage of the scope of alt.VIS to communicate our research in a sincere, humorous, and engaging format.      
### 45.Moving Object Detection for Event-based vision using Graph Spectral Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2109.14979.pdf)
>  Moving object detection has been a central topic of discussion in computer vision for its wide range of applications like in self-driving cars, video surveillance, security, and enforcement. Neuromorphic Vision Sensors (NVS) are bio-inspired sensors that mimic the working of the human eye. Unlike conventional frame-based cameras, these sensors capture a stream of asynchronous 'events' that pose multiple advantages over the former, like high dynamic range, low latency, low power consumption, and reduced motion blur. However, these advantages come at a high cost, as the event camera data typically contains more noise and has low resolution. Moreover, as event-based cameras can only capture the relative changes in brightness of a scene, event data do not contain usual visual information (like texture and color) as available in video data from normal cameras. So, moving object detection in event-based cameras becomes an extremely challenging task. In this paper, we present an unsupervised Graph Spectral Clustering technique for Moving Object Detection in Event-based data (GSCEventMOD). We additionally show how the optimum number of moving objects can be automatically determined. Experimental comparisons on publicly available datasets show that the proposed GSCEventMOD algorithm outperforms a number of state-of-the-art techniques by a maximum margin of 30%.      
### 46.Capacity Enhancement for Reconfigurable Intelligent Surface-Aided Wireless Network: from Regular Array to Irregular Array  [ :arrow_down: ](https://arxiv.org/pdf/2109.14964.pdf)
>  Reconfigurable intelligent surface (RIS) is promising for future 6G wireless communications. However, the increased number of RIS elements results in the high overhead for channel acquisition and the non-negligible power consumption. Therefore, how to improve the system capacity with limited RIS elements is essential. Unlike the classical regular RIS whose elements are arranged on a regular grid, in this paper, we propose an irregular RIS structure. The key idea is to irregularly configure a given number of RIS elements on an enlarged surface, which provides extra spatial degrees of freedom compared with the regular RIS. In this way, the received signal power can be enhanced, and thus the system capacity can be improved. Then, we formulate a joint topology and precoding optimization problem to maximize the capacity for irregular RIS-aided communication systems. Accordingly, a joint optimization algorithm with low complexity is proposed to alternately optimize the RIS topology and the precoding design. Particularly, a tabu search-based method is used to design the irregular RIS topology, and a neighbor extraction-based cross-entropy method is introduced to optimize the precoding design. Simulation results demonstrate that, subject to the constraint of limited RIS elements, the proposed irregular RIS can significantly enhance the system capacity.      
### 47.A Fast Robust Numerical Continuation Solver to a Two-Dimensional Spectral Estimation Problem  [ :arrow_down: ](https://arxiv.org/pdf/2109.14926.pdf)
>  This paper presents a fast algorithm to solve a spectral estimation problem for two-dimensional random fields. The latter is formulated as a convex optimization problem with the Itakura-Saito pseudodistance as the objective function subject to the constraints of moment equations. We exploit the structure of the Hessian of the dual objective function in order to make possible a fast Newton solver. Then we incorporate the Newton solver to a predictor-corrector numerical continuation method which is able to produce a parametrized family of solutions to the moment equations. We have performed two sets of numerical simulations to test our algorithm and spectral estimator. The simulations on the frequency estimation problem shows that our spectral estimator outperforms the classical windowed periodograms in the case of two hidden frequencies and has a higher resolution. The other set of simulations on system identification indicates that the numerical continuation method is more robust than Newton's method alone in ill-conditioned instances.      
### 48.Stabilization Techniques for Iterative Algorithms in Compressed Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2109.14917.pdf)
>  Algorithms for signal recovery in compressed sensing (CS) are often improved by stabilization techniques, such as damping, or the less widely known so-called fractional approach, which is based on the expectation propagation (EP) framework. These procedures are used to increase the steady-state performance, i.e., the performance after convergence, or assure convergence, when this is otherwise not possible. In this paper, we give a thorough introduction and interpretation of several stabilization approaches. The effects of the stabilization procedures are examined and compared via numerical simulations and we show that a combination of several procedures can be beneficial for the performance of the algorithm.      
### 49.Forming a sparse representation for visual place recognition using a neurorobotic approach  [ :arrow_down: ](https://arxiv.org/pdf/2109.14916.pdf)
>  This paper introduces a novel unsupervised neural network model for visual information encoding which aims to address the problem of large-scale visual localization. Inspired by the structure of the visual cortex, the model (namely HSD) alternates layers of topologic sparse coding and pooling to build a more compact code of visual information. Intended for visual place recognition (VPR) systems that use local descriptors, the impact of its integration in a bio-inpired model for self-localization (LPMP) is evaluated. Our experimental results on the KITTI dataset show that HSD improves the runtime speed of LPMP by a factor of at least 2 and its localization accuracy by 10%. A comparison with CoHog, a state-of-the-art VPR approach, showed that our method achieves slightly better results.      
### 50.Learning Reflection Beamforming Codebooks for Arbitrary RIS and Non-Stationary Channels  [ :arrow_down: ](https://arxiv.org/pdf/2109.14909.pdf)
>  Reconfigurable intelligent surfaces (RIS) are expected to play an important role in future wireless communication systems. These surfaces typically rely on their reflection beamforming codebooks to reflect and focus the signal on the target receivers. Prior work has mainly considered pre-defined RIS beamsteering codebooks that do not adapt to the environment and hardware and lead to large beam training overhead. In this work, a novel deep reinforcement learning based framework is developed to efficiently construct the RIS reflection beam codebook. This framework adopts a multi-level design approach that transfers the learning between the multiple RIS subarrays, which speeds up the learning convergence and highly reduces the computational complexity for large RIS surfaces. The proposed approach is generic for co-located/distributed RIS surfaces with arbitrary array geometries and with stationary/non-stationary channels. Further, the developed solution does not require explicitly channel knowledge and adapts the codebook beams to the surrounding environment, user distribution, and hardware characteristics. Simulation results show that the proposed learning framework can learn optimized interaction codebooks within reasonable iterations. Besides, with only 6 beams, the learned codebook outperforms a 256-beam DFT codebook, which significantly reduces the beam training overhead.      
### 51.Impact of Channel Variation on One-Class Learning for Spoof Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.14900.pdf)
>  The value of Spoofing detection in increasing the reliability of the ASV system is unparalleled. In reality, however, the performance of countermeasure systems (CMs) degrades significantly due to channel variation. Multi-conditional training(MCT) is a well-established technique to handle such scenarios. However, "which data-feeding strategy is optimal for MCT?" is not known in the case of spoof detection. In this paper, various codec simulations were used to modify ASVspoof 2019 dataset, and assessments were done using data-feeding and mini-batching strategies to help address this question. Our experiments aim to test the efficacy of the various margin-based losses for training Resnet based models with LFCC front-end feature extractor to correctly classify the spoofed and bonafide samples degraded using codec simulations. Contrastingly to most of the works that focus mainly on architectures, this study highlights the relevance of the deemed-of-low-importance process of data-feeding and mini-batching to raise awareness of the need to refine it for better performance.      
### 52.HLIC: Harmonizing Optimization Metrics in Learned Image Compression by Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.14863.pdf)
>  Learned image compression is making good progress in recent years. Peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) are the two most popular evaluation metrics. As different metrics only reflect certain aspects of human perception, works in this field normally optimize two models using PSNR and MS-SSIM as loss function separately, which is suboptimal and makes it difficult to select the model with best visual quality or overall performance. Towards solving this problem, we propose to Harmonize optimization metrics in Learned Image Compression (HLIC) using online loss function adaptation by reinforcement learning. By doing so, we are able to leverage the advantages of both PSNR and MS-SSIM, achieving better visual quality and higher VMAF score. To our knowledge, our work is the first to explore automatic loss function adaptation for harmonizing optimization metrics in low level vision tasks like learned image compression.      
### 53.Terrain-Aware Foot Placement for Bipedal Locomotion Combining Model Predictive Control, Virtual Constraints, and the ALIP  [ :arrow_down: ](https://arxiv.org/pdf/2109.14862.pdf)
>  This paper draws upon three themes in the bipedal control literature to achieve highly agile, terrain-aware locomotion. By terrain aware, we mean the robot can use information on terrain slope and friction cone as supplied by state-of-the-art mapping and trajectory planning algorithms. The process starts with abstracting from the full dynamics of a Cassie 3D bipedal robot, an exact low-dimensional representation of its centroidal dynamics, parameterized by angular momentum. Under a piecewise planar terrain assumption, and the elimination of terms for the angular momentum about the robot's center of mass, the centroidal dynamics become linear and has dimension four. Four-step-horizon model predictive control (MPC) of the centroidal dynamics provides step-to-step foot placement commands. Importantly, we also include the intra-step dynamics at 10 ms intervals so that realistic terrain-aware constraints on robot's evolution can be imposed in the MPC formulation. The output of the MPC is directly implemented on Cassie through the method of virtual constraints. In experiments, we validate the performance of our control strategy for the robot on inclined and stationary terrain, both indoors on a treadmill and outdoors on a hill.      
### 54.A system on chip for melanoma detection using FPGA-based SVM classifier  [ :arrow_down: ](https://arxiv.org/pdf/2109.14840.pdf)
>  Support Vector Machine (SVM) is a robust machine learning model that shows high accuracy with different classification problems, and is widely used for various embedded applications. However , implementation of embedded SVM classifiers is challenging, due to the inherent complicated computations required. This motivates implementing the SVM on hardware platforms for achieving high performance computing at low cost and power consumption. Melanoma is the most aggressive form of skin cancer that increases the mortality rate. We aim to develop an optimized embedded SVM classifier dedicated for a low-cost handheld device for early detection of melanoma at the primary healthcare. In this paper, we propose a hardware/software co-design for implementing the SVM classifier onto FPGA to realize melanoma detection on a chip. The implemented SVM on a recent hybrid FPGA (Zynq) platform utilizing the modern UltraFast High-Level Synthesis design methodology achieves efficient melanoma classification on chip. The hardware implementation results demonstrate classification accuracy of 97.9%, and a significant hardware acceleration rate of 21 with only 3% resources utilization and 1.69W for power consumption. These results show that the implemented system on chip meets crucial embedded system constraints of high performance and low resources utilization, power consumption, and cost, while achieving efficient classification with high classification accuracy.      
### 55.GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.14813.pdf)
>  To achieve an accurate assessment of root canal therapy, a fundamental step is to perform tooth root segmentation on oral X-ray images, in that the position of tooth root boundary is significant anatomy information in root canal therapy evaluation. However, the fuzzy boundary makes the tooth root segmentation very challenging. In this paper, we propose a novel end-to-end U-Net like Group Transformer Network (GT U-Net) for the tooth root segmentation. The proposed network retains the essential structure of U-Net but each of the encoders and decoders is replaced by a group Transformer, which significantly reduces the computational cost of traditional Transformer architectures by using the grouping structure and the bottleneck structure. In addition, the proposed GT U-Net is composed of a hybrid structure of convolution and Transformer, which makes it independent of pre-training weights. For optimization, we also propose a shape-sensitive Fourier Descriptor (FD) loss function to make use of shape prior knowledge. Experimental results show that our proposed network achieves the state-of-the-art performance on our collected tooth root segmentation dataset and the public retina dataset DRIVE. Code has been released at <a class="link-external link-https" href="https://github.com/Kent0n-Li/GT-U-Net" rel="external noopener nofollow">this https URL</a>.      
### 56.Bitcoin Transaction Strategy Construction Based on Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.14789.pdf)
>  The emerging cryptocurrency market has lately received great attention for asset allocation due to its decentralization uniqueness. However, its volatility and brand new trading mode have made it challenging to devising an acceptable automatically-generating strategy. This study proposes a framework for automatic high-frequency bitcoin transactions based on a deep reinforcement learning algorithm-proximal policy optimization (PPO). The framework creatively regards the transaction process as actions, returns as awards and prices as states to align with the idea of reinforcement learning. It compares advanced machine learning-based models for static price predictions including support vector machine (SVM), multi-layer perceptron (MLP), long short-term memory (LSTM), temporal convolutional network (TCN), and Transformer by applying them to the real-time bitcoin price and the experimental results demonstrate that LSTM outperforms. Then an automatically-generating transaction strategy is constructed building on PPO with LSTM as the basis to construct the policy. Extensive empirical studies validate that the proposed method performs superiorly to various common trading strategy benchmarks for a single financial product. The approach is able to trade bitcoins in a simulated environment with synchronous data and obtains a 31.67% more return than that of the best benchmark, improving the benchmark by 12.75%. The proposed framework can earn excess returns through both the period of volatility and surge, which opens the door to research on building a single cryptocurrency trading strategy based on deep learning. Visualizations of trading the process show how the model handles high-frequency transactions to provide inspiration and demonstrate that it can be expanded to other financial products.      
### 57.A Novel Initialization Method for HybridUnderwater Optical Acoustic Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.14738.pdf)
>  To satisfy the high data rate requirement andreliable transmission demands in underwater scenarios, it isdesirable to construct an efficient hybrid underwater opticalacoustic network (UWOAN) architecture by considering the keyfeatures and critical needs of underwater terminals. In UWOANs,optical uplinks and acoustic downlinks are configured betweenunderwater nodes (UWNs) and the base station (BS), wherethe optical beam transmits the high data rate traffic to theBS, while the acoustic waves carry the control information torealize the network management. In this paper, we focus onsolving the network initializing problem in UWOANs, which isa challenging task due to the lack of GPS service and limiteddevice payload in underwater environments. To this end, weleverage acoustic waves for node localization and propose anovel network initialization method, which consists of UWNidentification, discovery, localization, as well as decomposition.Numerical simulations are also conducted to verify the proposedinitialization method.      
### 58.Tiny-CRNN: Streaming Wakeword Detection In A Low Footprint Setting  [ :arrow_down: ](https://arxiv.org/pdf/2109.14725.pdf)
>  In this work, we propose Tiny-CRNN (Tiny Convolutional Recurrent Neural Network) models applied to the problem of wakeword detection, and augment them with scaled dot product attention. We find that, compared to Convolutional Neural Network models, False Accepts in a 250k parameter budget can be reduced by 25% with a 10% reduction in parameter size by using models based on the Tiny-CRNN architecture, and we can get up to 32% reduction in False Accepts at a 50k parameter budget with 75% reduction in parameter size compared to word-level Dense Neural Network models. We discuss solutions to the challenging problem of performing inference on streaming audio with this architecture, as well as differences in start-end index errors and latency in comparison to CNN, DNN, and DNN-HMM models.      
### 59.Guaranteed Rejection-free Sampling Method Using Past Behaviours for Motion Planning of Autonomous Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.14687.pdf)
>  The paper presents a novel learning-based sampling strategy that guarantees rejection-free sampling of the free space in both biased and uniform conditions. Data of past configurations of the autonomous system performing a repetitive task is leveraged to estimate a non-parametric probabilistic description of the region of the free space where feasible solutions of the motion planning problem are likely to be found. The tuning parameters of the kernel density estimator -- the bandwidth and the kernel -- are then used to properly alter the description of the free space such that no sampled configuration can fall outside the original free space. <br>The paper demonstrates the proposed method on two case studies: the first showcases the sampling strategies on 2D historical data from real surface vessels, whereas the second applies the method on 3D drone data gathered from a real quadrotor system. Both instances show that the proposed biased and approximately uniform sampling schemes are able to guarantee rejection-free sampling of the considered workspaces.      
### 60.Segmentation of Roads in Satellite Images using specially modified U-Net CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2109.14671.pdf)
>  The image classification problem has been deeply investigated by the research community, with computer vision algorithms and with the help of Neural Networks. The aim of this paper is to build an image classifier for satellite images of urban scenes that identifies the portions of the images in which a road is located, separating these portions from the rest. Unlike conventional computer vision algorithms, convolutional neural networks (CNNs) provide accurate and reliable results on this task. Our novel approach uses a sliding window to extract patches out of the whole image, data augmentation for generating more training/testing data and lastly a series of specially modified U-Net CNNs. This proposed technique outperforms all other baselines tested in terms of mean F-score metric.      
### 61.Dynamic probabilistic predictable feature analysis for high dimensional temporal monitoring  [ :arrow_down: ](https://arxiv.org/pdf/2109.14666.pdf)
>  Dynamic statistical process monitoring methods have been widely studied and applied in modern industrial processes. These methods aim to extract the most predictable temporal information and develop the corresponding dynamic monitoring schemes. However, measurement noise is widespread in real-world industrial processes, and ignoring its effect will lead to sub-optimal modeling and monitoring performance. In this article, a probabilistic predictable feature analysis (PPFA) is proposed for high dimensional time series modeling, and a multi-step dynamic predictive monitoring scheme is developed. The model parameters are estimated with an efficient expectation-maximum algorithm, where the genetic algorithm and Kalman filter are designed and incorporated. Further, a novel dynamic statistical monitoring index, Dynamic Index, is proposed as an important supplement of $\text{T}^2$ and $\text{SPE}$ to detect dynamic anomalies. The effectiveness of the proposed algorithm is demonstrated via its application on the three-phase flow facility and a medium speed coal mill.      
### 62.FathomNet: A global underwater image training set for enabling artificial intelligence in the ocean  [ :arrow_down: ](https://arxiv.org/pdf/2109.14646.pdf)
>  Ocean-going platforms are integrating high-resolution camera feeds for observation and navigation, producing a deluge of visual data. The volume and rate of this data collection can rapidly outpace researchers' abilities to process and analyze them. Recent advances in machine learning enable fast, sophisticated analysis of visual data, but have had limited success in the oceanographic world due to lack of dataset standardization, sparse annotation tools, and insufficient formatting and aggregation of existing, expertly curated imagery for use by data scientists. To address this need, we have built FathomNet, a public platform that makes use of existing (and future), expertly curated data. Initial efforts have leveraged MBARI's Video Annotation and Reference System and annotated deep sea video database, which has more than 7M annotations, 1M framegrabs, and 5k terms in the knowledgebase, with additional contributions by National Geographic Society (NGS) and NOAA's Office of Ocean Exploration and Research. FathomNet has over 100k localizations of 1k midwater and benthic classes, and contains iconic and non-iconic views of marine animals, underwater equipment, debris, etc. We will demonstrate how machine learning models trained on FathomNet data can be applied across different institutional video data, (e.g., NGS' Deep Sea Camera System and NOAA's ROV Deep Discoverer), and enable automated acquisition and tracking of midwater animals using MBARI's ROV MiniROV. As FathomNet continues to develop and incorporate more image data from other oceanographic community members, this effort will enable scientists, explorers, policymakers, storytellers, and the public to understand and care for our ocean.      
