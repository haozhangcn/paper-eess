# ArXiv eess --Tue, 19 Oct 2021
### 1.DBSegment: Fast and robust segmentation of deep brain structures -- Evaluation of transportability across acquisition domains  [ :arrow_down: ](https://arxiv.org/pdf/2110.09473.pdf)
>  Segmenting deep brain structures from magnetic resonance images is important for patient diagnosis, surgical planning, and research. Most current state-of-the-art solutions follow a segmentation-by-registration approach, where subject MRIs are mapped to a template with well-defined segmentations. However, registration-based pipelines are time-consuming, thus, limiting their clinical use. This paper uses deep learning to provide a robust and efficient deep brain segmentation solution. The method consists of a pre-processing step to conform all MRI images to the same orientation, followed by a convolutional neural network using the nnU-Net framework. We use a total of 14 datasets from both research and clinical collections. Of these, seven were used for training and validation and seven were retained for independent testing. We trained the network to segment 30 deep brain structures, as well as a brain mask, using labels generated from a registration-based approach. We evaluated the generalizability of the network by performing a leave-one-dataset-out cross-validation, and extensive testing on external datasets. Furthermore, we assessed cross-domain transportability by evaluating the results separately on different domains. We achieved an average DSC of 0.89 $\pm$ 0.04 on the independent testing datasets when compared to the registration-based gold standard. On our test system, the computation time decreased from 42 minutes for a reference registration-based pipeline to 1 minute. Our proposed method is fast, robust, and generalizes with high reliability. It can be extended to the segmentation of other brain structures. The method is publicly available on GitHub, as well as a pip package for convenient usage.      
### 2.MRI Recovery with A Self-calibrated Denoiser  [ :arrow_down: ](https://arxiv.org/pdf/2110.09418.pdf)
>  Plug-and-play (PnP) methods that employ application-specific denoisers have been proposed to solve inverse problems, including MRI reconstruction. However, training application-specific denoisers is not feasible for many applications due to the lack of training data. In this work, we propose a PnP-inspired recovery method that does not require data beyond the single, incomplete set of measurements. The proposed method, called recovery with a self-calibrated denoiser (ReSiDe), trains the denoiser from the patches of the image being recovered. The denoiser training and a call to the denoising subroutine are performed in each iteration of a PnP algorithm, leading to a progressive refinement of the reconstructed image. For validation, we compare ReSiDe with a compressed sensing-based method and a PnP method with BM3D denoising using single-coil MRI brain data.      
### 3.Automatic Detection of COVID-19 and Pneumonia from Chest X-Ray using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.09384.pdf)
>  In this study, a dataset of X-ray images from patients with common viral pneumonia, bacterial pneumonia, confirmed Covid-19 disease was utilized for the automatic detection of the Coronavirus disease. The point of the investigation is to assess the exhibition of cutting edge convolutional neural system structures proposed over the ongoing years for clinical picture order. In particular, the system called Transfer Learning was received. With transfer learning, the location of different variations from the norm in little clinical picture datasets is a reachable objective, regularly yielding amazing outcomes. The datasets used in this trial. Firstly, a collection of 24000 X-ray images includes 6000 images for confirmed Covid-19 disease,6000 confirmed common bacterial pneumonia and 6000 images of normal conditions. The information was gathered and expanded from the accessible X-Ray pictures on open clinical stores. The outcomes recommend that Deep Learning with X-Ray imaging may separate noteworthy biological markers identified with the Covid-19 sickness, while the best precision, affectability, and particularity acquired is 97.83%, 96.81%, and 98.56% individually.      
### 4.Planning of EM Skins for Improved Quality-of-Service in Urban Areas  [ :arrow_down: ](https://arxiv.org/pdf/2110.09376.pdf)
>  The optimal planning of electromagnetic skins (EMSs) installed on the building facades to enhance the received signal strength, thus the wireless coverage and/or the quality-of-service (QoS) in large-scale urban areas, is addressed. More specifically, a novel instance of the System-by-Design (SbD) paradigm is proposed towards the implementation of a smart electromagnetic environment (SEME) where low-cost passive static reflective skins are deployed to enhance the level of the power received within selected regions-of-interest (RoIs). Thanks to the ad-hoc customization of the SbD functional blocks, which includes the exploitation of a digital twin (DT) for the accurate yet fast assessment of the wireless coverage condition, effective solutions are yielded. Numerical results, dealing with real-world test-beds, are shown to assess the capabilities, the potentialities, and the current limitations of the proposed EMSs planning strategy.      
### 5.An Analysis and Implementation of the HDR+ Burst Denoising Method  [ :arrow_down: ](https://arxiv.org/pdf/2110.09354.pdf)
>  HDR+ is an image processing pipeline presented by Google in 2016. At its core lies a denoising algorithm that uses a burst of raw images to produce a single higher quality image. Since it is designed as a versatile solution for smartphone cameras, it does not necessarily aim for the maximization of standard denoising metrics, but rather for the production of natural, visually pleasing images. In this article, we specifically discuss and analyze the HDR+ burst denoising algorithm architecture and the impact of its various parameters. With this publication, we provide an open source Python implementation of the algorithm, along with an interactive demo.      
### 6.On the Design of Modular Reflecting EM Skins for Enhanced Urban Wireless Coverage  [ :arrow_down: ](https://arxiv.org/pdf/2110.09350.pdf)
>  The design of modular, passive, and static artificial metasurfaces to be used as electromagnetic skins (EMSs) of buildings for improving the coverage in urban millimeter-wave communication scenarios is addressed. Towards this end, an ad-hoc design strategy is presented to determine optimal trade-off implementative solutions that assure a suitable coverage of the areas of interest, where the signal from the base station is too weak, with the minimum complexity. More specifically, the admissible surface in the building facade is first partitioned into tiles, which are the minimum-size elements of the artificial coating (i.e., the building block of an EMS). Then, the search for the optimal EMS layout (i.e., the minimum number and the positions of the tiles to be installed) is carried out with a binary multi-objective optimization method. Representative numerical results are reported and discussed to point out the features and the potentialities of the EMS solution in the smart electromagnetic environment (SEME) as well as the effectiveness of the proposed design method.      
### 7.Incremental Cross-Domain Adaptation for Robust Retinopathy Screening via Bayesian Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.09319.pdf)
>  Retinopathy represents a group of retinal diseases that, if not treated timely, can cause severe visual impairments or even blindness. Many researchers have developed autonomous systems to recognize retinopathy via fundus and optical coherence tomography (OCT) imagery. However, most of these frameworks employ conventional transfer learning and fine-tuning approaches, requiring a decent amount of well-annotated training data to produce accurate diagnostic performance. This paper presents a novel incremental cross-domain adaptation instrument that allows any deep classification model to progressively learn abnormal retinal pathologies in OCT and fundus imagery via few-shot training. Furthermore, unlike its competitors, the proposed instrument is driven via a Bayesian multi-objective function that not only enforces the candidate classification network to retain its prior learned knowledge during incremental training but also ensures that the network understands the structural and semantic relationships between previously learned pathologies and newly added disease categories to effectively recognize them at the inference stage. The proposed framework, evaluated on six public datasets acquired with three different scanners to screen thirteen retinal pathologies, outperforms the state-of-the-art competitors by achieving an overall accuracy and F1 score of 0.9826 and 0.9846, respectively.      
### 8.Vit-GAN: Image-to-image Translation with Vision Transformes and Conditional GANS  [ :arrow_down: ](https://arxiv.org/pdf/2110.09305.pdf)
>  In this paper, we have developed a general-purpose architecture, Vit-Gan, capable of performing most of the image-to-image translation tasks from semantic image segmentation to single image depth perception. This paper is a follow-up paper, an extension of generator-based model [1] in which the obtained results were very promising. This opened the possibility of further improvements with adversarial architecture. We used a unique vision transformers-based generator architecture and Conditional GANs(cGANs) with a Markovian Discriminator (PatchGAN) (<a class="link-external link-https" href="https://github.com/YigitGunduc/vit-gan" rel="external noopener nofollow">this https URL</a>). In the present work, we use images as conditioning arguments. It is observed that the obtained results are more realistic than the commonly used architectures.      
### 9.Comparative Analysis of Deep Learning Algorithms for Classification of COVID-19 X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.09294.pdf)
>  The Coronavirus was first emerged in December, in the city of China named Wuhan in 2019 and spread quickly all over the world. It has very harmful effects all over the global economy, education, social, daily living and general health of humans. To restrict the quick expansion of the disease initially, main difficulty is to explore the positive corona patients as quickly as possible. As there are no automatic tool kits accessible the requirement for supplementary diagnostic tools has risen up. Previous studies have findings acquired from radiological techniques proposed that this kind of images have important details related to the coronavirus. The usage of modified Artificial Intelligence (AI) system in combination with radio-graphical images can be fruitful for the precise and exact solution of this virus and can also be helpful to conquer the issue of deficiency of professional physicians in distant villages. In our research, we analyze the different techniques for the detection of COVID-19 using X-Ray radiographic images of the chest, we examined the different pre-trained CNN models AlexNet, VGG-16, MobileNet-V2, SqeezeNet, ResNet-34, ResNet-50 and COVIDX-Net to correct analytics for classification system of COVID-19. Our study shows that the pre trained CNN Model with ResNet-34 technique gives the higher accuracy rate of 98.33, 96.77% precision, and 98.36 F1-score, which is better than other CNN techniques. Our model may be helpful for the researchers to fine train the CNN model for the the quick screening of COVID patients.      
### 10.CT-SGAN: Computed Tomography Synthesis GAN  [ :arrow_down: ](https://arxiv.org/pdf/2110.09288.pdf)
>  Diversity in data is critical for the successful training of deep learning models. Leveraged by a recurrent generative adversarial network, we propose the CT-SGAN model that generates large-scale 3D synthetic CT-scan volumes ($\geq 224\times224\times224$) when trained on a small dataset of chest CT-scans. CT-SGAN offers an attractive solution to two major challenges facing machine learning in medical imaging: a small number of given i.i.d. training data, and the restrictions around the sharing of patient data preventing to rapidly obtain larger and more diverse datasets. We evaluate the fidelity of the generated images qualitatively and quantitatively using various metrics including Fréchet Inception Distance and Inception Score. We further show that CT-SGAN can significantly improve lung nodule detection accuracy by pre-training a classifier on a vast amount of synthetic data.      
### 11.Building a Smart EM Environment -- AI-Enhanced Aperiodic Micro-Scale Design of Passive EM Skins  [ :arrow_down: ](https://arxiv.org/pdf/2110.09183.pdf)
>  An innovative process for the design of static passive smart skins (SPSSs) is proposed to take into account, within the synthesis, the electromagnetic (EM) interactions due to their finite (macro-level) size and aperiodic (micro-scale) layouts. Such an approach leverages on the combination of an inverse source (IS) formulation, to define the SPSS surface currents, and of an instance of the System-by-Design paradigm, to synthesize the unit cell (UC) descriptors suitable for supporting these currents. As for this latter step, an enhanced Artificial Intelligence (IA)-based digital twin (DT) is built to efficiently and reliably predict the relationships among the UCs and the non-uniform coupling effects arising when the UCs are irregularly assembled to build the corresponding SPSS. Towards this end and unlike state-of-the-art approaches, an aperiodic finite small-scale model of the SPSS is derived to generate the training database for the DT implementation. A set of representative numerical experiments, dealing with different radiation objectives and smart skin apertures, is reported to assess the reliability of the conceived design process and to illustrate the radiation features of the resulting layouts, validated with accurate full-wave simulations, as well.      
### 12.Tackling the Score Shift in Cross-Lingual Speaker Verification by Exploiting Language Information  [ :arrow_down: ](https://arxiv.org/pdf/2110.09150.pdf)
>  This paper contains a post-challenge performance analysis on cross-lingual speaker verification of the IDLab submission to the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC-21). We show that current speaker embedding extractors consistently underestimate speaker similarity in within-speaker cross-lingual trials. Consequently, the typical training and scoring protocols do not put enough emphasis on the compensation of intra-speaker language variability. We propose two techniques to increase cross-lingual speaker verification robustness. First, we enhance our previously proposed Large-Margin Fine-Tuning (LM-FT) training stage with a mini-batch sampling strategy which increases the amount of intra-speaker cross-lingual samples within the mini-batch. Second, we incorporate language information in the logistic regression calibration stage. We integrate quality metrics based on soft and hard decisions of a VoxLingua107 language identification model. The proposed techniques result in a 11.7% relative improvement over the baseline model on the VoxSRC-21 test set and contributed to our third place finish in the corresponding challenge.      
### 13.Body Part Regression for CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.09148.pdf)
>  One of the greatest challenges in the medical imaging domain is to successfully transfer deep learning models into clinical practice. Since models are often trained on a specific body region, a robust transfer into the clinic necessitates the selection of images with body regions that fit the algorithm to avoid false-positive predictions in unknown regions. Due to the insufficient and inaccurate nature of manually-defined imaging meta-data, automated body part recognition is a key ingredient towards the broad and reliable adoption of medical deep learning models. While some approaches to this task have been presented in the past, building and evaluating robust algorithms for fine-grained body part recognition remains challenging. So far, no easy-to-use method exists to determine the scanned body range of medical Computed Tomography (CT) volumes. In this thesis, a self-supervised body part regression model for CT volumes is developed and trained on a heterogeneous collection of CT studies. Furthermore, it is demonstrated how the algorithm can contribute to the robust and reliable transfer of medical models into the clinic. Finally, easy application of the developed method is ensured by integrating it into the medical platform toolkit Kaapana and providing it as a python package at <a class="link-external link-https" href="https://github.com/MIC-DKFZ/BodyPartRegression" rel="external noopener nofollow">this https URL</a> .      
### 14.GAN-based disentanglement learning for chest X-ray rib suppression  [ :arrow_down: ](https://arxiv.org/pdf/2110.09134.pdf)
>  Clinical evidence has shown that rib-suppressed chest X-rays (CXRs) can improve the reliability of pulmonary disease diagnosis. However, previous approaches on generating rib-suppressed CXR face challenges in preserving details and eliminating rib residues. We hereby propose a GAN-based disentanglement learning framework called Rib Suppression GAN, or RSGAN, to perform rib suppression by utilizing the anatomical knowledge embedded in unpaired computed tomography (CT) images. In this approach, we employ a residual map to characterize the intensity difference between CXR and the corresponding rib-suppressed result. To predict the residual map in CXR domain, we disentangle the image into structure- and contrast-specific features and transfer the rib structural priors from digitally reconstructed radiographs (DRRs) computed by CT. Furthermore, we employ additional adaptive loss to suppress rib residue and preserve more details. We conduct extensive experiments based on 1,673 CT volumes, and four benchmarking CXR datasets, totaling over 120K images, to demonstrate that (i) our proposed RSGAN achieves superior image quality compared to the state-of-the-art rib suppression methods; (ii) combining CXR with our rib-suppressed result leads to better performance in lung disease classification and tuberculosis area detection.      
### 15.Cyclic Prefix (CP) Jamming Against Eavesdropping Relays in OFDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.09130.pdf)
>  Cooperative communication has been widely used to provide spatial diversity benefits for low-end user equipments, especially in ad hoc and wireless sensor networks. However, the lack of strong authentication mechanisms in these networks leaves them prone to eavesdropping relays. In this paper, we propose a secure orthogonal frequency division multiplexing (OFDM) transmission scheme, where the destination node transmits a jamming signal over the cyclic prefix (CP) duration of the received signal. Simulation results verify that as long as at least a part of the jamming signal falls to the actual data portion of the eavesdropping relay, it spreads through all the data symbols due to the fast Fourier transformation (FFT) operation, resulting in degraded interception at the eavesdropper.      
### 16.Salt and pepper noise removal method based on stationary Framelet transform with non-convex sparsity regularization  [ :arrow_down: ](https://arxiv.org/pdf/2110.09113.pdf)
>  Salt and pepper noise removal is a common inverse problem in image processing, and it aims to restore image information with high quality. Traditional salt and pepper denoising methods have two limitations. First, noise characteristics are often not described accurately. For example, the noise location information is often ignored and the sparsity of the salt and pepper noise is often described by L1 norm, which cannot illustrate the sparse variables clearly. Second, conventional methods separate the contaminated image into a recovered image and a noise part, thus resulting in recovering an image with unsatisfied smooth parts and detail parts. In this study, we introduce a noise detection strategy to determine the position of the noise, and a non-convex sparsity regularization depicted by Lp quasi-norm is employed to describe the sparsity of the noise, thereby addressing the first limitation. The morphological component analysis framework with stationary Framelet transform is adopted to decompose the processed image into cartoon, texture, and noise parts to resolve the second limitation. In this framework, the stationary Framelet regularizations with different parameters control the restoration of the cartoon and texture parts. In this way, the two parts are recovered separately to avoid mutual interference. Then, the alternating direction method of multipliers (ADMM) is employed to solve the proposed model. Finally, experiments are conducted to verify the proposed method and compare it with some current state-of-the-art denoising methods. The experimental results show that the proposed method can remove salt and pepper noise while preserving the details of the processed image.      
### 17.Synthetic Aperture Radar Image Change Detection via Siamese Adaptive Fusion Network  [ :arrow_down: ](https://arxiv.org/pdf/2110.09049.pdf)
>  Synthetic aperture radar (SAR) image change detection is a critical yet challenging task in the field of remote sensing image analysis. The task is non-trivial due to the following challenges: Firstly, intrinsic speckle noise of SAR images inevitably degrades the neural network because of error gradient accumulation. Furthermore, the correlation among various levels or scales of feature maps is difficult to be achieved through summation or concatenation. Toward this end, we proposed a siamese adaptive fusion network for SAR image change detection. To be more specific, two-branch CNN is utilized to extract high-level semantic features of multitemporal SAR images. Besides, an adaptive fusion module is designed to adaptively combine multiscale responses in convolutional layers. Therefore, the complementary information is exploited, and feature learning in change detection is further improved. Moreover, a correlation layer is designed to further explore the correlation between multitemporal images. Thereafter, robust feature representation is utilized for classification through a fully-connected layer with softmax. Experimental results on four real SAR datasets demonstrate that the proposed method exhibits superior performance against several state-of-the-art methods. Our codes are available at <a class="link-external link-https" href="https://github.com/summitgao/SAR_CD_SAFNet" rel="external noopener nofollow">this https URL</a>.      
### 18.An Adaptive-Importance-Sampling-Enhanced Bayesian Approach for Topology Estimation in an Unbalanced Power Distribution System  [ :arrow_down: ](https://arxiv.org/pdf/2110.09030.pdf)
>  The reliable operation of a power distribution system relies on a good prior knowledge of its topology and its system state. Although crucial, due to the lack of direct monitoring devices on the switch statuses, the topology information is often unavailable or outdated for the distribution system operators for real-time applications. Apart from the limited observability of the power distribution system, other challenges are the nonlinearity of the model, the complicated, unbalanced structure of the distribution system, and the scale of the system. To overcome the above challenges, this paper proposes a Bayesian-inference framework that allows us to simultaneously estimate the topology and the state of a three-phase, unbalanced power distribution system. Specifically, by using the very limited number of measurements available that are associated with the forecast load data, we efficiently recover the full Bayesian posterior distributions of the system topology under both normal and outage operation conditions. This is performed through an adaptive importance sampling procedure that greatly alleviates the computational burden of the traditional Monte-Carlo (MC)-sampling-based approach while maintaining a good estimation accuracy. The simulations conducted on the IEEE 123-bus test system and an unbalanced 1282-bus system reveal the excellent performances of the proposed method.      
### 19.Artificial Neural Network and Its Application Research Progress in Chemical Process  [ :arrow_down: ](https://arxiv.org/pdf/2110.09021.pdf)
>  Most chemical processes, such as distillation, absorption, extraction, and catalytic reactions, are extremely complex processes that are affected by multiple factors. The relationships between their input variables and output variables are non-linear, and it is difficult to optimize or control them using traditional methods. Artificial neural network (ANN) is a systematic structure composed of multiple neuron models. Its main function is to simulate multiple basic functions of the nervous system of living organisms. ANN can achieve nonlinear control without relying on mathematical models, and is especially suitable for more complex control objects. This article will introduce the basic principles and development history of artificial neural networks, and review its application research progress in chemical process control, fault diagnosis, and process optimization.      
### 20.AoA Estimation for OAM Communication Systems With Mode-Frequency Multi-Time ESPRIT Method  [ :arrow_down: ](https://arxiv.org/pdf/2110.09020.pdf)
>  Radio orbital angular momentum (OAM) communications require accurate alignment between the transmit and receive beam directions. Accordingly, a key feature of OAM receivers is the ability to reliably estimate the angle of arrival (AoA) of multi-mode OAM beams. Considering the limitations of existing AoA estimation techniques, in this paper, we propose an easier-to-implement AoA estimation method based on applying multiple times the estimating signal parameters via rotational invariance techniques (ESPRIT) algorithm to the received training signals in OAM mode and frequency domains, which is denoted as the mode-frequency (M-F) multi-time (MT)-ESPRIT algorithm. With this method, the misalignment error of real OAM channels can be greatly reduced and the performance approaches that of ideally aligned OAM channels.      
### 21.Similarity-and-Independence-Aware Beamformer with Iterative Casting and Boost Start for Target Source Extraction Using Reference  [ :arrow_down: ](https://arxiv.org/pdf/2110.09019.pdf)
>  Target source extraction is significant for improving human speech intelligibility and the speech recognition performance of computers. This study describes a method for target source extraction, called the similarity-and-independence-aware beamformer (SIBF). The SIBF extracts the target source using a rough magnitude spectrogram as the reference signal. The advantage of the SIBF is that it can obtain a more accurate signal than the spectrogram generated by target-enhancing methods such as speech enhancement based on deep neural networks. For the extraction, we extend the framework of deflationary independent component analysis (ICA) by considering the similarities between the reference and extracted target sources, in addition to the mutual independence of all the potential sources. To solve the extraction problem by maximum-likelihood estimation, we introduce three source models that can reflect the similarities. The major contributions of this study are as follows. First, the extraction performance is improved using two methods, namely boost start for faster convergence and iterative casting for generating a more accurate reference. The effectiveness of these methods is verified through experiments using the CHiME3 dataset. Second, a concept of a fixed point pertaining to accuracy is developed. This concept facilitates understanding the relationship between the reference and SIBF output in terms of accuracy. Third, a unified formulation of the SIBF and mask-based beamformer is realized to apply the expertise of conventional BFs to the SIBF. The findings of this study can also improve the performance of the SIBF and promote research on ICA and conventional beamformers. <br>Index Terms: beamformer, independent component analysis, source separation, speech enhancement, target source extraction      
### 22.Navigation of a UAV Equipped with a Reconfigurable Intelligent Surface for LoS Wireless Communication with a Ground Vehicle  [ :arrow_down: ](https://arxiv.org/pdf/2110.09012.pdf)
>  Unmanned aerial vehicles (UAVs) have been successfully adopted to enhance the flexibility and robustness of wireless communication networks. And recently, the reconfigurable intelligent surface (RIS) technology has been paid increasing attention to improve the throughput of the fifth-generation (5G) millimeter-wave (mmWave) wireless communication. In this work, we propose an RIS-outfitted UAV (RISoUAV) to secure an uninterrupted line-of-sight (LoS) link with a ground moving target (MT). The MT can be an emergency ambulance and need a secure wireless communication link for continuous monitoring and diagnosing the health condition of a patient, which is vital for delivering critical patient care. In this light, real-time communication is required for sending various clinical multimedia data including videos, medical images, and vital signs. This significant target is achievable thanks to the 5G wireless communication assisted with RISoUAV. A two-stage optimization method is proposed to optimize the RISoUAV trajectory limited to UAV motion and LoS constraints. At the first stage, the optimal tube path of the RISoUAV is determined by taking into account the energy consumption, instant LoS link, and UAV speed/acceleration constraints. At the second stage, an accurate RISoUAV trajectory is obtained by considering the communication channel performance and passive beamforming. Simulation results show the accuracy and effectiveness of the method.      
### 23.Unsupervised Learned Kalman Filtering  [ :arrow_down: ](https://arxiv.org/pdf/2110.09005.pdf)
>  In this paper we adapt KalmanNet, which is a recently pro-posed deep neural network (DNN)-aided system whose architecture follows the operation of the model-based Kalman filter (KF), to learn its mapping in an unsupervised manner, i.e., without requiring ground-truth states. The unsupervised adaptation is achieved by exploiting the hybrid model-based/data-driven architecture of KalmanNet, which internally predicts the next observation as the KF does. These internal features are then used to compute the loss rather than the state estimate at the output of the system. With the capability of unsupervised learning, one can use KalmanNet not only to track the hidden state, but also to adapt to variations in the state space (SS) model. We numerically demonstrate that when the noise statistics are unknown, unsupervised KalmanNet achieves a similar performance to KalmanNet with supervised learning. We also show that we can adapt a pre-trained KalmanNet to changing SS models without providing additional data thanks to the unsupervised capabilities.      
### 24.Supervised Metric Learning for Music Structure Feature  [ :arrow_down: ](https://arxiv.org/pdf/2110.09000.pdf)
>  Music structure analysis (MSA) methods traditionally search for musically meaningful patterns in audio: homogeneity, repetition, novelty, and segment-length regularity. Hand-crafted audio features such as MFCCs or chromagrams are often used to elicit these patterns. However, with more annotations of section labels (e.g., verse, chorus, and bridge) becoming available, one can use supervised feature learning to make these patterns even clearer and improve MSA performance. To this end, we take a supervised metric learning approach: we train a deep neural network to output embeddings that are near each other for two spectrogram inputs if both have the same section type (according to an annotation), and otherwise far apart. We propose a batch sampling scheme to ensure the labels in a training pair are interpreted meaningfully. The trained model extracts features that can be used in existing MSA algorithms. In evaluations with three datasets (HarmonixSet, SALAMI, and RWC), we demonstrate that using the proposed features can improve a traditional MSA algorithm significantly in both intra- and cross-dataset scenarios.      
### 25.Improving Robustness of Reinforcement Learning for Power System Control with Adversarial Training  [ :arrow_down: ](https://arxiv.org/pdf/2110.08956.pdf)
>  Due to the proliferation of renewable energy and its intrinsic intermittency and stochasticity, current power systems face severe operational challenges. Data-driven decision-making algorithms from reinforcement learning (RL) offer a solution towards efficiently operating a clean energy system. Although RL algorithms achieve promising performance compared to model-based control models, there has been limited investigation of RL robustness in safety-critical physical systems. In this work, we first show that several competition-winning, state-of-the-art RL agents proposed for power system control are vulnerable to adversarial attacks. Specifically, we use an adversary Markov Decision Process to learn an attack policy, and demonstrate the potency of our attack by successfully attacking multiple winning agents from the Learning To Run a Power Network (L2RPN) challenge, under both white-box and black-box attack settings. We then propose to use adversarial training to increase the robustness of RL agent against attacks and avoid infeasible operational decisions. To the best of our knowledge, our work is the first to highlight the fragility of grid control RL algorithms, and contribute an effective defense scheme towards improving their robustness and security.      
### 26.Joint SCSP-LROM: A novel approach to detect Cerebrovascular Anomalies from EEG signals  [ :arrow_down: ](https://arxiv.org/pdf/2110.08942.pdf)
>  It has always been a big challenge to identify subtle changes in Electroencephalogram (EEG) signals. Minor differences often lead to vital decisions, for example, which grade a certain tumour belong to or whether a haemorrhage can result in benign blood clots or cancerous ones. In recent studies on brain computer interfaces (BCIs), one of the biggest challenges is recovering maximum information for realistic predictions. In order to choose EEG channels with highest accuracy, a novel notion of including sparsity in a modified common spatial pattern (CSP) algorithm is introduced here. Being influenced by the existing concept of compressed sensing, an optimization model is also developed alongside to recover the cosparse signal and retain maximum information. The state-of-the-art Joint Sparsity Induced Modified Common Spatial Pattern Algorithm and Low Rank Optimization Model (SCSP-LROM) developed here is capable of identifying and describing tumours and lesions in great detail at an overall accuracy of 96.3%.      
### 27.MARTINI: Smart Meter Driven Estimation of HVAC Schedules and Energy Savings Based on WiFi Sensing and Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2110.08927.pdf)
>  HVAC systems account for a significant portion of building energy use. Nighttime setback scheduling is an energy conservation measure where cooling and heating setpoints are increased and decreased respectively during unoccupied periods with the goal of obtaining energy savings. However, knowledge of a building's real occupancy is required to maximize the success of this measure. In addition, there is the need for a scalable way to estimate energy savings potential from energy conservation measures that is not limited by building specific parameters and experimental or simulation modeling investments. Here, we propose MARTINI, a sMARt meTer drIveN estImation of occupant-derived HVAC schedules and energy savings that leverages the ubiquity of energy smart meters and WiFi infrastructure in commercial buildings. We estimate the schedules by clustering WiFi-derived occupancy profiles and, energy savings by shifting ramp-up and setback times observed in typical/measured load profiles obtained by clustering smart meter energy profiles. Our case-study results with five buildings over seven months show an average of 8.1%-10.8% (summer) and 0.2%-5.9% (fall) chilled water energy savings when HVAC system operation is aligned with occupancy. We validate our method with results from building energy performance simulation (BEPS) and find that estimated average savings of MARTINI are within 0.9%-2.4% of the BEPS predictions. In the absence of occupancy information, we can still estimate potential savings from increasing ramp-up time and decreasing setback start time. In 51 academic buildings, we find savings potentials between 1%-5%.      
### 28.Sensor Scheduling for Linear Systems: A Covariance Tracking Approach  [ :arrow_down: ](https://arxiv.org/pdf/2110.08924.pdf)
>  We consider the classical sensor scheduling problem for linear systems where only one sensor is activated at each time. We show that the sensor scheduling problem has a close relation to the sensor design problem and the solution of a sensor schedule problem can be extracted from an equivalent sensor design problem. We propose a convex relaxation to the sensor design problem and a reference covariance trajectory is obtained from solving the relaxed sensor design problem. Afterwards, a covariance tracking algorithm is designed to obtain an approximate solution to the sensor scheduling problem using the reference covariance trajectory obtained from the sensor design problem. While the sensor scheduling problem is NP-hard, the proposed framework circumvents this computational complexity by decomposing this problem into a convex sensor design problem and a covariance tracking problem. We provide theoretical justification and a sub-optimality bound for the proposed method using dynamic programming. The proposed method is validated over several experiments portraying the efficacy of the framework.      
### 29.Neural-adaptive Stochastic Attitude Filter on SO(3)  [ :arrow_down: ](https://arxiv.org/pdf/2110.08889.pdf)
>  Successful control of a rigid-body rotating in three dimensional space requires accurate estimation of its attitude. The attitude dynamics are highly nonlinear and are posed on the Special Orthogonal Group $SO(3)$. In addition, measurements supplied by low-cost sensing units pose a challenge for the estimation process. This paper proposes a novel stochastic nonlinear neural-adaptive-based filter on $SO(3)$ for the attitude estimation problem. The proposed filter produces good results given measurements extracted from low-cost sensing units (e.g., IMU or MARG sensor modules). The filter is guaranteed to be almost semi-globally uniformly ultimately bounded in the mean square. In addition to Lie Group formulation, quaternion representation of the proposed filter is provided. The effectiveness of the proposed neural-adaptive filter is tested and evaluated in its discrete form under the conditions of large initialization error and high measurement uncertainties. keywords / index-terms: Neuro-adaptive, stochastic differential equations (SDEs), Brownian motion process, attitude estimator, Special Orthogonal Group, Unit-quaternion, SO(3), IMU, MARG.      
### 30.Dynamic Tolling for Inducing Socially Optimal Traffic Loads  [ :arrow_down: ](https://arxiv.org/pdf/2110.08879.pdf)
>  How to design tolls that induce socially optimal traffic loads with dynamically arriving travelers who make selfish routing decisions? We propose a two-timescale discrete-time stochastic dynamics that adaptively adjusts the toll prices on a parallel link network while accounting for the updates of traffic loads induced by the incoming and outgoing travelers and their route choices. The updates of loads and tolls in our dynamics have three key features: (i) The total demand of incoming and outgoing travelers is stochastically realized; (ii) Travelers are myopic and selfish in that they choose routes according to a perturbed best response given the current latency and tolls on parallel links; (iii) The update of tolls is at a slower timescale as compared to the the update of loads. We show that the loads and the tolls eventually concentrate in a neighborhood of the fixed point, which corresponds to the socially optimal load and toll price. Moreover, the fixed point load is also a stochastic user equilibrium with respect to the toll price. Our results are useful for traffic authorities to efficiently manage traffic loads in response to the arrival and departure of travelers.      
### 31.Deep Learning Based EDM Subgenre Classification using Mel-Spectrogram and Tempogram Features  [ :arrow_down: ](https://arxiv.org/pdf/2110.08862.pdf)
>  Along with the evolution of music technology, a large number of styles, or "subgenres," of Electronic Dance Music(EDM) have emerged in recent years. While the classification task of distinguishing between EDM and non-EDM has been often studied in the context of music genre classification, little work has been done on the more challenging EDM subgenre classification. The state-of-art model is based on extremely randomized trees and could be improved by deep learning methods. In this paper, we extend the state-of-art music auto-tagging model "short-chunkCNN+Resnet" to EDM subgenre classification, with the addition of two mid-level tempo-related feature representations, called the Fourier tempogram and autocorrelation tempogram. And, we explore two fusion strategies, early fusion and late fusion, to aggregate the two types of tempograms. We evaluate the proposed models using a large dataset consisting of 75,000 songs for 30 different EDM subgenres, and show that the adoption of deep learning models and tempo features indeed leads to higher classification accuracy.      
### 32.Graph Wedgelets: Adaptive Data Compression on Graphs based on Binary Wedge Partitioning Trees and Geometric Wavelets  [ :arrow_down: ](https://arxiv.org/pdf/2110.08843.pdf)
>  We introduce graph wegdelets - a tool for data compression on graphs based on the representation of signals by piecewise constant functions on adaptively generated binary wedge partitionings of a graph. For this, we transfer partitioning and compression techniques known for 2D images to general graph structures and develop discrete variants of continuous wedgelets and binary space partitionings. We prove that continuous results on best $m$-term approximation with geometric wavelets can be transferred to the discrete graph setting, and show that our wedgelet representation of graph signals can be encoded and implemented in a simple way. Finally, we illustrate that this graph based method can be applied for the compression of images as well.      
### 33.The General sampling theorem, Compressed sensing and a method of image sampling and reconstruction with sampling rates close to the theoretical limit  [ :arrow_down: ](https://arxiv.org/pdf/2110.08831.pdf)
>  The article addresses the problem of image sampling with minimal possible sampling rates and reviews the recent advances in sampling theory and methods: modern formulations of the sampling theorems, potentials and limitations of Compressed sensing methods and a practical method of image sampling and reconstruction with sampling rates close to the theoretical minimum.      
### 34.Technological Trends and Key Communication Enablers for eVTOLs  [ :arrow_down: ](https://arxiv.org/pdf/2110.08830.pdf)
>  The world is looking for a new exciting form of transportation that will cut our travel times considerably. In 2021, the time has come for flying cars to become the new transportation system of this century. Electric vertical take-off and landing (eVTOL) vehicles, which are a type of flying cars, are predicted to be used for passenger and package transportation in dense cities. In order to fly safely and reliably, wireless communications for eVTOLs must be developed with stringent eVTOL communication requirements. Indeed, their communication needs to be ultra-reliable, secure with ultra-high data rate and low latency to fulfill various tasks such as autonomous driving, sharing a massive amount of data in a short amount of time, and high-level communication security. In this paper, we propose major key communication enablers for eVTOLs ranging from the architecture, air-interface, networking, frequencies, security, and computing. To show the relevance and the impact of one of the key enablers, we carried out comparative simulations to show the superiority compared to the current technology. We compared the usage of an air-based communication infrastructure with a tower mast in a realistic scenario involving eVTOLs, delivery drones, pedestrians, and vehicles.      
### 35.Adaptive Time-Channel Beamforming for Time-of-Flight Correction  [ :arrow_down: ](https://arxiv.org/pdf/2110.08823.pdf)
>  Adaptive beamforming can lead to substantial improvement in resolution and contrast of ultrasound images over standard delay and sum beamforming. Here we introduce the adaptive time-channel (ATC) beamformer, a data-driven approach that combines spatial and temporal information simultaneously, thus generalizing minimum variance beamformers. Moreover, we broaden the concept of apodization to the temporal dimension. Our approach reduces noises by allowing for the weights to adapt in both the temporal and spatial dimensions, thereby reducing artifacts caused by the media's inhomogeneities. We apply our method to in-silico data and show 12% resolution enhancement along with 2-fold contrast improvement, and significant noise reduction with respect to delay and sum and minimum variance beamformers.      
### 36.A deep learning pipeline for localization, differentiation, and uncertainty estimation of liver lesions using multi-phasic and multi-sequence MRI  [ :arrow_down: ](https://arxiv.org/pdf/2110.08817.pdf)
>  Objectives: to propose a fully-automatic computer-aided diagnosis (CAD) solution for liver lesion characterization, with uncertainty estimation. <br>Methods: we enrolled 400 patients who had either liver resection or a biopsy and was diagnosed with either hepatocellular carcinoma (HCC), intrahepatic cholangiocarcinoma, or secondary metastasis, from 2006 to 2019. Each patient was scanned with T1WI, T2WI, T1WI venous phase (T2WI-V), T1WI arterial phase (T1WI-A), and DWI MRI sequences. We propose a fully-automatic deep CAD pipeline that localizes lesions from 3D MRI studies using key-slice parsing and provides a confidence measure for its diagnoses. We evaluate using five-fold cross validation and compare performance against three radiologists, including a senior hepatology radiologist, a junior hepatology radiologist and an abdominal radiologist. <br>Results: the proposed CAD solution achieves a mean F1 score of 0.62, outperforming the abdominal radiologist (0.47), matching the junior hepatology radiologist (0.61), and underperforming the senior hepatology radiologist (0.68). The CAD system can informatively assess its diagnostic confidence, i.e., when only evaluating on the 70% most confident cases the mean f1 score and sensitivity at 80% specificity for HCC vs. others are boosted from 0.62 to 0.71 and 0.84 to 0.92, respectively. <br>Conclusion: the proposed fully-automatic CAD solution can provide good diagnostic performance with informative confidence assessments in finding and discriminating liver lesions from MRI studies.      
### 37.VISinger: Variational Inference with Adversarial Learning for End-to-End Singing Voice Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2110.08813.pdf)
>  In this paper, we propose VISinger, a complete end-to-end high-quality singing voice synthesis (SVS) system that directly generates audio waveform from lyrics and musical score. Our approach is inspired by VITS, which adopts VAE-based posterior encoder augmented with normalizing flow-based prior encoder and adversarial decoder to realize complete end-to-end speech generation. VISinger follows the main architecture of VITS, but makes substantial improvements to the prior encoder based on the characteristics of singing. First, instead of using phoneme-level mean and variance of acoustic features, we introduce a length regulator and a frame prior network to get the frame-level mean and variance on acoustic features, modeling the rich acoustic variation in singing. Second, we further introduce an F0 predictor to guide the frame prior network, leading to stabler singing performance. Finally, to improve the singing rhythm, we modify the duration predictor to specifically predict the phoneme to note duration ratio, helped with singing note normalization. Experiments on a professional Mandarin singing corpus show that VISinger significantly outperforms FastSpeech+Neural-Vocoder two-stage approach and the oracle VITS; ablation study demonstrates the effectiveness of different contributions.      
### 38.Rheumatoid Arthritis: Automated Scoring of Radiographic Joint Damage  [ :arrow_down: ](https://arxiv.org/pdf/2110.08812.pdf)
>  Rheumatoid arthritis is an autoimmune disease that causes joint damage due to inflammation in the soft tissue lining the joints known as the synovium. It is vital to identify joint damage as soon as possible to provide necessary treatment early and prevent further damage to the bone structures. Radiographs are often used to assess the extent of the joint damage. Currently, the scoring of joint damage from the radiograph takes expertise, effort, and time. Joint damage associated with rheumatoid arthritis is also not quantitated in clinical practice and subjective descriptors are used. In this work, we describe a pipeline of deep learning models to automatically identify and score rheumatoid arthritic joint damage from a radiographic image. Our automatic tool was shown to produce scores with extremely high balanced accuracy within a couple of minutes and utilizing this would remove the subjectivity of the scores between human reviewers.      
### 39.Attention W-Net: Improved Skip Connections for better Representations  [ :arrow_down: ](https://arxiv.org/pdf/2110.08811.pdf)
>  Segmentation of macro and microvascular structures in fundoscopic retinal images plays a crucial role in detection of multiple retinal and systemic diseases, yet it is a difficult problem to solve. Most deep learning approaches for this task involve an autoencoder based architecture, but they face several issues such as lack of enough parameters, overfitting when there are enough parameters and incompatibility between internal feature-spaces. Due to such issues, these techniques are hence not able to extract the best semantic information from the limited data present for such tasks. We propose Attention W-Net, a new U-Net based architecture for retinal vessel segmentation to address these problems. In this architecture with a LadderNet backbone, we have two main contributions: Attention Block and regularisation measures. Our Attention Block uses decoder features to attend over the encoder features from skip-connections during upsampling, resulting in higher compatibility when the encoder and decoder features are added. Our regularisation measures include image augmentation and modifications to the ResNet Block used, which prevent overfitting. With these additions, we observe an AUC and F1-Score of 0.8407 and 0.9833 - a sizeable improvement over its LadderNet backbone as well as competitive performance among the contemporary state-of-the-art methods.      
### 40.Stable Marriage Matching for Traffic-Aware Space-Air-Ground Integrated Networks: A Gale-Shapley Algorithmic Approach  [ :arrow_down: ](https://arxiv.org/pdf/2110.08796.pdf)
>  In keeping with the rapid development of communication technology, a new communication structure is required in a next-generation communication system. In particular, research using High Altitude Platform (HAP) or Unmanned Aerial Vehicle(UAV) in existing terrestrial networks is active. In this paper, we propose matching HAP and UAV using the Gale-Shapley algorithm in a relay communication situation. The numerical simulation results demonstrate that applying the Gale-Shapley algorithm shows superior performance compared to random matching.      
### 41.On Estimating the Probabilistic Region of Attraction for Partially Unknown Nonlinear Systems: An Sum-of-Squares Approach  [ :arrow_down: ](https://arxiv.org/pdf/2110.08781.pdf)
>  Estimating the region of attraction for partially unknown nonlinear systems is a challenging issue. In this paper, we propose a tractable method to generate an estimated region of attraction with probability bounds, by searching an optimal polynomial barrier function. Chebyshev interpolants, Gaussian processes and sum-of-squares programmings are used in this paper. To approximate the unknown non-polynomial dynamics, a polynomial mean function of Gaussian processes model is computed to represent the exact dynamics based on the Chebyshev interpolants. Furthermore, probabilistic conditions are given such that all the estimates are located in certain probability bounds. Numerical examples are provided to demonstrate the effectiveness of the proposed method.      
### 42.Self-Supervised U-Net for Segmenting Flat and Sessile Polyps  [ :arrow_down: ](https://arxiv.org/pdf/2110.08776.pdf)
>  Colorectal Cancer(CRC) poses a great risk to public health. It is the third most common cause of cancer in the US. Development of colorectal polyps is one of the earliest signs of cancer. Early detection and resection of polyps can greatly increase survival rate to 90%. Manual inspection can cause misdetections because polyps vary in color, shape, size and appearance. To this end, Computer-Aided Diagnosis systems(CADx) has been proposed that detect polyps by processing the colonoscopic videos. The system acts a secondary check to help clinicians reduce misdetections so that polyps may be resected before they transform to cancer. Polyps vary in color, shape, size, texture and appearance. As a result, the miss rate of polyps is between 6% and 27% despite the prominence of CADx solutions. Furthermore, sessile and flat polyps which have diameter less than 10 mm are more likely to be undetected. Convolutional Neural Networks(CNN) have shown promising results in polyp segmentation. However, all of these works have a supervised approach and are limited by the size of the dataset. It was observed that smaller datasets reduce the segmentation accuracy of ResUNet++. We train a U-Net to inpaint randomly dropped out pixels in the image as a proxy task. The dataset we use for pre-training is Kvasir-SEG dataset. This is followed by a supervised training on the limited Kvasir-Sessile dataset. Our experimental results demonstrate that with limited annotated dataset and a larger unlabeled dataset, self-supervised approach is a better alternative than fully supervised approach. Specifically, our self-supervised U-Net performs better than five segmentation models which were trained in supervised manner on the Kvasir-Sessile dataset.      
### 43.Robust Kalman filters with unknown covariance of multiplicative noise  [ :arrow_down: ](https://arxiv.org/pdf/2110.08740.pdf)
>  In this paper, state and noise covariance estimation problems for linear system with unknown multiplicative noise are considered. The measurement likelihood is modelled as a mixture of two Gaussian distributions and a Student's $\emph{t}$ distribution, respectively. The unknown covariance of multiplicative noise is modelled as an inverse Gamma/Wishart distribution and the initial condition is formulated as the nominal covariance. By using robust design and choosing hierarchical priors, two variational Bayesian based robust Kalman filters are proposed. Stability and covergence of the proposed filters, the covariance parameters, the VB inference, and the estimation error dynamics are analyzed. The lower and upper bounds are also provided to guarantee the performance of the proposed filters. A target tracking simulation is provided to validate the effectiveness of the proposed filters.      
### 44.Data Shapley Value for Handling Noisy Labels: An application in Screening COVID-19 Pneumonia from Chest CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2110.08726.pdf)
>  A long-standing challenge of deep learning models involves how to handle noisy labels, especially in applications where human lives are at stake. Adoption of the data Shapley Value (SV), a cooperative game theoretical approach, is an intelligent valuation solution to tackle the issue of noisy labels. Data SV can be used together with a learning model and an evaluation metric to validate each training point's contribution to the model's performance. The SV of a data point, however, is not unique and depends on the learning model, the evaluation metric, and other data points collaborating in the training game. However, effects of utilizing different evaluation metrics for computation of the SV, detecting the noisy labels, and measuring the data points' importance has not yet been thoroughly investigated. In this context, we performed a series of comparative analyses to assess SV's capabilities to detect noisy input labels when measured by different evaluation metrics. Our experiments on COVID-19-infected of CT images illustrate that although the data SV can effectively identify noisy labels, adoption of different evaluation metric can significantly influence its ability to identify noisy labels from different data classes. Specifically, we demonstrate that the SV greatly depends on the associated evaluation metric.      
### 45.CAE-Transformer: Transformer-based Model to Predict Invasiveness of Lung Adenocarcinoma Subsolid Nodules from Non-thin Section 3D CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2110.08721.pdf)
>  Lung cancer is the leading cause of mortality from cancer worldwide and has various histologic types, among which Lung Adenocarcinoma (LAUC) has recently been the most prevalent. Lung adenocarcinomas are classified as pre-invasive, minimally invasive, and invasive adenocarcinomas. Timely and accurate knowledge of the invasiveness of lung nodules leads to a proper treatment plan and reduces the risk of unnecessary or late surgeries. Currently, the primary imaging modality to assess and predict the invasiveness of LAUCs is the chest CT. The results based on CT images, however, are subjective and suffer from a low accuracy compared to the ground truth pathological reviews provided after surgical resections. In this paper, a predictive transformer-based framework, referred to as the "CAE-Transformer", is developed to classify LAUCs. The CAE-Transformer utilizes a Convolutional Auto-Encoder (CAE) to automatically extract informative features from CT slices, which are then fed to a modified transformer model to capture global inter-slice relations. Experimental results on our in-house dataset of 114 pathologically proven Sub-Solid Nodules (SSNs) demonstrate the superiority of the CAE-Transformer over the histogram/radiomics-based models and its deep learning-based counterparts, achieving an accuracy of 87.73%, sensitivity of 88.67%, specificity of 86.33%, and AUC of 0.913, using a 10-fold cross-validation.      
### 46.Measuring Total Transverse Reference-free Displacements of Railroad Bridges using 2 Degrees of Freedom (2DOF): Experimental Validation  [ :arrow_down: ](https://arxiv.org/pdf/2110.08701.pdf)
>  Railroad bridge engineers are interested in the displacement of railroad bridges when the train is crossing the bridge for engineering decision making of their assets. Measuring displacements under train crossing events is difficult. If simplified reference-free methods would be accurate and validated, owners would conduct objective performance assessment of their bridge inventories under trains. Researchers have developed new sensing technologies (reference-free) to overcome the limitations of reference point-based displacement sensors. Reference-free methods use accelerometers to estimate displacements, by decomposing the total displacement in two parts: a high-frequency dynamic displacement component, and a low-frequency pseudo-static displacement component. In the past, researchers have used the Euler-Bernoulli beam theory formula to estimate the pseudo-static displacement assuming railroad bridge piles and columns can be simplified as cantilever beams. However, according to railroad bridge managers, railroad bridges have a different degree of fixity for each pile of each bent. Displacements can be estimated assuming a similar degree of fixity for deep foundations, but inherent errors will affect the accuracy of displacement estimation. This paper solves this problem expanding the 1 Degree of Freedom (1DOF) solution to a new 2 Degrees of Freedom (2DOF), to collect displacements under trains and enable cost-effective condition-based information related to bridge safety. Researchers developed a simplified beam to demonstrate the total displacement estimation using 2DOF and further conducted experimental results in the laboratory. The estimated displacement of the 2DOF model is more accurate than that of the 1DOF model for ten train crossing events. With only one sensor added to the ground of the pile, this method provides owners with approximately 40% more accurate displacements.      
### 47.Fast Strain Estimation and Frame Selection in Ultrasound Elastography using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.08668.pdf)
>  Ultrasound Elastography aims to determine the mechanical properties of the tissue by monitoring tissue deformation due to internal or external forces. Tissue deformations are estimated from ultrasound radio frequency (RF) signals and are often referred to as time delay estimation (TDE). Given two RF frames I1 and I2, we can compute a displacement image which shows the change in the position of each sample in I1 to a new position in I2. Two important challenges in TDE include high computational complexity and the difficulty in choosing suitable RF frames. Selecting suitable frames is of high importance because many pairs of RF frames either do not have acceptable deformation for extracting informative strain images or are decorrelated and deformation cannot be reliably estimated. Herein, we introduce a method that learns 12 displacement modes in quasi-static elastography by performing Principal Component Analysis (PCA) on displacement fields of a large training database. In the inference stage, we use dynamic programming (DP) to compute an initial displacement estimate of around 1% of the samples, and then decompose this sparse displacement into a linear combination of the 12 displacement modes. Our method assumes that the displacement of the whole image could also be described by this linear combination of principal components. We then use the GLobal Ultrasound Elastography (GLUE) method to fine-tune the result yielding the exact displacement image. Our method, which we call PCA-GLUE, is more than 10 times faster than DP in calculating the initial displacement map while giving the same result. Our second contribution in this paper is determining the suitability of the frame pair I1 and I2 for strain estimation, which we achieve by using the weight vector that we calculated for PCA-GLUE as an input to a multi-layer perceptron (MLP) classifier.      
### 48.Sampling based Computation of Viability Domain to Prevent Safety Violations by Attackers  [ :arrow_down: ](https://arxiv.org/pdf/2110.08632.pdf)
>  This paper studies the security of cyber-physical systems under attacks. Our goal is to design system parameters, such as a set of initial conditions and input bounds so that it is secure by design. To this end, we propose new sufficient conditions to guarantee the safety of a system under adversarial actuator attacks. Using these conditions, we propose a computationally efficient sampling-based method to verify whether a set is a viability domain for a general class of nonlinear systems. In particular, we devise a method of checking a modified barrier function condition on a finite set of points to assess whether a set can be rendered forward invariant. Then, we propose an iterative algorithm to compute the set of initial conditions and input constraint set to limit what an adversary can do if it compromises the vulnerable inputs. Finally, we utilize a Quadratic Program approach for online control synthesis.      
### 49.Self-Learned Kernel Low Rank Approach TO Accelerated High Resolution 3D Diffusion MRI  [ :arrow_down: ](https://arxiv.org/pdf/2110.08622.pdf)
>  Diffusion Magnetic Resonance Imaging (dMRI) is a promising method to analyze the subtle changes in the tissue structure. However, the lengthy acquisition time is a major limitation in the clinical application of dMRI. Different image acquisition techniques such as parallel imaging, compressed sensing, has shortened the prolonged acquisition time but creating high-resolution 3D dMRI slices still requires a significant amount of time. In this study, we have shown that high-resolution 3D dMRI can be reconstructed from the highly undersampled k-space and q-space data using a Kernel LowRank method. Our proposed method has outperformed the conventional CS methods in terms of both image quality and diffusion maps constructed from the diffusion-weighted images      
### 50.SAGAN: Adversarial Spatial-asymmetric Attention for Noisy Nona-Bayer Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2110.08619.pdf)
>  Nona-Bayer colour filter array (CFA) pattern is considered one of the most viable alternatives to traditional Bayer patterns. Despite the substantial advantages, such non-Bayer CFA patterns are susceptible to produce visual artefacts while reconstructing RGB images from noisy sensor data. This study addresses the challenges of learning RGB image reconstruction from noisy Nona-Bayer CFA comprehensively. We propose a novel spatial-asymmetric attention module to jointly learn bi-direction transformation and large-kernel global attention to reduce the visual artefacts. We combine our proposed module with adversarial learning to produce plausible images from Nona-Bayer CFA. The feasibility of the proposed method has been verified and compared with the state-of-the-art image reconstruction method. The experiments reveal that the proposed method can reconstruct RGB images from noisy Nona-Bayer CFA without producing any visually disturbing artefacts. Also, it can outperform the state-of-the-art image reconstruction method in both qualitative and quantitative comparison. Code available: <a class="link-external link-https" href="https://github.com/sharif-apu/SAGAN_BMVC21" rel="external noopener nofollow">this https URL</a>.      
### 51.Numerical Overcurrent Relay: A Digitizing Element Testing Automation and Simulation Based on Wavelet Transform  [ :arrow_down: ](https://arxiv.org/pdf/2110.08617.pdf)
>  In todays modern technology and fast growth in electrical power supply, the need for precise protection relays has become a major issue in the power distribution system. That is, to ensure a safe operation at all levels of generation, distribution, and consumer load, the power relay should perform at the highest level of accuracy. Therefore, this work will investigate the enhancement of the Overcurrent Numerical Relay in terms of the quality of digitized incoming analog current waveform prior to numerical relay decision. However, as it is an expensive process of testing the digitizing element of numerical overcurrent relay, Analog to Digital Converter, this newly proposed method of combing test automation by LabView and advanced waveform analysis algorithms by Wavelet transform will be applied to enhance the testing process, reduce data compiling complexity, and decrease cost. In addition, the testing simulation will be presented using Matlab software to verify the testing result. As a result, with a bank of filters, denoising, and decomposition structures, Wavelet transform can provide promising results in testing and verifying the accuracy of numerical relay digitizing process along with automation process to increase the possibility of improving numerical relay digitizer testing reliability with lower cost.      
### 52.How can a Cognitive Radar Mask its Cognition?  [ :arrow_down: ](https://arxiv.org/pdf/2110.08608.pdf)
>  We study how a cognitive radar can mask (hide) its cognitive ability from an adversarial jamming device. Specifically, if the radar optimally adapts its waveform based on adversarial target maneuvers (probes), how should the radar choose its waveform parameters (response) so that its utility function cannot be recovered by the adversary. This paper abstracts the radar's cognition masking problem in terms of the spectra (eigenvalues) of the state and observation noise covariance matrices, and embeds the algebraic Riccati equation into an economics-based utility maximization setup. Given an observed sequence of radar responses, the adversary tests for utility maximization behavior of the radar and estimates its utility function that rationalizes the radar's responses. In turn, the radar deliberately chooses sub-optimal responses so that its utility function almost fails the utility maximization test, and hence, its cognitive ability is masked from the adversary. We illustrate the performance of our cognition masking scheme via simple numerical examples. Our approach in this paper is based on revealed preference theory in microeconomics for identifying rationality.      
### 53.PDMM: A novel Primal-Dual Majorization-Minimization algorithm for Poisson Phase-Retrieval problem  [ :arrow_down: ](https://arxiv.org/pdf/2110.08600.pdf)
>  In this paper, we introduce a novel iterative algorithm for the problem of phase-retrieval where the measurements consist of only the magnitude of linear function of the unknown signal, and the noise in the measurements follow Poisson distribution. The proposed algorithm is based on the principle of majorization-minimization (MM); however, the application of MM here is very novel and distinct from the way MM has been usually used to solve optimization problems in the literature. More precisely, we reformulate the original minimization problem into a saddle point problem by invoking Fenchel dual representation of the log (.) term in the Poisson likelihood function. We then propose tighter surrogate functions over both primal and dual variables resulting in a double-loop MM algorithm, which we have named as Primal-Dual Majorization-Minimization (PDMM) algorithm. The iterative steps of the resulting algorithm are simple to implement and involve only computing matrix vector products. We also extend our algorithm to handle various L1 regularized Poisson phase-retrieval problems (which exploit sparsity). The proposed algorithm is compared with previously proposed algorithms such as wirtinger flow (WF), MM (conventional), and alternating direction methods of multipliers (ADMM) for the Poisson data model. The simulation results under different experimental settings show that PDMM is faster than the competing methods, and its performance in recovering the original signal is at par with the state-of-the-art algorithms.      
### 54.A Variational Bayesian Approach to Learning Latent Variables for Acoustic Knowledge Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2110.08598.pdf)
>  We propose a variational Bayesian (VB) approach to learning distributions of latent variables in deep neural network (DNN) models for cross-domain knowledge transfer, to address acoustic mismatches between training and testing conditions. Instead of carrying out point estimation in conventional maximum a posteriori estimation with a risk of having a curse of dimensionality in estimating a huge number of model parameters, we focus our attention on estimating a manageable number of latent variables of DNNs via a VB inference framework. To accomplish model transfer, knowledge learnt from a source domain is encoded in prior distributions of latent variables and optimally combined, in a Bayesian sense, with a small set of adaptation data from a target domain to approximate the corresponding posterior distributions. Experimental results on device adaptation in acoustic scene classification show that our proposed VB approach can obtain good improvements on target devices, and consistently outperforms 13 state-of-the-art knowledge transfer algorithms.      
### 55.A MIMO Radar-based Few-Shot Learning Approach for Human-ID  [ :arrow_down: ](https://arxiv.org/pdf/2110.08595.pdf)
>  Radar for deep learning-based human identification has become a research area of increasing interest. It has been shown that micro-Doppler (\(\upmu\)-D) can reflect the walking behavior through capturing the periodic limbs' micro-motions. One of the main aspects is maximizing the number of included classes while considering the real-time and training dataset size constraints. In this paper, a multiple-input-multiple-output (MIMO) radar is used to formulate micro-motion spectrograms of the elevation angular velocity (\(\upmu\)-\(\omega\)). The effectiveness of concatenating this newly-formulated spectrogram with the commonly used \(\upmu\)-D is investigated. To accommodate for non-constrained real walking motion, an adaptive cycle segmentation framework is utilized and a metric learning network is trained on half gait cycles (\(\approx\) 0.5 s). Studies on the effects of various numbers of classes (5--20), different dataset sizes, and varying observation time windows 1--2 s are conducted. A non-constrained walking dataset of 22 subjects is collected with different aspect angles with respect to the radar. The proposed few-shot learning (FSL) approach achieves a classification error of 11.3 % with only 2 min of training data per subject.      
### 56.ASR4REAL: An extended benchmark for speech models  [ :arrow_down: ](https://arxiv.org/pdf/2110.08583.pdf)
>  Popular ASR benchmarks such as Librispeech and Switchboard are limited in the diversity of settings and speakers they represent. We introduce a set of benchmarks matching real-life conditions, aimed at spotting possible biases and weaknesses in models. We have found out that even though recent models do not seem to exhibit a gender bias, they usually show important performance discrepancies by accent, and even more important ones depending on the socio-economic status of the speakers. Finally, all tested models show a strong performance drop when tested on conversational speech, and in this precise context even a language model trained on a dataset as big as Common Crawl does not seem to have significant positive effect which reiterates the importance of developing conversational language models      
### 57.A Tutorial on Terahertz-Band Localization for 6G Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.08581.pdf)
>  Terahertz (THz) communications are celebrated as key enablers for converged localization and sensing in future sixth-generation (6G) wireless communication systems and beyond. Instead of being a byproduct of the communication system, localization in 6G is indispensable for location-aware communications. Towards this end, we aim to identify the prospects, challenges, and requirements of THz localization techniques. We first review the history and trends of localization methods and discuss their objectives, constraints, and applications in contemporary communication systems. We then detail the latest advances in THz communications and introduce the THz-specific channel and system models. Afterward, we formulate THz-band localization as a 3D position/orientation estimation problem, detailing geometry-based localization techniques and describing potential THz localization and sensing extensions. We further formulate the offline design and online optimization of THz localization systems, provide numerical simulation results, and conclude by providing insight into interdisciplinary future research directions. Preliminary results illustrate that under the same total transmission power and time, THz-based localization is ~5 (~20) times more accurate than mmWave-based localization without (with) prior position information.      
### 58.Deep Image Debanding  [ :arrow_down: ](https://arxiv.org/pdf/2110.08569.pdf)
>  Banding or false contour is an annoying visual artifact whose impact is even more pronounced in ultra high definition, high dynamic range, and wide colour gamut visual content, which is becoming increasingly popular. Since users associate a heightened expectation of quality with such content and banding leads to deteriorated visual quality-of-experience, the area of banding removal or debanding has taken paramount importance. Existing debanding approaches are mostly knowledge-driven. Despite the widespread success of deep learning in other areas of image processing and computer vision, data-driven debanding approaches remain surprisingly missing. In this work, we make one of the first attempts to develop a deep learning based banding artifact removal method for images and name it deep debanding network (deepDeband). For its training, we construct a large-scale dataset of 51,490 pairs of corresponding pristine and banded image patches. Performance evaluation shows that deepDeband is successful at greatly reducing banding artifacts in images, outperforming existing methods both quantitatively and visually.      
### 59.A Unified Speaker Adaptation Approach for ASR  [ :arrow_down: ](https://arxiv.org/pdf/2110.08545.pdf)
>  Transformer models have been used in automatic speech recognition (ASR) successfully and yields state-of-the-art results. However, its performance is still affected by speaker mismatch between training and test data. Further finetuning a trained model with target speaker data is the most natural approach for adaptation, but it takes a lot of compute and may cause catastrophic forgetting to the existing speakers. In this work, we propose a unified speaker adaptation approach consisting of feature adaptation and model adaptation. For feature adaptation, we employ a speaker-aware persistent memory model which generalizes better to unseen test speakers by making use of speaker i-vectors to form a persistent memory. For model adaptation, we use a novel gradual pruning method to adapt to target speakers without changing the model architecture, which to the best of our knowledge, has never been explored in ASR. Specifically, we gradually prune less contributing parameters on model encoder to a certain sparsity level, and use the pruned parameters for adaptation, while freezing the unpruned parameters to keep the original model performance. We conduct experiments on the Librispeech dataset. Our proposed approach brings relative 2.74-6.52% word error rate (WER) reduction on general speaker adaptation. On target speaker adaptation, our method outperforms the baseline with up to 20.58% relative WER reduction, and surpasses the finetuning method by up to relative 2.54%. Besides, with extremely low-resource adaptation data (e.g., 1 utterance), our method could improve the WER by relative 6.53% with only a few epochs of training.      
### 60.Locally Adaptive Structure and Texture Similarity for Image Quality Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2110.08521.pdf)
>  The latest advances in full-reference image quality assessment (IQA) involve unifying structure and texture similarity based on deep representations. The resulting Deep Image Structure and Texture Similarity (DISTS) metric, however, makes rather global quality measurements, ignoring the fact that natural photographic images are locally structured and textured across space and scale. In this paper, we describe a locally adaptive structure and texture similarity index for full-reference IQA, which we term A-DISTS. Specifically, we rely on a single statistical feature, namely the dispersion index, to localize texture regions at different scales. The estimated probability (of one patch being texture) is in turn used to adaptively pool local structure and texture measurements. The resulting A-DISTS is adapted to local image content, and is free of expensive human perceptual scores for supervised training. We demonstrate the advantages of A-DISTS in terms of correlation with human data on ten IQA databases and optimization of single image super-resolution methods.      
### 61.BAPGAN: GAN-based Bone Age Progression of Femur and Phalange X-ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.08509.pdf)
>  Convolutional Neural Networks play a key role in bone age assessment for investigating endocrinology, genetic, and growth disorders under various modalities and body regions. However, no researcher has tackled bone age progression/regression despite its valuable potential applications: bone-related disease diagnosis, clinical knowledge acquisition, and museum education. Therefore, we propose Bone Age Progression Generative Adversarial Network (BAPGAN) to progress/regress both femur/phalange X-ray images while preserving identity and realism. We exhaustively confirm the BAPGAN's clinical potential via Frechet Inception Distance, Visual Turing Test by two expert orthopedists, and t-Distributed Stochastic Neighbor Embedding.      
### 62.Can CAV Reduce Non-Recurrent Urban Road Congestion?  [ :arrow_down: ](https://arxiv.org/pdf/2110.08507.pdf)
>  A well-designed resilient and sustainable urban transportation system can recover quickly from the non-recurrent road congestion (NRC), which is often caused by en-route events (e.g., road closure due to car collisions). Existing solutions, such as on-board navigation systems and temporary rerouting road signs, are not effective due to delayed responses. Connected Autonomous Vehicles (CAV) can be helpful in improving recurrent traffic as they can autonomously adjust their speed according to their real-time surrounding traffic, sensed by vehicular communications. Preliminary simulation results in this short paper show that CAV can also improve traffic when non-recurrent congestion occurs. Other results in fuel consumption, CO2 emission, and traditional traffic safety indicators are open for future discussions.      
### 63.Rethinking Modern Communication from Semantic Coding to Semantic Communication  [ :arrow_down: ](https://arxiv.org/pdf/2110.08496.pdf)
>  Modern communications are usually designed to pursue a higher bit-level precision and fewer bits required to transmit a message. This article rethinks these two major features and introduces the concept and advantage of semantics that characterizes a new kind of semantics-aware communication mechanism, incorporating both the semantic encoding and the semantic communication problem. Within the unified framework, we analyze the underlying defects of existing semantics-aware techniques and establish a confidence-based distillation mechanism for the joint semantics-noise coding (JSNC) problem, and a reinforcement learning (RL)-powered semantic communication paradigm that endows a system the ability to convey the semantics instead of pursuing the bit level accuracy. On top of these technical contributions, this work provides a new insight to understand how the semantics are processed and represented in a semantics-aware coding and communication system, and verifies the significant benefits of doing so.      
### 64.Feedforward Control of DGs for a Self-healing Microgrid  [ :arrow_down: ](https://arxiv.org/pdf/2110.08494.pdf)
>  Network reconfiguration (NR) has recently received significant attention due to its potential to improve grid resilience by realizing self-healing microgrids (MGs). This paper proposes a new strategy for the real-time frequency regulation of a reconfigurable MG, wherein the feedforward control of synchronous and inverter-interfaced distributed generators (DGs) is achieved in coordination with the operations of sectionalizing and tie switches (SWs). This enables DGs to compensate more quickly, and preemptively, for a forthcoming variation in load demand due to NR-aided restoration. An analytical dynamic model of a reconfigurable MG is developed to analyze the MG frequency response to NR and hence determine the desired dynamics of the feedforward controllers, with the integration of feedback loops for inertial response emulation and primary and secondary frequency control. A small-signal analysis is conducted to analyze the contribution of the supplementary feedforward control to the MG frequency regulation. Simulation case studies of NR-aided load restoration are also performed. The results of the small-signal analysis and case studies confirm that the proposed strategy is effective for improving the MG frequency regulation under various conditions of load demand, model parameter errors, and communication time delays.      
### 65.Robust Adaptive Beamforming Maximizing the Worst-Case SINR over Distributional Uncertainty Sets for Random INC Matrix and Signal Steering Vector  [ :arrow_down: ](https://arxiv.org/pdf/2110.08444.pdf)
>  The robust adaptive beamforming (RAB) problem is considered via the worst-case signal-to-interference-plus-noise ratio (SINR) maximization over distributional uncertainty sets for the random interference-plus-noise covariance (INC) matrix and desired signal steering vector. The distributional uncertainty set of the INC matrix accounts for the support and the positive semidefinite (PSD) mean of the distribution, and a similarity constraint on the mean. The distributional uncertainty set for the steering vector consists of the constraints on the known first- and second-order moments. The RAB problem is formulated as a minimization of the worst-case expected value of the SINR denominator achieved by any distribution, subject to the expected value of the numerator being greater than or equal to one for each distribution. Resorting to the strong duality of linear conic programming, such a RAB problem is rewritten as a quadratic matrix inequality problem. It is then tackled by iteratively solving a sequence of linear matrix inequality relaxation problems with the penalty term on the rank-one PSD matrix constraint. To validate the results, simulation examples are presented, and they demonstrate the improved performance of the proposed robust beamformer in terms of the array output SINR.      
### 66.COVID-19 Detection in Chest X-ray Images Using Swin-Transformer and Transformer in Transformer  [ :arrow_down: ](https://arxiv.org/pdf/2110.08427.pdf)
>  The Coronavirus Disease 2019 (COVID-19) has spread globally and caused serious damages. Chest X-ray images are widely used for COVID-19 diagnosis and Artificial Intelligence method can assist to increase the efficiency and accuracy. In the Challenge of Chest XR COVID-19 detection in Ethics and Explainability for Responsible Data Science (EE-RDS) conference 2021, we proposed a method which combined Swin Transformer and Transformer in Transformer to classify chest X-ray images as three classes: COVID-19, Pneumonia and Normal (healthy) and achieved 0.9475 accuracy on test set.      
### 67.Deep learning-based detection of intravenous contrast in computed tomography scans  [ :arrow_down: ](https://arxiv.org/pdf/2110.08424.pdf)
>  Purpose: Identifying intravenous (IV) contrast use within CT scans is a key component of data curation for model development and testing. Currently, IV contrast is poorly documented in imaging metadata and necessitates manual correction and annotation by clinician experts, presenting a major barrier to imaging analyses and algorithm deployment. We sought to develop and validate a convolutional neural network (CNN)-based deep learning (DL) platform to identify IV contrast within CT scans. Methods: For model development and evaluation, we used independent datasets of CT scans of head, neck (HN) and lung cancer patients, totaling 133,480 axial 2D scan slices from 1,979 CT scans manually annotated for contrast presence by clinical experts. Five different DL models were adopted and trained in HN training datasets for slice-level contrast detection. Model performances were evaluated on a hold-out set and on an independent validation set from another institution. DL models was then fine-tuned on chest CT data and externally validated on a separate chest CT dataset. Results: Initial DICOM metadata tags for IV contrast were missing or erroneous in 1,496 scans (75.6%). The EfficientNetB4-based model showed the best overall detection performance. For HN scans, AUC was 0.996 in the internal validation set (n = 216) and 1.0 in the external validation set (n = 595). The fine-tuned model on chest CTs yielded an AUC: 1.0 for the internal validation set (n = 53), and AUC: 0.980 for the external validation set (n = 402). Conclusion: The DL model could accurately detect IV contrast in both HN and chest CT scans with near-perfect performance.      
### 68.Bridging the gap between paired and unpaired medical image translation  [ :arrow_down: ](https://arxiv.org/pdf/2110.08407.pdf)
>  Medical image translation has the potential to reduce the imaging workload, by removing the need to capture some sequences, and to reduce the annotation burden for developing machine learning methods. GANs have been used successfully to translate images from one domain to another, such as MR to CT. At present, paired data (registered MR and CT images) or extra supervision (e.g. segmentation masks) is needed to learn good translation models. Registering multiple modalities or annotating structures within each of them is a tedious and laborious task. Thus, there is a need to develop improved translation methods for unpaired data. Here, we introduce modified pix2pix models for tasks CT$\rightarrow$MR and MR$\rightarrow$CT, trained with unpaired CT and MR data, and MRCAT pairs generated from the MR scans. The proposed modifications utilize the paired MR and MRCAT images to ensure good alignment between input and translated images, and unpaired CT images ensure the MR$\rightarrow$CT model produces realistic-looking CT and CT$\rightarrow$MR model works well with real CT as input. The proposed pix2pix variants outperform baseline pix2pix, pix2pixHD and CycleGAN in terms of FID and KID, and generate more realistic looking CT and MR translations.      
### 69.Simultaneous Monitoring of Multiple People's Vital Sign Leveraging a Single Phased-MIMO Radar  [ :arrow_down: ](https://arxiv.org/pdf/2110.08401.pdf)
>  Vital sign monitoring plays a critical role in tracking the physiological state of people and enabling various health-related applications (e.g., recommending a change of lifestyle, examining the risk of diseases). Traditional approaches rely on hospitalization or body-attached instruments, which are costly and intrusive. Therefore, researchers have been exploring contact-less vital sign monitoring with radio frequency signals in recent years. Early studies with continuous wave radars/WiFi devices work on detecting vital signs of a single individual, but it still remains challenging to simultaneously monitor vital signs of multiple subjects, especially those who locate in proximity. In this paper, we design and implement a time-division multiplexing (TDM) phased-MIMO radar sensing scheme for high-precision vital sign monitoring of multiple people. Our phased-MIMO radar can steer the mmWave beam towards different directions with a micro-second delay, which enables capturing the vital signs of multiple individuals at the same radial distance to the radar. Furthermore, we develop a TDM-MIMO technique to fully utilize all transmitting antenna (TX)-receiving antenna (RX) pairs, thereby significantly boosting the signal-to-noise ratio. Based on the designed TDM phased-MIMO radar, we develop a system to automatically localize multiple human subjects and estimate their vital signs. Extensive evaluations show that under two-subject scenarios, our system can achieve an error of less than 1 beat per minute (BPM) and 3 BPM for breathing rate (BR) and heartbeat rate (HR) estimations, respectively, at a subject-to-radar distance of $1.6~m$. The minimal subject-to-subject angle separation is $40°$, corresponding to a close distance of $0.5~m$ between two subjects, which outperforms the state-of-the-art.      
### 70.Non-Isolated Single-Switch Zeta Based High-Step up DC-DC Converter with Coupled Inductor  [ :arrow_down: ](https://arxiv.org/pdf/2110.08390.pdf)
>  In this paper, a non-isolated high step-up DC-DC converter has been proposed for renewable energy applications. The proposed structure converter has been derived from the fundamental Zeta converter, in both of which only a single switch is employed. The voltage gain ratio has considerably enhanced in this converter with the absence of using switched capacity. To magnify voltage gain of the converter, a coupled-inductor has adopted. Increase and decrease of gain by changing the ratio of coupled-inductor assist the duty cycle. The number of components is low in this structure. The operating principle and evaluation of the proposed converter, considering designing approaches for elements, are discussed in detail. To verify the feasibility of the proposed converter, simulation results have been provided and evaluated.      
### 71.Orthogonal Transforms for Signals on Directed Graphs  [ :arrow_down: ](https://arxiv.org/pdf/2110.08364.pdf)
>  In this paper we consider the problem of defining transforms for signals on directed graphs, with a specific focus on defective graphs where the corresponding graph operator cannot be diagonalized. Our proposed method is based on the Schur decomposition and leads to a series of embedded invariant subspaces for which orthogonal basis are available. As compared to diffusion wavelets, our method is more flexible in the generation of subspaces, but these subspaces can only be approximately orthogonal.      
### 72.Comparing One-step and Two-step Scatter Correction and Density Reconstruction in X-ray CT  [ :arrow_down: ](https://arxiv.org/pdf/2110.08326.pdf)
>  In this work, we compare one-step and two-step approaches for X-ray computed tomography (CT) scatter correction and density reconstruction. X-ray CT is an important imaging technique in medical and industrial applications. In many cases, the presence of scattered X-rays leads to loss of contrast and undesirable artifacts in reconstructed images. Many approaches to computationally removing scatter treat scatter correction as a preprocessing step that is followed by a reconstruction step. Treating scatter correction and reconstruction jointly as a single, more complicated optimization problem is less studied. It is not clear from the existing literature how these two approaches compare in terms of reconstruction accuracy. In this paper, we compare idealized versions of these two approaches with synthetic experiments. Our results show that the one-step approach can offer improved reconstructions over the two-step approach, although the gap between them is highly object-dependent.      
### 73.Reduced Order Dynamical Models For Complex Dynamics in Manufacturing and Natural Systems Using Machine Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.08313.pdf)
>  Dynamical analysis of manufacturing and natural systems provides critical information about production of manufactured and natural resources respectively, thus playing an important role in assessing sustainability of these systems. However, current dynamic models for these systems exist as mechanistic models, simulation of which is computationally intensive and does not provide a simplified understanding of the mechanisms driving the overall dynamics. For such systems, lower-order models can prove useful to enable sustainability analysis through coupled dynamical analysis. There have been few attempts at finding low-order models of manufacturing and natural systems, with existing work focused on model development of individual mechanism level. This work seeks to fill this current gap in the literature of developing simplified dynamical models for these systems by developing reduced-order models using a machine learning (ML) approach. The approach is demonstrated on an entire soybean-oil to soybean-diesel process plant and a lake system. We use a grey-box ML method with a standard nonlinear optimization approach to identify relevant models of governing dynamics as ODEs using the data simulated from mechanistic models. Results show that the method identifies a high accuracy linear ODE models for the process plant, reflective of underlying linear stoichiometric mechanisms and mass balance driving the dynamics. For the natural systems, we modify the ML approach to include the effect of past dynamics, which gives non-linear ODE. While the modified approach provides a better match to dynamics of stream flow, it falls short of completely recreating the dynamics. We conclude that the proposed ML approach work well for systems where dynamics is smooth, such as in manufacturing plant whereas does not work perfectly well in case of chaotic dynamics such as water stream flow.      
### 74.Minimum $\ell_{1}$-norm interpolators: Precise asymptotics and multiple descent  [ :arrow_down: ](https://arxiv.org/pdf/2110.09502.pdf)
>  An evolving line of machine learning works observe empirical evidence that suggests interpolating estimators -- the ones that achieve zero training error -- may not necessarily be harmful. This paper pursues theoretical understanding for an important type of interpolators: the minimum $\ell_{1}$-norm interpolator, which is motivated by the observation that several learning algorithms favor low $\ell_1$-norm solutions in the over-parameterized regime. Concretely, we consider the noisy sparse regression model under Gaussian design, focusing on linear sparsity and high-dimensional asymptotics (so that both the number of features and the sparsity level scale proportionally with the sample size). <br>We observe, and provide rigorous theoretical justification for, a curious multi-descent phenomenon; that is, the generalization risk of the minimum $\ell_1$-norm interpolator undergoes multiple (and possibly more than two) phases of descent and ascent as one increases the model capacity. This phenomenon stems from the special structure of the minimum $\ell_1$-norm interpolator as well as the delicate interplay between the over-parameterized ratio and the sparsity, thus unveiling a fundamental distinction in geometry from the minimum $\ell_2$-norm interpolator. Our finding is built upon an exact characterization of the risk behavior, which is governed by a system of two non-linear equations with two unknowns.      
### 75.FMFCC-A: A Challenging Mandarin Dataset for Synthetic Speech Detection  [ :arrow_down: ](https://arxiv.org/pdf/2110.09441.pdf)
>  As increasing development of text-to-speech (TTS) and voice conversion (VC) technologies, the detection of synthetic speech has been suffered dramatically. In order to promote the development of synthetic speech detection model against Mandarin TTS and VC technologies, we have constructed a challenging Mandarin dataset and organized the accompanying audio track of the first fake media forensic challenge of China Society of Image and Graphics (FMFCC-A). The FMFCC-A dataset is by far the largest publicly-available Mandarin dataset for synthetic speech detection, which contains 40,000 synthesized Mandarin utterances that generated by 11 Mandarin TTS systems and two Mandarin VC systems, and 10,000 genuine Mandarin utterances collected from 58 speakers. The FMFCC-A dataset is divided into the training, development and evaluation sets, which are used for the research of detection of synthesized Mandarin speech under various previously unknown speech synthesis systems or audio post-processing operations. In addition to describing the construction of the FMFCC-A dataset, we provide a detailed analysis of two baseline methods and the top-performing submissions from the FMFCC-A, which illustrates the usefulness and challenge of FMFCC-A dataset. We hope that the FMFCC-A dataset can fill the gap of lack of Mandarin datasets for synthetic speech detection.      
### 76.Time separation technique with the basis of trigonometric functions as an efficient method for flat detector CT brain perfusion imaging  [ :arrow_down: ](https://arxiv.org/pdf/2110.09438.pdf)
>  Dynamic perfusion imaging is routinely used in the diagnostic workup of acute ischemic stroke (AIS). At present, perfusion imaging can also be performed within the angio suite using flat detector computed tomography (FDCT). However, higher noise level, slower rotation speed and lower frame rate need to be considered in FDCT perfusion (FDCTP) data processing algorithms. The Time Separation Technique (TST) is a model-based perfusion data reconstruction method developed to solve these problems. In this contribution, we used TST and dimension reduction, where we approximate the time attenuation curves by a linear combination of trigonometric functions. Our goal was to show that TST with this data reduction does not impair clinical perfusion measurements. We performed a realistic simulation of FDCTP acquisition based on CT perfusion (CTP) data. Using these FDCTP data, we showed that TST provides better results than classical straightforward processing. Moreover we found that TST is robust to additional noise. Furthermore, we achieved a total processing time from reconstruction of FDCTP data to generation of perfusion maps of under 5 minutes. Perfusion maps created using TST with a trigonometric basis from FDCTP data show equivalent perfusion deficits as CT perfusion maps. Therefore, this technique can be considered a fast reliable tool for FDCTP imaging in AIS.      
### 77.Streaming Machine Learning and Online Active Learning for Automated Visual Inspection  [ :arrow_down: ](https://arxiv.org/pdf/2110.09396.pdf)
>  Quality control is a key activity performed by manufacturing companies to verify product conformance to the requirements and specifications. Standardized quality control ensures that all the products are evaluated under the same criteria. The decreased cost of sensors and connectivity enabled an increasing digitalization of manufacturing and provided greater data availability. Such data availability has spurred the development of artificial intelligence models, which allow higher degrees of automation and reduced bias when inspecting the products. Furthermore, the increased speed of inspection reduces overall costs and time required for defect inspection. In this research, we compare five streaming machine learning algorithms applied to visual defect inspection with real-world data provided by Philips Consumer Lifestyle BV. Furthermore, we compare them in a streaming active learning context, which reduces the data labeling effort in a real-world context. Our results show that active learning reduces the data labeling effort by almost 15% on average for the worst case, while keeping an acceptable classification performance. The use of machine learning models for automated visual inspection are expected to speed up the quality inspection up to 40%.      
### 78.Ortho-Shot: Low Displacement Rank Regularization with Data Augmentation for Few-Shot Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.09374.pdf)
>  In few-shot classification, the primary goal is to learn representations from a few samples that generalize well for novel classes. In this paper, we propose an efficient low displacement rank (LDR) regularization strategy termed Ortho-Shot; a technique that imposes orthogonal regularization on the convolutional layers of a few-shot classifier, which is based on the doubly-block toeplitz (DBT) matrix structure. The regularized convolutional layers of the few-shot classifier enhances model generalization and intra-class feature embeddings that are crucial for few-shot learning. Overfitting is a typical issue for few-shot models, the lack of data diversity inhibits proper model inference which weakens the classification accuracy of few-shot learners to novel classes. In this regard, we broke down the pipeline of the few-shot classifier and established that the support, query and task data augmentation collectively alleviates overfitting in networks. With compelling results, we demonstrated that combining a DBT-based low-rank orthogonal regularizer with data augmentation strategies, significantly boosts the performance of a few-shot classifier. We perform our experiments on the miniImagenet, CIFAR-FS and Stanford datasets with performance values of about 5\% when compared to state-of-the-art      
### 79.Automatic Learning of Subword Dependent Model Scales  [ :arrow_down: ](https://arxiv.org/pdf/2110.09324.pdf)
>  To improve the performance of state-of-the-art automatic speech recognition systems it is common practice to include external knowledge sources such as language models or prior corrections. This is usually done via log-linear model combination using separate scaling parameters for each model. Typically these parameters are manually optimized on some held-out data. <br>In this work we propose to optimize these scaling parameters via automatic differentiation and stochastic gradient decent similar to the neural network model parameters. We show on the LibriSpeech (LBS) and Switchboard (SWB) corpora that the model scales for a combination of attentionbased encoder-decoder acoustic model and language model can be learned as effectively as with manual tuning. We further extend this approach to subword dependent model scales which could not be tuned manually which leads to 7% improvement on LBS and 3% on SWB. We also show that joint training of scales and model parameters is possible and gives additional 6% improvement on LBS.      
### 80.Power Systems Performance under 5G Radio Access Network in a Co-Simulation Environment  [ :arrow_down: ](https://arxiv.org/pdf/2110.09308.pdf)
>  Communication can improve control of important system parameters by allowing different grid components to communicate their states with each other. This information exchange requires a reliable and fast communication infrastructure. 5G communication can be a viable means to achieve this objective. This paper investigates the performance of several smart grid applications under a 5G radio access network. Different scenarios including set point changes and transients are evaluated, and the results indicate that the system maintains stability when a 5Gnetwork is used to communicate system states.      
### 81.Roles of Retailers in the Peer-to-Peer Electricity Market: A Single Retailer Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2110.09303.pdf)
>  Despite extensive research in the past five years and several successfully completed and on-going pilot projects, regulators are still reluctant to implement peer-to-peer trading at a large-scale in today's electricity market. The reason could partly be attributed to the perceived disadvantage of current market participants like retailers due to their exclusion from market participation - a fundamental property of decentralised peer-to-peer trading. As a consequence, recently, there has been growing pressure from energy service providers in favour of retailers' participation in peer-to-peer trading. However, the role of retailers in the peer-to-peer market is yet to be established as no existing study has challenged this fundamental circumspection of decentralized trading. In this context, this perspective takes the first step to discuss the feasibility of retailers' involvement in the peer-to-peer market. In doing so, we identify key characteristics of retail-based and peer-to-peer electricity markets and discuss our viewpoint on how to incorporate a single retailer in a peer-to-peer market without compromising the fundamental decision-making characteristics of both markets. Finally, we give an example of a hypothetical business model to demonstrate how a retailer can be a part of a peer-to-peer market with a promise of collective benefits for the participants.      
### 82.A Prior Guided Adversarial Representation Learning and Hypergraph Perceptual Network for Predicting Abnormal Connections of Alzheimer's Disease  [ :arrow_down: ](https://arxiv.org/pdf/2110.09302.pdf)
>  Alzheimer's disease is characterized by alterations of the brain's structural and functional connectivity during its progressive degenerative processes. Existing auxiliary diagnostic methods have accomplished the classification task, but few of them can accurately evaluate the changing characteristics of brain connectivity. In this work, a prior guided adversarial representation learning and hypergraph perceptual network (PGARL-HPN) is proposed to predict abnormal brain connections using triple-modality medical images. Concretely, a prior distribution from the anatomical knowledge is estimated to guide multimodal representation learning using an adversarial strategy. Also, the pairwise collaborative discriminator structure is further utilized to narrow the difference of representation distribution. Moreover, the hypergraph perceptual network is developed to effectively fuse the learned representations while establishing high-order relations within and between multimodal images. Experimental results demonstrate that the proposed model outperforms other related methods in analyzing and predicting Alzheimer's disease progression. More importantly, the identified abnormal connections are partly consistent with the previous neuroscience discoveries. The proposed model can evaluate characteristics of abnormal brain connections at different stages of Alzheimer's disease, which is helpful for cognitive disease study and early treatment.      
### 83.Reconfigurable Intelligent Surface-Enhanced OFDM Communications via Delay Adjustable Metasurface  [ :arrow_down: ](https://arxiv.org/pdf/2110.09291.pdf)
>  Reconfigurable intelligent surface (RIS) is a promising technology for establishing spectral- and energy-efficient wireless networks. In this paper, we study RIS-enhanced orthogonal frequency division multiplexing (OFDM) communications, which generalize the existing RIS-driven context focusing only on frequency-flat channels. Firstly, we introduce the delay adjustable metasurface (DAM) relying on varactor diodes. In contrast to existing reflecting elements, each one in DAM is capable of storing and retrieving the impinging electromagnetic waves upon dynamically controlling its electromagnetically induced transparency (EIT) properties, thus additionally imposing an extra delay onto the reflected incident signals. Secondly, we formulate the rate-maximization problem by jointly optimizing the transmit power allocation and the RIS reflection coefficients as well as the RIS delays. Furthermore, to address the coupling among optimization variables, we propose an efficient algorithm to achieve a high-quality solution for the formulated non-convex design problem by alternately optimizing the transmit power allocation and the RIS reflection pattern, including the reflection coefficients and the delays. Thirdly, to circumvent the high complexity for optimizing the RIS reflection coefficients, we conceive a low-complexity scheme upon aligning the strongest taps of all reflected channels, while ensuring that the maximum delay spread after introducing extra RIS delays does not exceed the length of the cyclic prefix (CP). Finally, simulation results demonstrate that the proposed design significantly improves the OFDM rate performance as well as the RIS's adaptability to wideband signals compared to baseline schemes without employing DAM.      
### 84.Gait-based Human Identification through Minimum Gait-phases and Sensors  [ :arrow_down: ](https://arxiv.org/pdf/2110.09286.pdf)
>  Human identification is one of the most common and critical tasks for condition monitoring, human-machine interaction, and providing assistive services in smart environments. Recently, human gait has gained new attention as a biometric for identification to achieve contactless identification from a distance robust to physical appearances. However, an important aspect of gait identification through wearables and image-based systems alike is accurate identification when limited information is available, for example, when only a fraction of the whole gait cycle or only a part of the subject body is visible. In this paper, we present a gait identification technique based on temporal and descriptive statistic parameters of different gait phases as the features and we investigate the performance of using only single gait phases for the identification task using a minimum number of sensors. It was shown that it is possible to achieve high accuracy of over 95.5 percent by monitoring a single phase of the whole gait cycle through only a single sensor. It was also shown that the proposed methodology could be used to achieve 100 percent identification accuracy when the whole gait cycle was monitored through pelvis and foot sensors combined. The ANN was found to be more robust to fewer data features compared to SVM and was concluded as the best machine algorithm for the purpose.      
### 85.SafeAccess+: An Intelligent System to make Smart Home Safer and Americans with Disability Act Compliant  [ :arrow_down: ](https://arxiv.org/pdf/2110.09273.pdf)
>  Smart homes are becoming ubiquitous, but they are not Americans with Disability Act (ADA) compliant. Smart homes equipped with ADA compliant appliances and services are critical for people with disabilities (i.e., visual impairments and limited mobility) to improve independence, safety, and quality of life. Despite all advancements in smart home technologies, some fundamental design and implementation issues remain. For example, people with disabilities often feel insecure to respond when someone knocks on the door or rings the doorbell. In this paper, we present an intelligent system called "SafeAccess+" to build safer and ADA compliant premises (e.g. smart homes, offices). The key functionalities of the SafeAccess+ are: 1) Monitoring the inside/outside of premises and identifying incoming people; 2) Providing users relevant information to assess incoming threats (e.g., burglary, robbery) and ongoing crimes 3) Allowing users to grant safe access to homes for friends/family members. We have addressed several technical and research challenges: - developing models to detect and recognize person/activity, generating image descriptions, designing ADA compliant end-end system. In addition, we have designed a prototype smart door showcasing the proof-of-concept. The premises are expected to be equipped with cameras placed in strategic locations that facilitate monitoring the premise 24/7 to identify incoming persons and to generate image descriptions. The system generates a pre-structured message from the image description to assess incoming threats and immediately notify the users. The completeness and generalization of models have been ensured through a rigorous quantitative evaluation. The users' satisfaction and reliability of the system has been measured using PYTHEIA scale and was rated excellent (Internal Consistency-Cronbach's alpha is 0.784, Test-retest reliability is 0.939 )      
### 86.Intent Classification Using Pre-Trained Embeddings For Low Resource Languages  [ :arrow_down: ](https://arxiv.org/pdf/2110.09264.pdf)
>  Building Spoken Language Understanding (SLU) systems that do not rely on language specific Automatic Speech Recognition (ASR) is an important yet less explored problem in language processing. In this paper, we present a comparative study aimed at employing a pre-trained acoustic model to perform SLU in low resource scenarios. Specifically, we use three different embeddings extracted using Allosaurus, a pre-trained universal phone decoder: (1) Phone (2) Panphone, and (3) Allo embeddings. These embeddings are then used in identifying the spoken intent. We perform experiments across three different languages: English, Sinhala, and Tamil each with different data sizes to simulate high, medium, and low resource scenarios. Our system improves on the state-of-the-art (SOTA) intent classification accuracy by approximately 2.11% for Sinhala and 7.00% for Tamil and achieves competitive results on English. Furthermore, we present a quantitative analysis of how the performance scales with the number of training examples used per intent.      
### 87.Efficient Sequence Training of Attention Models using Approximative Recombination  [ :arrow_down: ](https://arxiv.org/pdf/2110.09245.pdf)
>  Sequence discriminative training is a great tool to improve the performance of an automatic speech recognition system. It does, however, necessitate a sum over all possible word sequences, which is intractable to compute in practice. Current state-of-the-art systems with unlimited label context circumvent this problem by limiting the summation to an n-best list of relevant competing hypotheses obtained from beam search. <br>This work proposes to perform (approximative) recombinations of hypotheses during beam search, if they share a common local history. The error that is incurred by the approximation is analyzed and it is shown that using this technique the effective beam size can be increased by several orders of magnitude without significantly increasing the computational requirements. Lastly, it is shown that this technique can be used to effectively perform sequence discriminative training for attention-based encoder-decoder acoustic models on the LibriSpeech task.      
### 88.EIHW-MTG: Second DiCOVA Challenge System Report  [ :arrow_down: ](https://arxiv.org/pdf/2110.09239.pdf)
>  This work presents an outer product-based approach to fuse the embedded representations generated from the spectrograms of cough, breath, and speech samples for the automatic detection of COVID-19. To extract deep learnt representations from the spectrograms, we compare the performance of a CNN trained from scratch and a ResNet18 architecture fine-tuned for the task at hand. Furthermore, we investigate whether the patients' sex and the use of contextual attention mechanisms is beneficial. Our experiments use the dataset released as part of the Second Diagnosing COVID-19 using Acoustics (DiCOVA) Challenge. The results suggest the suitability of fusing breath and speech information to detect COVID-19. An Area Under the Curve (AUC) of 84.06% is obtained on the test partition when using a CNN trained from scratch with contextual attention mechanisms. When using the ResNet18 architecture for feature extraction, the baseline model scores the highest performance with an AUC of 84.26%.      
### 89.Learning Models for Query by Vocal Percussion: A Comparative Study  [ :arrow_down: ](https://arxiv.org/pdf/2110.09223.pdf)
>  The imitation of percussive sounds via the human voice is a natural and effective tool for communicating rhythmic ideas on the fly. Thus, the automatic retrieval of drum sounds using vocal percussion can help artists prototype drum patterns in a comfortable and quick way, smoothing the creative workflow as a result. Here we explore different strategies to perform this type of query, making use of both traditional machine learning algorithms and recent deep learning techniques. The main hyperparameters from the models involved are carefully selected by feeding performance metrics to a grid search algorithm. We also look into several audio data augmentation techniques, which can potentially regularise deep learning models and improve generalisation. We compare the final performances in terms of effectiveness (classification accuracy), efficiency (computational speed), stability (performance consistency), and interpretability (decision patterns), and discuss the relevance of these results when it comes to the design of successful query-by-vocal-percussion systems.      
### 90.Structured vector fitting framework for mechanical systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.09220.pdf)
>  In this paper, we develop a structure-preserving formulation of the data-driven vector fitting algorithm for the case of modally damped mechanical systems. Using the structured pole-residue form of the transfer function of modally damped second-order systems, we propose two possible structured extensions of the barycentric formula of system transfer functions. Integrating these new forms within the classical vector fitting algorithm leads to the formulation of two new algorithms that allow the computation of modally damped mechanical systems from data in a least squares fashion. Thus, the learned model is guaranteed to have the desired structure. We test the proposed algorithms on two benchmark models.      
### 91.A Primer on the Statistical Relation between Wireless Ultra-Reliability and Location Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2110.09215.pdf)
>  This letter statistically characterizes the impact of location estimation uncertainty in the wireless communication reliability, in which location information is used as a proxy to choose the rate. First, a Cramér-Rao bound for the localization error is derived. Then, through a simplified setup, we show that the reliability - characterized by how likely the outage probability is to be above a target threshold - can be sensitive to location errors, especially when the channel statistics are also sensitive to the location. Finally, we highlight the difficulty of choosing a rate that both meets target reliability and accounts for the location uncertainty, and that the most direct solutions suffer from being too conservative.      
### 92.Investigating Man-in-the-Middle-based False Data Injection in a Smart Grid Laboratory Environment  [ :arrow_down: ](https://arxiv.org/pdf/2110.09162.pdf)
>  With the increasing use of information and communication technology in electrical power grids, the security of energy supply is increasingly threatened by cyber-attacks. Traditional cyber-security measures, such as firewalls or intrusion detection/prevention systems, can be used as mitigation and prevention measures, but their effective use requires a deep understanding of the potential threat landscape and complex attack processes in energy information systems. Given the complexity and lack of detailed knowledge of coordinated, timed attacks in smart grid applications, we need information and insight into realistic attack scenarios in an appropriate and practical setting. In this paper, we present a man-in-the-middle-based attack scenario that intercepts process communication between control systems and field devices, employs false data injection techniques, and performs data corruption such as sending false commands to field devices. We demonstrate the applicability of the presented attack scenario in a physical smart grid laboratory environment and analyze the generated data under normal and attack conditions to extract domain-specific knowledge for detection mechanisms.      
### 93.Variance Reduction in Stochastic Reaction Networks using Control Variates  [ :arrow_down: ](https://arxiv.org/pdf/2110.09143.pdf)
>  Monte Carlo estimation in plays a crucial role in stochastic reaction networks. However, reducing the statistical uncertainty of the corresponding estimators requires sampling a large number of trajectories. We propose control variates based on the statistical moments of the process to reduce the estimators' variances. We develop an algorithm that selects an efficient subset of infinitely many control variates. To this end, the algorithm uses resampling and a redundancy-aware greedy selection. We demonstrate the efficiency of our approach in several case studies.      
### 94.SpecTNT: a Time-Frequency Transformer for Music Audio  [ :arrow_down: ](https://arxiv.org/pdf/2110.09127.pdf)
>  Transformers have drawn attention in the MIR field for their remarkable performance shown in natural language processing and computer vision. However, prior works in the audio processing domain mostly use Transformer as a temporal feature aggregator that acts similar to RNNs. In this paper, we propose SpecTNT, a Transformer-based architecture to model both spectral and temporal sequences of an input time-frequency representation. Specifically, we introduce a novel variant of the Transformer-in-Transformer (TNT) architecture. In each SpecTNT block, a spectral Transformer extracts frequency-related features into the frequency class token (FCT) for each frame. Later, the FCTs are linearly projected and added to the temporal embeddings (TEs), which aggregate useful information from the FCTs. Then, a temporal Transformer processes the TEs to exchange information across the time axis. By stacking the SpecTNT blocks, we build the SpecTNT model to learn the representation for music signals. In experiments, SpecTNT demonstrates state-of-the-art performance in music tagging and vocal melody extraction, and shows competitive performance for chord recognition. The effectiveness of SpecTNT and other design choices are further examined through ablation studies.      
### 95.Joint Spatial Division and Coaxial Multiplexing for Downlink Multi-User OAM Wireless Backhaul  [ :arrow_down: ](https://arxiv.org/pdf/2110.09123.pdf)
>  Orbital angular momentum (OAM) at radio frequency (RF) provides a novel approach of multiplexing a set of orthogonal modes on the same frequency channel to achieve high spectral efficiencies (SEs). However, the existing research on OAM wireless communications is mainly focused on pointto-point transmission in the line-of-sight (LoS) scenario. In this paper, we propose an overall scheme of the downlink multi-user OAM (MU-OAM) wireless backhaul based on uniform circular arrays (UCAs) for broadcasting networks, which can achieve the joint spatial division and coaxial multiplexing (JSDCM). A salient feature of the proposed downlink MU-OAM wireless backhaul systems is that the channel matrices are completely characterized by the position of each small base station (SBS), independent of the numbers of subcarriers and antennas, which avoids estimating large channel matrices required by the traditional downlink multi-user multiple-input multiple-output (MU-MIMO) wireless backhaul systems. Thereafter, we propose an OAM-based multiuser distance and angle of arrival (AoA) estimation method, which is able to simultaneously estimate the positions of multiple SBSs with a flexible number of training symbols. With the estimated distances and AoAs, a MU-OAM preprocessing scheme is applied to eliminate the co-mode and inter-mode interferences in the downlink MU-OAM channel. At last, the proposed methods are extended to the downlink MU-OAM-MIMO wireless backhaul system equipped with uniform concentric circular arrays (UCCAs), for which much higher spectral efficiency (SE) and energy efficiency (EE) than traditional MU-MIMO systems can be achieved. Both mathematical analysis and simulation results validate that the proposed scheme can effectively eliminate both interferences of the practical downlink MU-OAM channel and approaches the performance of the ideal MU-OAM channel.      
### 96.KaraTuner: Towards end to end natural pitch correction for singing voice in karaoke  [ :arrow_down: ](https://arxiv.org/pdf/2110.09121.pdf)
>  An automatic pitch correction system typically includes several stages, such as pitch extraction, deviation estimation, pitch shift processing, and cross-fade smoothing. However, designing these components with strategies often requires domain expertise and they are likely to fail on corner cases. In this paper, we present KaraTuner, an end-to-end neural architecture that predicts pitch curve and resynthesizes the singing voice directly from the tuned pitch and vocal spectrum extracted from the original recordings. Several vital technical points have been introduced in KaraTuner to ensure pitch accuracy, pitch naturalness, timbre consistency, and sound quality. A feed-forward Transformer is employed in the pitch predictor to capture long-term dependencies in the vocal spectrum and musical note. We also develop a pitch-controllable vocoder base on a novel source-filter block and the Fre-GAN architecture. KaraTuner obtains a higher preference than the rule-based pitch correction approach through A/B tests, and perceptual experiments show that the proposed vocoder achieves significant advantages in timbre consistency and sound quality compared with the parametric WORLD vocoder and phase vocoder.      
### 97.Real Additive Margin Softmax for Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2110.09116.pdf)
>  The additive margin softmax (AM-Softmax) loss has delivered remarkable performance in speaker verification. A supposed behavior of AM-Softmax is that it can shrink within-class variation by putting emphasis on target logits, which in turn improves margin between target and non-target classes. In this paper, we conduct a careful analysis on the behavior of AM-Softmax loss, and show that this loss does not implement real max-margin training. Based on this observation, we present a Real AM-Softmax loss which involves a true margin function in the softmax training. Experiments conducted on VoxCeleb1, SITW and CNCeleb demonstrated that the corrected AM-Softmax loss consistently outperforms the original one. The code has been released at <a class="link-external link-https" href="https://gitlab.com/csltstu/sunine" rel="external noopener nofollow">this https URL</a>.      
### 98.A Promising Technology for 6G Wireless Networks: Intelligent Reflecting Surface  [ :arrow_down: ](https://arxiv.org/pdf/2110.09114.pdf)
>  The intelligent information society, which is highly digitized, intelligence inspired and globally data driven, will be deployed in the next decade. The next 6G wireless communication networks are the key to achieve this grand blueprint, which is expected to connect everything, provide full dimensional wireless coverage and integrate all functions to support full-vertical applications. Recent research reveals that intelligent reflecting surface (IRS) with wireless environment control capability is a promising technology for 6G networks. Specifically, IRS can intelligently control the wavefront, e.g., the phase, amplitude, frequency, and even polarization by massive tunable elements, thus achieving fine-grained 3-D passive beamforming. In this paper, we first give a blueprint of the next 6G networks including the vision, typical scenarios and key performance indicators (KPIs). Then, we provide an overview of IRS including the new signal model, hardware architecture and competitive advantages in 6G networks. Besides, we discuss the potential application of IRS in the connectivity of 6G networks in detail, including intelligent and controllable wireless environment, ubiquitous connectivity, deep connectivity and holographic connectivity. At last, we summarize the challenges of IRS application and deployment in 6G networks. As a timely review of IRS, our summary will be of interest to both researchers and practitioners engaging in IRS for 6G networks.      
### 99.Patch-Based Deep Autoencoder for Point Cloud Geometry Compression  [ :arrow_down: ](https://arxiv.org/pdf/2110.09109.pdf)
>  The ever-increasing 3D application makes the point cloud compression unprecedentedly important and needed. In this paper, we propose a patch-based compression process using deep learning, focusing on the lossy point cloud geometry compression. Unlike existing point cloud compression networks, which apply feature extraction and reconstruction on the entire point cloud, we divide the point cloud into patches and compress each patch independently. In the decoding process, we finally assemble the decompressed patches into a complete point cloud. In addition, we train our network by a patch-to-patch criterion, i.e., use the local reconstruction loss for optimization, to approximate the global reconstruction optimality. Our method outperforms the state-of-the-art in terms of rate-distortion performance, especially at low bitrates. Moreover, the compression process we proposed can guarantee to generate the same number of points as the input. The network model of this method can be easily applied to other point cloud reconstruction problems, such as upsampling.      
### 100.LDNet: Unified Listener Dependent Modeling in MOS Prediction for Synthetic Speech  [ :arrow_down: ](https://arxiv.org/pdf/2110.09103.pdf)
>  An effective approach to automatically predict the subjective rating for synthetic speech is to train on a listening test dataset with human-annotated scores. Although each speech sample in the dataset is rated by several listeners, most previous works only used the mean score as the training target. In this work, we present LDNet, a unified framework for mean opinion score (MOS) prediction that predicts the listener-wise perceived quality given the input speech and the listener identity. We reflect recent advances in LD modeling, including design choices of the model architecture, and propose two inference methods that provide more stable results and efficient computation. We conduct systematic experiments on the voice conversion challenge (VCC) 2018 benchmark and a newly collected large-scale MOS dataset, providing an in-depth analysis of the proposed framework. Results show that the mean listener inference method is a better way to utilize the mean scores, whose effectiveness is more obvious when having more ratings per sample.      
### 101.Deep Learning-Based Power Control for Uplink Cell-Free Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.09001.pdf)
>  In this paper, a general framework for deep learning-based power control methods for max-min, max-product and max-sum-rate optimization in uplink cell-free massive multiple-input multiple-output (CF mMIMO) systems is proposed. Instead of using supervised learning, the proposed method relies on unsupervised learning, in which optimal power allocations are not required to be known, and thus has low training complexity. More specifically, a deep neural network (DNN) is trained to learn the map between fading coefficients and power coefficients within short time and with low computational complexity. It is interesting to note that the spectral efficiency of CF mMIMO systems with the proposed method outperforms previous optimization methods for max-min optimization and fits well for both max-sum-rate and max-product optimizations.      
### 102.Location Information Assisted Beamforming Design for Reconfigurable Intelligent Surface Aided Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.08980.pdf)
>  In reconfigurable intelligent surface (RIS) aided millimeter-wave (mmWave) communication systems, in order to overcome the limitation of the conventional channel state information (CSI) acquisition techniques, this paper proposes a location information assisted beamforming design without the requirement of the conventional channel training process. First, we establish the geometrical relation between the channel model and the user location, based on which we derive an approximate CSI error bound based on the user location error by means of Taylor approximation, triangle and power mean inequalities, and semidefinite relaxation (SDR). Second, for combating the uncertainty of the location error, we formulate a worst-case robust beamforming optimization problem. To solve the problem efficiently, we develop a novel iterative algorithm by utilizing various optimization tools such as Lagrange multiplier, matrix inversion lemma, SDR, as well as branch-and-bound (BnB). Particularly, the BnB algorithm is modified to acquire the phase shift solution under an arbitrary constraint of possible phase shift values. Finally, we analyse the algorithm complexity, and carry out simulations to validate the theoretical derivation of the CSI error bound and the robustness of the proposed algorithm. Compared with the existing non-robust approach and the robust beamforming techniques based on S-procedure and penalty convex-concave procedure (CCP), our method converges faster and achieves better performance in terms of the worst-case signal-to-noise ratio (SNR) at the receiver.      
### 103.Computing Semilinear Sparse Models for Approximately Eventually Periodic Signals  [ :arrow_down: ](https://arxiv.org/pdf/2110.08966.pdf)
>  Some elements of the theory and algorithmics corresponding to the computation of semilinear sparse models for discrete-time signals are presented. In this study, we will focus on approximately eventually periodic discrete-time signals, that is, signals that can exhibit an aperiodic behavior for an initial amount of time, and then become approximately periodic afterwards. The semilinear models considered in this study are obtained by combining sparse representation methods, linear autoregressive models and GRU neural network models, initially fitting each block model independently using some reference data corresponding to some signal under consideration, and then fitting some mixing parameters that are used to obtain a signal model consisting of a linear combination of the previously fitted blocks using the aforementioned reference data, computing sparse representations of some of the matrix parameters of the resulting model along the process. Some prototypical computational implementations are presented as well.      
### 104.Dynamic Slimmable Denoising Network  [ :arrow_down: ](https://arxiv.org/pdf/2110.08940.pdf)
>  Recently, tremendous human-designed and automatically searched neural networks have been applied to image denoising. However, previous works intend to handle all noisy images in a pre-defined static network architecture, which inevitably leads to high computational complexity for good denoising quality. Here, we present dynamic slimmable denoising network (DDS-Net), a general method to achieve good denoising quality with less computational complexity, via dynamically adjusting the channel configurations of networks at test time with respect to different noisy images. Our DDS-Net is empowered with the ability of dynamic inference by a dynamic gate, which can predictively adjust the channel configuration of networks with negligible extra computation cost. To ensure the performance of each candidate sub-network and the fairness of the dynamic gate, we propose a three-stage optimization scheme. In the first stage, we train a weight-shared slimmable super network. In the second stage, we evaluate the trained slimmable super network in an iterative way and progressively tailor the channel numbers of each layer with minimal denoising quality drop. By a single pass, we can obtain several sub-networks with good performance under different channel configurations. In the last stage, we identify easy and hard samples in an online way and train a dynamic gate to predictively select the corresponding sub-network with respect to different noisy images. Extensive experiments demonstrate our DDS-Net consistently outperforms the state-of-the-art individually trained static denoising networks.      
### 105.Deep Clustering For General-Purpose Audio Representations  [ :arrow_down: ](https://arxiv.org/pdf/2110.08895.pdf)
>  We introduce DECAR, a self-supervised pre-training approach for learning general-purpose audio representations. Our system is based on clustering: it utilizes an offline clustering step to provide target labels that act as pseudo-labels for solving a prediction task. We develop on top of recent advances in self-supervised learning for computer vision and design a lightweight, easy-to-use self-supervised pre-training scheme. We pre-train DECAR embeddings on a balanced subset of the large-scale Audioset dataset and transfer those representations to 9 downstream classification tasks, including speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct ablation studies identifying key design choices and also make all our code and pre-trained models publicly available.      
### 106.Exploring Novel Pooling Strategies for Edge Preserved Feature Maps in Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.08842.pdf)
>  With the introduction of anti-aliased convolutional neural networks (CNN), there has been some resurgence in relooking the way pooling is done in CNNs. The fundamental building block of the anti-aliased CNN has been the application of Gaussian smoothing before the pooling operation to reduce the distortion due to aliasing thereby making CNNs shift invariant. Wavelet based approaches have also been proposed as a possibility of additional noise removal capability and gave interesting results for even segmentation tasks. However, all the approaches proposed completely remove the high frequency components under the assumption that they are noise. However, by removing high frequency components, the edges in the feature maps are also smoothed. In this work, an exhaustive analysis of the edge preserving pooling options for classification, segmentation and autoencoders are presented. Two novel pooling approaches are presented such as Laplacian-Gaussian Concatenation with Attention (LGCA) pooling and Wavelet based approximate-detailed coefficient concatenation with attention (WADCA) pooling. The results suggest that the proposed pooling approaches outperform the conventional pooling as well as blur pooling for classification, segmentation and autoencoders.      
### 107.Compression-aware Projection with Greedy Dimension Reduction for Convolutional Neural Network Activations  [ :arrow_down: ](https://arxiv.org/pdf/2110.08828.pdf)
>  Convolutional neural networks (CNNs) achieve remarkable performance in a wide range of fields. However, intensive memory access of activations introduces considerable energy consumption, impeding deployment of CNNs on resourceconstrained edge devices. Existing works in activation compression propose to transform feature maps for higher compressibility, thus enabling dimension reduction. Nevertheless, in the case of aggressive dimension reduction, these methods lead to severe accuracy drop. To improve the trade-off between classification accuracy and compression ratio, we propose a compression-aware projection system, which employs a learnable projection to compensate for the reconstruction loss. In addition, a greedy selection metric is introduced to optimize the layer-wise compression ratio allocation by considering both accuracy and #bits reduction simultaneously. Our test results show that the proposed methods effectively reduce 2.91x~5.97x memory access with negligible accuracy drop on MobileNetV2/ResNet18/VGG16.      
### 108.Storage and Authentication of Audio Footage for IoAuT Devices Using Distributed Ledger Technology  [ :arrow_down: ](https://arxiv.org/pdf/2110.08821.pdf)
>  Detection of fabricated or manipulated audio content to prevent, e.g., distribution of forgeries in digital media, is crucial, especially in political and reputational contexts. Better tools for protecting the integrity of media creation are desired. Within the paradigm of the Internet of Audio Things(IoAuT), we discuss the ability of the IoAuT network to verify the authenticity of original audio using distributed ledger technology. By storing audio recordings in combination with associated recording-specific metadata obtained by the IoAuT capturing device, this architecture enables secure distribution of original audio footage, authentication of unknown audio content, and referencing of original audio material in future derivative works. By developing a proof-of-concept system, the feasibility of the proposed architecture is evaluated and discussed.      
### 109.On-board Fault Diagnosis of a Laboratory Mini SR-30 Gas Turbine Engine  [ :arrow_down: ](https://arxiv.org/pdf/2110.08820.pdf)
>  Inspired by recent progress in machine learning, a data-driven fault diagnosis and isolation (FDI) scheme is explicitly developed for failure in the fuel supply system and sensor measurements of the laboratory gas turbine system. A passive approach of fault diagnosis is implemented where a model is trained using machine learning classifiers to detect a given set of fault scenarios in real-time on which it is trained. Towards the end, a comparative study is presented for well-known classification techniques, namely Support vector classifier, linear discriminant analysis, K-neighbor, and decision trees. Several simulation studies were carried out to demonstrate and illustrate the proposed fault diagnosis scheme's advantages, capabilities, and performance.      
### 110.Divergence-degenerated spatial multiplexing towards ultrahigh capacity, low bit-error-rate optical communications  [ :arrow_down: ](https://arxiv.org/pdf/2110.08815.pdf)
>  Spatial mode (de)multiplexing of orbital angular momentum (OAM) beams is a promising solution to address future bandwidth issues, but the rapidly increasing divergence with the mode order severely limits the practically addressable number of OAM modes. Here we present a set of multi-vortex geometric beams (MVGBs) as high-dimensional information carriers, by virtue of three independent degrees of freedom (DoFs) including central OAM, sub-beam OAM, and coherent-state phase. The novel modal basis set has high divergence degeneracy, and highly consistent propagation behaviors among all spatial modes, capable of increasing the addressable spatial channels by two orders of magnitude than OAM basis as predicted. We experimentally realize the tri-DoF MVGB mode (de)multiplexing and shift keying encoding/decoding by the conjugated modulation method, demonstrating ultra-low bit error rates (BERs) caused by center offset and coherent background noise. Our work provides a useful basis for next generation of large-scale dense data communication.      
### 111.Taming Visually Guided Sound Generation  [ :arrow_down: ](https://arxiv.org/pdf/2110.08791.pdf)
>  Recent advances in visually-induced audio generation are based on sampling short, low-fidelity, and one-class sounds. Moreover, sampling 1 second of audio from the state-of-the-art model takes minutes on a high-end GPU. In this work, we propose a single model capable of generating visually relevant, high-fidelity sounds prompted with a set of frames from open-domain videos in less time than it takes to play it on a single GPU. <br>We train a transformer to sample a new spectrogram from the pre-trained spectrogram codebook given the set of video features. The codebook is obtained using a variant of VQGAN trained to produce a compact sampling space with a novel spectrogram-based perceptual loss. The generated spectrogram is transformed into a waveform using a window-based GAN that significantly speeds up generation. Considering the lack of metrics for automatic evaluation of generated spectrograms, we also build a family of metrics called FID and MKL. These metrics are based on a novel sound classifier, called Melception, and designed to evaluate the fidelity and relevance of open-domain samples. <br>Both qualitative and quantitative studies are conducted on small- and large-scale datasets to evaluate the fidelity and relevance of generated samples. We also compare our model to the state-of-the-art and observe a substantial improvement in quality, size, and computation time. Code, demo, and samples: <a class="link-external link-http" href="http://v-iashin.github.io/SpecVQGAN" rel="external noopener nofollow">this http URL</a>      
### 112.PixelPyramids: Exact Inference Models from Lossless Image Pyramids  [ :arrow_down: ](https://arxiv.org/pdf/2110.08787.pdf)
>  Autoregressive models are a class of exact inference approaches with highly flexible functional forms, yielding state-of-the-art density estimates for natural images. Yet, the sequential ordering on the dimensions makes these models computationally expensive and limits their applicability to low-resolution imagery. In this work, we propose Pixel-Pyramids, a block-autoregressive approach employing a lossless pyramid decomposition with scale-specific representations to encode the joint distribution of image pixels. Crucially, it affords a sparser dependency structure compared to fully autoregressive approaches. Our PixelPyramids yield state-of-the-art results for density estimation on various image datasets, especially for high-resolution data. For CelebA-HQ 1024 x 1024, we observe that the density estimates (in terms of bits/dim) are improved to ~44% of the baseline despite sampling speeds superior even to easily parallelizable flow-based models.      
### 113.Nonlinear Transform Induced Tensor Nuclear Norm for Tensor Completion  [ :arrow_down: ](https://arxiv.org/pdf/2110.08774.pdf)
>  The linear transform-based tensor nuclear norm (TNN) methods have recently obtained promising results for tensor completion. The main idea of this type of methods is exploiting the low-rank structure of frontal slices of the targeted tensor under the linear transform along the third mode. However, the low-rankness of frontal slices is not significant under linear transforms family. To better pursue the low-rank approximation, we propose a nonlinear transform-based TNN (NTTNN). More concretely, the proposed nonlinear transform is a composite transform consisting of the linear semi-orthogonal transform along the third mode and the element-wise nonlinear transform on frontal slices of the tensor under the linear semi-orthogonal transform, which are indispensable and complementary in the composite transform to fully exploit the underlying low-rankness. Based on the suggested low-rankness metric, i.e., NTTNN, we propose a low-rank tensor completion (LRTC) model. To tackle the resulting nonlinear and nonconvex optimization model, we elaborately design the proximal alternating minimization (PAM) algorithm and establish the theoretical convergence guarantee of the PAM algorithm. Extensive experimental results on hyperspectral images, multispectral images, and videos show that the our method outperforms linear transform-based state-of-the-art LRTC methods qualitatively and quantitatively.      
### 114.A Framework of Mahalanobis-Distance Metric with Supervised Learning for Clustering Multipath Components in MIMO Channel Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2110.08768.pdf)
>  As multipath components (MPCs) are experimentally observed to appear in clusters, cluster-based channel models have been focused in the wireless channel study. However, most of the MPC clustering algorithms for MIMO channels with delay and angle information of MPCs are based on the distance metric that quantifies the similarity of two MPCs and determines the preferred cluster shape, greatly impacting MPC clustering quality. In this paper, a general framework of Mahalanobis-distance metric is proposed for MPC clustering in MIMO channel analysis, without user-specified parameters. Remarkably, the popular multipath component distance (MCD) is proved to be a special case of the proposed distance metric framework. Furthermore, two machine learning algorithms, namely, weak-supervised Mahalanobis metric for clustering and supervised large margin nearest neighbor, are introduced to learn the distance metric. To evaluate the effectiveness, a modified channel model is proposed based on the 3GPP spatial channel model to generate clustered MPCs with delay and angular information, since the original 3GPP spatial channel model (SCM) is incapable to evaluate clustering quality. Experiment results show that the proposed distance metric can significantly improve the clustering quality of existing clustering algorithms, while the learning phase requires considerably limited efforts of labeling MPCs.      
### 115.Spectral Efficiency of OTFS Based Orthogonal Multiple Access with Rectangular Pulses  [ :arrow_down: ](https://arxiv.org/pdf/2110.08746.pdf)
>  In this paper we consider Orthogonal Time Frequency Space (OTFS) modulation based multiple-access (MA). We specifically consider Orthogonal MA methods (OMA) where the user terminals (UTs) are allocated non-overlapping physical resource in the delay-Doppler (DD) and/or time-frequency (TF) domain. To the best of our knowledge, in prior literature, the performance of OMA methods have been reported only for ideal transmit and receive pulses. In [20] and [21], OMA methods were proposed which were shown to achieve multi-user interference (MUI) free communication with ideal pulses. Since ideal pulses are not realizable, in this paper we study the spectral efficiency (SE) performance of these OMA methods with practical rectangular pulses. For these OMA methods, we derive the expression for the received DD domain symbols at the base station (BS) receiver and the effective DD domain channel matrix when rectangular pulses are used. We then derive the expression for the achievable sum SE. These expressions are also derived for another well known OMA method where guard bands (GB) are used to reduce MUI (called as the GB based MA methods) [19]. Through simulations, we observe that with rectangular pulses the sum SE achieved by the method in [21] is almost invariant of the Doppler shift and is higher than that achieved by the methods in [19], [20] at practical values of the received signal-to-noise ratio.      
### 116.Improving End-To-End Modeling for Mispronunciation Detection with Effective Augmentation Mechanisms  [ :arrow_down: ](https://arxiv.org/pdf/2110.08731.pdf)
>  Recently, end-to-end (E2E) models, which allow to take spectral vector sequences of L2 (second-language) learners' utterances as input and produce the corresponding phone-level sequences as output, have attracted much research attention in developing mispronunciation detection (MD) systems. However, due to the lack of sufficient labeled speech data of L2 speakers for model estimation, E2E MD models are prone to overfitting in relation to conventional ones that are built on DNN-HMM acoustic models. To alleviate this critical issue, we in this paper propose two modeling strategies to enhance the discrimination capability of E2E MD models, each of which can implicitly leverage the phonetic and phonological traits encoded in a pretrained acoustic model and contained within reference transcripts of the training data, respectively. The first one is input augmentation, which aims to distill knowledge about phonetic discrimination from a DNN-HMM acoustic model. The second one is label augmentation, which manages to capture more phonological patterns from the transcripts of training data. A series of empirical experiments conducted on the L2-ARCTIC English dataset seem to confirm the efficacy of our E2E MD model when compared to some top-of-the-line E2E MD models and a classic pronunciation-scoring based method built on a DNN-HMM acoustic model.      
### 117.AE-StyleGAN: Improved Training of Style-Based Auto-Encoders  [ :arrow_down: ](https://arxiv.org/pdf/2110.08718.pdf)
>  StyleGANs have shown impressive results on data generation and manipulation in recent years, thanks to its disentangled style latent space. A lot of efforts have been made in inverting a pretrained generator, where an encoder is trained ad hoc after the generator is trained in a two-stage fashion. In this paper, we focus on style-based generators asking a scientific question: Does forcing such a generator to reconstruct real data lead to more disentangled latent space and make the inversion process from image to latent space easy? We describe a new methodology to train a style-based autoencoder where the encoder and generator are optimized end-to-end. We show that our proposed model consistently outperforms baselines in terms of image inversion and generation quality. Supplementary, code, and pretrained models are available on the project website.      
### 118.Hand Gesture Recognition Using Temporal Convolutions and Attention Mechanism  [ :arrow_down: ](https://arxiv.org/pdf/2110.08717.pdf)
>  Advances in biosignal signal processing and machine learning, in particular Deep Neural Networks (DNNs), have paved the way for the development of innovative Human-Machine Interfaces for decoding the human intent and controlling artificial limbs. DNN models have shown promising results with respect to other algorithms for decoding muscle electrical activity, especially for recognition of hand gestures. Such data-driven models, however, have been challenged by their need for a large number of trainable parameters and their structural complexity. Here we propose the novel Temporal Convolutions-based Hand Gesture Recognition architecture (TC-HGR) to reduce this computational burden. With this approach, we classified 17 hand gestures via surface Electromyogram (sEMG) signals by the adoption of attention mechanisms and temporal convolutions. The proposed method led to 81.65% and 80.72% classification accuracy for window sizes of 300ms and 200ms, respectively. The number of parameters to train the proposed TC-HGR architecture is 11.9 times less than that of its state-of-the-art counterpart.      
### 119.Novel Secret-Key-Assisted Schemes for Secure MISOME-OFDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.08707.pdf)
>  We propose a new secure transmission scheme for uplink multiple-input single-output (MISO) orthogonal-frequency multiplexing (OFDM) systems in the presence of multiple eavesdroppers. Our proposed scheme utilizes the sub-channels orthogonality of OFDM systems to simultaneously transmit data and secret key symbols. The base station, Bob, shares secret key symbols with the legitimate user, Alice, using wiretap coding over a portion of the sub-channels. Concurrently, Alice uses the accumulated secret keys in her secret-key queue to encrypt data symbols using a one time pad (OTP) cipher and transmits them to Bob over the remaining sub-channels. if Alice did not accumulate sufficient keys in her secret-key queue, she employs wiretap coding to secure her data transmissions. We propose fixed and dynamic sub-channel allocation schemes to divide the sub-channels between data and secret keys. We derive the secrecy outage probability (SOP) and the secure throughput for the proposed scheme. We quantify the system's security under practical non-Gaussian transmissions where discrete signal constellation points are transmitted by the legitimate source nodes. Numerical results validate our theoretical findings and quantify the impact of different system design parameters.      
### 120.Visualization of Real-time Displacement Time History superimposed with Dynamic Experiments using Wireless Smart Sensors (WSS) and Augmented Reality (AR)  [ :arrow_down: ](https://arxiv.org/pdf/2110.08700.pdf)
>  Wireless Smart Sensors (WSS) process field data and inform structural engineers and owners about the infrastructure health and safety. In bridge engineering, inspectors make decisions using objective data from each bridge. They decide about repairs and replacements and prioritize the maintenance of certain structure elements on the basis of changes in displacements under loads. However, access to displacement information in the field and in real-time remains a challenge. Displacement data provided by WSS in the field undergoes additional processing and is seen at a different location by an inspector and a sensor specialist. When the data is shared and streamed to the field inspector, there is a inter-dependence between inspectors, sensor specialists, and infrastructure owners, which limits the actionability of the data related to the bridge condition. If inspectors were able to see structural displacements in real-time at the locations of interest, they could conduct additional observations, which would create a new, information-based, decision-making reality in the field. This paper develops a new, human-centered interface that provides inspectors with real-time access to actionable structural data (real-time displacements under loads) during inspection and monitoring enhanced by Augmented Reality (AR). It summarizes the development and validation of the new human-infrastructure interface and evaluates its efficiency through laboratory experiments. The experiments demonstrate that the interface accurately estimates dynamic displacements in comparison with the laser. Using this new AR interface tool, inspectors can observe and compare displacement data, share it across space and time, and visualize displacements in time history.      
### 121.Finding Critical Scenarios for Automated Driving Systems: A Systematic Literature Review  [ :arrow_down: ](https://arxiv.org/pdf/2110.08664.pdf)
>  Scenario-based approaches have been receiving a huge amount of attention in research and engineering of automated driving systems. Due to the complexity and uncertainty of the driving environment, and the complexity of the driving task itself, the number of possible driving scenarios that an ADS or ADAS may encounter is virtually infinite. Therefore it is essential to be able to reason about the identification of scenarios and in particular critical ones that may impose unacceptable risk if not considered. Critical scenarios are particularly important to support design, verification and validation efforts, and as a basis for a safety case. In this paper, we present the results of a systematic literature review in the context of autonomous driving. The main contributions are: (i) introducing a comprehensive taxonomy for critical scenario identification methods; (ii) giving an overview of the state-of-the-art research based on the taxonomy encompassing 86 papers between 2017 and 2020; and (iii) identifying open issues and directions for further research. The provided taxonomy comprises three main perspectives encompassing the problem definition (the why), the solution (the methods to derive scenarios), and the assessment of the established scenarios. In addition, we discuss open research issues considering the perspectives of coverage, practicability, and scenario space explosion.      
### 122.Dynamic Compressed Sensing of Unsteady Flows with a Mobile Robot  [ :arrow_down: ](https://arxiv.org/pdf/2110.08658.pdf)
>  Large-scale environmental sensing with a finite number of mobile sensor is a challenging task that requires a lot of resources and time. This is especially true when features in the environment are spatiotemporally changing with unknown or partially known dynamics. However, these dynamic features often evolve in a low-dimensional space, making it possible to capture their dynamics sufficiently well with only one or several properly planned mobile sensors. This paper investigates the problem of dynamic compressed sensing (DCS) of an unsteady flow field, which takes advantage of the inherently low dimensionality of the underlying flow dynamics to reduce number of waypoints for a mobile sensing robot. The optimal sensing waypoints are identified by an iterative compressed sensing algorithm that optimizes the flow reconstruction based on the proper orthogonal decomposition (POD) modes. An optimized robot trajectory is then found to traverse these waypoints while minimizing the energy consumption, time, and flow reconstruction error. Simulation results in an unsteady double-gyre flow field is presented to demonstrate the efficacy of the proposed algorithms. Experimental results with an indoor quadcopter are presented to show the feasibility of the resulting trajectory.      
### 123.Towards Robust Waveform-Based Acoustic Models  [ :arrow_down: ](https://arxiv.org/pdf/2110.08634.pdf)
>  We propose an approach for learning robust acoustic models in adverse environments, characterized by a significant mismatch between training and test conditions. This problem is of paramount importance for the deployment of speech recognition systems that need to perform well in unseen environments. Our approach is an instance of vicinal risk minimization, which aims to improve risk estimates during training by replacing the delta functions that define the empirical density over the input space with an approximation of the marginal population density in the vicinity of the training samples. More specifically, we assume that local neighborhoods centered at training samples can be approximated using a mixture of Gaussians, and demonstrate theoretically that this can incorporate robust inductive bias into the learning process. We characterize the individual mixture components implicitly via data augmentation schemes, designed to address common sources of spurious correlations in acoustic models. To avoid potential confounding effects on robustness due to information loss, which has been associated with standard feature extraction techniques (e.g., FBANK and MFCC features), we focus our evaluation on the waveform-based setting. Our empirical results show that the proposed approach can generalize to unseen noise conditions, with 150% relative improvement in out-of-distribution generalization compared to training using the standard risk minimization principle. Moreover, the results demonstrate competitive performance relative to models learned using a training sample designed to match the acoustic conditions characteristic of test utterances (i.e., optimal vicinal densities).      
### 124.Learning velocity model for complex media with deep convolutional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.08626.pdf)
>  The paper considers the problem of velocity model acquisition for a complex media based on boundary measurements. The acoustic model is used to describe the media. We used an open-source dataset of velocity distributions to compare the presented results with the previous works directly. Forward modeling is performed using the grid-characteristic numerical method. The inverse problem is solved using deep convolutional neural networks. Modifications for a baseline UNet architecture are proposed to improve both structural similarity index measure quantitative correspondence of the velocity profiles with the ground truth. We evaluate our enhancements and demonstrate the statistical significance of the results.      
### 125.Generative Adversarial Imitation Learning for End-to-End Autonomous Driving on Urban Environments  [ :arrow_down: ](https://arxiv.org/pdf/2110.08586.pdf)
>  Autonomous driving is a complex task, which has been tackled since the first self-driving car ALVINN in 1989, with a supervised learning approach, or behavioral cloning (BC). In BC, a neural network is trained with state-action pairs that constitute the training set made by an expert, i.e., a human driver. However, this type of imitation learning does not take into account the temporal dependencies that might exist between actions taken in different moments of a navigation trajectory. These type of tasks are better handled by reinforcement learning (RL) algorithms, which need to define a reward function. On the other hand, more recent approaches to imitation learning, such as Generative Adversarial Imitation Learning (GAIL), can train policies without explicitly requiring to define a reward function, allowing an agent to learn by trial and error directly on a training set of expert trajectories. In this work, we propose two variations of GAIL for autonomous navigation of a vehicle in the realistic CARLA simulation environment for urban scenarios. Both of them use the same network architecture, which process high dimensional image input from three frontal cameras, and other nine continuous inputs representing the velocity, the next point from the sparse trajectory and a high-level driving command. We show that both of them are capable of imitating the expert trajectory from start to end after training ends, but the GAIL loss function that is augmented with BC outperforms the former in terms of convergence time and training stability.      
### 126.Grayscale Based Algorithm for Remote Sensing with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2110.08493.pdf)
>  Remote sensing is the image acquisition of a target without having physical contact with it. Nowadays remote sensing data is widely preferred due to its reduced image acquisition period. The remote sensing of ground targets is more challenging because of the various factors that affect the propagation of light through different mediums from a satellite acquisition. Several Convolutional Neural Network-based algorithms are being implemented in the field of remote sensing. Supervised learning is a machine learning technique where the data is labelled according to their classes prior to the training. In order to detect and classify the targets more accurately, YOLOv3, an algorithm based on bounding and anchor boxes is adopted. In order to handle the various effects of light travelling through the atmosphere, Grayscale based YOLOv3 configuration is introduced. For better prediction and for solving the Rayleigh scattering effect, RGB based grayscale algorithms are proposed. The acquired images are analysed and trained with the grayscale based YOLO3 algorithm for target detection. The results show that the grayscale-based method can sense the target more accurately and effectively than the traditional YOLOv3 approach.      
### 127.Controllable Multichannel Speech Dereverberation based on Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.08439.pdf)
>  Neural network based speech dereverberation has achieved promising results in recent studies. Nevertheless, many are focused on recovery of only the direct path sound and early reflections, which could be beneficial to speech perception, are discarded. The performance of a model trained to recover clean speech degrades when evaluated on early reverberation targets, and vice versa. This paper proposes a novel deep neural network based multichannel speech dereverberation algorithm, in which the dereverberation level is controllable. This is realized by adding a simple floating-point number as target controller of the model. Experiments are conducted using spatially distributed microphones, and the efficacy of the proposed algorithm is confirmed in various simulated conditions.      
### 128.NN3A: Neural Network supported Acoustic Echo Cancellation, Noise Suppression and Automatic Gain Control for Real-Time Communications  [ :arrow_down: ](https://arxiv.org/pdf/2110.08437.pdf)
>  Acoustic echo cancellation (AEC), noise suppression (NS) and automatic gain control (AGC) are three often required modules for real-time communications (RTC). This paper proposes a neural network supported algorithm for RTC, namely NN3A, which incorporates an adaptive filter and a multi-task model for residual echo suppression, noise reduction and near-end speech activity detection. The proposed algorithm is shown to outperform both a method using separate models and an end-to-end alternative. It is further shown that there exists a trade-off in the model between residual suppression and near-end speech distortion, which could be balanced by a novel loss weighting function. Several practical aspects of training the joint model are also investigated to push its performance to limit.      
### 129.Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet  [ :arrow_down: ](https://arxiv.org/pdf/2110.08352.pdf)
>  From wearables to powerful smart devices, modern automatic speech recognition (ASR) models run on a variety of edge devices with different computational budgets. To navigate the Pareto front of model accuracy vs model size, researchers are trapped in a dilemma of optimizing model accuracy by training and fine-tuning models for each individual edge device while keeping the training GPU-hours tractable. In this paper, we propose Omni-sparsity DNN, where a single neural network can be pruned to generate optimized model for a large range of model sizes. We develop training strategies for Omni-sparsity DNN that allows it to find models along the Pareto front of word-error-rate (WER) vs model size while keeping the training GPU-hours to no more than that of training one singular model. We demonstrate the Omni-sparsity DNN with streaming E2E ASR models. Our results show great saving on training time and resources with similar or better accuracy on LibriSpeech compared to individually pruned sparse models: 2%-6.6% better WER on Test-other.      
### 130.Differentiable Network Pruning for Microcontrollers  [ :arrow_down: ](https://arxiv.org/pdf/2110.08350.pdf)
>  Embedded and personal IoT devices are powered by microcontroller units (MCUs), whose extreme resource scarcity is a major obstacle for applications relying on on-device deep learning inference. Orders of magnitude less storage, memory and computational capacity, compared to what is typically required to execute neural networks, impose strict structural constraints on the network architecture and call for specialist model compression methodology. In this work, we present a differentiable structured network pruning method for convolutional neural networks, which integrates a model's MCU-specific resource usage and parameter importance feedback to obtain highly compressed yet accurate classification models. Our methodology (a) improves key resource usage of models up to 80x; (b) prunes iteratively while a model is trained, resulting in little to no overhead or even improved training time; (c) produces compressed models with matching or improved resource usage up to 1.7x in less time compared to prior MCU-specific methods. Compressed models are available for download.      
### 131.Regional Stability Analysis of Transitional Fluid Flows  [ :arrow_down: ](https://arxiv.org/pdf/2110.08341.pdf)
>  A method to bound the maximum energy perturbation for which regional stability of transitional fluid flow models can be guaranteed is introduced. The proposed method exploits the fact that the fluid model's nonlinearities are both lossless and locally bounded and uses the axes lengths of the ellipsoids for the trajectory set containment as variables in the stability conditions. Compared to existing approaches, the proposed method leads to an average increase in the maximum allowable energy perturbation of 29% for the Waleffe-KimHamilton (WKH) shear flow model and of 38% for the 9-state reduced model of Couette flow.      
### 132.Solving Image PDEs with a Shallow Network  [ :arrow_down: ](https://arxiv.org/pdf/2110.08327.pdf)
>  Partial differential equations (PDEs) are typically used as models of physical processes but are also of great interest in PDE-based image processing. However, when it comes to their use in imaging, conventional numerical methods for solving PDEs tend to require very fine grid resolution for stability, and as a result have impractically high computational cost. This work applies BLADE (Best Linear Adaptive Enhancement), a shallow learnable filtering framework, to PDE solving, and shows that the resulting approach is efficient and accurate, operating more reliably at coarse grid resolutions than classical methods. As such, the model can be flexibly used for a wide variety of problems in imaging.      
### 133.Non-Euclidean Contractivity of Recurrent Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.08298.pdf)
>  Critical questions in neuroscience and machine learning can be addressed by establishing strong stability, robustness, entrainment, and computational efficiency properties of neural network models. The usefulness of such strong properties motivates the development of a comprehensive contractivity theory for neural networks. This paper makes two sets of contributions. First, we develop novel general results on non-Euclidean matrix measures and nonsmooth contraction theory. Regarding $\ell_1/\ell_\infty$ matrix measures, we show their quasiconvexity with respect to positive diagonal weights, their monotonicity with respect to principal submatrices, and provide closed form expressions for certain matrix polytopes. These results motivate the introduction of M-Hurwitz matrices, i.e., matrices whose Metzler majorant is Hurwitz. Regarding nonsmooth contraction theory, we show that the one-sided Lipschitz constant of a Lipschitz vector field is equal to the essential supremum of the matrix measure of its Jacobian. Second, we apply these general results to classes of recurrent neural circuits, including Hopfield, firing rate, Persidskii, Lur'e and other models. For each model, we compute the optimal contraction rate and weighted non-Euclidean norm via a linear program or, in some special cases, via an $M$-Hurwitz condition on the synaptic matrix. Our analysis establishes also absolute contraction and total contraction.      
