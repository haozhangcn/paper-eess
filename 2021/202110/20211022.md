# ArXiv eess --Fri, 22 Oct 2021
### 1.Improving Channel Charting using a Split Triplet Loss and an Inertial Regularizer  [ :arrow_down: ](https://arxiv.org/pdf/2110.11279.pdf)
>  Channel charting is an emerging technology that enables self-supervised pseudo-localization of user equipments by performing dimensionality reduction on large channel-state information (CSI) databases that are passively collected at infrastructure base stations or access points. In this paper, we introduce a new dimensionality reduction method specifically designed for channel charting using a novel split triplet loss, which utilizes physical information available during the CSI acquisition process. In addition, we propose a novel regularizer that exploits the physical concept of inertia, which significantly improves the quality of the learned channel charts. We provide an experimental verification of our methods using synthetic and real-world measured CSI datasets, and we demonstrate that our methods are able to outperform the state-of-the-art in channel charting based on the triplet loss.      
### 2.Multimode Diagnosis for Switched Affine Systems with Noisy Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2110.11253.pdf)
>  We study a diagnosis scheme to reliably detect the active mode of discrete-time, switched affine systems in the presence of measurement noise and asynchronous switching. The proposed scheme consists of two parts: (i) the construction of a bank of filters, and (ii) the introduction of a residual/threshold-based diagnosis rule. We develop an exact finite optimization-based framework to numerically solve an optimal bank of filters in which the contribution of the measurement noise to the residual is minimized. The design problem is safely approximated through linear matrix inequalities and thus becomes tractable. We further propose a thresholding policy along with probabilistic false-alarm guarantees to estimate the active system mode in real-time. In comparison with the existing results, the guarantees improve from a polynomial dependency in the probability of false-alarm to a logarithmic form. This improvement is achieved under the additional assumption of sub-Gaussianity, which is expected in many applications. The performance of the proposed diagnosis filters is validated through a synthesis numerical example and an application of the building radiant system.      
### 3.rct: random consistency training for semi-supervised sound event detection  [ :arrow_down: ](https://arxiv.org/pdf/2110.11144.pdf)
>  Sound event detection (SED), as a core module of acoustic environmental analysis, suffers from the problem of data deficiency. The integration of semi-supervised learning (SSL) largely mitigates such problem while bringing no extra annotation budget. This paper researches on several core modules of SSL, and introduces a random consistency training (RCT) strategy. First, a self-consistency loss is proposed to fuse with the teacher-student model to stabilize the training. Second, a hard mixup data augmentation is proposed to account for the additive property of sounds. Third, a random augmentation scheme is applied to flexibly combine different types of data augmentations. Experiments show that the proposed strategy outperform other widely-used strategies.      
### 4.Newtonian Mechanics Based Transient Stability PART II: Individual Machine  [ :arrow_down: ](https://arxiv.org/pdf/2110.11018.pdf)
>  The paper analyzes the mechanisms of the individual-machine and also its advantages in TSA. Based on the critical-machine monitoring of the original system trajectory, it is clarified that the individual-machine strictly follows the machine paradigms. These strict followings of the paradigms bring the two advantages of the individual-machine method in TSA: (i) the individual-machine trajectory stability is characterized precisely, and (ii) the individual-machine trajectory variance is depicted clearly at IMPP. The two advantages are fully reflected in the precise definitions of individual-machine based transient stability concepts. In particular, the critical machine swing is clearly depicted through the IDSP or IDLP of the machine, the critical stability of the system is strictly defined as the critical stability of the most-severely disturbed machine, and the individual-machine potential energy surface is also precisely modeled through the IMPE of the machine. Simulation results show the effectiveness of the individual-machine in TSA.      
### 5.Towards Reducing Aleatoric Uncertainty for Medical Imaging Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2110.11012.pdf)
>  In safety-critical applications like medical diagnosis, certainty associated with a model's prediction is just as important as its accuracy. Consequently, uncertainty estimation and reduction play a crucial role. Uncertainty in predictions can be attributed to noise or randomness in data (aleatoric) and incorrect model inferences (epistemic). While model uncertainty can be reduced with more data or bigger models, aleatoric uncertainty is more intricate. This work proposes a novel approach that interprets data uncertainty estimated from a self-supervised task as noise inherent to the data and utilizes it to reduce aleatoric uncertainty in another task related to the same dataset via data augmentation. The proposed method was evaluated on a benchmark medical imaging dataset with image reconstruction as the self-supervised task and segmentation as the image analysis task. Our findings demonstrate the effectiveness of the proposed approach in significantly reducing the aleatoric uncertainty in the image segmentation task while achieving better or on-par performance compared to the standard augmentation techniques.      
### 6.2020 CATARACTS Semantic Segmentation Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2110.10965.pdf)
>  Surgical scene segmentation is essential for anatomy and instrument localization which can be further used to assess tissue-instrument interactions during a surgical procedure. In 2017, the Challenge on Automatic Tool Annotation for cataRACT Surgery (CATARACTS) released 50 cataract surgery videos accompanied by instrument usage annotations. These annotations included frame-level instrument presence information. In 2020, we released pixel-wise semantic annotations for anatomy and instruments for 4670 images sampled from 25 videos of the CATARACTS training set. The 2020 CATARACTS Semantic Segmentation Challenge, which was a sub-challenge of the 2020 MICCAI Endoscopic Vision (EndoVis) Challenge, presented three sub-tasks to assess participating solutions on anatomical structure and instrument segmentation. Their performance was assessed on a hidden test set of 531 images from 10 videos of the CATARACTS test set.      
### 7.Joint Design of Transmit Waveform and Receive Filter for MIMO Radar with One-Bit DACs/ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2110.10960.pdf)
>  Adopting extremely low-resolution (e.g. one-bit) analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) is able to bring a remarkable saving of low-cost and circuit power for multiple-input multiple-output (MIMO) <a class="link-external link-http" href="http://radar.In" rel="external noopener nofollow">this http URL</a> this paper, the problem of joint design of transmit waveform and receive filter for collocated MIMO radar with a architecture of one-bit ADCs and DACs is investigated. Under this architecture, we derive the output quantized signal-to-interference-plus-noise ratio (QSINR), which is relative to the detection performance of target, in the presence of signal-dependent interference. The optimization problem is formulated by maximizing the QSINR with a binary waveform constraint. Due to the nonconvex objective and binary constraint, the resulting problem is hard to be directly solved. To this end, we propose an alternating minimization algorithm. More concretely, at each iteration, the closed-form solution of the receive filter is attained by exploiting the minimum variance distortionless response (MVDR) method, and then the one-bit waveform is optimized with the aid of the alternating direction method of multipliers (ADMM) algorithm. In addition, the performance gap between the one-bit MIMO radar and infinite-bit MIMO radar is theoretically analyzed under the noise-only case. Several numerical simulations are provided to demonstrate the effectiveness of the proposed methods.      
### 8.Evaluation of Various Open-Set Medical Imaging Tasks with Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.10888.pdf)
>  The current generation of deep neural networks has achieved close-to-human results on "closed-set" image recognition; that is, the classes being evaluated overlap with the training classes. Many recent methods attempt to address the importance of the unknown, which are termed "open-set" recognition algorithms, try to reject unknown classes as well as maintain high recognition accuracy on known classes. However, it is still unclear how different general domain-trained open-set methods from ImageNet would perform on a different but more specific domain, such as the medical domain. Without principled and formal evaluations to measure the effectiveness of those general open-set methods, artificial intelligence (AI)-based medical diagnostics would experience ineffective adoption and increased risks of bad decision making. In this paper, we conduct rigorous evaluations amongst state-of-the-art open-set methods, exploring different open-set scenarios from "similar-domain" to "different-domain" scenarios and comparing them on various general and medical domain datasets. We summarise the results and core ideas and explain how the models react to various degrees of openness and different distributions of open classes. We show the main difference between general domain-trained and medical domain-trained open-set models with our quantitative and qualitative analysis of the results. We also identify aspects of model robustness in real clinical workflow usage according to confidence calibration and the inference efficiency.      
### 9.CXR-Net: An Encoder-Decoder-Encoder Multitask Deep Neural Network for Explainable and Accurate Diagnosis of COVID-19 pneumonia with Chest X-ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.10813.pdf)
>  Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal patient treatment. Chest X-Ray (CXR) is the first line imaging test for COVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible. Inspired by the success of deep learning (DL) in computer vision, many DL-models have been proposed to detect COVID-19 pneumonia using CXR images. Unfortunately, these deep classifiers lack the transparency in interpreting findings, which may limit their applications in clinical practice. The existing commonly used visual explanation methods are either too noisy or imprecise, with low resolution, and hence are unsuitable for diagnostic purposes. In this work, we propose a novel explainable deep learning framework (CXRNet) for accurate COVID-19 pneumonia detection with an enhanced pixel-level visual explanation from CXR images. The proposed framework is based on a new Encoder-Decoder-Encoder multitask architecture, allowing for both disease classification and visual explanation. The method has been evaluated on real world CXR datasets from both public and private data sources, including: healthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases The experimental results demonstrate that the proposed method can achieve a satisfactory level of accuracy and provide fine-resolution classification activation maps for visual explanation in lung disease detection. The Average Accuracy, the Precision, Recall and F1-score of COVID-19 pneumonia reached 0.879, 0.985, 0.992 and 0.989, respectively. We have also found that using lung segmented (CXR) images can help improve the performance of the model. The proposed method can provide more detailed high resolution visual explanation for the classification decision, compared to current state-of-the-art visual explanation methods and has a great potential to be used in clinical practice for COVID-19 pneumonia diagnosis.      
### 10.REAL-M: Towards Speech Separation on Real Mixtures  [ :arrow_down: ](https://arxiv.org/pdf/2110.10812.pdf)
>  In recent years, deep learning based source separation has achieved impressive results. Most studies, however, still evaluate separation models on synthetic datasets, while the performance of state-of-the-art techniques on in-the-wild speech data remains an open question. This paper contributes to fill this gap in two ways. First, we release the REAL-M dataset, a crowd-sourced corpus of real-life mixtures. Secondly, we address the problem of performance evaluation of real-life mixtures, where the ground truth is not available. We bypass this issue by carefully designing a blind Scale-Invariant Signal-to-Noise Ratio (SI-SNR) neural estimator. Through a user study, we show that our estimator reliably evaluates the separation performance on real mixtures. The performance predictions of the SI-SNR estimator indeed correlate well with human opinions. Moreover, we observe that the performance trends predicted by our estimator on the REAL-M dataset closely follow those achieved on synthetic benchmarks when evaluating popular speech separation models.      
### 11.Modeling Human-Human Collaboration: A Connection Between Inter-Personal Motor Synergy and Consensus Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2110.10791.pdf)
>  Many day-to-day activities involve people working collaboratively toward reaching a desired outcome. Previous research in motor control and neuroscience have proposed inter-personal motor synergy (IPMS) as a mechanism of collaboration between people, referring to the idea of how two or more people may work together "as if they were one" to coordinate their motion. In motor control literature, uncontrolled manifold (UCM) is used for quantifying IPMS. According to this approach, coordinated motion is achieved through stabilization of a performance variable (e.g., an output in a collaborative output tracking task). We show that the UCM approach is closely related to the well-studied consensus approach in multi-agent systems that concerns processes by which a set of interacting agents agree on a shared objective. To explore the connection between these two approaches, in this paper, we provide a control-theoretic model that represents the systems-level behaviors in a collaborative task. In particular, we utilize the consensus protocol and show how the model can be systematically tuned to reproduce the behavior exhibited by human-human collaboration experiments. We discuss the association between the proposed control law and the UCM approach and validate our model using experimental results previously collected from an inter-personal finger force production task.      
### 12.Learning controllers for performance through LMI regions  [ :arrow_down: ](https://arxiv.org/pdf/2110.10777.pdf)
>  In an open-loop experiment, an input sequence is applied to an unknown linear time-invariant system (in continuous or discrete time) affected also by an unknown-but-bounded disturbance sequence (with an energy or instantaneous bound); the corresponding state sequence is measured. The goal is to design directly from the input and state sequences a controller that enforces a certain performance specification on the transient behaviour of the unknown system. The performance specification is expressed through a subset of the complex plane where closed-loop eigenvalues need to belong, a so called LMI region. For this control design problem, we provide here convex programs to enforce the performance specification from data in the form of linear matrix inequalities (LMI). For generic LMI regions, these are sufficient conditions to assign the eigenvalues within the LMI region for all possible dynamics consistent with data, and become necessary and sufficient conditions for special LMI regions. In this way, we extend classical model-based conditions from a seminal work in the literature to the setting of data-driven control from noisy data. Through two numerical examples, we investigate how these data-based conditions compare with each other.      
### 13.Ambiguities in Direction-of-Arrival Estimation with Linear Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2110.10756.pdf)
>  In this paper, we present a novel approach to compute ambiguities in thinned uniform linear arrays, i.e., sparse non-uniform linear arrays, via a mixed-integer program. Ambiguities arise when there exists a set of distinct directions-of-arrival, for which the corresponding steering matrix is rank-deficient and are associated with nonunique parameter estimation. Our approach uses Young tableaux for which a submatrix of the steering matrix has a vanishing determinant, which can be expressed through vanishing sums of unit roots. Each of these vanishing sums then corresponds to an ambiguous set of directions-of-arrival. We derive a method to enumerate such ambiguous sets using a mixed-integer program and present results on several examples.      
### 14.Toward Real-world Image Super-resolution via Hardware-based Adaptive Degradation Models  [ :arrow_down: ](https://arxiv.org/pdf/2110.10755.pdf)
>  Most single image super-resolution (SR) methods are developed on synthetic low-resolution (LR) and high-resolution (HR) image pairs, which are simulated by a predetermined degradation operation, e.g., bicubic downsampling. However, these methods only learn the inverse process of the predetermined operation, so they fail to super resolve the real-world LR images; the true formulation deviates from the predetermined operation. To address this problem, we propose a novel supervised method to simulate an unknown degradation process with the inclusion of the prior hardware knowledge of the imaging system. We design an adaptive blurring layer (ABL) in the supervised learning framework to estimate the target LR images. The hyperparameters of the ABL can be adjusted for different imaging hardware. The experiments on the real-world datasets validate that our degradation model can estimate LR images more accurately than the predetermined degradation operation, as well as facilitate existing SR methods to perform reconstructions on real-world LR images more accurately than the conventional approaches.      
### 15.The First Airborne Experiment of Sparse Microwave Imaging: Prototype System Design and Result Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2110.10675.pdf)
>  In this paper we report the first airborne experiments of sparse microwave imaging, conducted in September 2013 and May 2014, using our prototype sparse microwave imaging radar system. This is the first reported imaging radar system and airborne experiment that specially designed for sparse microwave imaging. Sparse microwave imaging is a novel concept of radar imaging, it is mainly the combination of traditional radar imaging technology and newly developed sparse signal processing theory, achieving benefits in both improving the imaging quality of current microwave imaging systems and designing optimized sparse microwave imaging radar system to reduce system sampling rate towards the sparse target scenes. During recent years, many researchers focus on related topics of sparse microwave imaging, but rarely few paid attention to prototype system design and experiment. We introduce our prototype sparse microwave imaging radar system, including its system design, hardware considerations and signal processing methods. Several design principles should be considered during the system designing, including the sampling scheme, antenna, SNR, waveform, resolution, etc. We select jittered sampling in azimuth and uniform sampling in range to balance the system complexity and performance. The imaging algorithm is accelerated $\ell_q$ regularization algorithm. To test the prototype radar system and verify the effectiveness of sparse microwave imaging framework, airborne experiments are carried out using our prototype system and we achieve the first sparse microwave image successfully. We analyze the imaging performance of prototype sparse microwave radar system with different sparsities, sampling rates, SNRs and sampling schemes, using three-dimensional phase transit diagram as the evaluation tool.      
### 16.Event-triggered Control for Nonlinear Systems with Center Manifolds  [ :arrow_down: ](https://arxiv.org/pdf/2110.10660.pdf)
>  In this work, we consider the problem of event-triggered implementation of control laws designed for the local stabilization of nonlinear systems with center manifolds. We propose event-triggering conditions which are derived from a local input-to-state stability characterization of such systems. The triggering conditions ensure local ultimate boundedness of the trajectories and the existence of a uniform positive lower bound for the inter-event times. The ultimate bound can be made arbitrarily small, but by allowing for smaller inter-event times. Under certain assumptions on the controller structure, local asymptotic stability of the origin is also guaranteed. Two sets of triggering conditions are proposed, that cater to the cases where the exact center manifold and only an approximation of the center manifold is computable. The closed-loop system exhibits some desirable properties when the exact knowledge of the center manifold is employed in checking the triggering conditions. Three illustrative examples that explore different scenarios are presented and the applicability of the proposed methods is demonstrated. The third example concerns the event-triggered implementation of a position stabilizing controller for the open-loop unstable Mobile Inverted Pendulum (MIP) robot.      
### 17.Combining Different V1 Brain Model Variants to Improve Robustness to Image Corruptions in CNNs  [ :arrow_down: ](https://arxiv.org/pdf/2110.10645.pdf)
>  While some convolutional neural networks (CNNs) have surpassed human visual abilities in object classification, they often struggle to recognize objects in images corrupted with different types of common noise patterns, highlighting a major limitation of this family of models. Recently, it has been shown that simulating a primary visual cortex (V1) at the front of CNNs leads to small improvements in robustness to these image perturbations. In this study, we start with the observation that different variants of the V1 model show gains for specific corruption types. We then build a new model using an ensembling technique, which combines multiple individual models with different V1 front-end variants. The model ensemble leverages the strengths of each individual model, leading to significant improvements in robustness across all corruption categories and outperforming the base model by 38% on average. Finally, we show that using distillation, it is possible to partially compress the knowledge in the ensemble model into a single model with a V1 front-end. While the ensembling and distillation techniques used here are hardly biologically-plausible, the results presented here demonstrate that by combining the specific strengths of different neuronal circuits in V1 it is possible to improve the robustness of CNNs for a wide range of perturbations.      
### 18.OSS-Net: Memory Efficient High Resolution Semantic Segmentation of 3D Medical Data  [ :arrow_down: ](https://arxiv.org/pdf/2110.10640.pdf)
>  Convolutional neural networks (CNNs) are the current state-of-the-art meta-algorithm for volumetric segmentation of medical data, for example, to localize COVID-19 infected tissue on computer tomography scans or the detection of tumour volumes in magnetic resonance imaging. A key limitation of 3D CNNs on voxelised data is that the memory consumption grows cubically with the training data resolution. Occupancy networks (O-Nets) are an alternative for which the data is represented continuously in a function space and 3D shapes are learned as a continuous decision boundary. While O-Nets are significantly more memory efficient than 3D CNNs, they are limited to simple shapes, are relatively slow at inference, and have not yet been adapted for 3D semantic segmentation of medical data. Here, we propose Occupancy Networks for Semantic Segmentation (OSS-Nets) to accurately and memory-efficiently segment 3D medical data. We build upon the original O-Net with modifications for increased expressiveness leading to improved segmentation performance comparable to 3D CNNs, as well as modifications for faster inference. We leverage local observations to represent complex shapes and prior encoder predictions to expedite inference. We showcase OSS-Net's performance on 3D brain tumour and liver segmentation against a function space baseline (O-Net), a performance baseline (3D residual U-Net), and an efficiency baseline (2D residual U-Net). OSS-Net yields segmentation results similar to the performance baseline and superior to the function space and efficiency baselines. In terms of memory efficiency, OSS-Net consumes comparable amounts of memory as the function space baseline, somewhat more memory than the efficiency baseline and significantly less than the performance baseline. As such, OSS-Net enables memory-efficient and accurate 3D semantic segmentation that can scale to high resolutions.      
### 19.PyPSA meets Africa: Developing an open source electricity network model of the African continent  [ :arrow_down: ](https://arxiv.org/pdf/2110.10628.pdf)
>  Electricity network modelling and grid simulations form a key enabling element for the integration of newer and cleaner technologies such as renewable energy generation and electric vehicles into the existing grid and energy system infrastructure. This paper reviews the models of the African electricity systems and highlights the gaps in the open model landscape. Using PyPSA (an open Power System Analysis package), the paper outlines the pathway to a fully open model and data to increase the transparency in the African electricity system planning. Optimisation and modelling can reveal viable pathways to a sustainable energy system, aiding strategic planning for upgrades and policy-making for accelerated integration of renewable energy generation and smart grid technologies such as battery storage in Africa.      
### 20.Vehicular Blockage Modelling and Performance Analysis for mmWave V2V Communications  [ :arrow_down: ](https://arxiv.org/pdf/2110.10576.pdf)
>  Vehicle-to-Everything (V2X) communications are revolutionizing the connectivity of transportation systems supporting safe and efficient road mobility. To meet the growing bandwidth eagerness of V2X services, millimeter-wave (e.g., 5G new radio over spectrum 26.50 - 48.20 GHz) and sub-THz (e.g., 120 GHz) frequencies are being investigated for the large available spectrum. Communication at these frequencies requires beam-type connectivity as a solution for the severe path loss attenuation. However, beams can be blocked, with negative consequences for communication reliability. Blockage prediction is necessary and challenging when the blocker is dynamic in high mobility scenarios such as Vehicle-to-Vehicle (V2V). This paper presents an analytical model to derive the unconditional probability of blockage in a highway multi-lane scenario. The proposed model accounts for the traffic density, the 3D dimensions of the vehicles, and the position of the antennas. Moreover, by setting the communication parameters and a target quality of service, it is possible to predict the signal-to-noise ratio distribution and the service probability, which can be used for resource scheduling. Exhaustive numerical results confirm the validity of the proposed model.      
### 21.Random-Fuzzy Dual Interpretation of Unknown Quantity for Estimation &amp; Recognition: with Demonstration of IMM Filter  [ :arrow_down: ](https://arxiv.org/pdf/2110.10572.pdf)
>  This paper is to consider the problems of estimation and recognition from the perspective of sigma-max inference (probability-possibility inference), with a focus on discovering whether some of the unknown quantities involved could be more faithfully modeled as fuzzy uncertainty. Two related key issues are addressed: 1) the random-fuzzy dual interpretation of unknown quantity being estimated; 2) the principle of selecting sigma-max operator for practical problems, such as estimation and recognition. Our perspective, conceived from definitions of randomness and fuzziness, is that continuous unknown quantity involved in estimation with inaccurate prior should be more appropriately modeled as randomness and handled by sigma inference; whereas discrete unknown quantity involved in recognition with insufficient (and inaccurate) prior could be better modeled as fuzziness and handled by max inference. The philosophy was demonstrated by an updated version of the well-known interacting multiple model (IMM) filter, for which the jump Markovian System is reformulated as a hybrid uncertainty system, with continuous state evolution modeled as usual as model-conditioned stochastic system and discrete mode transitions modeled as fuzzy system by a possibility (instead of probability) transition matrix, and hypotheses mixing is conducted by using the operation of "max" instead of "sigma". For our example of maneuvering target tracking using simulated data from both a short-range fire control radar and a long-range surveillance radar, the updated IMM filter shows significant improvement over the classic IMM filter, due to its peculiarity of hard decision of system model and a faster response to the transition of discrete mode.      
### 22.Receding Horizon Control in Deep Structured Teams: A Provably Tractable Large-Scale Approach with Application to Swarm Robotics  [ :arrow_down: ](https://arxiv.org/pdf/2110.10554.pdf)
>  In this paper, a deep structured tracking problem is introduced for a large number of decision-makers. The problem is formulated as a linear quadratic deep structured team, where the decision-makers wish to track a global target cooperatively while considering their local targets. For the unconstrained setup, the gauge transformation technique is used to decompose the resultant optimization problem in order to obtain a low-dimensional optimal control strategy in terms of the local and global Riccati equations. For the constrained case, however, the feasible set is not necessarily decomposable by the gauge transformation. To overcome this hurdle, we propose a family of local and global receding horizon control problems, where a carefully constructed linear combination of their solutions provides a feasible solution for the original constrained problem. The salient property of the above solutions is that they are tractable with respect to the number of decision-makers and can be implemented in a distributed manner. In addition, the main results are generalized to cases with multiple sub-populations and multiple features, including leader-follower setup, cohesive cost function and soft structural constraint. Furthermore, a class of cyber-physical attacks is proposed in terms of perturbed influence factors. A numerical example is presented to demonstrate the efficacy of the results.      
### 23.Hosting Capacity Approach Implications  [ :arrow_down: ](https://arxiv.org/pdf/2110.10551.pdf)
>  This paper revisits the generation hosting capacity (HC) calculation approach to account for grid operational flexibility--the ability to reconfigure the system safely. In essence, the generation hosting capacity is determined against the set of limiting factors--voltage, thermal (conductor loading), reverse flow (at the feeder head, station transformer, or substation), and change in the voltage (due to sudden change in generation output)). Not that long ago, California Investor-Owned Utilities (IOUs) added a new criterion that does not allow reverse flow at the supervisory control and data acquisition (SCADA) points that can change the system configuration, aiming to prevent the potential transfer of reverse flow to an adjacent feeder. This new criterion intended to capture operational constraints as part of hosting capacity-known as hosting capacity with operational flexibility (OpFlex). This paper explores the shortfalls of such an approach and proposes performing actual transfer analysis when determining hosting capacity rather than implementing the OpFlex approach. Furthermore, we discuss the need for transition to determining hosting capacity profile (all intervals) rather than a flat line (one, worst performing interval) hosting capacity. A hosting capacity profile would inform the developers of interval-by-interval limits and opportunities, creating new opportunities to reach higher penetration of DERs at a lower cost. With technological and computational advancements, such an approach is neither out of implementation reach nor that computationally expensive. In return, far more DER can be interconnected once programmed not to violate certain generation profiles as part of the interconnection requirement, and utilities would be better informed of their actual operational flexibility, benefiting society overall.      
### 24.Development and accuracy evaluation of Coded Phase-shift 3D scanner  [ :arrow_down: ](https://arxiv.org/pdf/2110.10520.pdf)
>  In this paper, we provide an overview of development of a structured light 3D-scanner based on combination of binary-coded patterns and sinusoidal phase-shifted fringe patterns called Coded Phase-shift technique. Further, we describe the experiments performed to evaluate measurement accuracy and precision of the developed system. A study of this kind is expected to be helpful in understanding the basic working of current structured-light 3D scanners and the approaches followed for their performance assessment.      
### 25.Transferring Reinforcement Learning for DC-DC Buck Converter Control via Duty Ratio Mapping: From Simulation to Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2110.10490.pdf)
>  Reinforcement learning (RL) control approach with application into power electronics systems has become an emerging topic whilst the sim-to-real issue remains a challenging problem as very few results can be referred to in the literature. Indeed, due to the inevitable mismatch between simulation models and real-life systems, offline trained RL control strategies may sustain unexpected hurdles in practical implementation during transferring procedure. As the main contribution of this paper, a transferring methodology via a delicately designed duty ratio mapping (DRM) is proposed for a DC-DC buck converter. Then, a detailed sim-to-real process is presented to enable the implementation of a model-free deep reinforcement learning (DRL) controller. The feasibility and effectiveness of the proposed methodology are demonstrated by comparative experimental studies.      
### 26.Evaluation of augmentation methods in classifying autism spectrum disorders from fMRI data with 3D convolutional neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2110.10489.pdf)
>  Classifying subjects as healthy or diseased using neuroimaging data has gained a lot of attention during the last 10 years. Here we apply deep learning to derivatives from resting state fMRI data, and investigate how different 3D augmentation techniques affect the test accuracy. Specifically, we use resting state derivatives from 1,112 subjects in ABIDE preprocessed to train a 3D convolutional neural network (CNN) to perform the classification. Our results show that augmentation only provide minor improvements to the test accuracy.      
### 27.Water Quality of Lake Mjøsa through Satellite Images: A Preliminary Study  [ :arrow_down: ](https://arxiv.org/pdf/2110.10465.pdf)
>  Since around 2010, the water quality in lake Mjøsa has begun to decline after an earlier successful effort to improve the water quality (\textit{Mjøsaksjonen}). In this study, we investigate the possibility of using satellite imagery to monitor the water quality of lake Mjøsa, and also compare the use of such an approach to current methods for monitoring the lake. Using satellite images for remote sensing has layers of complexity that were not apparent to us at first, and this paper summarizes some of our findings by discussing factors of water quality, state of the art for use of satellite imagery for water quality and some of our own initial results. While our results where not too accurate for this preliminary study, it gives an introduction to the field and some of the methods that may bring better results for further research. Satellite images can play an important role in monitoring changing waters. The field is in development and many factors of water quality may be analysed with increasing levels of accuracy.      
### 28.RSS-based Multiple Sources Localization with Unknown Log-normal Shadow Fading  [ :arrow_down: ](https://arxiv.org/pdf/2110.10435.pdf)
>  Multi-source localization based on received signal strength (RSS) has drawn great interest in wireless sensor networks. However, the shadow fading term caused by obstacles cannot be separated from the received signal, which leads to severe error in location estimate. In this paper, we approximate the log-normal sum distribution through Fenton-Wilkinson method to formulate a non-convex maximum likelihood (ML) estimator with unknown shadow fading factor. In order to overcome the difficulty in solving the non-convex problem, we propose a novel algorithm to estimate the locations of sources. Specifically, the region is divided into $N$ grids firstly, and the multi-source localization is converted into a sparse recovery problem so that we can obtain the sparse solution. Then we utilize the K-means clustering method to obtain the rough locations of the off-grid sources as the initial feasible point of the ML estimator. Finally, an iterative refinement of the estimated locations is proposed by dynamic updating of the localization dictionary. The proposed algorithm can efficiently approach a superior local optimal solution of the ML estimator. It is shown from the simulation results that the proposed method has a promising localization performance and improves the robustness for multi-source localization in unknown shadow fading environments. Moreover, the proposed method provides a better computational complexity from $O(K^3N^3)$ to $O(N^3)$.      
### 29.New Result on Interception of Stationary Targets at Arbitrary Time-Varying Velocity  [ :arrow_down: ](https://arxiv.org/pdf/2110.10433.pdf)
>  In this paper, some new results on time-varying missile against a stationary target using pure proportional navigation (PPN) are developed in the planar interception problem. First, the relative motion equation is established in arc-length domain based on the differential geometry theory, which eliminates the influence of time-varying missile speed. Then, the closed-form solution of time-varying speed missile intercepting stationary target with PPN is deduced, and the interception performance is analyzed. Additionally, considering the missile maneuvering acceleration limit, the capture region of time-varying speed missile is analyzed. Finally, the results derived in this paper are verified by numerical simulation analysis for various scenarios.      
### 30.Newtonian Mechanics Based Transient Stability PART I: Machine Paradigms  [ :arrow_down: ](https://arxiv.org/pdf/2110.10413.pdf)
>  Individual-machine, superimposed-machine and equivalent-machine can be seen as the three major perspectives of the power system transient stability. In this paper, the machine paradigms are established according to the common thinking among the three different machines. The machine paradigms comprise of the three components, i.e., trajectory paradigm, modeling paradigm and energy paradigm. The trajectory paradigm is the reflection of the trajectory stability; the modeling paradigm is the two-machine-system modeling of the trajectory stability; and the energy paradigm is the stability evaluation of the two-machine system. Based on this, it is clarified that the machine paradigms can be expressed into the individual machine form or the equivalent machine form. Then, the relationship between the machine stability and the system stability are analyzed. Simulation results show that the effectiveness of both the individual-machine and the equivalent machine is fully based on the strict followings of the machine paradigms.      
### 31.AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2110.10403.pdf)
>  Recent advances in transformer-based models have drawn attention to exploring these techniques in medical image segmentation, especially in conjunction with the U-Net model (or its variants), which has shown great success in medical image segmentation, under both 2D and 3D settings. Current 2D based methods either directly replace convolutional layers with pure transformers or consider a transformer as an additional intermediate encoder between the encoder and decoder of U-Net. However, these approaches only consider the attention encoding within one single slice and do not utilize the axial-axis information naturally provided by a 3D volume. In the 3D setting, convolution on volumetric data and transformers both consume large GPU memory. One has to either downsample the image or use cropped local patches to reduce GPU memory usage, which limits its performance. In this paper, we propose Axial Fusion Transformer UNet (AFTer-UNet), which takes both advantages of convolutional layers' capability of extracting detailed features and transformers' strength on long sequence modeling. It considers both intra-slice and inter-slice long-range cues to guide the segmentation. Meanwhile, it has fewer parameters and takes less GPU memory to train than the previous transformer-based models. Extensive experiments on three multi-organ segmentation datasets demonstrate that our method outperforms current state-of-the-art methods.      
### 32.Deep Learning for HDR Imaging: State-of-the-Art and Future Trends  [ :arrow_down: ](https://arxiv.org/pdf/2110.10394.pdf)
>  High dynamic range (HDR) imaging is a technique that allows an extensive dynamic range of exposures, which is important in image processing, computer graphics, and computer vision. In recent years, there has been a significant advancement in HDR imaging using deep learning (DL). This study conducts a comprehensive and insightful survey and analysis of recent developments in deep HDR imaging methodologies. We hierarchically and structurally group existing deep HDR imaging methods into five categories based on (1) number/domain of input exposures, (2) number of learning tasks, (3) novel sensor data, (4) novel learning strategies, and (5) applications. Importantly, we provide a constructive discussion on each category regarding its potential and challenges. Moreover, we review some crucial aspects of deep HDR imaging, such as datasets and evaluation metrics. Finally, we highlight some open problems and point out future research directions.      
### 33.Monolithic Integrated Multiband Acoustic Devices on Heterogeneous Substrate for Sub-6 GHz RF-FEMs  [ :arrow_down: ](https://arxiv.org/pdf/2110.10385.pdf)
>  Monolithic integration of multiband (1.4~ 6.0 GHz) RF acoustic devices were successfully demonstrated within the same process flow by using the lithium niobate (LN) thin film on silicon carbide (LNOSiC) substrate. A novel surface mode with sinking energy distribution was proposed, exhibiting reduced propagation loss. Surface wave and Lamb wave resonators with suppressed transverse modes and leaky modes were demonstrated, showing scalable resonances from 1.4 to 5.7 GHz, electromechanical coupling coefficients (k2) between 7.9% and 29.3%, and maximum Bode-Q (Qmax) larger than 3200. Arrayed filters with a small footprint (4.0 x 2.5 mm2) but diverse center frequencies (fc) and 3-dB fractional bandwidths (FBW) were achieved, showing fc from 1.4 to 6.0 GHz, FBW between 3.3% and 13.3%, and insertion loss (IL) between 0.59 and 2.10 dB. These results may promote the progress of hundred-filter sub-6 GHz RF front-end modules (RF-FEMs).      
### 34.Knowledge-Guided Multiview Deep Curriculum Learning for Elbow Fracture Classification  [ :arrow_down: ](https://arxiv.org/pdf/2110.10383.pdf)
>  Elbow fracture diagnosis often requires patients to take both frontal and lateral views of elbow X-ray radiographs. In this paper, we propose a multiview deep learning method for an elbow fracture subtype classification task. Our strategy leverages transfer learning by first training two single-view models, one for frontal view and the other for lateral view, and then transferring the weights to the corresponding layers in the proposed multiview network architecture. Meanwhile, quantitative medical knowledge was integrated into the training process through a curriculum learning framework, which enables the model to first learn from "easier" samples and then transition to "harder" samples to reach better performance. In addition, our multiview network can work both in a dual-view setting and with a single view as input. We evaluate our method through extensive experiments on a classification task of elbow fracture with a dataset of 1,964 images. Results show that our method outperforms two related methods on bone fracture study in multiple settings, and our technique is able to boost the performance of the compared methods. The code is available at <a class="link-external link-https" href="https://github.com/ljaiverson/multiview-curriculum" rel="external noopener nofollow">this https URL</a>.      
### 35.Medical Knowledge-Guided Deep Curriculum Learning for Elbow Fracture Diagnosis from X-Ray Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.10381.pdf)
>  Elbow fractures are one of the most common fracture types. Diagnoses on elbow fractures often need the help of radiographic imaging to be read and analyzed by a specialized radiologist with years of training. Thanks to the recent advances of deep learning, a model that can classify and detect different types of bone fractures needs only hours of training and has shown promising results. However, most existing deep learning models are purely data-driven, lacking incorporation of known domain knowledge from human experts. In this work, we propose a novel deep learning method to diagnose elbow fracture from elbow X-ray images by integrating domain-specific medical knowledge into a curriculum learning framework. In our method, the training data are permutated by sampling without replacement at the beginning of each training epoch. The sampling probability of each training sample is guided by a scoring criterion constructed based on clinically known knowledge from human experts, where the scoring indicates the diagnosis difficultness of different elbow fracture subtypes. We also propose an algorithm that updates the sampling probabilities at each epoch, which is applicable to other sampling-based curriculum learning frameworks. We design an experiment with 1865 elbow X-ray images for a fracture/normal binary classification task and compare our proposed method to a baseline method and a previous method using multiple metrics. Our results show that the proposed method achieves the highest classification performance. Also, our proposed probability update algorithm boosts the performance of the previous method.      
### 36.A Geometry-Based Stochastic Model for Truck Communication Channels in Freeway Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2110.10356.pdf)
>  Vehicle-to-vehicle (V2V) wireless communication systems are fundamental in many intelligent transportation applications, e.g., traffic load control, driverless vehicle, and collision avoidance. Hence, developing appropriate V2V communication systems and standardization require realistic V2V propagation channel models. However, most existing V2V channel modeling studies focus on car-to-car channels; only a few investigate truck-to-car (T2C) or truck-to-truck (T2T) channels. In this paper, a hybrid geometry-based stochastic model (GBSM) is proposed for T2X (T2C or T2T) channels in freeway environments. Next, we parameterize this GBSM from the extensive channel measurements. We extract the multipath components (MPCs) by using a joint maximum likelihood estimation (RiMAX) and then cluster the MPCs based on their evolution patterns.We classify the determined clusters as line-of-sight, multiple-bounce reflections from static interaction objects (IOs), multiple-bounce reflections from mobile IOs, multiple-bounce reflections, and diffuse scattering. Specifically, we model multiple-bounce reflections as double clusters following the COST 273/COST2100 method. This article presents the complete parameterization of the channel model. We validate this model by contrasting the root-mean-square delay spread and the angular spreads of departure/arrival derived from the channel model with the outcomes directly derived from the measurements.      
### 37.Computationally Efficient Safe Reinforcement Learning for Power Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.10333.pdf)
>  We propose a computationally efficient approach to safe reinforcement learning (RL) for frequency regulation in power systems with high levels of variable renewable energy resources. The approach draws on set-theoretic control techniques to craft a neural network-based control policy that is guaranteed to satisfy safety-critical state constraints, without needing to solve a model predictive control or projection problem in real time. By exploiting the properties of robust controlled-invariant polytopes, we construct a novel, closed-form "safety-filter" that enables end-to-end safe learning using any policy gradient-based RL algorithm. We then apply the safety filter in conjunction with the deep deterministic policy gradient (DDPG) algorithm to regulate frequency in a modified 9-bus power system, and show that the learned policy is more cost-effective than robust linear feedback control techniques while maintaining the same safety guarantee. We also show that the proposed paradigm outperforms DDPG augmented with constraint violation penalties.      
### 38.One model to enhance them all: array geometry agnostic multi-channel personalized speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2110.10330.pdf)
>  With the recent surge of video conferencing tools usage, providing high-quality speech signals and accurate captions have become essential to conduct day-to-day business or connect with friends and families. Single-channel personalized speech enhancement (PSE) methods show promising results compared with the unconditional speech enhancement (SE) methods in these scenarios due to their ability to remove interfering speech in addition to the environmental noise. In this work, we leverage spatial information afforded by microphone arrays to improve such systems' performance further. We investigate the relative importance of speaker embeddings and spatial features. Moreover, we propose a new causal array-geometry-agnostic multi-channel PSE model, which can generate a high-quality enhanced signal from arbitrary microphone geometry. Experimental results show that the proposed geometry agnostic model outperforms the model trained on a specific microphone array geometry in both speech quality and automatic speech recognition accuracy. We also demonstrate the effectiveness of the proposed approach for unseen array geometries.      
### 39.Identity Conversion for Emotional Speakers: A Study for Disentanglement of Emotion Style and Speaker Identity  [ :arrow_down: ](https://arxiv.org/pdf/2110.10326.pdf)
>  Expressive voice conversion performs identity conversion for emotional speakers by jointly converting speaker identity and speaker-dependent emotion style. Due to the hierarchical structure of speech emotion, it is challenging to disentangle the speaker-dependent emotional style for expressive voice conversion. Motivated by the recent success on speaker disentanglement with variational autoencoder (VAE), we propose an expressive voice conversion framework which can effectively disentangle linguistic content, speaker identity, pitch, and emotional style information. We study the use of emotion encoder to model emotional style explicitly, and introduce mutual information (MI) losses to reduce the irrelevant information from the disentangled emotion representations. At run-time, our proposed framework can convert both speaker identity and speaker-dependent emotional style without the need for parallel data. Experimental results validate the effectiveness of our proposed framework in both objective and subjective evaluations.      
### 40.EMF-Aware Cellular Networks in RIS-Assisted Environments  [ :arrow_down: ](https://arxiv.org/pdf/2110.10311.pdf)
>  The deployment of the 5th-generation cellular networks (5G) and beyond has triggered health concerns due to the electric and magnetic fields (EMF) exposure. In this paper, we propose a novel architecture to minimize the population exposure to EMF by considering a smart radio environment with a reconfigurable intelligent surface (RIS). Then, we optimize the RIS phases to minimize the exposure in terms of the exposure index (EI) while maintaining a minimum target quality of service. The proposed scheme achieves up to 20% reduction in EI compared to schemes without RISs.      
### 41.A New Automatic Change Detection Frame-work Based on Region Growing and Weighted Local Mutual Information: Analysis of Breast Tumor Response to Chemotherapy in Serial MR Images  [ :arrow_down: ](https://arxiv.org/pdf/2110.10242.pdf)
>  The automatic analysis of subtle changes between longitudinal MR images is an important task as it is still a challenging issue in scope of the breast medical image processing. In this paper we propose an effective automatic change detection framework composed of two phases since previously used methods have features with low distinctive power. First, in the preprocessing phase an intensity normalization method is suggested based on Hierarchical Histogram Matching (HHM) that is more robust to noise than previous methods. To eliminate undesirable changes and extract the regions containing significant changes the proposed Extraction Region of Changes (EROC) method is applied based on intensity distribution and Hill-Climbing algorithm. Second, in the detection phase a region growing-based approach is suggested to differentiate significant changes from unreal ones. Due to using proposed Weighted Local Mutual Information (WLMI) method to extract high level features and also utilizing the principle of the local consistency of changes, the proposed approach enjoys reasonable performance. The experimental results on both simulated and real longitudinal Breast MR Images confirm the effectiveness of the proposed framework. Also, this framework outperforms the human expert in some cases which can detect many lesion evolutions that are missed by expert.      
### 42.Patch Based Transformation for Minimum Variance Beamformer Image Approximation Using Delay and Sum Pipeline  [ :arrow_down: ](https://arxiv.org/pdf/2110.10220.pdf)
>  In the recent past, there have been several efforts in accelerating computationally heavy beamforming algorithms such as minimum variance distortionless response (MVDR) beamforming to achieve real-time performance comparable to the popular delay and sum (DAS) beamforming. This has been achieved using a variety of neural network architectures ranging from fully connected neural networks (FCNNs), convolutional neural networks (CNNs) and general adversarial networks (GANs). However most of these approaches are working with optimizations considering image level losses and hence require a significant amount of dataset to ensure that the process of beamforming is learned. In this work, a patch level U-Net based neural network is proposed, where the delay compensated radio frequency (RF) patch for a fixed region in space (e.g. 32x32) is transformed through a U-Net architecture and multiplied with DAS apodization weights and optimized for similarity with MVDR image of the patch. Instead of framing the beamforming problem as a regression problem to estimate the apodization weights, the proposed approach treats the non-linear transformation of the RF data space that can account for the data driven weight adaptation done by the MVDR approach in the parameters of the network. In this way, it is also observed that by restricting the input to a patch the model will learn the beamforming pipeline as an image non-linear transformation problem.      
### 43.Power Line Communication Based Smart Grid Asset Monitoring Using Time Series Forecasting  [ :arrow_down: ](https://arxiv.org/pdf/2110.10219.pdf)
>  Monitoring grid assets continuously is critical in ensuring the reliable operation of the electricity grid system and improving its resilience in case of a defect. In light of several asset monitoring techniques in use, power line communication (PLC) enables a low-cost cable diagnostics solution by re-using smart grid data communication modems to also infer the cable health using the inherently estimated communication channel state information. Traditional PLC-based cable diagnostics solutions are dependent on prior knowledge of the cable type, network topology, and/or characteristics of the anomalies. In contrast, we develop an asset monitoring technique in this paper that can detect various types of anomalies in the grid without any prior domain knowledge. To this end, we design a solution that first uses time-series forecasting to predict the PLC channel state information at any given point in time based on its historical data. Under the assumption that the prediction error follows a Gaussian distribution, we then perform chi-squared statistical test to determine the significance level of the resultant Mahalanobis distance to build our anomaly detector. We demonstrate the effectiveness and universality of our solution via evaluations conducted using both synthetic and real-world data extracted from low- and medium-voltage distribution networks.      
### 44.Cross-Sim-NGF: FFT-Based Global Rigid Multimodal Alignment of Image Volumes using Normalized Gradient Fields  [ :arrow_down: ](https://arxiv.org/pdf/2110.10156.pdf)
>  Multimodal image alignment involves finding spatial correspondences between volumes varying in appearance and structure. Automated alignment methods are often based on local optimization that can be highly sensitive to their initialization. We propose a global optimization method for rigid multimodal 3D image alignment, based on a novel efficient algorithm for computing similarity of normalized gradient fields (NGF) in the frequency domain. We validate the method experimentally on a dataset comprised of 20 brain volumes acquired in four modalities (T1w, Flair, CT, [18F] FDG PET), synthetically displaced with known transformations. The proposed method exhibits excellent performance on all six possible modality combinations, and outperforms all four reference methods by a large margin. The method is fast; a 3.4Mvoxel global rigid alignment requires approximately 40 seconds of computation, and the proposed algorithm outperforms a direct algorithm for the same task by more than three orders of magnitude. Open-source implementation is provided.      
### 45.OpenABC-D: A Large-Scale Dataset For Machine Learning Guided Integrated Circuit Synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2110.11292.pdf)
>  Logic synthesis is a challenging and widely-researched combinatorial optimization problem during integrated circuit (IC) design. It transforms a high-level description of hardware in a programming language like Verilog into an optimized digital circuit netlist, a network of interconnected Boolean logic gates, that implements the function. Spurred by the success of ML in solving combinatorial and graph problems in other domains, there is growing interest in the design of ML-guided logic synthesis tools. Yet, there are no standard datasets or prototypical learning tasks defined for this problem domain. Here, we describe OpenABC-D,a large-scale, labeled dataset produced by synthesizing open source designs with a leading open-source logic synthesis tool and illustrate its use in developing, evaluating and benchmarking ML-guided logic synthesis. OpenABC-D has intermediate and final outputs in the form of 870,000 And-Inverter-Graphs (AIGs) produced from 1500 synthesis runs plus labels such as the optimized node counts, and de-lay. We define a generic learning problem on this dataset and benchmark existing solutions for it. The codes related to dataset creation and benchmark models are available athttps://github.com/NYU-MLDA/OpenABC.git. The dataset generated is available athttps://archive.<a class="link-external link-http" href="http://nyu.edu/handle/2451/63311" rel="external noopener nofollow">this http URL</a>      
### 46.The Effect of Wearing a Face Mask on Face Image Quality  [ :arrow_down: ](https://arxiv.org/pdf/2110.11283.pdf)
>  Due to the COVID-19 situation, face masks have become a main part of our daily life. Wearing mouth-and-nose protection has been made a mandate in many public places, to prevent the spread of the COVID-19 virus. However, face masks affect the performance of face recognition, since a large area of the face is covered. The effect of wearing a face mask on the different components of the face recognition system in a collaborative environment is a problem that is still to be fully studied. This work studies, for the first time, the effect of wearing a face mask on face image quality by utilising state-of-the-art face image quality assessment methods of different natures. This aims at providing better understanding on the effect of face masks on the operation of face recognition as a whole system. In addition, we further studied the effect of simulated masks on face image utility in comparison to real face masks. We discuss the correlation between the mask effect on face image quality and that on the face verification performance by automatic systems and human experts, indicating a consistent trend between both factors. The evaluation is conducted on the database containing (1) no-masked faces, (2) real face masks, and (3) simulated face masks, by synthetically generating digital facial masks on no-masked faces according to the NIST protocols [1, 23]. Finally, a visual interpretation of the face areas contributing to the quality score of a selected set of quality assessment methods is provided to give a deeper insight into the difference of network decisions in masked and non-masked faces, among other variations.      
### 47.Modeling the AC Power Flow Equations with Optimally Compact Neural Networks: Application to Unit Commitment  [ :arrow_down: ](https://arxiv.org/pdf/2110.11269.pdf)
>  Nonlinear power flow constraints render a variety of power system optimization problems computationally intractable. Emerging research shows, however, that the nonlinear AC power flow equations can be successfully modeled using Neural Networks (NNs). These NNs can be exactly transformed into Mixed Integer Linear Programs (MILPs) and embedded inside challenging optimization problems, thus replacing nonlinearities that are intractable for many applications with tractable piecewise linear approximations. Such approaches, though, suffer from an explosion of the number of binary variables needed to represent the NN. Accordingly, this paper develops a technique for training an "optimally compact" NN, i.e., one that can represent the power flow equations with a sufficiently high degree of accuracy while still maintaining a tractable number of binary variables. We show that the resulting NN model is more expressive than both the DC and linearized power flow approximations when embedded inside of a challenging optimization problem (i.e., the AC unit commitment problem).      
### 48.Inverse Optimal Control Adapted to the Noise Characteristics of the Human Sensorimotor System  [ :arrow_down: ](https://arxiv.org/pdf/2110.11130.pdf)
>  Computational level explanations based on optimal feedback control with signal-dependent noise have been able to account for a vast array of phenomena in human sensorimotor behavior. However, commonly a cost function needs to be assumed for a task and the optimality of human behavior is evaluated by comparing observed and predicted trajectories. Here, we introduce inverse optimal control with signal-dependent noise, which allows inferring the cost function from observed behavior. To do so, we formalize the problem as a partially observable Markov decision process and distinguish between the agent's and the experimenter's inference problems. Specifically, we derive a probabilistic formulation of the evolution of states and belief states and an approximation to the propagation equation in the linear-quadratic Gaussian problem with signal-dependent noise. We extend the model to the case of partial observability of state variables from the point of view of the experimenter. We show the feasibility of the approach through validation on synthetic data and application to experimental data. Our approach enables recovering the costs and benefits implicit in human sequential sensorimotor behavior, thereby reconciling normative and descriptive approaches in a computational framework.      
### 49.Intelligent Reflecting Surface for Multi-Path Beam Routing with Active/Passive Beam Splitting and Combining  [ :arrow_down: ](https://arxiv.org/pdf/2110.11104.pdf)
>  Intelligent reflecting surface (IRS) can be densely deployed in wireless networks to significantly enhance the communication channels. In this letter, we consider the downlink transmission from a multi-antenna base station (BS) to a single-antenna user, by exploiting the cooperative passive beamforming (CPB) and line-of-sight (LoS) path diversity gains of multi-IRS signal reflection. Unlike existing works where only one single multi-IRS reflection path from the BS to user is selected, we propose a new and more general {\it \textbf{multi-path beam routing}} scheme. Specifically, the BS sends the user's information signal via multiple orthogonal active beams (termed as {\it \textbf{active beam splitting}}), which point towards different IRSs. Then, these beamed signals are subsequently reflected by selected IRSs via their CPB in different paths, and finally coherently combined at the user's receiver (thus named {\it \textbf{passive beam combining}}). For this scheme, we formulate a new multi-path beam routing design problem to jointly optimize the number of IRS reflection paths, the selected IRSs for each of the reflection paths, the active/passive beamforming at the BS/each selected IRS, as well as the BS's power allocation over different active beams, so as to maximize the received signal power at the user. To solve this challenging problem, we first derive the optimal BS/IRS beamforming and BS power allocation for a given set of reflection paths. The clique-based approach in graph theory is then applied to solve the remaining multi-path selection problem efficiently. Simulation results show that our proposed multi-path beam routing scheme significantly outperforms its conventional single-path beam routing special case.      
### 50.Continuous Authentication Using Mouse Movements, Machine Learning, and Minecraft  [ :arrow_down: ](https://arxiv.org/pdf/2110.11080.pdf)
>  Mouse dynamics has grown in popularity as a novel irreproducible behavioral biometric. Datasets which contain general unrestricted mouse movements from users are sparse in the current literature. The Balabit mouse dynamics dataset produced in 2016 was made for a data science competition and despite some of its shortcomings, is considered to be the first publicly available mouse dynamics dataset. Collecting mouse movements in a dull administrative manner as Balabit does may unintentionally homogenize data and is also not representative of realworld application scenarios. This paper presents a novel mouse dynamics dataset that has been collected while 10 users play the video game Minecraft on a desktop computer. Binary Random Forest (RF) classifiers are created for each user to detect differences between a specific users movements and an imposters movements. Two evaluation scenarios are proposed to evaluate the performance of these classifiers; one scenario outperformed previous works in all evaluation metrics, reaching average accuracy rates of 92%, while the other scenario successfully reported reduced instances of false authentications of imposters.      
### 51.Transfer beyond the Field of View: Dense Panoramic Semantic Segmentation via Unsupervised Domain Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2110.11062.pdf)
>  Autonomous vehicles clearly benefit from the expanded Field of View (FoV) of 360-degree sensors, but modern semantic segmentation approaches rely heavily on annotated training data which is rarely available for panoramic images. We look at this problem from the perspective of domain adaptation and bring panoramic semantic segmentation to a setting, where labelled training data originates from a different distribution of conventional pinhole camera images. To achieve this, we formalize the task of unsupervised domain adaptation for panoramic semantic segmentation and collect DensePASS - a novel densely annotated dataset for panoramic segmentation under cross-domain conditions, specifically built to study the Pinhole-to-Panoramic domain shift and accompanied with pinhole camera training examples obtained from Cityscapes. DensePASS covers both, labelled- and unlabelled 360-degree images, with the labelled data comprising 19 classes which explicitly fit the categories available in the source (i.e. pinhole) domain. Since data-driven models are especially susceptible to changes in data distribution, we introduce P2PDA - a generic framework for Pinhole-to-Panoramic semantic segmentation which addresses the challenge of domain divergence with different variants of attention-augmented domain adaptation modules, enabling the transfer in output-, feature-, and feature confidence spaces. P2PDA intertwines uncertainty-aware adaptation using confidence values regulated on-the-fly through attention heads with discrepant predictions. Our framework facilitates context exchange when learning domain correspondences and dramatically improves the adaptation performance of accuracy- and efficiency-focused models. Comprehensive experiments verify that our framework clearly surpasses unsupervised domain adaptation- and specialized panoramic segmentation approaches.      
### 52.Stability and performance analysis of NMPC: Detectable stage costs and general terminal costs  [ :arrow_down: ](https://arxiv.org/pdf/2110.11021.pdf)
>  We provide a stability and performance analysis for nonlinear model predictive control (NMPC) schemes. Given an exponential stabilizability and detectability condition w.r.t. the employed state cost, we provide a sufficiently long prediction horizon to ensure asymptotic stability and a desired performance bound w.r.t. the infinite-horizon optimal controller. Compared to existing results, the provided analysis is applicable to positive semi-definite (detectable) cost functions, provides tight bounds using a linear programming analysis, and allows for a seamless integration of general positive-definite terminal cost functions in the analysis. The practical applicability of the derived theoretical results are demonstrated in a numerical example.      
### 53.Learning Time-Varying Graphs from Online Data  [ :arrow_down: ](https://arxiv.org/pdf/2110.11017.pdf)
>  This work proposes an algorithmic framework to learn time-varying graphs from online data. The generality offered by the framework renders it model-independent, i.e., it can be theoretically analyzed in its abstract formulation and then instantiated under a variety of model-dependent graph learning problems. This is possible by phrasing (time-varying) graph learning as a composite optimization problem, where different functions regulate different desiderata, e.g., data fidelity, sparsity or smoothness. Instrumental for the findings is recognizing that the dependence of the majority (if not all) data-driven graph learning algorithms on the data is exerted through the empirical covariance matrix, representing a sufficient statistic for the estimation problem. Its user-defined recursive update enables the framework to work in non-stationary environments, while iterative algorithms building on novel time-varying optimization tools explicitly take into account the temporal dynamics, speeding up convergence and implicitly including a temporal-regularization of the solution. We specialize the framework to three well-known graph learning models, namely, the Gaussian graphical model (GGM), the structural equation model (SEM), and the smoothness-based model (SBM), where we also introduce ad-hoc vectorization schemes for structured matrices (symmetric, hollows, etc.) which are crucial to perform correct gradient computations, other than enabling to work in low-dimensional vector spaces and hence easing storage requirements. After discussing the theoretical guarantees of the proposed framework, we corroborate it with extensive numerical tests in synthetic and real data.      
### 54.Learning OFDM Waveforms with PAPR and ACLR Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2110.10987.pdf)
>  An attractive research direction for future communication systems is the design of new waveforms that can both support high throughputs and present advantageous signal characteristics. Although most modern systems use orthogonal frequency-division multiplexing (OFDM) for its efficient equalization, this waveform suffers from multiple limitations such as a high adjacent channel leakage ratio (ACLR) and high peak-to-average power ratio (PAPR). In this paper, we propose a learning-based method to design OFDM-based waveforms that satisfy selected constraints while maximizing an achievable information rate. To that aim, we model the transmitter and the receiver as convolutional neural networks (CNNs) that respectively implement a high-dimensional modulation scheme and perform the detection of the transmitted bits. This leads to an optimization problem that is solved using the augmented Lagrangian method. Evaluation results show that the end-to-end system is able to satisfy target PAPR and ACLR constraints and allows significant throughput gains compared to a tone reservation (TR) baseline. An additional advantage is that no dedicated pilots are needed.      
### 55.Optimizing Multi-Taper Features for Deep Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2110.10983.pdf)
>  Multi-taper estimators provide low-variance power spectrum estimates that can be used in place of the windowed discrete Fourier transform (DFT) to extract speech features such as mel-frequency cepstral coefficients (MFCCs). Even if past work has reported promising automatic speaker verification (ASV) results with Gaussian mixture model-based classifiers, the performance of multi-taper MFCCs with deep ASV systems remains an open question. Instead of a static-taper design, we propose to optimize the multi-taper estimator jointly with a deep neural network trained for ASV tasks. With a maximum improvement on the SITW corpus of 25.8% in terms of equal error rate over the static-taper, our method helps preserve a balanced level of leakage and variance, providing more robustness.      
### 56.Estimation of Covariance Matrix of Interference for Secure Spatial Modulation against a Malicious Full-duplex Attacker  [ :arrow_down: ](https://arxiv.org/pdf/2110.10952.pdf)
>  In a secure spatial modulation with a malicious full-duplex attacker, how to obtain the interference space or channel state information (CSI) is very important for Bob to cancel or reduce the interference from Mallory. In this paper, different from existing work with a perfect CSI, the covariance matrix of malicious interference (CMMI) from Mallory is estimated and is used to construct the null-space of interference (NSI). Finally, the receive beamformer at Bob is designed to remove the malicious interference using the NSI. To improve the estimation accuracy, a rank detector relying on Akaike information criterion (AIC) is derived. To achieve a high-precision CMMI estimation, two methods are proposed as follows: principal component analysis-eigenvalue decomposition (PCA-EVD), and joint diagonalization (JD). The proposed PCA-EVD is a rank deduction method whereas the JD method is a joint optimization method with improved performance in low signal to interference plus noise ratio (SINR) region at the expense of increased complexities. Simulation results show that the proposed PCA-EVD performs much better than the existing method like sample estimated covariance matrix (SCM) and EVD in terms of normalized mean square error (NMSE) and secrecy rate (SR). Additionally, the proposed JD method has an excellent NMSE performance better than PCA-EVD in the low SINR region (SINR &lt; 0dB) while in the high SINR region PCA-EVD performs better than JD.      
### 57.SMOF: Squeezing More Out of Filters Yields Hardware-Friendly CNN Pruning  [ :arrow_down: ](https://arxiv.org/pdf/2110.10842.pdf)
>  For many years, the family of convolutional neural networks (CNNs) has been a workhorse in deep learning. Recently, many novel CNN structures have been designed to address increasingly challenging tasks. To make them work efficiently on edge devices, researchers have proposed various structured network pruning strategies to reduce their memory and computational cost. However, most of them only focus on reducing the number of filter channels per layer without considering the redundancy within individual filter channels. In this work, we explore pruning from another dimension, the kernel size. We develop a CNN pruning framework called SMOF, which Squeezes More Out of Filters by reducing both kernel size and the number of filter channels. Notably, SMOF is friendly to standard hardware devices without any customized low-level implementations, and the pruning effort by kernel size reduction does not suffer from the fixed-size width constraint in SIMD units of general-purpose processors. The pruned networks can be deployed effortlessly with significant running time reduction. We also support these claims via extensive experiments on various CNN structures and general-purpose processors for mobile devices.      
### 58.High-resolution rainfall-runoff modeling using graph neural network  [ :arrow_down: ](https://arxiv.org/pdf/2110.10833.pdf)
>  Time-series modeling has shown great promise in recent studies using the latest deep learning algorithms such as LSTM (Long Short-Term Memory). These studies primarily focused on watershed-scale rainfall-runoff modeling or streamflow forecasting, but the majority of them only considered a single watershed as a unit. Although this simplification is very effective, it does not take into account spatial information, which could result in significant errors in large watersheds. Several studies investigated the use of GNN (Graph Neural Networks) for data integration by decomposing a large watershed into multiple sub-watersheds, but each sub-watershed is still treated as a whole, and the geoinformation contained within the watershed is not fully utilized. In this paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel deep learning model that makes full use of spatial information from high-resolution precipitation data, including flow direction and geographic information. When compared to baseline models, GNRRM has less over-fitting and significantly improves model performance. Our findings support the importance of hydrological data in deep learning-based rainfall-runoff modeling, and we encourage researchers to include more domain knowledge in their models.      
### 59.ReachBot: A Small Robot for Large Mobile Manipulation Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2110.10829.pdf)
>  Robots are widely deployed in space environments because of their versatility and robustness. However, adverse gravity conditions and challenging terrain geometry expose the limitations of traditional robot designs, which are often forced to sacrifice one of mobility or manipulation capabilities to attain the other. Prospective climbing operations in these environments reveals a need for small, compact robots capable of versatile mobility and manipulation. We propose a novel robotic concept called ReachBot that fills this need by combining two existing technologies: extendable booms and mobile manipulation. ReachBot leverages the reach and tensile strength of extendable booms to achieve an outsized reachable workspace and wrench capability. Through their lightweight, compactable structure, these booms also reduce mass and complexity compared to traditional rigid-link articulated-arm designs. Using these advantages, ReachBot excels in mobile manipulation missions in low gravity or that require climbing, particularly when anchor points are sparse. After introducing the ReachBot concept, we discuss modeling approaches and strategies for increasing stability and robustness. We then develop a 2D analytical model for ReachBot's dynamics inspired by grasp models for dexterous manipulators. Next, we introduce a waypoint-tracking controller for a planar ReachBot in microgravity. Our simulation results demonstrate the controller's robustness to disturbances and modeling error. Finally, we briefly discuss next steps that build on these initially promising results to realize the full potential of ReachBot.      
### 60.TPARN: Triple-path Attentive Recurrent Network for Time-domain Multichannel Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2110.10757.pdf)
>  In this work, we propose a new model called triple-path attentive recurrent network (TPARN) for multichannel speech enhancement in the time domain. TPARN extends a single-channel dual-path network to a multichannel network by adding a third path along the spatial dimension. First, TPARN processes speech signals from all channels independently using a dual-path attentive recurrent network (ARN), which is a recurrent neural network (RNN) augmented with self-attention. Next, an ARN is introduced along the spatial dimension for spatial context aggregation. TPARN is designed as a multiple-input and multiple-output architecture to enhance all input channels simultaneously. Experimental results demonstrate the superiority of TPARN over existing state-of-the-art approaches.      
### 61.Adapting Speech Separation to Real-World Meetings Using Mixture Invariant Training  [ :arrow_down: ](https://arxiv.org/pdf/2110.10739.pdf)
>  The recently-proposed mixture invariant training (MixIT) is an unsupervised method for training single-channel sound separation models in the sense that it does not require ground-truth isolated reference sources. In this paper, we investigate using MixIT to adapt a separation model on real far-field overlapping reverberant and noisy speech data from the AMI Corpus. The models are tested on real AMI recordings containing overlapping speech, and are evaluated subjectively by human listeners. To objectively evaluate our models, we also devise a synthetic AMI test set. For human evaluations on real recordings, we also propose a modification of the standard MUSHRA protocol to handle imperfect reference signals, which we call MUSHIRA. Holding network architectures constant, we find that a fine-tuned semi-supervised model yields the largest SI-SNR improvement, PESQ scores, and human listening ratings across synthetic and real datasets, outperforming unadapted generalist models trained on orders of magnitude more data. Our results show that unsupervised learning through MixIT enables model adaptation on real-world unlabeled spontaneous speech recordings.      
### 62.Part-X: A Family of Stochastic Algorithms for Search-Based Test Generation with Probabilistic Guarantees  [ :arrow_down: ](https://arxiv.org/pdf/2110.10729.pdf)
>  Requirements driven search-based testing (also known as falsification) has proven to be a practical and effective method for discovering erroneous behaviors in Cyber-Physical Systems. Despite the constant improvements on the performance and applicability of falsification methods, they all share a common characteristic. Namely, they are best-effort methods which do not provide any guarantees on the absence of erroneous behaviors (falsifiers) when the testing budget is exhausted. The absence of finite time guarantees is a major limitation which prevents falsification methods from being utilized in certification procedures. In this paper, we address the finite-time guarantees problem by developing a new stochastic algorithm. Our proposed algorithm not only estimates (bounds) the probability that falsifying behaviors exist, but also it identifies the regions where these falsifying behaviors may occur. We demonstrate the applicability of our approach on standard benchmark functions from the optimization literature and on the F16 benchmark problem.      
### 63.Auction Design through Multi-Agent Learning in Peer-to-Peer Energy Trading  [ :arrow_down: ](https://arxiv.org/pdf/2110.10714.pdf)
>  Distributed energy resources (DERs), such as rooftop solar panels, are growing rapidly and are reshaping power systems. To promote DERs, feed-in-tariff (FIT) is usually adopted by utilities to pay DER owners certain fixed rates for supplying energy to the grid. An alternative to FIT is a market-based approach; that is, consumers and DER owners trade energy in an auction-based peer-to-peer (P2P) market, and the rates are determined based on supply and demand. However, the auction complexity and market participants' bounded rationality may invalidate many well-established theories on auction design and hinder market development. To address the challenges, we propose an automated bidding framework based on multi-agent, multi-armed bandit learning for repeated auctions, which aims to minimize each bidder's cumulative regret. Numerical results indicate convergence of such a multi-agent learning game to a steady-state. Being particularly interested in auction designs, we have applied the framework to four different implementations of repeated double-side auctions to compare their market outcomes. While it is difficult to pick a clear winner, $k$-double auction (a variant of uniform pricing auction) and McAfee auction (a variant of Vickrey double-auction) appear to perform well in general, with their respective strengths and weaknesses.      
### 64.Predicting Tau Accumulation in Cerebral Cortex with Multivariate MRI Morphometry Measurements, Sparse Coding, and Correntropy  [ :arrow_down: ](https://arxiv.org/pdf/2110.10709.pdf)
>  Biomarker-assisted diagnosis and intervention in Alzheimer's disease (AD) may be the key to prevention breakthroughs. One of the hallmarks of AD is the accumulation of tau plaques in the human brain. However, current methods to detect tau pathology are either invasive (lumbar puncture) or quite costly and not widely available (Tau PET). In our previous work, structural MRI-based hippocampal multivariate morphometry statistics (MMS) showed superior performance as an effective neurodegenerative biomarker for preclinical AD and Patch Analysis-based Surface Correntropy-induced Sparse coding and max-pooling (PASCS-MP) has excellent ability to generate low-dimensional representations with strong statistical power for brain amyloid prediction. In this work, we apply this framework together with ridge regression models to predict Tau deposition in Braak12 and Braak34 brain regions separately. We evaluate our framework on 925 subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Each subject has one pair consisting of a PET image and MRI scan which were collected at about the same times. Experimental results suggest that the representations from our MMS and PASCS-MP have stronger predictive power and their predicted Braak12 and Braak34 are closer to the real values compared to the measures derived from other approaches such as hippocampal surface area and volume, and shape morphometry features based on spherical harmonics (SPHARM).      
### 65.Colosseum: Large-Scale Wireless Experimentation Through Hardware-in-the-Loop Network Emulation  [ :arrow_down: ](https://arxiv.org/pdf/2110.10617.pdf)
>  Colosseum is an open-access and publicly-available large-scale wireless testbed for experimental research via virtualized and softwarized waveforms and protocol stacks on a fully programmable, "white-box" platform. Through 256 state-of-the-art Software-defined Radios and a Massive Channel Emulator core, Colosseum can model virtually any scenario, enabling the design, development and testing of solutions at scale in a variety of deployments and channel conditions. These Colosseum radio-frequency scenarios are reproduced through high-fidelity FPGA-based emulation with finite-impulse response filters. Filters model the taps of desired wireless channels and apply them to the signals generated by the radio nodes, faithfully mimicking the conditions of real-world wireless environments. In this paper we describe the architecture of Colosseum and its experimentation and emulation capabilities. We then demonstrate the effectiveness of Colosseum for experimental research at scale through exemplary use cases including prevailing wireless technologies (e.g., cellular and Wi-Fi) in spectrum sharing and unmanned aerial vehicle scenarios. A roadmap for Colosseum future updates concludes the paper.      
### 66.Time-Domain Mapping Based Single-Channel Speech Separation With Hierarchical Constraint Training  [ :arrow_down: ](https://arxiv.org/pdf/2110.10593.pdf)
>  Single-channel speech separation is required for multi-speaker speech recognition. Recent deep learning-based approaches focused on time-domain audio separation net (TasNet) because it has superior performance and lower latency compared to the conventional time-frequency-based (T-F-based) approaches. Most of these works rely on the masking-based method that estimates a linear mapping function (mask) for each speaker. However, the other commonly used method, the mapping-based method that is less sensitive to SNR variations, is inadequately studied in the time domain. We explore the potential of the mapping-based method by introducing attention augmented DPRNN (AttnAugDPRNN) which directly approximates the clean sources from the mixture for speech separation. Permutation Invariant Training (PIT) has been a paradigm to solve the label ambiguity problem for speech separation but usually leads to suboptimal performance. To solve this problem, we propose an efficient training strategy called Hierarchical Constraint Training (HCT) to regularize the training, which could effectively improve the model performance. When using PIT, our results showed that mapping-based AttnAugDPRNN outperformed masking-based AttnAugDPRNN when the training corpus is large. Mapping-based AttnAugDPRNN with HCT significantly improved the SI-SDR by 10.1% compared to the masking-based AttnAugDPRNN without HCT.      
### 67.FairNet: A Measurement Framework for Traffic Discrimination Detection on the Internet  [ :arrow_down: ](https://arxiv.org/pdf/2110.10534.pdf)
>  Network neutrality is related to the non-discriminatory treatment of packets on the Internet. Any deliberate discrimination of traffic of one application while favoring others violates the principle of neutrality. Many countries have enforced laws against such discrimination. To enforce such laws, one requires tools to detect any net neutrality violations. However, detecting such violations is challenging as it is hard to separate any degradation in quality due to natural network effects and selective degradation. Also, legitimate traffic management and deliberate discrimination methods can be technically the same, making it further challenging to distinguish them. <br>We developed an end-to-end measurement framework named FairNet to detect discrimination of traffic. It compares the performance of similar services. Our focus is on HTTPS streaming services which constitute a predominant portion of the Internet traffic. The effect of confounding factors (congestion, traffic management policy, dynamic rate adaptation) is made `similar' on the test services to ensure a fair comparison. FairNet framework uses a ``replay server'' and user-client that exchanges correctly identifiable traffic streams over the Internet. The Server Name Indication (SNI) field in the TLS handshake, which goes in plaintext, ensures that the traffic from the replay server appears to network middle-boxes as that coming from its actual server. We validated that appropriate SNIs results in the correct classification of services using a commercial traffic shaper. FairNet uses two novel algorithms based on application-level throughput and connection status to detect traffic discrimination. We also validated the methodology's effectiveness by collecting network logs through mobile apps over the live Internet and analyzing them.      
### 68.A Study On Data Augmentation In Voice Anti-Spoofing  [ :arrow_down: ](https://arxiv.org/pdf/2110.10491.pdf)
>  In this paper, we perform an in-depth study of how data augmentation techniques improve synthetic or spoofed audio detection. Specifically, we propose methods to deal with channel variability, different audio compressions, different band-widths, and unseen spoofing attacks, which have all been shown to significantly degrade the performance of audio-based systems and Anti-Spoofing systems. Our results are based on the ASVspoof 2021 challenge, in the Logical Access (LA) and Deep Fake (DF) categories. Our study is Data-Centric, meaning that the models are fixed and we significantly improve the results by making changes in the data. We introduce two forms of data augmentation - compression augmentation for the DF part, compression &amp; channel augmentation for the LA part. In addition, a new type of online data augmentation, SpecAverage, is introduced in which the audio features are masked with their average value in order to improve generalization. Furthermore, we introduce a Log spectrogram feature design that improved the results. Our best single system and fusion scheme both achieve state-of-the-art performance in the DF category, with an EER of 15.46% and 14.46% respectively. Our best system for the LA task reduced the best baseline EER by 50% and the min t-DCF by 16%. Our techniques to deal with spoofed data from a wide variety of distributions can be replicated and can help anti-spoofing and speech-based systems enhance their results.      
### 69.Moiré Attack (MA): A New Potential Risk of Screen Photos  [ :arrow_down: ](https://arxiv.org/pdf/2110.10444.pdf)
>  Images, captured by a camera, play a critical role in training Deep Neural Networks (DNNs). Usually, we assume the images acquired by cameras are consistent with the ones perceived by human eyes. However, due to the different physical mechanisms between human-vision and computer-vision systems, the final perceived images could be very different in some cases, for example shooting on digital monitors. In this paper, we find a special phenomenon in digital image processing, the moiré effect, that could cause unnoticed security threats to DNNs. Based on it, we propose a Moiré Attack (MA) that generates the physical-world moiré pattern adding to the images by mimicking the shooting process of digital devices. Extensive experiments demonstrate that our proposed digital Moiré Attack (MA) is a perfect camouflage for attackers to tamper with DNNs with a high success rate ($100.0\%$ for untargeted and $97.0\%$ for targeted attack with the noise budget $\epsilon=4$), high transferability rate across different models, and high robustness under various defenses. Furthermore, MA owns great stealthiness because the moiré effect is unavoidable due to the camera's inner physical structure, which therefore hardly attracts the awareness of humans. Our code is available at <a class="link-external link-https" href="https://github.com/Dantong88/Moire_Attack" rel="external noopener nofollow">this https URL</a>.      
### 70.Knowledge distillation from language model to acoustic model: a hierarchical multi-task learning approach  [ :arrow_down: ](https://arxiv.org/pdf/2110.10429.pdf)
>  The remarkable performance of the pre-trained language model (LM) using self-supervised learning has led to a major paradigm shift in the study of natural language processing. In line with these changes, leveraging the performance of speech recognition systems with massive deep learning-based LMs is a major topic of speech recognition research. Among the various methods of applying LMs to speech recognition systems, in this paper, we focus on a cross-modal knowledge distillation method that transfers knowledge between two types of deep neural networks with different modalities. We propose an acoustic model structure with multiple auxiliary output layers for cross-modal distillation and demonstrate that the proposed method effectively compensates for the shortcomings of the existing label-interpolation-based distillation method. In addition, we extend the proposed method to a hierarchical distillation method using LMs trained in different units (senones, monophones, and subwords) and reveal the effectiveness of the hierarchical distillation method through an ablation study.      
### 71.Hyperspherical Dirac Mixture Reapproximation  [ :arrow_down: ](https://arxiv.org/pdf/2110.10411.pdf)
>  We propose a novel scheme for efficient Dirac mixture modeling of distributions on unit hyperspheres. A so-called hyperspherical localized cumulative distribution (HLCD) is introduced as a local and smooth characterization of the underlying continuous density in hyperspherical domains. Based on HLCD, a manifold-adapted modification of the Cramér-von Mises distance (HCvMD) is established to measure the statistical divergence between two Dirac mixtures of arbitrary dimensions. Given a (source) Dirac mixture with many components representing an unknown hyperspherical distribution, a (target) Dirac mixture with fewer components is obtained via matching the source in the sense of least HCvMD. As the number of target Dirac components is configurable, the underlying distributions is represented in a more efficient and informative way. Based upon this hyperspherical Dirac mixture reapproximation (HDMR), we derive a density estimation method and a recursive filter. For density estimation, a maximum likelihood method is provided to reconstruct the underlying continuous distribution in the form of a von Mises-Fisher mixture. For recursive filtering, we introduce the hyperspherical reapproximation discrete filter (HRDF) for nonlinear hyperspherical estimation of dynamic systems under unknown system noise of arbitrary form. Simulations show that the HRDF delivers superior tracking performance over filters using sequential Monte Carlo and parametric modeling.      
### 72.Non-invasive optical measurement of arterial blood flow speed  [ :arrow_down: ](https://arxiv.org/pdf/2110.10408.pdf)
>  Non-invasive measurement of the arterial blood speed gives rise to important healthinformation such as cardio output and blood supplies to vital organs. The magnitude andchange in arterial blood speed are key indicators of the health conditions and development andprogression of diseases. We demonstrated a simple technique to directly measure the blood flowspeed in main arteries based on the diffused light model. The concept is demonstrated with aphantom that uses intralipid hydrogel to model the biological tissue and an embedded glass tubewith flowing human blood to model the blood vessel. The correlation function of the measuredphotocurrent was used to find the electrical field correlation function via the Siegert relation.We have shown that the characteristic decorrelation rate (i.e. the inverse of the decoherent time)is linearly proportional to the blood speed and independent of the tube diameter. This strikingproperty can be explained by an approximate analytic solution for the diffused light equation inthe regime where the convective flow is the dominating factor for decorrelation. As a result, wehave demonstrated a non-invasive method of measuring arterial blood speed without any priorknowledge or assumption about the geometric or mechanic properties of the blood vessels.      
### 73.Development of an Ontology for an Integrated Image Analysis Platform to enable Global Sharing of Microscopy Imaging Data  [ :arrow_down: ](https://arxiv.org/pdf/2110.10407.pdf)
>  Imaging data is one of the most important fundamentals in the current life sciences. We aimed to construct an ontology to describe imaging metadata as a data schema of the integrated database for optical and electron microscopy images combined with various bio-entities. To realise this, we applied Resource Description Framework (RDF) to an Open Microscopy Environment (OME) data model, which is the de facto standard to describe optical microscopy images and experimental data. We translated the XML-based OME metadata into the base concept of RDF schema as a trial of developing microscopy ontology. In this ontology, we propose 18 upper-level concepts including missing concepts in OME such as electron microscopy, phenotype data, biosample, and imaging conditions.      
### 74.An Investigation of Enhancing CTC Model for Triggered Attention-based Streaming ASR  [ :arrow_down: ](https://arxiv.org/pdf/2110.10402.pdf)
>  In the present paper, an attempt is made to combine Mask-CTC and the triggered attention mechanism to construct a streaming end-to-end automatic speech recognition (ASR) system that provides high performance with low latency. The triggered attention mechanism, which performs autoregressive decoding triggered by the CTC spike, has shown to be effective in streaming ASR. However, in order to maintain high accuracy of alignment estimation based on CTC outputs, which is the key to its performance, it is inevitable that decoding should be performed with some future information input (i.e., with higher latency). It should be noted that in streaming ASR, it is desirable to be able to achieve high recognition accuracy while keeping the latency low. Therefore, the present study aims to achieve highly accurate streaming ASR with low latency by introducing Mask-CTC, which is capable of learning feature representations that anticipate future information (i.e., that can consider long-term contexts), to the encoder pre-training. Experimental comparisons conducted using WSJ data demonstrate that the proposed method achieves higher accuracy with lower latency than the conventional triggered attention-based streaming ASR system.      
### 75.Robust lEarned Shrinkage-Thresholding (REST): Robust unrolling for sparse recover  [ :arrow_down: ](https://arxiv.org/pdf/2110.10391.pdf)
>  In this paper, we consider deep neural networks for solving inverse problems that are robust to forward model mis-specifications. Specifically, we treat sensing problems with model mismatch where one wishes to recover a sparse high-dimensional vector from low-dimensional observations subject to uncertainty in the measurement operator. We then design a new robust deep neural network architecture by applying algorithm unfolding techniques to a robust version of the underlying recovery problem. Our proposed network - named Robust lEarned Shrinkage-Thresholding (REST) - exhibits an additional normalization processing compared to Learned Iterative Shrinkage-Thresholding Algorithm (LISTA), leading to reliable recovery of the signal under sample-wise varying model mismatch. The proposed REST network is shown to outperform state-of-the-art model-based and data-driven algorithms in both compressive sensing and radar imaging problems wherein model mismatch is taken into consideration.      
### 76.Artificial Intelligence-Based Detection, Classification and Prediction/Prognosis in PET Imaging: Towards Radiophenomics  [ :arrow_down: ](https://arxiv.org/pdf/2110.10332.pdf)
>  Artificial intelligence (AI) techniques have significant potential to enable effective, robust, and automated image phenotyping including identification of subtle patterns. AI-based detection searches the image space to find the regions of interest based on patterns and features. There is a spectrum of tumor histologies from benign to malignant that can be identified by AI-based classification approaches using image features. The extraction of minable information from images gives way to the field of radiomics and can be explored via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics analysis has the potential to be utilized as a noninvasive technique for the accurate characterization of tumors to improve diagnosis and treatment monitoring. This work reviews AI-based techniques, with a special focus on oncological PET and PET/CT imaging, for different detection, classification, and prediction/prognosis tasks. We also discuss needed efforts to enable the translation of AI techniques to routine clinical workflows, and potential improvements and complementary techniques such as the use of natural language processing on electronic health records and neuro-symbolic AI techniques.      
### 77.Beamforming Design for Intelligent Reflecting Surface-Enhanced Symbiotic Radio Systems  [ :arrow_down: ](https://arxiv.org/pdf/2110.10316.pdf)
>  This paper investigates multiuser multi-input single-output downlink symbiotic radio communication systems assisted by an intelligent reflecting surface (IRS). Different from existing methods ideally assuming the secondary user (SU) can jointly decode information symbols from both the access point (AP) and the IRS via multiuser detection, we consider a more practical SU that only non-coherent detection is available. To characterize the non-coherent decoding performance, a practical upper bound of the average symbol error rate (SER) is derived. Subsequently, we jointly optimize the beamformer at the AP and the phase shifts at the IRS to maximize the average sum-rate of the primary system taking into account the maximum tolerable SER constraint for the SU. To circumvent the couplings of variables, we exploit the Schur complement that facilitates the design of a suboptimal beamforming algorithm based on successive convex approximation. Our simulation results show that compared with various benchmark algorithms, the proposed scheme significantly improves the average sum-rate of the primary system, while guaranteeing the decoding performance of the secondary system.      
### 78.Theoretical Advances in Current Estimation and Navigation from a Glider-Based Acoustic Doppler Current Profiler (ADCP)  [ :arrow_down: ](https://arxiv.org/pdf/2110.10199.pdf)
>  We examine acoustic Doppler current profiler (ADCP) measurements from underwater gliders to determine glider position, glider velocity, and subsurface current. ADCPs, however, do not directly observe the quantities of interest; instead, they measure the relative motion of the vehicle and the water column. We examine the lineage of mathematical innovations that have previously been applied to this problem, discovering an unstated but incorrect assumption of independence. We reframe a recent method to form a joint probability model of current and vehicle navigation, which allows us to correct this assumption and extend the classic Kalman smoothing method. Detailed simulations affirm the efficacy of our approach for computing estimates and their uncertainty. The joint model developed here sets the stage for future work to incorporate constraints, range measurements, and robust statistical modeling.      
### 79.CoFi: Coarse-to-Fine ICP for LiDAR Localization in an Efficient Long-lasting Point Cloud Map  [ :arrow_down: ](https://arxiv.org/pdf/2110.10194.pdf)
>  LiDAR odometry and localization has attracted increasing research interest in recent years. In the existing works, iterative closest point (ICP) is widely used since it is precise and efficient. Due to its non-convexity and its local iterative strategy, however, ICP-based method easily falls into local optima, which in turn calls for a precise initialization. In this paper, we propose CoFi, a Coarse-to-Fine ICP algorithm for LiDAR localization. Specifically, the proposed algorithm down-samples the input point sets under multiple voxel resolution, and gradually refines the transformation from the coarse point sets to the fine-grained point sets. In addition, we propose a map based LiDAR localization algorithm that extracts semantic feature points from the LiDAR frames and apply CoFi to estimate the pose on an efficient point cloud map. With the help of the Cylinder3D algorithm for LiDAR scan semantic segmentation, the proposed CoFi localization algorithm demonstrates the state-of-the-art performance on the KITTI odometry benchmark, with significant improvement over the literature.      
### 80.Inverse Power Flow Problem  [ :arrow_down: ](https://arxiv.org/pdf/1610.06631.pdf)
>  This paper formulates the inverse power flow problem which is to infer the nodal admittance matrix (hence the network structure of the power system) from voltage and current phasors measured at a number of buses. We show that the admittance matrix can be uniquely identified from a sequence of measurements corresponding to different steady states when every node in the system is equipped with a measurement device, and a Kron-reduced admittance matrix can be determined even if some nodes in the system are not monitored (hidden nodes). Furthermore, we propose effective algorithms based on graph theory to uncover the actual admittance matrix of radial systems with hidden nodes. We provide theoretical guarantees for the recovered admittance matrix and demonstrate that the actual admittance matrix can be fully recovered even from the Kron-reduced admittance matrix under some mild assumptions. Simulations on standard test systems confirm that these algorithms are capable of providing accurate estimates of the admittance matrix from noisy sensor data.      
