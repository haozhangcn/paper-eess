# ArXiv eess --Fri, 12 Mar 2021
### 1.Fast Hyperspectral Image Denoising and Inpainting Based on Low-Rank and Sparse Representations  [ :arrow_down: ](https://arxiv.org/pdf/2103.06842.pdf)
>  This paper introduces two very fast and competitive hyperspectral image (HSI) restoration algorithms: fast hyperspectral denoising (FastHyDe), a denoising algorithm able to cope with Gaussian and Poissonian noise, and fast hyperspectral inpainting (FastHyIn), an inpainting algorithm to restore HSIs where some observations from known pixels in some known bands are missing. FastHyDe and FastHyIn fully exploit extremely compact and sparse HSI representations linked with their low-rank and self-similarity characteristics. In a series of experiments with simulated and real data, the newly introduced FastHyDe and FastHyIn compete with the state-of-the-art methods, with much lower computational complexity.      
### 2.Multi-Link Vehicular Wireless Channel Modelling: Impact of Large Obstructing Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2103.06790.pdf)
>  Multi-node channel sounding simultaneously captures multiple channel transfer functions in complex scenarios. It ensures that measurement conditions are identical for all observed links. This is particularly beneficial in analyzing highly-dynamic vehicular communication scenarios, where the measurement conditions and wireless channel statistics change rapidly. We present the first fully mobile multi-node vehicular wireless channel sounding results. We analyze the impact of a double-decker bus on the channel statistics in two measurement scenarios. The measurement data is used to calibrate an OpenStreetMap geometry-based stochastic channel model and include a more accurate modeling for large vehicles, absent from previous models. We validate our model using a link-level emulation, demonstrating a good match with measurement data. Finally, we propose a relaying scenario and test it on link-level by comparing the time-variant error rates of measured links.      
### 3.A bottom-up quantification of flexibility potential from the thermal energy storage in electric space heating  [ :arrow_down: ](https://arxiv.org/pdf/2103.06734.pdf)
>  Non-generating resources such as thermostatically controlled loads (TCLs) can arbitrage energy prices and provide balancing reserves when aggregated due to their thermal energy storage capacity. Based on a performed survey of Swedish single- and two-family dwellings with electric heating, this paper quantifies the potential of TCLs to provide reserves to the power system in Sweden. To this end, dwellings with heat pumps and direct electric heaters are modeled as thermal energy storage equivalents that can be included in a linear two-stage problem formulation. We approach the operational flexibility of the TCLs by modeling a risk-averse aggregator that controls decentralized TCLs and aims to maximize its own profit. The results show a potential of 2 GW/0.1Hz averaged over a year, and up to 6.4 GW/0.1Hz peak capacity. Based on a sensitivity analysis we derive policy implications regarding market timing and activation signal.      
### 4.Learning Word-Level Confidence For Subword End-to-End ASR  [ :arrow_down: ](https://arxiv.org/pdf/2103.06716.pdf)
>  We study the problem of word-level confidence estimation in subword-based end-to-end (E2E) models for automatic speech recognition (ASR). Although prior works have proposed training auxiliary confidence models for ASR systems, they do not extend naturally to systems that operate on word-pieces (WP) as their vocabulary. In particular, ground truth WP correctness labels are needed for training confidence models, but the non-unique tokenization from word to WP causes inaccurate labels to be generated. This paper proposes and studies two confidence models of increasing complexity to solve this problem. The final model uses self-attention to directly learn word-level confidence without needing subword tokenization, and exploits full context features from multiple hypotheses to improve confidence accuracy. Experiments on Voice Search and long-tail test sets show standard metrics (e.g., NCE, AUC, RMSE) improving substantially. The proposed confidence module also enables a model selection approach to combine an on-device E2E model with a hybrid model on the server to address the rare word recognition problem for the E2E model.      
### 5.BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06695.pdf)
>  Inspired by the recent progress in self-supervised learning for computer vision that generates supervision using data augmentations, we explore a new general-purpose audio representation learning approach. We propose learning general-purpose audio representation from a single audio segment without expecting relationships between different time segments of audio samples. To implement this principle, we introduce Bootstrap Your Own Latent (BYOL) for Audio (BYOL-A, pronounced "viola"), an audio self-supervised learning method based on BYOL for learning general-purpose audio representation. Unlike most previous audio self-supervised learning methods that rely on agreement of vicinity audio segments or disagreement of remote ones, BYOL-A creates contrasts in an augmented audio segment pair derived from a single audio segment. With a combination of normalization and augmentation techniques, BYOL-A achieves state-of-the-art results in various downstream tasks. Extensive ablation studies also clarified the contribution of each component and their combinations.      
### 6.Plane Spiral OAM Mode-Group Based MIMO Communications: An Experimental Study  [ :arrow_down: ](https://arxiv.org/pdf/2103.06677.pdf)
>  Spatial division multiplexing using conventional orbital angular momentum (OAM) has become a well-known physical layer transmission method over the past decade. The mode-group (MG) superposed by specific single mode plane spiral OAM (PSOAM) waves has been proved to be a flexible beamforming method to achieve the azimuthal pattern diversity, which inherits the spiral phase distribution of conventional OAM wave. Thus, it possesses both the beam directionality and vorticity. In this paper, it's the first time to show and verify novel PSOAM MG based multiple-in-multiple-out (MIMO) communication link (MG-MIMO) experimentally in a line-of-sight (LoS) scenario. A compact multi-mode PSOAM antenna is demonstrated experimentally to generate multiple independent controllable PSOAM waves, which can be used for constructing MGs. After several proof-of-principle tests, it has been verified that the beam directionality gain of MG can improve the receiving signal-to-noise (SNR) level in an actual system, meanwhile, the vorticity can provide another degree of freedom (DoF) to reduce the spatial correlation of MIMO system. Furthermore, a tentative long-distance transmission experiment operated at 10.2 GHz has been performed successfully at a distance of 50 m with a single-way spectrum efficiency of 3.7 bits/s/Hz/stream. The proposed MG-MIMO may have potential in the long-distance LoS back-haul scenario.      
### 7.Open GOP Resolution Switching in HTTP Adaptive Streaming with VVC  [ :arrow_down: ](https://arxiv.org/pdf/2103.06675.pdf)
>  The user experience in adaptive HTTP streaming relies on offering bitrate ladders with suitable operation points for all users and typically involves multiple resolutions. While open GOP coding structures are generally known to provide substantial coding efficiency benefit, their use in HTTP streaming has been precluded through lacking support of reference picture resampling (RPR) in AVC and HEVC. The newly emerging Versatile Video Coding (VVC) standard supports RPR, but only conversational scenarios were primarily investigated during the design of VVC. This paper aims at enabling usage of RPR in HTTP streaming scenarios through analysing the drift potential of VVC coding tools and presenting a constrained encoding method that avoids severe drift artefacts in resolution switching with open GOP coding in VVC. In typical live streaming configurations, the presented method achieves up to -8.7% BD-rate reduction compared to closed GOP coding while in a typical Video on Demand configuration, up to -2.4% BD-rate reduction is reported. The constraints penalty compared to regular open GOP coding is 0.53% BD-rate in the worst case. The presented method will be integrated into the publicly available open source VVC encoder VVenC v0.3.      
### 8.Optimizing the Level of Challenge in Stroke Rehabilitation using Iterative Learning Control: a Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06664.pdf)
>  The level of challenge in stroke rehabilitation has to be carefully chosen to keep the patient engaged and motivated while not frustrating them. This paper presents a simulation where this level of challenge is automatically optimized using iterative learning control. An iterative learning controller provides a simulated stroke patient with a target task that the patient then learns to execute. Based on the error between the target task and the execution, the controller adjusts the difficulty of the target task for the next trial. The patient is simulated by a nonlinear autoregressive network with exogenous inputs to mimic their sensorimotor system and a second-order model to approximate their elbow joint dynamics. The results of the simulations show that the rehabilitation approach proposed in this paper results in more difficult tasks and a smoother difficulty progression as compared to a rehabilitation approach where the difficulty of the target task is updated according to a threshold.      
### 9.UAV Deployment, Device Scheduling and Resource Allocation for Energy-Efficient UAV-Aided IoT Networks with NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2103.06618.pdf)
>  This article investigates the energy efficiency issue in non-orthogonal multiple access (NOMA)-enhanced Internet-of-Things (IoT) networks, where a mobile unmanned aerial vehicle (UAV) is exploited as a flying base station to collect data from ground devices via the NOMA protocol. With the aim of maximizing network energy efficiency, we formulate a joint problem of UAV deployment, device scheduling and resource allocation. First, we formulate the joint device scheduling and spectrum allocation problem as a three-sided matching problem, and propose a novel low-complexity near-optimal algorithm. We also introduce the novel concept of `exploration' into the matching game for further performance improvement. By algorithm analysis, we prove the convergence and stability of the final matching state. Second, in an effort to allocate proper transmit power to IoT devices, we adopt the Dinkelbach's algorithm to obtain the optimal power allocation solution. Furthermore, we provide a simple but effective approach based on disk covering problem to determine the optimal number and locations of UAV's stop points to ensure that all IoT devices can be fully covered by the UAV via line-of-sight (LoS) links for the sake of better channel condition. Numerical results unveil that: i) the proposed joint UAV deployment, device scheduling and resource allocation scheme achieves much higher EE compared to predefined stationary UAV deployment case and fixed power allocation scheme, with acceptable complexity; and ii) the UAV-aided IoT networks with NOMA greatly outperforms the OMA case in terms of number of accessed devices.      
### 10.Probabilistic Scheduling of UFLS to Secure Credible Contingencies in Low Inertia Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.06616.pdf)
>  The reduced inertia levels in low-carbon power grids necessitate explicit constraints to limit frequency's nadir and rate of change during scheduling. This can result in significant curtailment of renewable energy due to the minimum generation of thermal plants that are needed to provide frequency response (FR) and inertia. Additional consideration of fast FR, a dynamically reduced largest loss and under frequency load shedding (UFLS) allows frequency security to be achieved more cost effectively. This paper derives a novel nadir constraint from the swing equation that, for the first time, provides a framework for the optimal comparison of all these services. We demonstrate that this constraint can be accurately and conservatively approximated for moderate UFLS levels with a second order cone, resulting in highly tractable convex problems. Case studies performed on a Great Britain 2030 system demonstrate that UFLS as an option to contain single plant outages can reduce annual operational costs by up to Â£559m, 52% of frequency security costs. The sensitivity of this value to wind penetration, abundance of alternative frequency services, UFLS amount and cost is explored.      
### 11.Uncoordinated and Decentralized Processing in Extra-Large MIMO Arrays  [ :arrow_down: ](https://arxiv.org/pdf/2103.06592.pdf)
>  We propose a decentralized receiver for extra-large multiple-input multiple-output (XL-MIMO) arrays. Our method operates with no central processing unit (CPU) and all the signal detection tasks are done in distributed nodes. We exploit a combined message-passing framework to design an uncoordinated detection scheme that overcomes three major challenges in the XL-MIMO systems: computational complexity, scalability and non-stationarities in user energy distribution. Our numerical evaluations show a significant performance improvement compared to benchmark distributed methods while operating very close to the centralized receivers.      
### 12.Forward-Backward Convolutional Recurrent Neural Networks and Tag-Conditioned Convolutional Neural Networks for Weakly Labeled Semi-supervised Sound Event Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.06581.pdf)
>  In this paper we present our system for the detection and classification of acoustic scenes and events (DCASE) 2020 Challenge Task 4: Sound event detection and separation in domestic environments. We introduce two new models: the forward-backward convolutional recurrent neural network (FBCRNN) and the tag-conditioned convolutional neural network (CNN). The FBCRNN employs two recurrent neural network (RNN) classifiers sharing the same CNN for preprocessing. With one RNN processing a recording in forward direction and the other in backward direction, the two networks are trained to jointly predict audio tags, i.e., weak labels, at each time step within a recording, given that at each time step they have jointly processed the whole recording. The proposed training encourages the classifiers to tag events as soon as possible. Therefore, after training, the networks can be applied to shorter audio segments of, e.g., 200 ms, allowing sound event detection (SED). Further, we propose a tag-conditioned CNN to complement SED. It is trained to predict strong labels while using (predicted) tags, i.e., weak labels, as additional input. For training pseudo strong labels from a FBCRNN ensemble are used. The presented system scored the fourth and third place in the systems and teams rankings, respectively. Subsequent improvements allow our system to even outperform the challenge baseline and winner systems in average by, respectively, 18.0% and 2.2% event-based F1-score on the validation set. Source code is publicly available at <a class="link-external link-https" href="https://github.com/fgnt/pb_sed" rel="external noopener nofollow">this https URL</a>.      
### 13.An unsupervised deep learning framework for medical image denoising  [ :arrow_down: ](https://arxiv.org/pdf/2103.06575.pdf)
>  Medical image acquisition is often intervented by unwanted noise that corrupts the information content. This paper introduces an unsupervised medical image denoising technique that learns noise characteristics from the available images and constructs denoised images. It comprises of two blocks of data processing, viz., patch-based dictionaries that indirectly learn the noise and residual learning (RL) that directly learns the noise. The model is generalized to account for both 2D and 3D images considering different medical imaging instruments. The images are considered one-by-one from the stack of MRI/CT images as well as the entire stack is considered, and decomposed into overlapping image/volume patches. These patches are given to the patch-based dictionary learning to learn noise characteristics via sparse representation while given to the RL part to directly learn the noise properties. K-singular value decomposition (K-SVD) algorithm for sparse representation is used for training patch-based dictionaries. On the other hand, residue in the patches is trained using the proposed deep residue network. Iterating on these two parts, an optimum noise characterization for each image/volume patch is captured and in turn it is subtracted from the available respective image/volume patch. The obtained denoised image/volume patches are finally assembled to a denoised image or 3D stack. We provide an analysis of the proposed approach with other approaches. Experiments on MRI/CT datasets are run on a GPU-based supercomputer and the comparative results show that the proposed algorithm preserves the critical information in the images as well as improves the visual quality of the images.      
### 14.Adaptive Control for Flow and Volume Regulation in Multi-Producer District Heating Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.06568.pdf)
>  Flow and storage volume regulation is essential for the adequate transport and management of energy resources in district heating systems. In this paper, we propose a novel and suitably tailored -- decentralized -- adaptive control scheme addressing this problem whilst offering closed-loop stability guarantees. We focus on a system configuration comprising multiple heat producers, consumers and storage tanks exchanging energy through a common distribution network, which are features of modern and prospective district heating installations. The effectiveness of the proposed controller is illustrated via numerical simulations.      
### 15.Advanced Geometry Surface Coding for Dynamic Point Cloud Compression  [ :arrow_down: ](https://arxiv.org/pdf/2103.06549.pdf)
>  In video-based dynamic point cloud compression (V-PCC), 3D point clouds are projected onto 2D images for compressing with the existing video codecs. However, the existing video codecs are originally designed for natural visual signals, and it fails to account for the characteristics of point clouds. Thus, there are still problems in the compression of geometry information generated from the point clouds. Firstly, the distortion model in the existing rate-distortion optimization (RDO) is not consistent with the geometry quality assessment metrics. Secondly, the prediction methods in video codecs fail to account for the fact that the highest depth values of a far layer is greater than or equal to the corresponding lowest depth values of a near layer. This paper proposes an advanced geometry surface coding (AGSC) method for dynamic point clouds (DPC) compression. The proposed method consists of two modules, including an error projection model-based (EPM-based) RDO and an occupancy map-based (OM-based) merge prediction. Firstly, the EPM model is proposed to describe the relationship between the distortion model in the existing video codec and the geometry quality metric. Secondly, the EPM-based RDO method is presented to project the existing distortion model on the plane normal and is simplified to estimate the average normal vectors of coding units (CUs). Finally, we propose the OM-based merge prediction approach, in which the prediction pixels of merge modes are refined based on the occupancy map. Experiments tested on the standard point clouds show that the proposed method achieves an average 9.84\% bitrate saving for geometry compression.      
### 16.A learning-based view extrapolation method for axial super-resolution  [ :arrow_down: ](https://arxiv.org/pdf/2103.06510.pdf)
>  Axial light field resolution refers to the ability to distinguish features at different depths by refocusing. The axial refocusing precision corresponds to the minimum distance in the axial direction between two distinguishable refocusing planes. High refocusing precision can be essential for some light field applications like microscopy. In this paper, we propose a learning-based method to extrapolate novel views from axial volumes of sheared epipolar plane images (EPIs). As extended numerical aperture (NA) in classical imaging, the extrapolated light field gives re-focused images with a shallower depth of field (DOF), leading to more accurate refocusing results. Most importantly, the proposed approach does not need accurate depth estimation. Experimental results with both synthetic and real light fields show that the method not only works well for light fields with small baselines as those captured by plenoptic cameras (especially for the plenoptic 1.0 cameras), but also applies to light fields with larger baselines.      
### 17.Deep Multiway Canonical Correlation Analysis for Multi-Subject EEG Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2103.06478.pdf)
>  The normalization of brain recordings from multiple subjects responding to the natural stimuli is one of the key challenges in auditory neuroscience. The objective of this normalization is to transform the brain data in such a way as to remove the inter-subject redundancies and to boost the component related to the stimuli. In this paper, we propose a deep learning framework to improve the correlation of electroencephalography (EEG) data recorded from multiple subjects engaged in an audio listening task. The proposed model extends the linear multi-way canonical correlation analysis (CCA) for audio-EEG analysis using an auto-encoder network with a shared encoder layer. The model is trained to optimize a combined loss involving correlation and reconstruction. The experiments are performed on EEG data collected from subjects listening to natural speech and music. In these experiments, we show that the proposed deep multi-way CCA (DMCCA) based model significantly improves the correlations over the linear multi-way CCA approach with absolute improvements of 0.08 and 0.29 in terms of the Pearson correlation values for speech and music tasks respectively.      
### 18.Automated segmentation of choroidal layers from 3-dimensional macular optical coherence tomography scans  [ :arrow_down: ](https://arxiv.org/pdf/2103.06425.pdf)
>  Background: Changes in choroidal thickness are associated with various ocular diseases and the choroid can be imaged using spectral-domain optical coherence tomography (SDOCT) and enhanced depth imaging OCT (EDIOCT). New Method: Eighty macular SDOCT volumes from 80 patients were obtained using the Zeiss Cirrus machine. Eleven additional control subjects had two Cirrus scans done in one visit along with EDIOCT using the Heidelberg Spectralis machine. To automatically segment choroidal layers from the OCT volumes, our graph-theoretic approach was utilized. The segmentation results were compared with reference standards from two graders, and the accuracy of automated segmentation was calculated using unsigned to signed border positioning thickness errors and Dice similarity coefficient (DSC). The repeatability and reproducibility of our choroidal thicknesses were determined by intraclass correlation coefficient (ICC), coefficient of variation (CV), and repeatability coefficient (RC). Results: The mean unsigned to signed border positioning errors for the choroidal inner and outer surfaces are 3.39plusminus1.26microns (mean plusminus SD) to minus1.52 plusminus 1.63microns and 16.09 plusminus 6.21microns to 4.73 plusminus 9.53microns, respectively. The mean unsigned to signed choroidal thickness errors are 16.54 plusminus 6.47microns to 6.25 plusminus 9.91microns, and the mean DSC is 0.949 plusminus 0.025. The ICC (95% CI), CV, RC values are 0.991 (0.977 to 0.997), 2.48%, 3.15microns for the repeatability and 0.991 (0.977 to 0.997), 2.49%, 0.53microns for the reproducibility studies, respectively. Comparison with Existing Method(s): The proposed method outperformed our previous method using choroidal vessel segmentation and inter-grader variability. Conclusions: This automated segmentation method can reliably measure choroidal thickness using different OCT platforms.      
### 19.SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver CT segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06419.pdf)
>  Background and objective: In this paper, a modified U-Net based framework is presented, which leverages techniques from Squeeze-and-Excitation (SE) block, Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and robust liver CT segmentation, and the effectiveness of the proposed method was tested on two public datasets LiTS17 and SLiver07. <br>Methods: A new network architecture called SAR-U-Net was designed. Firstly, the SE block is introduced to adaptively extract image features after each convolution in the U-Net encoder, while suppressing irrelevant regions, and highlighting features of specific segmentation task; Secondly, ASPP was employed to replace the transition layer and the output layer, and acquire multi-scale image information via different receptive fields. Thirdly, to alleviate the degradation problem, the traditional convolution block was replaced with the residual block and thus prompt the network to gain accuracy from considerably increased depth. <br>Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other closely related 2D-based models, the proposed method achieved the highest accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD, ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared with other closely related models, the proposed method achieved the highest segmentation accuracy except for the RVD. <br>Conclusion: The proposed model enables a great improvement on the accuracy compared to 2D-based models, and its robustness in circumvent challenging problems, such as small liver regions, discontinuous liver regions, and fuzzy liver boundaries, is also well demonstrated and validated.      
### 20.Age of Information Optimization in a RIS-Assisted Wireless Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.06405.pdf)
>  Future wireless communication networks are envisioned to revolutionize the digital world by offering super-low latency, unrivalled speed and reliable communication. Consequently, a myriad of propitious applications such as augmented/virtual reality, industry 4.0, etc., is anticipated to flourish. These applications rely on real-time information to make critical decisions and hence the temporal value of information generation and dissemination carries a paramount importance. One of the acute challenges restraining to unleash the full potentials of these applications is the time-varying wireless communication environment implying the unpredictable fading effects. In this paper, we consider a wireless network consisting of a base stations (BS) that is serving multiple traffic streams to forward their information updates to the destinations over an unreliable wireless channel. We study the benefits of utilizing reconfigurable intelligent surface (RIS) to mitigate the propagation-induced impairments of the wireless environment, enhance the link quality and ensure that the required freshness of information is achieved for the real-time applications. To quantify the freshness of information at each destination, we utilize the concept of Age of Information (AoI). AoI is determined by the time for a fresh update to arrive to its queue at the BS till it is successfully received at the destination. A joint RIS phase shift and scheduling optimization problem is formulated with the goal of minimizing the AoI. In order to solve this optimization problem, we propose an efficient algorithm based on semi-definite relaxation (SDR). We then extend our proposed system model and studied a use-case of real-time edge video analytics utilizing multi-access edge computing (MEC) system. Finally, we perform extensive simulations to verify the effectiveness of our proposed methods against other approaches.      
### 21.Automated liver tissues delineation based on machine learning techniques: A survey, current trends and future orientations  [ :arrow_down: ](https://arxiv.org/pdf/2103.06384.pdf)
>  There is no denying how machine learning and computer vision have grown in the recent years. Their highest advantages lie within their automation, suitability, and ability to generate astounding results in a matter of seconds in a reproducible manner. This is aided by the ubiquitous advancements reached in the computing capabilities of current graphical processing units and the highly efficient implementation of such techniques. Hence, in this paper, we survey the key studies that are published between 2014 and 2020, showcasing the different machine learning algorithms researchers have used to segment the liver, hepatic-tumors, and hepatic-vasculature structures. We divide the surveyed studies based on the tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels), highlighting the studies that tackle more than one task simultaneously. Additionally, the machine learning algorithms are classified as either supervised or unsupervised, and further partitioned if the amount of works that fall under a certain scheme is significant. Moreover, different datasets and challenges found in literature and websites, containing masks of the aforementioned tissues, are thoroughly discussed, highlighting the organizers original contributions, and those of other researchers. Also, the metrics that are used excessively in literature are mentioned in our review stressing their relevancy to the task at hand. Finally, critical challenges and future directions are emphasized for innovative researchers to tackle, exposing gaps that need addressing such as the scarcity of many studies on the vessels segmentation challenge, and why their absence needs to be dealt with in an accelerated manner.      
### 22.UAV Swarm-Enabled Aerial Reconfigurable Intelligent Surface  [ :arrow_down: ](https://arxiv.org/pdf/2103.06361.pdf)
>  Reconfigurable intelligent surface (RIS) offers tremendous spectrum and energy efficiency in wireless networks by adjusting the amplitudes and/or phases of passive reflecting elements to optimize signal reflection. With the agility and mobility of unmanned aerial vehicles (UAVs), RIS can be mounted on UAVs to enable three-dimensional signal reflection. Compared to the conventional terrestrial RIS (TRIS), the aerial RIS (ARIS) enjoys higher deployment flexibility, reliable air-to-ground links, and panoramic full-angle reflection. However, due to UAV's limited payload and battery capacity, it is difficult for a UAV to carry a RIS with a large number of reflecting elements. Thus, the scalability of the aperture gain could not be guaranteed. In practice, multiple UAVs can form a UAV swarm to enable the ARIS cooperatively. In this article, we first present an overview of the UAV swarm-enabled ARIS (SARIS), including its motivations and competitive advantages compared to TRIS and ARIS, as well as its new transformative applications in wireless networks. We then address the critical challenges of designing the SARIS by focusing on the beamforming design, SARIS channel estimation, and SARIS's deployment and movement. Next, the potential performance enhancement of SARIS is showcased and discussed with preliminary numerical results. Finally, open research opportunities are illustrated.      
### 23.A Computed Tomography Vertebral Segmentation Dataset with Anatomical Variations and Multi-Vendor Scanner Data  [ :arrow_down: ](https://arxiv.org/pdf/2103.06360.pdf)
>  With the advent of deep learning algorithms, fully automated radiological image analysis is within reach. In spine imaging, several atlas- and shape-based as well as deep learning segmentation algorithms have been proposed, allowing for subsequent automated analysis of morphology and pathology. The first Large Scale Vertebrae Segmentation Challenge (VerSe 2019) showed that these perform well on normal anatomy, but fail in variants not frequently present in the training dataset. Building on that experience, we report on the largely increased VerSe 2020 dataset and results from the second iteration of the VerSe challenge (MICCAI 2020, Lima, Peru). VerSe 2020 comprises annotated spine computed tomography (CT) images from 300 subjects with 4142 fully visualized and annotated vertebrae, collected across multiple centres from four different scanner manufacturers, enriched with cases that exhibit anatomical variants such as enumeration abnormalities (n=77) and transitional vertebrae (n=161). Metadata includes vertebral labelling information, voxel-level segmentation masks obtained with a human-machine hybrid algorithm and anatomical ratings, to enable the development and benchmarking of robust and accurate segmentation algorithms.      
### 24.Enhancing VMAF through New Feature Integration and Model Combination  [ :arrow_down: ](https://arxiv.org/pdf/2103.06338.pdf)
>  VMAF is a machine learning based video quality assessment method, originally designed for streaming applications, which combines multiple quality metrics and video features through SVM regression. It offers higher correlation with subjective opinions compared to many conventional quality assessment methods. In this paper we propose enhancements to VMAF through the integration of new video features and alternative quality metrics (selected from a diverse pool) alongside multiple model combination. The proposed combination approach enables training on multiple databases with varying content and distortion characteristics. Our enhanced VMAF method has been evaluated on eight HD video databases, and consistently outperforms the original VMAF model (0.6.1) and other benchmark quality metrics, exhibiting higher correlation with subjective ground truth data.      
### 25.Advancing Trajectory Optimization with Approximate Inference: Exploration, Covariance Control and Adaptive Risk  [ :arrow_down: ](https://arxiv.org/pdf/2103.06319.pdf)
>  Discrete-time stochastic optimal control remains a challenging problem for general, nonlinear systems under significant uncertainty, with practical solvers typically relying on the certainty equivalence assumption, replanning and/or extensive regularization. Control as inference is an approach that frames stochastic control as an equivalent inference problem, and has demonstrated desirable qualities over existing methods, namely in exploration and regularization. We look specifically at the input inference for control (i2c) algorithm, and derive three key characteristics that enable advanced trajectory optimization: An `expert' linear Gaussian controller that combines the benefits of open-loop optima and closed-loop variance reduction when optimizing for nonlinear systems, inherent adaptive risk sensitivity from the inference formulation, and covariance control functionality with only a minor algorithmic adjustment.      
### 26.Super-Resolving Beyond Satellite Hardware Using Realistically Degraded Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.06270.pdf)
>  Modern deep Super-Resolution (SR) networks have established themselves as valuable techniques in image reconstruction and enhancement. However, these networks are normally trained and tested on benchmark image data that lacks the typical image degrading noise present in real images. In this paper, we test the feasibility of using deep SR in real remote sensing payloads by assessing SR performance in reconstructing realistically degraded satellite images. We demonstrate that a state-of-the-art SR technique called Enhanced Deep Super-Resolution Network (EDSR), without domain specific pre-training, can recover encoded pixel data on images with poor ground sampling distance, provided the ground resolved distance is sufficient. However, this recovery varies amongst selected geographical types. Our results indicate that custom training has potential to further improve reconstruction of overhead imagery, and that new satellite hardware should prioritise optical performance over minimising pixel size as deep SR can overcome a lack of the latter but not the former.      
### 27.Swarm Robots in Agriculture  [ :arrow_down: ](https://arxiv.org/pdf/2103.06732.pdf)
>  Agricultural mechanization is an area of knowledge that has evolved a lot over the past century, its main actors being agricultural tractors that, in 100 years, have increased their powers by 3,300%. This evolution has resulted in an exponential increase in the field capacity of such machines. However, it has also generated negative results such as excessive consumption of fossil fuel, excessive weight on the soil, very high operating costs, and millionaire acquisition value. This paper aims to present an antiparadigmatic alternative in this area. It is proposing a swarm of small electric robotic tractors that together have the same field capacity as a large tractor with an internal combustion engine. A comparison of costs and field capacity between a 270 kW tractor and a swarm of ten swarm tractors of 24 kW each was carried out. The result demonstrated a wide advantage for the small robot team. It was also proposed the preliminary design of an electric swarm robot tractor. Finally, research challenges were suggested to operationalize such a proposal, calling on the Brazilian Robotics Research Community to elaborate a roadmap for research in the area of swarm robot for mechanized agricultural operations.      
### 28.Developing and evaluating an human-automation shared control takeover strategy based on Human-in-the-loop driving simulation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06700.pdf)
>  The purpose of this paper is to develop a shared control takeover strategy for smooth and safety control transition from an automation driving system to the human driver and to approve its positive impacts on drivers' behavior and attitudes. A "human-in-the-loop" driving simulator experiment was conducted to evaluate the impact of the proposed shared control takeover strategy under different disengagement conditions. Results of thirty-two drivers showed shared control takeover strategy could improve safety performance at the aggregated level, especially at non-driving related disengagements. For more urgent disengagements caused by another vehicle's sudden brake, a shared control strategy enlarges individual differences. The primary reason is that some drivers had higher self-reported mental workloads in response to the shared control takeover strategy. Therefore, shared control between driver and automation can involve driver's training to avoid mental overload when developing takeover strategies.      
### 29.Topological Data Analysis of Korean Music in Jeongganbo: A Cycle Structure  [ :arrow_down: ](https://arxiv.org/pdf/2103.06620.pdf)
>  Jeongganbo is a unique music representation invented by Sejong the Great. Contrary to the western music notation, the pitch of each note is encrypted and the length is visualized directly in a matrix form in Jeongganbo. We use topological data analysis (TDA) to analyze the Korean music written in Jeongganbo for Suyeonjang, Songuyeo, and Taryong, those well-known pieces played at the palace and among noble community. We are particularly interested in the cycle structure. We first define and determine the node elements of each music, characterized uniquely with its pitch and length. Then we transform the music into a graph and define the distance between the nodes as their adjacent occurrence rate. The graph is used as a point cloud whose homological structure is investigated by measuring the hole structure in each dimension. We identify cycles of each music, match those in Jeongganbo, and show how those cycles are interconnected. The main discovery of this work is that the cycles of Suyeonjang and Songuyeo, categorized as a special type of cyclic music known as Dodeuri, frequently overlap each other when appearing in the music while the cycles found in Taryong, which does not belong to Dodeuri class, appear individually.      
### 30.Multi-Format Contrastive Learning of Audio Representations  [ :arrow_down: ](https://arxiv.org/pdf/2103.06508.pdf)
>  Recent advances suggest the advantage of multi-modal training in comparison with single-modal methods. In contrast to this view, in our work we find that similar gain can be obtained from training with different formats of a single modality. In particular, we investigate the use of the contrastive learning framework to learn audio representations by maximizing the agreement between the raw audio and its spectral representation. We find a significant gain using this multi-format strategy against the single-format counterparts. Moreover, on the downstream AudioSet and ESC-50 classification task, our audio-only approach achieves new state-of-the-art results with a mean average precision of 0.376 and an accuracy of 90.5%, respectively.      
### 31.Nesterov Acceleration for Equality-Constrained Convex Optimization via Continuously Differentiable Penalty Functions  [ :arrow_down: ](https://arxiv.org/pdf/2103.06494.pdf)
>  We propose a framework to use Nesterov's accelerated method for constrained convex optimization problems. Our approach consists of first reformulating the original problem as an unconstrained optimization problem using a continuously differentiable exact penalty function. This reformulation is based on replacing the Lagrange multipliers in the augmented Lagrangian of the original problem by Lagrange multiplier functions. The expressions of these Lagrange multiplier functions, which depend upon the gradients of the objective function and the constraints, can make the unconstrained penalty function non-convex in general even if the original problem is convex. We establish sufficient conditions on the objective function and the constraints of the original problem under which the unconstrained penalty function is convex. This enables us to use Nesterov's accelerated gradient method for unconstrained convex optimization and achieve a guaranteed rate of convergence which is better than the state-of-the-art first-order algorithms for constrained convex optimization. Simulations illustrate our results.      
### 32.Robust High-speed Running for Quadruped Robots via Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.06484.pdf)
>  Deep reinforcement learning has emerged as a popular and powerful way to develop locomotion controllers for quadruped robots. Common approaches have largely focused on learning actions directly in joint space, or learning to modify and offset foot positions produced by trajectory generators. Both approaches typically require careful reward shaping and training for millions of time steps, and with trajectory generators introduce human bias into the resulting control policies. In this paper, we instead explore learning foot positions in Cartesian space, which we track with impedance control, for a task of running as fast as possible subject to environmental disturbances. Compared with other action spaces, we observe less needed reward shaping, much improved sample efficiency, the emergence of natural gaits such as galloping and bounding, and ease of sim-to-sim transfer. Policies can be learned in only a few million time steps, even for challenging tasks of running over rough terrain with loads of over 100% of the nominal quadruped mass. Training occurs in PyBullet, and we perform a sim-to-sim transfer to Gazebo, where our quadruped is able to run at over 4 m/s without a load, and 3.5 m/s with a 10 kg load, which is over 83% of the nominal quadruped mass. Video results can be found at <a class="link-external link-https" href="https://youtu.be/roE1vxpEWfw" rel="external noopener nofollow">this https URL</a>.      
### 33.Distributed Principal Subspace Analysis for Partitioned Big Data: Algorithms, Analysis, and Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06406.pdf)
>  Principal Subspace Analysis (PSA) is one of the most popular approaches for dimensionality reduction in signal processing and machine learning. But centralized PSA solutions are fast becoming irrelevant in the modern era of big data, in which the number of samples and/or the dimensionality of samples often exceed the storage and/or computational capabilities of individual machines. This has led to study of distributed PSA solutions, in which the data are partitioned across multiple machines and an estimate of the principal subspace is obtained through collaboration among the machines. It is in this vein that this paper revisits the problem of distributed PSA under the general framework of an arbitrarily connected network of machines that lacks a central server. The main contributions of the paper in this regard are threefold. First, two algorithms are proposed in the paper that can be used for distributed PSA in the case of data that are partitioned across either samples or (raw) features. Second, in the case of sample-wise partitioned data, the proposed algorithm and a variant of it are analyzed, and their convergence to the true subspace at linear rates is established. Third, extensive experiments on both synthetic and real-world data are carried out to validate the usefulness of the proposed algorithms. In particular, in the case of sample-wise partitioned data, an MPI-based distributed implementation is carried out to study the interplay between network topology and communications cost as well as to study of effect of straggler machines on the proposed algorithms.      
### 34.Reliable Power Grid: Long Overdue Alternatives to Surge Pricing  [ :arrow_down: ](https://arxiv.org/pdf/2103.06355.pdf)
>  This paper takes a fresh look at the economic theory that is motivation for pricing models, such as critical peak pricing (CPP), or surge pricing, and the demand response models advocated by policy makers and in the power systems literature. <br>The economic analysis in this paper begins with two premises: 1) a meaningful analysis requires a realistic model of stakeholder/consumer rationality, and 2) the relationship between electric power and the ultimate use of electricity are only loosely related in many cases. The most obvious examples are refrigerators and hot water heaters that consume power intermittently to maintain constant temperature. Based on a realistic model of user preferences, it is shown that the use of CPP and related pricing schemes will eventually destabilize the grid with increased participation. Analysis of this model also leads to a competitive equilibrium, along with a characterization of the dynamic prices in this equilibrium. However, we argue that these prices will not lead to a robust control solution that is acceptable to either grid operators or consumers. These findings are presented in this paper to alert policy makers of the risk of implementing real time prices to control our energy grid. <br>Competitive equilibrium theory can only provide a caricature of a real-world market, since complexities such as sunk cost and risk are not included. The paper explains why these approximations are especially significant in the power industry. It concludes with policy recommendations to bolster the reliability of the power grid, with a focus on planning across different timescales and alternate approaches to leveraging demand-side flexibility in the grid.      
### 35.Learning-Based Vulnerability Analysis of Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.06271.pdf)
>  This work focuses on the use of deep learning for vulnerability analysis of cyber-physical systems (CPS). Specifically, we consider a control architecture widely used in CPS (e.g., robotics), where the low-level control is based on e.g., the extended Kalman filter (EKF) and an anomaly detector. To facilitate analyzing the impact potential sensing attacks could have, our objective is to develop learning-enabled attack generators capable of designing stealthy attacks that maximally degrade system operation. We show how such problem can be cast within a learning-based grey-box framework where parts of the runtime information are known to the attacker, and introduce two models based on feed-forward neural networks (FNN); both models are trained offline, using a cost function that combines the attack effects on the estimation error and the residual signal used for anomaly detection, so that the trained models are capable of recursively generating such effective sensor attacks in real-time. The effectiveness of the proposed methods is illustrated on several case studies.      
