# ArXiv eess --Thu, 11 Mar 2021
### 1.Data-Driven Control and Data-Poisoning attacks in Buildings: the KTH Live-In Lab case study  [ :arrow_down: ](https://arxiv.org/pdf/2103.06208.pdf)
>  This work investigates the feasibility of using input-output data-driven control techniques for building control and their susceptibility to data-poisoning techniques. The analysis is performed on a digital replica of the KTH Livein Lab, a non-linear validated model representing one of the KTH Live-in Lab building testbeds. This work is motivated by recent trends showing a surge of interest in using data-based techniques to control cyber-physical systems. We also analyze the susceptibility of these controllers to data-poisoning methods, a particular type of machine learning threat geared towards finding imperceptible attacks that can undermine the performance of the system under consideration. We consider the Virtual Reference Feedback Tuning (VRFT), a popular data-driven control technique, and show its performance on the KTH Live-In Lab digital replica. We then demonstrate how poisoning attacks can be crafted and illustrate the impact of such attacks. Numerical experiments reveal the feasibility of using data-driven control methods for finding efficient control laws. However, a subtle change in the datasets can significantly deteriorate the performance of VRFT.      
### 2.Are we using appropriate segmentation metrics? Identifying correlates of human expert perception for CNN training beyond rolling the DICE coefficient  [ :arrow_down: ](https://arxiv.org/pdf/2103.06205.pdf)
>  In this study, we explore quantitative correlates of qualitative human expert perception. We discover that current quality metrics and loss functions, considered for biomedical image segmentation tasks, correlate moderately with segmentation quality assessment by experts, especially for small yet clinically relevant structures, such as the enhancing tumor in brain glioma. We propose a method employing classical statistics and experimental psychology to create complementary compound loss functions for modern deep learning methods, towards achieving a better fit with human quality assessment. When training a CNN for delineating adult brain tumor in MR images, all four proposed loss candidates outperform the established baselines on the clinically important and hardest to segment enhancing tumor label, while maintaining performance for other label channels.      
### 3.Poisoning Attacks against Data-Driven Control Methods  [ :arrow_down: ](https://arxiv.org/pdf/2103.06199.pdf)
>  This paper investigates poisoning attacks against data-driven control methods. This work is motivated by recent trends showing that, in supervised learning, slightly modifying the data in a malicious manner can drastically deteriorate the prediction ability of the trained model. We extend these analyses to the case of data-driven control methods. Specifically, we investigate how a malicious adversary can poison the data so as to minimize the performance of a controller trained using this data. We show that identifying the most impactful attack boils down to solving a bi-level non-convex optimization problem, and provide theoretical insights on the attack. We present a generic algorithm finding a local optimum of this problem and illustrate our analysis in the case of a model-reference based approach, the Virtual Reference Feedback Tuning technique, and on data-driven methods based on Willems et al. lemma. Numerical experiments reveal that minimal but well-crafted changes in the dataset are sufficient to deteriorate the performance of data-driven control methods significantly, and even make the closed-loop system unstable.      
### 4.Weak labels and anatomical knowledge: making deep learning practical for intracranial aneurysm detection in TOF-MRA  [ :arrow_down: ](https://arxiv.org/pdf/2103.06168.pdf)
>  Supervised segmentation algorithms yield state-of-the-art results for automated anomaly detection. However, these models require voxel-wise labels which are time-consuming to draw for medical experts. An interesting alternative to voxel-wise annotations is the use of weak labels: these can be coarse or oversized annotations that are less precise, but considerably faster to create. In this work, we address the task of brain aneurysm detection by developing a fully automated, deep neural network that is trained utilizing oversized weak labels. Furthermore, since aneurysms mainly occur in specific anatomical locations, we build our model leveraging the underlying anatomy of the brain vasculature both during training and inference. We apply our model to 250 subjects (120 patients, 130 controls) who underwent Time-Of-Flight Magnetic Resonance Angiography (TOF-MRA) and presented a total of 154 aneurysms. To assess the robustness of the algorithm, we participated in a MICCAI challenge for TOF-MRA data (93 patients, 20 controls, 125 aneurysms) which allowed us to obtain results also for subjects coming from a different institution. Our network achieves an average sensitivity of 77% on our in-house data, with a mean False Positive (FP) rate of 0.72 per patient. Instead, on the challenge data, we attain a sensitivity of 59% with a mean FP rate of 1.18, ranking in 7th/14 position for detection and in 4th/11 for segmentation on the open leaderboard. When computing detection performances with respect to aneurysms' risk of rupture, we found no statistical difference between two risk groups (p = 0.12), although the sensitivity for dangerous aneurysms was higher (78%). Our approach suggests that clinically useful sensitivity can be achieved using weak labels and exploiting prior anatomical knowledge; this expands the feasibility of deep learning studies to hospitals that have limited time and data.      
### 5.Model-inspired Deep Learning for Light-Field Microscopy with Application to Neuron Localization  [ :arrow_down: ](https://arxiv.org/pdf/2103.06164.pdf)
>  Light-field microscopes are able to capture spatial and angular information of incident light rays. This allows reconstructing 3D locations of neurons from a single <a class="link-external link-http" href="http://snap-shot.In" rel="external noopener nofollow">this http URL</a> this work, we propose a model-inspired deep learning approach to perform fast and robust 3D localization of sources using light-field microscopy images. This is achieved by developing a deep network that efficiently solves a convolutional sparse coding (CSC) problem to map Epipolar Plane Images (EPI) to corresponding sparse codes. The network architecture is designed systematically by unrolling the convolutional Iterative Shrinkage and Thresholding Algorithm (ISTA) while the network parameters are learned from a training dataset. Such principled design enables the deep network to leverage both domain knowledge implied in the model, as well as new parameters learned from the data, thereby combining advantages of model-based and learning-based methods. Practical experiments on localization of mammalian neurons from light-fields show that the proposed approach simultaneously provides enhanced performance, interpretability and efficiency.      
### 6.Point Cloud Sampling via Graph Balancing and Gershgorin Disc Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2103.06153.pdf)
>  3D point cloud (PC) -- a collection of discrete geometric samples of a physical object's surface -- is typically large in size, which entails expensive subsequent operations like viewpoint image rendering and object recognition. Leveraging on recent advances in graph sampling, we propose a fast PC sub-sampling algorithm that reduces its size while preserving the overall object shape. Specifically, to articulate a sampling objective, we first assume a super-resolution (SR) method based on feature graph Laplacian regularization (FGLR) that reconstructs the original high-resolution PC, given 3D points chosen by a sampling matrix $\H$. We prove that minimizing a worst-case SR reconstruction error is equivalent to maximizing the smallest eigenvalue $\lambda_{\min}$ of a matrix $\H^{\top} \H + \mu \cL$, where $\cL$ is a symmetric, positive semi-definite matrix computed from the neighborhood graph connecting the 3D points. Instead, for fast computation we maximize a lower bound $\lambda^-_{\min}(\H^{\top} \H + \mu \cL)$ via selection of $\H$ in three steps. Interpreting $\cL$ as a generalized graph Laplacian matrix corresponding to an unbalanced signed graph $\cG$, we first approximate $\cG$ with a balanced graph $\cG_B$ with the corresponding generalized graph Laplacian matrix $\cL_B$. Second, leveraging on a recent theorem called Gershgorin disc perfect alignment (GDPA), we perform a similarity transform $\cL_p = §\cL_B §^{-1}$ so that Gershgorin disc left-ends of $\cL_p$ are all aligned at the same value $\lambda_{\min}(\cL_B)$. Finally, we perform PC sub-sampling on $\cG_B$ using a graph sampling algorithm to maximize $\lambda^-_{\min}(\H^{\top} \H + \mu \cL_p)$ in roughly linear time. Experimental results show that 3D points chosen by our algorithm outperformed competing schemes both numerically and visually in SR reconstruction quality.      
### 7.Semi-supervised Learning for COVID-19 Image Classification via ResNet  [ :arrow_down: ](https://arxiv.org/pdf/2103.06140.pdf)
>  Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic in over 200 countries and territories, which has resulted in a great public health concern across the international community. Analysis of X-ray imaging data can play a critical role in timely and accurate screening and fighting against COVID-19. Supervised deep learning has been successfully applied to recognize COVID-19 pathology from X-ray imaging datasets. However, it requires a substantial amount of annotated X-ray images to train models, which is often not applicable to data analysis for emerging events such as COVID-19 outbreak, especially in the early stage of the outbreak. To address this challenge, this paper proposes a two-path semi-supervised deep learning model, ssResNet, based on Residual Neural Network (ResNet) for COVID-19 image classification, where two paths refer to a supervised path and an unsupervised path, respectively. Moreover, we design a weighted supervised loss that assigns higher weight for the minority classes in the training process to resolve the data imbalance. Experimental results on a large-scale of X-ray image dataset COVIDx demonstrate that the proposed model can achieve promising performance even when trained on very few labeled training images.      
### 8.Conversion Matrix Method of Moments for Time-Varying Electromagnetic Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2103.06135.pdf)
>  A conversion matrix approach to solving network problems involving time-varying circuit components is applied to the method of moments for electromagnetic scattering analysis. Detailed formulations of this technique's application to the scattering analysis of structures loaded with time-varying circuit networks or constructed from general time-varying media are presented. The computational cost of the method is discussed, along with an analysis of compression techniques capable of significantly reducing computational cost for partially loaded systems. Several numerical examples demonstrate the capabilities of the technique along with its validation against conventional methods of modeling time-varying electromagnetic systems, such as finite difference time domain and transient circuit co-simulation.      
### 9.Machine Learning Prediction of Time-Varying Rayleigh Channels  [ :arrow_down: ](https://arxiv.org/pdf/2103.06131.pdf)
>  Channel state information (CSI) rapidly becomes outdated in high mobility scenarios, degrading the performance of wireless communication systems. In these cases, time series prediction techniques can be applied to combat the effects of outdated CSI. Recently, it has been shown that recurrent neural networks (RNNs) exhibit outstanding performance in time series prediction tasks. In this paper, we investigate the performance of RNN and long short term memory (LSTM) predictors in a simple Rayleigh flat-fading channel. We conduct numerical experiments to evaluate whether these machine-learning (ML)-based predictors can outperform the optimal linear minimum mean square error Wiener predictor. Our simulation results indicate that the considered neural network predictors outperform the Wiener predictor for small observation window lengths and are more robust under weak channel correlation as well as in the presence of noise. Furthermore, we show that simple shallow RNNs are sufficient to model Rayleigh channels over a wide range of Doppler shifts.      
### 10.Spatial Attention-based Non-reference Perceptual Quality Prediction Network for Omnidirectional Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.06116.pdf)
>  Due to the strong correlation between visual attention and perceptual quality, many methods attempt to use human saliency information for image quality assessment. Although this mechanism can get good performance, the networks require human saliency labels, which is not easily accessible for omnidirectional images (ODI). To alleviate this issue, we propose a spatial attention-based perceptual quality prediction network for non-reference quality assessment on ODIs (SAP-net). To drive our SAP-net, we establish a large-scale IQA dataset of ODIs (IQA-ODI), which is composed of subjective scores of 200 subjects on 1,080 ODIs. In IQA-ODI, there are 120 high quality ODIs as reference, and 960 ODIs with impairments in both JPEG compression and map projection. Without any human saliency labels, our network can adaptively estimate human perceptual quality on impaired ODIs through a self-attention manner, which significantly promotes the prediction performance of quality scores. Moreover, our method greatly reduces the computational complexity in quality assessment task on ODIs. Extensive experiments validate that our network outperforms 9 state-of-the-art methods for quality assessment on ODIs. The dataset and code have been available on \url{ <a class="link-external link-https" href="https://github.com/yanglixiaoshen/SAP-Net" rel="external noopener nofollow">this https URL</a>}.      
### 11.U-Net Transformer: Self and Cross Attention for Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06104.pdf)
>  Medical image segmentation remains particularly challenging for complex and low-contrast anatomical structures. In this paper, we introduce the U-Transformer network, which combines a U-shaped architecture for image segmentation with self- and cross-attention from Transformers. U-Transformer overcomes the inability of U-Nets to model long-range contextual interactions and spatial dependencies, which are arguably crucial for accurate segmentation in challenging contexts. To this end, attention mechanisms are incorporated at two main levels: a self-attention module leverages global interactions between encoder features, while cross-attention in the skip connections allows a fine spatial recovery in the U-Net decoder by filtering out non-semantic features. Experiments on two abdominal CT-image datasets show the large performance gain brought out by U-Transformer compared to U-Net and local Attention U-Nets. We also highlight the importance of using both self- and cross-attention, and the nice interpretability features brought out by U-Transformer.      
### 12.Physics-Guided Neural Networks for Inversion-based Feedforward Control applied to Linear Motors  [ :arrow_down: ](https://arxiv.org/pdf/2103.06092.pdf)
>  Ever-increasing throughput specifications in semiconductor manufacturing require operating high-precision mechatronics, such as linear motors, at higher accelerations. In turn this creates higher nonlinear parasitic forces that cannot be handled by industrial feedforward controllers. Motivated by this problem, in this paper we develop a general framework for inversion-based feedforward controller design using physics-guided neural networks (PGNNs). In contrast with black-box neural networks, the developed PGNNs embed prior physical knowledge in the input and hidden layers, which results in improved training convergence and learning of underlying physical laws. The PGNN inversion-based feedforward control framework is validated in simulation on an industrial linear motor, for which it achieves a mean average tracking error twenty times smaller than mass-acceleration feedforward in simulation.      
### 13.On the Dual Implementation of Collision-Avoidance Constraints in Path-Following MPC for Underactuated Surface Vessels  [ :arrow_down: ](https://arxiv.org/pdf/2103.06085.pdf)
>  A path-following collision-avoidance model predictive control (MPC) method is proposed which approximates obstacle shapes as convex polygons. Collision-avoidance is ensured by means of the signed distance function which is calculated efficiently as part of the MPC problem by making use of a dual formulation. The overall MPC problem can be solved by standard nonlinear programming (NLP) solvers. The dual signed distance formulation yields, besides the (dual) collision-avoidance constraints, norm, and consistency constraints. A novel approach is presented that combines the arising norm equality with the dual collision-avoidance inequality constraints to yield an alternative formulation reduced in complexity. Moving obstacles are considered using separate convex sets of linearly predicted obstacle positions in the dual problem. The theoretical findings and simplifications are compared with the often-used ellipsoidal obstacle formulation and are analyzed with regard to efficiency by the example of a simulated path-following autonomous surface vessel during a realistic maneuver and AIS obstacle data from the Kiel bay area.      
### 14.Grid-Graph Signal Processing (Grid-GSP): A Graph Signal Processing Framework for the Power Grid  [ :arrow_down: ](https://arxiv.org/pdf/2103.06068.pdf)
>  The underlying theme of this paper is to explore the various facets of power systems data through the lens of graph signal processing (GSP), laying down the foundations of the Grid-GSP framework. Grid-GSP provides an interpretation for the spatio-temporal properties of voltage phasor measurements, by showing how the well-known power systems modeling supports a generative low-pass graph filter model for the state variables, namely the voltage phasors. Using the model we formalize the empirical observation that voltage phasor measurement data lie in a low-dimensional subspace and tie their spatio-temporal structure to generator voltage dynamics. The Grid-GSP generative model is then successfully employed to investigate the problems pertaining to the grid of data sampling and interpolation, network inference, detection of anomalies and data compression. Numerical results on a large synthetic grid that mimics the real-grid of the state of Texas, ACTIVSg2000, and on real-world measurements from ISO-New England verify the efficacy of applying Grid-GSP methods to electric grid data.      
### 15.Distributed Channel Access for Control Over Unknown Memoryless Communication Channels  [ :arrow_down: ](https://arxiv.org/pdf/2103.06048.pdf)
>  We consider the distributed channel access problem for a system consisting of multiple control subsystems that close their loop over a shared wireless network. We propose a distributed method for providing deterministic channel access without requiring explicit information exchange between the subsystems. This is achieved by utilizing timers for prioritizing channel access with respect to a local cost which we derive by transforming the control objective cost to a form that allows its local computation. This property is then exploited for developing our distributed deterministic channel access scheme. A framework to verify the stability of the system under the resulting scheme is then proposed. Next, we consider a practical scenario in which the channel statistics are unknown. We propose learning algorithms for learning the parameters of imperfect communication links for estimating the channel quality and, hence, define the local cost as a function of this estimation and control performance. We establish that our learning approach results in collision-free channel access. The behavior of the overall system is exemplified via a proof-of-concept illustrative example, and the efficacy of this mechanism is evaluated for large-scale networks via simulations.      
### 16.Signal Temporal Logic Task Decomposition via Convex Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2103.06047.pdf)
>  In this paper we focus on the problem of decomposing a global Signal Temporal Logic formula (STL) assigned to a multi-agent system to local STL tasks when the team of agents is a-priori decomposed to disjoint sub-teams. The predicate functions associated to the local tasks are parameterized as hypercubes depending on the states of the agents in a given sub-team. The parameters of the functions are, then, found as part of the solution of a convex program that aims implicitly at maximizing the volume of the zero level-set of the corresponding predicate function. Two alternative definitions of the local STL tasks are proposed and the satisfaction of the global STL formula is proven when the conjunction of the local STL tasks is satisfied.      
### 17.Event-Triggered Distributed Estimation With Decaying Communication Rate  [ :arrow_down: ](https://arxiv.org/pdf/2103.06035.pdf)
>  We study distributed estimation of a high-dimensional static parameter vector through a group of sensors whose communication network is modeled by a fixed directed graph. Different from existing time-triggered communication schemes, an event-triggered asynchronous scheme is investigated in order to reduce communication while preserving estimation convergence. A distributed estimation algorithm with a single step size is first proposed based on an event-triggered communication scheme with a time-dependent decaying threshold. With the event-triggered scheme, each sensor sends its estimate to neighbor sensors only when the difference between the current estimate and the last sent-out estimate is larger than the triggering threshold. Different sensors can have different step sizes and triggering thresholds, enabling the parameter estimation process to be conducted in a fully distributed way. We prove that the proposed algorithm has mean-square and almost-sure convergence respectively, under proper conditions of network connectivity and system collective observability. The collective observability is the possibly mildest condition, since it is a spatially and temporally collective condition of all sensors and allows sensor observation matrices to be time-varying, stochastic, and non-stationary. Moreover, we provide estimates for the convergence rates, which are related to the step sizes as well as the triggering thresholds. Furthermore, we prove that the communication rate is decaying to zero with a certain rate almost surely as time goes to infinity. We show that it is feasible to tune the thresholds and the step sizes such that requirements of algorithm convergence and communication rate decay are satisfied simultaneously.Numerical simulations are provided to illustrate the developed results.      
### 18.Relaxed bearing rigidity and bearing formation control under persistence of excitation  [ :arrow_down: ](https://arxiv.org/pdf/2103.06024.pdf)
>  This paper addresses the problem of bearing formation control in $d$ $(d\ge 2)$-dimensional space by exploring persistence of excitation (PE) of the desired bearing reference. A general concept of bearing persistently exciting (BPE) formation defined in $d$-dimensional space is fully developed for the first time. By providing a desired formation that is BPE, distributed control laws for multi-agent systems under both single- and double-integrator dynamics are proposed using bearing measurements (also velocity measurements for double-integrator dynamics), which guarantee exponential stabilization of the desired formation up to a translation vector. A key contribution of this work is to show that the classical bearing rigidity condition on the graph topology, required for achieving the stabilization of a formation up to a scaling factor, is relaxed in a natural manner by exploring PE conditions imposed solely on a specific set of desired bearing vectors. Simulation results are provided to illustrate the performance of the proposed control method.      
### 19.Performance Optimization of Surface Electromyography (sEMG) based Biometric Sensing System for both Verification and Identification  [ :arrow_down: ](https://arxiv.org/pdf/2103.06015.pdf)
>  Recently, surface electromyography (sEMG) emerged as a novel biometric authentication method. Since EMG system parameters, such as the feature extraction methods and the number of channels, have been known to affect system performances, it is important to investigate these effects on the performance of the sEMG-based biometric system to determine optimal system parameters. In this study, three robust feature extraction methods, Time-domain (TD) feature, Frequency Division Technique (FDT), and Autoregressive (AR) feature, and their combinations were investigated while the number of channels varying from one to eight. For these system parameters, the performance of sixteen static wrist and hand gestures was systematically investigated in two authentication modes: verification and identification. The results from 24 participants showed that the TD features significantly (p&lt;0.05) and consistently outperformed FDT and AR features for all channel numbers. The results also showed that the performance of a four-channel setup was not significantly different from those with higher number of channels. The average equal error rate (EER) for a four-channel sEMG verification system was 4% for TD features, 5.3% for FDT features, and 10% for AR features. For an identification system, the average Rank-1 error (R1E) for a four-channel configuration was 3% for TD features, 12.4% for FDT features, and 36.3% for AR features. The electrode position on the flexor carpi ulnaris (FCU) muscle had a critical contribution to the authentication performance. Thus, the combination of the TD feature set and a four-channel sEMG system with one of the electrodes positioned on the FCU are recommended for optimal authentication performance.      
### 20.Full reconstruction of acoustic wavefields by means of pointwise measurements  [ :arrow_down: ](https://arxiv.org/pdf/2103.06014.pdf)
>  Sound propagation in the ocean is considered. We demonstrate a novel algorithm for full wavefield reconstruction using pointwise measurements by means of a vertical array. The algorithm is based on the so-called discrete variable representation and can be implemented both for tonal and pulse signals. It is shown that the algorithm is robust against array distortions and ambient noise of moderate amplitude. Efficiency of reconstruction is verified by means of numerical simulation with a model of a shallow-sea waveguide. It is found that the effect of bottom sound attenuation enables accurate reconstruction with arrays having relatively low density of hydrophones.      
### 21.Robust graph-filter identification with graph denoising regularization  [ :arrow_down: ](https://arxiv.org/pdf/2103.05976.pdf)
>  When approaching graph signal processing tasks, graphs are usually assumed to be perfectly known. However, in many practical applications, the observed (inferred) network is prone to perturbations which, if ignored, will hinder performance. Tailored to those setups, this paper presents a robust formulation for the problem of graph-filter identification from input-output observations. Different from existing works, our approach consists in addressing the robust identification by formulating a joint graph denoising and graph-filter identification problem. Such a problem is formulated as a non-convex optimization, suitable relaxations are proposed, and graph-stationarity assumptions are incorporated to enhance performance. Finally, numerical experiments with synthetic and real-world graphs are used to assess the proposed schemes and compare them with existing (robust) alternatives.      
### 22.Frequency Logarithmic Perturbation on the Group-Velocity Dispersion Parameter with Applications to Passive Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05972.pdf)
>  Signal propagation in an optical fiber can be described by the nonlinear Schrödinger equation (NLSE). The NLSE has no known closed-form solution, mostly due to the interaction of dispersion and nonlinearities. In this paper, we present a novel closed-form approximate model for the nonlinear optical channel, with applications to passive optical networks. The proposed model is derived using logarithmic perturbation in the frequency domain on the group-velocity dispersion (GVD) parameter of the NLSE. The model can be seen as an improvement of the recently proposed regular perturbation (RP) on the GVD parameter. RP and logarithmic perturbation (LP) on the nonlinear coefficient have already been studied in the literature, and are hereby compared with RP on the GVD parameter and the proposed LP model. As an application of the model, we focus on passive optical networks. For a 20 km PON at 10 Gbaud, the proposed model improves upon LP on the nonlinear coefficient by 1.5 dB. For the same system, a detector based on the proposed LP model reduces the uncoded bit-error-rate by up to 5.4 times at the same input power or reduces the input power by 0.4 dB at the same information rate.      
### 23.Self-supervised Change Detection in Multi-view Remote Sensing Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.05969.pdf)
>  The vast amount of unlabeled multi-temporal and multi-sensor remote sensing data acquired by the many Earth Observation satellites present a challenge for change detection. Recently, many generative model-based methods have been proposed for remote sensing image change detection on such unlabeled data. However, the high diversities in the learned features weaken the discrimination of the relevant change indicators in unsupervised change detection tasks. Moreover, these methods lack research on massive archived images. In this work, a self-supervised change detection approach based on an unlabeled multi-view setting is proposed to overcome this limitation. This is achieved by the use of a multi-view contrastive loss and an implicit contrastive strategy in the feature alignment between multi-view images. In this approach, a pseudo-Siamese network is trained to regress the output between its two branches pre-trained in a contrastive way on a large dataset of multi-temporal homogeneous or heterogeneous image patches. Finally, the feature distance between the outputs of the two branches is used to define a change measure, which can be analyzed by thresholding to get the final binary change map. Experiments are carried out on five homogeneous and heterogeneous remote sensing image datasets. The proposed SSL approach is compared with other supervised and unsupervised state-of-the-art change detection methods. Results demonstrate both improvements over state-of-the-art unsupervised methods and that the proposed SSL approach narrows the gap between unsupervised and supervised change detection.      
### 24.Compositional Construction of Safety Controllers for Networks of Continuous-Space POMDPs  [ :arrow_down: ](https://arxiv.org/pdf/2103.05906.pdf)
>  In this paper, we propose a compositional framework for the synthesis of safety controllers for networks of partially-observed discrete-time stochastic control systems (a.k.a. continuous-space POMDPs). Given an estimator, we utilize a discretization-free approach to synthesize controllers ensuring safety specifications over finite-time horizons. The proposed framework is based on a notion of so-called local control barrier functions computed for subsystems in two different ways. In the first scheme, no prior knowledge of estimation accuracy is needed. The second framework utilizes a probability bound on the estimation accuracy using a notion of so called stochastic simulation functions. In both proposed schemes, we drive sufficient small-gain type conditions in order to compositionally construct control barrier functions for interconnected POMDPs using local barrier functions computed for subsystems. Leveraging compositionality results, the constructed control barrier functions enable us to compute lower bounds on the probabilities that the interconnected POMDPs avoid certain unsafe regions in finite-time horizons. We demonstrate the effectiveness of our proposed approaches by applying them to an adaptive cruise control problem.      
### 25.Co-design of Control and Scheduling in Networked Systems under Denial-of-Service attacks  [ :arrow_down: ](https://arxiv.org/pdf/2103.05893.pdf)
>  We consider the joint design of control and scheduling under stochastic Denial-of-Service (DoS) attacks in the context of networked control systems. A sensor takes measurements of the system output and forwards its dynamic state estimates to a remote controller over a packet-dropping link. The controller determines the optimal control law for the process using the estimates it receives. An attacker aims at degrading the control performance by increasing the packet-dropout rate with a DoS attack towards the sensor-controller channel. Assume both the controller and the attacker are rational in a game-theoretic sense. We establish a partially observable stochastic game to derive the optimal joint design of scheduling and control. Using dynamic programming we prove that the control and scheduling policies can be designed separately without sacrificing optimality, making the problem equivalent to a complete information game. We employ Nash Q-learning to solve the problem and prove that the solution is guaranteed to constitute an $\epsilon$-Nash equilibrium. Numerical examples are provided to illustrate the tradeoffs between control performance and communication cost.      
### 26.Temporal Feature Fusion with Sampling Pattern Optimization for Multi-echo Gradient Echo Acquisition and Image Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2103.05878.pdf)
>  Quantitative imaging in MRI usually involves acquisition and reconstruction of a series of images at multi-echo time points, which possibly requires more scan time and specific reconstruction technique compared to conventional qualitative imaging. In this work, we focus on optimizing the acquisition and reconstruction process of multi-echo gradient echo pulse sequence for quantitative susceptibility mapping as one important quantitative imaging method in MRI. A multi-echo sampling pattern optimization block extended from LOUPE-ST is proposed to optimize the k-space sampling patterns along echoes. Besides, a recurrent temporal feature fusion block is proposed and inserted into a backbone deep ADMM network to capture the signal evolution along echo time during reconstruction. Experiments show that both blocks help improve multi-echo image reconstruction performance.      
### 27.A Sequential Variational Mode Decomposition Method  [ :arrow_down: ](https://arxiv.org/pdf/2103.05874.pdf)
>  In this paper, we introduce a sequential variational mode decomposition method to separate non-stationary mixed signals successively. This method is inspired by the variational method, and can precisely recover the original components one by one from the raw mixture without prior knowing or assuming the number of components. And in such a way, the mode number also can be determined during the separation procedure. Such character brings great convenience for real application and differs from the current VMD method. Furthermore, we also conduct a principal elongation for the mixture signal before the decomposing operation. By applying such an approach, the end effect can be reduced to a low level compared with the VMD method. To obtain higher accuracy, a refinement process has been introduced after gross extraction. Combined these techniques together, the final decomposition result implies a significant improvement compared with the VMD method and EMD method.      
### 28.Single-photon imaging over 200 km  [ :arrow_down: ](https://arxiv.org/pdf/2103.05860.pdf)
>  Long-range active imaging has widespread applications in remote sensing and target recognition. Single-photon light detection and ranging (lidar) has been shown to have high sensitivity and temporal resolution. On the application front, however, the operating range of practical single-photon lidar systems is limited to about tens of kilometers over the Earth's atmosphere, mainly due to the weak echo signal mixed with high background noise. Here, we present a compact coaxial single-photon lidar system capable of realizing 3D imaging at up to 201.5 km. It is achieved by using high-efficiency optical devices for collection and detection, and what we believe is a new noise-suppression technique that is efficient for long-range applications. We show that photon-efficient computational algorithms enable accurate 3D imaging over hundreds of kilometers with as few as 0.44 signal photons per pixel. The results represent a significant step toward practical, low-power lidar over extra-long ranges.      
### 29.Fusing Medical Image Features and Clinical Features with Deep Learning for Computer-Aided Diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2103.05855.pdf)
>  Current Computer-Aided Diagnosis (CAD) methods mainly depend on medical images. The clinical information, which usually needs to be considered in practical clinical diagnosis, has not been fully employed in CAD. In this paper, we propose a novel deep learning-based method for fusing Magnetic Resonance Imaging (MRI)/Computed Tomography (CT) images and clinical information for diagnostic tasks. Two paths of neural layers are performed to extract image features and clinical features, respectively, and at the same time clinical features are employed as the attention to guide the extraction of image features. Finally, these two modalities of features are concatenated to make decisions. We evaluate the proposed method on its applications to Alzheimer's disease diagnosis, mild cognitive impairment converter prediction and hepatic microvascular invasion diagnosis. The encouraging experimental results prove the values of the image feature extraction guided by clinical features and the concatenation of two modalities of features for classification, which improve the performance of diagnosis effectively and stably.      
### 30.Learning to Estimate Kernel Scale and Orientation of Defocus Blur with Asymmetric Coded Aperture  [ :arrow_down: ](https://arxiv.org/pdf/2103.05843.pdf)
>  Consistent in-focus input imagery is an essential precondition for machine vision systems to perceive the dynamic environment. A defocus blur severely degrades the performance of vision systems. To tackle this problem, we propose a deep-learning-based framework estimating the kernel scale and orientation of the defocus blur to adjust lens focus rapidly. Our pipeline utilizes 3D ConvNet for a variable number of input hypotheses to select the optimal slice from the input stack. We use random shuffle and Gumbel-softmax to improve network performance. We also propose to generate synthetic defocused images with various asymmetric coded apertures to facilitate training. Experiments are conducted to demonstrate the effectiveness of our framework.      
### 31.Best of Both Worlds: Robust Accented Speech Recognition with Adversarial Transfer Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.05834.pdf)
>  Training deep neural networks for automatic speech recognition (ASR) requires large amounts of transcribed speech. This becomes a bottleneck for training robust models for accented speech which typically contains high variability in pronunciation and other semantics, since obtaining large amounts of annotated accented data is both tedious and costly. Often, we only have access to large amounts of unannotated speech from different accents. In this work, we leverage this unannotated data to provide semantic regularization to an ASR model that has been trained only on one accent, to improve its performance for multiple accents. We propose Accent Pre-Training (Acc-PT), a semi-supervised training strategy that combines transfer learning and adversarial training. Our approach improves the performance of a state-of-the-art ASR model by 33% on average over the baseline across multiple accents, training only on annotated samples from one standard accent, and as little as 105 minutes of unannotated speech from a target accent.      
### 32.Spectrum Congruency of Multiscale Local Patches for Edge Detection  [ :arrow_down: ](https://arxiv.org/pdf/2103.05828.pdf)
>  This paper proposes a novel feature called spectrum congruency for describing edges in images. The spectrum congruency is a generalization of the phase congruency, which depicts how much each Fourier components of the image are congruent in phase. Instead of using fixed bases in phase congruency, the spectrum congruency measures the congruency of the energy distribution of multiscale patches in a data-driven transform domain, which is more adaptable to the input images. Multiscale image patches are used to acquire different frequency components for modeling the local energy and amplitude. The spectrum congruency coincides nicely with human visions of perceiving features and provides a more reliable way of detecting edges. Unlike most existing differential-based multiscale edge detectors which simply combine the multiscale information, our method focuses on exploiting the correlation of the multiscale patches based on their local energy. We test our proposed method on synthetic and real images, and the results demonstrate that our approach is practical and highly robust to noise.      
### 33.A Cyber-Physical Perspective to Pinning-Decision for Distributed Multi-Agent Control in Microgrid against Stochastic Communication Disruptions  [ :arrow_down: ](https://arxiv.org/pdf/2103.05824.pdf)
>  In this study, we propose a decision-making strategy for pinning-based distributed multi-agent (PDMA) automatic generation control (AGC) in islanded microgrids against stochastic communication disruptions. The target microgrid is construed as a cyber-physical system, wherein the physical microgrid is modeled as an inverter-interfaced autonomous grid with detailed system dynamic formulation, and the communication network topology is regarded as a cyber-system independent of its physical connection. The primal goal of the proposed method is to decide the minimum number of generators to be pinned and their identities amongst all distributed generators (DGs). The pinningdecisions are made based on complex network theories using the genetic algorithm (GA), for the purpose of synchronizing and regulating the frequencies and voltages of all generator busbars in a PDMA control structure, i.e., without resorting to a central AGC agent. Thereafter, the mapping of cyber-system topology and the pinning decision is constructed using deeplearning (DL) technique, so that the pinning-decision can be made nearly instantly upon detecting a new cyber-system topology after stochastic communication disruptions. The proposed decision-making approach is verified using a 10-generator, 38-bus microgrid through time-domain simulation for transient stability analysis.      
### 34.Joint Active and Passive Beam Training for IRS-Assisted Millimeter Wave Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.05812.pdf)
>  Intelligent reflecting surface (IRS) has emerged as a competitive solution to address blockage issues in millimeter wave (mmWave) and Terahertz (THz) communications due to its capability of reshaping wireless transmission environments. Nevertheless, obtaining the channel state information of IRS-assisted systems is quite challenging because of the passive characteristics of the IRS. In this paper, we consider the problem of beam training/alignment for IRS-assisted downlink mmWave/THz systems, where a multi-antenna base station (BS) with a hybrid structure serves a single-antenna user aided by IRS. By exploiting the inherent sparse structure of the BS-IRS-user cascade channel, the beam training problem is formulated as a joint sparse sensing and phaseless estimation problem, which involves devising a sparse sensing matrix and developing an efficient estimation algorithm to identify the best beam alignment from compressive phaseless measurements. Theoretical analysis reveals that the proposed method can identify the best alignment with only a modest amount of training overhead. Simulation results show that, for both line-of-sight (LOS) and NLOS scenarios, the proposed method obtains a significant performance improvement over existing state-of-art methods. Notably, it can achieve performance close to that of the exhaustive beam search scheme, while reducing the training overhead by 95%.      
### 35.Introduction to Brain and Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.05772.pdf)
>  This article is based on the first chapter of book Chung (2013), where brain and medical images are introduced. The most widely used brain imaging modalities are magnetic resonance images (MRI), functional-MRI (fMRI) and diffusion tensor images (DTI). A brief introduction to each imaging modality is explained. Further, we explain what kind of curve, volume and surface data that can be extracted from each modality.      
### 36.Content-Preserving Unpaired Translation from Simulated to Realistic Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.05745.pdf)
>  Interactive simulation of ultrasound imaging greatly facilitates sonography training. Although ray-tracing based methods have shown promising results, obtaining realistic images requires substantial modeling effort and manual parameter tuning. In addition, current techniques still result in a significant appearance gap between simulated images and real clinical scans. In this work we introduce a novel image translation framework to bridge this appearance gap, while preserving the anatomical layout of the simulated scenes. We achieve this goal by leveraging both simulated images with semantic segmentations and unpaired in-vivo ultrasound scans. Our framework is based on recent contrastive unpaired translation techniques and we propose a regularization approach by learning an auxiliary segmentation-to-real image translation task, which encourages the disentanglement of content and style. In addition, we extend the generator to be class-conditional, which enables the incorporation of additional losses, in particular a cyclic consistency loss, to further improve the translation quality. Qualitative and quantitative comparisons against state-of-the-art unpaired translation methods demonstrate the superiority of our proposed framework.      
### 37.Optimized Power-Balanced Hybrid Phase-Coded Optics and Inverse Imaging for Achromatic EDoF  [ :arrow_down: ](https://arxiv.org/pdf/2103.05720.pdf)
>  The power-balanced hybrid optical imaging system is a special design of a computational camera, introduced in this paper, with image formation by a refractive lens and Multilevel Phase Mask (MPM) as a diffractive optical element (DoE). This system provides a long focal depth and low chromatic aberrations thanks to MPM, and a high energy light concentration due to the refractive lens. This paper additionally introduces the concept of a optimal power balance between lens and MPM for achromatic extended-depth-of-field (EDoF) imaging. To optimize this power-balance as well as to optimize MPM using Neural Network techniques, we build a fully-differentiable image formation model for joint optimization of optical and imaging parameters for the designed computational camera. Additionally, we determine a Wiener-like inverse imaging optimal optical transfer function (OTF) to reconstruct a sharp image from the defocused observation. We numerically and experimentally compare the designed system with its counterparts, lensless and just-lens optical systems, for the visible wavelength interval (400-700) nm and the EDoF range (0.5-1000) m. The attained results demonstrate that the proposed system equipped with the optimal OTF overcomes its lensless and just-lens counterparts (even when they are used with optimized OTFs) in terms of reconstruction quality for off-focus distances.      
### 38.Secrecy Outage Probability of Cognitive Small-Cell Network with Unreliable Backhaul Connections  [ :arrow_down: ](https://arxiv.org/pdf/2103.05717.pdf)
>  In this paper, we investigate the secrecy performance of underlay cognitive small-cell radio network with unreliable backhaul connections. The secondary cognitive small-cell transmitters are connected to macro base station by wireless backhaul links. The small-cell network is sharing the same spectrum with the primary network ensuring that a desired outage probability constraint in the primary network is always satisfied. We propose an optimal transmitter selection (OTS) scheme for small-cell network to transfer information to the destination. The closed-form expression of secrecy outage probability are derived. Our result shows that increasing the primary transmitter's transmit power and the number of small-cell transmitter can improve the system performance. The backhaul reliability of secondary and the desired outage probability of the primary also have significant impact on the system.      
### 39.Incremental Relaying for Power Line Communication: Performance Analysis and Power Allocation  [ :arrow_down: ](https://arxiv.org/pdf/2103.05711.pdf)
>  In this paper, incremental decode-and-forward (IDF) and incremental selective decode-and-forward (ISDF) relaying are proposed to improve the spectral efficiency of power line communication. Contrary to the traditional decode-and-forward (DF) relaying, IDF and ISDF strategies utilize the relay only if the direct link ceases to attain a certain information rate, thereby improving the spectral efficiency. The path gain through the power line is assumed to be log-normally distributed with high distance-dependent attenuation and the additive noise is from a Bernoulli-Gaussian process. Closed-form expressions for the outage probability, and approximate closed-form expressions for the end-to-end average channel capacity and the average bit error rate for binary phase-shift keying are derived. Furthermore, a closed-form expression for the fraction of times the relay is in use is derived as a measure of the spectral efficiency. Comparative analysis of IDF and ISDF with traditional DF relaying is presented. It is shown that IDF is a specific case of ISDF and can obtain optimal spectral efficiency without compromising the outage performance. By employing power allocation to minimize the outage probability, it is realized that the power should be allocated in accordance with the inter-node distances and channel parameters.      
### 40.Resource-Aware Stochastic Self-Triggered Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2103.05681.pdf)
>  This paper considers the control of uncertain systems that are operated under limited resource factors, such as battery life or hardware longevity. We consider here resource-aware self-triggered control techniques that schedule system operation non-uniformly in time in order to balance performance against resource consumption. <br>When running in an uncertain environment, unknown disturbances may deteriorate system performance by acting adversarially against the planned event triggering schedule. In this work, we propose a resource-aware stochastic predictive control scheme to tackle this challenge, where a novel zero-order hold feedback control scheme is proposed to accommodate a time-inhomogeneous predictive control update.      
### 41.Automatic Speaker Independent Dysarthric Speech Intelligibility Assessment System  [ :arrow_down: ](https://arxiv.org/pdf/2103.06157.pdf)
>  Dysarthria is a condition which hampers the ability of an individual to control the muscles that play a major role in speech delivery. The loss of fine control over muscles that assist the movement of lips, vocal chords, tongue and diaphragm results in abnormal speech delivery. One can assess the severity level of dysarthria by analyzing the intelligibility of speech spoken by an individual. Continuous intelligibility assessment helps speech language pathologists not only study the impact of medication but also allows them to plan personalized therapy. It helps the clinicians immensely if the intelligibility assessment system is reliable, automatic, simple for (a) patients to undergo and (b) clinicians to interpret. Lack of availability of dysarthric data has resulted in development of speaker dependent automatic intelligibility assessment systems which requires patients to speak a large number of utterances. In this paper, we propose (a) a cost minimization procedure to select an optimal (small) number of utterances that need to be spoken by the dysarthric patient, (b) four different speaker independent intelligibility assessment systems which require the patient to speak a small number of words, and (c) the assessment score is close to the perceptual score that the Speech Language Pathologist (SLP) can relate to. The need for small number of utterances to be spoken by the patient and the score being relatable to the SLP benefits both the dysarthric patient and the clinician from usability perspective.      
### 42.Learning to Generate Music With Sentiment  [ :arrow_down: ](https://arxiv.org/pdf/2103.06125.pdf)
>  Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy. A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.      
### 43.Variable-rate discrete representation learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.06089.pdf)
>  Semantically meaningful information content in perceptual signals is usually unevenly distributed. In speech signals for example, there are often many silences, and the speed of pronunciation can vary considerably. In this work, we propose slow autoencoders (SlowAEs) for unsupervised learning of high-level variable-rate discrete representations of sequences, and apply them to speech. We show that the resulting event-based representations automatically grow or shrink depending on the density of salient information in the input signals, while still allowing for faithful signal reconstruction. We develop run-length Transformers (RLTs) for event-based representation modelling and use them to construct language models in the speech domain, which are able to generate grammatical and semantically coherent utterances and continuations.      
### 44.Multi-Objective Resource Allocation for IRS-Aided SWIPT  [ :arrow_down: ](https://arxiv.org/pdf/2103.06067.pdf)
>  In this letter, we study the resource allocation for a multiuser intelligent reflecting surface (IRS)-aided simultaneous wireless information and power transfer (SWIPT) system. Specifically, a multi-antenna base station (BS) transmits energy and information signals simultaneously to multiple energy harvesting receivers (EHRs) and information decoding receivers (IDRs) assisted by an IRS. Under this setup, we introduce a multi-objective optimization (MOOP) framework to investigate the fundamental trade-off between the data sum-rate maximization and the total harvested energy maximization, by jointly optimizing the energy/information beamforming vectors at the BS and the phase shifts at the IRS. This MOOP problem is first converted to a single-objective optimization problem (SOOP) via the $\epsilon$-constraint method and then solved by majorization minimization (MM) and inner approximation (IA) techniques. Simulation results unveil a non-trivial trade-off between the considered competing objectives, as well as the superior performance of the proposed scheme as compared to various baseline schemes.      
### 45.Search Disaster Victims using Sound Source Localization  [ :arrow_down: ](https://arxiv.org/pdf/2103.06049.pdf)
>  Sound Source Localization (SSL) are used to estimate the position of sound sources. Various methods have been used for detecting sound and its localization. This paper presents a system for stationary sound source localization by cubical microphone array consisting of eight microphones placed on four vertical adjacent faces which is mounted on three wheel omni-directional drive for the inspection and monitoring of the disaster victims in disaster areas. The proposed method localizes sound source on a 3D space by grid search method using Generalized Cross Correlation Phase Transform (GCC-PHAT) which is robust when operating in real life scenario where there is lack of visibility. The computed azimuth and elevation angle of victimized human voice are fed to embedded omni-directional drive system which navigates the vehicle automatically towards the stationary sound source.      
### 46.Exploring Blockchain for The Coordination of Distributed Energy Resources  [ :arrow_down: ](https://arxiv.org/pdf/2103.06046.pdf)
>  The fast growth of distributed energy resources (DERs), such as distributed renewables (e.g., rooftop PV panels), energy storage systems, electric vehicles, and controllable appliances, drives the power system toward a decentralized system with bidirectional power flow. The coordination of DERs through an aggregator, such as a utility, system operator, or a third-party coordinator, emerges as a promising paradigm. However, it is not well understood how to enable trust between the aggregator and DERs to integrate DERs efficiently. In this paper, we develop a trustable and distributed coordination system for DERs using blockchain technology. We model various DERs and formulate a cost minimization problem for DERs to optimize their energy trading, scheduling, and demand response. We use the alternating direction method of multipliers (ADMM) to solve the problem in a distributed fashion. To implement the distributed algorithm in a trustable way, we design a smart contract to update multipliers and communicate with DERs in a blockchain network. We validate our design by experiments using real-world data, and the simulation results demonstrate the effectiveness of our algorithm.      
### 47.Quantum Algorithms in Cybernetics  [ :arrow_down: ](https://arxiv.org/pdf/2103.05952.pdf)
>  A new method for simulation of a binary homogeneous Markov process using a quantum computer was proposed. This new method allows using the distinguished properties of the quantum mechanical systems -- superposition, entanglement and probability calculations. Implementation of an algorithm based on this method requires the creation of a new quantum logic gate, which creates entangled state between two qubits. This is a two-qubit logic gate and it must perform a predefined rotation over the X-axis for the qubit that acts as a target, where the rotation accurately represents the transient probabilities for a given Markov process. This gate fires only when the control qubit is in state &lt;0|. It is necessary to develop an algorithm, which uses the distribution for the transient probabilities of the process in a simple and intuitive way and then transform those into X-axis offsets. The creation of a quantum controlled n-th root of X gate using only the existing basic quantum logic gates at the available cloud platforms is possible, although the hardware devices are still too noisy, which results in a significant measurement error increase. The IBM's Yorktown 'bow-tie' back-end performs quite better than the 5-qubit T-shaped and the 14-qubit Melbourne quantum processors in terms of quantum fidelity. The simulation of the binary homogeneous Markov process on a real quantum processor gives best results on the Vigo and Yorktown (both 5-qubit) back-ends with Hellinger fidelity of near 0.82. The choice of the right quantum circuit, based on the available hardware (topology, size, timing properties), would be the approach for maximizing the fidelity.      
### 48.Deep Convolutional Sparse Coding Network for Pansharpening with Guidance of Side Information  [ :arrow_down: ](https://arxiv.org/pdf/2103.05946.pdf)
>  Pansharpening is a fundamental issue in remote sensing field. This paper proposes a side information partially guided convolutional sparse coding (SCSC) model for pansharpening. The key idea is to split the low resolution multispectral image into a panchromatic image related feature map and a panchromatic image irrelated feature map, where the former one is regularized by the side information from panchromatic images. With the principle of algorithm unrolling techniques, the proposed model is generalized as a deep neural network, called as SCSC pansharpening neural network (SCSC-PNN). Compared with 13 classic and state-of-the-art methods on three satellites, the numerical experiments show that SCSC-PNN is superior to others. The codes are available at <a class="link-external link-https" href="https://github.com/xsxjtu/SCSC-PNN" rel="external noopener nofollow">this https URL</a>.      
### 49.RMP2: A Structured Composable Policy Class for Robot Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.05922.pdf)
>  We consider the problem of learning motion policies for acceleration-based robotics systems with a structured policy class specified by RMPflow. RMPflow is a multi-task control framework that has been successfully applied in many robotics problems. Using RMPflow as a structured policy class in learning has several benefits, such as sufficient expressiveness, the flexibility to inject different levels of prior knowledge as well as the ability to transfer policies between robots. However, implementing a system for end-to-end learning RMPflow policies faces several computational challenges. In this work, we re-examine the message passing algorithm of RMPflow and propose a more efficient alternate algorithm, called RMP2, that uses modern automatic differentiation tools (such as TensorFlow and PyTorch) to compute RMPflow policies. Our new design retains the strengths of RMPflow while bringing in advantages from automatic differentiation, including 1) easy programming interfaces to designing complex transformations; 2) support of general directed acyclic graph (DAG) transformation structures; 3) end-to-end differentiability for policy learning; 4) improved computational efficiency. Because of these features, RMP2 can be treated as a structured policy class for efficient robot learning which is suitable encoding domain knowledge. Our experiments show that using structured policy class given by RMP2 can improve policy performance and safety in reinforcement learning tasks for goal reaching in cluttered space.      
### 50.HVAC Scheduling under Data Uncertainties: A Distributionally Robust Approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.05850.pdf)
>  The heating, ventilation and air condition (HVAC) system consumes the most energy in commercial buildings, consisting over 60% of total energy usage in the U.S. Flexible HVAC system setpoint scheduling could potentially save building energy costs. This paper first studies deterministic optimization, robust optimization, and stochastic optimization to minimize the daily operation cost with constraints of indoor air temperature comfort and mechanic operating requirement. Considering the uncertainties from ambient temperature, a Wasserstein metric-based distributionally robust optimization (DRO) method is proposed to enhance the robustness of the optimal schedule against the uncertainty of probabilistic prediction errors. The schedule is optimized under the worst-case distribution within an ambiguity set defined by the Wasserstein metric. The proposed DRO method is initially formulated as a two-stage problem and then reformulated into a tractable mixed-integer linear programming (MILP) form. The paper evaluates the feasibility and optimality of the optimized schedules for a real commercial building. The numerical results indicate that the costs of the proposed DRO method are up to 6.6% lower compared with conventional techniques of optimization under uncertainties. They also provide granular risk-benefit options for decision making in demand response programs.      
### 51.Analyzing Human Models that Adapt Online  [ :arrow_down: ](https://arxiv.org/pdf/2103.05746.pdf)
>  Predictive human models often need to adapt their parameters online from human data. This raises previously ignored safety-related questions for robots relying on these models such as what the model could learn online and how quickly could it learn it. For instance, when will the robot have a confident estimate in a nearby human's goal? Or, what parameter initializations guarantee that the robot can learn the human's preferences in a finite number of observations? To answer such analysis questions, our key idea is to model the robot's learning algorithm as a dynamical system where the state is the current model parameter estimate and the control is the human data the robot observes. This enables us to leverage tools from reachability analysis and optimal control to compute the set of hypotheses the robot could learn in finite time, as well as the worst and best-case time it takes to learn them. We demonstrate the utility of our analysis tool in four human-robot domains, including autonomous driving and indoor navigation.      
### 52.Spheroidal Ambisonics: a Spatial Audio Framework Using Spheroidal Bases  [ :arrow_down: ](https://arxiv.org/pdf/2103.05719.pdf)
>  Ambisonics is an established framework to capture, process, and reproduce spatial sound fields based on its spherical harmonics representation. We propose a generalization of conventional spherical ambisonics to the spheroidal coordinate system and spheroidal microphone arrays, which represent sound fields by means of spheroidal wave functions. This framework is referred to as spheroidal ambisonics and a formulation for the case of prolate spheroidal coordinates is presented. Spheroidal ambisonics allows analytical encoding of sound fields using spheroidal microphone arrays. In addition, an analytical conversion formula from spheroidal ambisonics to spherical ambisonics is derived in order to ensure compatibility with the existing ecosystem of spherical ambisonics. Numerical experiments are performed to verify spheroidal ambisonic encoding and transcoding when used for spatial sound field recording. It is found that the sound field reconstructed from the transcoded coefficients has a zone of accurate reconstruction which is prolonged towards the long axis of a prolate spheroidal microphone array.      
### 53.Capturing Omni-Range Context for Omnidirectional Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2103.05687.pdf)
>  Convolutional Networks (ConvNets) excel at semantic segmentation and have become a vital component for perception in autonomous driving. Enabling an all-encompassing view of street-scenes, omnidirectional cameras present themselves as a perfect fit in such systems. Most segmentation models for parsing urban environments operate on common, narrow Field of View (FoV) images. Transferring these models from the domain they were designed for to 360-degree perception, their performance drops dramatically, e.g., by an absolute 30.0% (mIoU) on established test-beds. To bridge the gap in terms of FoV and structural distribution between the imaging domains, we introduce Efficient Concurrent Attention Networks (ECANets), directly capturing the inherent long-range dependencies in omnidirectional imagery. In addition to the learned attention-based contextual priors that can stretch across 360-degree images, we upgrade model training by leveraging multi-source and omni-supervised learning, taking advantage of both: Densely labeled and unlabeled data originating from multiple datasets. To foster progress in panoramic image segmentation, we put forward and extensively evaluate models on Wild PAnoramic Semantic Segmentation (WildPASS), a dataset designed to capture diverse scenes from all around the globe. Our novel model, training regimen and multi-source prediction fusion elevate the performance (mIoU) to new state-of-the-art results on the public PASS (60.2%) and the fresh WildPASS (69.0%) benchmarks.      
### 54.Formulating Intuitive Stack-of-Tasks with Visuo-Tactile Perception for Collaborative Human-Robot Fine Manipulation  [ :arrow_down: ](https://arxiv.org/pdf/2103.05676.pdf)
>  Enabling robots to work in close proximity with humans necessitates to employ not only multi-sensory information for coordinated and autonomous interactions but also a control framework that ensures adaptive and flexible collaborative behavior. Such a control framework needs to integrate accuracy and repeatability of robots with cognitive ability and adaptability of humans for co-manipulation. In this regard, an intuitive stack of tasks (iSOT) formulation is proposed, that defines the robots actions based on human ergonomics and task progress. The framework is augmented with visuo-tactile perception for flexible interaction and autonomous adaption. The visual information using depth cameras, monitors and estimates the object pose and human arm gesture while the tactile feedback provides exploration skills for maintaining the desired contact to avoid slippage. Experiments conducted on robot system with human partnership for assembly and disassembly tasks confirm the effectiveness and usability of proposed framework.      
