# ArXiv eess --Thu, 18 Mar 2021
### 1.Fault Current Limiter Dynamic Voltage Restorer (FCL-DVR) with Reduced Number of Components  [ :arrow_down: ](https://arxiv.org/pdf/2103.09793.pdf)
>  In this paper, a new dual function fault current limiter-dynamic voltage restorer (FCL-DVR) topology is proposed. The proposed structure, in addition to performing routine FCL tasks, can be used to improve the voltage quality of point of common coupling (PCC). A salient feature of this FCL-DVR is its reduced number of semiconductor switches and gate drive and control circuit components. Perhaps, variety structure of FCL-DVR have been proposed but most distinctive feature of proposed structure is lower power loss. The operation modes and the control strategy of the FCL have been presented and studied. In addition, the proposed structure has been compared with other structures to prove the efficiency of proposed structure. The simulation results as well as experimental outcomes from a laboratory scaled-down prototype are provided, which prove the efficiency and feasibility of the proposed structure.      
### 2.Integrated 3C in NOMA-enabled Remote-E-Health Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.09749.pdf)
>  A novel framework is proposed to integrate communication, control and computing (3C) into the fifth-generation and beyond (5GB) wireless networks for satisfying the ultra-reliable low-latency connectivity requirements of remote-e-Health systems. Non-orthogonal multiple access (NOMA) enabled 5GB network architecture is envisioned, while the benefits of bringing to the remote-e-Health systems are demonstrated. Firstly, the application of NOMA into e-Health systems is presented. To elaborate further, a unified NOMA framework for e-Health is proposed. As a further advance, NOMA-enabled autonomous robotics (NOMA-ARs) systems and NOMA-enabled edge intelligence (NOMA-EI) towards remote-e-Health are discussed, respectively. Furthermore, a pair of case studies are provided to show the great performance enhancement with the use of NOMA technique in typical application scenarios of 5GB in remote-e-Health systems. Finally, potential research challenges and opportunities are discussed.      
### 3.Deep Contextual Bandits for Fast Neighbor-Aided Initial Access in mmWave Cell-Free Networks  [ :arrow_down: ](https://arxiv.org/pdf/2103.09694.pdf)
>  Access points (APs) in millimeter-wave (mmWave) and sub-THz-based user-centric (UC) networks will have sleep mode functionality. As a result of this, it becomes challenging to solve the initial access (IA) problem when the sleeping APs are activated to start serving users. In this paper, a novel deep contextual bandit (DCB) learning method is proposed to provide instant IA using information from the neighboring active APs. In the proposed approach, beam selection information from the neighboring active APs is used as an input to neural networks that act as a function approximator for the bandit algorithm. Simulations are carried out with realistic channel models generated using the Wireless Insight ray-tracing tool. The results show that the system can respond to dynamic throughput demands with negligible latency compared to the standard baseline 5G IA scheme. The proposed fast beam selection scheme can enable the network to use energy-saving sleep modes without compromising the quality of service due to inefficient IA      
### 4.A New Parameterized Family of Stochastic Particle Flow Filters  [ :arrow_down: ](https://arxiv.org/pdf/2103.09676.pdf)
>  We derive a parameterized family of stochastic particle flows driven by a nonzero diffusion process for nonlinear filtering, Bayesian inference, or target tracking. This new family of stochastic flows takes the form of a linear combination of prior knowledge and measurement likelihood information. It is shown that several particle flows existing in the literature are special cases of this family. We prove that the particle flows are unbiased under the assumption of linear measurement and Gaussian distributions, and examine the consistency of estimates constructed from the stochastic flows. We further establish several finite time stability concepts for this new family of stochastic particle flows.      
### 5.Pseudo Supervised Solar Panel Mapping based on Deep Convolutional Networks with Label Correction Strategy in Aerial Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.09659.pdf)
>  Solar panel mapping has gained a rising interest in renewable energy field with the aid of remote sensing imagery. Significant previous work is based on fully supervised learning with classical classifiers or convolutional neural networks (CNNs), which often require manual annotations of pixel-wise ground-truth to provide accurate supervision. Weakly supervised methods can accept image-wise annotations which can help reduce the cost for pixel-level labelling. Inevitable performance gap, however, exists between weakly and fully supervised methods in mapping accuracy. To address this problem, we propose a pseudo supervised deep convolutional network with label correction strategy (PS-CNNLC) for solar panels mapping. It combines the benefits of both weak and strong supervision to provide accurate solar panel extraction. First, a convolutional neural network is trained with positive and negative samples with image-level labels. It is then used to automatically identify more positive samples from randomly selected unlabeled images. The feature maps of the positive samples are further processed by gradient-weighted class activation mapping to generate initial mapping results, which are taken as initial pseudo labels as they are generally coarse and incomplete. A progressive label correction strategy is designed to refine the initial pseudo labels and train an end-to-end target mapping network iteratively, thereby improving the model reliability. Comprehensive evaluations and ablation study conducted validate the superiority of the proposed PS-CNNLC.      
### 6.Detecting Anomalous Swarming Agents with Graph Signal Processing  [ :arrow_down: ](https://arxiv.org/pdf/2103.09629.pdf)
>  Collective motion among biological organisms such as insects, fish, and birds has motivated considerable interest not only in biology but also in distributed robotic systems. In a robotic or biological swarm, anomalous agents (whether malfunctioning or nefarious) behave differently than the normal agents and attempt to hide in the "chaos" of the swarm. By defining a graph structure between agents in a swarm, we can treat the agents' properties as a graph signal and use tools from the field of graph signal processing to understand local and global swarm properties. Here, we leverage this idea to show that anomalous agents can be effectively detected using their impacts on the graph Fourier structure of the swarm.      
### 7.Regularized Estimation of Kronecker-Structured Covariance Matrix With Applications to Polarization Space-Time Adaptive Processing  [ :arrow_down: ](https://arxiv.org/pdf/2103.09628.pdf)
>  This paper investigates regularized estimation of Kronecker-structured covariance matrices (CM) for complex elliptically symmetric (CES) data. To obtain a well-conditioned estimate of the CM, we add penalty terms of Kullback-Leibler divergence to the negative log-likelihood function of the associated complex angular Gaussian (CAG) distribution. This is shown to be equivalent to regularizing Tyler's fixed-point equations by shrinkage. A sufficient condition that the solution exists is discussed. An iterative algorithm is applied to solve the resulting fixed-point iterations and its convergence is proved. In order to solve the critical problem of tuning the shrinkage factors, we then introduce three methods by exploiting oracle approximating shrinkage (OAS) and cross-validation (CV). The proposed algorithms are applied to the CM estimation for polarization-space-time adaptive processing (PSTAP) in the context of heterogeneous clutter, where the clutter can be modeled using CES distributions with a Kronecker product-structured CM. When the training samples are limited, the proposed estimator, referred to as the robust shrinkage Kronecker estimator (RSKE), has better performance compared with several existing methods. Simulations are conducted for validating the proposed estimator and demonstrating its high performance.      
### 8.Color image segmentation based on a convex K-means approach  [ :arrow_down: ](https://arxiv.org/pdf/2103.09565.pdf)
>  Image segmentation is a fundamental and challenging task in image processing and computer vision. The color image segmentation is attracting more attention due to the color image provides more information than the gray image. In this paper, we propose a variational model based on a convex K-means approach to segment color images. The proposed variational method uses a combination of $l_1$ and $l_2$ regularizers to maintain edge information of objects in images while overcoming the staircase effect. Meanwhile, our one-stage strategy is an improved version based on the smoothing and thresholding strategy, which contributes to improving the accuracy of segmentation. The proposed method performs the following steps. First, we specify the color set which can be determined by human or the K-means method. Second, we use a variational model to obtain the most appropriate color for each pixel from the color set via convex relaxation and lifting. The Chambolle-Pock algorithm and simplex projection are applied to solve the variational model effectively. Experimental results and comparison analysis demonstrate the effectiveness and robustness of our method.      
### 9.Big Plastic Masses Detection using Sentinel 2 Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.09560.pdf)
>  This communication describes a preliminary research on detection of big masses of plastic (marine litter) on the oceans and seas using EO (Earth Observation) satellite systems. Free images from the Sentinel 2 (Copernicus Project) platform are used. To develop a plastic recognizer, we start with an image where we can find a big accumulation of "nonfloating" plastic: AlmerÃ­a greenhouses. We made a test using remote sensing differential indexes, but we got much better results using all available wavelengths (thirteen frequency bands) and applying Neural Networks to that feature vector.      
### 10.A New Method for Features Normalization in Motor Imagery Few-Shot Learning using Resting-State  [ :arrow_down: ](https://arxiv.org/pdf/2103.09507.pdf)
>  Brain-computer interface (BCI) systems are usually designed specifically for each subject based on motor imagery. Therefore, the usability of these networks has become a significant challenge. The network has to be designed separately for each user, which is time-consuming for the user. Therefore, this study proposes a method by which the calibration time is significantly reduced while the classification accuracy is increased. In this method, we calibrated the features extracted from the motor imagery task by dividing the features extracted from the resting-state into both open-eye and closed-eye modes and the state in which the subject moves his eyes. The best classification accuracy was obtained using the SVM classifier using the resting-state signal in the open eye, which increased by 3.64% to 74.04%. In this paper, we also investigated the effect of recording time of the resting-state signal and the impact of eye state on the classification accuracy.      
### 11.STYLER: Style Modeling with Rapidity and Robustness via SpeechDecomposition for Expressive and Controllable Neural Text to Speech  [ :arrow_down: ](https://arxiv.org/pdf/2103.09474.pdf)
>  Previous works on expressive text-to-speech (TTS) have a limitation on robustness and speed when training and inferring. Such drawbacks mostly come from autoregressive decoding, which makes the succeeding step vulnerable to preceding error. To overcome this weakness, we propose STYLER, a novel expressive text-to-speech model with parallelized architecture. Expelling autoregressive decoding and introducing speech decomposition for encoding enables speech synthesis more robust even with high style transfer performance. Moreover, our novel noise modeling approach from audio using domain adversarial training and Residual Decoding enabled style transfer without transferring noise. Our experiments prove the naturalness and expressiveness of our model from comparison with other parallel TTS models. Together we investigate our model's robustness and speed by comparison with the expressive TTS model with autoregressive decoding.      
### 12.Improving Zero-shot Voice Style Transfer via Disentangled Representation Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.09420.pdf)
>  Voice style transfer, also called voice conversion, seeks to modify one speaker's voice to generate speech as if it came from another (target) speaker. Previous works have made progress on voice conversion with parallel training data and pre-known speakers. However, zero-shot voice style transfer, which learns from non-parallel data and generates voices for previously unseen speakers, remains a challenging problem. We propose a novel zero-shot voice transfer method via disentangled representation learning. The proposed method first encodes speaker-related style and voice content of each input voice into separated low-dimensional embedding spaces, and then transfers to a new voice by combining the source content embedding and target style embedding through a decoder. With information-theoretic guidance, the style and content embedding spaces are representative and (ideally) independent of each other. On real-world VCTK datasets, our method outperforms other baselines and obtains state-of-the-art results in terms of transfer accuracy and voice naturalness for voice style transfer experiments under both many-to-many and zero-shot setups.      
### 13.Model-Free Design of Stochastic LQR Controller from Reinforcement Learning and Primal-Dual Optimization Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2103.09407.pdf)
>  To further understand the underlying mechanism of various reinforcement learning (RL) algorithms and also to better use the optimization theory to make further progress in RL, many researchers begin to revisit the linear-quadratic regulator (LQR) problem, whose setting is simple and yet captures the characteristics of RL. Inspired by this, this work is concerned with the model-free design of stochastic LQR controller for linear systems subject to Gaussian noises, from the perspective of both RL and primal-dual optimization. From the RL perspective, we first develop a new model-free off-policy policy iteration (MF-OPPI) algorithm, in which the sampled data is repeatedly used for updating the policy to alleviate the data-hungry problem to some extent. We then provide a rigorous analysis for algorithm convergence by showing that the involved iterations are equivalent to the iterations in the classical policy iteration (PI) algorithm. From the perspective of optimization, we first reformulate the stochastic LQR problem at hand as a constrained non-convex optimization problem, which is shown to have strong duality. Then, to solve this non-convex optimization problem, we propose a model-based primal-dual (MB-PD) algorithm based on the properties of the resulting Karush-Kuhn-Tucker (KKT) conditions. We also give a model-free implementation for the MB-PD algorithm by solving a transformed dual feasibility condition. More importantly, we show that the dual and primal update steps in the MB-PD algorithm can be interpreted as the policy evaluation and policy improvement steps in the PI algorithm, respectively. Finally, we provide one simulation example to show the performance of the proposed algorithms.      
### 14.Collapsible Linear Blocks for Super-Efficient Super Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2103.09404.pdf)
>  With the advent of smart devices that support 4K and 8K resolution, Single Image Super Resolution (SISR) has become an important computer vision problem. However, most super resolution deep networks are computationally very expensive. In this paper, we propose SESR, a new class of Super-Efficient Super Resolution networks that significantly improve image quality and reduce computational complexity. Detailed experiments across six benchmark datasets demonstrate that SESR achieves similar or better image quality than state-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate (MAC) operations. As a result, SESR can be used on constrained hardware to perform x2 (1080p to 4K) and x4 SISR (1080p to 8K). Towards this, we simulate hardware performance numbers for a commercial mobile Neural Processing Unit (NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the challenges faced by super resolution on AI accelerators and demonstrate that SESR is significantly faster than existing models. Overall, SESR establishes a new Pareto frontier on the quality (PSNR)-computation relationship for the super resolution task.      
### 15.A New TOA Localization and Synchronization System with Virtually Synchronized Periodic Asymmetric Ranging Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.09399.pdf)
>  In this article, we design a new time-of-arrival (TOA) system for simultaneous user device (UD) localization and synchronization with a periodic asymmetric ranging network, namely PARN. The PARN includes one primary anchor node (PAN) transmitting and receiving signals, and many secondary ANs (SAN) only receiving signals. All the UDs can transmit and receive signals. The PAN periodically transmits sync signal and the UD transmits response signal after reception of the sync signal. Using TOA measurements from the periodic sync signal at SANs, we develop a Kalman filtering method to virtually synchronize ANs with high accuracy estimation of clock parameters. Employing the virtual synchronization, and TOA measurements from the response signal and sync signal, we then develop a maximum likelihood (ML) approach, namely ML-LAS, to simultaneously localize and synchronize a moving UD. We analyze the UD localization and synchronization error, and derive the Cramer-Rao lower bound (CRLB). Different from existing asymmetric ranging network-based TOA systems, the new PARN i) uses the periodic sync signals at the SAN to exploit the temporal correlated clock information for high accuracy virtual synchronization, and ii) compensates the UD movement and clock drift using various TOA measurements to achieve consistent and simultaneous localization and synchronization performance. Numerical results verify the theoretical analysis that the new system has high accuracy in AN clock offset estimation and simultaneous localization and synchronization for a moving UD. We implement a prototype hardware system and demonstrate the feasibility and superiority of the PARN in real-world applications by experiments.      
### 16.A Comprehensive Optimization Method for Commercial Building Loads with Renewable Generation and Energy Storage from Utility Rate Structure Perspective  [ :arrow_down: ](https://arxiv.org/pdf/2103.09381.pdf)
>  To accommodate the changes in the nature and pattern of electricity consumption with the available resources, utility companies have introduced a variety of rate structures over the years. This paper develops a comprehensive optimization method that addresses the diversity of utility rate structure of a commercial building. It includes a general set of constraints that can be used for any system with a building load, a renewable source, and a battery energy storage system (BESS). A cost function is formulated for each type of rate structure that can be exercised by a utility on a commercial building. A novel algorithm is developed to apply the optimization model and generate the desired optimal outputs by using the appropriate cost function. The results for several building loads and rate structure types were obtained and compared. A sensitivity analysis was done on the optimization model based on the changes in the rates using historical data. The results exhibit that adding BESS is more effective for buildings with lower load factor and CPP rate structures in comparison to the buildings with flat energy rates. Savings from adding renewables such as solar is primarily influenced by energy charges whereas additional benefits from BESS are dominated by demand charges. These results can help a customer with deciding on the different rate structure options and resource planning of their renewable generation and energy storage. Utilities may also benefit from this work by designing a unified rate structure, considering the increasing renewable penetration and BESS deployment in the grid.      
### 17.Accelerating Quantitative Susceptibility Mapping using Compressed Sensing and Deep Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2103.09375.pdf)
>  Quantitative susceptibility mapping (QSM) is an MRI phase-based post-processing method that quantifies tissue magnetic susceptibility distributions. However, QSM acquisitions are relatively slow, even with parallel imaging. Incoherent undersampling and compressed sensing reconstruction techniques have been used to accelerate traditional magnitude-based MRI acquisitions; however, most do not recover the full phase signal due to its non-convex nature. In this study, a learning-based Deep Complex Residual Network (DCRNet) is proposed to recover both the magnitude and phase images from incoherently undersampled data, enabling high acceleration of QSM acquisition. Magnitude, phase, and QSM results from DCRNet were compared with two iterative and one deep learning methods on retrospectively undersampled acquisitions from six healthy volunteers, one intracranial hemorrhage and one multiple sclerosis patients, as well as one prospectively undersampled healthy subject using a 7T scanner. Peak signal to noise ratio (PSNR), structural similarity (SSIM) and region-of-interest susceptibility measurements are reported for numerical comparisons. The proposed DCRNet method substantially reduced artifacts and blurring compared to the other methods and resulted in the highest PSNR and SSIM on the magnitude, phase, local field, and susceptibility maps. It led to 4.0% to 8.8% accuracy improvements in deep grey matter susceptibility than some existing methods, when the acquisition was accelerated four times. The proposed DCRNet also dramatically shortened the reconstruction time by nearly 10 thousand times for each scan, from around 80 hours using conventional approaches to only 30 seconds.      
### 18.Colorectal Cancer Segmentation using Atrous Convolution and Residual Enhanced UNet  [ :arrow_down: ](https://arxiv.org/pdf/2103.09289.pdf)
>  Colorectal cancer is a leading cause of death worldwide. However, early diagnosis dramatically increases the chances of survival, for which it is crucial to identify the tumor in the body. Since its imaging uses high-resolution techniques, annotating the tumor is time-consuming and requires particular expertise. Lately, methods built upon Convolutional Neural Networks(CNNs) have proven to be at par, if not better in many biomedical segmentation tasks. For the task at hand, we propose another CNN-based approach, which uses atrous convolutions and residual connections besides the conventional filters. The training and inference were made using an efficient patch-based approach, which significantly reduced unnecessary computations. The proposed AtResUNet was trained on the DigestPath 2019 Challenge dataset for colorectal cancer segmentation with results having a Dice Coefficient of 0.748.      
### 19.Statistical Characterization of Random Errors Present in Synchrophasor Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2103.09285.pdf)
>  The statistical characterization of the measurement errors of a phasor measurement unit (PMU) is currently receiving considerable interest in the power systems community. This paper focuses on the characteristics of the errors in magnitude and angle measurements introduced only by the PMU device (called random errors in this paper), during ambient conditions, using a high-precision calibrator. The experimental results indicate that the random errors follow a non-Gaussian distribution. They also show that the M-class and P-class PMUs have distinct error characteristics. The results of this analysis will help researchers design algorithms that account for the non-Gaussian nature of the errors in synchrophasor measurements, thereby improving the practical utility of the said-algorithms in addition to building on precedence for using high-precision calibrators to perform accurate error tests.      
### 20.Weakly Supervised Reinforcement Learning for Autonomous Highway Driving via Virtual Safety Cages  [ :arrow_down: ](https://arxiv.org/pdf/2103.09726.pdf)
>  The use of neural networks and reinforcement learning has become increasingly popular in autonomous vehicle control. However, the opaqueness of the resulting control policies presents a significant barrier to deploying neural network-based control in autonomous vehicles. In this paper, we present a reinforcement learning based approach to autonomous vehicle longitudinal control, where the rule-based safety cages provide enhanced safety for the vehicle as well as weak supervision to the reinforcement learning agent. By guiding the agent to meaningful states and actions, this weak supervision improves the convergence during training and enhances the safety of the final trained policy. This rule-based supervisory controller has the further advantage of being fully interpretable, thereby enabling traditional validation and verification approaches to ensure the safety of the vehicle. We compare models with and without safety cages, as well as models with optimal and constrained model parameters, and show that the weak supervision consistently improves the safety of exploration, speed of convergence, and model performance. Additionally, we show that when the model parameters are constrained or sub-optimal, the safety cages can enable a model to learn a safe driving policy even when the model could not be trained to drive through reinforcement learning alone.      
### 21.SeeingGAN: Galactic image deblurring with deep learning for better morphological classification of galaxies  [ :arrow_down: ](https://arxiv.org/pdf/2103.09711.pdf)
>  Classification of galactic morphologies is a crucial task in galactic astronomy, and identifying fine structures of galaxies (e.g., spiral arms, bars, and clumps) is an essential ingredient in such a classification task. However, seeing effects can cause images we obtain to appear blurry, making it difficult for astronomers to derive galaxies' physical properties and, in particular, distant galaxies. Here, we present a method that converts blurred images obtained by the ground-based Subaru Telescope into quasi Hubble Space Telescope (HST) images via machine learning. Using an existing deep learning method called generative adversarial networks (GANs), we can eliminate seeing effects, effectively resulting in an image similar to an image taken by the HST. Using multiple Subaru telescope image and HST telescope image pairs, we demonstrate that our model can augment fine structures present in the blurred images in aid for better and more precise galactic classification. Using our first of its kind machine learning-based deblurring technique on space images, we can obtain up to 18% improvement in terms of CW-SSIM (Complex Wavelet Structural Similarity Index) score when comparing the Subaru-HST pair versus SeeingGAN-HST pair. With this model, we can generate HST-like images from relatively less capable telescopes, making space exploration more accessible to the broader astronomy community. Furthermore, this model can be used not only in professional morphological classification studies of galaxies but in all citizen science for galaxy classifications.      
### 22.Single Underwater Image Restoration by Contrastive Learning  [ :arrow_down: ](https://arxiv.org/pdf/2103.09697.pdf)
>  Underwater image restoration attracts significant attention due to its importance in unveiling the underwater world. This paper elaborates on a novel method that achieves state-of-the-art results for underwater image restoration based on the unsupervised image-to-image translation framework. We design our method by leveraging from contrastive learning and generative adversarial networks to maximize mutual information between raw and restored images. Additionally, we release a large-scale real underwater image dataset to support both paired and unpaired training modules. Extensive experiments with comparisons to recent approaches further demonstrate the superiority of our proposed method.      
### 23.A Robust Tube-Based Smooth-MPC for Robot Manipulator Planning  [ :arrow_down: ](https://arxiv.org/pdf/2103.09693.pdf)
>  Model Predictive Control (MPC) has shown the great performance of target optimization and constraint satisfaction. However, the heavy computation of the Optimal Control Problem (OCP) at each triggering instant brings the serious delay from state sampling to the control signals, which limits the applications of MPC in resource-limited robot manipulator systems over complicated tasks. In this paper, we propose a novel robust tube-based smooth-MPC strategy for nonlinear robot manipulator planning systems with disturbances and constraints. Based on piecewise linearization and state prediction, our control strategy improves the smoothness and optimizes the delay of the control process. By deducing the deviation of the real system states and the nominal system states, we can predict the next real state set at the current instant. And by using this state set as the initial condition, we can solve the next OCP ahead and store the optimal controls based on the nominal system states, which eliminates the delay. Furthermore, we linearize the nonlinear system with a given upper bound of error, reducing the complexity of the OCP and improving the response speed. Based on the theoretical framework of tube MPC, we prove that the control strategy is recursively feasible and closed-loop stable with the constraints and disturbances. Numerical simulations have verified the efficacy of the designed approach compared with the conventional MPC.      
### 24.Fourier Transform of Percoll Gradients Boosts CNN Classification of Hereditary Hemolytic Anemias  [ :arrow_down: ](https://arxiv.org/pdf/2103.09671.pdf)
>  Hereditary hemolytic anemias are genetic disorders that affect the shape and density of red blood cells. Genetic tests currently used to diagnose such anemias are expensive and unavailable in the majority of clinical labs. Here, we propose a method for identifying hereditary hemolytic anemias based on a standard biochemistry method, called Percoll gradient, obtained by centrifuging a patient's blood. Our hybrid approach consists on using spatial data-driven features, extracted with a convolutional neural network and spectral handcrafted features obtained from fast Fourier transform. We compare late and early feature fusion with AlexNet and VGG16 architectures. AlexNet with late fusion of spectral features performs better compared to other approaches. We achieved an average F1-score of 88% on different classes suggesting the possibility of diagnosing of hereditary hemolytic anemias from Percoll gradients. Finally, we utilize Grad-CAM to explore the spatial features used for classification.      
### 25.The Invertible U-Net for Optical-Flow-free Video Interframe Generation  [ :arrow_down: ](https://arxiv.org/pdf/2103.09576.pdf)
>  Video frame interpolation is the task of creating an interface between two adjacent frames along the time axis. So, instead of simply averaging two adjacent frames to create an intermediate image, this operation should maintain semantic continuity with the adjacent frames. Most conventional methods use optical flow, and various tools such as occlusion handling and object smoothing are indispensable. Since the use of these various tools leads to complex problems, we tried to tackle the video interframe generation problem without using problematic optical flow. To enable this, we have tried to use a deep neural network with an invertible structure and developed an invertible U-Net which is a modified normalizing flow. In addition, we propose a learning method with a new consistency loss in the latent space to maintain semantic temporal consistency between frames. The resolution of the generated image is guaranteed to be identical to that of the original images by using an invertible network. Furthermore, as it is not a random image like the ones by generative models, our network guarantees stable outputs without flicker. Through experiments, we confirmed the feasibility of the proposed algorithm and would like to suggest invertible U-Net as a new possibility for baseline in video frame interpolation. This paper is meaningful in that it is the worlds first attempt to use invertible networks instead of optical flows for video interpolation.      
### 26.Prediction-assistant Frame Super-Resolution for Video Streaming  [ :arrow_down: ](https://arxiv.org/pdf/2103.09455.pdf)
>  Video frame transmission delay is critical in real-time applications such as online video gaming, live show, etc. The receiving deadline of a new frame must catch up with the frame rendering time. Otherwise, the system will buffer a while, and the user will encounter a frozen screen, resulting in unsatisfactory user experiences. An effective approach is to transmit frames in lower-quality under poor bandwidth conditions, such as using scalable video coding. In this paper, we propose to enhance video quality using lossy frames in two situations. First, when current frames are too late to receive before rendering deadline (i.e., lost), we propose to use previously received high-resolution images to predict the future frames. Second, when the quality of the currently received frames is low~(i.e., lossy), we propose to use previously received high-resolution frames to enhance the low-quality current ones. For the first case, we propose a small yet effective video frame prediction network. For the second case, we improve the video prediction network to a video enhancement network to associate current frames as well as previous frames to restore high-quality images. Extensive experimental results demonstrate that our method performs favorably against state-of-the-art algorithms in the lossy video streaming environment.      
### 27.Mechanical principles of dynamic terrestrial self-righting using wings  [ :arrow_down: ](https://arxiv.org/pdf/2103.09450.pdf)
>  Terrestrial animals and robots are susceptible to flipping-over during rapid locomotion in complex terrains. However, small robots are less capable of self-righting from an upside-down orientation compared to small animals like insects. Inspired by the winged discoid cockroach, we designed a new robot that opens its wings to self-right by pushing against the ground. We used this robot to systematically test how self-righting performance depends on wing opening magnitude, speed, and asymmetry, and modeled how kinematic and energetic requirements depend on wing shape and body/wing mass distribution. We discovered that the robot self-rights dynamically using kinetic energy to overcome potential energy barriers, that larger and faster symmetric wing opening increases self-righting performance, and that opening wings asymmetrically increases righting probability when wing opening is small. Our results suggested that the discoid cockroach's winged self-righting is a dynamic maneuver. While the thin, lightweight wings of the discoid cockroach and our robot are energetically sub-optimal for self-righting compared to tall, heavy ones, their ability to open wings saves them substantial energy compared to if they had static shells. Analogous to biological exaptations, our study provided a proof-of-concept for terrestrial robots to use existing morphology in novel ways to overcome new locomotor challenges.      
### 28.Hybrid Precoding for mmWave V2X Doubly-Selective Multiuser MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2103.09444.pdf)
>  Millimeter wave (mmWave) is a practical solution to provide high data rate for the vehicle-to-everything (V2X) communications. This enables the future autonomous vehicles to exchange big data with the base stations (BSs) such as the velocity and the location to improve the awareness of the advanced driving assistance system (ADAS). In this context, we consider a single-cell multiuser doubly-selective system wherein the BS simultaneously serves multiple vehicles. To accomplish this requirement, the BS is implemented in hybrid architecture to support multiple spatial streams while the vehicles have analog-only structures. In this work, we develop a low-complexity hybrid precoding algorithm wherein the design of the hybrid precoder at the BS and the analog combiner at the vehicles require small training and feedback overhead. We propose a two-stage hybrid precoding algorithm wherein the first stage designs the analog beamformers as in single user scenario while the second stage designs the multiuser digital precoder at the BS. In the second stage, we derive closed-form digital precoders such as Maximum Ratio Transmission (MRT), Zero-Forcing (ZF) and Minimum Mean Square Error (MMSE) as a first variant while we propose iterative digital precoder as a second variant. The design of the digital precoders for the two variants requires the limited feedback sent from the vehicles to BS. We refer to the random vector quantization (RVQ) and the beamsteering codebooks to quantize the feedbacks for variants I and II, respectively, since the perfect feedback requires long overhead and large training. We evaluate the rate loss incurred by the quantization of the digital and analog codebooks against the perfect channel state information at the transmitter (CSIT).      
### 29.Contrastive Learning of Musical Representations  [ :arrow_down: ](https://arxiv.org/pdf/2103.09410.pdf)
>  While supervised learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations, to form a simple framework for self-supervised learning of raw waveforms of music: CLMR. This approach requires no manual labeling and no preprocessing of music to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets. A linear classifier fine-tuned on representations from a pre-trained CLMR model achieves an average precision of 35.4% on the MagnaTagATune dataset, superseding fully supervised models that currently achieve a score of 34.9%. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that they capture important musical knowledge. Lastly, we show that self-supervised pre-training allows us to learn efficiently on smaller labeled datasets: we still achieve a score of 33.1% despite using only 259 labeled songs during fine-tuning. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper on GitHub.      
### 30.Exoplanet Detection in Starshade Images  [ :arrow_down: ](https://arxiv.org/pdf/2103.09385.pdf)
>  A starshade suppresses starlight by a factor of 1E11 in the image plane of a telescope, which is crucial for directly imaging Earth-like exoplanets. The state of the art in high contrast post-processing and signal detection methods were developed specifically for images taken with an internal coronagraph system and focus on the removal of quasi-static speckles. These methods are less useful for starshade images where such speckles are not present. This paper is dedicated to investigating signal processing methods tailored to work efficiently on starshade images. We describe a signal detection method, the generalized likelihood ratio test (GLRT), for starshade missions and look into three important problems. First, even with the light suppression provided by the starshade, rocky exoplanets are still difficult to detect in reflected light due to their absolute faintness. GLRT can successfully flag these dim planets. Moreover, GLRT provides estimates of the planets' positions and intensities and the theoretical false alarm rate of the detection. Second, small starshade shape errors, such as a truncated petal tip, can cause artifacts that are hard to distinguish from real planet signals; the detection method can help distinguish planet signals from such artifacts. The third direct imaging problem is that exozodiacal dust degrades detection performance. We develop an iterative generalized likelihood ratio test to mitigate the effect of dust on the image. In addition, we provide guidance on how to choose the number of photon counting images to combine into one co-added image before doing detection, which will help utilize the observation time efficiently. All the methods are demonstrated on realistic simulated images.      
### 31.An ELEGANT dataset with Denial of Service and Man in The Middle attacks  [ :arrow_down: ](https://arxiv.org/pdf/2103.09380.pdf)
>  This document describes a dataset with diverse types of Denial of Service (DoS) attacks and Man-in-the-Middle (MiTM) attacks. The data is available at the following DOI <a class="link-https link-external" data-doi="10.21227/mewp-g646" href="https://arxiv.org/ct?url=https%3A%2F%2Fdx.doi.org%2F10.21227%2Fmewp-g646&amp;v=b0da0d9f" rel="external noopener nofollow">10.21227/mewp-g646</a>. This document describes the data collection process and provides useful information on how data can be employed to devise models for cybersecurity in critical infrastructures using Programmable Logic Controllers (PLCs)      
### 32.A comparative study of deep learning methods for building footprints detection using high spatial resolution aerial images  [ :arrow_down: ](https://arxiv.org/pdf/2103.09300.pdf)
>  Building footprints data is of importance in several urban applications and natural disaster management. In contrast to traditional surveying and mapping, using high spatial resolution aerial images, deep learning-based building footprints extraction methods can extract building footprints accurately and efficiently. With rapidly development of deep learning methods, it is hard for novice to harness the powerful tools in building footprints extraction. The paper aims at providing the whole process of building footprints extraction from high spatial resolution images using deep learning-based methods. In addition, we also compare the commonly used methods, including Fully Convolutional Networks (FCN)-8s, U-Net and DeepLabv3+. At the end of the work, we change the data size used in models training to explore the influence of data size to the performance of the algorithms. The experiments show that, in different data size, DeepLabv3+ is the best algorithm among them with the highest accuracy and moderate efficiency; FCN-8s has the worst accuracy and highest efficiency; U-Net shows the moderate accuracy and lowest efficiency. In addition, with more training data, algorithms converged faster with higher accuracy in extraction results.      
### 33.Quadratic-exponential functionals of Gaussian quantum processes  [ :arrow_down: ](https://arxiv.org/pdf/2103.09279.pdf)
>  This paper is concerned with exponential moments of integral-of-quadratic functions of quantum processes with canonical commutation relations of position-momentum type. Such quadratic-exponential functionals (QEFs) arise as robust performance criteria in control problems for open quantum harmonic oscillators (OQHOs) driven by bosonic fields. We develop a randomised representation for the QEF using a Karhunen-Loeve expansion of the quantum process on a bounded time interval over the eigenbasis of its two-point commutator kernel, with noncommuting position-momentum pairs as coefficients. This representation holds regardless of a particular quantum state and employs averaging over an auxiliary classical Gaussian random process whose covariance operator is specified by the commutator kernel. This allows the QEF to be related to the moment-generating functional of the quantum process and computed for multipoint Gaussian states. For stationary Gaussian quantum processes, we establish a frequency-domain formula for the QEF rate in terms of the Fourier transform of the quantum covariance kernel in composition with trigonometric functions. A differential equation is obtained for the QEF rate with respect to the risk sensitivity parameter for its approximation and numerical computation. The QEF is also applied to large deviations and worst-case mean square cost bounds for OQHOs in the presence of statistical uncertainty with a quantum relative entropy description.      
### 34.Co-Generation and Segmentation for Generalized Surgical Instrument Segmentation on Unlabelled Data  [ :arrow_down: ](https://arxiv.org/pdf/2103.09276.pdf)
>  Surgical instrument segmentation for robot-assisted surgery is needed for accurate instrument tracking and augmented reality overlays. Therefore, the topic has been the subject of a number of recent papers in the CAI community. Deep learning-based methods have shown state-of-the-art performance for surgical instrument segmentation, but their results depend on labelled data. However, labelled surgical data is of limited availability and is a bottleneck in surgical translation of these methods. In this paper, we demonstrate the limited generalizability of these methods on different datasets, including human robot-assisted surgeries. We then propose a novel joint generation and segmentation strategy to learn a segmentation model with better generalization capability to domains that have no labelled data. The method leverages the availability of labelled data in a different domain. The generator does the domain translation from the labelled domain to the unlabelled domain and simultaneously, the segmentation model learns using the generated data while regularizing the generative model. We compared our method with state-of-the-art methods and showed its generalizability on publicly available datasets and on our own recorded video frames from robot-assisted prostatectomies. Our method shows consistently high mean Dice scores on both labelled and unlabelled domains when data is available only for one of the domains. <br>*M. Kalia and T. Aleef contributed equally to the manuscript      
