# ArXiv eess --Mon, 27 Sep 2021
### 1.ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation  [ :arrow_down: ](https://arxiv.org/pdf/2109.12108.pdf)
>  The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly re?fing the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available.      
### 2.Sequential TOA-Based Moving Target Localization in Multi-Agent Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.12027.pdf)
>  Localizing moving targets in unknown harsh environments has always been a severe challenge. This letter investigates a novel localization system based on multi-agent networks, where multiple agents serve as mobile anchors broadcasting their time-space information to the targets. We study how the moving target can localize itself using the sequential time of arrival (TOA) of the one-way broadcast signals. An extended two-step weighted least squares (TSWLS) method is proposed to jointly estimate the position and velocity of the target in the presence of agent information uncertainties. We also address the large target clock offset (LTCO) problem for numerical stability. Analytical results reveal that our method reaches the Cramer-Rao lower bound (CRLB) under small noises. Numerical results show that the proposed method performs better than the existing algorithms.      
### 3.Compositional Verification of Initial-State Opacity for Switched Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12024.pdf)
>  In this work, we propose a compositional framework for the verification of approximate initial-state opacity for networks of discrete-time switched systems. The proposed approach is based on a notion of approximate initial-state opacity-preserving simulation functions (InitSOPSFs), which characterize how close concrete networks and their finite abstractions are in terms of the satisfaction of approximate initial state opacity. We show that such InitSOPSFs can be obtained compositionally by assuming some small-gain type conditions and composing so-called local InitSOPSFs constructed for each subsystem separately. Additionally, for switched systems satisfying certain stability properties, we provide an approach to construct their finite abstractions together with the corresponding local InitSOPSFs. Finally, the effectiveness of our results is illustrated through an example.      
### 4.A Scenario-oriented Approach to Multi-period Energy-Reserve Joint Procurement and Pricing  [ :arrow_down: ](https://arxiv.org/pdf/2109.11993.pdf)
>  In [1], a single-period co-optimization model of energy and reserve is considered to better illustrate the properties of the co-optimization model and the associated market mechanism. To make the discussion more general, in this paper, the single-period co-optimization model (II) in [1] will be extended into a one-shot multi-period co-optimization model, where the coupling of ramping and reserve in multi-period operations will be considered. Also, the multi-period pricing approach and settlement process associated with the multi-period co-optimization model will be discussed. In addition, in case study, the average system total costs of the proposed multi-period model and the multi-period traditional model are compared, and the effectiveness of some market properties in multi-period operation is tested.      
### 5.Implementation of Linear Model Predictive Control -- Tutorial  [ :arrow_down: ](https://arxiv.org/pdf/2109.11986.pdf)
>  This tutorial shows an overview of Model Predictive Control with a linear discrete-time system and constrained states and inputs. The focus is on the implementation of the method under consideration of stability and recursive feasibility. The MATLAB code for the examples and plots is available online.      
### 6.Compositional synthesis of almost maximally permissible safety controllers  [ :arrow_down: ](https://arxiv.org/pdf/2109.11977.pdf)
>  In this work, we present a compositional safety controller synthesis approach for the class of discrete-time linear control systems. Here, we leverage a state-of-the-art result on the computation of robust controlled invariant sets. To tackle the complexity of controller synthesis over complex interconnected systems, this paper introduces a decentralized controller synthesis scheme. Rather than treating the interconnected system as a whole, we first design local safety controllers for each subsystem separately to enforce local safety properties, with polytopic state and input constraints as well as bounded disturbance set. Then, by composing the local controllers, the interconnected system is guaranteed to satisfy the overall safety specification. Finally, we provide a vehicular platooning example to illustrate the effectiveness of the proposed approach by solving the overall safety controller synthesis problem by computing less complex local safety controllers for subsystems and then composing them.      
### 7.Quantitative Matching of Forensic Evidence Fragments Utilizing 3D Microscopy Analysis of Fracture Surface Replicas  [ :arrow_down: ](https://arxiv.org/pdf/2109.11972.pdf)
>  Fractured surfaces carry unique details that can provide an accurate quantitative comparison to support comparative forensic analysis of those fractured surfaces. In this study, a statistical analysis comparison protocol was applied to a set of 3D topological images of fractured surface pairs and their replicas to provide confidence in the quantitative statistical comparison between fractured items and their replicas. A set of 10 fractured stainless steel samples was fractured from the same metal rod under controlled conditions and were cast using a standard forensic casting technique. Six 3D topological maps with 50% overlap were acquired for each fractured pair. Spectral analysis was utilized to identify the correlation between topological surface features at different length scales of the surface topology. We selected two frequency bands over the critical wavelength (which is greater than two-grain diameters) for statistical comparison. Our statistical model utilized a matrix-variate-$t$ distribution that accounts for the image-overlap to model the match and non-match population densities. A decision rule was developed to identify the probability of matched and unmatched pairs of surfaces. The proposed methodology correctly classified the fractured steel surfaces and their replicas with a posterior probability of match exceeding 99.96%. Moreover, the replication technique shows the potential to accurately replicate fracture surface topological details with a wavelength greater than 20$\mu$m, which far exceeds the range for comparison of most metallic alloys of 50-200$\mu$m. The developed framework establishes the basis of forensic comparison of fractured articles and their replicas while providing a reliable quantitative statistical forensic comparison, utilizing fracture mechanics-based analysis of the fracture surface topology.      
### 8.RIS-Enabled Localization Continuity Under Near-Field Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2109.11965.pdf)
>  Reconfigurable intelligent surfaces (RISs) have the potential to enable user localization in scenarios where traditional approaches fail. Building on prior work in single-antenna RIS-enabled localization, we investigate the potential to exploit wavefront curvature in geometric near-field conditions. Via a Fisher information analysis, we demonstrate that while near-field improves localization accuracy mostly at short distances when the line-of-sight (LoS) path is present, it could still provide reasonable performance when this path is blocked by relying on a single RIS reflection.      
### 9.Online Robust MPC based Emergency Maneuvering System for Autonomous Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2109.11959.pdf)
>  Nonlinear Robust Model Predictive Control (RMPC) provides a very promising solution to the problem of automatic emergency maneuvering, which is capable of handling multiple possibly conflicting objectives of robustness and performance. Even though RMPC gives a suboptimal solution, the key challenge in real-time implementation is that it is computationally very demanding. In this paper a real-time capable robust tube MPC based framework for steering control during emergency obstacle avoidance maneuver is presented. The novelty of this framework lies in the robust integration of path planning and path following tasks of autonomous vehicles. A simulation study showcases the robust performance improvements due to the proposed strategy over a non-robust MPC in different extreme maneuvering scenarios.      
### 10.Energy-Efficient Design for RIS-assisted UAVcommunications in beyond-5G Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.11933.pdf)
>  The usage of Reconfigurable Intelligent Surfaces (RIS) in conjunction with Unmanned Ariel Vehicles (UAVs) is being investigated as a way to provide energy-efficient communication to ground users in dense urban areas. In this paper, we devise an optimization scenario to reduce overall energy consumption in the network while guaranteeing certain Quality of Service (QoS) to the ground users in the area. Due to the complex nature of the optimization problem, we provide a joint UAV trajectory and RIS phase decision to minimize transmission power of the UAV and Base Station (BS) that yields good performance with lower complexity. So, the proposed method uses a Successive Convex Approximation (SCA) to iteratively determine a joint optimal solution for UAV Trajectory, RIS phase and BS and UAV Transmission Power. The approach has, therefore, been analytically evaluated under different sets of criterion.      
### 11.Kinematic Control of 2-wheeled Segway  [ :arrow_down: ](https://arxiv.org/pdf/2109.11919.pdf)
>  The Segway is a popular self-balancing two-wheeled vehicle. In this paper, we present a control mechanism for the planar Segway problem. The open-loop analysis validates the fact that the system is unstable by default and there is a need to design a closed-loop feedback to establish control for the system. This has been done by implementing a PD controller for the multiple-output system. Since the kinematic equations of the system are non-linear initially, the controller has been designed by linearizing the equations about the equilibrium point. Later, the response of the non-linear system is examined using the designed controller.      
### 12.Distributionally Robust Joint Chance-Constrained Optimization for Networked Microgrids Considering Contingencies and Renewable Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2109.11887.pdf)
>  In light of a reliable and resilient power system under extreme weather and natural disasters, networked microgrids integrating local renewable resources have been adopted extensively to supply demands when the main utility experiences blackouts. However, the stochastic nature of renewables and unpredictable contingencies are difficult to address with the deterministic energy management framework. The paper proposes a comprehensive distributionally robust joint chance-constrained (DR-JCC) framework that incorporates microgrid island, power flow, distributed batteries and voltage control constraints. All chance constraints are solved jointly and each one is assigned to an optimized violation rate. To highlight, the JCC problem with the optimized violation rates has been recognized to be NP-hard and challenging to be solved. This paper proposes a novel evolutionary algorithm that successfully tackles the problem and reduces the solution conservativeness (i.e. operation cost) by around 50% comparing with the baseline Bonferroni Approximation. Considering the imperfect solar power forecast, we construct three data-driven ambiguity sets to model uncertain forecast error distributions. The solution is thus robust for any distribution in sets with the shared moment and shape assumptions. The proposed method is validated by robustness tests based on those sets and firmly secures the solution robustness.      
### 13.Towards Goal-Oriented Semantic Signal Processing: Applications and Future Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2109.11885.pdf)
>  Advances in machine learning technology have enabled real-time extraction of semantic information in signals which can revolutionize signal processing techniques and improve their performance significantly for the next generation of applications. With the objective of a concrete representation and efficient processing of the semantic information, we propose and demonstrate a formal graph-based semantic language and a goal filtering method that enables goal-oriented signal processing. The proposed semantic signal processing framework can easily be tailored for specific applications and goals in a diverse range of signal processing applications. To illustrate its wide range of applicability, we investigate several use cases and provide details on how the proposed goal-oriented semantic signal processing framework can be customized. We also investigate and propose techniques for communications where sensor data is semantically processed and semantic information is exchanged across a sensor network.      
### 14.Learning-based Noise Component Map Estimation for Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2109.11877.pdf)
>  A problem of image denoising when images are corrupted by a non-stationary noise is considered in this paper. Since in practice no a priori information on noise is available, noise statistics should be pre-estimated for image denoising. In this paper, deep convolutional neural network (CNN) based method for estimation of a map of local, patch-wise, standard deviations of noise (so-called sigma-map) is proposed. It achieves the state-of-the-art performance in accuracy of estimation of sigma-map for the case of non-stationary noise, as well as estimation of noise variance for the case of additive white Gaussian noise. Extensive experiments on image denoising using estimated sigma-maps demonstrate that our method outperforms recent CNN-based blind image denoising methods by up to 6 dB in PSNR, as well as other state-of-the-art methods based on sigma-map estimation by up to 0.5 dB, providing same time better usage flexibility. Comparison with the ideal case, when denoising is applied using ground-truth sigma-map, shows that a difference of corresponding PSNR values for most of noise levels is within 0.1-0.2 dB and does not exceeds 0.6 dB.      
### 15.Estimating Mean Speed-of-Sound from Sequence-Dependent Geometric Disparities  [ :arrow_down: ](https://arxiv.org/pdf/2109.11819.pdf)
>  In ultrasound beamforming, focusing time delays are typically computed with a spatially constant speed-of-sound (SoS) assumption. A mismatch between beamforming and true medium SoS then leads to aberration artifacts. Other imaging techniques such as spatially-resolved SoS reconstruction using tomographic techniques also rely on a good SoS estimate for initial beamforming. In this work, we exploit spatially-varying geometric disparities in the transmit and receive paths of multiple sequences for estimating a mean medium SoS. We use images from diverging waves beamformed with an assumed SoS, and propose a model fitting method for estimating the SoS offset. We demonstrate the effectiveness of our proposed method for tomographic SoS reconstruction. With corrected beamforming SoS, the reconstruction accuracy on simulated data was improved by 63% and 29%, respectively, for an initial SoS over- and under-estimation of 1.5%. We further demonstrate our proposed method on a breast phantom, indicating substantial improvement in contrast-to-noise ratio for local SoS mapping.      
### 16.Modeling of Low Rank Time Series  [ :arrow_down: ](https://arxiv.org/pdf/2109.11814.pdf)
>  Rank-deficient stationary stochastic vector processes are present in many problems in network theory and dynamic factor analysis. In this paper we study hidden dynamical relations between the components of a discrete-time stochastic vector process and investigate their properties with respect to stability and causality. More specifically, we construct transfer functions with a full-rank input process formed from selected components of the given vector process and having a vector process of the remaining components as output. An important question, which we answer in the negative, is whether it is always possible to find such a deterministic relation that is stable. We also show how our results could be used to investigate the structure of the latent low-rank stochastic process in a dynamic factor model.      
### 17.Generalized autocorrelation analysis for multi-target detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.11813.pdf)
>  We study the multi-target detection problem of recovering a target signal from a noisy measurement that contains multiple copies of the signal at unknown locations. Motivated by the structure reconstruction problem in cryo-electron microscopy, we focus on the high noise regime, where noise hampers accurate detection of signal occurrences. Previous works proposed an autocorrelation analysis framework to estimate the signal directly from the measurement, without detecting signal occurrences. Specifically, autocorrelation analysis entails finding a signal that best matches the observable autocorrelations by minimizing a least squares objective. This paper extends this line of research by developing a generalized autocorrelation analysis framework that replaces the least squares by a weighted least squares. The optimal weights can be computed directly from the data and guarantee favorable statistical properties. We demonstrate signal recovery from highly noisy measurements, and show that the proposed framework outperforms autocorrelation analysis in a wide range of parameters.      
### 18.Predicting pigging operations in oil pipelines  [ :arrow_down: ](https://arxiv.org/pdf/2109.11812.pdf)
>  This paper presents an innovative machine learning methodology that leverages on long-term vibroacoustic measurements to perform automated predictions of the needed pigging operations in crude oil trunklines. Historical pressure signals have been collected by Eni (e-vpms monitoring system) for two years on discrete points at a relative distance of 30-35 km along an oil pipeline (100 km length, 16 inch diameter pipes) located in Northern Italy. In order to speed up the activity and to check the operation logs, a tool has been implemented to automatically highlight the historical pig operations performed on the line. Such a tool is capable of detecting, in the observed pressure measurements, the acoustic noise generated by the travelling pig. All the data sets have been reanalyzed and exploited by using field data validations to guide a decision tree regressor (DTR). Several statistical indicators, computed from pressure head loss between line segments, are fed to the DTR, which automatically outputs probability values indicating the possible need for pigging the pipeline. The procedure is applied to the vibroacoustic signals of each pair of consecutive monitoring stations, such that the proposed predictive maintenance strategy is capable of tracking the conditions of individual pipeline sections, thus determining which portion of the conduit is subject to the highest occlusion levels in order to optimize the clean-up operations. Prediction accuracy is assessed by evaluating the typical metrics used in statistical analysis of regression problems, such as the Root Mean Squared Error (RMSE).      
### 19.Few-shot Learning Based on Multi-stage Transfer and Class-Balanced Loss for Diabetic Retinopathy Grading  [ :arrow_down: ](https://arxiv.org/pdf/2109.11806.pdf)
>  Diabetic retinopathy (DR) is one of the major blindness-causing diseases current-ly known. Automatic grading of DR using deep learning methods not only speeds up the diagnosis of the disease but also reduces the rate of misdiagnosis. However, problems such as insufficient samples and imbalanced class distribu-tion in DR datasets have constrained the improvement of grading performance. In this paper, we introduce the idea of multi-stage transfer into the grading task of DR. The new transfer learning technique leverages multiple datasets with differ-ent scales to enable the model to learn more feature representation information. Meanwhile, to cope with imbalanced DR datasets, we present a class-balanced loss function that performs well in natural image classification tasks, and adopt a simple and easy-to-implement training method for it. The experimental results show that the application of multi-stage transfer and class-balanced loss function can effectively improve the grading performance metrics such as accuracy and quadratic weighted kappa. In fact, our method has outperformed two state-of-the-art methods and achieved the best result on the DR grading task of IDRiD Sub-Challenge 2.      
### 20.Adversarial Domain Feature Adaptation for Bronchoscopic Depth Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.11798.pdf)
>  Depth estimation from monocular images is an important task in localization and 3D reconstruction pipelines for bronchoscopic navigation. Various supervised and self-supervised deep learning-based approaches have proven themselves on this task for natural images. However, the lack of labeled data and the bronchial tissue's feature-scarce texture make the utilization of these methods ineffective on bronchoscopic scenes. In this work, we propose an alternative domain-adaptive approach. Our novel two-step structure first trains a depth estimation network with labeled synthetic images in a supervised manner; then adopts an unsupervised adversarial domain feature adaptation scheme to improve the performance on real images. The results of our experiments show that the proposed method improves the network's performance on real images by a considerable margin and can be employed in 3D reconstruction pipelines.      
### 21.A low complexity and high modularity design for continuously variable bandwidth digital filters  [ :arrow_down: ](https://arxiv.org/pdf/2109.11771.pdf)
>  Digital filters with variable bandwidth can be used for a variety of applications. Arbitrary change in the bandwidth of a digital Finite Impulse Response (FIR) filter can be acquired using sampling rate converters. In this paper, a sampling rate converter is proposed which is generated from Pascal structure, a fractional delay filter having low hardware complexity and high modularity. The proposed sampling rate converter requires lesser number of multipliers for implementation when compared with the sampling rate converters in the literature. A low pass filter having a single bandwidth sandwiched between two sampling rate converters can contribute multiple bandwidths in such a way that each bandwidth is an arbitrary variation of the original bandwidth. A two stage Frequency response masking approach (FRM) is used for the hardware efficient design of the original low pass filter. A low complexity and high modular novel design for a continuously varying bandwidth of a digital FIR filter is proposed in this paper using the proposed sampling rate converter. The modularity of the Pascal structure can be used to control both pass band ripple as well as stop band attenuation of the continuously variable bandwidth FIR filter design. Different communication standards in a Software defined radio (SDR) channelizer is realized using the proposed design of continuously variable bandwidth filter.      
### 22.Locality, Delays, and Internal Feedback in Sensorimotor Control Part 2: System Level Synthesis and Architecture  [ :arrow_down: ](https://arxiv.org/pdf/2109.11757.pdf)
>  The presence of internal feedback pathways (IFP) is an ubiquitous yet unexplained phenomenon in the brain. Motivated by experimental observations on 1) motor-related signals in visual areas, and 2) massively distributed processing in the brain, we approach this problem from a sensorimotor standpoint and make use of distributed optimal controllers to explain IFP. We use the System Level Synthesis (SLS) controller to model neuronal phenomena such as signaling delay, local processing, and local reaction. Based on the SLS controller, we make qualitative theoretical predictions about IFP that has strong alignment with experimental and imaging studies. In particular, we introduce a necessary `mesocircuit' for optimal performance with distributed and local processing, and local disturbance rejection; this `mesocircuit' requires extreme amounts of IFP and memory for proper function. This is the first theory that can replicate the massive amounts of IFP in the brain purely from a priori principles, providing a new and promising theoretical basis upon which we can build to better understand the inner workings of the brain.      
### 23.Locality, Delays, and Internal Feedback in Sensorimotor Control Part 1: Motivation and Introductory Theory  [ :arrow_down: ](https://arxiv.org/pdf/2109.11752.pdf)
>  Neural architectures in organisms support efficient and robust control that is beyond the capability of engineered architectures. Unraveling the function of such architectures is highly challenging; they contain massive diversity and heterogeneity in components and cryptic internal feedback pathways (IFPs). We introduce the key concepts of speed-accuracy tradeoffs (SATs), diversity-enabled sweet spots (DESS), and controller architecture which aim to help decipher complex neural architecture. As a case study in diversity and complex architectures, we consider nervous system control of movement. In this system, diversity in sensors corresponds to diverse morphology, physiology, and biochemistry of neurons. which can be interpreted as SATs in component specification. While such tradeoffs are often implicit in controller design, we aim here to make them explicit in the canonical LQR setting. We consider an LQR problem with two types of sensors, one fast but sparse and one dense but slow. The sensors achieved substantially lower performance costs when combined than when separate, demonstrating a DESS. The DeSS is also observed for the dual problem in actuation, corresponding to diverse muscle fibers. Controller-internal dynamics, sometimes called internal feedback pathways (IFPs) in biology, were necessary to achieve these combinations. This paper, intended as the first of two parts, bridges familiar control theory problems and novel applications of the System Level Synthesis framework to problems in biology and cyber-physical systems. We additionally outline future experimental plans which will utilize this new theory.      
### 24.Indoor Localization Using Smartphone Magnetic with Multi-Scale TCN and LSTM  [ :arrow_down: ](https://arxiv.org/pdf/2109.11750.pdf)
>  A novel multi-scale temporal convolutional network (TCN) and long short-term memory network (LSTM) based magnetic localization approach is proposed. To enhance the discernibility of geomagnetic signals, the time-series preprocessing approach is constructed at first. Next, the TCN is invoked to expand the feature dimensions on the basis of keeping the time-series characteristics of LSTM model. Then, a multi-scale time-series layer is constructed with multiple TCNs of different dilation factors to address the problem of inconsistent time-series speed between localization model and mobile users. A stacking framework of multi-scale TCN and LSTM is eventually proposed for indoor magnetic localization. Experiment results demonstrate the effectiveness of the proposed algorithm in indoor localization.      
### 25.Distributed Deep Reinforcement Learning for Adaptive Medium Access and Modulation in Shared Spectrum  [ :arrow_down: ](https://arxiv.org/pdf/2109.11723.pdf)
>  Spectrum scarcity has led to growth in the use of unlicensed spectrum for cellular systems. This motivates intelligent adaptive approaches to spectrum access for both WiFi and 5G that improve upon traditional carrier sensing and listen-before-talk methods. We study decentralized contention-based medium access for base stations (BSs) of a single Radio Access Technology (RAT) operating on unlicensed shared spectrum. We devise a learning-based algorithm for both contention and adaptive modulation that attempts to maximize a network-wide downlink throughput objective. We formulate and develop novel distributed implementations of two deep reinforcement learning approaches - Deep Q Networks and Proximal Policy Optimization - modelled on a two stage Markov decision process. Empirically, we find the (proportional fairness) reward accumulated by the policy gradient approach to be significantly higher than even a genie-aided adaptive energy detection threshold. Our approaches are further validated by improved sum and peak throughput. The scalability of our approach to large networks is demonstrated via an improved cumulative reward earned on both indoor and outdoor layouts with a large number of BSs.      
### 26.Training Automatic View Planner for Cardiac MR Imaging via Self-Supervision by Spatial Relationship between Views  [ :arrow_down: ](https://arxiv.org/pdf/2109.11715.pdf)
>  View planning for the acquisition of cardiac magnetic resonance imaging (CMR) requires acquaintance with the cardiac anatomy and remains a challenging task in clinical practice. Existing approaches to its automation relied either on an additional volumetric image not typically acquired in clinic routine, or on laborious manual annotations of cardiac structural landmarks. This work presents a clinic-compatible and annotation-free system for automatic CMR view planning. The system mines the spatial relationship -- more specifically, locates and exploits the intersecting lines -- between the source and target views, and trains deep networks to regress heatmaps defined by these intersecting lines. As the spatial relationship is self-contained in properly stored data, e.g., in the DICOM format, the need for manual annotation is eliminated. Then, a multi-view planning strategy is proposed to aggregate information from the predicted heatmaps for all the source views of a target view, for a globally optimal prescription. The multi-view aggregation mimics the similar strategy practiced by skilled human prescribers. Experimental results on 181 clinical CMR exams show that our system achieves superior accuracy to existing approaches including conventional atlas-based and newer deep learning based ones, in prescribing four standard CMR views. The mean angle difference and point-to-plane distance evaluated against the ground truth planes are 5.98 degrees and 3.48 mm, respectively.      
### 27.Untrained Graph Neural Networks for Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2109.11700.pdf)
>  A fundamental problem in signal processing is to denoise a signal. While there are many well-performing methods for denoising signals defined on regular supports, such as images defined on two-dimensional grids of pixels, many important classes of signals are defined over irregular domains such as graphs. This paper introduces two untrained graph neural network architectures for graph signal denoising, provides theoretical guarantees for their denoising capabilities in a simple setup, and numerically validates the theoretical results in more general scenarios. The two architectures differ on how they incorporate the information encoded in the graph, with one relying on graph convolutions and the other employing graph upsampling operators based on hierarchical clustering. Each architecture implements a different prior over the targeted signals. To numerically illustrate the validity of the theoretical results and to compare the performance of the proposed architectures with other denoising alternatives, we present several experimental results with real and synthetic datasets.      
### 28.A Multi-Agent Deep Reinforcement Learning Coordination Framework for Connected and Automated Vehicles at Merging Roadways  [ :arrow_down: ](https://arxiv.org/pdf/2109.11672.pdf)
>  The steady increase in the number of vehicles operating on the highways continues to exacerbate congestion, accidents, energy consumption, and greenhouse gas emissions. Emerging mobility systems, e.g., connected and automated vehicles (CAVs), have the potential to directly address these issues and improve transportation network efficiency and safety. In this paper, we consider a highway merging scenario and propose a framework for coordinating CAVs such that stop-and-go driving is eliminated. We use a decentralized form of the actor-critic approach to deep reinforcement learning$-$multi-agent deep deterministic policy gradient. We demonstrate the coordination of CAVs through numerical simulations and show that a smooth traffic flow is achieved by eliminating stop-and-go driving. Videos and plots of the simulation results can be found at this supplemental $\href{<a class="link-external link-https" href="https://sites.google.com/view/ud-ids-lab/MADRL" rel="external noopener nofollow">this https URL</a>}{site}$.      
### 29.Turn-to-Diarize: Online Speaker Diarization Constrained by Transformer Transducer Speaker Turn Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.11641.pdf)
>  In this paper, we present a novel speaker diarization system for streaming on-device applications. In this system, we use a transformer transducer to detect the speaker turns, represent each speaker turn by a speaker embedding, then cluster these embeddings with constraints from the detected speaker turns. Compared with conventional clustering-based diarization systems, our system largely reduces the computational cost of clustering due to the sparsity of speaker turns. Unlike other supervised speaker diarization systems which require annotations of time-stamped speaker labels for training, our system only requires including speaker turn tokens during the transcribing process, which largely reduces the human efforts involved in data collection.      
### 30.An operating system for extra long urban trains  [ :arrow_down: ](https://arxiv.org/pdf/2109.11619.pdf)
>  An operating system (OS) for subways and other urban railways is presented. The system uses extra long trains (XLTs) that can protrude beyond both ends of the station platforms. No added infrastructure is needed;only more rolling stock. The system's only technological requirement is that the doors in different parts of each train, e.g. its cars, can be operated independently. The system can preserve the level of service and evenly fill with passengers all the cars of an XLT so no space is wasted. With sufficiently long trains, a railway's productivity can be more than doubled. <br>The proposed OS has a train side and a passenger side. On the train side, it includes train organization and station-stopping protocols and on the passenger side a new information system that organizes passengers at the platforms as required by the train side protocols. These protocols specify the composition of each train and what it does at each station; i.e., whether it stops or not; how it aligns its doors along the platform; whether each door opens or not; and the set of destinations advertised by each open door. A general menu of train side protocols is presented, as well as a mathematical framework for their optimization and analysis. Numerous examples illustrating key concepts are also presented.      
### 31.Remaining useful life prediction with uncertainty quantification: development of a highly accurate model for rotating machinery  [ :arrow_down: ](https://arxiv.org/pdf/2109.11579.pdf)
>  Rotating machinery is essential to modern life, from power generation to transportation and a host of other industrial applications. Since such equipment generally operates under challenging working conditions, which can lead to untimely failures, accurate remaining useful life (RUL) prediction is essential for maintenance planning and to prevent catastrophic failures. In this work, we address current challenges in data-driven RUL prediction for rotating machinery. The challenges revolve around the accuracy and uncertainty quantification of the prediction, and the non-stationarity of the system degradation and RUL estimation given sensor data. We devise a novel architecture and RUL prediction model with uncertainty quantification, termed VisPro, which integrates time-frequency analysis, deep learning image recognition, and nonstationary Gaussian process regression. We analyze and benchmark the results obtained with our model against those of other advanced data-driven RUL prediction models for rotating machinery using the PHM12 bearing vibration dataset. The computational experiments show that (1) the VisPro predictions are highly accurate and provide significant improvements over existing prediction models (three times more accurate than the second-best model), and (2) the RUL uncertainty bounds are valid and informative. We identify and discuss the architectural and modeling choices made that explain this excellent predictive performance of VisPro.      
### 32.SAME: Deformable Image Registration based on Self-supervised Anatomical Embeddings  [ :arrow_down: ](https://arxiv.org/pdf/2109.11572.pdf)
>  In this work, we introduce a fast and accurate method for unsupervised 3D medical image registration. This work is built on top of a recent algorithm SAM, which is capable of computing dense anatomical/semantic correspondences between two images at the pixel level. Our method is named SAME, which breaks down image registration into three steps: affine transformation, coarse deformation, and deep deformable registration. Using SAM embeddings, we enhance these steps by finding more coherent correspondences, and providing features and a loss function with better semantic guidance. We collect a multi-phase chest computed tomography dataset with 35 annotated organs for each patient and conduct inter-subject registration for quantitative evaluation. Results show that SAME outperforms widely-used traditional registration techniques (Elastix FFD, ANTs SyN) and learning based VoxelMorph method by at least 4.7% and 2.7% in Dice scores for two separate tasks of within-contrast-phase and across-contrast-phase registration, respectively. SAME achieves the comparable performance to the best traditional registration method, DEEDS (from our evaluation), while being orders of magnitude faster (from 45 seconds to 1.2 seconds).      
### 33.A Graph Policy Network Approach for Volt-Var Control in Power Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12073.pdf)
>  Volt-var control (VVC) is the problem of operating power distribution systems within healthy regimes by controlling actuators in power systems. Existing works have mostly adopted the conventional routine of representing the power systems (a graph with tree topology) as vectors to train deep reinforcement learning (RL) policies. We propose a framework that combines RL with graph neural networks and study the benefits and limitations of graph-based policy in the VVC setting. Our results show that graph-based policies converge to the same rewards asymptotically however at a slower rate when compared to vector representation counterpart. We conduct further analysis on the impact of both observations and actions: on the observation end, we examine the robustness of graph-based policy on two typical data acquisition errors in power systems, namely sensor communication failure and measurement misalignment. On the action end, we show that actuators have various impacts on the system, thus using a graph representation induced by power systems topology may not be the optimal choice. In the end, we conduct a case study to demonstrate that the choice of readout function architecture and graph augmentation can further improve training performance and robustness.      
### 34.Optimized Power Normalized Cepstral Coefficients towards Robust Deep Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2109.12058.pdf)
>  After their introduction to robust speech recognition, power normalized cepstral coefficient (PNCC) features were successfully adopted to other tasks, including speaker verification. However, as a feature extractor with long-term operations on the power spectrogram, its temporal processing and amplitude scaling steps dedicated on environmental compensation may be redundant. Further, they might suppress intrinsic speaker variations that are useful for speaker verification based on deep neural networks (DNN). Therefore, in this study, we revisit and optimize PNCCs by ablating its medium-time processor and by introducing channel energy normalization. Experimental results with a DNN-based speaker verification system indicate substantial improvement over baseline PNCCs on both in-domain and cross-domain scenarios, reflected by relatively 5.8% and 61.2% maximum lower equal error rate on VoxCeleb1 and VoxMovies, respectively.      
### 35.Parameterized Channel Normalization for Far-field Deep Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2109.12056.pdf)
>  We address far-field speaker verification with deep neural network (DNN) based speaker embedding extractor, where mismatch between enrollment and test data often comes from convolutive effects (e.g. room reverberation) and noise. To mitigate these effects, we focus on two parametric normalization methods: per-channel energy normalization (PCEN) and parameterized cepstral mean normalization (PCMN). Both methods contain differentiable parameters and thus can be conveniently integrated to, and jointly optimized with the DNN using automatic differentiation methods. We consider both fixed and trainable (data-driven) variants of each method. We evaluate the performance on Hi-MIA, a recent large-scale far-field speech corpus, with varied microphone and positional settings. Our methods outperform conventional mel filterbank features, with maximum of 33.5% and 39.5% relative improvement on equal error rate under matched microphone and mismatched microphone conditions, respectively.      
### 36.From images in the wild to video-informed image classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.12040.pdf)
>  Image classifiers work effectively when applied on structured images, yet they often fail when applied on images with very high visual complexity. This paper describes experiments applying state-of-the-art object classifiers toward a unique set of images in the wild with high visual complexity collected on the island of Bali. The text describes differences between actual images in the wild and images from Imagenet, and then discusses a novel approach combining informational cues particular to video with an ensemble of imperfect classifiers in order to improve classification results on video sourced images of plants in the wild.      
### 37.Distributed Estimation of Sparse Inverse Covariance Matrices  [ :arrow_down: ](https://arxiv.org/pdf/2109.12020.pdf)
>  Learning the relationships between various entities from time-series data is essential in many applications. Gaussian graphical models have been studied to infer these relationships. However, existing algorithms process data in a batch at a central location, limiting their applications in scenarios where data is gathered by different agents. In this paper, we propose a distributed sparse inverse covariance algorithm to learn the network structure (i.e., dependencies among observed entities) in real-time from data collected by distributed agents. Our approach is built on an online graphical alternating minimization algorithm, augmented with a consensus term that allows agents to learn the desired structure cooperatively. We allow the system designer to select the number of communication rounds and optimization steps per data point. We characterize the rate of convergence of our algorithm and provide simulations on synthetic datasets.      
### 38.A data acquisition setup for data driven acoustic design  [ :arrow_down: ](https://arxiv.org/pdf/2109.12014.pdf)
>  In this paper, we present a novel interdisciplinary approach to study the relationship between diffusive surface structures and their acoustic performance. Using computational design, surface structures are iteratively generated and 3D printed at 1:10 model scale. They originate from different fabrication typologies and are designed to have acoustic diffusion and absorption effects. An automated robotic process measures the impulse responses of these surfaces by positioning a microphone and a speaker at multiple locations. The collected data serves two purposes: first, as an exploratory catalogue of different spatio-temporal-acoustic scenarios and second, as data set for predicting the acoustic response of digitally designed surface geometries using machine learning. In this paper, we present the automated data acquisition setup, the data processing and the computational generation of diffusive surface structures. We describe first results of comparative studies of measured surface panels and conclude with steps of future research.      
### 39.Visual Scene Graphs for Audio Source Separation  [ :arrow_down: ](https://arxiv.org/pdf/2109.11955.pdf)
>  State-of-the-art approaches for visually-guided audio source separation typically assume sources that have characteristic sounds, such as musical instruments. These approaches often ignore the visual context of these sound sources or avoid modeling object interactions that may be useful to better characterize the sources, especially when the same object class may produce varied sounds from distinct interactions. To address this challenging problem, we propose Audio Visual Scene Graph Segmenter (AVSGS), a novel deep learning model that embeds the visual structure of the scene as a graph and segments this graph into subgraphs, each subgraph being associated with a unique sound obtained by co-segmenting the audio spectrogram. At its core, AVSGS uses a recursive neural network that emits mutually-orthogonal sub-graph embeddings of the visual graph using multi-head attention. These embeddings are used for conditioning an audio encoder-decoder towards source separation. Our pipeline is trained end-to-end via a self-supervised task consisting of separating audio sources using the visual graph from artificially mixed sounds. In this paper, we also introduce an "in the wild'' video dataset for sound source separation that contains multiple non-musical sources, which we call Audio Separation in the Wild (ASIW). This dataset is adapted from the AudioCaps dataset, and provides a challenging, natural, and daily-life setting for source separation. Thorough experiments on the proposed ASIW and the standard MUSIC datasets demonstrate state-of-the-art sound separation performance of our method against recent prior approaches.      
### 40.Evaluating X-vector-based Speaker Anonymization under White-box Assessment  [ :arrow_down: ](https://arxiv.org/pdf/2109.11946.pdf)
>  In the scenario of the Voice Privacy challenge, anonymization is achieved by converting all utterances from a source speaker to match the same target identity; this identity being randomly selected. In this context, an attacker with maximum knowledge about the anonymization system can not infer the target identity. This article proposed to constrain the target selection to a specific identity, i.e., removing the random selection of identity, to evaluate the extreme threat under a whitebox assessment (the attacker has complete knowledge about the system). Targeting a unique identity also allows us to investigate whether some target's identities are better than others to anonymize a given speaker.      
### 41.OTFS Without CP in Massive MIMO: Breaking Doppler Limitations with TR-MRC and Windowing  [ :arrow_down: ](https://arxiv.org/pdf/2109.11899.pdf)
>  Orthogonal time frequency space (OTFS) modulation has recently emerged as an effective waveform to tackle the linear time-varying channels. In OTFS literature, approximately constant channel gains for every group of samples within each OTFS block are assumed. This leads to limitations for OTFS on the maximum Doppler frequency that it can tolerate. Additionally, presence of cyclic prefix (CP) in OTFS signal limits the flexibility in adjusting its parameters to improve its robustness against channel time variations. Therefore, in this paper, we study the possibility of removing the CP overhead from OTFS and breaking its Doppler limitations through multiple antenna processing in the large antenna regime. We asymptotically analyze the performance of time-reversal maximum ratio combining (TR-MRC) for OTFS without CP. We show that doubly dispersive channel effects average out in the large antenna regime when the maximum Doppler shift is within OTFS limitations. However, for considerably large Doppler shifts exceeding OTFS limitations, a residual Doppler effect remains. Our asymptotic derivations reveal that this effect converges to scaling of the received symbols in delay dimension with the samples of a Bessel function that depends on the maximum Doppler shift. Hence, we propose a novel residual Doppler correction (RDC) windowing technique that can break the Doppler limitations of OTFS and lead to a performance close to that of the linear time-invariant channels. Finally, we confirm the validity of our claims through simulations.      
### 42.On model reduction by least squares moment matching  [ :arrow_down: ](https://arxiv.org/pdf/2109.11869.pdf)
>  The paper addresses the model reduction problem by least squares moment matching for continuous-time, linear, time-invariant systems. The basic idea behind least squares moment matching is to approximate a transfer function by ensuring that the interpolation conditions imposed by moment matching are satisfied in a least squares sense. This idea is revisited using invariance equations and steady-state responses to provide a new time-domain characterization of least squares moment matching. The characterization, in turn, is then used to obtain a parameterized family of models achieving least squares moment matching. The theory is illustrated by a worked-out numerical example.      
### 43.Training dataset generation for bridge game registration  [ :arrow_down: ](https://arxiv.org/pdf/2109.11861.pdf)
>  This paper presents a method for automatic generation of a training dataset for a deep convolutional neural network used for playing card detection. The solution allows to skip the time-consuming processes of manual image collecting and labelling recognised objects. The YOLOv4 network trained on the generated dataset achieved an efficiency of 99.8% in the cards detection task. The proposed method is a part of a project that aims to automate the process of broadcasting duplicate bridge competitions using a vision system and neural networks.      
### 44.Modeling and Measurements for Multi-path Mitigation with Reconfigurable Intelligent Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2109.11820.pdf)
>  A reconfigurable intelligent surface (RIS) is capable of manipulating electromagnetic waves with its flexibly configurable unit cells, thus is an appealing technology to resist fast fading caused by multi-path in wireless communications. In this paper, a two-path propagation model for RIS-assisted wireless communications is proposed by considering both the direct path from the transmitter to the receiver and the assisted path provided by the RIS. The proposed propagation model unveils that the phase shifts of RISs can be optimized by appropriate configuration for multi-path fading mitigation. In particular, four types of RISs with different configuration capabilities are introduced and their performances on improving received signal power in virtue of the assisted path to resist fast fading are compared through extensive simulation results. In addition, an RIS operating at 35 GHz is used for experimental measurement. The experimental results verify that an RIS has the ability to combat fast fading and thus improves the receiving performance, which may lay a foundation for further researches.      
### 45.Causal Analysis of Carnatic Music: A Preliminary Study  [ :arrow_down: ](https://arxiv.org/pdf/2109.11782.pdf)
>  The musicological analysis of Carnatic music is challenging, owing to its rich structure and complexity. Automated \textit{rga} classification, pitch detection, tonal analysis, modelling and information retrieval of this form of southern Indian classical music have, however, made significant progress in recent times. A causal analysis to investigate the musicological structure of Carnatic compositions and the identification of the relationships embedded in them have never been previously attempted. In this study, we propose a novel framework for causal discovery, using a compression-complexity measure. Owing to the limited number of compositions available, however, we generated surrogates to further facilitate the analysis of the prevailing causal relationships. Our analysis indicates that the context-free grammar, inferred from more complex compositions, such as the \textit{M\d{l}akarta} \textit{rga}, are a \textit{structural cause} for the \textit{Janya} \textit{rga}. We also analyse certain special cases of the \textit{Janya rga} in order to understand their origins and structure better.      
### 46.Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples with On-Policy Experience  [ :arrow_down: ](https://arxiv.org/pdf/2109.11767.pdf)
>  Soft Actor-Critic (SAC) is an off-policy actor-critic reinforcement learning algorithm, essentially based on entropy regularization. SAC trains a policy by maximizing the trade-off between expected return and entropy (randomness in the policy). It has achieved state-of-the-art performance on a range of continuous-control benchmark tasks, outperforming prior on-policy and off-policy methods. SAC works in an off-policy fashion where data are sampled uniformly from past experiences (stored in a buffer) using which parameters of the policy and value function networks are updated. We propose certain crucial modifications for boosting the performance of SAC and make it more sample efficient. In our proposed improved SAC, we firstly introduce a new prioritization scheme for selecting better samples from the experience replay buffer. Secondly we use a mixture of the prioritized off-policy data with the latest on-policy data for training the policy and the value function networks. We compare our approach with the vanilla SAC and some recent variants of SAC and show that our approach outperforms the said algorithmic benchmarks. It is comparatively more stable and sample efficient when tested on a number of continuous control tasks in MuJoCo environments.      
### 47.A 3D Mesh-based Lifting-and-Projection Network for Human Pose Transfer  [ :arrow_down: ](https://arxiv.org/pdf/2109.11719.pdf)
>  Human pose transfer has typically been modeled as a 2D image-to-image translation problem. This formulation ignores the human body shape prior in 3D space and inevitably causes implausible artifacts, especially when facing occlusion. To address this issue, we propose a lifting-and-projection framework to perform pose transfer in the 3D mesh space. The core of our framework is a foreground generation module, that consists of two novel networks: a lifting-and-projection network (LPNet) and an appearance detail compensating network (ADCNet). To leverage the human body shape prior, LPNet exploits the topological information of the body mesh to learn an expressive visual representation for the target person in the 3D mesh space. To preserve texture details, ADCNet is further introduced to enhance the feature produced by LPNet with the source foreground image. Such design of the foreground generation module enables the model to better handle difficult cases such as those with occlusions. Experiments on the iPER and Fashion datasets empirically demonstrate that the proposed lifting-and-projection framework is effective and outperforms the existing image-to-image-based and mesh-based methods on human pose transfer task in both self-transfer and cross-transfer settings.      
### 48.Indoor Navigation Algorithm Based on a Smartphone Inertial Measurement Unit and Map Matching  [ :arrow_down: ](https://arxiv.org/pdf/2109.11706.pdf)
>  We propose an indoor navigation algorithm based on pedestrian dead reckoning (PDR) using an inertial measurement unit in a smartphone and map matching. The proposed indoor navigation system is user-friendly and convenient because it requires no additional device except a smartphone and works with a pedestrian in a casual posture who is walking with a smartphone in their hand. Because the performance of the PDR decreases over time, we greatly reduced the position error of the trajectory estimated by PDR using a map matching method with a known indoor map. To verify the proposed indoor navigation algorithm, we conducted an experiment in a real indoor environment using a commercial Android smartphone. The performance of our algorithm was demonstrated through the results of the experiment.      
### 49.A Parallel Tempering Approach for Efficient Exploration of the Verification Tradespace in Engineered Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.11704.pdf)
>  Verification is a critical process in the development of engineered systems. Through verification, engineers gain confidence in the correct functionality of the system before it is deployed into operation. Traditionally, verification strategies are fixed at the beginning of the system's development and verification activities are executed as the development progresses. Such an approach appears to give inferior results as the selection of the verification activities does not leverage information gained through the system's development process. In contrast, a set-based design approach to verification, where verification activities are dynamically selected as the system's development progresses, has been shown to provide superior results. However, its application under realistic engineering scenarios remains unproven due to the large size of the verification tradespace. In this work, we propose a parallel tempering approach (PTA) to efficiently explore the verification tradespace. First, we formulate exploration of the verification tradespace as a tree search problem. Second, we design a parallel tempering (PT) algorithm by simulating several replicas of the verification process at different temperatures to obtain a near-optimal result. Third, We apply the PT algorithm to all possible verification states to dynamically identify near-optimal results. The effectiveness of the proposed PTA is evaluated on a partial model of a notional satellite optical instrument.      
### 50.Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.11692.pdf)
>  Cooperative multi-agent reinforcement learning is a decentralized paradigm in sequential decision making where agents distributed over a network iteratively collaborate with neighbors to maximize global (network-wide) notions of rewards. Exact computations typically involve a complexity that scales exponentially with the number of agents. To address this curse of dimensionality, we design a scalable algorithm based on the Natural Policy Gradient framework that uses local information and only requires agents to communicate with neighbors within a certain range. Under standard assumptions on the spatial decay of correlations for the transition dynamics of the underlying Markov process and the localized learning policy, we show that our algorithm converges to the globally optimal policy with a dimension-free statistical and computational complexity, incurring a localization error that does not depend on the number of agents and converges to zero exponentially fast as a function of the range of communication.      
### 51.Performance Improvement of Dimmable OFDM-Visible Light Communication using Subcarrier Index Modulation and Reed Solomon Encoding  [ :arrow_down: ](https://arxiv.org/pdf/2109.11667.pdf)
>  In this paper, we propose a new subcarrier index modulation scheme for orthogonal frequency division multiplexing (OFDM), which incorporates the Reed-Solomon (RS) encoding in a visible light communication (VLC) system. In this scheme, the incoming bits are first encoded using the RS encoder, then a set of symbols in the resulting RS codeword are punctured, and the remaining symbols are modulated and mapped onto the OFDM subcarriers. This system is referred to as RS-OFDM-IM. Unlike the traditional subcarrier index modulation (SIM) schemes, the proposed scheme operates based on conveying extra information by inactivating the selected subcarriers, which facilitates simultaneous clipping noise reduction and spectral efficiency enhancement in OFDM-VLC. The bit error rate (BER) and throughput of the proposed technique is theoretically and numerically analyzed. The simulation results show the superiority of the proposed technique as compared to the coded DCO-OFDM without SIM and the classical SIM in OFDM-VLC in a system with practical clipping conditions.      
### 52.Sparse multi-reference alignment: sample complexity and computational hardness  [ :arrow_down: ](https://arxiv.org/pdf/2109.11656.pdf)
>  Motivated by the problem of determining the atomic structure of macromolecules using single-particle cryo-electron microscopy (cryo-EM), we study the sample and computational complexities of the sparse multi-reference alignment (MRA) model: the problem of estimating a sparse signal from its noisy, circularly shifted copies. Based on its tight connection to the crystallographic phase retrieval problem, we establish that if the number of observations is proportional to the square of the variance of the noise, then the sparse MRA problem is statistically feasible for sufficiently sparse signals. To investigate its computational hardness, we consider three types of computational frameworks: projection-based algorithms, bispectrum inversion, and convex relaxations. We show that a state-of-the-art projection-based algorithm achieves the optimal estimation rate, but its computational complexity is exponential in the sparsity level. The bispectrum framework provides a statistical-computational trade-off: it requires more observations (so its estimation rate is suboptimal), but its computational load is provably polynomial in the signal's length. The convex relaxation approach provides polynomial time algorithms (with a large exponent) that recover sufficiently sparse signals at the optimal estimation rate. We conclude the paper by discussing potential statistical and algorithmic implications for cryo-EM.      
### 53.Synthesis of parametrically-coupled networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.11628.pdf)
>  We show that a common language can be used to unify the description of parametrically-coupled circuits--parametric amplifiers, frequency converters, and parametric nonreciprocal devices--with that of band-pass filter and impedance matching networks. This enables one to readily adapt network synthesis methods from microwave engineering in the design of parametrically-coupled devices having prescribed transfer characteristics, e.g. gain, bandwidth, return loss, and isolation. We review basic practical aspects of coupled mode theory and filter synthesis, and then show how to apply both, on an equal footing, to the design of multi-pole, broadband parametric and non-reciprocal networks. We supplement the discussion with a range of examples and reference designs.      
### 54.Home Energy Management Systems: Operation and Resilience of Heuristics against Cyberattacks  [ :arrow_down: ](https://arxiv.org/pdf/2109.11627.pdf)
>  Internet of Things (IoT) and advanced communication technologies have demonstrated great potential to manage residential energy resources by enabling demand-side management (DSM). Home energy management systems (HEMSs) can automatically control electricity production and usage inside homes using DSM techniques. These HEMSs will wirelessly collect information from hardware installed in the power system and in homes with the objective to intelligently and efficiently optimize electricity usage and minimize costs. However, HEMSs can be vulnerable to cyberattacks that target the electricity pricing model. The cyberattacker manipulates the pricing information collected by a customer's HEMS to misguide its algorithms toward non-optimal solutions. The customer's electricity bill increases, and additional peaks are created without being detected by the system operator. This article introduces demand-response (DR)-based DSM in HEMSs and discusses DR optimization using heuristic algorithms. Moreover, it discusses the possibilities and impacts of cyberattacks, their effectiveness, and the degree of resilience of heuristic algorithms against cyberattacks. This article also opens research questions and shows prospective directions.      
### 55.Enhancing SUMO simulator for simulation based testing and validation of autonomous vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2109.11620.pdf)
>  Current autonomous vehicle (AV) simulators are built to provide large-scale testing required to prove capabilities under varied conditions in controlled, repeatable fashion. However, they have certain failings including the need for user expertise and complex inconvenient tutorials for customized scenario creation. Simulation of Urban Mobility (SUMO) simulator, which has been presented as an open-source AV simulator, is used extensively but suffer from similar issues which make it difficult for entry-level practitioners to utilize the simulator without significant time investment. In that regard, we provide two enhancements to SUMO simulator geared towards massively improving user experience and providing real-life like variability for surrounding traffic. Firstly, we calibrate a car-following model, Intelligent Driver Model (IDM), for highway and urban naturalistic driving data and sample automatically from the parameter distributions to create the background vehicles. Secondly, we combine SUMO with OpenAI gym, creating a Python package which can run simulations based on real world highway and urban layouts with generic output observations and input actions that can be processed via any AV pipeline. Our aim through these enhancements is to provide an easy-to-use platform which can be readily used for AV testing and validation.      
### 56.Implementation of interactive tools for investigating fundamental frequency response of voiced sounds to auditory stimulation  [ :arrow_down: ](https://arxiv.org/pdf/2109.11594.pdf)
>  We introduced a measurement procedure for the involuntary response of voice fundamental-frequency to frequency modulated auditory stimulation. This involuntary response plays an essential role in voice fundamental frequency control while less investigated due to technical difficulties. This article introduces an interactive and real-time tool for investigating this response and supporting tools adopting our new measurement method. The method enables simultaneous measurement of multiple system properties based on a novel set of extended time-stretched pulses combined with orthogonalization. We made MATLAB implementation of these tools available as an open-source repository. This article also provides the detailed measurement procedure using the interactive tool followed by offline measurement tools for conducting subjective experiments and statistical analyses. It also provides technical descriptions of constituent signal processing subsystems as appendices. This application serves as an example for adopting our method to biological system analysis.      
