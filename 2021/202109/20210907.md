# ArXiv eess --Tue, 7 Sep 2021
### 1.AIMD scheduling and resource allocation in distributed computing systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.02589.pdf)
>  We consider the problem of simultaneous scheduling and resource allocation of an incoming flow of requests to a set of computing units. By representing each computing unit as a node, we model the overall system as a multi-queue scheme. Inspired by congestion control approaches in communication networks, we propose an AIMD-like (additive increase multiplicative decrease) admission control policy that is stable irrespective of the total number of nodes and AIMD parameters. The admission policy allows us to establish an event-driven discrete model, triggered by a locally identifiable enabling condition. Subsequently, we propose a decentralized resource allocation strategy via a simple nonlinear state feedback controller, guaranteeing global convergence to a bounded set in finite time. Last, we reveal the connection of these properties with Quality of Service specifications, by calculating local queuing time via a simple formula consistent with Little's Law.      
### 2.Improving Speaker Identification for Shared Devices by Adapting Embeddings to Speaker Subsets  [ :arrow_down: ](https://arxiv.org/pdf/2109.02576.pdf)
>  Speaker identification typically involves three stages. First, a front-end speaker embedding model is trained to embed utterance and speaker profiles. Second, a scoring function is applied between a runtime utterance and each speaker profile. Finally, the speaker is identified using nearest neighbor according to the scoring metric. To better distinguish speakers sharing a device within the same household, we propose a household-adapted nonlinear mapping to a low dimensional space to complement the global scoring metric. The combined scoring function is optimized on labeled or pseudo-labeled speaker utterances. With input dropout, the proposed scoring model reduces EER by 45-71% in simulated households with 2 to 7 hard-to-discriminate speakers per household. On real-world internal data, the EER reduction is 49.2%. From t-SNE visualization, we also show that clusters formed by household-adapted speaker embeddings are more compact and uniformly distributed, compared to clusters formed by global embeddings before adaptation.      
### 3.Joint Multi-User Communication and Sensing Exploiting Both Signal and Environment Sparsity  [ :arrow_down: ](https://arxiv.org/pdf/2109.02552.pdf)
>  As a potential technology feature for 6G wireless networks, the idea of sensing-communication integration requires the system not only to complete reliable multi-user communication but also to achieve accurate environment sensing. In this paper, we consider such a joint communication and sensing (JCAS) scenario, in which multiple users use the sparse code multiple access (SCMA) scheme to communicate with the wireless access point (AP). Part of the user signals are scattered by the environment object and reflected by an intelligent reflective surface (IRS) before they arrive at the AP. We exploit the sparsity of both the structured user signals and the unstructured environment and propose an iterative and incremental joint multi-user communication and environment sensing scheme, in which the two processes, i.e., multi-user information detection and environment object detection, interweave with each other thanks to their intrinsic mutual dependence. The proposed algorithm is sliding-window based and also graph based, which can keep on sensing the environment as long as there are illuminating user signals. The trade-off relationship between the key system parameters is analyzed, and the simulation result validates the convergence and effectiveness of the proposed algorithm.      
### 4.XMUSPEECH System for VoxCeleb Speaker Recognition Challenge 2021  [ :arrow_down: ](https://arxiv.org/pdf/2109.02549.pdf)
>  This paper describes the XMUSPEECH speaker recognition and diarisation systems for the VoxCeleb Speaker Recognition Challenge 2021. For track 2, we evaluate two systems including ResNet34-SE and ECAPA-TDNN. For track 4, an important part of our system is VAD module which greatly improves the performance. Our best submission on the track 4 obtained on the evaluation set DER 5.54% and JER 27.11%, while the performance on the development set is DER 2.92% and JER 20.84%.      
### 5.Secondary frequency control stabilizing voltage dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2109.02468.pdf)
>  The ongoing energy transition challenges the stability of the electrical power system. Stable operation of the electrical power grid requires both the voltage (amplitude) and the frequency to stay within operational bounds. While much research has focused on frequency dynamics and stability, the voltage dynamics has been neglected. Here, we study frequency and voltage stability in the case of the simplest network (two nodes) and an extended all-to-all network via linear stability and bulk analysis. In particular, our linear stability analysis of the network shows that the frequency secondary control guarantees the stability of a particular electric network. Even more interesting, while we only consider secondary frequency control, we observe a stabilizing effect on the voltage dynamics, especially in our numerical bulk analysis.      
### 6.Interpretable Automated Diagnosis of Retinal Disease using Deep OCT Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.02436.pdf)
>  30 million Optical Coherence Tomography (OCT) imaging tests are issued every year to diagnose various retinal diseases, but accurate diagnosis of OCT scans requires trained ophthalmologists who are still prone to making misclassifications. With better systems for diagnosis, many cases of vision loss caused by retinal disease could be entirely avoided. In this work, we developed a CNN-based model for accurate classification of CNV, DME, Drusen, and Normal OCT scans. Furthermore, we placed an emphasis on producing both qualitative and quantitative explanations of the model's decisions. Our class-weighted EfficientNet B2 classification model performed at 99.79% accuracy. We then produced and analyzed heatmaps of where in the OCT scan the model focused. After producing the heatmaps, we created breakdowns of the specific retinal layers the model focused on. While highly accurate models have been previously developed, our work is the first to produce detailed explanations of the model's decisions. The combination of accuracy and interpretability in our work can be clinically applied for better patient care. Future work can use a similar model for classification on larger and more diverse data sets.      
### 7.Automated detection of COVID-19 cases from chest X-ray images using deep neural network and XGBoost  [ :arrow_down: ](https://arxiv.org/pdf/2109.02428.pdf)
>  In late 2019 and after COVID-19 pandemic in the world, many researchers and scholars have tried to provide methods for detection of COVID-19 cases. Accordingly, this study focused on identifying COVID-19 cases from chest X-ray images. In this paper, a novel approach to diagnosing coronavirus disease from X-ray images was proposed. In the proposed method, DenseNet169 deep neural network was used to extract the features of X-ray images taken from the patients' chest and the extracted features were then given as input to the Extreme Gradient Boosting (XGBoost) algorithm so that it could perform the classification task. Evaluation of the proposed approach and its comparison with the methods presented in recent years revealed that the proposed method was more accurate and faster than the existing ones and had an acceptable performance in detection of COVID-19 cases from X-ray images.      
### 8.Stochastic on-off switching dynamics in a model of intermittent control during human quiet stance: A finite Markov chain approximation of switched Fokker-Planck equations  [ :arrow_down: ](https://arxiv.org/pdf/2109.02427.pdf)
>  Intermittent on-off switching of feedback control has been considered as a major mechanism of generating human postural sway during quiet stance. Such an intermittent control model is described by a switched stochastic delay system with unstable subsystems, driven by additive Gaussian white noise. Dynamics of the model can be analyzed by the corresponding switched-type hybrid Fokker-Planck (FP) equations, describing time evolutions of probability density function (PDF) for a state point to be located at each point of the state space. Here, in order to perform detailed numerical analysis of the intermittent control model, we develop a comprehensive numerical recipe to represent and simulate hybrid FP equations, and apply it to the intermittent control model. To this end, the hybrid FP equations are approximated by a finite state Markov chain model under certain assumptions and by using the finite element method. Then, stochastic on-off switching dynamics of the Markov chain model, including time evolutions of PDFs, stationary PDFs and power spectral density functions of model-simulated postural sway are analyzed. We also investigate how the stationary PDF alters as values of important parameters of the model change. Dynamics of the Markov chain model are compared with Monte Carlo-based dynamics of the original intermittent control model, by which the developed numerical recipe and the resultant Markov chain model are validated.      
### 9.Evaluation of Convolutional Neural Networks for COVID-19 Classification on Chest X-Rays  [ :arrow_down: ](https://arxiv.org/pdf/2109.02415.pdf)
>  Early identification of patients with COVID-19 is essential to enable adequate treatment and to reduce the burden on the health system. The gold standard for COVID-19 detection is the use of RT-PCR tests. However, due to the high demand for tests, these can take days or even weeks in some regions of Brazil. Thus, an alternative for detecting COVID-19 is the analysis of Digital Chest X-rays (XR). Changes due to COVID-19 can be detected in XR, even in asymptomatic patients. In this context, models based on deep learning have great potential to be used as support systems for diagnosis or as screening tools. In this paper, we propose the evaluation of convolutional neural networks to identify pneumonia due to COVID-19 in XR. The proposed methodology consists of a preprocessing step of the XR, data augmentation, and classification by the convolutional architectures DenseNet121, InceptionResNetV2, InceptionV3, MovileNetV2, ResNet50, and VGG16 pre-trained with the ImageNet dataset. The obtained results demonstrate that the VGG16 architecture obtained superior performance in the classification of XR for the evaluation metrics using the methodology proposed in this article. The obtained results for our methodology demonstrate that the VGG16 architecture presented a superior performance in the classification of XR, with an Accuracy of 85.11%, Sensitivity of 85.25%, Specificity of $85.16%, F1-score of $85.03%, and an AUC of 0.9758.      
### 10.A Decoupled Uncertainty Model for MRI Segmentation Quality Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.02413.pdf)
>  Quality control (QC) of MR images is essential to ensure that downstream analyses such as segmentation can be performed successfully. Currently, QC is predominantly performed visually and subjectively, at significant time and operator cost. We aim to automate the process using a probabilistic network that estimates segmentation uncertainty through a heteroscedastic noise model, providing a measure of task-specific quality. By augmenting training images with k-space artefacts, we propose a novel CNN architecture to decouple sources of uncertainty related to the task and different k-space artefacts in a self-supervised manner. This enables the prediction of separate uncertainties for different types of data degradation. While the uncertainty predictions reflect the presence and severity of artefacts, the network provides more robust and generalisable segmentation predictions given the quality of the data. We show that models trained with artefact augmentation provide informative measures of uncertainty on both simulated artefacts and problematic real-world images identified by human raters, both qualitatively and quantitatively in the form of error bars on volume measurements. Relating artefact uncertainty to segmentation Dice scores, we observe that our uncertainty predictions provide a better estimate of MRI quality from the point of view of the task (gray matter segmentation) compared to commonly used metrics of quality including signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR), hence providing a real-time quality metric indicative of segmentation quality.      
### 11.Automated Cardiac Resting Phase Detection Targeted on the Right Coronary Artery  [ :arrow_down: ](https://arxiv.org/pdf/2109.02342.pdf)
>  Purpose: Static cardiac imaging such as late gadolinium enhancement, mapping, or 3-D coronary angiography require prior information, e.g., the phase during a cardiac cycle with least motion, called resting phase (RP). The purpose of this work is to propose a fully automated framework that allows the detection of the right coronary artery (RCA) RP within CINE series. Methods: The proposed prototype system consists of three main steps. First, the localization of the regions of interest (ROI) is performed. Second, as CINE series are time-resolved, the cropped ROI series over all time points are taken for tracking motions quantitatively. Third, the output motion values are used to classify RPs. In this work, we focused on the detection of the area with the outer edge of the cross-section of the RCA as our target. The proposed framework was evaluated on 102 clinically acquired dataset at 1.5T and 3T. The automatically classified RPs were compared with the ground truth RPs annotated manually by a medical expert for testing the robustness and feasibility of the framework. Results: The predicted RCA RPs showed high agreement with the experts annotated RPs with 92.7% accuracy, 90.5% sensitivity and 95.0% specificity for the unseen study dataset. The mean absolute difference of the start and end RP was 13.6 ${\pm}$ 18.6 ms for the validation study dataset (n=102). Conclusion: In this work, automated RP detection has been introduced by the proposed framework and demonstrated feasibility, robustness, and applicability for diverse static imaging acquisitions.      
### 12.Automatic Segmentation of the Optic Nerve Head Region in Optical Coherence Tomography: A Methodological Review  [ :arrow_down: ](https://arxiv.org/pdf/2109.02322.pdf)
>  The optic nerve head represents the intraocular section of the optic nerve (ONH), which is prone to damage by intraocular pressure. The advent of optical coherence tomography (OCT) has enabled the evaluation of novel optic nerve head parameters, namely the depth and curvature of the lamina cribrosa (LC). Together with the Bruch's membrane opening minimum-rim-width, these seem to be promising optic nerve head parameters for diagnosis and monitoring of retinal diseases such as glaucoma. Nonetheless, these optical coherence tomography derived biomarkers are mostly extracted through manual segmentation, which is time-consuming and prone to bias, thus limiting their usability in clinical practice. The automatic segmentation of optic nerve head in OCT scans could further improve the current clinical management of glaucoma and other diseases. <br>This review summarizes the current state-of-the-art in automatic segmentation of the ONH in OCT. PubMed and Scopus were used to perform a systematic review. Additional works from other databases (IEEE, Google Scholar and ARVO IOVS) were also included, resulting in a total of 27 reviewed studies. <br>For each algorithm, the methods, the size and type of dataset used for validation, and the respective results were carefully analyzed. The results show that deep learning-based algorithms provide the highest accuracy, sensitivity and specificity for segmenting the different structures of the ONH including the LC. However, a lack of consensus regarding the definition of segmented regions, extracted parameters and validation approaches has been observed, highlighting the importance and need of standardized methodologies for ONH segmentation.      
### 13.Height-Dependent LoS Probability Model for A2G MmWave Communications under Built-up Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2109.02263.pdf)
>  Based on the three-dimensional propagation characteristic under built-up scenarios, a height-dependent line-of-sight (LoS) probability model for air-to-ground (A2G) millimeter wave (mmWave) communications is proposed in this paper. With comprehensive considerations of scenario factors, i.e., building height distribution, building width, building space, and the heights of transceivers, this paper upgrades the prediction method of International Telecommunication Union-Radio (ITU-R) standard to both low altitude and high altitude cases. In order to speed up the LoS probability prediction, an approximate parametric model is also developed based on the theoretical expression. The simulation results based on ray-tracing (RT) method show that the proposed model has good consistency with existing models at the low altitude. However, it has better performance at the high altitude. The new model can be used for the A2G channel modeling and performance analysis such as cell coverage, outage probability, and bit error rate of A2G communication systems.      
### 14.Estimating Leaf Water Content using Remotely Sensed Hyperspectral Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.02250.pdf)
>  Plant water stress may occur due to the limited availability of water to the roots/soil or due to increased transpiration. These factors adversely affect plant physiology and photosynthetic ability to the extent that it has been shown to have inhibitory effects in both growth and yield [18]. Early identification of plant water stress status enables suitable corrective measures to be applied to obtain the expected crop yield. Further, improving crop yield through precision agriculture methods is a key component of climate policy and the UN sustainable development goals [1]. Leaf water content (LWC) is a measure that can be used to estimate water content and identify stressed plants. LWC during the early crop growth stages is an important indicator of plant productivity and yield. The effect of water stress can be instantaneous [15], affecting gaseous exchange or long-term, significantly reducing [9, 18, 22]. It is thus necessary to identify potential plant water stress during the early stages of growth [15] to introduce corrective irrigation and alleviate stress. LWC is also useful for identifying plant genotypes that are tolerant to water stress and salinity by measuring the stability of LWC even under artificially induced water stress [18, 25]. Such experiments generally employ destructive procedures to obtain the LWC, which is time-consuming and labor intensive. Accordingly, this research has developed a non-destructive method to estimate LWC from UAV-based hyperspectral data.      
### 15.Geometry-Based Stochastic Line-of-Sight Probability Model for A2G Channels under Urban Scenarios  [ :arrow_down: ](https://arxiv.org/pdf/2109.02222.pdf)
>  Line-of-sight (LoS) path is essential for the reliability of air-to-ground (A2G) communications, but the existence of LoS path is difficult to predict due to random obstacles on the ground. Based on the statistical geographic information and Fresnel clearance zone, a general stochastic LoS probability model for three-dimensional (3D) A2G channels under urban scenarios is developed. By considering the factors, i.e., building height distribution, building width, building space, carrier frequency, and transceiver's heights, the proposed model is suitable for different frequencies and altitudes. Moreover, in order to get a closed-form expression and reduce the computational complexity, an approximate parametric model is also built with the machine-learning (ML) method to estimate model parameters. The simulation results show that the proposed model has good consistency with existing models at the low altitude. When the altitude increases, it has better performance by comparing with that of the ray-tracing Monte-Carlo simulation data. The analytical results of proposed model are helpful for the channel modeling and performance analysis such as cell coverage, outage probability, and bit error rate in A2G communications.      
### 16.Privacy-Preserved Average Consensus Algorithms with Edge-based Additive Perturbations  [ :arrow_down: ](https://arxiv.org/pdf/2109.02192.pdf)
>  In this paper, we consider the privacy preservation problem in both discrete- and continuous-time average consensus algorithms with strongly connected and balanced graphs, against either internal honest-but-curious agents or external eavesdroppers. A novel algorithm is proposed, which adds edge-based perturbation signals to the process of consensus computation. Our algorithm can be divided into two phases: a coordinated scrambling phase, which is for privacy preservation, and a convergence phase. In the scrambling phase, each agent is required to generate some perturbation signals and add them to the edges leading out of it. In the convergence phase, the agents update their states following a normal updating rule. It is shown that an internal honest-but-curious agent can obtain the privacy of a target agent if and only if no other agents can communicate with the target agent.      
### 17.Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition  [ :arrow_down: ](https://arxiv.org/pdf/2109.02171.pdf)
>  Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is a crucial step for cardiac morphology and function analysis. However, automatic RV segmentation from MRI is still challenging, mainly due to the heterogeneous intensity, the complex variable shapes, and the unclear RV boundary. Moreover, current methods for the RV segmentation tend to suffer from performance degradation at the basal and apical slices of MRI. In this work, we propose an automatic RV segmentation framework, where the information from long-axis (LA) views is utilized to assist the segmentation of short-axis (SA) views via information transition. Specifically, we employed the transformed segmentation from LA views as a prior information, to extract the ROI from SA views for better segmentation. The information transition aims to remove the surrounding ambiguous regions in the SA views. %, such as the tricuspid valve regions. We tested our model on a public dataset with 360 multi-center, multi-vendor and multi-disease subjects that consist of both LA and SA MRIs. Our experimental results show that including LA views can be effective to improve the accuracy of the SA segmentation. Our model is publicly available at <a class="link-external link-https" href="https://github.com/NanYoMy/MMs-2" rel="external noopener nofollow">this https URL</a>.      
### 18.FBCNN: A Deep Neural Network Architecture for Portable and Fast Brain-Computer Interfaces  [ :arrow_down: ](https://arxiv.org/pdf/2109.02165.pdf)
>  Objective: To propose a novel deep neural network (DNN) architecture -- the filter bank convolutional neural network (FBCNN) -- to improve SSVEP classification in single-channel BCIs with small data lengths. <br>Methods: We propose two models: the FBCNN-2D and the FBCNN-3D. The FBCNN-2D utilizes a filter bank to create sub-band components of the electroencephalography (EEG) signal, which it transforms using the fast Fourier transform (FFT) and analyzes with a 2D CNN. The FBCNN-3D utilizes the same filter bank, but it transforms the sub-band components into spectrograms via short-time Fourier transform (STFT), and analyzes them with a 3D CNN. We made use of transfer learning. To train the FBCNN-3D, we proposed a new technique, called inter-dimensional transfer learning, to transfer knowledge from a 2D DNN to a 3D DNN. Our BCI was conceived so as not to require calibration from the final user: therefore, the test subject data was separated from training and validation. <br>Results: The mean test accuracy was 85.7% for the FBCCA-2D and 85% for the FBCCA-3D. Mean F1-Scores were 0.858 and 0.853. Alternative classification methods, SVM, FBCCA and a CNN, had mean accuracy of 79.2%, 80.1% and 81.4%, respectively. <br>Conclusion: The FBCNNs surpassed traditional SSVEP classification methods in our simulated BCI, by a considerable margin (about 5% higher accuracy). Transfer learning and inter-dimensional transfer learning made training much faster and more predictable. <br>Significance: We proposed a new and flexible type of DNN, which had a better performance than standard methods in SSVEP classification for portable and fast BCIs.      
### 19.K-Step Opacity in Discrete Event Systems: Verification, Complexity, and Relations  [ :arrow_down: ](https://arxiv.org/pdf/2109.02158.pdf)
>  Opacity is a property expressing whether a system may reveal its secret to a passive observer (an intruder) who knows the structure of the system but has a limited observation of its behavior. Several notions of opacity have been studied, including current-state opacity, K-step opacity, and infinite-step opacity. We study K-step opacity that generalizes both current-state opacity and infinite-step opacity, and asks whether the intruder cannot decide, at any time, whether or when the system was in a secret state during the last K observable steps. We design a new algorithm deciding K-step opacity the complexity of which is lower than that of existing algorithms and that does not depend on K. We then compare K-step opacity with other opacity notions and provide new transformations among the notions that do not use states that are neither secret nor non-secret (neutral states) and that are polynomial with respect to both the size of the system and the binary encoding of K.      
### 20.Adaptive Turbo Equalization for Nonlinearity Compensation in WDM Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.02148.pdf)
>  In this paper, the performance of adaptive turbo equalization for nonlinearity compensation (NLC) is investigated. A turbo equalization scheme is proposed where a recursive least-squares (RLS) algorithm is used as an adaptive channel estimator to track the time-varying intersymbol interference (ISI) coefficients associated with inter-channel nonlinear interference (NLI) model. The estimated channel coefficients are used by a MIMO 2x2 soft-input soft-output (SISO) linear minimum mean square error (LMMSE) equalizer to compensate for the time-varying ISI. The SISO LMMSE equalizer and the SISO forward error correction (FEC) decoder exchange extrinsic information in every turbo iteration, allowing the receiver to improve the performance of the channel estimation and the equalization, achieving lower bit-error-rate (BER) values. The proposed scheme is investigated for polarization multiplexed 64QAM and 256QAM, although it applies to any proper modulation format. Extensive numerical results are presented. It is shown that the scheme allows up to 0.7 dB extra gain in effectively received signal-to-noise ratio (SNR) and up to 0.2 bits/symbol/pol in generalized mutual information (GMI), on top of the gain provided by single-channel digital backpropagation.      
### 21.Guidance Trajectory Mathematical Modeling  [ :arrow_down: ](https://arxiv.org/pdf/2109.02141.pdf)
>  A trajectory of a destination-directed moving object (e.g. an aircraft from an origin airport to a destination airport) has three main components: an origin, a destination, and motion in between. We call such a trajectory that end up at the destination \textit{destination-directed trajectory (DDT)}. A class of conditionally Markov (CM) sequences (called CM$_\text{L}$) has the following main components: a joint density of two endpoints and a Markov-like evolution law. A CM$_\text{L}$ dynamic model can describe the evolution of a DDT but not of a guided object chasing a moving guide. The trajectory of a guided object is called a \textit{guided trajectory (GT)}. Inspired by a CM$_\text{L}$ model, this paper proposes a model for a GT with a moving guide. The proposed model reduces to a CM$_\text{L}$ model if the guide is not moving. We also study filtering and trajectory prediction based on the proposed model. Simulation results are presented.      
### 22.Machine Learning-Based 3D Channel Modeling for U2V mmWave Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.02104.pdf)
>  Unmanned aerial vehicle (UAV) millimeter wave (mmWave) technologies can provide flexible link and high data rate for future communication networks. By considering the new features of three-dimensional (3D) scattering space, 3D velocity, 3D antenna array, and especially 3D rotations, a machine learning (ML) integrated UAV-to-Vehicle (U2V) mmWave channel model is proposed. Meanwhile, a ML-based network for channel parameter calculation and generation is developed. The deterministic parameters are calculated based on the simplified geometry information, while the random ones are generated by the back propagation based neural network (BPNN) and generative adversarial network (GAN), where the training data set is obtained from massive ray-tracing (RT) simulations. Moreover, theoretical expressions of channel statistical properties, i.e., power delay profile (PDP), autocorrelation function (ACF), Doppler power spectrum density (DPSD), and cross-correlation function (CCF) are derived and analyzed. Finally, the U2V mmWave channel is generated under a typical urban scenario at 28 GHz. The generated PDP and DPSD show good agreement with RT-based results, which validates the effectiveness of proposed method. Moreover, the impact of 3D rotations, which has rarely been reported in previous works, can be observed in the generated CCF and ACF, which are also consistent with the theoretical and measurement results.      
### 23.Recognition of COVID-19 Disease Utilizing X-Ray Imaging of the Chest Using CNN  [ :arrow_down: ](https://arxiv.org/pdf/2109.02103.pdf)
>  Since this COVID-19 pandemic thrives, the utilization of X-Ray images of the Chest (CXR) as a complementary screening technique to RT-PCR testing grows to its clinical use for respiratory complaints. Many new deep learning approaches have developed as a consequence. The goal of this research is to assess the convolutional neural networks (CNNs) to diagnosis COVID-19 utisizing X-ray images of chest. The performance of CNN with one, three, and four convolution layers has been evaluated in this research. A dataset of 13,808 CXR photographs are used in this research. When evaluated on X-ray images with three splits of the dataset, our preliminary experimental results show that the CNN model with three convolution layers can reliably detect with 96 percent accuracy (precision being 96 percent). This fact indicates the commitment of our suggested model for reliable screening of COVID-19.      
### 24.(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.02084.pdf)
>  Segmentation plays a crucial role in diagnosis. Studying the retinal vasculatures from fundus images help identify early signs of many crucial illnesses such as diabetic retinopathy. Due to the varying shape, size, and patterns of retinal vessels, along with artefacts and noises in fundus images, no one-stage method can accurately segment retinal vessels. In this work, we propose a multi-scale, multi-level attention embedded CNN architecture ((M)SLAe-Net) to address the issue of multi-stage processing for robust and precise segmentation of retinal vessels. We do this by extracting features at multiple scales and multiple levels of the network, enabling our model to holistically extracts the local and global features. Multi-scale features are extracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We also aggregate the features from all the network levels. These effectively resolved the issues of varying shapes and artefacts and hence the need for multiple stages. To assist in better pixel-level classification, we use the Squeeze and Attention(SA) module, a smartly adapted version of the Squeeze and Excitation(SE) module for segmentation tasks in our network to facilitate pixel-group attention. Our unique network design and novel D-DPP module with efficient task-specific loss function for thin vessels enabled our model for better cross data performance. Exhaustive experimental results on DRIVE, STARE, HRF, and CHASE-DB1 show the superiority of our method.      
### 25.Guaranteed State Estimation via Indirect Polytopic Set Computation for Nonlinear Discrete-Time Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.02016.pdf)
>  This paper proposes novel set-theoretic approaches for state estimation in bounded-error discrete-time nonlinear systems, subject to nonlinear observations/constraints. By transforming the polytopic sets that are characterized as zonotope bundles (ZB) and/or constrained zonotopes (CZ), from the state space to the space of the generators of ZB/CZ, we leverage a recent result on the remainder-form mixed-monotone decomposition functions to compute the propagated set, i.e., a ZB/CZ that is guaranteed to enclose the set of the state trajectories of the considered system. Further, by applying the remainder-form decomposition functions to the nonlinear observation function, we derive the updated set, i.e., an enclosing ZB/CZ of the intersection of the propagated set and the set of states that are compatible/consistent with the observations/constraints. Finally, we show that the mean value extension result in [1] for computing propagated sets can also be extended to compute the updated set when the observation function is nonlinear.      
### 26.The DKU-DukeECE-Lenovo System for the Diarization Task of the 2021 VoxCeleb Speaker Recognition Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2109.02002.pdf)
>  This report describes the submission of the DKU-DukeECE-Lenovo team to the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2021 track 4. Our system including a voice activity detection (VAD) model, a speaker embedding model, two clustering-based speaker diarization systems with different similarity measurements, two different overlapped speech detection (OSD) models, and a target-speaker voice activity detection (TS-VAD) model. Our final submission, consisting of 5 independent systems, achieves a DER of 5.07% on the challenge test set.      
### 27.Image Compression with Recurrent Neural Network and Generalized Divisive Normalization  [ :arrow_down: ](https://arxiv.org/pdf/2109.01999.pdf)
>  Image compression is a method to remove spatial redundancy between adjacent pixels and reconstruct a high-quality image. In the past few years, deep learning has gained huge attention from the research community and produced promising image reconstruction results. Therefore, recent methods focused on developing deeper and more complex networks, which significantly increased network complexity. In this paper, two effective novel blocks are developed: analysis and synthesis block that employs the convolution layer and Generalized Divisive Normalization (GDN) in the variable-rate encoder and decoder side. Our network utilizes a pixel RNN approach for quantization. Furthermore, to improve the whole network, we encode a residual image using LSTM cells to reduce unnecessary information. Experimental results demonstrated that the proposed variable-rate framework with novel blocks outperforms existing methods and standard image codecs, such as George's ~\cite{002} and JPEG in terms of image similarity. The project page along with code and models are available at <a class="link-external link-https" href="https://khawar512.github.io/cvpr/" rel="external noopener nofollow">this https URL</a>      
### 28.Risk of Cascading Failures in Time-Delayed Vehicle Platooning  [ :arrow_down: ](https://arxiv.org/pdf/2109.01963.pdf)
>  We develop a systemic risk framework to explore cascading systemic failures in networked control systems. A time-delayed version of the vehicle platooning problem is used as a benchmark to study the interplay among network connectivity, system dynamics, physical limitations, and uncertainty onto the possibility of cascading failure phenomena. The measure of value-at-risk is employed to investigate the domino effect of failures among pairs of vehicles within the platoon. The systemic risk framework is suitably extended to quantify the robustness of cascading failures via a novel manipulation of bi-variate distribution. We establish closed-form risk formulas that explain the effect of network parameters (e.g., Laplacian eigen-spectrum, time delay), noise statistics, and systemic event sets onto the cascading failures. Our findings can be applied to the design of robust platoons to lower the cascading risk. We support our theoretical results with extensive simulations.      
### 29.A Fully-Integrated 5mW, 0.8Gbps Energy-Efficient Chip-to-Chip Data Link for Ultra-Low-Power IoT End-Nodes in 65-nm CMOS  [ :arrow_down: ](https://arxiv.org/pdf/2109.01961.pdf)
>  The increasing complexity of Internet-of-Things (IoT) applications and near-sensor processing algorithms is pushing the computational power of low-power, battery-operated end-node systems. This trend also reveals growing demands for high-speed and energy-efficient inter-chip communications to manage the increasing amount of data coming from off-chip sensors and memories. While traditional micro-controller interfaces such as SPIs cannot cope with tight energy and large bandwidth requirements, low-voltage swing transceivers can tackle this challenge thanks to their capability to achieve several Gbps of the communication speed at milliwatt power levels. However, recent research on high-speed serial links focused on high-performance systems, with a power consumption significantly larger than the one of low-power IoT end-nodes, or on stand-alone designs not integrated at a system level. This paper presents a low-swing transceiver for the energy-efficient and low power chip-to-chip communication fully integrated within an IoT end-node System-on-Chip, fabricated in CMOS 65nm technology. The transceiver can be easily controlled via a software interface; thus, we can consider realistic scenarios for the data communication, which cannot be assessed in stand-alone prototypes. Chip measurements show that the transceiver achieves 8.46x higher energy efficiency at 15.9x higher performance than a traditional microcontroller interface such as a single-SPI.      
### 30.Asymptotically Stable Observer-based Controller for Attitude Tracking with Systematic Convergence  [ :arrow_down: ](https://arxiv.org/pdf/2109.01937.pdf)
>  This paper proposes a novel unit-quaternion observer-based controller for attitude tracking (attitude and angular velocity) with guaranteed transient and steady-state performance. The proposed approach is computationally cheap and can operate based on measurements provided, for instance by a typical low-cost inertial measurement unit (IMU) or magnetic, angular rate, and gravity (MARG) sensor without the knowledge of angular velocity. First, an observer evolved on $\mathbb{S}^{3}\times\mathbb{R}^{3}$ is developed guaranteeing asymptotic stability of the closed loop error signals starting from any initial condition. Afterwards, the observer is combined with the proposed controller such that the observer-based controller ensures asymptotic stability of the closed loop error signals starting from any initial condition. Simulation performed in discrete form at low sampling rate reveals the robustness and effectiveness of the proposed approach. Keywords: Observer-based controller, attitude, estimation, control, MARG, IMU, asymptotic stability.      
### 31.Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke  [ :arrow_down: ](https://arxiv.org/pdf/2109.01887.pdf)
>  This paper presents an automatic algorithm for the segmentation of areas affected by an acute stroke on the non-contrast computed tomography brain images. The proposed algorithm is designed for learning in a weakly supervised scenario when some images are labeled accurately, and some images are labeled inaccurately. Wrong labels appear as a result of inaccuracy made by a radiologist in the process of manual annotation of computed tomography images. We propose methods for solving the segmentation problem in the case of inaccurately labeled training data. We use the U-Net neural network architecture with several modifications. Experiments on real computed tomography scans show that the proposed methods increase the segmentation accuracy.      
### 32.Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging  [ :arrow_down: ](https://arxiv.org/pdf/2109.01880.pdf)
>  Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging method providing multiplex molecular and functional information from the rodent brain. It can be greatly augmented by magnetic resonance imaging (MRI) that offers excellent soft-tissue contrast and high-resolution brain anatomy. Nevertheless, registration of multi-modal images remains challenging, chiefly due to the entirely different image contrast rendered by these modalities. Previously reported registration algorithms mostly relied on manual user-dependent brain segmentation, which compromised data interpretation and accurate quantification. Here we propose a fully automated registration method for MSOT-MRI multimodal imaging empowered by deep learning. The automated workflow includes neural network-based image segmentation to generate suitable masks, which are subsequently registered using an additional neural network. Performance of the algorithm is showcased with datasets acquired by cross-sectional MSOT and high-field MRI preclinical scanners. The automated registration method is further validated with manual and half-automated registration, demonstrating its robustness and accuracy.      
### 33.Predicting isocitrate dehydrogenase mutationstatus in glioma using structural brain networksand graph neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.01854.pdf)
>  Glioma is a common malignant brain tumor that shows distinct survival among patients. The isocitrate dehydrogenase (IDH) gene mutation status provides critical diagnostic and prognostic value for glioma and is now accepted as the standard of care. A non-invasive prediction of IDH mutation based on the pre-treatment MRI has crucial clinical significance. Machine learning and deep learning models show reasonable performance in predicting IDH mutation status. However, most models neglect the systematic brain alterations caused by tumor invasion, where the infiltration along white matter tracts throughout the brain is identified as a hallmark of glioma. Structural brain network provides an effective tool to characterise brain organisation, which could be captured by the graph neural networks (GNN) for a more accurate prediction of IDH mutation status. <br>Here we propose a method to predict the IDH mutation using GNN, based on the structural brain network of patients. Specifically, we firstly construct a network template of healthy subjects, which consists of atlases of edges (white matter tracts) and nodes (cortical and subcortical brain regions) to provide regions of interest (ROI). Next, we employ autoencoders to extract the latent multi-modal MRI features from the ROIs of the edge and node in patients. These features of edge and node of brain networks are used to train a GNN architecture in predicting IDH mutation status. The results show that the proposed method outperforms the baseline models using 3D-CNN and 3D-DenseNet. In addition, the model interpretation suggests its ability to identify the tracts infiltrated by tumor and corresponds to clinical prior knowledge. In conclusion, integrating brain networks with GNN offers a new avenue to study brain lesions using computational neuroscience and computer vision approaches.      
### 34.A Privacy-Preserving Image Retrieval Scheme Using A Codebook Generated From Independent Plain-Image Dataset  [ :arrow_down: ](https://arxiv.org/pdf/2109.01841.pdf)
>  In this paper, we propose a privacy-preserving image-retrieval scheme using a codebook generated by using a plain-image dataset. Encryption-then-compression (EtC) images, which were proposed for EtC systems, have been used in conventional privacy-preserving image-retrieval schemes, in which a codebook is generated from EtC images uploaded by image owners, and extended SIMPLE descriptors are then calculated as image descriptors by using the codebook. In contrast, in the proposed scheme, a codebook is generated from a dataset independent of uploaded images. The use of an independent dataset enables us not only to use a codebook that does not require recalculation but also to constantly provide a high retrieval accuracy. In an experiment, the proposed scheme is demonstrated to maintain a high retrieval performance, even if codebooks are generated from a plain image dataset independent of image owners' encrypted images.      
### 35.OCTAVA: an open-source toolbox for quantitative analysis of optical coherence tomography angiography images  [ :arrow_down: ](https://arxiv.org/pdf/2109.01835.pdf)
>  Optical coherence tomography angiography (OCTA) performs non-invasive visualization and characterization of microvasculature in research and clinical applications mainly in ophthalmology and dermatology. A wide variety of instruments, imaging protocols, processing methods and metrics have been used to describe the microvasculature, such that comparing different study outcomes is currently not feasible. With the goal of contributing to standardization of OCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA (OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and quantitative analysis of en face OCTA maximum intensity projection images in a standardized workflow. We present each analysis step, including optimization of filtering and choice of segmentation algorithm, and definition of metrics. We perform quantitative analysis of OCTA images from different commercial and non-commercial instruments and samples and show OCTAVA can accurately and reproducibly determine metrics for characterization of microvasculature. Wide adoption could enable studies and aggregation of data on a scale sufficient to develop reliable microvascular biomarkers for early detection, and to guide treatment, of microvascular disease.      
### 36.Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.01824.pdf)
>  Sleep stage classification is essential for sleep assessment and disease diagnosis. Although previous attempts to classify sleep stages have achieved high classification performance, several challenges remain open: 1) How to effectively utilize time-varying spatial and temporal features from multi-channel brain signals remains challenging. Prior works have not been able to fully utilize the spatial topological information among brain regions. 2) Due to the many differences found in individual biological signals, how to overcome the differences of subjects and improve the generalization of deep neural networks is important. 3) Most deep learning methods ignore the interpretability of the model to the brain. To address the above challenges, we propose a multi-view spatial-temporal graph convolutional networks (MSTGCN) with domain generalization for sleep stage classification. Specifically, we construct two brain view graphs for MSTGCN based on the functional connectivity and physical distance proximity of the brain regions. The MSTGCN consists of graph convolutions for extracting spatial features and temporal convolutions for capturing the transition rules among sleep stages. In addition, attention mechanism is employed for capturing the most relevant spatial-temporal information for sleep stage classification. Finally, domain generalization and MSTGCN are integrated into a unified framework to extract subject-invariant sleep features. Experiments on two public datasets demonstrate that the proposed model outperforms the state-of-the-art baselines.      
### 37.Efficient Estimation of Sensor Biases for the 3-Dimensional Asynchronous Multi-Sensor System  [ :arrow_down: ](https://arxiv.org/pdf/2109.01823.pdf)
>  An important preliminary procedure in multi-sensor data fusion is sensor registration, and the key step in this procedure is to estimate sensor biases from their noisy measurements. There are generally two difficulties in this bias estimation problem: one is the unknown target states which serve as the nuisance variables in the estimation problem, and the other is the highly nonlinear coordinate transformation between the local and global coordinate systems of the sensors. In this paper, we focus on the 3-dimensional asynchronous multi-sensor scenario and propose a weighted nonlinear least squares (NLS) formulation by assuming that there is a target moving with a nearly constant velocity. We propose two possible choices of the weighting matrix in the NLS formulation, which correspond to classical NLS estimation and maximum likelihood (ML) estimation, respectively. To address the intrinsic nonlinearity, we propose a block coordinate descent (BCD) algorithm for solving the formulated problem, which alternately updates different kinds of bias estimates. Specifically, the proposed BCD algorithm involves solving linear LS problems and nonconvex quadratically constrained quadratic program (QCQP) problems with special structures. Instead of adopting the semidefinite relaxation technique, we develop a much more computationally efficient algorithm (with global performance guarantee under certain conditions) to solve the nonconvex QCQP subproblems. The effectiveness and efficiency of the proposed BCD algorithm are demonstrated via numerical simulations.      
### 38.Measurement-based Condition Monitoring of Railway Signaling Cables  [ :arrow_down: ](https://arxiv.org/pdf/2109.01781.pdf)
>  We propose a composite diagnostics solution for railway infrastructure monitoring. In particular, we address the issue of soft-fault detection in underground railway cables. We first demonstrate the feasibility of an orthogonal multitone time domain reflectometry based fault detection and location method for railway cabling infrastructure by implementing it using software defined radios. Our practical implementation, comprehensive measurement campaign, and our measurement results guide the design of our overall composite solution. With several diagnostics solutions available in the literature, our conglomerated method presents a technique to consolidate results from multiple diagnostics methods to provide an accurate assessment of underground cable health. We present a Bayesian framework based cable health index computation technique that indicates the extent of degradation that a cable is subject to at any stage during its lifespan. We present the performance results of our proposed solution using real-world measurements to demonstrate its effectiveness.      
### 39.Fast Task-Based Adaptive Sampling for 3D Single-Photon Multispectral Lidar Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.01743.pdf)
>  3D single-photon LiDAR imaging plays an important role in numerous applications. However, long acquisition times and significant data volumes present a challenge to LiDAR imaging. This paper proposes a task-optimized adaptive sampling framework that enables fast acquisition and processing of high-dimensional single-photon LiDAR data. Given a task of interest, the iterative sampling strategy targets the most informative regions of a scene which are defined as those minimizing parameter uncertainties. The task is performed by considering a Bayesian model that is carefully built to allow fast per-pixel computations while delivering parameter estimates with quantified uncertainties. The framework is demonstrated on multispectral 3D single-photon LiDAR imaging when considering object classification and/or target detection as tasks. It is also analysed for both sequential and parallel scanning modes for different detector array sizes. Results on simulated and real data show the benefit of the proposed optimized sampling strategy when compared to fixed sampling strategies.      
### 40.Resource and data efficient self supervised learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.01721.pdf)
>  We investigate the utility of pretraining by contrastive self supervised learning on both natural-scene and medical imaging datasets when the unlabeled dataset size is small, or when the diversity within the unlabeled set does not lead to better representations. We use a two step approach which is analogous to supervised training with ImageNet initialization, where we pretrain networks that are already pretrained on ImageNet dataset to improve downstream task performance on the domain of interest. To improve the speed of convergence and the overall performance, we propose weight scaling and filter selection methods prior to second step of pretraining. We demonstrate the utility of this approach on three popular contrastive techniques, namely SimCLR, SWaV and BYOL. Benefits of double pretraining include better performance, faster convergence, ability to train with smaller batch sizes and smaller image dimensions with negligible differences in performance. We hope our work helps democratize self-supervision by enabling researchers to fine-tune models without requiring large clusters or long training times.      
### 41.Structural polyhedral stability of a biochemical network is equivalent to finiteness of the associated generalised Petri net  [ :arrow_down: ](https://arxiv.org/pdf/2109.01709.pdf)
>  We consider biochemical systems associated with a generalised class of Petri nets with possibly negative token numbers. We show that the existence of a structural polyhedral Lyapunov function for the biochemical system is equivalent to the boundedness of the associated Petri net evolution or, equivalently, to the finiteness of the number of states reachable from each initial condition. For networks that do not admit a polyhedral Lyapunov function, we investigate whether it is possible to enforce polyhedral structural stability by applying a strong negative feedback on some pinned nodes: in terms of the Petri net, this is equivalent to turning pinned nodes into black holes that clear any positive or negative incoming token. If such nodes are chosen so that the transformed Petri net has bounded discrete trajectories, then there exists a stabilising pinning control: the biochemical network becomes Lyapunov stable if a sufficiently strong local negative feedback is applied to the pinned nodes. These results allow us to structurally identify the critical nodes to be locally controlled so as to ensure the stability of the whole network.      
### 42.How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?  [ :arrow_down: ](https://arxiv.org/pdf/2109.01668.pdf)
>  The recent achievements of Deep Learning rely on the test data being similar in distribution to the training data. In an ideal case, Deep Learning models would achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make predictions on out-of-distribution data. Yet in practice, models usually fail to generalize well when facing a shift in distribution. Several methods were thereby designed to improve the robustness of the features learned by a model through Regularization- or Domain-Prediction-based schemes. Segmenting medical images such as MRIs of the hippocampus is essential for the diagnosis and treatment of neuropsychiatric disorders. But these brain images often suffer from distribution shift due to the patient's age and various pathologies affecting the shape of the organ. In this work, we evaluate OoD Generalization solutions for the problem of hippocampus segmentation in MR data using both fully- and semi-supervised training. We find that no method performs reliably in all experiments. Only the V-REx loss stands out as it remains easy to tune, while it outperforms a standard U-Net in most cases.      
### 43.Hierarchical 3D Feature Learning for Pancreas Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.01667.pdf)
>  We propose a novel 3D fully convolutional deep network for automated pancreas segmentation from both MRI and CT scans. More specifically, the proposed model consists of a 3D encoder that learns to extract volume features at different scales; features taken at different points of the encoder hierarchy are then sent to multiple 3D decoders that individually predict intermediate segmentation maps. Finally, all segmentation maps are combined to obtain a unique detailed segmentation mask. We test our model on both CT and MRI imaging data: the publicly available NIH Pancreas-CT dataset (consisting of 82 contrast-enhanced CTs) and a private MRI dataset (consisting of 40 MRI scans). Experimental results show that our model outperforms existing methods on CT pancreas segmentation, obtaining an average Dice score of about 88%, and yields promising segmentation performance on a very challenging MRI data set (average Dice score is about 77%). Additional control experiments demonstrate that the achieved performance is due to the combination of our 3D fully-convolutional deep network and the hierarchical representation decoding, thus substantiating our architectural design.      
### 44.Ghost Projection  [ :arrow_down: ](https://arxiv.org/pdf/2109.01666.pdf)
>  Ghost imaging is a developing imaging technique that employs random masks to image a sample. Ghost projection utilizes ghost-imaging concepts to perform the complementary procedure of projection of a desired image. The key idea underpinning ghost projection is that any desired spatial distribution of radiant exposure may be produced, up to an additive constant, by spatially-uniformly illuminating a set of random masks in succession. We explore three means of achieving ghost projection: (i) weighting each random mask, namely selecting its exposure time, according to its correlation with a desired image, (ii) selecting a subset of random masks according to their correlation with a desired image, and (iii) numerically optimizing a projection for a given set of random masks and desired image. The first two protocols are analytically tractable and conceptually transparent. The third is more efficient but less amenable to closed-form analytical expressions. A comparison with existing image-projection techniques is drawn and possible applications are discussed. These potential applications include: (i) a data projector for matter and radiation fields for which no current data projectors exist, (ii) a universal-mask approach to lithography, (iii) tomographic volumetric additive manufacturing, and (iv) a ghost-projection photocopier.      
### 45.Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2109.01664.pdf)
>  Super-resolving the Magnetic Resonance (MR) image of a target contrast under the guidance of the corresponding auxiliary contrast, which provides additional anatomical information, is a new and effective solution for fast MR imaging. However, current multi-contrast super-resolution (SR) methods tend to concatenate different contrasts directly, ignoring their relationships in different clues, \eg, in the foreground and background. In this paper, we propose a separable attention network (comprising a foreground priority attention and background separation attention), named SANet. Our method can explore the foreground and background areas in the forward and reverse directions with the help of the auxiliary contrast, enabling it to learn clearer anatomical structures and edge information for the SR of a target-contrast MR image. SANet provides three appealing benefits: (1) It is the first model to explore a separable attention mechanism that uses the auxiliary contrast to predict the foreground and background regions, diverting more attention to refining any uncertain details between these regions and correcting the fine areas in the reconstructed results. (2) A multi-stage integration module is proposed to learn the response of multi-contrast fusion at different stages, obtain the dependency between the fused features, and improve their representation ability. (3) Extensive experiments with various state-of-the-art multi-contrast SR methods on fastMRI and clinical \textit{in vivo} datasets demonstrate the superiority of our model.      
### 46.Global-Local Transformer for Brain Age Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.01663.pdf)
>  Deep learning can provide rapid brain age estimation based on brain magnetic resonance imaging (MRI). However, most studies use one neural network to extract the global information from the whole input image, ignoring the local fine-grained details. In this paper, we propose a global-local transformer, which consists of a global-pathway to extract the global-context information from the whole input image and a local-pathway to extract the local fine-grained details from local patches. The fine-grained information from the local patches are fused with the global-context information by the attention mechanism, inspired by the transformer, to estimate the brain age. We evaluate the proposed method on 8 public datasets with 8,379 healthy brain MRIs with the age range of 0-97 years. 6 datasets are used for cross-validation and 2 datasets are used for evaluating the generality. Comparing with other state-of-the-art methods, the proposed global-local transformer reduces the mean absolute error of the estimated ages to 2.70 years and increases the correlation coefficient of the estimated age and the chronological age to 0.9853. In addition, our proposed method provides regional information of which local patches are most informative for brain age estimation. Our source code is available on: \url{<a class="link-external link-https" href="https://github.com/shengfly/global-local-transformer" rel="external noopener nofollow">this https URL</a>}.      
### 47.Gaussian Process Uniform Error Bounds with Unknown Hyperparameters for Safety-Critical Applications  [ :arrow_down: ](https://arxiv.org/pdf/2109.02606.pdf)
>  Gaussian processes have become a promising tool for various safety-critical settings, since the posterior variance can be used to directly estimate the model error and quantify risk. However, state-of-the-art techniques for safety-critical settings hinge on the assumption that the kernel hyperparameters are known, which does not apply in general. To mitigate this, we introduce robust Gaussian process uniform error bounds in settings with unknown hyperparameters. Our approach computes a confidence region in the space of hyperparameters, which enables us to obtain a probabilistic upper bound for the model error of a Gaussian process with arbitrary hyperparameters. We do not require to know any bounds for the hyperparameters a priori, which is an assumption commonly found in related work. Instead, we are able to derive bounds from data in an intuitive fashion. We additionally employ the proposed technique to derive performance guarantees for a class of learning-based control problems. Experiments show that the bound performs significantly better than vanilla and fully Bayesian Gaussian processes.      
### 48.Robust Control Barrier Functions with Sector-Bounded Uncertainties  [ :arrow_down: ](https://arxiv.org/pdf/2109.02537.pdf)
>  This paper focuses on safety critical control with sector-bounded uncertainties at the plant input. The uncertainties can represent nonlinear and/or time-varying components. We propose a new robust control barrier function (RCBF) approach to enforce safety requirements in the presence of these uncertainties. The primary objective is to minimally alter the given baseline control command to guarantee safety in the presence of modeled uncertainty. The resulting min-norm optimization problem can be recast as a Second-Order Cone Program (SOCP) to enable online implementation. Properties of this controller are studied and a numerical example is provided to illustrate the effectiveness of this approach.      
### 49.Optimal Lockdown Strategy in a Pandemic: An Exploratory Analysis for Covid-19  [ :arrow_down: ](https://arxiv.org/pdf/2109.02512.pdf)
>  The paper addresses the question of lives versus livelihood in an SIRD model augmented with a macroeconomic structure. The constraints on the availability of health facilities - both infrastructure and health workers determine the probability of receiving treatment which is found to be higher for the patients with severe infection than the patients with mild infection for the specific parametric configuration of the paper. Distinguishing between two types of direct intervention policy - hard lockdown and soft lockdown, the study derives alternative policy options available to the government. The study further indicates that the soft lockdown policy is optimal from a public policy perspective under the specific parametric configuration considered in this paper.      
### 50.Jammer Mitigation via Beam-Slicing for Low-Resolution mmWave Massive MU-MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2109.02502.pdf)
>  Millimeter-wave (mmWave) massive multi-user multiple-input multiple-output (MU-MIMO) promises unprecedented data rates for next-generation wireless systems. To be practically viable, mmWave massive MU-MIMO basestations (BSs) must rely on low-resolution data converters which leaves them vulnerable to jammer interference. This paper proposes beam-slicing, a method that mitigates the impact of a permanently transmitting jammer during uplink transmission for BSs equipped with low-resolution analog-to-digital converters (ADCs). Beam-slicing is a localized analog spatial transform that focuses the jammer energy onto few ADCs, so that the transmitted data can be recovered based on the outputs of the interference-free ADCs. We demonstrate the efficacy of beam-slicing in combination with two digital jammer-mitigating data detectors: SNIPS and CHOPS. Soft-Nulling of Interferers with Partitions in Space (SNIPS) combines beam-slicing with a soft-nulling data detector that exploits knowledge of the ADC contamination; projeCtion onto ortHOgonal complement with Partitions in Space (CHOPS) combines beam-slicing with a linear projection that removes all signal components co-linear to an estimate of the jammer channel. Our results show that beam-slicing enables SNIPS and CHOPS to successfully serve 65% of the user equipments (UEs) for scenarios in which their antenna-domain counterparts that lack beam-slicing are only able to serve 2% of the UEs.      
### 51.Audio-based Musical Version Identification: Elements and Challenges  [ :arrow_down: ](https://arxiv.org/pdf/2109.02472.pdf)
>  In this article, we aim to provide a review of the key ideas and approaches proposed in 20 years of scientific literature around musical version identification (VI) research and connect them to current practice. For more than a decade, VI systems suffered from the accuracy-scalability trade-off, with attempts to increase accuracy that typically resulted in cumbersome, non-scalable systems. Recent years, however, have witnessed the rise of deep learning-based approaches that take a step toward bridging the accuracy-scalability gap, yielding systems that can realistically be deployed in industrial applications. Although this trend positively influences the number of researchers and institutions working on VI, it may also result in obscuring the literature before the deep learning era. To appreciate two decades of novel ideas in VI research and to facilitate building better systems, we now review some of the successful concepts and applications proposed in the literature and study their evolution throughout the years.      
### 52.Developing and validating multi-modal models for mortality prediction in COVID-19 patients: a multi-center retrospective study  [ :arrow_down: ](https://arxiv.org/pdf/2109.02439.pdf)
>  The unprecedented global crisis brought about by the COVID-19 pandemic has sparked numerous efforts to create predictive models for the detection and prognostication of SARS-CoV-2 infections with the goal of helping health systems allocate resources. Machine learning models, in particular, hold promise for their ability to leverage patient clinical information and medical images for prediction. However, most of the published COVID-19 prediction models thus far have little clinical utility due to methodological flaws and lack of appropriate validation. In this paper, we describe our methodology to develop and validate multi-modal models for COVID-19 mortality prediction using multi-center patient data. The models for COVID-19 mortality prediction were developed using retrospective data from Madrid, Spain (N=2547) and were externally validated in patient cohorts from a community hospital in New Jersey, USA (N=242) and an academic center in Seoul, Republic of Korea (N=336). The models we developed performed differently across various clinical settings, underscoring the need for a guided strategy when employing machine learning for clinical decision-making. We demonstrated that using features from both the structured electronic health records and chest X-ray imaging data resulted in better 30-day-mortality prediction performance across all three datasets (areas under the receiver operating characteristic curves: 0.85 (95% confidence interval: 0.83-0.87), 0.76 (0.70-0.82), and 0.95 (0.92-0.98)). We discuss the rationale for the decisions made at every step in developing the models and have made our code available to the research community. We employed the best machine learning practices for clinical model development. Our goal is to create a toolkit that would assist investigators and organizations in building multi-modal models for prediction, classification and/or optimization.      
### 53.LoRa-RL: Deep Reinforcement Learning for Resource Management in Hybrid Energy LoRa Wireless Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.02392.pdf)
>  LoRa wireless networks are considered as a key enabling technology for next generation internet of things (IoT) systems. New IoT deployments (e.g., smart city scenarios) can have thousands of devices per square kilometer leading to huge amount of power consumption to provide connectivity. In this paper, we investigate green LoRa wireless networks powered by a hybrid of the grid and renewable energy sources, which can benefit from harvested energy while dealing with the intermittent supply. This paper proposes resource management schemes of the limited number of channels and spreading factors (SFs) with the objective of improving the LoRa gateway energy efficiency. First, the problem of grid power consumption minimization while satisfying the system's quality of service demands is formulated. Specifically, both scenarios the uncorrelated and time-correlated channels are investigated. The optimal resource management problem is solved by decoupling the formulated problem into two sub-problems: channel and SF assignment problem and energy management problem. Since the optimal solution is obtained with high complexity, online resource management heuristic algorithms that minimize the grid energy consumption are proposed. Finally, taking into account the channel and energy correlation, adaptable resource management schemes based on Reinforcement Learning (RL), are developed. Simulations results show that the proposed resource management schemes offer efficient use of renewable energy in LoRa wireless networks.      
### 54.Printed Texts Tracking and Following for a Finger-Wearable Electro-Braille System Through Opto-electrotactile Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2109.02385.pdf)
>  This paper presents our recent development on a portable and refreshable text reading and sensory substitution system for the blind or visually impaired (BVI), called Finger-eye. The system mainly consists of an opto-text processing unit and a compact electro-tactile based display that can deliver text-related electrical signals to the fingertip skin through a wearable and Braille-dot patterned electrode array and thus delivers the electro-stimulation based Braille touch sensations to the fingertip. To achieve the goal of aiding BVI to read any text not written in Braille through this portable system, in this work, a Rapid Optical Character Recognition (R-OCR) method is firstly developed for real-time processing text information based on a Fisheye imaging device mounted at the finger-wearable electro-tactile display. This allows real-time translation of printed text to electro-Braille along with natural movement of user's fingertip as if reading any Braille display or book. More importantly, an electro-tactile neuro-stimulation feedback mechanism is proposed and incorporated with the R-OCR method, which facilitates a new opto-electrotactile feedback based text line tracking control approach that enables text line following by user fingertip during reading. Multiple experiments were designed and conducted to test the ability of blindfolded participants to read through and follow the text line based on the opto-electrotactile-feedback method. The experiments show that as the result of the opto-electrotactile-feedback, the users were able to maintain their fingertip within a $2mm$ distance of the text while scanning a text line. This research is a significant step to aid the BVI users with a portable means to translate and follow to read any printed text to Braille, whether in the digital realm or physically, on any surface.      
### 55.Reconfigurable Intelligent Surface Empowered Over-the-Air Federated Edge Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.02353.pdf)
>  Federated edge learning (FEEL) has emerged as a revolutionary paradigm to develop AI services at the edge of 6G wireless networks as it supports collaborative model training at a massive number of mobile devices. However, model communication over wireless channels, especially in uplink model uploading of FEEL, has been widely recognized as a bottleneck that critically limits the efficiency of FEEL. Although over-the-air computation can alleviate the excessive cost of radio resources in FEEL model uploading, practical implementations of over-the-air FEEL still suffer from several challenges, including strong straggler issues, large communication overheads, and potential privacy leakage. In this article, we study these challenges in over-the-air FEEL and leverage reconfigurable intelligent surface (RIS), a key enabler of future wireless systems, to address these challenges. We study the state-of-the-art solutions on RIS-empowered FEEL and explore the promising research opportunities for adopting RIS to enhance FEEL performance.      
### 56.Towards an Approach to Contextual Detection of Multi-Stage Cyber Attacks in Smart Grids  [ :arrow_down: ](https://arxiv.org/pdf/2109.02336.pdf)
>  Electric power grids are at risk of being compromised by high-impact cyber-security threats such as coordinated, timed attacks. Navigating this new threat landscape requires a deep understanding of the potential risks and complex attack processes in energy information systems, which in turn demands an unmanageable manual effort to timely process a large amount of cross-domain information. To provide an adequate basis to contextually assess and understand the situation of smart grids in case of coordinated cyber-attacks, we need a systematic and coherent approach to identify cyber incidents. In this paper, we present an approach that collects and correlates cross-domain cyber threat information to detect multi-stage cyber-attacks in energy information systems. We investigate the applicability and performance of the presented correlation approach and discuss the results to highlight challenges in domain-specific detection mechanisms.      
### 57.Full-color photon-counting single-pixel imaging  [ :arrow_down: ](https://arxiv.org/pdf/2109.02333.pdf)
>  We propose and experimentally demonstrate a high-efficiency single-pixel imaging (SPI) scheme by integrating time-correlated single-photon counting (TCSPC) with time-division multiplexing to acquire full-color images at extremely low light level. This SPI scheme uses a digital micromirror device to modulate a sequence of laser pulses with preset delays to achieve three-color structured illumination, then employs a photomultiplier tube into the TCSPC module to achieve photon-counting detection. By exploiting the time-resolved capabilities of TCSPC, we demodulate the spectrum-image-encoded signals, and then reconstruct high-quality full-color images in a single-round of measurement. Based on this scheme, the strategies such as single-step measurement, high-speed projection, and undersampling can further improve the imaging efficiency.      
### 58.Robust Congestion Control for Demand-Based Optimization in Precoded Multi-Beam High Throughput Satellite Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.02327.pdf)
>  High-throughput satellite communications systems are growing in strategic importance thanks to their role in delivering broadband services to mobile platforms and residences and/or businesses in rural and remote regions globally. Although precoding has emerged as a prominent technique to meet ever-increasing user demands, there is a lack of studies dealing with congestion control. This paper enhances the performance of multi-beam high throughput geostationary (GEO) satellite systems under congestion, where the users' quality of service (QoS) demands cannot be fully satisfied with limited resources. In particular, we propose congestion control strategies, relying on simple power control schemes. We formulate a multi-objective optimization framework balancing the system sum-rate and the number of users satisfying their QoS requirements. Next, we propose two novel approaches that effectively handle the proposed multi-objective optimization problem. The former is a model-based approach that relies on the weighted sum method to enrich the number of satisfied users by solving a series of the sum-rate optimization problems in an iterative manner. Meanwhile, the latter is a data-driven approach that offers a low-cost solution by utilizing supervised learning and exploiting the optimization structures as continuous mappings. The proposed general framework is evaluated for different linear precoding techniques, for which the low computational complexity algorithms are designed. Numerical results manifest that our proposed framework effectively handles the congestion issue and brings superior improvements of rate satisfaction to many users than previous works. Furthermore, the proposed algorithms show low run-time, which make them realistic for practical systems.      
### 59.Surgery Scene Restoration for Robot Assisted Minimally Invasive Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2109.02253.pdf)
>  Minimally invasive surgery (MIS) offers several advantages including minimum tissue injury and blood loss, and quick recovery time, however, it imposes some limitations on surgeons ability. Among others such as lack of tactile or haptic feedback, poor visualization of the surgical site is one of the most acknowledged factors that exhibits several surgical drawbacks including unintentional tissue damage. To the context of robot assisted surgery, lack of frame contextual details makes vision task challenging when it comes to tracking tissue and tools, segmenting scene, and estimating pose and depth. In MIS the acquired frames are compromised by different noises and get blurred caused by motions from different sources. Moreover, when underwater environment is considered for instance knee arthroscopy, mostly visible noises and blur effects are originated from the environment, poor control on illuminations and imaging conditions. Additionally, in MIS, procedure like automatic white balancing and transformation between the raw color information to its standard RGB color space are often absent due to the hardware miniaturization. There is a high demand of an online preprocessing framework that can circumvent these drawbacks. Our proposed method is able to restore a latent clean and sharp image in standard RGB color space from its noisy, blur and raw observation in a single preprocessing stage.      
### 60.Generative Models Improve Radiomics Performance in Different Tasks and Different Datasets: An Experimental Study  [ :arrow_down: ](https://arxiv.org/pdf/2109.02252.pdf)
>  Radiomics is an active area of research focusing on high throughput feature extraction from medical images with a wide array of applications in clinical practice, such as clinical decision support in oncology. However, noise in low dose computed tomography (CT) scans can impair the accurate extraction of radiomic features. In this article, we investigate the possibility of using deep learning generative models to improve the performance of radiomics from low dose CTs. We used two datasets of low dose CT scans -NSCLC Radiogenomics and LIDC-IDRI - as test datasets for two tasks - pre-treatment survival prediction and lung cancer diagnosis. We used encoder-decoder networks and conditional generative adversarial networks (CGANs) trained in a previous study as generative models to transform low dose CT images into full dose CT images. Radiomic features extracted from the original and improved CT scans were used to build two classifiers - a support vector machine (SVM) and a deep attention based multiple instance learning model - for survival prediction and lung cancer diagnosis respectively. Finally, we compared the performance of the models derived from the original and improved CT scans. Encoder-decoder networks and CGANs improved the area under the curve (AUC) of survival prediction from 0.52 to 0.57 (p-value&lt;0.01). On the other hand, Encoder-decoder network and CGAN can improve the AUC of lung cancer diagnosis from 0.84 to 0.88 and 0.89 respectively (p-value&lt;0.01). Moreover, there are no statistically significant differences in improving AUC by using encoder-decoder network and CGAN (p-value=0.34) when networks trained at 75 and 100 epochs. Generative models can improve the performance of low dose CT-based radiomics in different tasks. Hence, denoising using generative models seems to be a necessary pre-processing step for calculating radiomic features from low dose CTs.      
### 61.Supervised DKRC with Images for Offline System Identification  [ :arrow_down: ](https://arxiv.org/pdf/2109.02241.pdf)
>  Koopman spectral theory has provided a new perspective in the field of dynamical systems in recent years. Modern dynamical systems are becoming increasingly non-linear and complex, and there is a need for a framework to model these systems in a compact and comprehensive representation for prediction and control. The central problem in applying Koopman theory to a system of interest is that the choice of finite-dimensional basis functions is typically done apriori, using expert knowledge of the systems dynamics. Our approach learns these basis functions using a supervised learning approach where a combination of autoencoders and deep neural networks learn the basis functions for any given system. We demonstrate this approach on a simple pendulum example in which we obtain a linear representation of the non-linear system and then predict the future state trajectories given some initial conditions. We also explore how changing the input representation of the dynamic systems time series data can impact the quality of learned basis functions. This alternative representation is compared to the traditional raw time series data approach to determine which method results in lower reconstruction and prediction error of the true non-linear dynamics of the system.      
### 62.Achieving QoS for Real-Time Bursty Applications over Passive Optical Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.02186.pdf)
>  Emerging real-time applications such as those classified under ultra-reliable low latency (uRLLC) generate bursty traffic and have strict Quality of Service (QoS) requirements. Passive Optical Network (PON) is a popular access network technology, which is envisioned to handle such applications at the access segment of the network. However, the existing standards cannot handle strict QoS constraints. The available solutions rely on instantaneous heuristic decisions and maintain QoS constraints (mostly bandwidth) in an average sense. Existing works with optimal strategies are computationally complex and are not suitable for uRLLC applications. This paper presents a novel computationally-efficient, far-sighted bandwidth allocation policy design for facilitating bursty traffic in a PON framework while satisfying strict QoS (age of information/delay and bandwidth) requirements of modern applications. To this purpose, first we design a delay-tracking mechanism which allows us to model the resource allocation problem from a control-theoretic viewpoint as a Model Predictive Control (MPC). MPC helps in taking far-sighted decisions regarding resource allocations and captures the time-varying dynamics of the network. We provide computationally efficient polynomial-time solutions and show its implementation in the PON framework. Compared to existing approaches, MPC reduces delay violations by approximately 15% for a delay-constrained application of 1ms target. Our approach is also robust to varying traffic arrivals.      
### 63.Implementation of MPC in embedded systems using first order methods  [ :arrow_down: ](https://arxiv.org/pdf/2109.02140.pdf)
>  This Ph.D. dissertation contains results in two different but related fields: the implementation of model predictive control (MPC) in embedded systems using first order methods, and restart schemes for accelerated first order methods (AFOM). We start by presenting three novel restart schemes for AFOM. These schemes can improve the convergence of the AFOM by suppressing the undesirable oscillations that they are prone to present. The schemes we develop have theoretical guarantees and do not require knowledge of difficult-to-obtain parameters of the optimization problem. Next, we present sparse solvers for various MPC formulations which take advantage of the structures of the optimization problems. The solvers have been made available in an open-source toolbox for Matlab called SPCIES (<a class="link-external link-https" href="https://github.com/GepocUS/Spcies" rel="external noopener nofollow">this https URL</a>). Finally, we present a novel MPC formulation that displays a larger domain of attraction and better performance than other MPC formulations, especially when using small prediction horizons. This, along with its recursive feasibility and asymptotic stability, makes it especially suitable for its implementation in embedded systems.      
### 64.Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.02096.pdf)
>  This research project investigates the application of deep learning to timbre transfer, where the timbre of a source audio can be converted to the timbre of a target audio with minimal loss in quality. The adopted approach combines Variational Autoencoders with Generative Adversarial Networks to construct meaningful representations of the source audio and produce realistic generations of the target audio and is applied to the Flickr 8k Audio dataset for transferring the vocal timbre between speakers and the URMP dataset for transferring the musical timbre between instruments. Furthermore, variations of the adopted approach are trained, and generalised performance is compared using the metrics SSIM (Structural Similarity Index) and FAD (Frecht Audio Distance). It was found that a many-to-many approach supersedes a one-to-one approach in terms of reconstructive capabilities, and that the adoption of a basic over a bottleneck residual block design is more suitable for enriching content information about a latent space. It was also found that the decision on whether cyclic loss takes on a variational autoencoder or vanilla autoencoder approach does not have a significant impact on reconstructive and adversarial translation aspects of the model.      
### 65.Nonparametric Extrema Analysis in Time Series for Envelope Extraction, Peak Detection and Clustering  [ :arrow_down: ](https://arxiv.org/pdf/2109.02082.pdf)
>  In this paper, we propose a nonparametric approach that can be used in envelope extraction, peak-burst detection and clustering in time series. Our problem formalization results in a naturally defined splitting/forking of the time series. With a possibly hierarchical implementation, it can be used for various applications in machine learning, signal processing and mathematical finance. From an incoming input signal, our iterative procedure sequentially creates two signals (one upper bounding and one lower bounding signal) by minimizing the cumulative $L_1$ drift. We show that a solution can be efficiently calculated by use of a Viterbi-like path tracking algorithm together with an optimal elimination rule. We consider many interesting settings, where our algorithm has near-linear time complexities.      
### 66.The Phonexia VoxCeleb Speaker Recognition Challenge 2021 System Description  [ :arrow_down: ](https://arxiv.org/pdf/2109.02052.pdf)
>  We describe the Phonexia submission for the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC-21) in the unsupervised speaker verification track. Our solution was very similar to IDLab's winning submission for VoxSRC-20. An embedding extractor was bootstrapped using momentum contrastive learning, with input augmentations as the only source of supervision. This was followed by several iterations of clustering to assign pseudo-speaker labels that were then used for supervised embedding extractor training. Finally, a score fusion was done, by averaging the zt-normalized cosine scores of five different embedding extractors. We briefly also describe unsuccessful solutions involving i-vectors instead of DNN embeddings and PLDA instead of cosine scoring.      
### 67.Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.02051.pdf)
>  Many endeavors have sought to develop countermeasure techniques as enhancements on Automatic Speaker Verification (ASV) systems, in order to make them more robust against spoof attacks. As evidenced by the latest ASVspoof 2019 countermeasure challenge, models currently deployed for the task of ASV are, at their best, devoid of suitable degrees of generalization to unseen attacks. Upon further investigation of the proposed methods, it appears that a broader three-tiered view of the proposed systems. comprised of the classifier, feature extraction phase, and model loss function, may to some extent lessen the problem. Accordingly, the present study proposes the Efficient Attention Branch Network (EABN) modular architecture with a combined loss function to address the generalization problem...      
### 68.The ByteDance Speaker Diarization System for the VoxCeleb Speaker Recognition Challenge 2021  [ :arrow_down: ](https://arxiv.org/pdf/2109.02047.pdf)
>  This paper describes the ByteDance speaker diarization system for the fourth track of the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC-21). The VoxSRC-21 provides both the dev set and test set of VoxConverse for use in validation and a standalone test set for evaluation. We first collect the duration and signal-to-noise ratio (SNR) of all audio and find that the distribution of the VoxConverse's test set and the VoxSRC-21's test set is more closer. Our system consists of voice active detection (VAD), speaker embedding extraction, spectral clustering followed by a re-clustering step based on agglomerative hierarchical clustering (AHC) and overlapped speech detection and handling. Finally, we integrate systems with different time scales using DOVER-Lap. Our best system achieves 5.15\% of the diarization error rate (DER) on evaluation set, ranking the second at the diarization track of the challenge.      
### 69.A Two-stage Complex Network using Cycle-consistent Generative Adversarial Networks for Speech Enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2109.02011.pdf)
>  Cycle-consistent generative adversarial networks (CycleGAN) have shown their promising performance for speech enhancement (SE), while one intractable shortcoming of these CycleGAN-based SE systems is that the noise components propagate throughout the cycle and cannot be completely eliminated. Additionally, conventional CycleGAN-based SE systems only estimate the spectral magnitude, while the phase is unaltered. Motivated by the multi-stage learning concept, we propose a novel two-stage denoising system that combines a CycleGAN-based magnitude enhancing network and a subsequent complex spectral refining network in this paper. Specifically, in the first stage, a CycleGAN-based model is responsible for only estimating magnitude, which is subsequently coupled with the original noisy phase to obtain a coarsely enhanced complex spectrum. After that, the second stage is applied to further suppress the residual noise components and estimate the clean phase by a complex spectral mapping network, which is a pure complex-valued network composed of complex 2D convolution/deconvolution and complex temporal-frequency attention blocks. Experimental results on two public datasets demonstrate that the proposed approach consistently surpasses previous one-stage CycleGANs and other state-of-the-art SE systems in terms of various evaluation metrics, especially in background noise suppression.      
### 70.The SpeakIn System for VoxCeleb Speaker Recognition Challange 2021  [ :arrow_down: ](https://arxiv.org/pdf/2109.01989.pdf)
>  This report describes our submission to the track 1 and track 2 of the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC 2021). Both track 1 and track 2 share the same speaker verification system, which only uses VoxCeleb2-dev as our training set. This report explores several parts, including data augmentation, network structures, domain-based large margin fine-tuning, and back-end refinement. Our system is a fusion of 9 models and achieves first place in these two tracks of VoxSRC 2021. The minDCF of our submission is 0.1034, and the corresponding EER is 1.8460%.      
### 71.Horizontal and Vertical Collaboration for VR Delivery in MEC-Enabled Small-Cell Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.01971.pdf)
>  Due to the large bandwidth, low latency and computationally intensive features of virtual reality (VR) video applications, the current resource-constrained wireless and edge networks cannot meet the requirements of on-demand VR delivery. In this letter, we propose a joint horizontal and vertical collaboration architecture in mobile edge computing (MEC)-enabled small-cell networks for VR delivery. In the proposed architecture, multiple MEC servers can jointly provide VR head-mounted devices (HMDs) with edge caching and viewpoint computation services, while the computation tasks can also be performed at HMDs or on the cloud. Power allocation at base stations (BSs) is considered in coordination with horizontal collaboration (HC) and vertical collaboration (VC) of MEC servers to obtain lower end-to-end latency of VR delivery. A joint caching, power allocation and task offloading problem is then formulated, and a discrete branch-reduce-and-bound (DBRB) algorithm inspired by monotone optimization is proposed to effectively solve the problem. Simulation results demonstrate the advantage of the proposed architecture and algorithm in terms of existing ones.      
### 72.Iterative Threshold Decoding of Spatially Coupled, Parallel-Concatenated Codes  [ :arrow_down: ](https://arxiv.org/pdf/2109.01955.pdf)
>  Spatially coupled, parallel concatenated codes (SC-PCCs) have been shown to approach channel capacity when decoded using optimal iterative methods. However, under complexity constraints such decoding strategies can result in unacceptable power and latency costs. In this work, we employ convolutional self-orthogonal component codes along with low-complexity, suboptimal a posteriori probability (APP) threshold decoders with SC-PCCs to reduce decoding complexity. The proposed code design is faster, more energy efficient, and easier to implement than optimal methods, while offering significant coding gain over existing threshold decodable, turbo-like constructions of similar latency and complexity. The design also serves to further illustrate the advantages spatial coupling can provide to existing code constructions and decoder implementations.      
### 73.Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment  [ :arrow_down: ](https://arxiv.org/pdf/2109.01949.pdf)
>  Self-supervised learning provides an opportunity to explore unlabeled chest X-rays and their associated free-text reports accumulated in clinical routine without manual supervision. This paper proposes a Joint Image Text Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray images and their radiology reports. The model was pre-trained on both the global image-sentence level and the local image region-word level for visual-textual matching. Both are bidirectionally constrained on Cross-Entropy based and ranking-based Triplet Matching Losses. The region-word matching is calculated using the attention mechanism without direct supervision about their mapping. The pre-trained multi-modal representation learning paves the way for downstream tasks concerning image and/or text encoding. We demonstrate the representation learning quality by cross-modality retrievals and multi-label classifications on two datasets: OpenI-IU and MIMIC-CXR      
### 74.Network Modulation Synthesis: New Algorithms for Generating Musical Audio Using Autoencoder Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.01948.pdf)
>  A new framework is presented for generating musical audio using autoencoder neural networks. With the presented framework, called network modulation synthesis, users can create synthesis architectures and use novel generative algorithms to more easily move through the complex latent parameter space of an autoencoder model to create audio. <br>Implementations of the new algorithms are provided for the open-source CANNe synthesizer network, and can be applied to other autoencoder networks for audio synthesis. Spectrograms and time-series encoding analysis demonstrate that the new algorithms provide simple mechanisms for users to generate time-varying parameter combinations, and therefore auditory possibilities, that are difficult to create by generating audio from handcrafted encodings.      
### 75.Low SNR Capacity of Keyhole MIMO Channel in Nakagami-m Fading With Full CSI  [ :arrow_down: ](https://arxiv.org/pdf/2109.01817.pdf)
>  In this paper, we derive asymptotic expressions for the ergodic capacity of the multiple-input multiple-output (MIMO) keyhole channel at low SNR in independent and identically distributed (i.i.d.) Nakagami-$m$ fading conditions with perfect channel state information available at both the transmitter (CSI-T) and the receiver (CSI-R). We show that the low-SNR capacity of this keyhole channel scales proportionally as $\frac{\textrm{SNR}}{4} \log^2 \left(1/{\textrm{SNR}}\right)$. Further, we develop a practically appealing On-Off transmission scheme that is aymptotically capacity achieving at low SNR; it requires only one-bit CSI-T feedback and is robust against both mild and severe Nakagami-$m$ fadings for a very wide range of low-SNR values. These results also extend to the Rayleigh keyhole MIMO channel as a special case.      
### 76.SEC4SR: A Security Analysis Platform for Speaker Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.01766.pdf)
>  Adversarial attacks have been expanded to speaker recognition (SR). However, existing attacks are often assessed using different SR models, recognition tasks and datasets, and only few adversarial defenses borrowed from computer vision are considered. Yet,these defenses have not been thoroughly evaluated against adaptive attacks. Thus, there is still a lack of quantitative understanding about the strengths and limitations of adversarial attacks and defenses. More effective defenses are also required for securing SR systems. To bridge this gap, we present SEC4SR, the first platform enabling researchers to systematically and comprehensively evaluate adversarial attacks and defenses in SR. SEC4SR incorporates 4 white-box and 2 black-box attacks, 24 defenses including our novel feature-level transformations. It also contains techniques for mounting adaptive attacks. Using SEC4SR, we conduct thus far the largest-scale empirical study on adversarial attacks and defenses in SR, involving 23 defenses, 15 attacks and 4 attack settings. Our study provides lots of useful findings that may advance future research: such as (1) all the transformations slightly degrade accuracy on benign examples and their effectiveness vary with attacks; (2) most transformations become less effective under adaptive attacks, but some transformations become more effective; (3) few transformations combined with adversarial training yield stronger defenses over some but not all attacks, while our feature-level transformation combined with adversarial training yields the strongest defense over all the attacks. Extensive experiments demonstrate capabilities and advantages of SEC4SR which can benefit future research in SR.      
### 77.F3S: Free Flow Fever Screening  [ :arrow_down: ](https://arxiv.org/pdf/2109.01733.pdf)
>  Identification of people with elevated body temperature can reduce or dramatically slow down the spread of infectious diseases like COVID-19. We present a novel fever-screening system, F3S, that uses edge machine learning techniques to accurately measure core body temperatures of multiple individuals in a free-flow setting. F3S performs real-time sensor fusion of visual camera with thermal camera data streams to detect elevated body temperature, and it has several unique features: (a) visual and thermal streams represent very different modalities, and we dynamically associate semantically-equivalent regions across visual and thermal frames by using a new, dynamic alignment technique that analyzes content and context in real-time, (b) we track people through occlusions, identify the eye (inner canthus), forehead, face and head regions where possible, and provide an accurate temperature reading by using a prioritized refinement algorithm, and (c) we robustly detect elevated body temperature even in the presence of personal protective equipment like masks, or sunglasses or hats, all of which can be affected by hot weather and lead to spurious temperature readings. F3S has been deployed at over a dozen large commercial establishments, providing contact-less, free-flow, real-time fever screening for thousands of employees and customers in indoors and outdoor settings.      
### 78.Revisiting 3D ResNets for Video Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.01696.pdf)
>  A recent work from Bello shows that training and scaling strategies may be more significant than model architectures for visual recognition. This short note studies effective training and scaling strategies for video recognition models. We propose a simple scaling strategy for 3D ResNets, in combination with improved training strategies and minor architectural changes. The resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0 on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated in a self-supervised setup using contrastive learning, demonstrating improved performance. Code is available at: <a class="link-external link-https" href="https://github.com/tensorflow/models/tree/master/official" rel="external noopener nofollow">this https URL</a>.      
### 79.Multimodal Detection of COVID-19 Symptoms using Deep Learning &amp; Probability-based Weighting of Modes  [ :arrow_down: ](https://arxiv.org/pdf/2109.01669.pdf)
>  The COVID-19 pandemic is one of the most challenging healthcare crises during the 21st century. As the virus continues to spread on a global scale, the majority of efforts have been on the development of vaccines and the mass immunization of the public. While the daily case numbers were following a decreasing trend, the emergent of new virus mutations and variants still pose a significant threat. As economies start recovering and societies start opening up with people going back into office buildings, schools, and malls, we still need to have the ability to detect and minimize the spread of COVID-19. Individuals with COVID-19 may show multiple symptoms such as cough, fever, and shortness of breath. Many of the existing detection techniques focus on symptoms having the same equal importance. However, it has been shown that some symptoms are more prevalent than others. In this paper, we present a multimodal method to predict COVID-19 by incorporating existing deep learning classifiers using convolutional neural networks and our novel probability-based weighting function that considers the prevalence of each symptom. The experiments were performed on an existing dataset with respect to the three considered modes of coughs, fever, and shortness of breath. The results show considerable improvements in the detection of COVID-19 using our weighting function when compared to an equal weighting function.      
### 80.Reinforcement Learning for Battery Energy Storage Dispatch augmented with Model-based Optimizer  [ :arrow_down: ](https://arxiv.org/pdf/2109.01659.pdf)
>  Reinforcement learning has been found useful in solving optimal power flow (OPF) problems in electric power distribution systems. However, the use of largely model-free reinforcement learning algorithms that completely ignore the physics-based modeling of the power grid compromises the optimizer performance and poses scalability challenges. This paper proposes a novel approach to synergistically combine the physics-based models with learning-based algorithms using imitation learning to solve distribution-level OPF problems. Specifically, we propose imitation learning based improvements in deep reinforcement learning (DRL) methods to solve the OPF problem for a specific case of battery storage dispatch in the power distribution systems. The proposed imitation learning algorithm uses the approximate optimal solutions obtained from a linearized model-based OPF solver to provide a good initial policy for the DRL algorithms while improving the training efficiency. The effectiveness of the proposed approach is demonstrated using IEEE 34-bus and 123-bus distribution feeders with numerous distribution-level battery storage systems.      
### 81.Artificial Intelligence in Dry Eye Disease  [ :arrow_down: ](https://arxiv.org/pdf/2109.01658.pdf)
>  Dry eye disease (DED) has a prevalence of between 5 and 50\%, depending on the diagnostic criteria used and population under study. However, it remains one of the most underdiagnosed and undertreated conditions in ophthalmology. Many tests used in the diagnosis of DED rely on an experienced observer for image interpretation, which may be considered subjective and result in variation in diagnosis. Since artificial intelligence (AI) systems are capable of advanced problem solving, use of such techniques could lead to more objective diagnosis. Although the term `AI' is commonly used, recent success in its applications to medicine is mainly due to advancements in the sub-field of machine learning, which has been used to automatically classify images and predict medical outcomes. Powerful machine learning techniques have been harnessed to understand nuances in patient data and medical images, aiming for consistent diagnosis and stratification of disease severity. This is the first literature review on the use of AI in DED. We provide a brief introduction to AI, report its current use in DED research and its potential for application in the clinic. Our review found that AI has been employed in a wide range of DED clinical tests and research applications, primarily for interpretation of interferometry, slit-lamp and meibography images. While initial results are promising, much work is still needed on model development, clinical testing and standardisation.      
### 82.A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences  [ :arrow_down: ](https://arxiv.org/pdf/2109.01132.pdf)
>  Purpose: Echocardiography is commonly used as a non-invasive imaging tool in clinical practice for the assessment of cardiac function. However, delineation of the left ventricle is challenging due to the inherent properties of ultrasound imaging, such as the presence of speckle noise and the low signal-to-noise ratio. Methods: We propose a semi-automated segmentation algorithm for the delineation of the left ventricle in temporal 3D echocardiography sequences. The method requires minimal user interaction and relies on a diffeomorphic registration approach. Advantages of the method include no dependence on prior geometrical information, training data, or registration from an atlas. Results: The method was evaluated using three-dimensional ultrasound scan sequences from 18 patients from the Mazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual delineations provided by an expert cardiologist and four other registration algorithms. The segmentation approach yielded the following results over the cardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff distance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02). Conclusions: The method performed well compared to the four other registration algorithms.      
### 83.Towards disease-aware image editing of chest X-rays  [ :arrow_down: ](https://arxiv.org/pdf/2109.01071.pdf)
>  Disease-aware image editing by means of generative adversarial networks (GANs) constitutes a promising avenue for advancing the use of AI in the healthcare sector. Here, we present a proof of concept of this idea. While GAN-based techniques have been successful in generating and manipulating natural images, their application to the medical domain, however, is still in its infancy. Working with the CheXpert data set, we show that StyleGAN can be trained to generate realistic chest X-rays. Inspired by the Cyclic Reverse Generator (CRG) framework, we train an encoder that allows for faithfully inverting the generator on synthetic X-rays and provides organ-level reconstructions of real ones. Employing a guided manipulation of latent codes, we confer the medical condition of cardiomegaly (increased heart size) onto real X-rays from healthy patients. This work was presented in the Medical Imaging meets Neurips Workshop 2020, which was held as part of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020) in Vancouver, Canada      
